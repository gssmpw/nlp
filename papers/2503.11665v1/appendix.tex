\newpage
\begin{comment}
\section{Flexible Data Placement: Additional Information}
\label{sec:appendix:fdp:details}
\subsection{Reclaim Unit Handle (RUH) Types}
The FDP interface defines two types of reclaim unit handles that provide different data movement guarantees upon garbage collection with their associated trade-offs. During garbage collection, the RUH type is used to determine the source and destination RUs of data to be moved. FDP defines two RUH types namely:
\begin{enumerate}[leftmargin=*]
    \item \textbf{Initially Isolated} - All the reclaim units within a reclaim group pointed to by the RUHs of this type are candidates for data movement. For multiple RUHs of initially isolated type, data starts off being isolated from data written using another RUH of initially isolated type. However, upon garbage collection valid data written using these two handles can be intermixed. This type is the cheapest to implement on the SSD controller as it does not require explicit tracking of data written using RUHs and imposes the least constraints on data movement during garbage collection. 
    \item \textbf{Persistently Isolated} - All the reclaim units within a reclaim group that have been written utilizing the RUH are the only candidates for data movement upon garbage collection. This RUH type provides a stronger guarantee of data isolation but is expensive to implement on the controller as it requires explicit tracking of data written using RUHs and imposes more constraints on data movement during garbage collection.
\end{enumerate}

\minisec{Example} Consider a write pattern using two RUHs, RUH0 and RUH1 where RUH0 has written LBAs to RU0 and RU1 while RUH1 has written LBAs to RU2. For simplicity, let us assume that all the RUs belong to the same reclaim group. If RUH0 and RUH1 are of initially isolated type, then upon garbage collection valid data from RU0, RU1 and RU2 are candidates for movement and can be intermixed. If RUH0 and RUH1 are of persistently isolated type, then only data from RU0 and RU1 can be intermixed upon garbage collection while the data in RU2 is isolated from data in RU0 and RU1.
\end{comment}

\begin{comment}
\minisec{Placement Handle} This is an FDP spec construct and is not to be confused with the Placement Handle abstract concept discussed in the Design section of the paper. This is a namespace-scoped handle that maps to an RUH. For example, the namespace1 chooses to map only RUH0, RUH4 and RUH5 out of the available 8. Then these RUHs(0, 4 and 5) are mapped as Placement Handles 0, 1 and 2 in namespace1.

\minisec{Placement Identifier} This specifies a Reclaim Group and Placement Handle. The host uses placement identifiers in the FDP-enabled NVMe IOs to reference a Reclaim Unit while writing to a namespace.
\end{comment}

\section{Theoretical Model of DLWA in FDP-enabled CacheLib}
\subsection{System Model: Assumptions and Observations}
\label{ref:appendix:theoretical-model}
We make the following key observations and assumptions about CacheLib's Flash cache that form the basis of our theoretical model.
\begin{itemize}[noitemsep, topsep=1pt, partopsep=0pt,leftmargin=*]
    \item Items are inserted into the LOC in a log-structured manner where evictions occur using a FIFO policy. This results in a purely sequential write pattern of LBAs to the SSD. 
    \item With FDP, the LOC writes are written into a separate RU and physical block. The purely sequential write pattern of LOC and its segregation into a separate physical block and RU means that LOC data invalidates itself nicely and contributes to negligible (or no) live data movement. 
    \item Items are inserted into the SOC buckets using a uniform hash function. Every SOC item insertion results in the entire hashed bucket (a 4KB page) being written. While a modulo is not purely sequential, for this model we assume that the SOC generates a uniform random write pattern. 
    \item With FDP, all SOC data is segregated into a separate RU and physical block. The random nature of SOC writes means that its data contributes to live data movement.
    \item As the LOC writes are sequential and will not need any live data movement, it is safe to assume that only SOC writes contribute to device-level write amplification.
    \item The SSD has some amount of overprovisioning space.
    \item In the case of FDP, since the physical blocks and RUs that contain LOC data free themselves up due to the sequential write pattern, we assume that the overprovisioned space can be entirely used by the writes from SOC data.
\end{itemize}

\begin{table}[!t]
    \centering    
    \begin{tabular}{ll}
    \toprule
        \textbf{Variable} & \textbf{Description} \\
        \midrule
        $\text{N}_{\text{B}}$ & Number of SOC Buckets \\
        $\text{N}_{\text{BB}}$  & Number of SOC Buckets per Erase Block \\
        $\text{S}_{\text{SOC}}$ & Total SOC logical space  \\
        $\text{S}_{\text{Bucket}}$ & SOC bucket size \\
        $\text{S}_{\text{Total}}$ & Total Physical Space \\
        $\text{S}_{\text{Usable}}$ & Total Usable Physical Space \\
        $\text{S}_{\text{OP}}$ & Total OP Space \\
        $\text{S}_{\text{LOC}}$ & Total LOC logical Space \\
        $\text{S}_{\text{P-LOC}}$ & Total Physical Space for LOC \\
        $\text{S}_{\text{P-SOC}}$ & Total Physical Space for SOC data \\
        $\text{S}_{\text{NVM}}$ & Total logical space used for the NVM Cache \\
        $\delta$ & Avg. live SOC bucket migrations due to GC \\
        \bottomrule
    \end{tabular}
    \caption{System Variables}
    \label{tab:params}
\end{table}

\subsection{Derivation}
We follow the same methodology as proposed in ~\cite{DayanBB15} to model the DLWA of SOC writes in the SSD. The SOC DLWA equates to the overall CacheLib DLWA because the write amplification of LOC data after data segregation with FDP is $\sim$1.

The total number of SOC buckets can be calculated as:
\begin{equation}
   \text{N}_{\text{B}} =   \frac{\text{S}_{\text{SOC}}}{\text{S}_{\text{Bucket}}}
\end{equation}
Items are inserted into buckets using a \% $\text{N}_{\text{B}}$. As the bucket size increases, the total number of SOC buckets reduces. \\

The total physical space in the SSD can be given by:
\begin{equation}  \label{eq:total_space}
    \text{S}_{\text{Total}} = \text{S}_{\text{Usable}} + \text{S}_{\text{OP}}
\end{equation}

We assume that LOC data will use $\sim$0 OP space, i.e.
\begin{equation} \label{eq:loc_assumption}
   \text{S}_{\text{LOC}} = \text{S}_{\text{P-LOC}}
\end{equation} 

Using Equation \ref{eq:total_space} and Equation \ref{eq:loc_assumption}, and assuming the entire usable space will make up the NVM Cache capacity (i.e. no host overprovisioning),

\begin{equation} \label{eq:soc_assumption}
    \text{S}_{\text{P-SOC}} = \text{S}_{\text{SOC}} + \text{S}_{\text{OP}}
\end{equation}

The LBA space of the SOC data i.e. $\text{S}_{\text{SOC}}$ is uniformly distributed. We assume that the number of SOC buckets in an erase block is $\text{N}_{\text{BB}}$. Then, it follows that after X SOC insertions the probability that a particular SOC bucket gets updated in an erase block is:
\begin{equation} \label{eq:prob_soc_re_write}
    \text{p(SOC bucket rewrite)} = \frac{\text{N}_{\text{BB}}}{\text{S}_{\text{SOC}}}
\end{equation}

This follows a geometric distribution. This is similar to other DLWA modelling work done before~\cite{DayanBB15}. 
In a geometric distribution, after X trials we get a success of a particular bucket getting overwritten or updated. In a geometric distribution, the mean is $\frac{1}{p}$. So,

\begin{equation}\label{eq:geo_mean}
    \mu = \frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}}}
\end{equation}

Equation \ref{eq:geo_mean} is analogous to saying that on average after $\frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}}}$ updates/SOC writes, the first SOC bucket gets overwritten in a particular erase block. Similarly, the second SOC bucket gets overwritten after  $\frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}} - 1}$ SOC inserts on average, and so on. \\ It follows that the number of updates after which there are no SOC buckets yet to be overwritten is,

\begin{equation}\label{eq:euler}
    \frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}}} + \frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}} - 1} + \frac{\text{S}_{\text{SOC}}}{\text{N}_{\text{BB}} -2} + ... + \frac{\text{S}_{\text{SOC}}}{1}
\end{equation}

This harmonic series can be simplified using Euler's approximation. 
\begin{equation}\label{eq:euler_simple}
    \text{X} =  \text{S}_{\text{SOC}} \sum_{i = 1}^{\text{N}_{\text{BB}}} \frac{1}{i}
\end{equation}

If $\text{L}$ SOC buckets are yet to be overwritten on average in an erase block after X SOC insertions. Then,
\begin{equation}\label{eq:alive_soc_buckets}
     \text{X} = \text{S}_{\text{SOC}} \sum_{i = 1}^{\text{N}_{\text{BB}}} \frac{1}{i} - \text{S}_{\text{SOC}} \sum_{i = 1}^{L} \frac{1}{i}
\end{equation}

This can be expressed as:
\begin{equation}\label{eq:alive_soc_buckets_simple}
    \text{L} = \text{N}_{\text{BB}} e^{- X/\text{S}_{\text{SOC}}}
\end{equation}

We now factor in the overprovisioned space available to the SOC data during SSD garbage collection operations to cushion the DLWA. \\

Say the average SOC buckets that remain valid in an erase block when GC is triggered is $\delta$. Then each GC operation on average involves $\text{N}_{\text{BB}} \times \delta$ migrations. It also follows that the number of SOC bucket writes or insertions that can be accommodated due to this migration is $\text{N}_{\text{BB}} \times (1- \delta) $. \\
We assume a greedy GC policy (i.e. the erase block with least valid pages will be picked first for GC). Say the expected number of GC operations between two successive victim selections of the same block is $\text{G}$. Given the uniform workload pattern we can see that, 
\begin{equation} \label{eq:pba}
    \text{G} = \frac{\text{S}_{\text{P-SOC}}}{\text{N}_{\text{BB}}}
\end{equation}
This can be interpreted as follows, 
\begin{itemize}
    \item We have $\frac{\text{S}_{\text{P-SOC}}}{\text{N}_{\text{BB}}}$ number of erase blocks holding SOC data in the SSD.
    \item Once an erase block is picked, on average it is picked after $\frac{\text{S}_{\text{P-SOC}}}{\text{N}_{\text{BB}}}$ GC operations.
\end{itemize}

As $\text{G}$ erase blocks are picked for GC before the same block is picked again, $ \text{G} \times \text{N}_{\text{BB}} \times (1- \delta) $ SOC writes occur on average between two GC operations on the same erase block. \\

Using this in Equation \ref{eq:alive_soc_buckets_simple} and Equation \ref{eq:pba} we get,

\begin{equation}\label{eq:lba_pba}
    \frac{\text{S}_{\text{SOC}}}{\text{S}_{\text{P-SOC}}} = \frac{\delta - 1}{\text{ln}(\delta)}
\end{equation}

Equation \ref{eq:lba_pba} can be simplified using the Lambert W function ~\cite{DayanBB15, Desnoyers12, StoicaA13} as follows:

\begin{equation}\label{eq:final}
    \delta = - \frac{\text{S}_{\text{SOC}}}{\text{S}_{\text{P-SOC}}} \times \mathcal{W} (- \frac{\text{S}_{\text{P-SOC}}}{\text{S}_{\text{SOC}}} \times e^{- \frac{\text{S}_{\text{P-SOC}}}{\text{S}_{\text{SOC}}}})
\end{equation}

The SSD DLWA can be calculated using $\delta$ as,
\begin{equation}\label{eq:WAF}
    DLWA = \frac{1}{1 - \delta}
\end{equation}

\subsection{Validation of the DLWA Model with FDP-enabled CacheLib Empirical Result}
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.70\linewidth]{svg_to_pdf/eurosys_formula_vs_actual.pdf}
  \caption{DLWA obtained from experiments using the KV Cache workload at 100\% device utilization, 42GB RAM and varying SOC size in comparison to the DLWA obtained from the formula. We see minimal error in DLWA estimation using the formula.} 
  \label{fig:modelling}
  %\vspace{-3ex}
\end{figure}

Figure \ref{fig:modelling} shows the minimal error in estimating CacheLib's DLWA using the DLWA model (Section \ref{theorem:DLWA}) in comparison to the observed DLWA obtained from experiments with the KV Cache workload. \edit{We see that at high SOC values the model diverges by a maximum of \textasciitilde16\% from the observed DLWA. We observe that at high SOC values, the model diverges from the empirical results because it assumes a uniform distribution of keys to SOC buckets. This is actually not the case due to skew causing the observed DLWA to be lower than the predicted DLWA. Note that we ran this experiment at 100\% device utilization. At lower device utilization, the error will be similar to low SOC values due to low DLWA.}

\section{FDP-based Segregation Benefits with WO KV Cache Workload}
\begin{figure*}[!t]
        \begin{minipage}{0.95\linewidth}
        \hspace{-1em}
        \centerline{\includegraphics[width=0.95\linewidth]{svg_to_pdf/eurosys_wo_kv_cache_waf_lat_sep_3util_kv_cache.pdf}}
        \caption{Effect of varying SSD utilization for caching with WO KV Cache Workload on DLWA and p99 read and write latency. FDP-based segregation results in a DLWA of 1 without affecting performance irrespective of device utilization. At higher utilizations, FDP improves p99 read and write latency.} 
        \label{fig:waf:wo-kvcache-varying-util}
    \end{minipage}
%\vspace{-3ex}
\end{figure*}

Figure \ref{fig:waf:wo-kvcache-varying-util} shows the observed DLWA, p99 read and write latencies with FDP-enabled CacheLib using the WO KV Cache workload across different device utilization. We see similar trends as observed before with other workloads  in Section~\ref{sec:eval:waf:kv-varying-util} and Section~\ref{sec:eval:waf:varying-workloads} i.e., increasing gains in DLWA and lowering of p99 read and write latency at higher device utilizations. At 100\% device utilization, FDP-based data segregation obtains 3.5x gains in DLWA, 2.2x gains in p99 read latency, and 9.5x gains in p99 write latency.

