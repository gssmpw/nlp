\section{Related Work}
\label{sec:related-work}
\subsection{Data Placement in SSDs}
Write amplification in Flash-based SSDs____ is a well studied problem that has received attention through various data placement proposals over the past years. To tackle device-level write amplification (DLWA), SSD controllers use various heuristics to segregate data based on its characteristics (e.g., access patterns, temperature, etc.) on Flash media to minimize garbage collection and data movement costs____. There have been various proposals in the NVMe data placement space for cooperation between host and FTL to leverage host application domain knowledge. One approach is to pass hints to the FTL as proposed in Multi-Streamed SSDs____. Another contrary approach proposed in Open-Channel SSDs____ and DFS____ is to expose the SSD internals completely paving the way for host-based FTLs. There have also been software-defined Flash proposals to expose NAND channels in SDF____ and to explore alternative storage abstractions that can be leveraged by key-value stores____. 

Zoned Namespaces (ZNS)____ was a follow-up on Open-Channel SSD 2.0 specification designed to leverage existing support of SMR HDDs. Despite impressive DLWA results, the append-only write model and host-based garbage collection imposes upfront software engineering costs for applications that do not conform to log-structured access patterns. This has posed a challenge for the wide adoption of ZNS. FDP TP____ was proposed based on lessons learned from the past. It consolidates Google's SmartFTL____ and Meta's Direct Placement Mode proposals to fill the cost-benefit gap between ZNS and conventional SSDs. The concept of RUHs in FDP borrows heavily from the concept of streams in multi-streamed SSDs. FDP was crafted with backward compatibility in mind, enabling an application storage stack to operate seamlessly without modifications. Additionally, applications have the option to harness FDP features by opting to enable them. FDP also does not introduce any new command sets. In this work, we have leveraged FDP features for data placement using Linux kernel I/O Passthru features____. The backward compatibility of FDP and its ease of integration into the CacheLib software have been key drivers in the upstreaming process of our work.

\subsection{Key-Value Stores and Hybrid Caching}
A lot of research effort has been dedicated to the design of key-value stores____ that conform to the performance tradeoffs of Flash-based SSDs____. Their inadequacy for hybrid caching use cases at Meta that are dominated by random writes of small objects causing severe DLWA has been previously highlighted____. Log-structured caches____ designed for Flash to minimize write amplification suffer from DRAM overhead issues (especially for numerous small items) which formed the motivation for the work in Kangaroo____. Our present work is complementary to these efforts because we keep the cache architecture and design of CacheLib____ unchanged and leverage FDP features for data placement in its I/O write paths to minimize DLWA. \edit{FairyWren____ extended Kangaroo to integrate ZNS devices for lower DLWA. We observe similar DLWA and carbon emission gains through data placement alone, without modifying the original architecture and design of CacheLib.}