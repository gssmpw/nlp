\section{Background}
\label{sec:background}
In this section, we highlight important concepts to facilitate a better understanding of the rest of the paper. We provide a summary of how SSDs work, the challenges associated with Flash caches, and the architecture of CacheLib~\cite{berg2020cachelib}.
\move{
\subsection{SSDs and Write Amplification}
\minisec{SSD Basics} A SSD NAND package is organised into dies, planes, blocks, and pages ~\cite{ssd_agarwal}. NAND SSDs cannot be directly overwritten due to the erase before write property. The erase operation happens in terms of erase blocks (EBs) (tens to hundreds of MBs) while the writes happen in terms of pages (16KB, 48KB, 64 KB, etc.). The Flash Translation Layer (FTL) in SSDs handles the overwrites by (1) writing (programming) the new data in a free page, (2)  invalidating the old data, and (3) updating the metadata to point to the new data. The metadata translates the logical addresses to physical addresses in the NAND media. Writing to logical addresses in an SSD creates invalid pages, that have to be reclaimed by a process called Garbage Collection (GC).

\minisec{SSD Garbage Collection (GC)} The garbage collection process is triggered whenever there is a scarcity of free blocks in the SSD. This process reads the remaining valid pages from an erase block and programs them to a new location. The now fully invalid erase block is available in the free pool. Any erase block in the free pool may be erased and programmed with the next incoming data to be written.  Garbage collection is an expensive operation and the energy consumed by the SSD is directly proportional to the number and duration of garbage collection operations.

\minisec{Device-level Write Amplification (DLWA)} DLWA is a metric used to quantify the amount of data that was written internally in the SSD compared to the actual amount of data sent by the host to the SSD. It can be calculated as follows:
%\vspace{-1ex}
\begin{equation}
    \text{DLWA} = \frac{\text{Total NAND Writes}}{\text{Total SSD Writes}}
\end{equation}

\minisec{Application-level Write Amplification (ALWA)} ALWA is a metric used to quantify the amount of data that was sent to the SSD to be written compared to the actual amount of data received to be written by the application. It can be calculated as follows:
%\vspace{-1ex}
\begin{equation}
    \text{ALWA} = \frac{\text{Total SSD Writes}}{\text{Total Application Writes}}
\end{equation}

\minisec{Importance of DLWA} The additional reads and writes from garbage collection interfere with the processing of other commands in the SSD affecting the QoS. Moreover, the additional NAND activity will consume the limited endurance of NAND media. A DLWA of 2 implies that for every 4 KB of data that the user writes, the FTL has written an extra 4 KB due to garbage collection. Since NAND media has a fixed number of Program and Erase cycles (P/E cycles) after which it can either only be read or becomes faulty, a DLWA of 2 causes the device's lifetime to be halved. Device-level write amplification impacts other SSD performance metrics, such as QoS, bandwidth, lifetime, reliability, and power consumption. It is often used as a simple proxy metric for monitoring SSD performance.
}
\subsection{DLWA and Carbon Emissions}
The lifetime of an SSD is inversely proportional to the device-level write amplification ~\cite{tbw_liftime}. A DLWA of 2 causes the SSD to fail twice as fast compared to DLWA of 1. A high DLWA results in premature SSD failure and requires frequent replacement of the device. \textit{SSD manufacturing produces millions of metric tonnes of CO2 emissions per year ~\cite{embodiedcarbon}. These emissions are broadly categorized as embodied carbon emissions.} With systems moving away from HDDs to SSDs, the need to reduce DLWA is crucial because the embodied carbon cost of SSDs is at least an order of magnitude larger than HDDs. Reduction of DLWA amortizes both capital costs and embodied carbon emissions of Flash-based systems at scale. 

High DLWA results from increased garbage collection operations to move valid pages to free up SSD blocks. Consequently, the SSD spends more time in the active state than in the idle state which results in a larger energy consumption~\cite{ssdpower, ssdenergy, sustainability-sdc}. Lower DLWA results in lower consumption of operational energy and translates to higher operational carbon efficiency. Although operational carbon efficiency optimization is important, big carbon efficiency gains are not expected from it. This is because SSDs are designed to be energy efficient and optimized to switch to idle state when not in use.

\subsection{Flash Caches and CacheLib}
\label{sec:background:flash-cache-cachelib}
\begin{figure}[!t]
    \centering
    \includegraphics[width=0.8 \linewidth]{svg_to_pdf/cachelib_drawio_latest.pdf}
    \caption{CacheLib Architecture Overview}
    %\vspace{-3ex}
    \label{fig:cachelib}
\end{figure}

\minisec{Caching in Flash} Caching is widely employed in large-scale web services to provide high performance and reduce operational costs. Flash-based SSDs have become popular for caching large working sets of popular web services~\cite{berg2020cachelib, EisenmanCPHSAK19:flashield, TangHLKL15:ripq, WongWMGLKSBBG24:baleen} due to their excellent price performance tradeoff compared to DRAM and HDD. Caching on Flash is write-intensive since evictions upon read from DRAM translate to writes on Flash. Writes to Flash caches increase with the size of the working set, churn in keys, and reduction of DRAM sizes in the deployment. However, the limited write endurance of Flash coupled with unpredictable workloads pose a challenge in the design of these caches. 

Recent research has investigated the design of caching algorithms, admission policies, and application-level write amplification to manage the limited device endurance of Flash while delivering high hit ratios but device-level write amplification (DLWA) has been an understudied problem in this area. \textit{As sustainability challenges mount, Flash-based SSDs will become increasingly attractive compared to DRAM to cache large working set sizes at acceptable performance. However, before claiming Flash-based SSDs to be a panacea for sustainability, DLWA of Flash caches needs to be studied}.

\minisec{CacheLib Architecture} \textit{CacheLib~\cite{berg2020cachelib} is an open-source caching library that is widely used and deployed as a fundamental caching building block by 100s of services at Meta}. It employs a hybrid cache architecture (see Figure ~\ref{fig:cachelib}) to leverage both DRAM and Flash-based SSD to cache data hierarchically. DRAM is used to cache the most popular items while the SSD caches data that is less popular and evicted from the DRAM cache. The SSD cache is further decomposed into a small object cache (SOC) and a large object cache (LOC) to store objects of different sizes. The threshold for characterizing objects as small or large is configurable at deployment, along with the sizes of the DRAM and SSD. A single instance of CacheLib can consist of multiple DRAM and SSD cache engines, each with their configured resource budgets.

The SOC employs a set-associative cache design to perform in-place SSD writes in terms of buckets (typically aligned with 4 KB page size) and utilizes a uniform hashing function to minimize the overhead of tracking numerous small objects. The LOC employs a log-structured design to perform SSD-friendly writes in terms of large regions (16 MB, 256 MB, etc.) that align with erase block sizes. The LOC can be configured to use FIFO or LRU eviction policies. The strengths and weaknesses of the SOC and LOC complement each other. The LOC has SSD-friendly write patterns but has DRAM overheads for tracking objects, while the SOC has SSD-unfriendly write patterns and almost no overhead for tracking objects.\\  

\minisec{Challenges in Production Flash Caches and CacheLib}
The central challenge of Flash caches deployed in the wild is to manage the limited endurance of Flash while ensuring a high hit ratio and low indexing overhead. They have to deal with mixed workloads with objects of varying sizes and access patterns. \textbf{Large caching services typically handle billions of frequently accessed small items and millions of infrequently accessed large items ~\cite{berg2020cachelib, mcallister2021kangaroo, twemcache}}. The use of host overprovisioning and threshold admission policy is common for reducing DLWA. \textbf{In production CacheLib deployments, 50\% of the Flash capacity is overprovisioned to keep DLWA within acceptable levels of \textasciitilde1.3}~\cite{mcallister2021kangaroo, berg2020cachelib}. Increasing utilization of Flash with low DLWA is crucial for sustainable deployments of Flash caches at scale in the future.

\vspace{-2ex}