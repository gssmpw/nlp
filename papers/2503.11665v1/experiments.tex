\section{Evaluation}
\label{sec:evaluation}
In this section, we explore the benefits and limitations of segregating data in the small object cache and the large object cache of CacheLib on FDP SSDs. 
\subsection{Experimental Setup}
This section describes the setup of the system, the workloads for our experiments, and the metrics used in our evaluation. \\
\minisec{Hardware and Software Setup}
We use two different machines in our experimental setup with similar hardware characteristics. Both machines have two 24-core Intel Xeon Gold 6432 processors with \textasciitilde 528 GB of DRAM. Each machine uses a 1.88 TB Samsung PM9D3 SSD with a firmware version that supports the FDP specification. The FDP configuration on the device supports 2 namespaces, 1 RG and 8 initially isolated RU handles that can be used concurrently. For all experiments, we create a single namespace and map all the RU handles to it. Each RU is \textasciitilde 6 GB in size. One of the machines runs Ubuntu 22.04 with a 6.1.64 Linux kernel, while the other machine runs CentOS 9 with a 6.1.53 Linux kernel. We use \texttt{nvme-cli}~\cite{nvme-cli} version 2.7.1 for configuring FDP features on the SSD.\\
\minisec{System Comparisons}
For our experiments, we use the main branch of the CacheLib repository which contains our upstreamed FDP-based data placement changes to segregate SOC and LOC \cite{fdp-pr, cachelib-repo}. We use \texttt{nvme-cli} to enable and disable FDP features on the controller to contrast between a conventional SSD and an FDP SSD. In the rest of the paper, we use the following terms to highlight the system comparisons under test,
\begin{enumerate}[noitemsep, topsep=1pt, partopsep=0pt]
    \item \textbf{FDP:} FDP-based data segregation enabled in CacheLib and FDP configuration enabled on the SSD.
    \item \textbf{Non-FDP:} FDP-based data segregation disabled in CacheLib and FDP configuration disabled on the SSD.
\end{enumerate}
We use the CacheBench workload generator and trace replaying tool~\cite{cachebench} to run the workloads. CacheBench is an application that invokes the CacheLib cache API in the same process and can be used to run captured traces or generate benchmarks. \edit{All the scripts used to run experiments are available publicly~\cite{cachelib-devops}.}\\
\minisec{Metrics}
We focus on DLWA as the primary metric to evaluate the efficacy of our data segregation changes since it has a direct correlation to endurance and embodied carbon emissions. We measure DLWA by using the \texttt{nvme-cli} tool to query log pages (\texttt{nvme get-log}) from the SSD controller that tracks the host bytes written to it and the device bytes that were written on NAND media over an interval of 10 minutes. We reset the SSD to a clean state before every experiment by issuing a TRIM for the entire device size. We use the CacheBench tool to measure and report throughput, latency, DRAM and NVM cache hit ratios, and ALWA for our experiments.\\
\minisec{Workloads}
Our experiments use sampled 5 day anonymized traces from Meta's key-value cache (KV Cache) cluster \cite{berg2020cachelib, cachelib-traces} and 7-day anonymized traces from Twitter's cluster12 \cite{twemcache} that are publicly available for research and experimentation. \textbf{KV Cache} is a read-intensive workload where the GETs outnumber SETs by a 4:1 ratio. For KV Cache, we use \textasciitilde42 GB of DRAM and \textasciitilde930 GB of SSD (50\% device utilization) for caching as the default setup. Twitter's cluster12 workload is write-intensive, where the SETs outnumber GETs by a 4:1 ratio. For the \textbf{Twitter workload}, we use \textasciitilde16 GB of DRAM and \textasciitilde930 GB of SSD (50\% device utilization) as the default setup. To stress the SSDs more and generate high DLWA scenarios in a shorter duration, we generated an additional write-only KV cache workload by removing the GET operations from the KV cache trace so it almost exclusively consists of SET operations. We refer to it as the \textbf{WO KV Cache} workload and use the same DRAM and SSD configurations as mentioned for KV Cache. 

%\vspace{-4ex}
\subsection{FDP-based segregation achieves a DLWA of $\sim$1}
\label{sec:eval:kv-cache:waf-50}
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\columnwidth]{svg_to_pdf/eurosys_kv_cache_waf_interval_50.pdf}
  \caption{DLWA over 60 hours with the KV Cache workload using 50\% device utilization, 42GB of RAM and 4\% SOC size. FDP-based segregation results in a 1.3x reduction in DLWA.} 
  \label{fig:waf:kv-waf-50}
  \vspace{-3ex}
\end{figure}
We run the KV Cache workload with the default DRAM cache size of \textasciitilde42 GB and SSD cache size of \textasciitilde930 GB out of a total size of 1.88 TB SSD for an effective utilization of 50\%. The SOC size in the default configuration is set to 4\% of the SSD size. Figure \ref{fig:waf:kv-waf-50} shows the interval DLWA over a run duration of more than 2 days. We can see that the SOC and LOC data segregation into two different reclaim unit handles helps to lower DLWA from 1.3 observed without data segregation to 1.03. This validates our analysis that segregating the sequential and cold write pattern of LOC from the random and hot write pattern of SOC by utilizing device overprovisioning helps control DLWA. \textit{Thus, we achieve an ideal DLWA of $\sim$1 with the FDP-based segregation in CacheLib.}

%\vspace{-2ex}
\subsection{FDP-based segregation enables better SSD utilization without affecting performance}
\label{sec:eval:waf:kv-varying-util}
\begin{figure*}[!t]
        \begin{minipage}{0.9\linewidth}
        \hspace{-1em}
        \centerline{\includegraphics[width=0.95\linewidth]{svg_to_pdf/eurosys_waf_lat_sep_3util_kv_cache.pdf}}
        \caption{Effect of varying SSD utilization for caching with KV Cache Workload on DLWA and other CacheLib performance metrics like throughput, p99 read and write latency, and DRAM and SSD cache hit ratios. FDP-based segregation results in a DLWA of 1 without affecting performance irrespective of device utilization. At higher utilizations, FDP improves p99 read and write latency.} 
        \label{fig:waf:kvcache-varying-util}
    \end{minipage} \hfill    
%\vspace{-1ex}
\end{figure*}
Figure \ref{fig:waf:kvcache-varying-util} shows the impact of varying the SSD capacity used for caching upon various important metrics in CacheLib. This experiment quantifies the effect of increasing the host-level utilization of the SSD for caching. We see that the DLWA increases from 1.3 at 50\% utilization to 3.5 at 100\% utilization when data segregation is not employed, but the DLWA remains unchanged at \textasciitilde1.03 across utilizations when data segregation is employed. We omit presentation of data points for utilization between 50\% and 90\% for brevity because they are similar to 50\%. This result validates our analysis of device overprovisioning and the SOC invalidation rate being key factors affecting the DLWA. At 4\% SOC size, the collision rate of SOC buckets is very high. This results in a high invalidation rate of SOC data in the SSD. As SOC data is segregated from LOC data, this high invalidation rate will result in minimal valid data movement due to garbage collection. Furthermore, the SSD device overprovisioning capacity, which ranges from 7-20\% of SSD capacity, can now be used exclusively by SOC data to reduce the impact of garbage collection. Since the random writes of SOC constitute only 4\% of the SSD capacity, their impact even at 100\% device utilization is absorbed by the extra free blocks reserved by the the SSD. \textit{We can see that the throughput, DRAM and NVM cache hit ratio metrics remain unchanged with FDP-based segregation compared to the baseline. The p99 read and write latency also show improvement with increasing utilization due to lower interference from the garbage collection process.} At 100\% device utilization, the p99 read and write latency shows an improvement of 1.75X and 10X respectively. \textbf{Since we made no changes to how data is stored in SOC and LOC, we did not expect to see any change in the ALWA which we confirmed from the CacheBench logs.}

\subsection{FDP-based segregation lowers DLWA with other write-intensive workloads}
\label{sec:eval:waf:varying-workloads}
\begin{figure*}[!t]
    \centering
    \begin{minipage}{\columnwidth}
        \includegraphics[width=0.46\linewidth]{svg_to_pdf/eurosys_twitter_waf_interval_50.pdf}
        %\caption{a}
        \hfill
        \includegraphics[width=0.46\linewidth]{svg_to_pdf/eurosys_twitter_waf_interval_100.pdf}
    
        \caption{DLWA over 60 hours with the Twitter cluster 12 workloads with 16GB RAM, 4\% SOC size. a.) 50\% device utilization and b.) 100\% device utilization. FDP-based segregation achieves a DLWA of 1.}
        \label{fig:twt}
    \end{minipage}\hfill
    \begin{minipage}{\columnwidth}
    \includegraphics[width=0.46\linewidth]{svg_to_pdf/eurosys_wo_kv_cache_waf_interval_50.pdf}
    \hfill
    \includegraphics[width=0.46\linewidth]{svg_to_pdf/eurosys_wo_kv_cache_waf_interval_100.pdf}
        \caption{DLWA over 60 hours with the WO KV Cache workload with 42 GB RAM and 4\% SOC size. a.) 50\% device utilization and b.) 100\% device utilization. FDP-based segregation achieves a DLWA of 1.} \label{fig:wo-kv-waf}
    \end{minipage}
\end{figure*}
In previous sections, we demonstrated the effect of the FDP-based segregation on DLWA with the read-intensive KV Cache workload. We now explore the efficacy of our solution with the write-heavy Twitter cluster12 workload and the write-only KV Cache workload (WO KV Cache). We run experiments with both workloads at 50\% and 100\% device utilization, 4\% of SOC size and use $\sim$16GB and $\sim$42GB of DRAM for the Twitter and WO KV cache workloads respectively as used in past research~\cite{berg2020cachelib, mcallister2021kangaroo}. From Figure \ref{fig:twt} and \ref{fig:wo-kv-waf}, we can see that the DLWA trends observed with the KV Cache workload at 50\% and 100\% device utilizations remain consistent with the Twitter and WO KV Cache workloads as well. \textit{The FDP-based segregation achieves a DLWA of $\sim$1 with both these challenging write-intensive workloads dominant in small object accesses.}

\subsection{FDP-based segregation gains diminish with increase in SOC size}
\begin{figure}[!hb]
  \centering
  \includegraphics[width=0.8\linewidth]{svg_to_pdf/eurosys_vary_soc_bar.pdf}
  \caption{Average DLWA with the KV Cache workload using 100\% device utilization, 42GB of RAM and varying SOC size from 4\% to 96\% of the SSD size. FDP's DLWA gains diminish with an increase in the SOC size beyond the device overprovisioning size.} \label{fig:waf:kv-vary-soc}
  %\vspace{-3ex}
\end{figure}
In previous sections, we observed excellent DLWA behaviour for the KV cache workload primarily because our implementation segregates SOC's random and hot data from LOC's sequential and cold data. The small SOC size of 4\% paired with data segregation enabled a high invalidation of SOC data in the SSD and allowed device overprovisioning to efficiently cushion garbage collection of SOC data. To further validate our analysis, we study the impact of increase of SOC size (random writes) on DLWA beyond the device overprovisioning size, which is typically 7-20\% of SSD capacity. We can see from Figure \ref{fig:waf:kv-vary-soc} that when SOC size is increased from the default 4\% to 64\% the DLWA of our implementation increases from 1.03 to 2.5 while the DLWA without segregation remains above 3 for all SOC sizes. 

As the SOC size crosses the device overprovisioning size, the DLWA with FDP no longer remains 1. The high DLWA at larger SOC sizes occurs because the spare blocks are fewer than the SOC data blocks. Consequently, the spare blocks coming from the device overprovisioning space fail to provide an adequate cushion for the garbage collection of SOC data. \edit{Despite the lack of cushioning, we can see that data segregation is helpful in invalidating the SOC data blocks and reduces movement of LOC data upon garbage collection. At very high SOC sizes e.g., 90\% and 96\% we observe that data segregation does not yield any benefits. At very high SOC sizes, there is a high probability of erase blocks containing both valid and invalid SOC data. In this scenario, data intermixing might be beneficial since sharing of invalid SOC and LOC data in an erase block would minimize data movement. Moreover, garbage collection has a lower threshold when FDP is enabled compared to when it is disabled that may accentuate the DLWA of data segregation. We observe that increasing the SOC size does not benefit the cache behavior of the workload dominant in small objects since the hit ratio almost remains unchanged.}

\begin{comment}

 \begin{figure}[!t]
  \centering
  \subfloat[a][50\% device utilization]{\includegraphics[width=0.5\linewidth]{svg_to_pdf/eurosys_twitter_waf_interval_50.pdf} \label{fig:twt-50}} 
  \subfloat[b][100\% device utilization]{\includegraphics[width=0.5\linewidth]{svg_to_pdf/eurosys_twitter_waf_interval_100.pdf} \label{fig:twt-100}}
  \caption{DLWA over 60 hours with the Twitter cluster 12 workload with 16GB RAM, 4\% SOC size. a.) 50\% device utilization and b.) 100\% device utilization. FDP-based segregation achieves a DLWA of 1.} \label{fig:twt}
\end{figure}

\vspace{-3ex}

\begin{figure}[!t]
  \centering
  \subfloat[a][50\% device utilization]{\includegraphics[width=0.5\linewidth]{svg_to_pdf/eurosys_wo_kv_cache_waf_interval_50.pdf} \label{fig:wo-kv-50}} 
  \subfloat[b][100\% device utilization]{\includegraphics[width=0.5\linewidth]{svg_to_pdf/eurosys_wo_kv_cache_waf_interval_100.pdf} \label{fig:wo-kv-100}}
  \caption{DLWA over 60 hours with the WO KV Cache workload with 42 GB RAM and 4\% SOC size. a.) 50\% device utilization and b.) 100\% device utilization. FDP-based segregation achieves a DLWA of 1.} \label{fig:wo-kv-waf}
\end{figure}
\end{comment}

\subsection{FDP-based segregation enables cost-effective and carbon-efficient CacheLib deployments}
In previous sections, we showed large gains in DLWA from using FDP-based segregation in CacheLib. These gains translate to a longer SSD lifetime that leads to reductions in embodied carbon emissions. The DLWA gains also aid in improving the operational efficiency of SSDs due to fewer device garbage collection operations. This results in a reduction in operational carbon emissions. In this section, we discuss the promise of FDP as a sustainable solution to combat carbon emissions.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\linewidth]{svg_to_pdf/eurosys_co2e_gc_combined.pdf}
  \caption{Analysis of carbon savings on FDP vs Non-FDP with the KV Cache workload. a.) Embodied carbon emissions reduce drastically with FDP and b.) Garbage Collection events are reduced by a factor of $\sim$3.6 with FDP.} \label{fig:kv-co2e}
\end{figure}
%\vspace{-2ex}
\minisec{FDP-based segregation reduces carbon emissions}
To calculate the embodied emissions, we use Theorem 2 presented in Section \ref{theorem:embodied-co2e} which models the embodied carbon emissions as a function of DLWA, system lifecycle period, SSD warranty, and carbon emitted by the SSD manufacturing process. We use a system lifecycle and SSD warranty of 5 years and 0.16 CO2e (Kg) as the carbon emitted per GB of SSD manufactured~\cite{embodiedcarbon}. Figure \ref{fig:kv-co2e}a.) shows the embodied carbon emissions with and without FDP-based segregation in CacheLib. We can see that FDP enables substantial embodied carbon savings as a result of the DLWA gains. These embodied carbon gains are for a single SSD over a 5-year lifecycle. If we factor the deployment of 1000s of CacheLib clusters each consisting of 1000s of nodes, the embodied carbon emission gains from using FDP are massive.

The operational energy consumption of an SSD is directly proportional to the garbage collection events (see Theorem 3 in Section \ref{theorem:embodied-co2e}). Utilizing FDP's \texttt{Media Relocated} Event in the SSD log~\cite{fdp_tp} we calculate the total number of garbage collection events that occurred when FDP is enabled. For the case of Non-FDP, we run the experiment with FDP enabled but force SOC and LOC to use a single RUH to simulate the Non-FDP scenario. We ensure the garbage collection events occur for the same amount of host writes to the SSD to account for the energy consumption due to internal operations. Figure \ref{fig:kv-co2e}b.) shows that with the KV Cache workload, FDP-based segregation resulted in $\sim$3.6x fewer GC events for the same amount of host writes to the SSD. This shows that FDP-based segregation helps in improving the operational energy consumption of the SSD leading to operational carbon savings. 
Based on DLWA gains obtained for Twitter and WO KV Cache in Section \ref{sec:eval:waf:varying-workloads}, large embodied and operational carbon gains can be realized for both those workloads. \\

\minisec{FDP-based segregation allows for carbon-efficient deployments with lower DRAM requirements}
%\vspace{2ex}
\begin{table}[t]
%\addtolength{\tabcolsep}{-0.15ex}
    \centering
    \begin{tabular}{lcccc}
\toprule
   \makecell{Configuration \\  }& \makecell{Hit Ratio \\ (\%)} & \makecell{NVM Hit \\ Ratio (\%)} & KGET/s & \makecell{CO2e \\ (Kg)}  \\
\midrule
    FDP 4GB & 86.3 & 37.74 & 303.1 & 347.2 \\
    Non-FDP 4GB & 86.11 & 37.41 & 298.8 & 1081.1 \\
    \midrule
    FDP 20GB &  91.9  & 31.37 & 412.2 &  372.8\\
    Non-FDP 20GB &  92.1 &  33 &  399.1 & 1106.8\\
    \midrule
    FDP 42GB & 90.32 & 17.51 & 445.9 & 409.6 \\
    Non-FDP 42GB & 90.22 & 17.34 & 434.4 & 1143.6 \\
\bottomrule
    \end{tabular}\vspace{2ex}
    \caption{KV Cache workload with 100\% device utilization, 4\% SOC size and RAM size of 4GB and 42GB. We see that FDP enables carbon-efficient deployments with reduced DRAM for a tradeoff in hit ratio and throughput.}
    \label{tab:vary-ram}
    \vspace{-2ex}
\end{table}
In this section, we explore whether increase in SSD utilization in the Flash Cache of CacheLib can help reduce the DRAM Cache size. The main motivation behind this exploration is to reduce the cost and carbon footprint of CacheLib deployments. DRAM's embodied carbon footprint is at least an order of magnitude higher than an SSD~\cite{GuptaEHWL0W22}. A similar trend also exists for cost. We run the KV Cache workload with 100\% device utilization, 4\% SOC size and vary the DRAM used in the RAM Cache layer of CacheLib. 

%\vspace{-3ex}
%\vspace{1ex}
Table \ref{tab:vary-ram} shows the overall hit ratios, NVM hit ratios, throughput and effective carbon emission with different RAM cache sizes. We see that a lower DRAM leads to a reduction in hit ratio and throughput while being more carbon-efficient. An SSD utilization of 100\% enables more items to be cached in the Flash Cache. Consequently, we see that as the RAM cache size reduces, the NVM hit ratios improve massively for both FDP and Non-FDP while the overall hit ratios and throughput drop. \textit{A deployment with lower DRAM and 100\% device utilization was not viable without FDP-based segregation due to the high DLWA of $\sim$3.5}. While the 1.5x reduction in throughput might not be acceptable to applications with strict SLAs, applications that can tolerate this decrease for a 4x gain in carbon savings might find this deployment appealing. Since CacheLib is used as a building block for services~\cite{berg2020cachelib}, the flexibility in deployment provided by FDP to achieve greater carbon efficiency may be highly desirable. 
\begin{comment}
    
\begin{figure}[!h]
  \centering
  \includegraphics[width=1.0\linewidth]{svg_to_pdf/eurosys_vary_ram_100_bar.pdf}
  \caption{Hit Ratios with the KV Cache workload with 100\% device utilization, 4\% SOC size and RAM size of 4GB and 42GB. With FDP a DLWA of 1 at 100\% utilization affords a deployment with a 10x reduction in RAM for a 4\% drop in Hit Ratio } \label{fig:vary-ram-100}
  \vspace{-3ex}
\end{figure} \hfill
\end{comment}

\subsection{FDP-based segregation enables multi-tenant KV Cache deployments}
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\linewidth]{svg_to_pdf/multi_tenant_wo_kv.pdf}
  \caption{DLWA over 60 hours with the WO KV Cache workload running on two tenants each using 930GB SSD space and 4\% SOC size and 42 GB RAM. FDP enables a 3.5x reduction in DLWA in this multi-tenant deployment.} \label{fig:multi_tenant}
  %\vspace{-3ex}
\end{figure}
Without the use of FDP, 50\% of the SSD had to be reserved for host overprovisioning to achieve an acceptable DLWA. With FDP, however, we demonstrated that a DLWA of \textasciitilde1 can be achieved without any host overprovisioning. This effectively frees up half of the device, allowing it to be utilized for other purposes. One option we explored was to increase the SSD capacity for a single CacheLib instance from 50\% to 100\%. Another option involves running two CacheLib instances that share the SSD, each using half of the available space for its Flash cache to simulate a multi-tenant setup.

We evaluate the effectiveness of SOC and LOC data segregation under this multi-tenant configuration, with two KV cache instances running the WO KV cache workload on a shared 1.88 TB SSD without any overprovisioning. We partition the SSD into two equal parts (\textasciitilde930 GB each), with each KV cache instance (tenant) assigned to one partition. Both SOC and LOC sizes are set at 4\% and 96\% respectively. Our placement policy maps the SOC and LOC of the two tenants to different reclaim unit handles while all other parameters remain unchanged. Figure \ref{fig:multi_tenant} shows that the DLWA remains \textasciitilde1 because each tenant segregates its SOC and LOC data. In contrast, without FDP the DLWA increases to \textasciitilde3.5.
%\vspace{-2ex}