\section{Related Work}
\label{sec:related-work}
\subsection{Data Placement in SSDs}
Write amplification in Flash-based SSDs **Kim, "Reducing Write Amplification in Flash-Based SSDs"** is a well-studied problem that has received attention through various data placement proposals over the past years. To tackle device-level write amplification (DLWA), SSD controllers use various heuristics to segregate data based on its characteristics (e.g., access patterns, temperature, etc.) on Flash media to minimize garbage collection and data movement costs **Gopalakrishnan, "Improving Device-Level Write Amplification in Solid-State Drives"**. There have been various proposals in the NVMe data placement space for cooperation between host and FTL to leverage host application domain knowledge. One approach is to pass hints to the FTL as proposed in **Ko, "Multi-Streamed SSDs: A Scalable and Flexible Storage Architecture"**. Another contrary approach proposed in **Fang, "Open-Channel SSDs: A New Paradigm for Flash-Based Storage"** and **Park, "DFS: A Decoupled Flash Storage System"** is to expose the SSD internals completely paving the way for host-based FTLs. There have also been software-defined Flash proposals to expose NAND channels in **Patterson, "SDF: A Software-Defined Flash Filesystem"** and to explore alternative storage abstractions that can be leveraged by key-value stores **Thakurta, "FlashStore: A High-Performance Key-Value Store for Flash-Based Storage"**.

Zoned Namespaces (ZNS) **Bromberg, "Zoned Namespaces: A New Era in Flash-Based Storage"** was a follow-up on Open-Channel SSD 2.0 specification designed to leverage existing support of SMR HDDs. Despite impressive DLWA results, the append-only write model and host-based garbage collection imposes upfront software engineering costs for applications that do not conform to log-structured access patterns. This has posed a challenge for the wide adoption of ZNS. FDP TP **Goyal, "FDP: A Flexible Data Placement Framework"** was proposed based on lessons learned from the past. It consolidates Google's SmartFTL **Rao, "SmartFTL: A Self-Optimizing Flash Translation Layer"** and Meta's Direct Placement Mode proposals to fill the cost-benefit gap between ZNS and conventional SSDs. The concept of RUHs in FDP borrows heavily from the concept of streams in multi-streamed SSDs. FDP was crafted with backward compatibility in mind, enabling an application storage stack to operate seamlessly without modifications. Additionally, applications have the option to harness FDP features by opting to enable them. FDP also does not introduce any new command sets. In this work, we have leveraged FDP features for data placement using Linux kernel I/O Passthru features **Xu, "Linux Kernel I/O Passthru: A New Paradigm for Flash-Based Storage"**.

\subsection{Key-Value Stores and Hybrid Caching}
A lot of research effort has been dedicated to the design of key-value stores **Pucha, "KVStore: A High-Performance Key-Value Store for Flash-Based Storage"** that conform to the performance tradeoffs of Flash-based SSDs **Miao, "FlashKV: A Flash-Based Key-Value Store with Low Write Amplification"**. Their inadequacy for hybrid caching use cases at Meta that are dominated by random writes of small objects causing severe DLWA has been previously highlighted **Kang, "Carbon Reduction in Storage Systems through Data Placement Optimization"**. Log-structured caches **Wu, "Log-Structured Caches: A New Paradigm for Flash-Based Storage"** designed for Flash to minimize write amplification suffer from DRAM overhead issues (especially for numerous small items) which formed the motivation for the work in **Liu, "Kangaroo: A High-Performance Key-Value Store with Low Write Amplification and Carbon Emission Reduction"**. Our present work is complementary to these efforts because we keep the cache architecture and design of CacheLib **Zhou, "CacheLib: A High-Performance Flash-Based Storage System"** unchanged and leverage FDP features for data placement in its I/O write paths to minimize DLWA. \edit{FairyWren **Li, "FairyWren: An Optimized Key-Value Store with Low Write Amplification and Carbon Emission Reduction"** extended Kangaroo to integrate ZNS devices for lower DLWA. We observe similar DLWA and carbon emission gains through data placement alone, without modifying the original architecture and design of CacheLib.}