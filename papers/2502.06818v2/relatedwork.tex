\section{Related Work}
\label{sec:related_work}

\noindent \textbf{Pre-trained vision-language models}
Pre-trained vision-language models (VLMs)~\cite{Uniter, Virtex, Unicoder-vl, AlignBeforeUse, Hero} have experienced rapid development, thanks to the abundant large-scale image-text pairs accessible on the Internet.
Recently, CLIP~\cite{CLIP}, ALIGN~\cite{ALIGN} and Slip~\cite{Slip} have made great progress on learning visual and textual representations jointly by using contrastive learning. 
Among these, CLIP trained on WIT-400M exhibits robust zero-shot capability for image classification task, due to its image-level alignment with text.
However, directly applying CLIP to dense prediction tasks, such as object detection and semantic segmentation, results in suboptimal performance.
A series of methods~\cite{DetPro,UniDetector, MaskCLIP,GroupViT,TCL,ReCLIP} have successfully adapted CLIP for various downstream tasks and this paper specifically addresses the adaptation of CLIP for the task of training-free open-vocabulary semantic segmentation.

\noindent \textbf{Open-vocabulary semantic segmentation (OVSS)}
OVSS refers to segmenting an image with arbitrary categories under the guidance of a textual description.
Among these, fully supervised OVSS~\cite{OpenSeg,LSeg,ODISE,OVSeg,CATSeg} methods still rely on high-quality pixel-level annotated masks.
Usually, they generate mask proposals by an extra mask generator, \emph{e.g.}, Mask2Former~\cite{Mask2Former}, and further align the visual embeddings with the textual features.
Most methods extract visual features by CLIP, while ODISE leverages the internal representations of pre-trained Diffusion models~\cite{Diffusion}.
Methods for fully supervised OVSS usually train on a large-scale dataset equipped with fully annotated masks, like COCO Stuff~\cite{cocostuff}, and directly perform zero-shot inference on other datasets that may contain unseen categories during the training process.
There also exists a set of OVSS methods~\cite{CoDe,GroupViT,viewco,CoCu},which mainly exploit large-scale image-caption pairs, such as CC12M~\cite{cc12m} and YFCC~\cite{yfcc}, for training.
For example, GroupViT~\cite{GroupViT} introduces grouping tokens into the vision transformer and conducts hierarchical clustering for segmentation.
It finally obtains an image-level feature, which is then aligned with textual features by contrastive learning loss.

\noindent \textbf{Training-free open-vocabulary semantic segmentation}
% GEM, CLIPSurgery显式提一下
Methods for TF-OVSS~\cite{MaskCLIP,CLIPSurgery,SCLIP,CLIPtrase,ClearCLIP} adopt CLIP for OVSS without any training.
Existing works explore to enhance the distinction across the patch-wise visual features from CLIP mainly by modifying the attention mechanism in its final block, which forces each patch to primarily focus on itself and the neighbors in a narrow local window.
%Among these, MaskCLIP~\cite{MaskCLIP} directly replace the Query-Key attention map with an identical matrix, while others~\cite{ClearCLIP,CLIPSurgery,CLIPtrase,SCLIP,GEM} employ a self-self attention mechanism.
For example, CLIPSurgery~\cite{CLIPSurgery} and GEM~\cite{GEM} replace the conventional Query-Key attention with Value-Value attention. During forward, 
they additionally align new self-attention input with vanilla input to avoid deviation accumulation.
%additionally align the inputs between vanilla and new self-self attention, to avoid deviation accumulation after modification.
However, with the proposed self-self attention, the ability of CLIP to aggregate global context information, which is known to be useful for distinguishing confusing categories, is weakened.
Our proposed GCLIP in this paper belongs to the category of TF-OVSS methods and we mainly compare with the methods under the same setting for fairness.


\begin{figure*}
  \centering
\includegraphics[width=1\linewidth]{framework-gclip-12.pdf}
  \caption{\textbf{Method Overview.} 
  (a) \textbf{Overview.} In this paper, we propose a new framework GCLIP, consisting of Attention Map Fusion (AMF) and Channel Suppression (CS), for Training-Free Open-Vocabulary Semantic Segmentation. 
  (b) \textbf{Attention Map Fusion.} We fuse the attentions of early global-token emerging blocks ($L_g$,$L_{g+1}$, $\cdots$) with the Query-Query attention of the last-block ($L_{f}$) to emphasize the effect of global knowledge.
  %equip the last-block attention with image-level properties from global tokens, while not introducing homogeneous attention patterns across patches, by integrating the attention from the emerging blocks of global tokens into the Query-Query attention.
  (c) \textbf{Channel Suppression.} We suppress the weight norm of the specific output channel $\hat{d}$ of FFN by a re-nomalizing operation $\varphi$ as depicted in Eq.~(\ref{formula:renormalize}) to enhance the semantic correlation of Value embeddings.}
\label{method_fig}
\end{figure*}