\section{introduction}

Click-through rate (CTR) prediction is a fundamental task for recommendation systems on today's social media platforms. The objective is to predict the likelihood that a user will click on an item presented to them. The accuracy of this prediction heavily depends on understanding the user's interests with respect to the candidate items.

In recent years, deep neural networks (DNNs) have significantly improved CTR prediction accuracy in most scenarios. The key to this success lies in the ability of these networks to model users' historical behavior sequences. Central to this process is the attention mechanism, which provides a relevance score between each item in the sequence and the candidate items. These scores, known as attention scores, are then used to perform a weighted summation of the sequence to generate the final interest representations for the candidate item. Much work has been done to improve the efficiency of attention mechanisms \cite{zhou2018deep, zhou2019deep}.

As user behavior becomes richer, the length of the user's historical behavior sequence grows dramatically, potentially extending to a lifelong span. This increase in sequence length leads to a significant rise in the computational cost of the attention. An efficient approach to mitigate this computational burden in lifelong sequential modeling (LSM) is to divide the attention mechanism into two units: a General Search Unit (GSU) and an Exact Search Unit (ESU) \cite{pi2020search}. The GSU's role is to sift through the lifelong sequence to identify items that are most relevant to the candidate items. Subsequently, the ESU extracts user interest representations from the items identified by the GSU. This division allows the model to handle much longer sequences, which contain richer information about user interests, thereby further improving CTR prediction accuracy. 

However, most previous work considers attention as a point-wise scoring process, typically analyzing the relevance between candidate items and each individual item in the sequence. This approach overlooks the valuable information that adjacent items can provide in understanding user intent. This is particularly important on today's social media platforms such as TikTok, YouTube, and WeChat Channels, where users often consume a series of items consecutively. For instance, as illustrated in Figure \ref{fig:contextAware}, in a sequence of user behaviors generated from a consecutive consumption flow, the user has a very long watch time for the first three items, but the watch time starts to decrease dramatically from the fourth item onward. From a point-wise perspective, the third item might appear to be satisfactory since the user watched it for a long duration. However, from a context-aware perspective, the third item may not be as favorable because it is the last item the user was willing to watch extensively. There may be certain properties of the third item that caused the user to lose interest, leading to a decrease in watch time for subsequent items. Therefore, it is crucial for the model to employ context-aware LSM, which considers the context information contained in the adjacent items of each item in the sequence. This approach ensures a more comprehensive understanding of user interests with respect to candidate items and can provide more continuous and relevant recommendation outcomes.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/ctxAware.png}
    \caption{A comparison of attention scores from point-wise and context-aware perspectives. }
    \label{fig:contextAware}
    \vspace{-0.5cm}
\end{figure}

To achieve context-aware LSM, we propose the Context-Aware Interest Network (CAIN). CAIN first extracts context-aware representations of each item in the historical behavior sequence by performing a convolution operation along the temporal axis of the sequence, known as the Temporal Convolutional Network (TCN) \cite{bai2018empirical}. To the best of our knowledge, this is the first network to incorporate TCN in LSM. TCN offers two major advantages: it is lighter and more computationally efficient compared to methods such as Recurrent Neural Networks (RNN) \cite{hochreiter1997long,chung2014empirical} and Self-Attention \cite{vaswani2017attention}, and the context length can be easily controlled by adjusting the filter size of the convolution. The output representations from the TCN are then used in the subsequent attention module, instead of the original item representations, to extract context-aware interest representations with respect to the candidate items. Building upon this TCN framework, we have embedded a Multi-Scope Interest Aggregator (MSIA) module in CAIN. The MSIA module contains multiple stacked TCN layers that gradually extend the receptive field of the output representations. The output of each layer is sent to its corresponding attention modules to extract interest representations of different context scopes with respect to the candidate items. The computational cost of the attention decreases for the latter convolution layers as the sequence length is reduced through the TCN layers. Finally, to enhance the personalization capability of the convolution operations, we propose the Personalized Extractor Generation (PEG) module. This module generates convolution filters for different users based on their basic profile features. Instead of using uniform convolution filters for all users, we use the filters generated by the PEG module in all TCN layers. This makes the output representations of the TCN more user-specific, which can further improve the representativeness of the final interest representations.

We conducted extensive experiments on both a public dataset and an industrial dataset collected from user traffic logs on the WeChat Channels platform. The results indicate that the proposed CAIN achieves higher CTR prediction accuracy compared to existing methods. Additionally, the results show that the TCN framework and the MSIA module are highly adaptive and provide performance gains over multiple LSM baselines with different attention designs. Notably, CAIN also achieved significant improvements in online A/B tests. These findings demonstrate the effectiveness and robustness of CAIN in enhancing CTR prediction in complex environments.
