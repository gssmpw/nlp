\section{methodology}

In this paper, we propose a novel Context-Aware Interest Network (CAIN) to enable the model to consider the context information of each item in the user's historical sequence when performing lifelong sequential modeling (LSM). The proposed CAIN incorporates the Temporal Convolutional Network (TCN) to calculate context-aware representations of the items in the sequence. These representations are then used in the subsequent attention module instead of the original item representations, allowing the model to extract context-aware interest representations with respect to the candidate items.

Based on this TCN framework, we further include two major modules in CAIN to enhance its performance: the Multi-Scope Interest Aggregator (MSIA) module and the Personalized Extractor Generation (PEG) module. The MSIA module consists of several TCN layers to extract representations within different context lengths, which are then fed into their corresponding attention modules to extract interest representations of varying context scopes. The PEG module contains a lightweight network to generate convolution filters based on the basic profile features of the user.

We provide an overview of the proposed CAIN and its comparison to traditional LSM networks in Figure \ref{fig:architecture}. 

\begin{figure*}
    \centering
    \includegraphics[width=0.95\textwidth]{figure/CAIN.png}
    \caption{An overview of the proposed CAIN and its comparison to traditional LSM models. Figure (a) shows our baseline network that incorporates the LAP \cite{hou2024cross} for LSM. Figure (b) shows an upgraded model that incorporates the TCN framework proposed in CAIN. Figure (c) is the final version of the proposed CAIN, which includes the MSIA and PEG modules. }
    \label{fig:architecture}
    \vspace{-0.3cm}
\end{figure*}

\subsection{Context Information Extraction}

A common approach to extract a user's interest with respect to candidate items from the lifelong sequence is to conduct an attention process. In this process, the candidate item, denoted as $v$, serves as the query to form pairs with the items in the sequence <$v, lh_t$>, where $lh_t \in \vec{LH}$. Each $<v, lh_t>$ pair is assigned an attention score based on the networks applied in the attention module:
\begin{equation} 
    s_t = Attn(ve, lhe_t;\theta_a),
\label{eq:attention}
\end{equation}
where $ve$ and $lhe_t$ represent the item representations of $v$ and $lh_t$, $theta_a$ represents the parameters in the attention module and $s_{i}$ is the attention score. These attention scores are then used in subsequent retrieval or weighted summation stages. 

It is worth noting that the above attention process primarily focuses on understanding the relationship between $v$ and $lh_t$. However, in most real-world scenarios, user behavior often occurs in a consecutive manner, where a user interacts with a series of items in succession. This underscores the importance of considering the context, specifically the items before and after each center item in the sequence, to fully understand the behavior and its relevance to the candidate items.

In the proposed CAIN, this context-aware LSM is achieved by utilizing the Temporal Convolutional Network (TCN) \cite{bai2018empirical} before the attention module. 

\subsubsection{Representation Extraction}

Compared to methods such as Recurrent Neural Networks (RNNs) \cite{hochreiter1997long, chung2014empirical} or Self-Attention \cite{vaswani2017attention}, TCNs offer two primary advantages. First, TCNs are lightweight and computationally efficient. They require only a single matrix multiplication to slide the convolution filters over the sequence. In contrast, RNNs and Self-Attention involve significantly more computation. In particular, RNNs include operations that cannot be parallelized. Second, TCNs allow for explicit control over the context length. In RNNs and Self-Attention, context length is typically learned implicitly during training, which can complicate generalization. We will explore this further in the Experiment section.

The TCN operation can be conceptualized as a linear layer moving through the sequence. Given a context length $cl$, the size of the convolution filter $W^C$ is $2*cl + 1$, encompassing the context on both sides of the center item $lh_{t}$. Let $ctxe_t$ represent the item representations within the context $ctx_t$, the calculation within each convolution window is expressed as:
\begin{equation} 
    cr_t = (ctxe_t \cup lhe_t) \times W^C + b_c,
\label{eq:conv}
\end{equation}
where $cr_t$ is the output context-aware representation of $lh_{t}$, and $b_c$ is the bias term. 

For the first and last few elements in the sequence that lack sufficient context items before or after them, we apply zero-padding to ensure they fit the specified context length. 

\subsubsection{Representation Substitution}

After the TCN, the original item representations $lhe_t$ are transformed into context-aware representations $cr_t$, which contain information about both the center item and its adjacent context items. To obtain a context-aware interest representation of the user with respect to the candidate item, we replace $lhe_t$ used in the attention module with $cr_t$. Consequently, equation \ref{eq:attention} is updated to:
\begin{equation} 
    s_t = Attn(ve, cr_t;\theta_a).
\label{eq:attention_c}
\end{equation}

Similarly, in other processes within the attention module, we use $cr_t$ instead of $lhe_t$. This modification ensures that the attention module considers the broader context of user interactions, leading to more accurate and context-aware interest representations. 

\subsection{Multi-Scope Interest Aggregator}

In many applications, different lengths of context provide varying insights into the sequence. For the context of items in the user's historical behavior sequence, a longer context length includes more profound influences of the item on the user, such as interest migration or degradation. In contrast, a shorter context length generally captures information about the user's immediate behavior changes after being presented with the item. Both types of information are crucial for fully understanding user intent. 

To extract context-aware interest representations under different context scopes, we propose the Multi-Scope Interest Aggregator (MSIA) module, which extends the above TCN framework. An illustration of the MSIA module is provided in Figure \ref{fig:MSIA}. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/MSIA.png}
    \caption{An illustration of the MSIA module. It contains several TCN layers and their corresponding attention modules to extract interest representations with respect to the candidate items within different context scopes. }
    \label{fig:MSIA}
    \vspace{-0.5cm}
\end{figure}

\subsubsection{Stacking Layers}

The nature of TCN layers allows us to efficiently expand the context length by stacking more cascading layers. The receptive field of the output from the TCN layers gradually increases as the network deepens. Let $cl_n$ represent the context length of the $n-th$ layer, With a convolution stride size of 1, the context length of the $n+1-th$ layer is given by:
\begin{equation} 
    cl_{n+1} = cl_n + fs_{n+1} - 1,
\label{eq:receptive}
\end{equation}
where $fs_{n+1}$ is the filter size of the $n+1-th$ layer. 

As a result, the output of each layer contains information from different context lengths. This approach is more adaptive than tuning the context length of a single TCN layer, as it allows us to obtain various representations under multiple context scopes. At the same time, the representativeness of the output from the latter layers increases due to the non-linearity of the TCN operations, which also helps improve the final performance. 

To reduce the computational cost of processing the output from the deeper layers, the strides of the layers, except for the first layer, are set to values greater than 1. This significantly reduces the length of the output in these layers, thereby decreasing the computation required in their subsequent attention modules. Using a stride greater than 1 also accelerates the expansion of the context length, enabling the achievement of larger context scopes with fewer TCN layers. The stride of the first layer is fixed at 1 to ensure that each center item in the lifelong sequence has its own representation. With a stride greater than 1 in the subsequent layers, the information from multiple inputs is merged into a single output representation. Consequently, individual inputs no longer have their distinct representations in the output. This is acceptable for the deeper layers, as the information about individual items becomes less critical in longer context scopes. We will discuss this further in the Experiment section. 

\subsubsection{Individual Attentions}

To extract the interest representation of different context scopes with respect to the candidate items, we use attention modules with individual parameters to process the outputs of each TCN layer. This approach avoids inter-layer influence and allows for parallel computation. The attention modules employed can be categorized into two types: major attention and auxiliary attention. 

The major attention is applied only to the output of the first TCN layer to extract the interest representation $IR_1$ from the context-aware representations $cr^1_t$ of this layer. The attention technique should be suitable for a lifelong setting, as the length of the output of this layer matches the length of the input lifelong behavioral sequence. We apply the Lifelong Attention Pyramid (LAP) \cite{hou2024cross} for the major attention, as the LAP is used in our baseline. However, it is compatible with other lifelong attention mechanisms such as ETA \cite{chen2022efficient} and SDIM \cite{cao2022sampling}. 

The auxiliary attention is applied to the outputs of the subsequent TCN layers. Since the length of the output in these layers is much shorter than that of the first layer due to convolution strides greater than 1, and the focus is more on the information of items within the entire context scope rather than individual items, the attention mechanism used can be simpler than the major attention. We employ target attention with linear projection layers for auxiliary attention. Formally, the attention score $sa_t$ and the interest representation output $IR_n$ of the auxiliary attention for the $n-th$ layer are calculated using the following formulas: 
\begin{equation}
    sa_t = \frac{(W^Q_n \times ve) \times (W^K_n \times cr^n_t)\top}{\sqrt{d}},
\label{eq:attention_aux3}
\end{equation}
\begin{equation}
    IR_n = \sum_{t=1}^{T_n} sa^n_t*(W^V_n \times cr^n_t),
\label{eq:attention_aux4}
\end{equation}
where $W^Q_n$, $W^K_n$ and $W^V_n$ denote the projection weights, $d$ represents the inner dimension, and $cr^n_t$ is the representation output of the $n-th$ TCN layer. 

At the end of the MSIA module, all interest representations $IR_n$, including $IR_1$, are concatenated to form an integrated representation of multiple context scopes. This integrated representation captures a comprehensive view of user interests across different temporal scales, balancing the need for detailed individual item information and broader context understanding. 

\subsection{Personalized Extractor Generation}

Traditional convolution filters generally share parameters across different inputs, operating under the assumption that the inputs are drawn from very similar distributions. However, when using TCN to process users' historical sequences, this assumption may not hold. User behavior can vary significantly across different kinds of users, and the influence of an item on subsequent behavior can also differ greatly. For instance, a highly active user may decide to watch an item primarily based on its content, with minimal influence from previously presented items, even if some of those items were unsatisfactory. Conversely, a less active user, whose behavior is more susceptible to external factors, may be more influenced by the items presented before a given item. 

To make the context extraction more personalized, we propose a Personalized Extractor Generation (PEG) module, inspired by the work presented in the Personalized Cold Start Modules (POSO) \cite{dai2021poso}. The PEG module provides users with their own convolution filters, tailored to their unique behavior patterns. An illustration of the PEG module is presented in Figure \ref{fig:PEG}. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figure/PEG.png}
    \caption{An illustration of the PEG module. It is a lightweight sub-network that takes the user's basic profile features as input and outputs the convolution filters that will be used in the TCN layers. }
    \label{fig:PEG}
    \vspace{-0.3cm}
\end{figure}

\subsubsection{Filter Generation}

The PEG module contains a lightweight sub-network designed to generate personalized convolution filters $W^{PC}_n$ for each user. This sub-network consists of two fully connected layers that capture the interactions between the user's basic profile features and transform these inputs into the filter parameter space. The output of this sub-network is then reshaped to form the personalized convolution filters. Formally, the calculation of this generation process is as follows: 
\begin{equation}
    O_n=ReLU(W^{P1}_n \times I + b^{P1}_n),
\label{eq:peg1}
\end{equation}
\begin{equation}
    W^{PC}_n=Reshape(W^{P2}_n \times O_n + b^{P2}_n),
\label{eq:peg2}
\end{equation}
where $W^{P1}_n$ and $b^{P1}_n$ are the weights and biases of the first fully connected layer, and $W^{P2}_n$, $b^{P2}_n$ are the weights and biases of the second fully connected layer. $I$ is the representation of the user's basic profile features drawn from the set {$B$}. 

The basic profile features used in the PEG module include demographic information such as age, gender, location, and educational background, as well as behavioral statistics such as the number of items presented, the number of items clicked, and the most interacted authors of the user, such as the most watched authors. All types of features are crucial for achieving optimal performance. More detailed discussions on the impact of these features are provided in the Experiment section. 

\subsubsection{Personalized Convolution}

After generating the convolution filters, we use these personalized filters to replace the original global filters in each TCN layer. An alternative approach is to use both the global and personalized filters and combine the results. However, we found that this method only provides marginal improvements compared to a complete replacement. Consequently, in the proposed CAIN, equation \ref{eq:conv} is rewritten as: 
\begin{equation} 
    cr_t = (ctxe_t \cup lhe_t) \times W^{PC} + b_c,
\label{eq:pconv}
\end{equation}

By using personalized convolution filters, the model can better capture the unique patterns of individual user behaviors, leading to more precise predictions. 
