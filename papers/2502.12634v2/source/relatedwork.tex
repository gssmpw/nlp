\section{related work}

\subsection{Sequential Modeling}

Deep learning based models have achieved significant improvements in industrial applications, such as online advertising and recommendation systems \cite{cheng2016wide, qu2016product, wang2017deep, guo2017deepfm, wang2021dcn, lian2018xdeepfm, song2019autoint, zhou2018deep, pi2020search, xiao2017attentional}. The modeling of the user's historical behavior sequence, known as sequential modeling (SM), is crucial for these models to understand user intent and achieve personalized predictions. Much work has been done in this area \cite{zhou2018deep, pi2020search, zhou2019deep, feng2019deep, chen2022efficient, chang2023twin}.

As user behavior becomes more complex, the length of historical behavior sequences grows dramatically. Consequently, more work has focused on lifelong sequential modeling (LSM) in recent years. Sim \cite{pi2020search} and UBR4CTR \cite{qin2020user} are two methods that introduce a two-stage framework to model user lifelong sequences, consisting of a General Search Unit (GSU) and an Exact Search Unit (ESU). The GSU retrieves the top-k items most relevant to the target item from the entire user behavior history, which are then fed into the ESU for subsequent attention. This framework heavily relies on pre-trained embeddings, which can reduce the consistency between the GSU and ESU stages. To address this, ETA \cite{chen2022efficient} was proposed to use SimHash \cite{charikar2002similarity} to retrieve relevant items and encode item embeddings via locality-sensitive hash (LSH) in the ESU. SDIM \cite{cao2022sampling} was also proposed to generate hash signatures for both the candidate and behavior items, then gather behavior items with matching hash signatures to represent user interest. Both methods allow the two stages to share identical embeddings to increase consistency. Furthermore, TWINS \cite{chang2023twin} was proposed to enhance consistency by introducing CP-GSU, which retrieves behaviors that are not only target-relevant but also considered important by the ESU. Additionally, some work has upgraded the two-stage framework into a three-level attention pyramid \cite{hou2024cross} to further enhance consistency among stages.

However, most of this work considers LSM as a point-wise process, focusing solely on the relationship between individual items in the sequence and the candidate item. They overlook the importance of the context information provided by adjacent items in the sequence. In this paper, we aim to achieve a context-aware LSM that takes into account the adjacent items of an item in the sequence.

\subsection{Context-Aware Modeling}

Context-aware modeling methods are widely used in the fields of natural language processing (NLP), computer vision (CV) and speech recognition (SR). Long Short-Term Memory (LSTM) networks \cite{hochreiter1997long} and Gated Recurrent Units (GRUs) \cite{chung2014empirical} are classic RNN models that are extensively utilized in various NLP and SR tasks \cite{yin2017comparative,8690387,liu2016recurrent,graves2013speech,graves2014,mirsamadi2017automatic}. In recent years, the Transformer model and Self-Attention mechanisms \cite{vaswani2017attention} have become fundamental components in NLP, featuring an encoder and decoder based solely on attention mechanisms. GPT \cite{radford2018improving} and BERT \cite{devlin2018bert} are two well-known models built on this module. Besides RNN-based and attention-based approaches, other methods for context-aware modeling also exist. The Temporal Convolutional Network (TCN) \cite{bai2018empirical} is one such option. Convolutional Neural Networks (CNNs) dominate CV tasks \cite{krizhevsky2012imagenet, simonyan2014very, girshick2014rich, girshick2015fast, ren2015faster, redmon2016you, he2016deep}, as the convolution operation is well-suited for processing images to generate high-level feature maps by inherently considering the neighboring pixels of the target. TCNs conduct the convolution operation along the temporal dimension, enabling the generated representations to be context-aware \cite{Lea_2017_CVPR,Lea_2016_ECCV,Farha_2019_CVPR,8683634,Cheng_2020_ECCV}.

Researchers have incorporated RNNs and Self-Attention in SM for CTR prediction. CA-RNN \cite{liu2016context} and CRNNs \cite{smirnova2017contextual} are two methods that use RNNs to predict the probability of the next item given the user's historical items. DEIN \cite{zhou2019deep} employs GRUs to extract each user's interest state and utilizes AUGGRU to model the interest evolution with respect to the target item. Considering that sequences are composed of sessions, DSIN \cite{feng2019deep} uses a Self-Attention mechanism to extract users’ interests in each session and then applies Bi-LSTM to model how users’ interests evolve across sessions.

However, although these methods have brought certain improvements to the CTR task, they face challenges in scaling to LSM due to the heavy computational burden. Therefore, in the LSM setting, a lighter and more computationally efficient approach for capturing context information is required.