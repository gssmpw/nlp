\section{preliminaries}

In this paper, we explore the topic of context-aware lifelong sequential modeling (LSM). Unlike traditional LSM, which typically treats items in a sequence as isolated behaviors during the attention process to assess their relevance to candidate items, a context-aware LSM approach considers the context of each item within the sequence. The context, as referred to in this paper, consists of the adjacent items surrounding each item in the sequence, typically representing the items the user interacted with immediately before and after it.

To formalize, let $\vec{LH} = \{ lh_1, lh_2, \cdots, lh_{N} \}$ represents the lifelong behavior sequence of a user. The context of the item $lh_{t}$, referred to as the center item, can be denoted as:
\begin{equation} 
    \vec{cxt_t} = \{ lh_{t-cl}, lh_{t-cl+1}, \cdots, lh_{t-1}, lh_{t+1}, \cdots, lh_{t+cl-1}, lh_{t+cl} \},
\label{eq:context}
\end{equation}
where $cl$ represents the context length.

For all models presented in this paper, we mainly include three distinct categories of features for each user:

\begin{itemize}
 \item Basic profile features, denoted as ${\{B\}}$. 
 \item The short-term behavior sequence of the user, represented by $\vec{H} = \{ h_1, h_2, \cdots, h_{N} \}$. 
 \item The lifelong behavior sequence of the user, represented by $\vec{LH} = \{ lh_1, lh_2, \cdots, lh_{N} \}$. 
\end{itemize}

Note that each item $lh_{N}$ or $h_N$ in the lifelong and short-term behavior sequences includes its item ID and side information, such as the user's watch time. For a given user-item pair <$u_{i}$, $v_{i}$>, the model aims to predict the click-through rate (CTR) for user $u_{i}$ with respect to item $v_{i}$ as follows:
\begin{equation} 
    p_{i} = P(y_i=1|~u_{i},v_{i},B,\vec{H}^{t},\vec{LH}^{t};\theta),
\label{eq:target}
\end{equation}
where $\theta$ represents the parameters of the model.

The main network is optimized using a cross-entropy loss function, which is defined as follows:
\begin{equation}
\begin{split}
    \mathcal{L}_{CTR} = - \frac{1}{BS} \sum_{i=1}^{BS} & ~ \Big(~ y_i*log(p_i) + (1-y_i)*log(1-p_i) \Big),
\end{split}
\label{eq:cross_entropy}
\end{equation}
where $y_{i} \in \{0, 1\}$ represents the actual user feedback, and $BS$ denotes the total number of sample pairs in a training batch. 
