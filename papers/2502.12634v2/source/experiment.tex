\section{experiments}

To evaluate the performance of the proposed CAIN, we conducted extensive experiments using both a public dataset and an industrial dataset collected from the WeChat Channels platform. Additionally, we performed online A/B testing to assess the model's performance in a real-world environment. In this section, we provide a detailed description of our experimental setup, results, and analyses. 

\subsection{Experimental Setup}

\subsubsection{\textbf{Datasets}}

Our experiments were conducted using two datasets: one public dataset and one industrial dataset.

\textbf{Public Dataset}. We used the Taobao dataset, which is collected from the traffic logs of Taobao's recommendation system\footnote{https://tianchi.aliyun.com/dataset/649}. The dataset contains more than 100 million instances from over 1 million users within a period of 9 days. The average length of the user historical behavior sequence is 101, with the maximum length being 848. Since the dataset only includes instances with positive action feedback, we pre-processed the data similarly to previous methods \cite{pi2019practice, chen2022efficient} to generate positive and negative samples. The dataset was partitioned temporally, with the initial 8 days' data reserved for training and the data from the 9th day used as the test set. 

\textbf{Industrial Dataset}. This dataset was collected from user traffic logs on the WeChat Channels platform. It contains 13 billion instances from 0.3 billion users, collected over a period of 7 days. Instances with click feedback are treated as positive samples, while those without are considered negative samples. The average length of the user's historical behavior sequence is around 1500, with a maximum length of 2000. The dataset was partitioned temporally, with the initial 6 days' data reserved for training and the data from the 7th day used as the test set. 

\subsubsection{\textbf{Competitors}}

We compared the proposed CAIN to a series of state-of-the-art (SOTA) LSM methods. Our baseline model is built based on LAP \cite{hou2024cross}. Below, we detail the methods used in our comparisons:

\begin{itemize}

\item LAP \cite{hou2024cross}: Our initial baseline. It utilizes a three-level attention pyramid to refine the process of LSM.

\item SIM Soft \cite{pi2020search}: An early work that splits LSM into the GSU and ESU stages.

\item ETA \cite{chen2022efficient}: An approach that incorporates SimHash to achieve end-to-end learning of the GSU stage.

\item SDIM \cite{cao2022sampling}: A method that selects relevant items using the same hash signature across the GSU and ESU stages.

\item TWIN \cite{chang2023twin}: A method that adopts dimension compression to increase the consistency between GSU and ESU.

\end{itemize}

When implementing the above methods, we adopted the parameters and configurations reported in their original publications or open-source implementations.

\subsubsection{\textbf{Metrics}}

We conducted both offline and online experiments to evaluate the performance of the proposed CAIN model. For offline experiments, we used three widely recognized metrics in CTR prediction: Area Under the ROC Curve (AUC), Grouped Area Under the Curve (GAUC), and Logarithmic Loss (logloss). AUC and GAUC assess the model's pairwise ranking ability between positive and negative samples, while logloss measures the overall convergence of the training loss. In the online experiments, we evaluated the model's performance using industry-standard metrics: Click-Through Rate (CTR) and user stay time on presented items. Additionally, we considered online inference latency as a secondary metric to analyze the model's computational cost. 

\subsubsection{\textbf{Parameter Settings}} 

Our network architecture consists of two fully connected layers with dimensions set to 2048 and 1024, respectively. All input features are transformed into feature representations with an embedding size of 64 and concatenated before being fed into the first fully connected layer. We initialize the model parameters using the Xavier Initialization method \cite{glorot2010understanding} and optimize the model with the Adam optimizer \cite{kingma2014adam}, setting the learning rate to 0.001. The batch size is configured to 1536. All models are trained using the TensorFlow framework \cite{abadi2016tensorflow}. For the public dataset, we train the models on a single A100 GPU, while for the industrial dataset, we utilize 16 distributed A100 GPUs to handle the larger scale of data. 

\subsection{Model Analyses}
We designed experiments to analyze the effectiveness of the three key enhancements in CAIN: the TCN framework, the MSIA module, and the PEG module. All models in this section were trained using the industrial dataset, which is more suitable for LSM due to its substantial data volume and extended sequence length. 

\subsubsection{\textbf{Analyses of the TCN Framework}}

To understand the effectiveness of the TCN framework, we conducted an ablation study and analyzed its performance under different parameter settings. We also integrated the TCN framework with other LSM methods to evaluate its compatibility across various LSM backbones. The experiments in this section consider settings with only a single TCN layer, excluding the MSIA and PEG modules. 

\textbf{Influence of Context Length.} We conducted a set of experiments to analyze the performance of the model trained with different context lengths in the TCN framework. The results are shown in Figure \ref{fig:contextLength}. For experiments with a context length of -1, the TCN framework was not applied. For experiments with a context length of 0, no context was included, and the TCN layer degraded to a fully connected layer applied only to the center item. 

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/ctxLen.jpg}
    \caption{Performance comparison with different context lengths. }
    \label{fig:contextLength}
    \vspace{-0.5cm}
\end{figure}

The results indicate that the TCN framework significantly improves the model's performance when the context length is greater than 0. The performance of the model with context lengths set to -1 and 0 showed little difference, suggesting that the improvements brought by the TCN framework do not stem from the additional computation introduced by the TCN layer. Instead, they primarily arise from the context information encoded in the context-aware representations output by the TCN layer, which helps better understand the user's intent regarding the items in the sequence.

The results also show that the model's performance continues to improve as the context length increases, peaking when the context length is set to a value between 7 and 15. This indicates that including more adjacent items in the context extraction provides a more comprehensive view of the changes in user interest around the center item. However, the performance starts to decrease when the context length continues to increase. This may be because the model becomes more difficult to converge, as behavior patterns within longer context lengths become more complex and unstable. Additionally, the information of the center item may be overwhelmed by its adjacent items, which could affect the subsequent attention module. For the remaining experiments, we set the context length of the first TCN layer to 7. 

\textbf{Comparison to Other Methods.} We conducted a set of experiments to compare the effectiveness of the TCN layer against the RNN layer \cite{hochreiter1997long,chung2014empirical} and Self-Attention \cite{vaswani2017attention}. In these experiments, we simply replaced the TCN layer with the compared methods. The results are presented in Table \ref{tab:ctxmethod}. 

\begin{table}[!htbp]
\center
\vspace{-0.3cm}
\caption{Performance comparison between different context extraction methods.}
\begin{tabular}{l|lll}
  \toprule
  Methods & AUC  & GAUC  & Logloss  \\ 
  \midrule
  RNN    & 0.76342  & 0.65494 & 0.15998 \\
  Self-Attention  & 0.76385     & 0.65516   & 0.15986 \\
  Ours & \textbf{0.76590} & \textbf{0.65759} & \textbf{0.15947} \\
  \bottomrule
\end{tabular}
\label{tab:ctxmethod}
\vspace{-0.3cm}
\end{table}

The results show that the proposed TCN framework outperforms models trained with RNN and Self-Attention. This is primarily because RNN and Self-Attention models are more difficult to converge, as they must not only learn the context of the center item but also determine the appropriate length of the context to include for a given item. In contrast, the context length can be explicitly controlled when using the TCN layer by adjusting the size of the convolution filters. This simplifies the modeling of context information and allows the model to focus more on learning the relevance between the output context-aware representations and the candidate items. It is also worth noting that RNN and Self-Attention methods are much more computationally costly than TCN and may not be suitable for deployment in models with lifelong sequence settings. 

\textbf{Backbone Substitution.} We conducted a set of experiments to incorporate the TCN framework with other LSM methods. In these experiments, the item representations used by these methods were replaced with the context-aware representations extracted by the TCN layer. The results are presented in Table \ref{tab:tcnLSMs}. 

\begin{table}[t]
    \centering
    \caption{Performance comparison of the TCN Framework incorporated with different LSM backbones.}
    % \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{cccc}
    \toprule
     Methods & AUC & GAUC & Logloss \\
    \midrule
    \multicolumn{1}{l|}{SIM Soft} & 0.75022 & 0.64371 & 0.17268 \\
    \multicolumn{1}{l|}{SIM Soft$_{TCN}$} & \textbf{0.75209} & \textbf{0.64535} & \textbf{0.17181} \\
    \midrule
    \multicolumn{1}{l|}{ETA} & 0.74896 & 0.64238 & 0.17315 \\
    \multicolumn{1}{l|}{ETA$_{TCN}$} & \textbf{0.75108} & \textbf{0.64425} & \textbf{0.17165} \\
    \midrule
    \multicolumn{1}{l|}{SDIM} & 0.74657 & 0.64339 & 0.17304 \\
    \multicolumn{1}{l|}{SDIM$_{TCN}$} & \textbf{0.74851} & \textbf{0.64518} & \textbf{0.17194} \\
    \midrule
    \multicolumn{1}{l|}{TWIN} & 0.75211 & 0.64515 & 0.17208 \\
    \multicolumn{1}{l|}{TWIN$_{TCN}$} & \textbf{0.75409} & \textbf{0.64696} & \textbf{0.17089} \\
    \midrule
    \multicolumn{1}{l|}{LAP} & 0.76331 & 0.65483 & 0.16006 \\
    \multicolumn{1}{l|}{LAP$_{TCN}$ (Ours)} & \textbf{0.76590} & \textbf{0.65759} & \textbf{0.15947} \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}  
    \label{tab:tcnLSMs}
    \vspace{-0.3cm}
\end{table}	

The results show that models trained with the proposed TCN framework consistently outperform their corresponding baselines. This improvement can be attributed to the context information embedded in the context-aware representations extracted by the TCN layer. This demonstrates the effectiveness and compatibility of the proposed TCN framework across different LSM methods. 

\subsubsection{\textbf{Analyses of the MSIA Module}}

We conducted a set of experiments to evaluate the effectiveness of the proposed MSIA module under various parameter settings, including the number of layers and the stride of the TCN layers. Additionally, we integrated the MSIA module with other LSM methods to assess its compatibility. 

\textbf{Influence of the Layer Depth.} We conducted a set of experiments to analyze the performance of the MSIA module with varying numbers of TCN layers. When the number of layers is set to 1, the MSIA module is not implemented. In these experiments, the context length and the stride of the convolution operation are set to match those of the first TCN layer for a fair comparison. The results are shown in Figure \ref{fig:MSIALayerDepth}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/ctxDeep.jpg}
    \caption{Performance comparison with different number of TCN layers. }
    \label{fig:MSIALayerDepth}
    \vspace{-0.5cm}
\end{figure}

The results indicate that the model's accuracy gradually increases as the number of TCN layers increases. This improvement is primarily due to the latter layers in the MSIA module introducing context-aware representations with a longer context length than the first layer. This helps the model capture user interest across a broader spectrum of context scopes. However, the model's performance peaks when the number of layers reaches 4 and does not improve further with additional layers. This may be because items too far from the center item contribute little to understanding the user's intent regarding the center item. Consequently, the information gain from increasing the context length becomes limited. 

It is important to note that increasing the context length by adding more TCN layers is very different from increasing the size of the convolution filters in the first TCN layer (tested in Figure \ref{fig:contextLength}). With only one TCN layer of large filter size, the model relies solely on this single representation with a long context length, making it harder to converge when the length is very long. In contrast, by stacking more TCN layers and their corresponding attention modules and combining their outputs, the model can obtain information from different context scopes, including those with shorter context lengths in the initial TCN layers. This allows the model to learn both immediate behavior changes that are more relevant to the center item and interest changes that reveal the influence of the center item over a longer period. At the same time, the representativeness of the output from the latter layers increases due to the non-linearity of the TCN operations, which also contributes to the final performance. We set the number of layers to 4 in the remaining experiments. 

\textbf{Influence of the Stride Setting.} We conducted a set of experiments to analyze the model's performance when the stride of the first TCN layer or the stride of the subsequent TCN layers is set to a value greater than 1. The results are shown in Figure \ref{fig:MSIAStride}.

\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figure/ctxStride.jpg}
    \caption{Performance comparison with different convolution strides. }
    \label{fig:MSIAStride}
    \vspace{-0.7cm}
\end{figure}

The results indicate that the model's performance deteriorates as the stride of the first TCN layer increases. The performance is even worse than our baseline model without the TCN framework and the MSIA module. The side effect of increasing the stride of the convolution operation is that different items in the input sequence share representations in the output sequence. This implies that some unique information of the individual center items in the input may be lost. When performing attention in a lifelong sequence, this information may be crucial to ensure that the most relevant items are retained until the final level of the attention pyramid (or the ESU stage for other LSM methods). 

In contrast, the results show little performance difference among models trained with a convolution stride of 1 to 6 in the subsequent TCN layers. This suggests that the information of the center item is less important for the latter TCN layers, as it is already included in the output of the first TCN layer and the major attention. The subsequent TCN layers and auxiliary attentions are more focused on providing representations with longer context lengths. It is worth noting that although there is no accuracy gain from increasing the convolution stride, larger strides can significantly reduce the length of the TCN layer outputs, thereby greatly reducing the computational cost in the auxiliary attentions and subsequent layers. Therefore, we set the stride of the subsequent TCN layers to 4. 

\textbf{Backbone Substitution.} We conducted a set of experiments to test the compatibility of the MSIA module with other LSM methods. In these experiments, the item representations used by these methods were replaced with the context-aware representations extracted by the first TCN layer in the MSIA module. The interest representations from the subsequent MSIA layers were simply concatenated with those from the first layer. The results are presented in Table \ref{tab:MSIA_LSMs}.
		
\begin{table}[t]
    \centering
    \caption{Performance comparison of the MSIA module incorporated with different LSM backbones.}
    % \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{cccc}
    \toprule
     Methods & AUC & GAUC & Logloss \\
    \midrule
    \multicolumn{1}{l|}{SIM Soft} & 0.75022 & 0.64371 & 0.17268 \\
    \multicolumn{1}{l|}{SIM Soft$_{MSIA}$} & \textbf{0.75287} & \textbf{0.64643} & \textbf{0.17138} \\
    \midrule
    \multicolumn{1}{l|}{ETA} & 0.74896 & 0.64238 & 0.17315 \\
    \multicolumn{1}{l|}{ETA$_{MSIA}$} & \textbf{0.75152} & \textbf{0.64491} & \textbf{0.17119} \\
    \midrule
    \multicolumn{1}{l|}{SDIM} & 0.74657 & 0.64339 & 0.17304 \\
    \multicolumn{1}{l|}{SDIM$_{MSIA}$} & \textbf{0.74906} & \textbf{0.64603} & \textbf{0.17128} \\
    \midrule
    \multicolumn{1}{l|}{TWIN} & 0.75211	& 0.64515 & 0.17208 \\
    \multicolumn{1}{l|}{TWIN$_{MSIA}$} &  \textbf{0.75478} & \textbf{0.64796} & \textbf{0.17033} \\
    \midrule
    \multicolumn{1}{l|}{LAP} & 0.76331 & 0.65483 & 0.16006 \\
    \multicolumn{1}{l|}{LAP$_{MSIA}$ (Ours)} &  \textbf{0.76639} & \textbf{0.65810} & \textbf{0.15927} \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}  
\label{tab:MSIA_LSMs}
\vspace{-0.4cm}
\end{table}		

The results show that the performance of the models trained with the MSIA module is better than those trained without it. This conclusion is consistent across different LSM methods. Additionally, the performance of the models trained with the MSIA module is even better than their corresponding performance in Table \ref{tab:tcnLSMs}. This demonstrates the effectiveness of the MSIA module and its robustness to changes in the LSM backbones. 

\subsubsection{\textbf{Analyses of the PEG Module}} 

We conducted an ablation study to assess the effectiveness of the PEG module. Additionally, we analyzed the impact of input selection and the method of aggregating outputs on the final performance. 

\textbf{Ablation Study.} We conducted a set of experiments to analyze the effectiveness of the PEG module when implemented on the first TCN layer, the subsequent TCN layers in the MSIA module, or both. The results are presented in Table \ref{tab:PEG_layers}. In this experiment, we used a comprehensive set of input features for the PEG module. When the PEG module is implemented, the original global convolution filters are replaced by those generated by the PEG module. 

\begin{table}[t]
    \centering
    \caption{Performance comparison with PEG module implemented on different TCN layers. }
    % \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{cccc}
    \toprule
     TCN Layers used PEG & AUC & GAUC & Logloss \\
    \midrule
    \multicolumn{1}{l|}{MSIA} & 0.76639 & 0.65810 & 0.15927 \\
    \midrule
    \multicolumn{1}{l|}{MSIA + PEG for 1st layer} & 0.76704 & 0.65919 & 0.15891 \\
    \midrule
    \multicolumn{1}{l|}{MSIA + PEG for latter layer} & 0.76676 & 0.65868 & 0.15901 \\
    \midrule
    \multicolumn{1}{l|}{MSIA + PEG} & \textbf{0.76718} & \textbf{0.65965} & \textbf{0.15883} \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}  
\label{tab:PEG_layers}
\vspace{-0.5cm}
\end{table}

The results show that the PEG module further improves the model's performance, with the improvement being more significant when the PEG module is implemented for all TCN layers in the MSIA module. Additionally, the improvement in the GAUC metric is greater than the improvement in the AUC metric. This indicates that the PEG module has enhanced the model's ability to produce more personalized results, thereby significantly improving the ranking results for individual users.

\textbf{Influence of the Input Features.} We conducted a set of experiments to analyze the impact of the three types of input features used in the PEG module. The results are shown in Table \ref{tab:PEG_inputFea}.

\begin{table}[t]
    \centering
    \caption{Performance comparison with different input features for the PEG module. }
    % \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{cccc}
    \toprule
     Input features & AUC & GAUC & Logloss \\
    \midrule
    \multicolumn{1}{l|}{Demographic information} & 0.76664 & 0.65856 & 0.15905 \\
    \midrule
    \multicolumn{1}{l|}{Behavioral statistics} & 0.76669 & 0.65889 & 0.15899 \\
    \midrule
    \multicolumn{1}{l|}{Most interacted authors} & 0.76681 & 0.65893 & 0.15892 \\
    \midrule
    \multicolumn{1}{l|}{All features} & \textbf{0.76718} & \textbf{0.65965} & \textbf{0.15883} \\
    \bottomrule
    \end{tabular}
    % \end{adjustbox}  
\label{tab:PEG_inputFea}
\vspace{-0.3cm}
\end{table}

The results indicate that while the best performance is achieved when all features are included, the feature of most interacted authors contributes the most to the final performance. This is expected, as these features are highly unique to different users, making them the most personalized in describing user interests. However, even when the most interacted authors are included, adding demographic and statistical features still improves the model's accuracy. This may be because, for less active users who do not have many positive interactions with authors, these additional features help the model generalize among users with similar basic profiles. This demonstrates the importance of including both personalized and generalized features as input to the PEG module to generate convolution filters that cater to all types of users. 

\textbf{Influence of the Aggregation Method.} We conducted a set of experiments to compare the effectiveness of the PEG module under different aggregation methods. The original implementation replaces the results of the global filters with those from the personalized filters generated by the PEG module. In addition to this, we tested the performance when results from both filters are used, and when results from different filters are summed or concatenated to produce the final output. The results are shown in Table \ref{tab:PEG_agg}.

\begin{table}[t]
    \centering
    \caption{Performance comparison with different aggregation methods applied in the PEG module. }
    \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{cccc}
    \toprule
     Methods & AUC & GAUC & Logloss \\
    \midrule
    \multicolumn{1}{l|}{Replace} & 0.76718 & 0.65965 & 0.15883 \\
    \midrule
    \multicolumn{1}{l|}{Sum} & 0.76716 & 0.65962 & 0.15889 \\
    \midrule
    \multicolumn{1}{l|}{Concat} & 0.76723 & 0.65967 & 0.15880 \\
    \bottomrule
    \end{tabular}
    \end{adjustbox} 
\label{tab:PEG_agg}
\vspace{-0.5cm}
\end{table}

The results indicate that there is no significant difference between the various methods of aggregating the outputs. Using only the outputs from the personalized filters is sufficient to provide robust performance. This may be because the information in the representations extracted by the global filters is already contained in those extracted by the personalized filters, as the inputs to the PEG module also include more generalized features. Therefore, the information gain from including the original representations in the final output is limited and does not further improve the model. 

\subsection{Overall Performance}

The final performance of the proposed CAIN, along with its comparison to previous LSM methods, is presented in Table \ref{table:CAIN_overall}. Note that when the MSIA and PEG modules are not implemented, CAIN consists solely of the TCN framework with a single TCN layer.

\begin{table}[t]
    \centering
    \caption{Overall performance comparison.}
    \begin{adjustbox}{max width=1.0\linewidth}
    \begin{tabular}{ccccccc}
    \toprule
        \multirow{2}*{Methods} & \multicolumn{3}{c}{Industrial.} & \multicolumn{3}{c}{Public.} \\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}
        & AUC & GAUC & Logloss  & AUC & GAUC & Logloss \\
        \midrule
        SIM Soft    & 0.75022 & 0.64371 & 0.17268 & 0.87872 & 0.78671 & 0.14919 \\
        ETA         & 0.74896 & 0.64238 & 0.17315 & 0.87798 & 0.78573 & 0.14932 \\ 
        SDIM        & 0.74657 & 0.64339 & 0.17304 & 0.87705 & 0.78509 & 0.14943  \\
        TWIN        & 0.75211 & 0.64515 & 0.17208 & 0.87971 & 0.78752 & 0.14867 \\
        LAP         & 0.76331 & 0.65483 & 0.16006 & 0.88513 & 0.79423 & 0.14809 \\
        \cmidrule(lr){1-7}		
        CAIN $w/o$ MSIA and PEG & 0.76590 & 0.65759 & 0.15947 & 0.88664 & 0.79589 & 0.14761  \\
        CAIN $w/o$ PEG           & 0.76639 & 0.65810 & 0.15927 & 0.88696 & 0.79621 & 0.14743 \\
        CAIN          & \textbf{0.76718} & \textbf{0.65965} & \textbf{0.15883} & \textbf{0.88748} & \textbf{0.79714} & \textbf{0.14696} \\
        \bottomrule
    \end{tabular}
    \end{adjustbox}
    \vspace{-0.5cm}
    \label{table:CAIN_overall}
\end{table}		

The results demonstrate that across all metrics and datasets, the proposed CAIN consistently outperforms the other methods. Notably, the margin of improvement on the industrial dataset is greater than that on the public datasets. This is primarily because the sequence length and data volume are smaller for the public datasets. Therefore, the performance on the industrial dataset provides a more accurate illustration of the model's capability to handle complex real-world environments.

\subsection{Online Performance}

To further validate the efficiency of the proposed CAIN, we conducted an online A/B test to assess its performance in real-world industrial scenarios. We collected user feedback from the online platform for metrics calculation over a period of seven days. Compared to the control group, where the baseline LAP is implemented, the proposed CAIN in the experimental group received significantly better user feedback, with a 1.93\% increase in stay time and a 3.43\% increase in CTR. Moreover, the inference latency of the proposed CAIN is only 8 ms longer than the baseline model, owing to the light and fast nature of the TCN operations. This slight increase in latency is negligible compared to the substantial improvement in recommendation quality. 