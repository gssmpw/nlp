\documentclass[10pt]{article}
\usepackage{natbib}
% \usepackage[backend=biber]{biblatex}
% \addbibresource{denoising.bib}
% \usepackage{parskip}

% \usepackage{ragged2e}

\usepackage[normalem]{ulem}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
% \makeatletter
% \renewcommand{\paragraph}{%
%   \@startsection{paragraph}{4}%
%   {\z@}{1.25ex \@plus 1ex \@minus .2ex}{-1em}%
%   {\normalfont\normalsize\bfseries}%
% }
% \makeatother

% \mathtoolsset{showonlyrefs}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
% \usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref} 
\hypersetup{colorlinks=true, 
linkcolor=black,
citecolor=black,
filecolor=black,
urlcolor=black}

% \usepackage{nicefrac,xfrac}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MACROS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def \E{{\mathbb E}}
\def \P{{\mathbb P}}
\newcommand{\idm}{I}
\newcommand{\rb}{\mathbb{R}}
\newcommand{\tr}{\mathop{ \rm tr}}
\newcommand{\sign}{\mathop{ \rm sign}}

\newcommand{\BEAS}{\begin{eqnarray*}}
\newcommand{\EEAS}{\end{eqnarray*}}
\newcommand{\BEA}{\begin{eqnarray}}
\newcommand{\EEA}{\end{eqnarray}}
\newcommand{\BEQ}{\begin{equation}}
\newcommand{\EEQ}{\end{equation}}
\newcommand{\BIT}{\begin{itemize}}
\newcommand{\EIT}{\end{itemize}}
\newcommand{\BNUM}{\begin{enumerate}}
\newcommand{\ENUM}{\end{enumerate}}
\newcommand{\mysec}[1]{Section~\ref{sec:#1}}
\newcommand{\eq}[1]{Eq.~(\ref{eq:#1})}
\newcommand{\myfig}[1]{Figure~\ref{fig:#1}}

\newcommand{\note}[1]{{\textbf{\color{red}#1}}}
\usepackage[mathcal]{eucal}

\usepackage{mathrsfs}
\newcommand{\normal}{\mathscr{N}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Sampling Binary Data by Denoising through Score Functions}

\begin{document}

% \title{Sampling Binary Data by Denoising through Score Functions}

\title{\bf \Large Sampling Binary Data by Denoising through Score Functions}

 \author{\!\!\!\!\!
 Francis Bach\thanks{Inria, Ecole Normale Supérieure, Université PSL, \texttt{francis.bach@inria.ir}.}
\ \ \ 
Saeed Saremi\thanks{Frontier Research, Prescient Design, Genentech, \texttt{saremi.saeed@gene.com}.
}
}

\date{\normalsize February 1, 2025}
\maketitle

\begin{abstract}
Gaussian smoothing combined with  a probabilistic framework for denoising via the empirical Bayes formalism, i.e., the Tweedie-Miyasawa formula (TMF), are the two key ingredients in the success of score-based generative models in Euclidean spaces. Smoothing holds the key for easing the problem of learning and sampling in high dimensions, denoising is needed for recovering the original signal, and TMF ties these together via the score function of noisy data. In this work, we extend this paradigm to the problem of learning and sampling the distribution of binary data on the Boolean hypercube by adopting Bernoulli noise, instead of Gaussian noise, as a smoothing device. We first derive a TMF-like expression for the optimal denoiser for the Hamming loss, where a score function naturally appears. Sampling noisy binary data is then achieved using a Langevin-like sampler which we theoretically analyze for different noise levels. At high Bernoulli noise levels sampling becomes easy, akin to log-concave sampling in Euclidean spaces. In addition, we  extend the sequential multi-measurement sampling of~\citet{saremichain} to the binary setting where we can bring the ``effective noise'' down by sampling multiple noisy measurements at a fixed noise level, without the need for continuous-time stochastic processes. We validate our formalism and theoretical findings by experiments on synthetic data and binarized images.
\end{abstract}



\section{Introduction}
We would like to draw samples from a distribution $p$ on the Boolean hypercube $\{-1,1\}^d$. For the problem of sampling from a distribution $\mu$ in the Euclidean space $\mathbb{R}^d$, Langevin Markov chain Monte Carlo (MCMC) is a general-purpose class of gradient-based algorithms whose convergence properties are studied extensively with the assumption that $\mu$ is log-concave~\citep{dalalyan2017theoretical, durmus2017nonasymptotic, cheng2018underdamped,chewi}. Recently, Gaussian smoothing was effectively used for mapping the general problem of sampling in Euclidean space to log-concave sampling~\citep{saremichain}. Inspired by this line of work, we approach the problem of sampling binary data with a ``smoothing philosophy'', where Bernoulli noise plays a prominent role. 



In Euclidean space one can ease the sampling problem by opting for sampling noisy data. In particular, instead of the random variable $x$, we opt for sampling the random variable $y=x+\varepsilon$, where $\varepsilon \sim \mathcal{N}(0,\sigma^2 I)$ follows a Gaussian distribution. This scheme involves a single hyperparameter, the standard deviation $\sigma$. Algebraically, this is akin to sampling the smoother density $\nu_\sigma$ of~$y$, which is the convolution of the distribution $\mu$ of $x$ with the Gaussian distribution. From a geometric perspective, the noise effectively ``fills up'' the space with probability mass (the degree of which one controls with $\sigma$) thus making navigating the space easier. One can then ``clean up'' the mass that is added to the space using denoising. In particular,  classical results in statistics~\citep{robbins1956empirical, miyasawa1961empirical} state:
$$ %\label{eq:TMF}
\E[x|y] = y + \sigma^2 \nabla\log \nu_\sigma(y),
$$
which we refer to as the Tweedie-Miyasawa formula (TMF). Note that  $\mathbb{E}[x|y]$ is the least-squares estimator of clean data~$x$ given a noisy observation $y$, and $\nabla\log \nu_\sigma$ is known as the score function~\citep{hyvarinen2005estimation}.

In the generative modeling setting, where the distribution is unknown but we have access to data $\{x^{(i)}\}_{i=1}^n$, one can turn TMF into a supervised least-squares denoising objective for learning the score function of noisy data, where the noisy data $y=x+\varepsilon$, $\varepsilon \sim \mathcal{N}(0,\sigma^2 I)$ is the input and the clean data $x$ is the target~\citep{hyvarinen2005estimation, vincent2011connection, raphan2011least, saremi2019neural}. One can then use Langevin MCMC (``walk'') to sample from the learned $\nu_\sigma(y)$; the noisy samples can be cleaned up with the learned denoiser (``jump''). This sampling scheme was referred to as the walk-jump sampling which we denote by WJS-1 (``1'' anticipates the extension we discuss below). There is a clear trade-off here: for higher $\sigma$, sampling from $\nu_\sigma(y)$ becomes easier, but the distribution of denoised samples itself goes further away from $\mu(x)$. Despite this trade-off, WJS-1 has proven to be effective in some  applications~\citep{pinheiro2023d, frey2023learning, kirchmeyer2024score}.
%

The sampling trade-off in WJS-1 is addressed in multi-measurement models~\citep{saremi2022multimeasurement, saremichain}, in which one is interested in the distribution $\nu_\sigma(y_{1:m})$ associated with $y_{1:m}\coloneqq (y_1,\dots,y_m)$, where $y_k = x\!+\!\varepsilon_k$, $k \in [m]$, and $\varepsilon_k \! \sim \! \mathcal{N}(0,\sigma^2 I)$ all independent and independent of $x$. \citet{saremichain} studied a sequential strategy for sampling from $\nu_\sigma(y_{1:m})$ and showed that the noise level effectively  goes down (as far as the denoiser is concerned) at the rate $\sigma/\sqrt{m}$. Furthermore, if one chooses $\sigma$ such that the distribution $\nu_\sigma(y_1)$ is log-concave, all the subsequent conditional distributions $\nu_\sigma(y_k|y_{1:k-1})$, $k\in [m]$ remain log-concave. The general sampling problem is therefore mapped to a sequence of log-concave sampling while the effective noise goes down via this accumulation of measurements. We refer to this scheme as WJS-$m$, which involves two hyperparameters: the noise level $\sigma$, and the number of measurements $m$. This approach has deep connections to sampling via diffusion and stochastic localization~\citep{montanari2023sampling}. The main conceptual difference is that the multi-measurement sampling does not involve discretizing an SDE~\citep{song2020score, campbell2022continuous} for bringing the noise down. Fundamentally, this is due to the discrete nature of measurement accumulation. 

\subsection{Contributions}
Given this background, we approached the problem of sampling from a distribution $p(x)$ on $\{-1,1\}^d$ by devising a  smoothing method, with the key restriction to stay on the Boolean hypercube (in this purely binary world Gaussian noise does not exist). The natural choice to ``smooth'' the binary data is to use (isotropic) random sign flips dictated by the Bernoulli noise: $y = x \circ \varepsilon$, where $\circ$ denotes the Hadamard (i.e., pointwise) product. The noise $\varepsilon$ is drawn from the Bernoulli distribution, $\P(\varepsilon_i=1) = \sigma(2\alpha)$, where~$\sigma$ is the sigmoid function, and $\alpha \geqslant 0$ is the noise parameter. The probability mass function of the noisy data $q_\alpha(y)$ happens to be a  transformation of $p(x)$ via an exponential tilt governed by $\exp(\alpha x^\top y)$.
As $\alpha$ decreases, the probability mass get more spread out on the hypercube, thus easing the sampling problem, where in the extreme case, $\alpha=0$, we arrive at the uniform distribution. 

For denoising there are subtle differences between the Boolean/Bernoulli and Euclidean/Gaussian setups, where the optimal denoiser $f$ takes values on the Boolean hypercube and the Hamming loss is the natural loss. We show in Lemma \ref{lemma:optimal-denoiser} that $f$ takes the form $f(y) = \sign( \E[ x|y ])$, and in Lemma~\ref{lemma:denoise-with-score} we show that
$$ 
\E[ x| y] = \frac{1}{\alpha} \nabla \log q_\alpha(y).
$$
This is essentially the form of TMF for the Bernoulli noise, where crucially the score function appears again. We should emphasize that the score function is well-defined here since $q_\alpha(y)$ has an analytical form (dictated by the exponential tilt) beyond $\{-1,1\}^d$. Finally, similar to the Gaussian case discussed earlier we can learn the score function given a dataset $\{x^{(i)}\}_{i=1}^n$ by denoising, the subtle difference here is that since $x$ and $y$ are both binary, we can also set up the denoising objective via logistic regression (\mysec{learning-score}). Naturally, denoising becomes harder as $\alpha$ decreases, which we can characterize by the Wasserstein distance between the law of $x$ and the law of $\E[x|y]$ (Lemma~\ref{lemma:denoising-performance}). 



Our second main contribution in this work is to analytically study sampling from $q_\alpha(y)$ using gradient-based methods with a formal understanding of the role the Bernoulli noise level $\alpha$ plays in easing the problem of sampling binary data. There has been a recent interest on devising gradient-based sampling strategies for discrete distributions from the perspective of Gibbs sampling~\citep{grathwohl2021oops}, and Langevin MCMC~\citep{zhang2022langevin}. Our approach here is close to the later, where in addition we introduce a new \emph{two-stage} discrete Langevin MCMC algorithm (\mysec{two-stage-langevin}), with improved behavior at high noise.

Langevin-like  updates are especially motivated here on two fronts: (i) the probably mass of noisy data is more spread out on the Boolean hypercube and it demands coming up with Markov moves where many coordinates are updated in parallel (in contrast to ``cautious'' single-coordinate Gibbs updates), (ii) the score function $\nabla \log q_\alpha(y)$ is readily accessible to be used via denoising and our binary TMF.  Regarding the first point, we theoretically analyze the contraction properties of the vanilla (one-stage) and two-stage discrete Langevin MCMC, where the noise level $\alpha$ plays a prominent role in the exponential convergence of the algorithms. To our knowledge, there are no prior work on the exponential convergence of discrete Langevin-like algorithms in the Wasserstein metric (Propositions~\ref{prop:contractivity} and~\ref{prop:contractivity-twostage}). Furthermore, we extend our contraction results by proving bounds on the distance between the stationary distributions of the discrete Langevin algorithms and the target distribution $q_\alpha$ in the Wasserstein metric (Propositions~\ref{prop:stationary} and~\ref{prop:stationary-2}). These results again highlight the important role the noise level $\alpha$ plays.    



Informally, there are parallels between contractivity results for high Bernoulli noise (small $\alpha$) and the exponential convergence of Langevin MCMC for log-concave distributions achieved for large $\sigma$ in the Euclidean/Gaussian case~\citep[Theorem 1]{saremichain}. We make this connection formal from the angle of sampling multiple noisy data, where multiple Bernoulli noise is added independently to clean data, where the noise level is held fixed. TMF for the multiple  Bernoulli measurements  takes the form
$$  \E[x|y_{1:m}] = \frac{1}{m\alpha} \nabla \log q_{m\alpha}(\bar{y}_{1:m}),$$
which corresponds to a reduced noise dictated by $m\alpha$ (Lemma~\ref{lemma:denoising-performance-m}).


We conduct a set of experiments on synthetic data, where we study a mixture model on $\{-1,1\}^d$, akin to mixture of Gaussians in $\mathbb{R}^d$. The experiments were desgined to quantify denoising for strong and weak priors and probe the sampling properties of our scheme. We also conduct experiments on binarized MNIST by qualitatively studying the role of $\alpha$, and demonstrate the fast mixing our algorithm can achieve with essentially no tuning (the step-size is simply set to~$1/\alpha$).




\subsection{Related work} There is a growing body of work on sampling from discrete distributions with score-based models that build on denoising diffusion models~\citep{sohl2015deep, hoogeboom2021argmax, austin2021structured, campbell2022continuous, loudiscrete}.     There are variations between these models, but they are all fundamentally formulated based on a forward/backward continuous-time diffusion process for corrupting the data and learning score functions, via denoising, to reverse the process. Algorithmically, these continuous-time processes are then discretized using various schemes.  Our approach here is fundamentally different with a single noise scale sampling strategy: at each stage of measurement accumulation the data is sampled at a fixed noise scale. The process to bring the noise down is therefore discrete by nature, characterized by a single hyperparameter, the number of measurements $m$, in contrast to devising a noise schedule in diffusion-based prior works.









\section{Denoising and binary score functions}
We consider a binary random vector $x \in \{-1,1\}^d$, with probability mass function $p(x)$ (that sums to one). 

\subsection{Noise models for binary vectors}
\label{sec:noisemodel}
A natural noise model is to use random sign flips, that is, 
\BEQ
\label{eq:noise}
y = x \circ \varepsilon
\EEQ
(for the component-wise product $\circ$), where $\varepsilon \in \{-1,1\}^d$ has independent components, and, for $i \in \{-1,\dots,1\}^d$,
$$
\P( \varepsilon_i = 1) = \sigma(2\alpha),
$$
where $\sigma$ is the sigmoid function, and $\alpha \geqslant 0$ is the noise parameter.
When $\alpha$ is large, $\sigma(2\alpha)$ is close to one, and thus $\varepsilon$ is the vector of all ones with high probability, and $y$ is close to $x$ (small noise). When $\alpha$ is equal to zero, then $\sigma(2\alpha)=1/2$, and  $\varepsilon$ is uniform, and so is $y$ (high noise). Moreover, The expected number of flips is equal is $d \sigma(-2\alpha)$, and goes from $d/2$ when $\alpha=0$ to $0$ exponentially fast when $\alpha=+
\infty$. 


We can write the probability mass $r$ function of $\varepsilon$ as
$$ r(\varepsilon) = \prod_{i=1}^d \frac{e^{\alpha \varepsilon_i}}{e^\alpha + e^{-\alpha}} = \frac{1}{ ( 2 \cosh \alpha )^d} e^{ \alpha 1_d^\top \varepsilon},$$
and
the probability mass function $q_\alpha$ of $y$ defined in \eq{noise} as:
\BEA
\notag
q_\alpha(y) 
& = & \sum_{x \in \{-1,1\}^d} p(x) r(y \circ x) \\[-.05cm]
 \label{eq:q}  &
= &  \frac{1 }{ ( 2 \cosh \alpha )^d}  \sum_{x \in \{-1,1\}^d} p(x) e^{ \alpha x^\top y }
\\
\notag & =  & \sigma(2\alpha)^d  \sum_{x \in \{-1,1\}^d} p(x) e^{ - \frac{\alpha}{2} \| x-y\|_2^2 },
\EEA
since on the hypercube $\| x\|_2^2 = \| y \|_2^2 = d$.


When $\alpha = 0$, then $q_\alpha$ is the uniform distribution, while for $\alpha = +\infty$, $q_\alpha = p$. Thus, $\alpha$ plays exactly the role of the \emph{inverse} variance, as can be seen with last expression above that mimics Gaussian noise.



A key observation is that the function $q_\alpha(y)$ defined in \eq{q} is defined for \emph{all} $y \in \rb^d$, and not only in $\{-1,1\}^d$, so that we can take continuous gradients\textemdash not discrete gradients as sometimes done for score matching extensions~\citep{hyvarinen2007some, meng2022concrete}.



\subsection{Denoising}
Given the noisy (random) version $y \in \{-1,1\}^d$, how can we recover a good denoised $x \in \{-1,1\}^d$? Like for the Gaussian case, once given a loss function, the optimal denoiser has a closed-form expression. We consider the Hamming loss, which has several expressions when $x,x' \in \{-1,1\}^d$, as an $\ell_1$-norm or a squared $\ell_2$-norm:
\BEQ \notag
\ell(x,x') = \sum_{i=1}^d 1_{x_i \neq x'_i} =  \frac{1}{2} \sum_{i=1}^d |x_i - x'_i| = \frac{1}{2} \| x - x'\|_1 =  \frac{1}{4} \sum_{i=1}^d |x_i - x'_i|^2 = \frac{1}{4} \| x - x'\|_2^2.
\EEQ
It simply counts the number of mistakes, between $0$ and $d$. We then obtain the optimal denoiser from the conditional expectation (which extends classical results from binary classification, see~\citet[Section 4.1]{ltfp}).

\begin{lemma}[Optimal denoiser] \label{lemma:optimal-denoiser}
Given a joint distribution on $(x,y)$, the function $f:\{-1,1\}^d \to \{-1,1\}^d$   that minimizes $\E [ \ell(x,f(y))]$ is $f(y) = \sign( \E[ x|y ])$.
\end{lemma}
\begin{proof}
We have:
$$
\E [ \ell(x,f(y))]
\!= \!\!\!\sum_{y \in \{-1,1\}^d } p(y) \sum_{x \in \{-1,1\}^d } p(x|y) \ell(x,f(y)),$$
and $f_i(y) \in \{-1,1\}$ can be optimized independently for each $y \in \{-1,1\}^d$ and $i \in \{1,\dots,d\}$, and maximizes $\sum_{x \in \{-1,1\}^d } p(x|y) 1_{x_i = f_i(y)}
= \P( x_i = f_i(y) | y )$. Thus, $f_i(y) = 1$ if $\P( x_i = 1 | y ) > \P( x_i = -1 | y )$, which exactly leads to the sign of $\E[ x_i | y]$. The value of the sign at zero can be taken to be uniformly at random in $\{-1,1\}$.
\end{proof}

We can now consider the noise model in \eq{q} from \mysec{noisemodel} and compute the gradient of $\log q_\alpha$ as 
\BEQ
\label{eq:grad}
\nabla \log q_\alpha(y)
 = \frac{\sum_{x \in \{-1,1\}^d} p(x) \alpha x  e^{ \alpha \, x^\top y }}{\sum_{x \in \{-1,1\}^d} p(x) e^{ \alpha \, x^\top y }}, 
 \EEQ
 which is exactly $\alpha \E [ x | y ]$, leading to the following lemma.

\begin{lemma}[Denoising through score functions] \label{lemma:denoise-with-score}
For the function $q_\alpha$ defined in \eq{q} for all $y \in \rb^d$ that characterizes the random sign flip model, we have:
$$
\E[ x| y] = \frac{1}{\alpha} \nabla \log q_\alpha(y).
$$
\end{lemma}

We refer to the function $\nabla \log q_\alpha(y)$ as the score function. It allows to obtain the optimal denoiser by taking the sign. This denoiser has a performance that degrades when $\alpha$ goes to zero. We consider the Wasserstein distance derived from the loss $\ell$, that is, given two distributions $p$ and $q$ on $\{-1,1\}^d$, we consider $W(p,q)$ as the minimum expectation $\E[ \ell(x,y)]$ over all distributions on $(x,y)$ with marginals $p$ on $x$ and $q$ on $y$~\citep{peyre2019computational}. The following lemma provides an upper-bound that extends the Gaussian result from~\citet{saremichain}.

\begin{lemma}[Denoising performance] \label{lemma:denoising-performance}
For the noise model defined in \eq{q}, we have:
$$
W( \mbox{law of } x, \mbox{law of } \sign(\E[x|y]))
\leqslant d e^{-2\alpha}.
$$
\end{lemma}
\begin{proof} We consider the natural coupling with $y = x \circ \varepsilon$ where $\varepsilon$ is independent of $x$ and $\varepsilon$ has independent components, and simply use the fact that
$ \E \big[\ell(x,f(y)) \big]
$ is minimized exactly by $f(y) = \sign(\E[x|y])$, and thus is less than the loss of the naive denoiser that simply outputs $y$, for which $\E \big[\ell(x,y) \big] \!=\!  d \, \!\sigma(-2\alpha)$, which is less than $de^{-2\alpha}$.
\end{proof}
Like for the Gaussian case, this bound is true regardless of the strength of the prior on $x$. If $p(x)$ is uniform, it cannot really be improved. However, when the prior is strong, better bounds could be derived. 

Note that as opposed to Gaussian noise, the denoising performance goes exponentially  to zero when $\alpha$ grows.


\subsection{Learning the score function} \label{sec:learning-score}
In order to learn the denoiser, it is natural to consider observations $x^{(1)},\dots,x^{(n)} \in \{-1,1\}^d$, generate independent noise variables $\varepsilon^{(1)},\dots,\varepsilon^{(n)}\in \{-1,1\}^d$ (with $\P( \varepsilon^{(i)}_j=1) = \sigma(2\alpha)$), and parameterize a denoiser $\E[x|y] = 2 \sigma(   f_\theta(y)) - 1  = \tanh \frac{f_\theta(y)}{2} \in \rb^d$,
with thus $f_\theta(y)$ of the form $ 2 {\rm atanh } \big[ \frac{1}{\alpha}\nabla \log q_\alpha(y) \big]$.

We can learn it through the following denoising criterion (which is exactly logistic regression):
$$
\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^d\log\big( 1 + \exp\big( - x^{(i)}_j f_\theta( x^{(i)} \circ \varepsilon^{(i)})_j \big) \big).
$$
We could also use a least-squares objective, $\frac{1}{n} \sum_{i=1}^n \sum_{j=1}^d \big|  x^{(i)}_j -  g_\theta( x^{(i)} \circ  \varepsilon^{(i)})_j \big|^2$, where the optimal $g(y)$ is $\E[x|y]$.


\subsection{Multiple measurements}
\label{sec:multiple}
\label{sec:sequential}
Following~\citet{saremichain},
if we assume that we have $m$ measurements $y_1,\dots,y_m \in \{-1,1\}^d$ obtained by adding independent noises to the same $x$, then we have from \eq{grad}:
\BEAS
\E[x|y_1,\dots,y_m]
& = & \frac{1}{m\alpha} \nabla \log q_{m\alpha}( \bar{y}_{1:m}),
\EEAS
with $\bar{y}_{1:m} = \frac{1}{m} ( y_1+\cdots + y_m ) \in [-1,1]^d$. Note that this requires to know the function $\nabla \log q_{m \alpha}$ beyond $\{-1,1\}^d$.


Since the denoiser has to ``work'' for $y \notin \{-1,1\}^d$, and for noise levels $m\alpha$, to learn the score function, we can simply generate multiple measurements and average the corresponding $y$'s (in the Gaussian case, this was possible directly by adding a noise with variance divided by $m$), and use the same denoising objective as \mysec{learning-score}.



\paragraph{Denoising performance.}
We can extend the denoising performance result when given $m$ measurements by studying the sign of $ y_1+\cdots + y_m$. This is uniquely defined as soon as $m$ is odd, and when $m$ is even, and $y_1+\cdots + y_m$ is equal to zero, we output $-1$ or $1$ with equal probabilities. We can then extend Lemma~\ref{lemma:denoising-performance} (see the proof based on Chernoff's bound in Appendix~\ref{app:denoising-performance-m}).

\begin{lemma}[Denoising performance, multiple measurements] \label{lemma:denoising-performance-m}
For the noise model defined in \eq{q} and $m$ measurements, we have:
$$
W( \mbox{law of } x, \mbox{law of } \sign(\E[x|y_1, \dots, y_m]))
\leqslant d e^{-m\alpha}.
$$
\end{lemma}


\paragraph{Score functions for sequential sampling.} Extending the Gaussian case, multiple measurement can be efficiently sampled \emph{sequentially}. If we want to sample $y_m$ given $y_1, \dots, y_{m-1}$ using score functions, we have the joint probability mass function:
$$p(y_1,\dots,y_m)=
\sum_{x \in \{-1,1\}^d}
p(x) \frac{e^{\alpha x^\top (y_1 + \cdots + y_m ) }}{(2 \cosh \alpha)^{dm}},
$$
with
\BEA \notag
\nabla_{y_m}
\log p(y_1,\dots,y_m) = \alpha \frac{\sum_{x \in \{-1,1\}^d}
p(x) x e^{\alpha x^\top (y_1 + \cdots + y_m ) }}
{\sum_{x \in \{-1,1\}^d}
p(x) e^{\alpha x^\top (y_1 + \cdots + y_m ) }} =   \frac{1}{m} \nabla \log q_{m\alpha}( \bar{y}_{1:m}),
\EEA
which depends on the score function with parameter $m\alpha$, taken at $\bar{y}_{1:m} \in [-1,1]^d$ (as mentioned earlier we are able to learn a score functions for elements in the interior of the hypercube).


\section{Sampling with discrete Langevin}
We need to sample from the model in \eq{q}, for a probability mass function $q$ which is defined not only on all $y \in \{-1,1\}^d$ but on $\rb^d$, for which we only know $s(y) = \nabla \log q(y) $. This is the same for the conditional sampling from \mysec{multiple}.

\paragraph{Assumptions.}
Note that the density $q$ is uniquely defined up to a constant for vertices of the hypercube, but that there are multiple versions on the whole hypercube. In particular, one can add to the score $s$ any linear function (and there are additional invariances).

We will use the following regularity conditions on $s$ that are satisfied by~$q$ defined in \eq{q}, that is, for all $y, y' \in \rb^d$, 
\BEQ
\label{eq:reg}
\| s(y) \|_\infty \leqslant \beta_1, \mbox{ and } 
\| s(y) - s(y') \|_\infty \leqslant \beta_2 \| y - y \|_1. 
\EEQ
For the function $q$ defined in \eq{q} and the associated $s = \nabla \log q$, we have $\beta_1 = \alpha$ and $\beta_2 = \alpha^2$ (This is a direct consequence of taking another derivative in \eq{grad}, leading to $
\nabla^2 \log q(y) = \alpha^2 {\rm cov}(x|y).$). The same bounds hold for sequential sampling from \mysec{sequential} (thus benefitting from the same speed as a small $\alpha$ while denoising has the same performance as $m \alpha$).



\subsection{One-stage discrete Langevin sampler}
\label{sec:langevin-1}
In order to obtain an approximate sample from $q$, we consider the following Markov transition kernel proposed by~\citet{zhang2022langevin}, which is adapted to log-probability-mass functions that are defined on $\rb^d$:
\BEA
\label{eq:t} 
t(y'|y)  \propto   \exp\big( \frac{1}{2} s(y)^\top ( y' - y)  - \frac{1}{2\eta} \| y' - y \|_2^2 \big) \propto   \exp\big(\big( \frac{1}{2} s(y) + \frac{1}{\eta} y \big)^\top y'  \big),
\EEA
which given $y$, has independent components for $y'$.
Note that without the constraint that $y' \in \{-1,1\}^d$, the second expression above becomes $y'\!=\!  y\! +\! \frac{\eta}{2} s(y) \!+\!  \mathcal{N}(0,\eta\idm)$, i.e., \emph{exactly} (Gaussian) Langevin MCMC.

As opposed to Gaussian Langevin, even with a vanishing step-size $\eta$, the stationary distribution of this Markov chain may not approach $q$. We can however prove convergence results that are adapted to our situation.



\paragraph{Convergence results.}
We now provide two propositions characterizing the convergence of the Markov chain defined in \eq{t}. The first proposition below implies an exponential convergence of the Markov chain (which is not studied by~\citet{zhang2022langevin}). See proof in Appendix~\ref{app:contractivity}. 
\begin{proposition}[Contractivity]
\label{prop:contractivity}
Assume regularity conditions in \eq{reg} with  $4\beta_2 d e^{2\beta_1} \leqslant 1$. Given $y,z \in \{-1,1\}^d$, we have, for the transition kernel defined in \eq{t}:
$$
W( t(\cdot|y) , t(\cdot| z) ) \leqslant 
\big(  
  1 - \frac{1}{2} e^{  - \frac{2}{\eta} -   \beta_1}  \big) \ell(y,z).
$$
\end{proposition}

We note that when $\beta_1 = \alpha$ and $\beta_2 = \alpha^2$, the constraint becomes $4\alpha^2 d e^{2\alpha} \leqslant 1$ and is satisfied as soon as $\alpha \leqslant \frac{1}{4\sqrt{d}}$, without any assumption about the distribution we want to sample from.  



From the previous proposition, we deduce that for each $\eta>0$, the Markov chain always converge exponentially fast to the unique stationary distribution for the Wasserstein distance~\citep{levin2017markov}. Given the exponential rate in $1-\frac{1}{2}e^{-2/\eta-\alpha}$, we can choose a step-size $\eta = 1/\alpha$ without losing too much in mixing time (this extends the strategy of~\citet{saremi2019neural} in the Gaussian case, where the step-size for the Langevin algorithm is taken to be the noise variance).


We now show that the stationary distribution of the Markov chain defined by the transition kernel $t$ cannot be too far from the one of $y$. See proof in Appendix~\ref{app:stationary}. Note that \citet{zhang2022langevin} only study the situations where the log-density is quadratic (or close to quadratic) without explicit constants.

\begin{proposition}[Distance to stationary distribution]
\label{prop:stationary}
Assume regularity conditions in \eq{reg} with  $4\beta_2 d e^{2\beta_1} \leqslant 1$.  Let $q'$ be the stationary distribution of the transition kernel defined in \eq{t}. Then,
$$W(q',q) \leqslant 2d \big(
     2 d \beta_1  e^{2\beta_1} + \sqrt{d \beta_1  e^{2\beta_1}}
     \big).$$
\end{proposition}
In our situation where $\beta_1=\alpha$ and $\beta_2 = \alpha^2$, this is small compared to the diameter $d$ of the hypercube (its maximal value) only when $\alpha$ is small compared to $1/d$.






\subsection{Two-stage Langevin sampler} \label{sec:two-stage-langevin}
\label{sec:langevin-2}

The default Langevin sampler does not have the nice property that if $q(y) = e^{s^\top y}$ for some $s \in \rb^d$ (i.e., independent components), then the stationary distribution is exact.

Following~\citet{lee2021structured,chewi}, we consider instead the idealized two-stage sampler defined below, which is Gibbs sampling for the joint model on $(y,z)$ in~$ \{-1,1\}^d \times \{-1,1\}^d$:
$$ 
q(y,z) = q(y) \frac{1}{(2 \cosh \frac{1}{\eta} )^d} 
e^{ \frac{1}{\eta} y^\top z }\propto 
q(y)  e^{ \frac{1}{\eta} y^\top z },
$$
for which the conditional distributions can be computed as
\BEAS
q(z|y) & =  &  \frac{1}{(2 \cosh \frac{1}{\eta} )^d} e^{ \frac{1}{\eta} y^\top z }\\[-.05cm]
q(y|z) & \propto & q(y)  e^{ \frac{-1}{2 \eta} \| y - z\|_2^2 },
\EEAS
which we approximate by expanding $\log q(y) \approx \log q(z) + s(z)^\top ( y-z)$, leading to:
\BEA
\label{eq:uu} u(z|y) & =  &  q(z|y) = \frac{1}{(2 \cosh \frac{1}{\eta} )^d}e^{ \frac{1}{\eta} y^\top z } \\[-.05cm]
\label{eq:u} u(y|z) & \propto & q(z) e^{ \nabla \log q(z)^\top ( y - z) }  e^{ \frac{-1}{2 \eta} \| y - z\|_2^2 } \propto    e^{ y^\top ( \frac{z}{\eta} + s(z)   ) }.
\EEA
Thus the Markov chain (with transition kernel $v$) defined by Gibbs sampling to go from $y$ to $y'$ is defined as follows:  (1) take $z$ with a random uniform flip with probability $1-\sigma(2/\eta)$, and then perform independent (non uniform) flips with probability $1- \sigma( 2/\eta + 2 z_i s(z)_i)$ to obtain $z'$.  


When $\log q(y)$ is linear in $y$, then the proposals defined by~$u$ are equal to the ones defined by $q$ (and thus the stationary distribution is exact).
Otherwise, we can show the following contractivity result (see proof in Appendix~\ref{app:contractivity-twostage}).

\begin{proposition}[Contractivity, two-stage sampler]
\label{prop:contractivity-twostage}
Assume regularity conditions in \eq{reg} with  $8d \beta_2   e^{4\beta_1} \leqslant 1$. Given $ y^{(1)}, y^{(2)} \in \{-1,1\}^d$, we have, for the transition kernel $v$ defined in \eq{uu} and \eq{u}:
$$
W( v(\cdot|y^{(2)}) , v(\cdot| y^{(1)}) ) \leqslant
\big(  
  1 - \frac{1}{2} e^{  - \frac{2}{\eta} -   2\beta_1}  \big) \ell( y^{(1)}, y^{(2)}).
$$
\end{proposition}

Like in \mysec{langevin-1}, this leads to exponential convergence to a unique stationary distribution. 
We can now look at the distance between the stationary distribution of the Markov chain and $q$. We make the assumption that the score $s$ that we use satisfies the usual inequality of convex smooth functions~\citep[Section 5.2]{ltfp}, that is, for all $y,z \in \{-1,1\}^d$,
\BEQ
\label{eq:CCA}
0 \! \leqslant \!\log q(y) \!-\! \log q(z)\! -\! s(z)^\top ( y\!-\! z) \! \leqslant \! \frac{\beta_2}{2} \| y \!-\! z\|_1^2,
\EEQ
which is satisfied by $s(y) = \nabla \log q(y)$ in \eq{q}. See proof of Prop.~\ref{prop:stationary-2} in Appendix~\ref{app:convergence-twostage}.

\begin{proposition}[Distance to stationary distribution]
\label{prop:stationary-2}
Assume regularity conditions in \eq{reg} and \eq{CCA} with  $8 \beta_2 d e^{4\beta_1} \leqslant 1$.  
Assume  $e^{-2/\eta + 2 \beta_1} \leqslant \frac{1}{d}$. Let $q'$ be the stationary distribution of the two-stage sampler. Then,
$$W(q',q) \leqslant 12 d \sqrt{ \beta_2 d}.$$
\end{proposition}
Moreover, as shown in Appendix~\ref{app:convergence-twostage}, if the step-size $\eta$ is small enough, we get a dependence in $d \cdot d \beta_2$.

For our problem where $\beta_1=\alpha$ and $\beta_2 = \alpha^2$, the constraint in $\eta$ leads to a mixing time proportional to $d$, but to a distance to the true distribution $q$ proportional to $d \cdot \alpha^2 d$ or $d \cdot \alpha \sqrt{d}$ as opposed to $d \cdot \sqrt{\alpha} d$ for the one-stage sampler, thus an advantage for small $\alpha$ (high noise), where we only need $\alpha \ll 1/\sqrt{d}$ instead of $\alpha \ll 1/d$ for one-stage sampling.




\section{Comparison with Gaussian noise}

An alternative is to add Gaussian noise and define
$$
y_{\rm G} = x  + \varepsilon,\ \varepsilon \sim \mathcal{N}(0,\frac{1}{\alpha}\idm).
$$
We then have
$
\E [ x | y_G]
 = y_G + \frac{1}{\alpha} \nabla \log q_\alpha^{\rm G} (y_{\rm G}),
$
from the classical Tweedie-Miyasawa formula, with $q_\alpha^{\rm G}$ the density of $y_G$:
\begin{equation*}
q_\alpha^{\rm G}(y_{\rm G}) \propto  \sum_{x \in \{-1,1\}^d}\!\! p(x) e^{ \alpha x^\top y} e^{- \frac{\alpha}{2} \| y\|_2^2 }
 e^{- \frac{\alpha}{2} \| x\|_2^2 }
 \propto \sum_{x \in \{-1,1\}^d}\!\! \!p(x) e^{ \alpha x^\top y} e^{- \frac{\alpha}{2} \| y\|_2^2 }
 \propto q_\alpha(y_{\rm G}) e^{- \frac{\alpha}{2} \| y\|_2^2 },
\end{equation*}
where $q_\alpha$ is the density of the binary case defined in \eq{q}. Thus,
$$
\E [ x | y_{\rm G}] = \frac{1}{\alpha} \nabla \log q_\alpha (y_{\rm G}),
$$
that is, the exact same denoiser function (now applied to an element of $\rb^d$ and not $\{-1,1\}^d$).

In terms of denoising performance, for the same $\alpha$, we see in our experiments that they behave similarly. However, in terms of mixing time of Langevin,
 for the Gaussian case (e.g., when sampling by adding Gaussian noise), the known upperbounds based on log-concavity obtained for Gaussian mixtures by \citet{saremichain} is $\alpha \leqslant \frac{1}{d}$, which is significantly worse than our two-stage sampler.





\section{Experiments}
We now study how our new sampling scheme operates, first on synthetic data to understand the role of the noise parameter $\alpha$ and step-size $\eta$, then on binarized MNIST digits.

\subsection{Synthetic data}
We consider in this section mixtures of two independent binary vectors, that is,
we consider, for $\beta>0$,
$$
p(x) = \frac{1}{(2 \cosh \beta)^d}
\Big[ \frac{1}{2} e^{\beta 1_n^\top x}  +
\frac{1}{2} e^{-\beta 1_n^\top x} \Big],
$$
for which all score functions can be computed (see Appendix~\ref{app:mixtures}). When $\beta$ is small, $p$ is close to the uniform distribution (a ``weak'' prior), while when $\beta$ is large, $p$ is close to a sum of two Diracs at opposite points in the hypercube (a ``strong'' prior).

For $d$ small ($d=8$ below), it is possible to perform all computations in closed form, (e.g., with infinitely many replications), by computing transition matrices of size $2^d \times 2^d$. This allows to analyze precisely the denoising performance.


\textbf{Denoising performance with exact samples from $y$.} In Fig.~\ref{fig:denoising}, we consider three values of $\beta$ and vary $\alpha$. As expected, when $\alpha$ decreases, the noise increases, and the denoising performance degrades. When $\beta$ is large, the prior has a strong effect, so denoising helps. When $\beta$ is small, the prior is not strong, denoising has little effect. Note also that when $\alpha$ is large, denoising simply outputs $y$ (the threshold where it happens depend on the strength of the prior).

Moreover, since the upper bound in Lemma~\ref{lemma:denoising-performance} is obtained from the mean-square-error, it shows that the denoising performance is significantly better than the bound suggests.

 

\begin{figure}
\begin{center}
\hspace*{-1cm}   
% from text_mixture_population.m
\includegraphics[width=0.85\columnwidth]{figs/mixture/denoising_betas_final}
% from test_mixture_population.m
\end{center}
\vspace*{-.6cm}

\caption{Optimal denoising from strong priors (large $\beta$) to weak priors (small $\beta$): comparison between Wasserstein distance and mean-square-error of denoising performance.
\label{fig:denoising}}
\end{figure}


\textbf{Denoising performance with multiple measurements.} 
In Fig.~\ref{fig:multiple}, we assess the benefits of multiple measurements by showing how the Wasserstein distance between our desired (noiseless) distribution on $x$ is estimated more closely by optimal denoising from $m$ measurements when $m$ is increasing, in particular for small $\alpha$ (high noise).

\begin{figure}
\begin{center}
\hspace*{-.85cm}    
\includegraphics[width=0.85\columnwidth]{figs/mixture/multiple_measurements}
% from test_multiple_measurements.m
\end{center}

\vspace*{-.6cm}

\caption{Optimal denoising from multiple measurements, for $m=1,3,5$ (one curve per $m$), for $d=6$, and three values of $\beta$, from strong priors (large $\beta$) to weak priors (small $\beta$).
\label{fig:multiple}}
\end{figure}



\textbf{Comparisons of mixing times and distance to stationary distribution.}
We compare our two  sampling schemes (one-stage from \mysec{langevin-1}, and two-stage from \mysec{langevin-2}), and study the associated step sizes in terms of distance between the stationary distribution of the Markov chain and the desired distribution, and mixing time, which is here characterized by $1/(1-\lambda_2)$, where $\lambda_2$ is the second largest eigenvalues of the transition matrix, a classical characterization of mixing time~\citep{levin2017markov}.


We see in Fig.~\ref{fig:mix} that when the step-size $\eta$ is too small, the mixing time explodes for all schemes (as predicted by Props.~\ref{prop:contractivity} and~\ref{prop:contractivity-twostage}), and that that for $\eta = 1/\alpha$ we obtain reasonable mixing times.


\begin{figure}[t!]
\begin{center}
\hspace*{-.7cm}    
\includegraphics[width=0.75\columnwidth]{figs/mixture/mixing_times_vs_lr_noMH}
% from sample_mixture_population_test_stationary_noMH.m
\end{center}

\vspace*{-.62cm}

\caption{Comparison of 1-stage and 2-stage Langevin sampling. Top: distance to desired distribution $W(y,y_{\rm stat})$, bottom: mixing time (in log scale).\label{fig:mix}}
\end{figure}


 \textbf{Denoising performance with samples obtained by discrete Langevin.} We consider in Fig.~\ref{fig:perfmix} a learning rate equal to $1/\alpha$, and plot the Wasserstein distance to the distribution of $x$ for our two samplers. When $\alpha$ is large, the stationary distribution is far from the one of $y$, with bad performance. With $\alpha$ small, then the denoising performance is not great because too much noise is added. When $\beta$ is large (right plot), there is a clear sweet spot. Moreover, the two-stage sampler only provides improvements for small $\alpha$'s. 

 

\begin{figure}[t!]
\begin{center}
\hspace*{-.55cm}    
\includegraphics[width=0.75\columnwidth]{figs/mixture/stationary_betas_final}
% from sample_mixture_population_final.m
\end{center}

\vspace*{-.62cm}

\caption{Comparison of 1-stage and 2-stage Langevin sampling. Top: distance to desired distribution $W(y,y_{\rm stat})$, bottom: denoising performance of the stationary distributions, measured in Wasserstein distance. \label{fig:perfmix}}
\end{figure}





\begin{figure}[t!]
    \centering
    % Subfigure 1
    \subfigure[$x$ from the test set]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/x-run330.png}
        \label{fig:mnist-denoise:a}
    }
%    \hfill
    % Subfigure 2
    \subfigure[$y=x \circ \varepsilon$, $\alpha=0.25$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/y-run331.png}
        \label{fig:mnist-denoise:b}
    }    
%    \hfill
    % Subfigure 3
    \subfigure[$\E\text{[}x|y\text{]}$, $\alpha=0.25$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/xhat-run331.png}
        \label{fig:mnist-denoise:c}
    }    
    \subfigure[$\sign(\E\text{[}x|y\text{]})$, $\alpha=0.25$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/xhat-sign-run331.png}
        \label{fig:mnist-denoise:d}
    }    
    \subfigure[$y=x \circ \varepsilon$, $\alpha=0.5$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/y-run330.png}
        \label{fig:mnist-denoise:e}
    }
%    \hfill
    % Subfigure 3
    \subfigure[$\E\text{[}x|y\text{]}$, $\alpha=0.5$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/xhat-run330.png}
        \label{fig:mnist-denoise:f}
    }
    \subfigure[$\sign(\E\text{[}x|y\text{]})$, $\alpha=0.5$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/xhat-sign-run330.png}
        \label{fig:mnist-denoise:g}
    }    
   
    
    \caption{The denoising performance on binarized MNIST at two high Bernoulli noise levels ($\alpha=0.25$, and $\alpha=0.5$). }
    % all moderls are atrained with the cross-entropy loss
    \label{fig:mnist-denoise}
\end{figure}

\subsection{Binarized MNIST} \label{sec:mnist}
In this section we present our experiments on MNIST~\citep{lecun1998gradient}. The clean binary data were prepared by scaling the pixel values be in $[0,1]$ which we set as the probability of the Bernoulli distribution. The denoising is set up using logistic regression as outlined in \mysec{learning-score}, where we parametrize $f_\theta$ using the U-Net architecture~\citep{ronneberger2015u} with the modifications by~\citet{dhariwal2021diffusion}. For optimization, we used AdamW~\citep{loshchilov2019decoupled} with the constant learning rate of $10^{-4}$ and the weight decay of $10^{-2}$. 
We present our experiments for $\alpha \in \{ 0.25, 0.5, 1, 2 \}$ in Figs.~\ref{fig:mnist-denoise} and~\ref{fig:mnist-langevin}.

\textbf{Denoising performance.} Fig.~\ref{fig:mnist-denoise:a} shows 20 random binarized samples from the test set.  Fig.~\ref{fig:mnist-denoise} (b-d) shows the denoising performance of a trained model for very high noise, $\alpha=0.25$, where we show both $\mathbb{E}[x|y]$, parametrized as $\tanh(f_\theta(y)/2)$ (see \mysec{learning-score}) and $\sign(\mathbb{E}[x|y])$ which is optimal under the Hamming loss. In Fig.~\ref{fig:mnist-denoise} (e-g) we repeat this experiment for a trained model at lower noise level $\alpha=0.5$. Qualitively, this is a sweet spot as the noise is high, yet the denoising performance is acceptable. Note the ``5'' flipping to ``3'', and ``3'' flipping to ``8'' by the denoiser due to the high noise. This already anticipates the fast mixing that could be achieved at this level of noise.



\textbf{Sampling performance.} Fig.~\ref{fig:mnist-langevin} (a-f) illustrates the mixing performance of our algorithm for various noise levels. The step-size $\eta$ is set to $1/\alpha$ in all experiments. All panels show 100 steps of the algorithm, where the sampler is initialized at random bits. In Fig.~\ref{fig:mnist-langevin} (a-c) we see the performance of the algorithm in ``real time'', where all the steps are shown ($\Delta k \!=\! 1$). These results show the remarkable mixing our algorithm can achieve. Fig.~\ref{fig:mnist-langevin} (d-e) shows the typical performance of the algorithm for smaller noise $\alpha=1$, where the samples are sharper but there is less mixing; here the results are shown by skipping 5 steps ($\Delta k \!=\! 5$). Finally, in Fig.~\ref{fig:mnist-langevin:f} we see the sampling performance for smaller noise ($\alpha=2$), where the sampling algorithm simply breaks down.



\begin{figure}[t!]
    \centering
    \subfigure[two-stage discrete Langevin ($\Delta k = 1$), $\alpha=0.5$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-twostage-walk-run330.png}
        \label{fig:mnist-langevin:a}
    }
        \subfigure[denoised samples, $\alpha=0.5$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-twostage-jump-run330.png}
        \label{fig:mnist-langevin:b}
    }
        \subfigure[vanilla discrete Langevin ($\Delta k = 1$),  $\alpha=0.5$, denoised]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-jump-run330.png}
        \label{fig:mnist-langevin:c}
    }    
        \subfigure[two-stage discrete Langevin ($\Delta k = 5$), $\alpha=1$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-twostage-walk-run352-deltaK5.png}
        \label{fig:mnist-langevin:d}
    }  
        \subfigure[two-stage discrete Langevin ($\Delta k = 5$), $\alpha=1$, denoised]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-twostage-jump-run352-deltaK5.png}
        \label{fig:mnist-langevin:e}
    }      
        \subfigure[two-stage discrete Langevin ($\Delta k = 5$), $\alpha=2$]{
        \includegraphics[width=0.75\columnwidth]{figs/mnist/wj1-twostage-walk-run353-deltaK5.png}
        \label{fig:mnist-langevin:f}
    }         

    % \vspace*{-.25cm}

    
    \caption{The sampling performance of our algorithm for binarized MNIST at three Bernoulli noise levels, visualized on single Markov chains (viewed left to right, top to bottom). (a) Two-stage discrete Langevin at $\alpha=0.5$, (b) the denoised samples are shown, (c) due to space only denoised samples are shown for the vanilla (single-stage) algorithm, (d,e) here, $\alpha=1$, and we skip every 5 steps, (f) $\alpha=2$, denoised samples are not shown as the noise is small. }
    \label{fig:mnist-langevin}
\end{figure}

\section{Conclusion} This study was motivated by whether we can reproduce the success of sampling through denoising while staying within the binary world. This required to reproduce the three key factors: (i) denoising through score functions, (ii) sampling noisy data via ``smoothed'' score functions, (iii) benefiting from multiple Bernoulli measurements. We achieved all three in an algorithmically simple framework.

There are several avenues for future research: (1) our framework relies on using a noise process from an exponential family (here, Bernoulli) and can readily be extended to more complex ones; (2) sharper denoising results for strong priors could also be examined; finally, (3) faster sampling could be achieved through the proper use of Metropolis-Hasting's step~\citep{robert2004metropolis}.


\subsection*{Acknowledgements}
This work has received support from the French government, managed by the National Research Agency, under the France 2030 program with the reference "PR[AI]RIE-PSAI" (ANR-23-IACL-0008).





% \clearpage




\bibliography{denoising}
\bibliographystyle{apalike}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% \usepackage[top=1in, bottom=1in, left=0.5in, right=1in]{geometry}

\section{Proof of Lemma~\ref{lemma:denoising-performance-m}}
\label{app:denoising-performance-m}

We denote $\varepsilon_1,\dots, \varepsilon_m \in \{-1,1\}^d$ the $m$ independent noise variables defined in \eq{noise}. We assume that $m$ is odd for simplicity. The case $m$ even can be done similarly by splitting the case where $\sum_{i=1}^m \varepsilon_i=0$. 

Following the same reasoning that in the proof of Lemma~\ref{lemma:denoising-performance-m}, the Wasserstein distance is less than $d$ times the probability that $\sum_{i=1}^m (\varepsilon_i)_1 \leqslant 0$. Since $(\varepsilon_i)_1 = 2 u_i - 1$ where $u_i$ is a Bernoulli random variable with parameter $\sigma(2\alpha) \in [1/2,1]$. We need to upper bound, using the Chernoff bound,\footnote{See \url{https://en.wikipedia.org/wiki/Chernoff_bound}.} the following probability as: 
\BEAS
\P \Big( \frac{1}{m} \sum_{i=1}^m u_i \leqslant \frac{1}{2} \Big)
& \leqslant & \exp( - m \cdot D\big(\frac{1}{2} \| \sigma(2\alpha)\big)\big),
\EEAS
where, for $\alpha \geqslant 0$,
\BEAS
D\big(\frac{1}{2} \| \sigma(2\alpha)\big)
& = & \frac{1}{2} \log \frac{1}{2 \sigma(2\alpha)}
+\frac{1}{2} \log \frac{1}{2 \sigma(-2\alpha)}\\
& = & \frac{1}{2} \log \frac{1+e^{-2\alpha}}{2}
+\frac{1}{2} \log \frac{1+e^{2\alpha}}{2 } = - \log(2) + \frac{1}{2} \log( 2 + 2 \cosh (2\alpha))
\\
& = &   - \frac{1}{2}\log(2) + \frac{1}{2} \log( 1 +   \cosh (2\alpha)) \leqslant \alpha,
\EEAS
by convexity and the fact that the derivative of the function above tends to one at infinity. This leads to the desired result.



\section{Proof of Proposition~\ref{prop:contractivity}}
\label{app:contractivity}

\begin{proof}
   We consider two random variables $y'$ and $z'$ marginally distributed from $t(\cdot|y)$ and $ t(\cdot|z)$. We have, by definition of the Wasserstein distance:
\BEA
\notag W( t(\cdot|y) , t(\cdot| z) ) & = &
  \inf_{\rm joint \ coupling} \sum_{i=1}^d \P( y_i' \neq z_i' ) \mbox{ by definition of } W, \\ & \leqslant    & \sum_{i=1}^d \inf_{\rm marginal \ coupling}  \P( y_i' \neq z_i') \\
\notag  & & \hspace*{.1cm} \mbox{because we can construct canonically a joint coupling from marginal couplings,} \\
\notag & = &    \sum_{i=1}^d \big| \P(y'_i = 1  ) -\P(z'_i = 1  )
\big|   \mbox{ because of properties of total variation,}
\\
\label{eq:G} & = &   \sum_{i=1}^d \big|\sigma\big(  \frac{2}{\eta} y_i +  s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta} z_i +   s(z)_i\big )  
\big|,
\EEA
by definition of the transition kernel $t$ in \eq{t} For proprieties of the total variation distance, see {\small
\url{https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures}}. We can then separate $i$'s according to  $y_i=z_i$ or $y_i = z_i$, to get from \eq{G}:
\BEAS
W( t(\cdot|y) , t(\cdot| z) ) 
& \leqslant  &   \sum_{i, y_i = z_i }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +    s(z)_i\big )  
\big| \\
& & 
+  \sum_{i, y_i = -  z_i }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +  s(z)_i\big )  
\big|.
\EEAS
We can now divide in two cases, whether $y_i=1$ or $+1$, leading to
\BEAS
W( t(\cdot|y) , t(\cdot| z) )
& \leqslant & 
  \sum_{i, y_i = z_i = 1 }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +    s(z)_i\big )  
\big| \\
  & & + \sum_{i, y_i = z_i = -1 }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +    s(z)_i\big )  
\big| \\
& & 
+  \sum_{i, y_i = -  z_i = 1 }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +  s(z)_i\big )  
\big|
\\
& & +  \sum_{i, y_i = -  z_i = -1 }\big|\sigma\big(  \frac{2}{\eta} y_i +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}  z_i +  s(z)_i\big )  
\big| \\
& = & 
  \sum_{i, y_i = z_i = 1 }\big|\sigma\big(  \frac{2}{\eta}   +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}    +    s(z)_i\big )  
\big| \\
  & & + \sum_{i, y_i = z_i = -1 }\big|\sigma\big(  -\frac{2}{\eta}  +   s(y)_i\big )  
-\sigma\big(  -\frac{2}{\eta}   +   s(z)_i\big )  
\big| \\
& & 
+  \sum_{i, y_i = -  z_i = 1 }\big|\sigma\big(  \frac{2}{\eta}  +   s(y)_i\big )  
-\sigma\big(  -\frac{2}{\eta}    +  s(z)_i\big )  
\big|
\\
& & +  \sum_{i, y_i = -  z_i = -1 }\big|\sigma\big(  -\frac{2}{\eta}  +   s(y)_i\big )  
-\sigma\big(  \frac{2}{\eta}   + s(z)_i\big )  
\big|
\EEAS
We can now use the facts that $\sigma(-u)=1-\sigma(u)$, and that for $v,v' \geqslant u$, $\sigma'(v) = \sigma(v) \sigma(-v)  = \frac{1}{2 + e^{-v}+e^v}\leqslant \frac{1}{2+e^{u}}$, and thus, by Taylor's formula, $|\sigma(v)-\sigma(v')| \leqslant  \frac{1}{2+e^{u}} | v - v'| $, to get
\BEAS
W( t(\cdot|y) , t(\cdot| z) )
& \leqslant &    \sum_{y_i = z_i } \frac{1}{2+ \exp\big(   \frac{2}{\eta} - \beta_1  \big)}   	\big|    s(y)_i -   s(z)_i 
\big| \\
& & 
+    \sum_{i } 1_{y_i \neq z_i} \big|  \sigma\big(   \frac{2}{\eta} +    y_i s(y)_i\big )  
-\sigma\big(  -\frac{2}{\eta}  -y_i  s(z)_i\big )  
\big|.
\EEAS
We can now use the monotonicity of $\sigma$ and the bounds  $ \|  s(y)\|_\infty, \| s(z)\|_\infty \leqslant \beta_1$, and the $\beta_2$-Lipschitz-continuity of $s$ (all from \eq{reg}) to get
\BEAS
W( t(\cdot|y) , t(\cdot| z) )
& \leqslant &   \sum_{y_i = z_i } \frac{1}{2+ \exp\big(   \frac{2}{\eta} - \beta_1  \big)}  	2 \beta_2 \ell(y,z) 
  \\
& & 
+    \sum_{i } 1_{y_i \neq z_i} \Big(  \sigma\big(  \frac{2}{\eta} +     \beta_1\big )  
-\sigma\big(  - \frac{2}{\eta}  -   \beta_1 \big )  
\Big)
\\
& \leqslant &    \frac{1}{2+ \exp\big(   \frac{2}{\eta} - \beta_1  \big)} 	2 \beta_2  d \ell(y,z)
 +
\big| 1 - 2 \sigma\big(  - \frac{2}{\eta} -    \beta_1\big )  
\big| \cdot  \ell(y,z)
\\
& \leqslant &  \Big[
\frac{2\beta_2 d}{2+ \exp\big(   \frac{2}{\eta} - \beta_1  \big)} 
+ 1 - \frac{2}{1+\exp\big(   \frac{2}{\eta} + \beta_1  \big) }
\Big] \cdot   \ell(y,z) \\
& \leqslant &
\Big[
  1 - \exp \big(  - \frac{2}{\eta} -   \beta_1\big)
+     2\beta_2 d \exp\big(  -  \frac{2}{\eta} + \beta_1  \big)     
\Big] \cdot  \ell(y,z) .
\EEAS
If  $4\beta_2 d e^{2\beta_1} \leqslant 1$, then
$2\beta_2 d \exp\big(  -  \frac{2}{\eta} + \beta_1  \big)  
\leqslant \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   \beta_1\big)$,
and
we get the desired result:
$$
W( t(\cdot|y) , t(\cdot| z) )
  \leqslant   \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   \beta_1\big)  \Big)  \ell(y,z) .
$$
\end{proof}



\section{Proof of Proposition~\ref{prop:stationary}}

\label{app:stationary}

\begin{proof}
1For the purpose of the proof, we consider adding on top of the transition kernel $t$ a Metropolis-Hasting (MH) step~\citep[see, e.g.,][]{robert2004metropolis} that keeps the proposal $y'$ given $y$ unchanged with probability
\BEQ
\label{eq:accept}
  \min \Big\{ 1
,  \frac{q(y') t( y | y')}{q(y) t( y' | y)}
\Big\},
\EEQ
and go back to $y$ instead. The stationary distribution associated with this transition kernel is then exactly~$q$. 

 We consider an arbitrary probability distribution $r$.
 
 We consider two coupled samples $y$ from $r$ and $z$ from $q$, so that $W(r,s) = \E [ \ell(y,z) ]$. We also assume that, given $y,z$, the binary vectors $y', z'$ are sampled jointly respectively from $t(\cdot|y)$ and $t(\cdot|z)$, so that the Wasserstein distance given $y,z$ between 
 the distributions $t(\cdot|y)$ and $t(\cdot|z)$ is   $W(t(\cdot|y),t(\cdot|z)) = \E[ \ell(y',z')|y,z]$. We consider $z''$ obtained from $z'$ by a Metropolis-Hasting step; $z''$ is then marginally distributed from $q$, while $y'$ is marginally distributed according to $\sum_{u \in \{-1,1\}^d} r(u) t( \cdot |u )$. Thus, by definition of $W$ as the loss for the optimal coupling, we have:
 \BEAS && W\Big(  \sum_{u \in \{-1,1\}^d} r(u) t( \cdot |u ), q  \Big) \\
 & \!\!\!\leqslant  \!\!\!&  \E [ \ell(y',z'') ] 
 = \E [1_{ {\rm accept}(z,z')}  \ell(z'',y') ] 
 +  \E [1_{ {\rm reject}(z,z')}  \ell(z'',y') ] \\
& \!\!\! =  \!\!\!&  \E [1_{ {\rm accept}(z,z')}  \ell(z',y') ] 
 +  \E [1_{ {\rm reject}(z,z')}  \ell(z,y') ]  \mbox{ by definition of the MH step},\\
 &  \!\!\!\leqslant  \!\!\! & \E [1_{ {\rm accept}(z,z')}  \ell(z',y') ] 
 +  \E [1_{ {\rm reject}(z,z')}  (\ell(z',z) + \ell(z',y')) ] \mbox{ by the triangular inequality,}\\
 & \!\!\! =  \!\!\!&  \E [  \ell(z',y') ]  + 
  \E [1_{ {\rm reject}(z,z')}  \ell(z',z) ] \\
 & \!\!\! \leqslant  \!\!\!&  \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   \beta_1\big)  \Big) W(r,q)  + 
  \E [1_{ {\rm reject}(z,z')}  \ell(z',z) ],
 \EEAS
 from the convergence result in Proposition~\ref{prop:contractivity}.
We have, by definition of the accept probability in \eq{accept},
\BEA
\label{eq:rej}
\E [1_{ {\rm reject}(z,z')}  \ell(z',z) ] 
& = & \sum_{z,z' \in \{-1,1\}^d} q(z) t(z'|z) \ell(z',z)
\Big(
1 -  \min \Big\{ 1
,  \frac{q(z') t( z | z')}{q(z) t( z' | z)}
\Big\} \Big) \\
\notag & \leqslant & \frac{1}{2}  \sum_{z,z'}   \ell(z',z)
  \big| q(z) t(z'|z) 
-   q(z') t( z | z') 
\big| ,
\EEA
with the transition kernel defined in \eq{t}, that is,
\BEAS
t(z'|z) 
& \propto & \exp\big( ( \frac{1}{2} s (z) + \frac{1}{\eta}z)^\top z' \big) .
\EEAS
In order to prove a convergence result, we have to understand under which condition we obtain an approximate \emph{detailed balance} condition~\citep{levin2017markov}. This will be a consequence of $s$ being small (e.g., here, $\nabla \log q(z)$ small).

We define the two additional transition kernels and distributions
\BEAS
\hat{t}(z'|z) & \propto & \exp\big( (   \frac{1}{\eta}z)^\top z' \big) \\
\hat{q}(z) & \propto & 1,
\EEAS
for which we have the detailed balance condition $\hat{q}(z) \hat{t}(z'|z) 
-   \hat{q}(z') \hat{t}( z | z') = 0$.
We then get, from \eq{rej},
\BEAS
\E [1_{ {\rm reject}(z,z')}  \ell(z',z) ] 
& \leqslant & \frac{1}{2} \sum_{z,z'}   \ell(z',z)
  \big| q(z) - \hat{q}(z) | \cdot \hat{t}(z'|z) 
+\frac{1}{2} 
   \sum_{z,z'}   \ell(z',z)
  {q} (z) \cdot | \hat{t}(z'|z) -    t(z'|z) 
\big| ,
\EEAS using detailed balance for  $\hat{q}$ and $\hat{t}$.
For the left term, we can use the symmetry of $\hat{t}$ and an explicit computation, to get
\BEAS \sum_{z,z'}   \ell(z',z)
  \big| q(z) - \hat{q}(z) | \cdot \hat{t}(z'|z) 
  & = &   \sum_{z}    
  \big| q(z) - \hat{q}(z) | \cdot \sum_{z'}  \hat{t}(z'|1_d)  \ell(z'|1_d) \\
  & = &   \sum_{z}    
  \big| q(z) - \hat{q}(z) | \cdot
  \sum_{i=1}^d  \hat{t}(z'_i = - 1 |1_d)    \\
  & \leqslant & 2 d \sigma(-2/\eta)  
  {\rm TV}(q, \hat{q}) \leqslant 
  2 d \exp(-2/\eta)  
  {\rm TV}(q, \hat{q}),
     \EEAS
     where $ {\rm TV}(q, \hat{q}) = \frac{1}{2}  \sum_{z}    
  \big| q(z) - \hat{q}(z) |$ is the total variation distance\footnote{See \url{https://en.wikipedia.org/wiki/Total_variation_distance_of_probability_measures}.} between $q$ and $\hat{q}$.
 For the second term, we have
\BEAS
 \sum_{z,z'}   \ell(z',z)
  {q} (z) \cdot | \hat{t}(z'|z) -    t(z'|z) 
\big| & \leqslant &   
     \max_{z} \sum_{z'}\ell(z',z)  | \hat{t}(z'|z) -    t(z'|z) 
\big| \\
& \leqslant &   
  d \cdot  \max_{z} \sum_{z'} | \hat{t}(z'|z) -    t(z'|z) 
\big| \mbox{ since } \ell(\cdot,\cdot) \leqslant d,\\
& \leqslant &
 d \cdot  \frac{4d \beta_1 }{2 + e^{2/\eta-\beta_1}}.
 \EEAS
We have used the small lemma for the two probability mass functions on $\{-1,1\}^d$ proportional to $A(y) \propto e^{y^\top a}$ and $B(y) \propto e^{y^\top b}$:
$\sum_{y} | A(y) - B(y)| \leqslant 2 \sum_{i=1}^d |\sigma(2a_i) - \sigma(2b_i)|
 \leqslant 2 \sum_{i=1}^d \frac{2}{2+ \exp(\min\{2a_i,2b_i\})} | a_i - b_i| $.

 Overall, we get
 \BEAS W\Big(  \sum_{u \in \{-1,1\}^d} r(u) t( \cdot |u ), q  \Big)
  & \leqslant &  \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   \beta_1\big)  \Big) W(r,q)  + 
     d \exp(-2/\eta)  
  {\rm TV}(q, \hat{q})+
  d \cdot  \frac{2d \beta_1 }{ e^{2/\eta-\beta_1}}.
 \EEAS
 Thus, if $q'$ denotes the stationary distribution of the Markov kernel $t$, we get, applying the above inequality to $s=q'$,
 \BEAS W(q', q )
  & \leqslant &  \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   \beta_1\big)  \Big) W(q',q)  + 
     d \exp(-2/\eta)  
  {\rm TV}(q, \hat{q})+
  d \cdot  \frac{2d \beta_1 }{ e^{2/\eta-\beta_1}},
 \EEAS
 leading to  
 $$
 W(q',q) \leqslant 
   2 d e^{  \beta_1}
  {\rm TV}(q, \hat{q})+
     {4 d^2 \beta_1 } e^{ 2 \beta_1}. $$
We can now use Pinsker's inequality\footnote{See \url{https://en.wikipedia.org/wiki/Pinsker\%27s_inequality}.}, to get:
\BEAS
 {\rm TV}(q, \hat{q})
 &  \leqslant & \Big(
\frac{1}{2} {\rm KL}( q\| \hat{q} ) 
 \Big)^{1/2}
 = 
 \Big(\frac{1}{2} \E_{q(z)}[ \log q(z) - \log \frac{1}{2^d} ]
 \Big)^{1/2} \\
 & \leqslant & 
  \Big(\frac{1}{2} \E_{q(z)}[ \log q(z_0) + \beta_1 \| z - z_0\|_1 - \log \frac{1}{2^d} ]
 \Big)^{1/2} \mbox{ using the boundedness of } s, \\
 & \leqslant & 
  \Big(\frac{1}{2} \E_{q(z)}[   \beta_1 \| z - z_0\|_1   ]
 \Big)^{1/2}  \leqslant \sqrt{ \beta_1 d},
\EEAS
by choosing $z_0$ such that $q(z_0)\leqslant \frac{1}{2^d}$ (there has to be one). We thus get
 the desired result
 $$
 W(q',q) \leqslant 
   2 d e^{  \beta_1}
  \sqrt{ \beta_1 d} +
     {4 d^2 \beta_1 } e^{ 2 \beta_1}
     = 2d \big(
     2 d \beta_1  e^{2\beta_1} + \sqrt{d \beta_1  e^{2\beta_1}}
     \big). $$
\end{proof}


\section{Proof of Proposition~\ref{prop:contractivity-twostage}}

\label{app:contractivity-twostage}
 

\begin{proof}
We can reuse the proof for the one-stage sampler to obtain the contractivity of the second step of the two-stage sampler (updating the fact that we have no $1/2$ factor) to get a contracting rate
$$
\Big[
  1 - \exp \big(  - \frac{2}{\eta} -   2\beta_1\big)
+     4\beta_2 d \exp\big(  -  \frac{2}{\eta} + 2\beta_1  \big)     
\Big].
$$
We thus need the condition  $8\beta_2 d e^{4\beta_1} \leqslant 1$ to get  the contraction
$$\Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   2 \beta_1\big)\Big).
  $$
We now need to compute the contraction for the first step using simply $\beta_1=\beta_2=0$ in the same reasoning, leading to a contraction
$$\Big(  
  1 -   \exp \big(  - \frac{2}{\eta} \big)\Big).
  $$
  Multiplying the two contractions, we get
  $$
  \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   2 \beta_1\big)\Big)
  \Big(  
  1 -   \exp \big(  - \frac{2}{\eta} \big)\Big) \leqslant 
  \Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   2 \beta_1\big)\Big),
  $$
  which leads to the desired result.
\end{proof}


\section{Proof of Proposition~\ref{prop:stationary-2}}


\label{app:convergence-twostage}

\begin{proof}
    

We consider  $y^{(1)} \in \{-1,1\}^d $ distributed from an arbitrary distribution $r$, and $y^{(2)}$ sampled from the distribution $q$. We sample $z^{(1)}$ and $z^{(2)}$ from $u(\cdot | y^{(1)})$ and  $u(\cdot | y^{(2)})$, as well as $\bar{y}^{(1)}$ and $\bar{y}^{(2)}$, from $u(\cdot | z^{(1)})$ and  $u(\cdot | z^{(2)})$ (so that we get a full approximate Gibbs sampling step, with transition kernel $v$, from $y^{(1)}$ to $\bar{y}^{(1)}$ and $y^{(2)}$ to $\bar{y}^{(2)}$), all coupled so that the Wasserstein distance between
$$
\sum_{y^{(1)}} r(y^{(1)}) v(\cdot|y^{(1)})$$
and
$$
\sum_{y^{(2)}} q(y^{(2)}) v( \cdot|y^{(2)})$$
(that is, one step of the Markov transition kernel), is less than $\E[ \ell(\bar{y}^{(1)},\bar{y}^{(2)})]$.

For the purpose of the proof, we can now add a Metropolis Hasting step to the second chain (which leads to $\bar{\bar{y}}^{(2)}$) so that, since it starts from the stationary distribution $q$ of the full Gibbs sampling step, it remains at $q$. Thus, like in Appendix~\ref{app:stationary},
\BEAS
W\Big(
\sum_{y^{(1)}} r(y^{(1)}) v(\cdot|y^{(1)}), q \Big)
& \leqslant & \E[ \ell(\bar{y}^{(1)},\bar{\bar{y}}^{(2)})] \\
& \leqslant & 
\E[ \ell(\bar{y}^{(1)},{\bar{y}}^{(2)})]
+ \E[ \ell(\bar{y}^{(2)},\bar{\bar{y}}^{(2)})]
\\
& \leqslant & 
\Big(  
  1 - \frac{1}{2} \exp \big(  - \frac{2}{\eta} -   2\beta_1\big)\Big) W(r,q)
+ \E[1_{\rm reject}({y}^{(2)},\bar{y}^{(2)}) \ell(\bar{y}^{(2)},y^{(2)})].
\EEAS
Moreover, like in Appendix~\ref{app:stationary}, we have, now dropping the superscripts $^{(2)}$:
\BEAS\E[1_{\rm reject}(y,\bar{y}) \ell(\bar{y},y)]
& = & \frac{1}{2} 
\sum_{y, \bar{y}}
\ell(\bar{y},y) \big| q(y) v(\bar{y}|y) - q(\bar{y}) v(y| \bar{y}) \big|.
\EEAS
We have, using that $q(z|y) = u(z|y)$, and by definition of $v$,
$$
q(y) v(\bar{y}|y) = \sum_{z} q(y)  u(\bar{y}|z)u(z|y)  = \sum_{z} q(y) u(\bar{y}|z) q(z|y) 
 = \sum_{z} q(z) q(y|z) u(\bar{y}|z),
$$
because $q(y) q(z|y)=q(y,z) = q(z) q(y|z)$,
leading to:
\BEA
\E[1_{\rm reject}(y,\bar{y}) \ell(\bar{y},y)]
\notag & = & \frac{1}{2} 
\sum_{y, \bar{y}}
\ell(\bar{y},y) \bigg| \sum_{z} q(z) q(y|z) u(\bar{y}|z)- \sum_{z} q(z) q(\bar{y}|z) u(y|z) \bigg|
\\
\notag & = & \frac{1}{2} 
\sum_{y, \bar{y}}
\ell(\bar{y},y) \bigg| \sum_{z} q(z)
\big( q(y|z) u(\bar{y}|z)-  q(\bar{y}|z) u(y|z) \big) \bigg|
\\
\notag &\leqslant & \frac{1}{2}
\sum_{z,y,\bar{y}}\ell(\bar{y},y) q(z) 
\big| q(y|z) u(\bar{y}|z) 
- u(y|z) q(\bar{y}|z)\big|
\mbox{ by the triangular inequality,}
\\
\notag &\leqslant & \frac{1}{2}
\max_{z} \sum_{y,\bar{y}}\ell(\bar{y},y) 
| q(y|z) u(\bar{y}|z) 
- u(y|z) q(\bar{y}|z)|
\mbox{ by bounding the expectation by the max},
\\
\notag &= & \frac{1}{2}
\max_{z} \sum_{y,\bar{y}}\ell(\bar{y},y) 
| q(y|z) u(\bar{y}|z) - u(y|z) u(\bar{y}|z) + u(y|z) u(\bar{y}|z) 
- u(y|z) q(\bar{y}|z)|
\\
\notag &\leqslant & \frac{1}{2}
\max_{z} \sum_{y,\bar{y}}\ell(\bar{y},y) 
\Big\{
u(\bar{y}|z) \big| q(y|z)- u(y|z)  \big|   + u(y|z) \big| u(\bar{y}|z) 
-   q(\bar{y}|z)\big| \Big\}
\\
\notag &= & 
\max_{z} \sum_{y,\bar{y}}\ell(\bar{y},y) 
  u(y|z)|  q(\bar{y}|z) - u(\bar{y}|z)| \mbox{ by symmetry,}
\\
\notag & \leqslant & 
\max_{z} \sum_{y,\bar{y}} \big[\ell(\bar{y},z) + \ell(z,y) \big] 
  u(y|z)|  q(\bar{y}|z) - u(\bar{y}|z)| \mbox{ by the triangular inequality,}
\\
\notag & \leqslant & 
\max_{z} \bigg\{ \sum_{y,\bar{y}}  \ell(\bar{y},z)  
  u(y|z)|  q(\bar{y}|z) - u(\bar{y}|z)| 
  + \sum_{y,\bar{y}}    \ell(z,y)  
  u(y|z)|  q(\bar{y}|z) - u(\bar{y}|z)| \bigg\}
\\
\notag & & \hspace*{9cm} \mbox{ by separating the sum,}\\
\notag & = & 
\max_{z} \bigg\{ \sum_{\bar{y}}  \ell(\bar{y},z)  
  |  q(\bar{y}|z) - u(\bar{y}|z)| 
  + \sum_{y}  \ell(z,y)  
  u(y|z)
  \sum_{\bar{y}} 
  |  q(\bar{y}|z) - u(\bar{y}|z)| \bigg\}
  \\
 \notag  && \hspace*{7cm} \mbox{by summing out } y \mbox{ in the first term}, \\
 \notag & = & 
\max_{z} \sum_{\bar{y}}\Big[ \ell(\bar{y},z) + d  \sigma ( -\frac{2}{\eta} +2  \beta_1  ) \Big]
\cdot  |  q(\bar{y}|z) - u(\bar{y}|z)|, \\
 \label{eq:AAA}& \leqslant & 
\max_{z} \sum_{\bar{y}} \ell(\bar{y},z) 
\cdot  |  q(\bar{y}|z) - u(\bar{y}|z)|
+ 2d  \sigma ( -\frac{2}{\eta} +2  \beta_1  )  \max_{z}       {\rm TV} ( q(\cdot|z) , u(\cdot|z)),
\EEA
using that
$\displaystyle \sum_{y}  \ell(z,y)  
  u(y|z)= \sum_{i=1}^d \P_{u(y_i|z_i)}(y_i \neq z_i| z_i)
  =  \sum_{i=1}^d \sigma(-2/\eta - 2s(z)_i) \leqslant d\sigma(-2/\eta + 2\beta_1) $ and  that $u(\cdot|z)$ has independent components.

We can now write, because of our assumption in \eq{CCA},
$$
q(y|z) = u(y|z) \frac{1}{Z(z)} e^{
\varphi(y,z)},$$
with $\varphi(y,z)=\log q(y) - \log q(z) - s(z)^\top(y-z)$, which satisfies
  $$0 \leqslant \varphi(y,z) \leqslant  \frac{\beta_2}{2} \| y  - z\|_1^2$$
  and \BEA
  \notag \log Z(z) & = &  \log \sum_{y} u(y|z) e^{\varphi(y,z)} 
  \leqslant \log \sum_{y} u(y|z) e^{ \frac{\beta_2}{2} \| y  - z\|_1^2} \\
 \notag   & \leqslant & 
   \log \sum_{y} u(y|z) e^{ {d \beta_2} \| y  - z\|_1}
   = \sum_{i=1}^d \log \sum_{y_i} u(y_i|z_i) 
   e^{ {d \beta_2} | y_i  - z_1|}
   \mbox{ using that } \| y - z\|_1 \leqslant 2d, \\
 \notag    & \leqslant & 
   \sum_{i=1}^d \log 
   \Big( \sigma(2/\eta + 2s(z)_i)
   + \sigma(- 2/\eta - 2s(z)_i)
   e^{ {2 d \beta_2}}
   \Big) \mbox{ by definition of } u(\cdot|z),\\
  \notag    & = & 
   \sum_{i=1}^d \log 
   \Big( 1
   + \sigma(- 2/\eta - 2s(z)_i)
   (e^{ {2 d \beta_2}} - 1)
   \Big)
   \leqslant  d \log
   \Big( 1
   + \sigma(- 2/\eta + 2\beta_1)
   (e^{ {2 d \beta_2}} - 1)
   \Big)\\
 \label{eq:D}  &  \leqslant &   d \log
   \Big( 1
   + e^{ - 2/\eta + 2\beta_1}
   (e^{ {2 d \beta_2}} - 1)
   \Big) \leqslant d e^{ - 2/\eta + 2\beta_1}
   (e^{ {2 d \beta_2}} - 1) \mbox{ using } \log ( 1+ c) \leqslant c.
  \EEA
Moreover, we have $Z(z) \geqslant 1$.
 
We treat the two terms in \eq{AAA} separately. For the second term, we have, using Pinsker's inequality,
\BEAS
{\rm TV} ( q(\cdot|z) , u(\cdot|z))
& \leqslant & \Big( \frac{1}{2} 
{\rm KL}(    u(\cdot|z) \| q(\cdot|z)) \Big)^{1/2}
= \Big( \frac{1}{2} 
\E_{ u(y|z)}   \log \frac{u(y|z)}{  q(y|z)} \Big)^{1/2} \\
& \leqslant & 
 \Big( \frac{1}{2} 
\E_{ u(y|z)}  \log Z(z) - \varphi (y,z)  \Big)^{1/2}
\leqslant \Big( \frac{1}{2} 
\  \log Z(z)  \Big)^{1/2},
\EEAS
because $\varphi \geqslant 0$.
Thus the second term in \eq{AAA} can be bounded as follows, using \eq{D}:
\BEA
\notag 2d  \sigma ( -\frac{2}{\eta} +2  \beta_1  )  \max_{z}       {\rm TV} ( q(\cdot|z) , u(\cdot|z))
& \leqslant & 
2d  e^{  -\frac{2}{\eta} +2  \beta_1}  \Big(
 \frac{d}{2} e^{ - 2/\eta + 2\beta_1}
   (e^{ {2 d \beta_2}} - 1)
\Big)^{1/2}
\\
\label{eq:Z} & = & 
\sqrt{2}d^{3/2}  e^{  -\frac{3}{\eta} +3  \beta_1}  \big(
     e^{ {2 d \beta_2}} - 1 
\big)^{1/2}.
\EEA

For the first term in \eq{AAA}, we have:
\BEAS
\sum_{\bar{y}} \ell(\bar{y},z) 
\cdot  \big|  q(\bar{y}|z) - u(\bar{y}|z)\big|
& = & 
\sum_{\bar{y}} \sum_{i=1}^d 1_{\bar{y}_i \neq z_i}
u(\bar{y}|z)
\cdot  \Big|  \frac{1}{Z(z)} e^{
\varphi(\bar{y},z)}- 1 \Big|
\mbox{ by definition of } \varphi,\\
& = & 
\sum_{\bar{y}} \sum_{i=1}^d 1_{\bar{y}_i \neq z_i} \sigma( -2/\eta - 2s(z)_i)
\prod_{j \neq i} u(\bar{y}_j|z_j)
\cdot  \Big|  \frac{1}{Z(z)} e^{
\varphi(\bar{y},z)}- 1 \Big|
\mbox{ by definition of } u.
\EEAS
We now use the inequality
\BEAS
\Big|  \frac{1}{Z(z)} e^{
\varphi(\bar{y},z)}- 1 \Big|
& = & 
\Big(  \frac{1}{Z(z)} e^{
\varphi(\bar{y},z)}- 1 \Big)_+ + \Big(1-   e^{
\varphi(\bar{y},z) - \log Z(z) } \Big)_+ \\
& \leqslant & 
\Big( e^{
\varphi(\bar{y},z)}- 1 \Big)_+ + \Big(\log Z(z) -  
\varphi(\bar{y},z) \Big)_+
\leqslant e^{
\varphi(\bar{y},z)}- 1 + \log Z(z),
\EEAS
which is the result of $\varphi \geqslant 0$ and $Z \geqslant 1$, to get, using $\varphi(\bar{y},z)\leqslant d \beta_2 \| \bar{y}-z\|_1$,
\BEAS
\sum_{\bar{y}} \ell(\bar{y},z) 
\cdot  \big|  q(\bar{y}|z) - u(\bar{y}|z)\big|
& \leqslant & 
\sum_{\bar{y}} \sum_{i=1}^d 1_{\bar{y}_i \neq z_i} \sigma( -2/\eta - 2s(z)_i)
\prod_{j \neq i} u(\bar{y}_j|z_j)
\cdot  \Big|  e^{
d \beta_2\| \bar{y} - z \|_1 } - 1 + \log Z(z) \Big|
\\
& \leqslant & 
\sum_{i=1}^d \sum_{\bar{y}_j, j \neq i} 
  \sigma( -2/\eta + 2\beta_1)
\prod_{j \neq i} u(\bar{y}_j|z_j)
\cdot  \Big(  e^{ 2d \beta_2} e^{
d \beta_2\sum_{j \neq i} |\bar{y}_j - z_j| } - 1  + \log Z(z)\Big)
\\
& = &  \sigma( -2/\eta + 2\beta_1)
\sum_{i=1}^d \Big(
 e^{ 2d \beta_2}
 \prod_{j \neq i}
  \Big\{
 \sigma(2/\eta + 2s(z)_j)
 +  \sigma(-2/\eta - 2s(z)_j)  e^{ 2d \beta_2} \Big\}
- 1 + \log Z(z) \Big)
\\
& = &  \sigma( -2/\eta + 2\beta_1)
\sum_{i=1}^d \Big(
 e^{ 2d \beta_2}
 \prod_{j \neq i}
 \Big\{ 1 
 +  \sigma(-2/\eta + 2s(z)_j)  (e^{ 2d \beta_2} -1) \Big\}
- 1 + \log Z(z)\Big)
\\
& \leqslant &  e^{ -2/\eta + 2\beta_1}
\sum_{i=1}^d \Big(
 e^{ 2d \beta_2}
 \prod_{j \neq i}
 \Big\{ 1 
 +  \sigma(-2/\eta +2 \beta_1)  (e^{ 2d \beta_2} -1) \Big\}
- 1+ \log Z(z) \Big)
\\
& = & d e^{ -2/\eta + 2\beta_1}
  \Big(
 e^{ 2d \beta_2}
 \Big( 1 
 +  \sigma(-2/\eta +2 \beta_1)  (e^{ 2d \beta_2} -1) \Big)^{d-1}
- 1 + \log Z(z)\Big).
\EEAS
From the constraint $8 d \beta_2 e^{4 \beta_1} \leqslant 1$, we have $\beta_2 d \leqslant \frac{1}{8}$, which implies that 
$e^{ 2d \beta_2} -1 \leqslant \frac{5}{2} d\beta_2$. Moreover, we assume that
$ e^{-2/\eta + 2 \beta_1} \leqslant \frac{1}{d}$. This leads to, using \eq{D},
\BEA
\notag \sum_{\bar{y}} \ell(\bar{y},z) 
\cdot  \big|  q(\bar{y}|z) - u(\bar{y}|z)\big|
& \leqslant & 
d e^{ -2/\eta + 2\beta_1}
  \Big(
 e^{ 2d \beta_2}
 \Big( 1 
 +  \frac{1}{d} \frac{5}{2} d\beta_2 \Big)^{d}
- 1 + d e^{ - 2/\eta + 2\beta_1}
   (e^{ {2 d \beta_2}} - 1) \Big)
\\
\notag & \leqslant & 
d e^{ -2/\eta + 2\beta_1}
  \Big(
 e^{ 2d \beta_2}
 e^{ \frac{5}{2}d \beta_2}
- 1 + d e^{ - 2/\eta + 2\beta_1} \frac{5}{2} d \beta_2 \Big) \\
\label{eq:Zp} &
\leqslant &
d e^{ -2/\eta + 2\beta_1}
  6d \beta_2 + \frac{5}{2} d^3 \beta_2 e^{ -4/\eta + 4\beta_1}
  \EEA
  using $1+c \leqslant e^c$.

Thus, assembling the terms in \eq{Z} and \eq{Zp},
\BEAS
\E[1_{\rm reject}(y,\bar{y}) \ell(\bar{y},y)]
& \leqslant & 
d e^{ -2/\eta + 2\beta_1}
  6d \beta_2
  + \frac{5}{2} d^3 \beta_2 e^{ -4/\eta + 4\beta_1}
  + \sqrt{2}d^{3/2}  e^{  -\frac{3}{\eta} +3  \beta_1}  \big(
     \frac{5}{2} d \beta_2
\big)^{1/2}.
\EEAS
Thus, using the same reasoning as in Appendix~\ref{app:stationary}, and using that $\beta_2 d \leqslant \frac{1}{8}$,
\BEA
\notag W(q',q) & \leqslant & 2 e^{\frac{2}{\eta} +   2\beta_1} 
\E[1_{\rm reject}(y,\bar{y}) \ell(\bar{y},y)] \\
\notag & \leqslant & 
2d e^{4\beta_1}
  6d \beta_2
  + 5 d^3 \beta_2 e^{ -2/\eta + 6\beta_1}
  + 2\sqrt{2}d^{3/2}  e^{  -\frac{1}{\eta} +5  \beta_1}  \big(
     \frac{5}{2} d \beta_2
\big)^{1/2}
\\
\label{eq:Zpp} & \leqslant & 
17d^2  e^{4\beta_1}
    \beta_2
  + 2 \sqrt{5} d^{2}  e^{  -\frac{1}{\eta} +   5\beta_1} 
   \sqrt{ \beta_2} \mbox{ using } e^{-2/\eta + 2 \beta_1} \leqslant \frac{1}{d}, \\
 \notag   & = & 
17d^2  e^{4\beta_1}
    \beta_2
  + 2 \sqrt{5} d^{2}  e^{  -\frac{1}{\eta} +   \beta_1} e^{4 \beta_1}
   \sqrt{ \beta_2}
\\
 \notag   & \leqslant & 
17d^2  e^{4\beta_1}
    \beta_2
  + 2 \sqrt{5} d^{2}  \frac{1}{\sqrt{d}} e^{4 \beta_1}
   \sqrt{ \beta_2}
   =  d e^{4\beta_1}
   [ 17 d \beta_2 + \sqrt{ 20 d\beta_2}]
   \leqslant  d e^{4\beta_1}
   [ 17 \frac{1}{\sqrt{8}} \sqrt{d \beta_2} +  \sqrt{ 20 d \beta_2}] 
   \\
 \notag   & \leqslant & 12 d e^{4 \beta_1} \sqrt{ d \beta_2}.
\EEA
Overall, we get a bound in $d$ times $\sqrt{\beta_2 d^2}$ and $d$ times $\beta_2 d^2$ if $\eta$ is small enough, by \eq{Zpp}.



\end{proof}
\section{Mixtures of independent variables}
\label{app:mixtures}

In this section, we provide details on score functions for mixtures of two independent variables. We start with a few facts about independent variables.

\paragraph{A few facts about independent variables.}
If $p(x) \propto e^{\beta^\top x}$, then $$p(x) = \frac{e^{\beta^\top x}}{\prod_{i=1}^d 2 \cosh \beta_i}$$
and
$$
\sum_{x \in \{-1,1\}^d} p(x) x = \tanh(\beta x)
$$
(taken componentwise).

If $q(y) \propto \sum_{x \in \{-1,1\}^d} p(x) e^{\alpha y^\top x}$, then $y = x \circ z$, where $p(z) \propto e^{\alpha 1_n^\top x}$ is independent from $x$. Since for independent variables, the first moment characterizes the distributions, we have
$$
q(y) \propto e^{\gamma^\top x}
$$
with $\tanh \gamma_i = \tanh \alpha \cdot \tanh \beta_i$. We then have two different formulas for $q(y)$:
\BEAS
q(y) & = & \frac{e^{\gamma^\top x}}{\prod_{i=1}^d 2 \cosh \gamma_i} \\
& = & \frac{1}{( 2 \cosh \alpha)^d}
\frac{\prod_{i=1}^d \cosh ( \beta_i + \alpha y_i) }{\prod_{i=1}^d   \cosh \beta_i},
\EEAS
the second being obtained by computing the sum with respect to $x$. Note that these two formulas are equal for $y \in \{-1,1\}^d$, not for generic $y$'s. Moreover, one can check that 
 $\alpha =0 $ or $\beta=0$ lead to a constant $q$.


We can compute $\E[x|y]$ as
$$
\E[x|y] = \frac{1}{\alpha} \nabla \log q(x).
$$
Using the first formula for $q$ leads to 
$\E[X|Y=y] = \frac{\gamma}{\alpha}$, which is incorrect. With the second formula, we get:
$$
\frac{1}{\alpha}\nabla \log q(y) =
\tanh ( \beta + \alpha y).
$$

\paragraph{Mixtures of two independent variables.}
\label{app:mixtures]}
We consider
$$
p(x) = \frac{1}{(2 \cosh \beta)^d}
\Big[ \frac{1}{2} e^{\beta 1_n^\top x}  +
\frac{1}{2} e^{-\beta 1_n^\top x} \Big].
$$
With $\tanh \gamma = \tanh \alpha \cdot \tanh \beta$, then $q$ is a mixture of 
$ \frac{1}{(2 \cosh \gamma)^d} e^{ \gamma 1_n^\top y} $ and $ \frac{1}{(2 \cosh \gamma)^d} e^{ -\gamma 1_n^\top y} $, with 
$$
q(y) =  \frac{1}{( 2 \cosh \alpha)^d
 (\cosh \beta)^d }
 \bigg[\frac{1}{2} 
 \prod_{i=1}^d \cosh ( \beta + \alpha y_i)
+
\frac{1}{2} 
 \prod_{i=1}^d \cosh ( \beta - \alpha y_i) \bigg]
$$
and
\BEAS
\frac{1}{\alpha} \nabla \log q(y)
& = & \frac{ \prod_{i=1}^d \cosh ( \beta + \alpha y_i) \cdot \tanh ( \beta + \alpha y)
-
 \prod_{i=1}^d \cosh ( \beta - \alpha y_i) \cdot 
 \tanh ( \beta - \alpha y) }{ \prod_{i=1}^d \cosh ( \beta + \alpha y_i)
+
 \prod_{i=1}^d \cosh ( \beta - \alpha y_i)}
\\
& = & \frac{ e^{\gamma 1_n^\top x}  \tanh ( \beta + \alpha y)
-
  e^{-\gamma 1_n ^\top x} 
 \tanh ( \beta - \alpha y) }{
  e^{\gamma 1_n^\top x}   +  e^{-\gamma 1_n^\top x}  }.
\EEAS
Above, the first formula is valid for all $y\in \rb^d$, while the second formula is only true for $y \in \{-1,1\}^d$.
 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}
