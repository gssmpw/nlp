\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\usepackage{colortbl}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{float}
\usepackage{placeins}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{lineno}
\usepackage{epsfig}
\usepackage{comment}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{textcomp}
\usepackage{gensymb}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{kantlipsum, wrapfig,graphicx}
\usepackage{sidecap, caption}
\usepackage{soul}
\usepackage[ruled,linesnumbered,noend,vlined]{algorithm2e}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter for `enumiv' to
% the number of references you have in the main paper (here, 6).
%\let\oldthebibliography=\thebibliography
%\let\oldendthebibliography=\endthebibliography
%\renewenvironment{thebibliography}[1]{%
%     \oldthebibliography{#1}%
%     \setcounter{enumiv}{6}%
%}{\oldendthebibliography}


\usepackage{xcolor}
\newcommand{\RI}{\textcolor{red}{\textbf{R1 Wtf7}}}
\newcommand{\RII}{\textcolor{orange}{\textbf{R2 MisN}}}
\newcommand{\RIII}{\textcolor{blue}{\textbf{R3 g7gE}}}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{16639} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
%\title{GDA: Generalized Diffusion for Robust Test-time Adaptation}  % **** Enter the paper title here

\maketitle
\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
%\footnotesize
% \noindent We are very grateful to Reviewers Wtf721 (\RI), MisN19 (\RII), and g7gE14 (\RIII) for your efforts in reviewing our paper and for your insightful questions and suggestions.\\

\vspace{-3mm}

%-------------------------------------------------------------------------

% \noindent $\triangleright$ \textbf{General Response 1.} \textbf{@\RI, @\RII: Clarification of Eq.3 and Eq.4 in Algo 1.}

%-------------------------------------------------------------------------



%-------------------------------------------------------------------------
\noindent $\triangleright$ \textbf{Reply to @\RI.} 
% Thank you for recognizing our work is impressive and well-written. 
\textbf{Q1: Interpretability of models} 
We thank the reviewer for pointing this out. In Appx Fig. 7, we show visualization of the true positive sample. Here, we collect the Top-3 high freq. phrases from GPT-4o's explanations of the most used features in each sample category (TP, TN, FP, FN) to illustrate the model's focus. 

\begin{figure}[h]
   \vspace{-4mm}
    \centering
    \includegraphics[width=0.75\linewidth]{figs/interpret_bigram.jpg}
        \vspace{-3mm}
        % \caption{Visualization.}
    \label{fig:vis}
    \vspace{-2mm}
\end{figure}

\textbf{Q2: Advantages in text processing}
We have explored the advantages and wish to highlight two key contributions: the use of structured prompts (SP) and the rewriting of prompts during online adaptation (see Sec. 3.4 and 4.2.2). Our in-depth analysis reveals SP enables LVLMs to maintain a more consistent logical path and reduces the incidence of hallucinations in output responses (see Fig. 5 in main paper). Besides, the template rewrite function is crucial for adapting LVLMs to specific analytical needs through small batches of video input.
\textbf{Q3: Benchmark is too small}
We appreciate the reviewer’s mention of GenVideo[1] and GenVidDet[2] and will include them. It is important to highlight the superior quality of our benchmark compared to [1]. Our dataset employs more advanced generation models, specifically using Gen3 technology for video generation, whereas [1] relies on the older Gen2. Additionally, we use VidBench to ensure that every video meets high-quality, human-centric standards. Regarding [2], although their paper has been published, the dataset itself has not been released.
% Furthermore, our research uniquely focuses on developing a general LVLM-based detection framework that operates entirely on zero-shot detection, in contrast to the approaches in [1][2], which require training specific classifiers.
\textbf{Q4: Compared with video classification network}
In main paper, we have highlighted the difficulty of the detection task, both on the diffusion-generated dataset and deepfake dataset. In Fig. 4, we compare LAVID with two simple classification models, including SVM and XGBoost trained with explicit knowledges (EKs). We discover the difficulty of using multiple EKs to train the classifiers. 
We show more results on traditional classifier, such as CNN + Transformer as follows.
\begin{table}[h]
\vspace{-3mm}
\centering
\scriptsize
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}
\hline
\textbf{Model/ Dataset}                    & \textbf{kling} & \textbf{Sora} & \textbf{Gen-3} & \textbf{OpenSora} \\ \hline
\textbf{CNN + Transformer}  &   75.51    &   61.14   &   83.34    &   81.33    \\
% cnn+transformer (gemini) &       &      &       &          \\
\textbf{GPT4o (raw)}              &   66.50    &   68.89   &    86.00   &     82.50     \\
% gemini(raw)              &       &      &       &          \\
\textbf{LAVID (GPT4o)}             &   \textbf{77.00}    &   \textbf{70.93}   &   \textbf{91.00}    &     \textbf{86.50}     \\ \hline  
% LAVID(gemini)            &       &      &       &  \\   \hline     
\end{tabular}
\vspace{-3mm}
\end{table}
% Additionally, LAVID can even generalize to other Deepfake dataset (See Table 6 (main paper)) and outperform traditional deep-learning baselines. 
% we provide an analysis of deepfake detection and compare LAVID with three deep learning-based baselines. The results show that LAVID w/ Gpt-4o has better detection performance, indicating the generalizability of LAVID is better than other traditional methods.
\textbf{Q5: Show FP, FN samples} Please see Q1. response.
\textbf{Q6, Q7:} We will clarify the source of citation and 
correct typo.

%------------------------------------------------------------------------

\noindent $\triangleright$ \textbf{Reply to @\RII.} 
Thank you for endorsing our ideas.
\textbf{Q1: Dependency on LVLMs}
Based on our thorough investigation, we confirm that the zero-shot reasoning capabilities of current state-of-the-art (SOTA) LVLMs are inadequate. Our proposed solution significantly enhances LVLMs by enabling the selection of explicit knowledge and the detection of artifacts. As LVLMs continue to evolve and integrate greater reasoning abilities, it is imperative that how to guide these models to specific reasoning and employ chain-of-thought reasoning strategies.
% Second, although LAVID has better performance on larger LVLMs, we can still improve LAVID with those small LVLMs by exploring additional EK and diving deep into the text-processing capabilities of models. It is undoubted that we can further improve the performance of small LVLMs with few-shot learning on explicit knowledge.
\textbf{Q2: Computational Resources} We summarize the cost of detecting video as follows. The video cost is calculated by \# of text token * \$ of text token + \# of frame (8) * \$ of input frame.
\begin{table}[h!]
\vspace{-4mm}
\centering
\scriptsize
\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{0.5} % 加大行间距
\begin{tabular}{@{}lccc@{}}
\toprule
Name             & Input Text Token & 1080p Input Frame & Input Video\\ \midrule
GPT4o-0806       & \$2.50/1M                   & \$2.763/1K & \$22.215/1K                                          \\
Gemini-1.5-pro-002 & \$1.25/1M                  & \$0.323/1K  & \$2.639/1K                                       \\
Qwen-VL-MAX-0809 & \$2.78/1M         & \$3.451/1K   & \$27.728/1K                                     \\ \bottomrule
\end{tabular}
\vspace{-4mm}
\end{table}
\textbf{Q3: Can LAVID use on smaller model?}
In Table 3, we show performance of Llava-7B, which is a relatively small model. Our method consistently improves Acc. on each video dataset.
% For a Gven 1920*1080 image's token: 1) GPT could calculate from its official website; 2) Gemini will rescale to a fixed token 258 (https://ai.google.dev/gemini-api/docs/tokens?lang=python&hl=zh-cn); 3) Qwen will rescale to a fixed unkown token (maybe 1024*1024) (https://help.aliyun.com/zh/model-studio/user-guide/vision/?spm=a2c4g.11186623.0.0.23d2253aI2yPqg)

% Per Video cost is calculated using 8 frames and avgerage tokens of three baselines 
%-------------------------------------------------------------------------

\noindent $\triangleright$ \textbf{Reply to @\RIII.} 
Thank you for appreciating our paper and endorsing our insights. We agree with your suggestion to clarify our approach in the next revision.
Due to page constraints, we included more information in Appdx 8.4 about templates for prompt rewriting and evolution logging. 
% Our template rewrite function is crucial for adapting LVLMs to specific analytical needs through small batches of video input. 
% Requirements, Analysis Guidelines, Constraints, Prohibited Fields, and Additional Notes'
The system prompt incorporates five key criteria, guiding LVLMs to adjust the prompts accordingly. For example, under \textit{Requirements}, one rule states: 'You may add any other fields you deem necessary for analyzing video data with \texttt{\{tool name\}}.'
% in analysis guidelines, we have “Consider the aspects of videos that {tool name} excels at analyzing.”; and in the additional notes, we have  ”The total number of fields must not exceed five”... etc. More information can be found in appendix 8.4.
In OA, we first initiate a template, a class structure with fields.
% including “is\_ai\_generated: Bool”, “raw\_frame\_analysis: str”, “\{tool\_name\}\_analysis: str”, “explanation: str”. 
From the first iteration, we direct LVLMs to generate a new template using the specified system prompt. If this new template improves the F1 score, we adopt it; otherwise, we retain the previous one. The optimal template from the final Online Adaptation (OA) step is then applied to the current batch of video test inputs.
\textbf{Q2: Choices of prompts}
 Apologies for the confusion. The term refers to the design of baseline prompts. To demonstrate our method's robustness, we designed three different prompt templates outlined in the baseline section. Each has the same meaning but uses different paraphrasing. We found that varying the paraphrases can significantly impact LVLM performance. For instance, instead of directly asking 'Is this video AI-generated?', we found that the more subtly phrased baseline 'p2: Tell me if there are synthetic artifacts...' tends to result in performance drops. \textbf{Q3: Benefit of Non-SP} We want to clarify that our focus is on the hallucination analysis of non-SP for discriminative tasks. Non-SP allows for free-formatted responses, offering more diversity for generative tasks. However, for discriminative tasks, SPs are generally more effective. While SPs are preferable, their efficacy depends on model training; our findings show that only GPT-4o 
 consistently supports SP.
 \textbf{Q4:} We will correct the repeated reference
%-------------------------------------------------------------------------
\\{\footnotesize [1]  Chen, Haoxing \textit{et al.} DeMamba: AI-Generated...(arXiv'24)}
\\{\footnotesize [2] Ji, Lichuan \textit{et al.} Distinguish Any Fake Video... (arXiv'24)}
% \\{\footnotesize [C3] Serin Yang \textit{et al.} Zero shot contrastive...Transfer. (ECCV'23)}



%%%%%%%%% REFERENCES
% {
%     \scriptsize
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }


\end{document}
