\section{Ablation Studies}

\label{sec:abalation}

\paragraph{Comparison with supervised learning methods}
% \qy{Multi-histogram could be used here to show the difference between deep-learning}

One key motivation of this work is to propose a more general detection method that overcomes the limitations of supervised learning approaches. We are particularly interested in comparing the performance of LVLMs and traditional machine learning classifiers under the same explicit knowledge base.
Additionally, prior work has shown that explicit knowledge could effectively reveal the artifacts in the AI-generated video content~\cite{chang2024mattersaigeneratedvideos}.
% XGBoost, and Video Transformer models
We select SVM and XGBoost as our two baseline classifiers for this comparison. We train the classifier using the same EK tools that we select for LVLMs. For instance, we compare GPT-4o with both SVM and XGBoost trained with $\textit{\{landmark, saturation, and edge\}}$ features. In Fig.~\ref{fig:toolkit_dp}, we show the results of SVM, XGBoost for GPT-4o  and Gemini-1.5-pro based on their corresponding toolkits (See Table~\ref{tab:ek_tool_set}). \lavid outperforms those supervised learning methods over all datasets. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.52\textwidth]{figs/toolkit_dp.png} % Adjusted width to half of text width
    % \captionsetup{justification=centering}
     \vspace{-2mm}
    \caption{Comparison between supervised learning methods and \lavid. Both SVM and XGBoost are trained with the same EK of the LVLMs. (RAW) represents the results using raw frame only.}
    \label{fig:toolkit_dp}
    % \vspace{-1mm}
\end{figure}

% Though we conduct only basic training on simple models, the substantial lead in the performance of LVLM-based detection strongly suggests that \lavid is a more effective way for leveraging features for video detection.
% For the specific explicit knowledge experiment \ref{fig:tool_dp}, we selected optical flow and depth information due to their widespread use in previous studies. Furthermore, recent exploration of these features also reveals its potential for video artifact detection.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figs/tool_dp.png} % Adjusted width to half of text width
%     \captionsetup{justification=centering}
%     \caption{Comparison between deep learning based and LVLMs-based detection performance on the same one explicit knowledge.}
%     \label{fig:tool_dp}
% \end{figure}


\paragraph{Analysis on Deepfake detection}

% Recent work \cite{jia2024chatgptdetectDeepfakesstudy} has demonstrated that LVLMs can be effectively applied to Deepfake detection tasks for its increasing visual capability and generalizability. This motivated us to explore whether explicit knowledge could also aid LVLMs on Deepfake forensics. To investigate, we conducted a study to evaluate models against the test split of Celeb-DF-v1 \cite{Celeb_DF_cvpr20}, applying \lavid to Gemini 1.5 Pro and GPT-4o. 

% In Table~\ref{tab:abalation_study_Deepfake}, we compare the cross-domain evaluation results of \lavid against previous state-of-the-art methods \cite{9878441, guo2023controllableguidespacegeneralizableface, zhao2021multiattentionalDeepfakedetection}, trained on FaceForensics++ \cite{roessler2019faceforensicspp}. We also report the best evaluation results on three different vanilla prompting approaches using the same two LVLMs to be as baselines. 

% We observed that \lavid -GPT-4o demonstrates comparable performance against previous state-of-the-art methods by achieving 74.0\% video-level detection accuracy. Comparing to baseline prompting approaches, \lavid improves Gemini-1.5-pro by adding 6\% accuracy and 16.56\% F1-score ; imrpoves Gpt-4o by adding 10.05\% accuracy and 6.67\% F1-score. Moreover, we found that in the initial toolkit preparation stage, LVLMs suggesst Face Segmentation (F-Seg) as a toolkit candidate, if we explicitly specify the Deepfake detection task. Therefore, we leveraged  \emph{Language Segment-Anything }\footnote{Language Segment-Anything: \url{https://github.com/luca-medeiros/lang-segment-anything}.} to segment face and treat as an additional explicit knowledge in \lavid. By keeping all other configuration unchanged, we observed that it will help \lavid improves additional 3.29\% F1-score with Gemini-1.5-pro, and 1\% accuracy, 0.91\% F1-score with Gpt-4o. This study inspires us on the capability of \lavid on Deepfake video detection. 

Recent work~\cite{jia2024chatgptdetectDeepfakesstudy} shows that LVLMs can be effectively applied to Deepfake detection tasks.
 % whether \lavid could further enhance the performance of LVLMs in Deepfake forensics. 
To investigate this, we adopt \lavid to Gemini-1.5-Pro~\cite{google2024gemini} and GPT-4o on Celeb-DF-v1~\cite{Celeb_DF_cvpr20}, a Deepfake dataset. 
In Table~\ref{tab:abalation_study_Deepfake}, we compare \lavid with three deep learning-based baselines~\cite{9878441, guo2023controllableguidespacegeneralizableface, zhao2021multiattentionalDeepfakedetection} trained on FaceForensics++ ~\cite{roessler2019faceforensicspp} (FF++). 
% We also report the best evaluation results from three different vanilla prompting approaches using the same two LVLMs as baselines.
Additionally, prior work~\cite{jia2024chatgptdetectDeepfakesstudy} shows decomposed face features can potentially improve the Deepfake detection. Therefore, we utilize open-source tool, \emph{Language Segment-Anything}\footnote{Language Segment-Anything: \url{https://github.com/luca-medeiros/lang-segment-anything}.} to segment the face features (Face-Seg), treating it as an additional explicit knowledge for \lavid. In Table~\ref{tab:abalation_study_Deepfake},
we observe that \lavid(GPT-4o) demonstrates comparable performance to baseline methods by achieving 75.0\% video-level detection accuracy. Compared to baseline prompting approaches, \lavid improves Gemini-1.5-Pro~\cite{google2024gemini} by 6.0\% in accuracy and 19.85\% in F1-score, and it improves GPT-4o by 10.05\% in accuracy and 6.67\% in F1-score. This study demonstrates the capability of \lavid in Deepfake detection.

% \begin{table}[h]
% \centering
% \scriptsize
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{@{}c|c|ccc@{}}
% \toprule
% \multirow{2}{*}{\textbf{Method}}        & \multirow{2}{*}{\textbf{Trainset}} & \multicolumn{3}{c}{\textbf{Celeb-DF-v1}} \\ \cmidrule(l){3-5}
%               &           & \textbf{Acc.} & \textbf{F1} & \textbf{AUC} \\ \midrule
% Guo et al. \cite{guo2023controllableguidespacegeneralizableface}          &     FF++ (HQ)      &   73.19    &          &  84.97    \\
% RECCE \cite{9878441}         &    FF++       &     71.81  &          &  77.90    \\
% MAT  \cite{zhao2021multiattentionalDeepfakedetection}        &    FF++       &   71.81    &          &  77.16   \\
% \midrule
% % REMOVE LATER Gemini-1.5-pro (baseline1)        &           &  42    &  17.14        &     \\
% % REMOVE LATER Gemini-1.5-pro (baseline2)        &           &  40    &  9.09        &     \\
% % REMOVE LATER Gemini-1.5-pro (baseline3)        &           &  44    &  17.65        &     \\
% % REMOVE LATER GPT-4o (baseline1)        &           &  64.95    &   74.24       &     \\
% % REMOVE LATER GPT-4o (baseline2)        &           &   64   &     65.38     &     \\
% % REMOVE LATER GPT-4o (baseline3)        &           &   66   &     66     &     \\
% % \midrule
% Gemini-1.5-pro        &     --      &  44.00    &  17.65        & --    \\
% GPT-4o        &     --       &  64.95    &   74.24       & --    \\
% \midrule
% \textbf{\textit{Ours(Gemini-1.5-pro) w/ SAM}}  &    --        &   50.00   &    37.50      & --     \\
% \textbf{\textit{Ours(GPT-4o) w/ SAM}}     &  --          &   \textbf{75.00}   &    \textbf{80.91}      &  --    \\
% % \midrule
% % \textbf{\textit{Ours (Gemini-1.5-pro)}}  &     --       &   50.00   &    34.21      &  --    \\
% % \textbf{\textit{Ours (GPT-4o)}}     &     --       &   \textbf{74.00}   &   \textbf{80.00}     & --     \\ 
% \bottomrule
% \end{tabular}%
% }




\begin{table}[t]
\centering
\setlength{\tabcolsep}{2pt} % 减小列间距
\tiny
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{@{}l|c|cc@{}}
\hline
\multirow{2}{*}{\textbf{Method}}        & \multirow{2}{*}{\textbf{Trainset}} & \multicolumn{2}{c}{\textbf{Celeb-DF-v1}} \\
\cline{3-4}
                   &  & \textbf{Acc.} & \textbf{F1} \\ \hline
Guo et al.~\cite{guo2023controllableguidespacegeneralizableface}            &   FF++ ~\cite{roessler2019faceforensicspp}    &   73.19    &      --       \\
RECCE~\cite{9878441}               &   FF++ ~\cite{roessler2019faceforensicspp}  &   71.81    &     --       \\
MAT~\cite{zhao2021multiattentionalDeepfakedetection}            &   FF++ ~\cite{roessler2019faceforensicspp}    &   71.81    &    --      \\
\hline
Baseline (Gemini-1.5-pro)              &  --  &  44.00    &  17.65           \\
Baseline (GPT-4o)             &  --  &  64.95    &   74.24          \\
\hline
% \textbf{\textit{\lavid (Gemini-1.5-pro)}}       &   --   &   50.00   &    34.21       \\
% \textbf{\textit{\lavid (Gpt-4o)}}             &   --   &   74.00   &    80.00      \\
% \hline
\textbf{\textit{\lavid (Gemini-1.5-pro) w/ Face-Seg}}          &   --   &   50.00   &    37.50       \\
\textbf{\textit{\lavid (GPT-4o) w/ Face-Seg}}             &   --   &   \textbf{75.00}   &    \textbf{80.91}       \\
% \midrule
% \textbf{\textit{Ours (Gemini-1.5-pro)}}  &     --       &   --   &   50.00   &    34.21      &  --    \\
% \textbf{\textit{Ours (GPT-4o)}}     &     --       &   --   &   \textbf{74.00}   &   \textbf{80.00}     & --     \\ 
\hline
\end{tabular}%
}

\caption{Performance comparison of existing Deepfake detection baselines, the baseline prompts, and \lavid on Celeb-DF-v1. Video-level accuracy (Acc.) and F1-score (F1) are used as evaluation metrics where available. The reported performance of RECCE and MAT are referenced from~\cite{Wang_2024}.}
\label{tab:abalation_study_Deepfake}
\vspace{-2mm}
\end{table}


\paragraph{Hallucination analysis of non-structured prompt}
We hypothesize that employing a structured output format in GPT-4o provides a "thinking framework" that enables LVLMs to follow a more consistent logical path, thereby reducing the likelihood of hallucination. 
Although OpenAI has demonstrated some advantages of structured output\footnote{OpenAI Structured Output :\url{https://openai.com/index/introducing-structured-outputs-in-the-api/}.}, it has not yet been validated in vision tasks. 
Therefore, we evaluate the GPT-4o model on whole set of Pika~\cite{pika}, Kling~\cite{klingai}, and corresponding real video dataset Panda~\cite{chen2024panda70mcaptioning70mvideos}. We use the same three baseline prompts as in the main experiment. Our results in Table \ref{tab:ablation_impact_of_structured} indicate a consistent improvement in LVLMs' visual capabilities when the structured prompt is provided.


\begin{table}[t!]
\centering
\scriptsize
\begin{tabular}{@{}c|cc|cc|cc@{}}
\hline
Dataset & \multicolumn{2}{c|}{Baseline Prompt1} & \multicolumn{2}{c|}{Baseline Prompt2} & \multicolumn{2}{c}{Baseline Prompt3} \\ \cline{2-7}
                         & SP       & NSP     & SP       & NSP      & SP       & NSP      \\ \hline
% Kling              & 49.52 & 39.21              & 83.17 & 57.62              & 49.68 & 40.50              \\
% % Gen3               & 82.67            & \underline{84.82} & \underline{90.92}            & 86.63  & \underline{93.00}            & 86.47  \\
% Pika               & 92.56 & 70.39              &96.28 & 73.49              & 73.18 & 73.18  \\   
% \midrule
% Panda               & 90.35 & 94.73              & 49.61 & 71.74              & 89.61 & 92.40              \\
% \midrule
Kling~\cite{klingai}               & \textbf{69.94} & 66.97              & \textbf{66.39} & 64.68              & \textbf{69.65} & 66.45              \\
Pika~\cite{pika}               & \textbf{91.46} & 82.56             & \textbf{72.95} & 72.62              & 81.40 & \textbf{82.79}             \\
\hline

\end{tabular}
    \vspace{-1mm}
\caption{Impact of structural prompt (SP) v.s. non-structured prompt (NSP) based on GPT-4o. Both dataset are combined with corresponding real video from Panda~\cite{chen2024panda70mcaptioning70mvideos}.}
\label{tab:ablation_impact_of_structured}
    \vspace{-2mm}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.999\linewidth]{figs/refusal_rate_heatmap_combined_v2.png}
    \vspace{-2mm}
    \caption{Heatmap of refusal rate for both non-structured and structured prompt on GPT-4o across different baselines and datasets}
    \label{fig:combined_analysis_heatmap}
    \vspace{-2mm}
\end{figure}

% \fs{I think we should report Deepfake in section 5.2 as another application than Diffusion AI-generated videos, deep learning method should also be reported as a baseline}

% \fs{We can compare with other paper's method on our dataset as a OOD, however, we need to make sure "no label" is used in our method to claim it is a OOD benchmark, to do that, I suggest we can split our method in to something like ours (no label) and ours (with label, TTA, F1-score based tool selection)}

% \fs{ablation studies are supposed to show the impact of each component in our pipeline, for example, if we remove structural output / tool selection / TTA, what will be the difference}
In addition, the refusal rate of the LVLMs could be another indicator of the hallucination~\cite{magesh2024hallucination}. 
We estimate it by checking if LVLMs reject to provide a response when giving baseline prompts. As shown in Fig~\ref{fig:combined_analysis_heatmap}, the non-structured prompt shows an average rate of 2.97\% on \vidfor high-quality subset, while for the query with structured prompt, the refuse rate is zero. This demonstrates that structured prompts improve adherence to the intended classification task, effectively reducing hallucination.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figs/refusal_rate_heatmap.png}
%     \vspace{-1mm}
%     \caption{Heatmap of refusal rate for non-structured prompt on GPT-4o across different baselines and datasets}
%     \label{fig:refusal_rate_heatmap_non_structured}
%     \vspace{-2mm}
% \end{figure}




% \begin{figure}[h]
% \centering
%     \begin{subfigure}{0.45\linewidth}
%          \centering
%           % include first image
%          \includegraphics[width=.99\linewidth]{figs/refusal_rate_heatmap.png}  
%          \caption{Non-Structured Prompt}
%          % \label{fig:sub-first}
%     \end{subfigure}    
%     \hfill
%     \begin{subfigure}{.45\linewidth}
%           \centering
%           % include third image
%           \includegraphics[width=.99\linewidth]{figs/refusal_rate_heatmap_structured.png} 
%           \caption{Structured Prompt}
%           % \label{fig:sub-fourth}
%     \end{subfigure}
 
%      \caption{Heatmap of refusal rate for structured prompt on GPT-4o across different baselines and datasets}
%     \label{fig:analysis_heatmap}
%     \vspace{-4mm}
% \end{figure}







% \begin{table*}[h!]
% \centering
% \scriptsize
% \caption{Impact of Structural Output based on Gemini-1.5-pro}
% \begin{tabular}{@{}c|cc|cc|cc@{}}
% \toprule
% \multirow{Dataset} & \multicolumn{2}{c|}{Baseline Prompt1} & \multicolumn{2}{c|}{Baseline Prompt2} & \multicolumn{2}{c}{Baseline Prompt3} \\ \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(l){6-7}
%                          & Structural       & Non-Structural      & Structural       & Non-Structural      & Structural       & Non-Structural      \\ \midrule
% Kling              & 3.48           & \underline{18.32}  & 3.63            & \underline{21.8}  & 2.53            & \underline{6.95}  \\
% Gen3               & 22.28            & \underline{37.13}  & 10.07            & \underline{44.22}  & 22.44           & \underline{32.18}  \\
% Pika               & 24.03           & \underline{38.14}  & 25.89             & \underline{50.54}  & 29.61            & \underline{37.21}  \\

% Panda               & \underline{100.0} & 98.14              & \underline{98.91} & 95.97              & \underline{99.69} & 98.60              \\ \bottomrule
% \end{tabular}
% \end{table*}


% \textbf{}


% \paragraph{Data visualization}
% \fs{proposing this section to include some samples that baseline cannot detect but our method is able to detect. For example, we can select some fake examples that can only be detected by depth / optical flow and put agents' responses beside.}
