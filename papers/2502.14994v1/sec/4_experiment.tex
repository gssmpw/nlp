\section{Experiment}
\label{sec:experiment}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figs/data_preprocess.png}

%     \vspace{-3mm}
%     \caption{data preprocess}
%         \label{fig:step_analysis}
%  \vspace{-4mm}
% \end{figure}
\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt} % 减小列间距
\renewcommand{\arraystretch}{1.5} % 加大行间距
\begin{tabular}{ll|cccc|ccccc}
\toprule
\multirow{2}{*}{\textbf{LVLM}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{VidForensic (VidProM)~\cite{wang2024vidprommillionscalerealpromptgallery}}} & \multicolumn{4}{c}{\textbf{VidForensic (Self-collected)}} & \multirow{2}{*}{\textbf{Avg.}} \\ \cline{3-10}
                      &                         & \textbf{Pika~\cite{pika}}   & \textbf{T2vz~\cite{khachatryan2023text2videozerotexttoimagediffusionmodels}}  & \textbf{Vc2~\cite{chen2024videocrafter2overcomingdatalimitations}}  & \textbf{Ms~\cite{wang2023modelscopetexttovideotechnicalreport}}  & \textbf{OpenSORA~\cite{opensora}} & \textbf{Gen3~\cite{runway3}} & \textbf{Kling~\cite{klingai}} & \textbf{SORA~\cite{videoworldsimulators2024}} &                       \\  
\midrule
\multirow{4}{*}{\textbf{Llava-OV-7B}~\cite{li2024llavaonevisioneasyvisualtask}} & Baseline1 (w/o SP)    & 53.50/14.68      & 61.00/37.10     & 61.00/37.10    & 58.50/30.25   & 52.50/12.11        & 50.00/1.96    & \underline{50.00}/1.96     & 54.44/16.33    & 55.12/18.94                     \\ 

                             & Baseline2 (w/o SP)         & 50.50/1.98      & 51.00/3.92    & 51.50/5.83    & 53.50/13.08   & 52.00/7.69        & 50.00/0.00    & 50.00/0.00     & 50.00/0.00    & 51.06/4.06                     \\ 
                             
                             & Baseline3 (w/o SP)         & \textbf{54.50}/\textbf{18.02}      & \underline{62.00}/\underline{39.68}     & \underline{65.00}/\underline{46.97}    & \underline{62.00}/\underline{39.68}   & \underline{54.00}/\underline{16.36}        & \underline{51.00}/\underline{5.77}    & 50.00/\underline{1.96}     & \underline{55.56}/\underline{20.00}    & \underline{56.76}/\underline{23.56}                     \\ 
                             
\cline{2-11}
                             & \textbf{\lavid  (w/o SP)}            & \textbf{54.50}/\textbf{18.02}     & \textbf{70.00}/\textbf{57.75}     & \textbf{69.00}/\textbf{55.71}    & \textbf{68.00}/\textbf{53.62}   & \textbf{58.00}/\textbf{28.81}        & \textbf{51.50}/\textbf{7.62}    & \textbf{50.50}/\textbf{3.88}     & \textbf{55.56}/\textbf{20.00}    & \textbf{59.63}/\textbf{32.69}                     \\
\midrule
                             
\multirow{4}{*}{\textbf{Qwen-VL-Max}~\cite{qwen2023qwenvl}} & Baseline1 (w/o SP)    & 72.50/63.09      & 75.00/67.53     & 82.00/78.57    & 76.00/69.23   & 67.50/53.24        & 62.00/40.62    & \underline{54.50}/19.47     & 58.89/39.34    & 68.55/51.24                     \\ 

                             & Baseline2 (w/o SP)        & 60.50/38.76      & 75.00/68.35    & 71.50/62.25    & 72.50/64.05   & 60.50/38.76        & 52.00/14.29    & 50.00/7.41     & 56.67/26.42    & 62.33/39.56                     \\ 
                             
                             & Baseline3 (w/o SP)        & \underline{74.00}/\underline{67.90}      & \underline{79.00}/\underline{75.58}     & \underline{84.50}/\underline{83.06}    & \textbf{79.50}/\underline{76.30}   & \underline{69.50}/\underline{60.13}        & \underline{65.50}/\underline{52.41}    & 54.00/\underline{24.59}     & \underline{61.11}/\underline{47.76}    & \underline{70.89}/\underline{60.97}                     \\ 
                             
\cline{2-11}
                             & \textbf{\lavid  (w/o SP)}         & \textbf{87.00}/\textbf{88.39}     & \textbf{81.50}/\textbf{82.63}     & \textbf{86.00}/\textbf{87.39}    & \underline{77.00}/\textbf{77.45}   & \textbf{79.00}/\textbf{79.81}        & \textbf{82.50}/\textbf{83.72}    & \textbf{60.00}/\textbf{52.94}     & \textbf{67.78}/\textbf{71.84}    & \textbf{77.60}/\textbf{76.08}                     \\
                             
                    
                             
\midrule
\multirow{4}{*}{\textbf{Gemini-1.5-pro}~\cite{google2024gemini}} & Baseline1 (w/o SP)        & 68.33/54.32      & 71.00/59.72     & 67.00/51.47    & 75.00/67.11   & 68.50/54.68        & 64.00/44.62    & 58.00/28.81    & 58.89/41.27    & \underline{66.34}/49.83                     \\ 

                             & Baseline2 (w/o SP)        & \underline{73.50}/\underline{66.24}      & \underline{81.00}/\underline{77.91}     & \underline{76.00}/\underline{70.37}    & \underline{85.00}/\underline{83.33}  & \underline{71.50}/\underline{62.75}       & \underline{71.50}/\underline{62.75}  & \underline{59.50}/\underline{37.21}     & \underline{71.11}/\underline{64.86}    & 72.51/\underline{58.28}                     \\ 
                             
                             & Baseline3 (w/o SP)        & 64.50/45.80      & 77.00/70.51     & 71.00/59.72    & 76.50/69.68   & 64.50/45.80        & 62.00/39.68    & 52.50/11.21     & 61.11/42.62    & 66.08/51.28                     \\ 
                             
\cline{2-11}
                             & \textbf{\lavid  (w/o SP)}            & \textbf{92.00}/\textbf{91.73}     & \textbf{96.33}/\textbf{96.38}    & \textbf{95.83}/\textbf{95.87}    & \textbf{97.50}/\textbf{97.56}   & \textbf{92.17}/\textbf{91.93}        & \textbf{88.50}/\textbf{87.67}    & \textbf{74.83}/\textbf{68.46}     & \textbf{76.67}/\textbf{78.36}    & \textbf{89.23}/\textbf{88.43}                     \\
                             
\bottomrule
\end{tabular}

\caption{Performance comparison of baselines and our method without using structured prompt (SP) on eight datasets. For each dataset except SORA, we mix the real dataset from Panda-70M \& AI-generated dataset together. For SORA, we mix it with 45 youtube videos that collected by ourselves. We use three representative LVLMs, which currently only support free-format prompts, to serve as the detector in our framework, including Llava-OV-7B~\cite{li2024llavaonevisioneasyvisualtask}, Qwen-VL-Max~\cite{qwen2023qwenvl}, and Gemini-1.5-pro~\cite{google2024gemini}. The results are presented as Accuracy / F1-score in each cell. Numbers in bold show the top-1 best results, and numbers with underlined show the top-2 best results.}
\label{tab:non-str-results}
\end{table*}










\subsection{Experiment Setting}
\paragraph{Model}
We evaluate the \lavid framework using four leading Large Language Vision Models (LVLMs):
\textbf{1) Llava-OV-7B}~\cite{li2024llavaonevisioneasyvisualtask} represent Llava-OneVision-7B, a open-source LVLM well known for its strong visual understanding capabilities. The model is selected to test \lavid enhancement for small LVLMs.
\textbf{2) Qwen-VL-Max}~\cite{qwen2023qwenvl} refer to Qwen-VL-Max-0809, a top-performing commercial LVLM from Alibaba~\cite{alibaba}. For evaluation, we assess its performance without utilizing structural prompts.
\textbf{3) Gemini-1.5-pro}~\cite{google2024gemini} is one of the most advanced commercial LVLMs from Google. We choose the Gemini-1.5-pro-002 version.
\textbf{4) GPT-4o}~\cite{openai2024gpt4o} is the most advanced LVLM from OpenAI. It offers the structural prompt configuration in our evaluation. We select the GPT-4o-0806 version.

\paragraph{Dataset}
We introduce \vidfor, our video detection benchmark composed of a diverse set of real videos and diffusion-generated videos generated from open-source text-to-video generation tools. 
\vidfor consists of selections of videos from PANDA-70M and VidProM datasets and is enhanced with our in-house combination of real videos sourced from YouTube and generated videos created by four SOTA text-to-video generation models: Kling~\cite{klingai}, Gen3~\cite{runway3}, SORA~\cite{openaisora}, and OpenSORA~\cite{opensora}. 
\textbf{Kling}, a video generation platform created by KuaiShou. With a combination of model architectures, including 3D-VAE, and 3D-spatio-temporal join attention mechanism,  Kling can generate high-quality videos (up to two minutes) that conform to physical laws~\cite{klingai}.
\textbf{Gen3}, created by Runway~\cite{runway3}, was trained with multimodal dataset and released with a set of safeguards. Gen3 produces videos that feature photorealistic human characters with advanced motion and stylistic control.
Developed by OpenAI, \textbf{SORA} is a diffusion-based text-to-video model with a profound understanding of scene complexity, real world objects~\cite{openaisora,videoworldsimulators2024}. \textbf{OpenSORA} is an opensource product of HPC-AI Tech trained on $\sim$30 million data and highlights an innovative video compression network~\cite{opensora}.
% To ensure the accuracy of model testing, we divide the dataset into three parts: a randomly selected 50\% for the main test set, which includes all videos from the SORA~\cite{videoworldsimulators2024} dataset; 25\% of the data for tool selection testing; and the remaining 25\% for the test time adaptation dataset. Additionally, The real videos in our dataset are selected based on the text prompt similarity, so actually for each fake video in our dataset, it has a corresponding real video with the same or similar caption. Thus, each dataset we put in Table~\ref{tab:non-str-results} and Table~\ref{tab:str-results} are the combination of the fake and real videos to show the comprehensive enhancement brought by \lavid. 
% \fs{talk about model version, data splitting, components enabled in our pipeline, also we run each (video, setting) pair 3 times }


\paragraph{Baseline}
We perform the baseline method for each LVLM by directly asking itself if the consecutive frames input is generated by AI or not. To thoroughly evaluate the general performance of these models in video detection, we carefully design three zero-shot prompts as shown below. Experimental results demonstrate that the choice of prompt can significantly impact the model's predictions.
We do test with few-shot prompts, incorporating detection criteria suggested by the LVLM along with examples of correctly detected cases in the prompt. However, this approach proved far less effective than directly querying the LVLM in our experiments, so we leave this for future work.
Additionally, we observe that even for close-source large models, setting the temperature to zero does not entirely eliminate prediction variability, with fluctuations of approximately 2\%. To ensure accurate measurements, for all result in our tables, we report the average results across three runs.
We describe the baseline prompt as following:
\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
\begin{adjustbox}{minipage=[c][0.33\linewidth]{1\linewidth}}
Baseline Prompt: "These 8 images are consecutive frames of a video. \{\texttt{prompt p}\}. Must return with 1) Yes or No only; 2) if Yes, explain the reason."\\

\texttt{p1.} Do you think this video is generated by AI or not? \\
\texttt{p2.} Tell me if there are synthetic artifacts in the video or not? \\
\texttt{p3.} Do you think this video was created with the help of AI?
\end{adjustbox}
\end{tcolorbox}
The baseline prompt is constructed by replacing the placeholder $\{\texttt{prompt p}\}$ with prompt \texttt{p1}, \texttt{p2}, and \texttt{p3}. For non-structured setting, we ask the LVLM to provide responses with default free-format. For structured setting, we ask the LVLM to give us structured format response.



\begin{table*}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{4pt} % 减小列间距
\renewcommand{\arraystretch}{1.5} % 加大行间距
\begin{tabular}{ll|cccc|ccccc}
\toprule
\multirow{2}{*}{\textbf{LVLM}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{VidForensic (VidProM)~\cite{wang2024vidprommillionscalerealpromptgallery}}} & \multicolumn{4}{c}{\textbf{VidForensic (Self-collected)}} & \multirow{2}{*}{\textbf{Avg.}} \\ \cline{3-10}
                      &                         & \textbf{Pika~\cite{pika}}   & \textbf{T2vz~\cite{khachatryan2023text2videozerotexttoimagediffusionmodels}}  & \textbf{Vc2~\cite{chen2024videocrafter2overcomingdatalimitations}}  & \textbf{Ms~\cite{wang2023modelscopetexttovideotechnicalreport}}  & \textbf{OpenSORA~\cite{opensora}} & \textbf{Gen3~\cite{runway3}} & \textbf{Kling~\cite{klingai}} & \textbf{SORA~\cite{videoworldsimulators2024}} &                       \\  
\midrule

\multirow{5}{*}{\textbf{GPT-4o}~\cite{openai2024gpt4o}}      & Baseline1 (w/ SP)         & 89.00/89.22     & 90.00/90.29     & \textbf{92.50}/92.89    & 85.00/84.69   & 82.50/81.68        & 86.00/85.86    & 66.50/57.86     & \underline{68.89}/64.10    & 82.55/80.82                     \\  

                             & Baseline2 (w/ SP)       & 72.00/77.95      & 70.00/76.00     & 71.00/76.98    & 66.50/72.43   & 68.00/73.98        & 68.00/73.98    & 64.50/70.29   & 65.56/\underline{70.84}   & 68.20/74.06                     \\ 
                             
                             & Baseline3 (w/ SP)        & 89.50/88.66      & 90.50/90.73     & 92.00/92.31    & 86.00/85.71  & 82.00/80.85        & 85.00/84.54    & 69.00/61.73     & 63.33/50.75   & 82.17/79.41                     \\  
                             
\cline{2-11}
                             & \textbf{\lavid  (w/ SP)}            & \textbf{93.00}/\textbf{93.46}      & \underline{91.50}/\underline{91.94}     & \textbf{92.50}/\underline{92.96}   & \underline{89.00}/\underline{89.32}   & \textbf{86.50}/\underline{86.57}        & \textbf{91.00}/\underline{91.43}    &  \underline{75.50}/\underline{72.63}& \underline{68.89}/68.89    & \underline{85.99}/\underline{85.90}                     \\ 
                             
                             & \textbf{\lavid  (OA w/ SP)}   & 
                             \underline{91.50}/\underline{92.17}      & \textbf{92.00}/\textbf{92.52}     & \textbf{92.50}/\textbf{93.02}   & \textbf{90.50}/\textbf{91.24}   & \textbf{86.50}/\textbf{86.79}        & \textbf{91.00}/\textbf{91.59}    &  \textbf{77.00}/\textbf{76.77}& \textbf{70.93}/\textbf{72.11}    & \textbf{86.49}/\textbf{87.03}  
                             \\
                             
\bottomrule
\end{tabular}
\caption{Performance comparison of baseline methods and our method with structured prompt (SP) on eight datasets. We use the SOTA LVLM, GPT-4o~\cite{openai2024gpt4o}, which supports the structured prompt, to serve as our detector.  The results are presented as Accuracy / F1-score in each cell. Numbers in bold show the top-1 best results, and numbers with underlined show the top-2 best results.}
\label{tab:str-results}
\end{table*}

\paragraph{Implementation Details}
In our experiments, all LVLMs are configured to accept multiple image inputs. 
% While the Gemini-1.5-pro supports full video input, we opt against this due to its huge time cost. 
Videos in \vidfor are all processed to a maximum of 100 consecutive frames, and for each video, we select the middle 8 frames as input to the model. 
We also test the impact of using the first 8 frames and the last 8 frames on detection results and observe that the results are consistent across these three frame selections.
We set the hyperparameters for model generation, such as temperature $T=0$. For online adaptation implementation, we process the adaptation set in batches of 25 examples, using an F1-score threshold of 0.8 to encourage adaptation while maintaining performance standards. We set the adaptation iteration limit to 20. For template re-writing, we provide specific guidance to focus on high-level analysis perspectives. In each iteration, we ask the LVLM to propose a new field name in our structured prompt. After each template trial, we record all the rewriting records and corresponding F1 scores, allowing the LVLM to analyze past results and identify valuable fields for continuous improvement. In Appendix~\ref{supp:prompt_detail}~\ref{supp:prompt-adjust-process}, we show the prompt details for selecting explicit knowledge.

% \paragraph{Online Adaptation Implementation}
% This process is conducted entirely within the adaptation set. We process the adaptation set in batches of 25 examples, using an F1-score threshold of 0.8 to encourage adaptation while maintaining performance standards. For template rewriting, we provide specific guidance to focus on high-level analysis perspectives rather than technical parameters (e.g., frame rate=\yun{number?}, resolution=\yun{number?}) or model-specific details. \yun{need to be more specific here}. This guidance directs the LVLM to leverage the tool’s strengths, focusing on patterns, anomalies, and temporal or spatial features. After each template trial, we log the template history and related outputs \yun{how many history results? need number}, allowing the LVLM to analyze past results and identify valuable fields for continuous improvement.

\paragraph{Evaluation Metrics}
In our experiment, we aim for the model to identify artifacts in the additional information that are not present in the raw form of the original video. Using the provided toolkit, when LVLMs are presented with a video for detection, they first perform an independent analysis of each explicit knowledge information. Then integrate the prediction of each explicit knowledge using an OR operation to ensemble the final result for the video. Video-level accuracy and F1 score are adopted as the evaluation metrics for all experiments.
% As closed-source LVLMs output binary classification results directly, Accuracy provides a straightforward measurement of overall correctness by calculating the percentage of correctly classified instances. However, as Accuracy alone can sometimes mask the balance between true positive and false positive rates, we also employ the F1-score in the evaluation metrics.




\subsection{Experimental Results}
% \paragraph{Result on LVLMs with Non-structured Response}
In Table~\ref{tab:non-str-results}, the experiment is conducted under the setting of non-structured prompt with three LVLM models.
% Even though we can force the model to output a structured format through prompt engineering, to better reflect the model's true capabilities, we only set structured prompt for the model that has been well trained on structured data type. 
The result demonstrates that our \lavid framework could consistently surpass its baseline setting with the zero-shot prompt across all eight datasets. For Qwen-VL-Max~\cite{qwen2023qwenvl} and Gemini-1.5-pro~\cite{google2024gemini}, compared to the best-performing baselines, \lavid outperforms them on average F1 score by \textbf{15.1}\% and \textbf{30.2\%} gain. 
% For the high-quality VidForensic dataset part, the average improvement for both models exceeds \textbf{25.9\%} and \textbf{24.7\%}. 
For Llava-OV-7B, the average F1 score slightly improves by 7.12\% points across all eight datasets, compared to baselines.  
We believe this outcome is because the model capacity of Llava-OV-7B is too small and has limited understanding of explicit knowledge. 
Table~\ref{tab:str-results} shows the result of GPT-4o~\cite{openai2024gpt4o} with structured prompt. Additionally, considering the practical setting, we also demonstrate the result with online adaptation. Although GPT-4o's own impressive multimodal performance and its status as the highest-performing baseline (achieving an avg. F1 of 80.8\%) among all models, \lavid still outperforms it with an average improvement of \textbf{6.2\%} across all datasets and a stable average improvement of \textbf{9.4\%} on the high-quality VidForensic subsets.
In Appendix~\ref{supp:more_results}, we show results on video-specific settings.

% \begin{table*}[ht]
% \centering
% \begin{tabular}{l | c | c c c c | c c c c c}
% \toprule
% Method & Note & \multicolumn{4}{c|}{VidProM} & \multicolumn{5}{c}{Ours} \\
%  &  & Pika & T2Vz & VC2 & MS & Gen3 & Kling & OpenSORA & SORA \\
% \midrule
% AIGVDet &  &  &  &  &  &  &  &  &  &  \\
% % DeMamba(OR DIRE?) &  &  &  &  &  &  &  &  &  &  \\
% \midrule
% Qwen &  & 88.14 & 79.54 & 84.88 & 77.91 & 67.24 & 53.84 & 69.38 & 61.04 &  \\
% Gemini 1.5 Pro &  & 73.26 & 81.01 & 76.74 & 85.50 & 70.90 & 59.23 & 69.48 & 74.13 &  \\
% GPT4o &  & 95.02 & 93.32 & 93.01 & 88.12 & 86.91 & 69.94 & 84.33 & 68.98 &  \\
% \midrule
% Ours(Qwen) &  &  &  &  &  &  &  &  &  &  \\
% Ours(Gemini 1.5 Pro)  & & 90.31 & 93.41 & 94.26 & 95.66 & 85.29 & 71.60 &  89.46 & 73.88 &  \\
% Ours(GPT4o) &  &  &  &  &  &  &  &  &  &  \\


% \bottomrule
% \end{tabular}
% \caption{Comparison of Methods based on explicit knowledge}
% \end{table*}

\begin{table}[t]
\centering
% \setlength{\tabcolsep}{3pt} % 减小列间距
\setlength{\tabcolsep}{1pt} % 减小列间距
\scriptsize
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}l|ccccccccc@{}}
\toprule
\textbf{Model} & \textbf{Land.} & \textbf{Depth} & \textbf{Enhan.} & \textbf{Edge} & \textbf{Sharp.} & \textbf{Denoise} & \textbf{OPflow} & \textbf{Sat.} & \textbf{SAM}\\ \midrule
Llava-OV-7B~\cite{li2024llavaonevisioneasyvisualtask} &  &  & \checkmark &  & & & \checkmark & \checkmark   \\
Qwen-VL-Max~\cite{qwen2023qwenvl}  & \checkmark & \checkmark & & & \checkmark & & \checkmark &    \\
Gemini-1.5-pro~\cite{google2024gemini} & \checkmark & \checkmark & \checkmark & \checkmark &  & \checkmark & \checkmark &  \\ 
GPT-4o~\cite{openai2024gpt4o} & \checkmark & & & \checkmark &  & &  & \checkmark  \\ 

% Gemini-1.5-pro(DF) & \checkmark & \checkmark & \checkmark & \checkmark &  & \checkmark & \checkmark & & \checkmark\\ 
% Gpt-4o(DF)& \checkmark & & & \checkmark &  & &  & \checkmark& \checkmark\\ 
% Add additional rows as needed, with \checkmark in appropriate columns
\bottomrule
\end{tabular}%
% }
\caption{Model-specific explicit knowledge tool selection.}
\label{tab:ek_tool_set}
\end{table}
