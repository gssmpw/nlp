\onecolumn
\onehalfspacing

\begin{center}
\Large
\textbf{LAVID: An Agentic LVLM Framework for Diffusion-Generated Video Detection} \\
\vspace{0.5em}
Supplementary Material \\
\vspace{1.0em}
\end{center}

\setcounter{page}{1}



\section{More Details}
\label{supp:more_details}



\subsection{Pipeline for \vidfor Collection and Prompt Generation}
\label{supp:prompt_generation_process}

In Fig.~\ref{fig:dataset_pipeline}, we present the dataset collection pipeline for \vidfor.
 The first step is to collect fake video subset from VidProM. We use VidBench\footnote{VBench, video quality assessment tool. \url{https://github.com/Vchitect/VBench}.}, the video quality assessment tool, to filter out low-quality videos. The second step is to collect the natural video subset pairing with the VidProM subset collected from the first step. By leveraging the text prompts from the VidProM subset and video captions from Panda-70M, we compute the cosine similarity of two texts and find similar video pairs in Panda-70M. We then go through a second filtering by asking LVLM if the contents in videos are natural scenes. After finishing the second step, we collect a subset of natural videos from Panda-70M. In the third step, we use 200 real-world video captions from Panda-70M subset as the text prompts for self-generating fake videos. We use several commercial video generation platforms, including OpenSORA~\cite{openaisora}, Kling AI~\cite{klingai}, Pika Lab~\cite{pika}, and Runway Gen-3~\cite{runway3} to generate high-quality videos.

\vspace{2mm}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1\textwidth]{figs/data_preprocess_lavid.png}
    \caption{Dataset collection pipeline for \vidfor. Component marked with the logo$~\includegraphics[width=3mm, height=2.5mm]{figs/gpt_logo.png}$ are developed with the LVLM like GPT-4o~\cite{openai2024gpt4o}.}
    \label{fig:dataset_pipeline}
\end{figure*}

% Define listing style for Python
\lstdefinestyle{PythonStyle}{
    % language=Python,
    basicstyle=\ttfamily\small,
    % keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{gray},
    numbers=left,
    numberstyle=\tiny,
    stepnumber=1,
    numbersep=5pt,
    showstringspaces=false,
    breaklines=true,
    breakatwhitespace=true,
    frame=single,
    captionpos=b
}
\clearpage

\subsection{Details for Selected Explicit Knowledge}
In main paper Table~\ref{tab:ek_category}, we categorize each explicit knowledge into three video representation angles, including appearance, motion, and geometry. Here, In Table~\ref{tab:ek_category_details}, We demonstrate the understandability of LVLM on all nine explicit knowledge tools. The description of each EK tools are summarized from LVLM. In our pipeline, we select EK tools based on the reasoning ability of LVLM on them.
\label{supp:explicit-knowleged-intro}
% \textbf{Explicit Knowledge Toolkits:}
% \begin{lstlisting}[style=PythonStyle, caption={Explicit Knowledge Descriptions}]
% Appearance:  
% - Saturation: AI-generated videos may exhibit anomalies in color rendering. Saturation estimation detects color unevenness, oversaturation, or undersaturation to identify artificial elements.  
% - Denoised: Denoising isolates unnatural noise patterns present in AI-generated videos. Residual artifacts after denoising can signal synthesized or forged content.  
% - Sharpen: Sharpening frames emphasizes edges, making it easier to spot unnatural boundaries or blending artifacts, which may indicate forgery.  
% - Enhance: Image enhancement boosts details and contrast, revealing synthetic artifacts like unnatural textures or color inconsistencies.  
% - Segmentation Map: Segmentation maps identify mismatched regions in synthesized content, such as areas where the object segmentation boundaries do not align with real-world logic.

% Motion:  
% - Optical Flow: AI-generated videos may have abnormal motion patterns, such as discontinuous movements or unnatural trajectories. Optical flow estimation detects whether object motion in the video is smooth and adheres to physical laws.  
% - Landmark: In AI-generated videos, facial or body key point localization may show anomalies, such as misalignment or unnatural movement. Landmark estimation detects these anomalies to identify potential forgery.

% Geometry:  
% - Depth Map: Depth information is consistent in real scenes but may exhibit anomalies in AI-generated videos. Depth estimation detects issues like depth dislocation and discontinuity, helping identify forged content.  
% - Edge: Synthetic videos often feature unnatural edge details, such as blurred, jagged, or discontinuous object boundaries. Edge detection identifies such abnormalities to pinpoint fake or synthetic elements.

% \end{lstlisting}
\vspace{2mm}
\begin{table}[h]
\begin{tabular}{|l|l|l|}
\toprule
\textbf{Category}                    & \textbf{EK Name}          & \textbf{EK Description (Summarized from LVLM)}                                                                                                                                                                                                                                              \\ \midrule
\multirow{5}{*}{\textbf{Appearance}} & \textbf{Saturation}       & \begin{tabular}[c]{@{}l@{}}AI-generated videos may exhibit anomalies in color rendering. \\ Saturation estimation detects color unevenness, oversaturation, \\ or undersaturation to identify artificial elements.\end{tabular}                                               \\ \cmidrule{2-3} 
                                     & \textbf{Denoised}         & \begin{tabular}[c]{@{}l@{}}Denoising isolates unnatural noise patterns present in AI-generated \\ videos. Residual artifacts after denoising can signal synthesized \\ or forged content.\end{tabular}                                                                        \\  \cmidrule{2-3} 
                                     & \textbf{Sharpen}          & \begin{tabular}[c]{@{}l@{}}Sharpening frames emphasizes edges, making it easier to spot \\ unnatural boundaries or blending artifacts, which may indicate forgery.\end{tabular}                                                                                               \\ \cmidrule{2-3} 
                                     & \textbf{Enhance}          & \begin{tabular}[c]{@{}l@{}}Image enhancement boosts details and contrast, revealing synthetic \\ artifacts like unnatural textures or color inconsistencies.\end{tabular}                                                                                                     \\  \cmidrule{2-3} 
                                     & \textbf{Segmentation Map} & \begin{tabular}[c]{@{}l@{}}Segmentation maps identify mismatched regions in synthesized \\ content, such as areas where the object segmentation boundaries \\ do not align with real-world logic.\end{tabular}                                                                \\ \midrule
\multirow{2}{*}{\textbf{Motion}}     & \textbf{Optical Flow}     & \begin{tabular}[c]{@{}l@{}}AI-generated videos may have abnormal motion patterns, \\ such as discontinuous movements or unnatural trajectories. \\ Optical flow estimation detects whether object motion in the \\ video is smooth and adheres to physical laws.\end{tabular} \\  \cmidrule{2-3} 
                                     & \textbf{Landmark}         & \begin{tabular}[c]{@{}l@{}}In AI-generated videos, facial or body key point localization \\ may show anomalies, such as misalignment or unnatural movement. \\ Landmark estimation detects these anomalies to identify potential forgery.\end{tabular}                        \\ \midrule
\multirow{2}{*}{\textbf{Geometry}}   & \textbf{Depth Map}        & \begin{tabular}[c]{@{}l@{}}Depth information is consistent in real scenes but may exhibit anomalies \\ in AI-generated videos. Depth estimation detects issues like depth \\ dislocation and discontinuity, helping identify forged content.\end{tabular}                     \\  \cmidrule{2-3}  
                                     & \textbf{Edge}             & \begin{tabular}[c]{@{}l@{}}Synthetic videos often feature unnatural edge details, such as blurred, \\ jagged, or discontinuous object boundaries. Edge detection identifies\\  such abnormalities to pinpoint fake or synthetic elements.\end{tabular}                        \\ \bottomrule
\end{tabular}
\caption{Details for nine explicit knowledge tools}
\label{tab:ek_category_details}
\end{table}



 \clearpage
 
\subsection{Details for Explicit knowledge Preparation}
\label{supp:prompt_detail}
At the toolkit preparation step, we first call LVLMs to summarize the potential toolkit by themselves, then we further filter tools based on usefulness and capabilities. The summarization prompt shown as bellow:
\label{supp:explicit-knowleged-summarize}
% \textbf{Explicit Knowledge Toolkits:}

\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
\begin{adjustbox}{minipage=[c][0.24\linewidth]{1\linewidth}}
\begin{lstlisting}[style=PythonStyle, frame=none, numbers=none]
Preparation_Prompt = """
    This is an AI-generated video detection task based on large vision-language models (LVLMs). Besides using raw frames from the video, are there any external tools that could help extract additional video information? These tools will used to facilitate LVLMs-based detection. Specifically, I'm looking for methods or tools that can generate features from the video like optical flow and sharpening. Please summarize the tool list for me.
"""
\end{lstlisting}
\end{adjustbox}
\end{tcolorbox}

% \yun{qinyuan what is the output response from this preparation prompt?}
% \begin{lstlisting}[style=PythonStyle, caption={Explicit Knowledge Summarize}]
% Preparation_Prompt = """
% This is an AI-generated video detection task based on
% large vision-language models (LVLMs). Besides using raw frames from the video, are there any external tools that could help extract additional video information? These tools will used to facilitate LVLMs-based detection. Specifically, I'm looking for methods or tools that can generate features from the video like optical flow and sharpening. Please summarize the tool list for me.
% """
% \end{lstlisting}
% \usepackage{listings}
% \usepackage{xcolor}

We show the sample responses with three recommended features from chatGPT as follows. The keywords highlighted in blue are the useful information provided by chatGPT to facilitate our tool selection. 

\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
\begin{adjustbox}{minipage=[c][0.7\linewidth]{1\linewidth}}
\texttt{Certainly, for an AI video detection task based on LVLMs, leveraging external tools can significantly enhance the quality and diversity of the features extracted, which can, in turn, improve detection performance. Here are several methods and tools that can provide valuable additional features from videos:}\\

\textbf{\texttt{1. Optical Flow Extraction}} \\
\texttt{
Optical flow captures the motion between consecutive frames and can help LVLMs understand temporal dynamics. Tools such as {\color{blue}OpenCV’s Optical Flow API}(specifically {\color{blue}calcOpticalFlowFarneback} or {\color{blue}calcOpticalFlowPyrLK}) and {\color{blue}FlowNet2} can extract high-quality optical flow features that capture the movement patterns within videos.}\\

\textbf{\texttt{2. Sharpening Filters}} \\
\texttt{To enhance edges and fine details in frames, which may be beneficial for object detection or recognizing subtle transitions, consider using sharpening filters. 
{\color{blue}OpenCV’s filtering functions} ({\color{blue}cv2.filter2D} or {\color{blue}cv2.addWeighted}) can be applied to sharpen frames. Alternatively, {\color{blue}ffmpeg} offers customizable sharpen filters, which can be applied in bulk to a video.}\\

\textbf{\texttt{3. Depth Map Estimation}} \\
\texttt{Depth maps provide depth information per frame, aiding in 3D scene understanding. Tools like {\color{blue}MiDaS} (an open-source depth estimation model from Intel) and {\color{blue}DepthAI} (an AI toolkit for real-time depth estimation) can create depth maps for each frame, helping the model grasp scene structure beyond 2D features.} \\

\texttt{...... [more]}

\end{adjustbox}
\end{tcolorbox}
% \begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
% \begin{adjustbox}{minipage=[c][0.1\linewidth]{0.95\linewidth}}
% Baseline Prompt: "These 16 images contain 8 consecutive frames and its corresponding {Explicit Knowledge(EK)} images of a video. Do you think the original video is generated by AI or not? Must return with 1) Yes or No only; 2) if Yes, explain the reason."\\

% EK: \{Edge, Landmark, Enhance, Saturation, Optical flow, Depth, Segment Anything, Sharpen, Denoise\}
% \end{adjustbox}
% \end{tcolorbox}
\clearpage




\subsection{Details for Online Adaptation on Structured Prompt}
\label{supp:prompt-adjust-process}
In this subsection, we introduce our template re-write function, which is a key component of the online adaptation process. This function ensures that the LVLMs model adapts its structured format to specific analytical needs. To achieve this, we provide a detailed system prompt consisting of five key criteria: \textit{Requirements}, \textit{Analysis Guidelines}, \textit{Constraints}, \textit{Prohibited Fields}, and \textit{Additional Notes}. These criteria collectively guide the structured prompt generation process, ensuring flexibility, precision, and relevance in adapting templates to specific tools and use cases.
\vspace{-4mm}
\paragraph{System Prompt for Template Rewrite:} \mbox{} 
\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
\begin{adjustbox}{minipage=[c][0.77\linewidth]{1\linewidth}}
\texttt{prompt = f"""}\\
{\color{blue}\textbf{\texttt{As a Python developer, your task is to create a new Pydantic class for analyzing video data using the tool \{tool\_name\}.}}}\\

{\color{ForestGreen}\textbf{\texttt{Requirements:}}}\\
- \texttt{The class must include the field 
'is\_ai\_generated: bool'}.\\
- \texttt{You may add any other fields you deem necessary to analyze video data with \{tool\_name\}.}\\
- \texttt{All additional fields must be of type 'str'}.\\
- \texttt{Fields should represent analysis perspectives specific to the capabilities of  \{tool\_name\}}.\\

{\color{ForestGreen}\textbf{\texttt{Analysis Guidelines:}}}\\
- \texttt{Consider the aspects of videos that \{tool\_name\} excels at analyzing.}\\
- \texttt{Reflect on patterns or anomalies that 
\{tool\_name\} might reveal.}\\
- \texttt{Emphasize high-level analysis perspectives that leverage the strengths of \{tool\_name\}.}\\

{\color{ForestGreen}\textbf{\texttt{Constraints:}}}\\
- \texttt{You may modify only one or two fields from previous class definitions at a time.}\\
- \texttt{Focus on high-level abstractions specific to the purpose of \{tool\_name\}.}\\

{\color{ForestGreen}\textbf{\texttt{Prohibited Fields:}}}\\
- \texttt{Technical parameters (e.g., frame\_rate, resolution, format, duration).}\\
- \texttt{Algorithm or implementation specifics.}\\

{\color{ForestGreen}\textbf{\texttt{Additional Notes:}}}\\
- \texttt{The total number of fields must not exceed five (5).}\\
- \texttt{There must be at least one field that differs from previous class definitions.}\\

{\color{ForestGreen}\textbf{\texttt{Previous outputs:}}} \texttt{\{history\_str\}}
\end{adjustbox}
\end{tcolorbox}



\paragraph{Template Evolution Logging} We provide the output logging for each round of template evolution. For each slot, we provide a batch of 25 real and 25 AI-generated samples as input to the LVLM. At the beginning of the first slot, we initialize the prompt template with simple key fields. In every slot, we compute the F1 score on the proposed and prior templates. If the proposed template achieves a higher F1 score, we update it; otherwise, the old template is retained. Each slot allows up to five rewrite attempts. The adaptation process terminates after several iterations. Here we set up the iteration as $4$.
\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=1pt, right=1pt]
\begin{adjustbox}{minipage=[c][1.3\linewidth]{1\linewidth}}
\texttt{Starting Template Evolution with 89 Real and 89 AI-Generated Test Videos.}\\
{\color{blue}\textbf{\texttt{{--------- Slot 1/4 for edge ---------}}}}\\
\textbf{\texttt{Initial Template:}}\\
 \begin{adjustbox}{minipage=[c][0.20\linewidth]{0.5\linewidth}}
 \begin{tcolorbox}[colback=blue!10!white,,colframe=white!5!white, top=0.1pt,bottom=0.1pt]
\begin{lstlisting}[basicstyle=\footnotesize, style=PythonStyle, frame=none]
class AIAnalysisResult(BaseModel):
    is_ai_generated: bool
    raw_frame_analysis: str
    edge_analysis: str
    explanation: str
\end{lstlisting}
\end{tcolorbox}
\end{adjustbox}\\
\textbf{\texttt{Initial F1 Score: 84.94\%}}\\
\texttt{Attempt 1/5}\\
\textbf{\texttt{Proposed Template:}}\\
 \begin{adjustbox}{minipage=[c][0.22\linewidth]{0.5\linewidth}}
 \begin{tcolorbox}[colback=blue!10!white,,colframe=white!5!white, top=0.1pt,bottom=0.1pt]
\begin{lstlisting}[basicstyle=\footnotesize, style=PythonStyle, frame=none]
class NewAnalysisResult(BaseModel):
    is_ai_generated: bool
    boundary_clarity: str
    texture_consistency: str
    object_delineation: str
    spatial_anomaly_detection: str
\end{lstlisting}
\end{tcolorbox}
\end{adjustbox}\\
\textbf{\texttt{Combined F1 Score: 93.62\%}}\\
\texttt{Combined Real Success Rate: 86.36\%}\\
\texttt{Combined AI Success Rate: 100.00\%}\\
{\color{ForestGreen}$\checkmark$ \textbf{\texttt{Template improved!}}} \\
\textbf{\texttt{Slot 1 Complete}}\\
\textbf{\texttt{Best F1 Score so far: 93.62\%}}\\
{\color{blue}\textbf{\texttt{{--------- Slot 2/4 for edge ---------}}}}\\
\texttt{Evaluating previous best template...}\\
\textbf{\texttt{Previous Template F1 Score: 83.72\%}}\\
\texttt{Attempt 1/5}\\
\textbf{\texttt{Proposed Template:}}\\
 \begin{adjustbox}{minipage=[c][0.22\linewidth]{0.5\linewidth}}
 \begin{tcolorbox}[colback=blue!10!white,,colframe=white!5!white, top=0.1pt,bottom=0.1pt]
\begin{lstlisting}[basicstyle=\footnotesize, style=PythonStyle, frame=none]
class NewAnalysisResult(BaseModel):
    is_ai_generated: bool
    boundary_clarity: str
    texture_consistency: str
    object_delineation: str
    temporal_edge_coherence: str
\end{lstlisting}
\end{tcolorbox}
\end{adjustbox}\\
\textbf{\texttt{Combined F1 Score: 88.37\%}}\\
\texttt{Combined Real Success Rate: 90.91\%}\\
\texttt{Combined AI Success Rate: 86.36\%}\\
{\color{ForestGreen}$\checkmark$ \textbf{\texttt{Template improved!}}} \\
\textbf{\texttt{Slot 2 Complete}}\\
\textbf{\texttt{Best F1 Score so far: 93.62\%}}\\
\texttt{[continued...]}
\end{adjustbox}
\end{tcolorbox}

\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small, left=2pt, right=2pt]
\begin{adjustbox}{minipage=[c][0.56\linewidth]{1\linewidth}}
{\color{blue}\textbf{\texttt{{--------- Slot 3/4 for edge ---------}}}}\\
\texttt{Evaluating previous best template...}\\
\textbf{\texttt{Previous Template F1 Score: 87.50\%}}\\
{\color{BrickRed} \textbf{\texttt{Previous template performs well on new slot!}}} \\
\textbf{\texttt{Slot 3 Complete}}\\
{\color{blue}\textbf{\texttt{{--------- Slot 4/4 for edge ---------}}}}\\
\texttt{Evaluating previous best template...}\\
\textbf{\texttt{Previous Template F1 Score: 93.02\%}}\\
{\color{BrickRed} \textbf{\texttt{Previous template performs well on new slot!}}} \\
\textbf{\texttt{Slot 4 Complete}}\\
{\color{blue}\textbf{\texttt{{--------- Template Evolution Completed ---------}}}}\\
\textbf{\texttt{Final Template:}}\\
 \begin{adjustbox}{minipage=[c][0.22\linewidth]{0.5\linewidth}}
 \begin{tcolorbox}[colback=blue!10!white,,colframe=white!5!white, top=0.1pt,bottom=0.1pt]
\begin{lstlisting}[basicstyle=\footnotesize, style=PythonStyle, frame=none]
class NewAnalysisResult(BaseModel):
    is_ai_generated: bool
    boundary_clarity: str
    texture_consistency: str
    object_delineation: str
    temporal_edge_coherence: str
\end{lstlisting}
\end{tcolorbox}
\end{adjustbox}\\
% \textbf{\texttt{Best F1 Score Achieved: 93.62\%}}
\end{adjustbox}
\end{tcolorbox}


% \paragraph{Prompt for Template Rewrite:}
% { % You can adjust the color as needed
% \begin{verbatim}
% prompt = f"""
% \end{verbatim}}{\color{blue}\textbf{\texttt{{--------- Slot 3/4 for edge ---------}}}}\\

% {\color{blue}\textbf{\texttt{{--------- Slot 4/4 for edge ---------}}}}\\

% {\color{blue}\textbf{As a Python developer, your task is to create a new Pydantic class for analyzing video data using the tool \{tool\_name\}.}}

% {\color{darkgreen}\textbf{Requirements:}}
% \begin{verbatim}
% - The class must include the field 'is_ai_generated: bool'.
% - You may add any other fields you deem necessary to analyze video data with {tool_name}.
% - All additional fields must be of type 'str'.
% - Fields should represent analysis perspectives specific to the capabilities of {tool_name}.
% \end{verbatim}

% {\color{darkgreen}\textbf{Analysis Guidelines:}}
% \begin{verbatim}
% - Consider the aspects of videos that {tool_name} excels at analyzing.
% - Reflect on patterns or anomalies that {tool_name} might reveal.
% - Emphasize high-level analysis perspectives that leverage the strengths of {tool_name}.
% \end{verbatim}

% {\color{darkgreen}\textbf{Constraints:}}
% \begin{verbatim}
% - You may modify only one or two fields from previous class definitions at a time.
% - Focus on high-level abstractions specific to the purpose of {tool_name}.
% \end{verbatim}

% {\color{darkgreen}\textbf{Prohibited Fields:}}
% \begin{verbatim}
% - Technical parameters (e.g., frame_rate, resolution, format, duration).
% - Algorithm or implementation specifics.
% \end{verbatim}

% {\color{darkgreen}\textbf{Additional Notes:}}
% \begin{verbatim}
% - The total number of fields must not exceed five (5).
% - There must be at least one field that differs from previous class definitions.


% Previous outputs:
% {history_str}
% \end{verbatim}


% {\begin{verbatim}
% """
% \end{verbatim}}

% \textbf{Template Evolution Logging:}
% \begin{verbatim}
% Starting Template Evolution with 89 Real and 89 AI-Generated Test Videos
% \end{verbatim}

% {\color{blue}\textbf{--- Slot 1/4 for edge ---}}
% \begin{verbatim}
% Initial Template:
% class AIAnalysisResult(BaseModel):
%     is_ai_generated: bool
%     raw_frame_analysis: str
%     edge_analysis: str
%     explanation: str
% Initial F1 Score: 84.94%
% Attempt 1/5
% Proposed Template:
% class NewAnalysisResult(BaseModel):
%     is_ai_generated: bool
%     boundary_clarity: str
%     texture_consistency: str
%     object_delineation: str
%     spatial_anomaly_detection: str
% Combined F1 Score: 93.62%
% Combined Real Success Rate: 86.36%
% Combined AI Success Rate: 100.00%
% \end{verbatim}
% {\color{darkgreen}$\checkmark$ Template improved!}
% \begin{verbatim}
% Slot 1 Complete
% Best F1 Score so far: 93.62%
% \end{verbatim}

% {\color{blue}\textbf{--- Slot 2/4 for edge ---}}
% \begin{verbatim}
% Evaluating previous best template...
% Previous Template F1 Score: 83.72%
% Attempt 1/5
% Proposed Template:
% class NewAnalysisResult(BaseModel):
%     is_ai_generated: bool
%     boundary_clarity: str
%     texture_consistency: str
%     object_delineation: str
%     temporal_edge_coherence: str
% Combined F1 Score: 88.37%
% Combined Real Success Rate: 90.91%
% Combined AI Success Rate: 86.36%
% \end{verbatim}
% {\color{darkgreen}$\checkmark$ Template improved!}
% \begin{verbatim}
% Slot 2 Complete
% Best F1 Score so far: 93.62%
% \end{verbatim}

% {\color{blue}\textbf{--- Slot 3/4 for edge ---}}
% \begin{verbatim}
% Evaluating previous best template...
% Previous Template F1 Score: 87.50%
% \end{verbatim}
% {\color{darkgreen}$\checkmark$ Previous template performs well on new slot!}

% {\color{blue}\textbf{--- Slot 4/4 for edge ---}}
% \begin{verbatim}
% Evaluating previous best template...
% Previous Template F1 Score: 93.02%
% \end{verbatim}
% {\color{darkgreen}$\checkmark$ Previous template performs well on new slot!}

% {\color{darkgreen}\textbf{--- Template Evolution Completed ---}}

% \begin{verbatim}
% Final Template:
% class NewAnalysisResult(BaseModel):
%     is_ai_generated: bool
%     boundary_clarity: str
%     texture_consistency: str
%     object_delineation: str
%     spatial_anomaly_detection: str
% Best F1 Score Achieved: 93.62%
% \end{verbatim}

\clearpage
\section{More Results for Video-specific Tool Selection}
\label{supp:more_results} 

% \subsection{}
% \label{supp:video-specific-results}

In Table~\ref{tab:video-specific-results}, we show the results of \lavid with video-specific tool selection, which means after selecting the toolkit for each model, when giving a test video, the model could select the tools based on its own understanding of this video, then facilitate the detection. In addition, \lavid with video-specific tool selection will further reduce the detection cost. For the Qwen-VL-Max model, the number of tools it uses per video dropped from 4 to 1.8, a decrease of 55\%; the Gemini-1.5-pro model dropped from 6 tools per video to 1.0, a decrease of 83.3\%; and the GPT-4o dropped from 3 to 2.7, a decrease of 10\%. \\
Nevertheless, the \lavid with video-specific tool selection maintains a competitive edge over the highest baseline methods. For Qwen-VL-Max, the average F1 score improves by 10. 07\% points across the eight datasets, compared to the top baseline. For Gemini-1.5-pro, the improvement is 18.25\%. And for GPT-4o, the increase is 5.93\%.

\begin{table*}[h]
\centering
\scriptsize
\setlength{\tabcolsep}{3pt} % 减小列间距
\renewcommand{\arraystretch}{1.5} % 加大行间距
\begin{tabular}{ll|cccc|ccccc}
\toprule
\multirow{2}{*}{\textbf{LVLM}} & \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c|}{\textbf{VidForensic (VidProM)~\cite{wang2024vidprommillionscalerealpromptgallery}}} & \multicolumn{4}{c}{\textbf{VidForensic (Self-collected)}} & \multirow{2}{*}{\textbf{Avg.}} \\ \cline{3-10}
                      &                         & \textbf{Pika~\cite{pika}}   & \textbf{T2vz~\cite{khachatryan2023text2videozerotexttoimagediffusionmodels}}  & \textbf{Vc2~\cite{chen2024videocrafter2overcomingdatalimitations}}  & \textbf{Ms~\cite{wang2023modelscopetexttovideotechnicalreport}}  & \textbf{OpenSORA~\cite{opensora}} & \textbf{Gen3~\cite{runway3}} & \textbf{Kling~\cite{klingai}} & \textbf{SORA~\cite{videoworldsimulators2024}} &                       \\  
\midrule
                             
\multirow{4}{*}{\textbf{Qwen-VL-Max}~\cite{qwen2023qwenvl}} & Baseline1 (w/o SP)    & 72.50/63.09      & 75.00/67.53     & 82.00/78.57    & 76.00/69.23   & 67.50/53.24        & 62.00/40.62    & 54.50/19.47     & 58.89/39.34    & 68.55/51.24                     \\ 

                             & Baseline2 (w/o SP)        & 60.50/38.76      & 75.00/68.35    & 71.50/62.25    & 72.50/64.05   & 60.50/38.76        & 52.00/14.29    & 50.00/7.41     & 56.67/26.42    & 62.33/39.56                     \\ 
                             
                             & Baseline3 (w/o SP)        & \underline{74.00}/\underline{67.90}      & \underline{79.00}/75.58     & \underline{84.50}/\underline{83.06}    & \underline{79.50}/76.30   & 69.50/60.13        & 65.50/52.41    & 54.00/24.59     & 61.11/47.76    & 70.89/60.97                     \\ 
                             
\cline{2-11}
                             & \textbf{\lavid  (w/o SP)}         & \textbf{87.00}/\textbf{88.39}     & \textbf{81.50}/\textbf{82.63}     & \textbf{86.00}/\textbf{87.39}    & 77.00/\underline{77.45}   & \textbf{79.00}/\textbf{79.81}        & \textbf{82.50}/\textbf{83.72}    & \underline{60.00}/\underline{52.94}     & \underline{67.78}/\textbf{71.84}    & \textbf{77.60}/\textbf{76.08}                     \\
                             & \textit{w/ video-specific Sel.}            & 70.14/62.83     & 78.50/\underline{76.76}     & 82.25/81.38    & \textbf{80.17}/\textbf{78.70}   & \underline{77.25}/\underline{74.48}        & \underline{69.44}/\underline{61.53}    & \textbf{70.27}/\textbf{62.65}     & \textbf{74.02}/\underline{69.99}    & \underline{75.26}/\underline{71.04}                     \\                                   
                    
                             
\midrule
\multirow{4}{*}{\textbf{Gemini-1.5-pro}~\cite{google2024gemini}} & Baseline1 (w/o SP)        & 68.33/54.32      & 71.00/59.72     & 67.00/51.47    & 75.00/67.11   & 68.50/54.68        & 64.00/44.62    & 58.00/28.81    & 58.89/41.27    & 66.34/49.83                     \\ 

                             & Baseline2 (w/o SP)        & 73.50/66.24      & 81.00/77.91     & 76.00/70.37    & \underline{85.00}/\underline{83.33}  & 71.50/62.75       & 71.50/62.75  & 59.50/37.21     & 71.11/64.86    & 72.51/58.28                    \\ 
                             
                             & Baseline3 (w/o SP)        & 64.50/45.80      & 77.00/70.51     & 71.00/59.72    & 76.50/69.68   & 64.50/45.80        & 62.00/39.68    & 52.50/11.21     & 61.11/42.62    & 66.08/51.28                     \\ 
                             
\cline{2-11}
                             & \textbf{\lavid  (w/o SP)}            & \textbf{92.00}/\textbf{91.73}     & \textbf{96.33}/\textbf{96.38}    & \textbf{95.83}/\textbf{95.87}    & \textbf{97.50}/\textbf{97.56}   & \textbf{92.17}/\textbf{91.93}        & \textbf{88.50}/\textbf{87.67}    & \underline{74.83}/\underline{68.46}     & \underline{76.67}/\textbf{78.36}    & \textbf{89.23}/\textbf{88.43}                     \\
                             & \textit{w/ video-specific Sel.}            & \underline{77.31}/\underline{71.84}     & \underline{84.00}/\underline{82.02}     & \underline{82.00}/\underline{79.25}    & 83.35/81.25   & \underline{81.50}/\underline{78.33}        & \underline{76.99}/\underline{71.29}    & \textbf{77.16}/\textbf{71.44}     & \textbf{80.09}/\underline{76.84}    & \underline{80.30}/\underline{76.53}                     \\      
\midrule
                             
\multirow{5}{*}{\textbf{GPT-4o}~\cite{openai2024gpt4o}}      & Baseline1 (w/ SP)         & 89.00/\underline{89.22}     & 90.00/90.29     & \textbf{92.50}/\underline{92.89}    & 85.00/84.69   & 82.50/81.68        & \underline{86.00}/\underline{85.86}    & 66.50/57.86     & 68.89/64.10    & 82.55/80.82                     \\  

                             & Baseline2 (w/ SP)       & 72.00/77.95      & 70.00/76.00     & 71.00/76.98    & 66.50/72.43   & 68.00/73.98        & 68.00/73.98    & 64.50/70.29   & 65.56/\underline{70.84}   & 68.20/74.06                     \\ 
                             
                             & Baseline3 (w/ SP)        & \underline{89.50}/88.66      & \underline{90.50}/\underline{90.73}     & 92.00/92.31    & 86.00/85.71  & 82.00/80.85        & 85.00/84.54    & 69.00/61.73     & 63.33/50.75   & 82.17/79.41                     \\  
                             
\cline{2-11}
                             & \textbf{\lavid  (w/ SP)}            & \textbf{93.00}/\textbf{93.46}      & \textbf{91.50}/\textbf{91.94}     & \textbf{92.50}/\textbf{92.96}   & \underline{89.00}/\underline{89.32}   & \underline{86.50}/\underline{86.57}        & \textbf{91.00}/\textbf{91.43}    &  \underline{75.50}/\underline{72.63}& \underline{68.89}/68.89    & \underline{85.99}/\underline{85.90}                     \\ 
                             & \textit{w/ video-specific Sel.}            & 84.22/83.93     & 90.00/90.65     & 90.50/91.16    & \textbf{89.67}/\textbf{90.30}   & \textbf{88.50}/\textbf{89.05}        & 83.18/82.75    & \textbf{82.46}/\textbf{81.87}     & \textbf{84.36}/\textbf{84.25}    & \textbf{86.61}/\textbf{86.75}                     \\                                   
                             
\bottomrule
\end{tabular}

\caption{Performance comparison of baselines and \lavid with and without video-specific tool selection on eight datasets. For each dataset except SORA, we mix the real dataset from Panda-70M \& AI-generated dataset together. For SORA, we mix it with 45 youtube videos that collected by ourselves. We use three representative LVLMs, including Qwen-VL-Max~\cite{qwen2023qwenvl}, Gemini-1.5-pro~\cite{google2024gemini}, and GPT-4o~\cite{openai2024gpt4o}. The results are presented as Accuracy / F1-score in each cell. Numbers in bold show the top-1 best results, and numbers with underlined show the top-2 best results.}
\label{tab:video-specific-results}
\end{table*}

\clearpage

\section{Pseudo-algorithm}
\label{supp:prompt_generation_process}
In Algo.~\ref{algo:lavid}, we provide the pseudo-algorithm for \lavid. Our detection pipeline includes two main steps (1.) EK tools selection (2.) Online adaptation for structured prompt.
\begin{algorithm}[h]
\setstretch{1.1}
\KwIn{Input Images $x$, Adaptation Set $\mathcal{X}_1$, Inference Set $\mathcal{X}_2$, Initial Prompt Template $p$, Detector $\mathcal{M}(\cdot)$, Explicit Knowledge Set $\text{EK}=\{t_1, t_2, ..., t_q\}$, Optimal Explicit Knowledge Set $\text{EK}^\star$,  Tool-Selection Metric ($S_{\text{Tool}}$,$\text{F1}_{\text{weighted}}$, $S_{\text{MP}}$), History set $\mathcal{P}_\text{history}$, Prompt Rewrite Function $\texttt{Rewrite}(\cdot)$, Detector $\mathcal{M}(\cdot)$, Number of Adaptation Iteration $\mathcal{T}$. Batch Size $B$}
\KwOut{Output prediction $\hat{y}$ from Detector $\mathcal{M}(\cdot)$.} 
\textbf{\#\#\# Adaptation} \\
    $x \sim \mathcal{X}_1, \quad \text{EK}^\star \gets \emptyset$ \Comment{Parameter onitialization}\\
    $ S_{\text{Baseline}}(x) = \alpha \cdot \text{F1}_{\text{weighted}}(x) + (1 - \alpha) \cdot S_{\text{MP}}(t=\texttt{"RGB"})$  \Comment{Compute baseline score}\\
\textbf{\#\#\# EK Tools Selection} \\
\For{$i\in\{0,...,q\}$}   
{  
       \hspace{1mm} $ S_{\text{Tool}}(t_i, x) = \alpha \cdot \text{F1}_{\text{weighted}}(t_i,x) + (1 - \alpha) \cdot S_{\text{MP}}(t_i)$  \Comment{Compute score for each tool}\\
    \If{$S_{\text{Tool}}(t_i, x) \geq S_{\text{Baseline}}(x)$} {
        $\text{EK}^\star \gets \text{EK}^\star \| t_i $ \Comment{Append tool to Optimal EK Set}
     }
     \Else{continue}
}
\textbf{\#\#\# Online adaptation for Structured  prompt} \\
$p_0 \gets p , \quad \mathcal{P}_\text{history} \gets \emptyset$  \Comment{Initialize prompt template and history set}\\
\For{$b\in\{0,...,|\mathcal{X}_2|//\text{B}\}$}{
       \hspace{1mm} $x \gets \mathcal{X}_2^{[b*B: (b+1)*B]}$   \Comment{Extract sample by batch}\\
    \For{$i\in\{0,...,\mathcal{T}\}$}{
        \hspace{1mm} $\hat{y} = \mathcal{M}(\text{EK}^\star, x, p_i)$, \quad         $\text{f1}_{score} = \text{F1}(\hat{y}, y)$  \Comment{Compute score for current prompt}\\
        $p'_i \gets \texttt{Rewrite}(p_i, \mathcal{P}_\text{history})$ \Comment{Rewrite prompt}\\
        $\hat{y}' = \mathcal{M}(\text{EK}^\star, x, p'_i)$, \quad         $\text{f1}'_{score} = \text{F1}(\hat{y}', y)$  \Comment{Compute score for rewritten prompt}\\
        \If{$\text{f1}^{'}_{score} \geq \text{f1}_{score}$}{
         $ \mathcal{P}_\text{history} \gets  \mathcal{P}_\text{history} \| (p'_i, \text{f1}'_{score})$  \Comment{Append rewritten prompt to history set}\\
        }
        \Else{
         $ \mathcal{P}_\text{history} \gets  \mathcal{P}_\text{history} \| (p_i, \text{f1}_{score})$ \Comment{Append current prompt to history set}\\ 
        }
    \hspace{1mm} $p^\star \gets p_{\mathcal{T}}$ \\
    % $\hat{y}' = \mathcal{M}(\text{EK}^\star, x, p\star)$
    $\hat{y}^\star =\bigcup_{i=1}^{n}\mathcal{M}(\text{EK}^\star_i, x, p^\star)$ \Comment{Union the prediction for batch sample} 
    }
}
% \textbf{Inference} \\
% $\delta^0 \sim \mathcal{U}(-\epsilon, \epsilon), \quad x^0 \gets x$ \Comment{Initialization of parameters}  \\
% \# \textbf{Iteration of GIDAttack} \\
% \For{$t\in\{0,...,\mathcal{T}\}$}
% {   
%     \hspace{0.5mm} $[x_1^t, ...,  x_{Q-1}^t, x_Q^t] = \textbf{I}(x^t)$ \Comment{Sample sequence from $Q$ steps of $\text{\textbf{I}}$}  \\
%     $[x^{'t}_{Q-1}, ...,  x^{'t}_1,  x^{'t}] = \textbf{R}(x_Q^t)$ \Comment{Sample sequence from $Q$ steps of $\text{\textbf{R}}$} \\
%     % $d_t = \vert \text{\textbf{R}}(\text{\textbf{I}}(x^t)) - x^t \vert$ \Comment{DIRE from input sample} \\
%     $\mathcal{L}_{All} = \mathcal{L}_{dire}(x^{'t}, x^t) + \mathcal{L}_{dr}([x_1^t, ..., x_{Q-1}^t],  [x^{'t}_1, ..., x^{'t}_{Q-1}] )$ \Comment{Loss calculation} \\
%     $\delta^{t+1} = \text{clip}_{\epsilon} (\delta^{t} + \beta \cdot \nabla_{\delta^t} ( \mathcal{L}_{All}(\cdot)))$ \\
%     $x^{t+1} = x^t + \delta^{t+1}$
    
% }
% \hspace{0.5mm} $x^\star \gets x^T$ \Comment{Get optimal sample}\\
% $d^\star = \vert \text{\textbf{R}}(\text{\textbf{I}}(x^\star)) - x^\star \vert$ \Comment{Get adversarial DIRE} \\
% \KwRet $\hat{y} \gets \mathcal{F}(d^\star)$

\caption{\textbf{Pseudo-algorithm for \lavid detection pipeline}}
\label{algo:lavid}
\end{algorithm}%

\clearpage

\section{Sample Visualization} 
% \subsection{sample visualization}
\label{supp:sample-visualization}
In Fig.~\ref{fig:data_visualization}, we provide a visualization of diffusion-generated video detection through our agentic framework LAVID and a baseline model. LAVID analyzes the video using EK tools and utilizes the results of the analysis to make robust detection decisions while the baseline model simply outputs a "yes" or "no" when prompted "do you think the original video is generated by AI or not?".
% \yun{vicky todo}

\begin{figure}[h!]
    \centering
    \includegraphics[width=.999\linewidth]{figs/data_visualization_v4.png}
    \caption{Sample Visualization}
    \label{fig:data_visualization}
\end{figure}
 










