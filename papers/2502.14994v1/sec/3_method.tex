\section{Preliminary}
\label{preliminary}
\subsection{Task Definition}
Our task objective is to explore LVLM's reasoning capability to detect video generated from any sources of video generative models. Given a video input $v$ and a corresponding selected set of explicit knowledge ($\text{EK}$), we ask LVLM to classify $v$ as candidate label $y=\{\textit{real}, \textit{fake}\}$ based on following criteria: (1) Whether there are artifacts from the selected set ($\text{EK}$) for $v$. (2) Whether there are inconsistencies from the selected set ($\text{EK}$).  Here, we view each tool in $\text{EK}$ as an individual detection sub-task. 

\subsection{Video Dataset Exploration}
% To facilitate our research, we create a new benchmark for video forensic detection called $\emph{VidForensic}$. The $\textit{VidForensic}$ dataset features 200 text-to-video prompts and more than 1.4k videos generated from 8 SOTA commercial generative models, including Kling, Runway Gen3, and SORA.
% We first applied rigorous filtering to the videos in VidProM, using Vbench evaluations to systematically exclude low-quality generated videos, thereby ensuring a focus on human-centered subjects and maintaining high standards of video quality. 
% For the corresponding real video set is collected based246
% on the prompt similarity score between VidProM subset247
% and PANDA-70M, which is a large-scale video dataset with248
% millions of videos sourced from YouTube. For our own249
% high quality fake video generation, we use these 200 text250
% prompts generated the video with these prompts using three251
% generation tools, including Open-SORA, Kling, and Runway252
% Gen3. In Table 1, we show the details of our collected253
% benchmark.
To facilitate our research, we create a new benchmark called \emph{\textbf{\vidfor}}. \vidfor dataset features 200 text-to-video prompts and more than 1.4k high-quality videos, collected or generated from eight generative models. In Table~\ref{tab:video_dataset_detail}, we show the details of \vidfor benchmark.
For real videos, we collect them from PANDA-70M~\cite{chen2024panda70mcaptioning70mvideos}, a real-world video dataset with millions of videos sourced from YouTube. 
For fake videos, we either collect them from VidProM~\cite{wang2024vidprommillionscalerealpromptgallery} or generate by ourselves to incorporate latest generative models. 
To ensure video quality, during the collection from VidProM, we carefully filter out low-quality videos (e.g., with background inconsistencies, subject inconsistencies, or unsmooth motion) by using \emph{VBench}\footnote{VBench, video quality assessment tool. \url{https://github.com/Vchitect/VBench}.}, the SOTA video quality assessment tool. 
% We also leverage the text-embedding similarity score between prompt and caption to filter out best matching pairs of videos from VidProM and PANDA-70M. 
% The filter process results in 200 high-relevance and quality fake and real video pairs.
For the video set generated by us, we utilize the SOTA generation tools: OpeneSORA, Kling~\cite{klingai}, and Runway Gen3~\cite{runway3}, to generate high-quality videos based on the 200 prompts collected from the captions in PANDA-70M videos. In Appendix~\ref{supp:prompt_generation_process}, we provide details of high-quality prompt generation process.

% For the corresponding real video set is collected based on the prompt similarity score between VidProM subset and PANDA-70M, which is a large-scale video dataset with millions of videos sourced from YouTube.
% For our own high quality fake video generation, we use these 200 text prompts generated the video with these prompts using three generation tools, including Open-SORA, Kling, and Runway Gen3. In Table~\ref{tab:video_dataset_detail}, we show the details of our collected benchmark.

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{1pt} % 减小列间距
% \renewcommand{\arraystretch}{1.5} % 加大行间距
\begin{tabular}{l|l|ccccc}
\toprule
 \textbf{Dataset Source} &\textbf{Video Source} & \textbf{Type} & \textbf{\# Videos}  & \textbf{Res.} & \textbf{FPS} & \textbf{Length} \\
\midrule
PANDA-70M~\cite{chen2024panda70mcaptioning70mvideos} & Youtube & Real & 200 & - & - & 1$\sim$10s \\
\midrule
VidProM~\cite{wang2024vidprommillionscalerealpromptgallery} & Text2Video-Zero~\cite{khachatryan2023text2videozerotexttoimagediffusionmodels} & AI & 200 & 512*512 & 4 & 2s \\
 & VideoCrafter2~\cite{chen2024videocrafter2overcomingdatalimitations} & AI & 200 & 512*320 & 10 & 1s \\
 & ModelScope~\cite{wang2023modelscopetexttovideotechnicalreport} & AI & 200 & 256*256 & 8 & 2s \\
 & Pika~\cite{pika} & AI & 200 & - & 24 & 3s \\

\midrule

Self-Collected & Youtube & Real & 45  & - & 30 & 1$\sim$4s \\
 & SORA~\cite{videoworldsimulators2024} & AI & 45 & - & 30 & 8$\sim$60s \\

\midrule

Self-Generated & OpenSORA~\cite{opensora} & AI & 200  & 1280*720 & 24 & 4s \\
 & Kling~\cite{klingai} & AI & 200 & 1280*720 & 30 & 5s \\
 % & SORA~\cite{videoworldsimulators2024} & AI & 45 & - & 30 & - \\
 & Runway-Gen3~\cite{runway3} & AI & 200 & 1280*768 & 30 & 5$\sim$10s \\

\bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Composition of the \vidfor. We collect high-quality video from multiple sources. For dataset source own-generated, we generate text-to-video samples with generators conditioned on text prompts collected from PANDA-70M~\cite{chen2024panda70mcaptioning70mvideos} by ourselves.}
\label{tab:video_dataset_detail}
\vspace{-2mm}
\end{table}

\begin{table}[t]
\centering
\scriptsize
\setlength{\tabcolsep}{2pt} % 减小列间距
\begin{tabular}{l|l}
\toprule
\textbf{Category}   & \textbf{Explicit Knowledge (EK) Toolkits}                     \\ \midrule
\textbf{Appearance} & Saturation, Denoised,  Sharpen, Enhance, Segmentation Map \\
\textbf{Motion}     & Optical flow, Landmark                                        \\
\textbf{Geometry}   & Depth map, Edge                                               \\ \bottomrule
\end{tabular}
\vspace{-2mm}
\caption{Categories of explicit knowledge toolkits. Though all tools are proposed by LVLMs, we list and categorize all explicit knowledge  that we collect from LVLM in the process of initial toolkit preparation into three VR categories.}
\label{tab:ek_category}
\vspace{-2mm}
\end{table}

\subsection{Explicit Knowledge Exploration}
Recent research has shown that explicit knowledge extracted from video samples can help to improve detection on video forensic~\cite{chang2024mattersaigeneratedvideos}. The explicit knowledge is collected from the video representation (VR) decomposed by the video frames. VR can be categorized into three angles~\cite{chang2024mattersdetectingaigeneratedvideos}, including \textit{appearance}, \textit{motion}, and \textit{geometry}. The appearance refers to the visual attribute of the video frame, such as color, lightning, or texture. Motion refers to the temporal or dynamic change in the video frame, such as optical flow. Geometry refers to the object shape structure and spatial information in the video frame, such as 3D depth map. we explore the LVLM's understanding capabilities in three VR angles. Our pipeline leverages LVLM to automatically select a set of explicit knowledge that can benefit the detection performance.
In Table~\ref{tab:ek_category}, we categorize EK toolkits into three VR angles.
In Appendix~\ref{supp:explicit-knowleged-intro}, we provide details of each explicit knowledge.


\subsection{Prompting Approach}
We mainly explore two kinds of prompting approaches, including $\textit{non-structured}$ and $\textit{structured}$ prompting to test LVLM's inherent capabilities in our general detection task and the explainanility of each explicit knowledge in $\text{EK}$ set. 
\begin{itemize}
    \item \textbf{Non-structured prompt}: We directly prompt the LVLM with the message, formatting as the template shown in Fig.~\ref{fig:prompt_exp}, to get the prediction and reasoning. The non-structured prompt provides free-formated text response as default result. 
    \item \textbf{Structured prompt}: Recent works~\cite{,zhong2022proqastructuralpromptbasedpretraining} on pre-trained LVLM indicates that there may be tight connection among QA reasoning tasks, ranging from diverse question types, domains, to answer types. A structurally designed prompt-based input schema can help to model the knowledge commonalities for general detection tasks while keeping knowledge customization on different explicit knowledges.
    We carefully study and explore the reasoning ability of SOTA LVLMs with structured prompting by designing a specific class structure for LVLM's response. It is worth noticing that structured prompting is still new in the LVLM field; not all of the models currently support the structured prompt format as their input. We take GPT-4o from OpenAI as our representative model for the detection with structured prompts~\cite{openaistructured}.
\end{itemize}


\begin{figure}[t]
\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small]
\begin{adjustbox}{minipage=[c][0.36\linewidth]{1\linewidth}}

\foo{role}{System}
\foo{content}{You are an AI video analyzer. Determine if
            the video is AI-generated or not?}
% \texttt{role:} \text{System} \\
% \texttt{content:} \textit{You are an AI video analyzer. Determine if \\
%                    \hspace{2mm} the video is AI-generated or not?}\\
\foo{role}{User}
\foo{content}{Video: $\{$
                        "\textit{text-decription}": "\textit{These 8 images are consecutive frames of a video.}", "\textit{image-url}": [\textit{url}]$\}$}
\vspace{1mm}
\foo{Result}{$\{\text
{Default}\}$ or $\{\textcolor{blue}{\text{Structured Response}}\}$}
\end{adjustbox}
\end{tcolorbox}
\vspace{-2mm}
\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small]
\begin{adjustbox}{minipage=[c][0.20\linewidth]{1\linewidth}}
\begin{lstlisting}[basicstyle=\footnotesize, style=base]
class @Structured_Response@(BaseModel):
    is_ai_generated: bool
    raw_frame_analysis: str
    {tool_name}_analysis: str
    explanation: str
\end{lstlisting}
\end{adjustbox}
\end{tcolorbox}
\vspace{-4mm}
\caption{Prompt example for LVLM}
\label{fig:prompt_exp}
\end{figure}



\section{LVLM-based Agentic Framework for Diffusion-Generaterd Video Detection}
\label{sec:method}
% In this section, we present \lavid, our LVLM-based agentic framework for diffusion-generated video detection.

\subsection{Initial Toolkit Preparation}
In the initial stage, we ask the LVLM to provide a candidate set of potential toolkits by giving some external knowledge as reference tools. For instance, we provide optical flow as our reference tool and ask LVLM to find similar tools that can benefit our detection tasks. In our experiment, we eventually chose nine relevant and capable tools from a candidate set with 30 tools provided by LVLM. Table~\ref{tab:ek_category} shows the nine tools in our EK set. In Appendix~\ref{supp:prompt_detail}, we show the prompt details and all toolkits provided by LVLM. 






\subsection{Explicit Knowledge-Enhanced Detection}

\subsubsection{Model-Specific EK Selection (EK Sel.)}
We observe that different LVLMs show different reasoning abilities in the EK set. For example, GPT-4o has better knowledge on saturation and can offer a more reasonable explanation, compared to other LVLMs such as Gemini~\cite{google2024gemini} or Qwen~\cite{qwen2023qwenvl}. To achieve better detection, in our framewo  rk, we select appropriate tools from EK set for each LVLM based on pre-defined tool selection metrics by giving a set of reference video samples. 
Given tools $t_i \in \text{EK}\{t_1, ..., t_q\}$ and a subset of reference samples $x \in \mathcal{X}$, where $q=9$ is the number of tools, our designed tool-selection metrics $S_{\text{Tool}}$ compute score for each tool $t$ upon model $\mathcal{M}$, considering on both subjective evaluation and weighted accuracy of the model. We describe the score as:
% In the tool selection process, the tool score is evaluated based on three aspects: the confidence score, the model preference, and the F1 score. Specifically, the tool score $S_{\text{Tool}}$ is calculated using the following formula:
\begin{equation}
    S_{\text{Tool}}(t, x) = \alpha \cdot \text{F1}_{\text{weighted}}(t,x) + (1 - \alpha) \cdot S_{\text{MP}}(t) \ \nonumber
\label{eqn:selection_metrics}
\end{equation}

\paragraph{Weighted accuracy:} The $\text{F1}_{\text{weighted}}(\cdot)$ is the confidence-weighted F1 score, reflecting an objective view of the model on the given tool $t$ for samples $x \in \mathcal{X}$. 
Specifically, given $N$ samples, each sample $x_i$ has $y_i \in \{real, AI \}$ as ground truth. 
we process $x_i$ with given tool $t$ and extract the explicit knowledge feature $z_i$. The
model's prediction is $\mathcal{M}(z_i)=\hat{y}_i \in \{real, AI\}$ and confidence score is $c_i \in [0, 1]$. We calculate $\text{F1}_{\text{weighted}}$ with weighted $\text{TP}$, $\text{FP}$, and $\text{FN}$. For instance, the weighted true positive ($\text{TP}$) is denoted as $\sum_{i=1}^N c_i \cdot 1 (y_i=real, \hat{y}_i=real)$, where $1(\cdot)$ is an indicator function.  The confidence-weighted precision P, recall R, and F1 score are then:
\begin{equation}
   \text{P} = \frac{\text{TP}}{\text{TP}+\text{FP}}, \quad \text{R}= \frac{\text{TP}}{\text{TP}+\text{FN}}, \quad  
   \text{F1} =2*\frac{\text{P}*\text{R}}{\text{P}+\text{R}} \ \nonumber
\label{eqn:selection_metrics}
\end{equation}
We choose 25\% of video samples in whole dataset as our reference set $\mathcal{X}$ and sum up the $\text{F1}_{\text{weighted}}(\cdot
)$ score upon all samples $x\in\mathcal{X}$ for given tool $t$ as our subjective score. 

\paragraph{Subjective evaluation:} The model performance score $S_{\text{MP}}(\cdot)$ reflects the subjective view of models on the given tool $t$. A given example message as below is provided for prompting the LVLM to give us $S_{\text{MP}}$ for tool $t$ based on self-assessment.
\vspace{-2mm}

\begin{tcolorbox}[colback=white!5!white,colframe=gray!2!gray,arc=2mm,fontupper=\small]
\begin{adjustbox}{minipage=[c][0.60\linewidth]{1\linewidth}}
- Prompts: "You are given an AI-generated video detection task. Assess the the additional feature: \{\texttt{tool name}\} that could support your determination. \\
- Analysis History: \{\texttt{current fewshot results}\}

Evaluate your own analysis considering these factors:
\vspace{-1mm}
 \begin{tcolorbox}[colback=blue!20!white,,colframe=white!5!white, top=0.5pt,bottom=0.5pt]
\begin{lstlisting}[basicstyle=\footnotesize]
* Alignment with knowledge base
* Interpretability and transparency
* Robustness across scenarios
\end{lstlisting}
\end{tcolorbox}
\vspace{-1mm}
- Scoring: Provide a score from 0 to 10 based on your self-assessment. Higher score indicates an effective feature.
\end{adjustbox}
\end{tcolorbox}

$\alpha$ is a weighting factor that balances the relative importance of the F1 score against other evaluation factors. 
We setup $\alpha$ as 0.5. 

\paragraph{Tool selection by thresholding}
After calculating $S_{\text{Tool}}$ for each tool $t_i \in \text{EK}\{t_1, ..., t_q\}$, we selects tools from EK for model $\mathcal{M}$ based on a baseline threshold. We define the threshold as
\begin{equation}
    S_{\text{Baseline}}(x) = \alpha \cdot \text{F1}_{\text{weighted}}(x) + (1 - \alpha) \cdot S_{\text{MP}}(t=\texttt{"RGB"}), \ \nonumber
\label{eqn:selection_metrics}
\end{equation}
where the F1 score is calculated with raw samples $x \in \mathcal{X}$ and $S_{\text{MP}}$ is calculated by giving $\{$\texttt{tool name}$\}$ as \texttt{"RGB"}.
 The optimal set $\text{EK}^\star$ is composed by $t_i \in \text{EK}\{t_1, ..., t_q\}$ with smaller $S_{\text{Tool}}$, comparing to $S_{\text{Baseline}}$.
\begin{equation}
   t_i \in \text{EK}\{t_1, ..., t_q\} =
   \begin{cases}
      1, & \text{if}\ S_{\text{Tool}(t_i)} \geq S_{\text{Baseline}(t_i)} \\
      0, & \text{otherwise}
    \end{cases}  \ \nonumber
\label{eqn:selection_metrics}
\end{equation}

% The term F1 represents the improvement in the model's F1 score for the task, $S_{\text{Confidence}}$ is the confidence score, indicating the model’s certainty in its performance, and $S_{\text{MP}}$ denotes the model preference score, reflecting the model’s knowledge base for this particular tool. Additionally, a threshold serves as the baseline score to determine tool effectiveness.
% \fs{Thinking about another table contains all tools we have and what are those finally picked for each model, (if each model differs a lot) can be put in experiment section?}


\subsubsection{Online Adaptation (OA) w/ Structural Prompt (SP)}
% To further enhance the detection capabilities of LVLMs, we introduce Online Adaptation (OA) with Structural Prompt (SP), a method that leverages structured prompts in strict mode to iteratively improve the model's performance. This approach builds on the idea that structured prompts provide a consistent framework for reasoning, enabling the model to maintain logical coherence and minimize hallucinations. By using structured prompts, we aim to create a fixed response format that ensures the LVLM produces reliable classifications, which is particularly beneficial for detecting AI-generated videos.
In our OA framework, we adopt a self-rewriting mechanism that allows the LVLM to refine its prompt structure based on the output feedback from each batch of data processed, enabling the structured prompt to adapt in real-time without modifying the original textual prompt.

Specifically, each batch in the adaptation dataset initiates a structured prompt evolution process. Starting from an initial prompt template, the system evaluates the F1 score. If the template underperforms, incremental modifications will be applied to the key fields in the class-structure of prompts, ensuring adjustments focus on broader analytical aspects such as facial feature consistency or temporal anomaly detection. 
This iterative refinement improves the adaptability of the model, particularly in challenging data sets in the real world.
Our approach not only prevents the model from overfitting to specific words or phrases but also mitigates the hallucination issue in non-structured prompts. 
It encourages high-level improvements in classification accuracy rather than focusing on low-level, superficial changes. In Fig.~\ref{fig:combined_analysis_heatmap}, we show the hallucination analysis on non-structured prompt.

% It encourages high-level improvements in classification accuracy rather than focusing on low-level, superficial changes. In Sec.~\ref{sec:abalation} Fig.~\ref{fig:refusal_rate_heatmap}, we show the hallucination analysis on non-structured prompt.

% The online adaptation approach not only improves detection performance but also offers a cost-effective solution by avoiding the need to reprocess the entire dataset after each modification. Instead, the model is adapted in smaller batches, allowing for efficient fine-tuning while still capturing the complex features of AI-generated videos. This method demonstrates significant improvements in challenging scenarios but may yield minimal gains on simpler datasets.

% By utilizing structured prompts with an online learning strategy, we provide a flexible yet robust method for LVLMs to self-optimize. This approach is especially advantageous in dynamic, real-world environments where high-quality AI-generated content continues to evolve, demanding adaptable and scalable detection solutions.

% \subsection{Adaptation strategy in practical setting}
% In real-world scenarios, the synthetic video detection task is 364
% based on the mixed data of high-quality videos, as advance- 365
% ments in video generation technology make it challenging for 366
% humans to distinguish the videos generated by SOTA mod- 367
% els. We notice that our current video detection method still 368
% struggles to identify very high-quality videos. To address 369
% this issue, we utilize Test Time Adaptation(TTA) to improve 370
% our method’s performance in detecting these high-quality 371
% videos in real-world scenarios while keeping its ability to 372
% detect videos with lower quality. 373
% To achieve this, we create a dedicated adaptation dataset 374
% composed of high-quality video samples. This dataset serves 375
% as a reference for the TTA process, allowing the model 376
% to fine-tune its structural output format during adaptation 377
% time[Qingyuan: not sure]. By incorporating TTA, our 378
% method dynamically adapts to the higher quality of these 379
% videos, which enhances the model’s ability to identify the 380
% most subtle cues and artifacts associated with AI generation.
% \yun{leave it to the future discussion}



% \subsection{Dataset Collection}
% [TODO move to 3.2]
% \qy{(0) The process of VidProM dataset filter; (1) The process of text prompts selection; (2) The process of high quality video generation; }

% To better evaluate the performance of our method, we perform video filtering on existing video datasets and collect synthetic videos from state-of-the-art text-to-video (T2V) generative AI models. Specifically, we leverage PANDA as our real video source and VidProM as an existing synthetic video source. \fs{maybe a flow chart of data pipeline here?}

% \subsection*{Topic and Embedding Similarity Filtering} \fs{talk about how we filter data to People Topic (and why), use `all-MiniLM-L6-v2` to calculate similarity between prompt and real caption, use `facebook/bart-large-mnli` for zero-shot classification on prompt (or real-caption?) (topic filtering, people and other), v-bench, diagram }
