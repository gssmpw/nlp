\section{Related Works}
\label{sec:relatedworks}

% \textbf{WORKSHOP PARAGRAPH}
% $\bullet$ \textbf{Diffusion-generated Video}
% Diffusion-based video generation represents a leap forward from static image generation, addressing the complexities of temporal coherence, motion dynamics, and environment consistency. Tools like SORA by OpenAI~\cite{videoworldsimulators2024}, Stable Video Diffusion~\cite{blattmann2023stable}, MidJourney~\cite{midjourney}, RunwayML~\cite{runway}, Show-1~\cite{zhang2023show}, Pika~\cite{pika} and Deep Dream Generator~\cite{deepdream} enable users to generate videos with impressive visual and narrative quality. These tools illustrate the breadth of development in AI-driven video content creation, showcasing a range of capabilities from enhancing video quality to generating entirely new content. Each offers unique features tailored to specific creative or technical needs, reflecting the rapid evolution and growing accessibility of video generation technology.

\paragraph{\textbf{AI-Generated Video Detection}}
The success in high-quality machine-generated videos has heightened concerns about security, personal privacy, and digital integrity, emphasizing the need for a robust and generalizable detector capable of distinguishing videos produced by generative models.
% A large body of works has studied the detection methods for GAN-based generated images by using traditional deep neural networks. 
Recently, Deepfake video, generated by GAN-based models, can perform face manipulation with high realism~\cite{deepfake_pavel}. Agarwal et al.~\cite{deepfake_pavel} point out the challenges of detecting Deepfake video, where the traditional DNN networks or audio-visual approach based on lipsync inconsistency detection are not robust enough to detect Deepfake. David et al.,~\cite{8639163_deepfake} propose to use convolutional neural network (CNN) + Long short-term memory (LSTM) to build Deepfake video detectors. However, these methods did not account for cross-model transferability to state-of-the-art synthetic videos, especially those generated by diffusion models. Diffusion-based video generators ~\cite{opensora,blattmann2023stablevideodiffusionscaling,klingai,runway3,pika} have capability to produce high-quality synthetic videos indistinguishable by human. VidProM~\cite{wang2024vidprommillionscalerealpromptgallery} and DeMamba~\cite{chen2024demambaaigeneratedvideodetection} address the challenge and create synthetic video datasets containing millions of samples. DIVID~\cite{liu2024turnsimrealrobust} further refined the diffusion reconstruction error (DIRE~\cite{wang2023dire}) for diffusion-generated video detection, enhancing detection ability across temporal dimensions. AIGVDet~\cite{bai2024aigeneratedvideodetectionspatiotemporal} propose to use spatio-temporal CNNs to tackle synthetic video detection.  DuB3D~\cite{ji2024distinguishfakevideosunleashing} develope dual-branch 3D transformers to distinguish real and synthetic videos. Despite prior works' inspiring in-domain evaluation results, the robustness and generalizability of existing detectors' performance on unseen sources remain unexplored.


% % DELETE LATER
% Agarwal et al.~\cite{deepfake_pavel} point out the challenges of detecting deepfake video, where the traditional DNN networks or audio-visual approach based on lipsync inconsistency detection are not robust enough to detect Deepfake.
% Rössler et al.~\cite{rössler2019faceforensics} introduced a face forgery detection technique that begins by tracking and extracting facial information from the sample, followed by training a classifier to detect forgeries.
% Marra et al.~\cite{Marra2018} developed multiple CNN-based models for detecting fake images. David et al.,~\cite{8639163_deepfake} proposed to use CNN + LSTM to do the deepfake video detection. However, their approach did not account for cross-model transferability and was found ineffective for generalizing to diffusion-based images~\cite{Corvi_2023_ICASSP, ricker2024detection}.
% Lorenz et al.~\cite{Lorenz_2023_ICCV} modified the widely-used Local Intrinsic Dimensionality (LID) to detect diffusion-based images. Based on the observation that generated images vary less than real images after reconstruction. DIRE~\cite{wang2023dire} proposed to utilize reconstructed error to detect diffusion-generated images and was proved better than traditional RGB-based detectors. DIVID~\cite{liu2024turnsimrealrobust} further refined the diffusion reconstruction error for diffusion-generated video detection, enhancing detection ability across temporal dimensions. 
% SeDID~\cite{ma2023exposing} extended it by ensembling step-wise noise error of corresponding inverse and reverse stages. To facilitate diffusion image detection, benchmarks at image level~\cite{wang2023dire} and fine-grained region level~\cite{wang2023deter} were established for fair comparison. Despite the prior works, the robustness of detectors on diffusion-generated video remains unexplored.

\paragraph{\textbf{Video Detection With LVLM}}
Large Vision-Language Models (LVLMs) have emerged as a powerful framework for integrating visual and textual data, enabling models to perform complex multimodal tasks. Early LVLMs, such as CLIP~\cite{radford2021learningtransferablevisualmodels}and ALIGN~\cite{jia2021scalingvisualvisionlanguagerepresentation}, excel at mapping images and text into a shared embedding space, enabling efficient image recognition and captioning tasks. However, these models are limited in their ability to understand temporal information in the video data. To address this, models like Flamingo~\cite{alayrac2022flamingovisuallanguagemodel} and MERLOT~\cite{zellers2021merlotmultimodalneuralscript} have been introduced, significantly advancing LVLM capabilities in video understanding. Additionally, BLIP-2~\cite{li2023blip2bootstrappinglanguageimagepretraining} improve LVLM performance in image understanding by refining multimodal fusion techniques, enhancing the model's ability to comprehend nuanced relationships between visual objects and their linguistic descriptions. These models have paved the way for applying LVLMs to complex multimodal applications such as Video Question Answering and Image Understanding.

\paragraph{\textbf{Mitigation of LVLMs Hallucination}}
Hallucination in Large Vision-Language Models (LVLMs) refers to inconsistencies between visual input and textual output, often stemming from data biases and misalignment between the model's vision and language components. To address this, various improvements have been proposed, such as mitigation for data~\cite{hu2023ciemcontrastiveinstructionevaluation,you2023ferretrefergroundgranularity,you2023ferretrefergroundgranularity}, perceptual enhancement~\cite{jain2023vcoderversatilevisionencoders}, higher-quality annotations ~\cite{gunjal2024detectingpreventinghallucinationslarge}, enhanced alignment training~\cite{stiennon2022learningsummarizehumanfeedback, sun2023aligninglargemultimodalmodels} and aligning with human ~\cite{sun2023aligninglargemultimodalmodels,gunjal2024detectingpreventinghallucinationslarge, yu2024rlhfvtrustworthymllmsbehavior}. More recent developments focus on training-free approaches for hallucination mitigation like OPERA ~\cite{huang2024operaalleviatinghallucinationmultimodal} and VCD ~\cite{leng2023mitigatingobjecthallucinationslarge}. In our work, we choose structure prompts to mitigate the hallucination. While we can perform these methods for better results, we leave this for future work.

% \paragraph{\textbf{Prompt Engineering}}


% \paragraph{\textbf{Test-Time Adaptation}}




