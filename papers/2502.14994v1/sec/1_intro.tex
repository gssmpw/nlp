\section{Introduction}


\begin{figure}[t!]
    \centering
    \includegraphics[width=.999\linewidth]{figs/teaser_lavid_v2.png}
    \caption{An example of AI-generated video from Kling~\cite{klingai} where \lavid makes a correct prediction with the explicit knowledge enhancement. \lavid will facilitate LVLMs for video detection by calling explicit knowledge tools to extract useful information from the original videos and providing structure-formated output.}
    \label{fig:main_flow}
\end{figure}


\label{sec:intro}
The realm of video creation is undergoing a significant transformation with the advent of video generation tools, such as Stable Video Diffusion~\cite{blattmann2023stable}, SORA by OpenAI~\cite{videoworldsimulators2024}, Runway Gen3~\cite{runway3}, Pika~\cite{pika}, and Show-1~\cite{zhang2023show}. These cutting-edge tools are revolutionizing industries from design, marketing, and entertainment to education by creating high-quality video content. The pivotal shift is opening up a myriad of possibilities for creators everywhere, yet poses societal dangers, notably in their widespread use of spreading disinformation, propaganda, scams, and phishing -- evidenced by cases like the Taylor Swift deepfakes~\cite{taylorswift}. The potential threats underscore the importance of detecting video generated by these generative models.

Prior works on generative video detection focus on GAN-generated video. These methods aim to extract artifacts from the samples and train auxiliary deep neural networks as detectors~\cite{chang2024matters}. However, these methods face limitations such as lacking reasoning skills and poor recognition of artifacts unseen in training. Moreover, prior detectors have trouble with samples generated by current diffusion models~\cite{wang2019cnngenerated, patchforeccv20}.
% \qy{Considering current ai-generated video detection methods are mainly based on deep-learning, should we list the limitation of these deep-learning based method? like: current deep learning based methods face limitations such as a lack of transparency, risk for bias, and inability to recognize artifacts not written down}
% However, these detection methods are often lack of reasoning skills and can fail to detect samples generated from advanced video generation tools (e.g., Diffusion model, VLM) 

% The emergence of large vision language models (LVLMs) has significantly revolutionized the field of video generation. Its powerful ability to process and understand visual and textual information provides a new horizon for video detection. The intuitions of adopting LVLM for video detection are twofold. Firstly, the pretraining process includes large corpus as the training data, enabling LVLM to understand real-world context information. Secondly, the strong reasoning skills of LVLM enable the model to execute various tasks such as chain-of-thought mathematical reasoning~\cite{ahn2024largelanguagemodelsmathematical}, puzzle solving~\cite{giadikiaroglou2024puzzlesolvingusingreasoning}, question answering~\cite{kamalloo2023evaluatingopendomainquestionanswering}, and symbolic reasoning~\cite{wei2023chainofthoughtpromptingelicitsreasoning}. Moreover, LVLM shows a strong ability to generate reasoning in a zero-shot manner with chain-of-thought prompting~\cite{kojima2023largelanguagemodelszeroshot}. The literature has studied the use of LLM to perform factual detection by incorporating evidence retrieved from explicit knowledge~\cite{fatahi-bayat-etal-2023-fleek}. Their promising results demonstrate that LVLM can be an advantageous module for detecting real/fake video. 
% \jf{flow should be
% 1. our first idea is to leverage LVLM for video detection. say why LVLMs are good for this task
% 2. however, LVLMs cannot extract key knowledge of the video, so our second idea is to augment with explicit knowedge from tools
% 3. however, feeding all tools to LVLMs confuses them, so our third idea is EK selection
% 4. we observe unstructured prompts bad (why bad), so our fourth idea is to use structured prompts and adapt them online}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\textwidth]{figs/lavid_flow_v2.png}
    \caption{An agentic framework (\textbf{\lavid}) for video detection. The left part shows our main pipeline. First, LVLMs suggest tools relevant to video detection, and based on the model's preferences and the performance improvement each tool provides, we assemble a customized toolkit for each LVLM for video detection. 
    The right part shows the details of the online adaptation for structured prompt. The prompt tuning will be based on the LVLM itself. Component marked with the logo$~\includegraphics[width=3mm, height=2.5mm]{figs/gpt_logo.png}$ are developed with the LVLM like GPT-4o~\cite{openai2024gpt4o}.}
    \label{fig:main_flow}
\end{figure*}

We present a novel approach, \emph{\lavid}, an agentic LVLM framework for diffusion-generated video detection. Our first idea is to leverage LVLM's powerful reasoning ability in both visual and textual information for video understanding.
The intuitions of adopting LVLM for our task are: firstly, the pretraining process includes large corpus as the training data, enabling LVLM to understand real-world context information. Secondly, the strong reasoning skills of LVLM enable the model to execute various tasks such as chain-of-thought mathematical reasoning~\cite{ahn2024largelanguagemodelsmathematical}, puzzle solving~\cite{giadikiaroglou2024puzzlesolvingusingreasoning}, and question answering~\cite{kamalloo2023evaluatingopendomainquestionanswering}. Moreover, literature has studied the use of LVLM to perform factual detection by incorporating evidence retrieved from explicit knowledge~\cite{fatahi-bayat-etal-2023-fleek}. Their promising results demonstrate that LVLM can be an advantageous module for video detection. 




Despite the powerful ability of LVLMs to understand visual and textual information, they still struggle with understanding key knowledge of videos if we directly feed-forward the raw video sample to the LVLM and ask with the prompt \textit{"Tell me if this video is real or AI-generated."}. Our second idea is to extract additional explicit knowledge (EK) from videos (e.g., optical flow, depth map, saturation, etc.) that have beneficial functionality for detection. However, feeding all EK to LVLMs may confuse them in making decisions. Besides, different LVLMs have different comprehension of EK. Therefore, our third idea is to automatically select a useful EK set based on a few reference samples for different LVLM.

One of the other important factors that may affect the detection performance is the prompt format. We observe that a \emph{non-structured prompt} with free-formated output responses can not provide stable detection results. Our fourth idea is to use the \emph{structured prompt}, where the output response format is structurally designed with class structure. Our hypothesis is that structured output could provide LVLMs with a "thinking framework", thereby improving the visual interpretability and reduce the hallucination in non-structured prompt. Moreover, we adopt online adaptation for tuning the key fields in the structured prompts to avoid model overfitting on reference samples.



% Motivated by the intuitions mentioned above, we present a new framework to solve the video detection task. Here, we ask: \textbf{\emph{Can LVLM effectively detect video generated from any sources of video generation tools?}}. The most straightforward and simplest way to do the detection is just feed-forward the raw video sample to the LVLM and send the prompt \textit{"Tell me if this video is real or fake."}. Despite the powerful ability of LVLMs to understand visual and textual information, we found some main challenges while using SOTA LVLMs for detection. (1) The simple and non-structured prompt with raw video sample can not provide enough information for LVLM to do the detection. (2) The detection performance based on non-structured prompting can be unstable. (3) Different LVLMs have different comprehension of explicit knowledge tools. For example, we found GPT-4o is stronger than Gemini and can understand most of the tools. (4) LVLM struggles with understanding the explicit knowledge if we directly provide all explicit knowledge tools to model. \jf{unclear what explicit knowledge here mean.}

% To bridge this gap, we present our novel framework, \lavid,  Detection via Structure Prompt and Explicit Knowledge Enhancement. 
In Fig.~\ref{fig:main_flow}, we describe our schematic flow. Different from traditional deep learning-based methods, which require training detectors with auxiliary features, our detection pipeline includes three main steps: (1) \textbf{$\text{EK}$ Toolkit Selection}: we automatically search and collect a set of explicit knowledge ($\text{EK}$) tools by leveraging LVLM's reasoning capability. We filter a subset of useful tools from the toolkit set based on scoring metrics of LVLM with a given sample set drawn from a video dataset (We separate this set from the whole dataset as a reference set, and the rest of the part is the test set). (2) \textbf{Online Adaptaion with Structured Prompt}: we adaptively self-rewrite the format of structured prompts on the test set based on the feedback output from LVLM. 
% Our insights are that (1) LVLM can suggest a potential set of $\text{EK}$ tools with beneficial functionality for detection (2) Based on the output responses of LVLM, we can design a score metric that can determine if the tools are actually useful for current upcoming video dataset (3) Prior work has shown that the structural prompt based pre-training could mitigate the hallucination~\cite{zhong2022proqastructuralpromptbasedpretraining}. For video detection, we observe that LVLM requires structured prompt format to give us better reasoning and responses.

We highlight our main contribution: 
\begin{itemize}
    \item  We present a novel framework that enables LVLM to perform diffusion-generated video detection tasks precisely through an automated, training-free approach, which includes: (1) automatic toolkit proposal and preparation (2) feedback-based toolkit optimization (3) online adaptation with structured prompts
    % \item Our pipeline is training-free and includes three important components: (1) toolkit preparation, (2) toolkit selection, and (3) online structure prompt tuning.
    \item We discover that by using our designed tool selection score metric, the LVLM can effectively select the useful tools for detection. Besides, the structured prompt can largely reduce the hallucination problem during the detection. Our online adaptation process can real-time adjust the format of structured prompts based on upcoming testset.
    \item In addition to our proposed framework, we create a new benchmark \vidfor with 1.4k+ high quality fake videos, generated from multiple sources of video generation tools, such as Kling~\cite{klingai}, Runway Gen3~\cite{runway3}, and OpenSORA~\cite{opensora}. 
    % 800 are selected from the VidProM dataset, 600 are generated by the current sota models, 45 are from the SORA. 245 high quality real videos from youtube. One advantage of our generated videos is that we use the most expensive sota commercial generation models(kling/Gen3) which are not exist in other dataset}fake video samples generated from \yun{Qinyuan todo}. 
    \item Evaluation results show that \lavid improves F1 scores by 9.4\% to 25.9\% over the top baselines on high-quality datasets across three state-of-the-art LVLMs: Qwen-VL-Max~\cite{qwen2023qwenvl}, Gemini-1.5-pro~\cite{google2024gemini}, and GPT-4o~\cite{openai2024gpt4o}.
\end{itemize}

% \fs{are we actually training-free? TTA might need some "train" data. I think training-free -> we can't use any information from labels. Maybe we can claim TTA (and the use of label) is one-step further, but optional, that we can accomplish based on our base framework, which is training-free}

% \fs{We might want to revise title and Abstract accordingly? Title suggestion (open to change): XXXXX: A dynamic framework improving LVLM on AI-Generated Video Detection via Explicit Knowledge Enhancement}

% \qy{XXXXX: A Dynamic Framework for Enhancing LVLM Performance in AI-Generated Video Detection through Explicit Knowledge Augmentation}

% \qy{XXXXX: Empowering LVLMs for AI-Generated Video Detection with Enhanced Knowledge Precision}

% \qy{XXXXX: A Knowledge-Enhanced Approach to Boosting LVLM Accuracy in AI-Generated Video Detection}

% \qy{\\Contribution:\\
% 1. LVLMs-based methods enjoy better generalization performance compared to traditional deep-learning methods\\
% 2. The model can dynamically adjust detection methods based on its own knowledge and the content of the video. Compared with only based on raw video information, our new architecture performs better\\
% 3. Test time adaptation can apply our method to actual scenarios\\
% 4. Except for generative video detection tasks. Our method can be used in more scenarios, such as deepfake detection tasks\\
% 5. We Proposed a High quality generative video benchmark}

% \qy{\\Novelty:\\
% 1. Our framework LVLMs based detection pipeline with explicit knowledge enhancement could welly distinguish generative videos \\
% 2. We leverage test-time adaptation and structural prompt for practical application of our method\\}

% \qy{\\Insight:
% 1. The large model can propose unique unique solution for generating video detection. The proposed detection solution can help you make better judgments. \\
% 2. Not sure how to explain the structure prompt insight, previous work has shown that the structural prompt based pre-training could mitigate the hallucination somehow: ProQA: Structural Prompt-based Pre-training for Unified Question Answering\\}




