\section{Method}
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=\linewidth]{figures/pipeline_v3.pdf}
    \end{center}
    \caption{\textbf{Pipeline overview.}
    Given a boxing scene at the leftmost figure, where the \textcolor{agentblue}{blue agent} is thinking about its next move.
    The reaction policy (\Secref{sec:policy}) follows these steps: first, based on the observations, the history encoder encodes the current state and observations; then, the next latent predictor predicts the upcoming motion latent; and finally, an online motion decoder decodes this motion latent into the actual next pose.
    The same reaction policy can be applied to the \textcolor{agentpink}{pink agent}. 
    Through a streaming process for both agents, our reaction policy enables the continuous generation of two-character motion sequences without length limit (\Secref{sec:twochar}).
    }
    \label{fig:pipeline}
\figtabskip
\end{figure}

Our aim is to develop an intelligent reaction policy capable of generating online motions that dynamically respond to a counterpart’s movements.
We begin by outlining the formulation of the one-step reaction in \Secref{sec:problem}.
Then, we introduce the design of the reaction policy in \Secref{sec:policy}. 
We explain how this policy drives the generation of two-character motions in \Secref{sec:twochar}.
In \Secref{sec:detail}, we explain the training loss and implementation details.



\subsection{Problem Formulation}\label{sec:problem}
We first introduce the problem formulation.
There are two characters, the agent $\theagent$ and the opponent $\theoppo$.
The goal is to generate the future agent's motion $\agent_{\f}$ based on the opponent's movements $\oppo_{i \in [\f-W, \f)}$ and the agent's previous motion $\agent_{i \in [\f-W, \f)}$:
\begin{equation}
    \agent_{\f} = \agentmodel(\oppo_{i \in [\f-W, \f)},~\agent_{i \in [\f-W, \f)}),
\end{equation}
where $\agentmodel$ is the generative reaction policy, and $W$ is the visible past window size.
We use the same root coordinate definition in \cite{2020_localmotionphases,2021_neuralanimationlayering}, as shown in \Figref{fig:pipeline}.
Each agent's motion $\agent_i$ is defined as:
\begin{equation}
\agent_i = \{\rooto^{\theagent} \in \mathbb{R}^2,~\rootd^{\theagent} \in \mathbb{R}^2,~\posep^{\theagent} \in \mathbb{R}^{J \times 3},~\poser^{\theagent} \in \mathbb{R}^{J \times 6},~\posev^{\theagent} \in \mathbb{R}^{J \times 3}\},
\end{equation}
where $\rooto^{\theagent}$ and $\rootd^{\theagent}$ are the horizontal trajectory (excluding the y-axis) positions and directions relative to the $(i-1)$-th frame agent's root coordinate, respectively. 
The $\posep^{\theagent}$, $\poser^{\theagent}$ and $\posev^{\theagent}$ are the positions, 6D rotations and velocities relative to the $i$-th frame agent's root coordinate, respectively.
$J$ is the number of body joints.
Each opponent's motion $\oppo_i$ is defined as:
\begin{equation}
\oppo_i = \{\oposep^{\theagent} \in \mathbb{R}^{J \times 3},~\oposer^{\theagent} \in \mathbb{R}^{J \times 6},~\oposev^{\theagent} \in \mathbb{R}^{J \times 3}\},
\end{equation}
which includes the positions, 6D rotations and velocities relative to the $i$-th frame agent's ($\theagent$'s) root coordinate, respectively.


\subsection{Reaction Policy}\label{sec:policy}
In this section, we present our reaction policy, which predicts motion in the latent space. 
As illustrated in \Figref{fig:pipeline}, the \history{} module is responsible for encoding the observations of the current state. 
Based on this historical information and the opponent's motion, the \diffusion{} module predicts the future motion latent codes. 
Finally, the \decoder{} generates the future motions using the predicted latent and the current agent's state.


\PAR{Latent motion representation.}
Converting raw data to latent space has become a popular approach in generation pipelines \citep{2022_latentdiffusion}, and VQ-VAE has been proven to be effective in learning disentangled representations of human motion data \citep{2023_t2mgpt,2024_motiongpt,2024_t2lm, 2024_sebasvq}.
In this paper, we adopt a similar architecture as in \cite{2023_t2mgpt} to encode the raw motion data into latent sequences.
Given a sequence of agent motion $\agent = \{\agent_i~|~i \in [0, f),~i \in \mathbb{Z} \}$, the VQ-VAE motion encoder encodes it to $\latent = \{\latent_i~|~i \in [0, \lfloor \frac{f}{d} \rfloor), ~i \in \mathbb{Z} \}$ with a downsampling factor $d = 4$, and each latent code $\latent_i$ is quantized through the codebook $\codebook$ to find the most similar element:
\begin{equation}
    \hat{\latent}_i=\underset{\codebook_k \in \codebook}{\text{arg min}} \left\|\latent_i-\codebook_k\right\|_2.
\end{equation}\vskip-10pt

\PAR{History encoder.}
First, we need to encode the past information of both the agent and the opponent.
To represent the agent’s past motion $\agent$, we could directly utilize the VQ-VAE motion encoder to compress them into latent variables $\hat{\latent}$.
The opponent’s historical motion $\oppo$ is also downsampled by a factor of $d = 4$ and then passed through a single-layer MLP, which encodes it into a feature vector with the dimension of $\hat{\latent}$, ensuring consistency for subsequent processing.


\PAR{Next latent predictor.}
We then predict the next motion latent in an auto-regressive manner based on the historical information. 
Our approach employs a transformer-based condition encoder to effectively capture all visible information.
This encoded data is then passed to the diffusion-based motion latent predictor, which generates the next motion latent based on the encoded conditions.

Specifically, we begin by constructing a transformer-based encoder to encode the information accessible to the reaction policy.
As illustrated in \Figref{fig:pipeline}, the transformer's input consists of the motion latent code $\hat{\latent}$ and the opponent's feature obtained from the history encoder.
As a result, each output of the transformer-based encoder $\past_f$ encapsulates the information preceding the $f$-th frame.

Next, we introduce the diffusion process, which predicts the future motion latent codes $\tilde{\latent}_{\f}$ based on $\past_\f$ at the $\f$-th frame.
We use conditional diffusion models \citep{2022_mdm, 2022_dalle2} and $\past_\f$ serves as the conditioning input, as shown in \Figref{fig:pipeline}.
We employ a single-layer MLP as the generative model $\generator$, ensuring that our model can operate in real-time.
The predicted motion latent code $\tilde{\latent}_{\f}$ is then used to generate the future motion through the \decoder{}.

Compared to previous methods that rely on predicting motion tokens' probabilities and supervising GPT models with cross-entropy loss, our approach of predicting motion latent using a diffusion-based model preserves the smooth and continuous nature of motion. This results in fewer cumulative errors and less deviation from the intended motion over time, offering greater stability and accuracy than predicting token probabilities with GPT models.


\PAR{Online motion decoder.}
A remaining problem is to decode the predicted motion latent code $\tilde{\latent}_{\f}$ into the future agent motion $\tilde{\agent}_{i \in [\f, \f +d)}$ online. 
We propose an online motion decoder that takes a few previous frames and two consecutive motion latent codes to generate the next motion frames.
As shown in \Figref{fig:pipeline}, we use a transformer as the \decoder{}.
The inputs to the \decoder{} are the past agent motions $\agent_{i \in[\f-d, \f)}$, root information $\rooti_{i \in[\f-d, \f)}$, the last motion latent code $\tilde{\latent}_{f-1}$ and current motion latent code $\tilde{\latent}_{\f}$ from the \diffusion{}.
Here, each root information $\rooti_i$ is defined as:
\begin{equation}
    \rooti_i = \{\rooto^{\theoppo} \in \mathbb{R}^2, ~\rootd^{\theoppo} \in \mathbb{R}^2,~\rootc \in \mathbb{R}^1 \},
    \label{eq:root}
\end{equation}
which includes the agent's horizontal trajectory (excluding the y-axis) positions and directions relative to the opponent's root coordinate, and the distance between the agent's root and the center of the boxing ring.
The output of the \decoder{} are the future agent motions $\tilde{\agent}_{i \in[\f, \f+d)}$ and the future root information $\tilde{\rooti}_{i \in[\f, \f+d)}$.
In contrast to VQ-VAE decoder \citep{2023_t2mgpt}, which requires the entire sequence of tokens before decoding, our method decodes motion latent into explicit motion sequences in real-time using only a few tokens and historical data, enabling online generation. 


\subsection{Online Two-character Motion Generation} 
\label{sec:twochar}
Our reaction policy, as described in \Secref{sec:policy}, enables the generation of the next motion frame by leveraging both the opponent’s past motion and the agent’s own past motion. 
To implement online two-character interaction generation, we use the same reaction policy $\agentmodel$ for the two characters.

Starting with an initial input of \rebut{$s=4$} frames of poses, both characters use the policy $\agentmodel$ to generate the next $d$ frames by considering their own initial motion and the opponent’s motion. 
These generated motions are then added to a history buffer $\mathcal{H}$ with a maximum size of $W$. 
After this initial phase, both characters continuously update their predictions by incorporating the newly generated frames and the accumulated motion history. 
The interaction generation process operates in a streaming fashion. Motion that exceeds the buffer size $W$ is discarded as outdated information. 
Through this approach, both characters dynamically respond to the other, ensuring coherent and natural interactions while enabling two-character motion generation without a length limit.



\subsection{Training Loss and Implementation Details}\label{sec:detail}
The training process is divided into two stages: (1) pre-training the VQ-VAE model and (2) jointly training the \diffusion{} model and the \decoder{}.
Details about the network architecture are provided in Appendix \red{C}.

\PAR{Stage 1.}
We pre-train the VQ-VAE model following the approach in \cite{2023_t2mgpt,2019_vqvae2} for $40k$ iterations, using motion sequences cropped to 64 frames. 
The batch size is set to 128, with a codebook size = 512, codebook feature dimension = 512, and a downsampling rate of $d = 4$.
The codebook is updated with the exponential moving average (EMA) method \citep{2019_vqvae2}, as a replacement for the codebook loss.
Finally, the loss is defined as:
\begin{equation}
    \mathcal{L}_{\text{vqvae}}=\mathcal{L}_{rec} + \alpha \| sg[\hat{\latent}] - \latent \|_2^2,
\end{equation}
where $\mathcal{L}_{rec}$ is the L2 reconstruction loss, $\| sg[\hat{\latent}] - \latent \|_2^2$ is the commitment loss, the operator $sg$ refers to a stop-gradient operation, and $\alpha = 0.1$.

\PAR{Stage 2.}
Next, we train the \diffusion{} and \decoder{} jointly for $40k$ iterations while keeping the \vqvae{} fixed. 
During training, we applied causal masks (\Figref{fig:pipeline}) to ensure that the model can only access the current and previous inputs, preventing information from leaking into future time steps.
We use ground truth $\hat{\latent}_{f-1}$ instead of the predicted $\tilde{\latent}_{f-1}$ in \Figref{fig:pipeline} during training.
For this phase, motion sequences are cropped to $W = 60$ frames (2 seconds) for training. 
The batch size is set to 32, with time step $T = 1000$, and we employ DDIM \citep{2020_ddim} to sample only 50 steps during inference. 
The loss is defined as:
\begin{equation}
    \mathcal{L} = \mathcal{L}_{\text{diffusion}} + \beta \|\agent - \tilde{\agent}\|_2^2 + \gamma \|\rooti - \tilde{\rooti}\|_2^2,
\end{equation}
where $\beta = 1.0$, $\gamma = 1.0$.
$\mathcal{L}_{\text{diffusion}}$ is defined as:
\begin{equation}
    \mathcal{L}_{\text{diffusion}}=\mathrm{E}_{t \in[1, T], \mathbf{x}_0 \sim q\left(\mathbf{x}_0\right)} \left[\|\mathbf{x}_0 - \generator(\mathbf{x}_t, t, \mathbf{c})\|_2\right],
\end{equation}
where $\mathbf{x}_0 = \hat{\latent}$ is the ground truth next latent, $\generator$ is the generative model, $\mathbf{c} = \past_\f$ is the condition, and $\generator(\mathbf{x}_t, t, \mathbf{c}) = \tilde{\latent}$ is the predicted next latent.
All models are trained using the AdamW optimizer \citep{14_adam} with a learning rate of $0.0001$ on a single Nvidia RTX 4090 GPU. 