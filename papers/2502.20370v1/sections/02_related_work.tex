\section{Related Work}

\PAR{Single-character motion generation.}
In recent years, deep learning methods for motion synthesis have gained increasing attention \citep{2015_erd,2016_dlfcmse,2017_resrnn,2023_case,2024iclr_omnicontrol,2023iccv_hghoi,2024cvpr_textscene}.
Various techniques, including multi-layer perceptron (MLP) \citep{2017_pfnn}, mixture of experts (MoE) \citep{2018_mann}, and recurrent neural networks (RNN) \citep{2020_rmi} are employed to tackle this task. 
Additionally, to generate diverse and natural results, generative models such as conditional variational auto-encoders (cVAE) \citep{2020_motionvae}, generative adversarial networks (GAN) \citep{2022_ganimator}, and normalizing flows \citep{2020_moglow} have shown promise in addressing this challenge. 
Moreover, the success of generative pre-trained transformers (GPT) \citep{2023_t2mgpt} and diffusion models \citep{2022_mdm} further underscores their potential in this area.
Recently, \cite{2024_camdm,2024_amdm} adopt auto-regressive diffusion models to generate single-character motions.
Although our reaction policy generates one character's motion based on the other's, technically making it single-character motion generation, our work focuses on decision independence in two-character interactions.


\PAR{Reactive motion generation.}
Reactive motion generation is a subfield of human motion generation that focuses on generating human motion in response to external agents.
\cite{2022_ganreactive} introduces a semi-supervised GAN system with a part-based LSTM module to model temporal significance.
\cite{2023_interformer} employs a transformer network, enhanced by an interaction distance module using graphs. 
\cite{2024_duolando} proposes a GPT-based model to predict discrete motion tokens, enhanced by an off-policy reinforcement learning strategy. 
\cite{2024_regennet} utilizes a diffusion-based model with an explicit distance-based interaction loss. 
\cite{2024_remos} employs a diffusion-based model with a combined spatio-temporal cross-attention mechanism. 
However, \cite{2024_regennet} and \cite{2024_remos} require the complete motion sequence of the other agent, and cannot generate reactive motions online.
In contrast, our method enables online interaction generation by leveraging auto-regressive diffusion models, as inspired by \cite{2024_mar}.


\PAR{Two-character motion generation.}
While multi-character motion generation focuses on producing motion for groups with social relationships \citep{2023_socialmopred,2023_tbiformer,2023_groupdance,2023_mammos,2024_t2p},
two-character motion generation focuses on generating closer interaction between two characters.
\cite{2020_localmotionphases} proposes the local motion phase for complex, contact-rich interactions.
\cite{2021_neuralanimationlayering} combines a motion generator with task-dependent control modules. 
Both \cite{2020_localmotionphases} and \cite{2021_neuralanimationlayering} require user control signals. 
Physically-based methods \citep{2021_controlsrategies,2023_ncp,2023_maaip} ensure the physical plausibility of character interactions but still struggle to generate natural and diverse motions.
\cite{2019_ntu,2020_chi3d,2023_hi4d,2024_intergen,2024_interx} provide datasets with rich annotations that enable advancements in interaction modeling two-character scenarios.
Recent advances in text-driven two-character motion generation have introduced diffusion models \citep{2022_mdm} to enhance realism and control \citep{2023_roleaware,2024_contactgen,2024_in2in, 2024_digitallife}.
Despite these advancements, these methods jointly generate whole sequences for both characters and ignore the dynamic feedback inherent in real-life two-person interactions.
