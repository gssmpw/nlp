\section{Introduction}
This paper aims to learn a reaction policy from data that can generate two-character interactions in a streaming manner.
Such a policy is essential for applications in robotics, gaming, and virtual reality, where the character needs to interact with other entities in real-time.
Generating reasonable online reactions is quite challenging, considering two key perspectives.
Firstly, the policy should dynamically adjust its own actions based on the counterpart's responses at each time step, which is essential for enabling online applications.
Secondly, downstream applications require the policy to generate natural and physically plausible motions while maintaining consistency and diversity throughout these sequences.


Previously, there were two main settings to generate two-character interaction: (1) generating one's motions based on a complete sequence of the counterpart's motion \citep{2024_duolando,2024_remos,2024_regennet}, and (2) jointly generating two-character motion sequences based on specific conditions, such as textual input \citep{2023_roleaware,2024_contactgen,2024_in2in}.
However, we argue that both settings do not model the process of real-life two-person interactions.
As humans, we dynamically produce the reaction based on the counterpart's actions at each time step, engaging in independent mind rather than sharing a collective consciousness.
To achieve two-character online interaction, it is crucial to develop a reaction policy that can be separately applied to two characters, allowing them to react to each other like real humans.



In this paper, we propose a novel reaction policy, called \shortname{}, for generating two-character interactions, as illustrated in \Figref{fig:pipeline}.
Our core innovation lies in incorporating a diffusion head into an auto-regressive model,  which can respond to the counterpart's motions in a streaming manner while ensuring the naturalness and diversity of the motions.
Specifically, we first encode the observed motions into latent vectors using a motion encoder.
Given the history motion latent of the character and the counterpart's motion, our reaction policy uses an auto-regressive model to predict a conditioning feature vector.
This vector guides a diffusion model to generate the next motion latent, which is then decoded into the next character pose.
As illustrated in \Figref{fig:teaser}, the proposed reaction policy can generate long and natural boxing sequences compared to the baseline method.


We chose boxing as the task for online two-character interaction generation considering its fast pace and frequent shifts between offense and defense. These dynamic interactions make it an ideal scenario for testing and validating our approach.
We conducted experiments to validate the effectiveness of our approach on our self-collected boxing dataset \boxdataset{}.
Our method was evaluated under three settings: (1) reactive motion generation, (2) two-character interaction generation, and (3) long-term two-character interaction generation, where it outperforms the baselines. We show that our method can generate very long motion sequences ($\sim$ 1 minute), just relying on the initial poses of the two characters.
Additionally, we carried out experiments on sparse control motion generation, demonstrating that our method is well-suited for VR online interactive settings. 