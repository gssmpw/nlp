\section{Experiments} 
\subsection{Dataset, Experimental Setting, and Evaluation Metrics}
\PAR{Dataset.}
To evaluate our method, we collect a high-quality dataset, \boxdataset{}, using the OptiTrack Mocap system\footnote{\url{https://optitrack.com/}} equipped with 12 cameras. We invited three boxing enthusiasts to perform various boxing movements, recording multiple sequences of their actions. 
Please refer to Appendix \red{A} for the details of the data collection.
In total, \boxdataset{} consists of 63.4 minutes of motion data (approximately 457k frames) captured at 120 FPS.
For our experiments, we split the dataset into training (80\%) and testing (20\%) subsets, and downsample the original data to 30 FPS for training purposes. To further enrich the dataset, we apply the augmentation by swapping the roles of the agent and the opponent during training.

\PAR{Experimental setting.}
We evaluate our method in three scenarios: reactive motion generation, two-character interaction generation, and long-term two-character interaction generation.
In the reactive motion generation, we use the ground truth for the opponent's motion as input. For the two-character interaction generation, we provide only the initial 4 frames of poses for both characters. In both test scenarios, the motion of individual characters follows the procedure outlined in \Secref{sec:policy}, while the generation of two-character motions adheres to the process described in \Secref{sec:twochar}.

\PAR{Evaluation metrics.}
We evaluate the generated motion sequences using the following metrics:
(1) \textbf{Frechet Inception Distance (FID).} We follow \cite{2023_case} calculating per-frame, per-transition and per-clip FIDs. A lower FID indicates the generated motion is more similar to the real data.
(2) \textbf{Jitter.} We evaluate motion jittery following \cite{2024_gvhmr}. A closer value to the ground truth indicates better motion quality. 
(3) \textbf{Root Orient (RO).}
To assess the long-term consistency of the generated motion, we propose a new metric RO.
It calculates the percentage of frames where the facing direction between the two agents exceeds 45 degrees. This is motivated by the nature of boxing, where the athletes typically face to each other throughout the match.
Lower deviation from the ground truth means the generated motion aligns better with the expected interactive behavior.
(4) \textbf{Foot Sliding (FS).} Foot sliding \citep{2020_motionvae} measures the average sliding distance when the foot is close to the ground ($< 5 cm$). A value closer to the ground truth indicates better motion quality. Minimal foot slide may suggest that the generated motion remains stationary.
In addition, we provide the inference speed in Appendix \red{E} and motion diversity analysis in Appendix \red{F}.

\subsection{Comparison with Baselines} \label{sec:baseline}
\input{tables/reactive.tex}

We compare our method against several baselines, including \blinterformer{} \citep{2023_interformer}, \blcvae{}, \blcamdm{} \citep{2024_camdm}, \blgpt-online{} \citep{2023_t2mgpt}, and \blduolando-offline{} \citep{2024_duolando}. Among these, \blinterformer{} is a deterministic model and lacks the ability to generate diverse results. \blcvae{} is a CVAE-based approach that we construct specifically for comparison. For \blcamdm{}, we modify its input to make it function as a reaction policy. 
\blgpt-online{} refers to decoding each newly generated token immediately into raw motion using a VQ-VAE decoder as soon as the token is produced.
\blduolando-offline \citep{2024_duolando} decodes the tokens after the entire sequence has been generated.
We retrain all these methods on the \boxdataset{} using similar training configurations for a fair comparison. Details can be found in Appendix \red{D}.

\PAR{Reactive motion generation.}
We begin by evaluating our method in the context of generating reactive motion, where the opponent's motion is provided as ground truth. The results, presented in \Tabref{tab:reactive} (left: reactive), show that our method significantly outperforms the baseline across multiple metrics, including per-frame, per-transition, and per-clip FID scores, as well as reducing foot sliding. 
Additionally, the RO of our method closely matches the performance of the offline \blduolando{} generation. Notably, the root prediction in \blduolando{} is relative to the opponent, preventing drift over time.
Among the online prediction methods, \blinterformer{} may generate motions with implausible root position and orientation.
\blgpt{}, which decodes GPT-predicted tokens online, exhibits jitter and discontinuity.
Both \blcvae{} and \blcamdm{} produce motion that is easy to get stuck over time.
We also provide qualitative results in \Figref{fig:reactive} and the supplementary materials.

\PAR{Two-character interaction generation.}
Our method enables the simultaneous generation of motion for both agents. Starting with the first four frames, each agent's subsequent motion is generated by leveraging the interaction between their own and their opponent's past motions. 
In contrast, \blduolando-offline{} cannot generate both agents' motions simultaneously, as it predicts the root position relative to the opponent's at the same frame and requires future information through a looking-ahead mechanism. Moreover, it relies on having all tokens available before decoding, preventing online generation. 
As shown in \Tabref{tab:reactive} (right: two-character), our method significantly outperforms the baseline across all metrics in this more complex scenario.
We also provide qualitative results in \Figref{fig:twoagent} and the supplementary materials.


\input{tables/long.tex}
\begin{figure}[t]
    \begin{center}
        \includegraphics[width=\linewidth]{figures/ro_vis.pdf}
    \end{center}
    \caption{\textbf{Visualization of the face direction relative to time.}
    We compare our method with baselines in two scenarios described in \Secref{sec:baseline}. The x-axis represents the frame number \f, while the y-axis shows the angle between the facing directions of the two characters (in degrees). An angle of $0^{\circ}$ indicates that the two agents are facing each other, whereas $\pm 180^{\circ}$ means they are facing away from each other. The \textcolor{gtgreen}{green lines} represent the ground truth, the \textcolor{oursblue}{blue lines} represent our method, and the \textcolor{baselinered}{red lines} represent the baselines.
    }
    \label{fig:rootorient}
    \figtabskip
\end{figure}

\PAR{Long-term two-character interaction generation.}
We demonstrate our method's ability to generate extended sequences of two-character motion, highlighting its reduced error accumulation and superior motion quality over long durations.
Our method is capable of generating two-character motion for 1800 frames.
The results are shown in \Tabref{tab:long}. The FID of ours is much better than all the baselines.
To show the effect of error accumulation, we plot the face direction relative to time in \Figref{fig:rootorient}. \blgpt-online and \blinterformer{} will quickly generate motions that face away from each other after some time, and \blcamdm{} generates motions that are over-smoothed.
Please refer to the supplementary material for more visualizations.


\subsection{Ablation Study} \label{sec:ablation}
\input{tables/ablation.tex}
As shown in \Tabref{tab:ablation}, we compare our method with five main ablated versions:
(1) \textbf{\usevae.}
To demonstrate the stability of our method with different motion latent encodings, we replace the VQ-VAE with a VAE as the motion encoder. As shown in the table, using VAE as the motion encoder does not significantly affect the results.
(2) \textbf{\wovq.}
To highlight the importance of predicting motion latent codes rather than raw motions, we remove the \vqvae{} and directly predicted the pose sequence. The results show that directly predicting the raw pose sequence significantly degrades the motion quality.
(3) \textbf{\wodiff.}
We replace the diffusion model with GPT to predict the next token probabilities. The results show a decline in motion quality, with noticeably worse FID scores.
(4) \textbf{\wodec.}
To validate the necessity of training a new online motion decoder, we directly apply the VQ-VAE decoder to decode the motion latent codes into motion sequences. 
Using the VQ-VAE decoder at each step results in discontinuous motion, which in turn leads to a worse FID score.
(5) \textbf{\woroot.}
We remove the root sequence $\rooti$ in \Eqref{eq:root} as the input to the \decoder{}. Without $\rooti$, the model can easily predict motions with the wrong root facing direction.

In summary, we demonstrate the importance of different components in our model. More ablation studies and visual results can be found in Appendix \red{G} and the supplementary materials.