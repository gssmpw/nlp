\section{Related Work}
\label{Related_work:}

\textbf{Knowledge Graph Completion (KGC)} has evolved over the past decade and is a key task in the field of KGs. 
Mainstream KGC methods roughly fall into two groups: embedding-based and text-based methods.
Embedding-based methods~\cite{bordes2013translating, lin2015learning, sun2019rotate, balavzevic2019tucker} generate low-dimensional vectors for entities and relations and optimize various loss functions with the goal of $h+r \sim t$ to predict missing triplets. 
Although simple and effective, these methods neglect the extensive textual information in KGs and struggle to handle entities and relations not encountered during training.
On the other hand, text-based methods~\cite{yao2019kg, zhang2020pretrain, wang2022simkgc, liu2022know, wang2022language, yang2024knowledge} utilize the textual descriptions of entities and relations as input to pre-trained language models (PLMs) and introduce contrastive learning to enhance discriminative ability. 
However, these methods lack the inherent structural knowledge of KGs.
Consequently, some efforts~\cite{wang2021structure, chen2023dipping, he2024mocosa, yang2024knowledge, qiu2024joint} combine embedding- and text-based KGC methods, achieving improved performance.

\textbf{Graph Transformers} (GTs) are essentially a special type of GNN~\cite{bronstein2021geometric} and are gaining increasing attention in multiple application fields~\cite{chen2024survey}. In KGC, some studies~\cite{schlichtkrull2018modeling, vashishth2019composition, nathani2019learning, chen2020hitter, wang2022simple, tan2023kracl, galkin2023towards} leverage GNNs to encode structural information in KGs to train embeddings for entities and relations, while initializing them with semantic embeddings via PLMs. Recently, some efforts have explored applying GTs to KG-related tasks, e.g., graph-to-text generation~\cite{schmitt2020modeling, li2024unifying} and relation classification~\cite{plenz-frank-2024-graph}. However, they either train their models from scratch or split entities and relations into multiple tokens to construct complex positional encoding matrices.
For example, GLM~\cite{plenz-frank-2024-graph} is a graph transformer that fuses textual and structural information, enabling sequence PLMs to perform graph inference while maintaining their original ability.

However, GLM restricts the relative distance of individual triplets to between $0$ and $32$, which limits the processing of entities or relations with longer textual information. 
For instance, only $12.5\%$ of triplets in WN18RR (using the T5 tokenizer) fall within this distance range. 
Intuitively, the constraints of integrating textual and structural information also limit the size of processable subgraphs.
In addition, the attention mechanism may exhibit bias towards entities or relations with longer texts in each triplet. 
In this paper, we borrow the positional encoding strategy from GLM but shift our focus towards subgraph structural information while preserving GLM's strengths. We introduce a novel relative distinction matrix to achieve differentiated yet equal treatment of entities and relations in triplets. 
Our work is also the first to apply GT to the link prediction task.



\textbf{KGC with LLMs.} 
LLMs are deemed highly promising in the realm of KGC and have garnered extensive attention~\cite{ren2024survey, yu2024automated, pan2024unifying, si2024selecting, luo2024let, si2025aligning}.
For instance, \cite{yao2023exploring, zhu2024llms, wei2024kicgpt, li2024contextualization, xu2024multi} directly perform KGC via ICL or enhance textual information in KGs to improve text-based methods. However, these methods overlook the inherent structural information of KGs, leaving LLMs unable to perceive structural knowledge. To tackle this, \cite{zhang2024making, liu2024can, yang2024enhancing} integrate structural information with LLMs to boost KGC performance. Recently, MKGL~\cite{guo2024mkgl} enables LLMs to proficiently grasp entities and relations of KGs through three-word language, but how to make LLMs perceive graph information and improve the link prediction task remains an open problem.
Going beyond the aforementioned methods, there are a handful of recent studies~\cite{li2024cosign, xue2024unlock, jiang2024kg} on leveraging LLMs for KGC.

% LLMs have been explored by researchers for KGC tasks due to their powerful emergent capabilities~\cite{luo2024let, si2024selecting}. 


% KGC methods have evolved over the past decade, with the most classic embedding-based methods generating low-dimensional vectors for entities and relations~\cite{bordes2013translating, lin2015learning, sun2019rotate, balavzevic2019tucker}. They optimize various loss functions with the goal of $h+r \sim t$ to predict missing triplets. Soon after, test-based KGC methods emerged~\cite{yao2019kg, zhang2020pretrain, wang2022simkgc, liu2022know, wang2022language, yang2024knowledge}, which used textual descriptions of entities and relations as input and introduced contrastive learning to enhance discriminative ability. 
% Embedding-based methods~\cite{bordes2013translating, lin2015learning, sun2019rotate, balavzevic2019tucker} seek to embed entities and relations in KG into a continuous vector representation space, and train these embeddings using different scoring functions that aim to optimize objective, i.e., $h + r \sim t$. 
% Despite their simplicity and effectiveness, they overlook the vast textual information in KGs and struggle to handle entities and relations not encountered during training.
% Going beyond the said ones, \cite{chen2022knowledge, saxena2022sequence} employ encoder-decoder PLMs such as T5~\cite{raffel2020exploring} to turn KGC into a generative task~\cite{ye2022generative}. 


% For example, Nevertheless, integrating LLMs into link prediction tasks remains at the cutting edge of research. We contend that three key challenges need to be tackled: how LLMs can accurately identify entities and relations in KGs; how LLMs can effectively perceive the structural information of KGs; and how to leverage the abundant and vast knowledge embedded in LLMs to enhance link prediction task.