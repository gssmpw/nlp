\section{Related Work}
\subsection{Curse of Multilinguality}
Existing research has explored both understanding and addressing this issue in MMT, identifying critical factors such as resource imbalances, limited model capacity, and complex interactions between language pairs, particularly for low-resource languages~\cite{arivazhagan2019massively,aharoni-etal-2019-massively,shaham-etal-2023-causes}. Interestingly, studies have shown that while linguistic similarity enhances positive transfer, dissimilar languages can also act as regularizers, improving training stability~\cite{meng-monz-2024-disentangling}. 
To address these challenges, proposed solutions in recent research include language-specific modules (e.g., adapters, sparse experts) to dynamically allocate capacity and reduce interference~\cite{fan2021beyond,zhao2024sparse,xu-etal-2023-condensing}, vocabulary optimization to better support new languages through improved token representations~\cite{han2024adapters}, data sampling techniques to enhance representation for underrepresented languages~\cite{wang-etal-2020-balancing,wang-neubig-2019-target,lin-etal-2019-choosing} and continual learning techniques~\cite{liu-etal-2023-continual}. Notably, techniques, such as language-specific modules, have been integrated into LLM-based MMT systems, resulting in substantial improvements in multilingual performance~\cite{xu2024x}. In this work, we systematically investigate how post-training in LLM-based MMT contributes to the CoM, providing a fine-grained analysis of its impact on linguistic conflicts and synergies.
\subsection{LLMs for Multilingual MT}
Many efforts have been made to adapt LLMs for effective machine translation. A key approach is prompting, which enhances translation performance without additional training~\cite{he-etal-2024-exploring, lu-etal-2024-chain}. Beyond this, growing research focuses on fine-tuning open and smaller LLMs to achieve high translation quality while ensuring efficiency~\cite{xu2024a, yang2023bigtranslate, alves2024tower, aryabumi2024aya}.

\citet{yang2023bigtranslate} propose a training pipeline that integrates monolingual pre-training to improve language modeling and parallel instruction fine-tuning for enhanced translation performance. Similarly, \citet{xu2024a} emphasize the quality over quantity of parallel data, introducing a training recipe: (1) large-scale monolingual pre-training, followed by (2) small-scale, high-quality parallel fine-tuning. Further revisiting the role of parallel data, \citet{guo-etal-2024-novel} highlights its importance in the pre-training stage. Additionally, \citet{xu2024contrastive} underscore the necessity of alignment in post-training, proposing the CPO algorithm. More recently, with the need to scale models across more languages, \citet{xu2024x} introduces language-specific modules combined with group training to mitigate language conflicts. In this work, we focus on the post-training stage, which has been underexplored in previous studies, and propose a direction-aware training approach with model merging to achieve efficient and effective MMT.