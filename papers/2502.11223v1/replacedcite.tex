\section{Related Work}
\subsection{Curse of Multilinguality}
Existing research has explored both understanding and addressing this issue in MMT, identifying critical factors such as resource imbalances, limited model capacity, and complex interactions between language pairs, particularly for low-resource languages____. Interestingly, studies have shown that while linguistic similarity enhances positive transfer, dissimilar languages can also act as regularizers, improving training stability____. 
To address these challenges, proposed solutions in recent research include language-specific modules (e.g., adapters, sparse experts) to dynamically allocate capacity and reduce interference____, vocabulary optimization to better support new languages through improved token representations____, data sampling techniques to enhance representation for underrepresented languages____ and continual learning techniques____. Notably, techniques, such as language-specific modules, have been integrated into LLM-based MMT systems, resulting in substantial improvements in multilingual performance____. In this work, we systematically investigate how post-training in LLM-based MMT contributes to the CoM, providing a fine-grained analysis of its impact on linguistic conflicts and synergies.
\subsection{LLMs for Multilingual MT}
Many efforts have been made to adapt LLMs for effective machine translation. A key approach is prompting, which enhances translation performance without additional training____. Beyond this, growing research focuses on fine-tuning open and smaller LLMs to achieve high translation quality while ensuring efficiency____.

____ propose a training pipeline that integrates monolingual pre-training to improve language modeling and parallel instruction fine-tuning for enhanced translation performance. Similarly, ____ emphasize the quality over quantity of parallel data, introducing a training recipe: (1) large-scale monolingual pre-training, followed by (2) small-scale, high-quality parallel fine-tuning. Further revisiting the role of parallel data, ____ highlights its importance in the pre-training stage. Additionally, ____ underscore the necessity of alignment in post-training, proposing the CPO algorithm. More recently, with the need to scale models across more languages, ____ introduces language-specific modules combined with group training to mitigate language conflicts. In this work, we focus on the post-training stage, which has been underexplored in previous studies, and propose a direction-aware training approach with model merging to achieve efficient and effective MMT.