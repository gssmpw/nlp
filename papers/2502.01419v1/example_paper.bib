%%%%% application 
@inproceedings{belyaeva2023multimodal,
  title={Multimodal llms for health grounded in individual-specific data},
  author={Belyaeva, Anastasiya and Cosentino, Justin and Hormozdiari, Farhad and Eswaran, Krish and Shetty, Shravya and Corrado, Greg and Carroll, Andrew and McLean, Cory Y and Furlotte, Nicholas A},
  booktitle={Workshop on Machine Learning for Multimodal Healthcare Data},
  pages={86--102},
  year={2023},
  organization={Springer}
}

@article{han2023chartllama,
  title={Chartllama: A multimodal llm for chart understanding and generation},
  author={Han, Yucheng and Zhang, Chi and Chen, Xin and Yang, Xu and Wang, Zhibin and Yu, Gang and Fu, Bin and Zhang, Hanwang},
  journal={arXiv preprint arXiv:2311.16483},
  year={2023}
}

@article{hao2024multi,
  title={A Multi-Modal Foundation Model to Assist People with Blindness and Low Vision in Environmental Interaction},
  author={Hao, Yu and Yang, Fan and Huang, Hao and Yuan, Shuaihang and Rangan, Sundeep and Rizzo, John-Ross and Wang, Yao and Fang, Yi},
  journal={Journal of Imaging},
  volume={10},
  number={5},
  pages={103},
  year={2024},
  publisher={MDPI}
}

%%% Object Hallucination
@article{bai2024hallucination,
  title={Hallucination of multimodal large language models: A survey},
  author={Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2404.18930},
  year={2024}
}
@article{liu2024survey,
  title={A survey on hallucination in large vision-language models},
  author={Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
  journal={arXiv preprint arXiv:2402.00253},
  year={2024}
}
@article{cui2023holistic,
  title={Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges},
  author={Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and Wu, Shirley and Zhang, Linjun and Zou, James and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2311.03287},
  year={2023}
}
@article{li2023evaluating,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.10355},
  year={2023}
}
@inproceedings{gunjal2024detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18135--18143},
  year={2024}
}



%%%% Long context Hallucination
@inproceedings{favero2024multi,
  title={Multi-modal hallucination control by visual information grounding},
  author={Favero, Alessandro and Zancato, Luca and Trager, Matthew and Choudhary, Siddharth and Perera, Pramuditha and Achille, Alessandro and Swaminathan, Ashwin and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14303--14312},
  year={2024}
}
@article{zhong2024investigating,
  title={Investigating and mitigating the multimodal hallucination snowballing in large vision-language models},
  author={Zhong, Weihong and Feng, Xiaocheng and Zhao, Liang and Li, Qiming and Huang, Lei and Gu, Yuxuan and Ma, Weitao and Xu, Yuan and Qin, Bing},
  journal={arXiv preprint arXiv:2407.00569},
  year={2024}
}
@article{lee2024toward,
  title={Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage},
  author={Lee, Saehyung and Yoon, Seunghyun and Bui, Trung and Shi, Jing and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2412.15484},
  year={2024}
}
@article{yue2024less,
  title={Less is more: Mitigating multimodal hallucination from an eos decision perspective},
  author={Yue, Zihao and Zhang, Liang and Jin, Qin},
  journal={arXiv preprint arXiv:2402.14545},
  year={2024}
}

%%%% Self-refine 
@article{lee2023volcano,
  title={Volcano: mitigating multimodal hallucination through self-feedback guided revision},
  author={Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2311.07362},
  year={2023}
}
@article{zhou2023analyzing,
  title={Analyzing and mitigating object hallucination in large vision-language models},
  author={Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2310.00754},
  year={2023}
}

%%%% Decoding
@inproceedings{leng2024mitigating,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13872--13882},
  year={2024}
}

@inproceedings{huang2024opera,
  title={Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation},
  author={Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13418--13427},
  year={2024}
}
@article{huo2024self,
  title={Self-introspective decoding: Alleviating hallucinations for large vision-language models},
  author={Huo, Fushuo and Xu, Wenchao and Zhang, Zhong and Wang, Haozhao and Chen, Zhicheng and Zhao, Peilin},
  journal={arXiv preprint arXiv:2408.02032},
  year={2024}
}
    % model bias
@article{woo2024don,
  title={Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models},
  author={Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17820},
  year={2024}
}
    % text dependent 
@article{zhu2024ibd,
  title={Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding},
  author={Zhu, Lanyun and Ji, Deyi and Chen, Tianrun and Xu, Peng and Ye, Jieping and Liu, Jun},
  journal={arXiv preprint arXiv:2402.18476},
  year={2024}
}



%%%% Attention calibration
@article{li2025mitigating,
  title={Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding},
  author={Li, Jiaming and Zhang, Jiacheng and Jie, Zequn and Ma, Lin and Li, Guanbin},
  journal={arXiv preprint arXiv:2501.01926},
  year={2025}
}
@article{xing2024mitigating,
  title={Mitigating object hallucination via concentric causal attention},
  author={Xing, Yun and Li, Yiheng and Laptev, Ivan and Lu, Shijian},
  journal={arXiv preprint arXiv:2410.15926},
  year={2024}
}
@inproceedings{liu2025paying,
  title={Paying more attention to image: A training-free method for alleviating hallucination in lvlms},
  author={Liu, Shi and Zheng, Kecheng and Chen, Wei},
  booktitle={European Conference on Computer Vision},
  pages={125--140},
  year={2025},
  organization={Springer}
}
@article{jiang2024devils,
  title={Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens},
  author={Jiang, Zhangqi and Chen, Junkai and Zhu, Beier and Luo, Tingjin and Shen, Yankun and Yang, Xu},
  journal={arXiv preprint arXiv:2411.16724},
  year={2024}
}
@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

% Attention sink
@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{sun2024massive,
  title={Massive activations in large language models},
  author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
  journal={arXiv preprint arXiv:2402.17762},
  year={2024}
}

@inproceedings{
anonymous2024see,
title={See What You Are Told: Visual Attention Sink in Large Multimodal Models},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=7uDI7w5RQA},
note={under review}
}

@article{zhang2024seeing,
  title={Seeing clearly by layer two: Enhancing attention heads to alleviate hallucination in lvlms},
  author={Zhang, Xiaofeng and Quan, Yihao and Gu, Chaochen and Shen, Chen and Yuan, Xiaosong and Yan, Shaotian and Cheng, Hao and Wu, Kaijie and Ye, Jieping},
  journal={arXiv preprint arXiv:2411.09968},
  year={2024}
}



@article{yu2024unveiling,
  title={Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration},
  author={Yu, Zhongzhi and Wang, Zheng and Fu, Yonggan and Shi, Huihong and Shaikh, Khalid and Lin, Yingyan Celine},
  journal={arXiv preprint arXiv:2406.15765},
  year={2024}
}



@article{gong2024damro,
  title={Damro: Dive into the attention mechanism of lvlm to reduce object hallucination},
  author={Gong, Xuan and Ming, Tianshi and Wang, Xinpeng and Wei, Zhihua},
  journal={arXiv preprint arXiv:2410.04514},
  year={2024}
}

% data quality
@inproceedings{chen2025sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={370--387},
  year={2025},
  organization={Springer}
}

% Recemt MLLMs
@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}
@article{liu2024nvila,
  title={NVILA: Efficient Frontier Visual Language Models},
  author={Liu, Zhijian and Zhu, Ligeng and Shi, Baifeng and Zhang, Zhuoyang and Lou, Yuming and Yang, Shang and Xi, Haocheng and Cao, Shiyi and Gu, Yuxian and Li, Dacheng and others},
  journal={arXiv preprint arXiv:2412.04468},
  year={2024}
}
@inproceedings{lin2024vila,
  title={Vila: On pre-training for visual language models},
  author={Lin, Ji and Yin, Hongxu and Ping, Wei and Molchanov, Pavlo and Shoeybi, Mohammad and Han, Song},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26689--26699},
  year={2024}
}
@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}

%LLM
@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}
@article{peng2023instruction,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

% Baseline
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}
@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}
@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
% attention 
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}
@inproceedings{huang2019attention,
  title={Attention on attention for image captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4634--4643},
  year={2019}
}
@article{basu2024understanding,
  title={Understanding Information Storage and Transfer in Multi-modal Large Language Models},
  author={Basu, Samyadeep and Grayson, Martin and Morrison, Cecily and Nushi, Besmira and Feizi, Soheil and Massiceti, Daniela},
  journal={arXiv preprint arXiv:2406.04236},
  year={2024}
}
@article{osman2023survey,
  title={A survey on attention-based models for image captioning},
  author={Osman, Asmaa AE and Shalaby, Mohamed A Wahby and Soliman, Mona M and Elsayed, Khaled M},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={14},
  number={2},
  year={2023},
  publisher={Science and Information (SAI) Organization Limited}
}

% Metrics, datasets
@article{rohrbach2018object,
  title={Object hallucination in image captioning},
  author={Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
  journal={arXiv preprint arXiv:1809.02156},
  year={2018}
}

@article{chan2023clair,
  title={CLAIR: Evaluating image captions with large language models},
  author={Chan, David and Petryk, Suzanne and Gonzalez, Joseph E and Darrell, Trevor and Canny, John},
  journal={arXiv preprint arXiv:2310.12971},
  year={2023}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}
@article{garg2024imageinwords,
  title={ImageInWords: Unlocking Hyper-Detailed Image Descriptions},
  author={Garg, Roopal and Burns, Andrea and Ayan, Burcu Karagol and Bitton, Yonatan and Montgomery, Ceslee and Onoe, Yasumasa and Bunner, Andrew and Krishna, Ranjay and Baldridge, Jason and Soricut, Radu},
  journal={arXiv preprint arXiv:2405.02793},
  year={2024}
}

@inproceedings{onoe2025docci,
  title={Docci: Descriptions of connected and contrasting images},
  author={Onoe, Yasumasa and Rane, Sunayana and Berger, Zachary and Bitton, Yonatan and Cho, Jaemin and Garg, Roopal and Ku, Alexander and Parekh, Zarana and Pont-Tuset, Jordi and Tanzer, Garrett and others},
  booktitle={European Conference on Computer Vision},
  pages={291--309},
  year={2025},
  organization={Springer}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

%wasserstein
@article{vallender1974calculation,
  title={Calculation of the Wasserstein distance between probability distributions on the line},
  author={Vallender, SS},
  journal={Theory of Probability \& Its Applications},
  volume={18},
  number={4},
  pages={784--786},
  year={1974},
  publisher={SIAM}
}

@inproceedings{shen2018wasserstein,
  title={Wasserstein distance guided representation learning for domain adaptation},
  author={Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{wan2023efficient,
  title={Efficient large language models: A survey},
  author={Wan, Zhongwei and Wang, Xin and Liu, Che and Alam, Samiul and Zheng, Yu and Liu, Jiachen and Qu, Zhongnan and Yan, Shen and Zhu, Yi and Zhang, Quanlu and others},
  journal={arXiv preprint arXiv:2312.03863},
  year={2023}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}