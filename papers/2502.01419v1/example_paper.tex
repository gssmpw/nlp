%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{authblk}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage[anon]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{kotex}
\usepackage{xspace}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\lsh}[1]{\textcolor{orange}{#1\xspace}}
\newcommand{\ej}[1]{\textcolor{magenta}{#1\xspace}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

\icmltitlerunning{Visual Attention Never Fades: Selective Progressive Attention ReCalibration for Detailed Image Captioning in MLLMs}

\begin{document}

% \twocolumn[
% \icmltitle{Visual Attention Never Fades: Selective Progressive Attention ReCalibration\\for Detailed Image Captioning in Multimodal Large Language Models}

% % It is OKAY to include author information, even for blind
% % submissions: the style file will automatically remove it for you
% % unless you've provided the [accepted] option to the icml2025
% % package.

% % List of affiliations: The first argument should be a (short)
% % identifier you will use later to specify author affiliations
% % Academic affiliations should list Department, University, City, Region, Country
% % Industry affiliations should list Company, City, Region, Country

% % You can specify symbols, otherwise they are numbered in order.
% % Ideally, you should not use this facility. Affiliations will be numbered
% % in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Mingi Jung}{snu}
% \icmlauthor{Saehuyng Lee}{snu}
% \icmlauthor{Eunji Kim}{snu}
% \icmlauthor{Sungroh Yoon}{snu}
% %\icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}

% \end{icmlauthorlist}

% \icmlaffiliation{snu}{Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea}

% \icmlcorrespondingauthor{Mingi Jung}{mingi000508@gmail.com}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

% \vskip 0.3in
% ]


\title{\Large Visual Attention Never Fades: Selective Progressive Attention ReCalibration \\ 
\Large for Detailed Image Captioning in Multimodal Large Language Models}

\renewcommand\Authfont{\large} 
\renewcommand\Affilfont{\small} 
\author[1]{Mingi Jung\thanks{\texttt{mingi000508@gmail.com}}}
\author[1]{Saehuyng Lee}
\author[1]{Eunji Kim}
\author[1]{Sungroh Yoon}
\affil[1]{Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea}
\date{}
\maketitle

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract} 


% detailed image captiong은 중요하다. data 생성, 시각장애인 보조 등에 활용 가능. 좋은 caption이란? precision, recall 2가지 측면에서 좋아야 함. in this work 우리는 precision recall 관점에서 mllm potential을 끌어올릴 수 있음을 입증. 우리는 mllm의 visual token 들에 대한 attention에 대해 2가지 현상을 관찰: 1. response 길어질 수록 visual attention 약해지고; 2. attention pattern이 점점 noisy 해진다. 이 두가지 현상을 해결하는 a new method인 ~~를 제안한다. ~는 visual token 대한 attention을 선별적으로 보강한다. 특히 그 방법은 인접한 두 output tokens의 visual attention 차이에 근거함으로써 noise를 줄이고 실제로 중요한(생각해보기) token들을 선택할 수 있다. Our experiments demonstrates mllm의 precision 향상시키기 위한 기존 방식들이 recall은 매우 하락 시킬 수도 있음을 증명한다. 반면 Our proposed method(이름)은 training free임과 동시에 추가적인 cost를 거의 요구하지 않음에도 불구하고 mllm의 precision과 recall을 동시에 향상시킬 수 있음을 보인다.
Detailed image captioning is essential for tasks like data generation and aiding visually impaired individuals. High-quality captions require a balance between precision and recall, which remains challenging for current multimodal large language models (MLLMs).
%Detailed image captioning is crucial for a range of applications, such as data generation and assisting individuals with visual impairments.  High-quality captions require a balance between precision and recall, which remains challenging for current MLLMs.
In this work, we hypothesize that
%MLLMs fall short in precision and recall due to
this limitation stems from weakening and increasingly noisy visual attention as responses lengthen. To address this issue, we propose SPARC (Selective Progressive Attention ReCalibration), a training-free method that enhances the contribution of visual tokens during decoding. SPARC is founded on three key observations: (1) increasing the influence of all visual tokens reduces recall; thus, SPARC selectively amplifies visual tokens; (2) as captions lengthen, visual attention becomes noisier, so SPARC identifies critical visual tokens by leveraging attention differences across time steps; (3) as visual attention gradually weakens, SPARC reinforces it to preserve its influence.
%In this work, we demonstrate that multimodal large language models (MLLMs) can be further optimized to enhance both precision and recall. We identify two important phenomena in MLLMs’ attention to visual tokens: (1) as responses grow longer, the model’s visual attention becomes weaker; and (2) the attention pattern grows increasingly noisy. To address these issues, we propose a novel method, SPARC (Selective Progressive Attention ReCalibration), a novel method that selectively enhances attention to visual tokens. By dynamically adjusting attention based on differences between adjacent output tokens, our approach effectively reduces noise and pinpoints the truly important tokens.
Our experiments, incorporating both automated and human evaluations, demonstrate that existing methods improve the precision of MLLMs at the cost of recall. In contrast, our proposed method enhances both precision and recall with minimal computational overhead.
%Our experiments demonstrate that existing strategies aimed at boosting MLLM precision often lead to a significant drop in recall. In contrast, our proposed method is training-free, requires minimal additional cost, and yet improves both the precision and recall of MLLMs.



\end{abstract}

%% attention before after 
%\begin{figure}[ht]

%% Intro에 preciion recall 정의 무엇인가??
\section{Introduction}
\label{introduction}

% MLLM은 MLLM은 LLM의 powerful한 Laguage capability를 통해 이해한 visual 정보를 textual description으로 생성할 수 있다. 이를 통해 MLLM은 VQA, multimodal reasoning, image captioning과 같은 task를 수행할 수 있다.
Multimodal Large Language Models (MLLMs) have recently gained traction as a transformative approach in artificial intelligence by integrating visual and linguistic modalities~\cite{li2023blip,liu2024visual,lin2024vila}. These models leverage the powerful language capabilities of Large Language Models (LLMs) to generate textual descriptions from visual inputs~\cite{bai2023qwen, touvron2023llama, abdin2024phi, peng2023instruction}. This capability enables MLLMs to effectively perform a wide range of tasks, including Visual Question Answering (VQA), multimodal reasoning, and image captioning~\cite{chen2024internvl,wang2024qwen2, li2024llava,liu2024nvila}.

\begin{figure}[t]
\vskip 0.1in
\begin{center}

\centerline{
        \includegraphics[width=\columnwidth]{pr_increase_v2.pdf}
}
\vspace{-1em}
\caption{
%Percentage change in precision, recall, and F1-score compared to baseline. The baseline is set at 0\%, with positive values indicating improvement and negative values showing a decline.
Percentage change in precision, recall, and F1-score compared to the results before applying each method.
}
\label{pr_increase}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure}[h]
% \vskip 0.2in
\begin{center}
% \centerline{   
%     \includegraphics[trim=30 310 200 70, clip, width=\columnwidth]{intro_figure_v2.pdf}
% }
\includegraphics[width=\columnwidth]{intro_fugure_v3.pdf}
\vspace{-2em}

\caption{Visualization of image token attention at different context lengths. As the generation context length increases, image attention diminishes, reducing reliance on visual inputs. Our approach mitigates this by  preserving image attention, reducing hallucinations and enabling more detailed captions.}
\label{fig:intro}
\end{center}
% \vskip -0.2in
\end{figure}




% detailed image captiong은 중요하다. data 생성, 시각장애인 보조 등에 활용 가능. 좋은 caption이란? precision, recall 2가지 측면에서 좋아야 함.
Among these tasks, detailed image captioning aims to produce comprehensive yet accurate textual descriptions that capture both key elements and subtle nuances of an image. This capability is particularly important in applications such as content creation~\cite{belyaeva2023multimodal, han2023chartllama} or assistive technology for visually impaired individuals~\cite{hao2024multi}. However, a major challenge in current approaches is hallucination—where models introduce incorrect or irrelevant details—compromising the reliability of the generated captions and limiting their practical utility~\cite{bai2024hallucination, liu2024survey, cui2023holistic}.





% In response, 많은 연구들이 hallucination을 줄이기 위한 방법들을 제시. 우리는 기존 방식들이 model의 precision 향상시킴을 확인. 반면에 생성하는 caption의 datail을 희생함을 확인. 우리의 실험은 기존 방식들을 baseline에 대해 2가지 측면으. 1.precision: 생성한 caption의 요소들이 reference caption에 존재하는가? 2. recall: reference caption의 요소들을 생성한 caption이 포함하는가?. 이를 CHAIR benchmark를 통해 언급된 object 단위로 측정. 결과는 precision을 향상시키기 위한 기존 방식들이 recall을 매우 하락시킬 수도 있음을 보임. 이는 precision-recall을 모두 고려한 새로운 방식의 필요성을 나타냄.
%Although many methods have been proposed to reduce hallucinations~\cite{huang2024opera, leng2024mitigating, lee2023volcano, liu2025paying}, we found that they primarily enhance precision at the cost of recall, often resulting in captions that, while more accurate, omit essential details. To assess this trade-off, we compare existing methods against a baseline model~\cite{liu2024improved} using two key metrics: (1) Precision—the proportion of generated elements that appear in reference captions, and (2) Recall—the proportion of reference elements included in generated captions. As shown in \cref{pr_increase}, we evaluate changes in precision and recall at the object level using the CHAIR benchmark~\cite{rohrbach2018object}. The results reveal that precision-boosting methods substantially reduce recall, underscoring the need for a new strategy that balances both— a gap our work aims to address.
Although many methods have been proposed to reduce hallucinations~\cite{huang2024opera, leng2024mitigating, lee2023volcano, liu2025paying}, we demonstrate that existing methods primarily enhance \emph{precision}---the extent to which a caption accurately reflects an image---at the expense of \emph{recall}---the extent to which a caption comprehensively describes the image---often resulting in captions that, while more precise, omit essential details. To highlight this trade-off, we use the CHAIR benchmark~\cite{rohrbach2018object} to assess the effectiveness of existing hallucination mitigation methods in terms of recall and precision. Notably, our analysis reveals a previously overlooked limitation: these methods significantly reduce model recall (\cref{pr_increase}). Our work aims to introduce a new approach that balances precision and recall.

% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.






% In this paper, 우리는 training free, 추가적인 cost를 거의 요구하지 않음에도 mllm의 precision과 recall을 동시에 향상시킬수 있는 new method인 (이름)을 제안. 우리는 mllm의 visual token들에 대한 attention에서 2가지 현상을 관찰: 1. response가 길어질 수록 생성 text token에 대한 visual tokens의 attention이 약해지고; 2.attention pattern이 점점 noisy 해진다. As shown in \cref{fig:intro}, as more text is generated, attention to relevant visual tokens gradually weakens, reducing focus on salient content while increasing sensitivity to irrelevant noise or attention sink ~\cite{xiao2023efficient, sun2024massive, anonymous2024see}. (이름)은 context 길이가 길어짐에 따라 감소하는 visual token에 대한 attention을 선별적으로 보강한다. 특히 그 방법은 인접한 두 output tokens의 visual attention 차이에 근거함으로써 noise를 줄이고 실제로 중요한 token들을 선택할 수 있다. 이는 mllm이 caption을 생성하는 동안 관련 visual 정보에 대한 attention을 유지하여, 높은 quality의 caption을 생성하도록 함.

%In this paper, we propose \textbf{SPARC} (Selective Progressive Attention ReCalibration), a novel training-free method that enhances both the precision and recall of MLLMs with minimal computational overhead. Our approach is motivated by two key observations about MLLMs' visual attention: (1) as the response length increases, the attention to visual tokens for generated text tokens weakens; and (2) the attention pattern becomes progressively noisier. As illustrated in \cref{fig:intro}, when generating longer text, the model's focus on relevant visual tokens gradually diminishes, while becoming more sensitive to irrelevant noise~\cite{xiao2023efficient, sun2024massive, anonymous2024see}. SPARC selectively reinforces attention to visual tokens that declines as the context length grows. Specifically, it leverages attention differences between adjacent output tokens to reduce noise and retain critical visual information. This allows MLLMs to maintain attention to relevant visual information thereby producing high-quality captions.
In this work, we hypothesize that MLLMs do not fully realize their potential in terms of precision and recall. We attribute this limitation to the model’s focus on visual tokens gradually weakening as it generates longer text and its increasing sensitivity to irrelevant noise, as illustrated in \cref{fig:intro}. To address these issues, we propose \textbf{SPARC} (Selective Progressive Attention ReCalibration), a training-free method designed to enhance the contribution of key visual tokens during the decoding process of MLLMs. Specifically, SPARC is built upon the following three principles: (1) naively increasing the influence of all visual tokens reduces recall; therefore, SPARC selectively amplifies the influence of visual tokens; (2) despite noisy visual attention patterns, SPARC identifies key visual tokens by leveraging attention differences across time steps; (3) to compensate for the weakening influence of visual tokens, SPARC accumulates reinforcement effects to sustain their impact

%Our experiments demonstrates mllm의 precision 향상시키기 위한 기존 방식들이 recall은 매우 하락 시킬 수도 있음을 증명한다. 반면 Our proposed method(이름)은 training free임과 동시에 추가적인 cost를 거의 요구하지 않음에도 불구하고 mllm의 precision과 recall을 동시에 향상시킬 수 있음을 보인다.
% 실험, caption quality 향상. 기존 방식은 recall 희생, 우리꺼는 precision recall 다 챙김. human eval..
Our experiments demonstrate that SPARC significantly improves caption quality, outperforming existing methods. Unlike conventional methods that struggle to balance precision and recall, SPARC effectively enhances both. Furthermore, human evaluations validate the superiority of SPARC in producing more precise and comprehensive captions.

% 1. 우리는 response가 길어짐에 따라 visual attention이 약해지고 noisy 해지는걸 발견
% 2. 우리는 이를 해결하는 training-free, 추가적인 computational cost 거의 없는 method 제안
% 3. 우리 방법을 통해 precision, recall 측면에서 모두 개선된 high-quality caption 생성 가능하다는 것을 확인

Our contributions are summarized as follows:
\begin{itemize}
    %\item We identify and analyze the phenomenon where visual attention in MLLMs diminishes and becomes increasingly noisy as response length increases.
    %\item We propose SPARC, a training-free, computationally efficient method that selectively recalibrates visual attention to mitigate these issues.
    %\item Our experiments demonstrate that SPARC enhances both precision and recall, enabling the generation of high-quality image captions.
    \item We empirically show that existing MLLM hallucination mitigation methods overlook recall.
    \item We propose SPARC, a novel attention-based method that improves MLLM image captioning in both precision and recall. We provide empirical evidence supporting the design choices of the proposed method.
    \item Through automated and human evaluations, we show that SPARC, while training-free and computationally efficient, effectively enhances both precision and recall.
\end{itemize}

\section{Related Work}

\subsection{Mitigating Hallucinations in MLLMs}  
MLLMs often generate hallucinations, where text is inconsistent with visual input, and numerous studies have aimed to address this issue through various methods~\cite{li2023evaluating, liu2024survey, gunjal2024detecting}. Decoding-based methods tackle hallucination by penalizing uncertain token generations through techniques like text aggregation analysis~\cite{huang2024opera}, corrupted visual inputs~\cite{leng2024mitigating, gong2024damro}. Self-refinement strategies are also employed to iteratively align generated captions with visual content~\cite{zhou2023analyzing, lee2023volcano}. In addition, research indicates that hallucinations often arise from an over-reliance on textual context while neglecting visual information~\cite{zhu2024ibd, liu2025paying}. To address this imbalance, decoding strategies and attention calibration techniques have been developed to enhance the utilization of relevant visual elements based on their importance~\cite{huo2024self, liu2025paying,li2025mitigating}. Hallucination issues intensify with long-form text generation, as models rely more on text and less on image content~\cite{favero2024multi, lee2024toward, zhong2024investigating}. Methods such as shortening text length~\cite{yue2024less}, segmenting refinement~\cite{lee2024toward}, and leveraging image-only logits and contrastive decoding~\cite{zhong2024investigating,favero2024multi} have been explored. Existing solutions have not effectively addressed the challenge of enhancing context-relevant visual information, as vision-related signals tend to weaken over longer contexts.

\subsection{Visual Attention in MLLMs}

MLLMs utilize transformer-based architectures with attention mechanisms to integrate visual and textual modalities~\cite{vaswani2017attention, basu2024understanding, osman2023survey}. During text generation, attention weights do not always focus on the most relevant visual tokens~\cite{zhang2024seeing, woo2024don, jiang2024devils}. Prior studies have shown that enhancing image attention can help mitigate hallucinations by improving the model's ability to better align with relevant visual content~\cite{li2024inference, xing2024mitigating}. Efforts to achieve this include increasing image attention, adjusting positional embeddings~\cite{xing2024mitigating, li2025mitigating}, and correcting biases that direct attention to irrelevant regions.~\cite{jiang2024devils, gong2024damro, anonymous2024see}.
Several approaches have been proposed, such as boosting overall image attention~\cite{liu2025paying, jiang2024devils} and reweighting key tokens~\cite{xing2024mitigating} to better focus on meaningful visual regions. Our findings show that as the model generates longer text, its focus on key visual tokens weakens, while sensitivity to irrelevant noise increases. Existing methods struggle to retain key visual tokens in such cases, making it difficult to preserve their relevance. In contrast, our approach reinforces key visual tokens during decoding, ensuring their continued importance. This improves caption detail and accuracy with minimal computational cost.

\section{Why More Attention Doesn't Always Help}

\label{subsec:3.2}
% Paradox of Attention Amplification in Image Captioning
% Limitations of Attention Amplification: A Paradoxical Outcome


%% 분석.. 
% 1. % token을 왜 고를까? 안고르면 어떻게 되는데?: naive 하게 했더니 잘 안되는 이유. tokne 수  dynamic하게 된다는거 baseline vs naive를 설명 
% 안중요한게 많이 관여 .. / 상대적으로 attention pattern이 fix? 
% baseline / naive 하게 한거 attention visualize -> 얘가 좀 fix ?
% attention map 움직임 수치로?? pairwise attention 거리 차이 평균 -> dynamics..



% naive method .. image 영향 커지면 pre & recall 도 커져야 할 거 같은데? intro, 뭐 에서 recall떨어지는거 확인.. counterintuitive ..  chair 맞나? 더 정밀히 .. 이런거 제안.. 

%While existing approaches effectively reduce hallucinations during text generation by adjusting attention, it paradoxically leads to a decrease in the number of objects described from the image. Despite efforts to enhance the model's focus, the generated text often fails to capture sufficient visual detail. This raises concerns about the true efficacy of attention amplification, suggesting that simply increasing attention weights does not necessarily improve semantic understanding or descriptive accuracy. Instead, it highlights a disconnect between the intended goal and actual outcomes.

%In this section, we explore this paradox in depth, analyzing why conventional attention amplification can lead to less informative and diverse captions. We identify key factors contributing to these limitations and discuss potential strategies to address them.

Intuitively, enhancing the influence of visual tokens during MLLM decoding could lead to higher-quality captions. A recent study \cite{liu2025paying} has shown that amplifying visual attention can indeed improve MLLMs' precision. However, \cref{pr_increase} reveals that this improvement comes at the cost of a significant decline in recall.
In this section, we explore this issue in depth, analyzing why the naive amplification of visual attention---increasing all attention weights assigned to visual tokens---can lead to captions with lower recall. We identify key factors contributing to these limitations and discuss potential strategies to mitigate them.





% (이름도) naive approach .. 설명은 뒤에 자세히..
%%%%%%%%%%%%%%%%
% Attention amplification은 Attention pattern의 diversity를 낮출 수 있다. 
% 




\subsection{Does More Attention Reduce Diversity?}

% Enhanced Attention: A Path to Less Diversity?
% 우리는 기존의 method를 사용했을 때 생성된 caption에서 언급되는 object의 다양성이 감소하는 것을 확인. 이는 model이 text 생성과정 중에 특정한 부분에 과도하게 집중되고 있을 가능성을 나타냄. 이에 우리는 text 생성 과정 중 model이 맥락에 따라 유동적으로 다양한 부분에 attend하고 있는지 확인하기 위해, 생성 과정 동안의 visual token들의 attention score의 diversity를 evaluate 함. Visual attention diversity는 캡션 생성 과정 내의 각 output token을 생성할 때의 visual attention weights간의 distance를 pairwise로 측정함. 이를 dataset내의 sample들에 대해 평균냄. 캡션 생성 내의 각 step의 visual attention weights간의 distance가 클 수록, 다양한 부분을 유동적으로 모델이 집중하고 있다고 볼 수 있음. visual attention간의 거리는 visual attention을 normalize하고, 이 분포간의 Wasserstein distance를 쟀음. Figure3는 이를 basline model과 naive한 attention enhancement method(equation 1의)를 적용한 결과에 대해 측정한 것을 나타냄. 그 결과 naive하게 visual attention을 강화하였을 때 baseline보다 visual attention diversity가 현저히 감소하였고, 이는 모델이 생성할 때 중요한 visual token의 attention이 유동적으로 올바르게 강화되지 않았다 볼 수 있다. 따라서 우리는 visual attention을 강화할 때, 중요한 부분을 올바르게 골라내야 할 필요가 있다고 생각해 볼 수 있다.

%%%% 아래처럼 수정 전
% Our analysis indicates that the diversity of objects mentioned in captions generated by existing methods tends to decline, suggesting that the model may be overly focused on specific regions during the text generation process. 

% To examine whether the model attends to diverse regions in a contextually adaptive manner, we evaluate the diversity of visual token attention throughout the generation process. Specifically, for each image sample in the dataset, we generate a caption and compute pairwise distance matrix (\( L \times L \)) of visual attention weights across all output token indices, where \(L\) is the sequence length. Finally, we average these 2D distance matrices across all image samples in the dataset.

% A higher pairwise distance indicates that the visual attention has dynamically shifted across different regions. To compute the pairwise distance, we normalize the visual attention weights and compute the Wasserstein distance~\cite{vallender1974calculation, shen2018wasserstein} between their distributions.

% \cref{visual_attention_diversity}. illustrates the visual attention diversity observed during the captioning process, comparing the baseline model with a naive attention enhancement method (\cref{eq:naive}). Notably, applying naive attention enhancement significantly reduces visual attention diversity compared to the baseline, indicating that simply increasing attention strength does not effectively direct focus to the most important visual tokens during generation.

% These findings underscore the need for a more sophisticated approach to enhancing visual attention. Properly identifying and emphasizing the most relevant visual tokens is crucial for producing captions that are both contextually rich and informative.
%%%% 아래처럼 수정 전
%We found that existing methods that simply amplify attention to image tokens reduce recall, suggesting that the diversity of objects mentioned in generated captions declines. This implies that the model may overly focus on specific regions during text generation, limiting its ability to capture a broader range of visual details.
%It is common practice to focus on different regions while generating captions to ensure a high recall and provide a detailed description across various aspects. Therefore, we measured the diversity of attention patterns among visual tokens when the model generates output tokens during the captioning process.

%\lsh{만일 어떠한 사람이 어떠한 image의 자세하고 포괄적인 caption을 생성한다고 하면 그 사람의 시선은 그 이미지의 구석구석을 이동하게 될 것이다. 마찬가지로 우리는 MLLM들이 decoding 동안 이미지 내의 다양한 위치들을 옮겨다니며 attend하는 것이 높은 recall의 caption들로 이어진다는 가정을 하고 그 naive attention amplification이 visual attention dynamics에 끼치는 영향을 분석한다. 구체적으로 우리는 decoding 중에 얻어지는 visual attention pattern들 간의 pairwise distance들을 관찰한다. 만일 어떠한 caption이 생성되는 동안 그 MLLM이 그 이미지 내의 다양한 부분들을 옮겨다니며 attend 하였다면 그 visual attention pattern들 간의 distance가 클 것이다. 우리는 LLaVA-1.5~\cite{liu2024improved}와 a subset of DOCCI~\cite{onoe2025docci}을 사용하여 3,000개의 caption들을 생성한다. 생성 시에 각 caption마다 첫 100 tokens의 normalized visual attention weight들을 저장한다. We then compute a pairwise distance matrix ($100 \times 100$) of visual attention patterns. The distance are calculated using the Wasserstein distance~\cite{vallender1974calculation, shen2018wasserstein}.}
When generating a detailed and comprehensive caption for an image, a person's gaze naturally moves across different regions of the image. Similarly, we hypothesize that MLLMs producing high-recall captions attend to diverse locations within the image during decoding. Based on this hypothesis, we analyze the impact of naive attention amplification on visual attention dynamics.
Specifically, we examine the pairwise distances between visual attention patterns obtained during decoding. If an MLLM shifts its attention across various regions of an image while generating a caption, the distances between its visual attention patterns should be large. To investigate this, we generate 3,000 captions using LLaVA-1.5~\cite{liu2024improved} and a subset of DOCCI~\cite{onoe2025docci}. During generation, we store the normalized visual attention weights for the first 100 tokens of each caption. We then compute a pairwise distance matrix ($100 \times 100$) of visual attention patterns. The distance is calculated using the Wasserstein distance~\cite{vallender1974calculation, shen2018wasserstein}.

%To investigate how naive attention enhancement approach affects the diversity of visual token attention, we analyze changes in visual attention throughout the generation process. Using 3,000 images, we generate captions with a length of 100 tokens and extract the visual attention weights for each generated token. We then compute a pairwise distance matrix ($100 \times 100$) of visual attention weights between tokens, where a higher pairwise distance indicates more dynamic shifts in visual attention across different regions. The distances are calculated using the Wasserstein distance~\cite{vallender1974calculation, shen2018wasserstein} between normalized visual attention weights.


\cref{visual_attention_diversity} illustrates visual attention diversity during captioning, comparing the baseline model with a naive attention enhancement method ~\cite{liu2025paying}. The results show that naive attention enhancement (right) yields lower distance values compared to the baseline (left) , indicating a reduction in visual attention diversity. 
Notably, naive attention enhancement causes the model's visual attention pattern to remain static throughout caption generation. Consequently, we consider this a key factor contributing to recall degradation, as it reduces the diversity of objects mentioned in the captions and limits their descriptiveness.


These findings highlight the need for a carefully designed approach to visual attention enhancement. Properly identifying and emphasizing the most relevant visual tokens is crucial for generating captions that are both contextually rich and informative.


\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{
    \subfigure[]{
        \includegraphics[width=0.48\columnwidth]{attention_diversity_baseline.pdf}
        \label{vad_baseline}
    }
    
    \hfill
    \subfigure[]{
        \includegraphics[width=0.48\columnwidth]{attention_diversity_pai.pdf}
        \label{vad_pai}
    }
}
\vspace{-1em}
\caption{Visual attention diversity comparison between (a) the baseline model and (b) the naive attention enhancement approach. The naive approach reduces visual attention diversity, indicating ineffective adaptation to important visual tokens}
\label{visual_attention_diversity}
\end{center}
% \vskip -0.3in
\end{figure}



%%%%%%%%%%%%%%%%%%
% 2. 그럼 token을 어떻게 고를건데?  noise 저런 pattern (sink 얘기?) 이런애들 영향 줄이기 위해 인접한 애들 차이 -> noisy해지는거 plot (image 끼리만 해서 더 잘 보이게)



% 앞 내용: attention 높은거 기반으로 강화해줘도, context에 맞게 attention이 dynamically 보강되지 않는다. 심지어 diversity 감소.. 

% To further investigate  왜 attention 값이 높은 정도에 따라서 그대로 attention을 강화하는 것이 caption 생성 과정 내의 attention diversity를 줄이는지 확인 하기 위해 caption 생성 과정에 따라 visual token에 대한 attention이 어떻게 변화하는지 관찰하였음. caption에서 생성 길이가 길어짐에따라, 생성 과정에서 visual token에 대한 attention을 plot한 것이 Figure 4임. 이때 attention은 중후반 layer중 하나에서의 것을 head-wise 평균 낸 것을 전체 visual token에 대해 normalize하여 나타냄.

\subsection{Longer Context, More Noise?}
%\paragraph{Longer Contexts Amplify Noisy Attention} 
To investigate why directly amplifying attention based on its magnitude can reduce attention diversity, we analyze how the model’s attention to visual tokens evolves throughout the caption generation process. Specifically, we generate captions for images using LLaVA-1.5. During the generation process, we visualized how the normalized visual attention weights evolved over time, presenting the visual attention patterns alongside the corresponding images.

\cref{attention_plot} provides a visualization that reveals several notable patterns.
In the early stages of caption generation, attention is primarily directed toward image regions corresponding to salient visual elements, thereby ensuring that the visual attention mechanism itself remains aligned with key parts of the image. 
However, as the caption progresses, this alignment weakens. The intensity of attention on relevant regions diminishes, while attention increasingly concentrates on noise or on specific visual tokens that consistently attract high attention weights. These noisy attention patterns are often unrelated to the context of generation and tend to recur at the same visual token positions throughout the generation process. Consequently, naive attention enhancement method that amplify tokens according to their current attention values inadvertently exacerbate this problem: as the caption becomes longer, boosting already high-attention tokens can reinforce irrelevant or noisy regions, ultimately impairing the model’s ability to focus on truly important parts of the image.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
% \centerline{\includegraphics[trim=90 360 200 200, clip, width=\columnwidth]{attention_plot_v3.pdf}}
\includegraphics[width=0.95\columnwidth]{attention_plot_v4.pdf}
\vspace{-1em}
\caption{Visualization of the temporal dynamics of image attention during caption generation. Early in the process, attention is focused on contextually relevant regions, but as the caption grows longer, it increasingly shifts toward noise or consistently high-attention tokens.}\label{attention_plot}
\end{center}
% \vskip -0.3in
\end{figure}


% visual attentiond을 강화하기 위해서는 이러한 noise 영향을 받지 않고, 실제 관련된 부분의 중요한 image token들을 선택하는 방법이 필요하다. 
These findings underscore the need for an adaptive visual attention mechanism that can effectively identify and prioritize contextually meaningful visual tokens while filtering out noise. 
%By dynamically prioritizing relevant features, especially in the later stages of caption generation, this approach helps maintain focus on meaningful visual content.

%%%%%%%%%%%%%%%%%%%



% 3. 고른 애들로 어떻게 할건데? attention 경향 보면 줄어드니까 보강하기 위해서 .. 점진적으로 .. 뒤로갈수록 더 많이 보강해야.. -> image attention 감소하는 그래프
\subsection{Does Longer Context Weaken Visual Focus?}
%\paragraph{Longer Context, Less Visual Focus} 
We analyze the model’s focus on visual information, considering how it changes with caption length from an attention perspective. 
We examine the proportion of visual attention during the caption generation process. Specifically, we generate 3,000 captions using LLaVA-1.5 and a subset of DOCCI. During captioning, we calculate the proportion of attention weights allocated to visual tokens versus text tokens.

\cref{fig:baseline_attention} illustrates the proportion of attention allocated to image tokens and text tokens as a function of caption length. This demonstrates that as the context length increases, the proportion of attention allocated to image tokens decreases sharply, while the attention to text tokens correspondingly increases. Since the number of visual tokens is significantly larger than the number of text tokens, this trend cannot be solely attributed to the increase in the number of text tokens.
Also this finding aligns with previous studies~\cite{favero2024multi, lee2024toward}, which reported that longer captions often lead to increased hallucinations due to a decline in the model’s focus on relevant visual information.




%This diminishing focus on visual information, especially in later stages, reduces the influence of the image and may contribute to hallucination. 이에 대해 caption 생성 과정에서 점진적으로 뒤로 갈수록 output token에 대한 중요한 부분의 visual token attention을 더 많이 보강해주면, 뒤로 갈수록 visual attention 줄어드는 경향을 보완할 수 있을 것이고, caption 생성 과정 내내 model visual informaiton에 일관되게 focus하도록 할 수 있을거라 볼 수 있다.
% 
To address the diminishing focus on visual information, which can weaken the image's influence and potentially lead to hallucination, it is essential to ensure sustained visual attention throughout the caption generation process. A gradual increase in the emphasis on visual tokens, particularly in the later stages, helps counteract this declining trend. By progressively reinforcing the model's visual focus over time, it can better retain and utilize the image context, ultimately enhancing the accuracy and relevance of the generated captions.

The detailed implementation details of the above experiments can be found in \cref{appendix_analysis_1}
.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{
        \includegraphics[width=\columnwidth]{attention_length_v2.pdf}
}
\vspace{-1em}
\caption{Average attention weight trends for text and image tokens as a function of context length during caption generation. As the context length increases, the proportion of attention allocated to image tokens gradually decreases compared to text tokens. This indicates a significant shift in focus from visual to textual elements during the later stages of caption generation.}
\label{fig:baseline_attention}
\end{center}

%\vskip -0.25in
\end{figure}

\section{Method}
\label{sec:method}
%%%
% 앞서 section 3.2에서 mllm이 생성하는 context길이가 길어질 수록 visual token에 대한 attention이 noizy해지고, 그 비중이 감소하는 것을 확인했음. 우리는 이런 현상을 완화하기 위해 training-free, addtional cost거의 없는, Selective Progressive Attention ReCalibration(SPARC)를 소개한다. SPARC는 1)Relative Activation Score를 이용한 Token selection을 통해 실제 맥락과 관련있는 visual token들을 context길이가 늘어남에 따라 증가하는 noise에 robust하게 select할 수 있고, 2) Progressive Attention Re-Calibration을 통해 중요한 visual token들의 attention을 매 생성과정에서 누적적으로 강화함으로써, visual attention이 context 길어짐에 따라 감소하는 것을 완화한다. 

In Section \cref{subsec:3.2}, we observed that as the context length generated by the MLLM increases, the attention to visual tokens becomes noisier and its proportion decreases. To address this issue, we propose Selective Progressive Attention ReCalibration (\textbf{SPARC}), a training-free approach that incurs minimal additional computational cost. SPARC consists of two key mechanisms: 1) \textbf{Token selection using the Relative Activation Score}, which robustly selects visual tokens relevant to the actual context and resists the increasing noise as the context length grows, and 2) \textbf{Selective Progressive Attention Re-Calibration}, which progressively reinforces the attention to important visual tokens during each generation step, thereby alleviating the decline in visual attention as the context length increases.


% 생성 context길이가 길어져도 robust하게 각 생성 과정에서 실제 context와 관련된 실제 visual token들을 선택하는데 이용할 수 있는 relative activation score를 제안. 앞의 section에서 확인한 바로는, long context에서 두드러지는 noise pattern의 경우 생성 context와는 상관없이 step에 관련되지 않고, 매 생성 step따라서 유지가 됨. Relative activation score의 경우 current output token 생성 중의 visual token attention weight와, 이전 step output token 생성 중의 visual token attention weight의 차이를 이용하기 때문에, 이러한 temporal에 irrelevant 한 noise 영향에 robust하게 중요한 정도 측정. 
% 
\subsection{ Preliminaries: Attention Mechanisms in MLLMs}
%\subsection{Enhancing Visual Attention}
MLLMs are designed to process both image and text inputs, generating text outputs in an autoregressive manner~\cite{li2024llava, lin2024vila}. 
When generating the \(i\)-th text token, the models attend to an input sequence of length
\[
N_i \;=\; N_{\text{image}} \;+\; N_{\text{inst}} \;+\; (i-1),
\]
consisting of \(N_{\text{image}}\) image embeddings, \(N_{\text{inst}}\) instruction tokens, and the \(i-1\) tokens generated so far.

At the \(l\)-th layer and \(h\)-th attention head, let the query, key, and value vectors be
% \[
$Q_{i}^{(l,h)}, \; K_{j}^{(l,h)}, \; V_{j}^{(l,h)} \;\in\; \mathbb{R}^d$,
% \]
where \(i\) indicates the current token being generated and \(j\) ranges over all \(N_i\) tokens in the input (\textit{i.e.}, \(j \in \{1, 2, \dots, N_i\}\)). The attention weight \(\alpha_{i,j}^{(l,h)}\), denoting how much the \(i\)-th token attends to the \(j\)-th, is given by
\begin{equation}
\alpha_{i,j}^{(l,h)} \;=\; \text{softmax}_j\bigl(A_{i,j}^{(l,h)}\bigr),
\quad
A_{i,j}^{(l,h)} \;=\;
\frac{\bigl(Q_{i}^{(l,h)}\bigr)^\top \, K_{j}^{(l,h)}}{\sqrt{d}}.
\end{equation}
Here, \( d \) is the scaling factor. Using these attention weights, the output representation \(o_{i}^{(l,h)}\) for the \(i\)-th token at the \(l\)-th layer and \(h\)-th attention head is
\begin{equation}
o_{i}^{(l,h)} \;=\;
\sum_{j=1}^{N_i} \alpha_{i,j}^{(l,h)} \, V_{j}^{(l,h)}.    
\label{eq:attention_output}
\end{equation}
The outputs from all \(H\) attention heads at the \(l\)-th layer are concatenated and projected back to the original dimension.

%% naive하게 attention 올려주는 approches 설명
A key challenge in MLLMs is their limited ability to effectively utilize image information during text generation~\cite{zhu2024ibd, zhong2024investigating}. To address this issue, several methods have been proposed to explicitly enhance attention to image tokens during the text generation process~\cite{jiang2024devils, zhang2024seeing}. 
 %그중에서 naive하게 생각할 수 있는 방법은 
A naive attention enhancement method to boost attention to image tokens is to modify as follows~\cite{liu2025paying}:
 \begin{equation}
A_{i,j}^{(l,h)} \xleftarrow{} A_{i,j}^{(l,h)} + \alpha\cdot |A_{i,j}^{(l,h)}|,
\label{eq:naive}
 \end{equation}
where $\alpha$ is a scaling factor that amplifies the attention given to image tokens.

% 이런 방식들은 attention이 관련된 중요한 visual token들일수록 더 높게 나타나며 이 정도에 따라 attention을 강화해준다는 기저이다.
These approaches leverage the premise that attention mechanisms capture critical visual information, reinforcing attention to important visual tokens in proportion to their significance.




\subsection{Token Selection: Relative Activation Score}
\label{subsec:token_selection}

To effectively identify contextually relevant visual tokens as the generated context expands, we propose a \textbf{Relative activation score}, a dynamic metric that prioritizes tokens with significant relative increases in attention. This score is designed to compare the current attention value with a smoothed historical trend, emphasizing relative changes rather than absolute magnitudes. By normalizing these changes with respect to the smoothed values, our method ensures that meaningful variations remain detectable, even as attention scales evolve.

Instead of relying solely on instantaneous attention values, our approach focuses on temporal variations in attention. By comparing the current attention value against its historical trend, we minimize the influence of static biases or localized noise, effectively highlighting tokens with sharp increases in relevance—even if their raw attention values are not the highest. To stabilize the process, we apply an Exponential Moving Average (EMA), which smooths out transient fluctuations and prevents brief spikes or dips in attention from disproportionately influencing the selection process. As the context length grows, attention values may diminish overall, which can obscure important variations. By dynamically adapting to these scale changes through normalization, our method remains sensitive to significant shifts in attention, regardless of the overall reduction in attention magnitude over time.



Concretely, we employ an Exponential Moving Average (EMA) to track the historical trend of attention weights, allowing us to dynamically compare the current attention weight to its smoothed past values. This enables our method to detect relative increases in attention with high precision, while filtering out transient noise. we first maintain an EMA of the attention weights up to the \( (i-1) \)-th step:
\begin{equation}
\tilde{a}_{i-1,j}^l = \beta \tilde{a}_{i-2,j}^l + (1 - \beta)a_{i-1,j}^l,
\end{equation}
where \( \beta \in [0, 1] \) isis the smoothing factor that determines the relative weighting of past and current attention values. $a_{i-1,j}^l$ represents the attention weights for the \(j\)-th token at the (\(i-1\))-th step, averaged across all attention heads \(h\).

Using this smoothed trend, we define the \textbf{Relative Activation Score} for the \(j\)-th image token as:
\begin{equation}
r_{i,j}^l = \frac{a_{i,j}^l - \tilde{a}_{i-1,j}^l}{\tilde{a}_{i-1,j}^l}, \quad \text{for } j \in \{1, 2, \dots, N_{\text{image}}\}.
\label{eq:rac}
\end{equation}
Here, \( \tilde{a}_{i-1,j}^l \) represents the EMA-smoothed attention weight for token \( j \) at layer \( l \). This scoring mechanism emphasizes tokens with substantial relative increases in attention, effectively prioritizing those that become more salient over time while minimizing the impact of static or irrelevant tokens.

Based on these relative activation scores, we apply a thresholding mechanism to select the most relevant image tokens. The set of selected tokens \( S_i \) is defined as:
\begin{equation}
S_i = \{ j \mid r_{i,j}^l > \tau \},    
\end{equation}
where \( \tau \) is a predefined threshold. Tokens exceeding this threshold are treated as the significant visual elements for generating the \( i \)-th text token. By dynamically adjusting to shifts in attention patterns, this method focuses on visually relevant tokens while mitigating noise and static biases.

% 골랐는데 어떻게 할건데? 길어질수록 감소 -> 누적해서 강화해줘야함. 누적하는 정도를 앞에서 고른거 기반으로 평가. 정도를 재기 위해서 골라진 횟수 count함.. 이거 기반으로 키워주면, 매 step 점진적으로 올려줄 수 있음 -> 생성 context 길이 길어짐에 따라 줄어드는 visual attention, context에 맞게끔 보강해 줄 수 있음

% 앞의 section에서 생성 text 길이가 길어질 수록, 중요한 image 부분의 attention이 점점 줄어드는 것을 확인했음. 우리는 이 현상을 보완하기 위해서 매 생성 step마다, 뒤로 갈수록 관련된 image tokens에 누적적으로 더 많이 attention을 보강해 줄 수 있는 \textbf{Progressive Attention Re-Calibration} 방식을 소개함. 이를 통해 caption 생성 과정 내내, 맥락에 맞는 visual token 에 대한 attention을 유지할 수 있음. 

% Progressive Attention Re-Calibration은 뒤로 갈 수록 attention 더 많이 보강해주기 위해서, 보강해 줄 정도를 누적적으로 계산, 이에 따라서 attention re-calibration... (<- 2step이라고 나눠서 말할까?) 
% layer l=20 에서 고르고, attention calibration은 모든 layer에서 했다는 내용은 Experimental setting 부분에서 언급하면 될 듯. 
\subsection{Selective Progressive Attention Re-Calibration} \label{subsec:progressive_calibration} As text generation progresses, attention to important image regions often diminishes, leading to a misalignment between visual and textual contexts. To address this issue, we propose \textbf{Selective Progressive Attention Re-Calibration (SPARC)}, a mechanism that dynamically reinforces attention on relevant image tokens during decoding. SPARC ensures that contextually significant visual tokens maintain their importance throughout the captioning process, enabling the model to produce richer and more accurate descriptions.

The core idea of SPARC is to compute a cumulative relevance measure for each image token and adjust attention weights dynamically based on this measure. To achieve this, we introduce the \textbf{Selection Count}, which quantifies the evolving importance of each image token during text generation. The Selection Count is formally defined as:
\begin{equation}
c_{i,j} = \sum_{k=1}^{i-1} \mathbf{1}(j \in S_k),
\label{eq:selection_count}
\end{equation}
where \(\mathbf{1}(\cdot)\) is an indicator function that returns 1 if \( j \) is selected at step \( k \), and 0 otherwise. This metric accumulates the number of times each token has been deemed relevant in prior steps, reflecting its sustained importance in the evolving context.

At each text generation step \(i\), we first identify the set of contextually relevant tokens \( S_i \) using our \textbf{Token Selection} method and update the Selection Count $c_{i,j}$ for each token \(j\). Based on the updated count, we recalibrate the attention weights by amplifying those of frequently selected tokens:
\begin{equation}
a_{i,j}^l \leftarrow a_{i,j}^l \cdot \alpha^{c_{i,j}},    
\end{equation}
where \(\alpha > 1\) is a scaling parameter controlling the amplification of tokens with higher cumulative relevance. This exponential scaling prioritizes consistently relevant tokens while maintaining adaptability to new contexts, mitigating the decline in attention weights observed in longer text generation.

To further enhance computational efficiency, we implement this recalibration by directly adjusting the token's value vectors. Specifically, for each \( j \in S_i \), we update:
\begin{equation}
V_{j}^{(l,h)} \leftarrow V_{j}^{(l,h)} \cdot \alpha.    
\end{equation}
This is possible because key-value caching in large language models stores value vectors for each token~\cite{wan2023efficient}. Leveraging this cached information, SPARC efficiently updates value vectors with a simple implementation, ensuring seamless recalibration through weighted sums of value vectors (e.g., \cref{eq:attention_output}).


In summary, SPARC addresses attention decay by progressively reinforcing the significance of contextually relevant image tokens. This method prevents attention sinks and noise while preserving alignment between visual and textual contexts, resulting in more accurate, diverse, and visually grounded captions.


\section{Experiments}
We evaluate captioning quality by comparing 
baseline model performance using existing approaches versus our method. We also assess
models with and without the proposed method, conduct human evaluations against conventional techniques, and analyze the methods introduced in \cref{sec:method}. Qualitative results are provided in \cref{qualitative}.

\subsection{Experimental Setup}
\label{subsec:setup}
\paragraph{Models} We conduct experiments on three widely used multi-modal language models (MLLMs): LLaVA-1.5~\cite{liu2024improved}, LLaVA-Next~\cite{liu2024llavanext}, and Qwen2-VL~\cite{wang2024qwen2}, each with 7B parameters. For each model, we generat captions for given images using the prompt: ``Please describe this image in detail." The maximum token length for caption generation is set to 512 across all models.

\paragraph{Metrics} We use two evaluation metrics in our experiments. The first metric, CLAIR~\cite{chan2023clair}, measures overall caption quality by assessing alignment with reference captions. It determines whether the generated and reference captions effectively describe the same image, with higher scores indicating better quality. CLAIR leverages GPT-4o~\cite{hurst2024gpt} for reliable evaluation of detailed and accurate captions. The second metric, CHAIR~\cite{rohrbach2018object}, assesses hallucination by comparing objects mentioned in generated captions with those in reference captions. It provides precision-recall metrics to evaluate the trade-off between correctly identified and erroneously included objects.

\paragraph{Datasets} For CLAIR evaluation, we use the IIW-400~\cite{garg2024imageinwords} and DOCCI~\cite{onoe2025docci} datasets. IIW-400 consists of 400 image-caption pairs, while DOCCI contains 15K pairs. Both datasets provide highly detailed, hallucination-free captions, making them well-suited for evaluating caption quality. For CHAIR evaluation, we utilize the MS-COCO 2014 validation dataset~\cite{lin2014microsoft}. This dataset includes ground-truth object annotations, which enable the calculation of CHAIR metrics by comparing the objects mentioned in the generated captions with the reference objects.

\paragraph{Implementation Details} To compare our method with existing approaches, we conduct experiments on the baseline model, LLaVA-1.5. The hyperparameters for existing methods are implemented as specified in prior research. For our method, the following parameters are applied across all models: the scaling factor $\alpha$ is set to 1.1, the smoothing factor $\beta$ is set to 0.1, and the selection threshold $\tau$ is adjusted for each model. Specifically, $\tau$ is set to 1.5 for LLaVA-1.5, 4.0 for LLaVA-Next, and 3.0 for Qwen2-VL. 
For token selection, we extract visual attention from layer 20 for LLaVA-1.5 and LLaVA-Next, while layer 18 is used for Qwen2-VL. On the other hand, attention recalibration is applied across all layers' attention.

\subsection{Results}

\begin{table}[t]
\caption{Comparison of CLAIR scores on the IIW-400 and DOCCI datasets for the baseline model with existing methods and our proposed approach. The results highlight the performance improvements achieved by our method.}
\label{CLAIR_method}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcc|cc}
% \toprule
% Method & IIW-400 & & DOCCI \\
% \midrule
% Baseline    & 56.36  & -  & 59.26  & - \\
% OPERA       & 51.02  & \(-5.34\) & 56.81  & \(-2.45\) \\
% VCD         & 52.16  & \(-4.20\) & 55.60  & \(-3.66\) \\
% VOLCANO     & 55.84  & \(-0.52\) & 61.09  & \(+1.83\) \\
% PAI         & 56.86  & \(+0.50\) & 60.09  & \(+0.83\) \\
% \textbf{Ours} & \textbf{61.49} & \textbf{+5.13} & \textbf{62.70} & \textbf{+3.44} \\
% \bottomrule
% \end{tabular}
\begin{tabular}{lcc}
\toprule
Method & IIW-400 & DOCCI \\
\midrule
Baseline    & 56.36  & 59.26 \\
OPERA       & 51.02 \tiny{(-5.34)} & 56.81 \tiny{(-2.45)} \\
VCD         & 52.16 \tiny{(-4.20)} & 55.60  \tiny{(-3.66)} \\
VOLCANO     & 55.84  \tiny{(-0.52)} & 61.09 \tiny{(+1.83)} \\
PAI         & 56.86  \tiny{(+0.50)} & 60.09 \tiny{(+0.83)} \\
\textbf{Ours} & \textbf{61.49} \tiny{(\textbf{+5.13})} & \textbf{62.70} \tiny{(\textbf{+3.44})} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Comparison with Existing Approaches} We compar our method with the existing approaches on LLaVA-1.5, using CLAIR scores on the IIW-400 and DOCCI datasets. \cref{CLAIR_method} summarizes the results, showing that our method achieves the highest scores across both datasets, significantly outperforming prior approaches. This improvement demonstrates that our method enhances caption generation by producing captions that are more detailed and better aligned with reference captions.

%CHAIR metric에서 hallucination 평가하는 방식과 유사하게 (CHAIR는 생성 캡션에서 언급한 object중에서 얼마나 reference caption에 포함 안 된 object인지, object단위로 hallucination을 평가하는 metric인데 자세한 설명은 앞에서 했으니까 안하고) Object 단위로 precision, recall, F1을 측정했다는 내용, Precision, recall은 생성 캡션에서 언급한 object중에서 얼마나 reference caption에 포함 안됐는지, referece object중에 얼마나 생성 caption에서 언급했는지고 F1은 이들로 잼 이걸로 precision-recall trade off 나타냄.. 
\paragraph{Precision-Recall Tradeoff Analysis}
Generating high-quality captions requires a balanced improvement in both precision and recall. In the CHAIR metric, precision measures the proportion of objects in generated captions that do not appear in the reference captions, indicating hallucination. Recall quantifies the proportion of reference objects correctly identified in the generated captions. The F1 score combines these metrics to provide a holistic assessment of the trade-off between precision and recall.

We compar our method against existing approaches in terms of precision, recall, and F1 score using CHAIR, as shown in \cref{CHAIR}. To ensure a robust evaluation, we randomly sample 500 instances and repeated the evaluation five times. OPERA~\cite{huang2024opera} and VCD~\cite{leng2024mitigating}, which rely on decoding-based strategies fail to improve precision or recall. VOCANO~\cite{lee2023volcano}, which incorporates feedback to self-revise its initial response, enhances precision but slightly reduces recall. PAI~\cite{liu2025paying}, which increases attention to the image while incorporating additional decoding techniques, achieves the largest precision gain but suffers the lowest recall, resulting in a lower F1 score than our method. 

In contrast, our method successfully improves both precision and recall, achieving the highest F1 score. Specifically, it increases precision by 3.02\%p, recall by 0.52\%p, and F1 score by 1.68\%p. Notably, our approach is the only one that improves recall compared to the baseline, whereas all other methods sacrifice recall to boost precision.


These results demonstrate that our method minimizes incorrect object inclusions while enhancing the model’s ability to identify relevant objects. This improved balance is crucial for generating captions that are both accurate and comprehensive, setting a new benchmark for high-quality caption generation. The detailed CHAIR metric scores are provided in \cref{appendix_chair}.

\begin{table}[t]
\caption{Performance of various methods on detailed image captioning using the CHAIR benchmark. The table reports precision, recall, and F1-score for objects in generated captions. The best scores are \textbf{bolded}, while the second-best scores are \underline{underlined}.}
\label{CHAIR}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Methods & Precision & Recall & F1 \\
\midrule
Baseline    & 84.70  & \underline{79.46} & 81.99\\
OPERA       & 84.54  & 78.82 & 81.58\\
VCD         & 83.22  & 77.50 & 80.26\\
VOLCANO     & 87.64  & 77.82 & \underline{82.39}\\
PAI         & \textbf{90.64}&72.44&80.52\\
Ours        & \underline{87.72}  &\textbf{79.98}  &\textbf{83.67}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\begin{table}[t]
\caption{Comparison of CLAIR scores on the IIW-400 dataset between the baseline and our method applied to various models. The results demonstrate a consistent performance improvement with our approach.}
\label{CLAIR_model_IIW}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcc|c}
% \toprule
% Model & Baseline & Ours\\
% \midrule
% LLaVA-1.5  & 56.36 & \textbf{61.49} & \textbf{+5.13} \\
% LLaVA-NeXT & 58.86 & \textbf{64.94} & \textbf{+6.08} \\
% Qwen2-VL   & 78.34 & \textbf{79.70} & \textbf{+1.36} \\
% \bottomrule
% \end{tabular}
\begin{tabular}{lcc}
\toprule
Model & Baseline & Ours\\
\midrule
LLaVA-1.5  & 56.36 & \textbf{61.49} \tiny{(+5.13)}\\
LLaVA-NeXT & 58.86 & \textbf{64.94} \tiny{(+6.08)}\\
Qwen2-VL   & 78.34 & \textbf{79.70} \tiny{(+1.36)}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{table}[t]
\caption{Comparison of CLAIR scores on the DOCCI dataset between the baseline and our method applied to various models. The results demonstrate a consistent performance improvement with our approach.}
\label{CLAIR_model_DOCCI}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
% \begin{tabular}{lcc|cr}
% \toprule
% Model & Baseline & Ours \\
% \midrule
% LLaVA-1.5  & 59.26 & \textbf{62.70} & \textbf{+3.44} \\
% LLaVA-NeXT & 62.49 & \textbf{66.99} & \textbf{+4.50} \\
% Qwen2-VL   & 79.22 & \textbf{80.64} & \textbf{+1.42} \\
% \bottomrule
% \end{tabular}
\begin{tabular}{lcc}
\toprule
Model & Baseline & Ours \\
\midrule
LLaVA-1.5  & 59.26 & \textbf{62.70} \tiny{(+3.44)}\\
LLaVA-NeXT & 62.49 & \textbf{66.99} \tiny{(+4.50)} \\
Qwen2-VL   & 79.22 & \textbf{80.64} \tiny{(+1.42)} \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\paragraph{Performance Across Different Models}
To demonstrate the effectiveness of our method across diverse models, we evaluate it on three widely used MLLMs: LLaVA-1.5, LLaVA-Next, and Qwen2-VL. 
\cref{CLAIR_model_IIW,CLAIR_model_DOCCI} present the CLAIR scores for the IIW-400 and DOCCI datasets, respectively. For DOCCI, we randomly select 500 samples for evaluation. Across both datasets, our approach consistently improves caption quality, demonstrating its robustness across diverse model architectures. These findings confirm that our approach not only refines precision and recall but also generalizes well across different datasets and model configurations. The consistent performance gains further underscore the adaptability of our method in aligning generated captions more effectively with reference captions, as measured by the CLAIR metric.




\paragraph{Human Evaluation Results} To further assess the precision-recall tradeoff, we conduct a human evaluation. Specifically, we sample 100 captions generated by the LLaVA-1.5 model on images from the IIW-400 dataset. These captions are evaluated for precision and recall by human annotators, who compare different methods and selected the better one. The results are then aggregated into a winning ratio, showing how often our method is preferred over the baseline and naive attention enhancement approach.

As shown in \cref{human_eval},  our method achieves a higher recall compared to the baseline while also improving precision. Then compared to naive approach, our approach demonstrates superior recall with a slight decrease in precision. These results indicate that our method effectively balances recall and precision, reducing hallucinations while ensuring comprehensive caption generation.

%%%%
% \begin{table}[t]
% \caption{ Human evaluation results showing the winning ratio (\%) of our method compared to the baseline and naive approach in terms of precision and recall.}
% \vskip 0.15in
% \begin{center}
% \label{tab:human_eval}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcc|cc}
% \toprule
%             & Ours & Baseline & Ours & Naive \\
% \midrule
% Precision   & \textbf{53.85} & 46.15   & 46.54 & \textbf{53.46} \\
% Recall      & \textbf{59.28} & 40.72   & \textbf{64.19} & 35.81 \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

%% human eval
\begin{figure}[t]
%\vskip 0.2in
\begin{center}
\centerline{
        \includegraphics[width=\columnwidth]{human_eval_v2.pdf}
}
% \centerline{
%     \subfigure[]{
%         \includegraphics[width=0.47\columnwidth]{human_baseline.pdf}
%         \label{vsBaseline}
%     }
    
%     \subfigure[]{
%         \includegraphics[width=0.47\columnwidth
%         ]{human_naive.pdf}
%         \label{vsNaive}
%     }
% }
\vspace{-1em}
\caption{Human evaluation results showing the winning ratio (\%) of our method compared to (a) the baseline and (b) naive approach in terms of precision and recall.}
\label{human_eval}
\end{center}
%\vskip -0.2in
\end{figure}









\subsection{Analyses}
\label{subsec:an_ab}
%\paragraph{Analyses}
% method 적용한거 영향 분석..
% 1) seleciotn 해서 실제로 noise / sink 별로 안건들였나? 
% 2) context length <-> visual attention 어떻게 변했는지
To analyze the change in visual token attention, we compare it against a naive approach and the baseline model.
Figure 7(a) illustrates how visual attention changes with increasing context length for each method. It shows that SPARC effectively mitigates the decline in visual attention, preserving focus on visual tokens even in longer contexts.

Additionally, we analyze the attention distribution over sink tokens—tokens identified as unrelated or uninformative—during the caption generation process. Following ~\cite{anonymous2024see}, which identifies sink tokens based on hidden state dimensions with exceptionally high values, we compute the ratio of attention scales between sink and non-sink tokens and plotted the results for the three approaches, as shown in \cref{fig:attention_analysis}(b). The naive approach significantly increases the attention to sink tokens, which can detract from meaningful visual token focus. In contrast, SPARC maintains a sink token attention proportion comparable to the baseline, effectively avoiding unintended amplification of irrelevant tokens.

These results highlight the robustness of SPARC in preserving meaningful visual attention dynamics, even in challenging contexts with extended lengths, while avoiding the pitfalls observed in naive attention reinforcement methods. The detailed implementation details of the above experiments can be found in \cref{appendix_analysis_2}.

%% attention before after 
\begin{figure}[t]
\vskip 0.2in
\begin{center}
\centerline{
        \includegraphics[width=\columnwidth]{attn_anal.pdf}
}
% \centerline{
%     \subfigure[]{
%         \includegraphics[width=0.45\columnwidth]{methods_attention_length.pdf}
%         \label{attention_comparision}
%     }
    
%     \hfill
%     \subfigure[]{
%         \includegraphics[width=0.45\columnwidth, trim=0 -8 0 0]{sink_ratio.pdf}
%         \label{sink_ratio}
%     }
% }
\vspace{-0.5em}
\caption{Visual Attention Analysis.
(a) Change in visual attention with increasing context length. SPARC mitigates the decline compared to the baseline. (b) Ratio of attention scales between sink and non-sink visual tokens during captioning. SPARC maintains baseline-level proportions, unlike the naive approach.}
\label{fig:attention_analysis}
\end{center}
\vskip -0.2in
\end{figure}

%\paragraph{Ablations} 
Further ablation studies on the specific parameters or setting choices used in our approach are provided in \cref{sec:parameter_ablation} for a comprehensive analysis of their impact on model performance.



\section{Conclusion}
In this work, we address the challenge of balancing precision and recall in detailed image captioning for multimodal large language models. To mitigate this issue, we propose SPARC, a training-free method that enhances the contribution of visual tokens during decoding. SPARC identifies critical visual tokens by leveraging attention differences across generation steps and progressively reinforces visual attention to counteract its natural decline. Our experimental results, validated through both automated metrics and human evaluations, reveal that conventional methods often improve precision at the cost of recall. In contrast, SPARC effectively enhances both precision and recall with minimal computational overhead, offering a simple yet powerful solution for improving detailed image captioning in MLLMs.

%In conclusion, we propose a training-free, computationally efficient method to counteract the diminishing attention to critical visual tokens in Multimodal Large Language Models (MLLMs) during sequential text generation. By tracking changes in attention weights, our approach preserves focus on important visual regions, resulting in more accurate, comprehensive captions. Our experiments demonstrate its effectiveness and seamless integration with existing MLLMs. %This work not only highlights the role of continuous attention reinforcement in image captioning but also paves the way for extensions to other tasks and adaptive attention mechanisms for broader applicability.

% \section*{Impact Statement}

% %%  
% % Authors are required to include a statement of the potential broader impact of their work, including its ethical aspects and future societal consequences.

% % broader impact: 이 연구는 image captioning 성능 향상에 기여, 우리가 제안한 method인 SPARC는 MLLM의  captioning 성능을 precision과 recall 측면에서 모두 향상시켜, high quality detailed caption을 얻을수 있게 함. AI가 단순히 object를 인식하는 것을 넘어서 의미나, 문맥등의 정보들을 설명할 수 있게 발전함에 따라 생성하는 caption이 길어지고 다뤘던 문제가 더 critical하게 다가올 수 있음. 시각장애인 보조, data나 contents 생성등등
% As AI models evolve beyond simple object recognition to generating captions that incorporate contextual and semantic understanding, the length and complexity of generated descriptions are increasing. This trend makes the challenge of balancing precision and recall in image captioning even more critical. Our research addresses this issue by improving multimodal large language models (MLLMs), which can contribute to advancements in AI for accessibility and multimodal contextual understanding. More accurate and context-aware captions can greatly benefit visually impaired individuals by providing richer and more informative descriptions. Additionally, applications in education, automated documentation, and creative content generation can be enhanced through improved captioning capabilities.

% However, the societal impact of advanced image captioning must be carefully considered. While our method enhances caption quality, AI-generated descriptions can still suffer from hallucinations—incorrect details that seem plausible. As AI-generated captions become more refined and detailed, users may develop a stronger trust in their accuracy, even when the content is incorrect or misleading. This could lead to unintended consequences, such as misinterpretations of visual content or overreliance on AI-generated descriptions without verification. Ensuring that users remain aware of these limitations is crucial as AI-generated content becomes more prevalent in real-world applications.



% references without citing it in the main text, use \nocite

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn



\newpage
\section{Qualitative Results: Enhanced Caption Quality}
\label{qualitative}
\subsection{LLaVA-1.5}
\begin{figure}[H]
%\vskip 0.1in
\begin{center}
\centerline{
\includegraphics[trim=10 99 10 97, clip, width=\textwidth]{llava_qualitative.pdf}
}
\caption{Comparison of captions generated by applying our method to LLaVA-1.5 and the baseline. Red text highlights incorrect references in the captions, while blue text indicates additional details provided by our method compared to the baseline.}
\label{fig:llava_1.5}
\end{center}
%\vskip -0.1in
\end{figure}

\subsection{LLaVA-NeXT}
\begin{figure}[H]
%\vskip 0.1in
\begin{center}
\centerline{
\includegraphics[trim=3 66 3 43, clip, width=\textwidth, height=0.88\textheight]{llava_next_1.pdf}
}
\caption{Comparison of captions generated by applying our method to LLaVA-NeXT and the baseline. Red text highlights incorrect references in the captions, while blue text indicates additional details provided by our method compared to the baseline.}
\label{fig:llava_next}
\end{center}
%\vskip 0.1in
\end{figure}

\begin{figure}[H]
\vskip 0.1in
\begin{center}
\centerline{
\includegraphics[trim=0 65 0 40, clip, width=\textwidth]{llava_next_2.pdf}
}
\caption{Comparison of captions generated by applying our method to LLaVA-NeXT and the baseline. Red text highlights incorrect references in the captions, while blue text indicates additional details provided by our method compared to the baseline.}
\label{fig:llava_next_2}
\end{center}
%\vskip -0.1in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Details for Analyses}
% 본문에서 언급한 실험들의 implementation details를 정리하여 나타냄.
\subsection{Why More Attention Doesn’t Always Mean Better
Descriptions}
\label{appendix_analysis_1}
\paragraph{Enhanced Attention: A Path to Less Diversity?}
% visual attention diversity plot하는거 detail: Baseline model (LLaVA-1.5 7B)로 DOCCI dataset의 image sample 3000장에 대한 Caption 생성과정에서, output tokens을 생성할 때의 visual token들에 대한 attention weights를 이용하였음. visual attention weights의 경우 layer=20 에서 측정하였고, head-wise로 평균 내었음. 각 image sample에 대한 caption 생성과정에서의 output token 중 첫번째부터 L=100 번째 token에 대한 visual attention을 이용하였음. 각 sample에 대해서 output token index간의 visual attention들의 distance를 plot하여 이를 통해 생성 과정에서의 visual attention diversity를 확인했음. attention distance의 경우, 두 visual attention에 대해 각각 모든 token들에 대한 sum이 1이 되게 normalize 한뒤, normalize된 두 visual attention 끼리의 Wasserstein distance를 쟀음. 이 결과를 baseline과 visual attention을 attention weight에 비례하게 강화하는 naive attention 대해 plot 한게 figure 3.임. 
%이는 naive한 attention enhacement 방식은 caption생성 과정 내에서, visual attention pattern의 변화를 줄어들게 한다는 것을 나타내고, 생성된 caption에서 언급하는 object의 다양성을 감소시키는 결과와 일치함. 반면 우리 방법을 적용하였을 때의 visual attention diversity를 plot한 figure12를 보면, 우리 방법은 visual attention diversity를 심각하게 저하시키지 않는다는 것을 확인할 수 있음. 이를 통해 우리 방법을 적용하면, 생성 과정에 따라 context에 적절한 부분의 attention을 dinamically 강화해 줄 수 있다는 것을 확인할 수 있음. 
To analyze the diversity of visual attention during caption generation, we used the baseline model (LLaVA-1.5 7B) to generate captions for 3,000 image samples from the DOCCI dataset. During the caption generation process, we measured the attention weights assigned to visual tokens when generating output tokens. Specifically, we extracted visual attention weights from layer 20 and averaged them across attention heads.

For each image sample, we analyzed the visual attention corresponding to the first 100 output tokens. To quantify the variation in visual attention throughout the generation process, we computed the distance between the visual attention distributions of consecutive output tokens within each sample. The distance was calculated using the Wasserstein distance, where each visual attention distribution was first normalized so that the sum of attention weights across all tokens equaled 1.

The results of this analysis are presented in \cref{visual_attention_diversity}, which compares the baseline model with a naive attention enhancement method that proportionally increases visual attention weights. The plot illustrates that the naive enhancement method reduces the variation in visual attention patterns throughout the caption generation process. This finding aligns with our observations that the naive method decreases the diversity of objects mentioned in the generated captions.

\begin{figure}[H]
\centering
\subfigure[]{
    \includegraphics[width=0.3\columnwidth]{attention_diversity_baseline.pdf}
    \label{ap_vad_baseline}
}
\hfill
\subfigure[]{
    \includegraphics[width=0.3\columnwidth]{attention_diversity_pai.pdf}
    \label{ap_vad_pai}
}
\hfill
\subfigure[]{
    \includegraphics[width=0.3\columnwidth]{attention_diversity_ours.pdf}
    \label{ap_vad_ours}  % 중복된 label 수정
}

\caption{Visual attention diversity comparison between (a) the baseline model, (b) the naive attention enhancement approach, and (c) our method. The naive approach reduces visual attention diversity, indicating ineffective adaptation to important visual tokens. Instead, our proposed method does not severely degrade visual attention diversity}
\label{fig:appendix_attention_diversity}
\end{figure}

In contrast, the results shown in \cref{fig:appendix_attention_diversity} demonstrate that our proposed method does not severely degrade visual attention diversity. Instead, our approach dynamically reinforces attention to relevant regions of an image based on the evolving context during caption generation. This confirms that our method effectively maintains a balance between enhancing visual attention and preserving diversity, leading to more contextually appropriate and varied caption outputs.

%추가로 우리 방법이 attention 뿐만 아니라 caption내 설명하는 내용면에서도 실제로 diversity를 감소하지 않는 다는것을 분석한게 appencix C임. 여기서 caption내의 생성 문장들간의 diversity를 평가할 수 있는 metric 소개하고, 앞에 설명했던 방법들 대해서 이런 caption diversity를 평가함. 또한 우리가 제안한 token selction 방식을 통해 직접적으로 caption 상에서 언급? 의 diversity를 설명할 수 있음을 보임. 

In addition to its impact on visual attention, our method also preserves diversity in the content of generated captions. \cref{appendix:caption_similarity} presents an analysis of caption diversity, introducing metrics that assess the variation in generated sentences. Using these metrics, we evaluate caption diversity across different methods, including those discussed earlier in this appendix.





\paragraph{Longer Contexts Amplify Noisy Attention}
% 이 실험의 implementation detail은 다음과 같음. Baseline model (LLaVA-1.5 7B)로 image 한장에 대해 caption을 생성하는 과정에서, output tokens을 생성할 때의 visual token들에 대한 attention weights를 이용하였음. visual attention weights의 경우 layer=20 에서 측정하였고, head-wise로 평균 내었음. 그 후, visual token들에 대해서 sum이 1이 되게 normalize하고 image와 같이 plot하여 나타냈음. 
To analyze how longer contexts influence attention noise, we conducted an experiment using the baseline model (LLaVA-1.5 7B). During the caption generation process for a single image, we measured the attention weights assigned to visual tokens when generating output tokens. Specifically, visual attention weights were extracted from layer 20 and averaged across attention heads.

Next, we normalized the visual attention weights such that the sum of all attention weights for visual tokens equaled 1. Finally, as shown in \cref{attention_plot}, we plotted the normalized attention distribution along with the corresponding image to visualize how attention shifts across different tokens in the presence of longer contexts.


\paragraph{Longer Context, Less Visual Focus}
% 이 실험의 implementation detail은 다음과 같음. Baseline model (LLaVA-1.5 7B)로 DOCCI dataset의 image sample 3000장에 대한 Caption 생성과정에서, output tokens을 생성할 때의 visual token들에 대한 attention weights를 이용하였음. visual attention weights의 경우 layer=20 에서 측정하였고, head-wise로 평균 내었음. 각 output token 생성할 때의 visual tokens에 대한 attention weights의 총합,  text tokens (instruction tokens, generated tokens)에 대한 attention wiegths의 총합을 sample들에 대해 average하여 이를 context 길이에 따라 나타냈음. 
To analyze the effect of longer contexts on visual focus, we conducted an experiment using the baseline model (LLaVA-1.5 7B). Caption generation was performed on 3,000 image samples from the DOCCI dataset, and we measured the attention weights assigned to visual tokens during the generation of output tokens.

Specifically, visual attention weights were extracted from layer 20 and averaged across attention heads. For each output token, we computed the total attention weights allocated to visual tokens and the total attention weights allocated to text tokens (including instruction tokens and generated tokens). These values were averaged across all samples and plotted against different context lengths to examine how visual focus changes as context length increases. The results of this analysis are presented in \cref{fig:baseline_attention}, which illustrates the diminishing visual focus as context length grows. 

\subsection{Analyses in \cref{subsec:an_ab}}
\label{appendix_analysis_2}
\paragraph{Visual Attention Analysis}
%이 실험의 implementation detail은 다음과 같음. Baseline model (LLaVA-1.5 7B)로 DOCCI dataset의 image sample 3000장에 대한 Caption 생성과정에서, output tokens을 생성할 때의 visual token들에 대한 attention weights를 이용하였음. visual attention weights의 경우 layer=20 에서 측정하였고, head-wise로 평균 내었음. 각 output token 생성할 때의 visual tokens에 대한 attention weights의 총합을 Baseline model, SPARC를 적용했을 때에 대해 각각 plot하여 나타냈음.
To analyze visual attention during caption generation, we conducted an experiment using the baseline model (LLaVA-1.5 7B) on 3,000 image samples from the DOCCI dataset. During the caption generation process, we measured the attention weights assigned to visual tokens when generating output tokens.

Specifically, visual attention weights were extracted from layer 20 and averaged across attention heads. We computed the total attention weights allocated to visual tokens for each output token and compared the results between the baseline model and the SPARC-enhanced model. These values were plotted to illustrate the differences in visual attention between the two approaches, as shown in \cref{fig:attention_analysis}(a).

\paragraph{Impact on attention sinks}

%우리는 각 방법이 caption과정에서 실제 맥락과 관련있는 부분의 visual attention을 강화하는지, 관련없는 부분을 강화하는지 확인하기 위해, 각 방법이 (관련 없는 부분인데 큰 attention값을 가지는) attention sink를 얼마나 변화시키는지 확인하기로 했음. specifically, 생성 과정에서 output token에 대한 attention대해서, (sink인 token들의 평균 크기 / sink가 아닌 token들의 평균 크기) 이 비율을 재서 확인함.

%이 실험의 implementation detail은 다음과 같음. Baseline model (LLaVA-1.5 7B)로 DOCCI dataset의 image sample 3000장에 대한 Caption 생성과정에서, output tokens을 생성할 때의 visual token들에 대한 attention weights를 이용하였음. visual attention weights의 경우 layer=20 에서 측정하였고, head-wise로 평균 내었음. 각 image sample에 대한 visual attention sink의 경우 이전의 work에서 설명한 방식 그대로 이용하여, 각 sample마다 어떤 visual token들이 sink token인지 찾았음. 이전의 work에서 attention sink찾는 방식은 다음과 같음. attention sink의 경우 high dimensional의 hidden states(self attention layer의 inputs)의 여러 dimension의 값 중, 특정한 dimension들에서 매우 큰 값을 가지는 것을 확인. 이를 통해, 이 dimension에서 높은 값을 가지는 token들을 sink token으로 하고 고름. 
% 생성과정에서 각 output token에 대한 attention 대해서 visual token들 대해, (sink token attention 평균값) / (non-sink attention)을 구하고, 이를 모든 image sample들에 대해 average하여 plot하였고, 이를 baseline, naive approach, SPARC대해 비교했음, 그 결과 naive apprach는 sink attention비중을 상당히 증가시키지만, 우리꺼는 sink 그대로. 반면에 이전에 plot 한거 보면 우리방식은 context길어짐에 따라 감소하는 attention 값 보강한다는 걸 확인할 수 있었고, 이를 두가지를 종합하면 sparc는 길어지는 context 대해서, 중요한 부분 visual token대한 attention 만 selectively 증가시켜 줄 수 있다고 볼 수 있음.

To evaluate whether each method strengthens visual attention to contextually relevant regions or amplifies unrelated areas, we analyzed how different approaches affect attention sinks—tokens that receive large attention values despite being unrelated to the actual context.

Specifically, we measured the ratio of attention scales by computing the average attention weight of sink tokens divided by the average attention weight of non-sink tokens. This ratio was used to assess the extent to which each method influences attention sinks, as illustrated in \cref{fig:attention_analysis}(b).

For this experiment, we used the baseline model (LLaVA-1.5 7B) to generate captions for 3,000 image samples from the DOCCI dataset. During caption generation, we measured the attention weights assigned to visual tokens at layer 20 and averaged them across attention heads.

To identify visual attention sinks, we followed the approach described in previous work~\cite{anonymous2024see}. Specifically, attention sinks were determined based on high-dimensional hidden states (self-attention layer inputs), where certain dimensions exhibited significantly large values. Tokens with high values in these specific dimensions were classified as sink tokens.

During generation, for each output token, we computed the ratio of average attention weights between sink tokens and non-sink tokens. These values were then averaged across all image samples and plotted for comparison across the baseline model, a naive attention enhancement approach, and SPARC.

Results indicate that the naive approach significantly increases the proportion of attention allocated to sink tokens, while SPARC maintains the sink attention ratio at similar levels to the baseline. Additionally, prior results (\cref{fig:attention_analysis}(a)) demonstrated that SPARC counteracts the decrease in attention values caused by longer context lengths. Taken together, these findings suggest that SPARC selectively reinforces attention to important visual tokens as context length increases, rather than indiscriminately amplifying all attention values.

\newpage

\section{Ablation Study on the Effectiveness of Hyperparameter and Setting Varitions}
\label{sec:parameter_ablation}

% 방법론에 사용된 parameter들의 effectiveness를 검증하기 위해, 각 parameter들에 대한  abliation 및 setting choices에 대한 ablation을 진행함.
% logit에다가 한거? 

\paragraph{Ablation Study on Setting Choices}
To demonstrate the effectiveness of the proposed components in our method, we conducted a series of ablation experiments. Specifically, we evaluated the impact of our token selection strategy and the progressive attention calibration mechanism, as presented in \cref{ablation1}. For these experiments, we used LLaVA-1.5 as the baseline model and measured performance using the CLAIR metric on the IIW-400 dataset.

First, we examined the effect of omitting the token selection process and applying the progressive attention calibration to all image tokens. To ensure a fair comparison, we set the attention scaling factor $\alpha$ to 1.007, following the same scaling strategy as when token selection is employed, as shown in \cref{fig:attention_analysis}(a). While this approach led to performance improvements compared to the baseline, demonstrating the effectiveness of the progressive attention calibration mechanism, the results fell short of the performance achieved by our complete method. This indicates that while progressive attention calibration contributes to performance gains, the token selection strategy further enhances the model’s ability to focus on the most informative tokens, leading to superior overall performance.

Next, we analyzed the effect of modifying the token selection approach by comparing the tokens to those from only the immediately preceding step, rather than leveraging the exponential moving average (EMA) across previous steps. This simplified selection mechanism yielded better performance than the baseline but did not match the effectiveness of our proposed method.

These ablation results underscore the contributions of both the token selection strategy and the progressive attention calibration in enhancing model performance. Our full method effectively mitigates attention dilution and ensures the consistent integration of salient visual information throughout the caption generation process. 

\begin{table}[h]
\caption{CLAIR scores under different setting choices.}
\label{ablation1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Setting & CLAIR \\
\midrule
Baseline & 56.35 \\
Ours & 61.49 \\
~~w/o Selection & 59.10 \\
~~w/o EMA & 60.28 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


\paragraph{Ablation Study on Hyperparameter Impact}
% 다음으로는 우리 method에 포함되는 hyperparamter들의 effectiveness를 확인하기 위한 실험을 진행하였음. section 4.1에서 언급된, 우리 방법론 사용했던 paramter값들에서 하나씩 바꿔가면서 성능 어떻게 바뀌는지 평가했음. baseline model로는 LLaVA-1.5 model 이용, IIW-400 dataset에 대해 CLAIR score 쟀음. 먼저 token selction하는 layer l의 영향 평가. 어느 layer에서 token selection을 진행하고, 이를 기반으로 attention recalibration을 하던지 간에 모두 baseline보다는 clair score가 높아지는 것을 확인할 수 있었음. 특히 중후반 layer에서 token selection을 했을 때 성능 향상이 크게 나타낫음 (layer 20에서 제일 높은 CLAIR score). table7에 나타냄. 이는 이전 work에서 mllm의 visual token attention pattern이 중후반 layer에서 실제 semantic한 요소들과 잘 매치된다는 사실과 부함함. 다음으로는 token selection threshold (Relative activation score를 thresholding 해서 token selction하는..) \tau 대한 성능 쟀음. 이를 table 8에 나타냄. relative activation score는 각 visual token에 대해서, 현재 output token에 대한 attention이 이전 output token을 생성할 때의 attention에 비해 얼마나 많이 jump(?) 했나를 기반으로 재는 score였는데, 이거 threshold로 너무 작은 값을 고르면, 오히려 model의 captioning 성능을 저하시키는 걸 확인할 수 있음. 그게 아니라면 이 threshold로 골라진 token의 attention을 강화할 경우  baseline 보다 성능이 모두 증가하지만, \tau=1.5일 때 가장 높은 CLAIR score. 다음으로는 EMA smoothing factor \beta 대한 실험 진행. 결과는 table10. \beta=0일 때는 각 visual token에서 현재 생성에서의 attention과 단순히 바로 전 output 생성 시의 attention과의 차이를 이용하게 됨(0아니면 EMA하는거고...) \beta=0.1정도의 작은 값을 사용하여 history값들이 아니라, 바로 전 step에서의 atteniton 값에 가중된 것과의 차이를 재는 것이 token selection 효율적으로 하게 하는 것을 확인할 수 있음.

To further analyze the effectiveness of the hyperparameters in our method, we conducted a series of experiments by systematically varying key parameters and evaluating their impact on performance. Specifically, we assessed how changes to individual hyperparameters influenced the model's performance, using LLaVA-1.5 as the baseline model and measuring the CLAIR score on the IIW-400 dataset. The parameters examined in this study were those introduced in \cref{subsec:setup}.

First, we evaluated the impact of the layer \( l \) at which token selection is applied. Regardless of the layer at which token selection was performed and subsequent attention recalibration was applied, we observed an improvement in CLAIR scores compared to the baseline. Notably, selecting tokens in mid-to-late layers resulted in the most significant performance gains, with the highest CLAIR score observed at layer 20 (\cref{layer}). This finding aligns with prior research, which has shown that the visual token attention patterns in MLLMs tend to align more closely with semantically meaningful features in mid-to-late transformer layers~\cite{jiang2024devils}.

Next, we investigated the impact of the token selection threshold \( \tau \), which determines the relative activation score used for token selection (\cref{eq:rac}). This score quantifies how much the attention on a given visual token jumps compared to the previous output token during caption generation. The results, presented in \cref{tau}, reveal that excessively low threshold values degrade captioning performance. However, for appropriately chosen values, reinforcing attention on the selected tokens consistently led to improvements over the baseline. The best performance was observed at \( \tau = 1.5 \), where the model achieved the highest CLAIR score. 

Then, we evaluated the impact of the EMA (exponential moving average) smoothing factor \( \beta \), which controls the degree to which historical attention patterns influence token selection. The results, shown in \cref{beta}, indicate that when \( \beta = 0 \), the model relies solely on the difference between the current step’s attention and that of the immediately preceding step. In contrast, applying EMA smoothing enables a more stable token selection process. We found that using a small value of \( \beta = 0.1 \) was particularly effective, as it allowed token selection to be influenced primarily by the most recent step while still incorporating a degree of historical information. 

% 마지막으로 scaling parameter인 alpha대해서도 진행. 어느 정도 선 까지 올렸을 때 CLAIR score 올라가는 정도가 최대, 특히 alpha=1.1에서. 많이 올리게 되면 model의 captioning 성능 망가뜨리는것을 확인. scaling parameter \alpha대해서는 추가적인 ablation 실험 진행함. CHAIR benchmark이용해서  caption에서 언급하는 object단위로 Precision, recall, F1을 \alpha 값 변화하면서 잼. 그게 Table11. model은 LLaVA-1.5, LLaVA-NeXT 이용. table 11.(a)가 llava-1.5결과. alpha=1.1까지 올렸을 때 baseline 보다 precision, recall, f1측면에서 좋은 성능. alpha=1.12일 때는 recall은 baseline보다 조금 떨어지지만, precision 크게 향상됨. 재미있는 점은, 이때 table 4에서 Naive한 attentio 향상 approach와 비슷한 precision유지하면서 recall은 이보다 큰 값 유지. 따라서 우리 방법 적용하는게 precision recall-trade off 측면에서 좋다고 할 수 있음. LLaVA-NeXT model에 대해 동일한 실험 진행한 것이 Table 11 (b). 이때는 precision은 baseline수준으로 유지하면서 recall올리면서 caption quality 향상시키는 것을 볼 수 있음.

Finally, we conducted additional ablation experiments on the scaling parameter \( \alpha \). We investigated how increasing \( \alpha \) affects CLAIR scores and found that performance improved up to a certain threshold, with the highest CLAIR score observed at \( \alpha = 1.1 \). However, increasing \( \alpha \) beyond this point degraded the model’s captioning performance. 

To further analyze the effect of \( \alpha \), we conducted additional ablation experiments using the CHAIR benchmark, measuring Precision, Recall, and F1 score at the object level based on the caption content as \( \alpha \) varied. The results are presented in \cref{fig:precision_recall_alpha}, where we evaluated both LLaVA-1.5 and LLaVA-NeXT models. \cref{llava_pr} shows the results for LLaVA-1.5, where performance improved across Precision, Recall, and F1 scores as \( \alpha \) increased up to 1.1. At \( \alpha = 1.12 \), Recall slightly decreased compared to the baseline, but Precision improved significantly. Interestingly, at this setting, the Precision level was similar to that of the naive attention enhancement approach~\cite{liu2025paying} in \cref{CHAIR}, while maintaining a higher Recall, suggesting that our method provides a better balance in the Precision-Recall trade-off. \cref{llava_next_pr} presents the results for LLaVA-NeXT, where Precision remained close to the baseline, while Recall increased, indicating that our approach enhances caption quality by improving recall without sacrificing precision.

% Layer
\begin{table}[h]
\caption{CLAIR scores according to selected layers.}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
\textbf{Layer} & 5 & 10 & 15 & \textbf{20} & 25 & 30 \\
\midrule
 \textbf{CLAIR}     & 57.34   & 58.18     & 58.30       & \textbf{61.49}    & 60.85      & 58.29     \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\label{layer}
\end{table}

% tau
\begin{table}[h]
\caption{CLAIR scores according to hyperparameter $\tau$.}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
\textbf{$\tau$} & 0.5       & 1.0        & \textbf{1.5}        & 2.0        & 2.5              \\
\midrule
    \textbf{CLAIR} & 52.03    & 58.56    & \textbf{61.49}    & 60.24   & 59.50         \\

\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in

\label{tau}
\end{table}

% alpha
\begin{table}[h]
\label{alpha}
\caption{CLAIR scores according to hyperparameter $\alpha$.}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
\textbf{$\alpha$} & 1.05      & 1.075      & \textbf{1.1}        & 1.125      & 1.15               \\
\midrule
    \textbf{CLAIR}    & 58.60      & 60.06    & \textbf{61.49}    & 60.90       & 57.93\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

% beta
\begin{table}[H]
\caption{CLAIR scores according to hyperparameter $\beta$.}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccccc}
\toprule
\textbf{$\beta$}  & 0.3       & 0.2        & 0.15       & \textbf{0.1}        & 0.05       & 0        \\
\midrule
    \textbf{CLAIR}      & 60.00   & 60.10       & 61.20      & \textbf{61.49}    & 60.41   & 60.28\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\label{beta}
\end{table}

\begin{figure}[H]
%\vskip 0.1in
\begin{center}
\centerline{
    \subfigure[]{
        \includegraphics[width=0.35\columnwidth]{llava_pr.pdf}
        \label{llava_pr}
    }
    
    \subfigure[]{
        \includegraphics[width=0.35\columnwidth]{next_pr.pdf}
        \label{llava_next_pr}
    }
}
\caption{Object-level Precision, Recall, and F1 scores for LLaVA-1.5 (a) and LLaVA-NeXT (b) with scaling parameter \(\alpha\)}
\label{fig:precision_recall_alpha}
\end{center}
\vskip -0.2in
\end{figure}

\newpage


\section{Detailed Caption Similarity Analysis}
\label{appendix:caption_similarity}

% 앞에서 naive하게 visual token들에 대한 attention을 강화하는 것이 caption에서 언급하는 object들의 수를 줄어들게 한다는 것과, caption 생성과정에서 visual token에 대한 attention pattern의 diversity를 줄어들게 한다는 것을 확인했음. 우리는 visual token에 대한 attention pattern의 diversity가 줄어드는 현상과 caption에서 언급하는 object 가지수가 줄어드는 것의 연관성을 더 탐구하기 위해서, 생성 된 caption내의 문장들간의 similarity 한 정도를 통해 생성된 caption에서의 diversity(?)를 측정하는 metric을 제안하고 앞에 설명했던 방법들 대해서 이런 caption diversity를 평가함. 실제로 저런 방식들은 caption내의 문장들 간의 유사도를 증가시키고, caption전체의 diversity를 줄이는 것을 실험을 통해 확인함. 또한 우리가 제안한 token selction 방식을 통해 직접적으로 caption 상에서 언급? 의 diversity를 설명할 수 있음을 보임, 따라서 우리가 제안했던 token selection 방식이 유의마 하다고 볼 수 있음 

In the previous sections, we observed that naively enhancing the attention on visual tokens leads to a reduction in the number of objects mentioned in the generated captions and decreases the diversity of attention patterns across visual tokens during the captioning process. To further investigate the relationship between the diversity of attention patterns and the variety of objects mentioned in the generated captions, we propose a metric that quantifies the similarity between sentences within the generated captions. Using this metric, we evaluate the caption diversity of different methods and analyze their effects. Our experimental results demonstrate that naive attention amplification strategies increase the similarity between sentences within captions, thereby reducing overall caption diversity. Furthermore, we show that our proposed token selection method directly enhances the diversity of objects mentioned in captions, supporting the significance of our approach.

% 우선 우리는 이러한 captions내의 문장들 간의 similarity를 재기 위한 metric을 제안함. 
%To quantify this effect, we introduced a metric called caption similarity score $C_{sim}$ as score between the two sentences. $S_{sim}$ is calculated using a large language model (LLM), which evaluates the degree of overlap in visual content described by the paired sentences. Higher scores indicate greater similarity between sentences. 자세한 세팅으로는, IIW-400 dataset의 image sample에 대한 captiond을 생성. 이들에 대한 Caption similarity score를 측정.이 때 측정에 사용한 llm은 LLaMA-3.2 Vision 7B. 측정에 사용한 promtpt는 figure 13에 나타냈음. 
\paragraph{Caption Similarity Anlaysis} 
To quantify this effect, we introduce the \textbf{Caption Similarity Score} $C_{sim}$, which measures the similarity between pairs of sentences within a generated caption. It is defined as:
\begin{equation}
C_{sim}: \frac{1}{|P|}\sum_{(s_i, s_j) \in P} S_{sim}(s_i, s_j)
\end{equation}
where $P$ denotes the set of all possible sentence pairs within a given caption. Each pair $(s_i, s_j)\in P$ consists of two sentences $s_i, s_j$, and $S_{sim}(s_i,s_j)$ represents their similarity score.
Specifically, $S_{sim}$ is calculated using a large language model (LLM), which evaluates the degree of overlap in visual content described by the paired sentences. A higher $C_{sim}$ score indicates greater similarity between sentences, suggesting lower diversity in the generated caption.

%To validate the impact of our approach, we measured $C_{sim}$ for outputs from both the baseline model and the naive attention-enhanced method, and Ours. This comparison highlights the influence of increased image attention weights on the diversity of generated captions. As shown \cref{caption_similarity}, our experiments reveal that naive attention amplification strategies, while intended to enhance focus across the entire image, inadvertently lead to higher Caption Similarity Scores. This indicates that the generated captions tend to repeat descriptions of the same visual elements, reducing the diversity of objects and features mentioned, thereby limiting the informativeness and variety of the captions. 반면 Ours는 caption간의 diversity를 심지어 조금 더 다양하게 하는 효과 확인할 수 있음

For our experiments, we generated captions for images sampled from the IIW-400 dataset and computed the caption similarity scores using LLaMA-3.2-11B-Vision~\cite{dubey2024llama}. The prompt used for measuring similarity is as follows: 

\noindent
\parbox{\textwidth}{%
\texttt{
You are trying to tell if two sentences are describing the same visual contents.
Sentence1:
\{sentence1\}
Sentence2:
\{sentence2\}
On a precise scale from 0 to 100, how likely is it that the two sentences are
describing the same visual contents?}} 

This prompt is similar to the one used in CLAIR to compare the generated caption and the reference caption.

To validate the impact of our approach, we measured $C_{sim}$ for captions generated by the baseline model, the naive attention-enhanced method, and our proposed method. As illustrated in ~\cref{caption_similarity}, our results indicate that naive attention amplification strategies, while intended to enhance focus across the entire image, inadvertently lead to higher caption similarity scores. This suggests that the generated captions tend to repeat descriptions of the same visual elements, thereby reducing the diversity of objects and features mentioned, limiting the informativeness and variety of the captions. In contrast, our method increases the diversity of captions by reducing $C_{sim}$, demonstrating its effectiveness in improving caption variability.

% 자세한 내용 다음과 같음. We further analyze whether these selected tokens align with the visual content relevant to the generated text. Specifically, we observe how often an image token is chosen during the captioning process to gauge whether it is contextually meaningful.

%For each caption in the dataset, image tokens are selected at each generation step using the \textbf{Relative Activation Score} (\cref{subsec:token_selection}). To track overall selection dynamics, we use a cumulative \textbf{Selection Count} \(c_{i,j}\) defined in \cref{eq:selection_count} which indicates how many times each token has been chosen up to step \( i \).

%Using this metric, tokens are grouped into clusters based on their total Selection Count across the full caption-generation sequence. We divide tokens into two main groups: those frequently selected (high Selection Count) and those infrequently selected (low Selection Count). By comparing the mean token counts in each group, we can identify patterns in attention allocation.

%Interestingly, methods that select a larger number of tokens on average tend to yield lower similarity scores in the resulting captions (see \cref{sim_select}). This trend matches the intuition that  focusing on a broader range of tokens increases diversity in the generated descriptions. Prior methods may restrict the model to a narrow set of tokens, limiting diversity in the caption. In contrast, our proposed selection mechanism remains flexible, focusing on contextually important tokens without overconstraining the attention space. This flexibility helps overcome the limitations of earlier approaches by enabling richer and more coherent visual grounding in the final captions.
\begin{figure}[H]
\vskip 0.2in
\begin{center}

\centerline{
    \subfigure[]{
        \includegraphics[width=0.35\columnwidth]{Caption_similarity_score.pdf}
        \label{caption_similarity}
    }
    \hspace{0.8cm}
    %\hfill
    \subfigure[]{
        \includegraphics[width=0.35\columnwidth]{num_selected_patches.pdf}
        \label{num_selected_patches}
    }
}
\caption{(a) Comparison of caption similarity scores and number of selected tokens. (a) Caption similarity score across different methods. (b) Number of dynamically selected tokens during caption generation. Methods that identify more tokens as important tend to yield lower similarity scores, indicating that focusing on a broader range of tokens increases diversity in the generated descriptions.}
\label{sim_select}
\end{center}
\vskip -0.2in

\end{figure}


\paragraph{Impact of Token Selection on Caption Similarity}
We further investigate whether the visual tokens selected by our method dynamically align with the most relevant visual content for the generated text. Specifically, we examine how frequently each image token is selected during the captioning process to assess its contextual significance. For each caption in the dataset, visual tokens are chosen at each generation step using the \textbf{Relative Activation Score} (~\cref{subsec:token_selection}). To quantify the selection dynamics, we used a cumulative \textbf{Selection Count} in ~\cref{eq:selection_count}, which represents the total number of times each token has been selected throughout the caption generation process.

UWith this metric, we assess the extent of dynamic token selection by categorizing tokens according to their total selection count throughout the caption-generation process. We classify tokens into two groups: frequently selected tokens (high selection count) and infrequently selected tokens (low selection count). By comparing the number of tokens in the high-selection group across different methods, we evaluate how different approaches influence the degree of dynamic visual token activation during captioning.

We analyze the selection dynamics across 3,000 image samples from the DOCCI dataset by measuring the number of tokens in the high-selection group during the captioning process and computing the average. The results are presented in ~\cref{num_selected_patches}.

Interestingly, methods that dynamically select a larger number of tokens throughout the process tend to produce captions with lower similarity scores. This suggests that a higher degree of dynamic visual token selection contributes to greater variability in generated captions. This aligns with the intuition that focusing on a broader range of tokens increases diversity in the generated descriptions. The naive approach may restrict the model to a narrow set of tokens, thereby limiting caption diversity. In contrast, our proposed selection mechanism remains flexible, emphasizing contextually important tokens without over-constraining the attention space. This flexibility enables richer and more coherent visual grounding in the final captions, overcoming the limitations of earlier approaches.

In summary, our findings demonstrate that naive attention-enhancement strategies inadvertently reduce caption diversity by increasing similarity between sentences. In contrast, our proposed token selection method enhances diversity by dynamically selecting a broader set of visually relevant tokens. These results highlight the importance of effective token selection in generating informative and diverse captions.

\newpage
\section{CHAIR Metric Results and Precision-Recall Analysis}
\label{appendix_chair}

To provide a more detailed analysis of hallucination and precision-recall tradeoff, we present the CHAIR metric results along with recall, and F1 score. The CHAIR metric consists of two components:

\paragraph{Instance-level hallucination rate (\(\text{CHAIR}_i\))}  
\begin{equation}
\text{CHAIR}_i = \frac{\left| \{\text{hallucinated objects}\} \right|}{\left| \{\text{all objects mentioned}\} \right|}
\end{equation}
This metric measures the proportion of hallucinated objects among all mentioned objects in generated captions. Since precision in our primary evaluation is defined as \( 1 - \text{CHAIR}_i \), a lower \(\text{CHAIR}_i\) value directly translates to a higher precision, indicating fewer hallucinated objects in the generated captions.

\paragraph{Caption-level hallucination rate (\(\text{CHAIR}_s\))}  
\begin{equation}
\text{CHAIR}_s = \frac{\left| \{\text{captions with hallucinated object}\} \right|}{\left| \{\text{all sentences}\} \right|}
\end{equation}
This metric captures the proportion of captions containing at least one hallucinated object. A lower \(\text{CHAIR}_s\) value suggests that hallucinations are less frequent across generated captions at the caption level.

Table~\cref{CHAIR_appendix} summarizes the evaluation scores, where each value represents the average over five repeated experiments on randomly sampled 500 instances.  

Our method adopts a progressive reinforcement mechanism that enhances attention to visual tokens as the caption generation progresses. This strategy helps mitigate hallucinations in the latter part of the caption, leading to a noticeable improvement over baseline methods in later-token precision. However, since this approach primarily addresses hallucinations that occur later in the caption, it is inherently less effective in handling hallucinations that appear early in the sequence. As a result, our method may still exhibit limitations when evaluated under metrics such as CHAIR$_s$, which assess hallucination at the overall caption level.

Nevertheless, despite this inherent challenge, our approach demonstrates superior performance in F1 score by achieving a better balance between reducing hallucination (increasing precision) and improving recall. These results indicate that our method successfully enhances the model’s ability to generate captions that are both accurate and comprehensive, establishing a new standard for high-quality captioning.


\begin{table}[h]
\caption{Comparison of CHAIR metric results on the MS COCO validation dataset for existing methods and our proposed approach. Our method achieves a higher F1 score by effectively mitigating hallucination while maintaining recall, demonstrating a better balance compared to other approaches.}
\label{CHAIR_appendix}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Methods &  CHAIR$_s$& CHAIR$_i$ & Recall & F1 \\
\midrule
Baseline    & 53.96  & 15.30 & 79.46 & 81.99\\
OPERA       & 55.68  & 15.46 & 78.82 & 81.58\\
VCD         & 57.92  & 16.78& 77.50 & 80.26\\
VOLCANO     & 46.16 &12.46&77.82&82.39\\
PAI         & \textbf{34.92}&\textbf{9.36}&72.44&80.52\\
    Ours    &   51.52&12.28&\textbf{79.98}&\textbf{83.67}\\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}





\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
