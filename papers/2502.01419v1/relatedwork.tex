\section{Related Work}
\subsection{Mitigating Hallucinations in MLLMs}  
MLLMs often generate hallucinations, where text is inconsistent with visual input, and numerous studies have aimed to address this issue through various methods~\cite{li2023evaluating, liu2024survey, gunjal2024detecting}. Decoding-based methods tackle hallucination by penalizing uncertain token generations through techniques like text aggregation analysis~\cite{huang2024opera}, corrupted visual inputs~\cite{leng2024mitigating, gong2024damro}. Self-refinement strategies are also employed to iteratively align generated captions with visual content~\cite{zhou2023analyzing, lee2023volcano}. In addition, research indicates that hallucinations often arise from an over-reliance on textual context while neglecting visual information~\cite{zhu2024ibd, liu2025paying}. To address this imbalance, decoding strategies and attention calibration techniques have been developed to enhance the utilization of relevant visual elements based on their importance~\cite{huo2024self, liu2025paying,li2025mitigating}. Hallucination issues intensify with long-form text generation, as models rely more on text and less on image content~\cite{favero2024multi, lee2024toward, zhong2024investigating}. Methods such as shortening text length~\cite{yue2024less}, segmenting refinement~\cite{lee2024toward}, and leveraging image-only logits and contrastive decoding~\cite{zhong2024investigating,favero2024multi} have been explored. Existing solutions have not effectively addressed the challenge of enhancing context-relevant visual information, as vision-related signals tend to weaken over longer contexts.

\subsection{Visual Attention in MLLMs}

MLLMs utilize transformer-based architectures with attention mechanisms to integrate visual and textual modalities~\cite{vaswani2017attention, basu2024understanding, osman2023survey}. During text generation, attention weights do not always focus on the most relevant visual tokens~\cite{zhang2024seeing, woo2024don, jiang2024devils}. Prior studies have shown that enhancing image attention can help mitigate hallucinations by improving the model's ability to better align with relevant visual content~\cite{li2024inference, xing2024mitigating}. Efforts to achieve this include increasing image attention, adjusting positional embeddings~\cite{xing2024mitigating, li2025mitigating}, and correcting biases that direct attention to irrelevant regions.~\cite{jiang2024devils, gong2024damro, anonymous2024see}.
Several approaches have been proposed, such as boosting overall image attention~\cite{liu2025paying, jiang2024devils} and reweighting key tokens~\cite{xing2024mitigating} to better focus on meaningful visual regions. Our findings show that as the model generates longer text, its focus on key visual tokens weakens, while sensitivity to irrelevant noise increases. Existing methods struggle to retain key visual tokens in such cases, making it difficult to preserve their relevance. In contrast, our approach reinforces key visual tokens during decoding, ensuring their continued importance. This improves caption detail and accuracy with minimal computational cost.