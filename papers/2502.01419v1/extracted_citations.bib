@article{basu2024understanding,
  title={Understanding Information Storage and Transfer in Multi-modal Large Language Models},
  author={Basu, Samyadeep and Grayson, Martin and Morrison, Cecily and Nushi, Besmira and Feizi, Soheil and Massiceti, Daniela},
  journal={arXiv preprint arXiv:2406.04236},
  year={2024}
}

@inproceedings{favero2024multi,
  title={Multi-modal hallucination control by visual information grounding},
  author={Favero, Alessandro and Zancato, Luca and Trager, Matthew and Choudhary, Siddharth and Perera, Pramuditha and Achille, Alessandro and Swaminathan, Ashwin and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14303--14312},
  year={2024}
}

@article{gong2024damro,
  title={Damro: Dive into the attention mechanism of lvlm to reduce object hallucination},
  author={Gong, Xuan and Ming, Tianshi and Wang, Xinpeng and Wei, Zhihua},
  journal={arXiv preprint arXiv:2410.04514},
  year={2024}
}

@inproceedings{gunjal2024detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18135--18143},
  year={2024}
}

@inproceedings{huang2024opera,
  title={Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation},
  author={Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and Wang, Bin and He, Conghui and Wang, Jiaqi and Lin, Dahua and Zhang, Weiming and Yu, Nenghai},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13418--13427},
  year={2024}
}

@article{huo2024self,
  title={Self-introspective decoding: Alleviating hallucinations for large vision-language models},
  author={Huo, Fushuo and Xu, Wenchao and Zhang, Zhong and Wang, Haozhao and Chen, Zhicheng and Zhao, Peilin},
  journal={arXiv preprint arXiv:2408.02032},
  year={2024}
}

@article{jiang2024devils,
  title={Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens},
  author={Jiang, Zhangqi and Chen, Junkai and Zhu, Beier and Luo, Tingjin and Shen, Yankun and Yang, Xu},
  journal={arXiv preprint arXiv:2411.16724},
  year={2024}
}

@article{lee2023volcano,
  title={Volcano: mitigating multimodal hallucination through self-feedback guided revision},
  author={Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and Seo, Minjoon},
  journal={arXiv preprint arXiv:2311.07362},
  year={2023}
}

@article{lee2024toward,
  title={Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage},
  author={Lee, Saehyung and Yoon, Seunghyun and Bui, Trung and Shi, Jing and Yoon, Sungroh},
  journal={arXiv preprint arXiv:2412.15484},
  year={2024}
}

@inproceedings{leng2024mitigating,
  title={Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
  author={Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13872--13882},
  year={2024}
}

@article{li2023evaluating,
  title={Evaluating object hallucination in large vision-language models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.10355},
  year={2023}
}

@article{li2024inference,
  title={Inference-time intervention: Eliciting truthful answers from a language model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2025mitigating,
  title={Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding},
  author={Li, Jiaming and Zhang, Jiacheng and Jie, Zequn and Ma, Lin and Li, Guanbin},
  journal={arXiv preprint arXiv:2501.01926},
  year={2025}
}

@article{liu2024survey,
  title={A survey on hallucination in large vision-language models},
  author={Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
  journal={arXiv preprint arXiv:2402.00253},
  year={2024}
}

@inproceedings{liu2025paying,
  title={Paying more attention to image: A training-free method for alleviating hallucination in lvlms},
  author={Liu, Shi and Zheng, Kecheng and Chen, Wei},
  booktitle={European Conference on Computer Vision},
  pages={125--140},
  year={2025},
  organization={Springer}
}

@article{osman2023survey,
  title={A survey on attention-based models for image captioning},
  author={Osman, Asmaa AE and Shalaby, Mohamed A Wahby and Soliman, Mona M and Elsayed, Khaled M},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={14},
  number={2},
  year={2023},
  publisher={Science and Information (SAI) Organization Limited}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{woo2024don,
  title={Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models},
  author={Woo, Sangmin and Kim, Donguk and Jang, Jaehyuk and Choi, Yubin and Kim, Changick},
  journal={arXiv preprint arXiv:2405.17820},
  year={2024}
}

@article{xing2024mitigating,
  title={Mitigating object hallucination via concentric causal attention},
  author={Xing, Yun and Li, Yiheng and Laptev, Ivan and Lu, Shijian},
  journal={arXiv preprint arXiv:2410.15926},
  year={2024}
}

@article{yue2024less,
  title={Less is more: Mitigating multimodal hallucination from an eos decision perspective},
  author={Yue, Zihao and Zhang, Liang and Jin, Qin},
  journal={arXiv preprint arXiv:2402.14545},
  year={2024}
}

@article{zhang2024seeing,
  title={Seeing clearly by layer two: Enhancing attention heads to alleviate hallucination in lvlms},
  author={Zhang, Xiaofeng and Quan, Yihao and Gu, Chaochen and Shen, Chen and Yuan, Xiaosong and Yan, Shaotian and Cheng, Hao and Wu, Kaijie and Ye, Jieping},
  journal={arXiv preprint arXiv:2411.09968},
  year={2024}
}

@article{zhong2024investigating,
  title={Investigating and mitigating the multimodal hallucination snowballing in large vision-language models},
  author={Zhong, Weihong and Feng, Xiaocheng and Zhao, Liang and Li, Qiming and Huang, Lei and Gu, Yuxuan and Ma, Weitao and Xu, Yuan and Qin, Bing},
  journal={arXiv preprint arXiv:2407.00569},
  year={2024}
}

@article{zhou2023analyzing,
  title={Analyzing and mitigating object hallucination in large vision-language models},
  author={Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2310.00754},
  year={2023}
}

@article{zhu2024ibd,
  title={Ibd: Alleviating hallucinations in large vision-language models via image-biased decoding},
  author={Zhu, Lanyun and Ji, Deyi and Chen, Tianrun and Xu, Peng and Ye, Jieping and Liu, Jun},
  journal={arXiv preprint arXiv:2402.18476},
  year={2024}
}

