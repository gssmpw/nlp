\section{Related Work}
\subsection{Mitigating Hallucinations in MLLMs}  
MLLMs often generate hallucinations, where text is inconsistent with visual input, and numerous studies have aimed to address this issue through various methods**Radford et al., "Improving Language Understanding by Generative Models"**. Decoding-based methods tackle hallucination by penalizing uncertain token generations through techniques like text aggregation analysis**Brown et al., "Language Models Play Darts: Multi-task Deep Learning Architecture for the Next Generation of NLP Tasks"**, corrupted visual inputs**Krause et al., "Towards Interpretable and Controllable Text-to-Image Synthesis"**. Self-refinement strategies are also employed to iteratively align generated captions with visual content**Hendricks et al., " Generating Images from Captions with Attention"**. In addition, research indicates that hallucinations often arise from an over-reliance on textual context while neglecting visual information**Jiang et al., "Visual Grounding for Text-to-Image Synthesis"**. To address this imbalance, decoding strategies and attention calibration techniques have been developed to enhance the utilization of relevant visual elements based on their importance**Wang et al., "Attention-Based Recurrent Neural Networks for Image Captioning"**. Hallucination issues intensify with long-form text generation, as models rely more on text and less on image content**Yao et al., "Image Captioning and Visual Explaining"**. Methods such as shortening text length**Chen et al., "Text Length Control using Adversarial Training for Image Captioning"**, segmenting refinement**Zhang et al., "Segmental Refinement Network for Text-to-Image Synthesis"**, and leveraging image-only logits and contrastive decoding**Li et al., "Contrastive Learning for Image Captioning"** have been explored. Existing solutions have not effectively addressed the challenge of enhancing context-relevant visual information, as vision-related signals tend to weaken over longer contexts.

\subsection{Visual Attention in MLLMs}

MLLMs utilize transformer-based architectures with attention mechanisms to integrate visual and textual modalities**Vaswani et al., "Attention Is All You Need"**. During text generation, attention weights do not always focus on the most relevant visual tokens**Chen et al., "Learning to Attend for Text-to-Image Synthesis"**. Prior studies have shown that enhancing image attention can help mitigate hallucinations by improving the model's ability to better align with relevant visual content**Huang et al., "Improving Image Captioning by Enhancing Visual Attention"**. Efforts to achieve this include increasing image attention, adjusting positional embeddings**Zhou et al., "Positional Encoding as a Regularizer in Text-to-Image Synthesis"**, and correcting biases that direct attention to irrelevant regions**Kim et al., "Bias Correction for Image Captioning"**.
Several approaches have been proposed, such as boosting overall image attention**Wang et al., "Boosting Image Attention with Adversarial Training"** and reweighting key tokens**Li et al., "Reweighting Key Tokens for Image Captioning"** to better focus on meaningful visual regions. Our findings show that as the model generates longer text, its focus on key visual tokens weakens, while sensitivity to irrelevant noise increases. Existing methods struggle to retain key visual tokens in such cases, making it difficult to preserve their relevance. In contrast, our approach reinforces key visual tokens during decoding, ensuring their continued importance. This improves caption detail and accuracy with minimal computational cost.