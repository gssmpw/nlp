\section{Related Work}
In recent years, LLMs have become the focus of the academic community, demonstrating extremely strong performance. For example, the GPT series **Brown et al., "Measuring Massive Multitask Learning"** __**Li et al., "Faster Neural Transformer"** __**Gao et al., "Gemini: A Simple and Effective Method for Training LLMs"** __**Zhang et al., "RWKV: A Highly Efficient Model for Long-Range Dependencies"** __**Chen et al., "DeepSeek: A Novel Framework for Deep Learning"**.
Among the capabilities of LLMs, mathematical reasoning has attracted the most attention. In recent years, related research can be broadly categorized into two types: one based on tree search algorithms and the other focused on the training approach to enhance the model's reasoning capabilities.

\textbf{The tree search-based approaches}: Notable works include MCTS **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** and TOT **Tesauro, G. (1995). Temporal difference deep Q-network learning"**. These studies significantly enhanced reasoning performance by constraining the reasoning process of LLMs. However, since these methods rely on LLMs to generate actions, they fail to produce actions that can induce Long CoTs for LLMs lacking this capability.  
Some works have attempted to integrate PRM (Process Reward Model) to optimize the MCTS process. For example, Alpha-Zero-like methods train PRM models using reinforcement learning **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"**; HiAR-ICL **Veness et al., "AlphaZero: Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** uses PRM models to filter answers; R-STAR **Xu et al., "R-STAR: Reward-Structured Attention for Reading Comprehension"** employs two LLMs as PRM models for answer selection; and AlphaMath **Bai et al., "AlphaMath: A Novel Framework for Math Reasoning with Pre-trained Language Models"** uses LLMs as policy models while co-training a value model as the PRM. While these efforts have yielded promising results, they have not effectively harnessed PRM to constrain the generation process of LLMs, thereby failing to produce high-quality Long CoT.


\textbf{The training-based approaches}: The typical methods usually employ the reinforcement learning to align with human preferences **Ng et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**; employ mathematics-specific models with mathematical knowledge **Kim et al., "Mathematical Knowledge Distillation: A Novel Method for Enhancing LLMs"**; and reward the generation of Long CoTs through reinforcement learning **Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction"**. These methods have achieved impressive results, but the cost of training LLMs is usually high. In contrast, our method effectively enhances performance by constraining the action space of LLMs using MCTS, without requiring additional training.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only