% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{float}
\usepackage{array}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{enumitem}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{amssymb}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
\usepackage{amsmath}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{our: Generative Deep Unraveling Tree}

%\title{Leveraging Constrained Monte Carlo Tree Search to Generate Long Chain-of-Thought for Improved Mathematical Reasoning}
\title{Leveraging Constrained Monte Carlo Tree Search to Generate Reliable Long Chain-of-Thought for Mathematical Reasoning}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \And ... \And Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \And, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Qingwen Lin$^{1}$, Boyan Xu$^{1}$, Zijian Li$^{1}$, Zhifeng Hao$^{1,2}$, Keli Zhang$^{3}$, Ruichu Cai$^{1,4}$\thanks{Corresponding author, \href{mailto:cairuichu@gmail.com}{cairuichu@gmail.com}}\\
    $^{1}$School of Computer Science, Guangdong University of Technology \\
    $^{2}$College of Science, Shantou University \\
    $^{3}$Huawei Noah’s Ark Lab,
    $^{4}$Peng Cheng Laboratory \\
    \texttt{qingwen\_lin@foxmail.com,\{hpakyim,leizigin\}@gmail.com} \\
    \texttt{haozhifeng@stu.edu.cn}, 
    \texttt{zhangkeli1@huawei.com},
    \texttt{cairuichu@gmail.com} \\
}




%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\usepackage{tcolorbox}

\usepackage{mathtools}
\newtcolorbox[list inside=prompt,auto counter,number within=section]{prompt}[1][]{
    colbacktitle=black!60,
    coltitle=white,
    fontupper=\footnotesize,
    boxsep=5pt,
    left=0pt,
    right=0pt,
    top=0pt,
    bottom=0pt,
    boxrule=1pt,
    title={#1},
    #1, % 这里可以添加额外的选项
}

\begin{document}
\maketitle
\begin{abstract}


%crc version
%However, under zero-shot settings, existing post-training approaches (like Monte Carlo Tree Search) highly rely on LLMs to generate candidate actions freely, which results in an extremely vast action space and ineffective search strategies, ultimately failing to generate reliable Long CoTs. 
Recently, Long Chain-of-Thoughts (CoTs) have gained widespread attention for improving the reasoning capabilities of Large Language Models (LLMs). This necessitates that existing LLMs, which lack the ability to generate Long CoTs, to acquire such capability through post-training methods. 
Without additional training, LLMs typically enhance their mathematical reasoning abilities through inference scaling methods such as MCTS. However, they are hindered by the large action space and inefficient search strategies, making it challenging to generate Long CoTs effectively. 
To tackle this issue, we propose constraining the action space and guiding the emergence of Long CoTs through a refined search strategy. In our proposed Constrained Monte Carlo Tree Search (C-MCTS) framework, we limit the actions selected from a constrained action space, which is divided into five disjoint subsets: \emph{understanding}, \emph{planning}, \emph{reflection}, \emph{coding}, and \emph{summary}. Each subset is further constrained to a small number of predefined prompts, rather than allowing LLMs to generate actions arbitrarily. Additionally, we refine the search strategy by incorporating prior knowledge about the action sets, such as a human-like partial order of the action subsets and the pretrained process reward models. These strategies work together to significantly reduce the vast search space of Long CoTs. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our method enables the 7B model to achieve reasoning capabilities that surpass those of the 72B model.
\end{abstract}

\section{Introduction}
Improving the reasoning ability, especially the mathematical reasoning ability, occupies a central position in current large language models (LLMs) research. The Chain of Thought (CoT) technique \cite{wei2022chain} has emerged as a mainstream solution to enhance LLMs' reasoning ability in a step-by-step manner. Recently, the generation of Long Chains of Thought (Long CoTs) has led to significant performance improvements. Compared to the original CoT, Long CoTs not only focus on problem decomposition but also introduce additional reflection and detailed calculations into the reasoning process. However, existing works typically rely on supervised fine-tuning or reinforcement learning-based post-training, making them highly dependent on extensive training resources. Thus, lots of existing foundational open-source language models, such as Qwen \cite{yang2024qwen2a,yang2024qwen2b}, cannot generate Long CoTs effectively.

In contrast, foundational open-source models often achieve competitive results in structured reasoning through inference scaling. For instance, Monte Carlo Tree Search (MCTS) and its numerous variants \cite{hao2023reasoning,feng2023alphazero,qi2024mutual,sun2024beats} constrain the model to reason in the form of CoT through process supervision. React \cite{yao2022react} and Rethink \cite{schwarzschild2024rethinking} constrain the model to rethink and summarize the CoT in reasoning to arrive at new answers. NLRL \cite{feng2024natural} constrains the model to summarize and learn new experiences through continuous interaction with the environment. These existing research demonstrate that constraining MCTS through process supervision has led to notable improvements. However, due to the fundamental limitation of its large action space generated by the LLMs, MCTS struggles to generate Long CoTs effectively. First, within such a vast action space, LLMs often become trapped in an endless loop of ineffective decompositions, repeatedly generating redundant or suboptimal reasoning steps without making actual progress. Second, within such a vast action space, the process reward model fails to effectively constrain the model to generate human-like action sequences. While reward-based methods aim to guide reasoning, they struggle to enforce structured and meaningful transitions between actions, leading to erratic or unnatural reasoning trajectories.
The key to overcoming these limitations lies in effectively constraining the action space of MCTS.  

Based on this insight, we therefore develop a constrained MCTS (C-MCTS in short) framework for mathematical reasoning, as shown in Fig. \ref{fig:main}. Specifically, we first impose constraints on the selection of actions by restricting them to a predefined constrained action space. This action space is systematically divided into five disjointsubsets: \emph{understanding}, \emph{planning}, \emph{reflection}, \emph{coding}, and \emph{summary}. Each subset consists of carefully curated and predefined prompts rather than allowing LLMs to generate actions in an unrestricted manner. Moreover, we enhance the search strategy by incorporating prior knowledge related to these action sets. Specifically, we integrate human-like partial ordering to guide the decision-making process, ensuring that actions are executed in a logical and interpretable sequence. Additionally, we utilize process reward models to further refine action selection, enabling more informed and effective decision-making. These combined strategies collectively lead to a substantial reduction in the otherwise vast and complex search space of Long CoTs. As a result, our C-MCTS framework not only improves efficiency but also enhances the overall coherence and quality of the generated reasoning processes. Extensive evaluations on mathematical reasoning benchmarks show that, under zero-shot settings, our approach surprisingly enables a 7B model to achieve reasoning performance that surpasses that of a 72B model.

Our contributions can be summarized as follows:

\textbf{Constrained Action Space}: We define five disjoint action subsets, each containing predefined prompts instead of allowing the model to generate actions freely. This significantly reduces the search space and enables the model to generate structured Long CoTs.

\textbf{Search Strategy with Partial Ordering and Process Reward Model}: By incorporating the sequential rules of human reasoning and leveraging a pre-trained PRM to evaluate the Q-values of actions, the search process is further optimized. 

\textbf{Performance Breakthrough under Zero-shot Settings}: Experiments show that, without additional training, a 7B model equipped with the C-MCTS framework can generate Long CoTs that outperform a 72B model on multiple mathematical reasoning benchmarks. 

% Currently, large language models widely employ the Chain of Thought (COT) approach to solve mathematical problems \cite{wei2022chain}. Recent studies \cite{guo2025deepseek,zhong2024evaluation,team2024qwq,team2025kimi} have moved beyond the traditional CoT approach of merely answering questions step-by-step, and now focus on generating Long CoTs that can decompose problems, reflect on solutions, and perform detailed calculations from multiple perspectives.Moreover, these studies have universally achieved impressive results. Regrettably, commonly used open-source models, such as Qwen \cite{yang2024qwen2a,yang2024qwen2b}, currently lack the ability to perform such generate Long CoT.

% \clearpage
% Recent studies have found that constraining the reasoning process of LLMs can enhance the models' reasoning performance. For example, methods such as ToT \cite{yao2024tree}, MCTS \cite{hao2023reasoning,feng2023alphazero,qi2024mutual}, and BEATS \cite{sun2024beats} use process supervision to constrain the models to reason in the form of COT. Compared to inducing models to generate COT \cite{wei2022chain} using prompts, methods like MCTS that constrain the direction of reasoning typically achieve better results.Similarly, React \cite{yao2022react} and Rethink \cite{schwarzschild2024rethinking} constrain the model to rethink and summarize the COT in reasoning to arrive at new answers. NLRL \cite{feng2024natural} constrains the model to summarize and learn new experiences through continuous interaction with the environment.Existing methods have shown that model reasoning capabilities can be enhanced through constraints. Naturally, we can also constrain the model to think in the form of Long CoTs to improve its reasoning performance.


% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{latex/compare.pdf}
%   \caption{Comparison between the COT of MCTS and Ours.In contrast, actions in MCTS are generated by the LLM, whereas our actions are sampled from a predefined set.}
%   \label{fig:compare}
% \end{figure}

% As shown in Figure \ref{fig:compare}, in the current MCTS methods, the model is typically constrained to decompose the problem into multiple sub-problems and generate corresponding states for each sub-problem. However, since each action still relies on the generation of the LLM, for models that do not have the ability to generate Long CoT, they naturally cannot derive sub-problems that would induce the model to generate Long CoT. Therefore, we believe that the existing MCTS methods fail to achieve the ability to constrain the model to generate Long CoT.

% To constrain the model's thinking process and thereby enable it to solve problems in the form of Long CoT, we have adjusted the existing MCTS method.Specifically, instead of allowing the LLMs to generate actions on its own, we define an action set \(\mathcal{A}^n\). The set \(\mathcal{A}^n\) includes multiple subsets, each representing a reasoning mode of the LLM, such as \(\mathcal{A}^{\text{understand}}, \mathcal{A}^{\text{ planing}}, \mathcal{A}^{\text{reflect}}, \mathcal{A}^{\text{code}}, \mathcal{A}^{\text{summary}}\). The intersections between different subsets are empty.
% As shown in Figure \ref{fig:compare}, the LLMs samples actions only from our predefined action set \(\mathcal{A}^n\) at each step. This constrains the model to think about the problem in a multi-level, in-depth manner similar to humans. Although the sample space of the set is limited, the corresponding Cartesian product can essentially approximate the overall process of human problem-solving.
% To further strengthen the constraints, we calculate the Q-values of different actions and select the most suitable action for the current situation based on the Q-values. In addition, we have established rules to limit action transitions to avoid unreasonable action scenarios. We present an example in Figure \ref{fig:main}, where the model generates a Long CoTs by following predefined action instructions. Even for simple problems, the model can provide corresponding solutions after understanding the problem and check for any errors. To improve the accuracy of calculations, the model also uses Python code to assist with computations. Since the model can effectively execute the required actions, we can effectively constrain it to generate Long CoTs through multi-level thinking.

% We have incorporated the aforementioned modifications into the MCTS algorithm and designed an aggregation algorithm that is more suitable for the current situation. Experiments show that our algorithm enables a 7B model to achieve CoT reasoning performance that surpasses that of a 72B model.

% Our work includes the following three innovations:

% 1.Multi-Mode Reasoning Action Set: We define an action set that includes multiple reasoning modes, such as understanding, planning, reflecting, coding, and summarizing. This constrains the model to think about the problem from multiple angles, avoiding the limitations of a single generation mode.

% 2.Action Selection and Constraints Based on Q-values: We select the optimal action by calculating Q-values and set action transition rules to prevent the model from following unreasonable reasoning paths, thereby improving reasoning efficiency and accuracy.

% 3.Improved MCTS Algorithm and Aggregation Algorithm: We integrate these improvements into the MCTS algorithm and design a new aggregation algorithm. Our algorithm enables a 7B model to achieve CoT reasoning performance that surpasses that of a 72B model.

\begin{figure*}[t]
  \includegraphics[width=1\linewidth]{main_picture.pdf}
  \caption {The framework of our Constrained-MCTS, which is characterized by constrained action space and guided search strategy guidance. Specifically, actions are sampled from a predefined action list rather than being generated by LLMs. Meanwhile, the search process is guided by either predefined partial order rules or pre-trained PRMs.}
\label{fig:main}
\end{figure*}
\section{Related Work}

In recent years, LLMs have become the focus of the academic community, demonstrating extremely strong performance. For example, the GPT series \cite{brown2020language,chen2021evaluating,nakano2021webgpt,achiam2023gpt}, the LLaMA series \cite{touvron2023llama,taori2023alpaca,young2024yi,dubey2024llama}, the Gemma series \cite{team2024gemma_a,team2024gemma_b}, the RWKV series \cite{peng2024eagle,peng2023rwkv}, and the DeepSeek series \cite{bi2024deepseek,liu2024deepseek,guo2025deepseek}.
Among the capabilities of LLMs, mathematical reasoning has attracted the most attention. In recent years, related research can be broadly categorized into two types: one based on tree search algorithms and the other focused on the training approach to enhance the model's reasoning capabilities.

\textbf{The tree search-based approaches}: Notable works include MCTS \cite{hao2023reasoning} and TOT \cite{yao2023tree}. These studies significantly enhanced reasoning performance by constraining the reasoning process of LLMs. However, since these methods rely on LLMs to generate actions, they fail to produce actions that can induce Long CoTs for LLMs lacking this capability.  
Some works have attempted to integrate PRM (Process Reward Model) to optimize the MCTS process. For example, Alpha-Zero-like methods train PRM models using reinforcement learning \cite{feng2023alphazero}; HiAR-ICL \cite{wu2024beyond} uses PRM models to filter answers; R-STAR \cite{qi2024mutual} employs two LLMs as PRM models for answer selection; and AlphaMath \cite{chen2024alphamath} uses LLMs as policy models while co-training a value model as the PRM. While these efforts have yielded promising results, they have not effectively harnessed PRM to constrain the generation process of LLMs, thereby failing to produce high-quality Long CoT.


\textbf{The training-based approaches}: The typical methods usually employ the reinforcement learning to align with human preferences \cite{li2023reinforcement,ouyang2022training,rafailov2024direct,ethayarajh2024kto}; employ mathematics-specific models with mathematical knowledge \cite{shao2024deepseekmath,yang2024qwen2b,ying2024internlm}; and reward the generation of Long CoTs through reinforcement learning \cite{guo2025deepseek,zhong2024evaluation,team2024qwq,team2025kimi}. These methods have achieved impressive results, but the cost of training LLMs is usually high. In contrast, our method effectively enhances performance by constraining the action space of LLMs using MCTS, without requiring additional training.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only


\section{Problem Definition}
\label{sec:problem}

We formulate the problem of mathematical reasoning with Long CoTs as a constrained Markov Decision Process (MDP) to address the challenge that models lacking the capability to generate Long CoTs cannot produce effective high-quality Long CoTs under the MCTS algorithm. The formal definition is provided as follows:

Let \( Q \) be the space of mathematical problems, and \( \Pi \) denote the category of large language models (LLMs). Given a problem instance \( q \in Q \) and a reasoning strategy \( \pi \in \Pi \), the reasoning process is modeled as an MDP tuple \((\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R})\), where:

\begin{itemize}[leftmargin=0cm, itemindent=0cm]
    \item \(\mathcal{S}\) is the state space, representing the reasoning context at each step \( t \). The initial state \( s_0 \) is initialized by the problem instance \( q \).
    \item \(\mathcal{A}\) is the action space, which is constrained to a predefined finite set of actions. This set is divided into five disjoint subsets: \(\mathcal{A}^{\text{understand}}\), \(\mathcal{A}^{\text{ planing}}\), \(\mathcal{A}^{\text{reflect}}\), \(\mathcal{A}^{\text{code}}\), and \(\mathcal{A}^{\text{summary}}\). Each subset contains predefined prompts to guide the reasoning process.
    \item \(\mathcal{T}\) is the transition function, defined as \( s_{t+1} = \pi(s_t, a_{t+1}) \), where \( s_{t+1} \) is the next state generated by the LLMs based on the current state \( s_t \) and the selected action \( a_{t+1} \).
    \item \(\mathcal{R}\) is the reward function, providing feedback on the quality of the reasoning steps. Rewards are obtained using a process supervision model, which evaluates the correctness and coherence of the reasoning process.
\end{itemize}

Our goal is to generate one or more reasoning trajectories  as follows, and derive the optimal answer from them:
\begin{equation}
\nonumber
    p = [s_0, a_1, s_1, a_2, \dots, a_n, s_n]
\end{equation}
The solution is obtained by aggregating the terminal states of multiple search trajectories $ \mathcal{P}_k$ generated through an iterative search process, such as MCTS. This approach aims to efficiently explore the constrained action space and produce high-quality reasoning chains.

%Let \( Q \) denote the space of mathematical problems, and \( \Pi \) denote the category of large language models. Given a problem instance \( Q \) and a strategy \( \pi \), we formulate the reasoning process as a Markov Decision Process (MDP) tuple \((s_t, a_t, R_t)\), where:

% \( s_t \) is the current state, with each state \( s_t \in S \) representing the reasoning context at step \( t \);

% \( a_t \in \mathcal{A} \), which includes our predefined finite action space;

% \( R_t \) is the reward list for the current step, obtained using a process supervision model as the world model feedback.

%Here, \( s_0 \) is initialized by the problem \( Q \), and each subsequent state is generated through the transition dynamic:

%\begin{equation}
%\label{eq:generate}
%s_{t+1}  = \pi(s_t, a_{t+1})
%\end{equation}


%Our goal is to obtain one or more reasoning trajectories (Long CoT) as follows, and derive the optimal answer from them:

%\begin{equation}
%    \text{LongCoT} = [s_0, a_1, s_1, a_2, \dots, a_n, s_n]
%\end{equation}

%As shown in Figure \ref{fig:main}, our method is based on the MCTS framework and approximates the optimization by iteratively constructing a search tree. Each node in the tree corresponds to a MDP tuple \((s_t, a_t, r_t)\). The solution is obtained by aggregating the terminal states of multiple search trajectories.


\section{Method}

\subsection{The C-MCTS Framework}
Our method aims to address the challenge that existing MCTS methods cannot enable models like Qwen to generate Long CoTs without additional training. As show in Figure \ref{fig:main}, the C-MCTS framework consists of four main phases: Selection, Expansion, Simulation, and Back-propagation. Each phase plays a crucial role in guiding the reasoning process through a constrained search strategy.

\textbf{Selection}: In the Selection phase, we aim to identify the most promising part of the current search tree for further expansion. We employ the Upper Confidence bound for Trees (UCT) formula to balance exploration and exploitation. Specifically, each node \( s_t \) is selected based on:
\begin{equation}
\nonumber
\label{eq:select}
    s_t = \arg\max_a \left( \text{mean}(R_{t}) + c_{uct} \sqrt{\frac{\log N_t}{N_t}} \right),
\end{equation}
where \( R_{t} \) represents the cumulative reward for this node, \( c_{uct} \) is a constant balancing exploration and exploitation, and \( N_t \) is the visit count of the current node.

\textbf{Expansion}: In the Expansion phase, we sample actions from the predefined action set \(\mathcal{A}\) using a Q-value model to evaluate the potential of each action. To determine the most suitable action for the current state, we employ a Q-value model to evaluate the Q-values of all potential actions, selecting the one with the highest Q-value as \( a_t \). This selection process can be formally expressed as:
\begin{equation}
\nonumber
\label{eq:get_action}
a_t = \arg\max_{a_t^n \in \mathcal{A}} Q(s_t, a_t^n)
\end{equation}

After obtaining the action \( a_t \), we use the LLMs \(\pi\) to generate multiple candidate states. To ensure diversity, we generate \( m \) candidate states and select the one with the highest V-value:

\begin{equation}
\nonumber
\label{eq:v-select}
s_t = \arg\max_{s_t^j \in \mathcal{S}^m_t} V(s_t^j),
\end{equation}
where $\mathcal{S}^m_t$ is the set of candidate states generated by the LLM.

\textbf{Simulation}: In the Simulation phase, we repeatedly perform the Expansion phase until a terminal node is reached, thereby generating a complete reasoning trajectory . To ensure high-quality reasoning, we impose constraints on the action execution order using a search strategy that incorporates human-like partial ordering and process reward models. This strategy ensures that actions are executed in a logical and interpretable sequence.

\textbf{Back-propagation}: In the Back-propagation phase, we update the information of all nodes along the reasoning trajectory \([s_0, a_1, s_1, a_2, \dots, a_n, s_n]\), excluding the root node. We update the visit count \( N_t \) for each node and compute the new reward using the Q-value and V-value models:
\begin{equation}
\nonumber
\label{eq:reward}
\text{r}_t = Q(s_{t-1}, a_t) + V(s_t).
\end{equation}
The cumulative reward for each node is updated as:
\begin{equation}
\nonumber
R_{t} \coloneqq \frac{ \sum_{N_t} \left( \sum_{i=t}^{T} \text{r}_t \right)}{N_t}.
\end{equation}
The MCTS algorithm generates k subtrees through k iterations, with each subtree corresponding to a reasoning trajectory.After \( k \) iterations, we obtain \( k \) candidate reasoning trajectories \(\mathcal{P}_k\). We then design a voting-based aggregation algorithm to select the final answer from these trajectories.
However, in some cases, there may be multiple answers with the highest frequency. In such cases, we can select the path with the highest reward as the final answer path. For any path \( p \) of length \( m \) in the path set \(\mathcal{P}_k\), we use formula \(\ref{eq:reward}\) to calculate the reward of its terminal node and select the node with the highest reward among all nodes as the final answer:
\begin{equation}
\nonumber
p^* = \arg\max_{p \in \mathcal{P}_k}\space\text{r}_{p_{\text{terminal}}}
\end{equation}





%Our method still relies on the MCTS algorithm framework, as shown in Figure \ref{fig:main}, and consists of four steps: (a) Selection, (b) Expansion, (c) Simulation, and (d) Back-propagation.

%\textbf{Selection}:In the Selection phase,we need to choose the part of the current tree that has the most potential to be further expanded in the next stage.
%We use the UCT formula to select each node:

%\begin{equation}
%\label{eq:select}
%    s_t = \arg\max_a \left( mean(R_{t}) + c_{uct} \sqrt{\frac{\log N_t}{N_t}} \right),
%\end{equation}

%where \( R_{t} \) represents the cumulative reward list for this node, \( c_{uct} \) is a constant used to balance exploration and exploitation, and the \( N \) function primarily records the number of visits to the current node.

%\textbf{Expansion}:In the Expansion phase, we sample actions from the predefined action set \(\mathcal{A}^n\) using the Q-value model and select the new states generated by the LLMs using the V-value model. We will provide detailed descriptions in Section \ref{sec:Constraining}.


%\textbf{Simulation}:In the Simulation phase, we repeatedly perform Expansion phase until reaching a terminal node, generating our Long CoT. To ensure the quality of the generation, we will impose certain constraints using a Search Strategy. We will provide detailed descriptions in Section \ref{sec:Strategy}.

%\textbf{Back-propagation}:In the Back-propagation phase, we need to update the information of all nodes in the reasoning Long CoTs path \([s_0, a_1, s_1, a_2, \dots, a_n, s_n]\),excluding the root node. First, we update the visit count \( N_t \) of node \( t \). Then, we use the Q-value and V-value models to compute the new reward of each node. We define the reward calculation formula as follows:
%\begin{equation}
%\label{eq:reward}
%\text{r}_t = Q(s_{t-1}, a_t) + V(s_t)
%\end{equation}
%Then, we update the cumulative reward of each corresponding node using the following formula:
%\begin{equation}
%R_{t} \coloneqq \frac{ \sum_{N_t} \left( \sum_{i=t}^{T} \text{r}_t \right)}{N_t}
%\end{equation}

%Here, \(\coloneqq\) denotes adding the newly updated reward to the reward list \( R_t \).


%After \( k \) iterations, we obtain \( k \) candidate paths \(\mathcal{P}_k\), and we design a voting-based aggregation algorithm to obtain the final answer.We will provide detailed descriptions in Section \ref{sec:Aggregation}.

\subsection{Action Space Constraining}
\label{sec:Constraining}
In the Expansion phase of the C-MCTS framework, we need to obtain an action \( a_t \) based on the current state \( s_{t-1} \), and then use the LLMs to generate a new state \( s_t \) based on \( a_t \) and \( s_{t-1} \). However, if we rely solely on the LLMs to generate actions arbitrarily, models that lack the inherent capability to generate Long CoTs may fail to produce actions that induce Long CoT. This is because unconstrained action generation leads to an extremely vast action space, making it difficult for the model to accurately search for actions that can effectively induce the generation of Long CoTs.

To address this issue, we constrain the action selection to a predefined, finite action space. Specifically, we divide the action space \(\mathcal{A}\) into five mutually exclusive subsets, each representing a specific cognitive operation in the reasoning process:
\[
\mathcal{A} = \left\{ \mathcal{A}^{\text{understand}}, \mathcal{A}^{\text{ planing}}, \mathcal{A}^{\text{reflect}}, \mathcal{A}^{\text{code}}, \mathcal{A}^{\text{summary}} \right\}
\]
Each subset contains predefined prompts that guide the model's reasoning process:
\begin{itemize}[leftmargin=0cm, itemindent=0cm]
    \item \(\mathcal{A}^{\text{understand}}\): Problem Understanding and Information Extraction. This action set guides the model to deeply analyze the problem statement and identify key information points.
    \item \(\mathcal{A}^{\text{ planing}}\): Solution Planning and Execution. This subset helps the model devise problem-solving strategies, break down complex problems, plan reasoning paths, and execute solution ideas.
    \item \(\mathcal{A}^{\text{reflect}}\): Process Reflection and Error Checking. This subset requires the model to validate its reasoning process and identify potential errors, helping to avoid misunderstandings and correct erroneous steps.

    \item \(\mathcal{A}^{\text{code}}\): Coding Assistance and Result Verification. \\
    This subset leverages programming tools to assist with calculations and verify reasoning results, ensuring accuracy. For example, it uses the sympy library for symbolic computation or improves the accuracy of complex calculations.

    \item \(\mathcal{A}^{\text{summary}}\): Final Answer Summary and Presentation. This subset guides the model to integrate the reasoning process and standardize the output of the final answer.
    \end{itemize}

By constraining the action space in this manner, we ensure that the model's reasoning process is guided by predefined, logical steps rather than arbitrary generation. This not only reduces the vast action space but also improves the coherence and quality of the generated reasoning chains.



\subsection{Search Strategy Guidance}
\label{sec:Strategy}

To further efficiently guide the search on the contained action space,as shown in Figure \ref{fig:main}, we consider two types of supervision information, the human-like partial order rules and process reward models. These two kinds of guidance can be used individually  according to the problems. 

\subsubsection{Human-like Partial Order Rules}

These constraints are designed to align with human problem-solving patterns and improve the quality of the generated reasoning chains. Specifically, we establish the following fundamental rules:
\begin{itemize}[leftmargin=0cm, itemindent=0cm]
\item \textbf{Termination Rule}: When an action \( a_t \in \mathcal{A}^{\text{summary}} \) is selected, it signifies the completion of the final answer synthesis. Upon reaching the maximum iteration limit \( m \), we enforce \( a_m \in \mathcal{A}^{\text{summary}} \) to ensure the answer is output. Actions in \(\mathcal{A}^{\text{summary}}\) orchestrate a systematic review of preceding reasoning steps and format the final answer using [boxed{}] notation.

\item  \textbf{Initialization Rule}: To align with human problem-solving patterns, we initialize the reasoning sequence with \( a_0 \in \mathcal{A}^{\text{understand}} \), ensuring foundational comprehension precedes analytical operations.

\end{itemize}


Beyond these fundamental rules, we introduce additional rules to further guide the reasoning process:
\begin{itemize}[leftmargin=0cm, itemindent=0cm]
    \item \textbf{Action Continuity Rule}: Prohibit consecutive execution of code-assistance actions, as two code operations do not bring additional thinking:
   \begin{equation}
   \nonumber
   \neg(a_t \in \mathcal{A}^{\text{code}} \land a_{t+1} \in \mathcal{A}^{\text{code}})
   \end{equation}
   \item \textbf{Phase Progression Rule}: The initial phase must sequentially execute problem-understanding and solution-planning actions:
   \begin{equation}
   \nonumber
   a_1 \in (\mathcal{A}^{\text{ planing}} \cap \mathcal{A}^{\text{understand}})
   \end{equation}
   \item \textbf{Reflection Necessity Rule}: The complete reasoning chain must include at least one reflection-validation process:
   \begin{equation}
   \nonumber
   \exists t \in [0, d_{\max}), \ a_t \in \mathcal{A}^{\text{reflect}}
   \end{equation}
   \item \textbf{Action Diversity Rule}: Prohibit adjacent steps from using the same type of reasoning operation:
   \begin{equation}
   \nonumber
   \forall t < d_{\max}, \ \text{type}(a_t) \neq \text{type}(a_{t+1})
   \end{equation}
   \item \textbf{Depth-Aware Rule}: When the reasoning depth \( t \geq \lfloor d_{\max}/2 \rfloor \):
   - If code validation has not been executed, insert an \(\mathcal{A}^{\text{code}}\) action.
   - Subsequent action space is constrained to \(\mathcal{A}^{\text{reflect}} \cup \mathcal{A}^{\text{code}}\).
\end{itemize}
These rules dynamically adjust the action space \(\mathcal{A}_t \subseteq \mathcal{A}\) to control the reasoning process:
\begin{equation}
\nonumber
\mathcal{A}_t = 
\begin{cases}
\mathcal{A}^{\text{understand}} \cup \mathcal{A}^{\text{ planing}}, & t=0 \\
\mathcal{A}^{\text{reflect}} \cup \mathcal{A}^{\text{code}}, & t \geq \lfloor d_{\max}/2 \rfloor \\
\mathcal{A} \setminus \{a_{t-1}\}, & \text{otherwise}
\end{cases}
\end{equation}

When employing Human-like Partial Order Rules, we used the large language model itself as the Q-value evaluator, combined with manually defined partial-order rules to constrain actions. By implementing these constraints, we ensure that the reasoning process follows a logical and interpretable sequence, reducing the likelihood of errors and improving the overall coherence of the generated reasoning chains. Additionally, these constraints help prevent the model from getting stuck in suboptimal reasoning paths, thereby enhancing the efficiency and effectiveness of the C-MCTS framework.

\begin{table*}
  \small
  \renewcommand\arraystretch{1.1}  
  \centering
  \begin{tabular}{l|ccc|cc}
    \hline
    \textbf{Dataset} & \textbf{Qwen2.5-it-cot-7B} & \textbf{Qwen2.5-it-cot-72B} & \textbf{Qwen2.5-it-maj-7B} & \textbf{C-MCTS-RULE} & \textbf{C-MCTS-PRM} \\
    \hline
    \textbf{Gaokao2023} & 66.0 & \textbf{73.2} & 69.0 & 70.3 & 72.9 \\
    \textbf{MATH-500} & 77.0 & 83.4 & 79.6 & 78.6 & \textbf{84} \\
    \textbf{AauA} & 74.4 & 79.2 & 81.1 & 83.0 & \textbf{85.8} \\
    \textbf{SVAMP} & 93.9 & 95.4 & 93.5 & 95.3 & \textbf{95.9} \\
    \textbf{GSM8K} & 92.4 & \textbf{95.8} & 93.0 & 93.3 & 94.8 \\
    \textbf{CMath} & 89.7 & 93.0 & 93.1 & 93.0 & \textbf{94.5} \\
    \textbf{School} & 70.2 & 83.1 & 82.1 & 80.1 & \textbf{86.1} \\
    \textbf{GaoKao-QA} & 60.9 & 74.3 & 68.6 & 72.6 & \textbf{78.0} \\
    \textbf{weak12k} & 85.6 & 91.3 & 90.0 & 89.4 & \textbf{92.3} \\
    \textbf{avg} & 78.9 & 85.4 & 83.3 & 83.9 & \textbf{87.1} \\
    \hline
  \end{tabular}
  \caption{\label{table:main}Experiment results on various datasets}
\end{table*}
\subsubsection{Process Reward Models}

Different from the human-like partial order rules, recent studies have shown that PRM (Process Reward Model) has significant advantages in the stage-wise evaluation of mathematical reasoning tasks. Therefore, we propose an alternative strategy: while retaining the basic rules, we discard other constraints and introduce PRM as the core component for Q-value evaluation.

Specifically, the Q-value calculation process is as follows: Given the state \( s_t \) and the candidate action \( a_{t+1} \), we concatenate the state, action, and Q-value prompt template and input them into the PRM model. By extracting the logits for the "positive" and "negative" labels and normalizing them using softmax, we obtain the positive probability as the Q-value:
\begin{equation}
\nonumber
Q(s_t, a_{t+1}) = \frac{e^{\text{PRM}([p_{Q}, s_t, a_{t+1}])_{\text{pos}}}}{\sum_{t\in\{\text{pos},\text{neg}\}}e^{\text{PRM}([p_{Q}, s_t, a_{t+1}])_t}}
\end{equation}
Here, \( p_Q \) is the prompt template dedicated to Q-value evaluation. The calculation of V-value follows a similar paradigm but is not dependent on specific actions:
\begin{equation}
\nonumber
V(s_t) = \frac{e^{\text{PRM}([p_{V}, s_t])_{\text{pos}}}}{\sum_{t\in\{\text{pos},\text{neg}\}}e^{\text{PRM}([p_{V}, s_t])_t}}
\end{equation}

%Subsequent experiments revealed that PRM outperforms manually defined partial-order rules. However, in the absence of a PRM model, manually defined rules perform better than using LLMs themselves as Q-value models. This indicates that relying solely on the intrinsic knowledge of LLMs is insufficient for evaluating complex mathematical processes. Instead, a PRM model specifically trained for mathematical logical reasoning is needed to more efficiently search for reasoning paths. We also found that for native MCTS, the PRM model does not bring performance improvements, suggesting that when the search space is too vast, even the PRM model cannot help the model efficiently search for paths.




\section{Experiments}

\subsection{Experimental Setup}

\textbf{Benchmark Datasets}: 
We evaluate the performance of the proposed method on several datasets for mathematical reasoning. 
Specifically, we consider the following datasets: GSM8K \cite{cobbe2021training}, Weak12K \cite{liang2023generalizing}, Math-500 \cite{lightman2023let}, AquA \cite{ling2017program}, CMath \cite{wei2023CMath}, as well as the GaoKao-Math-QA,Gaokao2023 and CN-Middle-School datasets used in Qwen2.5-Math \cite{yang2024qwen2b}. These datasets cover a range of mathematical problems from elementary to middle school levels, providing a comprehensive test of our algorithm's mathematical capabilities. To follow the zero-shot setting, all the datasets can be only accessed in the inference procedure. Please refer to Appendix \ref{sec:dataset} for the description of these datasets.

% Detailed information about these datasets will be introduced in Appendix \ref{sec:dataset}.

% \textbf{Baselines and Evaluation Metric}: To conduct experiments under controlled conditions, we selected Qwen2.5-it-cot-7B and Qwen2.5-it-cot-72B \cite{yang2024qwen2b} as our baseline models. 

\textbf{Baselines}: For a controllable and fair comparison, we consider the two different model variants of the latest baseline \citet{yang2024qwen2b}  with different size of parameters, Qwen2.5-it-cot-7B and Qwen2.5-it-cot-72B. 
To maximize the performance of these baselines, we use the scripts from the official Qwen open-source code for problem reasoning. Additionally, considering that our method involves a voting process, we add a voting experiment based on Qwen2.5-it-7B, denoted as Qwen2.5-it-maj-7B, for a fair comparison. It is important to note that Qwen2.5-it-cot-7B uses greedy search in its official implementation, which is not suitable for voting methods. 
Moreover, for fairness, we employ the same hyper-parameters of Qwen2.5-it-maj-7B, such as prompt, top-p, and top-k.

% Therefore, under the principle of variable-controlling, all relevant parameters of Qwen2.5-it-maj-7B, such as prompt, top-p, and top-k, are kept consistent with our method.





\subsection{Experimental Results}
To ensure the fairness of the experiments, our algorithm also uses Qwen2.5-it-7B as the base reasoning model. We refer to the two variants mentioned in Section \ref{sec:Strategy} as \textbf{C-MCTS-RULE} and \textbf{C-MCTS-PRM}, respectively. For C-MCTS-PRM, we use Qwen2.5-PRM-7B \cite{zhang2025lessons} as the model for Q-values and V-values. While for C-MCTS-RULE, we use the LLMs themselves as the Q-value model and the V-value model.And we keep the hyperparameters of both completely the same. More details on hyperparameters will be elaborated in Appendix \ref{sec:Hyperparameter}.

% The experimental results, as shown in Table \\ref{table:main}, reveal several key findings:

According to the experiment results as shown in Table \ref{table:main}, we can draw the conclusions from the following three perspectives:
\begin{itemize}[leftmargin=0cm, itemindent=0cm]
    \item \textbf{Performance on Par with Larger Models}: The C-MCTS-PRM method outperforms the baseline models on eight out of nine datasets, demonstrating that the long Chain-of-Thought (CoT) generated through our constrained reasoning can significantly mitigate model capacity limitations. This effect is particularly evident in the MATH-500 dataset (84.0\% vs. 83.4\%) and the GaoKao-QA dataset (78.0\% vs. 74.3\%). These results underscore the effectiveness of C-MCTS in constraining the action space and generating long CoT, allowing the 7B model to overcome size-related disadvantages by exploring the solution space more efficiently.
    \item \textbf{Enhanced Complex Reasoning Capability}: Our method achieves substantial performance improvement on challenging datasets like Gaokao2023. For instance, the proposed method delivers an absolute improvement of 6.6\% on the AauA dataset (85.8\% vs. 79.2\%) compared to the 72B model. This significant gain is driven by the Long CoTs generated by our approach, enabling a more thorough analysis and deeper understanding of the problem before arriving at an answer. Moreover, the reflection operation plays a crucial role in mitigating error accumulation and preventing misunderstandings.
    \item \textbf{Efficiency of Constrained Search}: Compared to the voting baseline (Qwen2.5-it-maj-7B), our method achieves a significant average accuracy improvement of 3.9\% (87.2\% vs. 83.3\%). This result highlights that merely increasing solution diversity through sampling is far less effective than our systematic exploration of the state space, guided by well-designed search strategies.
 \item \textbf{Consistent Performance Improvement}:Our method exhibits strong generalization across a wide range of problem types, from basic arithmetic (GSM8K: 94.8\%) to advanced mathematics (GaoKao2023: 72.9\%). Its consistent performance across datasets of varying difficulty—using the same action sets except for \(\mathcal{A}^{\text{summary}}\)—demonstrates its robustness and adaptability. Moreover, these results show that simple action-space constraints on LLMs can naturally induce high-quality Long CoTs without requiring dataset-specific prompt customization, allowing the model to fully harness its potential.
 \item \textbf{Effectiveness of Rule-based Constraints}: The C-MCTS-RULE variant surpasses the baseline models Qwen2.5-it-maj-7B and Qwen2.5-it-cot-7B, achieving an average accuracy of 83.9\%. This result suggests that even manually defined rule-based ordering can significantly enhance model performance. However, the performance gap between C-MCTS-RULE and C-MCTS-PRM (83.9\% vs. 87.1\%) underscores the superior efficiency of the PRM model in selecting actions compared to static, rule-based processes. For instance, on the MATH-500 dataset, C-MCTS-RULE achieves 78.6\%, whereas C-MCTS-PRM reaches 84.0\%, demonstrating that static rules and inherent model validation fall short of the dynamic validation capabilities of PRM, particularly in multi-step algebraic reasoning. This finding aligns with our framework’s design philosophy: while human-designed rules establish foundational reasoning patterns, the PRM model provides adaptive mathematical validation tailored to the specific problem context.







\end{itemize}


% 1. \textbf{Performance on Par with Larger Models}: The C-MCTS-PRM method achieves performance comparable to or better than the 72B baseline model on eight out of nine datasets. This clearly demonstrates that the long Chain-of-Thought (CoT) generated through our constrained reasoning can, to a significant extent, compensate for model capacity limitations. This is particularly evident on the MATH-500 dataset (84.0\% vs. 83.4\%), and the GaoKao-QA dataset (78.0\% vs. 74.3\%). These results highlight the effectiveness of using C-MCTS to constrain the action space and generate Long CoT, enabling the 7B model to overcome its size-related disadvantages through more efficient exploration of the solution space.

% 2. \textbf{Enhanced Complex Reasoning Capability}: Our method achieves significant performance gains on challenging datasets where the baseline model performs poorly. For example, our method achieves an absolute improvement of 6.6\% on the AquA dataset (85.8\% vs. 79.2\%) compared to the 72B model. This significant gain is attributed to the Long CoTs generated by our method, which allows for thorough analysis and understanding of the problem before providing an answer. In the meanwhile, the reflection operation helps prevent the accumulation of errors and misunderstandings. 
% %Therefore, the more difficult the dataset, the more significant the performance improvement. The 3.7\% improvement on the GaoKao-QA dataset further highlights its advantage in handling high-stakes exam problems that require rigorous reasoning. In contrast, on relatively simple datasets such as the elementary-level SVAMP and CMath datasets, our performance gain is less pronounced, with only a 0.5\% and 1.5\% improvement over the 72B model, respectively. This is because these datasets contain a large number of simple problems (with two or fewer steps), where Long CoTs does not offer a significant advantage over conventional CoT.

% 3. \textbf{Efficiency of Constrained Search}: Our method, when compared to the voting baseline (Qwen2.5-it-maj-7B), achieves a notable average accuracy improvement of 3.9\% (87.2\% vs. 83.3\%). This result clearly shows that simply increasing the diversity of solutions through sampling is far less effective than the systematic exploration of the state space guided by our search strategies. 

% 4. \textbf{Consistent Performance Improvement}: The method demonstrates robust generalization across different types of problems, ranging from basic arithmetic (GSM8K: 94.8\%) to advanced mathematics (GaoKao2023: 72.9\%). The consistency in performance across these diverse and varying difficulty datasets, with the same action sets except for \(\mathcal{A}^{\text{summary}}\), indicates the strong generalizability of our approach. It also shows that simple action-space constraints on LLMs can induce high-quality Long CoTs without the need for dataset-specific prompt customization, thereby fully leveraging their potential.

% 5.\textbf{Effectiveness of Rule-based Constraints}: The C-MCTS-RULE variant outperforms the baseline methods of Qwen2.5-it-maj-7B and Qwen2.5-it-cot-7B, achieving an average accuracy of 83.9\%. This indicates that even manually defined rule-based ordering can bring significant performance improvements to the model. However, the performance gap between C-MCTS-RULE and C-MCTS-PRM (83.9\% vs 87.1\%) highlights the superior efficiency of the specially trained PRM model in identifying actions compared to manually defined rule-based processes. For example, on the MATH-500 dataset, C-MCTS-RULE reaches 78.6\%, while C-MCTS-PRM achieves 84.0\%. This demonstrates that static rules and the model's own validation capabilities cannot match the dynamic validation capabilities of the PRM model in handling multi-step algebraic operations. This observation aligns with our framework's design philosophy—that while human-designed rules establish basic reasoning patterns, the PRM model provides the necessary mathematical validation adapted to the specific context of the problem.

In summary, these results confirm our core hypothesis: leveraging prior action spaces and process-aware reward constraints to guide the reasoning process of LLMs enables more effective Long CoTs generation. This approach facilitates a more structured exploration of solutions, surpassing the effectiveness of scale-driven capacity increases or conventional CoT methods.


\begin{table*}
  \small
  \renewcommand\arraystretch{1.1} 
  \centering
  \begin{tabular}{l|ccc|cc}
    \hline
    \textbf{Dataset} & \textbf{Native-MCTS} & \textbf{Native-MCTS+PRM} & \textbf{C-MCTS-wo-PRM} & \textbf{C-MCTS-RULE} & \textbf{C-MCTS-PRM} \\
    \hline
    \textbf{Gaokao2023} & 67.5 & 69.8 & 67.0 & 70.3 & \textbf{72.9} \\
    \textbf{MATH-500} & 75.0 & 77.2 & 76.8 & 78.6 & \textbf{84} \\
    \textbf{AauA} & 84.7 & 83.7 & 79.9 & 83.0 & \textbf{85.8} \\
    \textbf{SVAMP} & 93.0 & 93.5 & 95.5 & 95.3 & \textbf{95.9} \\
    \textbf{GSM8K} & 92.5 & 92.5 & 92.4 & 93.3 & \textbf{94.8} \\
    \textbf{CMath} & 93.0 & 93.3 & 92.1& 93.0 & \textbf{94.5} \\
    \textbf{School} & 83.1 & 83.1 & 75.2 & 80.1 & \textbf{86.1} \\
    \textbf{GaoKao-QA} & 72.0 & 72.6 & 69.8 & 72.6 & \textbf{78.0} \\
    \textbf{weak12k} & 89.5 & 89.3 & 87.0 & 89.4 & \textbf{92.3} \\
    \textbf{AVG} & 83.3 & 83.8 & 81.7 & 83.9 & \textbf{87.1} \\
    \hline
  \end{tabular}
  \caption{\label{table:Ablation}Ablation experiment results on various datasets}
\end{table*}


\subsection{Ablation Study}
To thoroughly investigate the effectiveness of our method, we conducted three ablation experiments based on the principle of controlled variables: (1) removing the PPM from C-MCTS (C-MCTS-wo-PRM), (2) replacing C-MCTS with the The ablation studies in Table \ref{table:Ablation} provide three key insights into the components of our framework: (1) evaluating the impact of the original MCTS algorithm while retaining PRM (Native-MCTS+PRM) and testing the standalone performance of the original MCTS (Native-MCTS), and (2) ensuring consistent hyperparameters across all experiments to isolate the effects of methodological changes. These studies evaluate the contributions of each component to the overall effectiveness of our approach as follows.
\begin{itemize}[leftmargin=0cm, itemindent=0cm]
    \item \textbf{Critical Role of PRM}: Comparing C-MCTS-PRM (87.1\%) with C-MCTS-wo-PRM (81.7\%), we observe a 5.4\% drop in average accuracy when process supervision is removed. This gap is even more pronounced on the MATH-500 dataset, widening to 7.2\% (84.0\% vs. 76.8\%), underscoring the critical role of PRM in validating complex reasoning. PRM’s step-by-step validation effectively mitigates error accumulation, a challenge that manually defined rules alone cannot fully address. Additionally, C-MCTS-RULE (83.9\%) surpasses C-MCTS-wo-PRM by 1.0\% in average accuracy, demonstrating that even simple rule-based constraints enhance reasoning consistency, though they still fall short of PRM-driven supervision. This further emphasizes the importance of constraining the generation process in LLMs—while strong PRM rules provide optimal guidance, even basic rule-based constraints contribute to improved performance.
    \item \textbf{Advantage of Constrained Action Space}: Despite sharing the PRM component with C-MCTS-PRM, Native-MCTS+PRM (83.8\%) lags behind C-MCTS-PRM by 3.3\%. Additionally, removing PRM results in a 5.4\% performance drop for C-MCTS, whereas Native-MCTS experiences only a 0.5\% decline. This disparity highlights a fundamental limitation of Native-MCTS—its reliance on unrestricted action generation prevents it from efficiently inducing Long CoTs. Even with the PRM component, Native-MCTS struggles to improve due to the excessively large search space. In contrast, C-MCTS-RULE achieves an accuracy of 83.9\%, outperforming Native-MCTS and reinforcing the effectiveness of predefined action constraints in enhancing reasoning quality. However, the 3.2\% gap between C-MCTS-RULE and C-MCTS-PRM underscores the necessity of integrating PRM-driven validation with action constraints to maximize performance.
    \item \textbf{Synergistic Effects}: The complete C-MCTS-PRM framework surpasses the cumulative improvements of its individual components (Native-MCTS: 83.3\%, Native-MCTS+PRM: 83.8\%, C-MCTS-wo-PRM: 81.7\%), achieving an overall accuracy of 87.1\%. This 5.4\% synergistic gain highlights the mutual reinforcement between constrained search and process supervision—by narrowing the action space, PRM validation is concentrated on high-potential reasoning patterns, leading to more effective solution exploration. Furthermore, Native-MCTS gains only 0.5\% in performance when combined with PRM, indicating that relying solely on LLM-generated actions is inefficient. Without constrained action spaces, the model cannot fully exploit PRM’s validation capabilities, reinforcing the importance of structured guidance in optimizing reasoning performance.
\end{itemize}


% 1. \textbf{Critical Role of PRM}: Comparing C-MCTS-PRM (87.2\%) with C-MCTS-wo-PRM (82.9\%), we find that the average accuracy drops by 4.3\% when process supervision is removed. On the MATH-500 dataset, this gap widens to 7.2\% (84.0\% vs 76.8\%), highlighting the crucial role of PRM in complex reasoning validation. The step-by-step validation provided by PRM effectively prevents the accumulation of errors, which cannot be fully addressed by manually defined rules. Notably, C-MCTS-RULE (83.9\%) outperforms C-MCTS-wo-PRM by 1.0\% in average accuracy, indicating that even basic rule-based constraints can improve reasoning consistency, though still not matching PRM-driven supervision. This also underscores the importance of constraining the generation process of LLMs ; even without strong PRM rules, simple rule-based constraints can enhance performance.

% 2. \textbf{Advantage of Constrained Action Space}: Despite sharing the PRM component with C-MCTS-PRM, Native-MCTS+PRM (83.8\%) underperforms C-MCTS-PRM by 3.4\%. We also observed that after removing the PRM, the performance of C-MCTS dropped by 5.4\%, while the performance of Native-MCTS only decreased by 0.5\% after removing the PRM. This difference can be attributed to the fact that Native-MCTS relies on unrestricted action generation, which prevents the model from efficiently inducing Long CoTs. Even when the PRM component is added to Native-MCTS, its performance cannot be effectively improved due to the excessively large search space. In contrast, C-MCTS-RULE achieves an accuracy of 83.9\%, surpassing Native-MCTS, thus validating that predefined action constraints significantly improve reasoning quality. However, the 3.2\% gap between C-MCTS-RULE and C-MCTS-PRM emphasizes the necessity of combining PRM-driven validation with action constraints to achieve optimal performance.


% 3. \textbf{Synergistic Effects}: The complete C-MCTS-PRM framework outperforms the sum of individual component improvements (Native-MCTS: 83.3\% vs Native-MCTS+PRM: 83.8\% vs C-MCTS-wo-PRM: 82.9\%), achieving an overall accuracy of 87.2\%. This 3.4\% synergistic gain confirms that constrained search and process supervision reinforce each other—the constrained action space focuses PRM validation on high-potential reasoning patterns. Similarly, we find that Native-MCTS only achieves a 0.5\% performance gain when combined with PRM. This suggests that relying solely on actions generated by the LLMs is inefficient and cannot fully leverage the validation power of PRM.


\section{Conclusion}
In this paper, we propose Constrained Monte Carlo Tree Search (C-MCTS) as a method for generating reliable Long Chain-of-Thoughts (CoTs). By effectively constraining the action space and guiding the search strategy, C-MCTS reduces the complexity introduced by an expansive reasoning space, enabling more efficient and structured exploration. Experimental results on mathematical reasoning benchmarks demonstrate its effectiveness in enhancing the reasoning capabilities of smaller models, particularly in resource-constrained environments, where computational efficiency is crucial. These findings highlight the significant potential of C-MCTS as a robust framework for improving structured reasoning in LLMs.

\section*{Limitations}
Despite the significant improvements achieved by our method, the design of our action set remains imperfect. We have not fully explored the completeness of the action space, nor have we attempted to incorporate additional domain-specific knowledge into our action set. In the future, we will build on this foundation to further refine our work and enhance the adaptability and scalability of our framework.
\bibliography{custom}

\clearpage

\appendix


\section{Implementation Details of Code}  
We refer to existing studies \cite{li2024rethinkMCTS,yao2022react} and build an automated unit testing module \( G_{\text{test}} \) based on the Python interpreter. For a generated candidate state \( s_t^0 \), we use \( G_{\text{test}} \) to check whether the code can run successfully. If it can, we accept the code; if not, \( G_{\text{test}} \) will automatically generate an error report \( \text{report} \). Subsequently, based on the existing \( s_t^0 \) and \( \text{report} \), we rethink and generate a new code until \( G_{\text{test}} \) no longer reports errors or the maximum number of iterations is reached.  

If the problem remains unresolved after reaching the maximum number of iterations, an error log will be recorded in the state \( s_t^0 \); if the problem is resolved, the execution result will be recorded in \( s_t^0 \).


\section{Dataset}
\label{sec:dataset}

\textbf{GSM8K}: GSM8K is a dataset containing 8.5K high-quality, linguistically diverse elementary school math problems. Its main features include: Each problem requires multi-step reasoning involving basic arithmetic operations.  The problems are challenging enough to test the limits of models but not so difficult as to be unmanageable. In our work, we only use their test set.

\textbf{Weak12K}: Weak12K is a new Chinese math dataset containing 12,117 math problems. Compared to the commonly used Math23k dataset, problems in Weak12K are more complex and difficult. We extracted one thousand problems as the test set.

\textbf{Math-500}: The Math-500 dataset contains 500 math problems and is a subset of the benchmark MATH dataset. It covers a wide range of mathematical fields and difficulty levels, including algebra, geometry, probability, and more. Each problem is carefully selected to ensure its completeness in mathematical logic and solution steps. We use the entire dataset as the test set.

\textbf{AquA}: This dataset, constructed by the DeepMind team, aims to support research in math problem solving, especially for tasks that guide program induction through solution generation. Problems in the dataset cover a variety of mathematical topics and difficulty levels, with each problem including a problem description, options, solution process, and correct answer label. It is a multiple-choice dataset with a test set size of approximately 200.

\textbf{CMath}: A primary school level dataset mainly for Chinese students, with 600 data points in the test set.

\textbf{GaoKao-Math-QA}: Composed of multiple-choice questions from real Chinese college entrance exam math problems over the years. The dataset covers knowledge points such as sequences, sets, complex numbers, and is a high-difficulty dataset. The test set contains 351 questions.Abbreviated as "GaoKao-QA".

\textbf{CN-Middle-School}: A simplified version of the GaoKao-Math-QA dataset, mainly consisting of junior high school knowledge points in China, including equations, geometry, inequalities, and other knowledge points. Unlike the GaoKao-Math-QA dataset, this one is not a multiple-choice dataset.Abbreviated as "School".
\textbf{Gaokao2023}: This is the English version of the 2023 Chinese College Entrance Examination (GaoKao) questions. It includes multiple-choice questions, multiple-answer questions, fill-in-the-blank questions, and extended-response questions. The content covers sequences, ellipses, probability theory, and more. The difficulty level is extremely high, and it is challenging for an average Chinese high school student to achieve a passing score. It has been widely used in numerous studies to test performance \cite{chen2024step,chen2024alphamath,yang2024qwen2b}. We refer to it as GaoKao2023 for short.

\section{Hyperparameter Setting}
\label{sec:Hyperparameter}
In our experiments, we strive to maintain consistent hyperparameter settings across different datasets. For example, when transitioning to a new state, we require the LLMs to generate no more than 512 tokens and produce 3 candidate states each time, for all datasets. More detailed hyperparameter settings are documented in Table \ref{tab:hyperparameters}.

It is important to note that in the MCTS iterations, our method generates up to \( k \) subtrees at a time. We have observed that more subtrees do not necessarily lead to better reasoning outcomes. Therefore, we incrementally increase the number of subtrees until reaching the maximum value specified by the “Max Subtree Number” hyperparameter listed in Table \ref{tab:hyperparameters}. Finally, we select the optimal result from the “Maximum Number of Subtrees” results as the final outcome.

\begin{table*}
  \centering
  \begin{tabular}{lccccc}
    \hline
    \textbf{Dataset} & \textbf{Max Subtree Number} & \textbf{Temperature} & \textbf{Top-p} & \textbf{Top-k} & \textbf{Depth\_limit} \\
    \hline
    \textbf{GSM8K} & 7 & 1 & 0.5 & 32 & 8 \\
    \textbf{MATH-500} & 9 & 1 & 0.5 & 32 & 7 \\
    \textbf{GaoKao-QA} & 7 & 1 & 0.8 & 32 & 8 \\
    \textbf{Weak12k} & 7 & 1.05 & 0.7 & 50 & 8 \\
    \textbf{CMath} & 9 & 1.05 & 0.7 & 50 & 8 \\
    \textbf{SVAMP} & 5 & 1 & 0.5 & 32 & 8 \\
    \textbf{AauA} & 9 & 1 & 0.7 & 32 & 8 \\
    \textbf{School} & 9 & 1 & 0.8 & 32 & 8 \\
    \textbf{GaoKao2023} & 7 & 1 & 0.8 & 32 & 7 \\
    \hline
  \end{tabular}
  \caption{Comparison of Hyperparameters in the Main Experiment}
  \label{tab:hyperparameters}
\end{table*}


\section{Prompt}
Our work only requires simple prompts to function effectively. Below, I will list the prompts used in our method, which are universal across all datasets with at most minor differences of just a few words between datasets.

\begin{prompt}[title={Prompt \thetcbcounter: Instruction}, label=prompt:Instruction]
Below is a mathematical problem. Please think step by step and solve it. Enclose each thought process with the <think> and </think> symbols. The thought process should be as rich and detailed as possible, delving into every content and detail deeply, rather than just skimming over,
After you feel that the thought process is sufficient to solve the problem, organize your thought process into a complete answer, and write the final answer in boxed{{}}.\\
\end{prompt}


\begin{prompt}[title={Prompt \thetcbcounter: understand action}, label=prompt:understandaction]
We need to think step by step to understand the meaning of the problem\\
Let's consider if there are any ambiguities in the problem statement\\
This problem is quite difficult; we should first analyze what knowledge points it involves, what mathematical formulas and related properties it utilizes\\
\end{prompt}
\begin{prompt}[title={Prompt \thetcbcounter: plan action}, label=prompt:planaction]
We need to plan what our goal is in solving this, and what conditions and corresponding sub-problems we may need to satisfy first\\
We can try to give a preliminary solution first\\
The problem can be divided into several parts, with several questions that need to be addressed\\
\end{prompt}
\begin{prompt}[title={Prompt \thetcbcounter: code action}, label=prompt:codeaction]
We can write a piece of code to validate our idea\\
We can write a piece of code to assist with the calculation\\
We can write a piece of code to check our calculation results\\
\end{prompt}
\begin{prompt}[title={Prompt \thetcbcounter: reflect action}, label=prompt:reflectaction]
We should step by step check if the above process is reasonable and correct\
There may be errors and inaccuracies in the above questions; we need to step by step check for any mistakes\\
We need to combine the above thought process to see if it aligns with the problem's intention\\
There are some details in the problem that were not considered clearly; we need to check them\\
\end{prompt}

\begin{prompt}[title={Prompt \thetcbcounter: summary action}, label=prompt:summaryaction]
Based on the above thought process, we have solved this problem. Now, let's organize our thoughts into a complete answer and write the final answer in boxed{}. If there are multiple questions, I will answer each one in turn, separated by commas.\\
Now, let's compile our thought process into a complete answer and place the final answer in boxed{}. If there are multiple questions, I will answer each one in turn, separated by commas.\\
\end{prompt}

However, it is important to note that for multiple-choice questions, the corresponding instruction and summary action will be slightly different. We need to encourage the LLMs to directly select from the options rather than just providing the calculation result. For specific prompt templates, refer to Prompt \ref{prompt:mul-Instruction} and Prompt \ref{prompt:mul-summary}.


\begin{prompt}[title={Prompt \thetcbcounter: Multiple-choice question Instruction}, label=prompt:mul-Instruction]
You will be given a mathematics problem. Please think through and solve it step by step. Enclose each thought process with <think> and </think> tags. Make your thought process as rich and detailed as possible, deeply considering every content and detail, rather than briefly skimming over them.
Once you believe the thought process is sufficient to solve the problem, organize your thoughts into a complete answer, and choose the option from “A”, “B”, “C”, “D”, “E” that is closest to your answer. Write the final answer in boxed{{}}.

\end{prompt}




\begin{prompt}[title={Prompt \thetcbcounter:Multiple-choice question summary action}, label=prompt:mul-summary]
Based on the above thought process, we have solved this problem. First, we will recall the problem, then organize our thought process and write down the final solution, and finally choose the closest answer from “A”, “B”, “C”, “D”, “E”. The final answer will be written in boxed{{}}.\\
Now, let's first recall the problem. Then, we will organize our thought process into a complete answer, and finally choose the closest answer from “A”, “B”, “C”, “D”, “E”. The final answer will be written in boxed{{}}.\\
\end{prompt}
\end{document}
