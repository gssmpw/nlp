\section{Related Work}
In recent years, LLMs have become the focus of the academic community, demonstrating extremely strong performance. For example, the GPT series ____, the LLaMA series ____, the Gemma series ____, the RWKV series ____, and the DeepSeek series ____.
Among the capabilities of LLMs, mathematical reasoning has attracted the most attention. In recent years, related research can be broadly categorized into two types: one based on tree search algorithms and the other focused on the training approach to enhance the model's reasoning capabilities.

\textbf{The tree search-based approaches}: Notable works include MCTS ____ and TOT ____. These studies significantly enhanced reasoning performance by constraining the reasoning process of LLMs. However, since these methods rely on LLMs to generate actions, they fail to produce actions that can induce Long CoTs for LLMs lacking this capability.  
Some works have attempted to integrate PRM (Process Reward Model) to optimize the MCTS process. For example, Alpha-Zero-like methods train PRM models using reinforcement learning ____; HiAR-ICL ____ uses PRM models to filter answers; R-STAR ____ employs two LLMs as PRM models for answer selection; and AlphaMath ____ uses LLMs as policy models while co-training a value model as the PRM. While these efforts have yielded promising results, they have not effectively harnessed PRM to constrain the generation process of LLMs, thereby failing to produce high-quality Long CoT.


\textbf{The training-based approaches}: The typical methods usually employ the reinforcement learning to align with human preferences ____; employ mathematics-specific models with mathematical knowledge ____; and reward the generation of Long CoTs through reinforcement learning ____. These methods have achieved impressive results, but the cost of training LLMs is usually high. In contrast, our method effectively enhances performance by constraining the action space of LLMs using MCTS, without requiring additional training.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only