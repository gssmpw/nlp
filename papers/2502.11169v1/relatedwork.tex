\section{Related Work}
In recent years, LLMs have become the focus of the academic community, demonstrating extremely strong performance. For example, the GPT series \cite{brown2020language,chen2021evaluating,nakano2021webgpt,achiam2023gpt}, the LLaMA series \cite{touvron2023llama,taori2023alpaca,young2024yi,dubey2024llama}, the Gemma series \cite{team2024gemma_a,team2024gemma_b}, the RWKV series \cite{peng2024eagle,peng2023rwkv}, and the DeepSeek series \cite{bi2024deepseek,liu2024deepseek,guo2025deepseek}.
Among the capabilities of LLMs, mathematical reasoning has attracted the most attention. In recent years, related research can be broadly categorized into two types: one based on tree search algorithms and the other focused on the training approach to enhance the model's reasoning capabilities.

\textbf{The tree search-based approaches}: Notable works include MCTS \cite{hao2023reasoning} and TOT \cite{yao2023tree}. These studies significantly enhanced reasoning performance by constraining the reasoning process of LLMs. However, since these methods rely on LLMs to generate actions, they fail to produce actions that can induce Long CoTs for LLMs lacking this capability.  
Some works have attempted to integrate PRM (Process Reward Model) to optimize the MCTS process. For example, Alpha-Zero-like methods train PRM models using reinforcement learning \cite{feng2023alphazero}; HiAR-ICL \cite{wu2024beyond} uses PRM models to filter answers; R-STAR \cite{qi2024mutual} employs two LLMs as PRM models for answer selection; and AlphaMath \cite{chen2024alphamath} uses LLMs as policy models while co-training a value model as the PRM. While these efforts have yielded promising results, they have not effectively harnessed PRM to constrain the generation process of LLMs, thereby failing to produce high-quality Long CoT.


\textbf{The training-based approaches}: The typical methods usually employ the reinforcement learning to align with human preferences \cite{li2023reinforcement,ouyang2022training,rafailov2024direct,ethayarajh2024kto}; employ mathematics-specific models with mathematical knowledge \cite{shao2024deepseekmath,yang2024qwen2b,ying2024internlm}; and reward the generation of Long CoTs through reinforcement learning \cite{guo2025deepseek,zhong2024evaluation,team2024qwq,team2025kimi}. These methods have achieved impressive results, but the cost of training LLMs is usually high. In contrast, our method effectively enhances performance by constraining the action space of LLMs using MCTS, without requiring additional training.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only