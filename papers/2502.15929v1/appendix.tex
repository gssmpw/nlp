\section{Appendix}

\subsection{Omitted Proofs From Upper Bound}
\label{subsec:appendix_upper}

\normDiffMonotonicity*
\begin{proof}
By the law of cosines,
        \begin{align*}
            \|y-e_1\|_2 =&\ \sqrt{\|y\|_2^2 + \|e_1\|_2^2 - 2\|y\|_2\|e_1\|_2\cos(\theta)} \\
            =&\ \sqrt{r^2 + 1 - 2r\cos(\theta)},
        \end{align*}
    so as $y$ moves clockwise through $S_{r,0} \cap H$ from $-e_1$ to $e_1$, $\theta$ decreases from $\pi$ to $0$, $\cos(\theta)$ grows from $-1$ to 1, and $\|y-e_1\|_2$ shrinks from $r+1$ to $|r-1|$. Since $\|y\|_2 = r$ remains constant, $\|y-e_1\|_2 - \|y\|_2$ decreases as $\theta$ decreases. The same conclusion holds if we choose $y$ in the lower half plane and consider the analogous counterclockwise angle.
\end{proof}

\expressionOfh*
\begin{proof}
    \arxiv{\begin{equation*}
        \|p - e_1\|_2  = \sqrt{(r+1-h(r))^2 + 2h(r)r-h(r)^2} = \sqrt{(r+1)^2 - 2h(r)}
    \end{equation*}}
    \narxiv{\begin{align*}
        \|p - e_1\|_2  =& \sqrt{(r+1-h(r))^2 + 2h(r)r-h(r)^2} \\
        =&\ \sqrt{(r+1)^2 - 2h(r)}
    \end{align*}}
    and
    \begin{flalign*}
        \eps =& \frac{1}{\sigma}(\|p - e_1\|_2 - \|p\|_2) \\
        \eps =& \frac{1}{\sigma}(\sqrt{(r+1)^2 - 2h(r)} - r) \\
        (\sigma\eps + r)^2 =& (r+1)^2 - 2h(r) \\
        2h(r) =& 2r + 1 -(\sigma\eps)^2 - 2r\sigma\eps \\
        h(r) =& r(1-\eps\sigma) + \frac{1-\eps^2\sigma^2}{2}
    \end{flalign*}
\end{proof}

\radiusRangeForValidh*
\begin{proof}
The upper constraint of $r(1-\eps \sigma)  + \frac{1 - \eps^2\sigma^2}{2} \leq 2r$ is equivalent to $r \geq \frac{1-\eps\sigma}{2}$ as follows
    \begin{flalign*}
    r(1-\eps \sigma)  + \frac{1 - \eps^2\sigma^2}{2} &\leq 2r \\
    \frac{1 - \eps^2\sigma^2}{2} &\leq r(1 + \eps\sigma) \\
    \frac{1-\eps\sigma}{2} &\leq r
    \end{flalign*}
    The lower constraint of $r(1-\eps \sigma)  + \frac{1 - \eps^2\sigma^2}{2} \geq 0$ is satisfied for any $r$ since $1-\eps \sigma \geq 0$ implies
    \begin{equation*}
        r(1-\eps \sigma) + \frac{1- \eps^2 \sigma^2}{2} = (1-\eps \sigma)\left(r + \frac{1+\eps \sigma}{2}\right) \geq 0
    \end{equation*}
\end{proof}

\rBound*
\begin{proof}
    The density for $Y$ is $f(y) \propto \exp\left(-\|y\|_2/\sigma\right)$, so we compute the distribution's normalization factor $Z$. We use two facts. First, a $(d-1)$-sphere, i.e., a sphere in $\mathbb{R}^d$, with radius $s$ has surface area $\frac{2\pi^{d/2}}{\Gamma(d/2)} \cdot s^{d-1}$. Second, by $u$-substitution with $u=s/\sigma$, 
    \begin{equation*}
        \int_0^\infty e^{-s/\sigma}s^{d-1}ds = \int_0^\infty e^{-u} \cdot u^{d-1}\sigma^d du = \Gamma(d)\sigma^d
    \end{equation*}
    since $\Gamma(z) = \int_0^\infty e^{-t}t^{z-1}dt$. We compute the integral $Z$ using hyperspherical coordinates $s, \theta_1,...,\theta_{d-1}$ where $s \geq 0$, $\theta_1 \in [0, 2\pi]$, and $\theta_j \in [0, \pi]$ for $2 \leq j \leq d-1$. Let
    \begin{equation*}
        V(\theta_{1},...,\theta_{d-1}) = \sin^{d-2}(\theta_1)\sin^{d-3}(\theta_2)...\sin(\theta_{d-1})
    \end{equation*}
    be the angle dependent terms of the hyperspherical volume element. Then
    \begin{align*}
        Z &= \int_{0}^{\infty}\int_{0}^{2\pi}\int_{0}^{\pi}...\int_{0}^{\pi}e^{-s/\sigma}s^{d-1}V(\theta_{1},...,\theta_{d-1})\partial_{d-1} \ldots \partial_1 \partial s \\
        &= \frac{2\pi^{d/2}}{\Gamma(d/2)} \int_0^\infty e^{-s/\sigma}s^{d-1}\partial s \\
        &= \frac{2\pi^{d/2}\sigma^d}{\Gamma(d/2)} \cdot \Gamma(d).
    \end{align*}
    This gives
    \begin{align*}
        \P{Y}{\|y\| \leq r} =&\ \frac{1}{Z} \cdot \frac{2\pi^{d/2}}{\Gamma(d/2)} \int_0^r e^{-s/\sigma} s^{d-1}ds \\
        =&\ \frac{1}{Z} \cdot \frac{2\pi^{d/2}\sigma^d}{\Gamma(d/2)} \cdot \gamma(d, r/\sigma) \\
        =&\ \frac{\gamma(d, r/\sigma)}{\Gamma(d)}.
    \end{align*}
\end{proof}

\narxiv{\FMonotonic*
\begin{proof}
    Shorthand $\tau = \eps \sigma$. By \Cref{cor:small_r_high_loss}, $F_{r,h(r)} = 1$ for $r \leq \frac{1-\tau}{2}$. Suppose $r > \frac{1-\tau}{2}$. Then by \Cref{lem:loss_cap}, $h(r) = r(1-\tau) + \frac{1-\tau^2}{2}$.
    
    \underline{Case 1}: $r < \frac{1-\tau^2}{2\tau}$. Then $h(r) > r$, and $F_{r,h(r)} = 1 - F_{r,2r-h(r)}$. Since we want to prove that $F_{r,h(r)}$ decreases with $r$, it suffices to show that $F_{r,2r-h(r)}$ increases with $r$. By \Cref{lem:cap_fraction},
    \begin{equation*}
        F_{r,2r-h(r)} = \frac{1}{2}I_{(2r[2r-h(r)] - [2r-h(r)]^2)/r^2}\left(\frac{d-1}{2}, \frac{1}{2}\right).
    \end{equation*}
    We expand the subscript for $I$
     \begin{align}
        \frac{2r(2r-h(r)) - (2r-h(r))^2}{r^2} =&\ \frac{4r^2 - 2rh(r) - (4r^2 - 4rh(r) + h(r)^2)}{r^2} \nonumber \\
        =&\ \frac{2rh(r) - h(r)^2}{r^2} \label{eq:h_middle}.
    \end{align}
    Since $h(r) \in [0,2r]$ (\Cref{lem:loss_cap}), $2rh(r) - h(r)^2 \geq 0$. Because $I_x(a, b)$ increases with $x$ for $x \geq 0$, it is enough to show that $[2rh(r) - h(r)^2]/r^2$ increases with $r$. Expanding yields
    \begin{equation*}
        \frac{2rh(r) - h(r)^2}{r^2} = \frac{2r(1-\tau) + (1-\tau^2)}{r} - \frac{r^2(1-\tau)^2 + r(1-\tau)(1-\tau^2) + \frac{(1-\tau^2)^2}{4}}{r^2}.
    \end{equation*}
    We drop terms that don't depend on $r$ to get
    \begin{equation*}
        \frac{1-\tau^2}{r} - \frac{(1-\tau)(1-\tau^2)}{r} - \frac{(1-\tau^2)^2}{4r^2} = (1-\tau^2)\left[\frac{\tau}{r} - \frac{(1-\tau^2)}{4r^2}\right].
    \end{equation*}
    Differentiating the second term with respect to $r$ gives $\frac{1-2\tau r-\tau^2}{2r^3}$, and this is positive exactly when $r < \frac{1-\tau^2}{2\tau}$.
    
    \underline{Case 2}: $r \geq \frac{1-\tau^2}{2\tau}$. Then $h(r) \leq r$, and
    \begin{equation*}
        F_{r,h(r)} = \frac{1}{2}I_{(2rh(r)-h(r)^2)/r^2}\left(\frac{d-1}{2}, \frac{1}{2}\right).
    \end{equation*}
    By similar logic, it suffices to show that $(2rh(r)-h(r)^2)/r^2$ is nonincreasing in $r$ for $r \geq \frac{1-\tau^2}{2\tau}$. This follows from the analysis of the previous case.
\end{proof}}

\subsection{Omitted Proofs From Lower Bound}
\label{subsec:appendix_lower}
\lowerCap*
\narxiv{\begin{proof}
    Shorthand $\tau = \eps \sigma$ for neatness. To verify the claim, we start with an arbitrary $y \in S_{R,1}$ and attempt to determine a cutoff $X \in \mathbb{R}$ such that $y \in V$ iff $y_1 \leq X$. For any two points $y,y' \in S_{R,1}$ such that $y_1 = y_{1}'$, it is true that $y \in V$ iff $y' \in V$ since $V$ is spherically symmetric around $e_1$. If $y' = y_{1}e_{1} + v$ for some $v$ orthogonal to $e_1$, then by $S_{R,1}$'s spherical symmetry around $e_1$, the point $y = y_{1}e_{1} + |v|e_{2}$ is also in $S_{R,1}$. Therefore, our goal is to find the minimum cutoff $X$ for the point $y = (y_1, y_2, 0,...,0)$ such that $y \in V$ iff $y_{1} \leq X$.
    
    We know $y \in S_{r',0}$ for some $r' > 0$. Since $y_1^2 + y_2^2 = r'^2$, and $y \in S_{R,1}$ implies $(y_1-1)^2 + y_2^2 = R^2$, then combining these yields $r' = \sqrt{R^2+2y_1-1}$. Thus we have $y \in V$ if and only if $y_1 \leq -r' + h(r')$. By \Cref{lem:loss_cap}, $-r' + h(r') = \min(-\tau r' + \frac{1-\tau^2}{2}, 2r')$. We have $-\tau r' + \frac{1-\tau^2}{2} = -\tau\sqrt{R^2+2y_1-1} + \frac{1-\tau^2}{2}$,
    so we solve for the largest $X$ where $X \leq  \min(-\tau\sqrt{R^2+2X-1} + \frac{1-\tau^2}{2}, 2\sqrt{R^2+2X-1})$.
    
    Solving for $X$ under the first constraint yields 
    \begin{align}
        \left(X - \frac{1-\tau^2}{2}\right)^2 \geq&\ \tau^2(R^2+2X-1) \nonumber \\
        X^2 - X(1+\tau^2) + \frac{\tau^4-2\tau^2+1 - 4\tau^2R^2 + 4\tau^2}{4} \geq&\ 0 \nonumber \\
        X^2 - X(1+\tau^2) + \frac{\tau^4+2\tau^2+1 - 4\tau^2R^2}{4} \geq&\ 0 \label{eq:X_inequality}.
    \end{align}
    The roots of the LHS are given by 
    \begin{equation*}
        X =\frac{1 + \tau^2 \pm \sqrt{(1+\tau^2)^2 - ([1+\tau^2]^2 - 4\tau^2R^2)}}{2} \\
        =\frac{1 + \tau^2 \pm 2\tau R}{2}.
    \end{equation*}
    Let $x_1 = \frac{1+\tau^2 -2\tau R}{2}$ and $x_2 = \frac{1+\tau^2 + 2\tau R}{2}$. As the LHS of \Cref{eq:X_inequality} is a convex parabola, the inequality is satisfied on the intervals $(-\infty, x_1] \cup [x_2, \infty)$. But the first constraint on $X$ also implies the weaker inequality $X < \frac{1-\tau^2}{2}$ so $X \notin [x_2, \infty)$. Then $x_1$ is the largest value that satisfies the first constraint.
    
    For any $X \in (x_1, x_2)$, we have $X > -\tau\sqrt{R^2+2X-1} + \frac{1-\tau^2}{2} \geq \min(-\tau\sqrt{R^2+2X-1} + \frac{1-\tau^2}{2}, 2\sqrt{R^2+2X-1})$. So if we can show that $x_1 \leq 2\sqrt{R^2+2x_{1}-1}$, then $x_1$ will indeed be the desired cutoff. We actually prove a stronger inequality
    \begin{align*}
        x_1 \leq&\ \sqrt{R^2+2x_{1}-1} \\
        \frac{1+\tau^2 - 2\tau R}{2} \leq&\ R - \tau \\
        (1+\tau)^2 \leq&\ 2R(1 + \tau) \\
        \frac{1+\tau}{2} \leq&\ R
    \end{align*}
    which follows from our starting assumption on $R$. So $X = \frac{1+\tau^2 -2\tau R}{2}$ is the desired cutoff. This leads to a cap on $S_{R,1}$ of height
    \begin{equation*}
        H(R) = X-(1-R) = \frac{1+\tau^2 -2\tau R}{2} - 1 + R = R(1-\tau) - \frac{1-\tau^2}{2}.
    \end{equation*}
    The last step is verifying that this is a valid height lying in $[0,2R]$. The lower bound follows from $R(1-\tau) \geq \frac{1-\tau^2}{2}$ rearranging into the starting assumption $R \geq \frac{1+\tau}{2}$. We prove a stronger upper bound of $R$ by rearranging
    \begin{align*}
        R(1-\tau) - \frac{1-\tau^2}{2} \leq&\ R \\
        -\frac{1-\tau^2}{2} \leq& \tau R
    \end{align*}
    which uses $0 < \tau <1$ and $R > 0$.
\end{proof}}

\subsection{Omitted Proofs From Sampler}
\label{subsec:appendix_sampler}
\gammaSample*
\begin{proof}
    We first show that $-\log(U(0,1)) \sim \expo{1}$, an exponential random variable. Let $f$ be the CDF of $\expo{1}$. Then $\mathbb{P}[f^{-1}(U) \leq t] = \mathbb{P}[U \leq f(t)] = f(t)$ so $f^{-1}(U) \sim \expo{1}$. Since $f^{-1}(t) = -\log(1-t)$ for $0 \leq t \leq 1$, and $U \sim (1 - U)$, we get $-\log(U) \sim \expo{1}$. 

    Note that $\expo{1}$ corresponds to a random variable that measures the time required for the first arrival from a Poisson process with rate 1. Moreover, $\gammad{d+1}{\sigma} \sim \sigma\gammad{d+1}{1}$ and $\gammad{d+1}{1}$ corresponds to a random variable that measures the time of the $(d+1)$th arrival of a Poisson process with rate 1. The random variable of the $(d+1)$th arrival is equal to the sum of the random variables of interarrival times for the first $(d+1)$ arrivals. Since a Poisson process has stationary increments, each of these interarrival times are i.i.d. as $\expo{1}$. It follows that $\gammad{d+1}{1} \sim \sum_{i=1}^{d+1}E_{i} \sim -\sum_{i=1}^{d+1}\log(U_i)$ where $E_{i} \sim \expo{1}$.
\end{proof}

\ballSample*
\begin{proof}
    The term $\frac{(X_1, \ldots, X_d)}{\sqrt{\sum_{i=1}^d X_i^2}}$ is a  normalized draw from a $d$-dimensional multivariate Gaussian with an identity covariance matrix. As this distribution is spherically symmetric, normalizing the draw to have unit length produces a uniform draw from the unit sphere. Define the function $f$ to be the CDF of the random variable of the $\ell_2$ norm of a uniform sample from $B_{2}^{d}$. Then $f(r) = r^{d}$. We show that $f$ is also the CDF of $Y^{1/d}$. We have $\mathbb{P}[Y^{1/d} \leq r] = \mathbb{P}[U(0,1)^{1/d} \leq r] = \mathbb{P}[U(0,1) \leq r^{d}] = r^{d}$, and the lemma follows.
\end{proof}

\subsection{Omitted Proofs From Experiments}
\label{subsec:appendix_experiments}
The following result about the expected squared $\ell_2$ norm of $\ell_p$ balls will be useful.

\begin{lemma}[\cite{JRY25}]
\label{lem:expected_squared_norm}
    Let $\mathbb{E}_{2}^{2}(X)$ denote the expected squared $\ell_2$ norm of a uniform sample from $X$, and let $rB_p^d$ denote the $d$-dimensional $\ell_p$ ball of radius $r$. Then $\mathbb{E}_{2}^{2}(rB_{p}^{d}) = r^2 \cdot \frac{d}{3}\left(\frac{3d}{d+2}\right)\left(\frac{\Gamma(\frac{d}{p}) \Gamma(\frac{3}{p})}{\Gamma(\frac{1}{p}) \Gamma(\frac{d+2}{p})}\right)$.
\end{lemma}

\expectedKNorm*
\begin{proof}
    Consider the mechanism releasing a noisy version of $T(X) = 0$. Call this mechanism $M_\sigma^p$. Recall from \Cref{lem:k_norm} that we can sample it by sampling $r \sim \gammad{d+1}{\sigma}$, sampling $z \sim B_p^d$, and outputting $rz$. The distribution $\gammad{d+1}{\sigma}$ has density
    \begin{equation}
    \label{eq:gamma_density}
        f(x) = \frac{x^de^{-x/\sigma}}{\Gamma(d+1)\sigma^{d+1}}.
    \end{equation}
    so
    \begin{align*}
        \E{y \sim M_\sigma^p}{\|y\|_2^2} =& \int_0^\infty f(r) \mathbb{E}_2^2(rB_p^d) dr \\
        =&\ \int_0^\infty \frac{r^de^{-r/\sigma}}{\Gamma(d+1)\sigma^{d+1}} r^2 \cdot \frac{d}{3}\left(\frac{3d}{d+2}\right)\left(\frac{\Gamma(\frac{d}{p}) \Gamma(\frac{3}{p})}{\Gamma(\frac{1}{p}) \Gamma(\frac{d+2}{p})}\right) dr \\
        =&\  \frac{d}{3\Gamma(d+1)\sigma^{d+1}}\left(\frac{3d}{d+2}\right)\left(\frac{\Gamma(\frac{d}{p}) \Gamma(\frac{3}{p})}{\Gamma(\frac{1}{p}) \Gamma(\frac{d+2}{p})}\right) \int_0^\infty r^{d+2}e^{-r/\sigma}dr \\
        =&\  \frac{d}{3\Gamma(d+1)\sigma^{d+1}}\left(\frac{3d}{d+2}\right)\left(\frac{\Gamma(\frac{d}{p}) \Gamma(\frac{3}{p})}{\Gamma(\frac{1}{p}) \Gamma(\frac{d+2}{p})}\right) \cdot \Gamma(d+3)\sigma^{d+3} \\
        =&\  (d\sigma)^2(d+1)\left(\frac{\Gamma(\frac{d}{p}) \Gamma(\frac{3}{p})}{\Gamma(\frac{1}{p}) \Gamma(\frac{d+2}{p})}\right).
    \end{align*}
\end{proof}

\gaussianExpectedSquaredNorm*
\begin{proof}
    Denote the mechanism by $N_\sigma$. Then by linearity of expectation and the fact that the Gaussian mechanism has independent Gaussian marginals,
    \begin{align*}
        \E{y \sim N_\sigma}{\|y\|_2^2
        } =&\ \E{}{\sum_{j=1}^d y_j^2} \\
        =&\ d\E{z \sim N(0, \sigma^2)}{z^2} \\
        =& d\sigma^2
    \end{align*}
    where the last equality used
    \begin{equation*}
        \E{z \sim N(0, \sigma^2)}{z^2} = \text{Var}(z) + \E{}{z}^2 = \sigma^2.
    \end{equation*}
\end{proof}

The following result provides evidence that, with reasonable parameters, approximate DP does not yield meaningful utility improvements over pure DP for the Laplace mechanism. A result like this is likely folklore, but we include it here for completeness.
\begin{lemma}
\label{lem:laplace_approx}
    The Laplace mechanism with parameter $\sigma \leq 1/\eps$ does not satisfy $(\eps, \delta)$-DP for $\eps < 2\ln(1-\delta) + \frac{1}{\sigma}$.
\end{lemma}
\begin{proof}
    Let $T(X) = 0$ and let $T(X') = e_1$. Then $\|T(X) - T(X')\|_1 = 1$, and
    \begin{equation*}
        \P{y \sim M(0)}{\ln\left(\frac{f_X(y)}{f_{X'}(y)}\right) \geq \eps} = \P{y \sim M(0)}{ \|y-e_1\|_1 - \|y\|_1 \geq \sigma \eps} = \P{y \sim M(0)}{|y_1-1| - |y_1| \geq \sigma \eps}.
    \end{equation*}
    Mechanism $M$ is equivalent to the spherical Laplace distribution where each dimension is drawn from $\lap{\sigma}$. This distribution has CDF $F(x) = 1 - \frac{1}{2}\exp(-x/\sigma)$ for $x \geq 0$. Condition $|y_1 - 1| -|y_1| \geq \sigma \eps$ holds if and only if $y_1 \leq \frac{1}{2}(1 - \sigma \eps)$, so the probability of drawing such a $y$ is
    \begin{equation*}
        1 - \frac{1}{2}\exp\left(-\frac{1}{\sigma}\left[\frac{1}{2}(1-\sigma \eps)\right]\right) = 1 - \frac{1}{2}\exp\left(\frac{\eps - \frac{1}{\sigma}}{2}\right)
    \end{equation*}
    Therefore $\P{}{\ell_{M,X,X'} \geq \eps} = 1 - \frac{1}{2}\exp\left(\frac{\eps - \frac{1}{\sigma}}{2}\right)$.
    
    We now analyze $\P{}{\ell_{M,X',X} \leq -\eps}$. Because $\ell_{M,X',X} = \log(f_{X'}(y)/f_X(y)) = \frac{1}{\sigma} \cdot (|y_1| - |y_1-1|)$, we get \begin{equation*}
        \P{}{\ell_{M,X',X} \leq -\eps} = \P{y \sim M(1)}{|y_1-1| - |y_1| \geq \sigma \eps}
    \end{equation*}
    where $M(1)$ denotes the $d$-dimensional Laplace mechanism centered at $e_1$. By the same logic used above, $|y_1-1| - |y_1| \geq \sigma \eps$ if and only if $y_1 \leq \frac{1}{2}(1-\sigma \eps)$. For $y \sim M(1)$, this event has the same probability as $y_1' \leq \frac{1}{2}(1 - \sigma \eps) - 1 = \frac{1}{2}(-1 - \sigma \eps)$ when $y' \sim M(0)$. Furthermore, $\lap{\sigma}$ has CDF $F(x) = \frac{1}{2}\exp(x/\sigma)$ for $x < 0$. Thus $\P{}{\ell_{M,X',X} \leq -\eps} = \frac{1}{2}\exp\left(\frac{1}{2}\left[-\frac{1}{\sigma} - \eps\right]\right)$.
    
    Combining these results and applying $1+x \leq e^x$ yields
    \begin{align*}
        \P{}{\ell_{M,X,X'} \geq \eps} - e^{\eps}\P{}{\ell_{M,X',X} \leq -\eps} =&\ 1 - \frac{1}{2}\exp\left(\frac{\eps - \frac{1}{\sigma}}{2}\right) - e^{\eps}\frac{1}{2}\exp\left(\frac{1}{2}\left[-\frac{1}{\sigma} - \eps\right]\right) \\
        =&\ 1 - \exp\left(\frac{\eps - \frac{1}{\sigma}}{2}\right)
    \end{align*}
    By \Cref{lem:approx_dp}, this last quantity must be upper bounded by $\delta$ for $M$ to be $(\eps, \delta)$-DP. Rearranging yields the expression in the claim.
\end{proof}