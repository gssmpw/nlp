%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{bm} 
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{graphicx,mathrsfs,booktabs,mdwlist,multirow,colortbl,bm}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\definecolor{mygray}{gray}{.9}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer}

\begin{document}

\twocolumn[
\icmltitle{Zeroth-order Informed Fine-Tuning for Diffusion Model: A Recursive Likelihood Ratio Optimizer}


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}


\icmlsetsymbol{equal}{*}
\icmlsetsymbol{corr}{$\dagger$}

\begin{icmlauthorlist}
\icmlauthor{Tao Ren}{equal,gsm}
\icmlauthor{Zishi Zhang}{equal,gsm}
\icmlauthor{Zehao Li}{equal,gsm}
\icmlauthor{Jingyang Jiang}{equal,gsm}
\icmlauthor{Shentao Qin}{qh}
\icmlauthor{Guanghao Li}{qh}
\icmlauthor{Yan Li}{hkust}
\icmlauthor{Yi Zheng}{gsm}
\icmlauthor{Xinping Li}{Eco}
\icmlauthor{Min Zhan}{corr,hunan}
\icmlauthor{Yijie Peng}{corr,gsm}
\end{icmlauthorlist}

\icmlaffiliation{gsm}{Guanghua School of Management, Peking University}
\icmlaffiliation{qh}{Tsinghua University}
\icmlaffiliation{hkust}{The Hong Kong University of Science and Technology}
\icmlaffiliation{Eco}{School of Economics, Peking University}
\icmlaffiliation{hunan}{Hunan University of Technology and Business}

\icmlcorrespondingauthor{Tao Ren}{\textit{RTkenny@stu.pku.edu.cn}}
\icmlcorrespondingauthor{Yijie Peng}{\textit{pengyijie@pku.edu.cn}}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}



\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/teaser.pdf}
    \vspace{-0.3cm}
    \caption{Image and video samples generated from model fine-tuned by our RLR optimizer. More qualitative examples are included in {\color{black}Appendices \ref{Qualitative Results of Text2Image} and \ref{Qualitative Results of Text2Video}}.}
    \label{fig:teaser}
    \vspace{-0.3cm}
\end{figure*}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}

The probabilistic diffusion model (DM), generating content by inferencing through a recursive chain structure, has emerged as a powerful framework for visual generation. After pre-training on enormous unlabeled data, the model needs to be properly aligned to meet requirements for downstream applications. How to efficiently align the foundation DM is a crucial task. Contemporary methods are either based on Reinforcement Learning (RL) or truncated Backpropagation (BP). However, RL and truncated BP suffer from low sample efficiency and biased gradient estimation respectively, resulting in limited improvement or, even worse, complete training failure. To overcome the challenges, we propose the Recursive Likelihood Ratio (RLR) optimizer, a zeroth-order informed fine-tuning paradigm for DM. The zeroth-order gradient estimator enables the computation graph rearrangement within the recursive diffusive chain, making the RLR's gradient estimator \emph{an unbiased one with the lower variance} than other methods. 
We provide theoretical guarantees for the performance of the RLR. Extensive experiments are conducted on image and video generation tasks to validate the superiority of the RLR. Furthermore, we propose a novel prompt technique that is natural for the RLR to achieve a synergistic effect. See our implementation at \url{https://github.com/RTkenny/RLR-Opimtizer}.

\end{abstract}

\section{Introduction}
Probabilistic diffusion model (DM) \cite{sohl2015deep, ddpm, lzh_friend} has emerged as a transformative framework in high-fidelity data generation, demonstrating {\color{black}unmatched} capabilities in diverse applications such as image synthesis \cite{podell2023sdxl}, video generation \cite{modelscope}, and multi-modal data modeling. These models operate by iteratively denoising noisy data representations, effectively {\color{black}capturing} complex underlying distributions. Despite their success, {\color{black}even} fine-tuning DMs remains a daunting challenge due to the computational and {\color{black}caching demands associated with gradient computation during} training, especially for large-scale tasks \cite{draft}. This challenge has limited the broader deployment of DM in dynamic and resource-constrained environments.

{\color{black}It is a natural way to fine-tune DMs} via full backpropagation (BP) through all time steps \cite{BP}{\color{black}, which is theoretically functional}, providing precise gradient estimation over the entire diffusion chain. However, the computational and memory overhead of BP scales prohibitively with the model size and the number of diffusion steps \cite{alignprop, yuan2024instructvideo, draft, vader}, making full BP impractical for most real-world scenarios. {\color{black}Thus, truncating recursive differentiation becomes a common practice} to alleviate memory constraints{\color{black}. But truncated BP suffers} from structural bias, as {\color{black}it terminates gradient computation before sourcing to the input, only considering} a limited subset of the diffusion chain. Fine-tuning based on a biased gradient estimation can inadvertently impair the optimization performance, resulting in \textbf{model collapse}, i.e., contents generated reducing to pure noise. {\color{black}Moreover}, truncated BP \textbf{{\color{black} fails} to capture the multi-scale information} across all time steps {\color{black}due to the absence of differentiation on early steps.}
{\color{black}The hierarchical feature of generation imposes a thorough gradient evaluation} to retain fidelity {\color{black}from pixel to structural level}.

Reinforcement learning (RL) \cite{ppo} {\color{black} as a gradient computation trick has enabled another branch of DM fine-tuning} \cite{rwr, ddpo, diffusion_dpo, RL_DPOK}. {\color{black}It typically ignores the differentiable connection between steps and recovers the gradient by estimation.}
RL avoids caching intermediate activations, significantly reducing memory requirements. 
{\color{black}It also supports gradient computation in a divided manner under extreme circumstances to accommodate insufficient memory.
The cost to pay is the high variance of the estimated gradient. Even if the estimator is unbiased, the variance can result in wild sample-inefficient updates demonstrated by the slow convergence during training. }


These limitations of BP and RL {\color{black}underscore the necessity of} a more efficient, scalable, and stable fine-tuning approach {\color{black}that harmonizes computational tractability with optimization efficacy.}
{\color{black} To address this, we first investigate the recursive architecture of the DM and its relationship with perturbation-based gradient estimation. Subsequently, we conduct a rigorous analysis of the bias and variance properties of various estimation techniques}, motivating %the formulation of 
{\color{black}our integrated} optimizer.
 
Informed by perturbation-based analysis using Likelihood Ratio (LR) techniques, we propose the Recursive Likelihood Ratio (RLR) optimizer, a novel approach designed to address these challenges. The optimizer combines advantages from BP and RL but {\color{black}alleviates} their shortcomings. By utilizing the inherent noise in the DM, a local computational chain is enabled, which can reduce the variance and better capture the multi-scale information.
{\color{black}The gradient estimator from the RLR optimizer \textbf{is unbiased and has low variance under the same computation budgets as other methods.}} Specifically, RLR {\color{black}manages} memory overhead by limiting BP to strategically chosen segments of the diffusion chain while leveraging perturbation-based methods to estimate gradients for the remaining steps. This hybrid approach mitigates the structural bias of truncated BP and the high variance of RL, enabling stable and effective fine-tuning. Our contributions are threefold:
\begin{itemize}
    \item We give a thorough analysis of the bias and variance of different estimators in the recursive structure of DM, showing that the inherent noise in the DM can be used to perform computational graph rearrangement.
    \item Based on the theoretical and empirical insight, we formulate the RLR optimizer which has desirable theoretical properties regarding bias and variance, and the convergence of the RLR optimizer is also guaranteed.
    \item Extensive evaluation of Text2Image and Text2Video tasks are conducted. Our RLR surpasses all the baselines by a large margin. Furthermore, we propose a prompt technique for our RLR, validating the applicability of our method.
\end{itemize}

\section{Related works}
\paragraph{Diffusion Probabilistic Models.} Denoising Diffusion Model \cite{ddpm} is one of the strongest models for generation tasks, especially for visual generation \cite{LDM, DiT, videocrafter2}. Extensive research has been conducted from theoretical and empirical perspectives \cite{ddim, EDM, score_sde}. It has achieved phenomenal success in muti-modality generation, including image, video, audio, and 3D shapes. The DM is trained on enormous images and videos from the internet \cite{frozen_in_time, wang2023internvid_viclip, schuhmann2022laion}. Empowered by modern architecture \cite{transformer}, it has powerful learning capability for Pixel Space Distribution. 

\paragraph{Alignment and Post-training.} After pre-training to learn the distribution of the targeted modality \cite{achiam2023gpt4, scaling_neural_LM}, post-training is conducted to align the model toward specific preferences or tune the model to optimize a particular objective. RL has been utilized to align the foundation models toward various objectives \cite{RLHF, RLHF_blog, ddpo}. Supervised learning can also be applied to the post-training phase \cite{DPO}, either optimizing an equivalent objective \cite{diffusion_dpo} or directly differentiating the reward model \cite{draft, alignprop, vader}. For DM, most methods use a neural reward model to align the pre-trained model, and there has been a continual effort to design better reward models \cite{he2024videoscore, xu2024visionreward, imagereward}.

\paragraph{Forward Learning Methods.} 
\textcolor{black}{After extensive exploration of model training using forward inferences only \citep[see, e.g.,][]{peng2022new, hinton2022forward}, forward-learning methods \cite{salimans2017evolution, mezo, deepzero, oneforward} based on stochastic gradient estimation have recently emerged as a promising alternative to classical BP for large-scale machine learning problems \cite{HiZOO, revisi_ZO}. Subsequent research \cite{ren2024flops, chen2024enhancing} has further optimized computational and storage overhead from various perspectives, achieving greater efficiency.}


\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{figures/RLR.pdf}
    \vspace{-0.5cm}
    \caption{The computation paradigm of the RLR optimizer.}
    \label{fig:algdiagram}
    \vspace{-0.3cm}
\end{figure*}


\section{Diffusion Model under Perturbation-based Analysis}

\subsection{Recursive Structure in Diffusion Model.}

DMs constituted of the forward process and the backward process work by injecting noises or perturbations into a recursive computation chain to generate data from a given distribution. By analyzing the final output and the inherent intermediate perturbation, a forward learning estimator is a free byproduct without ``wasting the noise".


In the forward process, data points (e.g., images) are gradually corrupted with Gaussian noise over a series of time steps, resulting in a sequence of noisy samples. The forward process is represented by a stochastic transformation \( q(\bm{x}_t | \bm{x}_0) \), where \( \bm{x}_0 \) is the original data, and \( \bm{x}_t \) is the noisy sample at time \( t \). The ultimate goal is for the model to learn the reverse process, starting from pure noise \( \bm{x}_T \) and progressively recovering the original data \( \bm{x}_0 \).

The training objective is to learn the reverse diffusion process by predicting the noise added at each time step. This is achieved by minimizing the difference between the predicted noise and the actual noise added in the forward process. The objective is defined as follows:
\begin{equation}
    \begin{aligned}
        %&\min_\theta \mathbb{E}[-\log p_{\theta}(\bm{x}_0)] =\\&
    \min_\theta \mathbb{E}_{q(x_0), t\sim \mathcal{U}(0,T), q(\mathbf{x}_t | x_0)} \left[ \| \epsilon_\theta(x_t, t) - \epsilon \|^2 \right],
    \end{aligned}
\end{equation}

where \( \epsilon_\theta(x_t, t) \) is the predicted noise at time \( t \), and \( \epsilon \) is the actual noise added during the forward process. The model's neural network \( \epsilon_\theta \) is trained to accurately predict this noise, enabling it to progressively denoise data during generation.

The backward process of the DM can be generally expressed by a stochastic differential equation \cite{EDM,score_sde}:

\begin{equation}
    \begin{aligned}
        d \bm{x}_t = (h(\bm{x}_t,t)+g(t)\epsilon_\theta(\bm{x}_t,t))dt+ d\bm{w}_t,
    \end{aligned}
    \label{equa:diffusion_sde}
\end{equation}

where $h(\bm{x}_t,t)$ and $g(t)$ have analytical forms, and $d\bm{w}_t$ is the stochastic drift term. So the discretization to Equation (\ref{equa:diffusion_sde}) can be formulated as
\begin{align}\label{eq:half-order estimator}
    \bm{x}_{t-1}=\varphi_t(\bm{x}_t;\theta)+\bm{z}_t, 
\end{align}
where 
$\varphi_t(\bm{x}_t;\theta)$ denotes the deterministic part of the discretization, and $\bm{z}_t=\sigma_t\epsilon$ stands for the stochastic part for $1\leq t\leq T$. Consider a general form of the sampling procedure:
\begin{align}
    \bm{x}_{t-1}=\phi_t(\bm{x}_t,\bm{z}_t;\theta)=\varphi_t(\bm{x}_t;\theta)+\sigma_t\epsilon. 
\end{align}
Then, when the backward process is deterministic, we define the diffusion chain as

\begin{equation}
    \begin{aligned}
        &\bm{x}_0 = \phi_{1:T}(\bm{x}_T,\bm{z}_{1:T};\theta)=\phi_1 \circ ...\circ \phi_T(\bm{x}_T,\bm{z}_{1:T};\theta)=\\
        &\phi_1(\phi_{2:T}(\bm{x}_T,\bm{z}_{2:T};\theta),\bm{z}_1;\theta)=\phi_{1:T-1}(\phi_T(\bm{x}_{T},\bm{z}_T;\theta),\\ &\bm{z}_{1:T-1};\theta)
        =\phi_1(\phi_{2:T-1}(\phi_T(\bm{x}_{T},\bm{z}_T;\theta),\bm{z}_{2:T-1};\theta),\bm{z}_1;\theta).\\
    \end{aligned}
\end{equation}
After the pre-training phase, we need to conduct post-training to align the model to generate visually pleasing content. A reward model $R(\cdot)$ is deployed to evaluate the quality of the generated content $\bm{x_0}$.
Therefore, we aim to solve the optimization problem for the post-training:
\begin{align}
    \max_\theta \mathbb{E}[R(\bm{x}_0)]=\max_\theta \mathbb{E}_{\bm{z}_{1:T}}[R(\phi_{1:T}(\bm{x}_T,\bm{z}_{1:T};\theta))].
\end{align}
Typically, a gradient estimator of the expected reward function is required for the optimization problem. 


\subsection{Perturbation-based Gradient Estimators} 
The LR method, also called the score function method, is a perturbation-based gradient estimation approach used in neural network training \cite{oneforward, ren2024flops}. It provides an alternative to BP by bypassing recursive computations for the backward chain, making it computationally friendly and memory efficient. Especially, when {\color{black}applied} to differentiate a very deep network or long time horizon of RNN, LR serves as an efficient surrogate gradient estimator for BP. There are two types of LR estimators, both of which have been proven to be \textit{unbiased} in the literature \cite{glasserman1990gradient,oneforward}. 

\paragraph{Half-Order (HO) Estimator}
The first version borrows the intermediate noise as the perturbation to perform gradient estimation {\color{black}in} Equation (\ref{eq:half-order estimator}): $\bm{x}_{t-1}=\phi_t(\bm{x}_t,\bm{z}_t;\theta)=\varphi(\bm{x}_t;\theta)+\bm{z}_t$, where $\bm{z}_t$ is the {\color{black}inherent noise of the DM following} Gaussian distribution: $\bm{z}_t = \sigma_t \epsilon \sim \mathcal{N}(0, \sigma_t \bm{I})$.  The gradient estimator for the first version takes the form:
\begin{equation}\label{eq_lr2}R(\bm{x}_0)\sum_j D_{\theta}^{\top} \phi_{jh:jh+h-1}(\bm{x}_{jh},\bm{z}_{jh:jh+h-1};\theta) \nabla\ln f(\bm{z}_{jh}),
\end{equation}
where {\color{black}$f(\cdot)$ denotes the noise density, and} $D_{\theta}\phi \in\mathbb{R}^{d_{\phi}\times d_{\theta}}$ denotes the Jacobian matrix of $\phi \in \mathbb{R}^{d_{\phi}}$ with respect to $\theta\in\mathbb{R}^{d_{\theta}}$. 
Since this estimator enables a local {\color{black}differentiation} chain, \( D_{\theta}^{\top} \phi_{jh:jh+h-1}(\bm{x}_{jh}, \bm{z}_{jh:jh+h-1}; \theta) \), of length \( h \) after the perturbation \( \bm{z}_{jh} \), {\color{black}it is a combination of Zeroth- and First-Order optimization, which we refer to as a \textit{Half-Order} estimator.}

\textcolor{black}{Moreover, RL in DM training is fundamentally {\color{black}an} HO method with a local chain length of 1 \cite{oneforward}. Action sampling can also be interpreted as perturbations applied to the model's latent variables. However, it encounters challenges such as low sample efficiency and high variance in gradient estimation, as it disregards on-hand information along the forward computation.}


\paragraph{Zeroth-Order (ZO) Estimator}
The second version of LR directly injects {\color{black}exogenous} noise to the parameters of the targeted module, $\bm{x}_{t-1}=\phi_t(\bm{x}_t,\bm{z}_t;\theta)=\varphi(\bm{x}_t;\theta+\bm{z}_t).$ The estimator {\color{black}is then given by}
\begin{equation}\label{eq:ZO}
    \begin{aligned}        \sum_{t=1}^{T} R(\bm{x}_0) \nabla\ln f(\bm{z}_t),
    \end{aligned}
\end{equation}
{\color{black}which is commonly identified as the \textit{Zeroth-Order} estimator. It is completely blind to all structural information from the forward computation, relying solely on output evaluations to estimate gradients. The well-known Simultaneous Perturbation Stochastic Approximation (SPSA) is covered by this version, when the antithetic variable trick is applied to perform randomized finite differences \cite{ren2024flops}.}

\paragraph{First-Order (FO) Estimator}

\textcolor{black}{
As a contrasting extreme, BP is an FO method that requires the entire computational process to be pathwise differentiable. It leverages the chain rule of differentiation to compute gradients through the diffusive chain and the reward model, expressed as
\begin{equation}\label{eq:FO}
\begin{aligned}
\nabla_\theta R(\bm{x}_0)=
&\bigg[\frac{\partial \phi_1(\bm{x}_1,\bm{z}_1;\theta)}{\partial \theta} +\sum_{i=2}^{T}\frac{\partial \phi_i(\bm{x}_i,\bm{z}_i;\theta)}{\partial \theta}\\ &\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j,\bm{z}_j;\theta)}{\partial \bm{x}_j}\bigg]^{\top}\frac{d R(\bm{x}_0)}{d\bm{x_0}}.
\end{aligned}
\end{equation}
This approach ensures efficient gradient computation by utilizing the differentiable structure of the model.}

The unbiasedness of the above estimators is {\color{black}provided in} subsequent Theorem \ref{prop3}. We will compare their properties and then build a new RLR estimator based on their different merits in the next section.


\section{Bias and Variance of Gradient Estimators}
From the perspective of stochastic gradient estimation, there are three types of estimators for fine-tuning DM. BP and RL are {\color{black}covered by the aforementioned} estimators, utilizing the differentiability or the inherent noise in the DM. We will further analyze the bias and variance of gradient estimations, motivating the formulation of the RLR optimizer. 


\subsection{\textcolor{black}{Bias Analysis}}
The stochastic nature of the DM results in the variance of the FO estimator. As expected, the variance of the FO estimator is lower than that of the HO and ZO estimators because the differentiation leverages the precise structure of the neural network.

Performing \textbf{full BP} over the diffusive chain during training is computationally infeasible due to the large memory requirements. For example, training Stable Diffusion 1.4 with a batch size of 1 and 50 time steps would require approximately 1TB of GPU RAM \cite{alignprop}.

\textcolor{black}{To alleviate the memory burden of full-step BP, the truncated variant is often employed, where the total number of time steps $T$ in Equation (\ref{eq:FO}) is replaced with a smaller surrogate $T' \ll T$. However, the truncation introduces a \textbf{structural bias} into the gradient estimator. We have the following proposition to justify this structural bias.
\begin{proposition}\label{prop1}
Assume $R$ and $\phi$ are differentiable almost everywhere, $R$ and $\phi$ have bounded gradients or Lipschitz constants, then the FO estimator is unbiased. However, the truncate BP estimator $\nabla_{\theta}R(\bm{x}_0)_{\mathrm{truncated}}$ has a structural bias, which can be specified as below:
\begin{equation*}%\label{eq_BP}
\begin{aligned}
&\nabla_{\theta}\mathbb{E}[R(\bm{x}_0)] - \mathbb{E}[\nabla_{\theta}R(\bm{x}_0)_{\mathrm{truncated}}]= \mathbb{E}_{z_{1:T}} \\
&\bigg[\bigg(
\sum_{i=T'-1}^{T}\frac{\partial \phi(\bm{x}_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j;\theta)}{\partial \bm{x}_j}\bigg)^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}\bigg],
\end{aligned}
\end{equation*}
where $\phi(\bm{x}_j;\theta)$ denotes $\phi_j(\bm{x}_j,z_j;\theta)$
 for simplicity. 
\end{proposition}
The bias in the estimator can lead to numerical instability, resulting in suboptimal updates or even training failure, as the truncated gradient may not be a descent direction. The undesired consequences are as follows.
\begin{itemize}
    \item \textbf{Model collapse}: The model may fail to converge, degenerating into generating pure noise due to unstable optimization.
    \item \textbf{Failure to capture multi-scale information}: By limiting the gradient computation to fewer time steps, truncated BP loses essential multi-scale visual information along the diffusive chain.
\end{itemize}
We conduct empirical studies to investigate the effects of truncation. As shown in Figure \ref{fig:model_collapse}, truncated gradients result in a significant drop in reward scores during training; the fewer the truncated time steps, the more severe the model collapse. In our subsequent Diffusive Chain-of-Thought experiment, we demonstrate the enhanced capability of the proposed RLR optimizer to capture multi-scale information for visual generation, effectively mitigating the limitations of the truncated BP variant.}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/model_collapse.pdf}
    \vspace{-0.3cm}
    \caption{Model collapse caused by the truncation: training SD 1.4 on the aesthetic reward model by truncated BP.}
    \label{fig:model_collapse}
    \vspace{-0.5cm}
\end{figure}


\subsection{Variance Analysis}
We know that FO, HO and ZO estimators are all unbiased. However, BP introduces significant computational and storage overhead. The following proposition demonstrates that this additional cost is, to some extent, justified, as BP leverages accurate internal structural to reduce estimation variance, and therefore the variance of BP is lower than zeroth-order methods.
\begin{proposition}\label{prop2}
Under Assumptions (A.1-3) in the Appendix, the variance of FO estimators in Equation (\ref{eq:FO}) is less or equal to ZO estimators in Equation  (\ref{eq:ZO}), i.e.
\begin{equation}
    \mathrm{Var}(\nabla_\theta R(\bm{x}_0)) \le \mathrm{Var}(R(\bm{x}_0) \nabla\ln f(\bm{z})).
\end{equation}
\end{proposition}
\textcolor{black}{Additionally, it is straightforward to conclude that the variance of HO estimator is also  less or equal to that of ZO estimator, as it is essentially an FO estimator with a perturbation at the end of the chain. This argument is also discussed in \cite{salimans2017evolution}.}
The following Table \ref{tab1} presents all the gradient computation methods discussed above. 
\begin{table}[htbp]
    \vspace{-0.5cm}
    \centering
    \caption{Comparison of different gradient estimators}
    \begin{tabular}{cccc}
    \toprule
    \textbf{Methods}  & \textbf{Unbiasedness} & \textbf{Variance} & \textbf{Memory} \\
    \midrule
    % BP & \checkmark & zero & big   \\
    First-order & \checkmark & Small & Large \\
    Half-order & \checkmark & Medium & Medium \\
    Zeroth-order & \checkmark & Large & Small  \\
    Truncated BP & $\times$ & Small & Medium  \\
    \bottomrule
    \end{tabular}    
    \label{tab1}
    \vspace{-0.5cm}
\end{table}

% \xmark

\section{Recursive Likelihood Ratio Optimizer}
As summarized by Table \ref{tab1}, in DM fine-tuning, the full BP imposes prohibitively high computational and memory requirements, while RL suffers from excessively high variance. In this section, we propose the ZO-informed RLR optimizer, which integrates the above three gradient estimators within a recursive framework.
\textcolor{black}{The RLR estimator is an unbiased gradient estimator that pursues minimal variance under a ``feasible" computational and memory budget.}
The RLR estimator is constituted of three parts and takes the form:
{\begin{equation}
    \begin{aligned}
&\underbrace{{\frac{\partial\varphi(\bm{x}_1;\theta)}{\partial\theta}}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x}_0}}_{\mathrm{One-step\ first-order\ estimator}}\\&+
\underbrace{R(\bm{x}_0){D^{\top}_\theta\phi_{j:j+h}(\bm{x}_{j+h};\theta)}\nabla\ln f(\bm{z}_j)}_{h-\mathrm{length\ half-order \ estimator}}\\&+\underbrace{\sum_{i \in C}R(\bm{x}_0)\nabla\ln f(\bm{z}_i)}_{\mathrm{zeroth-order\ estimator}},
    \end{aligned}
    \label{equ:RLR_estimator}
\end{equation}}
where $j\sim \mathcal{U}(1,T-h)=\mathcal{U}(\{1,2,...,T-h\})$, $C=\{1, 2, \dots, T\} \setminus \{j, j+1, \dots, j+h\}$, $\bm{z}=\{\bm{z}_i,\bm{z}_j\}$ and $\bm{z}_i\sim\mathcal{N}(0,\sigma_i \bm{I}),\bm{z}_j\sim\mathcal{N}(0,\sigma_j\bm{I})$ independently.
\paragraph{Differentiating the reward model.} In the first part of the RLR estimator, we differentiate the reward model and the first time-step of the diffusive chain, as shown in Figure \ref{fig:algdiagram}. With the availability of the reward model structure, it is unnecessary to use zeroth-order methods to bypass BP.
% random selection; sliding window
\paragraph{Fixed-horizon half-order optimization.} 
The generation process of the DM follows a coarse-to-fine structure, with each time-step in the chain controlling a different scale of generation. Incorporating precise gradient information from every time-step is essential, but full BP is computationally prohibitive. Truncated BP introduces bias, while zeroth-order optimization leads to high variance by ignoring structural information.
To address this, the second part of RLR applies half-order optimization on a randomly selected \( h \)-length sub-chain, ensuring unbiasedness while minimizing variance. Specifically, before each optimization step, a time-step \( j \sim \mathcal{U}(1, T-h) \) is uniformly sampled, natural perturbations are applied to the corresponding latent variable, and localized BP is performed over the \( h \)-length sub-chain, where the hype-parameter $h$ is chosen based on the available computation budget.

\paragraph{Surrogate estimator via parameter perturbation.} For the remaining times steps, $C=\{1, 2, \dots, T\} \setminus \{j, j+1, \dots, j+h\}$, we inject noise directly into the model's parameters to construct an ZO estimator. This approach does not require caching intermediate latent variables or performing BP, making it computationally cheap.


Overall, the RLR reorganizes the recursive computation chain by perturbation-based estimation, seamlessly integrating zeroth-order, half-order, and first-order optimization techniques. RLR strikes a balance between computational cost and gradient accuracy, achieving both unbiasedness and low variance. The following Theorem \ref{prop3} establishes its unbiasedness.
\begin{theorem}\label{prop3}
    The RLR estimator is an unbiased estimator:
    \begin{equation}
    \begin{aligned}
        &\nabla_\theta \mathbb{E}[R(\phi_{1:T}(\bm{x}_T;\theta))]=
        \mathbb{E}_{\bm{z}\sim f(\bm{z}),j\sim \mathcal{U}(1,T-h)} \\ &\bigg[\frac{\partial\varphi(\bm{x}_1;\theta)}{\partial\theta}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x}_0}+
        \sum_{i \in C}R(\bm{x}_0)\nabla\ln f(\bm{z}_i)\\&+R(\bm{x}_0)D^{\top}_\theta\phi_{j:j+h}(\bm{x}_{j+h};\theta)\nabla\ln f(\bm{z}_j)\bigg].
    \end{aligned}
\end{equation}
\end{theorem}
The variance of the RLR estimator, denoted by $\sigma^2_{\mathrm{RLR}}$, is discussed in the appendix. Under limited computational resources where full BP is infeasible, RLR achieves substantially lower variance compared to other unbiased gradient estimators. Finally, the convergence rate of RLR is provided in the following Theorem \ref{theo_convergence}.


\begin{theorem}\label{theo_convergence}
    Suppose that the reward function $R(\cdot)$ is L-smooth. By appropriately selecting the step size, the convergence rate of the RLR is given by
    $$\frac{1}{K+1}\sum_{k=0}^K\mathbb{E}(\|\nabla R(\theta_k)\|^2)\leq \sqrt{\frac{8L\Delta_0\sigma^2_{\mathrm{RLR}}}{K+1}}+\frac{2L\Delta_0}{K+1},$$
where $K$ is the number of iterations, $\theta_k$ is the trainable parameter in the $k$-th iteration, and
$\Delta_0=|R(\theta_0)-R^\ast|$ is difference between initialization performance and optimal performance.
\end{theorem}


\section{Experiments}

\begin{table*}[th]
    \setlength{\tabcolsep}{4pt}
    \small
    \centering
    \caption{Text to Image reward score. We evaluate methods under different DM under different reward models. The higher the score, the better the performance.}
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{cc|cccc|cccc}
    \toprule
    \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Methods}}
    & \multicolumn{4}{c}{\textbf{Pick-a-Pic}} & \multicolumn{4}{c}{\textbf{HPD v2}} \\
    & & PickScore & HPSv2 & AES & ImageReward & PickScore & HPSv2 & AES & ImageReward \\
    \midrule
    \multirow{4}{*}{\textbf{SD1.4}}
    & Base & 16.24 & 21.03 & 4.48 & 32.74 & 16.19 & 22.08 & 4.42 & 32.90 \\
    & DDPO & 17.56 & 23.15 & 5.47 & 49.33 & 17.53 & 22.79 & 5.52 & 52.06 \\
    & Alignprop & 18.08 & 26.64 & 5.91 & 65.07 & 19.17 & 27.02 & 6.02 & 67.18 \\
    & \cellcolor{mygray} RLR & \cellcolor{mygray} \textbf{20.14} & \cellcolor{mygray} \textbf{28.57} & \cellcolor{mygray} \textbf{6.53} & \cellcolor{mygray} \textbf{75.65} & \cellcolor{mygray} \textbf{21.38} & \cellcolor{mygray} \textbf{29.22} & \cellcolor{mygray} \textbf{6.65} & \cellcolor{mygray} \textbf{76.55} \\
    \midrule
    \multirow{4}{*}{\textbf{SD2.1}}
    & Base & 16.37 & 22.14 & 4.53 & 35.40 & 16.25 & 23.32 & 4.57 & 36.03 \\
    & DDPO & 17.70 & 24.55 & 5.58 & 52.48 & 17.43 & 24.56 & 5.62 & 52.85 \\
    & Alignprop & 19.23 & 27.20 & 6.07 & 68.09 & 21.60 & 27.40 & 6.11 & 72.62 \\
    & \cellcolor{mygray} RLR & \cellcolor{mygray} \textbf{22.58} & \cellcolor{mygray} \textbf{30.11} & \cellcolor{mygray} \textbf{6.76} & \cellcolor{mygray} \textbf{77.26} & \cellcolor{mygray} \textbf{23.22} & \cellcolor{mygray} \textbf{30.98} & \cellcolor{mygray} \textbf{6.94} & \cellcolor{mygray} \textbf{83.07} \\
    \bottomrule
    \end{tabular}\end{adjustbox}
    
    \label{tab:image}
    \vspace{-0.3cm}
\end{table*}

\begin{table*}[th]  
    \setlength{\tabcolsep}{4pt}
    \small
    \centering
    \caption{Text2Video Generation Evaluation on the Vbench. The weighted average is calculated by assigning a weight of 1 to all metrics, except for the Dynamic Degree metric, which is assigned a weight of 0.5.}
    %\begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{c|ccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & \textbf{Subject} & \textbf{Background} & \textbf{Motion} & \textbf{Dynamic} & \textbf{Aesthetic} & \textbf{Imaging} & \textbf{Weighted} \\
    & \textbf{Consistency} & \textbf{Consistency} & \textbf{Smoothness} & \textbf{Degree} & \textbf{Quality} & \textbf{Quality} & \textbf{Average} \\
    \midrule
    VideoCrafter & 95.44 & 96.52 & 96.88 & 53.46 & 57.52 & 66.77 & 79.97 \\%[0.1cm]      
    Pika & 96.76 & \cellcolor{mygray} \textbf{98.95} & 99.51 & 37.22 & 63.15 & 62.33 & 79.87 \\ %[0.1cm] 
    Gen-2 & \cellcolor{mygray} \textbf{97.61} & 97.61 & \cellcolor{mygray} \textbf{99.58} & 18.89 & 66.96 & 67.42 & 79.75 \\ %[0.1cm] 
    T2V-Turbo & 96.28 & 97.02 & 97.34 & 49.17 & 63.04 & \cellcolor{mygray} \textbf{72.49} & 81.96 \\ %[0.1cm]
    \midrule
    DOODL & 95.47 & 96.57 & 96.84 & 55.46 & 58.27 & 66.79 & 80.30 \\ %[0.1cm]
    DDPO  & 95.53 & 96.63 & 96.92 & 58.29 & 59.23 & 66.84 & 80.78 \\ %[0.1cm]
    VADER & 95.79 & 96.71 & 97.06 & 66.94 & 66.04 & 69.93 & 83.45\\ %[0.1cm] 
    \cellcolor{mygray} \textbf{RLR} & 97.53 & 97.19 & 98.05 & \cellcolor{mygray} \textbf{70.69} & \cellcolor{mygray} \textbf{66.15} & 71.08 & \cellcolor{mygray} \textbf{84.61} \\ %[0.1cm] 
    \bottomrule
    \end{tabular}%\end{adjustbox}
    
    \label{tab:vit}
    \vspace{-0.3cm}
\end{table*}

We verify the superiority and applicability of the RLR optimizer against various baselines on two generation tasks: Text2Image and Text2Video. Finally, we propose a novel prompt technique that is natural for the RLR optimizer to further demonstrate the intuition of RLR and applicability. 

\subsection{Setting} 
\paragraph{Prompts dataset.} We use Pick-a-Pic \cite{pickapic} and HPD v2 \cite{hpsv2} for the Text2Image experiments. We report the performance of the trained model on unseen prompts from the training phase.
For the Text2Video task, we prompt ChatGPT to generate a series of prompts that describe motions and train models under the prompts. After the training, we evaluate the model performance on the unseen prompts from the vbench \cite{vbench}.

\paragraph{Reward model and benchmark.} We adopt multiple human preference reward models to train and test our methods, including PickScore \cite{pickapic}, HPS v2 \cite{hpsv2}, and ImageReward \cite{imagereward}. All the models are trained on large-scale preference datasets. We also included the traditional aesthetic models \cite{aes}, i.e. AES.

For the video generation task, we use the VBench \cite{vbench} to rate the methods according to various perspectives of the generated videos. The generated videos will be evaluated from 6 aspects: Subject Consistency (SC), Background Consistency (BC), Motion Smoothness (MS), Dynamic Degree (DD), Aesthetic Quality (AQ), and Imaging Quality (IQ).

\paragraph{Baselines.} We compared our methods with RL-based methods and BP-based methods. DDPO \cite{ddpo} is the state-of-the-art RL method for DMs. For the Text2Image experiment, we include the AlignProp \cite{alignprop}, a randomized truncated BP method. For the Text2Video experiment, we included VADER \cite{vader}, a BP-based method especially catering to video generation.

\subsection{Text2Image Generation}

We evaluate our methods on two DMs: Stable Diffusion 1.4 and 2.1 \cite{LDM}. As shown in Table \ref{tab:image}, the RLR methods achieve higher reward scores on the unseen prompts from the test set. The RL-based method have limited improvement with respect to the base model, due to the sample inefficiency nature. Alignprop has considerable improvement over the base model. However, the biased estimator limits its further improvement. Training details and hyperparameters can be found in the appendix.

\begin{figure}[!h]
    \centering
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/aes_se.pdf}
        \caption{Reward curves for aesthetic score.}
        \label{fig:subfig1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/hps_se.pdf}
        \caption{Reward curves for HPS v2 score.}
        \label{fig:subfig2}
    \end{subfigure}
    \vspace{-0.3cm}
    \caption{Sample efficiency analysis, reward curves under the same training steps for SD 1.4.}
    \label{fig:Sample_efficiency}
    \vspace{-0.3cm}
\end{figure}


\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/DCoT.pdf}
    \vspace{-0.3cm}
    \caption{The framework of Diffusive Chain-of-Thought. The DM generates images in a multi-scale manner: earlier steps for low-resolution features and later steps for high-resolution features. If a specific scale has deficiencies, we utilize the HO estimator to enhance the corresponding steps.}
    \label{fig:dcot}
    \vspace{-0.3cm}
\end{figure*}



\subsubsection{Sample Efficiency Analysis}

The compare the sample efficiency and the variance of different methods, we show the reward curves of SD 1.4 when training on the AES and HPS v2 models in Figure \ref{fig:Sample_efficiency}. The DDPO optimizes the reward in a very slow pace, indicating high variance and low sample efficiency. In the earlier phase, the AlignProp has comparable performance as the RLR. In the later phase, while the RLR can continue to improve the reward, the AlignProp suffers from severe model collapse. 


\subsection{Text2Video Generation}
We compare our RLR not only with RL and truncated BP but also with a series of open-source or API-based Text2Video models. In the metric of DD and AQ, the RLR surpasses other methods by a large margin. In other metrics, RLR achieves considerable improvement over the base model, VideoCrafter. Some API-based models have better performance on some metrics, but the gaps are small. In terms of the weighted average score, our RLR has the best performance over all baselines.

\subsection{Diffusive Chain-of-Thought}

Furthermore, we propose the Diffusive Chain-of-Thought (DCoT), a prompt technique, that is natural for our RLR optimizer to demonstrate the applicability of our method. 

It is worth noting that the generation process of the DM is essentially a \textbf{multi-scale} or coarse-to-fine process, \emph{first generating global structures and then focusing on local details.} We propose dividing all the diffusion process steps into three groups: coarse-level, mid-level, and fine-level. The coarse-level chain includes steps adjacent to the initial noise, focusing on generating a rough outline. The fine-level chain includes steps adjacent to the final output, focusing on the fine-grained details. The mid-level chain in between focuses on the geometric structure of the content. 

The idea of DCoT is shown in Figure \ref{fig:dcot}, which converts the original prompt into multi-scale prompts reflecting the coarse-to-fine nature. Different generation steps should conditioned on different prompts instead of conditioned on the same prompt.

\begin{table}[h]
    \vspace{-0.4cm}
    \caption{Experiment results for Diffusive Chain-of-Thought}
    \setlength{\tabcolsep}{4pt}
    \small
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{c|cccc}
    \toprule
    \textbf{Methods} & PickScore & HPSv2 & AES & ImageReward \\
    \midrule
    SD1.4 & 15.33 & 20.50 & 4.76 & 33.45 \\
    SD1.4-DCoT & 16.38 & 21.68 & 4.83 & 39.24 \\
    RLR w/o & 18.05 & 23.58 & 5.27 & 43.09 \\
    % RLR-DCoT (all steps) &  &  &  &  \\
    % RLR-DCoT (specific steps)  &  &  &  &  \\ 
    \cellcolor{mygray} \textbf{RLR-DCoT} & \cellcolor{mygray} \textbf{19.45} & \cellcolor{mygray} \textbf{25.80} & \cellcolor{mygray} \textbf{5.83} & \cellcolor{mygray} \textbf{49.88} \\
    \bottomrule
    \end{tabular}\end{adjustbox}
    \label{tab:DCoT}
    \vspace{-0.2cm}
\end{table}


The HO estimator term, having lower variance than the zeroth-order term, in the RLR enables a $h$-step local computational chain. As shown in Formula \ref{equ:RLR_estimator}, the HO estimator term uniformly picks a starting point $j \sim \mathcal{U}(1,T-h)$ from the entire $T$-step chain to start the local $h$-step BP chain. When applying the DCoT to the fine-tuning process, we should constrain the sample range of $j$ in the steps where deficiencies exist, $j \sim \mathcal{U}(a,b), 1<a<b<T-h, b-a>h$. In our experiment for the hand task, we set $a=30$ and $b=40$. 

We write 5 prompts for hand generation and then prompt ChatGPT to generate the multi-scale prompts for the three levels. We report the performance in Table \ref{tab:DCoT}. As shown in the table, either simply applying DCoT to the base model or combining it with the RLR can improve the performance significantly. The RLR with the DCoT has the best performance. Qualitative results are shown in the appendix.




\subsection{Ablation Study}
We conduct the ablation study, using SD 1.4 and HPD v2, to verify the contribution of different parts in the RLR optimizer. In Table \ref{tab:ablation}, we evaluate the RLR and its variant (V1: the RLR without HO and ZO; V2: the RLR without ZO; V3: the RLR without HO). The V1 performs the worst since it actually reduces to the truncated BP with only one time-step. The V2 and V3 perform better than the V1. It is worth noting that the V2 is better than the V3. The V2 without HO is actually an unbiased estimator since it takes all time steps into account when estimating the gradient. Even though the V2 rearranges the computational graph by the HO, it still a biased estimator. This phenomena indicates the importance of unbiasness when conducting the fine-tuning task.
\begin{table}[h]
    \vspace{-0.4cm}
    \caption{Ablation of the RLR}
    \setlength{\tabcolsep}{4pt}
    \small
    \centering
    \begin{adjustbox}{max width=\linewidth}
    \begin{tabular}{c|cccc}
    \toprule
    \textbf{Methods} & PickScore & HPSv2 & AES & ImageReward \\
    \midrule
    RLR w/o HO and ZO & 18.43 & 23.66 & 5.78 & 60.07 \\
    RLR w/o ZO & 19.28 & 26.70 & 5.92 & 63.85 \\
    RLR w/o HO & 20.11 & 27.07 & 6.23 & 68.35 \\
    RLR & 21.38 & 29.22 & 6.65 & 76.55 \\
    \bottomrule
    \end{tabular}\end{adjustbox}
    \label{tab:ablation}
    \vspace{-0.3cm}
\end{table}

\section{Conclusion}
In this paper, we propose a novel zeroth-informed optimizer for the alignment of the DM. We give a perturbation-based analysis for the diffusion chain and draw theoretical insight into the bias, variance, and convergence of the RLR estimator and optimizer. The empirical study on Text2Image and Text2Video is conducted. Our RLR outperforms all baselines under various metrics. Furthermore, we propose a prompt technique that is natural for the RLR optimizer to validate the applicability.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}
%\newpage
\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{appendix}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiments settings}

\subsubsection{Prompts}
\textbf{HPD v2.} Human Preference Dataset v2 (HPD v2) is a large-scale collection designed to evaluate human preferences for images generated from text prompts, comprising 798,090 human-annotated preference choices from 433,760 image pairs. It includes images from diverse text-to-image models and utilizes cleaned prompts, processed with ChatGPT to remove biases and style-related words. 

\textbf{Pick-a-Pic.} The Pick-a-Pic dataset is a publicly available collection of over half a million human preferences for images generated from 35,000 text prompts. Users generate images using state-of-the-art text-to-image models and select their preferred image from pairs, with each example including a prompt, two images, and a preference label. Collected via a web application, this dataset better reflects real-world user preferences and is used to train the PickScore scoring function, which enhances model evaluation and improvement.

\textbf{ChatGPT Created Prompts.} We ask ChatGPT to generate imaginative and detailed text descriptions for various scenarios, including people engaging in sports, animals wearing clothes, and animals playing musical instruments. We use the ChatGPT-generated prompts to train the model.

\textbf{Vbench Prompt Suite.} The Prompt Suite in VBench is a carefully curated set of text prompts designed to evaluate video generation models across 16 distinct evaluation dimensions. Each dimension is represented by approximately 100 prompts tailored to test specific aspects of video quality and consistency. The prompts are organized to reflect different categories, such as animals, architecture, human actions, and scenery, ensuring comprehensive coverage across diverse content types. These prompts are used to assess modelsâ€™ abilities, such as subject consistency, object class generation, motion smoothness, and more, providing insights into the models' strengths and weaknesses across various scenarios.

\subsubsection{Reward Models and Evaluation Metrics}
\textbf{PickScore.} The PickScore Reward Model is a scoring function trained on the Pick-a-Pic dataset, which includes human preferences for text-to-image generated images \cite{pickapic}. It uses a CLIP-based architecture to compute scores by comparing text and image representations. Trained to predict user preferences, it minimizes KL-divergence between true preferences (preferred image or tie) and predicted scores.

\textbf{HPSv2.} Human Preference Score v2 (HPSv2) \cite{hpsv2} is a model designed to evaluate human preferences for images generated by text-to-image models. Trained on the Human Preference Dataset v2, which includes 798,000 human preference annotations on 433,760 image pairs from various generative models, HPSv2 predicts which images are preferred based on text-image alignment and aesthetic quality, offering a more human-aligned evaluation compared to traditional metrics like Inception Score or FrÃ©chet Inception Distance. 

\textbf{AES.} The Aesthetic Score (AES) \cite{aes} is obtained from a model that builds on CLIP embeddings and incorporates additional multilayer perceptron (MLP) layers to capture the visual attractiveness of images. This metric serves as a tool for assessing the aesthetic quality of generated images, offering insights into how closely they match human aesthetic preferences.

\textbf{ImageReward.} ImageReward \cite{imagereward} is a reward model designed to evaluate human preferences in text-to-image generation. It is trained on a large dataset of 137k expert comparisons, using a systematic annotation pipeline that rates and ranks images based on alignment with text, fidelity, and harmlessness. Built on the BLIP model, ImageReward accurately predicts human preferences, outperforming models like CLIP, Aesthetic, and BLIP. It serves as a promising automatic evaluation metric for text-to-image synthesis, aligning well with human rankings.

\subsubsection{Baselines}
\textbf{DOODL.} DOODL (Direct Optimization of Diffusion Latents) optimizes diffusion latents to improve image generation by directly adjusting latents based on a model-based loss. Unlike traditional classifier guidance methods, DOODL avoids the need for retraining models or using approximations, providing more accurate and efficient guidance. It enhances text-conditioned generation, expands pre-trained model vocabularies, enables personalized image generation, and improves aesthetic quality, offering better control and higher-quality outputs in generative image models.

\textbf{DDPO.} DDPO (Denoising Diffusion Policy Optimization) is an RL-based method for optimizing diffusion models towards specific goals like image quality or compressibility. By treating denoising as a multi-step decision-making task, DDPO uses policy gradients to maximize a reward function, unlike traditional likelihood-based methods. DDPO also shows strong generalization across diverse prompts, making it highly effective for fine-tuning generative models.

% \textbf{DRAFT.} Direct Reward Fine-Tuning (DRaFT) is a method for fine-tuning diffusion models on differentiable reward functions. DRaFT backpropagates reward gradients through the entire sampling process, improving images to maximize specific rewards like aesthetic quality or human preferences. It reduces gradient variance by using multiple noise samples. DRaFT is faster and more efficient than reinforcement learning-based methods like DDPO, achieving high-quality results for various rewards, including aesthetic improvement, human preference alignment, and tasks like compressibility or object detection.

\textbf{AlignProp.} AlignProp fine-tunes text-to-image diffusion models by backpropagating gradients through the entire denoising process using randomized truncated backpropagation. This method reduces memory and computational costs by employing low-rank adapter modules and gradient checkpointing. The randomized TBTT approach, which randomly selects the number of backpropagation steps, prevents overfitting and mode collapse, improving both sample efficiency and reward optimization. AlignProp outperforms other methods in terms of generalization, image-text alignment, and aesthetic quality, making it a highly efficient and effective tool for optimizing diffusion models to specific downstream objectives.

\textbf{VADER.} VADER (Video Diffusion via Reward Gradients) fine-tunes video diffusion models by backpropagating gradients from pre-trained reward models. It enhances sample and computational efficiency, using reward models to assess aesthetics, text-video alignment, and other video-specific tasks. VADER maintains temporal consistency, and generalizes well to unseen prompts, making it an effective tool for adapting video models to complex objectives.

\subsubsection{Orthogonal tricks}

\textbf{LoRA} applies low-rank adaptation to the original parameters, $\theta$, by fine-tuning only the low-rank components rather than the full parameters. Specifically, each linear layer in the backbone (U-Net or Transformer) of the diffusion model is modified as \(h = W x + BAx,\) where \( W \in \mathbb{R}^{m \times m} \), \( A \in \mathbb{R}^{m \times k} \), and \( B \in \mathbb{R}^{k \times m} \), with \( k \ll m \). The LoRA weights are initialized to zero, ensuring no initial impact on the pre-trained modelâ€™s performance. This method reduces the number of parameters to be trained while achieving performance comparable to full-parameter fine-tuning. We apply LoRA with $k=16$ to all experiments.


\textbf{Gradient checkpointing} is a well-known technique for reducing memory usage during neural network training \cite{mem_eff_bptt, checkpoint}. Instead of storing all intermediate activations for backpropagation, it selectively saves only those needed for gradient computation and transfers the rest to the CPU's main memory. However, this comes with the cost of additional data transfer and computation overhead, which can increase training time. In the case of truncated backpropagation, checkpointing is unavoidable. For our RLR optimizer, though, gradient checkpointing is not a necessary technique.

\subsection{Hypeparameters}
All the experiments are conducted on a machine with 8 NVIDIA V100 GPUs. Each GPU has 32GB of memory.

For the Text2Image and the DCoT experiment, we use Adam optimizer with the learning rate of $5\times10^{-4}$. The batch size is $4$ and the gradient accumulation steps are $2$. The DDIM steps are $50$ and the classifier guidance weight is $7.5$. The local sub-chain has a length of $2$. We use Gaussian noise with a standard deviation of $1\times10^{-3}$ for perturbing the parameters.

For the Text2Video experiment, we use Adam optimizer with the learning rate of $1\times10^{-4}$. The batch size is $1$ and the gradient accumulation steps are $8$. The DDIM steps are $25$ and the classifier guidance weight is $7.5$. The local sub-chain has a length of $2$. We use Gaussian noise with a standard deviation of $1\times10^{-4}$ for perturbing the parameters.

\newpage

\subsection{Qualitative Results of Text2Image}\label{Qualitative Results of Text2Image}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/q_image.pdf}
    \vspace{-0.3cm}
    \caption{Qualitative results for Text2Image generation.}
    \label{fig:dcot}
    \vspace{-0.3cm}
\end{figure*}

\newpage

\subsection{Qualitative Results of Text2Video}\label{Qualitative Results of Text2Video}
\begin{figure*}[!h]
    \centering
    \includegraphics[width=\textwidth]{figures/q_video.pdf}
    \vspace{-0.3cm}
    \caption{Qualitative results for Text2Video generation.}
    \label{fig:dcot}
    \vspace{-0.3cm}
\end{figure*}

\newpage

\subsection{Details for Diffusive Chain-of-Thought experiment}
Original prompts for the hand task: 1, A realistic open palm facing upward. 2, A picture of a hand facing downward. 3, A hand in a relaxed position. 4, A hand. 5, A photo of a hand.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/q_DCoT.png}
    \vspace{-0.3cm}
    \caption{Qualitative results for the hand task in DCoT.}
    \label{fig:dcot}
    \vspace{-0.3cm}
\end{figure*}


% \subsection{Sample Efficiency Analysis}
% maybe a reward curve to show the performance under different training steps and training time

\subsection{Memory Profile}
We give the memory cost for the experiments with Text2Image on SD 1.4 in Table \ref{tab:mem}. We offload the memory to the CPU RAM to avoid the out-of-memory error. The AlignProp based on truncated BP has the largest memory consumption. The DDPO, based on RL, has the smallest consumption, while the sample efficiency is terrible, as shown in Figure \ref{fig:Sample_efficiency}. Our RLR has significantly lower memory consumption than the AlignProp.

\begin{table*}[htbp]
    \vspace{-0.5cm}
    \centering
    \caption{Memory cost of Text2Image experiments on SD 1.4}
    \begin{tabular}{cccc}
    \toprule
    \textbf{Methods}  & \textbf{VRAM} & \textbf{System RAM} & \textbf{Total} \\
    \midrule
    % BP & \checkmark & zero & big   \\
    DDPO & 12.4 GB & 0 GB & 12.4 GB \\
    AlignProp & 25.4 GB & 78.5 GB & 103.9 GB \\
    RLR & 22.4 GB & 0 & 22.4 GB  \\
    \bottomrule
    \end{tabular}    
    \label{tab:mem}
    \vspace{-0.5cm}
\end{table*}


\newpage
\subsection{Proofs}
\subsubsection{Proof of Proposition \ref{prop1}}
\begin{proof}
The interchange of 
 derivative and expectation can be justified by the dominated convergence theorem \citep{rudin1987real}. Furthermore, the condition of uniform integrability, which is necessary to ensure the safe exchange of limit operations with expectation, can be relaxed by employing a Lipschitz condition \citep{glasserman1990gradient}. 

 By chain rule and  $\bm{x}_0(z;\theta)=\phi_{1:T}(\bm{x}_T,z;\theta)=\phi_1 (\bm{x}_{1},\bm{z}_1;\theta)$, we have
\begin{equation*}
    \begin{aligned}
        \frac{\partial R(\bm{x}_0(z;\theta))}{\partial\theta}&= \frac{\partial \bm{x}_0(z;\theta)}{\partial\theta}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}} =         \bigg[\frac{\partial\phi(\bm{x}_{1},\bm{z}_1;\theta)}  {\partial \theta} + \frac{\partial \phi(\bm{x}_{1},\bm{z}_1;\theta)}{\partial \bm{x}_{1}}\frac{\partial \bm{x}_{1}(\bm{z}_{2:T};\theta)}{\partial \theta}\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}} \\ &=\bigg[\frac{\partial \phi(\bm{x}_1,\bm{z}_1;\theta)}  {\partial \theta} + 
        \frac{\partial \phi(\bm{x}_{1},\bm{z}_1;\theta)}{\partial \bm{x}_{1}}\bigg[\frac{\partial\phi(\bm{x}_{2},\bm{z}_2;\theta)}  {\partial \theta} + \frac{\partial \phi(\bm{x}_{2},\bm{z}_2;\theta)}{\partial \bm{x}_{2}}\frac{\partial \bm{x}_{2}(\bm{z}_{3:T};\theta)}{\partial \theta}\bigg]\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}} \\ 
        &= \dots \\
        &=\bigg[\frac{\partial \phi(\bm{x}_1;\theta)}  {\partial \theta} + 
        \sum_{i=2}^{T}\frac{\partial \phi(x_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j;\theta)}{\partial \bm{x}_j}\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}.
    \end{aligned}
\end{equation*}
Therefore, we can reach the conclusion that
\begin{equation}
   \begin{aligned}
&\nabla_\theta\mathbb{E}_z[R(\bm{x}_0(z;\theta))] = \mathbb{E}_z[\nabla_\theta R(\bm{x}_0(z;\theta))] = \mathbb{E}_z\bigg[\bigg[\frac{\partial \phi(\bm{x}_1;\theta)}  {\partial \theta} + 
        \sum_{i=2}^{T}\frac{\partial \phi(x_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j;\theta)}{\partial \bm{x}_j}\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}} \bigg], %\\
%        =& \mathbb{E}_z\bigg[\large[\frac{\partial \phi(\bm{x}_1;\theta)}{\partial \theta}+  \sum_{i=2}^{T}\frac{\partial \phi(x_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j;\theta)}{\partial \bm{x}_j}]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}\bigg],
   \end{aligned}
\end{equation}
which means the unbiasedness of  the FO estimator.

Furthermore, the truncated BP estimator is
\begin{equation}\label{eq_truncatedBP}
    \begin{aligned}
\nabla_{\theta}R(\bm{x}_0)_{\mathrm{truncated}}=\bigg[\frac{\partial \phi_1(\bm{x}_1,\bm{z}_1;\theta)}{\partial \theta}+         \sum_{i=2}^{K}\frac{\partial \phi_i(\bm{x}_i,\bm{z}_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j,\bm{z}_j;\theta)}{\partial \bm{x}_j}\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}.
    \end{aligned}
\end{equation}
The structural bias of the truncated BP estimator can be specified by combining the above results:
\begin{equation*}%\label{eq_BP}
\begin{aligned}
\nabla_{\theta}\mathbb{E}[R(\bm{x}_0)] - \mathbb{E}[\nabla_{\theta}R(\bm{x}_0)_{\mathrm{truncated}}]= \mathbb{E}_{z_{1:T}} 
\bigg[
\sum_{i=T'-1}^{T}\frac{\partial \phi(\bm{x}_i;\theta)}{\partial \theta}\prod_{j=1}^{i-1}\frac{\partial \phi(\bm{x}_j;\theta)}{\partial \bm{x}_j}\bigg]^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}.
\end{aligned}
\end{equation*}
So we end the proof.
\end{proof}
\subsubsection{Proof of Proposition \ref{prop2} and additional assumptions}
\begin{assumption}\label{ass1}
Define $R(z;\theta)$ as $R(\bm{x}_0(z;\theta))$. Assume that $R(z;\theta)$ is differentiable with respect to $\theta$ almost surely, $\mathbb{P}(R(\theta)=r)=0$ for every $r\in \mathbb{R}$, and Lipschitz condition holds for every $\theta_1$ and $\theta_2$:
\begin{equation*}
    |R(z;\theta_1)-R(z;\theta_2)| \le m_1(z)|\theta_1-\theta_2|,
\end{equation*}
where $m_1(z)$ is integrable.
\end{assumption}

\begin{assumption}\label{ass2}
    For any $\bm{x}_t$, whose randomness comes from $z$, the density $f(x_t;\theta)$  is differentiable with 
 respect to $\theta$, and uniform integrability holds:
 \begin{equation*}
\sup_{\theta}\bigg|R(x_t;\theta) \frac{\partial}{\partial \theta}f(x_t;\theta)\bigg| \le m_2(x_t),
 \end{equation*}
 where $m_2(x_t)$ is integrable.
\end{assumption}
\begin{assumption}\label{ass3}
$R(z;\theta)$ is  twice continuously differentiable. 
    The following functions are integrable: $m_1(\cdot)^2$, $m_2(\cdot)^2$, $\sup_{\theta}|R(\cdot;\theta)|\times m_1(\cdot)$, $\sup_{\theta}|R(\cdot;\theta)|\times \sup_{\theta}|R''(\cdot;\theta)|$.
\end{assumption}

%Proof of Proposition \ref{prop2}:
\begin{proof}[Proof of Proposition \ref{prop2}]
Since $R(z;\theta)$ is the reward function  and the random variables $\bm{z}_{1:T}$ are Gaussian distributions in our case, it is easy to check the above assumptions are satisfied. By applying the Theorem 2 in \cite{cui2020variance}, we can reach the conclusion.
\end{proof}

\subsubsection{Proof of Theorem \ref{prop3}}
\begin{proof}
 The RLR estimator contains three parts: FO estimator terms, HO estimator terms, and ZO estimator terms. For simplicity, we can consider there terms with an FO estimator term, an HO estimator term, and an ZO estimator term. Substituting the specific form of the iteration process, we have  $$\bm{x}_0 = \varphi(\bm{x}_1, \bm{z}_1; \theta),\quad \bm{x}_1 = \varphi(\bm{x}_2;\theta+\bm{z}_2),\quad \bm{x}_2=\varphi(\bm{x}_3;\theta)+\bm{z}_3,$$
where $\bm{x}_1 = \varphi(\bm{x}_2;\theta+\bm{z}_2)$ is an HO estimator term and $\bm{x}_2=\varphi(\bm{x}_3;\theta)+\bm{z}_3$ is an ZO estimator term. 

Then we have
\begin{equation}
    \begin{aligned}
\nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}[R(\bm{x}_0)] &= \nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}[R(\varphi(\bm{x}_1, \bm{z}_1; \theta))] = \nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}[R(\varphi(\varphi(\bm{x}_2;\theta+\bm{z}_2), \bm{z}_1; \theta))] \\
&=\nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}[R(\varphi(\varphi(\varphi(\bm{x}_3;\theta)+\bm{z}_3;\theta+\bm{z}_2), \bm{z}_1; \theta))].
    \end{aligned}
\end{equation}
 Here $\theta$ appears three times, once in each $\varphi$. When taking derivative with respect to $\theta$, we need to alternate between the chain rule and the compound function rule. First, we apply chain rule and take derivative with respect $\theta$ in the first layer of $\varphi$. The interchange of 
 derivative and expectation can also be justified by the dominated convergence theorem \citep{rudin1987real}:
\begin{equation}\label{eq1}
\nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}[R(\bm{x}_0)] =  \mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}\bigg[\frac{\partial \varphi(\bm{x}_1;\theta)}{\partial \theta}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}\bigg] + \nabla_{\theta}\mathbb{E}_{\bm{z}_2,\bm{z}_3}\bigg[R(\varphi(\varphi(\bm{x}_2;\theta+\bm{z}_2)))\bigg], 
\end{equation}
where the first term is the first-order term. Then we continue to decompose the second term:
\begin{equation}\label{eq2}
    \begin{aligned}
\nabla_{\theta}\mathbb{E}_{\bm{z}_2,\bm{z}_3}\bigg[R(\varphi(\varphi(\bm{x}_2;\theta+\bm{z}_2)))\bigg] &= \mathbb{E}_{\bm{z}_2+\theta \sim \mathcal{N}(\theta, \sigma^2I), \bm{z}_3}\bigg[R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_2+\theta)\bigg] \\ &\  \quad+ 
\mathbb{E}_{\bm{z}_2,\bm{z}_3}\bigg[\frac{\partial}{\partial \bm{x}_2}R(\varphi(\varphi(\bm{x}_2;\theta+\bm{z}_2)))\frac{\partial \bm{x}_2}{\partial \theta}\bigg]\\
&= \mathbb{E}_{\bm{z}_2, \bm{z}_3}\bigg[R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_2)\bigg] + \nabla_{\theta}\mathbb{E}_{\bm{z}_3}\bigg[R(\varphi(\varphi(\varphi(\bm{x}_3;\theta)+\bm{z}_3)))\bigg]\\
&= \mathbb{E}_{\bm{z}_2, \bm{z}_3}\bigg[R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_2)\bigg] + \nabla_{\theta}\mathbb{E}_{x_2\sim  \mathcal{N}(\varphi(\bm{x}_3;\theta), \sigma^2I)}\bigg[R(\varphi(\varphi(\bm{x}_2)))\bigg]\\
&= \mathbb{E}_{\bm{z}_2, \bm{z}_3}\bigg[R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_2)\bigg] + \mathbb{E}_{\bm{z}_3}\bigg[R(\bm{x}_0)D^{\top}_\theta\varphi(\bm{x}_{3};\theta)\nabla_{\theta}\ln f(\bm{z}_3)\bigg].
    \end{aligned}
\end{equation}
Here, the first equation and the fourth equation come from the unbiasedness of LR estimator. That is to say, 
\begin{equation*}
    \nabla_{\theta}\mathbb{E}_{z\sim f(z;\theta)}[R(z)] = \mathbb{E}_{z\sim f(z;\theta)}[R(z)\nabla_{\theta}\ln f(z)].
\end{equation*}

Combine Equation (\ref{eq1}) and (\ref{eq2}), we can get the unbiasedness of RLR estimator with three terms: 
\begin{equation*}
    \begin{aligned}
&\nabla_{\theta}\mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}\bigg[R(\varphi(\varphi(\varphi(\bm{x}_3;\theta)+\bm{z}_3;\theta+\bm{z}_2), \bm{z}_1; \theta))\bigg] \\ &=  \mathbb{E}_{\bm{z}_1,\bm{z}_2,\bm{z}_3}\bigg[\frac{\partial \varphi(\bm{x}_1;\theta)}{\partial \theta}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x_0}}\bigg] + \mathbb{E}_{\bm{z}_2, \bm{z}_3}\bigg[R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_2)\bigg] + \mathbb{E}_{\bm{z}_3}\bigg[R(\bm{x}_0)D^{\top}_\theta\varphi(\bm{x}_{3};\theta)\nabla_{\theta}\ln f(\bm{z}_3)\bigg].
\end{aligned}
\end{equation*}

Since the sum of unbiased estimators is still unbiased, it is easy to generalize the result for any number of terms along the lines of the above proof. Also,  the unbiasedness of the estimators does not change no matter how we pick the combination of the HO estimator and the ZO estimator. Therefore, we can reach the conclusion that 
\begin{equation*}
    \begin{aligned}
        &\nabla_\theta \mathbb{E}[R(\phi_{1:T}(\bm{x}_T;\theta))] \\ &=
        \mathbb{E}_{\bm{z}\sim f(\bm{z}),j\sim \mathcal{U}(1,T-h)} \bigg[\frac{\partial\varphi(\bm{x}_1;\theta)}{\partial\theta}^{\top}\frac{dR(\bm{x}_0)}{d\bm{x}_0}+
        \sum_{i \in C}R(\bm{x}_0)\nabla_{\theta}\ln f(\bm{z}_i)+R(\bm{x}_0)D^{\top}_\theta\phi_{j:j+h}(\bm{x}_{j+h};\theta)\nabla_{\theta}\ln f(\bm{z}_j)\bigg].
    \end{aligned}
\end{equation*}
\end{proof}


\subsubsection{Variance of the RLR estimator}

In this section we discuss the variance of the RLR estimator, which consists of 3 terms, which are denoted as A, B and C respectively:
\begin{equation}
    \begin{aligned}
RLR=\underbrace{\frac{\partial\varphi(\bm{x}_1;\theta)}{\partial\theta}}_{A}+
&\underbrace{R(\bm{x}_0)D^{\top}_\theta\phi_{j:j+h}(\bm{x}_{j+h};\theta)\nabla\ln f(\bm{z}_j)}_{B}+\underbrace{\sum_{i \in C}R(\bm{x}_0)\nabla\ln f(\bm{z}_i)}_{C}.
    \end{aligned}
\end{equation}

It is easy to verify that the variance of RLR can be bounded by the variances of terms A, B and C:
\begin{equation}
\begin{aligned}
    \text{Var}(RLR)&=\text{Var}(A + B + C)\\ &= \text{Var}(A) + \text{Var}(B) + \text{Var}(C) + 2\ \text{Cov}(A, B) + 2\ \text{Cov}(A, C) + 2\ \text{Cov}(B, C)\\
    &\leq \text{Var}(A) + \text{Var}(B) + \text{Var}(C)+2\bigg(\sqrt{\text{Var}(A)\text{Var}(B)}+\sqrt{\text{Var}(A)\text{Var}(C)}+\sqrt{\text{Var}(B)\text{Var}(C)} \bigg),
\end{aligned}  
\end{equation}
which gives a upper bound for the variance of the RLR estimator. Next we derive the specific expression of the terms $\text{Var}(A)$, $\text{Var}(B)$ and $\text{Var}(C)$.

First, the term \( A \) represents the original exact BP without any injected noise, so its variance is 0. 
Next, for the terms \( B \) and \( C \), since both are LR estimators, we present them in the following unified form for simplicity:
$$\mathbb{\eta}=\mathbb{E}[ R(z)\nabla_\theta \ln f(z,\theta)],$$
where $\nabla_\theta \ln f(z,\theta)=D^{\top}_\theta\phi_{j:j+h}(\bm{x}_{j+h};\theta)\nabla\ln f(\bm{z}_j)$ in the term B. The variance of $\eta$ is given by
$$\mathrm{Var}(\eta)=\mathbb{E}[(R(z)\nabla_\theta \ln f(z,\theta))^2]-\mu(\theta)^2,$$where $\mu(\theta)$ is the estimator mean.

To better characterize the variance, we now derive an alternative form of the gradient:
\begin{equation}\label{appendix_alter_form}
    \begin{aligned}
        \mathbb{E}[ R(z)\nabla_\theta \ln f(z,\theta)]&=\int \lim_{h\to 0}\frac{f(z;\theta+h)-f(z;\theta)}{h} R(z)dz\\
        &= \lim_{h\to 0}\int\frac{f(z;\theta+h)-f(z;\theta)}{h} R(z)dz\\
        &=\lim_{h\to 0}\frac{1}{h}\int f(z;\theta)\bigg(\frac{f(z;\theta+h)}{f(z;\theta)}-1\bigg)R(z)dz\\
        &=\lim_{h\to 0}\frac{1}{h}(\mathbb{E}[\omega(\theta,h)R(z)]-\mathbb{E}_{f}[R(z)]),
    \end{aligned}
\end{equation}
where the importance weight $\omega(\theta,h)=\frac{f(z;\theta+h)}{f(z;\theta)}$.

With this alternative form Equation (\ref{appendix_alter_form}), we have the variance of the LR estimator
\begin{equation}
    \begin{aligned}
        \mathrm{Var}(\eta)=\lim_{h\to0}\mathbb{E}[(w(\theta,h)-1)^2R(z)^2]-\mu(\theta)^2,
    \end{aligned}
\end{equation}
 By the Hammersley-Chapman-Robbins bound, we can derive an upper bound for the variance:
$$\mathrm{Var}(\eta)\geq \sup_{h}\frac{(\mu(\theta+h)-\mu(\theta)^2)}{\mathbb{E}[w(\theta,h)-1]^2},$$
which is a generalization of the more widely-known Cramer-Rao bound and describes the minimal
variance achievable by the estimator.


Under limited computational resources where full BP is infeasible, the only unbiased gradient estimation methods available are RLR, zeroth-order optimization, and RL. Compared to the latter two, RLR incorporates \( h \)-length half-order optimization and a single-step precise BP. According to Proposition \ref{prop2} and the subsequent discussion, it is evident that RLR achieves the lowest variance among these methods.


\subsubsection{\text{Proof of Theorem \ref{theo_convergence}}}
\begin{proof}
  Since \( R(\cdot) \) is \( L \)-smooth, we have
\begin{equation}
    \begin{aligned}
E[R(\theta_{k+1}) | \mathcal{F}_k] &= R(\theta_k) + \mathbb{E}\left[ \langle \nabla R(\theta_k),\theta_{k+1}-\theta_k\rangle| \mathcal{F}_k \right] + \frac{L}{2} \mathbb{E}\left[ \| \theta_{k+1} - \theta_k \|^2 | \mathcal{F}_k \right]\\        
&= R(\theta_k) -\gamma\mathbb{E}\left[ \langle \nabla R(\theta_k),\nabla R(\theta_k)+\epsilon\rangle | \mathcal{F}_k \right] + \frac{L\gamma^2}{2} \mathbb{E}\left[ \|\nabla R(\theta_k)+\epsilon \|^2 | \mathcal{F}_k \right]\\
&\leq R(\theta_k)-\gamma(1-\frac{L\gamma}{2})\|\nabla R(\theta_k)\|^2+\frac{L\gamma^2\sigma_{\mathrm{RLR}}^2}{2}\\
&\leq R(\theta_k)-\frac{\gamma}{2}\|\nabla R(\theta_k)\|^2+\frac{L\gamma^2\sigma{_{\mathrm{RLR}}}^2}{2},
\end{aligned}
\end{equation}
where the last inequality holds if \( \gamma \leq 1/L \). By taking expectations over the filtration \( \mathcal{F}_k \), we have
\[
\mathbb{E}[R(\theta_{k+1})] \leq \mathbb{E}[R(\theta_k)] -\frac{\gamma}{2}\mathbb{E}[\nabla R(\theta_k)^2] + \frac{L\gamma^2\sigma^2_{\mathrm{RLR}}}{2},
\]
which is equivalent to
\[
\frac{\gamma}{2}\mathbb{E}[\nabla R(\theta_k)^2]\leq\frac{2}{\gamma}(\mathbb{E}(\theta_{k})-\mathbb{E}(\theta_{k+1}))+\gamma L\sigma^2_{\mathrm{RLR}}.
\]

Taking the average over \( k = 0, 1, \dots, K \), we have
\[
\frac{1}{K+1} \sum_{k=0}^K [\mathbb{E}[\nabla R(\theta_k)^2]] \leq  \frac{2(R(x_0) - R^\ast)}{K+1} + \gamma L \sigma^2_{\mathrm{RLR}}.
\]
Defining \( \Delta_0 := f(x_0) - f^\ast \), if we set the step size
\[
\gamma= \bigg[\bigg(\frac{2 \Delta_0}{(K+1) L\sigma^2_{\mathrm{RLR}}}\bigg)^{-\frac{1}{2}} + L\bigg]^{-1},
\]
then we have
$$\frac{1}{K+1}\sum_{k=0}^K\mathbb{E}(\|\nabla R(\theta_k)\|^2)\leq \sqrt{\frac{8L\Delta_0\sigma^2_{\mathrm{RLR}}}{K+1}}+\frac{2L\Delta_0}{K+1},$$
which completes the proof.
\end{proof}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
