\section{Related works}
\paragraph{Diffusion Probabilistic Models.} Denoising Diffusion Model \cite{ddpm} is one of the strongest models for generation tasks, especially for visual generation \cite{LDM, DiT, videocrafter2}. Extensive research has been conducted from theoretical and empirical perspectives \cite{ddim, EDM, score_sde}. It has achieved phenomenal success in muti-modality generation, including image, video, audio, and 3D shapes. The DM is trained on enormous images and videos from the internet \cite{frozen_in_time, wang2023internvid_viclip, schuhmann2022laion}. Empowered by modern architecture \cite{transformer}, it has powerful learning capability for Pixel Space Distribution. 

\paragraph{Alignment and Post-training.} After pre-training to learn the distribution of the targeted modality \cite{achiam2023gpt4, scaling_neural_LM}, post-training is conducted to align the model toward specific preferences or tune the model to optimize a particular objective. RL has been utilized to align the foundation models toward various objectives \cite{RLHF, RLHF_blog, ddpo}. Supervised learning can also be applied to the post-training phase \cite{DPO}, either optimizing an equivalent objective \cite{diffusion_dpo} or directly differentiating the reward model \cite{draft, alignprop, vader}. For DM, most methods use a neural reward model to align the pre-trained model, and there has been a continual effort to design better reward models \cite{he2024videoscore, xu2024visionreward, imagereward}.

\paragraph{Forward Learning Methods.} 
\textcolor{black}{After extensive exploration of model training using forward inferences only \citep[see, e.g.,][]{peng2022new, hinton2022forward}, forward-learning methods \cite{salimans2017evolution, mezo, deepzero, oneforward} based on stochastic gradient estimation have recently emerged as a promising alternative to classical BP for large-scale machine learning problems \cite{HiZOO, revisi_ZO}. Subsequent research \cite{ren2024flops, chen2024enhancing} has further optimized computational and storage overhead from various perspectives, achieving greater efficiency.}


\begin{figure*}[!th]
    \centering
    \includegraphics[width=\textwidth]{figures/RLR.pdf}
    \vspace{-0.5cm}
    \caption{The computation paradigm of the RLR optimizer.}
    \label{fig:algdiagram}
    \vspace{-0.3cm}
\end{figure*}