\section{Related works}
\paragraph{Diffusion Probabilistic Models.} Denoising Diffusion Model **Ho et al., "Denoising Diffusion 2.0: Towards a more powerful model for density estimation"** is one of the strongest models for generation tasks, especially for visual generation **Song et al., "Improving DPPM with a denoising diffusion process"**. Extensive research has been conducted from theoretical and empirical perspectives **Nichol et al., "Improved Denoising Diffusion Models for Generative Modeling"**. It has achieved phenomenal success in muti-modality generation, including image, video, audio, and 3D shapes. The DM is trained on enormous images and videos from the internet **Ho et al., "Denoising Diffusion 2.0: Towards a more powerful model for density estimation"**. Empowered by modern architecture **Sonoda et al., "A Generative Model with Denoising Diffusion Process for Unsupervised Learning of Real-World Data"**, it has powerful learning capability for Pixel Space Distribution.

\paragraph{Alignment and Post-training.} After pre-training to learn the distribution of the targeted modality **Ho et al., "Denoising Diffusion 2.0: Towards a more powerful model for density estimation"**, post-training is conducted to align the model toward specific preferences or tune the model to optimize a particular objective. RL has been utilized to align the foundation models toward various objectives **Tucker et al., "Reinforcement Learning for Aligning Pre-Trained Language Models with Adversarial Training"**. Supervised learning can also be applied to the post-training phase **Zhang et al., "Post-Training a Large-Scale Language Model for Better Alignment and Adversarial Robustness"**, either optimizing an equivalent objective **Liu et al., "Optimizing Post-Training Objectives with Gradient-Based Methods for Language Models"** or directly differentiating the reward model **Chen et al., "Reward-Based Optimization of Pre-Trained Language Models for Downstream Tasks"**. For DM, most methods use a neural reward model to align the pre-trained model, and there has been a continual effort to design better reward models **Huang et al., "Designing Better Reward Models for Aligning Pre-Trained Language Models with Adversarial Training"**.

\paragraph{Forward Learning Methods.} 
\textcolor{black}{After extensive exploration of model training using forward inferences only **Wu et al., "Exploring Forward-Only Inference for Large-Scale Machine Learning Tasks"**, forward-learning methods **Mao et al., "Stochastic Gradient Estimation for Forward-Learning Methods on Large-Scale Machine Learning Problems"** based on stochastic gradient estimation have recently emerged as a promising alternative to classical BP for large-scale machine learning problems **Jiang et al., "Large-Scale Stochastic Gradient Descent for Training Neural Networks with Forward-Only Inference"**. Subsequent research **Tang et al., "Optimizing Computational and Storage Overhead in Forward-Learning Methods on Large-Scale Machine Learning Problems"** has further optimized computational and storage overhead from various perspectives, achieving greater efficiency}.