@article{Elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Neel, Nanda and Olsson, Catherine and others},
  journal={Transformer Circuits Thread},
  url={https://transformer-circuits.pub/2021/framework/index.html},
  year={2020}
}

@article{brandon2024reducing,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan},
  journal={arXiv preprint arXiv:2405.12981},
  year={2024}
}

@inproceedings{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={36},
  pages={16318--16352},
  year={2023}
}

@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}

@inproceedings{dao2024transformers,
  title={Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality},
  author={Dao, Tri and Gu, Albert},
  booktitle={Proceedings of the Forty-First International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{elnokrashy2022depth,
  title={Depth-wise attention (dwatt): A layer fusion method for data-efficient classification},
  author={ElNokrashy, Muhammad and AlKhamissi, Badr and Diab, Mona},
  journal={arXiv preprint arXiv:2209.15168},
  year={2022}
}

@article{fang2023cross,
  title={Cross-Layer Retrospective Retrieving via Layer Attention},
  author={Fang, Yanwen and Cai, Yuxi and Chen, Jintai and Zhao, Jingyu and Tian, Guangjian and Li, Guodong},
  journal={arXiv preprint arXiv:2302.03985},
  year={2023}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={5232--5270},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  booktitle={Proceedings of the First Conference on Language Modeling},
  year={2024}
}

@inproceedings{hanna2024have,
  title={Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms},
  author={Hanna, Michael and Pezzelle, Sandro and Belinkov, Yonatan},
  booktitle={Proceedings of Conference on Language Modeling (COLM)},
  year={2024}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages={770--778},
  year={2016}
}

@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages={4700--4708},
  year={2017}
}

@article{leviathan2024selective,
  title={Selective attention improves transformer},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  journal={arXiv preprint arXiv:2410.02703},
  year={2024}
}

@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@inproceedings{ni2024benchmarking,
  title={Benchmarking and Understanding Compositional Relational Reasoning of LLMs},
  author={Ni, Ruikang and Xiao, Da and Meng, Qingye and Li, Xiangyu and Zheng, Shihui and Liang, Hongliang},
  booktitle={Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI)},
  year={2025}
}

@inproceedings{pagliardini2024denseformer,
  title={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},
  author={Pagliardini, Matteo and Mohtashami, Amirkeivan and Fleuret, Francois and Jaggi, Martin},
  booktitle={Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@article{peng2024eagle,
  title={Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence},
  author={Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Du, Xingjian and Ferdinan, Teddy and Hou, Haowen and others},
  journal={arXiv preprint arXiv:2404.05892},
  year={2024}
}

@inproceedings{tay2021omninet,
  title={Omninet: Omnidirectional representations from transformers},
  author={Tay, Yi and Dehghani, Mostafa and Aribandi, Vamsi and Gupta, Jai and Pham, Philip M and Qin, Zhen and Bahri, Dara and Juan, Da-Cheng and Metzler, Donald},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={10193--10202},
  year={2021},
  organization={PMLR}
}

@article{wang2024strengthening,
  title={Strengthening Layer Interaction via Dynamic Layer Attention},
  author={Wang, Kaishen and Xia, Xun and Liu, Jian and Yi, Zhang and He, Tao},
  journal={arXiv preprint arXiv:2406.13392},
  year={2024}
}

@inproceedings{yang2024parallelizing,
  title={Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author={Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon},
  booktitle={Proceedings of the Thirty-Eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@article{ye2024differential,
  title={Differential transformer},
  author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  booktitle={The Thirteenth International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{zhu2024hyper,
  title={Hyper-Connections},
  author={Zhu, Defa and Huang, Hongzhi and Huang, Zihao and Zeng, Yutao and Mao, Yunyao and Wu, Banggu and Min, Qiyang and Zhou, Xun},
  booktitle={Proceedings of the Thirteenth International Conference on Learning Representations (ICLR)},
  year={2025}
}

