[
  {
    "index": 0,
    "papers": [
      {
        "key": "he2016deep",
        "author": "He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian",
        "title": "Deep residual learning for image recognition"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "huang2017densely",
        "author": "Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q",
        "title": "Densely connected convolutional networks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "pagliardini2024denseformer",
        "author": "Pagliardini, Matteo and Mohtashami, Amirkeivan and Fleuret, Francois and Jaggi, Martin",
        "title": "DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhu2024hyper",
        "author": "Zhu, Defa and Huang, Hongzhi and Huang, Zihao and Zeng, Yutao and Mao, Yunyao and Wu, Banggu and Min, Qiyang and Zhou, Xun",
        "title": "Hyper-Connections"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "elnokrashy2022depth",
        "author": "ElNokrashy, Muhammad and AlKhamissi, Badr and Diab, Mona",
        "title": "Depth-wise attention (dwatt): A layer fusion method for data-efficient classification"
      },
      {
        "key": "fang2023cross",
        "author": "Fang, Yanwen and Cai, Yuxi and Chen, Jintai and Zhao, Jingyu and Tian, Guangjian and Li, Guodong",
        "title": "Cross-Layer Retrospective Retrieving via Layer Attention"
      },
      {
        "key": "wang2024strengthening",
        "author": "Wang, Kaishen and Xia, Xun and Liu, Jian and Yi, Zhang and He, Tao",
        "title": "Strengthening Layer Interaction via Dynamic Layer Attention"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "conmy2023towards",
        "author": "Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\\`a}",
        "title": "Towards automated circuit discovery for mechanistic interpretability"
      },
      {
        "key": "hanna2024have",
        "author": "Hanna, Michael and Pezzelle, Sandro and Belinkov, Yonatan",
        "title": "Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "Elhage2021mathematical",
        "author": "Elhage, Nelson and Neel, Nanda and Olsson, Catherine and others",
        "title": "A mathematical framework for transformer circuits"
      },
      {
        "key": "wang2024strengthening",
        "author": "Wang, Kaishen and Xia, Xun and Liu, Jian and Yi, Zhang and He, Tao",
        "title": "Strengthening Layer Interaction via Dynamic Layer Attention"
      },
      {
        "key": "ni2024benchmarking",
        "author": "Ni, Ruikang and Xiao, Da and Meng, Qingye and Li, Xiangyu and Zheng, Shihui and Liang, Hongliang",
        "title": "Benchmarking and Understanding Compositional Relational Reasoning of LLMs"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "brandon2024reducing",
        "author": "Brandon, William and Mishra, Mayank and Nrusimha, Aniruddha and Panda, Rameswar and Kelly, Jonathan Ragan",
        "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "tay2021omninet",
        "author": "Tay, Yi and Dehghani, Mostafa and Aribandi, Vamsi and Gupta, Jai and Pham, Philip M and Qin, Zhen and Bahri, Dara and Juan, Da-Cheng and Metzler, Donald",
        "title": "Omninet: Omnidirectional representations from transformers"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ye2024differential",
        "author": "Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu",
        "title": "Differential transformer"
      },
      {
        "key": "leviathan2024selective",
        "author": "Leviathan, Yaniv and Kalman, Matan and Matias, Yossi",
        "title": "Selective attention improves transformer"
      },
      {
        "key": "liu2024deepseek",
        "author": "Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others",
        "title": "Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      },
      {
        "key": "dao2024transformers",
        "author": "Dao, Tri and Gu, Albert",
        "title": "Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality"
      },
      {
        "key": "peng2024eagle",
        "author": "Peng, Bo and Goldstein, Daniel and Anthony, Quentin and Albalak, Alon and Alcaide, Eric and Biderman, Stella and Cheah, Eugene and Du, Xingjian and Ferdinan, Teddy and Hou, Haowen and others",
        "title": "Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence"
      },
      {
        "key": "yang2024parallelizing",
        "author": "Yang, Songlin and Wang, Bailin and Zhang, Yu and Shen, Yikang and Kim, Yoon",
        "title": "Parallelizing Linear Transformers with the Delta Rule over Sequence Length"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "fedus2022switch",
        "author": "Fedus, William and Zoph, Barret and Shazeer, Noam",
        "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity"
      },
      {
        "key": "dai2024deepseekmoe",
        "author": "Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others",
        "title": "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models"
      }
    ]
  }
]