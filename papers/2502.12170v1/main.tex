\PassOptionsToPackage{prologue,dvipsnames}{xcolor} % XD for importing xcolor with dvipsnames
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{array} % mqy
\usepackage{multirow} % mqy
% \usepackage{minted} %mqy
% https://tex.stackexchange.com/questions/280590/work-around-for-minted-code-highlighting-in-arxiv
% \usepackage[finalizecache,cachedir=.]{minted}
\usepackage[frozencache,cachedir=.]{minted}
\usepackage{pifont} %mqy

\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[accepted]{icml2024} 

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

% \usepackage{algorithm} %mqy
% \usepackage{algpseudocode} %mqy

% from gptfuzzer tdsc
%\usepackage{xeCJK}
%\setCJKmainfont{SimSun}
%\usepackage{titlesec}
\usepackage{CJKutf8}

\usepackage[dvipsnames]{xcolor}
% XD: https://tex.stackexchange.com/questions/668241/lowercase-mathscr-and-mathcal-letters
% https://tex.stackexchange.com/questions/231322/how-to-get-the-lowercase-calligraphic-symbols
% https://tex.stackexchange.com/questions/247531/how-to-use-boondox-calligraphic-font-in-latex-without-replacing-mathcal-command
\usepackage[scr=boondoxo,  % heavily sloped. XD: append o to boondox to be less sloped, for w
            % cal=esstix   % slightly sloped
            ]{mathalpha}
\usepackage[mathcal]{euscript}
% \usepackage[mathscr]{euscript}
\usepackage{makecell}

\DeclareMathAlphabet{\mathbbb}{U}{bbold}{m}{n}
% https://tex.stackexchange.com/questions/601132/use-of-package-bbold-and-mathbb-at-same-time
\newcommand{\x}{\times}
\newcommand{\X}{\texttimes}
\newcommand{\R}{\mathbb{R}}
\newcommand{\W}{\mathcal{W}}
\newcommand{\w}{\mathscr{w}}
\newcommand{\TW}{\tilde{W}}
\newcommand{\T}{\top}
\newcommand{\D}{$\Delta$}
\newcommand{\op}[1] {\operatorname{#1}}
\newcommand{\B}[1] {\textbf{#1}}
% \newcommand{\arrow}{\verb|->|}

% https://github.com/yihui/knitr/issues/1992
% https://tex.stackexchange.com/questions/312/correctly-typesetting-a-tilde
\usepackage{textcomp}
\newcommand{\tildemid}{\raisebox{0.5ex}{\texttildelow}}
% https://tex.stackexchange.com/questions/60453/reducing-font-size-in-equation
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}%
\newcommand*{\Resize}[2]{\resizebox{#1}{!}{$#2$}}%

\newcommand{\pct}[1] { $\Scale[0.8]{\x\textbf{#1}}$}
\newcommand{\pcttwo}[2] { $\Scale[0.8]{\x\textbf{#1}/{#2}}$}


\newcommand{\etal}{\emph{et al.~}}
\newcommand{\ie}{\emph{i.e.,~}}
\newcommand{\eg}{\emph{e.g.,~}}

\usepackage{soul}  % XD
\newcommand{\xdd}[1] {\textcolor{red}{\st{#1}}} % XD delete
% \newcommand{\xdd}[1] {} % XD delete

\newcommand{\xda}[1] {\textcolor{red}{#1}} % XD add
% \newcommand{\xda}[1] {#1} % XD add
\newcommand{\xdr}[2]{\st{#1}\textcolor{red}{#2}} % XD replace
% \newcommand{\xdr}[2]{#2} % XD replace


\newcommand{\cmark}{\ding{51}}% #mqy
\newcommand{\xmark}{\ding{55}}% #mqy
% \newcommand{\mqytodo}[1]{\textcolorblue}{TODO:{#1}}} % mqy todo
\newcommand{\mqyadd}[1]{\textcolor{blue}{#1}} % mqy add 
\newcommand{\mqyr}[2]{\st{#1}\textcolor{blue}{#2}} % mqy replace
\newcommand{\mqyd}[1] {\textcolor{blue}{\st{#1}}} % mqy delete
\newcommand{\mqyc}[1] {\begin{CJK}{UTF8}{gbsn}
\textcolor{blue}{\underline{#1}}
\end{CJK}} % mqy comment

\newcommand{\xdc}[1] {\begin{CJK}{UTF8}{gbsn}
\textcolor{red}{\underline{#1}}
\end{CJK}} % XD comment


\newcommand{\lxyc}[1] {\begin{CJK}{UTF8}{gbsn}
\textcolor{teal}{#1}\end{CJK}} % LXY comment

% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Multiway Dynamic Dense Connections}

\begin{document}

\twocolumn[
\icmltitle{MUDDFormer: Breaking Residual Bottlenecks in Transformers \\via Multiway Dynamic Dense Connections}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Da Xiao}{yyy}
\icmlauthor{Qingye Meng}{comp}
\icmlauthor{Shengping Li}{comp}
\icmlauthor{Xingyuan Yuan}{comp}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Beijing University of Posts and Telecommunications, Beijing, China}
\icmlaffiliation{comp}{ColorfulClouds Technology Co., Ltd., Beijing, China}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Da Xiao}{xiaoda99@bupt.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Multi-Head Attention, Transformer}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution

\begin{abstract}

We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block.
MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with \tildemid1.8\X--2.4\X\ compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23\% parameters and 0.4\% computation. Code in JAX and PyTorch and pre-trained models are available at \url{https://github.com/Caiyun-AI/MUDDFormer}.
\end{abstract}

\section{Introduction} \label{intro}
Residual connections \cite{he2016deep}, which help mitigate the vanish gradient problem, have become indispensable to training deep learning architectures from CNNs to Transformers, the latter becoming the de facto backbone for foundation models.
Though being very simple and effective, residual connections still have limitations to be solved, especially with deep Transformers with dozens of layers made common by prevailing Transformer-based LLMs.

\begin{figure}[!t]
% \vskip -0.1in
\centering
\centerline{\includegraphics[width=0.9\columnwidth]{Downstream_tasks_acc.pdf}}
\vskip -0.1in
\caption{Downstream average accuracy of Pythia and MUDDPythia with different sizes.}% The larger the model, the larger the gains brought by MUDD connections.}
\label{fig:muddpythia-downstream}
\vskip -0.1in
\end{figure}

On one hand, although theoretical \cite{merrill2022saturated} and experimental \cite{tay2021scale} work have suggested that adding layers increases the expressive capacity and generalization performance of Transformers,
it is observed that increasing depth beyond a certain point yields diminishing returns \cite{petty2023impact}.
The common practice of using Pre-Norm to stabilize training leads to the issue of representation collapse \cite{liu2020understanding}, where hidden features in deeper layers become highly similar, and for popular families of open-weight LLMs, a large fraction of the layers can be removed with minimal degradation of performance \cite{gromov2024unreasonable}. 

On the other hand, mechanistic interpretability studies reveal that Transformers do in-context learning tasks by composing model components (attention heads and MLPs) across different layers to form circuits, \cite{Elhage2021mathematical,wang2211interpretability,merullo2024talking,ni2024benchmarking},
where layers communicate with each other by writing to and reading from different subspaces of the residual stream.
The residual stream as the shared communication channel may be overloaded and become the bottleneck for very deep models,
hindering the formation of sophisticated circuits spanning distant layers necessary for complex tasks.

Dense connections \cite{huang2017densely} was proposed as a promising solution to the above issues,
by allowing subsequent layers to directly access outputs of all preceding layers (\cref{fig:arch} (a)).
It has shown effectiveness for CNNs with DenseNet \cite{huang2017densely}, and recently for Transformers with DenseFormer \cite{pagliardini2024denseformer}.
However, these dense connection approaches use static (either fixed \cite{huang2017densely} or learnable \cite{pagliardini2024denseformer}) dense connection weights that are shared across sequence positions and different input streams of a Transformer block.
As will be shown, this rigidity severely limits their expressive capacity in Transformers.

In this work, we propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective approach to address the shortcomings of residual connections. %by unlocking the full potential of dense connections.
Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input (the query, key, value or residual) of a Transformer block.
These weights are used by depth-wise aggregate modules to combine outputs from all preceding layers, creating multiple input streams for the current layer.
MUDD connections can be seen as depth-wise multi-head attention \cite{vaswani2017attention} and the cross-layer communication bandwidth is expanded far beyond the restriction of the residual stream.

MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer models. 
We conduct extensive experiments focusing on language model pretraining to evaluate MUDDFormer's effectiveness, efficiency and scalability.
MUDDFormer significantly outperforms Transformer across various model architectures and scales (from 405M model on 7B tokens to 2.8B models on 300B tokens), achieving performance of Transformers trained with \tildemid1.8\X--2.4\X\ compute.
Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining perplexity and downstream tasks and even rivals Pythia-12B in five-shot in-context learning settings (\cref{fig:muddpythia-downstream}), while adding only 0.23\% parameters and 0.4\% computation.
We also evaluate MUDD connections on vision Transformers and analyze the trained models to elucidate why MUDD connections work.

\section{Method} \label{method}
We begin with a standard Transformer decoder with $L$ layers on input sequence $X = \{x_0, ..., x_T\}$: %(omitting the final LayerNorm and UnEmbedding for simplicity):
\begin{equation} \label{eq:transformer}
\begin{split}
    & X_0 = \op{Embedding(X)} \\
    & X_i = \op{B}_i(X_{i-1}),\; i \in [1, L] \\
    & \op{Transformer}(X) = X_L
\end{split}
\end{equation}
where $\op{B}_i(\cdot)$ is the $i$th Transformer block\footnote{In this paper we use ``layer'' and ``block'' interchangeably.} composed of a multi-head attention (MHA) module followed by a fully connected feed-forward network (FFN), both wrapped with Pre-LayerNorm (LN) residual connections: %(\xda{$X_F$} is the output of the MHA module \xda{swap equations}):
\begin{equation} \label{eq:block}
\begin{split}
    X_{\textrm{A}} &= \op{MHA}(\textrm{LN}(X), \textrm{LN}(X), \textrm{LN}(X)) + X \\
    \op{B}(X) &= \op{FFN}(\textrm{LN}(X_{\textrm{A}})) + X_{\textrm{A}} \\
\end{split}
\end{equation}
In this architecture, the output $X_i \in \R^{T \x D}$ ($D$ is model dim) of layer $i$ is used as input to layer $i + 1$.
With \emph{dense connections}, the input to layer $i+1$ is an aggregation $\overline{X}_i \in \R^{T \x D}$ of outputs of \emph{all} $i + 1$ preceding layers, from the embedding till layer $i$:  $X_{:i} := \{X_0,...,X_i\}$ (\cref{fig:arch} (a)).
In this way, the cross-layer communication bandwidth is significantly increased compared to residual connections.
To obtain Transformer with dense connections, we just add a \emph{Depth-wise Aggregate} (DA) module after each layer to provide input for the next layer (cf. Eq. \ref{eq:transformer}):
\begin{equation} \label{eq:denseformer}
\begin{split}
    \overline{X}_0 &= X_0 = \op{Embedding(X)} \\
    X_i &= \op{B}_i(\overline{X}_{i-1});\;
    \overline{X}_i = \op{DA}_i(X_{:i}),\;i \in [1,L] \\
    & \op{DenseTransformer}(X) = \overline{X}_L
\end{split}
\end{equation}
In the following subsections, we progressively derive Transformer with MUDD Connections (MUDDFormer), focusing on the computation inside the DA module after layer $i$.

\subsection{Static Dense Connections} \label{static dense}
In the simplest case, the DA module aggregates previous layers' outputs by taking a weighted sum of them (\cref{fig:arch} (b), also equivalent to DenseFormer \cite{pagliardini2024denseformer}):
\begin{equation} \label{eq:static dense}
\begin{split}
    \overline{X}_i = \textrm{DA}_i^\textrm{static}(X_{:i}; \theta_i^s) = & \op{wsum}(\stackrel{\raisebox{0.3em}{\scriptsize $i+1$}}{a_i}, \stackrel{(i+1) \x T \x D}{X_{:i}}) \\
    := & \sum_{j=0}^i \stackrel{\raisebox{0.25em}{\scriptsize $1$}}{a_{ij}} \stackrel{T \x D}{X_j}
\end{split}
\end{equation}
where $\op{wsum}(\cdot)$ is the weighted sum function taking the sequences of weights and values as inputs. Scalar $a_{ij}$ is the $j$th value of the dense connection weight vector $a_i \in \R^{i+1}$ which is trainable parameters, i.e., $\theta_i^s = \{a_i\}$.

\begin{figure*}[htb]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{muddformer_arch.pdf}}
\vskip -0.1in
\caption{Architecture of Multiway Dynamic Dense Connections.} %Best viewed in color
\label{fig:arch}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Dynamic Dense Connections} \label{dynamic dense}
In Transformer-like sequence models, each layer processes information from multiple sequence positions and may benefit from differentiated and input-dependent dense connectivity for each position. %dependent on the hidden state at that position.
Dynamic dense connections expand the connection weight for $X_j$ from a static scalar $a_{ij}$ to a vector $A_{ij} \in \R^{T}$, allowing $X_j$ to contribute differentially to each position $t \in [1, T]$ of $\overline{X}_i$ based on the hidden state $X_i[t] \in \R^D$ at that position.
The $i+1$ weight vectors stack into a matrix $A_i \in \R^{T \x (i+1)}$ which is generated dynamically by a function $\mathcal{A}_i(\cdot)$ depending on $X_i$ (\cref{fig:arch} (c), cf. Eq. (\ref{eq:static dense})):
\begin{equation} \label{eq:dynamic dense}
\begin{split}
    \overline{X}_i = & \op{DA}_i^{\textrm{dynamic}}(X_{:i}; \theta_i^d)\\
    = & \op{wsum}(\stackrel{T \x (i + 1)}{A_i} = \mathcal{A}_i(\stackrel{T \x D}{X_i}), \stackrel{(i+1) \x T \x D}{X_{:i}}) \\
    := & \sum_{j=0}^i \stackrel{T \x 1}{A_{ij}} \odot \stackrel{T \x D}{X_j} \;(\textrm{with broadcasting})
\end{split}
\end{equation}
% i.e., for each position $t \in [1,T]$, $\overline{X}_i[t] = \sum_{j=0}^i A_{ij}[t] X_j[t]$,
where $A_{ij}$ is the $j$th column of $A_i$ (with a slight abuse of notation). We instantiate $\mathcal{A}_i:\R^{D} \rightarrow \R^{i+1}$ with an MLP parameterized by $W_1$ and $W_2$ which computes connection weights position-wise:
\begin{equation}
    \mathcal{A}_i(X_i) = \textrm{GELU}(\textrm{RMSNorm}(X_i)W_1)W_2 + a_i
\end{equation}
We apply RMSNorm to $X_i$ before MLP to stabilize training.
We also add a static weight vector $a_i$ acting as learnable prior for dense connectivity.
The trainable parameters are $\theta_i^d = \{W_1 \in \R^{D \x (i+1)}, W_2 \in \R^{(i+1) \x (i+1)}, a_i \in \R^{i+1} \}$.


Overall, dynamic dense connections can be viewed as a form of depth-wise single-headed self-attention \cite{vaswani2017attention} %where the output of the current layer attends to those of the previous layers (including itself) 
with query $X_i$ and keys $X_{:i}$, and the attention weights are computed from the query side.

\subsection{Multiway Dynamic Dense Connections} \label{multiway dense}
In a Transformer block, a single input is reused simultaneously as the query, key, value and residual of the MHA module (\cref{fig:arch} (e) top).
These input streams play divergent roles and we hypothesize that they benefit from differentiated dense connectivity.
To enable this, we first turn a normal Transformer block $\op{B}(X)$ into a multi-input one $\op{B}'(X^Q, X^K, X^V, X^R)$ by \emph{decoupling} its input into four streams for query, key, value and residual, respectively (Eq. (\ref{eq:multi-input block}), \cref{fig:arch} (e) bottom),
and then instantiate four DA modules, each specializing in one stream's dense connectivity (Eq. (\ref{eq:multiway densformer}), \cref{fig:arch} (d))\footnote{This is a logical view for clarity. In practice, these DSs can be combined for efficiency. See pseudocode in \cref{pseudo code}.}:
\begin{equation} \label{eq:multi-input block}
\begin{split}
    X_\textrm{A}' = \op{MHA}(\textrm{LN}(X^Q), \textrm{LN}(X^K), \textrm{LN}(X^V)) &+ X^R \\
    \op{B}'(X^Q, X^K, X^V, X^R) = \op{FFN}(\textrm{LN}(X_\textrm{A}')) &+ X_\textrm{A}' \\
\end{split}
\end{equation}
\begin{equation} \label{eq:multiway densformer}
\begin{split}
    \overline{X}_0^Q = \overline{X}_0^K &= \overline{X}_0^V = \overline{X}_0^R = X_0 = \op{Embedding(X)} \\
    X_i &= \op{B}'_i(\overline{X}_{i-1}^Q, \overline{X}_{i-1}^K, \overline{X}_{i-1}^V, \overline{X}_{i-1}^R); \\
    \overline{X}_i^Q,..,\overline{X}_i^R &= \op{DA}_i^Q(X_{:i}),..,\op{DA}_i^R(X_{:i}),\;i \in [1,L] \\
    &\op{MUDDFormer}(X) = \overline{X}_L^R
\end{split}
\end{equation}
By making the dense connections multiway, the cross-layer communication bandwidth is further increased significantly.
Multiway dynamic dense connections can be seen as depth-wise multi(4)-head attention.
This vertical cross-layer attention can be composed with the horizontal cross-token attention in Transformer to form pathways adaptively, enhancing information flow across the whole model when performing in-context learning tasks.\footnote{For an illustrative example of this, see \cref{fig:mudd_circuits}.}
At this point, we finally obtain MUDDFormer by integrating static, dynamic and multiway dense connections.
Complete pseudo-code for MUDDFormer is given in Appendix \ref{pseudo code}.

\subsection{Parameter Re-allocation} \label{param re-alloc}
Due to the dense connections, MUDDFormer's upper layers have the opportunity to process more information than lower layers and thus may need more parameters. We re-allocate the parameters of a standard Transformers to make the size of FFN sub-layers grows with depth. Specifically, let $D_f$ be the hidden dim of the original FFN, we compute $D_f'(i)$, the hidden dim of FFN at layer $i$ for MUDDFormer using linear interpolation:
\begin{equation}
    D_f'(i) = \frac{0.5(L-i) + 1.5(i-1)}{L-1}D_f
\end{equation}
i.e., the FFN hidden dim $D_f'$ grows linearly from $0.5D_f$ to $1.5D_f$. The total number of parameters remains unchanged.

\subsection{Optional Normalization}
To stabilize training models with large depth/width ratios, 
we propose a variant of MUDDFormer by applying RMSNorm before and after DA module, and adding a residual connection to DA module \emph{after} the post-RMSNorm:
\begin{equation} \label{eq:prepostnorm}
\begin{split}
    X_{:i} &= \{\textcolor{blue}{\op{Norm}}(X_0), ..., \textcolor{blue}{\op{Norm}}(X_i)\}\;\;(\textrm{PreDANorm})\\
    \overline{X}_i &= \textcolor{blue}{\op{Norm}}(\op{DA}_i(X_{:i})) \textcolor{blue}{+ X_i}\;\;\;\;\;\;\;\;\;\;(\textrm{PostDANorm})\\
\end{split}
\end{equation}
It is similar to the hybrid-norm strategy used by recent models such as Gemma 2 \cite{team2024gemma} and Grok-1 \cite{xai2024grok}, though we apply it to DA modules instead of MHA/MLP modules.
We use this PrePostDANorm variant to train the DeepNarrow models in scaling law experiments in \cref{scalinglaws} and the MUDDViT model in \cref{vit}. %(\cref{tab:vit results}).

\subsection{Complexity Analysis} \label{complexity}
\cref{tab:complexty analysis} shows the ratios of extra parameters and computation introduced by MUDD connections with both analytical results and typical concrete values.
The derivations are in Appendix \ref{append:complexity analysis}.
The ratio of extra parameters, i.e. parameter count of $W_1$ and $W_2$ of DA modules divided by that of the whole model, is proportional to the rectified depth/width ratio $\eta = \frac{L+3}{D}$.
The ratio of extra computation, i.e. FLOPs of generating MUDD connection weights and cross-layer aggregation divided by FLOPs of the whole forward pass, besides proportional to $\eta$, decreases with $\rho = \frac{T}{D}$. %, where $T$ is sequence length. 
Both ratios are negligible for commonly used settings.

\begin{table}[htb]
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\vskip -0.15in
\caption{Ratios of extra parameters and computation introduced by MUDD connections: (last row) analytical results and (upper rows) concrete values for typical model architectures and hyperparameters. L = number of layers, T = sequence length.}%\xda{TODO (xd): explain remove static, bold dyn.}}
\label{tab:complexty analysis}
\vskip 0.1in
\centering
% \begin{center}
\begin{small}
    \centering
    \begin{tabular}{>{\centering\arraybackslash}p{1.0cm}
    c|c|
    % >{\centering\arraybackslash}p{1.4cm}|c|
    >{\centering\arraybackslash}p{0.15cm}
    >{\centering\arraybackslash}p{0.5cm}
    >{\centering\arraybackslash}p{0.5cm}|
    >{\centering\arraybackslash}p{0.8cm}
    >{\centering\arraybackslash}p{0.55cm}
    }
    \toprule
         \makecell{Model \\Size} & $\mathrm{R_{\Delta params}}$ & $\mathrm{R_{\Delta FLOPs}}$ & L & D & T & \makecell{$\mathrm{\eta}=$ \\ 
         $\mathrm{\frac{L+3}{D}}$} & \makecell{$\mathrm{\rho}=$ \\ $\mathrm{\frac{T}{D}}$}  \\
         % Model size & $R_{params}$ & $R_{FLOPs}$ &R&$D_h$ &S&H \\
    \midrule
        % \rule{0pt}{25pt} 
        % \makecell{Theoretical \\ estimate} 
        1.4B & $0.22\%$ & $0.38\%$  & 24 & 2048 & 4096 & 0.0132 & 2 \\
        1.34B & $0.49\%$ & $0.8\%$  & 42 & 1536 & 4096 & 0.0293 & 2.67 \\
        2.8B & $0.23\%$ & $0.4\%$  & 32 & 2560 & 4096 & 0.0137 & 1.6 \\
        6.9B & $0.14\%$ & $0.26\%$  & 32 & 4096 & 4096 & 0.0085 & 1 \\
        \hline
       Formula & $\displaystyle{\frac{\eta}{6}}$  & \multicolumn{5}{l}{$\displaystyle{\frac{\eta}{3+\rho/4}}$}
        \\
    \bottomrule
    \end{tabular}
\end{small}
% \end{center}
\vskip -0.1in
\end{table}

\section{Experiments} \label{experiments}
\textbf{Implementation Details}
We implement MUDDFormer model and training in JAX. %based on Paxml framework\footnote{https://github.com/google/paxml}.
We initialize the MUDD connection weight generating parameters $W_1$ and $W_2$ with $\mathcal{N}(0, \frac{1}{D})$ and 0 respectively, and initialize the static weight vector $a_i$ with 1 at $a_{ii}$ and 0 elsewhere.
This reduces MUDDFormer to Transformer at the beginning of training, which is found to be critical for good performance.
If PrePostDANorm is used, we initialize the scale parameters of Pre-DA and Post-DA RMSNorms with 1 and 1e-3, respectively, and initialize $a_i$ to 0 because $X_i$ is added as the residual after DA modules (\cref{eq:prepostnorm}).
For the other parameters outside DA modules, we use Xavier normal initializer.

\textbf{Organization}
Our evaluation focuses on language modeling with decoder-only Transformer architecture, analyzing both pretraining scaling laws (\cref{scalinglaws}) and downstream task performance (\cref{downstream}) with large scale training on the Pile dataset \cite{gao2020pile}.
\cref{analyzing} elucidates why MUDDFormer works through analyzing trained models, followed by efficiency analysis (\cref{overhead}) and ablations (\cref{ablation}). 
Extended vision experiments are provided in \cref{vit}.

\subsection{Scaling Laws} \label{scalinglaws}
\textbf{Settings}
\cref{tab:scaling-configs} (top half) specifies the model sizes and hyperparameters for scaling experiments, which are mostly taken from GPT-3 specifications \cite{brown2020language}.
We untie input and output embedding matrices.
We train with context length 2048 and set the number of training tokens to roughly match Chinchilla scaling laws \cite{hoffmann2022empirical}. %, which specifies that training tokens should increase proportionally to model size.
The other hyperparameters are in \cref{hparams for lm}.
In another set of scaling experiments we trade off some width for depth to train \emph{DeepNarrow} models (\cref{tab:scaling-configs} (bottom half)) to see if MUDD connections offer any benefits for this style of scaling.
\begingroup
\setlength{\tabcolsep}{3pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1} % Default value: 1
\begin{table}[htb]
\vskip -0.15in
\caption{Model sizes and hyperparameters for scaling experiments.}
\label{tab:scaling-configs}
\vskip 0.05in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{ccccccc}
\toprule
\makecell{params} & $\mathrm{n_{layers}}$ & $\mathrm{d_{model}}$ & $\mathrm{n_{heads}}$ &\makecell{learning \\ rate}& \makecell{batch size \\ (in tokens)}&tokens\\
\midrule
405M& 24& 1024& 16& 3e-4 &  0.5M&7B \\
834M& 24& 1536& 24& 2.5e-4 & 0.5M&15B \\
1.4B& 24& 2048& 32& 2e-4 &  0.5M&26B \\
\midrule \midrule
% \textit{Scaling} & \textit{by depth} \\
\multicolumn{2}{c}{\textit{Scaling by depth}} \\
% 405M& 24& 1024& 16& 3e-4 &  0.5M&7B \\
797M& 34& 1280& 20& 2.5e-4 & 0.5M&15B \\
1.34B& 42& 1536& 24& 2e-4 &  0.5M&26B \\
% 1414M
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.15in
\end{table}
\endgroup

\textbf{Baselines}
We compare MUDD with two recently proposed approaches to enhancing residual connections in Transformers: DenseFormer \cite{pagliardini2024denseformer} (same as the static dense connections described in \cref{dynamic dense}) and Hyper-Connections \cite{zhu2024hyper}.
We also compare to Transformer with dynamic dense connections (DDFormer) as described in \cref{dynamic dense}.
All these approaches are applied to an improved and now widely adopted Transformer architecture \cite{touvron2023llama} with rotary positional encoding (RoPE) \cite{su2024roformer}, SwiGLU MLP \cite{shazeer2020glu}, etc. (often called Transformer++).
We also include the plot for the original Transformer architecture used by GPT-3 as a comparison.
The details for these baselines are in \cref{baselines for lm}.

\begin{figure}[tb]
% \vskip 0.2in
\centering
\centerline{\includegraphics[width=0.9\columnwidth]{Scaling_Law_MUDDFormer.pdf}}
\vskip -0.1in
\caption{Scaling curves of MUDDFormer and baseline models.} \label{fig:scaling-muddformer}
\vskip -0.1in
\end{figure}

%%%%%%%%%%%%%
\begin{figure}[t]
% \vskip 0.2in
\centering
% \begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{Scaling_Law_MUDDFormer_deep.pdf}}
\vskip -0.1in
\caption{Depth scaling of MUDDFormer and Transformer++.} \label{fig:scaling-muddformer-by-depth}
% \end{center}
\vskip -0.1in
\end{figure}

\textbf{Results}
\cref{fig:scaling-muddformer} plots Pile validation loss scaling curves of the models.
While DenseFormer and Hyper-Connections show clear advantage over Transformer++, DDFormer outperforms these two baselines by adding dynamicity to dense connections.
MUDDFormer further improves upon DDFormer by making the dense connections multiway, significantly outperforming all baselines  on models ranging from 405M to 1.4B.
It can be estimated that MUDDFormer-834M matches the loss of Transformer++ trained with 1.89\X\ compute.

\cref{fig:scaling-muddformer} also shows that as an architectural improvement, MUDDFormer's gain over Transformer++ ($\Delta L_\textrm{MUDD}$) remains stable while scaling, exceeding Transformer++'s own gain over Transformer ($\Delta L_{++}$) beyond 834M parameters. %which decreases with scale.
This shows the favorable scalability of MUDDFormer, particularly considering that Transformer++ has incorporated major architectural improvements (RoPE, SwiGLU MLP, etc.) over original Transformer since its invention.

\cref{fig:scaling-muddformer-by-depth} demonstrates MUDDFormer's enhanced \emph{depth utilization}:
while Transformer++ shows diminishing returns beyond 24 layers (almost coincident scaling curves),
MUDDFormer DeepNarrow maintains gains up to 42 layers.
This validates that MUDD connections alleviate depth-induced bottlenecks by enhancing cross-layer information flow.

\subsection{Large Scaling Training and Downstream Evaluations} \label{downstream}
\textbf{Settings} We compare MUDDFormer with the open source Pythia model suit \cite{biderman2023pythia} at large scale training on 300B tokens of Pile. Specifically, we train two models, MUDDPythia-1.4B and MUDDPythia-2.8B, and compare them with Pythia models ranging from 1.4B to 12B.
For fair comparison and clear quantification of the gain brought by MUDD, except adding MUDD connections as described in \cref{method}, MUDDPythia uses exactly the same architecture choices (e.g. parallel attention and MLP, rotary embedding with 1/4 head dim) and training hyperparameters (e.g. optimizer settings, learning rate schedule, batch size, context length, initialization methods) as Pythia (refer \citet{biderman2023pythia} Appendix E for details).
To evaluate if MUDD connections also work well with more advanced Transformer++ architecture and training recipe at this large scale, we also train MUDDFormer-2.8B based on Transformer++ instead of Pythia architecture with a larger learning rate of 3.2e-4 cosine decayed to 3.2e-6. Except these two changes, the other architectural and training hyperparameters are kept the same as MUDDPythia-2.8B. %\mqyadd{(QKNorm?)}

\textbf{Evaluation Datasets} Besides the datasets used by Pythia for downstream evaluation (LAMBADA \cite{paperno2016lambada}, PIQA \cite{bisk2020piqa}, WinoGrande \cite{sakaguchi2021winogrande}, ARC \cite{clark2018think}, SciQ \cite{welbl2017crowdsourcing}, LogiQA \cite{liu2020logiqa}), we also include BoolQ \cite{clark2019boolq} and HellaSwag \cite{zellers2019hellaswag} for commonsense reasoning, RACE \cite{lai2017race} for reading comprehension, all of which are widely used benchmarks.
We evaluate zero-shot and five-shot results using LM evaluation harness \cite{eval-harness}.

\begingroup
\setlength{\tabcolsep}{2.5pt} % Default value: 6pt
\begin{table*}[htb]
\vskip -0.1in
\centering
\caption{Zero-shot and five-shot downstream evaluations results. %Key observations: 1) MUDDPythia-2.8B outperforms Pythia-6.9B and MUDDFormer++-2.8B outperforms Pythia-12B; 2) The gains of MUDD connections on FLAN and on 5-shot ICL are larger.
}
\label{tab:downstream}
\vskip 0.1in
\centering
\begin{small}
% \begin{sc}
\begin{tabular}{l
% >{\centering\arraybackslash}p{1.5cm}>{\centering\arraybackslash}p{1.5cm}|
>{\raggedright}p{0.7cm}>{\raggedright}p{0.85cm}|
>{\centering\arraybackslash}p{0.9cm}>
{\centering\arraybackslash}p{0.6cm}>{\centering\arraybackslash}p{1cm}>{\centering\arraybackslash}p{0.7cm}>{\centering\arraybackslash}p{0.7cm}>{\centering\arraybackslash}p{0.5cm}>{\centering\arraybackslash}p{0.7cm}>{\centering\arraybackslash}p{0.65cm}>{\centering\arraybackslash}p{0.8cm}>{\centering\arraybackslash}p{0.85cm}>{\centering\arraybackslash}p{0.85cm}l}
\toprule
Model & \makecell{Pile \\ ppl$\downarrow$} & \makecell{FLAN \\ ppl$\downarrow$} & \makecell{LAM \\ BADA} & PIQA & \makecell{Wino \\ Grande} & \makecell{ARC \\ -E} & \makecell{ARC \\ -C} & SciQ & \makecell{Logi \\ QA} & BoolQ & \makecell{Hella \\ Swag} & \makecell{RACE \\ -M} & \makecell{RACE \\ -H} & \makecell{Avg \\ acc$\uparrow$ /$\Delta$acc} \\
\midrule
&&&\textit{0-shot}&\\
Pythia-1.4B  & 7.29 & 9.30 & 61.6 & 71.0 & 57.2 & 60.5 & 26.1 & 86.6 & 21.4 & \B{63.3} & 40.5 & 37.3 & 33.9 & 50.8\\ 
\B{MUDDPythia-1.4B} & \B{6.92} & \B{8.54} & \B{63.9} & \B{71.8} & \B{57.4} & \B{61.6} & \B{26.2} & \B{87.2} & \B{23.0} & 62.0 & \B{42.6} & \B{38.7} & \B{34.7} & \B{51.7}/+0.9 \\
\midrule
Pythia-2.8B  & 6.63 & 8.16 & 64.7 & 73.9 & 59.4 & 64.4 & 29.5 & 88.2 & 21.2 & 64.5 & 45.4 & 38.1 & 34.9 & 53.1\\ 
\B{MUDDPythia-2.8B} & \B{6.29} & \B{7.50} & \B{68.5} & \B{74.6} & \B{61.4} & \B{66.5} & \B{31.9} & \B{90.4} & \B{21.5} & \B{68.1} & \B{46.8} & \B{39.0} & \B{36.7} & \B{55.0}/+1.9 \\
\midrule
Pythia-6.9B   & 6.29 &     7.85   & 67.3 & 75.2 & 60.9 & 67.3 & 31.3 & 89.7 & 25.3 & 63.7 & 48.0 & 40.6 & 37.0 & 55.1\\
Pythia-12B  & 6.01 & 7.26 & 70.5 & 76.0 & 63.9 & 70.2 & 31.8 & 90.2 & 22.4 & 67.4 & 50.3 & 40.6 & 38.3 & 56.5\\
\midrule
\B{MUDDFM-2.8B} & 6.01 & 7.08 & 70.7 & 75.7 & 63.4 & 70.4 & 34.2 & 91.8 & 24.0 & 67.4 & 49.5 & 40.6 & 38.1 & \B{56.9} \\
\midrule
\midrule
 & & & \textit{5-shot}& \\
Pythia-1.4B   & - & - & 54.5 & 71.0 & 57.5 & 63.1 & \B{28.9} & 92.2 & 22.9 & \B{63.0} & 40.5 & 35.4 & 34.6 & 51.2\\
\B{MUDDPythia-1.4B} & - & - & \B{58.2} & \B{73.0} & \B{59.0} & \B{64.1} & 28.2 & \B{94.0} & \B{23.8} & 61.5 & \B{42.6} & \B{37.9} & \B{35.2} & \B{52.5}/+1.3\\
\midrule
Pythia-2.8B   & - & - & 60.5 & 73.6 & 60.6 & 67.3 & 32.3 & 94.3 & 21.7 & 65.6 & 45.1 & 38.4 & 35.6 & 54.1\\ 
\B{MUDDPythia-2.8B} & - & - & \B{63.6} & \B{75.5} & \B{63.6} & \B{70.3} & \B{34.0} & \B{95.5} & \B{28.1} & \B{67.5} & \B{47.1} & \B{44.5} & \B{37.3} & \B{57.0}/+2.9\\
\midrule
Pythia-6.9B            & - & - & 63.8 & 75.5 & 63.7 & 70.2 & 35.6 & 95.1 & 27.0 & 65.7 & 48.1 & 39.0 & 36.5 & 56.4\\
Pythia-12B    & - & - & 67.3 & 76.0 & 64.2 & 71.0 & 36.5 & 95.3 & 21.8 & 68.0 & 50.3 & 40.1 & 38.8 & 57.2\\
\midrule
\B{MUDDFM-2.8B} & - & - & 65.6 & 76.4 & 66.8 & 73.0 & 39.2 & 95.6 & 25.2 & 70.9 & 49.8 & 41.4 & 38.0 & \B{58.4} \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\vskip -0.1in
\end{table*}
\endgroup

\textbf{Results} As shown in \cref{tab:downstream} and \cref{fig:muddpythia-downstream}, besides lower Pile validation ppl, MUDDPythia also significantly outperforms Pythia at 1.4B and 2.8B scales on downstream task accuracies.
Notably, MUDDPythia-2.8B matches Pythia-6.9B (2.46\X\ compute) on both pretraining ppl and downstream evaluation.
Augmented with better Transformer++ architecture and training recipe, MUDDFormer-2.8B even outperforms Pythia-12B.

\cref{tab:downstream} also reports target span perplexities on a randomly sampled subset of the FLAN Collection dataset \cite{longpre2023flan}, % \cite{ghosal2023flacuna}
% We sample 320K examples and compute loss on the target span. This dataset
which features data of instructing following, chain-of-thought, in-context few-shot learning, etc. 
The advantage of MUDDPythia on FLAN is even larger than on Pile with MUDDPythia-2.8B significantly outperforming Pythia-6.9B, showing that MUDD connections have more advantage in improving these valued emergent abilities of LLMs \cite{wei2022emergent}.
The enhanced in-context learning ability is also manifested by the larger $\Delta$accuracies of 5-shot results compared to 0-shot (e.g. 2.9\% vs. 1.9\%),
where MUDDPythia-2.8B is on par with Pythia-12B (4.29\X\ compute).
The multiway dense connections applied to the query, key, and value streams of MHA modules are likely to enhance the functionality of attention heads, which are crucial for in-context learning (also see analysis in \cref{analyzing}).

Finally, the gains of MUDD connections with the 2.8B model are larger than those with the 1.4B model in both zero-shot and five-shot evaluations, further demonstrating the scalability of MUDD connections.

\subsection{Analyzing and Understanding} \label{analyzing}
We analyze and compare Pythia-2.8B and MUDDPythia-2.8B trained in \cref{downstream} to elucidate why MUDD connections work.
The analysis is done on 1024 randomly sampled sequences of length 2048 from Pile validation set.

\textbf{Representation Collapse}
\cref{fig:cosine_sim} quantifies representation collapse through cosine similarity between the inputs of adjacent layers.
While Pythia exhibits progressive collapse with $>$0.97 similarity in later layers, MDDDPythia maintains more distinct input representations, particularly in the value stream. 
Thanks to input stream decoupling and stream-specific aggregation,
DA modules can freely aggregate distinct value input streams for MHA modules of each layer at each sequence position.
These values will then be moved to \emph{other} positions by MHA at this layer without polluting the residual stream of the \emph{current} position. 
The relative importance of dense connections for the value stream is also evidenced by ablation studies (\cref{ablation}). %also shows that the value stream is the most important among the four input streams.
We compare illustrative V-composition circuits\footnote{common and important in many tasks, e.g. \citet{wang2211interpretability,ni2024benchmarking}. For introduction, %of Q/K/V-compositions and circuits, 
refer to \citet{Elhage2021mathematical}.}
in Transformer and MUDDFormer in \cref{fig:mudd_circuits} to highlight the benefit of MUDD connections and input stream decoupling, which results in more direct and cleaner information pathways. MUDD has similar effect on Q/K-composition circuits.

\begin{figure}[htb]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{cosine_similarity_of_hidden_states.pdf}}
\vskip -0.1in
\caption{Cosine similarity between the inputs of the current layer and the preceding layer.}
\label{fig:cosine_sim}
\end{center}
\vskip -0.3in
\end{figure}

\begin{figure}[!h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{mudd_circuits.pdf}}
\vskip -0.05in
\caption{Illustrative V-composition circuits in Transformer vs. in MUDDFormer. Colored circles are MHA's inputs of the query (yellow), key (red), value (green) and residual (black) streams and output (blue). LayerNorms and MLPs are omitted.}
\label{fig:mudd_circuits}
\end{center}
\vskip -0.3in
\end{figure}

\textbf{Attention Head Activation}
Transformer models often exhibit \emph{null attention} \cite{vig2019analyzing} where attention heads focus on the initial tokens or some special tokens as default ``attention sinks'' \cite{xiao2023efficient} when no relevant tokens are found.
We define a head to be \emph{active} at a position if its maximum attention weight \emph{does not} fall on the first two positions of the sequence or on tokens ``\verb|<|bos\verb|>|'', ``.'' and ``\textbackslash n''.
We compute the activation ratio of attention heads for each layer by averaging over all heads of that layer and all sequence positions and plot the results in \cref{fig:head_act}.
In Pythia, most heads remain inactive beyond the first few layers, limiting their contribution.
MUDDPythia demonstrates \tildemid2.4\X\ higher activation ratio across layers, particularly in deeper layers.\footnote{Visualization of sampled attention patterns for heads in Pythia and MUDDPythia are shown in \cref{interpretation}.}
This vitalization of attention heads stems from the multiway dense connections on the Q/K/V streams of MHA modules, ultimately improving in-context learning.

\begin{figure}[htb]
% \vskip 0.2in
\begin{center}
% Answer: [trim={left bottom right top},clip]
\centerline{\includegraphics[width=0.8\columnwidth]{head_activations.pdf}}
\vskip -0.1in
\caption{Attention head activation ratio by layers.}
\label{fig:head_act}
\end{center}
\vskip -0.3in
\end{figure}

\subsection{Training and Inference Efficiency} \label{overhead}
Besides theoretical complexity analysis in \cref{complexity}, we assess training and inference efficiency of MUDDFormer compared with Transformer in real-world settings.

\textbf{Settings} 
Though we use Pythia's architecture in large scale training, the evaluation in this section is done on untrained models with Transformer++ (Llama) architecture, which is of more practical relevance due to better performance.
We measure on three model sizes: 1.3B, 2.8B and 6.9B, which are ``Llama-ed'' version of Pythia 1.4B, 2.8B and 6.9B respectively.
We train on Google Cloud TPU v5p-128 pods with context length of 2048, batch size of 2M tokens and measure training throughput.
We do inference on NVIDIA A100 80G GPU with prompt length of 4096, batch size of 1 and measure the speed to generate 128 tokens. We repeat the measurements 3 times and take the average.
We implement training and inference in pure JAX and PyTorch respectively without writing any Pallas/Triton-based custom kernels.
We use $\mathrm{torch.compile}$ (PyTorch 2.5.1) to accelerate both Transformer++ and MUDDFormer.

\textbf{Results}
As shown in \cref{tab:overheadl}, the training and inference overheads, while larger than the theoretical estimates in \cref{tab:complexty analysis} and not negligible, are entirely acceptable considering the significant performance gain.
The overheads primarily stem from the series of small operations and additional I/O introduced by DA modules. We believe that kernel fusion techniques offer potential for further acceleration and leave it for future work.

\begingroup
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{table}[htb]
\vskip -0.1in
\caption{Training throughput and inference speed comparison between Transformer++ and MUDDFormer.}
\label{tab:overheadl}
\vskip 0.1in
\centering
\begin{footnotesize}
    \begin{tabular}{c|cc|cc}
        \toprule
        Model & \multicolumn{2}{c|}{Training (K tokens/s)} & \multicolumn{2}{c}{Inference (tokens/s)} \\
        Size & TFM++ & MUDDFM         & TFM++& MUDDFM         \\
        \midrule
        1.3B & 1147  & 1030\pct{89.8\%} & 325  & 286 \pct{88.1\%} \\
        2.8B & 684   & 575 \pct{84.0\%} & 181  & 163 \pct{90.0\%} \\
        6.9B & 332   & 318 \pct{95.6\%} & 95.5 & 89.7\pct{94.0\%} \\
        \bottomrule
    \end{tabular}
\end{footnotesize}
\vskip -0.1in
\end{table}
\endgroup

\subsection{Ablations and Variants} \label{ablation}
We conduct ablation studies with the 405M Transformer++/MUDDFormer models in scaling law experiments for language modeling in \cref{scalinglaws}.

\textbf{Ablation settings}
We do two groups of experiments and report the perplexity results in \cref{tab:ablation}.
In the first group, we progressively add the four components, i.e. static (\cref{static dense}), dynamic (\cref{dynamic dense}), multiway (\cref{multiway dense}) dense connections and parameter re-allocation (\ref{param re-alloc})) to Transformer++ to finally obtain MUDDFormer to compare the contribution of each component.
In the second group, we focus on the multiway aspect and study the effect of dense connections for the four decoupled inputs streams by replacing each of them with normal residual connection respectively.

\textbf{Results}
All three ingredients of dense connections, i.e. \emph{static}, \emph{dynamic} and \emph{multiway}, make contributions. %and dynamic dense connections improve the most.
While parameter re-allocation is effective on MUDDFormer, it \emph{deteriorates} Transformer++.
Removing dense connections for each of the four streams hurts performance and the value stream benefits most from dense connections.

\begingroup
\setlength{\tabcolsep}{3pt} % Default value: 6pt
\begin{table}[htb]
\vskip -0.1in
\caption{Ablations of MUDDFormer's components.} \label{tab:ablation}
\vskip 0.1in
\begin{center}
\begin{small}
% \begin{sc}
\begin{tabular}{lc|lc}
\toprule
Config & ppl & Config & ppl \\
\midrule
Transformer++                & 11.68 & MUDDFormer & \B{10.77} \\
$+$Static Dense              & 11.44 & $-$Q dense & 10.89 \\
$+$Dynamic Dense             & 11.09 & $-$K dense & 10.90 \\
$+$Multiway Static Dense     & 11.27 & $-$V dense & 11.05 \\
$+$Multiway Dynamic Dense    & 10.83 & $-$R dense & 11.14 \\
$+$Mul. Dyn. Dense$+$Re-alloc& \B{10.77} &  \\
$+$Re-alloc                  & 11.93 &  \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\endgroup

\textbf{Variants with Sparse Connectivity}
We design MUDDFormer variants by approximating its dense connections with two sparse connectivity patterns:
1) \emph{dilation and periodicity} (MUDDFormer-$k$\X$p$, also used in \cite{pagliardini2024denseformer}): each DA module aggregates the outputs of every $k$-th block, and the DA modules are inserted after every $p$ blocks. 
2) \emph{sliding window} (MUDDFormer-SW$n$): each DA module accesses to the outputs from only previous $n$ blocks plus the embeddings.
\cref{fig:variants} shows the Pile validation perplexities and relative training and inference speed (compared to Transformer++) of these MUDDFormer variants.
We measure perplexities with 405M models to align with ablation results in \cref{tab:ablation},
while training and inference speeds are measured using 1.3B untrained models as in \cref{tab:overheadl}, the size of which is of more practical value.

While fully dense (1\X1) connectivity achieves the best performance, these sparse variants provide a spectrum of performance-efficiency trade-offs.
For example, switching from MUDDFormer-1\X1 to MUDDFormer-2\X2 increases relative training/inference speed from 89.8\%/88.1\% to 97.8\%/93.4\% with only a 0.18 increase in ppl.
In comparison, MUDDFormeer-SW8 is inferior with a lower speed / ppl ratio, highlighting the indispensability of long-range ($>$8) cross-layer interactions.

\begin{figure}[tb]
% \vskip 0.2in
\begin{center}
% Answer: [trim={left bottom right top},clip]
\centerline{\includegraphics[width=\columnwidth]{muddformer_overhead_tradeoff.pdf}}
\vskip -0.1in
\caption{PPL vs. relative training and inference speed of MUDDFormer variants.}
\label{fig:variants}
\end{center}
\vskip -0.3in
\end{figure}

\section{Related Work} \label{related_work}
\textbf{Enhancing Residual Connections}
Despite the pervasive use of residual connections \cite{he2016deep} in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication.
\citet{huang2017densely} introduced DenseNet for CNNs.
Inspired by it, \citet{pagliardini2024denseformer} proposed DenseFormer for Transformers, which uses \emph{Depth Weighted Averaging} modules to aggregate outputs from all preceding layers with static, learnable weights. 
Most recently, \citet{zhu2024hyper} proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies.
Other research has explored different forms of cross-layer attention \cite{elnokrashy2022depth,fang2023cross,wang2024strengthening}
which retrieve or update representations across different layers in a more flexible manner.
MUDDFormer is closely related to DenseFormer and HC but differs in critical ways. 
First, unlike DenseFormer, our MUDD connections \emph{dynamically} compute per-position weights conditioned on the hidden states. 
Second, although HC uses a combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. 
Moreover, none of existing approaches consider decoupling the four input streams of a Transformer block by a \emph{multiway} design, which is shown to bring significant performance gain in MUDDFormer. 

\textbf{Mechanistic Interpretability}
Research in this field employs various attribution methods \cite{conmy2023towards,hanna2024have} to uncover the circuits within Transformers that underlie specific capabilities \cite{Elhage2021mathematical,wang2024strengthening,ni2024benchmarking}. 
These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - a key insight motivating MUDD connections' design, which explicitly facilitates such interactions.

\textbf{Cross-Layer KV Cache Optimization} 
\citet{brandon2024reducing} proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. 
Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams.
This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments.
OmniNet \cite{tay2021omninet} achieves a fully global KV Cache receptive field by allowing each token to attend to \emph{all} tokens in \emph{all} layers.
MUDDFormer attains a similar effect in a much more efficient way via composition of cross-layer dense connections and within-layer attention.

\textbf{Intra-Layer Architectural Innovations}
Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms \cite{ye2024differential,leviathan2024selective,liu2024deepseek}, sub-quadratic linear attention or RNN/SSM architectures \cite{gu2023mamba,dao2024transformers,peng2024eagle,yang2024parallelizing} and sparse Mixture-of-Experts (MoE) \cite{fedus2022switch,dai2024deepseekmoe}.
By contrast, MUDD connections focus on \emph{cross-layer} communication, making it orthogonal and complementary to these approaches.
We leave the exploration of combining MUDD connections with these within-layer optimizations for future work.

\section{Conclusion}
We introduced Multiway Dynamic Dense connections to address the limitations of residual connections and enhance cross-layer information flow in Transformers. 
Experimental results showed that MUDDFormer is effective, efficient and scalable. It significantly outperforms Transformer baselines in language and vision tasks and improves emergent abilities such as in-context learning with minimal overhead.
MUDD connections have the potential to become an indispensable component of next-generation foundation models.

\section*{Acknowledgements}
We are grateful to Google Cloud for providing the compute for model training, and to Shun Wang and Tingting Zhang for their technical support and help in troubleshooting TPU resource allocation and training.

\bibliography{main}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{PyTorch Style Pseudo-code for MUDDFormer}
\label{pseudo code}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
% bgcolor=LightGray,
fontsize=\footnotesize,
% highlightlines={1, 3-4},
numbersep=-10pt,
linenos
]{python}
    # B = batch_size;  T = seq_len; D = model_dim
    # L = layer_index; C = num_ways = 4;  K = DA_hidden_dim = C*(L+1)
    
    def generate_dw(x, mudd_theta): # x: BxTxD
        w1, w2, a = mudd_theta  # w1: DxK, w2: Kx(C*(l+1)), a: Cx(l+1)
        dw = GELU(RMSNorm(x) @ w1) @ w2 + a
        dw = rearrange(dw, 'B T (C L)-> C B T L', C=4)
        return dw
        
    def DA(Xs, mudd_theta): # Xs: List (l+1)x[BxTxD]
        dw = generate_dw(Xs[-1], mudd_theta)
        xs = []
        for c, way in enumerate(['Q', 'K', 'V', 'R']):
            x = sum([dw[c, :, :, j:j+1] * Xs[j]  # BT1,BTD->BTD
                     for j in range(len(Xs))])
            xs.append(x)
        return xs
    
    def muddformer(x, model):
        x = model.embedding(x)
        Xs = [x]
        xq, xk, xv, xr = x, x, x, x 
        for block in model.blocks:
            attn_out = block.attn(LN(xq), LN(xk), LN(xv)) + xr
            x = block.ffn(LN(attn_out)) + attn_out
            Xs.append(x)
            xq, xk, xv, xr = DA(Xs, block.mudd_theta)
        return xr
\end{minted}

\section{Details of Complexity Analysis}
\label{append:complexity analysis}
Compared to Transformer++, extra compute and parameters in MUDDFormer are introduced by DA modules, increasing from bottom layer to top layer, due to varied hidden dim $K_i$ of DA at layer i. We approximately calculates $R_{\Delta params}$ and $R_{\Delta FLOPs}$, the ratios of extra parameters and computation by considering an average layer in the middle of MUDDFormer. 
In this layer, $\overline{K} = 4(\overline{L} + 1) = 4(\frac{L+1}{2}+1) = 2(L+3)$. We assume $D \gg \overline{K}$ in a typical Transformer architecture. We denote $\rho = \frac{T}{D}$ and $\eta = \frac{L+3}{D}$.
To simplify, we ignore RMSNorm because it is negible in terms of parameters and computation.

\textbf{Ratio of extra parameters}
\begin{equation}
\begin{split}
R_{\Delta params} = \frac{ \sum_{i=1}^L ( \overbrace{DK_i}^{W_1} + \overbrace{K_i^2}^{W_2})}{12LD^2} 
\approx 
    \frac{
    D\overline{K} + \overline{K}^2
    }{12  D^2}
\approx \frac{D  \overline{K}}{12 D^2} 
(\mathrm{assume}\ D \gg \overline{K})
= \frac{L+3}{6 D}
%(\mathrm{where}\ \overline{K}= C\overline{L} = C\frac{L+3}{2})\\
= \frac{\eta}{6} \\
\end{split}
\end{equation}

\textbf{Ratio of extra FLOPs.}  
\begin{equation}
\begin{split}
R_{\Delta FLOPs} & = \frac{ \sum_{i=1}^L ( \overbrace{TDK_i + TK_i^2}^{\substack{\text{generate dense weight\ $A_{ij} $} \\ \text{Code Line 6}}} + 
\overbrace{TDK_i}^{\substack{\text{Depthwise Aggregate} \\ \text{Code Line 13-15 }}}
)}{L D T (12 D +T)} \\
& \approx \frac{
2TD\overline{K} + T\overline{K}^2}{D T (12 D +T)} 
 = \frac{2D\overline{K}+ \overline{K}^2}{D (12 D +T)} \ \ 
 \approx
\frac{2\overline{K}}{12 D +T} \ (\mathrm{assume}\ \ 2D \gg \overline{K} )
 = 
\frac{4(L+3)}{12 D +T} 
= \frac{\eta}{3+\rho/4}
\end{split}
\end{equation}

\section{Hyperparameters and Baselines for Scaling Law Experiments} \label{hyperparameters}
\subsection{Hyperparameters} \label{hparams for lm}
We use the AdamW optimizer with $\beta_1$ = 0.9, $\beta_2$ = 0.95, gradient clip value of 1.0, weight decay of 0.1, 1\% learning rate warmup steps followed by cosine decay to 10\% of its maximal value, and no dropout. These hyperparameters are mostly taken from the GPT-3 paper \cite{brown2020language} and are also used by all the baseline models listed below.

\subsection{Baseline Models} \label{baselines for lm}
\begin{itemize}
    \item \textbf{Transformer}: The standard Transformer based on GPT-3.
    \item \textbf{Transformer++}: An improved Transformer architecture adopted by Llama \cite{touvron2023llama} etc. with rotary positional encoding (RoPE) \cite{su2024roformer}, SwiGLU MLP \cite{shazeer2020glu}, RMSNorm instead of LayerNorm and no linear bias.
    \item \textbf{DenseFormer}: The DenseFormer model from \citet{pagliardini2024denseformer} without dilation, which has the best performance according to the paper. We implemented the model in JAX based on the PyTorch code released by the authors\footnote{https://github.com/epfml/DenseFormer}.
    \item \textbf{Hyper-Connections}: The dynamic hyper-connections with expansion rate $n = 4$ (DHC\X 4) from \citet{zhu2024hyper}, which achieves superior results on language model pre-training and is the recommended configuration in the paper. We implemented the model in JAX based on the PyTorch Implementation given in Appendix J of the paper.
    \item \textbf{DDFormer}: Transformer with dynamic dense connections but without multiway splitting as described in \cref{dynamic dense}.
\end{itemize}

\section{Image Classification with ViT} \label{vit}
Besides decoder-only transformer for language modeling, we apply MUDD connections to Vision Transformer (ViT, an encoder-only Transformer) \cite{dosovitskiy2020image} for image classification on the ImageNet-1k dataset (ILSVRC-2012).
Implementation and experimental settings (e.g. the use of RandAugmention+MixUp, fixed 2D sincos position embedding and global average pooling) are based on the Big Vision codebase\footnote{https://github.com/google-research/big\_vision}.
We use AdamW with $\beta_1$ = 0.9, $\beta_2$ = 0.999, gradient clip value of 1.0, weight decay of 0.3, learning rate of 0.003 with 10000 warmup steps followed by cosine decay to 0, batch size of 4096, RandAugment of 1evel 10, Mixup of 0.2 and dropout rate of 0.1. 
We use ViT-S/16 as the baseline model and equip it with MUDD connections to obtain MUDDViT-S/16. We also compare with a 1.72\X\ larger model ViT-M/16 (\cref{tab:vit models}).
We report validation loss and top-1 accuracy results on 90 and 300 epochs in \cref{tab:vit results}.
As can be seen, the gain from MUDD connections decreases a bit during the training progress, probably because many epochs of repeated passes over the same dataset diminish the additional expressive capacity brought by MUDD connections.
Despite this, MUDDViT-S/16 still outperforms ViT-S/16 by 2\% on epoch 300, also surpassing ViT-M/16.

\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\begin{table}[htb]
\vskip -0.1in
\caption{ViT Model architectures for ImageNet-1k classification.}
\label{tab:vit models}
\vskip 0.1in
\centering
\begin{small}
% \begin{sc}
\begin{tabular}{cccccc}
\toprule
Model          & $\mathrm{n_{layers}}$ & $\mathrm{d_{model}}$ & $\mathrm{d_{mlp}}$ & $\mathrm{n_{heads}}$ & params \\
\midrule
(MUDD)ViT-S/16   & 12         & 384       & 1536    & 6         & 22M \\
ViT-M/16       & 12         & 512       & 2048    & 8         & 39M \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\vskip -0.1in
\end{table}
\endgroup

\begingroup
\setlength{\tabcolsep}{4pt} % Default value: 6pt
\begin{table}[htb]
% \vskip -0.1in
\caption{ViT for ImageNet-1k classification results.}
\label{tab:vit results}
\vskip 0.1in
\centering
\begin{small}
% \begin{sc}
\begin{tabular}{ccccc}
\toprule
           % & \multicolumn{2}{c}{Epoch 90} & \multicolumn{2}{c}{Epoch 300} & Relative \\
Model        & val. loss & acc@e90 & acc@e300 & Rel. size \\
\midrule
ViT-S/16     & 0.993    & 53.4     & 76.0      & 1 \\
MUDDViT-S/16 & \B{0.877}& \B{56.0} & \B{78.0}  & 1.007 \\
ViT-M/16     & 0.890    & 55.2     & 77.9      & 1.72 \\
\bottomrule
\end{tabular}
% \end{sc}
\end{small}
\vskip -0.1in
\end{table}
\endgroup

\section{Visualization} \label{interpretation}

\textbf{Head activation from attention patterns}
In Section \ref{analyzing}, we show that the ratio of head activations in MUDDPythia-2.8B is larger than that in Pythia-2.8B. 
Here we draw the actual attention patterns on a randomly sampled sequence of length 32 from Pile validation set for the 32 heads in layer 25 of these two models in Figure \ref{fig:attn_pattern_pythia} and \ref{fig:attn_pattern_muddpythia}. It is clear that attentions in Pythia mainly concentrate on the sink token (inactive) while attentions in MUDDPythia disperse on various tokens (active).

\textbf{Cross-layer dynamic weights} To better understand MUDDPythia, we visualize dynamic dense connection weights. Due to the high variance of the norm of hidden states $X_i$, we scale those weights by the average norm of each layer, rectifying the importance of weights. The rectified mean and standard deviation of dynamic connection weights in MUDDPythia-2.8B are shown in Figure \ref{fig:mudd_pattern_mean} and \ref{fig:mudd_pattern_std}, respectively. It is evident that the patterns of the four streams (query, key, value, residual) differ from each other, validating the necessity of separating them from the standard residual stream. It is noteworthy that in the value-stream connections, most layers have a salient and more dynamic weight on output of the first layer, thus forming a long-range channel to transport bottom information for attention heads in upper layers.  

\begin{figure}[htb]
% \vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{attn_pattern_pythia.pdf}}
\vskip -0.2in
\caption{Attention patterns for the 32 heads in the 25th layer of Pythia-2.8B. }
\label{fig:attn_pattern_pythia}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htb]
% \vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=0.95\columnwidth]{attn_pattern_muddpythia.pdf}}
\vskip -0.2in
\caption{Attention patterns for the 32 heads in the 25th layer of MUDDPythia-2.8B.}
\label{fig:attn_pattern_muddpythia}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htb]
% \vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=0.65\columnwidth]{mudd_pattern_mean.pdf}}
\vskip -0.2in
\caption{Mean of dynamic dense connections of MUDDPythia-2.8B.}
\label{fig:mudd_pattern_mean}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[htb]
% \vskip -0.05in
\begin{center}
\centerline{\includegraphics[width=0.65\columnwidth]{mudd_pattern_std.pdf}}
\vskip -0.2in
\caption{Standard deviation of dynamic dense connections of MUDDPythia-2.8B.}
\label{fig:mudd_pattern_std}
\end{center}
\vskip -0.2in
\end{figure}


\end{document}
