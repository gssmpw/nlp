\section{Related Work}
\label{related_work}
\textbf{Enhancing Residual Connections}
Despite the pervasive use of residual connections \cite{he2016deep} in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication.
\citet{huang2017densely} introduced DenseNet for CNNs.
Inspired by it, \citet{pagliardini2024denseformer} proposed DenseFormer for Transformers, which uses \emph{Depth Weighted Averaging} modules to aggregate outputs from all preceding layers with static, learnable weights. 
Most recently, \citet{zhu2024hyper} proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies.
Other research has explored different forms of cross-layer attention \cite{elnokrashy2022depth,fang2023cross,wang2024strengthening}
which retrieve or update representations across different layers in a more flexible manner.
MUDDFormer is closely related to DenseFormer and HC but differs in critical ways. 
First, unlike DenseFormer, our MUDD connections \emph{dynamically} compute per-position weights conditioned on the hidden states. 
Second, although HC uses a combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. 
Moreover, none of existing approaches consider decoupling the four input streams of a Transformer block by a \emph{multiway} design, which is shown to bring significant performance gain in MUDDFormer. 

\textbf{Mechanistic Interpretability}
Research in this field employs various attribution methods \cite{conmy2023towards,hanna2024have} to uncover the circuits within Transformers that underlie specific capabilities \cite{Elhage2021mathematical,wang2024strengthening,ni2024benchmarking}. 
These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - a key insight motivating MUDD connections' design, which explicitly facilitates such interactions.

\textbf{Cross-Layer KV Cache Optimization} 
\citet{brandon2024reducing} proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. 
Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams.
This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments.
OmniNet \cite{tay2021omninet} achieves a fully global KV Cache receptive field by allowing each token to attend to \emph{all} tokens in \emph{all} layers.
MUDDFormer attains a similar effect in a much more efficient way via composition of cross-layer dense connections and within-layer attention.

\textbf{Intra-Layer Architectural Innovations}
Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms \cite{ye2024differential,leviathan2024selective,liu2024deepseek}, sub-quadratic linear attention or RNN/SSM architectures \cite{gu2023mamba,dao2024transformers,peng2024eagle,yang2024parallelizing} and sparse Mixture-of-Experts (MoE) \cite{fedus2022switch,dai2024deepseekmoe}.
By contrast, MUDD connections focus on \emph{cross-layer} communication, making it orthogonal and complementary to these approaches.
We leave the exploration of combining MUDD connections with these within-layer optimizations for future work.