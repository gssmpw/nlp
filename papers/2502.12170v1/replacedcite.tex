\section{Related Work}
\label{related_work}
\textbf{Enhancing Residual Connections}
Despite the pervasive use of residual connections ____ in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication.
____ introduced DenseNet for CNNs.
Inspired by it, ____ proposed DenseFormer for Transformers, which uses \emph{Depth Weighted Averaging} modules to aggregate outputs from all preceding layers with static, learnable weights. 
Most recently, ____ proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies.
Other research has explored different forms of cross-layer attention ____
which retrieve or update representations across different layers in a more flexible manner.
MUDDFormer is closely related to DenseFormer and HC but differs in critical ways. 
First, unlike DenseFormer, our MUDD connections \emph{dynamically} compute per-position weights conditioned on the hidden states. 
Second, although HC uses a combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. 
Moreover, none of existing approaches consider decoupling the four input streams of a Transformer block by a \emph{multiway} design, which is shown to bring significant performance gain in MUDDFormer. 

\textbf{Mechanistic Interpretability}
Research in this field employs various attribution methods ____ to uncover the circuits within Transformers that underlie specific capabilities ____. 
These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - a key insight motivating MUDD connections' design, which explicitly facilitates such interactions.

\textbf{Cross-Layer KV Cache Optimization} 
____ proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. 
Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams.
This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments.
OmniNet ____ achieves a fully global KV Cache receptive field by allowing each token to attend to \emph{all} tokens in \emph{all} layers.
MUDDFormer attains a similar effect in a much more efficient way via composition of cross-layer dense connections and within-layer attention.

\textbf{Intra-Layer Architectural Innovations}
Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms ____, sub-quadratic linear attention or RNN/SSM architectures ____ and sparse Mixture-of-Experts (MoE) ____.
By contrast, MUDD connections focus on \emph{cross-layer} communication, making it orthogonal and complementary to these approaches.
We leave the exploration of combining MUDD connections with these within-layer optimizations for future work.