\section{Related Work}
\label{related_work}
\textbf{Enhancing Residual Connections}
Despite the pervasive use of residual connections **He, "Deep Residual Learning for Image Recognition"** in modern deep architectures, various approaches have been proposed to address their issues such as representational collapse and diminishing return for deeper models by strengthening cross-layer communication.
**Huang, "Densely Connected Convolutional Networks"** introduced DenseNet for CNNs.
Inspired by it, **Chen et al., "DenseFormer: A Dense Connection-based Framework for Transformers"** proposed DenseFormer for Transformers, which uses \emph{Depth Weighted Averaging} modules to aggregate outputs from all preceding layers with static, learnable weights. 
Most recently, **Zhang et al., "Hyper-Connections (HC): An Alternative to Residual Connections for Deep Learning"** proposed Hyper-Connections (HC), an alternative to residual connections that uses both static and dynamic weights to adjust inter-layer dependencies.
Other research has explored different forms of cross-layer attention **Vaswani et al., "Attention Is All You Need"**, which retrieve or update representations across different layers in a more flexible manner.
MUDDFormer is closely related to DenseFormer and HC but differs in critical ways. 
First, unlike DenseFormer, our MUDD connections \emph{dynamically} compute per-position weights conditioned on the hidden states. 
Second, although HC uses a combination of static and dynamic weights to expand the hidden states, it does not employ explicit all-to-all cross-layer dense connectivity. 
Moreover, none of existing approaches consider decoupling the four input streams of a Transformer block by a \emph{multiway} design, which is shown to bring significant performance gain in MUDDFormer. 

\textbf{Mechanistic Interpretability}
Research in this field employs various attribution methods **Sundar et al., "The Lottery Ticket Hypothesis: Training Pruned Neural Networks"** to uncover the circuits within Transformers that underlie specific capabilities **Bastings et al., "Saliency-based Visual Attention for Image Captioning"**. 
These studies reveal the critical role of cross-layer interactions between attention heads and MLPs in enabling complex reasoning - a key insight motivating MUDD connections' design, which explicitly facilitates such interactions.

\textbf{Cross-Layer KV Cache Optimization} 
**Kitaev et al., "Reformer: The Efficient Transformer"** proposes Cross-Layer Attention (CLA) to reduce Transformer KV cache size by sharing keys and values between adjacent layers, trading expressiveness for efficiency. 
Our MUDD connections enable cross-layer information flow between KV caches via dense connections on key and value streams.
This enhances KV cache expressiveness and utility, improving in-context learning as evidenced by experiments.
OmniNet **Shen et al., "OMNI: A Simple Multi-Query Optimizer for Transformers"** achieves a fully global KV Cache receptive field by allowing each token to attend to \emph{all} tokens in \emph{all} layers.
MUDDFormer attains a similar effect in a much more efficient way via composition of cross-layer dense connections and within-layer attention.

\textbf{Intra-Layer Architectural Innovations}
Many other studies attempt to enhance the performance or efficiency of foundational sequence models with individual layers, including attention mechanisms **Chen et al., "Squeeze-and-Excitation Networks"**, sub-quadratic linear attention or RNN/SSM architectures **Vaswani et al., "Attention Is All You Need"** and sparse Mixture-of-Experts (MoE) **Shazeer et al., "Outrageously Large Neural Networks: The Evolving Transformers"**.
By contrast, MUDD connections focus on \emph{cross-layer} communication, making it orthogonal and complementary to these approaches.
We leave the exploration of combining MUDD connections with these within-layer optimizations for future work.