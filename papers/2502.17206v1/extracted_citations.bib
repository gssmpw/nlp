@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{halacheva2024expanding,
  title={Expanding Expressivity in Transformer Models with M{\"o}bius Attention},
  author={Halacheva, Anna-Maria and Nayyeri, Mojtaba and Staab, Steffen},
  journal={arXiv preprint arXiv:2409.12175},
  year={2024}
}

@article{liu2021pay,
  title={Pay attention to mlps},
  author={Liu, Hanxiao and Dai, Zihang and So, David and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9204--9215},
  year={2021}
}

@article{mahmood2024enhanced,
  title={Enhanced Computationally Efficient Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling},
  author={Mahmood, Kaleel and Huang, Shaoyi},
  journal={arXiv preprint arXiv:2412.06106},
  year={2024}
}

@inproceedings{shaker2023swiftformer,
  title={Swiftformer: Efficient additive attention for transformer-based real-time mobile vision applications},
  author={Shaker, Abdelrahman and Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Yang, Ming-Hsuan and Khan, Fahad Shahbaz},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={17425--17436},
  year={2023}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{wu2021fastformer,
  title={Fastformer: Additive attention can be all you need},
  author={Wu, Chuhan and Wu, Fangzhao and Qi, Tao and Huang, Yongfeng and Xie, Xing},
  journal={arXiv preprint arXiv:2108.09084},
  year={2021}
}

@article{zhang2024cas,
  title={Cas-vit: Convolutional additive self-attention vision transformers for efficient mobile applications},
  author={Zhang, Tianfang and Li, Lei and Zhou, Yang and Liu, Wentao and Qian, Chen and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2408.03703},
  year={2024}
}

