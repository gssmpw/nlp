\section{Related Work}
%\linenumber
\label{sec:related_work}
The attention mechanism was first introduced within the context of machine translation by Bahdanau et al.**Bahdanau, "Neural Machine Translation by Jointly Learning to Align and Translate"**, when it was used for dynamic alignment of words between input and output sequences. This work introduced the concept of "additive attention", in which two vectors are independently processed by their own feed-forward networks to produce scalars that are summed to calculate the final attention score. With Neural Attention, instead of processing each vector separately, we concatenate the two vectors and perform a nonlinear operation on the combined representation, projecting them jointly into a space that encodes their relationship. This approach captures richer dependencies between vectors, whereas additive attention ultimately relies on a linear combination of independently projected representations.

Building upon this foundation, Vaswani et al.**Vaswani, "Attention Is All You Need"** proposed the transformer architecture, eliminating the need for recurrence while still allowing flexibility of input sequence length. This innovation significantly improved the ability of sequence processing to be parallelized, addressing some of the bottlenecks in recurrent models. In this work, they adopted Dot-Product Attention as the mechanism for calculating attention scores, citing its practical advantages over additive attention when considering computational and memory efficiency. However, their work did not explore whether additive attention could offer advantages in expressivity, leaving the representational limitations of Dot-Product Attention largely unexamined. Neural Attention addresses this gap by prioritizing enhanced expressivity, even as we tackle the associated complexity challenges, as explained in section~\ref{sec:methodology}.

While these foundational innovations laid the groundwork for modern transformers, much of the subsequent research has shifted toward improving computational efficiency, rather than addressing representational limitations. Fastformer**Dong, "FastFormer: Additive Attention for Efficient Transformers"** explored additive attention to achieve faster processing, while SwiftFormer**Lin, "SwiftFormer: Efficient Transformers with Non-Local Neural Networks"** revisited additive attention for real-time applications in mobile vision tasks. Similarly, Xu et al.**Xu, "Additive and Convolution-based Attention Mechanisms for Vision Transformers"** proposed additive and convolution-based attention mechanisms tailored for vision transformers, focusing on practical implementation benefits. Linformer**Zhang, "Linformer: Linearly Scalable Neural Networks for Efficient Recurrent Models"** addressed the $O(n^2)$ complexity of self-attention by employing low-rank approximations to reduce computational and memory costs. More recently, Mahmood and Huang**Mahmood, "Segformer: Segmented Attention Approach for Efficient Transformers"** introduced a segmented attention approach, computing attention only on pairs of consecutive overlapping segments to further optimize computational efficiency. While these efforts underscore the ongoing interest in rethinking attention mechanisms, they largely prioritize efficiency improvements over representational power. Neural Attention distinguishes itself by directly addressing the expressivity limitations of standard attention mechanisms, providing a novel approach to modeling complex, nonlinear relationships.

In the current literature, there are not many examples of work solely focused on improving the expressive power of transformers by exploring alternative approaches to the attention mechanism. However, RoFormer **Kong, "RoFormer: Improving Long-Term Dependencies with Relative Position Relationships"** improved the capability of transformers to capture long-term dependencies by introducing a novel technique for preserving relative position relationships in self-attention calculations. gMLP **Tang, "gMLP: Generalized Multi-Layer Perceptions for Efficient Transformers"** replaced the attention mechanism entirely by using multi-layer perceptions with gating. This work relates to ours in that it uses feed-forward networks to learn spatial dependencies. This provides a justification for our work, however, it is a fundamentally different approach and cannot be easily adapted into existing attention-based transformer architectures, as ours can be. Moreover, their implementation does not allow for nor was it tested on autoregressive tasks. The work most similar to ours is M{\"o}bius Attention **Chen, "M{\"o}bius Attention: Improving Expressive Power with Nonlinear Transformations"** which sought to improve the expressive power of attention mechanisms by adding a nonlinear M{\"o}bius transformation and calculating attention scores in a complex space. This differs from our work in that they only apply the nonlinear transformation to the embedding vectors in the $\mathit{Q}$ matrix, creating a richer representation prior to attention calculation. Unlike Neural Attention, which adds a nonlinear transformation into the attention calculation itself.


%\nolinenumber