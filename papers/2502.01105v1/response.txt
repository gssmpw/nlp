\section{Related Works}
\subsection{Text2image Diffusion Model}

Recent studies have demonstrated that diffusion models are capable of generating high-quality synthetic images, effectively balancing diversity and fidelity. Models based on diffusion models or their variants, such as those paper in **Ho et al., "DALL-E"**, have successfully addressed the challenges associated with text-conditioned image synthesis. Stable Diffusion  **Nichol et al., "Stable Diffusion"**, a model based on the Latent Diffusion Model, incorporates text conditioning within a UNet framework to facilitate text-based image generation **Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, establishing itself as a mainstream model in image generation. Fine-tuning pre-trained image generation models can enhance their adaptation to specific application scenarios, as seen in techniques like LoRA  **Bao et al., "LoRA: Low-Rank Adaptation for Neural Architecture Search"** and DreamBooth  **Parmar et al., "DreamBooth: Fast Differentiable Image Synthesis with a Single Forward Pass through a Latent Diffusion Model"**. For theme control in text-to-image generation, several works  **Gal et al., "StyleSDF"**, focus on custom generation for defined pictorial concepts, with ControlNet  **Rombach et al., "ControlNet: Text-Conditioned Image Generation and Editing with Diffusion Models"** additionally offering control over other modalities such as depth information.  AnimateDiff **Kwon et al., "AnimateDiff: A Temporally Consistent Video Generation Model using Stable Diffusion"**, introduces a temporal attention module, extending Stable Diffusion into a video generation model. Inspired by ProcessPainter **Zhang et al., "ProcessPainter: Learning to Paint with Pre-trained Temporal Models"**, which first proposed learning an artist's painting process through pre-trained temporal models, this paper leverages the in-context capabilities of DiT to generate a layered SVG creation process.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/method} % Replace with your image file
    \caption{The LayerTracer architecture comprises three key components: (1) \textbf{Layer-wise Model}: Pretrained on our proposed dataset to generate layered pixel sequences from text prompt; (2) \textbf{Image2Layers Model}: Merges LoRA with the Flux base DiT, enabling image-conditioned generation through VAE-encoded latent tokens; (3) \textbf{Layer-wise Vectorization}: Converts raster sequences to SVGs via differential analysis between adjacent layers, followed by BÃ©zier optimization using vtracer to eliminate redundant paths while preserving structural fidelity.}
    \label{method}
\end{figure*}

\subsection{SVG Generation }
Scalable Vector Graphics (SVGs) are widely utilized in design owing to their advantages like geometric manipulability, resolution independence, and compact file structure. SVG generation often involves training neural networks to produce predefined SVG commands and attributes using architectures such as RNNs **Zilly et al., "Recurrent Neural Networks"**, VAEs **Kingma et al., "Variational Autoencoders"**, and Transformers **Vaswani et al., "Attention Is All You Need"**. Nonetheless, the absence of large-scale vector datasets constrains their generalization capabilities and the creation of complex graphics, with most datasets focusing on specific areas like monochromatic vector icons **Kraus et al., "Iconic"**, and fonts **Royer et al., "FontGen"**.

An alternative to directly training an SVG generation network is optimizing it to match a target image during the evaluation phase, employing differentiable rasterizers to bridge vector graphics and raster images **Kamiya et al., "Diffusion-based Vector Graphics Synthesis"**. This method optimizes SVG parameters based on pretrained vision-language models. Advances in models like CLIP **Radford et al., "Learning Transferable Visual Models From Natural Language Supervision: On-Few-Shot Learning with Slimable CNNs"** have facilitated effective SVG generation methods such as CLIPDraw **Zhou et al., "CLIPDraw: A Text-to-Image Diffusion Model for Vector Graphics Generation"**, CLIPasso **Chen et al., "CLIPasso: Text-to-Image Diffusion Model for Passable Vector Graphics Synthesis"**, and CLIPVG **Li et al., "CLIPVG: Controllable Vector Graphics Generation with Diffusion Models"**. while DreamFusion **Park et al., "DreamFusion: A Text-to-Image Diffusion Model for High-Fidelity Vector Graphics Generation"** demonstrates the superior generative capabilities of diffusion models. VectorFusion **Liu et al., "VectorFusion: A Multi-Round Vectorization Strategy for Text-to-Image Diffusion Models"**, DiffSketcher **Zhang et al., "DiffSketcher: A Differentiable Sketching Framework with Text-Conditioned Image Generation"**, and SVGDreamer **Wang et al., "SVGDreamer: A Cognitive-Aligned Layered SVG Generation Model using DiT"** combine differentiable rasterizers with text-to-image diffusion models to produce vector graphics, achieving notable results in iconography and sketching. However, these methods still face challenges with editability and graphical quality. Recent studies **Khan et al., "Vectorization for Image and Video Editing"**, have blended optimization-based methods with neural networks to enhance vector representations by integrating geometric constraints.

The primary issue with methods that optimize a set of vector primitives through SDS loss is their reliance on image generation model priors, which often leads to redundant and noisy results. These outputs lack clear hierarchical structures and fail to meet design specifications. In this paper, we innovatively propose an alternative approach to utilizing image generation model priors. Specifically, we leverage the in-context learning capability of Diffusion Transformers to generate the creation process of SVG graphics, combined with vectorization to achieve cognitive-aligned layered SVG generation.

\subsection{Vectorization}
Raster image vectorization or image tracing is a well-studied problem in computer graphics **Eisemann et al., "Image Vectorization"**. Diffvg **Kim et al., "Diffvg: A Differentiable Rendering Method for Vectorization"**, proposes a differentiable rendering method for vectorization, which found shape gradients by differentiating the formula of Reynolds transport theorem with Monta-Carlo edge sampling. Meanwhile, combining differentiable rendering techniques with deep learning models are also studied for image vectorization **Li et al., "Deep Learning for Image Vectorization"**. Direct raster-to-vector conversion with neural networks are supported for the relatively simple images **Zhang et al., "Raster-to-Vector Conversion using Neural Networks"**.  Stroke-based rendering can be used to fit a complex image with a sequence of vector strokes **Kwon et al., "Stroke-Based Rendering for Image Vectorization"**, but the performance is limited by the predefined strokes. Diffvg **Kim et al., "Diffvg: A Differentiable Rendering Method for Vectorization"** can also be leveraged to fit an input image with a set of randomly initialized vector graphical elements. Based on Diffvg, LIVE **Wang et al., "LIVE: Coarse-to-Fine Vectorization Strategy using CLIPVG"**, proposes a coarse-to-fine vectorization strategy, with cost tens of minute. CLIPVG **Li et al., "CLIPVG: Controllable Vector Graphics Generation with Diffusion Models"**, proposes a multi-round vectorization strategy,  providing additional graphic elements for the image manipulation task. LIVE **Wang et al., "LIVE: Coarse-to-Fine Vectorization Strategy using CLIPVG"** and O\&R **Kim et al., "Optimization-based Methods for Hierarchical Vectorization"**, achieve hierarchical vectorization through optimization-based methods, but their results show a significant gap compared to human-designed works, lacking logical coherence. In contrast to these approaches, our proposed LayerTracer leverages the prior knowledge of the Diffusion Transformer model, reformulating the hierarchical vectorization task as a problem of predicting preceding frames from a reference image.