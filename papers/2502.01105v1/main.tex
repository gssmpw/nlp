%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro  \cite [option:text,text]
%TC:macro  \citep [option:text,text]
%TC:macro  \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%\documentclass[sigconf,authordraft]{acmart}
%\documentclass[sigconf]{acmart}
%\documentclass[acmtog,anonymous,review]{acmart}
\documentclass[acmtog]{acmart}

% \acmSubmissionID{391}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{siunitx}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \citestyle{acmauthoryear}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
  
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%
% Submission ID.
% Use this when submitting an article to a sponsored event. You'll
% receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{391}

%
% For managing citations, it is recommended to use bibliography
% files in BibTeX format.
%
% You can then either use BibTeX with the ACM-Reference-Format style,
% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
% support for advanced citation of software artefact from the
% biblatex-software package, also separately available on CTAN.
%
% Look at the sample-*-biblatex.tex files for templates showcasing
% the biblatex styles.
%

%
% The majority of ACM publications use numbered citations and
% references.  The command  \citestyle{authoryear} switches to the
% "author year" style.
%
% If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of
% citations and references.
% Uncommenting
% the next command will enable that style.
% \citestyle{acmauthoryear}


\begin{teaserfigure}
  \centering
  \includegraphics[width=0.92\textwidth]{image/teaser-f.pdf}
  \caption{LayerTracer enables the creation of cognitive-aligned layered SVGs, either from text prompts or by converting images into layer-wise SVGs.}
  \Description{.}
  \label{fig:teaser}
\end{teaserfigure}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{LayerTracer: Cognitive-Aligned Layered SVG Synthesis via  Diffusion Transformer}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \authornotemark[1]

\author{Yiren Song}
\email{yiren@nus.edu.sg}
\affiliation{%
  \institution{Show Lab, National University of Singapore}
  \country{Singapore}
}

\author{Danze Chen}
\email{chendanze@whu.edu.cn}
\affiliation{%
  \institution{Show Lab, National University of Singapore}
  \country{Singapore}
}

  
\author{Mike Zheng Shou}
\authornote{Corresponding author.}
\email{mike.zheng.shou@gmail.com}
\affiliation{%
  \institution{Show Lab, National University of Singapore}
  \country{Singapore}
}


% %%
% %% By default, the full list of authors will be used in the page
% %% headers. Often, this list is too long, and will overlap
% %% other information printed in the page headers. This command allows
% %% the author to define a more concise list
% %% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.


\begin{abstract}
  % Generating layered and concise SVG graphics like human designers is highly desirable but challenging. Existing vectorization methods typically produce single-layer outputs. Methods such as LIVE employ optimization techniques to minimize the distance between a set of vector primitives and the target image, achieving some degree of layering. However, the results are often noisy, with the generated SVG files containing complex and redundant shapes that hinder further editing. Specifically, the key hierarchical topology and fundamental semantics of the image remain underexplored and insufficiently understood. We propose a novel method called **VecDit**, which leverages the state-of-the-art DIT model to generate layered vector graphics. By learning the logic and steps used by designers to create layered SVGs, **VecDit** can generate a realistic process for creating SVG graphics from a blank canvas based on textual descriptions. Following a layer-by-layer vectorization and deduplication process, nearly perfect layered vector graphics are successfully created. Additionally, we designed a conditional mechanism that supports converting a raster image into a layered SVG. Extensive experiments and evaluations demonstrate the superiority of our approach..

% Creating clean and layered SVG graphics, akin to the work of human designers, has long been a highly anticipated but challenging task. Existing vectorization methods either produce single-layer outputs or rely on optimization algorithms to achieve layering, often resulting in complex and redundant shapes that fail to meet expectations. To address this, we propose LayerTracer, a DiT-based solution that constructs a high-quality layered SVG dataset to learn the logic and steps designers use to create layered graphics, enabling the generation and vectorization of layered SVGs. LayerTracer starts from a text description to generate layered raster images on a blank canvas and then refines them through a step-by-step vectorization and deduplication process, producing high-quality layered vector graphics. To achieve layered vectorization, we designed a conditional generation mechanism that encodes the reference image into condition tokens, guiding the denoising process to create layered SVGs that closely match the reference image. Extensive experiments and evaluations demonstrate the effectiveness and practicality of our method in layered SVG generation and vectorization tasks.

% Generating layered SVG graphics with human-like editability remains a significant challenge, as existing methods produce either oversimplified single-layer outputs or optimization-derived geometries with redundant shapes. We present LayerTracer, a Diffusion Transformer (DiT) framework that bridges this gap by learning designer cognitive processes from a novel dataset of 20,000+ layered SVG creation sequences. Our approach operates in two phases: (1) a text-conditioned DiT generates rasterized construction process simulating human design steps, followed by (2) layer-wise vectorization with path deduplication to produce clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding the denoising process to reconstruct layered SVGs while preserving structural fidelity. Evaluations demonstrate LayerTracer's superiority, reducing redundant paths against optimization baselines and achieving higher human preference rates. Extensive experiments and evaluations demonstrate the effectiveness and practicality of our method in layered SVG generation and vectorization tasks.

% Generating cognitive-aligned SVGs remains challenging due to existing methods' tendencies toward oversimplified single-layer outputs or optimization-induced shape redundancy. We propose LayerTracer, a Diffusion Transformer (DiT) framework that bridges this gap by learning designer cognitive processes from a novel dataset of layered SVG creation sequences. Our approach operates in two phases: a text-conditioned DiT first generates rasterized construction blueprints simulating human design steps, followed by layer-wise vectorization with path deduplication to produce clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural fidelity. Extensive experiments demonstrate LayerTracer's superior performance against optimization and other baselines in generation quality, effectively aligning AI-generated vectors with professional design workflows.

Generating cognitive-aligned layered SVGs remains challenging due to existing methods’ tendencies toward either oversimplified single-layer outputs or optimization-induced shape redundancies. We propose LayerTracer, a diffusion transformer based framework that bridges this gap by learning designers’ layered SVG creation processes from a novel dataset of sequential design operations. Our approach operates in two phases: First, a text-conditioned DiT generates multi-phase rasterized construction blueprints that simulate human design workflows. Second, layer-wise vectorization with path deduplication produces clean, editable SVGs. For image vectorization, we introduce a conditional diffusion mechanism that encodes reference images into latent tokens, guiding hierarchical reconstruction while preserving structural integrity. Extensive experiments demonstrate LayerTracer’s superior performance against optimization-based and neural baselines in both generation quality and editability, effectively aligning AI-generated vectors with professional design cognition. Code is released at \href{https://github.com/showlab/LayerTracer}{https://github.com/showlab/LayerTracer}

% 生成可编辑的分层SVG仍然是一个挑战，因为现有方法往往会生成过于简化的单层输出，或由于优化过程导致形状冗余。我们提出了 LayerTracer，一个基于扩散变换器（Diffusion Transformer, DiT）的框架，通过学习设计师的创建分层SVG过程，弥补这一空缺。我们的方法分为两个阶段：首先，基于文本条件的DiT生成模拟人类设计步骤的栅格化构建过程；其次，进行按层矢量化并通过路径去重生成清晰且可编辑的SVG。针对图像矢量化，我们引入了一种条件扩散机制，将参考图像编码为潜在令牌，引导分层重建，同时保持结构完整性。广泛的实验表明，LayerTracer 在生成质量和可编辑性方面显著优于基线方法，有效地使AI生成的矢量图与专业设计工作流程相对齐。

\end{abstract}

% 像人类设计师一样创建分层的简洁SVG 图形是一直想要的但是很难。现有矢量化方法要么造成单层输出, 要么采用优化的方法实现了层次化，结果包含复杂且冗余的形状而并不理想。本文提出了一种基于DiT的解决方案，LayerTracer, 通过构造一个高质量的分层SVG数据集，学习设计师创建分层图形的逻辑和步骤，实现分图层生成矢量图形的生成和矢量化。Layertracer能根据文本描述从空白画布生成分层的光栅图，然后经过逐层矢量化和去重，创建几乎完美的分层矢量图。 为了实现分层的矢量化，我们设计了条件生成机制，将参考图像编码为condition token控制去噪过程，生成与参考图一致的分层SVG。 广泛的实验和评估证明了本文方法的先进性。

% 生成具有类似人类可编辑性的分层SVG图形仍然是一个重大挑战，因为现有方法要么生成过于简化的单层输出，要么产生基于优化的几何结构，充满冗余形状。我们提出了**LayerTracer**，一个基于**Diffusion Transformer (DiT)** 的框架，通过从一个包含20,000+分层SVG创建序列的新数据集中学习设计师的认知过程来弥合这一差距。我们的方法分为两个阶段：  
% (1) 基于文本条件的DiT生成栅格化的“构建蓝图”，模拟人类设计步骤；  
% (2) 按层矢量化并进行路径去重，生成清晰、可编辑的SVG图形。  针对图像矢量化，我们引入了一个条件扩散机制，将参考图像编码为潜在令牌，引导去噪过程以重建分层SVG，同时保持结构的完整性。评估结果表明，**LayerTracer**显著优于其他方法：相较于基于优化的基线方法，冗余路径减少了，并获得了更高的设计师偏好率。通过将文本到SVG生成与图像矢量化统一为一个**过程感知扩散**框架，我们的方法推动了AI工具从资产创建者向实用设计协作者的转变。

% 生成可编辑的分层SVG仍然是一个挑战，因为现有方法往往会生成过于简化的单层输出，或由于优化过程导致形状冗余。我们提出了 LayerTracer，一个基于扩散变换器（Diffusion Transformer, DiT）的框架，通过学习设计师的创建分层SVG过程，弥补这一空缺。我们的方法分为两个阶段：首先，基于文本条件的DiT生成模拟人类设计步骤的栅格化构建过程；其次，进行按层矢量化并通过路径去重生成清晰且可编辑的SVG。针对图像矢量化，我们引入了一种条件扩散机制，将参考图像编码为潜在令牌，引导分层重建，同时保持结构完整性。广泛的实验表明，LayerTracer 在生成质量和可编辑性方面显著优于基线方法，有效地使AI生成的矢量图与专业设计工作流程相对齐。

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Diffusion Transformer, Image generation, Vectorization }
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010178.10010224</concept_id>
<concept_desc>Computing methodologies~Computer vision</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Computer vision}

% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{images/teaser.pdf}
%   \caption{This paper accomplishes the generation of a painting process that mimics human artists, transitioning from abstract to specific. Each row in the figure represents a different application: (1) generating painting processes from text prompt (Text2Painting), (2) converting artworks into painting processes (Image2Painting), (3) completing semi-finished paintings (Semi2Complete).}
%   \Description{.}
%   \label{fig:teaser}
% \end{teaserfigure}


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

\maketitle


% 注意力机制中的令牌倾向于关注空间上相邻的令牌。这种倾向源于扩散模型在预训练过程中捕获的邻近图像像素之间的强相关性。

% \section{Introduction}

% Scalable Vector Graphics (SVG) is a fundamental image-encoding approach in which visual elements—vector paths, curves, polygons, etc.—are defined via mathematical equations. This sharply contrasts with raster images, which rely on a pixel array laid out on a grid. A primary advantage of vector graphics is that they maintain high fidelity and consistent visual quality at any resolution, since they can be scaled indefinitely without losing detail. Among these, human-like layered vector graphics adhere to professional design standards, offering exceptional editability that allows designers to easily select, modify, and combine elements. This feature is crucial during the design process, enabling seamless adjustments and fostering creative exploration.

% However, current deep learning–based generation methods still struggle to produce high-quality, human-like layered SVG outputs. Due to the lack of large-scale, high-quality SVG datasets, native SVG generation models often show limited efficacy, and some methods have focused solely on font or icon generation. Other approaches attempt to leverage the priors of pretrained raster-based models through SDS optimization to generate SVG, but this iterative process typically yields noisy, hard-to-edit results that do not meet professional design needs. While some methods employ an SVG VAE to perform diffusion in vector space—substantially improving outcomes—another family of solutions relies on large language models (LLMs) to directly learn SVG data distributions. Unfortunately, token-length constraints and the inherent limitations of LLMs restrict such methods to relatively simple SVG graphics. Consequently, generating layered SVGs that align with designers’ workflows and fully utilize the broad range of SVG elements needed for complex applications remains a notable challenge.


% To address this issue, we introduce LayerTracer, a system that fine-tunes a DiT model on the process data of human designers creating SVGs, achieving human-style layered SVG generation. Our approach is based on three key insights: (1) DiT models trained on large-scale text-image datasets inherently possess "context generation" capabilities, which can be activated with minimal adjustments; (2) tokens in the attention mechanism tend to focus on spatially adjacent tokens, a tendency stemming from the strong correlations between adjacent image pixels captured during the diffusion model's pretraining; (3) Layered SVGs can be decomposed through channel dimensions, stacked, and arranged in a grid, making them more effectively understood and learned by image generation models.

% Specifically, to alleviate the scarcity of layered SVG creation data, we introduced an automated pipeline that collected over 20,000 SVG creation processes. We transformed designers' layered SVGs into step-by-step creation sequences, rasterized them, and then spatially arranged them in a grid-based training sample sequence. Based on Insight 2, we propose an S-shaped layout to ensure that temporally adjacent frames are always spatially adjacent. By fine-tuning the DiT model on this dataset, LayerTracer learned the logic and sequence of designers building layered SVGs. Based on simple text prompts, the model can generate realistic raster images of the SVG creation process and output high-quality, human-style layered vector graphics through a carefully designed layer-wise vectorization and deduplication process.

% In addition to generating layered vector graphics from text, there is strong demand for layering and vectorizing existing raster images. We redefine layered vectorization as a process-generation task conditioned on input images. Accordingly, we develop an image-conditioned model that transforms a given raster image into a layered sequence. Concretely, we build on a pretrained DiT model and repurpose the Flux model as a conditional generator that can ingest image context. During training, we arrange the creation sequence into logical 2×2 or 3×3 grids, then feed the context image into a VAE to obtain its latent representation, appending it to the end of the denoising latents. Through self-attention, this latent guides the denoising of all other frames. At inference time, the model uses the final frame to predict the preceding steps that construct each layer.

% Our main contributions are as follows:
% \begin{itemize}
% \item We propose LayerTracer, a method for generating and vectorizing layered SVGs in a human-like manner. By collecting designer-created SVGs and splitting them into layers, our approach effectively learns the ordering and strategies designers use to generate graphical elements, ensuring the output meets professional design standards.
% \item LayerTracer introduces a conditional-consistency control model, enabling a unified framework for both layered SVG generation and multi-layer raster-to-SVG vectorization.
% \item To address the scarcity of layered SVG creation data, LayerTracer incorporates an automated pipeline that collects over 20,000 SVG creation processes. Extensive experiments and evaluations demonstrate the advanced performance and effectiveness of LayerTracer in producing high-quality, human-like layered vector graphics.
% \end{itemize}

% 可扩展矢量图形（Scalable Vector Graphics, SVG） 是一种基础的图像编码方式，其中视觉元素（如矢量路径、曲线、多边形等）通过数学方程定义。这与依赖网格像素数组的光栅图像形成鲜明对比。矢量图形的主要优势在于其能够在任何分辨率下保持高保真度和一致的视觉质量，因为它们可以无限放大而不会丢失细节。在众多矢量图形中，**人类风格的分层矢量图形** 符合专业设计标准，提供了卓越的可编辑性，使设计师可以轻松选择、修改和组合元素。这一特性在设计过程中尤为重要，能够实现无缝调整并激发创意探索。

%然而，目前基于深度学习的生成方法在生成高质量、具有人类风格的分层SVG输出方面仍然存在困难。由于缺乏大规模且高质量的SVG数据集，原生SVG生成模型的效果往往有限，一些方法仅专注于字体或图标的生成。还有一些方法尝试利用基于光栅图像预训练模型的先验，通过SDS优化生成SVG，但这种迭代过程通常会产生嘈杂、难以编辑的结果，无法满足专业设计需求。虽然也有研究采用SVG VAE在矢量空间中进行扩散以显著提升效果，但另一类解决方案依赖大型语言模型（LLMs）直接学习SVG数据分布。然而，Token长度限制以及LLMs自身的局限性使这些方法仅能处理较为简单的SVG图形。因此，生成符合设计师工作流程、并充分利用SVG多样化图元以支持复杂应用的分层SVG仍然是一个显著的挑战。

% 为了解决这一问题，我们提出了 LayerTracer，通过在人类设计师创建SVG的过程数据上微调DiT模型，实现了具有人类风格的分层SVG生成。我们的方法基于三个关键洞察：（1）在大规模文本-图像数据集上训练的DiT模型本质上具有“上下文生成”能力，仅需少量调整即可激活；（2）注意力机制中的令牌倾向于关注空间上相邻的令牌。这种倾向源于扩散模型在预训练过程中捕获的邻近图像像素之间的强相关性； （3）分层SVG可以通过通道维度分解、堆叠并排布成网格，从而更有效地被图像生成模型理解和学习。

% 具体来说，为了缓解分层SVG创建数据的稀缺问题，我们引入了一条自动化管道，收集了超过20,000个SVG创建过程。我们将设计师创建的分层SVG转换为逐步的创建序列，光栅化后再按照顺序在空间上排版成基于网格的训练样本。基于洞察2，我们提出S形排版，以保证时序上的相邻帧在空间上总是相邻的。通过在这一数据集上微调DiT模型，LayerTracer学习了设计师构建分层SVG的逻辑和顺序。基于简单的文本提示，模型可以生成SVG创建过程的逼真光栅图，并通过精心设计的逐层矢量化和去重过程，输出高质量、具有人类风格的分层矢量图形。

%除了从文本生成分层矢量图形外，对现有光栅图像的分层和矢量化也有着广泛需求。我们将分层矢量化重新定义为一种基于输入图像的序列生成任务。为此，我们开发了一种图像条件模型，将给定的光栅图像转换为分层序列。具体来说，我们基于预训练的DiT模型，并LoRA微调将Flux模型改造为能够接受图像上下文的条件生成模型。在训练阶段，我们将创建序列按逻辑顺序排列成2×2或3×3的网格，然后将上下文图像输入VAE以获取其潜在表示，并将其拼接到去噪潜变量的末尾。通过自注意力机制，该潜变量指导其他帧的去噪过程。在推理阶段，模型使用最终帧预测出前序构建各层的步骤。

%本文的主要贡献如下：
% 1. 我们提出了Layer Tracer，实现了human-like层次矢量图生成和矢量化。通过收集设计师手动创建的分层SVG数据集， 并拆分图层，LLM4SVG能够有效学习设计师创建图元的顺序和策略，使生成的输出符合人类设计标准。
% 2. Layer Tracer 提出了条件一致性控制模型，支持将像素图拆分为不同图层并矢量化。实现了统一的框架，用于层次矢量图生成和矢量化。
% 3.为解决分层SVG创建过程数据的稀缺问题，LayerTracer引入了一条自动化管道，收集了20，000+ SVG 创建过程。广泛的实验和评估证明了LayerTracer的先进性和有效性

\section{Introduction}
Scalable Vector Graphics (SVG) serves as the cornerstone of modern digital design, defining visual elements—paths, curves, and geometric shapes—through mathematical equations rather than pixel grids. Unlike raster images, SVG maintains resolution-independent clarity at any scale, making it indispensable for precision-critical applications ranging from UI/UX design to industrial CAD systems. Layered SVGs, which adhere to professional standards, elevate this advantage further: designers can meticulously manipulate individual layers to refine stroke textures, spatial hierarchies, and composite effects. This editability transcends mere convenience—it forms the backbone of dynamic adjustments and collaborative iteration in contemporary design workflows.

Nevertheless, a significant gap persists between current deep learning-based SVG generation techniques and professional requirements. Existing approaches face three systemic challenges: First, the scarcity of large-scale layered SVG datasets forces models to rely on synthetic or oversimplified training data, resulting in outputs devoid of the nuanced hierarchical structures inherent to human designs. Second, methodological fragmentation prevails—optimization-based methods \cite{vectorfusion, svgdreamer, diffsketcher, live, diffvg, oar, layered_vectorization} generate vector paths using raster priors but often produce cluttered geometries with redundant anchor points; and large language models (LLMs) \cite{empoweringLLM, strokenuwa, starvector}, constrained by token limits, remain limited to basic icons. Most critically, no existing method addresses the designer’s cognitive process—the logical sequencing, spatial reasoning, and element grouping strategies employed during layer construction—resulting in AI-generated SVGs that resemble fragmented collages rather than intentionally editable professional designs.

To address these challenges, we present LayerTracer, a Diffusion Transformer (DiT)-based framework that redefines layered SVG synthesis by modeling designers’ layer-by-layer construction logic. Our approach is grounded in three key insights: (1). Cognitive alignment: DiT models pretrained on text-image corpora inherently capture contextual relationships between visual elements, which can be steered through targeted fine-tuning to mimic designer decision-making. (2). Spatiotemporal consistency: The self-attention mechanism’s bias toward local token interactions—a byproduct of training on natural image pixel correlations—can be repurposed to enforce coherence across sequential design steps. 3. Structured decomposition: Disassembling layered SVGs into channel-wise components and organizing them as grid sequences provides generation models with an interpretable blueprint of layer evolution.

In implementation, LayerTracer integrates two innovations. First, we curate a pioneering dataset of 20,000+ designer process traces, automatically converting layered SVGs into timestamped creation sequences. These sequences are rasterized and organized into training grids using a serpentin layout, ensuring temporally adjacent design steps remain spatially proximate. Second, we develop a dual-phase generation pipeline: (1) a text-conditioned DiT generates rasterized construction process sequences that simulate a designer’s workflow, followed by (2) a layer-wise vectorization module that converts these sequences into clean, editable SVG layers while eliminating redundant paths.

Beyond text-to-SVG synthesis, LayerTracer tackles the inverse task: converting raster images into layered vector graphics. We reframe this as a process-conditioned generation problem, where reference images guide the model to "reverse-engineer" plausible layer construction steps. Specifically, we build upon a pretrained DiT model and adapt it through LoRA fine-tuning to ingest image context. By encoding reference images into conditional tokens injected into the denoising process, the model autonomously deduces layer assembly sequences (e.g., "background first, then foreground elements"), faithfully reconstructing input images while adhering to practical editing constraints.

Our main contributions are as follows:
\begin{itemize}
\item Cognitive-aligned SVG synthesis: As the first framework to generate layered SVGs by learning designers’ construction logic—element ordering, layer grouping, and spatial reasoning—LayerTracer ensures outputs meet professional editing standards.
\item Unified DiT-based architecture: Our framework seamlessly integrates text-to-SVG generation and layer-wise vectorization tasks, eliminating the need for task-specific pipelines.
\item Process-centric dataset: We release a scalable pipeline for collecting designer workflow data, addressing the critical gap in layered vector graphics training resources. Extensive experiments validate LayerTracer’s state-of-the-art performance and effectiveness.
\end{itemize}

% deepseek
% 可缩放矢量图形（SVG）作为现代数字设计的基石，通过数学方程而非像素网格来定义视觉元素——路径、曲线与几何形状。相较于栅格图像，SVG具有分辨率无关的特性，能在任意缩放比例下保持锐利清晰，因而成为UI/UX设计、工业CAD等需高精度场景的首选格式。其中，符合专业标准的分层式SVG更将这一优势发挥到极致：设计师可对独立图层进行精细操控，逐步完善笔触质感、纹理叠加与空间层级，这种可编辑性不仅是创作便利，更是现代设计流程中动态调整与协同迭代的核心需求。

% 然而，当前基于深度学习的SVG生成技术仍与专业需求存在显著鸿沟。现有方法普遍面临三重困境：首先，缺乏大规模分层SVG数据集迫使模型依赖合成数据或过度简化的训练素材，导致输出结果缺失人类设计特有的层级结构；其次，方法论呈现碎片化——基于优化的方法（如分数蒸馏采样SDS）虽能借助栅格先验生成矢量路径，却常产生锚点冗余的杂乱几何；而受限于token长度的大语言模型（LLM）方案，仅能处理基础图标等简单SVG。最关键的是，现有技术均未触及设计师的认知过程——即构建图层时遵循的逻辑顺序、空间推理与元素组合策略，这使得AI生成的SVG往往呈现碎片化拼贴效果，而非真正可编辑的专业设计。

% 针对这些挑战，我们提出LayerTracer——基于扩散变换器（DiT）的分层SVG生成框架，通过建模设计师的逐层构建思维重新定义矢量图形合成范式。该方法建立在三个关键insight之上：1. 认知对齐机制：在文本-图像语料库预训练的DiT模型天然具备视觉元素间的语境关联能力，可通过定向微调模拟设计师的决策逻辑；2. 时空一致性约束：自注意力机制对局部token交互的偏好（源于自然图像像素相关性的训练先验）可被重新定向，用于保证设计步骤间的连贯性；3. 结构化解构策略：将分层SVG按通道维度拆解并组织为网格序列，为生成模型提供可解释的图层演化蓝图。

% 实现层面，LayerTracer融合两大创新：其一，我们构建了首个包含20,000+条设计师过程轨迹的数据集，通过自动化流程将分层SVG转换为带时间戳的创作序列。这些序列经栅格化后，采用蛇型空间布局组织为训练网格，确保时序相邻的设计步骤在空间上紧密相邻；其二，开发双阶段生成管线：（1）文本条件DiT生成模拟设计师工作流的"施工蓝图"栅格序列，（2）分层矢量化模块将其转换为干净可编辑的SVG图层，并消除冗余路径。

% 除文本到SVG生成外，LayerTracer还攻克了逆向任务：将栅格图像转换为分层矢量图形。我们将此重构为过程条件生成问题，使参考图像引导模型"逆向工程"出合理的图层构建步骤。具体来说，我们基于预训练的DiT模型，并通过LoRA微调将其改造为能够接受图像上下文的条件生成模型。通过将参考图像编码为条件token注入去噪过程，使其能自适应推测图层组装顺序（如"先背景后前景"），在重建输入图像的同时满足实际编辑约束。

% 主要贡献
% 人类认知对齐的SVG合成：首个通过学习设计师的构建逻辑（元素顺序、图层分组、空间推理）生成分层SVG的框架，确保输出符合专业编辑标准；

% 基于DiT框架实现统一生成架构，实现文本到SVG与层次矢量化任务的无缝整合；

% 过程中心化数据集：发布可扩展的设计师工作流数据采集管线，填补分层矢量图形训练资源的空白。广泛的实验和评估证明了LayerTracer的先进性和有效性。

\section{Related Works}


\subsection{Text2image Diffusion Model}

Recent studies have demonstrated that diffusion models are capable of generating high-quality synthetic images, effectively balancing diversity and fidelity. Models based on diffusion models or their variants, such as those paper in  \cite{sd, dit}, have successfully addressed the challenges associated with text-conditioned image synthesis. Stable Diffusion  \cite{sd}, a model based on the Latent Diffusion Model, incorporates text conditioning within a UNet framework to facilitate text-based image generation \cite{diffsim, idp, anti}, establishing itself as a mainstream model in image generation. Fine-tuning pre-trained image generation models can enhance their adaptation to specific application scenarios, as seen in techniques like LoRA  \cite{lora} and DreamBooth  \cite{dreamfusion}. For theme control in text-to-image generation, several works  \cite{ipa, instantid, ssr, fast_icassp, makeup, hair} focus on custom generation for defined pictorial concepts, with ControlNet  \cite{controlnet} additionally offering control over other modalities such as depth information.  AnimateDiff \cite{animatediff} introduces a temporal attention module, extending Stable Diffusion into a video generation model. Inspired by ProcessPainter \cite{processpainter}, which first proposed learning an artist's painting process through pre-trained temporal models, this paper leverages the in-context capabilities of DiT to generate a layered SVG creation process.

% 最近的研究表明，扩散模型能够生成高质量的合成图像，有效平衡多样性和保真度。基于扩散模型或其变体的模型，如文献中提到的模型，已成功解决了文本条件图像合成的挑战。基于潜在扩散模型的Stable Diffusion在UNet框架内整合了文本条件，促进了基于文本的图像生成，已成为图像生成领域的主流模型。如LoRA和DreamBooth等技术所示，微调预训练的图像生成模型可以增强其对特定应用场景的适应性。在文本到图像生成的主题控制中，一些研究聚焦于为特定的画面概念进行定制生成，ControlNet还额外提供了对深度信息等其他模态的控制。Animatediff 提出时序注意力模块，将Stable Diffusion扩展成视频生成模型。processpainter 首次提出通过预训练时序模型学习艺术家的绘画过程，受此启发， 本文使用DiT 的in-context 能力生成分层SVG创作过程。

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/method} % Replace with your image file
    \caption{The LayerTracer architecture comprises three key components: (1) \textbf{Layer-wise Model}: Pretrained on our proposed dataset to generate layered pixel sequences from text prompt; (2) \textbf{Image2Layers Model}: Merges LoRA with the Flux base DiT, enabling image-conditioned generation through VAE-encoded latent tokens; (3) \textbf{Layer-wise Vectorization}: Converts raster sequences to SVGs via differential analysis between adjacent layers, followed by Bézier optimization using vtracer to eliminate redundant paths while preserving structural fidelity.}
    \label{method}
\end{figure*}


\subsection{SVG Generation }
Scalable Vector Graphics (SVGs) are widely utilized in design owing to their advantages like geometric manipulability, resolution independence, and compact file structure. SVG generation often involves training neural networks to produce predefined SVG commands and attributes using architectures such as RNNs \cite{im2vec}, VAEs \cite{svg2, svg+vae, strokenuwa}, and Transformers \cite{svg2, deepvecfont, iconshop}. Nonetheless, the absence of large-scale vector datasets constrains their generalization capabilities and the creation of complex graphics, with most datasets focusing on specific areas like monochromatic vector icons \cite{iconshop} and fonts \cite{deepvecfont, clipfont}.

An alternative to directly training an SVG generation network is optimizing it to match a target image during the evaluation phase, employing differentiable rasterizers to bridge vector graphics and raster images \cite{diffvg}. This method optimizes SVG parameters based on pretrained vision-language models. Advances in models like CLIP \cite{clip} have facilitated effective SVG generation methods such as CLIPDraw \cite{clipdraw}, CLIPasso \cite{clipasso}, and CLIPVG \cite{clipvg}. while DreamFusion \cite{dreamfusion} demonstrates the superior generative capabilities of diffusion models. VectorFusion \cite{vectorfusion}, DiffSketcher \cite{diffsketcher}, and SVGDreamer combine differentiable rasterizers with text-to-image diffusion models to produce vector graphics, achieving notable results in iconography and sketching. However, these methods still face challenges with editability and graphical quality. Recent studies \cite{nivel, t2v} have blended optimization-based methods with neural networks to enhance vector representations by integrating geometric constraints. 

The primary issue with methods that optimize a set of vector primitives through SDS loss is their reliance on image generation model priors, which often leads to redundant and noisy results. These outputs lack clear hierarchical structures and fail to meet design specifications. In this paper, we innovatively propose an alternative approach to utilizing image generation model priors. Specifically, we leverage the in-context learning capability of Diffusion Transformers to generate the creation process of SVG graphics, combined with vectorization to achieve cognitive-aligned layered SVG generation.

% 可缩放矢量图形（SVG）因其几何可操作性、分辨率独立性和紧凑的文件结构等优点而广泛用于设计中。一种SVG生成方法涉及训练神经网络输出预定义的SVG命令和属性。这些网络通常使用如RNN、VAE和Transformer等架构。然而，缺乏大规模矢量数据集限制了它们的泛化能力和创建复杂图形的能力。大多数数据集集中在狭窄的领域，如单色矢量图标、表情符号和字体。

% 直接训练SVG生成网络的一种替代方法是在评估阶段优化以匹配目标图像, 基于可微栅格化器，以桥接矢量图形和栅格图像。最近的工作已经使用可微栅格化器来克服数据集限制。这种方法基于预训练的视觉语言模型优化SVG参数。如CLIP等模型的进步已经使得成功的草图生成和SVG编辑成为可能，例如CLIPDraw和CLIPasso，而DreamFusion则展示了扩散模型在生成能力上的优越性。VectorFusion、DiffSketcher和SVGDreamer结合了可微栅格化器和文本到图像的扩散模型来生成矢量图形，在图标和素描方面取得了令人瞩目的成果。然而，这些方法仍然存在可编辑性和图形质量有限的问题。最近的研究结合了基于优化的方法和神经网络来通过引入几何约束改善矢量表示。

% 通过SDS loss优化一组矢量图元，从而利用图像生成模型先验的方法最大的问题是结果冗余而嘈杂，没有清晰的分层结构，不符合设计规范。 本文创新性的提出另一种利用图像生成模型先验的方法， 利用DiT的in-context能力生成SVG图形的创建过程，结合矢量化实现cognitive-aligned layered SVG generation.



\subsection{Vectorization}
Raster image vectorization or image tracing is a well-studied problem in computer graphics\cite{svg1, svg2, svg3, svg4}. Diffvg\cite{diffvg} proposes a differentiable rendering method for vectorization, which found shape gradients by differentiating the formula of Reynolds transport theorem with Monta-Carlo edge sampling. Meanwhile, combining differentiable rendering techniques with deep learning models are also studied for image vectorization\cite{live, clipasso, cliptexture}. Direct raster-to-vector conversion with neural networks are supported for the relatively simple images\cite{svg+vae, svg2, im2vec}.  Stroke-based rendering can be used to fit a complex image with a sequence of vector strokes \cite{singh2022intelli, liu2021paint, hu2023stroke}, but the performance is limited by the predefined strokes. Diffvg\cite{diffvg} can also be leveraged to fit an input image with a set of randomly initialized vector graphical elements. Based on Diffvg, LIVE \cite{live} proposes a coarse-to-fine vectorization strategy, with cost tens of minute. CLIPVG\cite{clipvg} proposes a multi-round vectorization strategy,  providing additional graphic elements for the image manipulation task. LIVE \cite{live} and O\&R \cite{oar} achieve hierarchical vectorization through optimization-based methods, but their results show a significant gap compared to human-designed works, lacking logical coherence. In contrast to these approaches, our proposed LayerTracer leverages the prior knowledge of the Diffusion Transformer model, reformulating the hierarchical vectorization task as a problem of predicting preceding frames from a reference image.

% 光栅图像的矢量化或图像追踪是计算机图形学中一个经过深入研究的问题\cite{svg1,svg2,svg3,svg4,svg5}。Diffvg\cite{diffvg} 提出了一种可微分的渲染方法用于矢量化，该方法通过对Reynolds运输定理的公式进行微分，并使用蒙特卡洛边缘采样找到形状梯度，可以用来通过一组随机初始化的矢量图形元素拟合输入图像。 与此同时，结合可微分渲染技术与深度学习模型进行图像矢量化的研究也在进行中\cite{live}。对于相对简单的图像，可以直接使用神经网络进行光栅到矢量的转换\cite{svg+vae, deepsvg, im2vec}。基于笔触的渲染可以用来通过一系列矢量笔触拟合复杂图像\cite{learntopaint, stylizedneuralpainting, clipasso}，但性能受限于预定义的笔触。LIVE 和O&R 通过优化的方法实现了层次矢量化，但是结果和人类设计师的作品有较大差距，不具有逻辑性。不同于上述方法，我们提出的LayerTracer利用DiT模型的先验证， 把层次矢量化任务重新定义为从参考图预测之前的帧的任务。

\section{Method}

In this section, we begin by exploring the preliminaries on diffusion transformer as
detailed in section 3.1. Then introduce the overall architecture of our method in section 3.2, followed by detailed descriptions of the key modules: dataset construction methods in 3.3, Layer-wise image generation in section 3.4, Image Condition Model in section 3.5, and Layer-Wise Vectorization in section 3.6. 

% \subsection{Preliminary}

% The Diffusion Transformer (DiT) model \cite{}, employed in architectures like FLUX.1 \cite{}, Stable Diffusion 3 \cite{}, and PixArt~\cite{2}, uses a transformer as the denoising network to iteratively refine noisy image tokens.

% A DiT model processes two types of tokens: noisy image tokens $X \in \mathbb{R}^{N \times d}$ and text condition tokens $C_T \in \mathbb{R}^{M \times d}$, where $d$ is the embedding dimension, and $N$ and $M$ are the number of image and text tokens. Throughout the network, these tokens maintain consistent shapes as they pass through multiple transformer blocks.

% In FLUX.1, each DiT block consists of layer normalization followed by Multi-Modal Attention (MMA) \cite{28}, which incorporates Rotary Position Embedding (RoPE) \cite{33} to encode spatial information. For image tokens $X$, RoPE applies rotation matrices based on the token's position $(i,j)$ in the 2D grid:
% \begin{equation}
% X_{i,j} \rightarrow X_{i,j} \cdot R(i,j),
% \end{equation}
% where $R(i,j)$ is the rotation matrix at position $(i,j)$. Text tokens $C_T$ undergo the same transformation with their positions set to $(0,0)$.

% The multi-modal attention mechanism then projects the position-encoded tokens into query $Q$, key $K$, and value $V$ representations. It enables the computation of attention between all tokens:
% \begin{equation}
% \text{MMA}([X; C_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
% \end{equation}
% where $[X; C_T]$ denotes the concatenation of image and text tokens. This formulation enables bidirectional attention.

\subsection{Preliminary}

The Diffusion Transformer (DiT) model \cite{dit}, which appears in frameworks such as FLUX.1 \cite{flux2023}, Stable Diffusion 3 \cite{sd}, and PixArt~\cite{pixart}, employs a transformer-based denoising network to iteratively refine noisy image tokens.

DiT processes two categories of tokens: noisy image tokens $X \in \mathbb{R}^{N \times d}$ and text condition tokens $C_T \in \mathbb{R}^{M \times d}$, where $d$ is the embedding dimension, and $N$ and $M$ respectively represent the numbers of image and text tokens. As these tokens move through the transformer blocks, they retain consistent dimensions.

In FLUX.1, each DiT block applies layer normalization before Multi-Modal Attention (MMA) \cite{mma}, incorporating Rotary Position Embedding (RoPE) \cite{rope} to capture spatial context. For image tokens $X$, RoPE applies rotation matrices based on a token’s position $(i,j)$ in the 2D grid:
\begin{equation}
X_{i,j} \rightarrow X_{i,j} \cdot R(i,j),
\end{equation}
where $R(i,j)$ is the rotation matrix at position $(i,j)$. Text tokens $C_T$ are similarly transformed with their positions specified as $(0,0)$.

The multi-modal attention mechanism then projects these position-encoded tokens into query $Q$, key $K$, and value $V$ representations, enabling attention across all tokens:
\begin{equation}
\text{MMA}([X; C_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $[X; C_T]$ denotes concatenation of image and text tokens. This formulation ensures bidirectional attention among the tokens.


\subsection{Overall Architecture.}

%LayerTracer由以下组成部分构成：**过程LoRA**、**Image2Layers模型**和**逐层矢量化**。  
% 1. 我们收集设计师创建分层矢量图形的过程，将其按照蛇形展开成3*3和2*2的grid数据集。
% 2. 之后，我们在提出的数据集上使用LoRA方法进行预训练，实现了从文本描述生成分层像素图像的能力。  
% 3. 将第二步中的LoRA与Flux基础模型结合，创建一个新的基础模型。Image2Layer模型引入了一种基于图像的条件机制，通过进一步的LoRA微调，实现对参考图像的创建过程的预测。  
% 4. 在逐层矢量化阶段，模型逐层将生成的像素图像转换为高质量矢量图形， 经过相邻层差异分析，过滤和矢量化。

LayerTracer consists of the following components: Serpentine dataset construction, Layer-wise model training, Image2Layers model, and the layer-wise vectorization. Initially, we collected the processes by which designers create layered vector graphics, arranging them in a serpentine layout to form 3x3 and 2x2 grid datasets. Following this, we utilized the LoRA method for pre-training on the proposed dataset, thereby enabling the generation of layered pixel images from textual descriptions. Subsequently, we integrated the LoRA from the previous step with the Flux base model to establish a new foundational model. The Image2Layer model introduces an image-based conditional mechanism that, through additional LoRA fine-tuning, predicts the creation process of reference images. Finally, in the layer-wise vectorization stage, the model sequentially transforms the generated pixel images into high-quality vector graphics, which are analyzed, filtered, and vectorized based on the differences between adjacent layers. 


\subsection{Serpentine Dataset Construction}
Our dataset construction includes 20,000 layered SVGs created by designers, encompassing black outline icons, regular icons, emojis, and illustrative graphics. Each sequence is composed of either 9 or 4 frames, arranged in 3x3 or 2x2 grids, resulting in resolutions of 1056x1056 and 1024x1024 respectively. To capture the process of designers creating layered SVGs, we propose a automated data generation pipeline that deconstructs the layered SVG graphics into sequences based on the grouping logic and element hierarchy within the SVG files. Additionally, the pipeline incorporates a human-in-the-loop process to filter out nonsensical sequences.

In the attention mechanism of DiT, tokens tend to focus on spatially adjacent tokens. This tendency stems from the strong correlations between adjacent image pixels captured during the pre-training of diffusion models. To enhance the model's learning of grid sequences \cite{grid}, we introduce the serpentine dataset construction method. As shown in Figure \ref{method}, we arrange the sequences of 9 and 4 frames in a serpentine layout within the grid, ensuring that temporally adjacent frames are also spatially adjacent (either horizontally or vertically). In our ablation experiments, we confirmed that this design is crucial for the coherence of sequence generation. To facilitate subsequent hierarchical vectorization, for icons with black line strokes, we place the black line layer separately in the first frame during dataset creation. Similarly, during the generation phase, we vectorize the black line layer and overlay it onto the subsequent results.

% 我们的数据集构建包含了20,000个由设计师创建的分层SVG，涵盖黑色描边图标、普通图标、emoji和插图图形。每个序列由9帧或4帧组成，排列为3x3或2x2的网格，拼接后分辨率分别为1056x1056和1024*1024。为了捕捉设计师创建分层SVG的过程，我们提出了一种自动化的数据生成管道，该管道根据SVG文件中分组逻辑关系和元素层次，将设计师创建的分层SVG图形分解为序列。同时，管道引入了“人类参与”的流程，用于筛选出不合理的序列。

% DiT的注意力机制中的令牌倾向于关注空间上相邻的令牌。这种倾向源于扩散模型在预训练过程中捕获的邻近图像像素之间的强相关性。为了提升模型对Grid序列的学习效果，我们提出Serpentine dataset construction方法, 如图2所示，我们将9帧和4帧序列按照蛇形排列成网格，以保证时序上相邻的两帧在空间上也是相邻的（横向相邻或纵向相邻）。 我们在消融实验中证实了，这种设计对序列生成的连贯性至关重要。

% ！！！加一句，为了方便后续层次矢量化，对于带黑色线条描边的icon，我们在数据制作是，吧黑色线条图层单独放在第一帧，生成阶段同理，我们对黑色线条图层矢量化后叠加在后续结果上。


\subsection{Layer-wise Image Generation} 
% 在海量图文对上训练的DiT模型天生具有上下文生成能力，通过适当激活和增强这一能力，可以将其用于复杂生成任务。由于文本到图像模型可以解释合并的提示语，我们可以在不改变其架构的情况下复用这些模型用于上下文生成，仅需改变输入数据而无需修改模型本身。基于这些见解，我们设计了一个简单但有效的管道，用于学习人类设计师创建SVG的分层逻辑 ：1.图像拼接：将一组图像拼接成一个大图像，而不是拼接注意力tokens。这种方法在扩散变换器（DiTs）中大致等效于token拼接，忽略了变分自编码器（VAE）组件引入的差异。我们将9帧过程数据拼成3*3，将4帧过程数据拼成2*2， 我们采用S形和C形进行拼贴，这样会保证相步骤邻的帧在空间上彼此相邻， 有利于模型通过空间自注意力机制学习相关性。 2. 基于最先进的DIT架构的Flux模型， 采用LoRA微调的方式进行训练。 

DiT models, trained on massive image-text pairs, inherently possess contextual generation capabilities. By appropriately activating and enhancing this ability, they can be utilized for complex generation tasks. Since text-to-image models can interpret merged prompts, they can be reused for in-context generation without altering their architecture. This only requires changes to the input data rather than modifications to the model itself. Building on this insight, we designed a simple yet effective pipeline to learn the hierarchical logic employed by human designers in creating layered SVGs.


% \noindent \textbf{Image Concatenation.} We concatenate a set of images into a single large image instead of concatenating attention tokens. This method is roughly equivalent to token concatenation in DiT, disregarding differences introduced by the Variational Autoencoder (VAE) component. 

% Specifically, we arrange 9-frame process data into a 3×3 grid and 4-frame process data into a 2×2 grid. We use serpentine layouts for tiling, ensuring that temporally adjacent frames are spatially adjacent. This design helps the model learn correlations through spatial self-attention mechanisms.


% 我们将一组图像拼接成一张大图，而不是拼接注意力令牌（attention tokens）。这种方法大致等同于扩散变换器（Diffusion Transformers, DiTs）中的令牌拼接，忽略变分自编码器（Variational Autoencoder, VAE）组件引入的差异。具体来说，我们将 9 帧过程数据排列成 3×3 网格，将 4 帧过程数据排列成 2×2 网格。我们使用蛇布局进行平铺，确保时间上相邻的帧在空间上也相邻。这种设计有助于模型通过空间自注意力机制学习相关性。


\noindent \textbf{Layer-wise Model Training.} Due to the size of the dataset, we adopt LoRA fine-tuning for training which can be formulated as:
\begin{equation}
W = W_0 + \Delta W,
\end{equation}
where \(W_0\) represents the original weights of the pre-trained model, and \(\Delta W\) denotes the low-rank adaptation updates introduced during fine-tuning. This formulation enables efficient training by keeping \(W_0\) fixed and applying lightweight updates through \(\Delta W\), which allows the model to balance generalization from the pre-trained weights with task-specific adaptation provided by the fine-tuned updates.

\noindent \textbf{Loss function.} We employ the conditional flow matching loss function, integral to training and optimizing the generative model, is defined as follows:
\begin{equation}
L_{CFM} = E_{t, p_t(X|\epsilon), p(\epsilon)} \left[ \left\| v_\Theta(X, t) - u_t(X|\epsilon) \right\|^2 \right]
\end{equation}

Where \( v_\Theta(X, t) \) represents the velocity field parameterized by the neural network's weights,t is timestep, \( u_t(X|\epsilon) \) is the conditional vector field generated by the model to map the probabilistic path between the noise and true data distributions. 

% 本节我们介绍Image2Layers模型， 在上一节的基础上引入image condition， 将层次矢量化重新定义成一个“逆向工程”，预测SVG是如何被逐层创建的。

%Image2Layers模型的训练难点在于高质量序列的有限性。虽然少量数据对LORA训练已经足够，但从零初始化训练一个可控性插件（如 ControlNet 或 Adapter）在数据有限的情况下相当有挑战性。为此，我们设计了一种高效的可控性方案，通过复用预训练的DiT模型并将其改造成能够接受图像上下文作为条件的生成模型。具体来说：训练阶段：我们将制作过程序列按逻辑顺序拼成 2×2 或 3×3 的训练图像，将最后一帧（context image）输入 VAE 提取潜变量（latent），并直接拼接到去噪潜变量的最后。通过自注意力机制，context latent 为其他帧的去噪过程提供条件信息，提升生成的逻辑性和连贯性。推理阶段：使用尾参考图像作为condition预测前几帧，从而推断参考图中的SVG是如何逐层被制作的。


\subsection{Image2Layers Model}

In this section, we introduce the Image2Layers model, which builds upon the previous section by incorporating image conditioning. This approach redefines hierarchical vectorization as a "reverse engineering" task, predicting how an SVG is created layer by layer.

The primary challenge in training the Image2Layers model lies in the limited availability of high-quality sequential data. While a small dataset may suffice for LoRA training, initializing a controllable plugin (such as ControlNet \cite{controlnet} or IP-Adapter \cite{ipa}) from scratch with limited data is highly challenging. To address this, we design an efficient controllability framework by repurposing a pre-trained DiT model and adapting it to accept image context as a conditioning input. Specifically:

\noindent \textbf{Training Phase.} We concatenate procedural sequences into 2×2 or 3×3 training grids. The final frame (context image) is passed through the VAE to extract latent variables, which are directly appended to the end of the denoising latent. Through self-attention mechanisms, the context latent provides conditional information to the denoising processes of other frames, enhancing the logical consistency and coherence of the generation. The condition image is then fed into a VAE to obtain its latent representation, which is directly appended to the denoising latent at the end. Multi-modal attention mechanisms are used to provide conditional information for the denoising of other frames. 
\begin{equation}
\text{MMA}([X; C_I; C_T]) = \text{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V,
\end{equation}
where $[X; C_I; C_T]$ denotes the concatenation of image and text tokens. This formulation enables bidirectional attention.

\noindent  \textbf{Inference Phase.} During inference, we use the reference image as a condition to predict the earlier layers, thereby inferring how the SVG in the reference image was constructed layer by layer.

% \subsection{Layer-Wise Vectorization}
% The process begins by segmenting the generated grid sequence into individual images. Adjacent frames are then subjected to difference detection, and the differences are converted into binary images through threshold processing. The third step involves cleaning the difference images to address contour discrepancies between frames, removing noise, and filtering irrelevant contours to extract meaningful areas of change. The fourth step applies transparency adjustments to the difference images to prevent background regions from being redundantly overlaid after vectorization. Finally, the VTracer vectorization process is applied. Starting with the first frame (illustration) or the second frame (icon) as the base unit, the differential SVG path data is extracted frame by frame and appended to the base SVG. For icons, an additional black outline from the first frame is added, while illustrations do not require this step.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/text2image0.pdf} % Replace with your image file
    \caption{Given a text prompt, LayerTracer generates cognitive-aligned layered SVGs that mimic human design cognition.}
    \label{fig3}
\end{figure*}

% 输入text prompt， layerTracer 可以生成cognitive-aligned layered SVGs.

\subsection{Layer-Wise Vectorization}
To achieve hierarchical vectorization of input images, our process begins by segmenting grid images into individual cells. This facilitates independent processing for subsequent vectorization stages. Specifically for icons, accurate extraction of black lines is crucial as they represent key structural elements of the image. To address common issues of line distortion, we employ a series of preprocessing steps: grayscale conversion highlights contrast between lines and background; Gaussian blurring smooths out noise; and adaptive thresholding via the Otsu's thresholding method ensures robust line separation. These preprocessed lines are then integrated into a transparent PNG, focusing vectorization efforts on relevant areas.

Furthermore, to capture the layered details of images, we perform differential extraction between adjacent cells, identifying significant pixel changes to highlight areas of variation. This involves converting cell images to grayscale, computing absolute differences, applying binary thresholding, and refining the output with morphological operations to produce clean, meaningful contours of change. These contours are saved as transparent PNGs for subsequent vectorization.

The final step involves vectorizing these differential layers using tools like vtracer, optimizing parameters to balance detail retention and file size, and ultimately merging all vectorized layers into a single SVG file. This method preserves the image's global structure while highlighting intricate changes between cells, resulting in a layered and editable SVG suitable for detailed graphical representations.

% 为了实现输入图像的分层矢量化，我们首先将九宫格或四宫格图像分割成独立的单元格。这一步骤为后续的矢量化处理提供了独立的处理单元。对于图标，准确提取图像中的黑色线条是至关重要的，因为这些线条代表了图像的主要结构信息。为了解决矢量化过程中线条可能出现的失真问题，我们采用了一系列预处理步骤：灰度转换用于强调线条与背景的对比；高斯模糊帮助平滑噪声；OTSU方法的自适应阈值处理确保了线条的鲁棒分离。处理后的线条被整合到一个透明的PNG中，确保矢量化工具仅关注相关区域。

% 此外，为了捕捉图像的层次细节，我们对相邻单元格之间进行差异提取，通过比较像素值来识别显著的变化区域。这包括将单元格图像转换为灰度图，计算绝对差异，应用二值化处理，并通过形态学操作优化输出，产生清晰有意义的变化轮廓。这些轮廓以透明背景的PNG文件形式保存，为后续的矢量化提供输入。

% 最后一步是使用如 `vtracer` 等工具对这些差异层进行矢量化，优化参数以平衡细节保留和文件大小，并最终将所有矢量化层合并为一个SVG文件。这种方法不仅保留了图像的全局结构，还突出了单元格之间的细微变化，生成的分层和可编辑的SVG文件适用于详细的图形表示。

% The layer-wise vectorization process consists of the following stages:
% \textbf{Frame Segmentation and Difference Detection}: The generated grid sequence is segmented into individual image frames, and adjacent frames are compared to detect differences. These differences are then converted into binary images using a thresholding method.

% \textbf{Noise Removal and Contour Filtering}: The binary difference images are cleaned to resolve contour inconsistencies between frames, eliminate noise, and filter out irrelevant contours, leaving only meaningful regions of change.

% \textbf{Transparency Adjustment}: Transparency is applied to the filtered difference images to ensure that background areas are not redundantly overlaid after vectorization.

% \textbf{Vectorization with VTracer}: The VTracer vectorization process is employed. Starting with the first frame (illustration) or the second frame (icon) as the base unit, differential SVG path data is extracted for each frame and appended sequentially to the base SVG. For icons, an additional black outline is added from the first frame, while illustrations do not require this step.

% This structured approach ensures that the final vectorized output maintains clarity, precision, and consistency across all layers.


\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/vec.pdf} % Replace with your image file
    \caption{Given a raster image of an icon as input, LayerTracer predicts how the icon was created layer by layer, achieving cognitive-aligned layered vectorization.}
    \label{fig4}
\end{figure*}

\section{Experiment}

\subsection{Experiment Setting}

% 预训练阶段，我们基于预训练的DIT架构的Flux 1.0 dev模型，训练分辨率为1056*1056（3*3拼图），1024*1024（2*2）拼图， 采用LORA微调的方式，LoRA Rank 256， Batchsize 16， 微调20，000 step。 Image condition Model训练阶段，我们将LayerTracer LoRA和base model merge， LoRA merge的权重是 1.0， 然后在同样的数据集上， 通过LORA微调的方式微调20，000 step。

\textbf{Experiment Details.}  
During the pretraining stage, we utilized the Flux 1.0 dev model based on the pretrained DiT architecture. Training resolutions included 1056×1056 (3×3 grids) and 1024×1024 (2×2 grids). The LoRA fine-tuning approach was applied with a LoRA rank of 256, a batch size of 16, and 20,000 fine-tuning steps.  For training the Image Condition Model, we merged the Layer-wise Image Generation LoRA with the base model, setting the LoRA merge weight to 1.0. Subsequently, the model was fine-tuned on the same dataset using the LoRA approach for an additional 20,000 steps.



% 本文对比的Baseline Methods是diffvg， LIVE，O&R。

\noindent \textbf{Baseline Methods.}  
The baseline methods in text-to-svg genration methods are SVGDreamer \cite{svgdreamer}, Vecfusion \cite{vectorfusion}and DiffSketcher \cite{diffsketcher}. The baseline methods in vectorization include diffvg \cite{diffvg}, LIVE \cite{live}, and O\&R \cite{oar}.

% 为了解决高质量分层SVG数据集不足的问题， 本文提出包含20，000+ 个分层SVG和创建过程的数据集，称为LayerSVG dataset。为了对比实验的公平性， 本文还将noto-emoji 数据集加入benchmark进行定量评估。对于text-to-SVG 任务和 layer-wise vectorization 任务， 我们分别选择50个prompt和50张图片作为benchmark进行测试。

\noindent  \textbf{Benchmarks.}  To address the lack of high-quality layered SVG datasets, this paper introduces a dataset containing over 20,000 layered SVGs and their creation processes, named the LayerSVG Dataset. To ensure fairness in comparative experiments, the Noto-Emoji \cite{noto_emoji} dataset is also included in the benchmark for quantitative evaluation. For the text-to-SVG task and the layer-wise vectorization task, we select 50 prompts and 50 images, respectively, as benchmarks for testing.

% To address the scarcity of high-quality layered SVG datasets, we propose the \textit{LayerSVG dataset}, which consists of over 20,000 layered SVGs and their creation processes. For fair comparison in the experiments, we also conduct quantitative evaluations on the \textit{noto-emoji} dataset. 

% 图4 （a）展示了LayerTracer的分层图形生成结果，LayerTracer可以很好的生成符合文本描述的 Human-Like Layered SVG。 图4 （b）展示了LayerTracer的层次矢量化结果，生成结果分层清晰合理，和参考图一致性高。

\subsection{Generation Results}

% Fig. \ref{fig3} shows the layer-wise graphic generation results of LayerTracer, demonstrating its ability to produce cognitive-aligned layered SVGs that follwing well with text descriptions. Fig. \ref{fig4}  presents the layer-wise vectorization results, which exhibit clear and reasonable layering with high consistency to the input image.

Fig. \ref{fig3} demonstrates LayerTracer's capability to generate cognitively-aligned layered SVGs that adhere to text descriptions while maintaining logical layer hierarchies (e.g., background-to-foreground ordering and grouped semantic elements). The outputs preserve essential design properties including layer independence, non-overlapping paths, and topological editability. Fig. \ref{fig4} further illustrates layer-aware vectorization results, where input raster images are decomposed into clean vector layers with consistent spatial alignment and minimal shape redundancy. 


\subsection{Comparison and Evaluation}
In the text-to-SVG task, we compute FID and CLIP Score. For the hierarchical vectorization task, we follow the evaluation methodology from previous works. We calculate MSE to assess the consistency between the reconstructed image and the input image. Additionally, we record the number of SVG shapes used, as fewer shapes indicate a more concise and efficient result. Table \ref{tab1} and \ref{tab2} show that LayerTracer achieves the best results across most metrics.

%  在text 2 svg 任务中， 我们计算FID和CLIP Score。在层次矢量化任务中，follow之前工作的评估方式。我们我们计算MSE来评估图像重建和输入图片的一致性， 同时记录使用的SVG形状数目，数目越少意味着越简洁有效。 表1和表2显示， LayerTracer在大多数指标上取得了最好的结果。

% \begin{table}[ht]
% \centering
% \scriptsize % 调整文字大小
% \caption{Comparison with SOTA SVG Generation Methods.}
% \begin{tabular}{lp{2.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}
% \toprule
% & \textbf{Methods}   & \textbf{CLIP-Score} & \textbf{Time Cost} & \textbf{No. Paths} \\ \midrule
% & Vecfusion          &31.10                   & 4668.05                   & 128.00                     \\ 
% & SVGDreamer         & 32.68                   & 5715.44                   & 512.00                     \\ 
% & DiffSketcher       & 31.47                   & 3374.58                   & 512.00                     \\ 
% & Ours               & 31.78                   & 26.84                   & 35.39                     \\ \midrule
% \end{tabular}
% \label{tab:comparison}
% \end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/compare2.pdf} % Replace with your image file
    \caption{Compare with baseline methods in Text-to-SVG generation task.}
    \label{fig5}
\end{figure}

% \begin{table}[ht]
% \centering
% \footnotesize % 调整文字大小
% \caption{Comparison with SOTA SVG Generation Methods. The best results are denoted as \textbf{Bold}.}
% \begin{tabular}{lp{1.8cm}p{1.4cm}p{1.5cm}p{1.2cm}p{1.15cm}}

% \toprule
% & \textbf{Methods}   & \textbf{CLIP-Score}↓ & \textbf{Time Cost(s)}↓ & \textbf{No. Paths}↓ & \textbf{layer-wise}  \\ \midrule
% & Vecfusion \cite{vectorfusion}         &31.10                   & 4668                  & 128.00        & False               \\ 
% & SVGDreamer \cite{svgdreamer}        & 32.68                   & 5715                   & 512.00       & False              \\ 
% & DiffSketcher \cite{diffsketcher}   & 31.47                   & 3374                   & 512.00       & False              \\ 
% & Ours               & \textbf{33.76}                   & \textbf{27}                   & \textbf{35.39}         & True            \\ \midrule
% \end{tabular}
% \label{tab1}
% \end{table}

% \begin{table}[ht]
% \centering
% \footnotesize  % 调整文字大小
% \caption{Comparison with SOTA Vectorization Methods.  The best results are denoted as \textbf{Bold}.}
% \begin{tabular}{lp{1.4cm}p{1.1cm}p{1.5cm}p{1.2cm}p{1.4cm}}
% \toprule
% & \textbf{Methods}   & \textbf{MSE}↓ & \textbf{Time Cost(s)}↓ & \textbf{No. Paths}↓ & \textbf{Layer-wise} \\ \midrule
% & Diffvg \cite{diffvg}    &  $2.02 \times 10^{-4}$                  & 393                   & 256.00       & False               \\ 
% & LIVE \cite{live}     & $5.21 \times 10^{-4}$                  & 3147                  & 46.00         & True             \\ 
% & O\&R \cite{oar}     & $2.01 \times 10^{-4}$                  & 612                   & 64.00          & True            \\ 
% & Ours      & \textbf{$1.96 \times 10^{-4}$ }        & \textbf{34}          & \textbf{29.98}    & True         \\ \bottomrule
% \end{tabular}
% \label{tab2}
% \end{table}





\begin{figure*}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/vec-R.pdf} % Replace with your image file
    \caption{Compare with baseline methods in layer-wise vectorization task. Our results are more concise and exhibit more logical layering.}
    \label{fig6}
\end{figure*}

% \noindent \textbf{Qualitative Evaluation.} This section presents qualitative analysis results. As illustrated in Fig.~\ref{fig5}, our method achieves visually concise and aesthetically coherent outputs in text-to-SVG tasks, effectively adhering to application-specific requirements for icons and emojis. In contrast, baseline methods exhibit visual clutter with irregular path arrangements. For layer- wise vectorization tasks, which is shown in Fig.~\ref{fig6},  comparative evaluations against  baseline approaches demonstrate our method's superior adherence to design principles: generated layers maintain logical spatial hierarchies and semantic grouping, while baseline counterparts produce fragmented outputs with misaligned layer boundaries and topological inconsistencies.

\noindent \textbf{Qualitative Evaluation.} This section presents qualitative analysis results. Figure~\ref{fig5} shows that our method produces concise and coherent outputs for text-to-SVG tasks, aligning with specific requirements for icons and emojis. Baseline methods, in contrast, result in visual clutter and irregular paths. Figure~\ref{fig6} demonstrates our method's superior performance in layer-wise vectorization tasks, maintaining logical spatial hierarchies and semantic grouping, unlike baselines which yield fragmented and misaligned outputs.

% ```latex
% \noindent \textbf{定性评估。}本节提供定性分析结果。如图~\ref{fig5}所示，我们的方法在文本到SVG任务中产生了简洁且协调的输出，符合图标和表情符号的特定需求。相比之下，基线方法则展示了视觉上的杂乱和不规则路径排列。如图~\ref{fig6}所示，与基线方法的比较评估显示我们的方法在层次化矢量化任务中更好地遵循设计原则：生成的层保持逻辑空间层次和语义分组，而基线对应产生了碎片化的输出，层边界和拓扑结构不对齐。
% ```
\begin{table}[ht]
\centering
\footnotesize % 调整文字大小
\caption{Comparison with SOTA SVG Generation Methods. The best results are denoted as \textbf{Bold}.}
\vspace{-3mm} 
\begin{tabular}{lp{1.75cm}p{1.39cm}p{1.5cm}p{1.2cm}p{1.15cm}}

\toprule
& \textbf{Methods}   & \textbf{CLIP-Score}↓ & \textbf{Time Cost(s)}↓ & \textbf{No. Paths}↓ & \textbf{layer-wise}  \\ \midrule
& Vecfusion \cite{vectorfusion}         &31.10                   & 4668                  & 128.00        & False               \\ 
& SVGDreamer \cite{svgdreamer}        & 32.68                   & 5715                   & 512.00       & False              \\ 
& DiffSketcher \cite{diffsketcher}   & 31.47                   & 3374                   & 512.00       & False              \\ 
& Ours               & \textbf{33.76}                   & \textbf{27}                   & \textbf{35.39}         & True            \\ \midrule
\end{tabular}
\label{tab1}
\end{table}

\begin{table}[ht]
\centering
\footnotesize  % 调整文字大小
\caption{Comparison with SOTA Vectorization Methods.  The best results are denoted as \textbf{Bold}.}
\vspace{-3mm} 
\begin{tabular}{lp{1.4cm}p{1.1cm}p{1.5cm}p{1.2cm}p{1.4cm}}
\toprule
& \textbf{Methods}   & \textbf{MSE}↓ & \textbf{Time Cost(s)}↓ & \textbf{No. Paths}↓ & \textbf{Layer-wise} \\ \midrule
& Diffvg \cite{diffvg}    &  $2.02 \times 10^{-4}$                  & 393                   & 256.00       & False               \\ 
& LIVE \cite{live}     & $5.21 \times 10^{-4}$                  & 3147                  & 46.00         & True             \\ 
& O\&R \cite{oar}     & $2.01 \times 10^{-4}$                  & 612                   & 64.00          & True            \\ 
& Ours      & \textbf{$1.96 \times 10^{-4}$ }        & \textbf{34}          & \textbf{29.98}    & True         \\ \bottomrule
\end{tabular}
\label{tab2}
\end{table}



\noindent \textbf{Quantitative Evaluation.}
Table \ref{tab1} and \ref{tab2} present the quantitative evaluation results. In the SVG generation task, our method achieves the highest CLIP-Score with the lowest average number of paths and shortest time cost. Notably, baseline methods fail to produce rationally layered outputs. For the layer-wise vectorization task, our approach outperforms all baselines across metrics: the lowest average path count demonstrates superior simplicity and efficiency, while faster runtime and higher reconstruction consistency further validate its effectiveness.

% 表1和表2展示了量化评估结果。 在SVG生成任务中， 我们的方法用最少的path数（平均），最短的时间取得了最好的CLIP-Score， 此外，baseline方法的结果不是合理分层的。在layer-wise vectorization任务中，我们的方法同样在所有指标上领先，平均path数更少意味着更简洁有效，更快，重建一致性更高。

\begin{figure}[ht]
    \centering
    \includegraphics[width=1.0\linewidth]{image/Ablation.pdf} % Replace with your image file
    \caption{Ablation study of Serpentine Layout Strategy.}
    \label{fig7}
\end{figure}


\subsection{Ablation Study of Serpentine Layout Strategy.}
In this section, we conduct an ablation study on the serpentine layout strategy. As shown in Fig.~\ref{fig7}, when the serpentine layout strategy is not used to construct the training dataset, incomplete decomposition, undesirable repetitions, and abrupt changes between frames are more likely to occur. The quantitative evaluation results are presented in Table \ref{tab3}. For the layer-wise vectorization task, we calculate the MSE between the predicted results for 9 frames and the ground truth on both the training and test sets. When the serpentine layout strategy is not applied, the MSE is higher.

\begin{table}[ht]
\centering
\footnotesize  % Adjust text size
\caption{Quantitative Evaluation of Serpentine Layout Strategy.}
\begin{tabular}{lp{1.4cm}p{1.0cm}p{1.1cm}p{1.1cm}p{1.qcm}}
\toprule
\textbf{Methods} & \textbf{MSE\_train}↓ & \textbf{SSIM\_train}↑ & \textbf{MSE\_test}↓ & \textbf{SSIM\_test}↑ \\ \midrule
w/o Serpentine Layout         & $2.03 \times 10^{-4}$  & 0.964              & $2.41 \times 10^{-4}$ & 0.959 \\
Full                          & $1.65 \times 10^{-4}$  & 0.971              & $1.99 \times 10^{-4}$ & 0.963 \\
\bottomrule
\end{tabular}
\label{tab3}
\end{table}

% 本节，  我们对serpentine layout策略进行消融 如图7所示，当不采用serpentine layout策略构造训练集，容易出现拆分不彻底的情况，和一些不想要的重复，以及帧间突变。我们在表3中展示定量评估结果，在layer-wise vectorization任务中， 我们分别在训练集和测试集上计算9帧预测结果和ground truth的MSE。 当不采用serpentine layout策略，MSE更高。

% \begin{table}[h!]
% \centering
% \footnotesize
% \setlength{\tabcolsep}{0pt} 
% \caption{User Preferences for Human-Like Painting Styles Comparison}
% \begin{tabular}{lccc}
% \toprule
% Comparison Method & Anthropomorphic (\%) & Preference (\%)  \\
% \midrule
% Ours vs. LearnToPaint \cite{huang2019learning} & 78.2  & 68.2   \\
% Ours vs. Paint Transformer \cite{liu2021paint} & 80.4  & 65.9   \\
% Ours vs. Intelli-Paint \cite{singh2022intelli}  & 84.5 & 78.4   \\
% Ours vs. Stylized Neural Painting \cite{snp}  & 78.6  & 71.6   \\
% \bottomrule
% \end{tabular}
% \label{tab2}
% \end{table}


% 我们进行了用户研究，我们的实验对象是46名设计爱好者。我们通过数字问卷调研的形式，将我们的结果、和baseline的结果呈现给参与者。 在text2SVG generation任务中，参与者会选出更喜欢的结果和更符合prompt描述的结果，在layer-wise vectorization任务中，参与者会选出更喜欢的结果，和最合理的分层序列。 从图8可以看出， 和所有的baseline相比， LayerTracer显示出更好的用户偏好，prompt following和rationality of layering.

% 在层次矢量化过程中， laySVG 依赖现有矢量化方法，如Vtracer，Ptracer，同时继承了他们的缺陷。 对于image condition model， 给定光栅图将其转化成SVG，会受到训练数据分布的影响。 对于OOD的数据样本效果欠佳。

\subsection{User Study}
% We conducted a user study with 46 design enthusiasts using a digital questionnaire to present both our results and those of baseline methods. Participants were asked to select their preferred results and those that best matched the prompt descriptions in the text-to-SVG generation task. In the layer-wise vectorization task, they chose the results they preferred and the sequences with the most rational layering. As shown in Fig. \ref{fig8}, LayerTracer demonstrated superior user preferences, adherence to prompts, and rationality in layering compared to all baselines.

We conducted a user study with 46 design enthusiasts using a digital questionnaire, presenting results from both our method and baseline methods. Participants selected their preferred results and those best matching the prompt descriptions in the text-to-SVG task. In the layer-wise vectorization task, they chose their preferred results and the most logically layered sequences. As Fig. \ref{fig8} shows, LayerTracer outperformed all baselines in user preference, prompt adherence, and layer rationality.

% \begin{table}[ht]
% \centering
% \footnotesize  % Adjust text size
% \caption{Ablation study of Serpentine Layout Strategy.}
% \begin{tabular}{lcccc}
% \toprule
% \textbf{Methods} & \textbf{MSE\_train} & \textbf{SSIM\_train} & \textbf{MSE\_test} & \textbf{SSIM\_test} \\ \midrule
% w/o Serpentine Layout         & 0.000203           & 0.964              &0.000241      & 0.959 \\
% Full                          & 0.000165           & 0.971              &0.000199      & 0.963 \\
% \bottomrule
% \end{tabular}
% \label{tab:comparison}
% \end{table}






% \begin{table}[ht]
% \centering
% \footnotesize
% \caption{User preferences (\%) for different methods across two tasks (SVG generation and Vectorization).}
% \begin{tabular}{lccccc}
% \toprule
% \textbf{Task} & \textbf{User Group} & \textbf{Ours} & \textbf{LIVE} & \textbf{XX} & \textbf{Baseline 3} \\ 
% \midrule
% \multirow{\textbf{SVG Generation}} 
% & Professional (6) & 85 & 10 & 4 & 1 \\ 
% & Amateur (24)     & 78 & 12 & 8 & 2 \\ 
% \midrule
% \multirow{\textbf{Vectorization}}   
% & Professional (6) & 90 & 5  & 3 & 2 \\ 
% & Amateur (24)     & 82 & 10 & 6 & 2 \\ 
% \bottomrule
% \end{tabular}
% \label{tab:user_study}
% \end{table}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{image/userstudy.pdf} % Replace with your image file
    \caption{User Study Results. LayerTracer outperform in all three metrics.}
    \label{fig8}
\end{figure}


% \subsection{More Applications}

% \textbf{Emoji Anything.}  
% As shown in Figure 7(a), by integrating IPAdapter with LayerTracer, we developed a workflow capable of stylizing any photo into human-like SVG graphics.  

% \textbf{Customized SVG Generation.}  
% As shown in Figure 7(b), LayerTracer can be combined with IPAdapter or LoRA to achieve stylized SVG generation.  

% \textbf{Layer-wise SVG with Color Gradient.}  
% As shown in Figure 7(c), LayerTracer can be integrated with advanced vectorization methods that support color gradients to produce high-quality layered SVGs with gradient effects.




% 在分层矢量化的过程中，LayerTracer依赖于现有的矢量化方法，如VTracer，因此继承了它们的一些缺陷。此外，在超出分布（OOD）的数据样本或过于复杂的图像上的表现不理想。未来我们希望提出更智能的单层矢量化方案替代本文中使用的Vtracer。

\section{Limitations and Future Work}
During layer-wise vectorization, LayerTracer relies on methods like Vtracer, inheriting its limitations such as  need for manual adjustment of hyperparameters. Its performance also falters on out-of-distribution data and complex images. We aim to develop a smarter single-layer vectorization solution to replace Vtracer in the future.

% 本节展示LayerTracer 更多应用。

% 如图 7（a) 所示，当我们将IPAdapter运用于 LayerTracer， 我们获得了一个可以将任意照片风格化为Human-like SVG图形的工作流。
% 如图 7（b) 所示， LayerTracer 可以通过和IPAdapter 或LoRA 的组合使用获得风格化SVG效果。
% 如图 7（c) 所示， LayerTracer 可以与先进的支持颜色渐变的矢量化方法组合使用，获得高质量的带渐变的分层效果。




\section{Conclusion}

In this work, we introduced LayerTracer, a novel framework that bridges the gap between automated SVG generation and professional design standards. Leveraging the strengths of Diffusion Transformers, LayerTracer achieves  cognitive-aligned, layer-wise SVG generation and vectorization. By learning the workflows and design logic of human designers, LayerTracer effectively generates clean, editable, and semantically meaningful vector graphics from textual descriptions or raster images. To overcome the scarcity of layered SVG creation data, we established a pipeline that collects over 20,000 SVG creation sequences. We proposed Serpentin dataset construction method,  enabling effective model training. Extensive experiments demonstrate that LayerTracer not only excels in SVG generation quality but also offers unparalleled flexibility and interpretability, setting a new benchmark for scalable vector graphics creation.


% 本文提出了**LayerTracer**，一种弥合自动化SVG生成与专业设计标准之间差距的新框架。通过利用扩散变换器（DiTs）的强大能力，LayerTracer 实现了类人化的分层SVG生成和层次化矢量化。通过学习人类设计师的工作流程和设计逻辑，LayerTracer 能够从文本描述或光栅图像生成干净、可编辑且语义明确的矢量图形。为了解决分层SVG创建数据稀缺的问题，我们建立了一条管线，收集了超过20,000条SVG创建序列，支持模型的高效训练。大量实验表明，LayerTracer不仅在SVG生成质量上表现卓越，还提供了前所未有的灵活性和可解释性，树立了可扩展矢量图形生成的新基准。

\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{acmart}


\end{document}

