\section{Related Works}
\subsection{Text2image Diffusion Model}

Recent studies have demonstrated that diffusion models are capable of generating high-quality synthetic images, effectively balancing diversity and fidelity. Models based on diffusion models or their variants, such as those paper in  \cite{sd, dit}, have successfully addressed the challenges associated with text-conditioned image synthesis. Stable Diffusion  \cite{sd}, a model based on the Latent Diffusion Model, incorporates text conditioning within a UNet framework to facilitate text-based image generation \cite{diffsim, idp, anti}, establishing itself as a mainstream model in image generation. Fine-tuning pre-trained image generation models can enhance their adaptation to specific application scenarios, as seen in techniques like LoRA  \cite{lora} and DreamBooth  \cite{dreamfusion}. For theme control in text-to-image generation, several works  \cite{ipa, instantid, ssr, fast_icassp, makeup, hair} focus on custom generation for defined pictorial concepts, with ControlNet  \cite{controlnet} additionally offering control over other modalities such as depth information.  AnimateDiff \cite{animatediff} introduces a temporal attention module, extending Stable Diffusion into a video generation model. Inspired by ProcessPainter \cite{processpainter}, which first proposed learning an artist's painting process through pre-trained temporal models, this paper leverages the in-context capabilities of DiT to generate a layered SVG creation process.

% 最近的研究表明，扩散模型能够生成高质量的合成图像，有效平衡多样性和保真度。基于扩散模型或其变体的模型，如文献中提到的模型，已成功解决了文本条件图像合成的挑战。基于潜在扩散模型的Stable Diffusion在UNet框架内整合了文本条件，促进了基于文本的图像生成，已成为图像生成领域的主流模型。如LoRA和DreamBooth等技术所示，微调预训练的图像生成模型可以增强其对特定应用场景的适应性。在文本到图像生成的主题控制中，一些研究聚焦于为特定的画面概念进行定制生成，ControlNet还额外提供了对深度信息等其他模态的控制。Animatediff 提出时序注意力模块，将Stable Diffusion扩展成视频生成模型。processpainter 首次提出通过预训练时序模型学习艺术家的绘画过程，受此启发， 本文使用DiT 的in-context 能力生成分层SVG创作过程。

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{image/method} % Replace with your image file
    \caption{The LayerTracer architecture comprises three key components: (1) \textbf{Layer-wise Model}: Pretrained on our proposed dataset to generate layered pixel sequences from text prompt; (2) \textbf{Image2Layers Model}: Merges LoRA with the Flux base DiT, enabling image-conditioned generation through VAE-encoded latent tokens; (3) \textbf{Layer-wise Vectorization}: Converts raster sequences to SVGs via differential analysis between adjacent layers, followed by Bézier optimization using vtracer to eliminate redundant paths while preserving structural fidelity.}
    \label{method}
\end{figure*}


\subsection{SVG Generation }
Scalable Vector Graphics (SVGs) are widely utilized in design owing to their advantages like geometric manipulability, resolution independence, and compact file structure. SVG generation often involves training neural networks to produce predefined SVG commands and attributes using architectures such as RNNs \cite{im2vec}, VAEs \cite{svg2, svg+vae, strokenuwa}, and Transformers \cite{svg2, deepvecfont, iconshop}. Nonetheless, the absence of large-scale vector datasets constrains their generalization capabilities and the creation of complex graphics, with most datasets focusing on specific areas like monochromatic vector icons \cite{iconshop} and fonts \cite{deepvecfont, clipfont}.

An alternative to directly training an SVG generation network is optimizing it to match a target image during the evaluation phase, employing differentiable rasterizers to bridge vector graphics and raster images \cite{diffvg}. This method optimizes SVG parameters based on pretrained vision-language models. Advances in models like CLIP \cite{clip} have facilitated effective SVG generation methods such as CLIPDraw \cite{clipdraw}, CLIPasso \cite{clipasso}, and CLIPVG \cite{clipvg}. while DreamFusion \cite{dreamfusion} demonstrates the superior generative capabilities of diffusion models. VectorFusion \cite{vectorfusion}, DiffSketcher \cite{diffsketcher}, and SVGDreamer combine differentiable rasterizers with text-to-image diffusion models to produce vector graphics, achieving notable results in iconography and sketching. However, these methods still face challenges with editability and graphical quality. Recent studies \cite{nivel, t2v} have blended optimization-based methods with neural networks to enhance vector representations by integrating geometric constraints. 

The primary issue with methods that optimize a set of vector primitives through SDS loss is their reliance on image generation model priors, which often leads to redundant and noisy results. These outputs lack clear hierarchical structures and fail to meet design specifications. In this paper, we innovatively propose an alternative approach to utilizing image generation model priors. Specifically, we leverage the in-context learning capability of Diffusion Transformers to generate the creation process of SVG graphics, combined with vectorization to achieve cognitive-aligned layered SVG generation.

% 可缩放矢量图形（SVG）因其几何可操作性、分辨率独立性和紧凑的文件结构等优点而广泛用于设计中。一种SVG生成方法涉及训练神经网络输出预定义的SVG命令和属性。这些网络通常使用如RNN、VAE和Transformer等架构。然而，缺乏大规模矢量数据集限制了它们的泛化能力和创建复杂图形的能力。大多数数据集集中在狭窄的领域，如单色矢量图标、表情符号和字体。

% 直接训练SVG生成网络的一种替代方法是在评估阶段优化以匹配目标图像, 基于可微栅格化器，以桥接矢量图形和栅格图像。最近的工作已经使用可微栅格化器来克服数据集限制。这种方法基于预训练的视觉语言模型优化SVG参数。如CLIP等模型的进步已经使得成功的草图生成和SVG编辑成为可能，例如CLIPDraw和CLIPasso，而DreamFusion则展示了扩散模型在生成能力上的优越性。VectorFusion、DiffSketcher和SVGDreamer结合了可微栅格化器和文本到图像的扩散模型来生成矢量图形，在图标和素描方面取得了令人瞩目的成果。然而，这些方法仍然存在可编辑性和图形质量有限的问题。最近的研究结合了基于优化的方法和神经网络来通过引入几何约束改善矢量表示。

% 通过SDS loss优化一组矢量图元，从而利用图像生成模型先验的方法最大的问题是结果冗余而嘈杂，没有清晰的分层结构，不符合设计规范。 本文创新性的提出另一种利用图像生成模型先验的方法， 利用DiT的in-context能力生成SVG图形的创建过程，结合矢量化实现cognitive-aligned layered SVG generation.



\subsection{Vectorization}
Raster image vectorization or image tracing is a well-studied problem in computer graphics\cite{svg1, svg2, svg3, svg4}. Diffvg\cite{diffvg} proposes a differentiable rendering method for vectorization, which found shape gradients by differentiating the formula of Reynolds transport theorem with Monta-Carlo edge sampling. Meanwhile, combining differentiable rendering techniques with deep learning models are also studied for image vectorization\cite{live, clipasso, cliptexture}. Direct raster-to-vector conversion with neural networks are supported for the relatively simple images\cite{svg+vae, svg2, im2vec}.  Stroke-based rendering can be used to fit a complex image with a sequence of vector strokes \cite{singh2022intelli, liu2021paint, hu2023stroke}, but the performance is limited by the predefined strokes. Diffvg\cite{diffvg} can also be leveraged to fit an input image with a set of randomly initialized vector graphical elements. Based on Diffvg, LIVE \cite{live} proposes a coarse-to-fine vectorization strategy, with cost tens of minute. CLIPVG\cite{clipvg} proposes a multi-round vectorization strategy,  providing additional graphic elements for the image manipulation task. LIVE \cite{live} and O\&R \cite{oar} achieve hierarchical vectorization through optimization-based methods, but their results show a significant gap compared to human-designed works, lacking logical coherence. In contrast to these approaches, our proposed LayerTracer leverages the prior knowledge of the Diffusion Transformer model, reformulating the hierarchical vectorization task as a problem of predicting preceding frames from a reference image.

% 光栅图像的矢量化或图像追踪是计算机图形学中一个经过深入研究的问题\cite{svg1,svg2,svg3,svg4,svg5}。Diffvg\cite{diffvg} 提出了一种可微分的渲染方法用于矢量化，该方法通过对Reynolds运输定理的公式进行微分，并使用蒙特卡洛边缘采样找到形状梯度，可以用来通过一组随机初始化的矢量图形元素拟合输入图像。 与此同时，结合可微分渲染技术与深度学习模型进行图像矢量化的研究也在进行中\cite{live}。对于相对简单的图像，可以直接使用神经网络进行光栅到矢量的转换\cite{svg+vae, deepsvg, im2vec}。基于笔触的渲染可以用来通过一系列矢量笔触拟合复杂图像\cite{learntopaint, stylizedneuralpainting, clipasso}，但性能受限于预定义的笔触。LIVE 和O&R 通过优化的方法实现了层次矢量化，但是结果和人类设计师的作品有较大差距，不具有逻辑性。不同于上述方法，我们提出的LayerTracer利用DiT模型的先验证， 把层次矢量化任务重新定义为从参考图预测之前的帧的任务。