\begin{table*}[htbp]
\centering
\caption{Performance of models under few-shot learning. }
\label{tab:node_classification_results_few_shot_LLM}
\scalebox{0.73}{%
\begin{tabular}{l c c c c c c | c c c c c | c c c c c}
\toprule
\rowcolor{gray!10}
 & & \multicolumn{5}{c}{\textbf{Full fine-tune}} & \multicolumn{5}{c}{\textbf{5-shot}} & \multicolumn{5}{c}{\textbf{10-shot}} \\
\cline{3-7} \cline{8-12} \cline{13-17}
\rowcolor{gray!10}
\textbf{Models} & \textbf{Prompts} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} \\ 
\midrule

GCN &- & 88.19 & 88.00 & 69.90 & 82.30 \cellcolor{cyan!20} & 82.10 & 62.13 & 68.19 & 24.62 & 47.77 & 50.68 & 71.75 & 71.81 & 25.63 & 54.60 & 55.95 \\
GraphSAGE &- & 89.67 \cellcolor{cyan!100} & 89.02 & 71.35  \cellcolor{cyan!20} & 82.89 \cellcolor{cyan!50} & 83.23 \cellcolor{cyan!20} & 58.91 & 65.58 & 19.12 & 45.94 & 47.39 & 70.29 & 70.90 & 22.91 & 51.29 & 53.85 \\
GAT &- & 88.38 \cellcolor{cyan!20} & 87.90 & 68.69 & 82.10 & 81.77 & 54.95 & 63.95 & 19.08 & 32.65 & 42.66 & 69.26 & 70.60 & 25.34 & 43.59 & 52.20 \\
GraphCL &- & 83.58 & 82.86 & 67.87 & 80.20 & 78.63 & 54.03 & 54.86 & 11.24 & 34.10 & 38.56 & 57.96 & 55.23 & 16.84 & 46.08 & 44.03 \\
GraphMAE &- & 75.98 & 82.82 & 65.54 & 77.32 & 75.42 & 24.44 & 70.47 & 24.26 & 50.61 & 42.45 & 30.59 & 73.63 & 28.64 & 57.55 & 47.60 \\
All in one &- &- &- &- &- &- & 50.98 & 60.49 & 16.34 & 41.18 & 42.25 & 51.66 & 61.93 & 20.42 & 47.73 & 45.44 \\
GPF-plus &- &- &- &- &- &- & 67.00 & 66.91 & 60.07 & 64.50 & 64.62 & 73.22 & 64.39 & 65.35 & 68.02 & 67.75 \\
GraphPrompt &- &- &- &- &- &- & 65.12 & 68.11 & 81.88 \cellcolor{cyan!100} & 58.44 & 68.39 \cellcolor{cyan!20} & 69.81 & 70.38 & 87.05 \cellcolor{cyan!100} & 61.02 & 72.07 \\
\midrule
\multirow{3}{*}{Llama3B} & ego & 67.08 & 89.28 & 66.58 & 65.59 & 72.13 & 59.10 & 67.08 & 49.65 & 59.12 & 58.74 & 63.09 & 80.30 & 52.10 & 60.73 & 64.06 \\
& 1-hop w/o label & 82.04 & 90.02 & 71.32 & 73.07 & 79.11 & 74.81 \cellcolor{cyan!20} & 65.59 & 53.53 & 65.35 & 64.82 & 74.06 & 83.54 & 62.29 & 67.03 & 71.73 \\
& 2-hop w/o label & 85.04 & 91.52 & 72.82 \cellcolor{cyan!50} & 77.89 & 81.82 & 76.81 \cellcolor{cyan!50} & 71.32 & 55.24 & 67.32 \cellcolor{cyan!20} & 67.67 & 77.81 \cellcolor{cyan!20} & 85.53 \cellcolor{cyan!50} & 63.33 & 68.11 \cellcolor{cyan!20} & 73.70 \cellcolor{cyan!20} \\
\midrule
\multirow{3}{*}{Llama8B} & ego & 77.31 & 92.36 \cellcolor{cyan!20} & 65.59 & 73.74 & 78.38 & 65.84 & 76.81 \cellcolor{cyan!50} & 63.97 & 65.12 & 67.94 & 67.58 & 78.12 & 66.31 & 66.10 & 69.53 \\
& 1-hop w/o label & 84.54 & 93.90 \cellcolor{cyan!50} & 69.33 & 80.33 & 83.28 \cellcolor{cyan!50} & 74.56 & 76.81 \cellcolor{cyan!20} & 65.98 \cellcolor{cyan!20} & 70.50 \cellcolor{cyan!50} & 71.87 \cellcolor{cyan!50} & 79.55 \cellcolor{cyan!50} & 85.10 \cellcolor{cyan!20} & 68.24 \cellcolor{cyan!20} & 72.33 \cellcolor{cyan!50} & 76.31 \cellcolor{cyan!50} \\
& 2-hop w/o label & 89.67 \cellcolor{cyan!100} & 95.22 \cellcolor{cyan!100} & 76.01 \cellcolor{cyan!100} & 84.51 \cellcolor{cyan!100} & 86.35 \cellcolor{cyan!100} & 77.10 \cellcolor{cyan!100} & 79.43 \cellcolor{cyan!100} & 69.78 \cellcolor{cyan!50} & 73.12 \cellcolor{cyan!100} & 74.86 \cellcolor{cyan!100} & 80.55 \cellcolor{cyan!100} & 88.89 \cellcolor{cyan!100} & 71.12 \cellcolor{cyan!50} & 74.86 \cellcolor{cyan!100} & 78.86 \cellcolor{cyan!100} \\
\bottomrule
\end{tabular}
}
\end{table*}


% \section{Further Investigation }
\section{Further Investigation on LLMs with Instruction Tuning}
\label{sec:Research Questions}
%\ym{we can directly start with the instruction fine-tuning}
%Based on the results and analysis, it is clear that LLMs perform comparably to most graph machine learning methods when the model size is large and graph structural information is abundant. 
Instruction tuning enables even smaller LLMs to perform effectively. However, data scarcity remains a significant challenge in real-world applications \cite{xia2024opengraph}, where many graph models, such as GNNs and graph transformers, suffer a substantial performance drop due to their heavy reliance on rich graph structures and labeled data \cite{yu2024survey}. Recently, more advanced graph models like All in One \cite{sun2023allinone} and GPF-plus \cite{gpf-plus} have focused on improving performance with limited labeled data. Despite this, the performance of LLMs with instruction tuning under data scarcity has been relatively unexplored. Therefore, in this section, we discuss methods to alleviate data scarcity and further explore the performance of LLMs with instruction tuning in such scenarios.

%Based on the results and analysis, it is evident that LLMs perform comparably to most graph machine learning methods when model size is large and graph structural information is rich. Instruction tuning further enhances the performance of LLMs, enabling even smaller models to perform effectively. However, compared to most graph machine learning methods and LLMs without parameter optimization, instruction-tuned LLMs require more tuning time and computational resources. This raises an important question: \textit{\textbf{Are there unique advantages of instruction-tuned LLMs that cannot be easily replaced by other methods, or that justify the higher resource cost?}} \ym{This could be updated. How expensive is it compared with traditional GNN? We might need to be careful when mentioning its shortcomings.  }


Label scarcity is one of the most common forms of data scarcity. Improving model performance under few-shot learning conditions is a key focus for recent graph models \cite{yu2024survey, zhao2024pre}. For LLMs, the performance after few-shot instruction tuning offers insight into their sensitivity to label scarcity. The ability to generalize from a limited amount of labeled data is essential for LLMs’ adaptability across different tasks and domains, making it a crucial factor in practical applicability. This leads us to the following research question:

%The effectiveness of current graph representation learning techniques, such as GNNs and graph transformers, relies not only on rich graph structures but also on large amounts of labeled data \cite{yu2024survey}. However, data scarcity is a common challenge in real-world applications, which often limits their performance \cite{xia2024opengraph}. The ability to generalize from a small amount of labeled data is crucial for a model’s applicability across different tasks and domains. For LLMs, few-shot instruction tuning presents a clear advantage over full fine-tuning by significantly reducing resource requirements. This leads us to the following research question:


%In many real-world applications, data scarcity is a significant challenge, and a model’s performance in such scenarios directly impacts its practical applicability. \ym{Need some citations; also discuss about the graph case} The ability to generalize from a small amount of labeled data determines how widely a model can be used across different tasks and domains. For LLMs, few-shot instruction tuning offers a compelling advantage over full fine-tuning, as it greatly reduces resource consumption. This motivates us to investigate following research question:

\vspace{0.5\baselineskip}
\begin{mdframed}[backgroundcolor=gray!8]
\textbf{\textit{RQ1: How well do LLMs perform in few-shot instruction tuning scenarios?}}
\end{mdframed}
\vspace{0.5\baselineskip}


%When labeled data is scarce, the intuitive approach is to explore whether unlabeled data can be leveraged to enhance the model’s performance. \ym{need some citations to justify. Also, task about the graph case, make it specific to graph. We may mention that the idea of contious pre-training is widely seen in adapting a LLM to a general domain (cite). So, we xxx} This idea leads us to propose continuous pre-training, which utilizes unsupervised learning to improve a model’s understanding of graph structures before fine-tuning it for specific tasks. If effective, this approach could increase the adaptability of LLMs with instruction tuning since unlabeled data is far more abundant than labeled data. Given the potential of this method, we are interested in understanding how continuous pre-training influences the performance of LLMs in graph tasks. This leads us to explore the following research question:


% When labeled data is scarce, a natural strategy is to explore the potential of using unlabeled data to enhance model performance. This concept is often employed in the field of continual learning, where models are trained incrementally to adapt to new information without requiring large amounts of labeled data \cite{wang2024comprehensive, van2019three}. Building on this idea, we propose continuous pre-training, which utilizes unsupervised learning to improve the model understanding of graph structures before fine-tuning it for specific tasks. If effective, this approach could increase the adaptability of LLMs with instruction tuning since unlabeled data is far more abundant than labeled data. Given the potential of this method, we are interested in the following research question:

When labeled data is scarce, leveraging unlabeled data is a natural strategy to enhance model performance. This principle is widely applied in continual learning, where models are incrementally trained to adapt to new information without requiring extensive labeled supervision \cite{wang2024comprehensive, van2019three}. A well-established approach for adapting large language models (LLMs) to specific domains is continual domain-adaptive pre-training~\cite{ke2023continual,yildiz2024investigating}, where models are further trained on domain-specific corpora to improve their performance on downstream tasks. Inspired by this strategy, we propose continuous pre-training for graph tasks, where an LLM undergoes unsupervised pre-training on graph-structured data before fine-tuning on task-specific objectives. Since unlabeled graph data is far more abundant than labeled data, this method could significantly enhance the adaptability of LLMs when paired with instruction tuning. Given the potential of this approach, we seek to investigate the following research question:

%\paragraph{\textbf{RQ2: How does continuous pre-training impact the performance of LLMs in zero-shot and few-shot scenarios?}}

\vspace{0.5\baselineskip}
\begin{mdframed}[backgroundcolor=gray!8]
\textbf{\textit{RQ2: How does continuous pre-training impact the performance of LLMs?}}
\end{mdframed}
\vspace{0.5\baselineskip}


Models with strong transferability can help alleviate the performance degradation caused by label scarcity to some extent. For instance, when the target dataset contains few or no labeled data, we can first train a model on other datasets and then transfer the learned knowledge to the target dataset. LLMs have shown impressive transferability in natural language tasks \cite{du2024unlocking, ran2024alopex}, but their transferability in graph tasks has been less explored. If LLMs with instruction tuning can effectively transfer knowledge across different domains, the resource-intensive tuning process could be performed once, and subsequent graph tasks could be handled by the already fine-tuned models. This leads to the following research question:


%LLMs have demonstrated strong transferability in natural language tasks \cite{du2024unlocking, ran2024alopex}, but their transferability in graph tasks has been less explored, with most studies focusing primarily on in-domain tasks. \ym{Can explain it a bit more on what we mean by transferability for graph scenario and why we think they are important.} Transferability is crucial because it enables models to apply knowledge learned from one domain to new domains without the need for extensive retraining. If LLMs with instruction tuning can effectively transfer knowledge across domains, the resource-intensive tuning process would only need to be done once. Subsequent tasks could then be performed on the already tuned models, greatly expanding the applicability of LLMs for graph-related tasks. This leads to the following research question:

%\paragraph{\textbf{RQ3: How well do LLMs transfer knowledge across domains in node classification and link prediction?}}

\vspace{0.5\baselineskip}
\begin{mdframed}[backgroundcolor=gray!8]
\textbf{\textit{RQ3: How well do LLMs transfer knowledge across domains in node classification and link prediction?}}
\end{mdframed}
\vspace{0.5\baselineskip}

Missing attributes represent another form of data scarcity \cite{chen2024data}. In such cases, the model ability to understand graph structure becomes crucial. While much research has focused on the role of node attributes in graph tasks \cite{chen2024text, yan2023comprehensive}, less attention has been given to the LLM ability to learn and reason with graph structures without relying on node attributes. The structure of a graph is a fundamental distinguishing feature compared to natural language, and a model’s ability to comprehend these structures is vital for enhancing its performance on graph tasks. Given this, we are curious about:

%The results from Table \ref{tab:node_classification_results_LLM} and Table \ref{tab:LLM performance on link prediction} have demonstrated the importance of graph structure information for LLMs. While much research has focused on the role of node attributes in graph tasks \cite{chen2024text, yan2023comprehensive}, less attention has been given to LLMs’ ability to learn and reason with graph structures without relying on node attributes. The structure of a graph is one of its key distinguishing factors compared to natural language, and understanding these structures is essential for improving LLM performance on graph tasks. Given this, we are curious about:

%\paragraph{\textbf{RQ4: How effectively do LLMs learn and understand graph structures?}}
\vspace{0.4\baselineskip}
\begin{mdframed}[backgroundcolor=gray!8]
\textbf{\textit{RQ4: How effectively do LLMs learn and understand graph structures?}}
\end{mdframed}


% Finally, LLMs’ stability in graph tasks is critical for their adaptability in real-world applications, where graphs may experience structural changes like reduced node similarity or missing edges. Understanding how LLMs handle these perturbations is key to evaluating their robustness and potential for deployment in dynamic graph-based environments where structural stability cannot always be assured. This leads to the final research question:

% %\paragraph{\textbf{RQ5: How do LLMs perform under structural perturbations, such as reduced node similarity and missing edge information, compared to traditional graph models?}}

% \vspace{\baselineskip}
% \begin{mdframed}[backgroundcolor=gray!8]
% \textbf{\textit{RQ5: How do LLMs perform under structural perturbations compared to traditional graph models?}}
% \end{mdframed}
% % \vspace{\baselineskip}
