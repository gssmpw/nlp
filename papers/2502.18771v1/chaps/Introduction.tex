
\section{Introduction}
\label{sec:Introduction}
%The increasing prevalence of relational data in the form of graphs has highlighted the need for advanced graph learning models \cite{feyposition, jin2020graph, xia2024anygraph}. As these models are increasingly applied in diverse contexts, their performance and adaptability have become critical. 
%\ym{The first paragraph and the second one seems not well connected? What is the logic here?}

The increasing prevalence of relational data in the form of graphs has highlighted the need for advanced graph learning models \cite{feyposition, jin2020graph, xia2024anygraph}. As these models are increasingly applied in diverse domains, the ability to effectively process and analyze graph data has become a crucial challenge. This has led to the development of various approaches that aim to enhance the performance and adaptability of graph learning models across different tasks and contexts.

%As graph-structured data continues to be applied across various domains, the ability and performance of these models in different contexts have become crucial. 

% Among the current approaches, \textbf{Graph Neural Networks (GNNs)} \cite{gcn, graphsage, gat, gin}, which use message passing and aggregation techniques, are powerful tools for handling complex graph structures and node features. \textbf{Graph self-supervised learning (SSL)} methods like GraphCL \cite{you2020graphcl}, DGI \cite{dgi}, and GraphMAE \cite{hou2022graphmae} utilize self-supervised learning during pre-training to learn effective representations without requiring large amounts of labeled data. These methods improve the models’ ability to generalize and increase robustness across diverse graph structures. Inspired by transformers \cite{transformer}, \textbf{Graph Transformers (GT)} such as Graphormer \cite{graphormer} and GraphBERT \cite{graphbert} leverage self-attention mechanisms to capture long-range dependencies and global structural information, addressing the limitations of localized message passing in traditional GNNs. Additionally, \textbf{Foundational graph prompt models}, also known as \textbf{graph foundation models} (e.g., OFA \cite{ofa}, Prodigy \cite{huang2024prodigy}, All in One \cite{sun2023allinone}, ULTRA \cite{galkin2023towards}), have become a key focus in graph research. These models excel at few-shot/zero-shot learning and generalization, allowing them to adapt quickly to new tasks with minimal task-specific data. Their ability to handle a wide range of graph related tasks within a unified framework also makes them scalable and efficient, providing significant advantages in real-world applications.

%Current approaches for graph-related tasks include Graph Neural Networks (GNNs) \cite{gcn, graphsage, gat, gin}, which leverage message passing and aggregation techniques to handle complex graph structures. Graph self-supervised learning (SSL) methods like GraphCL \cite{you2020graphcl}, DGI \cite{dgi}, and GraphMAE \cite{hou2022graphmae} enable effective representation learning without large labeled datasets, improving generalization and robustness. Graph Transformers (GT) such as Graphormer \cite{graphormer} and GraphBERT \cite{graphbert} utilize self-attention to capture long-range dependencies, overcoming the limitations of localized message passing. Finally, foundational graph prompt models (e.g., OFA \cite{ofa}, Prodigy \cite{huang2024prodigy}, All in One \cite{sun2023allinone}, ULTRA \cite{galkin2023towards}) excel at few-shot/zero-shot learning, offering scalability and efficiency by adapting quickly to new tasks with minimal data.

Classic Graph Neural Networks (GNNs) \cite{gcn, graphsage, gat, gin} use message passing and aggregation to handle graph structures but heavily rely on labeled data. To reduce this dependency, Graph Self-Supervised Learning \cite{you2020graphcl, dgi, hou2022graphmae} adopt a “pre-training fine-tuning” paradigm, leveraging unsupervised learning to enhance graph understanding. Meanwhile, Graph Transformers (GT) \cite{graphormer, graphbert} use self-attention to capture long-range dependencies between nodes, addressing the limitations of localized message passing. Recently, foundational graph prompt models \cite{ofa, huang2024prodigy, sun2023allinone} have gained attention. They use graph prompts to bridge the gap between pre-trained models and downstream tasks, ultimately improving adaptability.



%The recent emergence of \textbf{large language models (LLMs)} with extensive context-awareness and semantic understanding abilities, such as LLaMa \cite{touvron2023llama}, GPTs \cite{achiam2023gpt4}, Claude \cite{claude}, and Deepseek \cite{liu2024deepseek}, represents a significant leap forward in AI research. One of the main strengths of LLMs lies in their ability to perform a wide range of tasks using a single model, demonstrating not only advanced language skills but also the ability to provide reasoning behind their responses. To endow LLMs with the potential to solve graph tasks, some studies \cite{dai2024revisiting, zhang2024llm4dyg, kim2023kg, jiang2023structgpt, nlgraph, fatemi2023talk} have focused on designing effective interfaces like prompt engineering that allow these models to comprehend graph semantics without requiring parameter tuning. Additionally, some approaches \cite{chen2024llaga, chai2023graphllm, tang2024graphgpt, perozzi2024let} involve integrating graph embeddings into the LLMs’ partial parameters using graph learning techniques like GNNs and GTs, bridging the gap between language processing and graph-based reasoning. 
The recent emergence of large language models (LLMs) like LLaMa \cite{touvron2023llama}, GPTs \cite{achiam2023gpt4}, Claude \cite{claude}, and Deepseek \cite{liu2024deepseek} has marked a significant advancement in AI research. LLMs excel at performing various tasks with a single model, showing not only advanced language skills but also the ability to reason behind their responses. To endow LLMs with the potential to solve graph tasks, some studies \cite{dai2024revisiting, zhang2024llm4dyg, kim2023kg, jiang2023structgpt, nlgraph, fatemi2023talk} have explored interfaces like prompt engineering to help LLMs comprehend graph semantics without parameter tuning. Additionally, some methods \cite{chen2024llaga, chai2023graphllm, tang2024graphgpt, perozzi2024let} integrate graph embeddings into LLMs using graph learning techniques like GNNs and GTs, bridging language and graph-based reasoning.
%Although the aforementioned studies have demonstrated that LLMs possess some graph reasoning capabilities, a significant semantic gap still exists between graphs and text, which can hinder the model’s ability to effectively reason and generate graph related outputs. The limitation underscores the need for more refined methods to bridge this gap and enhance the reliability of LLMs in graph tasks. To address this problem, \textbf{instruction tuning} \cite{ouyang2022training, sanh2021multitask} has been introduced to LLMs, enabling them to better understand graph features and structures through parameter efficient fine-tuning methods like LoRA \cite{hu2021lora}. Various types of instructions are applied to optimize the model’s parameters. Some approaches \cite{instructglm, tang2024graphgpt, zhang2023graph} use graph descriptions or graph embeddings, which help make the graph structure more clear and intuitive for the LLM without the need for complex graph-specific processing pipelines. Others \cite{wang2024instructgraph} propose unifying graph data into a code-like universal format, leveraging LLMs’ proven strength in code understanding and generation \cite{gao2023makes, ma2023training, yang2024if}. Regardless of the instruction format used, instruction-tuned LLMs have demonstrated strong performance on graph tasks, showcasing the potential of this approach to enhance both graph comprehension and generation.
To further mitigate the semantic gap between graphs and text, instruction tuning \cite{instructglm, tang2024graphgpt, zhang2023graph} has been introduced to LLMs, enabling them to better understand graph features and structures. 


%Various types of instructions are applied to optimize parameters of the model. Some \cite{instructglm, tang2024graphgpt, zhang2023graph} use graph descriptions or graph embeddings, which help make the graph structure more clear and intuitive for the LLM without the need for complex graph-specific processing pipelines. Others \cite{wang2024instructgraph} propose unifying graph data into a code-like universal format, leveraging proven strength of LLMs in code understanding and generation \cite{gao2023makes, ma2023training, yang2024if}. 
%Regardless of the instruction format used, instruction-tuned LLMs have demonstrated strong performance on graph tasks, showcasing the potential of this approach to enhance both graph comprehension and generation.

% However, no study has yet provided a systematic and comprehensive comparison of the performance of these methods on graph tasks.

% A comprehensive understanding of LLM applicability and effectiveness across different scenarios is still lacking.

Existing studies on LLMs for graph tasks often adopt inconsistent settings, including different datasets, data processing methods, and splitting strategies \cite{li2024glbench}. These variations make it difficult to systematically compare results, hindering a clear understanding of how LLMs perform relative to other graph models. Moreover, many prior works \cite{instructglm, tang2024graphgpt} have compared LLM-based approaches only to a limited set of baseline models, such as classic GNNs and graph transformers,  without considering recent advancements in graph foundation models \cite{ofa, huang2024prodigy} and other modern graph learning techniques. This omission limits the ability to fully assess the strengths and weaknesses of LLMs in graph tasks. To address these gaps, we evaluate LLMs alongside 16 diverse graph learning models, including GNNs, self-supervised graph learning models, graph transformers, LM-augmented graph learning methods, and foundational graph prompt models. Additionally, to ensure fair and consistent evaluation, we standardize data processing and splitting strategies across four widely used graph datasets for node classification and link prediction tasks. Our benchmark also includes a broad range of LLMs, covering open-source models like Llama3B and Llama8B, as well as proprietary models such as Qwen-plus, Qwen-max, GPT-4o, and Deepseek V3.

% Previous studies often use different datasets, data processing methods, and splitting strategies, making it difficult to compare results and hindering a systematic understanding of various graph models. To address this, we conduct node classification and link prediction tasks on four widely used graph datasets and ensure consistency in data processing and splitting strategies. In addition, many LLM-based graph models have been compared only to a limited set of baseline models, such as classic GNNs, which fail to capture the full range of the strengths and weaknesses of LLMs in graph tasks. For example, foundational graph prompt models, a recent focus in graph research, are often neglected in these comparisons. Besides, most studies focus solely on the reasoning capabilities of LLMs, ignoring the impact of instruction tuning on their performance in graph tasks. This narrow approach limits the development of a comprehensive understanding of LLM capabilities in the graph domain. To address this, we evaluate the performance of LLMs (both without parameter optimization and with instruction tuning) alongside 16 different graph learning models, including GNNs, Graph SSL models, Graph Transformers, LM-Augmented Graph Learning Models, and Foundational Graph Prompt Models. Our comparison also includes a comprehensive range of LLMs, covering both open-source models like Llama3B and Llama8B, as well as closed-source models such as Qwen-plus, Qwen-max, GPT-4o, and Deepseek V3.

Our benchmarking results (see details in Section~\ref{sec:benchmarking_results}) show that pure LLMs, especially larger LLMs, perform on par with or even surpass most baseline models in node classification and link prediction tasks. Instruction tuning further boosts LLM performance, enabling even smaller models to match or exceed the performance of top baseline models. Given the promising potential of instruction tuning, we further explore how LLMs with instruction tuning perform in other critical scenarios. 

While instruction tuning significantly improves LLM performance, it typically requires abundant labeled data, which may not always be available in real-world scenarios \cite{xia2024opengraph}. To better understand its effectiveness under data scarcity, we further investigate how instruction-tuned LLMs perform when labeled data is limited. Specifically, we examine their performance in a few-shot setting, assessing whether they can maintain strong predictive capability with minimal supervision. Additionally, we explore the transferability of instruction-tuned LLMs, as models that generalize well with limited data across different tasks and domains are more practical in low-resource settings.  Finally, we analyze how instruction-tuned LLMs perform when node features are missing, a common challenge in real-world graph applications where feature availability is inconsistent. In such cases, models must rely solely on graph structure, making it crucial to evaluate whether instruction-tuned LLMs can effectively capture and utilize structural information.

% Additionally, we explore the transferability of instruction-tuned LLMs, as models that generalize well across different domains can be more practical and cost-effective. Finally, we analyze how well instruction-tuned LLMs capture and utilize graph structures, as leveraging structural information is crucial for enhancing their ability to process graph-based tasks.

% While some studies have explored how instruction tuning can improve LLM performance on graph tasks, they mostly focus on designing instruction formats without providing a comprehensive evaluation of LLMs with instruction tuning across different scenarios. In this paper, we examine the performance of LLMs with instruction tuning in several key areas. First, we assess model performance in label-limited scenarios. If LLMs with instruction tuning can maintain strong performance after few-shot or zero-shot learning, it suggests they are well suited for real world applications. Second, we evaluate the transferability of instruction-tuned LLMs, as models with better transferability can adapt to a broader range of tasks and domains with minimal cost. Lastly, we investigate how well LLMs with instruction tuning understand graph structures. This is because leveraging the rich structural information in graphs is crucial for improving model performance.

% In summary, we make the following three contributions:
% \begin{enumerate}

% \item 

% \item 

% \item 

% \end{enumerate}


\paragraph{\textbf{Existing Benchmarks for LLMs in Graph Tasks}}
There are some benchmarking works that explore the performance of LLMs on graph tasks. Studies like \cite{chen2024text} and \cite{yan2023comprehensive} focus on how LLMs can enhance graph models (e.g., GNNs) rather than benchmarking pure LLMs on graph tasks. GraphICL \cite{sun2025graphicl} aims to improve LLM performance in node classification and link prediction through various graph prompts, with an emphasis on prompt engineering, but it does not explore the impact of instruction tuning on LLMs in graph tasks. GLBench \cite{li2024glbench} also centers on how LLMs can better assist graph models, without focusing on purely LLM-based performance in graph tasks. Although \cite{wu2025comprehensive} includes LLMs with instruction tuning, it mainly focuses on their zero-shot capabilities and their integration with graph models, without investigating the broader effects of instruction tuning or exploring link prediction tasks. \emph{To the best of our knowledge, our work is among the first to comprehensively benchmark pure LLMs on graph tasks while incorporating instruction tuning.  Moreover, we go beyond prior studies by systematically evaluating instruction tuning under practical data scarcity scenarios, providing a more thorough understanding of its impact on LLM performance in graph-based tasks.}


% To the best of our knowledge, our work is the first to comprehensively evaluate the performance of pure LLMs on graph tasks across multiple dimensions and compare them with various graph learning models.

%Despite the significant potential demonstrated by LLMs in graph tasks, a comprehensive understanding of their applicability and effectiveness across various scenarios is still lacking.  \ym{The following should motivate the necessity of a fair benchmarking.} \ym{Carefully think about our narrative, we are now arguing that we need a fair benchmarking, make sure this aligns with our main contents.}
%\begin{itemize}
    %\item[(a)] Much of the current research focuses on exploring LLMs' graph reasoning abilities, examining their performance across a wide range of graph tasks (e.g., node degree, shortest path, cycle check, etc.), emphasizing breadth rather than depth.
    %\item[(b)] 
    %Many LLM-based graph models are limited to specific problem settings, making it difficult to compare and evaluate the effectiveness of different approaches. This narrow focus hinders the development of a cohesive landscape for the field.
    % Many LLM-based graph models have only been compared to a limited set of baseline models, such as traditional GNNs, which fails to fully highlight the strengths and weaknesses of LLMs for graph tasks. For instance, foundational graph prompt models, a recent focus in graph research, are often overlooked in comparisons with LLM-based models. This limited scope impedes the development of a comprehensive understanding of LLMs’ capabilities in the graph domain.
    % \item[(c)] 
    %Existing studies tend to evaluate only a subset of the model's capabilities, often neglecting other important aspects such as transferability, few-shot, and zero-shot abilities, which are critical for understanding the full potential of LLMs in graph-related tasks.
    % Most existing studies focus on improving the prediction accuracy of LLM-based graph models across various datasets. However, there are other crucial aspects that need comparison and evaluation. First, the model performance in data-scarce scenarios is a key indicator of its capability. If models can maintain strong performance after few-shot or zero-shot learning, it suggests they are well-suited for real-world applications. Second, the transferability of the model should be assessed, as models with better transferability can adapt to a wider range of tasks and domains at a lower cost. Third, exploring how well LLMs understand graph structures is essential. Unlike natural language, graphs contain rich structural information, and enhancing LLMs’ ability to leverage this information is key to improving their performance. Lastly, the robustness of LLMs in graph tasks should not be overlooked.
% \end{itemize}

%In recent works, for example, \cite{chen2024text} and \cite{yan2023comprehensive} primarily focus on benchmarking graph models on text-attributed graphs, with LLMs being compared in a relatively simple manner. GraphICL \cite{sun2025graphicl} primarily analyzes different graph prompts to improve LLM performance in node classification and link prediction, with an emphasis on prompt engineering. On the other hand, GLBench \cite{li2024glbench} focuses more on GraphLLMs rather than purely LLM-based performance in graph tasks. In addition, \cite{wu2025comprehensive} places greater emphasis on the zero-shot capabilities of LLMs and their performance when combined with graph models in various ways, but does not delve deeply into aspects such as LLM’s understanding of graph structure, transferability, or robustness. In this work, to demystify LLMs’ performance on graph tasks and inspire future research directions, we introduce a benchmark designed to illuminate the capabilities of LLMs and derive novel insights. 
%\ym{Need to be careful for the following argument.} \ym{For the following, we can discuss that though their are some benchmarking, what they are doing, motivate and empahsize why we still need our work.}
%To the best of our knowledge, we are the first to explore the performance of pure LLMs on graph tasks from multiple dimensions and provide a comprehensive comparison with various graph learning models. \ym{I would discuss the existing benchmarking works then discuss what we are doing and how we are different from them.} Concurrently, \cite{chen2024text} and \cite{yan2023comprehensive} mainly benchmark graph models on text-attributed graphs, using LLMs as tools to enhance graph model performance. GraphICL \cite{sun2025graphicl} focuses on improving LLM performance in node classification and link prediction through different graph prompts, emphasizing prompt engineering. In contrast, GLBench \cite{li2024glbench} centers more on GraphLLMs rather than purely LLM-based performance in graph tasks. Additionally, \cite{wu2025comprehensive} emphasizes the zero-shot capabilities of LLMs and their integration with graph models but does not thoroughly explore LLMs’ understanding of graph structures, transferability, or robustness. Our contributions are as follows:

% \ym{The following narrative needs to be updated accordingly.}
% \begin{enumerate}
    %\item We focus on node classification and link prediction, which are two fundamental tasks in graph learning. We explore and evaluate the performance of pure LLMs, including LLMs without parameter optimization and LLMs with instruction tuning, on these tasks. Using consistent datasets and splits, \textbf{we provide a comprehensive comparison with 16 different graph learning models}, including GNNs, Graph SSL models, Graph Transformers, LM-Augmented Graph Learning Models, Foundational Graph Prompt Models and more.
    %\item We use a diverse set of LLMs, including open-source models like \textbf{Llama3B} and \textbf{Llama8B}, as well as closed-source models such as \textbf{Qwen-plus}, \textbf{Qwen-max}, \textbf{GPT-4o}, and \textbf{Deepseek V3}. Our evaluation not only assesses their graph reasoning abilities but also explores the impact of various factors on these abilities, including the role of different prompt formats.
    % \item We focus on node classification and link prediction, two key tasks in graph learning, and \textbf{evaluate the performance of pure LLMs (both without parameter optimization and with instruction tuning) alongside 16 different graph learning models}, including GNNs, Graph SSL models, Graph Transformers, LM-Augmented Graph Learning Models, and Foundational Graph Prompt Models. The evaluation involves \textbf{a diverse set of LLMs, such as Llama3B, Llama8B, Qwen-plus, Qwen-max, GPT-4o, and Deepseek V3}, using consistent datasets and splits, while also exploring the impact of various prompt formats on model performance.
    %\item We thoroughly explore and analyze the performance and capabilities of LLMs on graph tasks across various scenarios, including their \textbf{few-shot/zero-shot abilities under data scarcity, transferability in in-domain and cross-domain settings, understanding of graph structure, and robustness.} This provides valuable insights for the future application of LLMs in graph tasks.
    % \item We analyze LLMs’ performance across different scenarios, including \textbf{few-shot/zero-shot abilities under data scarcity, transferability in both in-domain and cross-domain settings, understanding of graph structure, and robustness}, offering insights into their potential for real-world graph tasks.
    % \item We introduce \textbf{continuous pre-training}, demonstrating its effectiveness in improving LLMs' performance on graph tasks, particularly in scenarios with limited data.
    %\item Our empirical results provide several novel insights, with the most crucial ones outlined as follows: \textit{LLMs perform better as model size increases and with more detailed node descriptions. Even though some GNNs training from scratch can achieve performance similar to tuned LLMs when trained on full data, LLMs clearly outperform all other models in few-shot scenarios. LLMs show strong domain transferability, especially in link prediction tasks.}
    % \item Our empirical results provide several novel insights, with the most crucial ones outlined as follows: \textit{LLMs perform better with larger model sizes and more detailed node descriptions. While some GNNs trained from scratch can match tuned LLMs on full data, LLMs outperform other models in few-shot settings and exhibit strong domain transferability.}
% \end{enumerate}

% The rest of the paper is organized as follows. Section \ref{sec:Fair Benchmark} provides a comprehensive benchmarking of LLMs and various graph learning methods on node classification and link prediction. Building on this, Section \ref{sec:Research Questions} extends the analysis with further investigation, introducing four key research questions for deeper exploration. Section \ref{sec:Empirical Studies} presents empirical studies and insights based on these questions. Finally, Section \ref{sec:conclusion} concludes the paper.

