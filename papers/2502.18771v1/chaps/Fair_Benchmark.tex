\section{Graph Learning with Pure LLMs}
In this section, we introduce how we could utilize pure LLMs for important real-world graph tasks including node classification and link prediction.

% \subsection{}

\subsection{Prompt Design}\label{sec:prompt_design}
% \subsubsection{Pipeline of Graph Encoding}

% \ym{This should be a part of "how we use LLMs for graphs", right?}

% \subsubsection{Experimental Pipeline for LLMs}
% \label{sec:Experiment Pipeline}
% We present the overall experimental pipeline for LLMs in Figure \ref{fig:The overall pipeline of our benchmark}, which consists of three main stages: graph encoding, off-the-shelf LLMs, and LLMs with instruction tuning. \ym{Off-the-shelf LLMs is not a "stage"? I feel these are not three "stages", rather, "Off-the-Shelf" and "Instruction Finetuning" are two ways to utilize the LLMs? }

% \ym{This part could be an independent component/subsection.}
% \paragraph{\textbf{Graph encoding}}
As shown in the graph encoding part of Figure \ref{fig:The overall pipeline of our benchmark}, we combine the original graph datasets with their corresponding raw text attributes to encode the graph into a format that LLMs can understand, i.e., prompts. The prompt formats required for node classification and link prediction differ based on the specific task.

\paragraph{\underline{Prompt formats for node classification}}
When designing the prompt formats, \cite{whenandwhy} considers three scenarios: using only the node own features, using 1-hop neighbor information, and using 2-hop neighbor information. When neighbor information is included, it also contains labels of the neighbor nodes, which significantly improves reasoning accuracy. However, label information provides too direct guidance for LLMs, potentially making the task too straightforward and limiting their ability to learn from the underlying graph structure itself. Therefore, in addition to the three prompt designs, we introduce two new formats by removing label information. Our five prompt formats are as follows:

%The target node is the focus of our classification task. Inspired by \cite{whenandwhy} and \cite{instructglm} \ym{How we are inspired by them? They already proposed all 5 prompt strategies? Are we different from them? If so, how?}, we use five distinct prompts, as outlined below:
\begin{enumerate}
    \item \textbf{ego}: Only the attribute of the target node is used.
    \item \textbf{1-hop w/o label}: The target node is described using both its node attributes and those of its 1-hop neighbors, without labels.
    \item \textbf{2-hop w/o label}: The description includes the attributes of the target node and those of its 2-hop neighbors, without labels.
    \item \textbf{1-hop w label}: Similar to \textbf{1-hop w/o label}, but the labels of 1-hop neighbors from the training set are included.
    \item \textbf{2-hop w label}: The labels of 2-hop neighbors from the training set are included.
\end{enumerate}
The detailed prompt structures can be found in Appendix \ref{sec:Prompt Formats for Node Classification}.

\paragraph{\underline{Prompt formats for link prediction}}
The goal of link prediction is to determine whether an edge exists between target node 1 and target node 2. We use two prompt formats: 1) \textbf{1-hop}: Both target nodes are described using their own node attributes and those of their 1-hop neighbors. 2) \textbf{2-hop}: Both target nodes are described using their own node attributes along with those of their 2-hop neighbors. It is important to note that, whether using 1-hop or 2-hop, the nodes at the ends of the link should not appear as neighbors of each other to avoid overly simplistic reasoning, ensuring that the LLM needs to perform more meaningful reasoning. The detailed prompt structures can be found in Appendix \ref{sec:Prompt Formats for Link Prediction}.

\subsection{Paradigm of Using LLMs for Graph Tasks}
As shown in Figure \ref{fig:The overall pipeline of our benchmark}, there are two ways to use LLMs: one is off-the-shelf LLMs, which refers to LLMs without parameter optimization, and the other is LLMs with instruction tuning. For our investigation, we adopt  open-source models Llama-3.2-3B-Instruct (Llama3B) \cite{touvron2023llama}, Llama-3.1-8B-Instruct (Llama8B) \cite{touvron2023llama}, and the closed-source Qwen-plus \cite{bai2023qwen}.

\paragraph{\textbf{Off-the-shelf LLMs}}
% In this part, we outline the process of querying LLMs. No additional modifications are made to the LLMs themselves; instead, we simply input the encoded graph prompts along with the corresponding questions. By comparing the LLM responses with the correct answers, we can assess its performance. 

In this part, we directly utilize LLMs for graph tasks by designing specific prompts to encode graph-related information. Without modifying the LLMs themselves, we input these structured prompts along with corresponding questions and evaluate the model’s performance by comparing its responses to the correct answers. In addition to plain prompt described in Section~\ref{sec:prompt_design}, we also explore commonly used prompt strategies such as Chain of Thought (CoT) \cite{CoT}, Build A Graph (BAG) \cite{nlgraph}, and in-context few-shot learning on more LLMs (e.g., Qwen-max \cite{bai2023qwen}, GPT-4o \cite{achiam2023gpt4}, and Deepseek V3 \cite{liu2024deepseek}) for node classification task. The results indicate that these prompt strategies show significant performance variation across different datasets and LLM sizes. They do not necessarily provide benefits in every case. Detailed experimental results and analysis are provided in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}.


%Additionally, in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}, we provide extended experiments that benchmark more LLMs (e.g., Qwen-max \cite{bai2023qwen}, GPT-4o \cite{achiam2023gpt4}, and Deepseek V3 \cite{liu2024deepseek}) and evaluate the performance of different prompt techniques (e.g., Chain of Thought (CoT) \cite{CoT}, Build A Graph (BAG) \cite{nlgraph}, and in-context few-shot learning) on graph tasks.
% Additionally, we explore commonly used prompt strategies such as Chain of Thought (CoT) \cite{CoT}, Build A Graph (BAG) \cite{nlgraph}, and in-context few-shot learning on more LLMs (e.g., Qwen-max \cite{bai2023qwen}, GPT-4o \cite{achiam2023gpt4}, and Deepseek V3 \cite{liu2024deepseek}) for node classification task. The results indicate that these prompt strategies show significant performance variation across different datasets and LLM sizes. They do not necessarily provide benefits in every case. Detailed experimental results and analysis are provided in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}.


\paragraph{\textbf{LLMs with instruction tuning}}
%\ym{If space limits, we could move detailed description of how instruction tuning works "two models for link prediction" into appendix. } \ym{Also, it seems the 2 formats vs 9 formats is not extremely significant? It is somehow not very important to the main narrative? If so, we could move the 9 format (or 2 format) investigation to appendix. We can refer to the investigation in the main  content.  } \ym{When mentioning the different formats, motivate a bit on why we would like to investigate this. If we are not going to present 9 format, in the main text, we can summarize their results briefly in the main text. }
We conduct instruction tuning only on open-sourced models Llama3B and Llama8B. To accelerate the learning process, we adopt \cite{hu2021lora} and DeepSpeed \cite{rasley2020deepspeed}. We find that the number of training epochs has little impact on the final results. Therefore, to save time and computational resources, each model is tuned for only one epoch.
% In the right part of Figure \ref{fig:The overall pipeline of our benchmark}, we provide a brief overview of LLMs with instruction tuning. We perform instruction tuning on Llama3B and Llama8B across various datasets, using LoRA \cite{hu2021lora} and DeepSpeed \cite{rasley2020deepspeed} to accelerate model training, with each model trained for just one epoch. 
For node classification, we limit instruction tuning to three prompt formats: ego, 1-hop w/o label, and 2-hop w/o label.
%, as labels often carry strong semantic information that may lead the model to overly rely on them.

For link prediction, we aim to explore not only the impact of instruction tuning on LLMs’ reasoning performance but also the effect of prompt format diversity on LLM performance, a factor that has not been extensively examined in previous instruction tuning studies. This investigation could provide valuable insights for future prompt design. To achieve this, we set up two modes. The first mode uses the same prompt formats as in the testing phase (1-hop and 2-hop). The second mode introduces a more diverse set of prompt formats (9 formats), incorporating different question formulations and a wider range of neighbors. Detailed descriptions of the link prompt formats can be found in Appendix \ref{sec:Prompt Formats for Link Prediction}.

% \begin{figure*}[]
%   \centering
%   \includegraphics[width=1\linewidth]{figs/big_picture.pdf}
%   %\vspace{-20pt}
%   \caption{The overall experimental pipeline for LLMs. Graph encoding outlines how prompts for LLMs are generated. Off-the-shelf LLMs show the question-answering process with LLMs. LLMs with instruction tuning describe the process of fine-tuning LLMs specifically for graph tasks.}
%   \label{fig:The overall pipeline of our benchmark}
% \end{figure*}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figs/big_picture.pdf}  % 设置宽度为页面宽度
  \caption{The overall experimental pipeline for LLMs. Graph encoding outlines how prompts for LLMs are generated. Off-the-shelf LLMs show the question-answering process with LLMs. LLMs with instruction tuning describe the process of fine-tuning LLMs specifically for graph tasks.}
  \label{fig:The overall pipeline of our benchmark}
\end{figure*}

\section{Comprehensive Benchmarking of LLMs for Graph Tasks}
\label{sec:Fair Benchmark}
% \ym{Instead of Fair, we may use "comprehensive"?}
%\ym{Are there any works focusing on utilizing LLMs for graph tasks? If so, we may cite and discuss them. Then introduce the limitations including the inconsistent settings, limited baselines xxx. } 
%\ym{We could make this part more concrete, i.e, clearly motivate which baselines are missing and why they are so important}

%\ym{We need to motivate from the beginning: There are many papers on xxx. However xxxx }
Existing studies \cite{tang2024graphgpt, zhao2023graphtext, li2024glbench} on LLMs for graphs often use different datasets, data processing techniques, and data splitting strategies. This lack of consistency makes it difficult to compare results directly, resulting in an incomplete understanding of the LLM performance on graph tasks.
%Previous studies often employ varying data preprocessing techniques and splits, making it challenging to fairly compare LLMs with other graph models \cite{li2024glbench} \ym{$\leftarrow$are we trying to argue this cited paper is with issues?}. 
Furthermore, many LLM-based graph models have been evaluated only against a limited set of baseline models. For example, \cite{yan2023comprehensive} and \cite{instructglm} focus on classic GNNs and graph transformer models, while \cite{tang2024graphgpt} only evaluates classic GNNs and graph SSL models. These studies overlook more advanced models in the graph domain, particularly foundational graph prompt models (e.g., OFA), which have become a recent hotspot in graph research due to their strong generalization and adaptability. This narrow focus fails to fully capture the strengths and weaknesses of LLMs in graph-related tasks. To address this gap, we provide a comprehensive benchmark of LLMs against a broader set of graph machine learning methods for node classification and link prediction based on a fair comparison environment.

%Previous studies often use varying data preprocessing techniques and splits, making it difficult to fairly compare LLMs with other graph models \cite{li2024glbench}. Moreover, many LLM-based graph models have only been tested against a limited set of baseline models, such as traditional GNNs \cite{yan2023comprehensive, instructglm, tang2024graphgpt}. This narrow focus fails to fully capture the strengths and weaknesses of LLMs in the context of graph tasks since it does not account for the diversity of graph learning methods available. To address this gap, we provide a comprehensive benchmark of LLMs against a broader set of graph machine learning methods for node classification and link prediction based on a fair comparison environment. 
%\ym{$\leftarrow$ Why so? What we should/would do?  } 

%Additionally, most research \cite{sun2025graphicl, nlgraph, fatemi2023talklikeagraph} focus on the reasoning capabilities of off-the-shelf LLMs, neglecting a thorough evaluation of how instruction tuning influences their performance on graph tasks. A few studies \cite{instructglm, tang2024graphgpt, zhao2023graphtext} have explored instruction tuning for graph tasks, but they tend to focus on its design and effectiveness \ym{What do you mean by its design and effectiveness?} rather than comparing the performance of off-the-shelf LLMs or evaluating how different prompt formats affect their outcomes \ym{We do not need to mention the prompt format as it is not very significant.}. Therefore, in this section, we examine how instruction tuning and different prompt formats affect LLM performance to provide a more complete comparison of their capabilities.

% Additionally, most benchmarking works focus on the reasoning capabilities of off-the-shelf LLMs, overlooking a comprehensive evaluation of how instruction tuning impacts their performance on graph tasks. Studies related to graph instruction tuning often emphasize the design of instruction formats rather than providing a comprehensive benchmarking against various models. \ym{Do they compare? This argument sounds somewhat weak.}. \ym{We need to have more clear motivations.} For instance, studies like \cite{sun2025graphicl, nlgraph, fatemi2023talklikeagraph} only benchmark off-the-shelf LLMs across different prompt formats and graph tasks, while works such as \cite{instructglm, tang2024graphgpt, zhao2023graphtext} propose instruction formats for fine-tuning LLMs but fail to conduct a comprehensive comparison of fine-tuned LLMs with comprehensive graph models \ym{Is it still the lack of baseline issue?}. Therefore, in this section, we will conduct a comprehensive comparison of LLMs with instruction tuning against other graph models.
% %LLMs with instruction tuning will take center stage, and we will conduct a comprehensive comparison of their performance against other graph models.


%Additionally, most research has focused on the reasoning capabilities of off-the-shelf LLMs, neglecting a thorough evaluation of how instruction tuning influences their performance on graph tasks \cite{sun2025graphicl, nlgraph, fatemi2023talklikeagraph}.  \ym{I think we mentioned that some of the papers have already been doing instruction tuning? } We also evaluate the impact of instruction tuning and various prompt formats on LLM performance.
%and present new research questions based on our findings.

\subsection{The Overall Setup} 
\label{sec:The Overall Setup}
This part outlines the overall setup of the benchmarking. We detail the baseline models, datasets, and evaluation metrics used for node classification and link prediction tasks.

\subsubsection{Baselines}
% \ym{In this part, we xxx}


%\ym{We could simplify this part and push some contents to the appendix if we need additional space.}
% For LLMs, we select the open-source models Llama-3.2-3B-Instruct (Llama3B) \cite{touvron2023llama}, Llama-3.1-8B-Instruct (Llama8B) \cite{touvron2023llama}, and the closed-source Qwen-plus \cite{bai2023qwen}. 
% \ym{$\leftarrow$ These are not baselines, right? These should be part of "LLMs for Graphs"}
%Instruction tuning is conducted using Llama3B and Llama8B. 
For baseline models, we offer a thorough comparison across six graph learning paradigms, including both traditional GNNs and more advanced models, ensuring a comprehensive evaluation of LLMs’ capabilities. The baseline models include the following categories:
% \begin{itemize}
%     \item \textbf{GNNs}: We use widely adopted supervised learning models such as GCN \cite{gcn}, GraphSAGE \cite{graphsage}, and GAT \cite{gat} all trained from scratch. These models are well-established for their effectiveness in graph-based tasks.

%     \item \textbf{Graph Self-Supervised Learning (SSL) Models}: We incorporate GraphCL \cite{you2020graphcl} and GraphMAE \cite{hou2022graphmae}, representing distinct self-supervised learning paradigms. 
%     %GraphCL employs contrastive learning by distinguishing augmented views of the same graph from others, while GraphMAE uses masked autoencoding, reconstructing masked graph components to learn node representations without requiring augmented views.

%     \item \textbf{Graph Transformer (GT) Models}: We choose Graphormer \cite{graphormer}, a transformer-based model designed specifically to handle graph-structured data.
%     %, enabling efficient processing and analysis of complex relational information.

%     \item \textbf{Foundational Graph Prompt Models}: We evaluate Prodigy \cite{huang2024prodigy} and OFA \cite{ofa}. They leverage graph prompts to enhance the performance of pre-trained models in downstream tasks.
%     %, demonstrating strong capabilities with effective knowledge transfer.

%     \item \textbf{LM-Augmented Graph Learning Models}: We choose GIANT \cite{giant} and TAPE \cite{tape}, which integrate LMs with graph learning. GIANT uses pre-trained LMs for node embeddings, while TAPE generates textual explanations to enhance node features.
%     %, improving performance in tasks where text and graph structures are closely related.

%     \item \textbf{LLM with Graph Projectors}: We include LLaGA \cite{chen2024llaga} due to its simplicity and effectiveness. LLaGA uses a projector to map graph structures into vector representations, enabling LLMs to perform reasoning tasks more effectively.
% \end{itemize}

\begin{enumerate}
\item \textbf{GNNs}: We choose GCN \cite{gcn}, GraphSAGE \cite{graphsage}, and GAT \cite{gat} all trained from scratch.
\item \textbf{Graph SSL Models}: GraphCL \cite{you2020graphcl} and GraphMAE \cite{hou2022graphmae} represent self-supervised learning paradigms.
\item \textbf{Graph Transformer Models}: We choose Graphormer \cite{graphormer}, a transformer-based model for graph-structured data.
\item \textbf{Foundational Graph Prompt Models}: Prodigy \cite{huang2024prodigy} and OFA \cite{ofa}, which enhance pre-trained models using graph prompts.
\item \textbf{LM-Augmented Graph Models}: GIANT \cite{giant} and TAPE \cite{tape} integrate LMs with graph learning for feature enhancement.
\item \textbf{LLM with Graph Projectors}: LLaGA \cite{chen2024llaga} uses a projector to map graph structures into vectors for improved reasoning.
\end{enumerate}
More details about baseline models can be found in Appendix \ref{sec:Baseline models}.

%We select these baseline models to ensure a comprehensive comparison across various graph learning paradigms. The chosen models represent key approaches in graph learning, covering both traditional and advanced methods. This wide coverage allows us to effectively evaluate LLMs, ensuring a well-rounded assessment of their capabilities.
%\ym{This could be moved in front of the baselines as it provides the motivation. Esepecially discuss and highlight the foundation models, which is important but ignored in the previous works.}
%We select these baseline models to provide a comprehensive comparison across various graph learning paradigms. Representing both traditional and advanced methods, they ensure a well-rounded evaluation of LLMs’ capabilities.

\subsubsection{Datasets}
For both node classification and link prediction, we use the Cora \cite{cora}, PubMed \cite{pubmed}, OGBN-ArXiv \cite{ogb}, and OGBN-Products \cite{ogb} datasets. For baseline models, we use their original node features (Appendix \ref{sec:Impacts of Different Node Feature Embedding Methods} discusses the impact of different node feature embedding methods). For LLMs, we preprocess the raw data to transform the node attributes into textual representations. Cora, PubMed, and OGBN-ArXiv belong to the citation domain, while OGBN-Products belongs to the e-commerce domain. Detailed descriptions of the datasets and their splitting methods can be found in Appendix \ref{sec:Datasets}.

\subsubsection{Evaluation Settings}
For both node classification and link prediction, we consistently use accuracy as the evaluation metric, the same as \cite{chen2024llaga} and \cite{instructglm}. In the case of link prediction, where the ratio of positive to negative samples in the test set is 1:1, accuracy is a suitable measure. To select the best model, we perform hyperparameter tuning, as different hyperparameters may cause model performance to vary across datasets. 
%\ym{The following is a bit confusing. Also, I feel such detailed contents could be moved to appendix.} To select the best model, we perform hyperparameter tuning, recognizing that different hyperparameters may lead to model preferences varying across datasets. Therefore, we compute the average validation performance across multiple datasets to identify the optimal configuration. 
Detailed experimental settings and the hyperparameter search ranges for each model are provided in Appendix \ref{sec:Detailed Experimental Settings}.




%We explore the reasoning capabilities of LLMs without parameter optimization on node classification and link prediction tasks, while also evaluating the impact of instruction tuning on their performance. We select Llama3B, Llama8B, and Qwen-plus as the LLMs. For comparison, we include a variety of baseline models such as GNNs, Graph SSL models, GT models, OFA, and LLaGA. Additional comparisons with other LLMs and the effect of different prompt techniques are provided in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}.

% \subsubsection{Pipeline of Graph Encoding}

% \ym{This should be a part of "how we use LLMs for graphs", right?}

% % \subsubsection{Experimental Pipeline for LLMs}
% % \label{sec:Experiment Pipeline}
% % We present the overall experimental pipeline for LLMs in Figure \ref{fig:The overall pipeline of our benchmark}, which consists of three main stages: graph encoding, off-the-shelf LLMs, and LLMs with instruction tuning. \ym{Off-the-shelf LLMs is not a "stage"? I feel these are not three "stages", rather, "Off-the-Shelf" and "Instruction Finetuning" are two ways to utilize the LLMs? }

% % \ym{This part could be an independent component/subsection.}
% % \paragraph{\textbf{Graph encoding}}
% As shown in the graph encoding part of Figure \ref{fig:The overall pipeline of our benchmark}, we combine the original graph datasets with their corresponding raw text attributes to encode the graph into a format that LLMs can understand, i.e., prompts. The prompt formats required for node classification and link prediction differ based on the specific task.

% \paragraph{\underline{Prompt formats for node classification}}
% When designing the prompt formats, \cite{whenandwhy} considers three scenarios: using only the node own features, using 1-hop neighbor information, and using 2-hop neighbor information. \ym{Just describe, no need to use "however".} However, when neighbor information is included, it also contains the neighbor nodes’ labels, which significantly improves reasoning accuracy. \ym{Why we would like to investigate these prompts? Provide motivation.} In addition to the three prompt designs, we introduce two new formats by removing label information. Our five prompt formats are as follows:

% %The target node is the focus of our classification task. Inspired by \cite{whenandwhy} and \cite{instructglm} \ym{How we are inspired by them? They already proposed all 5 prompt strategies? Are we different from them? If so, how?}, we use five distinct prompts, as outlined below:
% \begin{enumerate}
%     \item \textbf{ego}: Only the node attribute of the target node is used for description.
%     \item \textbf{1-hop w/o label}: The target node is described using both its own node attributes and those of its 1-hop neighbors, without including labels.
%     \item \textbf{2-hop w/o label}: The description includes the attributes of the target node and those of its 2-hop neighbors, without labels.
%     \item \textbf{1-hop w label}: Similar to \textbf{1-hop w/o label}, but the labels of 1-hop neighbors from the training set are included.
%     \item \textbf{2-hop w label}: The labels of 2-hop neighbors from the training set are included.
% \end{enumerate}
% The detailed prompt structures can be found in Appendix \ref{sec:Prompt Formats for Node Classification}.

% \paragraph{\underline{Prompt formats for link prediction}}
% The goal of link prediction is to determine whether an edge exists between target node 1 and target node 2. We use two prompt formats: 1) \textbf{1-hop}: Both target nodes are described using their own node attributes and those of their 1-hop neighbors. 2) \textbf{2-hop}: Both target nodes are described using their own node attributes along with those of their 2-hop neighbors. It is important to note that, whether using 1-hop or 2-hop, the nodes at the ends of the link should not appear as neighbors of each other to avoid overly simplistic reasoning, ensuring that the LLM needs to perform more meaningful reasoning. The detailed prompt structures can be found in Appendix \ref{sec:Prompt Formats for Link Prediction}.

%\ym{The following is about the pipeline of using LLM. They are different from the contents above. We could separate them.}
%\subsubsection{Pipeline of Using LLMs}

% \subsubsection{Pipeline of Using LLMs}
% As shown in Figure \ref{fig:The overall pipeline of our benchmark}, there are two ways to use LLMs: one is off-the-shelf LLMs, which refers to LLMs without parameter optimization, and the other is LLMs with instruction tuning.

% \paragraph{\textbf{Off-the-shelf LLMs}}
% In this part, we outline the process of querying LLMs. No additional modifications are made to the LLMs themselves; instead, we simply input the encoded graph prompts along with the corresponding questions. By comparing the LLM responses with the correct answers, we can assess its performance. 
% %Additionally, in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}, we provide extended experiments that benchmark more LLMs (e.g., Qwen-max \cite{bai2023qwen}, GPT-4o \cite{achiam2023gpt4}, and Deepseek V3 \cite{liu2024deepseek}) and evaluate the performance of different prompt techniques (e.g., Chain of Thought (CoT) \cite{CoT}, Build A Graph (BAG) \cite{nlgraph}, and in-context few-shot learning) on graph tasks.
% Additionally, we explore commonly used prompt strategies such as Chain of Thought (CoT) \cite{CoT}, Build A Graph (BAG) \cite{nlgraph}, and in-context few-shot learning on more LLMs (e.g., Qwen-max \cite{bai2023qwen}, GPT-4o \cite{achiam2023gpt4}, and Deepseek V3 \cite{liu2024deepseek}) for node classification task. The results indicate that these prompt strategies show significant performance variation across different datasets and LLM sizes. They do not necessarily provide benefits in every case. Detailed experimental results and analysis are provided in Appendix \ref{sec:Comparison of Different LLMs on Node Classification}.


% \paragraph{\textbf{LLMs with instruction tuning}}
% %\ym{If space limits, we could move detailed description of how instruction tuning works "two models for link prediction" into appendix. } \ym{Also, it seems the 2 formats vs 9 formats is not extremely significant? It is somehow not very important to the main narrative? If so, we could move the 9 format (or 2 format) investigation to appendix. We can refer to the investigation in the main  content.  } \ym{When mentioning the different formats, motivate a bit on why we would like to investigate this. If we are not going to present 9 format, in the main text, we can summarize their results briefly in the main text. }
% In the right part of Figure \ref{fig:The overall pipeline of our benchmark}, we provide a brief overview of LLMs with instruction tuning. We perform instruction tuning on Llama3B and Llama8B across various datasets, using LoRA \cite{hu2021lora} and DeepSpeed \cite{rasley2020deepspeed} to accelerate model training, with each model trained for just one epoch. For node classification, we limit instruction tuning to three prompt formats: ego, 1-hop w/o label, and 2-hop w/o label.
% %, as labels often carry strong semantic information that may lead the model to overly rely on them.

% For link prediction, we aim to explore not only the impact of instruction tuning on LLMs’ reasoning performance but also the effect of prompt format diversity on LLM performance, a factor that has not been extensively examined in previous instruction tuning studies. This investigation could provide valuable insights for future prompt design. To achieve this, we set up two modes. The first mode uses the same prompt formats as in the testing phase (1-hop and 2-hop). The second mode introduces a more diverse set of prompt formats (9 formats), incorporating different question formulations and a wider range of neighbors. Detailed descriptions of the link prompt formats can be found in Appendix \ref{sec:Prompt Formats for Link Prediction}.

%For link prediction, we set up two modes. The first mode uses the same prompt formats as in the testing phase (1-hop and 2-hop). The second mode introduces more diverse prompt formats (9 formats), incorporating different question formulations and a broader range of neighbors. Detailed descriptions of the link prompt formats can be found in Appendix \ref{sec:Prompt Formats for Link Prediction}. To ensure a balanced ratio of positive and negative samples (1:1) and prevent model bias, we generate two prompts for each node in the training set: one with a “yes” answer and one with a “no” answer.

%\xn{For continuous pertaining, if this is a new method, I think it should take a subsection, to describe this method in detail. eg., if it is an unsupervised learning method, how update the parameters? what is the loss function? what is the purpose of the pertaining and tuning process?why this pipeline can help the processing? If the whole pipeline can use formulations to be described can be better, but if no, I think the whole pipeline should be more clear, as the following has a case of impact of continuous training}


%\textbf{Continuous Pre-training (Con.PT)} consists of two key stages. First, a pre-trained model undergoes unsupervised learning on the target dataset. This phase is task-agnostic, meaning the model learns general graph representations rather than optimizing for the final task. Next, the model is instruction-tuned on a task that matches the inference objective. For example, the pre-training stage might involve link prediction, while the final inference task is node classification. This approach helps the model adapt to the dataset structure and mitigates the impact of limited labeled data, ultimately improving performance.


%After instruction tuning, the models enter the testing phase, following the same procedure as the “LLMs without parameter optimization”, but this time the LLMs being queried are tuned versions.


\begin{table}[htbp]
\centering
\caption{Performance of different models on node classification tasks. The background colors represent the top three values in each column, from dark to light.}
%Tuned Llama3B and Llama8B mean Llama3B and Llama8B after instruction tuning.
\label{tab:node_classification_results_LLM}
\scalebox{0.78}{%
\begin{tabular}{l c c c c c c}
\toprule
\rowcolor{gray!10}
\textbf{Model} & \textbf{Prompt} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg}\\ 
\midrule
GCN & - & 88.19 & 88.00 & 69.90 & 82.30 & 82.10\\
GraphSAGE & - & 89.67 \cellcolor{cyan!100} & 89.02 & 71.35 & 82.89 & 83.23\\
GAT & - & 88.38 & 87.90 & 68.69 & 82.10 & 81.77\\
GraphCL & - & 83.58 & 82.86 & 67.87 & 80.20 & 78.63\\
GraphMAE& - & 75.98 & 82.82 & 65.54 & 77.32 & 75.42\\
Graphormer& - & 81.20 & 88.05 & 71.99 & 81.75 & 80.75\\
Prodigy& - & 77.32 & 83.6 & 70.86 & 80.01 & 77.95 \\
OFA& - & 78.31 & 78.56 & 73.92 & 83.12 & 78.48\\
GIANT& - & 89.1 & 90.48 & 74.41 \cellcolor{cyan!20} & 84.33 \cellcolor{cyan!50} & 84.58 \cellcolor{cyan!20}\\
TAPE& - & 88.12 & 91.92 & 73.99 & 83.11 & 84.29\\ 

LLaGA& - & 88.94 & 94.57 \cellcolor{cyan!50} & 76.25 \cellcolor{cyan!100} & 83.98 \cellcolor{cyan!20} & 85.94 \cellcolor{cyan!50}\\
\midrule

\multirow{5}{*}{Llama3B} & ego & 24.72 & 63.20 & 23.10 & 40.80  & 37.96\\
& 1-hop w/o label & 39.48 & 64.50 & 29.50 & 53.00  & 46.62\\
& 2-hop w/o label & 49.63 & 69.90 & 29.50 & 56.10  & 51.28\\
& 1-hop w label & 77.49 & 70.90 & 66.00 & 68.80  & 70.80\\
& 2-hop w label & 83.03 & 72.00 & 65.20 & 71.20  & 72.86\\
\midrule
\multirow{5}{*}{Llama8B} & ego & 43.39 & 77.80 & 59.35 & 50.12  & 54.02\\
& 1-hop w/o label & 58.35 & 73.07 & 61.85 & 59.85  & 63.28\\
& 2-hop w/o label & 62.84 & 83.29 & 68.33 & 59.60  & 68.52\\
& 1-hop w label & 82.97 & 81.55 & 68.08 & 71.07  & 75.92\\
& 2-hop w label & 84.79 & 82.54 & 64.09 & 77.06  & 77.12\\
\midrule
\multirow{5}{*}{Qwen-plus} & ego & 52.32 & 80.74 & 70.20 & 64.24  & 69.69\\
& 1-hop w/o label & 68.87 & 85.73 & 73.83 & 72.19  & 75.16\\
& 2-hop w/o label & 76.16 & 88.98 & 73.51 & 71.56  & 77.55\\
& 1-hop w label & 87.42 & 88.74 & 73.55 & 74.83  & 81.14\\
& 2-hop w label & 89.40 \cellcolor{cyan!50} & 90.73 & 74.28 & 78.81  & 83.31\\
\midrule
\multirow{3}{*}{tuned Llama3B} & ego & 67.08 & 89.28 & 66.58 & 65.59 & 72.13\\
& 1-hop w/o label & 82.04 & 90.02 & 71.32 & 73.07 & 79.11\\
& 2-hop w/o label & 85.04 & 91.52 & 72.82 & 77.89 & 81.82\\
\midrule
\multirow{3}{*}{tuned Llama8B} & ego & 77.31 & 92.36 & 65.59 & 73.74 & 78.38\\
& 1-hop w/o label & 84.54 & 93.90 \cellcolor{cyan!20} & 69.33 & 80.33 & 83.28 \\
& 2-hop w/o label & 89.67 \cellcolor{cyan!100} & 95.22 \cellcolor{cyan!100} & 76.01 \cellcolor{cyan!50} & 84.51 \cellcolor{cyan!100} & 86.35 \cellcolor{cyan!100}\\
\bottomrule
\end{tabular}
}
\end{table}

\subsection{Results and Analysis}\label{sec:benchmarking_results}
In this section, we present and analyze the performance of various models across node classification and link prediction tasks, providing insights into the strengths and weaknesses of LLMs.
%\ym{Just include one more sentence to introduce what we do in this section.}
\paragraph{\underline{Node classification}}
%Table \ref{tab:node_classification_results_LLM} summarizes the performance of each model across different datasets. \ym{We make the following observations from Table~\ref{tab:node_classification_results_LLM}}:  \ym{Organize the observations into a nice itemized list.} Classic GNNs show consistent accuracy, while GIANT and TAPE outperform standalone GNNs by leveraging language models for improved node representations. On the other hand, LLMs \ym{Is this for non-tuned version?} demonstrate improved performance with larger models, which can perform comparably to GNNs under certain node description prompts. The choice of prompts is crucial, with multiple-hop prompts yielding better results than simpler single-hop or ego-based prompts, indicating that LLMs benefit from richer graph context. Additionally, label information enhances classification performance by strengthening the model’s decision-making process, similar to in-context learning.

%LLaGA also performs well, likely due to its use of a graph projector that more effectively integrates graph information compared to natural language descriptions. For instruction-tuned LLMs, both Llama3B and Llama8B show notable improvements, especially with multiple-hop prompts. Tuned Llama8B achieves the highest average score of 86.35, surpassing LLaGA and setting a new benchmark for LLMs in node classification.


Table \ref{tab:node_classification_results_LLM} summarizes the performance across different datasets. We make the following observations:
\begin{itemize}

\item Classic GNNs show consistent accuracy, while GIANT and TAPE outperform them by using language models for improved node representations. Larger off-the-shelf LLMs perform comparably to GNNs under certain prompts, with multiple-hop prompts yielding better results than simpler prompts, indicating that LLMs benefit from richer graph context.

\item Label information improves performance by strengthening the model decision-making process, similar to in-context learning.

\item For instruction-tuned LLMs, both Llama3B and Llama8B show notable improvements, especially with multiple-hop prompts. Tuned Llama8B achieves the highest average score, surpassing LLaGA and setting a new benchmark in node classification.
\end{itemize}



\begin{table}[htbp]
\centering
\caption{LLM performance on link prediction.}
\label{tab:LLM performance on link prediction}
\scalebox{0.71}{%
\begin{tabular}{l c c c c c c}
\toprule
\rowcolor{gray!10}
\textbf{Models} & \textbf{Prompts} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} \\ 
\midrule

GCN &- & 87.78 & 86.22 & 90.34 & 89.75 & 88.52 \\
GraphSAGE &- & 84.39 & 78.81 & 92.98 & 92.98 & 87.29 \\
GAT &- & 86.88 & 82.81 & 83.33 & 85.57 & 84.65 \\
GraphCL &- & 92.98 & 93.76 & 90.85 & 94.21 & 92.95 \\
GraphMAE &- & 82.01 & 75.71 & 85.24 & 88.32 & 82.82 \\
Prodigy &- & 90.9 & 91.67 & 89.22 & 92.99 & 91.2\\
OFA &- & 94.19 & 98.05 & 95.84 \cellcolor{cyan!20} & 96.90 \cellcolor{cyan!20} & 96.25 \\
LLaGA &- & 87.01 & 90.10 & 93.88 & 95.67 & 91.67 \\
\hline

\multirow{2}{*}{Llama3B} & 1-hop & 72.97 & 71.55 & 72.45 & 78.92 & 73.97 \\
& 2-hop & 68.21 & 59.95 & 68.55 & 79.17 & 68.97 \\
\midrule
\multirow{2}{*}{Llama8B} & 1-hop & 80.44 & 74.80 & 87.80 & 85.29 & 82.08 \\
& 2-hop & 89.39 & 77.30 & 92.30 & 90.77 & 87.44 \\
\midrule
\multirow{2}{*}{Qwen-plus} & 1-hop & 78.81 & 91.74 & 81.82 & 88.42 & 85.20 \\
& 2-hop & 90.91 & 95.04 & 93.39 & 90.12 & 92.37 \\
\midrule
\multirow{2}{*}{tuned Llama3B (2 formats)} & 1-hop & 83.12 & 93.95 & 92.20 & 90.07 & 89.84 \\
& 2-hop & 95.76 \cellcolor{cyan!50} & 98.35 & 95.45 & 94.65 & 96.05 \\
\midrule
\multirow{2}{*}{tuned Llama3B (9 formats)} & 1-hop & 87.18 & 94.40 & 93.30 & 95.45 & 92.58 \\
& 2-hop & 95.94 \cellcolor{cyan!100} & 99.20 \cellcolor{cyan!100} & 95.42 & 97.84 \cellcolor{cyan!50} & 97.10 \cellcolor{cyan!100} \\
\midrule
\multirow{2}{*}{tuned Llama8B (2 formats)} & 1-hop & 88.65 & 95.12 & 93.65 & 93.23 & 92.66 \\
& 2-hop & 95.39 \cellcolor{cyan!20} & 98.77 \cellcolor{cyan!20} & 96.11 \cellcolor{cyan!100} & 94.92 & 96.30 \cellcolor{cyan!20} \\
\midrule
\multirow{2}{*}{tuned Llama8B (9 formats)} & 1-hop & 88.47 & 96.01 & 95.21 & 96.33 & 94.01 \\
& 2-hop & 95.15 & 99.20 \cellcolor{cyan!100} & 95.89 \cellcolor{cyan!50} & 97.98 \cellcolor{cyan!100} & 97.06 \cellcolor{cyan!50} \\
\bottomrule
\end{tabular}
}
\end{table}


\paragraph{\underline{Link prediction}}

%The results for link prediction are presented in Table \ref{tab:LLM performance on link prediction}. Among baseline models, GraphCL outperforms both GNNs and LLaGA, while GraphMAE shows the weakest performance despite being another Graph SSL model. This discrepancy likely stems from GraphCL’s use of edge permutation in contrastive learning, which enhances its understanding of graph structures, while GraphMAE focuses only on node features. Additionally, OFA stands out as the best-performing baseline likely due to its use of edge features derived from LLMs during pre-training. In contrast, Llama3B and Llama8B without parameter optimization underperform most baseline models. However, the larger Qwen-plus model matches or even surpasses the baseline models, highlighting the critical role of model size in LLMs’ ability to understand graphs.

%The best performance in link prediction is achieved by instruction-tuned LLMs. Models using 2-hop prompts consistently outperform those using 1-hop prompts, and tuning with a diverse set of 9 prompt formats leads to better results than tuning with just 2 formats. This demonstrates that both reasoning and tuning benefit from richer graph descriptions, significantly enhancing LLMs’ ability to capture and interpret graph information.


The results for link prediction are presented in Table \ref{tab:LLM performance on link prediction}. We make the following observations:
\begin{itemize}

\item Among baseline models, GraphCL outperforms both GNNs and LLaGA, while GraphMAE shows the weakest performance despite being another Graph SSL model. This discrepancy likely because GraphCL uses edge permutation in contrastive learning, which enhances its understanding of graph structures, while GraphMAE focuses only on node features. Additionally, OFA stands out as the best-performing baseline likely due to its use of edge features derived from LLMs during pre-training.

\item Off-the-shelf Llama3B and Llama8B underperform most baseline models. However, the larger Qwen-plus model matches or even surpasses the baseline models, highlighting the critical role of model size in LLMs’ ability to understand graphs.

\item The best performance in link prediction is achieved by instruction-tuned LLMs. Models using 2-hop prompts consistently outperform those using 1-hop prompts, and tuning with a diverse set of 9 prompt formats leads to better results than tuning with just 2 formats. This demonstrates that both reasoning and tuning benefit from richer graph descriptions, significantly enhancing LLMs’ ability to capture and interpret graph information.

\end{itemize}



%\ym{We can call the following as a "remark" instead of "observation". The previous discussions on the results are the observations. The following are further insights/conclusions we develop from the observations. }
%\paragraph{\textbf{Observation 1:}}
%\ym{For both tasks?}
%LLMs without parameter optimization generally perform worse than most baseline models. However, as the model size increases and graph structure information is incorporated, their reasoning ability significantly improves. In some cases, LLMs can achieve performance comparable to or even surpassing the best baseline models.

%\paragraph{\textbf{Observation 2:}}
%\ym{This may be merged together with the previous one.}
%Instruction tuning significantly boosts the performance of LLMs on graph tasks. Even with smaller model sizes, LLMs that undergo richer and more diverse instruction tuning can achieve performance on par with, or even better than, the best baseline models.
\begin{remark}
   Although smaller off-the-shelf LLMs underperform most baseline models, their reasoning ability improves significantly as the model size increases and graph structure information is incorporated. Instruction tuning further enhances LLM performance on graph tasks, with even smaller models achieving performance comparable to or better than the best baseline models, particularly when richer and more diverse instruction tuning is applied.
\end{remark}

\vspace{\baselineskip}
LLMs with instruction tuning have shown strong potential in graph tasks. However, this section only explores their performance in scenarios with sufficient data, while data scarcity is more common in real world applications. Therefore, in the next chapter, we will focus on exploring the performance of LLMs with instruction tuning in data-limited scenarios.



