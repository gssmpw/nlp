

\section{Conclusion}
\label{sec:conclusion}
%This paper thoroughly explores the potential of large language models (LLMs) for graph tasks, specifically node classification and link prediction. The main goal is to investigate how well LLMs perform in various graph-related scenarios, particularly when faced with limited data and transferred across different domains. The key findings show that LLMs, especially those with instruction tuning, excel in few-shot settings, outperforming traditional models like GNNs and Graph SSL models. Furthermore, LLMs demonstrate strong domain transferability, particularly in link prediction tasks, and exhibit excellent generalization and robustness. Additionally, the introduction of continuous pre-training, a two-phase approach, significantly enhances LLM performance in both zero-shot and few-shot learning tasks, particularly in scenarios with limited labeled data. For future research, it is important to further investigate how LLMs can be fine-tuned for more specialized graph tasks and explore hybrid models that combine the strengths of LLMs and traditional graph learning methods. Additionally, examining the robustness and scalability of LLMs in more complex graph scenarios remains a crucial area for unlocking their full potential.


This paper demonstrates that LLMs, especially with instruction tuning, achieve strong performance and surpass most graph models in node classification and link prediction through a fair and comprehensive benchmarking approach. Our findings emphasize the potential of LLMs in few-shot learning, transferability, and understanding graph structures in data-scarce scenarios. The introduction of continuous pre-training further boosts LLM performance in such environments. These insights provide valuable guidance for the future application of LLMs in graph tasks, paving the way for more efficient and adaptable graph learning models in real-world settings.