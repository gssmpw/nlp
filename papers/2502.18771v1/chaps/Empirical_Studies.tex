
\section{Experiment and  Analysis}
\label{sec:Empirical Studies}
In this section, we conduct empirical studies on different research questions proposed in Section \ref{sec:Research Questions}. 
%Due to space constraints, the results and discussion for RQ5 are included in Appendix \ref{sec:Robustness of LLMs}.
%Based on the configurations outlined in Section \ref{sec:The Overall Setup}, we investigate the performance of LLMs across various scenarios and compare them with the corresponding baseline models. 
In the following subsections, we first introduce the experimental settings for each RQ, followed by an analysis of the experimental results and key remarks.





\subsection{Few-Shot Instruction Tuning of LLMs (RQ1)}
\label{sec:Case 2}
%Real-world applications often face data scarcity, requiring models to perform well with limited data, which is the essence of few-shot learning. In this case, we investigate the performance of LLMs on the node classification task under few-shot settings.
We focus on few-shot instruction tuning for node classification. Link prediction requires predicting edges between nodes, relying on more complex structural dependencies that are harder to capture in a few-shot setting.
\subsubsection{Experiment Settings}
We use ego, 1-hop w/o label, and 2-hop w/o label as prompt formats and randomly select 5 or 10 target nodes per class for instruction tuning, corresponding to “n-ways-5-shots” and “n-ways-10-shots” learning. 
% For instance, in Cora, which has 7 classes, n-ways-5-shots would use 7 classes \texttimes 5 shots \texttimes 3 prompt formats as tuning samples.
For baseline models, in addition to GNNs and Graph SSL models, we also include models from foundational graph prompt approaches, including All in one \cite{sun2023allinone}, GPF-plus \cite{gpf-plus}, and GraphPrompt \cite{liu2023graphprompt}. The three models excel in few-shot scenarios, leveraging pre-trained knowledge and graph prompts to adapt quickly to new tasks with minimal labeled data.



\subsubsection{Results}
%We summarize the results in Table \ref{tab:node_classification_results_few_shot_LLM}. As seen, all models experience accuracy drops under few-shot learning compared to full fine-tuning, with GNNs and Graph SSL models showing the largest declines. Larger datasets like ArXiv and Products show a more significant accuracy drop for these models, while LLMs exhibit a more consistent performance across all datasets, indicating greater robustness in data-scarce scenarios. Foundational graph prompt models generally outperform GNNs and Graph SSL models in few-shot settings, likely because graph prompts help pre-trained models adapt more effectively to new data.

%Among all models, LLMs perform the best in few-shot learning, with the 2-hop prompt providing the highest accuracy. This suggests that incorporating more context improves performance. Notably, Llama8B achieves the highest classification accuracy in both 5-shot and 10-shot scenarios, highlighting LLMs’ ability to learn quickly from limited data and perform well in data-scarce environments.

Table \ref{tab:node_classification_results_few_shot_LLM} summarizes the results. All models experience a decline in accuracy under few-shot learning compared to full fine-tuning, with GNNs and Graph SSL models showing the largest drops, particularly in larger datasets like ArXiv and Products. In contrast, LLMs exhibit more consistent performance, indicating greater robustness in data-scarce scenarios. Notably, Llama8B achieves the highest classification accuracy in both 5-shot and 10-shot scenarios, showing LLMs’ ability to learn quickly from limited data.
%\ym{Please check these observations, some of them are repeated. Also, should we discuss a bit more on ego, 1-hop, 2-hop?}


% \paragraph{\textbf{Remark 2:}}
%Even though some GNNs training from scratch can achieve performance similar to tuned LLMs when trained on full data, LLMs clearly outperform all other models in few-shot scenarios. Only a few foundational graph prompt models manage to reach comparable performance to LLMs, but this is limited to a small number of datasets. This demonstrates that LLMs have a significant advantage when working with limited data. Foundational graph prompt models generally outperform GNNs and Graph SSL models in few-shot settings, likely because graph prompts help pre-trained models adapt more effectively to new data.
\begin{remark}
LLMs outperform all other models in few-shot scenarios. Only a few foundational graph prompt models achieve comparable results on Arxiv dataset, underscoring LLMs’ clear advantage in data-scarce situations.
%LLMs outperform all of them in few-shot scenarios. \ym{please adjust the following sentence.} Only a few foundational graph prompt models achieve comparable results, but this is limited to specific datasets. This highlights LLMs’ clear advantage in data-scarce situations.
\end{remark}



\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figs/conpt.pdf}
  %\vspace{-20pt}
  \caption{Impact of continuous pre-training on LLMs}
  \label{fig:Continuous Pre-Trainings}
\end{figure}

\subsection{Impact of Continuous Pre-training (RQ2)}
\label{sec:Case 3}
%\ym{I think we do both zero-shot and few-shot? Need to introduce them.  }
As we can see from Figure \ref{fig:The overall pipeline of our benchmark}, continuous pre-training (Con.PT) consists of two stages. First, a pre-trained model undergoes unsupervised learning on the target dataset. This phase is task-agnostic, meaning the model learns general graph representations rather than optimizing for the final task. Next, the model is instruction-tuned on a task that matches the inference objective.
\subsubsection{Experiment Settings}
%\ym{We only do experiments on node classification? Why? Need to describe and explain.}

In this experiment, we evaluate both zero-shot and few-shot node classification. For the zero-shot setting, we begin by performing continuous pre-training on the relevant dataset using link prediction, treating it as an unsupervised learning task. We then carry out zero-shot node classification based on this pre-training. The baseline models compared in this setup include LLaGA and ZeroG \cite{li2024zerog}, which is a foundational graph prompt model designed specifically for zero-shot scenarios. For the few-shot setting, we conduct few-shot instruction tuning on top of the link prediction task and compare the results with those from direct few-shot instruction tuning without the link prediction step.

%We begin by performing continuous pre-training on the corresponding dataset using link prediction. This can be viewed as an unsupervised learning task, providing a foundation for zero-shot node classification. The baseline models compared in this setup include LLaGA and ZeroG \cite{li2024zerog}, a foundational graph prompt model designed for zero-shot scenarios. Additionally, we conduct few-shot instruction tuning on top of the link prediction task and compare the results with those of direct few-shot instruction tuning without the link prediction step.



\begin{table}[htbp]
\centering
\caption{Performance of continuous pre-training for LLM. "w Con.Pt" means zero-shot inference after continuous pre-training. "w 5shot" means direct 5-shot instruction tuning without continuous pre-training. "w Con.PT \& 5shot" means 5-shot instruction tuning after continuous pre-training.}
\label{tab:continuous pre-training for LLM}
\scalebox{0.67}{%
\begin{tabular}{l c c c c c c}
\toprule
\rowcolor{gray!10}
\textbf{Models} & \textbf{Prompts} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} \\ 
\midrule
ZeroG &- & 68.61 & 78.77 & 70.50 \cellcolor{cyan!20} \cellcolor{cyan!20} & 55.23 & 68.28 \\
LLaGA &- & 22.03 & 55.92 & 21.15 & 38.90 & 34.50 \\
\midrule
\multirow{3}{*}{Llama3B} & ego & 24.72 & 63.20 & 23.10 & 40.80 & 37.96 \\
& 1-hop w/o label & 39.48 & 64.50 & 29.50 & 53.00 & 46.62 \\
& 2-hop w/o label & 49.63 & 69.90 & 29.50 & 56.10 & 51.28 \\
\midrule
\multirow{3}{*}{Llama3B w Con.PT} & ego & 48.63 & 49.38 & 14.21 & 41.40 & 38.41 \\
& 1-hop w/o label & 49.38 & 69.33 & 30.01 & 55.86 & 51.15 \\
& 2-hop w/o label & 55.36 & 75.56 & 33.54 & 57.01 & 55.37 \\
\midrule
\multirow{3}{*}{Llama3B w 5shot} & ego & 59.10 & 67.08 & 49.65 & 59.12 & 58.74 \\
& 1-hop w/o label & 74.81 & 65.59 & 53.53 & 65.35 & 64.82  \\
& 2-hop w/o label & 76.81 & 71.32 & 55.24 & 67.32 & 67.67  \\
\midrule
\multirow{3}{*}{Llama3B w Con.PT \& 5shot} & ego & 59.60 & 84.29 & 50.37 & 60.88 & 63.79  \\
& 1-hop w/o label & 75.08 & 85.04 & 53.12 & 66.09 & 69.83  \\
& 2-hop w/o label & 79.58 \cellcolor{cyan!100}  & 88.53 \cellcolor{cyan!50}  & 54.11 & 68.08 & 72.58  \\
\midrule
% \rowcolor{gray!10}
 % & & & & &  &  \\
 

\multirow{3}{*}{Llama8B} & ego & 43.39 & 77.80 & 59.35 & 50.12 & 54.02 \\
& 1-hop w/o label & 58.35 & 73.07 & 61.85 & 59.85 & 63.28 \\
& 2-hop w/o label & 62.84 & 83.29 & 68.33 & 59.60 & 68.52 \\
\midrule
\multirow{3}{*}{Llama8B w Con.PT} & ego & 52.13 & 65.32 & 60.71 & 55.22 & 58.35 \\
& 1-hop w/o label & 64.44 & 80.20 & 63.10 & 62.84 & 67.65 \\
& 2-hop w/o label & 70.82  & 86.96 \cellcolor{cyan!20}  & 71.34 \cellcolor{cyan!100}  & 63.20 & 73.08  \\
\midrule
\multirow{3}{*}{Llama8B w 5shot} & ego & 65.84 & 76.81 & 63.97 & 65.12 & 67.94  \\
& 1-hop w/o label & 74.56 & 76.45 & 65.98 & 70.50 & 71.87  \\
& 2-hop w/o label & 77.1 \cellcolor{cyan!20}0  & 79.43 & 69.78  & 73.12 \cellcolor{cyan!50}  & 74.86 \cellcolor{cyan!20}  \\
\midrule
\multirow{3}{*}{Llama8B w Con.PT \& 5shot} & ego & 68.33 & 86.88  & 63.23 & 66.44 & 71.22 \\
& 1-hop w/o label & 76.82 & 86.83 & 66.77  & 70.99 \cellcolor{cyan!20}  & 75.35 \cellcolor{cyan!50}  \\
& 2-hop w/o label & 78.12 \cellcolor{cyan!50}  & 89.03 \cellcolor{cyan!100}  & 71.01 \cellcolor{cyan!50}  & 74.69 \cellcolor{cyan!100}  & 78.21 \cellcolor{cyan!100}  \\
\bottomrule
\end{tabular}
}
\end{table}



\subsubsection{Results}
%The results are presented in Table \ref{tab:continuous pre-training for LLM}. We only show the results for the 5-shot scenario in the table. It shows that LLMs perform better after continuous pre-training compared to direct zero-shot and few-shot learning, indicating that continuous pre-training effectively enhances the model’s understanding of graphs. Additionally, in few-shot scenario, for the smaller datasets like Cora and PubMed, Llama3B with continuous pre-training matches or even surpasses the performance of Llama8B. However, for larger and more complex datasets like Arxiv and Products, Llama8B still maintains a clear advantage over Llama3B, even though the Llama3B has been updated by continuous pre-training. This suggests that increasing the size of the LLM remains the most effective approach for larger and more complex graphs. 

Table \ref{tab:continuous pre-training for LLM} presents the results. LLMs perform better after continuous pre-training compared to direct zero-shot and few-shot learning, demonstrating its effectiveness in enhancing the model understanding of graphs. For smaller datasets like Cora and PubMed, Llama3B with continuous pre-training matches or even surpasses Llama8B. However, for larger and more complex datasets like Arxiv and Products, Llama8B retains an advantage even after Llama3B undergoes continuous pre-training. This suggests that increasing the size of the LLM remains the most effective approach for larger and more complex graphs. 

To illustrate the impact of continuous pre-training more clearly, we present the average results over different datasets for Llama3B and Llama8B under the "2-hop w/o label" setting in Figure \ref{fig:Continuous Pre-Trainings}. As shown in the figure, continuous pre-training helps the LLMs develop a deeper understanding of the graph, leading to improved performance. Notably, Llama8B outperforms ZeroG that specifically designed for zero-shot tasks by a margin of 5\% after continuous pre-training.

\begin{remark}
Continuous pre-training can significantly improve LLM performance in zero-shot and few-shot learning. However, for larger and more complex datasets, increasing the size of the LLM proves to be a more effective approach.
\end{remark}
\subsection{Domain Transferability of LLMs (RQ3)}
% \subsection{LLM Transferability Across Domains (RQ3)}
\label{sec:Case 4}
%Domain transferability is a key capability for general models. In this case, we focus on evaluating the domain transferability of LLMs for node classification and link prediction tasks, considering both in-domain and cross-domain scenarios.
Domain transferability can be classified into in-domain and cross-domain transferability based on difficulty. The former refers to the ability to transfer knowledge between different datasets within the same domain, while the latter involves transferring knowledge across different domains. In this section, we explore the performance of LLMs with instruction tuning in both settings.


%\ym{We should introduce what we would like to do here: describe and motivate the experiments we would like to do: transferability across different datasets (in-domain vs cross-domain?)}
\subsubsection{Experiment Settings}
In the in-domain setup, we train the model on citation graphs (Arxiv) and evaluate it using Cora, another citation graph. For the cross-domain scenario, we train on Arxiv and test on Products, an e-commerce graph. GNNs rely on task-specific classification heads, which limits their ability to perform zero-shot learning on node classification tasks, particularly when label sets differ. Therefore, our comparison focuses on LLaGA for node classification. For link prediction, since the feature dimensions vary across datasets, we use a simple linear mapping to unify them. The baseline models include GNNs, Graph SSL models, and LLaGA.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=1\linewidth]{figs/domain_transferability.pdf}
  %\vspace{-20pt}
  \caption{LLM domain transferability in node classification}
  \label{fig:Domain Transferability}
\end{figure}

\subsubsection{Results}
\paragraph{\underline{Node classification}}
%We present the results in Figure \ref{fig:Domain Transferability}, which shows the accuracy of different models in both in-domain and cross-domain scenarios. For node classification tasks, LLMs instruction tuned on Arxiv outperform those used for zero-shot prediction, though the improvement is modest. This is likely because category information is crucial in node classification. While the model learns the graph structure on Arxiv, it struggles to adapt to unseen categories in new datasets, limiting performance gains. In addition, On the Cora dataset, LLMs are comparable with LLaGA, but on the Products dataset, LLMs exhibit a clear edge. This suggests that LLaGA may struggle to capture and understand the full range of graph patterns because it only employs a simple graph projector. In contrast, LLMs can better adapt to the diversity of graph structures in more complex datasets like Products with their more sophisticated learning mechanisms.

Figure \ref{fig:Domain Transferability} presents the accuracy of different models in both in-domain and cross-domain scenarios. Instruction-tuned LLMs on Arxiv outperform off-the-shelf scenario, but the improvement is modest when additional structural information is incorporated. This is likely due to the fact that node classification relies heavily on category information, and adding more structural data does not significantly enhance performance. While LLMs learn graph information from Arxiv, adapting to unseen categories remains challenging, limiting performance gains. Besides, LLMs perform comparably to LLaGA on Cora dataset, but on the more complex Products dataset, LLMs show a clear advantage. 
%\ym{This is a very nice observation. Is it just graph patterns? Or also features? Do LLMs capture more diverse/general feature information?} %This suggests that LLaGA’s simple graph projector struggles to capture diverse graph patterns, whereas LLMs adapt better to varying structures with their more sophisticated learning mechanisms.
This suggests that the simple graph projector of LLaGA struggles to capture diverse graph patterns, while LLMs can adapt better to varying structures and are capable of learning diverse feature information with their sophisticated instruction tuning mechanisms.

\begin{table}[htbp]
\centering
\caption{LLM domain transferability in link prediction.}
\label{tab:LLM domain transferability}
\scalebox{0.85}{%
\begin{tabular}{l c c| c }
\toprule
\rowcolor{gray!10}
 & & \multicolumn{2}{c}{\textbf{Train $\longrightarrow$ Test}} \\
\cline{3-4}
\rowcolor{gray!10}
\textbf{Models} & \textbf{Prompts} & \textbf{Arxiv $\longrightarrow$ Cora} & \textbf{Arxiv $\longrightarrow$ Products} \\
\midrule
GCN &- & 55.54 & 67.07 \\
GraphSAGE &- & 50.00 & 51.11 \\
GAT &- & 85.41 & 71.18 \\
GraphCL &- & 78.30 & 82.62 \\
GraphMAE &- & 71.90 & 73.94 \\
LLaGA &- & 86.98 & 92.82 \cellcolor{cyan!20} \\
\midrule
\multirow{2}{*}{Llama3B} & 1-hop & 87.55 & 91.16 \\
& 2-hop & 95.11 \cellcolor{cyan!100} & 94.15 \cellcolor{cyan!50} \\
\midrule
\multirow{2}{*}{Llama8B} & 1-hop & 88.98 \cellcolor{cyan!20} & 91.97 \\
& 2-hop & 94.78 \cellcolor{cyan!50} & 95.43 \cellcolor{cyan!100} \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{\underline{Link prediction}}
%From Table \ref{tab:LLM domain transferability}, we observe that LLMs demonstrate better domain transferability compared to baseline models. In the in-domain transfer scenario, LLMs achieve performance on the Cora dataset comparable to models directly instruction-tuned on Cora. This suggests that LLMs can effectively learn sufficient structural information from larger datasets within the same domain and transfer it to downstream tasks. In the cross-domain scenario, while the accuracy of LLMs on the Products dataset is slightly lower than that of direct tuning, it remains strong. This could be because datasets from different domains often share similar topological patterns. Among the baseline models, only LLaGA achieves performance comparable to LLMs, likely because it also leverages LLMs for predictions.

From Table \ref{tab:LLM domain transferability}, we observe that LLMs significantly outperform traditional graph models. Only LLaGA achieve comparable performance, likely because it also leverages LLMs for predictions. 
%From Table \ref{tab:LLM domain transferability}, we observe that LLMs outperform baseline models in domain transferability. \ym{The observations could be several level: 1. LLMs beat traditional GNN based methods 2. LLMs acehive comparable performance with instruction-fine tuned counterparts. } 
In the in-domain transfer scenario, LLMs achieve performance on Cora comparable to models directly instruction-tuned on Cora, indicating they can effectively transfer knowledge from larger datasets to downstream tasks. In the cross-domain scenario, although LLM performance on Products is slightly lower than direct tuning, it still remains strong, possibly due to shared topological patterns across domains.


%LLMs demonstrate domain transferability in both node classification and link prediction tasks, with more pronounced success in link prediction. While the transferability in node classification is limited, LLMs effectively learn and apply structural information for link prediction, showing strong generalization across different datasets.

%LLMs exhibit strong domain transferability, particularly in link prediction tasks, where they effectively generalize across different datasets. While their transferability in node classification is more limited due to the challenge of adapting to unseen categories\ym{nice argument, we could elaborate a bit. Link prediction tasks among different domains share more "similarity" compared with node classification? After all, link prediction is always about binary classification? }, they still outperform baseline models in both in-domain and cross-domain scenarios.
\begin{remark}
LLMs with instruction tuning exhibit strong domain transferability, particularly in link prediction tasks, where they effectively generalize across different datasets. This may be because link prediction tasks across domains share more similarities, as they can be viewed as binary classification problems. In contrast, node classification is more challenging, as adapting learned knowledge to unseen categories is difficult. %Nevertheless, LLMs still outperform baseline models in both in-domain \ym{It did not? for the in-domain case?} and cross-domain scenarios for node classification.
\end{remark}


\subsection{LLM Understanding of Graph Structures (RQ4)}
%\ym{Still need some introduction/descriotin/motivation here before we go to the settings.}
\label{sec:Case 5}
The structure of a graph sets it apart from natural language, and the model ability to comprehend these structures is vital for enhancing its performance on graph tasks. In this section, we explore the ability of instruction-tuned LLMs to understand graph structures.
%In the previous cases, we discussed LLMs’ potential to understand graph information. However, due to the influence of node attributes, it’s unclear to what extent LLMs truly grasp the underlying graph structures. In this part, we aim to explore their ability to comprehend graph structures more directly.

\subsubsection{Experiment Settings}
We remove all node attributes and retain only node IDs to eliminate the influence of attributes on LLM reasoning. Examples of these prompt formats are provided in Appendix \ref{sec:Prompt Formats for Pure Graph Structure}.


\begin{figure}[]
  \centering
  \includegraphics[width=1\linewidth]{figs/node_classification_pure_structure.pdf}
  %\vspace{-20pt}
  \caption{LLM performance on node classification without node attributes}
  \label{fig:LLM performance on node classification without node attributes}
\end{figure}

\subsubsection{Results}
\paragraph{\underline{Node classification}}
We present the results of node classification in Figure \ref{fig:LLM performance on node classification without node attributes}. “Original” refers to Llama3B or Llama8B without parameter optimization, while “1-hop” and “2-hop” correspond to 1-hop w/o label and 2-hop w/o label, respectively. From the figure, we observe that off-the-shelf LLMs perform similarly to random guessing in node classification. For instance, with 7 classes in Cora, the probability of random guessing correctly is 14.28\%, and the experimental results align closely with this probability. This is because LLMs struggle to make accurate predictions based purely on graph structure without semantic information. After instruction tuning, LLMs start to learn some graph structural information, leading to improved accuracy. However, the improvement is limited, likely because the classes in these datasets are strongly correlated with node features, and the graph structural differences between categories are minimal. This explains why simpler models like MLPs \cite{hu2021graph} and our ego prompt format perform relatively well, as they rely more on the node features than on the graph structure itself.

\begin{table}[htbp]
\centering
\caption{LLM performance on link prediction without node attributes. Llama3B w attributes and Llama8B w attributes are for comparison.}
\label{tab:LLM performance on link prediction without node attributes}
\scalebox{0.7}{%
\begin{tabular}{l c c c c c c}
\toprule
\rowcolor{gray!10}
\textbf{Models} & \textbf{Prompts} & \textbf{Cora} & \textbf{PubMed} & \textbf{ArXiv} & \textbf{Products} & \textbf{Avg} \\ 
\midrule
\multirow{2}{*}{Llama3B w attributes} & 1-hop & 72.97 & 71.55 & 72.45 & 78.92 & 73.97 \\
& 2-hop & 68.21 & 59.95 & 68.55 & 79.17 & 68.97 \\
\midrule

\multirow{2}{*}{Llama8B w attributes} & 1-hop & 80.44 & 74.80 & 87.80 & 85.29 & 82.08 \\
& 2-hop & 89.39 \cellcolor{cyan!20} & 77.30 & 92.30 \cellcolor{cyan!50} & 90.77 \cellcolor{cyan!20} & 87.44 \cellcolor{cyan!20} \\
\midrule
\multirow{2}{*}{Llama3B w/o attributes} & 1-hop & 66.61 & 55.44 & 64.94 & 78.47 & 66.37 \\
& 2-hop & 72.22 & 58.62 & 65.62 & 74.52 & 67.75 \\
\midrule
\multirow{2}{*}{Llama8B w/o attributes} & 1-hop & 63.19 & 55.81 & 68.62 & 81.30 & 67.23 \\
& 2-hop & 85.58 & 69.50 & 84.88 & 87.78 & 81.94 \\
\midrule
\multirow{2}{*}{tuned Llama3B w/o attributes} & 1-hop & 75.88 & 74.70 & 78.30 & 77.38 & 76.57 \\
& 2-hop & 93.20 \cellcolor{cyan!50} & 97.66 \cellcolor{cyan!100} & 89.00 & 94.09 \cellcolor{cyan!50} & 93.49 \cellcolor{cyan!50} \\
\midrule
\multirow{2}{*}{tuned Llama8B w/o attributes} & 1-hop & 85.15 & 78.81 \cellcolor{cyan!20} & 89.34 \cellcolor{cyan!20} & 87.98 & 85.32 \\
& 2-hop & 94.11 \cellcolor{cyan!100} & 97.44 \cellcolor{cyan!50} & 93.67 \cellcolor{cyan!100} & 94.54 \cellcolor{cyan!100} & 94.94 \cellcolor{cyan!100} \\
\bottomrule
\end{tabular}
}
\end{table}


\paragraph{\underline{Link prediction}}
From Table \ref{tab:LLM performance on link prediction without node attributes}, we observe that LLMs with node attributes outperform those without, highlighting the positive role of node attributes in LLM reasoning. However, after instruction tuning without node attributes, the LLMs show a significant improvement in link prediction accuracy. This demonstrates that LLMs can effectively learn and understand graph structures, achieving high link prediction accuracy even in the absence of node attributes.


\begin{remark}
LLMs can learn graph structures effectively through instruction tuning. While node attributes improve performance, LLMs can still achieve high accuracy in link prediction by leveraging structural information alone. However, the improvement in node classification is limited, likely because the classes are closely related to node features and the structural differences between categories are minimal.
\end{remark}

\paragraph{\textbf{Further Probing}}
In real-world applications, graphs may experience structural changes like reduced node similarity or missing edges. Understanding how LLMs handle these perturbations is key to evaluating their robustness and potential for deployment in dynamic graph-based environments where structural stability cannot always be assured. We discuss this aspect further in Appendix \ref{sec:Robustness of LLMs}.


%\ym{We could have an independent section "Discussion" to summarize our key insights and discuss their potential impact. Only do it when time allows.}