\section{Background}

\subsection{Transformer Primer}
The attention-based transformer architecture~\cite{vaswani:attention} underpins most 
large language models (LLMs) today. 
A typical LLM comprises multiple transformer layers, each containing an attention module. 
This module maps input vectors to query, key, and value vectors, 
which it then combines to produce an output vector.
%
Specifically, when an attention module receives input vectors, or a sequence of hidden states 
$(x_1,x_2,...x_n)\in \mathbb{R}^{n \times d}$, where $n$ is the number of tokens in the sequence, 
and $d$ is the hidden dimension, it computes the key, query, and value vectors for each token as follows: 
$$k_i=W_kx_i, q_i=W_qx_i, v_i=W_vx_i.$$
%
where $W_k, W_q$, and $W_v$ are learnable weight matrices.  
%
The attention module then computes the attention score between tokens as follows:
%
$$a_{ij}=\frac{\text{exp}(q_i^{\top}k_j/\sqrt{d})}{\sum_{t=1}^{i}\text{exp}(q_i^{\top}k_t/\sqrt{d})}$$
%
Finally, the attention module computes the output vectors as weighted sums of the value vectors: 
$$o_i=\sum_{j=1}^{i}a_{ij}v_{j}$$
The output of the attention module can either serve as the input to the next layer or act as the final output.
\subsection{Multimodal Transformer}
% Since the attention-based transformer architecture is classical, we leave its introduction to Appendix~\ref{background}, and primarily describe the prevalent multimodal transformer here.
While the original transformer architecture~\cite{vaswani:attention} was initially developed for natural language processing (NLP) tasks, its attention-based mechanism has been successfully extended to a wide range of multi-modal applications. In the multi-modal setting, the key challenge is to effectively combine information from heterogeneous modalities, each with distinct structures and representations. Existing approaches address this by using modality-specific encoders. For example, in vision-language tasks, one encoder processes image features (often using convolutional neural networks or vision transformers), while another processes textual input, such as captions or queries. These encoders project modality-specific features into a shared embedding space, enabling the transformer model to process embeddings from different modalities uniformly.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figs/Illustration.pdf}
    \caption{Illustration of three processing methods. \textbf{Left part}: A query consists of multiple images and parts of text. Suppose that the first sentence of the query does not match any other previous queries, and the KV caches of images are stored in advance. \textit{Prefix caching} reuses the KV cache of system prompt, and recomputes that of the query. \textit{Full reuse} recomputes the KV cache of text, and reuses that of multimodal information. \textit{Partial reuse} recomputes more tokens compared to \textit{Full reuse}. \textbf{Right part}: \textit{Partial reuse} accelerates MLLM inference through reusing the KV caches of the most image tokens, while maintaining generation quality by selectively recomputing.}
    \label{fig:illustration}
\end{figure*}
\subsection{Autoregressive Generation \& KV Cache}
%
LLMs generate tokens autoregressively, predicting one token at a time based on the input. 
This process involves two phases: the prefill phase and the decode phase.
%
In the \textbf{prefill} phase, LLMs process the entire input prompt $(x_1, x_2, \dots, x_n)$, 
computing and caching the key and value vectors for each token. This phase can be slow for long inputs, 
and the time to generate the first token is measured by the Time-to-First-Token (TTFT) metric.
%
In the \textbf{decode} phase, LLMs generate one token at a time. 
The model computes the probability of the next token $x_{n+1}$, 
selects the most likely token, and appends its key and value vectors to the KV cache. 

The KV cache~\cite{pope2023}, 
which stores the key and value vectors computed by the attention module, 
accelerates generation by enabling intra-request and inter-request reuse. 
First, within a single request, the cache speeds up token generation by allowing the model 
to reuse cached KV vectors, thus only processing the new token instead of recalculating 
vectors for the entire sequence. Second, when a new request shares a prefix with a previous one, 
the KV cache for the prefix can be reused, thereby accelerating the prefill phase of the new request.

\subsection{Context Caching}

To better use KV cache, existing approaches propose context caching (CC) that exploits inter-request dependency to reuse KV cache across inference requests to avoid repeated computation of the same prompt tokens~\cite{kwon2023efficient, zheng2024sglang, yao2024cacheblend, hu2024epic}. There are two types of context caching. First, in \textbf{prefix-based context caching}, only the initial portion of the sequence is cached and reused. Almost all existing CC offerings and designs are prefix-based~\cite{kwon2023efficient, zheng2024sglang, hu2024memserve, zhong2024}. However, in this approach, if the prefix differs slightly (e.g., one or two words are different at the beginning), the entire cache cannot be reused, forcing the model to recompute all the key-value pairs for that request. This inefficiency can be particularly problematic in cases where many requests share a substantial amount of overlapping context but differ slightly in their initial tokens (e.g., in RAG).
%
Second, the \textbf{position-independent context caching }addresses the limitation of prefix-based caching by enabling KV cache reuse across requests, even when token sequences differ, without being constrained by the shared prefix. The position-independent approach was a new concept and was explored in two CacheBlend~\cite{yao2024cacheblend} and EPIC~\cite{hu2024epic} on Natural Language Processing (NLP) tasks. Our work is the first to explore a solution to the PIC problem in the multi-modality field\footnote{Specifically in this paper, we only discuss two modalities: texts and images. But our method is applicable to all modalities.}.