\section{Related Work}

% \hjh{Please add, delete, revise the ``our work'' part of each related work section}

\textbf{LLM Serving Optimizations.}
%
Several serving systems emerged in the past year. vLLM~\cite{kwon2023efficient} is a pioneering work in this space featuring PagedAttention for higher throughput. 
%
SGlang~\cite{zheng2024sglang} is another serving system featuring a novel frontend language and a backend runtime. SGlang proposes three novel techniques: RadixAttention, a compressed finite state machine, and API speculative execution.
Aside from full-systems, there are also many scheduling optimizations such as disaggregated prefill and decode~\cite{zhong2024, hu2024memserve, tetriinfer-2024, patel2024}, continuous batching~\cite{yu2022}, multi-lora~\cite{sheng2023s, li2024caraserve}, etc.

\textbf{Context Caching (CC).} Context Caching (CC) has two main categories. The first is Position-dependent caching, which can be further divided into prefix-based caching~\cite{yu2023stateful-pensieve, zheng2024sglang} and modular caching~\cite{gim2024prompt}. By mid-2024, vendors such as Kimi~\cite{kimi_api} and Gemini~\cite{gemini} began incorporating explicit CC features into their systems. The second category is Position-Independent Caching (PIC). To the best of our knowledge, CacheBlend~\cite{yao2024cacheblend} represents the first work addressing aspects of PIC, while EPIC~\cite{hu2024epic} formally defined the PIC problem and advances the state-of-the-art one step forward. However, these two pieces of work focus mainly on text-based applications. They disregard the cases like interleaved text and images, resulting in a two-step process during inference. In this paper, we revisit the PIC problem in multi-modal tasks, and design selective attention mechanism to process the prefill phase of queries in single step.
 
\textbf{Retrieval-Augmented-Generation (RAG).} RAG is a technique that combines retrieval-based methods with generation-based models to improve the performance of LLMs. It aims to address the limitations of purely generative models, which can sometimes lack factual accuracy or struggle with long-context understanding~\cite{li2022survey,jin2024ragcache,gao2023retrieval,jeong2024adaptive,ram2023context,mao2020generation}. For example, Adaptive-RAG~\cite{jeong2024adaptive} develops a dynamic framework to select the most suitable strategy for LLMs to deal with queries based on their complexity. To classify queries of different complexity, Adaptive-RAG trains a small model as a classifier to predict the complexity of queries. RAGCache~\cite{jin2024ragcache} proposes a RAG system that can cache the intermediate states of external knowledge and reuse them in multiple queries to reduce the latency. We also incorporate the RAG into \sys, and enhance its efficiency through the reuse of KV cache.

%\text{Long text interface}
%~\cite{dong2024get,jiang2023llmlingua,liu2024scissorhands,zhang2024h2o,xiao2023sink,liu2023cachegen}

\textbf{Sparsity.}
Sparsity is essential for improving long-context inference and can be divided into two types: dynamic and static. 
Dynamic sparsity (e.g., H2O~\cite{zhang2024h2o}, Quest~\cite{Quest-ICML24}) adapts in real-time by identifying and filtering out less important query-key connections as sequences are processed. 
In contrast, static sparsity (e.g., Longformer~\cite{longformer}) relies on predefined sparse patterns, which simplifies implementation but reduces flexibility.
%
In our work, we leverage static sparsity to solve the PIC problem.