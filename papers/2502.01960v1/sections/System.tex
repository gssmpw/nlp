\section{System Design and Implementation}
\subsection{Challenge of multimodal information management}
When dealing with multimodal information, such as images and videos, the size of the KV cache can be significantly larger than that of text information due to the higher complexity of multimodal data. The KV cache size of a single image can be as large as 1 GB, calculated as 2 (K and V tensors) $\times$ 2048 (number of image tokens) $\times$ 32 (number of layers) $\times$ 4096 (hidden state size) $\times$ 2 (bytes per FP16). Note that high-resolution images take up more storage space. For example, Qwen2-VL maps an image with 8204$\times$1092 pixels into 11427 image tokens \cite{wang2024qwen2vl}.

Given that the model parameters and the KV cache of current requests must be placed on GPU memory, there is insufficient space left for multimodal information caching. Consequently, they are mostly stored in CPU memory or even on the disk. This poses a challenge to multimodal data management and transfer.
\subsection{System overview}
This section presents the architecture of \sys. We will start with the key components of \sys, and then introduce the workflow based on \figurename~\ref{fig:infoblend}.

There are five key components in \sys. (1) \textbf{MLLM inference serving system} is responsible for generating output tokens in multiple steps (commonly referred to as the decode phase \cite{zhong2024}). It also contains a scheduler that manages users' queries. The MLLM subsystem incorporates advanced techniques such as PagedAttention \cite{kwon2023efficient} and continuous batching \cite{yu2022}. (2) \textbf{Static Library} stores the KV cache of files uploaded by users. The files from different users are logically separated. Each user can access only his/her own files. It is relatively static, as it can only be modified by the users. This component is analogous to the static-linked code: Users refer to these files in their queries, and \sys~links the KV cache of these files for the MLLM to inference. (3) \textbf{Dynamic Library} serves as the storage for multimedia references and related KV cache. This component is prepared for MRAG, for the sake of enhancing MLLM's quality and factuality. It is relatively dynamic, since the administrator of \sys~can update the references periodically according to the demand of applications. This component is analogous to the dynamic-linked library: The MLLM retrieves the references during decode phase, when it determines that a retrieval is required \cite{asai2023selfrag, jeong2024adaptive}. (4) \textbf{Retriever} is responsible for searching the relevant information based on the query. It is analogous to the relocation table when executing a program, in that the program needs the relocation table to find the address of dynamic-linked libraries. (5) \textbf{Linker} links the KV cache of multimodal information to users' queries. Linking without accuracy degradation is an algorithm-level challenge, and we will address this challenge through selective attention mechanism in the next section.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/InfoBlend.pdf}
    \caption{The Architecture of \sys~Serving System.}
    \label{fig:infoblend}
\end{figure}

The serving workflow of \sys~is outlined as follows. \textcircled{1} Users upload their respective files, and \sys~subsequently performs the computation of the KV cache, which is then stored in GPU memory for the purpose of serving. Concurrently, these caches are copied to disks and subsequently deleted following the expiration of their designated timeframe. \textcircled{2} The user submits a query, including questions and references to specific files. \textcircled{3} The MLLM accesses the files according to the user's ID and references. \textcircled{4} The retriever executes MRAG when triggered by the MLLM. \textcircled{5} Linker blends the KV caches together and sends them to the MLLM as an input. \textcircled{6} The MLLM generates an answer to respond to the user.

\subsection{KV cache transfer in parallel}
Normally, the KV caches of files from active users are loaded on the memory of computing chip (e.g., GPU, TPU, NPU, etc.). When the stored KV caches are not on the chip, we design a parallel transfer mechanism as shown in \figurename~\ref{fig:parallel}.
\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/parallel.pdf}
    \caption{Load and compute the KV cache of images in parallel. Note that we simply utilize images as an example here. The parallelization is applicable to the other forms of multimodal data as well.}
    \label{fig:parallel}
\end{figure}

Suppose that the input includes $n$ images. \sys~looks up the KV caches of these images at the beginning of processing. The KV caches of $m$ images are missing, due to deletion after expiration. The KV caches of remaining $n-m$ images are hit. Some of them are on GPU memory, while others are on CPU memory or disks. Subsequently, the computation and transfer of KV caches are executed concurrently, for the purpose of reducing the preparation time.

Other optimization techniques such as layer-wise transfer \cite{patel2024} and KV cache compression \cite{liu2024} are orthogonal to our work. They can be employed in our system as well.