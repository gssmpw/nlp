\section{Evaluation}
% In light of experiments of CacheBlend (\S\ref{eval:1}) and EPIC (\S\ref{eval:2}), we design our experiments (\S\ref{eval:3}).

% \noindent\textbf{LLM Dataset.} 2WikiMQA, MuSiQue, HotpotQA, SAMSum, MultiNews.

% \noindent\textbf{LLM Baselines.} Full KV recompute, Prefix caching, Full KV reuse, CacheBlend, EPIC.
% \subsection{CacheBlend evaluation}\label{eval:1}
% \begin{itemize}
%     \item TTFT-Score Comparison.
%     \item RPS-TTFT Comparison.
%     \item Sensitivity Analysis. (1) chunk number; (2) chunk length; (3) batch size; (4) recompute ratio; (5) storage device (CPU RAM / slower Disk).
% \end{itemize}
% \subsection{EPIC evaluation}\label{eval:2}
% \begin{itemize}
%     \item TTFT-Score Comparison.
%     \item (CCR+RPS)-TTFT/Throughput Comparison.
%     \item Context length-TTFT Comparison.
%     \item Semantic-based / fixed-token-based splitting.
% \end{itemize}
% \subsection{\sys}\label{eval:3}\
% \noindent\textbf{VLM Model.} InternVL 2.5-8B \cite{chen2024internvl}, Qwen2-VL-7B \cite{wang2024qwen2vl}, LLaVA-1.6-vicuna-7B, LLaVA-1.6-Mistral-7B \cite{liu2024llavanext}.

% \noindent\textbf{VLM Dataset.} SparklesDialogueCC, SparklesDialogueVG \cite{huang2024sparkles}, MMDU \cite{liu2024mmdu}.

% \noindent\textbf{VLM Baselines.} CacheBlend, Prefix caching, Full KV reuse, \sys.

% \begin{itemize}
%     \item TTFT-Score Comparison.
%     \item RPS-TTFT/Throughput Comparison.
%     \item Sensitivity Analysis: Image number.
%     \item Why does CacheBlend fail to work when serving MLLM?
% \end{itemize}

In this section, we evaluate \sys~in terms of response time and generation quality. We also investigate whether \sys~is applicable when the number of images is large.
\subsection{Experimental settings}
We select two prevalent MLLMs in the experiments: LLaVA-1.6-vicuna-7B and LLaVA-1.6-mistral-7B \cite{liu2024llavanext}. All experiments are run on a server with 1 NVIDIA H800-80 GB GPU, 20-core Intel(R) Xeon(R) Platinum CPUs, and 100GB DRAM.

Two datasets are used in our evaluation. (1) \textbf{MMDU} \cite{liu2024mmdu}: This dataset aims to evaluate MLLMs' abilities in multi-turn and multi-image conversations. Each conversation stitches together multiple images and sentence-level text (e.g., ``IMAGE\#1, IMAGE\#2. Can you describe these images as detailed as possible?"). (2) \textbf{SparklesEval} \cite{huang2024sparkles}: This is also a dataset for assessing MLLMs' conversational competence across multiple images and conversation turns. Unlike MMDU, SparklesEval integrates multiple images at word level (e.g., ``Can you link the celebration occurring in IMAGE\#1 and the dirt bike race in IMAGE\#2 ?"). As shown in the examples, the prompts of two datasets are open questions. Previous works adopt GPT score to evaluate the quality of MLLMs' responses to the open questions \cite{liu2024mmdu, huang2024sparkles}. GPT score is a GPT-assisted evaluation that uses a powerful judge model (e.g., GPT-4o, Qwen, etc.) to assess the answers. We also employ this metric and their evaluation prompt, as listed in Appendix~\ref{prompt}.
% (3) \textbf{V*Bench} \cite{wu2024v}:  A dataset specifically designed to evaluate
% MLLMs in their ability to process high-resolution images and focus on visual details. Each sample contains a high-resolution image, a question, and four options.
% We select 100 samples from each of the above datasets for testing, each including 1 to 5 images.

% We use the following metrics to measure the performance of algorithms. (1) Time-To-First-Token (TTFT) refers to the time it takes for LLMs, to generate and return the first token after receiving an request. This metric is designed to measure the time spent in the prefill stage, which can be optimized by addressing the PIC problem. (2) GPT score \cite{liu2024mmdu, huang2024sparkles} is a GPT-assited evaluation  that uses a judge model (e.g., GPT-4o, Qwen, etc.) to assess the quality of model-generated responses. We employ this metric to assess the quality of MLLMs' responses to the open questions in MMDU and SparklesEval. We apply the evaluation prompts in MMDU \cite{liu2024mmdu} to guide the judge model for scoring in the range of 10. 

% (3) F1 score is a metric used to evaluate the similarity between MLLMsâ€™ output and the groundtruth answer. We employ this metric to assess the accuracy of the MLLMs' answers to the multiple-choice questions in V*Bench.

We compare \sys-$k$ with three existing CC algorithms: prefix caching, full reuse, and CacheBlend \cite{yao2024cacheblend}. CacheBlend is also a position-independent algorithm designed for RAG system. It recomputes $r$\% of total tokens with largest KV deviation, so we denote it as CacheBlend-$r$. The primary focus of CacheBlend is the KV deviation, while the \sys's selection process involves the identification of tokens that exhibit both high attention scores and significant KV deviation. We implement the four CC algorithms based on vLLM 0.6.4 \cite{kwon2023efficient}.

% (1) Prefix Caching: This algorithm merely stores and reuses the KV chche of the prefix. And the KV cache of non-prefix tokens needs to be computed during prefill. (2) Full Reuse: This algorithm reduces TTFT by fully reusing the entire KV cache regardless of the position of multimodal data. (3) CacheBlend \cite{yao2024cacheblend}: This is a state-of-the-art partial reuse algorithm that achieves a trade-off between TTFT and generation quality by dynamically selecting partial tokens to recompute.
% Additionally, we evaluate various variants of CacheBlend, denoted as CacheBlend-r, where $r$ represents the ratio of tokens recomputed. Similarly, we test different variants of InfoBlend, denoted as InfoBlend-k, where $k$ indicates the number of tokens recomputed at each chunk boundary.

\subsection{Effectiveness of \sys}
Based on vLLM offline inference, we compare the performance of all algorithms. Specifically, we process all requests sequentially and evaluate their generation quality and processing time for prefill. The workflow initiates with the precomputation of the relevant KV cache for images. Subsequently, we send the user's query along with the cache\_ids of the images to the serving system. Prefix caching will process the query with the KV cache of system prompt only. \sys~concatenates the dummy cache and stored cache, and computes the first output token using selective attention mechanism in single step. Full reuse and CacheBlend first compute the KV cache of text, and then produce the first output token with the concatenated KV cache. We record the processing time of the algorithms and finally score for each response.
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/legend_result.pdf}
    % \vskip -0.2in
    \includegraphics[width=\columnwidth]{figs/results.pdf}
    \caption{Comparison of TTFT ($\downarrow$ Better) and Score ($\uparrow$ Better) using different models on different datasets. }
    \label{fig:ttft-score}
    % \vskip -0.2in
\end{figure}

\figurename~\ref{fig:ttft-score} presents the experimental results of all algorithms across different models and datasets. The results indicate that \sys~consistently outperforms CacheBlend in terms of both TTFT and score across various configurations. \sys-32 reduces TTFT by up to 54.1\% while maintaining a loss of score within 13.6\% compared to prefix caching. Additionally, it is clear that \sys~exhibits a slight decrease in TTFT compared to full reuse, since \sys~is a single-step process. Overall, compared to other algorithms, \sys~achieves the best trade-off between TTFT and score.

\subsection{Sensitivity analysis}
In order to achieve a more profound comprehension of \sys, a subsequent analysis is necessary to ascertain how the number of images impacts overall performance. We divide the dataset of MMDU into 10 groups in terms of the number of images. We evaluate the TTFT and score of \sys~and baselines on each group. The average value of results are shown in \figurename~\ref{fig:10}. The TTFT of \sys~is consistently shorter than that of prefix caching. When the number of images is 10, \sys~achieves 54.7\% reduction in TTFT. Furthermore, the performance of \sys~remains unaffected by the number of images, exhibiting negligible or no accuracy degradation.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{figs/legend_image_num.pdf}
    \vskip -0.2in
    \subfloat[]{
        \includegraphics[width=0.42\columnwidth]{figs/TTFT_all.pdf}
        \label{fig:10a}
    }
    \subfloat[]{
        \includegraphics[width=0.4\columnwidth]{figs/Score_all.pdf}
        \label{fig:10b}
    }
    \caption{The performance of \sys~as the number of images increases. For clarity, we only present the results of \sys-32. Other variants of \sys~show similar patterns.}
    \label{fig:10}
    % \vskip -0.2in
\end{figure}

% \subsection{Latency and throughput performance of InfoBlend}
% To assess Infoblend's latency and throughput performance, we leverage VLLM's OpenAI-compatible API server to simulate real-world user request patterns. We first select $n$ samples from MMDU and pre-generate KV caches for their contexts. Subsequently, we simulate user request behavior by repeatedly sending the user queries along with the cache\_ids of these 
% $n$ samples at a specified request rate over a period of time. by varying the request rate, We measure the latency and throughput across different experimental conditions.

% In Figure, we present a comparison of latency and throughput between InfoBlend and CacheBlend at varying request rates.  Compared to CacheBlend, InfoBlend achieves up to 80\% reduction in TTFT and 2-3 $\times$ improvement in throughput. This gap increases as the request rate rises.