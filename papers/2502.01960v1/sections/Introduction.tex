\section{Introduction}

Recent years have witnessed notable development in applications based on Multimodal Large Language Model (MLLM), including those for code generation \cite{zhu2023minigpt4}, graphical user interface agents \cite{hong_2024_CVPR, zhang2023appagent}, and medical image understanding \cite{li2023, zhang2023pmcvqa}. To enhance the perceptual capabilities of MLLM, the number of processed image tokens has increased considerably, from 576 in LLaVA 1.5 \cite{liu_2024_CVPR} to 2304 in LLaVA 1.6 \cite{liu2024llavanext}. This significant expansion in the number of tokens has the adverse effect of slowing down MLLM inference and constraining the performance of applications. This highlights the necessity for reducing latency and enhancing throughput.

In order to reduce the computational overhead, Context Caching (CC) is a prevalent technique employed by numerous prominent platforms, including Google Gemini \cite{gemini}, Moonshot Kimi \cite{kimi_api}, DeepSeek \cite{deepseek_api}, vLLM \cite{kwon2023efficient}, and SGLang \cite{zheng2024sglang}. As users may access the same text or image context on multiple occasions, the intermediate results (commonly referred to as KV cache \cite{pope2023}) of these information items can be stored for reuse. To illustrate, Gemini offers a CC API \cite{gemini}, which allows users to upload a video file in advance. Subsequently, the KV cache of the file and system prompt\footnote{System prompt is a set of instructions or guidelines that inform the model about its role, the nature of the task, and the expected behavior.} is pre-computed, and reused when responding to the user's query. In this manner, CC reduces the response time of MLLM, namely the Time-to-First-Token (TTFT).

However, all of the aforementioned CC systems merely reuse the KV cache of the initial sequence of tokens (the prefix). Given the autoregressive nature of MLLM, the KV value of each token depends on the preceding tokens. If the prefix of a query differs slightly from that of a previous query, the majority of existing CC systems will cease attempting to reuse the stored KV cache and instead recompute it for the new query. Such inefficiencies can prove particularly problematic in cases where interleaved text and images \cite{huang2024sparkles} are concerned, as well as in the context of Multimodal Retrieval-Augmented Generation (MRAG\footnote{MRAG refers to a retriever that retrieves multimodal data.}) \cite{wei2024uniir}. Suppose there is a dialogue as shown in \figurename~\ref{fig:example}. If a subsequent query is the same as this one, except that it starts with ``\textit{We're planing to ...}", then the entire KV cache cannot be reused. The use of interleaved text and images is a widespread phenomenon on the Internet, particularly in the context of blogs and news media. MRAG fetches relevant information to meet specific user requirements, which is also an important function for applications.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/Dialogue.pdf}
    \caption{An example of a dialogue between a user and MLLM assistant. The first round of dialogue represents an example of interleaving text and images, whereas the second round of dialogue is an example of information retrieval. IMAGE\#EIFEL2025 and IMAGE\#LOUVRE2025 are two images uploaded by the user, while IMAGE\#HOTEL01 and IMAGE\#HOTEL02 are two images fetched from external websites by MLLM assistant. In both examples, the KV cache of images cannot be reused by prefix-based CC systems, as the opening words may differ.}
    \label{fig:example}
\end{figure}

In this paper, we explore the possibilities of position-independent CC systems. We start with analyzing the limitations of two existing approaches: prefix caching and full reuse. Experimental results show that full reuse can save up to 69.4\% in TTFT, while concurrently leading to a substantial decline in generation quality. Consequently, we propose partial reuse of KV cache as a trade-off between the two approaches.

The implementation of partial reuse faces both system-level and algorithm-level challenges. First, the size of a single image's KV cache can reach 1 GB, so the majority of KV caches are stored on the local disk. Second, it is crucial and challenging to avoid quality degradation when reusing KV cache. Third, recent studies on RAG and LLM serving systems \cite{gim2024prompt, hu2024epic, yao2024cacheblend} undergo a two-step process in the field of MLLM, which is inefficient during MLLM serving.

We address the challenges by designing a system named \sys. Analogous to classical position-independent code, \sys~allows the KV caches to be reused at any position within a prompt, not merely at the prefix. To maintain generation quality, we have analyzed the attention mechanism of image tokens thoroughly, and select which tokens should be recomputed. Finally, we design and implement the selective attention mechanism to reuse the KV caches in single step.

Evaluations on multiple MLLMs and datasets illustrate that \sys~saves TTFT by 54.1\% compared to prefix caching, with accuracy loss within 13.6\%. In addition, the generation quality of \sys~does not degrade as the number of images grows.

% However, the primary challenge with this approach is accuracy degradation, which arises from violating the attention mechanism in transformer models.