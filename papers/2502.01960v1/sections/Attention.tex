\section{Implementation of Selective Attention}
In order to partially reuse the KV caches, we implement the selective attention mechanism. Only the selected tokens are passed into the MLLM, while they need to calculate the attention score with other tokens. We first introduce this mechanism, and then explain how to select tokens.
\subsection{Illustration of selective attention}
The process is illustrated in \figurename~\ref{fig:7}. $W_K$ and $W_Q$ in the figure are the parameters of MLLM. The recomputed K tensor substitutes part of the K cache for calculating the attention matrix. Here we utilize the tricky ``\textit{dummy cache}". Since the KV cache of text is not saved in advance, the tokens of text must be computed. In other words, the tokens of text are part of selected tokens. The KV cache of selected tokens will be replaced, so we do not need to pre-compute the KV cache of text, and instead fill its KV cache with zeros. In this way, the selective attention mechanism is a single-step process. This is more efficient than the two-step process of \textit{full reuse}. Our experiment in the next section exhibits the efficiency.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/Figure7.pdf}
    \caption{Illustration of selective attention. First, select the important tokens. Second, replace the respective K cache with generated $K$. Third, multiply $Q$ with $K_\text{link}^\top$ to obtain the attention matrix. The V cache also undergoes the same operation. The details such as positional embedding and softmax are omitted in the figure.}
    \label{fig:7}
\end{figure}

\subsection{Selecting important tokens}
As analyzed in \S~\ref{sec:moti}, the attention matrix is sparse, and only 5\% of tokens receive significant attention scores. In this section, we determine which tokens are important and require recomputation through a motivating experiment.

In the experiment, we compute two KV caches of a single image with different positions. Specifically, the first KV cache is created when the image is inserted before a question, while the second KV cache is created with the reverse order. We focus on the distance between the two KV caches. To find the tokens with large distance, we sort the image tokens by their K distance, and count the number of transformer layers where the token is in the top 50, as shown in \figurename~\ref{fig:imp_tokens}. We conclude the finding of the experiment as Insight~\ref{ins:3}.

\begin{insight}\label{ins:3}
    The tokens at the beginning of all image tokens exhibit a greater disparity in terms of the KV distance between reused KV tensor and recomputed KV tensor.
\end{insight}

In summary, the tokens at the beginning are more different (Insight~\ref{ins:3}) and receive more attention (Insight~\ref{ins:2}). Consequently, we select all text tokens and $k$ tokens at the beginning of image tokens to be recomputed in the selective attention mechanism. This method is referred to as \sys-$k$. Our evaluations in the next section verify the effectiveness of \sys-$k$.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/imp_tokens.pdf}
    \caption{Important tokens. The K distance of a token is defined as the L1 distance between its two K tensors. For clarity, we only show tokens whose number is greater than 24.}
    \label{fig:imp_tokens}
\end{figure}