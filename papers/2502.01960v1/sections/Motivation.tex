\section{Motivation}

\subsection{Benefits of Context Caching (CC)}

The CC technique has been shown to enhance the efficiency and performance of the MLLM through the storage and reuse of the context of prior interactions. In a video analysis task, for instance, the user is likely to refer to the video multiple times, and reusing the KV cache of the video can reduce the response time and enhance the user experience, especially when the video is of considerable length. Similar beneficial scenarios for CC include code analysis with repeated repository references, and Q\&A assistants with a collection of pictures.

The adoption of CC techniques offers notable advantages for both service platforms and users. For service platforms, CC contributes to a reduction in computational overhead, enabling providers to accommodate a greater number of users. Consequently, MLLM service providers actively promote the use of CC by users. For example, DeepSeek cuts API costs by up to 90\% for cache hits \cite{deepseek_api}. This incentivizes users to employ the CC API in pursuit of reduced costs.


\subsection{Limitations of two naive approaches}

At present, nearly all CC systems are of the prefix-based variety. In the remainder of this paper, the term ``\textit{prefix caching}'' is employed to denote the prefix-based CC system, as it merely stores and reuses the KV cache of the prefix. This approach is constrained by the necessity of an exact match in the prefix tokens across requests. 
We illustrate this phenomenon in the left part of \figurename~\ref{fig:illustration}. When the prefix of a new query differs from that of any other previous query, prefix caching recomputes all the tokens except the system prompt. This turns out to be inefficient, since the number of tokens of multimodal information is considerably greater than that of text. When the user's query becomes longer, prefix caching will be nearly as slow as computing without the CC system.

In order to explore the potential for reducing TTFT, we implemented a naive approach called ``\textit{full reuse}" which reuses the entire KV cache regardless of the position of multimodal data. Nevertheless, the text input of the user is not cached as it is unpredictable. As shown in the left part of \figurename~\ref{fig:illustration}, full reuse first recomputes the KV cache of text, and then concatenates it with stored KV caches, and finally computes the first output token with all KV caches. This approach is analogous to Prompt Cache \cite{gim2024prompt}. We conduct an experiment to compare prefix caching and full reuse, and the results are shown in \figurename~\ref{fig:3}. The TTFT of prefix caching grows quadratically with respect to the number of images, since the computational complexity of the attention mechanism is $O(n^2)$, where $n$ is the length of the prompt. In contrast, the TTFT of full reuse grows slowly, due to the reuse of KV cache. When the number of image is large, full reuse can reduce the TTFT by 69.4\% compared to prefix caching.

However, it is clear that full reuse violate the attention mechanism in transformer models. The stored KV cache differs from the required one due to the autoregressive nature of transformer. \figurename~\ref{fig:3b} illustrates that full reuse degrades the generation quality significantly. As the number of images increases, this approach is incapable of producing any meaningful answers. Moreover, full reuse is a two-step process: The first step is to calculate the KV cache of text, and the second step is to calculate the first output token using the concatenated KV cache. This triggers the LLM engine twice, introducing additional time overhead from a system perspective. \figurename~\ref{fig:3a} demonstrates this inefficiency. When the number of images is 1, the TTFT of full reuse is larger than that of prefix caching, due to the two-step process.

In summary, prefix caching is inefficient in decreasing TTFT, while full reuse exhibits poor performance in generation quality. In the rest of this paper, we aim to find a trade-off between prefix caching and full reuse.
\subsection{Opportunities of partial reuse}\label{sec:moti}
This section explores the possibility of improving generation quality through minor modifications to the stored KV caches. Recall that the KV cache of multimodal data is computed through modality encoder, connector, and self-attention. We posit that the KV cache contains the majority of the multimodal information, except for position information and cross-attention with other inputs. Our goal is to blend the missing position knowledge into the KV cache. This goal can be achieved through integrated reuse and recompute mechanism, named ``\textit{partial reuse}". As illustrated in \figurename~\ref{fig:illustration}, partial reuse recomputes a few tokens to mitigating accuracy degradation.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\columnwidth]{figs/legend3.2.pdf}
    \vskip -0.2in
    \subfloat[]{
        \includegraphics[width=0.42\columnwidth]{figs/TTFT3.2.pdf}
        \label{fig:3a}
    }
    \subfloat[]{
        \includegraphics[width=0.4\columnwidth]{figs/Score3.2.pdf}
        \label{fig:3b}
    }
    \caption{Comparison between prefix caching and full reuse in terms of TTFT and generation quality. The MLLM model is LLaVA-1.6-mistral-7B and the dataset is MMDU. The score in (b) is assessed by ChatGPT, which is a common practice in evaluating open questions.}
    \label{fig:3}
    % \vskip -0.2in
\end{figure}

In order to validate this idea, we conduct an experiment using the example in \figurename~\ref{fig:example}. The utilized MLLM is LLaVA-1.6-vicuna-7B \cite{liu2024llavanext}. It encodes the image ``IMAGE\#EIFFEL2025" into 1176 tokens. We collect the attention scores between these image tokens and the first output token from each transformer layer. Their values are normalized with softmax function. \figurename~\ref{fig:4a} shows the cumulative distribution function (CDF) of these attention scores. We find that less than 5\% of tokens receive more than $10^{-3}$ attention score. Since $10^{-3}$ is small, this means that less than 5\% of tokens affect the output. Many recent papers also point out this phenomenon \cite{zhang2024h2o, Quest-ICML24, longformer}. We conclude it as Insight~\ref{ins:1}.
\begin{insight}\label{ins:1}
    Attention matrix is extremely sparse.
\end{insight}

To find out which tokens receive more attention, we calculate the cumulative summation of attention scores as shown in \figurename~\ref{fig:4b}. It is evident that the first 500 tokens account for approximately 80\% of attention scores. This tendency aligns with the human attention mechanism, in which individuals tend to allocate more attention to the initial sentences of a paragraph. Prior work \cite{xiao2024} characterized this phenomenon as ``attention sinks", suggesting that the initial tokens tend to attract a disproportionate share of attention. We observed it through depicting the heatmap of attention matrix in Appendix~\ref{sinks}. We conclude this as Insight~\ref{ins:2}.
\begin{insight}\label{ins:2}
    The tokens at the beginning of all image tokens receive more attention.
\end{insight}

The results of the motivative experiment indicate that only a few tokens are of importance, and they typically occur at the beginning of the sequence. These insights are helpful for us to address the accuracy challenge.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figs/legend_cdf.pdf}
    \vskip -0.2in
    \subfloat[]{
        \includegraphics[width=0.45\columnwidth]{figs/attn_distribution.pdf}
        \label{fig:4a}
    }
    \subfloat[]{
        \includegraphics[width=0.45\columnwidth]{figs/cumulative_attn.pdf}
        \label{fig:4b}
    }
    \caption{(a) Distribution of all image tokens in terms of computed attention score with the first output token. Note that the x-axis is expressed on a log scale. (b) The summation of attention scores of the first $n$ image tokens. For the sake of clarity, we show only three representative layers, and other layers show similar patterns.}
    % \vskip -0.2in
\end{figure}