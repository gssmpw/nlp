%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}  % For advanced math features
\usepackage{amssymb}  % For \mathbb
\usepackage[switch]{lineno}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{array}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{forest}
\usepackage{xcolor}
\usepackage{makecell}
\usepackage{amssymb}


\definecolor{lightcoral}{rgb}{0.94, 0.5, 0.5}
\definecolor{lightgreen}{rgb}{0.56, 0.93, 0.56}
\definecolor{harvestgold}{rgb}{0.98, 0.85, 0.40}
\definecolor{brightlavender}{rgb}{0.75, 0.58, 0.89}
\definecolor{capri}{rgb}{0.0, 0.75, 1.0}
\definecolor{carminepink}{rgb}{0.92, 0.3, 0.26}
\definecolor{celadon}{rgb}{0.67, 0.88, 0.69}
\definecolor{darkpastelgreen}{rgb}{0.01, 0.75, 0.24}


\definecolor{hidden-draw}{RGB}{205, 44, 36}
\definecolor{hidden-blue}{RGB}{194,232,247}
\definecolor{hidden-orange}{RGB}{243,202,120}
\definecolor{hidden-yellow}{RGB}{242,244,193}
\definecolor{tree-level-1}{RGB}{245,20,85}
\definecolor{tree-level-2}{RGB}{246,86,118}
\definecolor{tree-level-3}{RGB}{248,177,193}
\definecolor{tree-leaf}{RGB}{176,230,198}

\definecolor{Self}{RGB}{255,0,128}
\definecolor{Ensemble}{RGB}{0,127,255}
\definecolor{Iterative}{RGB}{153,51,255}

\definecolor{exemplar1}{RGB}{136,98,148}
\definecolor{exemplar2}{RGB}{148,210,242}
\definecolor{knowledge1}{RGB}{249,219,152}
\definecolor{knowledge2}{RGB}{255,245,220}
\usepackage{xspace}
% \newcommand{\mllm}{(M)LLM\xspace}
\newcommand{\mllm}{LLM\xspace}

% USE THIS IN WHEN MENTIONING AUTHORS IN FLOWING TEXT
\newcommand{\mycite}[1]{\citeauthor{#1}~[\citeyear{#1}]}

% \usepackage{natbib} 
% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\setcounter{secnumdepth}{3}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{A Comprehensive Survey of Machine Unlearning Techniques \\ for Large Language Models}
% \title{A Comprehensive Survey of Large Language Model Unlearning}

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% % Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Jiahui Geng$^1$
\and
Qing Li$^1$\and
Herbert Woisetschl√§ger$^{2}$\And
Zongxiong Chen$^3$\\
Yuxia Wang$^1$ 
\and Preslav Nakov$^1$
\and Hans-Arno Jacobsen$^4$ 
\and Fakhri Karray$^1$
\\
\affiliations
$^1$Mohamed bin Zayed University of Artificial Intelligence \and
$^2$Technical University of Munich\\
$^3$Fraunhofer FOKUS\and
$^4$University of Toronto\\
\emails
\{jiahui.geng,qing.li,yuxia.wang,preslav.nakov,fakhri.karray\}@mbzuai.ac.ae,
herbert.woisetschlaeger@tum.de \and zongxiong.chen@fokus.fraunhofer.de \and
jacobsen@eecg.toronto.edu
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
% This study investigates the concept of the ``right to be forgotten" within the context of large language models, 
This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. 
Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.
% Despite increasing research interest, no comprehensive survey systematically organizes existing work and distills key insights. This study aims to bridge that gap. We begin by defining unlearning and its paradigms, followed by a comprehensive taxonomy of existing studies. Next, we categorize current unlearning approaches, analyzing their strengths and limitations. We also review evaluation metrics and benchmarks, offering a structured overview of current assessment methodologies. Finally, we highlight key challenges and opportunities, outlining promising directions for future research.
% This survey addresses this gap by summarizing recent technical advancements in LLM unlearning, reviewing existing evaluation metrics as well as benchmarks. Finally, we provide an outlook on promising directions for future research.

% LLM unlearning provides us with a way to eliminate undesirable data influence (e.g., sensitive or illegal information) present in the training data and the associated model capabilities while maintaining the integrity of essential knowledge generation and not affecting causally unrelated information. It is becoming a pivotal component in the lifecycle management of LLMs, serving as a critical foundation for developing generative AI that is safe, secure, trustworthy, and resource-efficient, eliminating the need for full retraining. There has been a lot of recent research on the application of LLM unlearning, but there has been no comprehensive overview to organize it and outline the main lessons learned. This survey aims to bridge this gap. In particular, we summarize recent technical advances for LLM unlearning, outline an effective assessment framework, and summarize widely used benchmarks. Finally, we offer an outlook on where future research should go.
% Machine unlearning for large language models (LLMs), or \textit{LLM unlearning}, aims to selectively remove the influence of specific data from a trained model while preserving its overall capabilities and knowledge integrity. This technique is crucial for mitigating privacy risks, eliminating harmful information, and ensuring compliance with evolving regulations, all without the prohibitive cost of full retraining. 
% In this paper, we systematically survey current efforts on the unlearning approaches, evaluation metrics and 
% Despite growing research interest, there remains a lack of holistic understanding of the approaches and 
% there remains a lack of a systematic framework to categorize and evaluate existing unlearning methods. In this survey, we provide the first comprehensive taxonomy of LLM unlearning, encompassing both unlearning approaches and evaluation metrics. We categorize existing techniques into optimization-based, data-centric, and model-editing approaches, highlighting their theoretical underpinnings and practical implications. Additionally, we introduce a structured evaluation framework, summarizing key benchmarks and metrics to assess the effectiveness, efficiency, and unintended side effects of unlearning methods. Finally, we discuss open challenges and future research directions, aiming to guide the development of more robust and scalable LLM unlearning techniques.

\end{abstract}

\section{Introduction}

% The increasing deployment of large language models (LLMs) in various domains has highlighted the critical importance of model unlearning capabilities, particularly as these systems become more deeply integrated into societal infrastructure and decision-making processes. 

% Forget quality and utility preservation are key performance metrics for unlearning algorithms. In the \textit{fine-tuning-then-unlearning} paradigm, the model first learns task-specific data in an additional Step 0, whereas the \textit{direct-unlearning} skips this step.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/framework_update1.pdf}
    \caption{\textbf{Overview of LLM Unlearning.} LLM unlearning focuses on removing specific data (forget set) while minimizing the impact on related knowledge (retain set) and general world knowledge. }
    \label{fig:workflow}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/taxonomy_update.drawio.pdf}
    \caption{Taxonomy of LLM unlearning.}
    \label{fig:llm_taxonomy}
\end{figure*}

The widespread adoption of large language models (LLMs) has brought significant challenges, particularly concerning user data privacy, copyright protection, and alignment with societal values. During training, these models can inadvertently memorize sensitive information, such as personally identifiable data or copyrighted materials~\cite{li2024the,li2024reference,zhang2024safe,yao-etal-2024-machine}. In addition to privacy and copyright issues, some training data may embed content that conflicts with contemporary social norms, such as discriminatory language based on race, ethnicity, etc~\cite{li2025internal}. These biases often manifest as harmful stereotypes, undermining the fairness and inclusivity of AI systems. Addressing these concerns is not only a societal imperative but also a regulatory requirement under privacy laws such as the General Data Protection Regulation (GDPR)~\cite{gdpr2022} and the EU Artificial Intelligence Act (EU AI Act)~\cite{com2021laying}. These laws mandate the ``right to be forgotten" and require mechanisms to delete specific data upon request.

To address these challenges, the field of LLM unlearning has emerged, focusing on removing specific information or behaviors from models while preserving their overall performance.
% To tackle these challenges, LLM unlearning has emerged for selectively removing specific information or behaviors from models while preserving their overall performance.
However, LLM unlearning faces significant technical challenges. One of the most pressing issues is the prohibitively high cost of retraining. Traditionally, addressing harmful or unwanted data required retraining the model from scratch after excluding problematic data from the training set~\cite{jang-etal-2023-knowledge}. This is impractical for LLMs due to the immense time and computational resources required~\cite{li2024the}. Moreover, the frequent unlearning requests that arise in deployed models highlight the need for more efficient unlearning techniques. The complexity of LLMs, with their millions or even hundreds of billions parameters, further complicates the task of removing specific information without causing unintended side effects, such as performance degradation or catastrophic forgetting~\cite{zhang2024negative}. 
% This complex interplay of factors underscores the urgency of developing robust unlearning methodologies for large language models, presenting not only technical challenges but also opportunities for innovation that could help better align artificial intelligence systems with human values and societal needs. 
% Despite recent advancements in LLM unlearning, to our knowledge, there is a lack of  comprehensive review of existing approaches and evaluation metrics has been conducted. This paper aims to bridge this gap, providing researchers with a comprehensive overview of the field.

Despite recent advances in LLM unlearning~\cite{eldan2024whos,maini2024tofu}, there is no comprehensive review of existing approaches and evaluation measures. Here we aim to bridge this gap. In particular, we offer a thorough overview of the field, including various unlearning and evaluation methods, and we make the following contributions:
\begin{itemize}
%     \item 
% \end{itemize}
    \item We formalize the LLM unlearning paradigms and propose a comprehensive taxonomy to categorize the existing approaches. This taxonomy not only offers a structured understanding of the research landscape, but also helps researchers identify their areas of interest.
    \item We systematically review the existing methods, analyzing their features, strengths, and weaknesses. We further discuss the existing evaluation measures and benchmarks, highlighting the challenges when balancing utility preservation and forgetting quality.
    \item We discuss future research opportunities for LLM unlearning, including extending techniques to multimodal models and addressing real-world unlearning requests. These avenues aim to advance the field and address emerging challenges. 
    % We hope these directions will inspire and guide other researchers in advancing this important area of study.
\end{itemize}


% Additionally, the absence of standardized benchmarks and evaluation metrics makes it challenging to assess and compare the effectiveness and efficiency of different unlearning approaches.
% This necessity originates from multiple interconnected factors spanning regulatory, societal, and economic dimensions. 
% Privacy regulations, such as the ``right to be forgotten'' in the General Data Protection Regulation~\cite{gdpr2022} and the EU AI Act~\cite{eu_ai_act} mandate mechanisms for data removal, while the need to address harmful biases, outdated information, and factual inaccuracies in trained models presents significant technical challenges. 
% % The societal implications of LLMs' are particularly relevant, as these models increasingly influence public discourse, educational systems, and cultural narratives~\cite{Qu2024}. 
% Without effective unlearning mechanisms, historically marginalized groups may continue to face algorithmic biases perpetuated through outdated or discriminatory training data, while individuals seeking to reclaim control over their digital footprint find themselves constrained by the technical limitations of current AI systems. 
% Traditionally, a model has to be fully retrained to remove undesirable information and concepts.
% The economic implications are equally substantial, as complete model retraining requires extensive computational resources, potentially limiting AI development to well-resourced organizations and exacerbating existing technological inequalities. 
% Furthermore, public trust and accountability in AI systems largely depend on the ability to address mistakes and remove harmful content, making unlearning capabilities crucial for broader AI adoption and social acceptance. 
% The protection of intellectual property rights and the management of proprietary information inadvertently included in training data present additional challenges that unlearning could address while raising important questions about the intersection of technological capability and social justice~\cite{yu2023_ip}. 
% As information and societal values evolve, the ability to selectively update model knowledge becomes increasingly important, reflecting broader ethical principles of human agency over AI systems and the right to shape one's digital presence~\cite{eu_ai_act}. 
% Organizations developing effective unlearning techniques may gain competitive advantages in an increasingly regulated global market while also supporting research integrity by enabling the removal of erroneous or retracted information, as this significantly reduces the regulatory exposure to fines~\cite{eu_ai_act}.





% \begin{itemize}
%     % \setlength{\itemsep}{0pt}
%     % \setlength{\parskip}{0pt}
%     \item How can we categorize LLM unlearning techniques?
%     \item How can we unlearning knowledge and what are current shortcomings?
%     \item How can we measure the effectiveness of LLM unlearning techniques?
%     % \item What can be done to promote the development of this field?
% \end{itemize}

% This survey offers a comprehensive overview of LLM unlearning, covering its theoretical foundations, practical methodologies, evaluation techniques, commonly used benchmarks, and key challenges. Specifically, in Section \ref{sec:preliminary}, we begin by presenting the definition and paradigm of \mllm unlearning. We further categorize the existing unlearning methods in Section \ref{sec:methods}. Next, we discuss evaluation methods for forget quality and model utility in Section \ref{subsec:forget} and Section \ref{subsec:utility}, respectively. Additionally, we summarize widely used benchmarks, encompassing both unimodal and multimodal datasets in Section \ref{sec:benchmark}. We conclude by exploring their challenges and looking at potential future research directions in Section \ref{sec:challenges}. Figure \ref{fig:llm_taxonomy} shows the work we explore in this survey, organized in a taxonomy.



% Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of tasks, yet their deployment is accompanied by significant risks. A primary concern is the unintentional retention of sensitive information, such as private data or copyrighted content, which may result in violations of privacy laws or intellectual property rights. Moreover, LLMs are prone to generating harmful outputs, including biased or discriminatory language, which can reinforce societal inequities. In more severe cases, these models can pose public safety threats, such as generating instructions for creating weapons, illicit drugs, or other dangerous content. Effectively addressing these risks is vital to ensuring the responsible use of LLMs.

% \begin{figure}[hbt!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/framework_update.png}
%     \caption{Demonstration of datasets, models, and evaluation in the process of LLMs unlearning.}
%     \label{fig:workflow}
% \end{figure}

% Traditionally, mitigating harmful or unwanted data would require retraining the entire model after excluding the problematic data from the training dataset~\cite{ThudiDCP22,jia2023model,fan2024salun}. However, retraining from scratch incurs significant time and cost, making it infeasible in most real-world scenarios. Additionally, retraining may inadvertently degrade the model‚Äôs performance on unrelated tasks or fail to completely eliminate the influence of the removed data.

% Given these challenges, developing efficient and scalable methods for unlearning specific information from LLMs has become a critical research focus. These methods aim to enable targeted unlearning while maintaining the model‚Äôs overall utility, robustness, and alignment with user expectations. Recent advancements in this field have introduced various unlearning methods for LLMs, including supervised fine-tuning~\cite{ishibashi2023knowledge,eldan2024whos}, reinforcement learning~\cite{NEURIPS2022_b125999b}, representation engineering~\cite{li2024the}, and so on. In addition, researchers have proposed various benchmarks to facilitate progress in the unlearning community.


% % These methods aim to enhance the safety and trustworthiness of LLMs by enabling the swift and precise removal of harmful content or behaviors without diminishing the model‚Äôs overall effectiveness.



% This survey offers a comprehensive overview of LLM unlearning, covering its theoretical foundations, practical methodologies, evaluation techniques, commonly used benchmarks, and key challenges. Specifically, in Section \ref{sec:preliminary}, we begin by presenting the definition and paradigm of \mllm unlearning. We further categorize the existing unlearning methods in Section \ref{sec:methods}. Next, we discuss evaluation methods for forget quality and model utility in Section \ref{subsec:forget} and Section \ref{subsec:utility}, respectively. Additionally, we summarize widely used benchmarks, encompassing both unimodal and multimodal datasets in Section \ref{sec:benchmark}. We conclude by exploring their challenges and looking at potential future research directions in Section \ref{sec:challenges}. Figure \ref{fig:llm_taxonomy} shows the work we explore in this survey, organized in a taxonomy.





% \subsection{Contributions of this Survey}
% \begin{itemize}
%     \item We
%     \item We
%     \item We
% \end{itemize}


% \subsection{}
% To address these risks, the concept of LLM unlearning has emerged as a promising solution. It involves selectively removing undesired knowledge or behavior from a trained model without the need for costly retraining from scratch. Although retraining a model after excluding undesirable data is a possible approach, it is often prohibitively expensive in terms of both computational resources and time, making it impractical for large-scale models.
% The rest of our survey unfolds as follows: 







\section{Preliminaries and Taxonomy}
\label{sec:preliminary}

\subsection{Problem Definition}

% LLM unlearning focuses on selectively erasing the influence of specific information, encapsulated in a forget set $\mathcal{D}_f$, while preserving the model‚Äôs utility and performance on other tasks represented by a retain set $\mathcal{D}_r$. To achieve this, both the forget and retain sets are carefully designed based on the principle of locality, ensuring high similarity to minimize unintended interference. As illustrated in Figure~\ref{fig:workflow} the unlearning framework involves two primary phases: learning and unlearning. In the learning phase, the model is trained on a task-specific dataset that includes both the forget set and the retain set. During the unlearning phase, optimization techniques are employed to reduce the model's reliance on the forget set while retaining its performance on the retain set and general world knowledge. 

% Mathematically, the unlearning process is formalized as the following optimization objective:
% \begin{equation}
% \min_{\theta} \mathcal{L}(\theta) = \min_{\theta} -\mathcal{L}_f(\theta) + \beta \mathcal{L}_r(\theta),
% \label{eq:definition}
% \end{equation}



The objective of LLM unlearning is to selectively remove the influence of specific information while maintaining the model's overall utility for other tasks. The optimization objective of the model parameters $\theta$ can be expressed as follows:

% and serves as a measure of its performance on other tasks
\begin{equation} \min_{\theta}  \mathcal{L}(\theta) = \min_{\theta} \{ -\mathcal{L}_f(\theta) + \lambda \mathcal{L}_r(\theta) \label{eq:definition} \} \end{equation}

\noindent Here, the \textit{forget loss} $\mathcal{L}_f(\theta)$ quantifies the model prediction error on the forget set $\mathcal{D}_{f}$, while the \textit{retain loss} $\mathcal{L}_r(\theta)$ ensures the preservation of the model's utility on the retain set $\mathcal{D}_{r}$. The regularization parameter $\lambda \ge 0$ controls the trade-off between effectively forgetting undesired information and preserving the model‚Äôs utility.

\subsection{LLM Unlearning Paradigms}
LLM unlearning follows two main paradigms. The \textbf{fine-tuning-then-unlearning} paradigm focuses on eliminating knowledge introduced during \textit{fine-tuning}. As illustrated in Figure~\ref{fig:workflow}, this paradigm typically leverages synthetic data (e.g., TOFU~\cite{maini2024tofu} and FIUBench~\cite{ma2024benchmarking}) and partitions a task-specific dataset into a forget set $\mathcal{D}_f$ and a retain set $\mathcal{D}_r$ to highlight the unlearning precision. The original model \(\theta_{ori}\) is first obtained by fine-tuning a pretrained model \(\theta_{pre}\) on the task-specific data to encode the target knowledge. Unlearning techniques are then applied to reduce the model's reliance on the forget set, resulting in the unlearned model \(\theta_{unl}\).
% There are two primary paradigms for LLM unlearning. The \textit{fine-tuning-then-unlearning} paradigm focuses on removing knowledge introduced during fine-tuning. As shown in Figure~\ref{fig:workflow}, this paradigm typically uses synthetic data (e.g., TOFU~\cite{maini2024tofu} and FIUBench~\cite{ma2024benchmarking}) and involves a task-specific dataset partitioned into a forget set and a retain set, designed to balance unlearning precision and performance retention. The original model \(\theta_{ori}\) is initially obtained by fine-tuning a pretrained model \(\theta_{pre}\) on the task-specific data to inject the target knowledge. Unlearning techniques then reduce the model‚Äôs reliance on the forget set, resulting in \(\theta_{unl}\).
The \textbf{direct-unlearning} paradigm focuses on eliminating knowledge acquired during the \textit{pretraining} stage of $\theta_{ori}$, assuming the target knowledge originates from multiple points within the pretraining dataset. The forget and retain sets are typically sampled from pretraining corpora~\cite{yao-etal-2024-machine} or publicly available datasets such as Wiki~\cite{jin2024rwku}. This paradigm is also extensively applied in safety alignment tasks, where it aims to eradicate hazardous knowledge and to mitigate risks such as misuse or jailbreak attacks~\cite{li2024the,zhang2024safe}.
% The \textbf{direct-unlearning} paradigm aims to remove knowledge introduced during the pretraining stage of $\theta_{ori}$. It claims to address the limitations of the first paradigm, where knowledge often stems from multiple points of the pretraining set. Forget and retain sets are typically sampled from pretraining corpora~\cite{yao-etal-2024-machine} or public Wiki datasets~\cite{jin2024rwku}. This paradigm is also widely used for safety alignment tasks, aiming to eliminate hazardous knowledge and mitigate risks like misuse or jailbreak attacks~\cite{li2024the,zhang2024safe}.

% To ensure effective unlearning without significantly compromising the model's functionality or the integrity of retained knowledge, both forget quality on the forget set and model utility, including performance on the retain set and other datasets reflecting world knowledge, are commonly used to evaluate unlearning methodologies.

% Commonly used replacements include empty strings~\cite{yao2023large} or refusal responses~\cite{maini2024tofu,ma2024benchmarking} like ``I don't know the answer."
\paragraph{Substitutes for Knowledge to be Forgotten} Existing unlearning paradigms primarily specify the knowledge to be forgotten, while leaving the substitutes undefined.  In this case, refusal responses, such as ``\emph{I don't know the answer},'' are commonly used as substitutes~\cite{maini2024tofu,ma2024benchmarking}.~\mycite{yao-etal-2024-machine} explored two relabeling strategies. The first strategy uses random labels generated by a randomly initialized model, while the second involves sampling adversarial examples from the model. For each token $w_t$ to be unlearned, an adversarial sample $a_t$ is generated to maximize the model's confusion while staying close to $w_t$. \mycite{eldan2024whos} proposed a fine-grained relabeling approach. 
% This method first identifies tokens with increased logit probabilities after fine-tuning the original model on the forget set. Then, a dictionary, generated using GPT-4, replaces specific entities with plausible generic counterparts (e.g., substituting ``Hogwarts" with ``Mystic Academy"), enabling more context-aware alternative labels.
Their method first identifies tokens whose logit probabilities increase after fine-tuning on the forget set. To facilitate unlearning, a dictionary generated by GPT-4 replaces specific entities with contextually appropriate generic alternatives (e.g., replacing ``Hogwarts'' with ``Mystic Academy''), ensuring more natural and context-aware substitutions.

\subsection{Taxonomy}
We present a comprehensive taxonomy of \mllm unlearning, as illustrated in Figure~\ref{fig:llm_taxonomy},
outlining existing research from the perspectives of methods, evaluation measures, and benchmarks. Existing methods can be categorized into four types: direct fine-tuning, localized parameter modification, leveraging auxiliary models, and input/output-based unlearning. Forgetting quality and utility preservation are critical measures for evaluating unlearning algorithms, particularly given recent discussions on whether knowledge is robustly forgotten or remains susceptible to adversarial recovery. This is often assessed through input-based or logit-based evaluation, as well as model intervention techniques. Additionally, we review commonly used unimodal and multimodal  benchmarks.

% Since unlearning datasets typically focus solely on identifying the knowledge to be forgotten, there have been various alternative introduced for replacing the forgotten knowledge. ~\mycite{yao2023large} use only whitespaces as alternatives, while \mycite{maini2024tofu,ma2024benchmarking} employ refusal responses, such as "I don't know the answer." \cite{yao-etal-2024-machine} implement two strategies: the first involves using random labels generated by a randomly initialized model, while the second involves sampling adversarial examples from the model. For each token  $w_t$ to be unlearned, an adversarial sample $a_t$ is generated to maximize the model's confusion while remaining close to $w_t$. \cite{eldan2024whos} introduce a fine-grained relabeling approach that combines reinforcement bootstrapping with anchored terms. Their method fine-tunes a baseline model to produce a reinforced auxiliary model. By comparing the logits of these models, tokens strongly associated with the unlearned data are identified and their probabilities adjusted to produce generic predictions. Additionally, a dictionary of anchored terms, generated using GPT-4, replaces specific entities with plausible generic counterparts (e.g., replacing "Hogwarts" with "Mystic Academy"), enabling more meaningful and context-aware alternative labels.

% \subsection{\mllm Unlearning Paradigm} 

% In classic unlearning paradigms \cite{ThudiDCP22,jia2023model,fan2024salun}, retraining a model from scratch after removing the forgotten data from the original training set is regarded as exact unlearning. However, retraining \mllm on the remaining large dataset is extremely costly, and establishing a performance upper bound for evaluating \mllm unlearning remains challenging. To access retraining, a solution is to use a surrogate unseen forget set from a domain close to the domain of the real forget set to approximate a retrained model‚Äôs performance on the real forget data \cite{yao-etal-2024-machine}. Despite the progress in approximating a retrained model‚Äôs performance, there is still a general need for precisely assessing the gap between (approximate) unlearning methods and exact unlearning. Moreover, even if retraining becomes computationally feasible in certain scenarios, identifying specific forget data within large pre-training datasets continues to be a significant obstacle. 

% Another approach is to incorporate fictitious data (e.g., synthetic author profiles) into the model training process, as demonstrated by recent LLMs unlearning benchmark, TOFU~\cite{maini2024tofu}, and VLMs unlearning benchmark, FIUBENCH~\cite{ma2024benchmarking}. Since the injected set never appeared in the original pretraining set, \mllm fine-tuning can simulate the retraining process over the newly-introduced set. Specifically, as illustrated in Figure~\ref{fig:workflow}, we first construct a task-specific dataset containing both a forget set and a retain set. These two sets typically share a similar data distribution, often created by partitioning a single dataset at varying ratios. This design aims to improve the precision of the unlearning process while minimizing its impact on the model‚Äôs performance on unrelated instances. Then, a pretrained language model $\theta_{pre}$ is fine-tuned on this task-specific dataset to produce the original model $\theta_{ori}$, which already encodes the target knowledge. During the unlearning phase, unlearning techniques are employed to reduce the model‚Äôs reliance on the forget set while preserving its performance on the retain set, resulting in the final unlearned model $\theta_{unl}$. To ensure that \mllm unlearning is achieved without significantly compromising the model‚Äôs functionality or the integrity of retained knowledge, both
% forget quality and model utility are commonly used to evaluate unlearning performance.  







% Furthermore, the model's retention of general world knowledge is evaluated post-unlearning to confirm that the utility and overall functionality of the model remain maximal uncompromised.

% assume the origin model $\theta_{ori}$ already possesses the target knowledge, establishing this prerequisite typically involves a learning phase that is conducted prior to the unlearning process. 

% During the learning phase, a pretrained language model $\theta_{pre}$ is fine-tuned on a task-specific dataset comprising both a forget set and a retain set. These two sets frequently share a similar data distribution, often generated by partitioning a single dataset at different ratios. This design aims to enhance the precision of the unlearning process while minimizing its impact on the model‚Äôs output for unrelated instances. In the unlearning phase, unlearning techniques are applied to reduce the model‚Äôs dependence on the forget set while maintaining its performance on the retain set. Furthermore, the model's retention of general world knowledge is evaluated post-unlearning to confirm that the utility and overall functionality of the model remain maximal uncompromised.

% This two-phase framework ensures a comprehensive and fair assessment of unlearning methods in both task-specific and general settings.


% \paragraph{Performance Metrics} \textit{Forget quality}, \textit{utility}, and \textit{efficiency} are key dimensions for evaluating unlearning algorithms. Forget quality measures how effectively specific knowledge is removed. For instance, in unlearning harmfulness, it assesses the reduction in harmful outputs, while in unlearning copyrighted data, it evaluates the suppression of leaked information. Most evaluations focus on the forget set, but recent work highlights the need to distinguish true unlearning from mere suppression, where knowledge can be recovered using paraphrased queries or jailbreak prompts. In Section 4, we comprehensively review existing methods for evaluating forget quality.
% Utility focuses on locality and overall performance. Locality ensures unlearning affects only the forget set while preserving unrelated knowledge in the retain set, while performance evaluates downstream task accuracy, output diversity, and fluency.
% Finally, {efficiency} is critical for deploying unlearning algorithms in dynamic environments, where frequent updates are required~\cite{liu2024rethinking,liu2022continual}. Despite its importance, efficiency has received limited attention, warranting further research.



% \textit{Forget quality} and \textit{utility} represent the two primary dimensions for evaluating unlearning algorithms. Forget quality assesses the effectiveness of unlearning specific knowledge or behaviors in a given task. For instance, in unlearning harmfulness, it measures the reduction in harmful outputs generated in response to harmful prompts, while in unlearning copyrighted data, it evaluates the reduction in leaked copyrighted information when prompted maliciously. The most common evaluation method focuses on performance within the forget set. However, recent studies have highlighted the need to distinguish between true unlearning‚Äîwhere knowledge is genuinely removed‚Äîand mere suppression, where the knowledge can still be recovered using paraphrased queries or jailbreak prompts. In Section 4, we provide a comprehensive review of existing methods for evaluating forget quality. Utility focuses on two key aspects: locality and overall performance. Locality ensures that unlearning affects only the forget set while preserving unrelated instances, knowledge, and concepts in the retain set. Performance includes the model's effectiveness on traditional downstream tasks, as well as its output diversity and fluency. In addition, the \textbf{efficiency} of unlearning algorithms is a crucial consideration. Their computational efficiency becomes essential for enabling rapid deployment in dynamic environments, where frequent unlearning tasks may be required to meet evolving needs~\cite{liu2024rethinking,liu2022continual}. However, current discussions on this aspect remain limited.


% Effective unlearning methods must demonstrate strong locality, focusing on precise removal of undesired knowledge while avoiding unintended degradation of related or broader information. 


% LLM unlearning typically involves defining a forget set and a retain set, which are carefully constructed to ensure high similarity based on the principle of locality. The forget set contains data to be unlearned, while the retain set ensures related knowledge is preserved. The workflow consists of two phases: learning and unlearning. In the learning phase, the model is trained on a broader dataset that includes the forget and retain sets. During unlearning, optimization techniques selectively reduce the model's reliance on the forget set while retaining performance on the retain set and general world knowledge. 

% The goal of Large Language Model (LLM) unlearning is to selectively eliminate the influence of specific information, represented as a forget set $\mathcal{D}_{f}$, from the outputs of a trained model while preserving its overall utility and performance on other tasks, represented by a retain set $\mathcal{D}_{r}$. This problem can be mathematically formalized as an optimization objective:


% \begin{equation}
% \min_{\theta} \mathcal{L}(\theta) = \min_{\theta} -\mathcal{L}_f(\theta) + \beta \mathcal{L}_r(\theta)
% \label{eq:definition}
% \end{equation}


% \noindent where $\mathcal{L}_f(\theta)$, which we call the \textit{forget loss} represents the loss function quantifying the prediction error of the model parameters $\theta$, on the forget set $\mathcal{D}_{f}$. $\mathcal{L}_r(\theta)$, which we call \textit{retain loss}, measures the prediction quality on the retain dataset $\mathcal{D}_r$. $\lambda$ is a regularization parameter that adjusts the trade-off between effectively forgetting the undesired data and preserving the model‚Äôs utility. The objective should ideally simultaneously achieve the aforementioned two goals.

% The typical workflow of LLM unlearning begins with the identification of a forget set and a retain set, which are carefully constructed datasets. The forget set consists of data points that the model must "unlearn," while the retain set includes similar data points to ensure the model retains related but distinct knowledge. To guarantee unlearning accuracy and preserve generalization, these datasets are designed to be highly similar, reflecting the principle of locality. The process involves two key phases: learning and unlearning. In the learning phase, the model assimilates knowledge from a broader dataset, including both the forget and retain sets. During unlearning, targeted optimization techniques are applied to selectively reduce the model‚Äôs reliance on the forget set while maintaining performance on the retain set and world facts‚Äîgeneralizable knowledge that extends beyond the datasets. 

% Determining what to forget and what to consider as alternatives is critical for tasks such as unlearning sensitive information or addressing biases. 



% For each token $w_t$ that needs to be unlearned, an adversarial sample $a_t$ is generated. This sample is chosen such that it is close to $w_t$ but maximizes the model's confusion:
% \begin{equation}
%     a_t = \arg\max_{a \neq w_t} P_{\theta}(a \mid w_1, w_2, \ldots, w_{t-1}).
% \end{equation}



% introduce a fine-grained relabeling approach that integrates reinforcement bootstrapping with anchored terms. The method begins by fine-tuning a baseline model to produce a reinforced auxiliary model. By comparing the logits of the baseline and reinforced models, tokens strongly associated with the unlearned data are identified, and their probabilities are adjusted to estimate generic predictions. Simultaneously, a dictionary of anchored terms, generated using GPT-4, is utilized to replace text-specific entities with generic counterparts (e.g., "Hogwarts" replaced with "Mystic Academy"), creating plausible alternative labels for the original data.



% Additionally, some studies propose relabeling methods~\cite{yao-etal-2024-machine,eldan2024whos} that sample different responses for different queries to help models preserve utility while unlearning specific knowledge.

% These approaches aim to help models preserve their utility while unlearning specific knowledge.

% In practice, the forget set is often provided in the form of instruction-response pairs. However, many approaches attempt to unlearn the entire response associated with the instruction. This is problematic because many tokens in the response are not tied to specific knowledge but rather reflect general language understanding. For example, in the phrase "My name is Harry Potter," the knowledge to be unlearned pertains to "Harry Potter," while unlearning "My name is" could harm the model's overall utility. To address issue, \cite{maini2024tofu} propose a fine-grained annotation approach. An auxiliary model is trained on the target data to identify the tokens most strongly associated with the content to be unlearned. This identification leverages the logits of the model, focusing on tokens whose  have increased logit values compared to baseline model. Additionally, \cite{maini2024tofu} introduce two methods to obtain the alternative responses to the unlearning instruction. 

%  Given $x, y$ as the sequence we aim to unlearn, let  $l(y|x;\theta_{ori})$ denote the output logits of the original LLM, and $l(y|x;\theta_{a})$ represent the output logits of the auxiliary LLM. Ideally, the output logits of the forget model, denoted as $l(y|x; \theta)$, are then obtained through the following logit subtraction operation:
% \begin{equation}
%     l(y|x; \theta) =  l(y|x;\theta_{ori}) - \alpha \cdot l(y|x;\theta_{a})
% \end{equation}
% where $\alpha$ is a hyperparameter that controls the intensity of forgetting. Notably, the logit operation is equivalent to rescaling the output distribution of the original LLM.



% \subsection{Taxonomy}


% Corresponding to our taxonomy, some representative papers, together with their characteristics, have been listed in Table 1.
% It comprehensively compares learning frameworks, enhancement methods, data modality, and applications.



% \begin{table*}[ht]
% \centering
% \small
% \setlength{\tabcolsep}{2.5pt}
% \caption{A summary of existing LLM unlearning studies.}

% \begin{tabular}{p{3.1cm} p{2.5cm} p{2.5cm} p{2.5cm} p{4.2cm}}
% \toprule
% % \textbf{Related Work} & \textbf{Method} & \textbf{Application} & \textbf{Model} & \textbf{Contribution} \\
% \textbf{Related Work} & \textbf{Tasks} &  \textbf{Method} & \textbf{Evaluation} & \textbf{Application} \\

% \midrule

% % --------------------------

% \cite{eldan2024whos} & 
% \textit{Relabeling-based Fine-tuning} & 
% \textit{Copyright} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------

% \cite{jang-etal-2023-knowledge} &
% \multirow{2}{2.7cm}{\raggedright \textit{Gradient Ascent}} & 
% \textit{Privacy} &
% \textit{Model A} &
% Key insight A \\
% \cite{yao-etal-2024-machine} &
% & 
% \textit{Privacy} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Preference Optimization (3 papers)
% \cite{maini2024tofu} &
% \multirow{3}{3cm}{\raggedright \textit{Preference Optimization}} &
% \textit{Privacy} &
% \textit{Model B} &
% Key insight B \\
% \cite{zhang2024negative} &
% & 
% \textit{Privacy} &
% \textit{Model B} &
% Key insight B \\
% \cite{zhang2024safe} &
% & 
% \textit{Safety Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------

% \cite{kassem-etal-2023-preserving} &
% \multirow{2}{2.7cm}{\raggedright \textit{Reinforcement Learning}} &
% % Â∫îÁî®Âàó‰∏∫Á©∫
% {} &
% \textit{Model B} &
% Key insight B \\
% \cite{NEURIPS2022_b125999b} &
% & 
% \textit{Safety Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Representation Engineering (2 papers)
% \cite{li2024the} &
% \multirow{2}{2.7cm}{\raggedright \textit{Representation Engineering}} &
% \textit{Safety Alignment} &
% \textit{Model B} &
% Key insight B \\
% \cite{huu2024effects} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Locate-then-Unlearn (6 papers)
% \cite{wu-etal-2023-depn} &
% \multirow{4}{2.7cm}{\raggedright \textit{Locate-then-\\Unlearn}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{guo2024mechanistic} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{guo2024mechanistic} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{hong2024intrinsic} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\

% \midrule

% % --------------------------
% % Task Vector (2 papers)
% \cite{ilharco2023editing} &
% \multirow{2}{2.7cm}{\raggedright \textit{Task Vector}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{liu-etal-2024-towards-safer} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Contrastive Decoding (1 paper)
% \cite{ji2024reversing} &
% {\raggedright \textit{Contrastive\\ Decoding}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Knowledge Distillation (2 papers)
% \cite{wang2024rkld} &
% \multirow{2}{2.7cm}{\raggedright \textit{Knowledge Distillation}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{dong2024unmemorization} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\

% \multirow{2}{2.7cm}{\raggedright \textit{Knowledge Distillation}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{dong2024unmemorization} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\

% \multirow{2}{2.7cm}{\raggedright \textit{Knowledge Distillation}} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \cite{dong2024unmemorization} &
% & 
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\

% \bottomrule
% \end{tabular}
% \label{tab:unlearning_comparison}
% \end{table*}





% \subsection{Application Areas}
% The current application areas of LLM Unlearning can be broadly divided into two categories:
% \paragraph{Privacy and Copyright Protection} 
% As 
% \paragraph{Mitigation of Harmful }

\section{\mllm Unlearning Methods}
\label{sec:methods}
% \subsection{Model-based Unlearning}



\subsection{Direct Fine-Tuning}
\label{sec:direct_fine_tuning}



\noindent\textbf{Relabeling-Based Fine-Tuning} first replaces the original responses by generic or neutral substitutes, as described earlier. Then, the model is fine-tuned on the relabeled data to reduce the influence of undesired training information~\cite{jin2024rwku,eldan2024whos}.
% It first replaces the original, private, or potentially harmful responses with generic or neutral substitutes, as mentioned above. The model is then fine-tuned directly on the relabeled data to mitigate the impact of undesired training data~\cite{jin2024rwku,eldan2024whos}. 

\paragraph{Reinforcement Learning}
The Quark method~\cite{NEURIPS2022_b125999b} is a pioneering approach to applying reinforcement learning for LLM unlearning. It uses a reward model and Proximal Policy Optimization (PPO)~\cite{schulman2017proximal} to reduce undesirable behaviors such as toxicity, repetition, and unwanted sentiment. The reward model assesses the output quality using task-specific measures of toxicity and sentiment. Quark alternates between collecting samples, sorting them into reward-based quantiles labeled with reward tokens, and applying language modeling loss conditioned on these tokens, with a KL-divergence penalty to stay close to the original model. \mycite{kassem-etal-2023-preserving} proposed DeMem, which leverages a negative similarity reward. This approach trains the LLMs to develop a paraphrasing policy on the forget dataset, generating dissimilar tokens that minimize memorization while preserving semantic coherence. 



% For instance, refusal tuning fine-tunes the model using refusal responses and is often used as a baseline~\cite{}. 

% By employing relabeling, models can preserve good performance on tasks beyond the unlearning scope while avoiding alterations to the model architecture or loss function.

% More methods fine-tune the target model to balance forgetting and utility preservation by designing a weighted formulation that combines different forgetting losses and utility losses.

\paragraph{Gradient Ascent (GA)} \mycite{jang-etal-2023-knowledge} performed gradient ascent (GA) on the next-token prediction loss over the forget set, which is mathematically equivalent to performing gradient descent on the negative prediction loss:
\begin{equation}
\small
\mathcal{L}_{GA}(\theta) = -\mathbb{E}_{(x, y) \sim \mathcal{D}_f} \left[ -\log \left( p(y \mid x; \theta) \right) \right]. 
% = \mathbb{E}_{(x, y) \sim \mathcal{D}_f} \left[ \log \left( p(y \mid \mathbf{X} = x; \theta) \right) \right]. \mathbf{X} =
\end{equation}
However, the GA-based approach often results in sizable model degradation, characterized by the generation of uniform and low-quality outputs. To mitigate this and to preserve model utility, ~\mycite{liu2022continual} introduced the gradient descent (GD) loss on the retain set $\mathcal{D}_r$ as a regularization:
% various retain losses have been developed, including those based on gradient descent loss and KL-divergence loss.
\begin{equation}
\small
\mathcal{L}_{GD}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}_r} \left[ -\log \left( p(y \mid x; \theta) \right) \right].
\label{eq:gd_loss}
\end{equation}
\mycite{yao-etal-2024-machine} incorporated the Kullback-Leibler (KL) divergence term as the regularization,  which captures the difference between the predictions of the original model and the fine-tuned model on the retain set:
\begin{equation}
\small
\textstyle
\mathcal{L}_{KL}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}_r} \left[ D_{KL} \left( p(y \mid x; \theta_{ori}) \, \| \, p(y \mid x; \theta) \right) \right].
\label{eq:kl_loss}
\end{equation}
These regularization techniques can also be applied in other unlearning methods to preserve the model's utility. 

% it combines gradient ascent on the forget set with 
% Preference optimization, including Direct Preference Optimization (DPO), simplifies the training process by directly optimizing a model's output based on human-defined preferences. Unlike Reinforcement Learning with Human Feedback (RLHF), which requires complex reward modeling and iterative adjustments, DPO adopts a direct classification approach using a binary cross-entropy objective. Unlike traditional optimization methods that rely on explicit loss functions, preference optimization 

\noindent \textbf{Preference Optimization} was first designed to align model behavior to human-defined preferences. Specifically, it leverages pairwise comparisons or ranking data to guide the model toward producing outputs that best match desired preferences. Given a preference dataset $\mathcal{D}_{p} = \left\{ (x_i, y_{i,w}, y_{i,l}) \right\}_{i \in [n]}$, where $y_{i,w}$ and $y_{i,l}$ represent responses to input $x_i$, the preference $y_{i,w} > y_{i,l}$ is derived from human comparisons. Direct preference optimization (DPO)~\cite{rafailov2023direct} minimizes the following objective function:
\begin{equation}
\small
\textstyle
\begin{aligned}
\mathcal{L}_{\text{DPO},\beta}(\theta) = &-\frac{1}{\beta} \mathbb{E}_{\mathcal{D}_{p}} 
\left[ \log \sigma \left( 
\beta \log \frac{p(y_w \mid x; \theta)}{p(y_w \mid x; \theta_{ori})} \right. \right.\\ 
&- \left. \left. \beta \log \frac{p(y_l \mid x; \theta)}{p(y_l \mid x; \theta_{ori})} \right) \right], 
\end{aligned}
\label{eq:dpo}
\end{equation}
where $\sigma$ is the sigmoid function and $\beta$ is the inverse temperature controlling the preference strength. \mycite{maini2024tofu} pioneered the application of DPO to unlearning by framing the forget set as a preference set. The original responses are denoted as $y_l$, while refusal responses, such as ``\emph{I do not know the answer},'' are designated as $y_w$. This formulation guides the unlearning process by aligning the model's behavior with the preferred alternative responses. Inspired by this idea, \mycite{zhang2024negative} proposed negative preference optimization (NPO), a DPO variant that uses only negative responses from the forget set, disregarding \(y_w\) in Eq.~\eqref{eq:dpo}:
\begin{equation}
\small
\textstyle
     \mathcal{L}_{\text{DPO}, \beta}(\theta) = -\frac{2}{\beta} \mathbb{E}_{(x, y) \sim \mathcal{D}_f} \left[ \log \sigma \left( -\beta \log \frac{p(y \mid x; \theta)}{p(y \mid x; \theta_{ori})} \right) \right].
\end{equation}
\mycite{zhang2024negative} further theoretically showed that NPO converges to GA as $\beta \to 0 $ and the speed toward collapse using NPO is exponentially slower than GA. 

% In conclusion, NPO-based methods generate more sensible outputs than GA-based methods, whose outputs are often gibberish.


% In conclusion, PO-based methods achieve a better balance between unlearning the undesirable data and maintaining the model‚Äôs utilities.

% \cite{zhang2024negative} further argues that NPO converges to GA as $\beta \to 0 $ and provides a mathematical demonstration showing that, in a simplified setting, NPO exhibits exponentially slower divergence compared to GA.
% Specifically, it requires another fixed dataset Didk containing all alternative responses.

% PO
% NPO
% \textit{safe unlearning}~\cite{zhang2024safe} 



% \cite{NEURIPS2022_b125999b} introduce the Quark method, which uses a reward model and PPO to unlearn undesirable behaviors in language models, such as toxicity, repetition, and unwanted sentiment. The reward model assesses output quality with task-specific metrics, such as toxicity scores from the Perspective API or sentiment scores from classifiers. Quark applies Quantized Reward Konditioning, alternating between exploration, quantization, and learning, where outputs are scored, ranked into quantiles, and labeled with reward tokens for fine-tuning.



\subsection{Localized Parameter Modification}
\label{sec:localized_parameter_modification}

\paragraph{Representation Engineering} RMU~\cite{li2024the} focuses on unlearning hazardous knowledge in LLMs by fine-tuning the lower layer $l$  to redirect internal representations of token $t$ in the forget set toward a fixed-noise vector $\mathbf{u}$: 
\begin{equation}
\small
\mathcal{L}_{\text{forget}}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}_f} \frac{1}{L_x} \sum_{t \in x} \| \mathcal{H}^{(l)}_{\theta}(t) - c \cdot \mathbf{u} \|_2^2,
\end{equation}
where $L_x$ is the number of tokens in $x$ and $c$ is some hyperparameter that controls activation scaling, $\mathcal{H}^{(l)}_{\theta}(t)$ denotes the internal activations of token $t$ at layer $l$. Simultaneously, it ensures that preserved knowledge remains consistent with the original model by aligning its representations, denoted as
\begin{equation}
\small
\mathcal{L}_{\text{retain}}(\theta) = \mathbb{E}_{(x, y) \sim \mathcal{D}_r}  \frac{1}{L_x} \sum_{t \in x} \| \mathcal{H}^{(l)}_{\theta}(t) - \mathcal{H}^{(l)}_{\theta_{\text{ori}}}(t) \|_2^2. 
\end{equation}

\mycite{huu2024effects} proposed adaptive RMU, which dynamically scales the random unit vector \(\mathbf{u}\) with an adaptive coefficient \(\beta \| \mathcal{H}_{\theta_{\text{ori}}}^{(l)}(x) \|_2^2\) for improved unlearning across layers, unlike RMU's fixed-scaling coefficient \(c\).

% \begin{equation}
%     \mathbb{E}_{(x, y) \sim \mathcal{D}_f} \left[\frac{1}{L_x} \sum_{t \in x} \| \mathcal{H}_{\theta}(t) - c \cdot \mathbf{u} \|_2^2 \right] 
%     + \beta \cdot \mathbb{E}_{(x, y) \sim \mathcal{D}_r} \left[\frac{1}{L_x} \sum_{t \in x} \| \mathcal{H}_{\theta}(t) - \mathcal{H}_{\text{ref}}(t) \|_2^2 \right]
% \end{equation}

\paragraph{Locate-Then-Unlearn Methods} This method focuses on identifying and localizing key model components (e.g., layers or neurons) that are critical for unlearning.
DEPN~\cite{wu-etal-2023-depn} leverages privacy attribution via {gradient integration} to identify privacy-sensitive neurons. It quantifies each neuron's contribution to privacy leakage by efficiently approximating the integral of the gradient changes using a limited number of steps (e.g., $m=20$). Specifically, the privacy attribution score $\text{Att}(w_k^l)$ for a neuron $w_k^l$ at layer $l$ is computed using the following cumulative gradient integration:
\begin{equation}
\small
\text{Att}(w_k^l)=\int_0^{\beta_k^l} \frac{\partial p(y|x, \alpha_k^l)}{\partial w_k^l} d\alpha_k^l  \approx \frac{\beta_k^l}{m} \sum_{j=1}^m \frac{\partial p(y|x, \frac{j}{m} \beta_k^l)}{\partial w_k^l},
\end{equation}
where $\beta_k^l$ is the activation value of the neuron, $\alpha_k^l$ represents the modified activation value, and $p(y|x)$ is the conditional probability of the model predicting the private information. 

% In practice, the integral is approximated using the Riemann sum.


% Locate-Then-Unlearn methods aim to identify and target specific model components (e.g., layers, weights, or neurons) critical to the unlearning process, enabling efficient and precise updates. DEPN \cite{wu-etal-2023-depn} uses privacy attribution via gradient integration to detect privacy-sensitive neurons, focusing on feedforward layers and editing top-ranked neurons to minimize computational costs while preserving accuracy. WAGLE \cite{jia2024wagle} balances unlearning efficacy and utility retention by formulating weight attribution as a bi-level optimization problem, leveraging implicit gradient analysis and a diagonal Hessian approximation for computational efficiency. Mechanistic unlearning approaches, such as \cite{guo2024mechanistic}, employ Fact Lookup (FLU) localization and fine-tuning, utilizing logistic regression probes \cite{alain2017understanding} or path patching \cite{goldowsky2023localizing} to identify components that enrich latent representations with factual attributes. Needle \cite{hong2024intrinsic} introduces a novel concept vector identification technique in MLP layers, validated through causal tests, and disrupts these vectors with Gaussian noise to erase specific knowledge. Similarly, \cite{pochinkov2024dissecting} measure neuron importance using activation-based functions, iteratively pruning high-scoring neurons to effectively remove private information while maintaining model utility. These diverse approaches demonstrate the versatility and efficiency of Locate-Then-Unlearn strategies in addressing privacy and knowledge editing challenges.

% By concentrating on critical layers, such as feedforward layers, and selectively editing only the top-ranked neurons with high privacy scores, DEPN achieves a balance between computational efficiency and precise detection.

% introduces a novel approach to identify influential weights that optimize the trade-off between unlearning efficacy and utility retention.
WAGLE~\cite{jia2024wagle} uses bi-level optimization to examine the interaction between weight adjustment and unlearning efficacy. By leveraging weight attribution, it quantifies the relationship between weight influence and the impact of forgotten or retained data on LLM outputs. The unlearning sensitivity score for weight perturbation is obtained from the forget loss $S_i = \mathcal{L}_f(\epsilon \odot \theta(\epsilon)) - \mathcal{L}_f(\theta(1))$, where \(\epsilon \odot \theta(\epsilon)\) is the weight-adjusted model, \(\epsilon\) represents weight modifications, and \(\epsilon=1\) indicates no interference. The weights \(\theta(\epsilon)\) minimize the retain loss \(\mathcal{L}_r\) to preserve utility. WAGLE uses a diagonal Hessian approximation for computational efficiency and accuracy, with the sensitivity score expressed as
\begin{equation}
    S_i \propto [\theta]_i [\nabla \mathcal{L}_f(\theta)]_i - \frac{1}{\gamma} [\nabla \mathcal{L}_r(\theta)]_i [\nabla \mathcal{L}_f(\theta)]_i,
\end{equation}
 where \([\theta]_i\) is the \(i\)-th weight, \([\nabla \mathcal{L}_f(\theta)]_i\) and \([\nabla \mathcal{L}_r(\theta)]_i\) are the gradients of the forget and the retain losses for the \(i\)-th weight, respectively, and \(\gamma\) is the Hessian parameter.

% \[
% \small
% ,
% \]


% \[
% \small

% \]
% \mycite{yu-etal-2023-unlearning} propose Partitioned Contrastive Gradient Unlearning (PCGU), which identifies bias-critical weights by analyzing gradients for minimally differing sentence pairs and ranking parameter groups based on gradient similarity. 

Beyond DEPN and WAGLE, \mycite{guo2024mechanistic} introduced a mechanistic unlearning framework that combines fact lookup localization, using logistic regression probes or path patching to assess causal importance, with localized fine-tuning. Similarly, Needle~\cite{hong2024intrinsic} identifies and disrupts concept vectors in MLP layers encoding specific knowledge using vocabulary projections and causal tests. 
% Beyond DEPN and WAGLE, \mycite{yu-etal-2023-unlearning} introduces Partitioned Contrastive Gradient Unlearning (PCGU), a method that identifies bias-critical weights by computing gradients for minimally differing sentence pairs, partitioning model parameters into groups, and ranking their importance based on gradient similarity. \mycite{guo2024mechanistic} propose a mechanistic unlearning framework combining fact lookup (FLU) localization and localized fine-tuning. FLU identifies components enriching latent representations with factual attributes using logistic regression probes~\cite{alain2017understanding} or path patching~\cite{goldowsky2023localizing} to assess causal importance. Furthermore, \mycite{hong2024intrinsic} introduces Needle, which locates concept vectors in multilayer perceptron (MLP) layers encoding specific knowledge via vocabulary projections and causal tests, then disrupts them with Gaussian noise to erase the target concept. Lastly, \mycite{pochinkov2024dissecting} uses importance functions to score neurons based on activation patterns relative to forget and retain sets, pruning top-scoring neurons to remove private information.



% In addition, \cite{guo2024mechanistic} propose a mechanistic unlearning method consisting of fact lookup (FLU) localization and localized fine-Tuning.  FLU localization identifies model components responsible for enriching latent representations with factual attributes. This is achieved by analyzing intermediate activations using logistic regression probes~\cite{alain2017understanding} or by employing path patching~\cite{goldowsky2023localizing} to measure the causal importance of components on model outputs. 
% \cite{hong2024intrinsic} introduce Needle, which first identifies concept vectors‚Äîspecific parameter vectors in the model's MLP layers that encode knowledge about concrete concepts. These vectors are pinpointed using vocabulary projections and validated through causal tests to ensure their association with the target concept. Once identified, the method directly disrupts the concept vector by adding Gaussian noise, effectively erasing the encoded knowledge and enabling the model to forget the concept. \cite{pochinkov2024dissecting} utilize importance functions to measure neuron significance based on activation patterns relative to forget and retain sets.  Neurons with the highest scores are iteratively pruned to effectively remove private information.




% Localization is formalized as $ C_\tau := \{c : c \in C, |S(c)| > \tau\}$, where the query, key, value, and output weights are considered as the components and  $S(c)$ maps component importance, and $\tau$ is the threshold. Localized Fine-Tuning then optimizes these localized components with a multi-objective loss function to achieve effective and precise unlearning while preserving model utility. 

% Localization is formalized as 

% FLU Localization identifies model components responsible for enriching latent representations with factual attributes by analyzing intermediate activations using logistic regression probes or applying path patching to assess the causal importance of components. 

% Causal Tracing \cite{meng2022locating}

% \paragraph{Neuron Edits and Pruning}
% \cite{wu-etal-2023-depn}


% \todo{Xiong: REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space}

% \textit{selective pruning}~\cite{pochinkov2024dissecting} 


\begin{table*}[ht]
\centering
\small
\setlength{\tabcolsep}{1.5pt}
\caption{A summary of existing LLM unlearning studies. \emph{C-Utility}, \emph{C-Forget}, \emph{C-Efficiency} denote the challenges to solve in the perspective of \textit{utility preservation}, \textit{forget quality}, and \textit{efficiency}, respectively.}

\begin{tabular}{p{2.4cm} p{3.4cm} >{\centering\arraybackslash}p{1.6cm}>{\centering\arraybackslash} p{1.6cm}>{\centering\arraybackslash} p{2cm}  p{3.1cm} p{3.5cm}}
\toprule
\textbf{Methodology} & \textbf{Features} & \textbf{C-Utility} & \textbf{C-Forget} & \textbf{C-Efficiency} & \textbf{Related Work} & \textbf{Application} \\

\midrule

% --------------------------
\makecell[l]{\textit{Relabeling-Based}\\ \textit{Fine-Tuning}}& 
\makecell[l]{Using refusal string or\\fine-grained alternatives } & 
 & \checkmark &  & 
\cite{eldan2024whos} &
\textit{Privacy \& Copyright} \\
\midrule

% --------------------------

\multirow{2}{*}{\raggedright \textit{Gradient Ascent}} & 
\multirow{2}{*}{\raggedright \makecell[l]{Maximizing cross-entropy\\loss w.r.t target sequence}} & 
\multirow{2}{*}{\checkmark} &  &  & 
\cite{jang-etal-2023-knowledge} & 
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{yao-etal-2024-machine} &
\textit{Privacy \& Copyright} \\
\midrule

% --------------------------

\multirow{3}{2.5cm}{\raggedright \textit{Preference Optimization}} & 
\multirow{3}{2.5cm}{\raggedright Better utility preservation} & 
\multirow{3}{*}{\checkmark} &  &  & 
\cite{maini2024tofu} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{zhang2024negative} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{zhang2024safe} &
\textit{AI Alignment} \\
\midrule

% --------------------------

\multirow{2}{2.5cm}{\raggedright \textit{Reinforcement Learning}} & 
\multirow{2}{2.5cm}{\raggedright \makecell[l]{Need for an accurate\\ reward model}} & 
\multirow{2}{*}{\checkmark} &  & \multirow{2}{*}{\checkmark} & 
\cite{kassem-etal-2023-preserving} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{NEURIPS2022_b125999b} &
\textit{AI Alignment} \\
\midrule

% --------------------------

\multirow{2}{*}{\raggedright \makecell[l]{\textit{Representation}\\ \textit{Engineering}}} & 
\multirow{2}{*}{\raggedright \makecell[l]{Modify localized weights to \\update internal representation}} & 
 & \multirow{2}{*}{\checkmark} &  & 
\cite{li2024the} &
\textit{AI Alignment} \\
& & & & & 
\cite{huu2024effects} &
\textit{AI Alignment} \\
\midrule

% --------------------------
%
\multirow{4}{2.5cm}{\raggedright \textit{Locate-then-\\Unlearn}} & 
\multirow{4}{2.5cm}{\raggedright  \makecell[l]{First locate the parameters\\ corresponding to the target\\ knowledge, then modify them.}} & 
 &  & \multirow{3}{*}{\checkmark} & 
\cite{wu-etal-2023-depn} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{jia2024wagle} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{guo2024mechanistic} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{hong2024intrinsic} &
\textit{Privacy \& Copyright} \\
\midrule

% --------------------------

\multirow{3}{2.5cm}{\raggedright \textit{Task Vector}} & 
\multirow{3}{2.5cm}{\raggedright \makecell[l]{Utilize the model weight\\ difference before and after\\ fine-tuning }} & 
 & \multirow{3}{*}{\checkmark} &  & 
\cite{ilharco2023editing} &
\textit{AI Alignment} \\
& & & & & 
\cite{liu-etal-2024-towards-safer} &
\textit{AI Alignment} \\
& & & & & 
\cite{gao-etal-2024-ethos} &
\textit{AI Alignment} \\
\midrule

% --------------------------

{\raggedright \makecell[l]{\textit{Contrastive}\\ \textit{Decoding}}} & 
{\raggedright \makecell[l]{Intervention during the\\ decoding process}} & 
 &  \multirow{1}{*}{\checkmark}  & \multirow{1}{*}{\checkmark} & 
\cite{ji2024reversing} &
\textit{AI Alignment} \\
\midrule

% --------------------------

\multirow{2}{2.5cm}{\raggedright \textit{Knowledge Distillation}} & 
\multirow{2}{2.5cm}{\raggedright \makecell[l]{Adjusting logits to guide\\the student model} }& 
 & \multirow{2}{*}{\checkmark} &  & 
\cite{wang2024rkld} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{dong2024unmemorization} &
\textit{Privacy \& Copyright} \\
\midrule

\multirow{3}{*}{\raggedright \makecell[l]{\textit{Input/Output-}\\\textit{Based}}} & 
\multirow{3}{*}{\raggedright \makecell[l]{Modify inputs or outputs\\ instead of model weights.}} & 
 & \multirow{3}{*}{\checkmark} & \multirow{3}{*}{\checkmark} & 
\cite{Incontextunlearning} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{liu2024large} &
\textit{Privacy \& Copyright} \\
& & & & & 
\cite{thaker2024guardrail} &
\textit{Privacy \& Copyright}  \\

\bottomrule
\end{tabular}
\label{tab:unlearning_comparison}
\end{table*}


\subsection{Leveraging Auxiliary Models}

These methods typically fine-tune an assistant model \(\theta_{a}\) to replicate knowledge from \(\mathcal{D}_{f}\). Its outputs are then used to adjust the original model's responses, mitigating the influence of \(\mathcal{D}_{f}\) through the auxiliary model's weights or logits.


% Among these methods, task vectors~\cite{ilharco2023editing,liu-etal-2024-towards-safer} leverage the auxiliary model's weights, while others utilize logits from auxiliary models, exemplified by methods like contrastive decoding~\cite{ji2024reversing} and knowledge distillation~\cite{dong2024unmemorization,wang2024rkld}. 


% Formally, denote l(Y|X; Œ∏) as the output logits of the original LLM, and la(Y|X; œï) as the output logits of an assistant LLM. Then the output logits of the forget model, denoted as lf(Y|X), is derived




% Task vectors provide an efficient mechanism for editing LLMs, enabling precise behavioral modifications through operations in the model‚Äôs weight space. The task vector $\tau$ is mathematically expressed as $\theta_{a}- \theta_{ori}$, representing the weights difference between the original model and the fine-tuned model. This representation captures the direction of adjustment required to enhance performance on a specific task. Task vectors facilitate operations such as negation and addition, enabling applications like unlearning and multi-task learning. Negating a task vector has proven effective in suppressing specific behaviors, such as mitigating toxic language generation in LLMs, as demonstrated in~\cite{ilharco2023editing}. \cite{liu-etal-2024-towards-safer} further improve the fine-tuning process by incorporating the intentional learning of harmful content through a combination of three novel modules: the guided distortion module, random disassociation module, and preservation divergence module, each targeting different aspects of harmful knowledge. Unlike the aforementioned methods, Ethos, proposed in \cite{gao-etal-2024-ethos}, distinguishes general beneficial and undesired knowledge when reconstructing task vectors. Specifically, Ethos first obtains a set of principal components from the pre-trained models using singular value decomposition. Then, by projecting the task vector onto principal components, Ethos separates the principal components that encode general from those associated with undesired knowledge. Ethos performs forgetting or unlearning by only negating the task vector with undesired knowledge, thereby minimizing collateral damage on general model utility.

\paragraph{Contrastive Decoding} 

% Contrastive decoding~\cite{li-etal-2023-contrastive} is initially proposed to optimize output quality by comparing the likelihood of generated text from a large expert language model with that of a smaller amateur model. 


  % is a contrastive coding-based approach that
% ULD~\cite{ji2024reversing} leverages an auxiliary model trained on the forget set to facilitate the unlearning during decoding. They claims the auxiliary LLM should focus exclusively on capturing the unique knowledge within the forget set while avoiding any retention of knowledge that is intended to preserve, ideally producing a uniform distribution over the retain set. The optimization objective of the auxiliary model should be the reversed version of Eq.~\eqref{eq:definition}:
ULD~\cite{ji2024reversing} uses an auxiliary model trained on the forget set to guide the unlearning process during decoding. It claims that the auxiliary LLM should exclusively capture the unique knowledge within the forget set while preventing the retention of any information meant to be preserved. Ideally, this results in a uniform distribution over the retain set. The optimization objective of the auxiliary model is formulated as the inverse of Eq.~\eqref{eq:definition}:
\begin{equation}
\small
\min_{\theta_a} \mathcal{L}(\theta_a) = \min_{\theta_a} \{ \mathcal{L}_f(\theta_a) - \beta \mathcal{L}_r(\theta_a) \},
\label{eq:reverse}
\end{equation}
The retain loss \(\mathcal{L}_r(\theta_a)\) is specifically formulated as the cross-entropy with respect to the uniform distribution. To enhance the efficiency in the auxiliary model's implementation, the first \(k\) transformer layers of the original LLM are reused, along with the language model head, to map hidden representations to output logits across the entire vocabulary.


% It help enhance coherence and relevance while effectively mitigating common issues such as repetition and incoherence.
% \begin{equation}
%     \mathcal{L}_r(\theta_a) = C
% \end{equation}



\paragraph{Knowledge Distillation} uses a specialized unlearning teacher model to guide the unlearning process, providing signals to the student model to adjust the logits and to selectively forget specific information. RKLD~\cite{wang2024rkld} first identifies tokens requiring unlearning by detecting such with consistently increased logit values after fine-tuning. The formula for the unlearning teacher model is as follows:
\begin{equation}
\small
       l_{}(y|x;\theta_{ori}) - \alpha \cdot {ReLU}(l(y|x;\theta_{a}) - l(y|x;\theta_{ori}))
\end{equation}
where $l(y|x;\theta_{ori})$ and $l(y|x;\theta_{a})$ represent the logits of the original and the auxiliary model, respectively, and $\alpha$ is a hyperparameter that controls the forgetting strength, respectively.
This strategy offers more precise guidance for intentional forgetting while safeguarding other information. Moreover, \mycite{dong2024unmemorization} introduced a self-distillation method that assumes tokens like named entities or nouns contain sensitive information requiring unlearning. To identify these key tokens, they used a syntactic analysis tool for extraction. The teacher model's logits were derived by subtracting hyperparameter-controlled one-hot vectors for the key tokens from the logits of the original model.

% combines self-distillation and deliberate
% imagination. Deliberate imagination guides LLMs to creatively imagine scenarios rather than merely forgetting, which shows promise in reducing memorization while maintaining model utility.

\paragraph{Task Vector} 
% Task vectors enable behavioral modifications by operating in the model‚Äôs weight space. Represented as \(\tau = \theta_{a} - \theta_{ori}\), they capture the weight adjustments needed to enhance task performance and support operations like negation and addition, facilitating applications such as unlearning and multi-task learning.
% Task vectors, defined as \(\tau = \theta_{a} - \theta_{ori}\), steer model behavior by editing the weight space, enabling operations like negation and addition for applications such as unlearning and multi-task learning.
Task vectors, defined as \(\tau = \theta_{a} - \theta_{ori}\), steer the model behavior by editing the weight space, thus enabling operations such as negation and addition for applications such as unlearning and multi-task learning.
Negating task vectors effectively suppress behaviors such as mitigating toxic language generation~\cite{ilharco2023editing}. \mycite{liu-etal-2024-towards-safer} enhanced fine-tuning with modules targeting harmful knowledge: guided distortion, random disassociation, and preservation divergence. Ethos~\cite{gao-etal-2024-ethos} distinguishes general and undesired knowledge by projecting task vectors onto principal components. By negating only undesired components, Ethos minimizes collateral damage to model utility, thus achieving unlearning.


% Additionally, they empirically observed that reverse KL-divergence serves as a better metric for measuring the distance and performing the distillation process.

\subsection{Input/Output-Based Unlearning}
Input/output-based unlearning methods offer flexibility by using prompt engineering and post-processing without modifying the model weights or the architecture. 
~\mycite{liu2024large} proposed training a prompt classifier to identify prompts within the scope of unlearning and efficiently corrupting them in the embedding space using zeroth-order optimization. \mycite{thaker2024guardrail} showed that simple guardrails like prompting and input/output filtering can effectively support unlearning independently or alongside fine-tuning. ~\mycite{Incontextunlearning} proposed in-context unlearning by constructing tailored prompts, where the labels of the data to be forgotten are flipped.
% Input/output-based unlearning methods enable flexible unlearning by leveraging prompt engineering and post-processing techniques, eliminating the need for modifications to model weights or architecture. In-Context Unlearning (ICUL)~\cite{Incontextunlearning} constructs tailored prompts that
% are then fed to the \mllm during inference. This process involves flipping the labels of the data points to be removed and augmenting the input context with accurately labeled examples from the training distribution. Furthermore, Embedding-Corrupted (ECO) Prompts~\cite{liu2024large} leverages a thresholded prompt classifier to identify and safeguard prompts to forget. Experiments using zeroth order optimization show that these embedding-corrupted prompts not only produce outputs aligned with the unlearning objective but also closely approximate the behavior of a model that was never trained on the data targeted for forgetting. In addition, \cite{thaker2024guardrail} explores the effectiveness of simple guardrail baselines, such as prompting and input/output filtering, and indicates that guardrails can be a highly effective approach for unlearning either alone or in conjunction with existing finetuning approaches

\paragraph{Brief Summary} Table~\ref{tab:unlearning_comparison} summarizes the existing unlearning methodologies, their features, representative work, and applications, highlighting challenges in forget quality, utility preservation, and efficiency based on their principles and empirical findings. Methods such as gradient ascent, preference optimization, and reinforcement learning often struggle with utility preservation due to optimization conflicts (e.g., maximizing cross-entropy loss), requiring regularization to mitigate the performance loss. Relabeling-based fine-tuning and knowledge distillation have been reported to suffer from knowledge relearning. This issue arises when models are fine-tuned using small amounts of related or even unrelated samples, or through in-context learning~\cite{lynch2024eight}. Representation engineering methods have been reported to be more susceptible to adversarial attacks, failing to robustly erase knowledge~\cite{lucki2024adversarial}. Contrastive decoding and input/output-based unlearning are controversial because they do not truly remove knowledge from the models. Task vectors, limited by imprecision in localizing knowledge, are mainly used in AI alignment. From an efficiency standpoint, reinforcement learning and locate-then-unlearn approaches involve higher algorithmic complexity. Reinforcement learning requires training high-quality models to generate reward signals, while locate-then-unlearn methods rely on precise knowledge localization. Moreover, contrastive decoding and input/output-based methods increase the computational overhead during inference, which leads to slower generation.
% From the perspective of efficiency, reinforcement learning algorithms and locate-then-unlearn approaches exhibit higher algorithmic complexity. Reinforcement learning methods require training additional high-quality models to provide reward signals, while locate-then-unlearn techniques demand precise knowledge localization. Additionally, contrastive decoding and input/output-based methods increase computation during inference, slowing down generation. 






\section{Benchmarks}
\label{sec:benchmark}

% Numerous benchmarks have been proposed,  
This section provides a detailed description of the commonly used benchmarks, addressing areas such as copyright, privacy, and AI alignment. Notably, WMDP and RWKU are designed for the direct-unlearning, while other benchmarks are used within the fine-tuning-then-unlearning paradigm.

\subsection{Unimodal Benchmarks}

\textbf{Who is Harry Potter (WHP)}~\cite{eldan2024whos} evaluates unlearning of Harry Potter-related information using a dataset combining the original books (2.1M tokens) with synthetic content (1M tokens). Unlearning effectiveness is assessed through 300 Harry Potter-related questions scored with a GPT-4-based evaluation. It explores the potential of unlearning in copyright protection.

\noindent \textbf{TOFU}~\cite{maini2024tofu} includes 200 synthetic author profiles with 20 question-answer examples each, ensuring no overlap with existing training data. In addition to offering three forget-retain splits (90-10, 95-5, and 99-1), the benchmark includes 100 real-world author profiles and 117 world facts, providing a comprehensive evaluation of model utility after unlearning.
% The benchmark provides three forget-retain splits (90-10, 95-5, and 99-1) and includes 100 real-world author profiles and 117 world facts to evaluate model utility after unlearning.

\noindent \textbf{WMDP}~\cite{li2024the} comprises 3,668 multiple-choice questions on biosecurity, cybersecurity, and chemical security, curated by experts to evaluate hazardous knowledge while excluding sensitive information. It serves as a benchmark for assessing LLMs' hazardous knowledge and unlearning methods for AI alignment.

\noindent \textbf{RWKU}~\cite{jin2024rwku} includes 200 unlearning targets focused on well-known public figures, with a total of 13,131 multi-level forget probes. These probes are divided into 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial attack probes. 

% RWKU is designed to rigorously evaluate the models' ability to forget specific knowledge while preserving relevant information.

% comprises 200 unlearning targets, specifically focusing on well-known public figures, and includes a total of 13,131 multi-level forget probes. These probes are categorized into 3,268 fill-in-the-blank probes, 2,879 question-answer probes, and 6,984 adversarial attack probes, which rigorously test the models' ability to forget specific knowledge while retaining relevant information. 

\subsection{Multimodal Benchmarks}

\noindent \textbf{FIUBench}~\cite{ma2024benchmarking} comprises 400 synthetic faces paired with fictitious private data such as personal backgrounds, health records, criminal histories, phone numbers, occupations and incomes are randomly assigned. GPT-4o generates 20 question-answer pairs for each profile.

\noindent \textbf{MLLMU-Bench}~\cite{liu2024protecting} contains 500 fictitious profiles and 153 celebrity profiles, each with 14+ question-answer pairs evaluated in multimodal (image+text) and unimodal (text-only) settings. It features 20.7k questions, with fictitious profiles generated by GPT-4o and real celebrity profiles reviewed by experts. The test set includes 3.5k paraphrased questions and 500 modified images with pose variations. An additional utility set uses celebrity profiles. 


\noindent\textbf{CLEAR}~\cite{dontsov2024clear} focuses on person unlearning, characterizing unlearning across textual and visual modalities. It generates consistent images through a comprehensive strategy and links them to the corresponding author-related questions from TOFU. It includes a total of 200 fictitious individuals linked with 3.7k visual question-answer pairs and 4k textual question-answer pairs, enabling a thorough evaluation of unimodal and multi-modal unlearning techniques.





\section{Evaluation Measures}
\label{sec:evaluation}
% Unlearning methodologies are expected to thoroughly forget the targeted knowledge while maintaining model utility. 

This section outlines common evaluation measures used to access both utility preservation and forgetting quality. 

% The efficiency of algorithms, including their computational complexity, is also crucial; however, it is rarely discussed in existing studies.

\subsection{Utility Preservation Evaluation}
\label{subsec:utility}

We categorize the methods for evaluating model utility into general utility and localized utility. General utility evaluates the model's overall knowledge, including downstream task performance and related metrics. In contrast, localized utility examines fine-grained changes, such as output or logit probability similarity, in the model's output before and after unlearning on task-specific data.

\subsubsection{General Utility}
 
% Researchers often use publicly available datasets as a source of world knowledge to assess the overall performance of unlearning models. \cite{maini2024tofu} evaluates the model's ability to answer world facts using metrics from the above subsection. In addition, Accuracy and GPT are commonly used to assess overall performance.

 % such as TriviaQA~\cite{joshi-etal-2017-triviaqa} and TruthfulQA~\cite{lin-etal-2022-truthfulqa}, MME~\cite{fu2023mme} 
\paragraph{Accuracy on General Benchmarks} Unlearned models must retain their original world knowledge without degradation during the unlearning process. It is common to use additional benchmarks~\cite{joshi-etal-2017-triviaqa,fu2023mme} to evaluate the utility preservation. 

\paragraph{GPT-Eval} LLM-as-a-Judge has been widely adopted across various evaluation scenarios due to its flexibility and ability to assess multiple dimensions. It provides semantic insights that traditional measures might overlook. For instance, \mycite{ma2024benchmarking}  leverage GPT-4o Mini to evaluate unlearning algorithms based on correctness, helpfulness, and relevance, assigning an overall score on a scale from 0 to 1.
% Model utility focuses on locality and overall performance. Locality ensures unlearning affects only the forget set while preserving unrelated knowledge in the retain set. We compute the values of ROUGE, Probability, Truth Ratio, Cosine Similarity, and Token Entropy on the retain set to evaluate locality performance. Overall performance evaluates downstream task accuracy, output diversity, and fluency. 

\subsubsection{Localized Utility}


\paragraph{ROUGE} is a measure for evaluating word-level alignment between a model's output and the ground truth answer. The ROUGE-L recall score is used to quantify the overlap between the generated output and the reference answer~\cite{maini2024tofu,yuan2024closer}. 

\paragraph{Probability}~\cite{maini2024tofu} evaluates the model's ability to predict the correct answer for a given question by computing the normalized conditional probability of the answer, defined as $P(y|x) = \frac{1}{T} \sum_{t=1}^{T} p(y_t | y_{<t} \circ x; \theta_{unl})$.

% \paragraph{Cosine Similarity (CS)}  \mycite{yuan2024closer} use the SentenceBERT~\cite{reimers-gurevych-2019-sentence} to get sentence embeddings of output before and after unlearning, and then calculate their cosine similarity and truncate the value less than 0, denote as $CS = \max(\cos(g(x; \theta_{ori}), g(x; \theta_{unl}))), 0)$.

% \paragraph{Entailment Score (ES)} utilizes a pre-trained natural language inference (NLI) model~\cite{yuan2024closer} to predict the relationship between the model's output and the corresponding ground truth for each question. The score is calculated as the proportion of pairs classified as ``entailment," which serves as the final entailment score.




\paragraph{Truth Ratio} evaluates the relative likelihood of a correct answer compared to an incorrect one for a given question~\cite{maini2024tofu}. It is defined as the ratio of the average normalized conditional probability of perturbed incorrect answers $\bar{y}$ from the $\mathcal{A}$, to that of paraphrased correct answer $\tilde{y}$ of the ground truth $y$. Ideally, when a model lacks the relevant knowledge, it should assign similar prediction probabilities to both correct and incorrect answers. Formally, the truth ratio on the forget set is defined as $R_{truth}(y) = \frac{\frac{1}{\mathcal{A}}\sum_{\bar{y}\in \mathcal{A}} p(\bar{y}|x;\theta_{unl})^{1/|\bar{y}|}}{p(\tilde{y}|x;\theta_{unl})^{1/|\tilde{y}|} }$,
% \begin{equation}
% R_{truth}(y) = \frac{\frac{1}{\mathcal{A}}\sum_{\bar{y}\in \mathcal{A}} p(\bar{y}|x;\theta_{unl})^{1/|\bar{y}|}}{p(\tilde{y}|x;\theta_{unl})^{1/|\tilde{y}|} },
% \label{eq:truth_ratio}
% \end{equation}
where $\bar{y}$ and $\tilde{y}$ can be generated by LLMs. It is expressed as $\max(0, 1-R_{truth}(y))$ on the retain set and world knowledge datasets.



\subsection{Forget Efficacy Evaluation}
\label{subsec:forget}
% Assessing utility on the forget set is a straightforward method for evaluating forgetting quality. However, 

% Recent studies~\cite{lynch2024eight,ma2024benchmarking} emphasize the importance of distinguishing genuine forgetting from mere suppression, as suppressed knowledge can often be recovered through adversarial techniques. This section reviews existing methods for comprehensive forgetting quality evaluation beyond utility-based evaluation, which utilizes the previously mentioned utility metrics on the forget set.
Utility-based evaluation, which compares the utility changes of a model on the forget set before and after unlearning, is the most commonly assessment method. However, recent studies~\cite{lynch2024eight,ma2024benchmarking} have highlighted the critical need to differentiate between real forgetting and mere suppression, where the suppressed knowledge can often be recovered through adversarial techniques.  This section reviews existing methods for comprehensive forgetting quality evaluation beyond utility-based evaluation.



\subsubsection{Input-Based Evaluation}

\paragraph{Paraphrasing and other Languages}  Using generative models such as GPT, paraphrased questions can be easily generated to test the unlearning robustness~\cite{maini2024tofu}. Additionally, inspired by the performance disparity of large models across different languages, \mycite{lynch2024eight} evaluates the robustness of LLM unlearning by translating inputs into Spanish and Russian.


\paragraph{In-Context Relearning} This method leverages demonstrations to interfere with the model's generation process, prompting it to answer questions it is supposed to forget rather than refusing them. \mycite{lynch2024eight} show that providing unlearned models with small amounts of general context related to Harry Potter can effectively resurface suppressed knowledge.

\paragraph{Jailbreak Prompts} 
Jailbreak methods manipulate LLM behavior using black-box techniques like role-playing and obfuscation or white-box strategies that optimize prefixes and suffixes to maximize affirmative response likelihood. In the context of learning,  \mycite{lynch2024eight} evaluated two jailbreak prompts that successfully access the model's memory of the target dataset through privilege escalation and role-playing. ~\mycite{lucki2024adversarial} introduced the improved GCG to bypass the unlearning mechanism  by optimizing prompt prefixes and leveraging early token loss. 
% , effectively circumventing the RMU~\cite{li2024the}.

% Jailbreaks have successfully induced dangerous, unsafe, or sensitive knowledge that LLMs typically do not produce~\cite{shah2023scalabletransferableblackboxjailbreaks}. Applying jailbreak to elicit unlearned knowledge is a good way to evaluate unlearning. \cite{lynch2024eight} test two jailbreaking prompts designed based on prior successful jailbreaks against Llama-2 models~\cite{shen2024donowcharacterizingevaluating}. This method does induce some knowledge that should be forgotten. Greedy Coordinate Gradient (GCG)~\cite{zou2023universal} is a straightforward attack method designed to find suffixes that, when attached to a wide range of queries, compel LLMs to generate objectionable content by maximizing the likelihood of an affirmative response (rather than a refusal). Unlike manually engineered prompts, GCG automatically generates adversarial suffixes using a combination of greedy and gradient-based search techniques. However, GCG has been reported ineffective against advanced unlearning RMU~\cite{li2024the,huu2024effects}. Further, \cite{lucki2024adversarial} propose enhanced GCG by adjusting the attack strategy (e.g., optimizing prompt prefixes and focusing on the loss of early tokens) and show enhanced GCG has the ability to bypass the interference mechanisms of RMU.



\subsubsection{Logit-Based Evaluation}
\noindent \textbf{Kolmogorov-Smirnov Test (KS-Test)} is a non-parametric statistical method widely used to compare two probability distributions. In the context of forget quality evaluation, the KS-Test assesses the divergence between the distributions of model outputs (e.g., token probabilities or embeddings) before and after unlearning specific data. By computing the maximum absolute difference between the cumulative distribution functions (CDFs) of the two distributions, the test quantifies the extent to which the influence of the unlearned data has been mitigated. A smaller KS statistic indicates a higher degree of successful unlearning. The test's non-parametric nature ensures its robustness across diverse datasets and scenarios, offering a reliable evaluation measure~\cite{maini2024tofu,ma2024benchmarking}.

% An ideal unlearning model should perform as similarly as the retained model, which is fine-tuned on the retain set only. Therefore, a natural idea is to quantify forget quality by measuring the distribution differences of metrics on the forget set between retained and unlearned models. KS-Test, , is a non-parametric test used to compare two samples to determine if they come from the same distribution. Specifically, we compute the empirical cumulative distribution functions (CDFs) of a specific metric, such as the truth ratio, for retained and unlearned models, and then calculate the KS-Test statistic and p-value using statistical tools like SciPy in Python. The KS-Test statistic $\mathcal{D}$ represents the maximum distance between the CDFs of the metric $m$ :
% \begin{equation}
% \small
% \mathcal{D} = \sup_{m}|F_{1}(m)-F_{2}(m)|
% \label{eq:ks_test}
% \end{equation}
% where $F_{1}(m)$ and $F_{2}(m)$ are the CDFs on retained and unlearned models. A small $\mathcal{D}$ value indicates similar distributions, while a large $\mathcal{D}$ value indicates differences. The p-value indicates the probability that the observed difference is due to chance. A low p-value suggests the distributions are significantly different, whereas a high p-value suggests no significant difference.

\noindent \textbf{Membership Inference Attacks (MIAs)} determine whether a specific data point is part of a model's training data (member) or originates from outside the training set (non-member). Therefore, it serves as a tool to evaluate unlearning efficacy~\cite{jin2024rwku,yao-etal-2024-machine}. ~\mycite{jin2024rwku} used various MIA methods to assess the robustness of unlearning and found that many unlearning methods failed under such attacks.


% A membership inference attack determines whether a specific data point was part of the model's training data (member) or originates from outside the training set (non-member).           


% can be used to verify whether the information intended for forgetting still exists in the model after unlearning. Min-K\% Prob~\cite{shi2024detecting} method is often used for performing MIA on LLMs or VLMs. The Min-K\% Prob computes the average log-likelihood for the K\% tokens within minimum probabilities in the generated answer of each question within the QA pairs. Consider the generated answer in a sequence denoted as $\alpha = (\alpha_1, \alpha_2,...,\alpha_N)$, the log-likelihood of a token $\alpha_i$ is computed as $\log p(\alpha_i|\alpha_1,...,\alpha_{i-1})$. Selecting the K\% tokens from $\alpha$ with the minimum token probability to form a set Min-K\%($\alpha$), we compute the MIA score:
% \begin{equation}
% \small
% \text{Min-K\% Pro} = \frac{1}{|\text{Min-K\%}(\alpha)|} \sum_{\alpha_i \in \text{Min-K\%($\alpha$)}} \log p(\alpha_i|\alpha_1,...,\alpha_{i-1})
% \end{equation}






\subsubsection{Model Intervention}

\paragraph{Relearning through Fine-tuning} \mycite{hu2024jogging} suggested to use benign, publicly available data loosely related to the unlearned information. For example, while specific details about virus design might be unavailable, general knowledge such as ``\emph{What is avian influenza?}'' is readily accessible. Their experiments demonstrate that augmenting with such benign data can inadvertently cause LLMs to relearn harmful knowledge, a finding further validated by \cite{lucki2024adversarial}.



\paragraph{LLM Quantization} facilitates faster inference and efficient deployment on resource-constrained devices with minimal performance degradation. However, recent studies~\cite{zhang2024does,lucki2024adversarial} indicated that quantization can inadvertently aid in recovering unlearned knowledge. \mycite{zhang2024does} reported that for unlearning methods constrained by utility requirements, unlearned models retain an average of 21\% of the intended forgotten knowledge in full precision, which dramatically increases to 83\% after 4-bit quantization.

% Research work~\cite{zhang2024does,lucki2024adversarial} find that applying quantization to an unlearned model can lead to the recovery of forgotten knowledge. For quantization, consider a group or block of weight $\textbf{w}$, the linear operation can be expressed as $y=\mathbf{wx}$; while the quantized version is denoted as $y=Q(\mathbf{w})\mathbf{x}$, where $Q(\cdot)$ is the quantization function. Specifically, the quantization function is defined as~\cite{MLSYS2024_42a452cb}:
% \begin{equation}
% Q(\mathbf{w})= \Delta \cdot \text{Round}(\frac{\mathbf{w}}{\Delta}), \quad \Delta=\frac{\max(|\mathbf{w}|)}{2^{N-1}}
% \end{equation}
% where $N$ is the number of quantization bits, and $\Delta$ is the quantization scale factor (step size) determined by the absolute maximum value of $\mathbf{w}$.

\paragraph{Activation Intervention} Recent research~\cite{arditi2024refusal} demonstrated that refusal behavior in language models is mediated by a single direction in the activation space. Building upon this, outputs from each transformer block of both original and unlearned models are collected using the forget preference dataset~\cite{lucki2024adversarial,seyitouglu2024extracting}, and the refusal direction for each layer is computed by analyzing the differences in means. During inference, the refusal direction is removed at each layer. Extensive experiments confirmed the effectiveness of this approach.



% \begin{table*}[ht]
% \centering
% \small
% \setlength{\tabcolsep}{1.5pt}
% \caption{A summary of existing LLM unlearning studies.}

% \begin{tabular}{p{2.4cm} p{3.2cm} p{3.1cm} p{2.8cm} p{2.5cm} p{2.2cm}}
% \toprule
% % \textbf{Tasks} & \textbf{Pro & Con} & \textbf{Related Work} & \textbf{Method} & \textbf{Evaluation} & \textbf{Application} \\
% \textbf{Methodology} & \textbf{Features} & \textbf{Related Work} & \textbf{Application} & \textbf{Evaluation} & \textbf{Contribution} \\

% \midrule

% % --------------------------
% \makecell[l]{\textit{Relabeling-Based}\\ \textit{Fine-Tuning}}& 
% \makecell[l]{ \makecell[l]{Easy to implement \& \\Not robust unlearning}} & 
% \cite{eldan2024whos} &
% \textit{Privacy\&Copyright} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------

% \multirow{2}{*}{\raggedright \textit{Gradient Ascent}} & 
% \multirow{2}{*}{\raggedright \makecell[l]{Risk of severe\\ model degradation}}  & 
% \cite{jang-etal-2023-knowledge} & 
% \textit{Privacy \& Copyright} &
% \textit{Model A} &
% Key insight A \\
% & & 
% \cite{yao-etal-2024-machine} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % \makecell[l]{Avoid several model\\degradation}
% % --------------------------
% % Preference Optimization (3 papers)
% \multirow{3}{2.5cm}{\raggedright \textit{Preference Optimization}} & 
% \multirow{3}{2.5cm}{\raggedright Better utility preservation} & 
% \cite{maini2024tofu} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{zhang2024negative} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{zhang2024safe} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------

% \multirow{2}{2.5cm}{\raggedright \textit{Reinforcement Learning}} & 
% \multirow{2}{2.5cm}{\raggedright \makecell[l]{Need for an accurate\\ reward model} } & 
% \cite{kassem-etal-2023-preserving} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{NEURIPS2022_b125999b} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Representation Engineering (2 papers)
% \multirow{2}{*}{\raggedright \makecell[l]{\textit{Representation}\\ \textit{Engineering}}} & 
% \multirow{2}{*}{\raggedright \makecell[l]{Robust to knowledge\\ restoration}} & 
% \cite{li2024the} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{huu2024effects} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Locate-then-Unlearn (6 papers)
% \multirow{4}{2.5cm}{\raggedright \textit{Locate-then-\\Unlearn}} & 
% \multirow{4}{2.5cm}{\raggedright \makecell[l]{More complex due to \\knowledge localization  \\\& enables more robust\\unlearning}  } & 
% \cite{wu-etal-2023-depn} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{jia2024wagle} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{guo2024mechanistic} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{hong2024intrinsic} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\

% \midrule

% % --------------------------
% % Task Vector (2 papers)
% \multirow{3}{2.5cm}{\raggedright \textit{Task Vector}} & 
% \multirow{3}{2.5cm}{\raggedright \makecell[l]{Hard to unlearn precisely\\\& Commonly use for AI\\Alignment} } & 
% \cite{ilharco2023editing} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{liu-etal-2024-towards-safer} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{gao-etal-2024-ethos} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Contrastive Decoding (1 paper)
% {\raggedright \makecell[l]{\textit{Contrastive}\\ \textit{Decoding}}} & 
% {\raggedright \makecell[l]{Higher computational \\cost for decoding} } & 
% \cite{ji2024reversing} &
% \textit{AI Alignment} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% % --------------------------
% % Knowledge Distillation (2 papers)
% \multirow{2}{2.5cm}{\raggedright \textit{Knowledge Distillation} } & 
% \multirow{2}{2.5cm}{\raggedright \textit{Privacy \& Copyright} } & 
% \cite{wang2024rkld} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{dong2024unmemorization} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% \midrule

% \multirow{3}{*}{\raggedright \makecell[l]{\textit{Input/Output-}\\\textit{Based}}} & 
% \multirow{3}{*}{\raggedright \makecell[l]{Easy to implement \&\\Not real knowledge\\ removal} } & 
% \cite{Incontextunlearning} &
% \textit{Privacy \& Copyright} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{liu2024large} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\
% & & 
% \cite{thaker2024guardrail} &
% \textit{Application B} &
% \textit{Model B} &
% Key insight B \\

% \bottomrule
% \end{tabular}
% \label{tab:unlearning_comparison}
% \end{table*}

% \paragraph{Brief Summary}



\section{Future Directions}
\label{sec:challenges}

% \paragraph{Robustness Against Attacks}

% \paragraph{Verifiable and Provable Unlearning} One critical future direction in LLM unlearning is developing verifiable methods that provide intrinsic guarantees for knowledge removal. Current approaches predominantly rely on behavioral tests, such as assessing whether a model refrains from generating outputs associated with the target knowledge. However, these tests often fall short of detecting residual traces of knowledge embedded in the model's parameters, creating vulnerabilities that adversaries can exploit. While preliminary efforts have been made toward unlearning verification, they are largely confined to simpler models, such as linear or logistic regression~\cite{10.5555/3524938.3525297,eisenhofer2022verifiable,10.14778/3641204.3641220}, and encounter substantial obstacles when scaled to large language models. Advancing this field requires the formulation of robust formal frameworks and algorithms that enable verifiable and provably secure unlearning, building greater trust in applications.

\paragraph{Theoretical framework} Existing LLM unlearning methods often lack formal guarantees of effectiveness. While locate-then-unlearn approaches~\cite{wu-etal-2023-depn,jia2024wagle} enhance interpretability, they do not establish a rigorous theoretical foundation. A crucial future direction is to develop a comprehensive framework that formally defines and ensures its effectiveness. This could involve leveraging principles from information theory~\cite{jeon2024information} and other theoretical approaches to provide a more principled understanding of LLM unlearning.


\paragraph{Unlearning in Multimodal Models} While numerous multimodal datasets have been introduced for multimodal unlearning, current methods remain largely confined to text-based unlearning approaches~\cite{ma2024benchmarking,liu2024protecting,dontsov2024clear}. 
Future research should prioritize the development of techniques capable of identifying and isolating modality-specific representations within MLLMs. 
% Such advancements would facilitate the targeted removal of visual or textual information while preserving unrelated knowledge. 
Additionally, robust evaluation benchmarks are essential for assessing the effectiveness of multimodal unlearning methods in disentangling representations where knowledge is intertwined across both textual and visual inputs.
% % With the rise of multimodal large language models (MLLMs) that incorporate both text and visual information, cross-modal unlearning presents a significant challenge. 
% While numerous multimodal datasets have been proposed for evaluating unlearning algorithms, existing approaches still primarily rely on text-based unlearning methods~\cite{ma2024benchmarking,liu2024protecting,dontsov2024clear}. Future research could focus on techniques for identifying and isolating modality-specific representations within MLLMs, enabling targeted unlearning of visual or textual information without disrupting unrelated knowledge. 
% Robust evaluation frameworks are needed to assess the effectiveness in disentangling representations where knowledge is intertwined across both textual and visual inputs.
% 


% There is currently a lack of algorithms specifically designed for unlearning visual information, as well as an absence of comprehensive evaluations to assess the robustness of multimodal unlearning. 

% Future research could investigate techniques that allow for the removal of specific knowledge across multiple modalities simultaneously, ensuring that the model forgets certain associations between text and image data without impacting its ability to process other types of input.

% \paragraph{Scalable and Efficient Unlearning}



\paragraph{Real-World Complex Requests} Current unlearning methods primarily focus on removing specific data points from the model, requiring explicit target data points (sequences) to be provided. However, real-world unlearning requests may differ from this assumption. A significant future direction for LLM unlearning lies in addressing more complex requests, such as entity-level unlearning, which aims to remove all knowledge related to a specific entity across diverse contexts and associations. This involves not only forgetting explicit facts, but also erasing implicit or derived knowledge. \mycite{Choi2024OptOutIE} introduced datasets to evaluate the effectiveness of algorithms in entity-level unlearning tasks. Looking ahead, even more complex scenarios may emerge, such as removing all information about a specific organization, or erasing entire domains of knowledge such as medical or criminal records.

% Additionally, time-sensitive or domain-specific unlearning could play a critical role in scenarios requiring comprehensive privacy protection, such as complying with GDPR data deletion requests or mitigating biases and harmful stereotypes linked to individuals or groups. 

\section{Conclusion} We provided a comprehensive survey of recent advances in large language model (LLM) unlearning. We began by defining the problem and outlining the foundational settings of LLM unlearning. To offer a structured understanding, we proposed a novel taxonomy that categorizes existing research from diverse perspectives. We futher explored the methodologies used to implement unlearning and evaluates the effectiveness of these approaches in achieving the desired forgetting. Finally, we examined the key challenges in the field and identified promising directions for future research, thus offering valuable insights for researchers and practitioners.
\appendix

% \section*{Ethical Statement}

% There are no ethical issues.

% \section*{Acknowledgments}

% The preparation of these instructions and the \LaTeX{} and Bib\TeX{}
% files that implement them was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Preparation of the Microsoft Word file was supported by IJCAI.  An
% early version of this document was created by Shirley Jowell and Peter
% F. Patel-Schneider.  It was subsequently modified by Jennifer
% Ballentine, Thomas Dean, Bernhard Nebel, Daniel Pagenstecher,
% Kurt Steinkraus, Toby Walsh, Carles Sierra, Marc Pujol-Gonzalez,
% Francisco Cruz-Mencia and Edith Elkind.

\newpage
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}

\end{document}

