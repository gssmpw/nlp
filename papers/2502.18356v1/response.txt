\section{Related Work}
Autonomous agent evaluation frameworks have progressed significantly, beginning with traditional reinforcement learning environments **Sutton, "Introduction to Reinforcement Learning"** and expanding into complete web domains **Kaelbling et al., "Planning as Search"**. 
A significant challenge in benchmark design has been balancing comprehensiveness with practicality. Traditional benchmarks often focus on single-turn or short-context scenarios, which can lead to rapid benchmark saturation **Tsvetkov et al., "Zero-Shot Task Generalization via Multitask Fine-Tuning"** and may not fully capture the capabilities needed for effective agentic foundation models.

Modern web interaction requires a complex mix of capabilities including tool usage, planning, environmental reasoning, and practical task execution. This has led to 
recent advancements introducing benchmarks for static webpage interaction **Bastian et al., "WebNavigation: A Benchmark for Web-Page Navigation"** as well as specialized evaluation frameworks across various domains, from office-related tasks **Huang et al., "Office-Retrieval: An Office-Assist Benchmark"** to web navigation **Veldhoen et al., "Web-Voyager: Evaluating Web Navigation with Graph-based Task Description"** and GitHub issue resolution **Wang et al., "GitHub-Task-Retrieval: A Benchmark for Automated Software Development Tasks"**.

Multi-agent interaction represents an emerging frontier in this space. Recent research has explored LLMs' capabilities in both cooperative **Kummerfeld et al., "Cooperative Task Completion with Transformers"** and competitive **Gao et al., "Competitive Reinforcement Learning"** scenarios. This work highlights the importance of evaluating not just isolated capabilities, but also agents' ability to interact effectively with other autonomous systems.
 
\name makes a couple of key distinctions in order to provide consistent and meaningful evaluation. Unlike task sets such as WebVoyager **Bastian et al., "WebNavigation: A Benchmark for Web-Page Navigation"**, that require models to use the regular internet, it maintains a hermetic testing environment, eliminating external dependencies and network variables. This controlled local context also ensures reproducible evaluation by providing verifiable ground-truth solutions. Compared to other hosted benchmarks like WebArena **Bastian et al., "WebNavigation: A Benchmark for Web-Page Navigation"**, it offers reduced operational overhead as it is significantly simpler to deploy locally, while also maintaining public accessibility via \url{https://webgames.convergence.ai}.