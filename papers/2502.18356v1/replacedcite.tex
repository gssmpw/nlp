\section{Related Work}
Autonomous agent evaluation frameworks have progressed significantly, beginning with traditional reinforcement learning environments ____, and expanding into complete web domains ____. 
A significant challenge in benchmark design has been balancing comprehensiveness with practicality. Traditional benchmarks often focus on single-turn or short-context scenarios, which can lead to rapid benchmark saturation ____ and may not fully capture the capabilities needed for effective agentic foundation models.

Modern web interaction requires a complex mix of capabilities including tool usage, planning, environmental reasoning, and practical task execution. This has led to 
recent advancements introducing benchmarks for static webpage interaction ____ as well as specialized evaluation frameworks across various domains, from office-related tasks ____ to web navigation ____ and GitHub issue resolution ____.

Multi-agent interaction represents an emerging frontier in this space. Recent research has explored LLMs' capabilities in both cooperative ____ and competitive ____ scenarios. This work highlights the importance of evaluating not just isolated capabilities, but also agents' ability to interact effectively with other autonomous systems.
 
\name makes a couple of key distinctions in order to provide consistent and meaningful evaluation. Unlike task sets such as WebVoyager ____, that require models to use the regular internet, it maintains a hermetic testing environment, eliminating external dependencies and network variables. This controlled local context also ensures reproducible evaluation by providing verifiable ground-truth solutions. Compared to other hosted benchmarks like WebArena ____, it offers reduced operational overhead as it is significantly simpler to deploy locally, while also maintaining public accessibility via \url{https://webgames.convergence.ai}.