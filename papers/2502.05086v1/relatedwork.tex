\section{Related Works}
In robotic manipulation, most simulated environments and datasets primarily focus on fundamental tasks such as picking, placing, in-hand manipulation, lifting, and stacking \cite{rtx, droid,shafiullah2023bringing,walke2023bridgedata}, as shown in Table \ref{tab:datasets}. While these tasks are essential for understanding basic robotic capabilities, they are typically limited to short-horizon primitive skills and fail to provide the critical data necessary for precise, contact-rich manipulation. In contrast, complex long-horizon tasks such as assembly and disassembly, which require stringent tolerances, demand accurate and high-resolution contact information, including force-torque data, that visual sensing alone cannot reliably capture. For instance, force-torque sensing plays a pivotal role in assembly tasks involving a NIST assembly board, where precise measurements are essential to detect and respond to contact events with higher accuracy.

Existing datasets, such as the Furniture Benchmark \cite{heo2023furniturebench}, have made progress in addressing long-horizon tasks like furniture assembly. However, they lack the high-quality force-torque data required for the tight tolerances demanded in these applications. Similarly, while datasets like RH20T include manipulation data with audio and force-torque sensor information, they do not address the long-horizon task sequences needed for complex assemblies \cite{fang2024rh20t}. Vision-based tactile datasets, such as SPARSH, provide valuable insights into contact-rich manipulation, but they too fall short in capturing the extended temporal dynamics and multimodal sensing required for precise assembly and disassembly tasks~\cite{higuera2024sparsh}. To address these limitations, REASSEMBLE introduces a novel dataset incorporating high-resolution force-torque sensing specifically tailored for low-tolerance, high-precision, and long-horizon tasks.

The increasing prevalence of automation in robotic manipulation tasks highlights the necessity of effective skill assessment, task monitoring, and summarization to enhance system performance and reliability. Temporal action segmentation (TAS) plays a critical role in achieving these objectives by identifying and classifying distinct actions within continuous temporal sequences. Numerous datasets have been developed to support temporal action segmentation~\cite{50Salads,GTEA,breakfast}. For example, the 50Salads dataset is a widely used benchmark where humans perform salad preparation tasks \cite{50Salads}. This dataset provides temporally labeled actions for long-duration videos, facilitating the training and evaluation of models for action segmentation. These models can learn to segment complex temporal sequences into meaningful subtasks. However, such datasets primarily focus on human activity and often lack relevance to robotic manipulation tasks. Robotics-specific datasets, such as RT-1, Bridge V2, and RoboSet, offer data tailored to specific robotic tasks \cite{brohan2022rt,walke2023bridgedata}. While these datasets are valuable for advancing task-specific learning, they are not designed for comprehensive temporal action segmentation across diverse manipulation contexts. Moreover, conventional action segmentation datasets like 50Salads predominantly rely on visual and temporal information, often overlooking critical multimodal data, such as force-torque measurements, which are essential for understanding robotic actions. Therefore, REASSEMBLE also addresses robotic action segmentation and incorporates multimodal data, including visual, force-torque, and temporal streams. 

Advancements in large vision-language models (LVLMs), such as PaLM-E, LLaVa, have shown impressive capabilities in reasoning about task conditions by leveraging visual information during robotic task execution \cite{driess2023palm,li2023m}. These models provide robust insights into system states, enabling robots to monitor and adapt to dynamic conditions. However, their significant computational requirements make them impractical for real-time applications, where lightweight and efficient models are essential for rapid decision-making. To address these challenges, the ConditionNet was introduced as a focused resource for condition monitoring in robotic systems \cite{sliwowski2024conditionnet}. While ConditionNet dataset ((Im)PerfectPour) is not aimed at contact-rich manipulation and long-horizon task. Building on these limitations, REASSEMBLE is designed to address the gaps in existing resources. It includes conditioned labeled multi-modal data such as visual, force-torque, and temporal streams, enabling more comprehensive condition monitoring. Crucially, it incorporates failure data to train models that can effectively learn to detect, understand, and respond to failures in real time. 

Event cameras are emerging as critical sensors in robotics, providing a fundamentally different data acquisition paradigm compared to traditional RGB cameras \cite{gehrig2024low}. Unlike frame-based cameras that capture static images at fixed intervals, event cameras asynchronously record changes in pixel intensity, resulting in high temporal resolution and low latency. These characteristics make them particularly suited for dynamic environments and fast-changing tasks such as SLAM and locomotion in unstructured settings \cite{eslam}. Despite the demonstrated utility of event cameras in applications like SLAM and dynamic navigation, to the best of our knowledge, no existing dataset incorporates event camera data for robotic manipulation tasks. This omission represents a significant gap in current research, as event cameras can provide invaluable information for robotic manipulation by capturing motion events with exceptional precision. For instance, during manipulation tasks, event cameras can highlight fine-grained movements, such as the subtle shifts of a robotic gripper or the motion of objects in the environment, even under challenging lighting conditions. One potential application of event camera data in robotic manipulation is the creation of attention masks derived from motion events. These attention masks focus on areas of movement, filtering out irrelevant static regions of the scene. This capability is particularly beneficial for monitoring tasks and identifying critical conditions, such as failures. REASSEMBLE addresses this gap by incorporating event camera data into its multi-modal framework. It provides a unique opportunity to explore the role of event cameras in robotic manipulation, particularly for complex tasks like assembly and disassembly with a NIST Board \#1.