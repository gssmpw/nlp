@inproceedings{50Salads,
author = {Stein, Sebastian and McKenna, Stephen J.},
title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493482},
doi = {10.1145/2493432.2493482},
abstract = {This paper introduces a publicly available dataset of complex activities that involve manipulative gestures. The dataset captures people preparing mixed salads and contains more than 4.5 hours of accelerometer and RGB-D video data, detailed annotations, and an evaluation protocol for comparison of activity recognition algorithms. Providing baseline results for one possible activity recognition task, this paper further investigates modality fusion methods at different stages of the recognition pipeline: (i) prior to feature extraction through accelerometer localization, (ii) at feature level via feature concatenation, and (iii) at classification level by combining classifier outputs. Empirical evaluation shows that fusing information captured by these sensor types can considerably improve recognition performance.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {729–738},
numpages = {10},
keywords = {accelerometers, activity recognition, computer vision, multi-modal dataset, sensor fusion},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@inproceedings{GTEA,
  title={Learning to recognize objects in egocentric activities},
  author={Fathi, Alireza and Ren, Xiaofeng and Rehg, James M},
  booktitle={CVPR 2011},
  pages={3281--3288},
  year={2011},
  organization={IEEE}
}

@INPROCEEDINGS{breakfast,
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities}, 
  year={2014},
  volume={},
  number={},
  pages={780-787},
  keywords={Hidden Markov models;Dairy products;Accuracy;Speech recognition;Speech;Grammar;Sugar},
  doi={10.1109/CVPR.2014.105}}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{droid,
  title={Droid: A large-scale in-the-wild robot manipulation dataset},
  author={Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and others},
  journal={arXiv preprint arXiv:2403.12945},
  year={2024}
}

@INPROCEEDINGS{eslam,
  author={Zhu, Alex Zihao and Atanasov, Nikolay and Daniilidis, Kostas},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Event-Based Visual Inertial Odometry}, 
  year={2017},
  volume={},
  number={},
  pages={5816-5824},
  keywords={Cameras;Optical imaging;Optical sensors;Adaptive optics;Tracking;Spatiotemporal phenomena},
  doi={10.1109/CVPR.2017.616}}

@inproceedings{fang2024rh20t,
  title={Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot},
  author={Fang, Hao-Shu and Fang, Hongjie and Tang, Zhenyu and Liu, Jirong and Wang, Chenxi and Wang, Junbo and Zhu, Haoyi and Lu, Cewu},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={653--660},
  year={2024},
  organization={IEEE}
}

@article{gehrig2024low,
  title={Low-latency automotive vision with event cameras},
  author={Gehrig, Daniel and Scaramuzza, Davide},
  journal={Nature},
  volume={629},
  number={8014},
  pages={1034--1040},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{heo2023furniturebench,
  title={Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation},
  author={Heo, Minho and Lee, Youngwoon and Lee, Doohyun and Lim, Joseph J},
  journal={arXiv preprint arXiv:2305.12821},
  year={2023}
}

@article{higuera2024sparsh,
  title={Sparsh: Self-supervised touch representations for vision-based tactile sensing},
  author={Higuera, Carolina and Sharma, Akash and Bodduluri, Chaithanya Krishna and Fan, Taosha and Lancaster, Patrick and Kalakrishnan, Mrinal and Kaess, Michael and Boots, Byron and Lambeta, Mike and Wu, Tingfan and others},
  journal={arXiv preprint arXiv:2410.24090},
  year={2024}
}

@article{li2023m,
  title={M\textsuperscript{3}IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
  author={Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others},
  journal={arXiv preprint arXiv:2306.04387},
  year={2023}
}

@INPROCEEDINGS{rtx,
  author={ O’Neill et. al., Abby},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0}, 
  year={2024},
  volume={},
  number={},
  pages={6892-6903},
  keywords={Learning systems;Adaptation models;Computer vision;Computational modeling;Collaboration;Data models;Task analysis},
  doi={10.1109/ICRA57147.2024.10611477}}

@article{shafiullah2023bringing,
  title={On bringing robots home},
  author={Shafiullah, Nur Muhammad Mahi and Rai, Anant and Etukuru, Haritheja and Liu, Yiqian and Misra, Ishan and Chintala, Soumith and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2311.16098},
  year={2023}
}

@article{sliwowski2024conditionnet,
  title={ConditionNET: Learning Preconditions and Effects for Execution Monitoring},
  author={Sliwowski, Daniel and Lee, Dongheui},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  publisher={IEEE}
}

@inproceedings{walke2023bridgedata,
  title={Bridgedata v2: A dataset for robot learning at scale},
  author={Walke, Homer Rich and Black, Kevin and Zhao, Tony Z and Vuong, Quan and Zheng, Chongyi and Hansen-Estruch, Philippe and He, Andre Wang and Myers, Vivek and Kim, Moo Jin and Du, Max and others},
  booktitle={Conference on Robot Learning},
  pages={1723--1736},
  year={2023},
  organization={PMLR}
}

