@article{heo2023furniturebench,
  title={Furniturebench: Reproducible real-world benchmark for long-horizon complex manipulation},
  author={Heo, Minho and Lee, Youngwoon and Lee, Doohyun and Lim, Joseph J},
  journal={arXiv preprint arXiv:2305.12821},
  year={2023}
}
@inproceedings{fang2024rh20t,
  title={Rh20t: A comprehensive robotic dataset for learning diverse skills in one-shot},
  author={Fang, Hao-Shu and Fang, Hongjie and Tang, Zhenyu and Liu, Jirong and Wang, Chenxi and Wang, Junbo and Zhu, Haoyi and Lu, Cewu},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={653--660},
  year={2024},
  organization={IEEE}
}

@article{shafiullah2023bringing,
  title={On bringing robots home},
  author={Shafiullah, Nur Muhammad Mahi and Rai, Anant and Etukuru, Haritheja and Liu, Yiqian and Misra, Ishan and Chintala, Soumith and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2311.16098},
  year={2023}
}


@article{higuera2024sparsh,
  title={Sparsh: Self-supervised touch representations for vision-based tactile sensing},
  author={Higuera, Carolina and Sharma, Akash and Bodduluri, Chaithanya Krishna and Fan, Taosha and Lancaster, Patrick and Kalakrishnan, Mrinal and Kaess, Michael and Boots, Byron and Lambeta, Mike and Wu, Tingfan and others},
  journal={arXiv preprint arXiv:2410.24090},
  year={2024}
}

@article{droid,
  title={Droid: A large-scale in-the-wild robot manipulation dataset},
  author={Khazatsky, Alexander and Pertsch, Karl and Nair, Suraj and Balakrishna, Ashwin and Dasari, Sudeep and Karamcheti, Siddharth and Nasiriany, Soroush and Srirama, Mohan Kumar and Chen, Lawrence Yunliang and Ellis, Kirsty and others},
  journal={arXiv preprint arXiv:2403.12945},
  year={2024}
}
@inproceedings{GTEA,
  title={Learning to recognize objects in egocentric activities},
  author={Fathi, Alireza and Ren, Xiaofeng and Rehg, James M},
  booktitle={CVPR 2011},
  pages={3281--3288},
  year={2011},
  organization={IEEE}
}

@inproceedings{
	RoboImitationPeng20,
	author = {Peng, Xue Bin and Coumans, Erwin and Zhang, Tingnan and Lee, Tsang-Wei Edward and Tan, Jie and Levine, Sergey},
	booktitle={Robotics: Science and Systems},
	year = {2020},
	month = {07},
	title = {Learning Agile Robotic Locomotion Skills by Imitating Animals},
	doi = {10.15607/RSS.2020.XVI.064}
}

@inproceedings{chhatpar2001search,
  title={Search strategies for peg-in-hole assemblies with position uncertainty},
  author={Chhatpar, Siddharth R and Branicky, Michael S},
  booktitle={Proceedings 2001 IEEE/RSJ International Conference on Intelligent Robots and Systems. Expanding the Societal Role of Robotics in the the Next Millennium (Cat. No. 01CH37180)},
  volume={3},
  pages={1465--1470},
  year={2001},
  organization={IEEE}
}
@article{kimble2022performance,
  title={Performance measures to benchmark the grasping, manipulation, and assembly of deformable objects typical to manufacturing applications},
  author={Kimble, Kenneth and Albrecht, Justin and Zimmerman, Megan and Falco, Joe},
  journal={Frontiers in Robotics and AI},
  volume={9},
  pages={999348},
  year={2022},
  publisher={Frontiers Media SA}
}
@article{zhao2023robotic,
  title={Robotic peg-in-hole assembly based on reversible dynamic movement primitives and trajectory optimization},
  author={Zhao, Huan and Chen, Yuxiang and Li, Xiangfei and Ding, Han},
  journal={Mechatronics},
  volume={95},
  pages={103054},
  year={2023},
  publisher={Elsevier}
}
@article{zhao2023learning,
  title={Learning fine-grained bimanual manipulation with low-cost hardware},
  author={Zhao, Tony Z and Kumar, Vikash and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2304.13705},
  year={2023}
}
@article{chi2023diffusion,
  title={Diffusion policy: Visuomotor policy learning via action diffusion},
  author={Chi, Cheng and Xu, Zhenjia and Feng, Siyuan and Cousineau, Eric and Du, Yilun and Burchfiel, Benjamin and Tedrake, Russ and Song, Shuran},
  journal={The International Journal of Robotics Research},
  pages={02783649241273668},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{saveriano2023dynamic,
  title={Dynamic movement primitives in robotics: A tutorial survey},
  author={Saveriano, Matteo and Abu-Dakka, Fares J and Kramberger, Alja{\v{z}} and Peternel, Luka},
  journal={The International Journal of Robotics Research},
  volume={42},
  number={13},
  pages={1133--1184},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{sliwowski2024conditionnet,
  title={ConditionNET: Learning Preconditions and Effects for Execution Monitoring},
  author={Sliwowski, Daniel and Lee, Dongheui},
  journal={IEEE Robotics and Automation Letters},
  year={2024},
  publisher={IEEE}
}
@article{li2023m,
  title={M\textsuperscript{3}IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
  author={Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others},
  journal={arXiv preprint arXiv:2306.04387},
  year={2023}
}
@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}
@article{liu2023reflect,
  title={Reflect: Summarizing robot experiences for failure explanation and correction},
  author={Liu, Zeyi and Bahety, Arpit and Song, Shuran},
  journal={arXiv preprint arXiv:2306.15724},
  year={2023}
}
@inproceedings{walke2023bridgedata,
  title={Bridgedata v2: A dataset for robot learning at scale},
  author={Walke, Homer Rich and Black, Kevin and Zhao, Tony Z and Vuong, Quan and Zheng, Chongyi and Hansen-Estruch, Philippe and He, Andre Wang and Myers, Vivek and Kim, Moo Jin and Du, Max and others},
  booktitle={Conference on Robot Learning},
  pages={1723--1736},
  year={2023},
  organization={PMLR}
}


@INPROCEEDINGS{eslam,
  author={Zhu, Alex Zihao and Atanasov, Nikolay and Daniilidis, Kostas},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Event-Based Visual Inertial Odometry}, 
  year={2017},
  volume={},
  number={},
  pages={5816-5824},
  keywords={Cameras;Optical imaging;Optical sensors;Adaptive optics;Tracking;Spatiotemporal phenomena},
  doi={10.1109/CVPR.2017.616}}


@article{gehrig2024low,
  title={Low-latency automotive vision with event cameras},
  author={Gehrig, Daniel and Scaramuzza, Davide},
  journal={Nature},
  volume={629},
  number={8014},
  pages={1034--1040},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@INPROCEEDINGS{breakfast,
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={The Language of Actions: Recovering the Syntax and Semantics of Goal-Directed Human Activities}, 
  year={2014},
  volume={},
  number={},
  pages={780-787},
  keywords={Hidden Markov models;Dairy products;Accuracy;Speech recognition;Speech;Grammar;Sugar},
  doi={10.1109/CVPR.2014.105}}

@inproceedings{50Salads,
author = {Stein, Sebastian and McKenna, Stephen J.},
title = {Combining embedded accelerometers with computer vision for recognizing food preparation activities},
year = {2013},
isbn = {9781450317702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493432.2493482},
doi = {10.1145/2493432.2493482},
abstract = {This paper introduces a publicly available dataset of complex activities that involve manipulative gestures. The dataset captures people preparing mixed salads and contains more than 4.5 hours of accelerometer and RGB-D video data, detailed annotations, and an evaluation protocol for comparison of activity recognition algorithms. Providing baseline results for one possible activity recognition task, this paper further investigates modality fusion methods at different stages of the recognition pipeline: (i) prior to feature extraction through accelerometer localization, (ii) at feature level via feature concatenation, and (iii) at classification level by combining classifier outputs. Empirical evaluation shows that fusing information captured by these sensor types can considerably improve recognition performance.},
booktitle = {Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
pages = {729–738},
numpages = {10},
keywords = {accelerometers, activity recognition, computer vision, multi-modal dataset, sensor fusion},
location = {Zurich, Switzerland},
series = {UbiComp '13}
}

@INPROCEEDINGS{rtx,
  author={ O’Neill et. al., Abby},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Open X-Embodiment: Robotic Learning Datasets and RT-X Models : Open X-Embodiment Collaboration0}, 
  year={2024},
  volume={},
  number={},
  pages={6892-6903},
  keywords={Learning systems;Adaptation models;Computer vision;Computational modeling;Collaboration;Data models;Task analysis},
  doi={10.1109/ICRA57147.2024.10611477}}

@article{McGeer01041990,
  author = {McGeer, Tad}, 
  title = {\href{http://ijr.sagepub.com/content/9/2/62.abstract}{Passive Dynamic Walking}}, 
  volume = {9}, 
  number = {2}, 
  pages = {62-82}, 
  year = {1990}, 
  doi = {10.1177/027836499000900206}, 
  URL = {http://ijr.sagepub.com/content/9/2/62.abstract}, 
  eprint = {http://ijr.sagepub.com/content/9/2/62.full.pdf+html}, 
  journal = {The International Journal of Robotics Research}
}

@article{kalman1960new,
  title={A new approach to linear filtering and prediction problems},
  author={Kalman, R.E.},
  journal={Journal of Basic Engineering},
  volume={82},
  number={1},
  pages={35--45},
  year={1960},
  publisher={Citeseer}
}


@article{michel_bilateral_2021,
	title = {Bilateral teleoperation with adaptive impedance control for contact tasks},
	volume = {6},
	number = {3},
	journal = {IEEE Robotics and Automation Letters},
	author = {Michel, Youssef and Rahal, Rahaf and Pacchierotti, Claudio and Giordano, Paolo Robuffo and Lee, Dongheui},
	year = {2021},
	note = {Publisher: IEEE},
	keywords = {Force, Robot sensing systems, Task analysis, Read, Robot kinematics, Robots, learning from demonstration, Impedance, Must Read, Compliance and impedance control, Safety, telerobotics and teleoperation},
	pages = {5429--5436},
}

@article{Yaprcr2019deeplearning,
author = {Yapıcı, Mutlu and Tekerek, Adem and Topaloglu, Nurettin},
year = {2019},
month = {12},
pages = {188-215},
title = {Literature Review of Deep Learning Research Areas},
volume = {5},
journal = {Gazi Journal of Engineering Sciences},
doi = {10.30855/gmbd.2019.03.01}
}

@article{rana2023sayplan,
  title={Sayplan: Grounding large language models using 3d scene graphs for scalable task planning},
  author={Rana, Krishan and Haviland, Jesse and Garg, Sourav and Abou-Chakra, Jad and Reid, Ian D and Suenderhauf, Niko},
  journal={CoRR},
  year={2023}
}

@inproceedings{saycan2022arxiv,
    title={Do As I Can and Not As I Say: Grounding Language in Robotic Affordances},
    author={Michael Ahn and Anthony Brohan and Noah Brown and Yevgen Chebotar and Omar Cortes and Byron David and Chelsea Finn and Chuyuan Fu and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Daniel Ho and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Eric Jang and Rosario Jauregui Ruano and Kyle Jeffrey and Sally Jesmonth and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Kuang-Huei Lee and Sergey Levine and Yao Lu and Linda Luu and Carolina Parada and Peter Pastor and Jornell Quiambao and Kanishka Rao and Jarek Rettinghouse and Diego Reyes and Pierre Sermanet and Nicolas Sievers and Clayton Tan and Alexander Toshev and Vincent Vanhoucke and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Mengyuan Yan and Andy Zeng},
    booktitle={arXiv preprint arXiv:2204.01691},
    year={2022}
}


@InProceedings{pmlr-v232-du23b,
  title = 	 {Vision-Language Models as Success Detectors},
  author =       {Du, Yuqing and Konyushkova, Ksenia and Denil, Misha and Raju, Akhil and Landon, Jessica and Hill, Felix and de Freitas, Nando and Cabi, Serkan},
  booktitle = 	 {Proceedings of The 2nd Conference on Lifelong Learning Agents},
  pages = 	 {120--136},
  year = 	 {2023},
  editor = 	 {Chandar, Sarath and Pascanu, Razvan and Sedghi, Hanie and Precup, Doina},
  volume = 	 {232},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22--25 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v232/du23b/du23b.pdf},
  url = 	 {https://proceedings.mlr.press/v232/du23b.html},
  abstract = 	 {Detecting successful behaviour is crucial for training intelligent agents. As such, generalisable reward models are a prerequisite for agents that can learn to generalise their behaviour. In this work we focus on developing robust success detectors that leverage large, pretrained vision-language models (Flamingo, Alayrac et al. (2022)) and human reward annotations. Concretely, we treat success detection as a visual question answering (VQA) problem, denoted SuccessVQA. We study three vastly different domains: (i) interactive language-conditioned agents in a simulated household, (ii) real world robotic manipulation, and (iii) “in-the-wild” human egocentric videos. We investigate the generalisation properties of a Flamingo-based success detection model across unseen language and visual changes in the first two domains, and find that the proposed method is able to outperform bespoke reward models in out-of-distribution test scenarios with either variation. In the last domain of “in-the-wild” human videos, we show that success detection on unseen real videos presents an even more challenging generalisation task warranting future work. We hope our results encourage further work in real world success detection and reward modelling with pretrained vision-language models.}
}

@INPROCEEDINGS{Sinha-RSS-24, 
    AUTHOR    = {Rohan Sinha AND Amine Elhafsi AND Christopher Agia AND Matt Foutter AND Edward Schmerling AND Marco Pavone}, 
    TITLE     = {{Real-Time Anomaly Detection and Reactive Planning with Large Language Models}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2024}, 
    ADDRESS   = {Delft, Netherlands}, 
    MONTH     = 7, 
    DOI       = {10.15607/RSS.2024.XX.114} 
} 


@INPROCEEDINGS{Brohan-RSS-23, 
    AUTHOR    = {Anthony Brohan AND Noah Brown AND Justice Carbajal AND Yevgen Chebotar AND Joseph Dabis AND Chelsea Finn AND Keerthana Gopalakrishnan AND Karol Hausman AND Alexander Herzog AND Jasmine Hsu AND Julian Ibarz AND Brian Ichter AND Alex Irpan AND Tomas Jackson AND Sally Jesmonth AND Nikhil Joshi AND Ryan Julian AND Dmitry Kalashnikov AND Yuheng Kuang AND Isabel Leal AND Kuang-Huei Lee AND Sergey Levine AND Yao Lu AND Utsav Malla AND Deeksha Manjunath AND Igor Mordatch AND Ofir Nachum AND Carolina Parada AND Jodilyn  Peralta AND Emily Perez AND Karl Pertsch AND Jornell  Quiambao AND Kanishka Rao AND Michael S Ryoo AND Grecia  Salazar AND Pannag R Sanketi AND Kevin  Sayed AND Jaspiar  Singh AND Sumedh  Sontakke AND Austin  Stone AND Clayton  Tan AND Huong  Tran AND Vincent Vanhoucke AND Steve  Vega AND Quan H Vuong AND Fei Xia AND Ted Xiao AND Peng Xu AND Sichun Xu AND Tianhe Yu AND Brianna  Zitkovich}, 
    TITLE     = {{RT-1: Robotics Transformer for Real-World Control at Scale}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2023}, 
    ADDRESS   = {Daegu, Republic of Korea}, 
    MONTH     = 7, 
    DOI       = {10.15607/RSS.2023.XIX.025} 
} 

@inproceedings{
zitkovich2023rt,
title={{RT}-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
author={Brianna Zitkovich and Tianhe Yu and Sichun Xu and Peng Xu and Ted Xiao and Fei Xia and Jialin Wu and Paul Wohlhart and Stefan Welker and Ayzaan Wahid and Quan Vuong and Vincent Vanhoucke and Huong Tran and Radu Soricut and Anikait Singh and Jaspiar Singh and Pierre Sermanet and Pannag R Sanketi and Grecia Salazar and Michael S Ryoo and Krista Reymann and Kanishka Rao and Karl Pertsch and Igor Mordatch and Henryk Michalewski and Yao Lu and Sergey Levine and Lisa Lee and Tsang-Wei Edward Lee and Isabel Leal and Yuheng Kuang and Dmitry Kalashnikov and Ryan Julian and Nikhil J Joshi and Alex Irpan and brian ichter and Jasmine Hsu and Alexander Herzog and Karol Hausman and Keerthana Gopalakrishnan and Chuyuan Fu and Pete Florence and Chelsea Finn and Kumar Avinava Dubey and Danny Driess and Tianli Ding and Krzysztof Marcin Choromanski and Xi Chen and Yevgen Chebotar and Justice Carbajal and Noah Brown and Anthony Brohan and Montserrat Gonzalez Arenas and Kehang Han},
booktitle={7th Annual Conference on Robot Learning},
year={2023},
url={https://openreview.net/forum?id=XMQgwiJ7KSX}
}

@article{wu2024tacdiffusion,
  title={TacDiffusion: Force-domain Diffusion Policy for Precise Tactile Manipulation},
  author={Wu, Yansong and Chen, Zongxie and Wu, Fan and Chen, Lingyun and Zhang, Liding and Bing, Zhenshan and Swikir, Abdalla and Knoll, Alois and Haddadin, Sami},
  journal={arXiv preprint arXiv:2409.11047},
  year={2024}
}

@ARTICLE{Kimble_2020_NIST,
  author={Kimble, Kenneth and Van Wyk, Karl and Falco, Joe and Messina, Elena and Sun, Yu and Shibata, Mizuho and Uemura, Wataru and Yokokohji, Yasuyoshi},
  journal={IEEE Robotics and Automation Letters}, 
  title={Benchmarking Protocols for Evaluating Small Parts Robotic Assembly Systems}, 
  year={2020},
  volume={5},
  number={2},
  pages={883-889},
  keywords={Task analysis;Belts;Robots;Fasteners;Robotic assembly;Benchmark testing;Gears;Flexible manufacturing systems;robotic assembly},
  doi={10.1109/LRA.2020.2965869}}

@inproceedings{radford2023robust,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@inproceedings{liu2023diffusion,
  title={Diffusion action segmentation},
  author={Liu, Daochang and Li, Qiyue and Dinh, Anh-Dung and Jiang, Tingting and Shah, Mubarak and Xu, Chang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10139--10149},
  year={2023}
}

@article{krishnan2017transition,
  title={Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning},
  author={Krishnan, Sanjay and Garg, Animesh and Patil, Sachin and Lea, Colin and Hager, Gregory and Abbeel, Pieter and Goldberg, Ken},
  journal={The International journal of robotics research},
  volume={36},
  number={13-14},
  pages={1595--1618},
  year={2017},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{willibald2022multi,
  title={Multi-level task learning based on intention and constraint inference for autonomous robotic manipulation},
  author={Willibald, Christoph and Lee, Dongheui},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={7688--7695},
  year={2022},
  organization={IEEE}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={402--419},
  year={2020},
  organization={Springer}
}

@inproceedings{
wu2023gello,
title={{GELLO}: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators},
author={Philipp Wu and Fred Shentu and Xingyu Lin and Pieter Abbeel},
booktitle={Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @ CoRL2023},
year={2023},
url={https://openreview.net/forum?id=sseGcw79Zh}
}

@inproceedings{Kubus2007calib,
author = {Kubus, Daniel and Kroger, Torsten and Wahl, Friedrich},
year = {2007},
month = {12},
pages = {1402 - 1408},
title = {On-line rigid object recognition and pose estimation based on inertial parameters},
journal = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2007.4399184}
}

@inproceedings{jang2021bc,
title={{BC}-Z: Zero-Shot Task Generalization with Robotic Imitation Learning},
author={Eric Jang and Alex Irpan and Mohi Khansari and Daniel Kappler and Frederik Ebert and Corey Lynch and Sergey Levine and Chelsea Finn},
booktitle={5th Annual Conference on Robot Learning},
year={2021},
url={https://openreview.net/forum?id=8kbp23tSGYv}}

@inproceedings{gao2014jhu,
  title={JHU-ISI gesture and skill assessment working set (JIGSAWS): A surgical activity dataset for human motion modeling},
  author={Gao, Yixin and Vedula, S Swaroop and Reiley, Carol E and Ahmidi, Narges and Varadarajan, Balakrishnan and Lin, Henry C and Hager, Gregory D},
  booktitle={MICCAI Workshop: M2CAI},
  volume={3},
  number={2014},
  pages={3},
  year={2014},
  month=9
}

@article{sener2022assembly101,
    title = {Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities},
    author = {F. Sener and D. Chatterjee and D. Shelepov and K. He and D. Singhania and R. Wang and A. Yao},
    journal = {CVPR 2022},
}

@inproceedings{inceoglu2021fino,
  title={Fino-net: A deep multimodal sensor fusion framework for manipulation failure detection},
  author={Inceoglu, Arda and Aksoy, Eren Erdal and Ak, Abdullah Cihan and Sariel, Sanem},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={6841--6847},
  year={2021},
  organization={IEEE}
}

@article{BT,
title = {A survey of Behavior Trees in robotics and AI},
journal = {Robotics and Autonomous Systems},
volume = {154},
pages = {104096},
year = {2022},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2022.104096},
url = {https://www.sciencedirect.com/science/article/pii/S0921889022000513},
author = {Matteo Iovino et al.},
keywords = {Behavior Trees, Robotics, Artificial Intelligence, Learning Behavior Trees},
abstract = {Behavior Trees (BTs) were invented as a tool to enable modular AI in computer games, but have received an increasing amount of attention in the robotics community in the last decade. With rising demands on agent AI complexity, game programmers found that the Finite State Machines (FSM) that they used scaled poorly and were difficult to extend, adapt and reuse. In BTs, the state transition logic is not dispersed across the individual states, but organized in a hierarchical tree structure, with the states as leaves. This has a significant effect on modularity, which in turn simplifies both synthesis and analysis by humans and algorithms alike. These advantages are needed not only in game AI design, but also in robotics, as is evident from the research being done. In this paper we present a comprehensive survey of the topic of BTs in Artificial Intelligence and Robotic applications. The existing literature is described and categorized based on methods, application areas and contributions, and the paper is concluded with a list of open research challenges.}
}