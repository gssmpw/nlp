\section{Related Works}
\subsection{Vision Transformers}
Transformer, which arises from natural language processing tasks, has sparked great interest in the field of computer vision \cite{yan2023learning}. ViT \cite{dosovitskiy2020image} pioneeringly introduces a pure transformer architecture on image classification by splitting images into sequences of patches, and attains excellent performance compared to some advanced convolutional neural networks. Later, DeiT \cite{touvron2021training} is proposed, where transformer can be efficiently trained on mid-size image datasets. Inspired by these, some efforts attempt to apply ViTs to other vision tasks, such as object detection \cite{carion2020end,DBLP:conf/nips/FangLWFQWNL21} and point cloud understanding \cite{guo2021pct,zhao2021point}. For instance, DETR \cite{carion2020end} regards object detection as a set prediction task, in which transformers are used for capturing the relationship between objects. After that, YOLOS \cite{DBLP:conf/nips/FangLWFQWNL21} uses an attention-only architecture and sequence-to-sequence strategy for object detection. Additionally, PCT \cite{guo2021pct} is proposed as the first transformer-based backbone for point cloud understanding. PCT takes each point as a token and performs vector attention between points in a local neighbor set, and attains promising performance on point cloud classification and point cloud part segmentation tasks. Generally, the impressive accomplishments of ViTs heavily depend on substantial computation and storage overhead, hindering their applications on devices with limited memory and computation resources. In this paper, we are concerned with the problem of producing quantized ViTs with low-bit weights and activations in a post-training manner. 

\subsection{Model Quantization}
One promising solution to compress complex models and accelerate inference is model quantization. Model quantization decreases the bit-width of weights and activations to alleviate the memory footprint and the computational overhead. Early works \cite{lin2015neural,nagel2022overcoming} commonly adopt the paradigm of quantization-aware training (QAT) to employ retraining on the entire training data for higher accuracy after quantization. Despite their effectiveness, these methods inevitably suffer from privacy problems and massive time overhead, particularly for large-scale ViTs. 

In contrast to QAT, post-training quantization (PTQ), which crafts quantized models without expensive retraining, has attracted increasing attention in model compression. Adaround \cite{DBLP:conf/icml/NagelABLB20} suggests that the naive round-to-nearest is not optimal for quantization, and designs an adaptive rounding strategy to lower the quantization error. After that, BRECQ \cite{li2020brecq} uses basic building blocks in neural networks to perform reconstruction, obtaining impressive image classification accuracy with 4-bit ResNet.
Despite their achievements on CNNs, they show limited performances on ViTs. Thus, many works \cite{DBLP:conf/ijcai/LinZSLZ22, li2023repq} attempt to apply PTQ to ViT models. Due to the uneven activation distributions of post-LayerNorm, RepQ-ViT \cite{li2023repq} uses channel-wise quantization to alleviate the severe inter-channel variations, and then uses reparameterization skills to convert scale factors into layer-wise formats. However, these methods typically rely on log2-based quantizers to deal with the post-Softmax activations, which focus on zero-around values, containing massive redundancy. Besides, these quantizers entail specialized operations to achieve efficiency \cite{lee2017lognet,DBLP:conf/ijcai/LinZSLZ22}.