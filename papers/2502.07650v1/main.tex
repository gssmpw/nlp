\documentclass[accepted]{uai2025}
\usepackage{natbib} %
\bibliographystyle{plainnat}
\renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %

\usepackage{hyperref}

\usepackage{algorithmic}
\usepackage{algorithm}

\usepackage[american]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\newcommand{\qtheta}{q(\boldx; \boldtheta)}
\input{notation_song}
\input{notations_leyang}

\title{Guiding Time-Varying Generative Models with Natural Gradients \\ on Exponential Family Manifold}
\author[1]{\href{mailto:song.liu@bristol.ac.uk}{Song Liu}}
\author[2]{\href{mailto:leyang.wang.24@ucl.ac.uk}{Leyang Wang}}
\author[1]{\href{mailto:yakun.wang@bristol.ac.uk}{Yakun Wang}}
\affil[1]{%
    School of Mathematics\\
    University of Bristol\\
    United Kingdom
}
\affil[2]{%
    Department of Computer Science\\
    University College London\\
    United Kingdom
}


\begin{document}
\maketitle

\begin{abstract}
Optimising probabilistic models is a well-studied field in statistics. However, its connection with the training of generative models 
remains largely under-explored. In this paper, we show that the evolution of time-varying generative models can be projected onto an exponential family manifold, naturally creating a link between the parameters of a generative model and those of a probabilistic model. We then train the generative model by moving its projection on the manifold according to the natural gradient descent scheme. 
This approach also allows us to approximate the natural gradient of the KL divergence  efficiently without relying on MCMC for intractable models.  Furthermore, we propose particle versions of the algorithm, which feature closed-form update rules for any parametric model within the exponential family. Through toy and real-world experiments, we validate the effectiveness of the proposed algorithms.

\end{abstract}

\section{Introduction}


Modern generative models \citep{goodfellow2014generative,ho2020denoising,song2021scorebased} have become indispensable tools in modern machine learning, achieving remarkable success in applications \citep{rombach2022high,gu2022vector,li2019neural,tan2024naturalspeech}.
These generative models are neural networks transforming a latent variable to a higher dimensional sample. They overcome \emph{classic restrictions imposed on probability density models}, such as positivity, normalization, and encoding of conditional independence via factorization; thus, they can be designed freely to capture complex, intricate patterns from the high-dimensional data. 
Albeit these models produce highly realistic outputs, they can be hard to train. The model training requires massive data and maintains a delicate balance between ``generators'' and ``discriminators'' \citep{goodfellow2014generative,arjovsky2017wasserstein,wang2022diffusion} or building effective  ``bridges'' between the reference and target dataset \citep{song2021scorebased,lipman2023flow,liu2023flow,bortoli2021diffusion}, which are undesirable when samples are limited, or such bridges are difficult to design. 




Parametric probabilistic models, particularly those in the 
exponential family \citep{casella2024statistical, wainwright2008graphical}, 
play a central role in modern day's statistical inference, and have
well-established theoretical framework and training algorithms. 
These models, characterised by their sufficient statistics and natural parameters, define a \emph{probability distributions manifold}, 
the geometric structure of which inspired efficient optimisation methods such as natural gradient descent (NGD) \cite{amari1998natural}, ensuring stable and efficient parameter estimation \citep{amari2000methods}. However, while exponential family models are versatile \emph{in theory}, their use may be limited in practice: the hand-crafted sufficient statistic may fail to capture complex relationships in data; more flexible sufficient statistics (such as neural nets) result in intractable likelihoods. Thus, parameter estimation that requires a likelihood, such as NGD, cannot be easily applied to fit the model. 

We aim to unlock the power of modern generative models through the principled training of probabilistic models.

Our work is inspired by two distinct research directions developed in recent years: time-varying generative models \citep{ho2020denoising,song2021scorebased,liu2023flow,lipman2023flow} and Time Score Matching (TSM) \citep{choi2022density}.
Time-varying generative models generate samples progressively, evolving them over time until they match the target distribution. Meanwhile, TSM learns the instantaneous change of a time-varying distribution from data. Recent work demonstrates that TSM can ``project'' temporal variations of a dataset onto the parameter space of exponential family distributions \citep{williams2024high}.


The core idea of this paper is to evolve a generative model such that its \emph{projected trajectory} on an exponential family manifold aligns with the trajectory induced by NGD. 
This way, we get the expressiveness of a modern generative model, and the training efficiency of NGD. The exponential family model acts as a guiding framework for the generative model throughout the training process.
An illustration of this idea is in \cref{fig.notions}. 
We align the projected changes of the generative model with the NGD update on a parametric manifold (shrinking the length of the red dotted line in \cref{fig.notions}). 

Although our technique applies to all time-varying generative models, we focus on drift-based generative models, where samples are iteratively perturbed by vector-valued functions. We develop two NGD-guided drift-based generative approaches: kernel NGD and neural tangent kernel NGD, both of which admit closed-form sample updates.



\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{Images/projection_matching.png}
    \caption{Technical notions illustrated. Matching the projected generative model change $\bolddelta(\boldw)$ to the NGD meaning reducing the length of the red dotted line. Symbols are defined in Section \ref{sec.background} and \ref{sec.prop}. }
    \label{fig.notions}
\end{figure}

\section{Backgrounds}
\label{sec.background}
In recent years, it has been recognized that there are many similarities between sampling and optimization \citep{wibisono2018sampling,arbel2019maximum,sinho2024logconcave,cheng2018underdamped,he2024training}. Encouraged by these results, we ask: 
Given a time-varying generative model \( g_t \), where \( t \) is time, can we move its output distribution \( q_{g_t} \) toward an optimal distribution \( q^* \) using NGD?  In literature, generative models are often referred to as implicit models and generative distribution \( q_{g_t} \) normally don't have a parametric density, thus optimization designed for parametric densities cannot be directly applied. 

Before stating our solutions, we first introduce a few ideas that are essential for developing the proposed algorithm. 

\textbf{Notations:} 
Vectors (\(\boldx\)) and matrices (\(\boldX\)) are denoted in bold. Random variables ($X$) are non-bold, capital letters. 
Euclidean norms are denoted as \(\|\cdot\|\), and for elements in a Hilbert space, the Hilbert norm is written as \(\|\cdot\|_{\mathcal{H}}\). 
Expectations and covariances under \(q\) are denoted as \(\mathbb{E}_{q}[\cdot]\) and \(\mathrm{Cov}_{q}[\cdot]\), respectively. 
$\nabla f(\boldx) = [\partial_1 f(\boldx), \partial_2 f(\boldx), \cdots]^\top$. 
$\nabla f(\boldx_0)$ means the gradient of $f$ evaluated at $\boldx_0$. Suppose $\boldf: \mathbbR^m \to \mathbbR^n, $
$\nabla \boldf(\boldx)$ denotes the Jacobian of $\boldf$ and is a matrix of size $n \times m$. 

\subsection{Time-varying Exponential Family}
\begin{definition}(See, e.g., Section 3.4, \citet{casella2024statistical})
\label{def.exp.fam}
A distribution is a member of the \textbf{exponential family} with sufficient statistic \(T\) if and only if its density function \(q(\boldx; \boldtheta)\) can be expressed as:
\begin{align*}
    q(\boldx; \boldtheta) := \exp\big(\langle \boldtheta, T(\boldx) \rangle - A(\boldtheta)\big),
\end{align*}
where \(\boldtheta \in \mathbbR^d\) is the natural parameter, and \(A(\boldtheta)\) is the log-normalisation function, defined as:
\begin{align*}
    A(\boldtheta) := \log \int \exp\big(\langle \boldtheta, T(\boldx) \rangle\big) \, d\boldx.
\end{align*}
\end{definition}
This family includes many known distributions, such as Gaussian, Gamma and Exponential distributions. We can also choose $T$ to be infinite-dimensional to enable more flexible modelling \citep{sriperumbudur2017density,arbel2018kernel}. 

\begin{definition}
The parametric manifold of exponential family distributions with a sufficient statistic $T$ is:
\begin{align*}
    \mathcal{M}(T) := \left\{\boldtheta \in \mathbb{R}^d \;\middle|\; \int q(\boldx; \boldtheta) \, d\boldx = 1 \right\},
\end{align*}
where $q(\boldx; \boldtheta)$ depends on $T$ as in Definition \ref{def.exp.fam}. 
\end{definition}

Although $\mathcal{M}(T)$ is a parametric manifold, with a slight abuse of the notation, we denote $q_\boldtheta \in \mathcal{M}(T)$ to indicate that $q_\boldtheta$ is a member of exponential family. 

\textbf{Time-varying exponential family distributions} refers to exponential family distributions whose natural parameter is a continuous function of time, i.e., \(q_{\boldtheta_t} = q(\boldx; \boldtheta(t))\). 
Moreover, 
the time derivative of \(\log q_t\) is:
\begin{align*}
    \partial_t \log q_{\boldtheta_t} = \langle \partial_t \boldtheta(t), T(\boldx) \rangle - \partial_t A(\boldtheta(t)).
\end{align*}

Equivalently, this can be expressed as the inner product between the rate of change of the natural parameter and the centered sufficient statistic (Proposition 3.1 in \citet{williams2024high}):
\begin{align}
\label{eq:partial_t.logqt}
    \partial_t \log q_{\boldtheta_t} = \langle \partial_t \boldtheta(t), T(\boldx) - \mathbb{E}_{q_{\boldtheta_t}}[T(\boldx)] \rangle.
\end{align}
By definition, the time-varying process $\boldtheta(t)$ is a curve on $\mathcal{M}(T)$ and $\partial_t \boldtheta(t)$ is the tangent vector of such curve. 

\subsection{Natural Gradient Descent}
Natural gradient descent is an effective optimization technique for parametric probabilistic models  \citep{amari1998natural,amari2000methods} and has been widely applied in various fields \citep{bernacchia2018exact,chenteng}.
\begin{definition}
    The natural gradient of a loss function $\mathcal{L}(\boldtheta)$ is the gradient of the loss function scaled by the inverse Fisher information matrix $\mathcal{F}$. 
    \begin{align*}
        \nabla^N \mathcal{L}(\boldtheta) := \mathcal{F}^{-1} \nabla \mathcal{L}(\boldtheta).
    \end{align*}
    Suppose $q_\boldtheta \in \mathcal{M}(T)$, the Fisher information matrix is 
    \[
    \mathcal{F} = \nabla^2_\boldtheta \log q(\boldx; \boldtheta) = \mathrm{Cov}_{q_{\boldtheta}}[T(\boldx)],
    \]
    describing the curvature of $\mathcal{M}(T)$ neighbouring $\boldtheta$.
\end{definition}
Although methods described in this paper are generic, we focus on KL divergence in this paper.

\begin{example}
    Suppose $\mathcal{L}$ is the KL divergence from $p$ to $q$.  
    \begin{align*}
        \mathcal{L} = \mathrm{KL}[p, q] = \mathbb{E}_{p}[\log p(\boldx) - \log q(\boldx)]. 
    \end{align*}
    Suppose $q$ contains parameters $\boldtheta$ and $q_\boldtheta \in \mathcal{M}(T)$, 
    then the natural gradient of $\mathcal{L}$ is given by
    \begin{align}
        \label{eq.ngd.kl}
        \nabla^N {\mathrm{KL}} &:= - \mathcal{F}^{-1} \mathbb{E}_{p}[\nabla_{\boldtheta} \log q(\boldx; \boldtheta)] \notag \\
        &= - \mathrm{Cov}_{q_{\boldtheta}}[T(\boldx)]^{-1} \left\{\mathbb{E}_{p}[T(\boldx)] - \mathbb{E}_{q_{\boldtheta}}[T(\boldx)]\right\}. 
    \end{align}
\end{example}
Except for certain specific choices of \( T \), \( \nabla^N {\mathrm{KL}} \) does not admit a closed-form solution, as neither \( \mathbb{E}_{q_{\boldtheta}}[T(\boldx)] \) nor \( \mathrm{Cov}_{q_{\boldtheta}}[T(\boldx)] \) can be expressed in closed form for a general $T$. If we could sample from \( q_{\boldtheta} \), these expectations and covariances could be approximated using Monte Carlo methods. However, generating samples from a complex distribution \( q_{\boldtheta} \) itself remains a challenging problem.  

\subsection{Time-varying Generative Model}
In recent years, there has been a growing trend of designing generative models as functions of time, in contrast to the classic generative models where the sample generating mechnism is independent of time. 
For example, the diffusion generative model \citep{song2021scorebased} can be interpreted as the solution to a Stochastic Differential Equation at time \( t \),  
with initial samples drawn from a reference distribution.  
Similarly, the rectified flow generative model \citep{liu2023flow} can be viewed as the solution to an Ordinary Differential Equation at time \( t \).
\begin{definition}
    A \textbf{generative model} is a data-generating mechanism defined as \(X = \boldg(Z; \boldw)\), where \( Z \) is a random variable sampled from a latent distribution \( p_Z \). \(\boldw \in \mathcal{W}\) are parameters of the generative model $\boldg$. 
    A \textbf{time-varying generative model} is a generative model with time dependency, defined as \( X_t = \boldg(Z, t; \boldw) \).
\end{definition}


\subsection{Time Score Matching}
In applications like time-series analysis, characterising the change of the data generative distribution is an essential task, and recently, a method measures the change of distribution over time was proposed \citep{choi2022density}. 

\begin{definition}
\textbf{Time score} is the time derivative of the log density of a time-varying distribution. Given a time-varying distribution $q_t$, its time score is $s_t := \partial_t (\log q_t)$. 
\end{definition}


Given a parametric time score model \({v}(\boldx;t)\) and a time-varying sample $X_t \sim q_t$, the time score can be learned by 
the \textbf{Time Score Matching (TSM)} which minimizes the objective:
$
    \int 
    \mathbb{E}\Bigl[
        \lambda(t)\,\bigl\|
        s_t(X_t)
        -
        {v} \bigl(X_t;t\bigr)
        \bigr\|^2
    \Bigr]\,
    \mathrm{d}t,
$
where \(\lambda(t)\) is a weighting function. 


A recent work shows, 
for the special case where \({q_t}(\mathbf{x}) \in \mathcal{M}(T)\), the time differential natural parameter \(\partial_t\boldsymbol{\theta}\) can directly learned by TSM \citep{williams2024high} and applied it to learning time-varying graphical models. 




\section{Proposed Algorithm: Evolution Projection}
\label{sec.prop}
\subsection{Problem Formulation}
Suppose we only have access to a target distribution \( p \) through samples \( Y \sim p \) and a latent variable $Z$. Our goal is to find a time-varying generative model \( g_t \) that progressively approximates the target distribution as \( t \to \infty \). Informally speaking, we seek a model such that \( Y \overset{d}{\approx}  g_\infty(Z) \).

Note that we do not want to train a generative model using GANs or diffusion models, as these models require a large number of samples and are hard to train. Instead, given a loss function \(\mathcal{L}(p, q_{\boldtheta_t})\) that measures the difference between \(p\) and a time-varying probabilistic model \(q_{\boldtheta_t}\), we aim to guide the training of the generative model by minimizing the loss \(\mathcal{L}\) of the probabilistic model \(q_{\boldtheta_t}\) over time.

\textbf{The key idea} of this paper is to align the evolution of the generative model with \(\nabla^N \mathcal{L}(\boldtheta)\) on the manifold \(\mathcal{M}(T)\). 
Consequently, minimizing the loss for \( q_\boldtheta \) simultaneously drives the generative model toward matching \( p \).


To achieve this, we first need to establish a direct correspondence between the instantaneous change in the generative model and the parametric update on \(\mathcal{M}(T)\).





\subsection{Projecting the Change}
\label{sec.proj.change}

Denote the sample of a time-varying generative model $g_t$ as $X_t \sim q_{g_t}$. We can measure the instantaneous change of the generative model via the its time score $s_t := \partial_t (\log q_{g_t})$. 

At a fixed time $t_0$, we can ``project'' the time score $s_t$ 
onto the manifold $\mathcal{M}(T)$
by minimizing the squared difference between $s_t$ and the time score of an exponential family distribution, $\partial_t (\log q_{\boldtheta_t}), q_{\boldtheta_t} \in \mathcal{M}(T)$, 
\begin{align}
    \label{eq.tsm}
    &\int \lambda_{t_0}(t) \mathbbE \left[\left(s_t(X_t) - \partial_t (\log q_{\boldtheta_t})(X_t) \right)^2\right] \mathrm{d}t, \notag \\
    = &\int \lambda_{t_0}(t) \mathbbE \left[\left(s_t(X_t) - \langle \partial_t \boldtheta(t), T(X_t) - \mathbbE [T(X_t')] \rangle\right)^2\right] \mathrm{d}t, 
\end{align}
where $\lambda_{t_0} = \exp(-(t-t_0)^2/\sigma^2)$ and $X_t'$ is an independent copy of $X_t$. $\sigma$ is a hyperparameter fixed in advance. The second line is due to \cref{eq:partial_t.logqt}. The integration is over the entire real line. 

Introducing a linear-in-time model $\boldtheta(t; \bolddelta) = t \bolddelta$, the above objective becomes 
\begin{align}
    \label{eq.locallinear.obj}
    J(\bolddelta) := \int \lambda_{t_0}(t) \mathbbE \left[\left(s_t(X_t) - \langle \bolddelta, T(X_t) - \mathbbE [T(X_t')] \rangle\right)^2\right] \mathrm{d}t, 
\end{align}
which now depends on the parameter $\bolddelta$. 
One can view \eqref{eq.locallinear.obj} as a local regression at the fixed time point $t_0$:  $\lambda$ is a time smoothing kernel, \cref{eq.locallinear.obj} finds the best score model that approximates the time-varying function $s_t(\boldx)$ at $t_0$. 

The minimizer to the above objective is a vector in $\mathbbR^d$ that best describes the instantaneous change in the generative model. Moreover, we can show that 
\begin{theorem}
\label{thm:delta3.1}
    Let $\bolddelta_{t_0} := \argmin J(\bolddelta)$, then 
    \begin{align*}
        &\bolddelta_{t_0} =\nonumber \\&  - \left(\int \lambda_{t_0}(t) \mathrm{Cov}[T(X_t)]
        \mathrm{d}t\right)^{-1} \int \partial_t \lambda_{t_0}(t) \mathbbE \left[ T(X_t) \right] \mathrm{d} t. 
    \end{align*} 
\end{theorem} 
The proof can be found in \cref{app:3.1}. 
Since $X_t = g(Z, t; \boldw)$, we can express $\bolddelta_{t_0}$ as a function of $\boldw$ using the reparameterization trick \citep{Kingma2014}. 
\begin{align}
\label{eq.deltasol}
    \bolddelta_{t_0}(\boldw) &=  - C^{-1} \int \partial_t \lambda_{t_0}(t) \mathbbE \left[ T(g(Z, t; \boldw)) \right]\mathrm{d}t, \notag \\
    C &= \int \lambda_{t_0}(t) \mathrm{Cov}[T(g(Z, t; \boldw))]
    \mathrm{d}t.  
\end{align}
This expression allows us to approximate $\mathbbE[\cdot]$ and $\mathrm{Cov}[\cdot]$ using samples of $Z$. 
We illustrate 
the projection process in Figure \ref{fig.notions} where the downward dotted arrow represents the projection by TSM. 


\paragraph{Remarks:}
Different time scores can be mapped to the same projection $\bolddelta_{t_0}$, especially when the sufficient statistic $T$ is restrictive.
However, if $T$ is chosen such that the exponential family is expressive enough, we expect that such information collapse could be avoided. The rigorous proof of this claim is left as a future work. 

The hyperparameter $\sigma$ in \cref{eq.tsm} will introduce additional biases to the estimation, similar to how a non-zero kernel bandwidth in local regression introduces biases to the estimate. In experiments, we observe that reasonable $\sigma$ choices (e.g., $0.1$) work well. Moreover, in \cref{sec.ngd.drift}, we show how to obtain an unbiased estimator for a special type of time-varying generative models.  




\subsection{Matching to NGD}
From \cref{eq.deltasol}, 
we can see the projection $\bolddelta_{t_0}(\boldw)$ creates a connection between the parameter evolution of a generative model and those of a probabilistic model. 
The key idea of this paper is 
to align the evolution of the generative model with that of the probabilistic model. 
In particular, we 
match $\bolddelta_{t_0}(\boldw)$ to the NGD update using the following objective 
\begin{align}
    \label{eq.align}
    \boldw_{t_0} = \argmin_{\boldw \in \mathcal{W}} \| \nabla^N \mathcal{L}(\boldtheta_{t_0}) - \bolddelta_{t_0}(\boldw) \|^2. 
\end{align}
In words, we find the generative model parameter $\boldw$ that results in the projected update $\bolddelta_{t_0}(\boldw)$ closest to the natural gradient update of the loss function. 

In the previous section, we have seen that 
$\bolddelta_{t_0}(\boldw)$ can be approximated using samples $Z$.
Assume that at the starting point, the generator $g(Z, 0; \boldw_{0})$ produces an output distribution $q_{g_0} \in \mathcal{M}(T)$, and the trajectory $q_{g_t}$ is precisely traced by the projected updates, we can expect that the samples generated from $g(Z, t; \boldw_{t})$ will be close to the samples from $q_{\boldtheta_t}$. Thus, we approximate $\nabla^N \mathcal{L}(\boldtheta)$ using samples from $g(Z, t; \boldw_{t})$.


After solving \eqref{eq.align}, instead of taking a natural gradient step, 
we directly sample from $g(Z, t_0 + \epsilon; \boldw_{t_0})$ using a small $\epsilon > 0$. 
Since we have already aligned the projected change of the generative model with the natural gradient step, we can expect that the samples generated from $g(Z, t_0 + \epsilon; \boldw_{t_0})$ will be close to the samples from $q(\boldx; \boldtheta_{t_0} + \epsilon \mathcal{F}_{t_0}^{-1}\nabla \mathcal{L}(\boldtheta_{t_0}))$ by actually taking a natural gradient step with step size $\epsilon$. 
We summarize the entire algorithm in \cref{alg:simple} and name it implicit NGD (iNGD). 

In \cref{fig:illu}, we show an example of the iNGD and compare it with the actual NGD on a Gaussian manifold $T(\boldx) = [\boldx, \boldx\boldx^\top ], \boldx \in \mathbbR^2$. In this example, the generative model is an MLP with one hidden layer consisting of 67 neurons. We can see that the samples generated from iNGD accurately retrace the steps of the classic NGD, ultimately producing a set of samples (red dots) that resemble the target distribution (black dotted line). 

\begin{figure}
    \centering
    \includegraphics[width=.23\textwidth]{Images/iNGD.png}
    \includegraphics[width=.23\textwidth]{Images/NGD.png}
    \caption{The evolution of parametric distributions under iNGD and NGD on the manifold of Gaussian distributions, i.e., $\mathcal{M}([\boldx, \boldx^\top \boldx])$, starting from $\mathcal{N}(\boldzero,\boldI)$. Each ellipse represents the 95\% confidence interval of $q_{\boldtheta_t}$. As $t$ increases, the ellipse turns from blue to red.   Black dotted line marks the confidence interval the target distribution $p$. For iNGD, the ellipses are approximated by fitting a Gaussian model to the generated samples.
    }
    \label{fig:illu}
\end{figure}


\section{Special Case: Drift Model}
\label{sec.ngd.drift}

One popular class of generative models is the \emph{drift-based generative model}. This model iteratively perturbs samples using a vector-valued function until convergence. 


\begin{definition}
    \label{ex.drift.model}
    At a fixed time $t_0$, 
    suppose we have samples $X_{t_0}$, 
    a drift generative model generates samples at a new time point $t$ via the following scheme: 
    \[
        \boldg(X_{t_0}, t; \boldw) = X_{t_0} + (t-t_0) \boldh(X_{t_0}; \boldw). 
    \] 
\end{definition}
This model could be seen as a local linear model and the amount of update $\boldh(X_{t_0}; \boldw)$ depends linearly on $t - t_0$. 
The input of the model is a sample at the time point $t_0$. 
To generate samples, we need to draw a batch of samples from an initial distribution $q_{g_0}$ and then successively apply the drift generative model for a sequence of $t$ until convergence.   
An Euler solver of flow-based generative model is an example of a drift generative model, in which case, $t-t_0$ is the step size of the Euler solver.  


We can prove that when using a model introduced in \cref{ex.drift.model}, \cref{eq.deltasol} has a limiting solution as $\sigma \to 0$, which eliminates the bias caused by the time-smoothing kernel $\lambda$.
\begin{theorem}
\label{thm.cf}
    The projection of the time score $s_t$ of a drift generative model has a closed form expression at the limit of $\sigma \to 0$, i.e., 
    \begin{align*}
    \lim_{\sigma \to 0} \bolddelta_{t_0}(\boldw) &= \mathrm{Cov}[T(X_{t_0})]^{-1} \mathbbE \left[ \nabla T(X_{t_0})  \boldh(X_{t_0}; \boldw) \right]\\
    &= \mathcal{F}_{t_0}^{-1} \mathbbE \left[ \nabla T(X_{t_0})  \boldh(X_{t_0} ; \boldw)\right]. 
\end{align*}
\end{theorem}
The proof can be found in \cref{app:kerneldelta}. Using this limiting solution, the objective of \cref{eq.align} can be rewritten as:  
\begin{align}
\label{eq.simple.obj}
    \| \nabla \mathcal{L}(\boldtheta_{t_0}) - \mathbbE \left[ \nabla^\top T(X_{t_0})  \boldh(X_{t_0}; \boldw)\right] \|_{\mathcal{F}_{t_0}^{-1}}^2. 
\end{align}
Note that the gradient $\nabla \mathcal{L}(\boldtheta_{t_0})$ is an Euclidean gradient.
In our experiments, this objective function is more stable and computationally efficient than \eqref{eq.align}, since it does not require back-propagating through a matrix inversion. 

\subsection{Kernel NGD}
Now let us consider an example of the drift model where the drift function $\boldh$ is defined as the gradient of an Reproducing Kernel Hilbert Space (RKHS) function.

\begin{example}[RKHS Drift Model]
    A kernel drift function is defined as
    $
    \boldh(\boldx; w) := \langle w, \nabla_\boldx k(\cdot, \boldx) \rangle_\mathcal{H}, w \in \mathcal{H}, 
    $
    where $\mathcal{H}$ is an RKHS with a kernel function $k$. 
\end{example}

\begin{algorithm}[t]
    \caption{Generative Model Training Guided by Natural Gradient Descent}
    \label{alg:simple}
    \begin{algorithmic}[1]
    \REQUIRE Target samples $Y \sim p$, latent samples $Z\sim p_Z$, step size $\epsilon$, number of iterations $n$ 
    
    \STATE $t[0] \gets 0$, initialize $\boldw$. 
    \FOR{\( i \gets 1 \) to \( n \)}
        \STATE Sample $X_{t[i]} \gets g(Z, t[i]; \boldw)$
        \STATE Approximate $\nabla^N \mathcal{L}(\boldtheta_{t[i]})$ with $X_{t[i]}$ and $Y$ using Monte Carlo.  
        \STATE Approximate $\bolddelta(\boldw)$ with $X_{t[i]}$ using Monte Carlo. 
        \STATE $\boldw \gets \argmin_{\boldw} \| \nabla^N \mathcal{L}(\boldtheta_{t[i]}) - \bolddelta(\boldw) \|^2$
        \STATE $t[i+1] \gets t[i] + \epsilon$
    \ENDFOR
    
    \STATE \textbf{Return:} Samples from $g(Z, t[n]; \boldw)$
\end{algorithmic}
\end{algorithm}

To align this generative model with the NGD, 
we introduce a regularized version of \cref{eq.simple.obj} 
\begin{align}
    \label{eq.simple.obj2}
    \|\nabla \mathcal{L}(\boldtheta_{t_0}) - \mathbb{E}[\nabla T(X_{t_0}) \boldh_w(X_{t_0})]\|_{\mathcal{F}_{t_0}^{-1}}^2 + \lambda \|w\|^2_\mathcal{H}.
\end{align}

\begin{theorem}
\label{them.kernelNGD}
 The optimal drift function that minimizes \cref{eq.simple.obj2} can be found as
\begin{align}
\label{eq.kernel}
&\boldh_{w^*}(\boldx) =  \mathbbE \left[\nabla T(X_{t_0})  \nabla  k(X_{t_0},  \boldx)\right]^\top \boldGamma^{-1} \nabla \mathcal{L}(\boldtheta_{t_0}) \\
&\boldGamma =  \lambda \mathcal{F}_{t_0} +
\mathbbE\left[\nabla T(X_{t_0}) \nabla \nabla k(X_{t_0},  X_{t_0}')  \nabla^\top T(X_{t_0}')\right] \notag 
\end{align}
where $X_{t_0}'$ is an independent copy of $X_{t_0}$ and $\nabla \nabla k(\boldx, \boldy)$ is the short hand for $\nabla_\boldx \nabla_\boldy k(\boldx, \boldy)$.     
\end{theorem}
The proof can be found in \cref{sec.proof.kernelNGD}. 
This result enables the \emph{direct calculation} of particle updates without fitting a generative model first.  
This inspires us to build a \emph{particle evolution strategy} guided by NGD: First, we sample $X_0$ from an initial distribution $q_0$, then we iteratively update each sample $X_{t_0}$ using the formula given by \cref{them.kernelNGD} until they converge. This algorithm is summarized in \cref{alg:parNGD} and we name it Kernel implicit NGD (KiNG).


\begin{algorithm}[t]
    \caption{RKHS/Neural Tangent Kernel Implicit Natural Gradient Descent}
    \label{alg:parNGD}
    \begin{algorithmic}[1]
    \REQUIRE Target samples $Y \sim p$, initial particles $X_0 \sim q_0$, step size $\epsilon$, number of iterations $n$ 
    
    \STATE $t[0] \gets 0$
    \FOR{\( i \gets 1 \) to \( n \)}
        \STATE Approximate $\boldh$ with $Y$ and $X_{t[i]}$ using \eqref{eq.kernel} or \eqref{eq.ntk.update}.
        \STATE $t[i+1] = t[i] + \epsilon$
        \STATE $X_{t[i+1]} = X_{t[i]} + \epsilon \boldh(X_{t[i]})$
    \ENDFOR
    
    \STATE \textbf{Return:} $X_{t[n]} \sim q_{t_{[n]}}$.
\end{algorithmic}
\end{algorithm}

In \cref{fig:kngd}, we plot the trajectories of KiNG with different one-dimensional initial and target distributions. We choose $\mathcal{M}(T)$ to be a Gaussian manifold, i.e., $T(x) = [x, x^2]^\top$. 
Note that in the right plot, the particles do not converge to the bi-modal target distribution, since the movements of our  particles are ``guided'' by the Gaussian manifold, and the best approximation is a Gaussian with a larger standard deviation. This example shows that the particles are indeed guided by the Gaussian manifold throughout the generative process. This phenomenon could be beneficial, if the target is to find the best approximation within a given family or we already have an informative probabilistic model (see \cref{sec.gm}). 

This behaviour could be changed by replacing the Gaussian manifold with a more expressive manifold. In the left plot of \cref{fig:newmanifold}, we run similar experiments by letting $T$ be the Radial Basis Functions (RBFs) and we can see that indeed the particles bifurcate and converge to both modes. 




\subsection{Neural Tangent Kernel NGD}
KiNG can be generalized to other types of kernels. 


\begin{example}[Neural Tangent Drift Model]
    \label{ex.nt}
    Given a neural network $\boldphi: \mathbbR^{\mathrm{dim}(\boldx)} \to \mathbbR^{\mathrm{dim}(\boldx)}$, a neural tangent drift function is defined as
    \[
    \boldh(\boldx) := \nabla_\boldbeta \boldphi(\boldx; \boldbeta_0) \boldw,
    \]
    where $\boldbeta_0$ are initial weights, $\nabla_\boldbeta\boldphi(\boldx; \boldbeta_0)$ is neural tangent. 
\end{example}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\linewidth]{Images/1d2.png}
    \includegraphics[width=0.47\linewidth]{Images/1d.png}
    \caption{The evolution of particles $X_t$ under KiNG on the manifold of Gaussian distributions. Each red line is a trajectory of a particle in the space-time.  
    The initial distribution $q_0$ is plotted on the left as black dotted lines, while the target distribution $p$ is plotted on the right as blue dotted lines. }
    \label{fig:kngd}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.47\linewidth]{Images/1d3.png}
    \includegraphics[width=0.47\linewidth]{Images/1d5.png}    \caption{Particle trajectories when using more expressive manifold. $T(\boldx) := [k(x_1, b_1), k(x_1, b_2), \cdots]^\top $, where $k$ is an RBF and $b_i$ are kernel basis randomly chosen from samples of $X_{t_0}$. Left: KiNG, Right, ntKiNG. }
    \label{fig:newmanifold}
\end{figure}

Similar to \cref{eq.simple.obj2}, we can solve the following regularized objective to find the update of the particles
\begin{align}
    \label{eq.simple.obj3}
    \|\nabla \mathcal{L}(\boldtheta_{t_0}) - \mathbb{E}[\nabla T(X_{t_0}) \boldh_\boldw(X_{t_0})]\|_{\mathcal{F}_{t_0}^{-1}}^2 + \lambda \|\boldw\|^2.
\end{align}

The optimal drift that minimizes \cref{eq.simple.obj3} can be expressed using the \emph{neural tangent kernel} \citep{Jacot2018Neural}:
\begin{align}
    \label{eq.ntk.update}
    &\boldh_{\boldw^*}(\boldx) =   \mathbbE\left[\nabla T(X_{t_0})  \boldK_\mathrm{NTK}(X_{t_0}, \boldx) \right]^\top \boldGamma^{-1} \nabla \mathcal{L}(\boldtheta_{t_0})  \\
    &\boldGamma := \lambda \mathcal{F}_{t_0} +  \mathbbE\left[\nabla T(X_{t_0}) \boldK_\mathrm{NTK}(X_{t_0}, X_{t_0}') \nabla^\top T(X_{t_0}') \right], \notag 
\end{align}
where $\boldK_\mathrm{NTK}$ is the matrix-valued \emph{neural tangent kernel}, defined as $\boldK_\mathrm{NTK}(\boldx, \boldy) := \nabla_\boldbeta \boldphi(\boldx)
\nabla^\top_\boldbeta \boldphi(\boldy)$. 
This result can be proven using the same technique described in \cref{sec.proof.kernelNGD}.
In this paper, we use empirical and a finite-width NTK for simplicity, but NTKs that are infinitely wide can be efficiently computed using off-the-shelf package such as \verb|neural-tangents| \citep{neuraltangents2020} for a variety of neural network architectures. 

The right plot of \cref{fig:newmanifold} shows the particle trajectory of a neural tangent KiNG with $T$ as RBF basis. We name this variant of KiNG as ntKiNG. %

\paragraph{Remark: }
\cref{eq.ntk.update} requires computing a matrix-valued kernel, which may be computationally demanding if the dimensionality of $X_{t_0}$ is high. However, 
in experiments, we observe that the formulation works well with a diagonalized scalar kernel, i.e., 
\begin{align*}
 [\boldK(X_{t_0}, X_{t_0}')]_{l,m \in \left\{1 \dots \mathrm{dim}(\boldx)\right\}} := \begin{cases}
     k(X_{t_0}, X_{t_0}'), & l = m\\
     0, & l \neq m, 
 \end{cases}
\end{align*}
where $k$ is any scalar kernel function. 

\section{Experiments}
\label{sec.exp}
In this section, we further validate our method on toy data and real-world data respectively. The summarized details of experiment setups can be found in \cref{app:expset}. In all experiments, $\mathcal{L}(p, q_\boldtheta) = \mathrm{KL}[p, q_\boldtheta]$. 

\subsection{KiNGs vs. Reverse KL Wasserstein Gradient Flow and MMD Flow}
In this experiment, we compare KiNG, ntKiNG, with reverse KL Wasserstein Gradient Flow (WGF) \citep{gaodeep19,liu2024minimizing} and Maximum Mean Discrepancy flow (MMD flow) \citep{hagemann2024posterior} on small datasets with different dimensions. See \cref{sec.competitor.methods} for more explanations of these methods. 

Since they all minimize different divergences, we measure their performance using MMD \citep{gretton2012akernel} between a fresh batch of target samples and $X_t$.
\begin{figure}
    \centering
    \includegraphics[width=0.47\linewidth]{Images/mixture_5d.png}
    \includegraphics[width=0.47\linewidth]{Images/mixture_20d.png}
    \caption{$\mathrm{MMD}[Y, X_t]$ over iterations, the lower the better. Left: 5 dimensions, Right: 20 dimensions. The error bar indicates the standard error. }
    \label{fig:enter-label}
\end{figure}

Let $p = 0.5\mathcal{N}(-2, \boldI) + 0.5\mathcal{N}(2, \boldI)$.
We draw 100 samples from $p$ as $Y$, 100 samples from $\mathcal{N}(0, \boldI)$ as $X_0$, and run KiNG, WGF, MMD flow to evolve particles $X_t$. We plot $\mathrm{MMD}(Y, X_t)$ over iterations. 
For all methods, we set learning rate to be 1, which is the largest learning rates without causing numerical instability. It can be seen that when dimension is small ($5$), all methods work relatively well and ntKiNG and KiNG can reduce MMD faster, but when we increase the dimension to 20 the performance gap widens. However, ntKiNG and KiNG still have a commanding lead. 

\subsection{Graphical Model Recovery with Informative Sufficient Statistics}
\label{sec.gm}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{Images/gmntk.png}
    \caption{Sparsity pattern of $\boldTheta$ recovered by graphical lasso, using samples trained by ntKiNG with different sufficient statistics and number of iterations (red boxes indicate missing edges, the less boxes the better)}
    \label{fig:graphical_model}
\end{figure}
In this experiment, we showcase how much improvement we can get when using informative sufficient statistics. Exponential family are commonly used to encode graphical models. For example, a Gaussian graphical model is a Gaussian density $p = \mathcal{N}(\boldzero, \boldTheta^{-1})$, where the sparsity pattern of $\boldTheta$ encodes an undirected graph, describing the interactions of the random variables. 
One can imagine that if the generated samples approximate $p$ well, we should recover the correct graphical model from these samples.  
In this experiment, we let $p$ be a 30-dimensional Gaussian graphical model, and draw 200 samples $Y \sim p$, 200 samples $X_0 \sim \mathcal{N}(\boldzero, \boldI)$, and move the $X_0$ toward $p$ using ntKiNG algorithm. Finally we apply the graphical lasso \citep{friedman2007sparse} to estimate graphical models displayed in the left and middle plots of \cref{fig:graphical_model}. Here $T(\boldx) := [\text{RBF basis}]$. 

Since our methods use a probabilistic model to guide its training process, one may wonder if knowing the graphical model structure would improve the performance of the algorithm. To test this, we design a new sufficient statistic $T(\boldx) := [\text{RBF basis}, \forall {(i,j) \in \{(i,j) | \Theta_{i,j} \neq 0} \}, x_i x_j]$, i.e., we added pairwise potential functions that corresponds to pairwise factors in this graphical model. We ran the ntKiNG again with this new, better informed sufficient statistic for 30 runs. The graphical lasso estimate is shown in the right plot of  \cref{fig:graphical_model}. It can be seen that, when using the informed sufficient statistics, ntKiNG could recover the almost-correct graphical structure in only 30 iterations, while it takes the regular ntKiNG much longer. This suggests, our methods can indeed use a pre-existing probabilistic model to accelerate its generative model training process. 


\subsection{Covariate Shift by Dist. Matching}
In domain adaptation tasks, samples are drawn from the source distribution $p_{XY}$ and the target distribution $q_{XY}$ where $X, Y$ are covariates and label respectively. The problem is that a classifier trained on source distribution samples may not work on target distribution samples. 
Covariate shift \citep{sugiyama2008direct,quionero2009dataset} refers to a special case where $p_{X} \neq q_{X}$ but $p_{Y|X} = q_{Y|X}$. We adopt a ``marginal transport'' assumption \citep{courty2016optimal} that $X \sim q_X$ are generated as $X = \psi(X')$ 
where $X' \sim p_X$. It means, samples are generated from the source distribution and then ``transported'' to the target domain. For example, images in the source domain contains photos of  objects, while in the target domain, photos contain the same objects but are filtered to reflect certain styles. 

In the covariate shift setting, we observe joint samples from the source $p_{X,Y}$, but only have target domain covariates from $q_X$. The goal is to find $\psi^{-1}$. We find the reverse process by minimizing $\mathrm{KL}[p_X, q_t]$ using \cref{alg:parNGD}, where $X_0 \sim q_0$ are set to be the target domain covariates. 

We demonstrate the effectiveness of this algorithm in \cref{fig.transfer.illus}, where the transfer $\psi$ is a clockwise rotation on samples by 45 degrees. An inverse $\psi^{-1}$ is a counter-clockwise rotation and has been correctly identified by ntKiNG.  

We further test our algorithm on the Office+Caltech classification dataset \citep{gong2012geodesic} which is an object recognition dataset with photos collected from four different places: \verb|amazon|, \verb|dslr|, \verb|webcam|, \verb|caltech|. The task is to train a source classifier using one of the places, and test it on samples from another place. We test the performance of ntKiNG against two other particle-based transport methods WGF and MMD flow that also matches $q_{t}$ with $p_X$. 
The performance is measured by the percentage gains compared with directly applying the source classifier to the target samples. 
The results in \cref{tab:accuracy_comparison} show that our method achieves the most accuracy gains comparing to WGF and MMD flow. In 10 out of 12 domain adaptations settings, ntKiNG improves the testing accuracy. 

\begin{figure}[t]
    \centering
    \includegraphics[width=.45\textwidth]{Images/illus_transfer.png}
    \caption{
    The inverse mapping $\psi^{-1}$ found by ntKiNG. The classification boundary of a source classifier trained on \( p \) is depicted in four distinct colors. Target domain samples are marked with \(\bullet\), and KiNG transforms these samples to new positions indicated by \(\times\). Notably, many samples---especially those in the cyan and blue classes---are transported from the incorrect side of the classification boundary to the correct side and the test accuracy increases as a result. }
    \label{fig.transfer.illus}
\end{figure}

\begin{table}[t]
\centering
\begin{tabular}{lcccc}
\hline
Src. $\to$ Tar.        & base ($\%$) & ntKiNG & WGF & MMD flow \\
\hline
amz $\to$ dslr      & 69.50   & \textbf{+13.75}     & -0.50  & +2.75 \\
amz $\to$ web    & 72.75   & \textbf{+8.25}      & -2.00  & +0.75 \\
amz $\to$ cal   & 91.50   & -0.75      & -5.75  & \textbf{+0.00} \\
dslr $\to$ amz      & 86.00   & \textbf{+1.27}      & -6.37  & \textbf{+1.27} \\
dslr $\to$ web      & 98.09   & \textbf{+0.00}      & -0.64  & \textbf{+0.00} \\
dslr $\to$ cal     & 84.08   & \textbf{+5.73}      & -7.64  & +0.64 \\
web $\to$ amz    & 77.97   & \textbf{+3.39}      & -2.71  & +1.02 \\
web $\to$ dslr      & 91.19   & \textbf{+3.39}      & -4.07  & +1.36 \\
web $\to$ cal   & 76.61   & \textbf{+3.39}      & -3.73  & +0.34 \\
cal $\to$ amz   & 82.00   & -2.50      & -4.50  & \textbf{-0.25} \\
cal $\to$ dslr     & 58.25   & \textbf{+16.50}     & +7.00  & +3.25 \\
cal $\to$ web   & 65.50   & \textbf{+10.25}     & +1.00  & +2.00 \\
\hline
\textbf{Average}     & \textbf{79.45}  & \textbf{+5.22} & \textbf{-2.49} & \textbf{+1.09} \\
\hline
\end{tabular}
\caption{Comparison of testing accuracy differences (in $\%$) relative to the base classifier without any transfer learning. }
\label{tab:accuracy_comparison}
\end{table}





\input{related}


\newpage























































































\bibliography{refs}


\newpage
\appendix
\onecolumn
\section{Infinite-dimensional Exponential Family}
One important class of exponential family is infinite dimensional exponential family. 

\begin{example}
    An infinite-dimensional exponential family is an exponential family with a sufficient statistic $T(\boldx) := k(\boldx, \cdot)$: 
    \begin{align*}
    q(\boldx; \theta) = \exp\big(\langle \theta, k(\boldx, \cdot) \rangle - A(\theta)\big),
\end{align*}
where $A(\theta) := \int \exp\big(\langle \theta, k(\boldx, \cdot) \rangle - A(\theta)\big) \dx$ and $\theta$ is in an RKHS $\mathcal{H}$ with kernel $k$.   
\end{example}

Given a dataset of $n$ data points $\{\boldx_i\}_{i=1}^n \sim q_\theta$, the empirical natural gradient is defined as 
\begin{definition}
\begin{align}
\label{eq.kernel.ngd.maximum}
    \mathrm{NGD} := \argmax_{f\in \mathcal{H}} \langle f, \nabla \mathcal{L}(\theta) \rangle_\mathcal{H} 
    - \frac{1}{2} \langle f, \hat{\boldSigma} f\rangle_\mathcal{H} 
    - \lambda \|f\|_\mathcal{H}^2, 
\end{align}
where $\hat{\boldSigma}$ is the empirical covariance operator defined as 
\begin{align*}
    \hat{\boldSigma} := \frac{1}{n} \sum_{i = 1}^{n} \phi(\boldx_i)^\top \boldH \phi(\boldx_i), 
\end{align*}
and $\boldH$ is a centering matrix and  $\lambda > 0$. 
\end{definition}
Due to the Representer Theorem, we can see that the optimal solution takes the form $\mathrm{NGD} := \sum_i \alpha_i \phi(\boldx_i, \cdot)$. 

Since our aim is to match the projection $\delta \in \mathcal{H}$ to the natural gradient, we let $\delta = \sum_{i=1}^n \alpha_i k(\boldx_i, \cdot)$. The regularized objective of \cref{eq.locallinear.obj} takes the form
\begin{align}
\label{eq.locallinear.obj.alpha}
    \small
    J(\boldalpha) := & \int \lambda_{t_0}(t) \mathbbE \left[\left(s_t(X_t) - \langle \boldalpha, \boldk(X_t) - \mathbbE [\boldk(X_t')] \rangle\right)^2\right] \mathrm{d}t + \lambda \boldalpha \boldK \boldalpha 
\end{align}
where $\boldk(X) := [k(\boldx_1, X), k(\boldx_2, X), \dots]$. The limiting solution of \cref{eq.locallinear.obj.alpha}
is 
\begin{align*}
    & \lim_{\sigma \to 0} \boldalpha_{t_0}(\boldw) = \left(\mathrm{Cov}[\boldk(X_{t_0})] + \lambda \boldK \right)^{-1} \mathbbE \left[ \nabla \boldk(X_{t_0})  \boldh^\top(X_{t_0}; \boldw) \right]. 
\end{align*}

Substituting the form of $\mathrm{NGD}$ and solving  \eqref{eq.kernel.ngd.maximum} for $\boldalpha$, we get 
\begin{align*}
    \boldalpha^* = \left(\mathrm{Cov}[\boldk(X_{t_0}) + \lambda \boldK ]\right)^{-1}\left(\mathbbE_p[\boldk(\boldx)] - \mathbbE_q[\boldk(\boldx) ]\right). 
\end{align*}

Thus, the optimal drift parameter could be obtained via $\boldw^* := \argmin_\boldw \|\boldalpha^* - \lim_{\sigma \to 0} \boldalpha_{t_0}(\boldw) \|^2$, which has a similar solution to \cref{eq.kernel} or \cref{eq.ntk.update} depending on which model is used for the drift. 

\section{Proof of Theorem \ref{thm:delta3.1}}
\label{app:3.1}
\begin{theorem}(Theorem 4.1 in \citep{williams2024high}) 
    \cref{eq.locallinear.obj} can be rewritten as the following form
\begin{align}
    & \mathcal{L}(\bolddelta) = \int_0^1 \lambda_{t_0}(t)\mathbb{E}\left[ \langle \bolddelta,T(X_t) - \mathbb{E}[T(X^{\prime}_t)] \rangle^2 \right]\mathrm{d}t + 
    2 \int_0^1 \partial_t \lambda_{t_0}(t) \mathbb{E}\left[ \langle{\bolddelta, T({X_t})} \rangle \right]\mathrm{d}t +\text{const.} 
    \label{eq:deriv:objfinal}
\end{align}
\end{theorem}
Notice that \cref{eq:deriv:objfinal} is a quadratic minimization problem. 
Let $B_t = T(X_t) - \mathbb{E}[T(X^{\prime}_t)]$. Firstly, we find the derivative of the quadratic term.   
   For each \(t\), since 
   \(\nabla_\bolddelta\,\bigl(\langle\bolddelta,B_t\rangle^{2}\bigr)
   = 2\,\langle \bolddelta,B_t\rangle\,B_t,\)
   it follows that
   \[
   \nabla_\bolddelta
    \int_t \lambda_{t_0}(t)\mathbb{E}\bigl[\,\langle\bolddelta,B_t\rangle^{2}\bigr]\,\mathrm{d}t
   \;=\;
   2\,\int_t \lambda_{t_0}(t)\,\mathbb{E}\bigl[\langle \bolddelta,B_t\rangle\,B_t\bigr]\,\mathrm{d}t.
   \]
   Since \(\mathbb{E}[B_t] = 0,\) one has
   \(\mathbb{E}\bigl[\langle \bolddelta,B_t\rangle\,B_t\bigr] 
     \;=\; \mathrm{Cov}[B_t]\,\bolddelta \;=\;\mathrm{Cov}[T(X_t)]\,\bolddelta.\)\\
 Hence this part becomes
   \[
   2\int_t \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\bolddelta\;\mathrm{d}t.
   \]
Let us differentiate the linear-in-\(\bolddelta\) term.   
   The term
   \(\int_t 2\,\partial_{t}\lambda_{t_0}(t)\,\mathbb{E}\bigl[\langle \bolddelta,\,T(X_t)\rangle\bigr]\mathrm{d}t\)
   is linear in \(\bolddelta\).  Its gradient w.r.t.\ \(\bolddelta\) is simply
   \[
   2\int_t \partial_{t}\lambda_{t_0}(t)\,\mathbb{E}[T(X_t)]\,\mathrm{d}t.
   \]
Putting these together, the gradient of \(\mathcal{L}(\bolddelta)\) is
\[
\nabla_{\bolddelta}\,\mathcal{L}(\bolddelta)
~=~
2\,\int_t \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\bolddelta\,\mathrm{d}t
\;+\;
2\,\int_t \partial_{t}\lambda_{t_0}(t)\,\mathbb{E}[T(X_t)]\,\mathrm{d}t.
\]
To find the minimizer, set this gradient to zero:
\[
0
~=~
2\,\int_t \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\bolddelta\,\mathrm{d}t
\;+\;
2\,\int_t \partial_{t}\lambda_{t_0}(t)\,\mathbb{E}[T(X_t)]\,\mathrm{d}t.
\]
which can be rewritten as
\[
\Bigl(\,\int_t \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\mathrm{d}t\Bigr)\;\bolddelta 
\;=\;
-\;\int_t \partial_{t}\lambda_{t_0}(t)\,\mathbb{E}[T(X_t)]\,\mathrm{d}t.
\]
If the matrix \(\int_0^1 \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\mathrm{d}t\) is invertible—which is precisely the non-degeneracy (invertibility) of the Fisher information—then we can solve uniquely for \(\bolddelta\):

\[
\bolddelta_{t_0}
~=\;
\Bigl(\int_t \lambda_{t_0}(t)\,\mathrm{Cov}[T(X_t)]\,\mathrm{d}t\Bigr)^{-1} 
\Bigl(-\!\!\int_t \partial_{t}\lambda_{t_0}(t)\,\mathbb{E}[T(X_t)]\,\mathrm{d}t\Bigr).
\]


\section{Proof of Theorem \ref{thm.cf}}\label{app:kerneldelta}

Recall: 
\begin{align*}
    \bolddelta_{t_0}(\boldw) = - C^{-1} \int_{-\infty}^{\infty} \partial_t\lambda(t, t_0) \mathbbE \left[ T(g(Z, t; \boldw)) \right]\mathrm{d}t, ~~~
    C = \int_{-\infty}^\infty \lambda(t,t_0) \mathrm{Cov}[T(g(Z, t; \boldw))]
    \mathrm{d}t. 
\end{align*}

The first lemma states a few properties of the Gaussian kernel. 
\begin{lemma}
\label{lem.kernel}
    $\int_{-\infty}^{\infty} \partial_t \lambda(t, t_0) \mathrm{d} t = 0$. 
    $\int_{-\infty}^{\infty} (t - t_0)\,\partial_t \lambda_\sigma(t,t_0)\,\mathrm{d}t = -1$. 
\end{lemma}
\begin{proof}
    The first result is due to the Fundamental Theorem of Calculus and the fact that $\lambda(t, t_0) \to 0$ as $|t| \to \infty$. Now, we prove the second statement. 
    Since 
\begin{align*}
    \lambda_\sigma(t,t_0) 
\;=\; 
\frac{1}{\sqrt{2\pi\,\sigma^2}}\,
\exp\!\Bigl(\!-\frac{(t - t_0)^2}{2\,\sigma^2}\Bigr), 
\end{align*}
we have 
\begin{align*}
    \int_{-\infty}^{\infty} \lambda_\sigma(t,t_0)\,\mathrm{d}t
\,=\,
1,
\quad
\text{and}
\quad
\partial_t \lambda_\sigma(t,t_0) 
\,=\,
-\frac{(t - t_0)}{\sigma^2}\,\lambda_\sigma(t,t_0).
\end{align*}
We then have with integration by parts
\begin{equation}
\int_{-\infty}^{\infty} (t - t_0)\,\partial_t \lambda_\sigma(t,t_0)\,\mathrm{d}t
\,=\,
\Bigl.(t - t_0)\,\lambda_\sigma(t,t_0)\Bigr|_{-\infty}^{\infty}
\;-\;
\int_{-\infty}^{\infty} \lambda_\sigma(t,t_0)\,\mathrm{d}t
\,=\,
0 \;-\; 1
\,=\,
-1.
\end{equation}
the last equality is due to \(\lim_{|t| \to \infty}(t - t_0)\,\lambda_\sigma(t,t_0) = 0\).
\end{proof}


First, we inspect $\int_{-\infty}^{\infty} \partial_t \lambda(t, t_0) \mathbb{E}\bigl[T\bigl(g(Z,t;\bw)\bigr)\bigr] \mathrm{d}t. $
Using the Taylor expansion on $\mathbb{E}\bigl[T\bigl(g(Z,t;\bw)\bigr)\bigr]$ at \(t_0\), we obtain
\begin{equation*}
    \mathbb{E}\bigl[T\bigl(g(Z,t;\bw)\bigr)\bigr]
\,=\,
\mathbb{E}\bigl[T\bigl(X_{t_0}\bigr)\bigr]
\;+\;
(t - t_0)\,\mathbb{E}\bigl[\nabla T\bigl(X_{t_0}\bigr)^\top\,\bh\bigl(X_{t_0};\bw\bigr)\bigr].
\end{equation*}
Note that we don't have higher order terms as the drift model $g(Z, t; \boldw)$ is a linear function of $t$ by definition (see Definition \ref{ex.drift.model}). 

Thus, due to \cref{lem.kernel}, we have
\begin{align}
\label{eq.numerator}
    \int_{-\infty}^{\infty} 
  \partial_t \lambda_\sigma(t, t_0)\,
  \Bigl[
  \mathbb{E}\bigl(T\bigl(X_{t_0}\bigr)\bigr)
  \;+\;
  (t - t_0)\,\mathbb{E}\bigl(\nabla T\bigl(X_{t_0}\bigr)^\top \bh\bigl(X_{t_0};\bw\bigr)\bigr)
  \Bigr]
\,\mathrm{d}t
   & \;=\;
   -\,\mathbb{E}\bigl[\nabla T\bigl(X_{t_0}\bigr)^\top \bh\bigl(X_{t_0};\bw\bigr)\bigr]. %
\end{align}
Now we shift our focus on  
\[
C 
\,=\, 
\int_{-\infty}^\infty 
\lambda_\sigma(t,t_0)\,\mathrm{Cov}\bigl[T\bigl(g(Z, t; \bw)\bigr)\bigr]
\,\mathrm{d}t.
\]
As \(\sigma \to 0\), \(\lambda_\sigma(t,t_0)\) converges to \(\delta(t - t_0)\), so $\lim_{\sigma \to 0} \left(\int \lambda(t, t_0) f(t) \mathrm{d}t \right) = f(t_0)$. 
Hence 
\begin{align}
\label{eq.lim.C}
    \lim_{\sigma \to 0} C = \lim_{\sigma \to 0} 
\int_{-\infty}^\infty 
\lambda_\sigma(t,t_0)\,\mathrm{Cov}\bigl[T\bigl(g(Z, t; \bw)\bigr)\bigr]
\,\mathrm{d}t
\,=\,
\mathrm{Cov}\bigl[T(X_{t_0})\bigr].
\end{align}
Finally, combining \cref{eq.numerator} and \cref{eq.lim.C} 
 we have the desired result. 

\section{Proof of Theorem \ref{them.kernelNGD}}
\label{sec.proof.kernelNGD}
\begin{proof}
First, we introduce Welling's Woodbury identity \citep{welling2019notes}:
\[
(P^{-1} + B^T R^{-1} B)^{-1} B^T R^{-1} = P B^T (B P B^T + R)^{-1}.
\]

Recall, that we try to minimize 
    \cref{eq.simple.obj2}  
\begin{align}
    \|\nabla \mathcal{L}(\boldtheta_{t_0}) - \mathbb{E}[\nabla T(X_{t_0}) \nabla \boldh_w(X_{t_0})]\|_{\mathcal{F}_{t_0}^{-1}}^2 + \lambda \|w\|^2_\mathcal{H}, 
\end{align}
thus, the closed form solution of the optimal solution $w^*$ is 
\begin{align}
\left(\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]^\top\underbrace{\mathcal{F}_{t_0}^{-1}}_{R^{-1}}
\underbrace{\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]}_{B} 
+ \underbrace{\lambda \boldI}_{P^{-1}} \right)^{-1}\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]^\top\mathcal{F}_{t_0}^{-1}\nabla \mathcal{L}(\boldtheta_{t_0}). 
\end{align}
Applying Woodbury's identity, 
we get: 
\begin{align}
\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]^\top \left(
\lambda \mathcal{F}_{t_0} + 
\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)] \mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]^\top
\right)^{-1}\nabla \mathcal{L}(\boldtheta_{t_0}). 
\end{align}
Notice the product $\mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)] \mathbb{E}[\nabla T(X_{t_0}) \nabla k(X_{t_0}, \cdot)]^\top = \mathbb{E}[\nabla T(X_{t_0}) \nabla\nabla k(X_{t_0}, X_{t_0}') \nabla^\top T(X_{t_0}')]$, we obtain the desired result. 

    
\end{proof}

\section{Stein Exponential Family}
 In some applications (e.g. Variational Inference), we do not have samples from the target distribution, instead, we have an unnormalized density. Below, we introduce a special type of exponential family, named Stein exponential family, that allows us to approximate the natural gradient of $\mathrm{KL}[p, q]$.  

\begin{definition}
    A distribution belongs to the Stein exponential family of a target density $p$ and a test function $\boldf \in \mathbbR^d$ if and only if it belongs to the exponential family with a sufficient statistic that is $T= S_p\boldf$. Given a test function $\boldf = [f_1, f_2, \dots, f_b]$, a Stein operator $S_p\boldf$ of a probability density $p$ is a vector of functions defined as $S_p\boldf = [S_p f_1, S_p f_2, \dots, S_p f_b]$, where 
    \begin{align}
        S_pf_i =  \partial_i(\log p) \boldf + \partial_i \boldf. 
    \end{align}
\end{definition}

An important property of this special type of exponential family is that the expectation of the sufficient statistic is zero under the target distribution, i.e., 
$\mathbbE_{p}[T(\boldx)] = \boldzero$. More discussions on the Stein operator could be found in \citep{}. 
Therefore, if $q_\boldtheta$ is a Stein exponential family distribution, we can write the natural gradient of $\mathrm{KL}[p,q_\boldtheta]$ in \cref{eq.ngd.kl} as
\begin{align}
    \nabla_\boldtheta \mathcal{L}(\boldtheta) = \mathcal{F}^{-1}_t \mathbbE_{q_\boldtheta} \left[S_p f(\boldx)\right].  
\end{align}
Since the Stein operator only requires the gradient of the log density $\log p$ taken with respect to its input, this special design of exponential family enables us to compute the gradient for $\mathrm{KL}[p, q_t]$ using unnormalized $p$ only. 

This parametric model opens up applications such as variational inference and sampling: Given an unnormalized density $p$, we want to train a generative model to sample from $p$. We can design a  Stein exponential family so that we can approximately minimize $\mathrm{KL}[p, q_t]$ using kernel NGD by pushing particles towards the target distribution. 




\section{Reverse KL Wasserstein Gradient Flow and MMD Flow}
\label{sec.competitor.methods}
The WGF dynamics that minimize $\mathrm{KL}[q_t, p]$ move particles $X_t$ using a simple velocity field: 
\begin{align*}
    \frac{\mathrm{d}X_t}{\mathrm{d}t} = \nabla \left(\log p\right)(X_t) - \nabla \left(\log q_t\right)(X_t),  
\end{align*}
where the gradient of log density could be easily estimated via kernel density estimation and the MMD flow minimizes $\mathrm{MMD}[X_1, X_t]$ using the following velocity field: 
\begin{align*}
    \frac{\mathrm{d}X_t}{\mathrm{d}t} =  N \nabla \left(\frac{1}{N}\sum_{i = 1}^{n}\|X_t - X^{(i)}_t\| - \frac{1}{M}\sum_{j=1}^M\|X_t - Y^{(j)}\|\right). 
\end{align*}

\section{Experiment Setup in Section \ref{sec.exp}}
\label{app:expset}
We summarize the experiments' setup details in this section. For each experiment, we provide details of the dataset and pre-processing procedure, as well as the details of tuning parameters.

\subsection{Comparison with Reverse KL Wasserstein Gradient Flow and MMD Flow}

\subsubsection{Dataset and Pre-processing}
In this experiments, we let $p = 0.5\mathcal{N}(-2, \boldI) + 0.5\mathcal{N}(2, \boldI)$.
We draw 100 samples from $p$ as the target samples $Y$, 100 samples from $\mathcal{N}(0, \boldI)$ as the initial samples $X_0$. No further processing is required. 

\subsubsection{Parameter Tuning }
The main tuning parameter are kernel bandwidth and step sizes. 

For all methods that uses RBF kernel/basis, we set the bandwidth to be the median pairwise distance of all samples. 

For all methods, we use step size 1, as any larger learning rate would result in numerical instability for each method. 

For all methods, we run 100 particle updates. 

The performance metric MMD uses a Gaussian kernel and the bandwidth is set as the median of pairwise distances of all samples $Y$ and $X_t$.  

\subsection{Graphical Model Recovery}

\subsubsection{Dataset and Pre-processing}
In this experiment, we let $p$ be a 30-dimensional Gaussian graphical model, and draw 200 samples $Y \sim p$, 200 samples $X_0 \sim \mathcal{N}(\boldzero, \boldI)$. 
The graphical model $\boldTheta$ is generated as a random graph, with edge probability 0.05. For each non-zero off-diagonal entry, we set $\Theta_{i,j} = 0.3$. No further processing is required. 

\subsubsection{Parameter Tuning }
For all methods, we use the median of sample pairwise distances as the bandwidth. 

For all methods, we set the step size to be 1. 

Parameter tuning of Graphical Lasso is handled by \verb|sklearn| internally using 5-fold cross validation and the sparse graph in graphical model is obtained by truncating all values smaller than 0.1. Below are the Python code. 

\begin{verbatim}
# Fit with cross-validation to select alpha
model_cv = GraphicalLassoCV(alphas=10,  # number of alphas or list of alphas
                            cv=5,       # how many folds in cross-validation
                            max_iter=100, 
                            tol=1e-4)
model_cv = model_cv.fit(x1_test.cpu().numpy())
Theta_cv = model_cv.precision_ 
Theta_cv = Theta_cv > 1e-1
\end{verbatim}

\subsection{Experiment 2: Covariate Shift}
\subsubsection{Dataset and Pre-processing}
We validate our method on the dataset Office+Caltech\footnote{\url{https://github.com/jindongwang/transferlearning/blob/master/data/dataset.md\#office+caltech}}, which is a dataset for domain adaptation, consisting of Office 10 and Caltech 10 datasets. It contains the 10 overlapping categories between the Office dataset and Caltech256 dataset \citep{gong2012geodesic}.

The original features are extracted using a DECAF network, and are 4096 dimensional. 
We apply PCA on the source and target domain to reduce the dimension to 50 with Python code 
\begin{verbatim}
        from sklearn.decomposition import PCA
        pca = PCA(n_components=50)
        pca.fit(X)
        X = pca.transform(X)
        X = X / 100
\end{verbatim}

Due to memory space limit, we also randomly pick 200 samples from all target domains as $X_0$. 



\subsubsection{Parameter Tuning }
For all methods, we set step size to 0.1. 

For ntKiNG, we run 100 steps due to reduce the computation cost. 

For WGF and MMD flow, we run 1000 steps. 

The source classifier is an RBF kernel Support Vector Machines with all hyper-parameters chosen by cross-validation with the following python code: 
\begin{verbatim}
    # Split the data into training and test sets (optional)
    X_train, X_test, y_train, y_test = train_test_split(x, y, 
                               test_size=0.3, random_state=42)

    # Define parameter grid
    param_grid = {
        'C': np.logspace(-3, 3, 5),
        'gamma': np.linspace(.2, 5, 5) * gamma,
        'kernel': ['rbf']
    }

    # Create a SVC classifier
    svc = SVC()

    # Initialize GridSearchCV
    grid_search = GridSearchCV(svc, param_grid, refit=True, verbose=2, cv=5)

    # Fit the model
    grid_search.fit(X_train, y_train)
\end{verbatim}
where gamma is the inverse of the median pairwise distances of all inputs. 

\end{document}
