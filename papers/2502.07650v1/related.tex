\section{Related Works}

Our methods bridge the gap between generative model training/sampling and parametric model optimization. Both domains are extensively studied in the literature. 

There has been a trend toward using optimization techniques to sample from unknown distributions. These methods first draw samples from an initial distribution and then move them according to a time-dependent velocity field \citep{liu2017stein,Chewi2020,maurais2024sampling}. A typical family of methods is the Wasserstein Gradient Flow \citep{ambrosio2008gradient}, which has found various applications outside of sampling, such as generative modelling \citep{gaodeep19,choi2024scalable} and missing data imputation \citep{chen2024rethinking}. Our method falls within this family of algorithms, and we compared two of its variants in our experiments. 
To the best of our knowledge, none of the existing approaches could leverage a pre-existing probabilistic model to guide the flow of particles. Our framework is also more general: \cref{alg:simple} works for non-particle based generative models as well (as we see in \cref{sec.gm}). 

Another trend in generative modelling is ``flow matching'', where one aligns the drift function with a pre-constructed flow \citep{lipman2023flow,liu2023flow}. In a similar spirit, our method also aligns the instantaneous change of the generative  distribution with a prescribed dynamics (NGD). However, instead of directly matching the velocity field in the sample space, we match the projections of these changes in the parametric space. This approach avoids building arbitrary "bridges" between the reference and target distributions in sample space and instead leverages an effective parametric optimization algorithm to guide the training of the generative model.

In recent years, there has also been efforts to accelerate and approximate NGD using kernel methods, for example,   \citep{Arbel2020Kernelized,Li2019Affine} propose to approximate the natural gradient by optimizing a dual formulation. 
However, both methods consider optimizing a probabilistic model, rather than a generative model as described in this paper. Performing NGD requires inverting a large matrix. 
Many research on NGD focuses on approximating the inverse the Fisher Information Matrix \citep{martens15optimizing,grosse2016kronecker,george2018fast}. 
Our particle update, e.g., \cref{them.kernelNGD} also requires us inverting a matrix with the dimension of the sufficient statistic. 
It would be an interesting future work to see if these techniques could be adapted to our approach. 





