\section{Related work}
\subsection{Mixup Methods}
Mixup methods have gained significant traction in recent years as a powerful data augmentation technique to enhance the generalization and robustness of machine learning models. The foundational work by Zhang et al. ____ introduced Mixup, which generates synthetic training examples through linear interpolations of pairs of examples and their labels: 
$\mathbf{x}_{\text{mix}}=\lambda \mathbf{x}_i + (1-\lambda)\mathbf{x}_j$ 
and $\mathbf{y}_{\text{mix}}=\lambda \mathbf{y}_i + (1-\lambda)\mathbf{y}_j$. 
This simple yet effective approach has been widely adopted and has inspired numerous variants and extensions.
Building upon the original Mixup concept, CutMix ____ advances the idea by combining patches from different images rather than entire images, thereby preserving more local information and improving model performance. 
AugMix ____ augments images using a combination of simple image transformations and mixes the resulting images to create robust augmentations. 
Manifold Mixup ____ performs mixup operations in the feature space rather than the input space, leading to smoother decision boundaries and better generalization.
AutoMixup ____ parameterizes the mixup process, 
providing a more automated way to generate mixup samples. 
Mixup has also been used to address various specific challenges, 
such as semi-supervised learning (SSL) and domain generalization. 
MixMatch ____ leverage mixup data 
to enhance SSL from limited labeled data. 
MixStyle ____ mixes the feature statistics of training samples across multiple source domains 
to increase the domain diversity of the source domains. 
Despite these advancements, most existing methods rely on linear combinations of pairwise samples, 
limiting their ability of 
generating diverse instances
for challenging and refining the model's learning process.

Our proposed method, Model-aware Parametric Batch-wise Mixup (MPBM), 
advances the mixup technique in key aspects and 
addresses these limitations by 
parametrizing a query-guided feature-wise mixup process 
using learnable attention mechanisms. 
This approach adaptively generates more complex and informative synthetic examples
based on both the current prediction model and the training set, 
thereby enhancing model robustness and generalization capabilities.

\subsection{Single Domain Generalization}
The concept of Single Domain Generalization (SDG) centers on 
the task of learning generalizable models using only a single source domain, without any exposure to target domain distributions. 
Unlike traditional domain generalization which benefits from multiple source domains to enhance model robustness and generalizability, 
SDG must rely solely on a single source domain to overcome potential distribution shifts that might be encountered in unseen target domains,
thereby posing much more difficult challenges. 
Current methodologies in SDG fall into three main categories,
primarily centering on enriching the training data through data augmentation. 
Firstly, traditional data augmentation techniques are employed to bolster in-domain robustness.
Seminal works in this category include improved augmentation strategies ____, AugMix ____, and AutoAugment ____, 
which primarily enhance sample variability within the source domain 
to improve out-of-domain generalization. 
Lian et al. ____ extended this notion by introducing geometric transformations aimed at enriching the diversity of training samples. 
ACVC ____ samples transformations from a pool of visual corruptions to simulate distinct training domains and enforces visual attention consistency between the original and corrupted samples.
%
The second category introduces adversarial data augmentation approaches, which creatively manipulate either pixel space or latent features to mimic domain variability. Volpi et al. ____ and Zhao et al. ____ pioneered augmenting images directly in the pixel space, aiming to conjure worst-case domain shifts. Subsequently, work by  Zhang et al. ____ advances this concept by perturbing latent feature statistics, yet consistently generating extensive domain shifts remains a challenge. 
Adversarial AutoAugment ____ formulates augmentation policies through adversarial learning 
to enhance in-domain generalization.
%
The third category leverages generative models to synthesize training data. Innovations by Qiao et al. ____, Wang et al. ____, and Li et al. ____ utilize generative adversarial networks (GANs) and variational autoencoders (VAEs) to generate diverse, albeit domain-constrained, samples. MCL ____ first simulates domain shift with an auxiliary target domain, then analyzes its causes, and finally reduces the shift for model adaptation. These methods produce synthetic data that, while stylistically varied, still reflect inherent characteristics of the source domain.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[t]
  \centering
 \includegraphics[width= 1\textwidth]{images/figure1.pdf} 
	 \caption{Overview of the training process for the mixup generator. The process begins with a pre-trained feature extractor $f_{\theta}$ and classifier $h_{\psi}$, whose parameters ($\theta$ and $\psi$) remain frozen during the training of the Mixup Generator Network (MGN) $g_{\phi}$. Query image inputs undergo Langevin Stochastic Query Augmentation. The Entry-Wise Feature Attention Mechanism captures intricate interactions among feature dimensions by incorporating a correlation matrix, facilitating precise feature mixups. The mixup generation loss $\mathcal{L}_{\text{mix}}^{\text{gen}}$ is being optimized for the generated mixup samples. The Adversarial Mixup Generation component aligns generated mixup features with real data through an adversarial training framework, optimizing the adversarial loss $\mathcal{L}_{\text{adv}}(\phi, \omega)$.
	}
   \label{fig:method}
   \vskip -0.1 in
\end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%