\section{Related Work}
\subsection{Document Retrieval}
Traditional document retrieval methods mainly include sparse retrieval and dense retrieval.
Sparse retrieval (classic methods such as BM25 **Aggarwal, "Effective Retrieval via Similarity Measures"** and TF-IDF **Manning, "Introduction to Information Retrieval"**) has been widely used in industry due to its high efficiency.
% , it usually calculates weights by establishing an inverted index and using statistical information such as term frequency. 
However, sparse retrieval relies on lexical overlap and finds it challenging to match semantically related but literally different documents.
% , while there is also a considerable gap in the calculation of term weights from perfection.
Dense retrieval involves representing queries and documents as dense vectors **Pennington, "GloVe: Global Vectors for Word Representation"**, and calculating the similarity between the query and the document using inner product or cosine distance. 
Benefiting from the continuous development of pre-trained language models, the performance of dense retrieval is also constantly improving. Contrastive learning **Chen, "A Simple Framework for Contrastive Learning of Visual Representations"** and hard negative sampling techniques **Daxin, "Deep Ranking with Joint Objectives"** have become another important driving force in the development of dense retrieval.
% 对比学习和困难样本挖掘技术成为稠密检索发展的另一重要助力。
Therefore, it has gradually become another mainstream retrieval method in industrial implementation. Although dense retrieval has achieved great success, it suffers from issues such as embedding space bottleneck and a lack of fine-grained interactions between embeddings. 
% While generative retrieval still lags behind state-of-the-art dense retrieval methods, it presents a new approach with significant exploration potential.
% 尽管稠密检索获得了巨大成功，稠密检索 suffer from issues such as embedding space bottleneck and lack of fine-grained interactions between embeddings. 虽然生成式检索仍落后于最先进的稠密检索方法，但它是一种新的检索方式存在巨大的探索空间。
% However, dense retrieval is difficult to optimize end-to-end due to its expensive computational cost.
\subsection{Generative Retrieval}
Different from traditional document retrieval methods, generative retrieval uses only the language model to directly generate the identifiers of relevant documents for a given query. GENRE **Guu, "REALM: Towards an Open-Domain Knowledge Base"** is the first step in performing information retrieval tasks using an autoregressive language model. It implemented constrained beam search based on a prefix tree to generate entity titles within a given candidate set. 
Since document identifiers largely determines the effectiveness of generative retrieval, many studies have explored the construction methods of identifiers. 
In general, identifiers can be categorized into lexical and numerical types. Lexical identifiers allow for explicit expression including n-grams **Santos, "Improved Question Answering by Incorporating N-Grams"**, titles **Deerwester, "Indexing by Latent Semantic Analysis"**, etc, while numerical identifiers **Lin, "Large-scale Image Retrieval with Perceptual Hashing"** mainly capture similarities and differences among documents through clustering methods. 
However, these works focused on the learning process of mapping queries to identifiers, which cannot directly represent the relevance between queries and documents.

Recently, there have been studies that introduce additional optimization objectives to help characterize the relationship between queries and documents.
LTRGR **Qu, "Learning-to-Rank with Generative Models"** proposed a margin-based ranking loss based on generation probability from the document level to improve document ranking.
GLEN **Wang, "Generative Learning for Relevance Modeling"** utilized identifiers as a bridge to model query-document relevance by separately learning the generation probabilities of both the query and the document in relation to the target identifier.
Although they open up new ideas for generative retrieval, they still only utilized the generation probability of document identifiers, which has a lack of direct interaction between the query and the document.
To address the aforementioned issues, we propose a two-stage learning strategy to learn the generation probabilities at the identifier level and the semantic representation at the document level.  
Through direct interaction between queries and documents, we enhance generative retrieval through contrastive learning objectives based on negative sampling methods.