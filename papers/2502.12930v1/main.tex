\documentclass[lettersize,journal,twoside,article]{IEEEtran}

\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{multirow}
\usepackage{hyperref}
\usepackage{pmboxdraw}
\usepackage[table,xcdraw]{xcolor}
\usepackage{nicematrix}
\usepackage[flushleft]{threeparttable}
\usepackage{enumitem}
\usepackage{orcidlink}

\newcommand{\defref}[1]{Def.~\ref{#1}}
\newcommand{\presref}[1]{Pres.~\ref{#1}}
\newcommand{\equref}[1]{Eq.~\(\ref{#1}\)}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tabref}[1]{Table~\ref{#1}}
\newcommand{\obsref}[1]{Observation~\ref{#1}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.

\title{Universal Embedding Function for Traffic Classification via QUIC Domain Recognition Pretraining: A Transfer Learning Success}

\author{Jan Luxemburk\orcidlink{0000-0003-0879-0054},
        Karel Hynek\orcidlink{0000-0002-8281-618X},
        Richard Plný\orcidlink{0000-0002-0544-8424},
        and~Tomáš Čejka\orcidlink{0000-0001-7794-9511}
\thanks{This research was funded by the Ministry of Interior of the Czech Republic, grant No. VJ02010024: \textit{"Flow-Based Encrypted Traffic Analysis"} and also supported by the Grant Agency of the CTU in Prague, grant No. SGS23/207/OHK3/3T/18. Computational resources were provided by the e-INFRA CZ project (ID:90254), supported by the Ministry of Education, Youth and Sports of the Czech Republic.}
\thanks{Jan Luxemburk, Richard Plný, and Karel Hynek are with the Faculty of Information Technology, Czech Technical University in Prague, Czech Republic. All authors are with the CESNET association, Czech Republic.}
\thanks{Corresponding author email: luxemburk@cesnet.cz.}}

\maketitle

\begin{abstract}
Encrypted traffic classification (TC) methods must adapt to new protocols and extensions as well as to advancements in other machine learning fields. In this paper, we follow a transfer learning setup best known from computer vision. We first pretrain an embedding model on a complex task with a large number of classes and then transfer it to five well-known TC datasets. The pretraining task is recognition of SNI domains in encrypted QUIC traffic, which in itself is a problem for network monitoring due to the growing adoption of TLS Encrypted Client Hello. Our training pipeline---featuring a disjoint class setup, ArcFace loss function, and a modern deep learning architecture---aims to produce universal embeddings applicable across tasks. The proposed solution, based on nearest neighbors search in the embedding space, surpasses SOTA performance on four of the five TC datasets. A comparison with a baseline method utilizing raw packet sequences revealed unexpected findings with potential implications for the broader TC field. We published the model architecture, trained weights, and transfer learning experiments.

\end{abstract}

\begin{IEEEkeywords}
Traffic classification, Transfer learning, Deep learning, Encrypted traffic, QUIC
\end{IEEEkeywords}

\section{Introduction}

In this paper, we propose a universal embedding (mapping) function that transforms packet sequences into an embedding vector space. The core idea is to map similar packet sequences close to each other in the embedding space while keeping dissimilar ones far apart. The embedding function serves as a feature extractor, enabling a nearest neighbors (k-NN) classifier to make predictions. As our focus is on encrypted traffic, we utilize the standard input representation unaffected by encryption: packet size, direction, and inter-packet time of the first \textit{N} packets. 

Building on our previous research in fine-grained traffic classification (TC) for TLS~\cite{Luxemburk2023TLS} and QUIC~\cite{Luxemburk2023QUIC}, we design and train the embedding function using the CESNET-QUIC22~\cite{Luxemburk2023QUICDataset} dataset. This dataset includes Server Name Indication (SNI) domains as labels, enabling a classification task focused on inferring exact domain names from packet sequences. This domain recognition task serves two important roles in this paper. First, a solution for this task is in itself relevant for network monitoring given the increasing adoption of the new Encrypted Client Hello (ECH) TLS extension. ECH encrypts entire ClientHello messages during TLS handshakes, making the traditional SNI parsing method ineffective. Using our embedding function and a k-NN classifier, we inferred the correct domain name in 94.83\% of cases when evaluated on test domains that were entirely disjoint from the embedding function's training set. Second, domain recognition is well suited as a pretraining task for transfer learning methods due to its large number of unique classes and straightforward labeling process. Transfer learning leverages models trained on one task to adapt them for a different, related task, assuming that some knowledge between the tasks is shared and transferable~(extracted features, learned traffic patterns and characteristics). This technique is popular in computer vision and natural language processing areas, where the standard experimental pipeline often involves fine-tuning of public models pretrained on large image or text datasets. Our findings suggest that the domain recognition task can serve as a foundation for model transfer methods in TC.

To evaluate the transfer learning setup, the trained embedding function was tested on five additional TC datasets (UCDAVIS19~\cite{dataset_UCDAVIS19}, UTMOBILENET21~\cite{dataset_UTMOBILENET21}, MIRAGE19~\cite{dataset_MIRAGE19}, MIRAGE22~\cite{dataset_MIRAGE22}, CESNET-TLS22~\cite{Luxemburk2023TLS}). Each dataset presents a task with different classes and challenges, allowing us to evaluate how well the embedding function generalizes under various conditions. The results are promising, as our embedding function surpassed the performance of the state-of-the-art (SOTA) classifiers on four of the five datasets. Furthermore, our experiments revealed an intriguing finding: a k-NN classifier using L1 distance on the first 10 packet features---referred to as the input-space baseline---performed quite well across all datasets, matching SOTA performance on MIRAGE19 and even surpassing it on UTMOBILENET21.

The paper is organized as follows: \secref{sec:experimental-setup} describes the experimental setup used to develop the embedding function, covering the dataset, data preparation, training loop, loss function, and deep learning (DL) architecture. \secref{sec:domain-recognition-results} presents results of the domain recognition task, along with ablation studies examining parts of the solution. \secref{sec:transfer-learning} describes the transfer of the trained embedding function to five additional TC datasets and discusses the achieved results. \secref{sec:related-work} provides a brief review of prior research in TC representation learning. \secref{sec:conclusion} summarizes key contributions, discusses computational performance, and outlines future directions.

\section{Experimental setup}
\label{sec:experimental-setup}
The overall domain recognition experimental setup is designed as a retrieval task for finding the most similar network flows, analogous to image retrieval tasks in computer vision. The SNI-based classes are divided into three disjoint sets: training, validation, and test. The training domain set is used to train a neural network, which serves as the embedding function that learns vector representations of network flows. This embedding function, denoted as $\Phi$, is formalized in~\equref{equ:embedding-function} and illustrated in~\figref{fig:embedding-function}.
\begin{equation}
\label{equ:embedding-function}
\Phi: \textit{Flow} \to \mathbb{R}^d,\,d = \textit{embedding size}
\end{equation}
The validation domain set is used to measure performance during training, select the best model, and for finding the best configuration of hyperparameters. The test domain set is reserved for measuring and reporting the final metrics. The training, validation, and test domain sets are \textbf{disjoint}, so this setup pushes for strong generalization capabilities of the embedding function. It must learn patterns and extract traffic characteristics that remain useful for domains not seen during training. We based our experiments on the CESNET-QUIC22~\cite{Luxemburk2023QUICDataset} dataset, which is described in the next section.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/embedding-function.pdf}
    \caption{A complete processing pipeline starting with network flows as input. The embedding function $\Phi$, which is implemented as a neural network, maps flows into a 256-dimensional vector space. The visualized ArcFace head is used during training to optimize the neural network, which is composed of a backbone model and a compression neck.}
    \label{fig:embedding-function}
\end{figure*}

\subsection{CESNET-QUIC22 dataset}
\label{sec:dataset}
The CESNET-QUIC22~\cite{Luxemburk2023QUICDataset} dataset includes four weeks of traffic captured at the monitoring points of the CESNET network, which is the national research and education network of the Czech Republic. The dataset consists of 153 million anonymized network flows enriched with various traffic features suitable for the classification of encrypted traffic. For this work, the relevant features are the packet sequences and SNI domains. The packet sequences include packet sizes, inter-packet times, and packet direction of the first 30 packets. The SNI domain is extracted from the Server Name Indication extension transmitted during the QUIC handshake. For further details on the dataset and its collection process, including the software used, sampling methods, flow export timeouts, and other relevant aspects, please refer to the original data article~\cite{Luxemburk2023QUICDataset}.

The dataset includes 102 service classes, where each \textit{service} groups one or more domain names under a single label. However, since our objective is to predict individual domains, we do not use these predefined service labels. Instead, we utilized the exact SNI domains that are available in the dataset. Moreover, since the dataset contains general background traffic, the domains are not limited to the 102 service classes but represent all domains observed within the CESNET network.

In our previous experiments~\cite{Luxemburk2023QUIC} with CESNET-QUIC22, we observed significant data drift in the traffic from the first two weeks of the dataset. In this work, we want to focus on evaluating the embedding function for domain recognition and transfer learning, without introducing extra complexities of the data drift. Therefore, we based our experiments on the third week of the dataset {(W-2022-46)} and used 33.7 million samples from this week for training, hyperparameter tuning, and the final evaluation of the proposed solution.

\subsection{Experimental pipeline}
The following subsections describe individual steps of the experimental setup, starting with domain preprocessing, data preparation, training \& validation, and model selection.

\subsubsection{Domain preprocessing and train/val/test split}
\label{sec:domain-preprocess}
The first step was to preprocess SNI domain names into class labels. We keep subdomains up to the fourth level and strip the rest (\texttt{a.tile.openstreetmap.org} is a fourth-level domain). Some domains contain a random string or parts related to locations or numbering. For example, the aforementioned openstreetmap domain has two "sister" domains \texttt{[b,c].tile.openstreetmap.org}. We decided to group such domains into a single class with the help of regexes. Another example would be \texttt{europe-west1-gcp.api.snapchat.com} and \texttt{us-east4-gcp.api.snapchat.com}, both remapped to a single class \texttt{\$LOC-gcp.api.snapchat.com}. In total, we created 40 regexes that remap thousands of domains with random parts into corresponding unified \textit{domain classes}.

After this domain preprocessing, we selected the 2000 most frequent \textit{domain classes} and divided them randomly into three subsets: 1000 for training, 500 for validation, and 500 for testing. These 2000 domains account for 99.38\% of the total flows in the dataset's third week.

\subsubsection{Database and query preparation}
\label{sec:database-preparation}
Next, we prepare databases and query samples for validation and test domain sets. A database serves as a mini-training set for a k-NN classifier, which is then tested on query samples to measure performance. The exact same process is used for validation and test domains, and we will describe it for validation.

Validation samples (those having one of the 500 validation domains) are split into database and query parts. This split is random and stratified, meaning the class frequencies are preserved in both parts. We set the query part to contain one million samples and leave the rest for building the database. Out of these remaining samples, we randomly select one million to be included in the database. However, this second database sampling is not uniform but instead we soften the class imbalances. We set the weight of a sample belonging to class $C$ to $N_{C}^{-\lambda_{db}}$, where $N_C$ is the $C$ class frequency and $\lambda_{db} > 0$ a parameter controlling the strength of the balancing effect. We ended up using $\lambda_{db} = \frac{1}{2}$, meaning the weight formula is $\frac{1}{\sqrt{N_C}}$. This method for the selection of database samples prioritizes classes that are less frequent at the expense of the most frequent ones. To summarize, for both validation and test domain sets, we selected one million query samples ($Q_{val}$, $Q_{test}$) that follow the dataset's original class distribution, and we created a database ($DB_{val}$, $DB_{test}$) from one million samples with a more balanced class distribution. 

\subsubsection{Training, validation, and ranking}
\label{sec:ranking}
After preprocessing domains, splitting them into disjoint sets, and preparing databases and query samples, we can start training the $\Phi$ embedding function using the training domain set. The training loop runs for 30 epochs, measuring validation metrics each two epochs. The training loop is described in detail in~\secref{sec:training-loop}. The validation is performed as a similarity search in the embedding space. For each $Q_{val}$ sample, we find the most similar $DB_{val}$ samples, a process we refer to as database ranking. We limit the ranking to the closest 20 samples and compute several metrics that are described in~\secref{sec:metrics} to measure the quality of the embeddings. Each time, validation reuses the same $DB_{val}$ and $Q_{val}$ samples---what changes are the embeddings that are recomputed with model weights from the current epoch.

To compute distances in the embedding space, we use cosine similarity, which is a metric calculated as the dot product of L2-normalized vectors. When applied to L2-normalized vectors, cosine similarity produces the same ranking as the well-known Euclidean distance. To efficiently compute cosine similarities between all query samples and all database samples, we used the \textit{faiss} library~\cite{johnson2019billion} specifically designed for efficient similarity search and clustering of vectors.

\subsubsection{Model selection and final evaluation}
We chose the macro-average validation recall as our main objective, favoring models that perform well on all validation domains with equal importance (macro-averaging disregards sample count per domain). After the 30 training epochs, the model from the epoch with the best validation recall is saved and evaluated on the test domain set. This final evaluation is identical to the validation process but is using the test query samples $Q_{test}$ and test database $DB_{test}$. A holistic overview of the entire experimental setup is provided in~\figref{fig:experimental-setup-overview}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.84\columnwidth]{figures/experimental-setup-overview.pdf}
    \caption{An overview of the experimental setup, highlighting the purpose of the disjoint domain split along with the database and query preparation for validation and testing.}
    \label{fig:experimental-setup-overview}
\end{figure}

\subsection{DL architecture}
\label{sec:dl-architecture}
We based the neural network architecture on our previous works on TLS and QUIC classification~\cite{Luxemburk2023TLS, Luxemburk2023QUIC}, but due to substantial modifications, we describe it here in detail. The architecture is single-modal and designed to process fixed-sized packet sequences ($N = 30$) with the following features: packet sizes, inter-packet times (IPT), and directions. Network flows with fewer than $N$ packets are padded with zeroes. The network is visualized in Figure~\ref{fig:backbone-model}. It follows the standard architecture of modern CNNs and consists of four main components: a stem, convolutional blocks, global pooling, and a feature refinement block.

\subsubsection{Stem}
\label{sec:stem}
The purpose of our network stem is to embed packet features into $R$-dimensional vectors to prepare them for subsequent processing with convolutions. This is achieved using two PyTorch \textit{Embedding}\footnote{\url{https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html}.} layers---one for embedding of packet sizes and another for IPT. Each \textit{Embedding} layer contains a learnable matrix with the shape \textit{number of embeddings} $\times$ \textit{embedding size}, where each row represents the embedding vector for a specific value. Packet sizes range from 0 to 1500, resulting in 1501 embeddings. For IPT, we first bin the values into 200 bins and then use the index of a bin as the input for the \textit{Embedding} layer. Packet directions are one-hot encoded, which we found to be more effective than the traditional $\pm1$ encoding scheme. We set the embedding size to 20 for packet sizes and 10 for IPT. Thus, the stem outputs data in a shape 30 $\times$ 32, where 30 corresponds to the packet sequence length, and 32 represents the combined embedding vector (20 for packet sizes, 10 for IPT, and 2 for directions).

The \textit{Embedding} layers for packet sizes and IPTs were initialized using the Piecewise Linear Encoding (PLE) method proposed in~\cite{gorishniy2022embeddings}, rather than the default random initialization. PLE creates initial embeddings structured as bins, where each bin corresponds to a segment of the feature's range (hence the name "piecewise"). Within each bin, linear relationships are preserved, maintaining the inherent ordering of numerical features. During training, the embeddings are optimized alongside the other model weights to adapt to the data. While embedding packet features before convolutional processing is not a novel concept---having been employed, for instance, in Nascita et al.~\cite{Nascita2023Embeddings}---to the best of our knowledge, the PLE initialization technique has not yet been used in the TC domain.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/backbone-model.pdf}
    \caption{The architecture of the \texttt{30pktTCNET} backbone model consists of four main components: a stem, convolutional blocks, global pooling, and feature refinement. The main processing is done in the convolutional blocks, which include four Bottleneck Residual Blocks described in detail in~\figref{fig:bottleneck-block}. Each block has a different configuration of the following parameters: the number of output channels (e.g., 256c), kernel size (e.g., 7k), and dropout rate.}
    \label{fig:backbone-model}
\end{figure*}
 
\subsubsection{Convolutional blocks}
The core processing in terms of feature extraction and parameter count is done with convolutions, implemented as four residual blocks adapted from the popular ResNet architecture. Specifically, we use Bottleneck Residual Block visualized in~\figref{fig:bottleneck-block}, which was proposed in~\cite{He2016DeepResidual}. This block design reduces the number of parameters while preserving representational power. The term "bottleneck" refers to the temporary reduction of channels with a 1$\times$1 convolution before the main convolution, followed by their restoration (or increase) afterward. Each block is defined by the following parameters: the main convolution kernel size $k$, the number of output channels $C_{out}$, the dropout rate, and the bottleneck ratio defining the reduction of channels for the main convolution (we use a common setting of $\frac{1}{4}$). The main convolution operation uses automatic padding to ensure that the spatial dimension (i.e., the length of packet sequences) remains unchanged. For the same purpose, we also use a stride of 1 in all convolutions, as we found that reducing the spatial dimension with stride led to decreased performance. 

The four bottleneck blocks use different parameters, which are shown in Figure~\ref{fig:bottleneck-block}. In summary, the number of output channels increases (192, 256, 384, 448), the kernel sizes decrease (7, 7, 5, 3), and dropout rates increase (0\%, 10\%, 20\%, 30\%).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.35\textwidth]{figures/bottleneck-block.pdf}
    \caption{The diagram of Bottleneck Residual Block. $k$: the kernel size of the main convolution, $C_{out}$: the number of output channels, $r$: the dropout rate. The number of channels of the main convolution $C_{mid}$ is set to $\frac{C_{out}}{4}$. All convolutions use a stride of 1 and automatic padding to ensure that the spatial dimension is kept intact. Convolutions do not use biases.}
    \label{fig:bottleneck-block}
\end{figure}

\subsubsection{Global pooling}
The next component of the network is a global pooling operation, which converts each feature map (30 $\times$ 448) into a single scalar value (1 $\times$ 448). Common pooling methods include either taking the average or the maximum of the values. We found maximum pooling to perform better than average pooling; however, our final choice was Generalized Mean Pooling (GeM)~\cite{Radenovic2018GeM}. GeM includes a parameter $p$, which allows to interpolate\footnote{The exact GeM formula is $\left( \frac{1}{|X_c|} \sum\limits_{x \in X_c} x^p \right)^{\frac{1}{p}}$, where $X_c$ is one channel of the convolutional blocks output (called feature maps).} between maximum ($p\to\infty$) and average pooling ($p = 1$). The parameter $p$ can be a fixed value or trained along with the other model weights. We initialized $p = 3$ and optimized it during training.

\subsubsection{Feature refinement \& Compression neck}
The output of the GeM pooling is passed through a \textit{feature refinement block}---a simple sequence of Linear, BatchNorm, and ReLU layers. This \textit{feature refinement block} preserves the shape of the features, resulting in an output size of 448.\footnote{Note that the batch size is omitted from all data shapes discussed in~\secref{sec:dl-architecture}.}

Up to this point, the defined neural network architecture can be used for standard classification tasks; adding one extra Linear classification layer with a shape 448 $\times$ \textit{number of classes} would do the job. However, our goal is to produce embeddings of network flows. Thus, as the final part of the network, we add a \textit{compression neck} that is composed of a Linear layer with a shape 448 $\times$ 256, BatchNorm, and a vector L2-normalization operation. The \textit{compression neck} excludes a ReLU activation function on purpose, as its task is to compress features into the desired embedding size of 256 without introducing additional non-linear transformations. 

We want to distinguish the backbone part---the stem, convolutional block, and the \textit{feature refinement block}---from the entire neural network. We refer to this backbone as \texttt{30pktTCNET} (visualized in~\figref{fig:backbone-model}), while the complete model representing the $\Phi$ embedding function is denoted as \texttt{30pktTCNET\_256} (visualized in~\figref{fig:embedding-function}). With the final hyperparameter configuration, \texttt{30pktTCNET\_256} has one million trainable parameters.

\subsection{Training loop}
\label{sec:training-loop}
This section outlines the training loop that we used to optimize the neural network, describing the training sampler, loss function, optimizer, and learning rate (LR) scheduler.

\subsubsection{Training sampler}
\label{sec:training-sampler}
The training loop consists of 30 epochs. In each epoch, one million training samples are randomly selected for training. We used a modified random sampler, where the weights of samples of class $C$ are set to $N_{C}^{-\lambda_{sampler}}$, with $N_C$ being the $C$ class frequency. We used the same value $\lambda_{sampler} = \frac{1}{2}$ as in the formula for selecting samples for the database. This approach softens class imbalances in the training set of each epoch, gives more focus on less frequent classes during training, and allows the model to learn traffic patterns from more diverse samples.

\subsubsection{ArcFace loss function}
In supervised metric learning, there are two main categories of loss functions used for training embedding models. The first consists of contrastive approaches, such as Contrastive~\cite{Chopra_2005} or Triplet~\cite{Schroff_2015} loss, which pull together embeddings of samples with the same label and pushes apart those with different labels. These methods operate within each mini-batch, using local sample-to-sample comparisons. The other group of loss functions includes softmax-based losses with class centers and margin modifications, such as ArcFace~\cite{Deng2019ArcFace} and CosFace~\cite{Wang_2018}. These methods introduce class-specific centers and enforce angular or cosine margins to better separate classes. Samples are pulled toward their class centers based on global sample-to-class comparisons. ArcFace, along with its sub-center variant~\cite{Deng2020Subcenter}, represents the current state-of-the-art. While originally developed for face recognition, ArcFace has proven effective across various tasks like image retrieval and fine-grained classification. The following paragraph contains a short overview of how the ArcFace loss works. For a more technical and formal description, please refer to the original ArcFace paper~\cite{Deng2019ArcFace}.

Training a neural network with the ArcFace loss involves adding an "ArcFace head" (see~\figref{fig:embedding-function}), which is detached and not used when the network is used for generating embeddings during validation and testing. The head contains a matrix of class centers, which are learnable during the training process. For computing the loss, both the embeddings and the class centers are normalized, which projects them onto a unit hypersphere. The ArcFace loss then calculates the angles between the flow embeddings and all the class centers. A fixed angular margin $m$ is added to the angle corresponding to the correct class, which improves class separation (a larger angular gap between neighboring class centers). The cosine of these angles is then computed and scaled by a parameter $s$, producing the logits. These scaled logits are then passed to a cross-entropy loss function to calculate the final loss. By incorporating the angular margin, the ArcFace loss encourages the model to cluster embeddings of the same class closer together while increasing the angular separation between different classes, resulting in more discriminative embeddings.

\subsubsection{Sub-center ArcFace with dynamic margins}
We experimented with an enhanced variant of ArcFace called sub-center ArcFace~\cite{Deng2020Subcenter}, which uses $K$ sub-centers per class instead of a single center. During training, samples are pulled toward the nearest positive sub-center. This loss is better suited for handling intra-class variations, which are common in TC tasks---for instance, due to different API endpoints hosted behind a single SNI domain. Moreover, we used an ArcFace variant with dynamic margins, as introduced in~\cite{Ha2020GoogleLandmark} for tasks with extreme class imbalance. Each class uses a different angular margin, calculated as $m_C = a * N_C^{-\lambda_{margin}} + b$, where $N_C$ is the $C$ class frequency, $a$ and $b$ define the minimum and maximum margins, and $\lambda_{margin} > 0$ controls the rate of change in the margin. We used $\lambda = \frac{1}{4}$, with $a$ and $b$ set to produce margins in the range [0.15, 0.25]. Less frequent classes, which require greater separation in the embedding space for accurate classification, are given larger margins (up to 0.25) to widen their decision boundaries, whereas more frequent classes are assigned smaller margins (down to 0.15).

\begin{table}[t]
\scriptsize
\centering
\setlength\extrarowheight{0.75pt}
\caption{An overview of the hyperparameter space.}
\label{tab:hyperparameters}
\begin{tabular}{c|m{6cm}}
    \multirow{3}{*}{\rotatebox[origin=c]{90}{DL arch.}} & Stem: size of embeddings (packet sizes, IPT), IPT bins edges, directions encoding, PLE initialization \\ \cline{2-2}
    & Convolutions: block architecture, number of blocks, per-block channels, strides, dropout rates \\ \cline{2-2}
    & Pooling operation, activation and normalization functions \\ \hline
    \multirow{4}{*}{\rotatebox[origin=c]{90}{Training loop}} & Number of epochs, number of samples per epoch, semi-balanced sampling ($\lambda_{sampler}$) \\ \cline{2-2} % [-0.5cm]
    & ArcFace loss function: scale, margins, $K$ subcenters, $\lambda_{margin}$ for dynamic margins \\ \cline{2-2}
    & Initial LR, AdamW optimizer params, LR scheduling, number of warm-up iterations, weight decay, weight initialization \\ \cline{2-2}
    & KoLeo regularization strength  \\ \hline 
    \multirow{2}{*}{\rotatebox[origin=c]{90}{DB}} & Database size, semi-balanced database sampling ($\lambda_{db}$) \\ \cline{2-2}
    & Embedding size \\   
\end{tabular}
\end{table}

\subsubsection{Optimizer, LR scheduler, regularization, and implementation details}
\label{sec:optimizer}
The training loop was implemented in PyTorch. We used the AdamW optimizer with the default parameters, a batch size of 1024, and an initial learning rate of 0.0025. We used cosine learning rate decay, with a linear warm-up phase (from $\frac{0.0025}{3}$ to $0.0025$) for the first 150 iterations. Weight decay of $0.0017 (\approx 1^{-2.75})$ was applied on all parameters except biases, BatchNorm affine parameters, packet size and IPT embedding matrices, and the GeM pooling $p$ parameter. All weights used PyTorch’s default initialization, except for biases, which were set to zeros. We also use the KoLeo~\cite{sablayrolles2019spreadingvectorssimilaritysearch} regularization technique, which promotes a more uniform distribution of flow embeddings in the embedding space.

\subsection{Hyperparameter search}
The hyperparameter search was conducted on one specific domain split (described in~\secref{sec:domain-preprocess}), and the best-found parameters were reused for other domain splits. The objective was to find a configuration with the best macro-average recall on the validation domain set. Due to the large hyperparameter space, a traditional grid search over all possible combinations was infeasible. Instead, we adopted a sequential approach, optimizing and fixing subsets of hyperparameters step by step. The most important hyperparameters are summarized in~\tabref{tab:hyperparameters}. We used the MetaCentrum grid computing service\footnote{\url{https://www.metacentrum.cz/en/index.html}.} to run more than 4000 trials, each lasting about two hours.

\section{Domain Recognition Results}
\label{sec:domain-recognition-results}
This section presents experimental results for the domain recognition task. First, we describe performance metrics, introduce a simple baseline approach, and present the results. We conclude this section with ablations analyzing the influence of various components and parameters in the experimental setup.

\subsection{Metrics}
\label{sec:metrics}
During validation and final testing, we perform database ranking to find the neighborhood of all query samples, as described in~\secref{sec:ranking}. The ranking relies on cosine similarity in the embedding space, where higher cosine similarity indicates more similar and close samples. To process a neighborhood into a domain prediction, we use three simple voting schemes: selecting the domain of the closest sample (top-1) or taking the majority domain among the three or five closest samples (maj-3, maj-5). In the case of ties (e.g., when all closest samples have different domains), the predicted domain is determined by the order of the neighboring samples. This approach is equivalent to using a k-NN classifier, where $k$ corresponds to the size of the neighborhood.

For each voting scheme, we compute classification accuracy and macro-average recall. Among these metrics, we consider macro-average recall to be more important because it reflects overall performance across all domains, regardless of their frequencies. To better understand performance differences between frequent and infrequent domains, we also calculate macro-average recall for quartiles of domains sorted by frequency. For example, Q1 recall represents the macro-average recall for the top 25\% most frequent domains, whereas Q4 recall corresponds to the bottom 25\% least frequent domains.

\subsection{Baseline definition}
\label{sec:input-space-baseline}
We consider it a good practice to compare deep learning models against simple baseline methods to better understand the contribution of the more complex solution. For this purpose, we devised an input-space baseline method that uses raw packet sequences as embeddings. The experimental setup for this baseline is identical to that of the $\Phi$ embedding function, but it represents flows using the first 10 packet sizes, directions, and scaled IPTs. Since IPTs are considered less informative than packet sizes, they are clipped to a maximum of one second and scaled by a factor of $\frac{1}{10}$ to reduce their relative importance in the input-space "embedding". The baseline employs the L1 (Manhattan) metric for ranking, and we evaluated it using the top-1 voting scheme.

\subsection{Classification performance}
The final results for the domain recognition task were obtained as averages of 10 domain splits. Each domain split randomly divides the top 2000 domains into 1000 training domains, 500 validation domains, and 500 test domains. Moreover, for each domain split, we perform 10 repetitions, resulting in a total of 100 runs per reported value. Averaging over multiple domain splits ensures that the results are not biased for one specific set of domains. Results are presented in~\tabref{tab:main-results}, followed by a detailed discussion.

\begin{table}[!ht]
    \begin{threeparttable}
    \centering
    \caption{Domain recognition results.}
    \setlength\extrarowheight{1.5pt}
    \label{tab:main-results}
    \begin{tabular}{|l|l|l|l|l|l|l|l|}
    \hline
       \multicolumn{2}{|c|}{\textbf{Method}} & \textbf{Accuracy} & \textbf{Recall} & \textbf{R-Q1} & \textbf{R-Q2} & \textbf{R-Q3} & \textbf{R-Q4} \\ \hline
        \multirow{3}{*}{\rotatebox[origin=c]{90}{Emb $\Phi$}} & Top-1 & 94.83 & 79.35 & 89.63 & 81.36 & 75.99 & 70.41 \\ \cline{2-8} 
        & Maj-3 & 95.3 & 79.01 & 90.15 & 81.3 & 75.35 & 69.26 \\ \cline{2-8}
        & Maj-5 & 95.54 & 78.37 & 90.44 & 80.94 & 74.38 & 67.73 \\ \hline
        \multicolumn{2}{|c|}{Baseline} & 71.44 & 37.86 & 51.91 & 38.95 & 32.77 & 27.81 \\ \hline
    \end{tabular}
    \begin{tablenotes}
    \footnotesize
    \item Each value is an average over 100 runs (10 domain splits $\times$ 10 repetitions), with the exception of the input-space baseline that average over 10 runs (10 domains splits $\times$ 1 repetition).
    \end{tablenotes}
    \end{threeparttable}
\end{table}

\subsubsection{Top-1 performance} 
The achieved top1-acc of 94.83\% and recall\footnote{All mentions of recall refer to top-1 recall, unless stated otherwise.} of 79.35\% are both surprising, and we consider them a success. This is remarkable given the challenging nature of our setup: the embedding function is evaluated on a set of domains disjoint from those used for training, and the task involves a large number of fine-grained classes. 

Measurements of recall across domain quartiles reveal interesting trends. For the most frequent Q1 domains, recall reaches 89.63\%. Between Q1 and Q2, there is a notable recall drop of around 8\%, and the subsequent gaps Q2 $\to$ Q3 and Q3 $\to$ Q4 are around 5\% each. It is evident that less frequent classes are much harder to recognize, a phenomenon that is well-known and understandable for a wide range of ML tasks. 

\subsubsection{Benefits of using a neighborhood with maj-3 and \mbox{maj-5}}
Considering a larger neighborhood of three or five samples introduces a trade-off between prioritizing Q1 domains and the rest. The Q1 recall improves for both maj-3 (90.15\%) and maj-5 (90.44\%) compared to top-1 (89.63\%). However, the Q2--Q4 recalls for both maj-3 and maj-5 are lower than for top-1. This decrease can be attributed to sparse embeddings of less frequent domains, which have fewer database samples and, therefore, lack sufficient representation to "win" in the maj-3 and maj-5 voting schemes. Overall, using a larger neighborhood proves advantageous for the top 25\% most frequent domains, while top-1 works better for the rest. The improved performance on the most frequent domains also explains that the classification accuracies of maj-3 (95.3\%) and maj-5 (95.54\%) exceed that of top-1 (94.83\%), as frequent classes have a significant impact on micro-averaged metrics.

\subsubsection{Baseline performance}
The input-space baseline was evaluated using the same experimental setup as the proposed $\Phi$ embedding function, enabling a direct performance comparison. The results show a significant 23.39\% improvement in top1-acc of the proposed $\Phi$ embedding function over the baseline and even bigger improvements in Q1--Q4 recalls. This demonstrates that the baseline method is inadequate for addressing the domain recognition task within the given setup. However, our transfer learning experiments revealed that the input-space baseline can match SOTA performance on other TC datasets, see~\ref{sec:input-space-transfer-results} for more results and baseline-related discussion.

\subsection{Ablations}
The purpose of ablation studies is to investigate a system's performance by removing or modifying certain components to gain a better understanding of their contributions to the overall system.
The following sections examine the role of certain hyperparameters and their effects on classification performance and ranking speed. All ablation experiments were conducted using one specific domain split (identical to that used in the hyperparameter search), and we report average results from 10 runs per configuration, if not stated otherwise.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/sampler-lambda.pdf}
    \caption{The impact of the $\lambda_{sampler}$ balancing parameter of training sampler. Both vertical axes of top1-acc and recall have a range of 2\% to make the shapes of the lines comparable.}
    \label{fig:sampler-lambda}
\end{figure}

\subsubsection{Packet features - direct scalar values, PLE encoding, or learnable Embedding layer with the PLE initialization}
\label{sec:ple}
We evaluated the impact of our packet embedding scheme, referred to as Emb+PLE and detailed in~\secref{sec:stem}, with the results in \tabref{tab:packet-embedding}. Related works typically use direct scalar values (with or without standardization), as did our previous architectures. In terms of recall, our embedding scheme (79.29\%) shows a 3.06\% improvement compared to the scalar approach (76.23\%). The benefits are most prominent for less frequent domains, with a Q4 recall improvement of 4.57\%. We also tested a variant denoted as PLE, which uses the initial embeddings created with PLE encoding without further optimization (i.e., the weights of the \textit{Embedding} layers are frozen). The results indicate that PLE encoding accounts for most of the benefits of packet embeddings, while the trainable \textit{Embedding} layer adds a smaller, incremental improvement. The gains in top1-acc are more modest, with Emb+PLE providing a 0.4\% improvement over scalars.

\subsubsection{Training sampler - the impact of the balancing parameter}
\label{sec:ablation-training-sampler}
In each training epoch, one million network flows are sampled using a semi-balanced random sampler, where the $\lambda_{sampler}$ parameter controls the balancing strength (see details in~\secref{sec:training-sampler}). When $\lambda_{sampler} = 0$, all samples are assigned equal weight, and the original imbalance is preserved (no balancing). When $\lambda_{sampler} = 1$, the sampler creates a per-domain distribution that is as uniform as possible (perfectly uniform distribution is not possible as we use sampling without replacement).

Our expectation was that changing $\lambda_{sampler}$ would provide a trade-off between focusing on more frequent with $\lambda_{sampler} = 0$ (higher top1-acc) or less frequent domains with $\lambda_{sampler} = 1$ (higher recall). However, it turned out that some degree of balancing has a positive impact even for the top1-acc metric. We ended up choosing $\lambda_{sampler} = \frac{1}{2}$, meaning the final weight formula is $\frac{1}{\sqrt{N_C}}$. Compared to a standard random sampler without balancing (which corresponds to $\lambda_{sampler} = 0$), this brings a 1.29\% improvement in recall and a minuscule improvement of 0.13\% in top1-acc. The impact on both metrics is showcased in~\figref{fig:sampler-lambda}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/db-lambda.pdf}
    \caption{The impact of the $\lambda_{db}$ balancing parameter. Both vertical axes of top1-acc and recall have a range 10\% to make the shapes of the lines comparable.}
    \label{fig:db-lambda}
\end{figure}

\begin{table}[!t]
    \centering
    \footnotesize
    \setlength\extrarowheight{1pt}
    \caption{Comparison of approaches for encoding packet features.}
    \label{tab:packet-embedding}
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
        \textbf{Method} & \textbf{Top1-Acc} & \textbf{Recall} & \textbf{R-Q1} & \textbf{R-Q2} & \textbf{R-Q3} & \textbf{R-Q4} \\ \hline
        Emb+PLE & 95.75 & 79.29 & 90.38 & 79.59 & 74 & 73.18 \\ \hline
        PLE  & 95.67 & 78.49 & 90.19 & 78.64 & 73.03 & 72.11 \\ \hline
        Scalar & 95.35 & 76.23 & 89.46 & 76.89 & 69.95 & 68.61 \\ \hline
    \end{tabular}
\end{table}

\subsubsection{Database - the impact of the balancing parameter}
\label{sec:ablation-db-sampler}
As described in~\secref{sec:database-preparation}, we also perform semi-balanced sampling for building the database. The weight formula is the same as for epoch training sampler. When $\lambda_{db} = 0$, the original imbalance is preserved in the database. When $\lambda_{db} = 1$, the database has the per-domain distribution as uniform as possible (perfectly uniform distribution is not possible as we use sampling without replacement). The graph investigating the impact of the $\lambda_{db}$ parameter in~\figref{fig:db-lambda} shows a clear trade-off between decreasing top1-acc and increasing recall when $\lambda_{db}$ moves from 0 to 1. We ended up choosing $\lambda_{db} = \frac{1}{2}$, which, compared to no balancing, brings a 7\% improvement in recall at the expanse of a 0.51\% decrease in top1-acc. We believe this trade-off is worthwhile in most scenarios.

\paragraph*{Combined effect}
We also examined the combined effect of $\lambda_{sampler}$ and $\lambda_{db}$. The results, presented in~\tabref{tab:db-sampler-lambdas}, highlight the performance gains compared to the case where neither the database nor the training set is balanced ($\lambda_{sampler} = 0 $ and $\lambda_{db} = 0$).\footnote{\figref{fig:sampler-lambda} shows the change between third and fourth rows of~\tabref{tab:db-sampler-lambdas} with $\lambda_{db} = \frac{1}{2}$ fixed, while \figref{fig:db-lambda} shows the change between second and fourth rows with $\lambda_{sampler} = \frac{1}{2}$ fixed.} As expected, balancing has the greatest impact on Q4 recall, for which it provides a remarkable 13.47\% improvement. The gains for Q3 (+11.04\%) and Q4 (+8.39\%) recalls are also impressive, especially given that the "cost" is merely a 0.4\% decrease in top1-acc. Furthermore, the results indicate that database balancing and training set balancing operate independently, as their combined effect is approximately the sum of their individual contributions.

\begin{table*}[t]
    \centering
    \setlength\extrarowheight{1pt}
    \caption{The combined effect of both database balancing  $\lambda_{db}$ and $\lambda_{sampler}$.}
    \label{tab:db-sampler-lambdas}
    \begin{tabular}{|cc|l|l|l|l|l|l|}
        \hline
         $\lambda_{db}$ & $\lambda_{sampler}$ & \textbf{Top1-Acc} & \textbf{Recall} & \textbf{R-Q1} & \textbf{R-Q2} & \textbf{R-Q3} & \textbf{R-Q4} \\ \hline
        0.0 & 0.0 & 96.15                      & 70.6 & 88.52 & 71.2 & 62.96 & 59.71                              \\ \hline
        0.0 & 0.5 & 96.26 \scriptsize{(+0.11)} & 72.29 \scriptsize{(+1.69)} & 88.95 \scriptsize{(+0.43)} & 72.70 \scriptsize{(+1.5)} & 64.75 \scriptsize{(+1.79)} & 62.74 \scriptsize{(+3.03)} \\ \hline
        0.5 & 0.0 & 95.62 \scriptsize{(-0.53)} & 78.00 \scriptsize{(+7.4)} & 90.06 \scriptsize{(+1.54)} & 78.42 \scriptsize{(+7.22)} & 72.57 \scriptsize{(+9.61)} & 70.95 \scriptsize{(+11.24)} \\ \hline
        0.5 & 0.5 & 95.75 \scriptsize{(-0.4)}  & 79.29 \scriptsize{(+8.69)} & 90.38 \scriptsize{(+1.86)} & 79.59 \scriptsize{(+8.39)} & 74.00 \scriptsize{(+11.04)} & 73.18 \scriptsize{(+13.47)} \\ \hline
    \end{tabular}
\end{table*}

\subsubsection{Database - how does the number of unique domains affect performance?}
This section examines the sensitivity of the domain recognition approach to the number of unique domains we want to recognize. Previous work~\cite{Yang2021} demonstrated that TC tasks often become trivial when the number of classes is small. To explore this, we tested our approach with the number of unique domains ranging from 100 to 1000.

To obtain up to 1000 domains, we first merge the validation and test domain sets.\footnote{We acknowledge that reusing validation domains for testing deviates from our defined evaluation protocol; however, we did so only in this experiment to demonstrate performance across a wider range of classes.} Then, we randomly select the desired number of domains, choose all samples of those domains, and split them into query and database parts. A database is created using the $\lambda_{db}$ semi-balanced sampling. We measure the performance of a trained model, which is reused for all domain counts and repetitions. For each domain count, we repeat this procedure 50 times and report the average. For the maximum of 1000 domains, we use all available validation and test domains. Additionally, we compare the described random domain sampling with a sorted approach, where we select the $N$ (100, 200, \dots, 1000) most frequent domains in each repetition. The results are presented in~\figref{fig:db-classes}. The top1-acc for random domain sampling ranges from 98.52\% for 100 domains to 93.22\% for 1000 domains, while the recall ranges from 88.65\% to 74.54\%. In contrast, the recall for the top 100 most frequent domains is as high as 93.04\%, which is notable considering these 100 domains cover 85\% of all dataset samples.

An interesting difference is observed between the domain selection methods: top1-acc is higher on random subsets of domains, while recall is better when using the top $N$ domains. This is because when domains are sampled, some less frequent and harder-to-recognize domains are included. Since recall is macro-average, these harder domains have a larger impact on the overall metric. In contrast, for micro-averaged top1-acc, the inclusion of less frequent domains has minimal effect on the overall metric. Moreover, top1-acc is higher with sampled domains because there are fewer misclassifications in the region of the most frequent domains. Among these domains, there are a lot of similar ones that are prone to mismatch, and thus, top1-acc is increased when some of those are not selected in the given repetition.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/db-classes.pdf}
    \caption{Sensitivity of the proposed domain recognition approach to the number of unique domain.}
    \label{fig:db-classes}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\columnwidth]{figures/embedding-size-vs-speed.pdf}
    \caption{The impact of the flow embedding size on domain recognition accuracy and ranking speed.}
    \label{fig:embedding-size-ranking-speed}
\end{figure}

\subsubsection{Embedding size - ranking speed vs recall trade-off}
\label{sec:ablation-ranking-speed}
The flow embedding size is an key parameter that influences the performance of the proposed domain recognition approach. Larger embeddings improve recognition accuracy but come at the cost of slower ranking speeds. To explore this trade-off, we ran experiments with embedding sizes ranging from 32 to 448. As with other ablations, we performed 10 repetitions per embedding size and report the average metrics. The results, summarized in~\figref{fig:embedding-size-ranking-speed}, show that reducing the embedding size has a limited impact on top1-acc (94.88\% for size 32, 95.79\% for size 448). However, recall is more sensitive, decreasing from 79.46\% (size 448) to 75.43\% (size 32).

We observed a clear inverse relationship between ranking speed and embedding size: the smallest embeddings achieved speeds of around 16.5k flows/s, while the largest slowed the ranking to 4k flows/s. In contrast, the speed of creating the embeddings with the neural network remained stable at around 33k flows/s, regardless of the used embedding size. Both tasks---creating embeddings and faiss\footnote{The \textit{faiss} library, which we use for database ranking, supports GPU indexes that offer a 5$\times$ - 10$\times$ performance boost compared to CPU implementations. \url{https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU}.} ranking---were performed on an Nvidia Tesla T4 16GB GPU. Further performance-related discussion is provided in the final chapter.

\begin{table}[t!]
    \footnotesize
    \centering
    \setlength\extrarowheight{0.75pt}
    \caption{KoLeo regularization impact.}
    \label{tab:koleo}
    \begin{tabular}{|l|l|l|l|l|l|}
    \hline
        \textbf{KoLeo} & \textbf{Best epoch} & \textbf{Top1-Acc} & \textbf{Recall}\\ \hline
        Yes ($\lambda = 1$) & 24.3 & 95.75 & 79.29  \\ \hline
        No ($\lambda = 0$) & 22.6 & 95.68 & 78.97 \\ \hline
    \end{tabular}
\end{table}

\subsubsection{KoLeo regularization}
We investigated the impact of KoLeo regularization, which uses a parameter $\lambda$ to control its strength. Although KoLeo was originally designed~\cite{sablayrolles2019spreadingvectorssimilaritysearch} to improve embedding discretization---a step we do not perform---we observed that without this regularization, training diverges before completing the 30 training epochs. \tabref{tab:koleo} compares the results with ($\lambda=1$) and without ($\lambda=0$) KoLeo. When KoLeo was used, the best validation performance was achieved later in training, indicating the neural network continued improving over more epochs. The later peak in validation performance suggests that KoLeo contributes to a more stable training process and enhances resistance to overfitting. However, the performance gains are modest: a 0.32\% improvement in recall and a 0.07\% in top1-acc. 

\section{Model transfer on other traffic classification datasets}
\label{sec:transfer-learning}
Our ambition was to develop an embedding function that is as universal across TC tasks as possible. To assess how well the proposed $\Phi$ embedding function generalizes, we evaluated it on five additional TC datasets and compared its performance to published SOTA results. The following sections describe the overall transfer setup, provide an overview of the datasets, and present the results of this cross-dataset experiment.

\subsection{Transfer setup}
\label{sec:trainsfer-setup}
We evaluated our $\Phi$ embedding function on five other datasets (UCDAVIS19~\cite{dataset_UCDAVIS19}, UTMOBILENET21~\cite{dataset_UTMOBILENET21}, MIRAGE19~\cite{dataset_MIRAGE19}, MIRAGE22~\cite{dataset_MIRAGE22}, CESNET-TLS22~\cite{Luxemburk2023TLS}), each corresponding to a different TC task. To streamline dataset handling, we used the \textit{tcbench}\footnote{\url{https://github.com/tcbenchstack/tcbench}.} framework that provides all these datasets except CESNET-TLS22. The datasets are cleaned and pre-filtered when needed, and five train/validation/test splits are prepared for each dataset. Overall, \textit{tcbench} facilitates cross-dataset evaluation and saved us a great amount of time needed for model transfer experiments. For getting the CESNET-TLS22 dataset, we used the CESNET DataZoo toolset~\cite{Luxemburk2023Datazoo}.

Since our classification approach relies on nearest neighbors in the embedding space, we did not perform fine-tuning in the traditional sense (e.g., running a few training epochs on new data with most of the model weights frozen). Rather, we created embeddings of an entire training set using the $\Phi$ embedding function, trained a k-NN classifier, and used it to predict test samples. Predictions were based on the label of the closest training sample (i.e., 1-NN, no majority voting). Also, we did not use the $\lambda_{db}$ semi-balancing technique.

We compare our results with SOTA reported for each dataset. We initially aimed to make this comparison using a single metric: classification accuracy. However, for the UTMOBILENET21, MIRAGE19, and MIRAGE22 datasets, SOTA papers report only the weighted F1-score metric. Therefore, we must use weighted F1-score for these datasets to ensure comparability. We also measured the performance of the input-space baseline, which was defined in~\secref{sec:input-space-baseline}. The motivation was to evaluate how well individual TC tasks can be addressed using this baseline and to establish a reference point for comparison with the $\Phi$ embedding function.

To select the best model (one specific set of trained weights) for transfer to another TC dataset, we chose the one with the highest sum of validation and test recalls on the domain recognition task, considering all training runs with the final hyperparameter configuration. We made a single modification to this model, targeting the packet size embedding technique implemented in the model stem.\footnote{In the model stem, the \textit{Embedding} layer generates a learned vector representation for each packet size. However, some packet sizes (e.g., 1453--1471, 1473--1500) are never observed during training, leaving their representations untrained. To address this, we assign them the representation of the nearest observed packet size, except for packet sizes 1--19, which are given the representation of packet size zero. This adjustment provides a modest performance improvement of around 0.14\% on three of the five datasets.}  We published this model and its pretrained weights in the CESNET Models framework~\cite{Luxemburk2024_CesnetModels} under the name \texttt{30pktTCNET\_256}. The architecture code is available on GitHub.\footnote{\url{https://github.com/CESNET/cesnet-models/blob/main/cesnet_models/architectures/multimodal_cesnet_enhanced.py}.} Thanks to the model's publication and the available open-source tools providing the datasets, the entire model transfer process presented in this section is reproducible. To this end, we also created and published\footnote{\url{https://github.com/CESNET/cesnet-tcexamples/blob/main/notebooks/cross_dataset_evaluation.ipynb}.} a Jupyter notebook that replicates the results shown in~\tabref{tab:cross-eval-results}.

\subsection{Datasets}
This section provides a brief overview of each dataset we used in the model transfer experiment.

\subsubsection*{UCDAVIS19}
A small dataset published in 2019~\cite{dataset_UCDAVIS19} containing QUIC traffic of five Google services: Google Drive, Google Docs, Google Search, Google Music, and  YouTube. It includes a \textit{pretraining} partition with 6.5k samples and two test sets---\textit{human} (83 samples) and \textit{script} (150 samples). We are afraid that the minuscule size of the test sets makes this dataset less representative. We opted not to use the prepared \textit{tcbench} splits for this dataset, as those contain class-balanced subsets of the \textit{pretraining} partition that are too small (100 samples per class). Instead, we used the entire \textit{pretraining} partition to train a k-NN classifier and evaluated the two test sets without averaging across training splits (as none exist). For a SOTA comparison, we use the best results\footnotemark{} from Finamore et al.~\cite{finamore2023replication}.

\subsubsection*{UTMOBILENET21}
A mobile traffic dataset containing 17 Android applications, with user interactions emulated through the Android API. Heng et al.~\cite{dataset_UTMOBILENET21} published this dataset in 2021, providing packet information in CSV format. The authors of \textit{tcbench} cleaned the data, assembled flows, filtered flows with less than 10 packets (9.5k samples remained), and prepared five 80/10/10 train/validation/test splits. In this work, we use these splits as follows: the training set is used to train a k-NN classifier, the validation set is left unused, and performance is measured on the test set. We report the average performance across all splits (the same approach is used for the MIRAGE datasets). For a SOTA comparison, we use the best results\footnotemark[\value{footnote}] from Finamore et al.~\cite{finamore2023replication}, which is the paper that introduced the \textit{tcbench} framework, and thus we can be sure that the data are identical and results are suitable for direct comparison.

\subsubsection*{MIRAGE19}
A well-known mobile traffic dataset from the MIRAGE dataset series. The traffic of this dataset is based on real users interactions with 20 Android applications. A private version that contains 40 applications exists but is not part of \textit{tcbench}. Aceto et al.~\cite{dataset_MIRAGE19} published this dataset in 2019, providing JSON files containing traffic capture experiments. The authors of \textit{tcbench} processed the JSON files, removed background traffic, removed ACKs from packet sequences, and discarded flows with fewer than 10 packets. This curation resulted in 64k samples, which were then used to create five 80/10/10 train/validation/test splits. We utilized the splits in the same manner as described for UTMOBILENET21. For a SOTA comparison, we use the best results\footnotemark[\value{footnote}] from Wang et al.~\cite{wang2024augmentation}. Even though Wang et al.~\cite{wang2024augmentation} do not explicitly mention using \textit{tcbench} data, their dataset curation, preprocessing steps, and splits are identical to those of \textit{tcbench}. To be sure, we asked the authors of both~\cite{wang2024augmentation} and \textit{tcbench}, and they confirmed it. Therefore, the results from this paper are suitable for direct comparison with our measurements.

\subsubsection*{MIRAGE22}
This mobile traffic dataset focuses on video meeting applications such as Zoom, Webex, or Teams. Guarino et al.~\cite{dataset_MIRAGE22} introduced this dataset in 2022, featuring traffic from nine Android applications in total. As the name suggests, this dataset is from the same research group as MIRAGE19. The \textit{tcbench} curation process is the same as for MIRAGE19, which is described in the previous section. We utilized the five prepared train/validation/test splits. For a SOTA comparison, we use the best results\footnotemark[\value{footnote}] from Wang et al.~\cite{wang2024augmentation}.

\begin{table*}[!ht]
    \centering
    \setlength\extrarowheight{1.25pt}
    \caption{Cross-dataset evaluation of the trained $\Phi$ embedding function.}
    \label{tab:cross-eval-results}
\begin{threeparttable}
\begin{NiceTabular}{|r|c|c|l|lr|lr|c|c|}
\hline
\multicolumn{3}{|c|}{\textbf{Dataset}} &
\multicolumn{5}{c|}{\textbf{Classification performance}}  &                                                 
\multicolumn{2}{c|}{\textbf{State of the Art }}  
  \\ \hline
%-----------------------------------------------------------
\textbf{Name} & 
\textbf{Classes} &
\textbf{Classification Task} &
\multicolumn{1}{c|}{\textbf{SOTA}} &
\multicolumn{2}{c|}{\textbf{Input Space}} &
\multicolumn{2}{c|}{\textbf{Embedding $\Phi$}} &
\textbf{Reference} &
\textbf{Year} \\ \hline
%-----------------------------------------------------------
UCDAVIS19-script&
5&
QUIC service& 
98.63 & 
\cellcolor[HTML]{FFE5E5} 98.00 & 
\cellcolor[HTML]{FFE5E5} \textit{($\Delta$\:$-$0.63)} & 
\cellcolor[HTML]{DDF2E8} 100.00 & 
\cellcolor[HTML]{DDF2E8} \textit{($\Delta$\,\,~~1.37)} & 
Finamore et al.~\cite{finamore2023replication} & 
2023 \\ \hline
%-----------------------------------------------------------
UCDAVIS19-human &
5&
QUIC service&
80.45 &
\cellcolor[HTML]{FFB8B8} 71.08 &
\cellcolor[HTML]{FFB8B8} \textit{($\Delta$\:$-$9.37)}  &
\cellcolor[HTML]{DDF2E8} 81.93 &
\cellcolor[HTML]{DDF2E8} \textit{($\Delta$\,\,~~1.48)} &
Finamore et al.~\cite{finamore2023replication} &
2023 \\ \hline
%-----------------------------------------------------------
UTMOBILENET21\tnote{$\dag$} &
17&
Mobile app&
81.91 & 
\cellcolor[HTML]{DDF2E8} 83.58 & 
\cellcolor[HTML]{DDF2E8} \textit{($\Delta$\,\,~~1.67)} & 
\cellcolor[HTML]{57BB8A} 86.65\tnote{$\star$} & 
\cellcolor[HTML]{57BB8A} \textit{($\Delta$\,\,~~4.74)} &  
Finamore et al.~\cite{finamore2023replication} & 
2023 \\ \hline
%-----------------------------------------------------------
MIRAGE19\tnote{$\dag$} &
20&
Mobile app&
80.06 &
\cellcolor[HTML]{FFFFFF} 79.98 &
\cellcolor[HTML]{FFFFFF} \textit{($\Delta$\:$-$0.08)}   &
\cellcolor[HTML]{57BB8A} 83.71\tnote{$\star$} &
\cellcolor[HTML]{57BB8A} \textit{($\Delta$\,\,~~3.65)} &
Wang et al.~\cite{wang2024augmentation} &
2024 \\ \hline
%-----------------------------------------------------------
MIRAGE22\tnote{$\dag$} &
9&
Mobile app& 
97.18 &
\cellcolor[HTML]{FFE5E5} 95.63 &
\cellcolor[HTML]{FFE5E5} \textit{($\Delta$\:$-$1.55)}   &
\cellcolor[HTML]{DDF2E8} 97.77\tnote{$\star$} &
\cellcolor[HTML]{DDF2E8} \textit{($\Delta$\,\,~~0.59)} &
Wang et al.~\cite{wang2024augmentation}        &
2024  \\ \hline
%-----------------------------------------------------------
CESNET-TLS22  &
191&
TLS service& 
97.20        &
\cellcolor[HTML]{FFB8B8} 90.96 &
\cellcolor[HTML]{FFB8B8} \textit{($\Delta$\:$-$6.24)}   &
\cellcolor[HTML]{FFE5E5} 95.24 &
\cellcolor[HTML]{FFE5E5} \textit{($\Delta$\:$-$1.96)} &
Fauvel et al.~\cite{fauvel2023lightweight}     &
2023 \\ \hline 
%-----------------------------------------------------------
CESNET-QUIC22 &
500&
QUIC SNI domain& 
&
~71.44 &
&
~94.83 &        
&       
This work & 
2024 \\ \hline
\end{NiceTabular}

\begin{tablenotes}
\leftskip=0.3cm
\item[$\dag$] SOTA results for these datasets, and consequently our measurements, are reported in weighted F1-score, as explained in~\secref{sec:trainsfer-setup}.
\item[$\star$] For completeness, we also provide the classification accuracy of $\Phi$: UTMOBILENET21: 86.68\%, MIRAGE19: 83.73\%, MIRAGE22: 97.77\%.\\The differences between the measured weighted F1-scores and accuracies are negligible; for MIRAGE22, the metrics are identical.
\end{tablenotes}
\end{threeparttable}
\end{table*}

\subsubsection*{CESNET-TLS22}
A large TLS traffic dataset from backbone lines of CESNET, which is the Czech national research and education network. The dataset spans two weeks of traffic, comprises 141 million flows, and categorizes traffic into 191 web service classes. Luxemburk et al.~\cite{Luxemburk2023TLS}, the dataset authors, used the first week for training and the second week for testing (we refer to this as a time-based train-test split). We followed the same approach for our experiments and ran five repetitions, each time sampling one million flows from the first week as the training set and one million flows from the second week as the test set. As with other datasets, we used the training set to train a k-NN and measured the classification performance on the test set. We report the average performance over the five repetitions.

Two related works are suitable for a SOTA comparison. Luxemburk et al.~\cite{Luxemburk2023TLS} reported 97.04\% accuracy on the entire, non-sampled dataset using a time-based train-test split. Fauvel et al.~\cite{fauvel2023lightweight} achieved 97.2\% with a lightweight and explainable convolutional network called LEXNet. They reported a dataset size of 38 million flows, suggesting some subsampling, and did not mention using the time-based train-test split. Given the differences between the evaluation setups of these works and our use of CESNET-TLS22, we selected the more favorable results\footnotemark[\value{footnote}] from Fauvel et al.~\cite{fauvel2023lightweight} for the SOTA comparison.

\footnotetext{SOTA comparison. For each dataset, we provide the best result, the evaluation metric used, and the table where it is reported.
\begin{itemize}[leftmargin=14pt]
    \item UCDAVIS19, accuracy --  Table 7 of Finamore et al.~\cite{finamore2023replication} containing results for an enlarged training set. The best result for \textit{human} is "SimCLR + fine-tuning" with 80.45\%, and for \textit{script}, it is "Packet loss" with 98.63\%.
    \item UTMOBILENET21, weighted F1-score -- Table 8 of Finamore et al.~\cite{finamore2023replication}. The best result for the \textgreater10pkts version is "Time shift" with 81.91\%.
    \item MIRAGE19, weighted F1-score -- Table 7 of Wang et al.~\cite{wang2024augmentation}. Deltas from this table need to be added to the baseline performance of 75.43\%. The best result is "MaskedStack (p = 0.7)" with 80.06\% (75.43\% + 4.63\%).
    \item MIRAGE22, weighted F1-score -- Table 7 of Wang et al.~\cite{wang2024augmentation}. Deltas from this table need to be added to the baseline performance of 94.92\%. The best result is "MaskedStack (p = 0.3)" with 97.18\% (94.92\% + 2.26\%). 
    \item CESNET-TLS22, accuracy -- Table 5 of Fauvel et al.~\cite{fauvel2023lightweight}. The best result of 97.2\% is achieved with the LEXNet architecture.
\end{itemize}}

\subsection{Results}
\label{sec:transfer-results}
Evaluating the transfer setup outlined in~\secref{sec:trainsfer-setup} on the five TC datasets described in the previous section, we achieved results summarized in~\tabref{tab:cross-eval-results}. For UTMOBILENET21, MIRAGE19, and MIRAGE22, weighted F1-score is reported, while classification accuracy is used for the other datasets. The results of SOTA, input-space baseline, and our $\Phi$ embedding function are compared, with deltas calculated relative to SOTA and color-coded to indicate performance changes. 

\subsubsection{Performance of $\Phi$ embedding function}
Our embedding function outperformed SOTA on four of the five datasets. Notably, it improved the weighted F1-score by 4.74\% on UTMOBILENET21 and 3.65\% on MIRAGE19, establishing new SOTA results for their \textit{tcbench} versions (curation and splits). On UCDAVIS19, it achieved almost a 1.5\% improvement across both tests, even reaching 100\% accuracy on the \textit{script} test set. A smaller gain of 0.59\% was observed on MIRAGE22. The only dataset where our approach did not achieve SOTA performance is CESNET-TLS22, likely due to its more challenging classification task, which has the highest number of classes among the datasets evaluated in the transfer setup, as well as its use of a time-based train-test split.

\subsubsection{Surprising performance of the input-space baseline}
\label{sec:input-space-transfer-results}
We initially expected both the $\Phi$ embedding function and the current SOTA to outperform the input-space baseline across all datasets. While the embedding function consistently surpassed the baseline, SOTA did not always do so. On UTMOBILENET21, the baseline outperformed SOTA by 1.67\%, while on MIRAGE19, their performances were nearly identical, differing by only 0.08\%. For the remaining datasets, the baseline lagged behind SOTA, with the largest gaps observed on CESNET-TLS22 (-6.24\%) and the \textit{human} test set of UCDAVIS19 (-9.37\%).

An intriguing finding is that the input-space baseline, despite being a simple k-NN classifier using features from the first 10 packets, performed close to SOTA on several datasets. We want to highlight that no dataset-specific modifications were made, such as adjusting the number of packets or the IPT scaling factor. If the baseline were tuned for each dataset, its performance would be even better. To the best of our knowledge, this phenomenon has not been reported in related works and remains unexplored in the TC research domain. We believe that the underlying cause is the high data redundancy of TC datasets. During dataset collection, it is highly probable---or almost certain for script-generated datasets---that multiple instances of the same network communication are captured. For example, these could be repeated API requests sent to the same server, all with identical TCP and TLS configurations. Consequently, TC datasets often contain numerous near-duplicate samples with the same label. When such a dataset is randomly split into training and test sets, these duplicates can end up in both sets. In this scenario, it is not surprising that a classifier relying on the closest training sample achieves high performance.

\section{Related work: Representation learning in traffic classification}
\label{sec:related-work}
We will discuss three influential papers on representational learning in TC, highlighting their similarities and differences compared to our work.

Guarino et al.~\cite{Guarino2023Many} embarked on the task of finding better representations for TC that remain valid across tasks, which is in line with the goals we set up for this work. The authors compared transfer-, meta-, and contrastive-learning approaches on MIRAGE19 and AppClassNet~\cite{Wang2022AppClassNet} datasets. Both datasets were partitioned into training, validation, and test sets with disjoint classes, similar to how we partition domain names for the domain recognition task. However, their partitioning approach was based on class frequencies: the training set contained the most frequent classes, while the test set included the least frequent ones. In contrast, we used random partitioning independent of class frequencies. Guarino et al. aimed to create natural classification tasks with a limited number of samples, as their overall focus was on the few-shot learning setup. On the other hand, we argue that it is worth exploring a broader range of scenarios, including cases where more frequent classes appear in the test set. This broader perspective is important when we consider the transfer of the $\Phi$ embedding function to other TC datasets, as real-world target datasets contain both less and more popular classes.

One of the findings of Guarino et al. is that supervised contrastive learning produces the best DL models and representations overall. Our work supports this finding, as there are notable similarities between the loss function used in our work, ArcFace, and the best-performing contrastive loss used by Guarino et al.:~SupCon~\cite{khosla2020supervised} extended with class embeddings and cosine distance, referred to as \texttt{SupCon(ClassEmb)}. Both ArcFace and SupCon aim to improve the discriminative power of embeddings. ArcFace does this in a classification setup by pulling samples to class centers and enforcing angular margins, while the original SupCon minimizes pairwise distances for similar and maximizes them for dissimilar samples. However, the modifications introduced in \texttt{SupCon(ClassEmb)} bring it closer to ArcFace, as both loss functions now utilize class centers (another name for class embeddings) and compute cosine distances towards them. Angular margins, which enhance class separation in the embedding space, remain the most significant distinction between these loss functions.

Finamore et al.~\cite{finamore2023replication} conducted a comprehensive evaluation of a flow representation called FlowPic, a 2D histogram capturing the evolution of packet sizes over time. This representation was studied in the context of contrastive learning, with a focus on data augmentation strategies. The authors replicated an earlier paper~\cite{Horowicz2022flowpic}, reproducing most of the original results while also incorporating three additional datasets. For our work, the most important contribution is the release of the \textit{tcbench} open-source framework, which we utilized for the cross-dataset evaluation. Also, the best classification accuracies achieved on UCDAVIS19 and UTMOBILENET21 datasets were used for the SOTA comparison, as presented in~\tabref{tab:cross-eval-results}.

Wang et al.~\cite{wang2024augmentation} presented a benchmark of data augmentations for TC, evaluating 18 of them on three datasets: MIRAGE19, MIRAGE22, and a private one. They utilized the standard sequential representation of packet metadata, consisting of the sizes, times, and directions of the first 20 packets. The training pipeline featured two stages: a contrastive self-supervised phase, where augmented versions of the same sample were pulled together, followed by a supervised phase, where a classification head was trained on the extracted features. Wang et al. also experimented with a class-weighted sampler to achieve perfect class balancing in each training epoch but found no success with it. In contrast, our semi-balancing technique, detailed in~\secref{sec:training-sampler}, proved to be beneficial in our experiments. The differing outcomes may be due to the use of perfect class balancing by Wang et al., whereas we employed the $\lambda_{sampler}$ parameter to control the strength of the balancing effect. The best classification accuracies achieved on MIRAGE19 and MIRAGE22 datasets were used for the SOTA comparison, as presented in~\tabref{tab:cross-eval-results}.

\section{Conclusion}
\label{sec:conclusion}
The main objective of this work was to design a universal embedding function suitable for a wide range of TC tasks. We first developed the embedding function for the exact domain recognition task on the CESNET-QUIC22 dataset and then evaluated how well it generalizes on other TC datasets. To summarize the core components of the proposed $\Phi$ embedding functions: (1) a CNN-based feature extractor that embeds packet features before processing with ResNet-like blocks, (2) the ArcFace loss, which enhances class separation by pulling samples toward class centers while enforcing angular margins, and (3) a nearest neighbors classifier using cosine distance in the embedding space.

The domain recognition task is a significant challenge that is relevant for monitoring networks with a lot of TLS traffic utilizing the ECH extension, which encrypts entire ClientHello messages and conceals SNI domains from network operators. We tackled this task in a setup where domain names were disjoint across training, validation, and test sets, which forced the embedding function to learn traffic patterns that generalize to unseen domains. After an extensive hyperparameter search and tuning of our training pipeline and model architecture, we achieved a classification accuracy of 94.83\% and a recall of 79.35\%, which we consider a strong outcome considering the difficulty of the disjoint-class setup. We also conducted six ablations, each focusing on a specific component to assess its contribution to the overall solution. Notably, the combination of training and database semi-balancing samplers, along with the PLE initialization method for packet size and IPT \textit{Embedding} layers, proved crucial for achieving high recall.

We then transferred the $\Phi$ embedding function to five TC datasets: UCDAVIS19, UTMOBILENET21, MIRAGE19, MIRAGE22, and CESNET-TLS22. Four of these datasets were accessed through the \textit{tcbench} framework, which provides curated and pre-split datasets to facilitate proper benchmarking. The $\Phi$ embedding function proved to be highly successful, beating SOTA performance on four of the five datasets. The improvements over SOTA were approximately 4\% on UTMOBILENET21 and MIRAGE19, 1.5\% on UCDAVIS19, and 0.5\% on MIRAGE22. CESNET-TLS22 was the only dataset where our approach did not match SOTA, falling short by 2\%. Overall, the embedding function demonstrated strong generalization across all tested tasks. To our knowledge, no similar transfer learning achievements have been reported in the TC domain. We conclude that the domain recognition task, on which we developed and trained the embedding function, is well suited for the pretraining of TC models due to its complexity, large number of classes, and straightforward labeling process.

\paragraph*{Computational performance of nearest neighbors search}
We want to address potential performance concerns related to our use of nearest neighbors search for the domain recognition task. In~\secref{sec:ablation-ranking-speed}, we measured the ranking speed across different embedding sizes. For instance, with an embedding size of 128, the ranking of a database containing one million samples achieves a speed of 10k samples per second. This speed is made possible thanks to \textit{faiss} that provides efficient methods for finding nearest neighbors and can even run on GPUs for faster speeds. In this work, we opted to use the \texttt{IndexFlatIP} index that provides exact ranking results. However, \textit{faiss} also offers other indexes that provide a trade-off between the ranking speed and the precision of nearest neighbors search. Thus, if higher ranking speeds were needed, a natural solution would be to use an index with faster ranking, such as \texttt{IndexIVFFlat}\footnote{\url{https://github.com/facebookresearch/faiss/wiki/Faiss-indexes}.}, at the cost of losing the guarantee of exact and exhaustive results. Another area for potential performance improvements is the database creation process. In this work, we used semi-balanced sampling to select one million samples for the database. We believe that a more informative approach for selecting the database samples, for example based on clustering, can decrease the database size while maintaining domain recognition performance and speeding up the nearest neighbors search.\footnote{We deliberately skipped a discussion about the speed of creating embeddings, which is around 33k samples per second in our case. This speed depends on factors like the model size (the number of parameters) and the used GPU. Our solution does not differ from related works in this regard.}

\paragraph*{Future research on the input-space baseline needed}
Our motivation for designing a simple baseline and evaluating it under the same conditions as the $\Phi$ embedding function was to establish a reference point. The results, however, turned out to be far more intriguing than anticipated. In the cross-dataset evaluation, the baseline achieved performance quite close to SOTA, even surpassing it for one dataset. Our hypothesis is that the data redundancy inherent in TC datasets, when combined with random splitting into training, validation, and test subsets, makes classification tasks trivial for nearest neighbors search. If validated in future research, this finding can have significant implications for how researchers should approach the creation and splitting of TC datasets.

\paragraph*{Ranking output potential}
We conclude this paper with an outline of future directions and a discussion of the advantages of leveraging the neighborhood of a network flow---the closest and most similar samples observed in the past. Ranking output in the form of the N closest samples is flexible and allows various approaches for processing into final predictions. In addition to the straightforward methods demonstrated in this work, the top-1 and maj-5 voting schemes, we can also utilize server IP addresses or AS numbers for additional post-filtering of the neighborhood. We see this as a reasonable direction for integrating IP-related information with traffic shape characteristics, and we plan to explore it in future works. The ranking output also includes distances that can be leveraged for out-of-distribution detection. When all closest samples exceed a certain distance threshold, the prediction can be rejected or adjusted---for example, by using the most common second-level domain in the neighborhood instead of the full domain prediction. Furthermore, predictions based on the most similar samples are more interpretable thanks to the provided examples, even though the embeddings used for computing similarities are still produced with a black-box neural network.

\bibliographystyle{IEEEtran}
\bibliography{references.bib}

\end{document}