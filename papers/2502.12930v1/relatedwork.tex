\section{Related work: Representation learning in traffic classification}
\label{sec:related-work}
We will discuss three influential papers on representational learning in TC, highlighting their similarities and differences compared to our work.

Guarino et al.~\cite{Guarino2023Many} embarked on the task of finding better representations for TC that remain valid across tasks, which is in line with the goals we set up for this work. The authors compared transfer-, meta-, and contrastive-learning approaches on MIRAGE19 and AppClassNet~\cite{Wang2022AppClassNet} datasets. Both datasets were partitioned into training, validation, and test sets with disjoint classes, similar to how we partition domain names for the domain recognition task. However, their partitioning approach was based on class frequencies: the training set contained the most frequent classes, while the test set included the least frequent ones. In contrast, we used random partitioning independent of class frequencies. Guarino et al. aimed to create natural classification tasks with a limited number of samples, as their overall focus was on the few-shot learning setup. On the other hand, we argue that it is worth exploring a broader range of scenarios, including cases where more frequent classes appear in the test set. This broader perspective is important when we consider the transfer of the $\Phi$ embedding function to other TC datasets, as real-world target datasets contain both less and more popular classes.

One of the findings of Guarino et al. is that supervised contrastive learning produces the best DL models and representations overall. Our work supports this finding, as there are notable similarities between the loss function used in our work, ArcFace, and the best-performing contrastive loss used by Guarino et al.:~SupCon~\cite{khosla2020supervised} extended with class embeddings and cosine distance, referred to as \texttt{SupCon(ClassEmb)}. Both ArcFace and SupCon aim to improve the discriminative power of embeddings. ArcFace does this in a classification setup by pulling samples to class centers and enforcing angular margins, while the original SupCon minimizes pairwise distances for similar and maximizes them for dissimilar samples. However, the modifications introduced in \texttt{SupCon(ClassEmb)} bring it closer to ArcFace, as both loss functions now utilize class centers (another name for class embeddings) and compute cosine distances towards them. Angular margins, which enhance class separation in the embedding space, remain the most significant distinction between these loss functions.

Finamore et al.~\cite{finamore2023replication} conducted a comprehensive evaluation of a flow representation called FlowPic, a 2D histogram capturing the evolution of packet sizes over time. This representation was studied in the context of contrastive learning, with a focus on data augmentation strategies. The authors replicated an earlier paper~\cite{Horowicz2022flowpic}, reproducing most of the original results while also incorporating three additional datasets. For our work, the most important contribution is the release of the \textit{tcbench} open-source framework, which we utilized for the cross-dataset evaluation. Also, the best classification accuracies achieved on UCDAVIS19 and UTMOBILENET21 datasets were used for the SOTA comparison, as presented in~\tabref{tab:cross-eval-results}.

Wang et al.~\cite{wang2024augmentation} presented a benchmark of data augmentations for TC, evaluating 18 of them on three datasets: MIRAGE19, MIRAGE22, and a private one. They utilized the standard sequential representation of packet metadata, consisting of the sizes, times, and directions of the first 20 packets. The training pipeline featured two stages: a contrastive self-supervised phase, where augmented versions of the same sample were pulled together, followed by a supervised phase, where a classification head was trained on the extracted features. Wang et al. also experimented with a class-weighted sampler to achieve perfect class balancing in each training epoch but found no success with it. In contrast, our semi-balancing technique, detailed in~\secref{sec:training-sampler}, proved to be beneficial in our experiments. The differing outcomes may be due to the use of perfect class balancing by Wang et al., whereas we employed the $\lambda_{sampler}$ parameter to control the strength of the balancing effect. The best classification accuracies achieved on MIRAGE19 and MIRAGE22 datasets were used for the SOTA comparison, as presented in~\tabref{tab:cross-eval-results}.