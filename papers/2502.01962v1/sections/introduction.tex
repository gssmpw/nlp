% -----------------------------------
\section{Introduction}
\label{sec:intro}
% -----------------------------------
State-of-the-art computer vision models typically follow the intuitive paradigm of pre-training on large single-modal general datasets, followed by fine-tuning on local task-specific datasets to achieve promising accuracy~\citep{chen2021pre,zhang2024boundary,radford2021learning,he2022masked,zhang2023cae}. 
% For example, in the semantic segmentation (SSeg) models, pre-trained backbone parameters based on ImageNet~\citep{deng2009imagenet} or MS-COCO~\citep{lin2014microsoft} are commonly used for downstream model initialization~\citep{he2016deep,xie2021segformer,liu2021swin,zhang2022deep}. 
However, this defacto paradigm requires downstream models to load the entire pre-trained model for fine-tuning, which may result in several undesirable drawbacks such as poor structural flexibility, large optimization gaps, and limited applicability in certain scenarios~\citep{jie2023fact,du2022survey,jie2022convolutional}. More importantly, these drawbacks have become more prominent and urgent problems that need to be addressed for dense prediction tasks, particularly in the current context of significant model size growth~\citep{gao2023clip,zhang2022unabridged,radford2021learning,zhang2023cae,Kirillov_2023_ICCV}.

Recently, with the increasing dominance of Vision Transformer (ViT) architectures~\citep{han2022survey,liu2021swin,khan2022transformers}, following the parameter-efficient transfer learning mechanism~\citep{houlsby2019parameter}, ViT adapter has become a central approach to learning vision-specific inductive biases from pre-trained ViT models~\citep{hu2022lora,jie2023fact,chen2022vision,ma2024segment,luo2023forgery,shao2023deepfake}, successfully addressing the drawbacks associated with the pre-training followed by fine-tuning paradigm.
%
The progressive ViT adapter enables downstream ViT models to achieve promising accuracy levels that are comparable to, or even higher than, those achieved by fine-tuning the entire model. 
% Fortunately, ViT adapter is usually accomplished using only approximately $5\%$ of the learning parameters~\citep{jie2023fact,chen2022vision,shao2023deepfake}. The classic ViT adapter is pre-training-free and mainly consists of a spatial \emph{feature extractor} and a spatial \emph{feature injector}~\citep{chen2022vision,hu2022lora,luo2023forgery,marouf2024mini}. The former extracts general features from a pre-trained ViT backbone network within each block, while the latter interacts the updated task-specific features with the ViT model~\citep{marouf2024mini,chen2022adaptformer,dong2024efficient}. In training, only the adapter's parameters need to be updated, while the ViT backbone's parameters remain fixed. 
In particular, thanks to the utilization of plain ViT models, which contain unique multi-modal information, adapters based on these models can effectively promote downstream models to learn beneficial semantic-rich feature representations~\citep{touvron2021training,hu2022lora,Kirillov_2023_ICCV,dosovitskiy2020image}. For example, ViT adapter under the plain ViT models has been successfully applied in multiple computer vision tasks, \eg,
% image classification~\citep{jie2023fact}, 
object detection (ODet)~\citep{li2022exploring}, instance segmentation (ISeg)~\citep{liu2024revisiting}, and semantic segmentation (SSeg)~\citep{xie2021segformer}.

Despite the significant progress made by existing ViT adapters~\citep{chen2022vision,liu2024revisiting,jie2023fact,marouf2024mini}, their inference speed is still somewhat unfavorable, which limits their implementations on edge computing devices~\citep{dong2024packqvit} and applications for real-time recognition scenarios~\citep{marouf2024mini}. 
%
Recently, it has been revealed that in addition to computation and parameter complexity (\eg, FLOPs and \#Params.), inefficient memory access operations~\citep{marouf2024mini,liu2023efficientvit}, such as standard normalization and frequent reshaping operations, play a critical role in hindering the ViT's inference speed~\citep{he2023simplifying,pan2022fast,fournier2023practical,shi2023evit}. 
%
In other words, the inefficient planning of memory access may cause delays and prevent models from fully utilizing the computing power of GPUs or CPUs, resulting in a significant negative impact on the speed of ViT models~\citep{liu2023efficientvit,dao2022flashattention,venkat2019swirl,gu2021towards,ivanov2021data}. However, these inefficient memory access operations are usually overlooked factors during the network design process for dense prediction tasks.
%
In this work, we explore how to solve this problem and accelerate the inference speed of ViT adapters in downstream models. \emph{Our solution is to decrease memory time consumption by reducing layer normalization and frequent reshaping operations.}

% -----------------------------------
% \input{figures/figure0}
% -----------------------------------
\begin{wrapfigure}{r}{0.53\textwidth}
\centering
\vspace{-4mm}
\includegraphics[width=.53\textwidth]{figures/fig0.pdf}
\vspace{-8mm}
\caption{\footnotesize Qualitative performance comparisons of different models with respect to training parameters, application gaps, memory access costs, and inference time costs.}
\vspace{-4mm}
\label{fig0}
\end{wrapfigure}
% -----------------------------------
We propose a simple and fast memory efficient transformer adapter (META) that can improve the model's memory efficiency and decrease memory time consumption by reducing the inefficient memory access operations. 
%
As illustrated in Figure~\ref{fig0}, our META demonstrates advantages over existing ViT models and ViT adapters in terms of memory consumptions and inference time costs.
%
The main contribution of this work is the proposal of a \emph{{memory-efficient adapter block}} that shares {normalization operations} between the self-attention and feed-forward network layers, which exist in a parallel manner (\emph{Ref.} Sec.~\ref{sec:3:2}). Within this block, the cross-shaped self-attention is employed to reduce the reliance for frequent {reshaping operations}. 
%
Consequently, the proposed two reverse designs are capable of significantly reducing the memory consumption of the ViT adapter, resulting in an improved inference speed.
%
Moreover, to enrich local inductive biases for dense predictions, a \emph{{lightweight convolutional branch}} is introduced into the \emph{{memory-efficient adapter block}}. 
%
In the process of interacting with the ViT backbone, a \emph{{cascaded mechanism}} is further proposed to compute different head features, which can enhance the diversity of the obtained feature representations. We conduct extensive experiments for ODet, ISeg and SSeg on two challenging datasets, namely MS-COCO~\citep{lin2014microsoft} and ADE20K~\citep{zhou2017scene}, for evaluating our META. 
%
The obtained results demonstrate that META substantially enhances prediction quality, attains a new state-of-the-art accuracy level, reduces the number of parameters and memory consumption requirements, and achieves faster inference speeds. Theoretically, we also prove that META can exhibit superior generalization capability and stronger adaptability compared to existing ViT adapter methods\footnote{Due to page limitations, the theoretical analysis will been provided in the supplementary materials.}.