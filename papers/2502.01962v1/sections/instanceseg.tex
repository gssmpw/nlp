% -------------------------------
\subsection{Experiments on Object Detection (ODet) and Instance Segmentation (ISeg)}
\label{sec4:2}
% -------------------------------
\myparagraph{Baselines and settings.} As in~\citep{chen2022vision,xiong2024efficient,jie2022convolutional,marouf2024mini}, Mask R-CNN~\citep{he2017mask}, Cascade Mask R-CNN~\citep{cai2019cascade}, ATSS~\citep{zhang2020bridging}, and GFL~\citep{li2020generalized} are employed as the baseline models, where the pre-trained ViT~\citep{li2022exploring} is used as the backbone. All the baseline models are pre-trained on ImageNet-1k by default~\citep{deng2009imagenet}. Unless otherwise specified, these baselines are set up to be consistent with their papers and the settings of the ViT-Adapter~\citep{chen2022vision} method.
% -------------------------------
\input{tables/table1}
% -------------------------------

% -------------------------------
\myparagraph{Comparisons with state-of-the-art (SOTA) methods.} Result comparisons with SOTA methods with Mask R-CNN~\citep{he2017mask} for ODet and ISeg are shown in Table~\ref{tab1}. From this table, we can obtain the following observations and conclusions: 
%
\emph{\textbf{\romannumeral1}})~Compared to the experimental results of the Mask R-CNN~\citep{he2017mask} model with ViT~\citep{li2021benchmarking}, our proposed META can consistently improve accuracy for ODet and ISeg across different model scales (\eg, ViT-T/S/B/L~\citep{li2021benchmarking}), while only adding a small number of training parameters. Even with different training schedules (\ie, 1$\times$, and 3$\times$ with MS), our method can also improve the model performance, demonstrating the plug-and-play advantage of META. 
%
\emph{\textbf{\romannumeral2}})~Compared to the SOTA ViT-Adapter~\citep{chen2022vision} and LoSA~\citep{mercea2024time}, META can achieve a new accuracy-efficiency trade-off. For example, in settings with the strong ViT-B~\citep{li2021benchmarking} as the backbone, our method achieves a performance gain of $0.5\%$AP$^\textrm{m}$ under 1$\times$ training schedule and $0.7\%$AP$^\textrm{m}$ under 3$\times$ training schedule with MS while reducing 4.9\textbf{M} model parameters, when compared to ViT-Adapter~\citep{chen2022vision}. 
%
\emph{\textbf{\romannumeral3}})~Even with stronger pre-trained models, META still improves performance on the baseline models and surpasses existing methods on accuracy, parameters and memory. For example, on the ImageNet-22k pre-trained weights from~\citep{steiner2021train}, our method achieves a performance gain of $0.8\%$/$0.7\%$AP$^\textrm{m}$ under 1$\times$/3$\times$ training schedule while reducing 8.2\textbf{M} parameters compared to ViT-Adapter~\citep{chen2022vision}, which validates its strong learning ability and flexible adaptability.
%
\emph{\textbf{\romannumeral4}})~Compared to SOTA ODet and ISeg methods, META also has very competitive performance. For example, compared to the strong ViTDet-B~\citep{li2022exploring} model, META-T has $2.2\%$AP$^\textrm{b}$ and $3.1\%$AP$^\textrm{m}$ gains under the 1$\times$ training schedule, and has META-T has $4.9\%$AP$^\textrm{b}$ and $2.7\%$AP$^\textrm{m}$ gains under the 3$\times$ training schedule.
%
\emph{\textbf{\romannumeral5}})~Compared to SOTA ViT adapter methods such as AdaptFormer~\citep{chen2022adaptformer}, FacT-TK~\citep{jie2023fact}, and LoSA~\citep{mercea2024time}, our method exhibits superior efficiency in terms of reduced parameter count, decreased FLOPs, and lower MC. Particularly, META only utilizes $62\%$ of the MC of LoSA while achieving superior prediction accuracy. Therefore, the experimental results and conclusions above demonstrate that our method achieves better accuracy and higher efficiency in dense prediction tasks.
% -------------------------------
\input{tables/table2}
% -------------------------------


% -------------------------------
\myparagraph{Superiority performance under different baselines.} 
% -------------------------------
In addition to Mask R-CNN, we also choose Cascade Mask R-CNN~\citep{cai2019cascade}, ATSS~\citep{zhang2020bridging}, and GFL~\citep{li2020generalized} as the baselines as in~\cite{chen2022vision,mercea2024time}. We explore the effectiveness and superiority performance of META on these baseline models, where the 3$\times$ training with MS strategy is used. 
% 
The experimental results are given in Table~\ref{tab2}. We can observe that META can consistently improve performance across different baselines, and exhibits more accurate and more efficient advantages compared to SOTA ViT adapter methods. 
%
For example, based on the Cascade Mask R-CNN~\citep{cai2019cascade}, META-S can achieve up to $44.8\%$ AP$^\textrm{m}$ with only $83$\textbf{M} parameters, $797$\textbf{G} FLOPs, and $8.1$\textbf{GB} MC, which is $1.3\%$ AP$^\textrm{m}$ higher, $3$\textbf{M} fewer parameters, $4$\textbf{G} fewer FLOPs, and $7.1$\textbf{GB} fewer MC compared to the competitive ViT-Adapter-T method~\citep{chen2022vision}. 
%
Besides, META-B can achieve a $1.7\%$ AP$^\textrm{b}$/$0.5\%$ AP$^\textrm{m}$ gain on the 3$\times$ training schedule, with $5$\textbf{M} fewer parameters and $5$\textbf{G} fewer FLOPs than the ViT-Adapter-B~\citep{chen2022vision} model.
%
On ATSS and GFL, our method also achieves competitive $43.2\%$ and $44.$0\% AP$^\textrm{m}$, respectively, which are $0.7\%$ and $0.9\%$ AP$^\textrm{m}$ higher than the competitive vision adapter ViT-Adapter-S~\citep{chen2022vision}. 
%
In terms of efficiency, compare with the SOTA LoSA~\citep{mercea2024time} method,  our method has fewer parameters ($33$\textbf{M} v.s. $35$\textbf{M}), fewer FLOPs ($265$\textbf{G} v.s. $268$\textbf{G} under ATSS, $279$\textbf{G} v.s. $284$\textbf{G} under GFL), and fewer MC ($8.1$\textbf{GB} v.s. $13.0$\textbf{GB} under ATSS and GFL), indicating its higher computation and memory efficiency.
%  \input{tables/table3}

% -------------------------------
% \myparagraph{Results on different pre-trained weights.} 
% -------------------------------
% In Table~\ref{tab3}, we present the experimental results of META with different pre-trained weights and compare them with other SOTA methods including SwinViT~\citep{liu2021swin} and ViT-Adapter~\citep{chen2022vision} as in~\citep{chen2022vision}. Mask R-CNN~\citep{he2017mask} is used as the baseline, and ViT-B~\citep{li2022exploring} is used as the backbone. The 3$\times$ training schedule with MS training strategy is used. From this table, we can observe that our method is applicable to different pre-trained weights (\ie, ImageNet-1k~\citep{deng2009imagenet}, ImageNet-22k~\citep{steiner2021train}, and Multi-Modal~\citep{zhu2022uni}), and achieves more accurate AP with fewer model parameters and FLOPs compared to ViT-Adapter~\citep{chen2022vision}, across different pre-trained weights. Our method achieves $0.7\%$, $0.6\%$, and $0.6\%$ higher AP than ViT-Adapter on these three weights, respectively. These results demonstrate the superiority of our method in terms of flexibility, accuracy and efficiency.