% -------------------------------
\subsection{Experiments on Semantic Segmentation (SSeg)}
\label{sec4:3}
% -------------------------------
\myparagraph{Baselines and Settings.} 
% Experiments on SSeg are conducted using the MMSegmentation framework~\citep{mmseg2020}. 
Following~\citep{chen2022vision,jie2023fact,jie2022convolutional}, we select Semantic FPN~\citep{kirillov2019panoptic} and UperNet~\citep{xiao2018unified} as baseline models, where the Semantic FPN is trained for $80$k iterations and the UperNet is trained for $160$k iterations as in~\citep{wang2021pyramid,liu2021swin}. 
%The input images are cropped to a fix size of 512 $\times$ 512 pixels as in~\citep{xiong2024efficient,chen2022vision}. The training batch size is set to $16$, and AdamW~\citep{loshchilov2017decoupled} is used as the optimizer with the initial learning rate of $1 \times 10^{-5}$ and the weight decay of $0.05$. Following~\citep{li2022exploring,liu2021swin}, the layer-wise learning rate decay is set to $0.9$ and the drop path rate is set to $0.4$. We report the experimental results on both single scale training and MS training strategies. 
Unless otherwise specified, the training and inference settings are set up to be consistent with the ViT-Adapter~\citep{chen2022vision}.
% -------------------------------
\input{tables/table4}
% -------------------------------

% -------------------------------
\myparagraph{Comparisons with state-of-the-art methods.} 
We show the SSeg experimental results of META under different settings and compare them with SOTA SSeg methods in Table~\ref{tab4}. We can observe that \emph{\textbf{\romannumeral1}}) Our method can consistently improve performance across different baselines, model scales, training strategies, and pre-training weights (including IN-1K, IN-22K, and MM), while having fewer model parameters and memory consumption compared to the advanced ViT-Adapter~\citep{chen2022vision} and LoSA~\citep{mercea2024time} methods. This demonstrates the strong applicability and learning ability of META, which can solve the problem of limited applicability in certain scenarios of downstream models in the traditional pre-training and fine-tuning strategy at the application level. \emph{\textbf{\romannumeral2}}) Compared to SOTA methods, META also can achieve a new SOTA accuracy-cost trade-off. The parameter from META is only about $0.2\%$ of the overall model parameter count, indicating a minimal impact on the total parameter count. Furthermore, the memory consumption of our method accounts for only 55.14\% of the SOTA LoSA~\citep{mercea2024time}.
\emph{\textbf{\romannumeral3}}) On larger model scales, META achieves higher efficiency with fewer parameters (\eg, -$1.4$\textbf{M} on {META-T} and -$4.2$\textbf{M} on {META-B}). This indicates that our method is suitable for fine-tuning large ViT models.