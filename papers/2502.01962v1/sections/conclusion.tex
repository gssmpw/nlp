\section{Conclusion}
We proposed a simple and fast META that aims at improving the ViT model's memory efficiency and reduce memory time consumption by minimizing layer normalization and frequent reshaping operations. The main contribution of this work was the proposal of the MEA block that shares the {layer normalization} between the self-attention and feed-forward network layers, which exist in a parallel manner. Within the proposed block, the cross-shaped self-attention was employed to reduce the requirements for frequent {reshaping operations}. Moreover, a lightweight local convolutional branch is introduced into META to enrich local inductive biases. 
%
Experimental results on object detection, instance segmentation, and semantic segmentation tasks demonstrated that META can achieve a new state-of-the-art accuracy-cost trade-off and at a faster inference speed. In addition to the experimental validation, we also theoretical proved that META exhibits superior generalization capability and stronger adaptability compared to current ViT adapters. In the future, we will explore the effectiveness of META on more ViT architectures, such as designing memory-efficient ViT models for a wider range of computer vision tasks. Besides, reducing the model's memory footprint is identified as a promising research direction to be explored for LLMs. 

\section*{Acknowledgment}
The authors would like to thank all anonymous reviewers for their positive comments and constructive suggestions. This work was partially supported by the Hong Kong SAR RGC General Research Fund under Grant 16208823 and ACCESS - AI Chip Center for Emerging Smart Systems, sponsored by InnoHK funding, Hong Kong SAR.
