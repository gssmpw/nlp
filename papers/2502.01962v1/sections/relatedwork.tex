% -------------------------------
\section{Related Work}
% -------------------------------
\myparagraph{Vision Transformer (ViT).}
% -------------------------------
% ViT models have become one of the defacto network architectures for state-of-the-art computer vision tasks~\citep{han2022survey,zhang2022graph,liu2021swin,khan2022transformers,zhang2023cae,Kirillov_2023_ICCV}.
Current ViT models for state-of-the-art computer vision tasks can be roughly divided into two camps: \emph{plain ViT models}, which are designed to learn general vision features (\eg, ViT~\citep{dosovitskiy2020image}, DeiT~\citep{touvron2021training} and TDE Transformer~\citep{touvron2021training}), and \emph{hierarchical ViT models}, which are designed to learn vision-specific features (\eg, Swin Transformer~\citep{liu2021swin} and PVT~\citep{wang2021pyramid}). Both camps of ViT models have their pros and cons. 
%
For example, the \emph{hierarchical ViT models} can learn powerful vision-specific feature representations, which makes them usually perform better than the \emph{plain ViT models} on accuracy~\citep{han2022survey,khan2022transformers,Kirillov_2023_ICCV}. However, their unvarnished disadvantage is the lack of multi-modal pre-training information for dense predictions~\citep{chen2022vision}. On the other hand, although the \emph{plain ViT models} have weak prior assumptions, which results in lower accuracy compared to the \emph{hierarchical ViT models}, their multi-modal pre-training information has great potential to provide semantic-rich feature representations for downstream models~\citep{guo2022attention,zhang2023cae,he2022masked,zhang2024boundary,jia2022visual}. In this paper, we focus on the \emph{plain ViT models} for dense predictions. Our contribution is the proposal of a straightforward and memory efficient ViT adapter, starting from the reduction of inefficient memory access operations.

% -------------------------------
% \subsection{Efficient ViT Models}
% -------------------------------
% Efficient ViT refers to a variant of the ViT model that has been optimized for computation complexity or/and memory access, while still maintaining high accuracy~\citep{cai2022efficientvit,liu2023efficientvit,dao2022flashattention,gu2021towards}. The efficiency improvements can be achieved through various aspects, \eg, attention mechanisms, normalization schemes, and network architectures~\citep{pan2022fast,wang2021gpt,zhang2023cae,zhang2022graph,xie2021segformer,jie2022convolutional,luo2023forgery}. Based on the optimization directions, current efficient ViTs can be divided into the following three basic types: \emph{computation efficiency ViT}~\citep{touvron2021training,michel2019sixteen}, \emph{parameter efficiency ViT}~\citep{chen2021autoformer,liu2018rethinking}, and \emph{memory efficiency ViT}~\citep{liu2023efficientvit,dao2022flashattention}. As the name implies, these efficient ViTs are optimized from reducing the effective computation frequency, lowering the actual computational cost, and improving the efficiency of the model's access to memory unit~\citep{dong2024efficient}. In dense predictions, existing models mainly focus on reducing the computational complexity, that is, improving the \emph{computation efficiency} and \emph{parameter efficiency}~\citep{wang2023repvit,he2023simplifying,zhang2023cae,ranftl2021vision,lee2022mpvit,shen2023survey,caron2024location}, while ignoring the problem of the model's inference speed being limited by frequent memory access operations. 
% In this work, we start with the advanced ViT adapter technology, aiming to improve the model's \emph{memory efficiency} and reduce memory time consumption by minimizing inefficient memory access operations, \ie, standard normalization and frequent reshaping operations.
% -------------------------------
\myparagraph{ViT for Dense Predictions.}
% -------------------------------
Dense predictions, including ODet, ISeg, and SSeg, aim at predicting a semantic mask where each pixel/object in the given image is assigned a class label~\citep{ranftl2021vision,zhang2023cae,vandenhende2021multi}. For ISeg, the obtained mask also distinguishes different objects of the same class. 
% Dense predictions have been extensively studied in the past and have achieved remarkable results~\citep{chen2021pre,liu2023efficientvit,lee2022mpvit}. 
With the development of ViT technologies, dense prediction models based on ViT have become one of the default choices for state-of-the-art methods~\citep{liu2021swin,wang2021pyramid,dosovitskiy2020image}. These methods can be mainly divided into two categories: those based on Transformers and those that incorporate a combination of CNN layers. The former, \eg, SegFormer~\citep{strudel2021segmenter}, HRViT~\citep{gu2022multi}, Segvit~\citep{zhang2022segvit}, Swin UNet~\citep{hatamizadeh2021swin}, and ISTR~\citep{hu2021istr}, extract features from a given image and connect with task-specific head networks to achieve specific recognition purposes. The latter compensates for the lack of local inductive bias in the pure ViT architecture by introducing local convolutional layers, and representative methods include ConFormer~\citep{peng2021conformer}, CVT~\citep{wu2021cvt}, NextViT~\citep{li2022next}, and CAE-GReaT~\citep{zhang2023cae}. While the former can capture long-range dependencies and achieve favorable recognition capability, they require sufficient training samples to optimize the model, and further improvement is needed on memory access and inference speed. To this end, we propose a solution to the problem of limited inference speed caused by frequent memory consumption using a pre-trained ViT model.%  Our main objective is to achieve memory efficiency by reducing inefficient memory access operations.


% -------------------------------
\myparagraph{ViT Adapters.}
% -------------------------------
Adapter methods are originally derived from the NLP tasks, such as PALs~\citep{stickland2019bert} and NLP adapter~\citep{houlsby2019parameter}. These methods add a small number of trainable modules to a pre-trained network, so that downstream fine-tuning models can quickly adapt to specific datasets and tasks via a parameter-efficient transfer learning manner~\citep{hu2022lora,houlsby2019parameter,chen2022vision,shen2023survey}. In the computer vision domain, ViT adapters have also adopted the similar paradigm~\citep{jie2023fact,chen2022vision,shao2023deepfake,luo2023forgery}. 
% For example, in the training process, the ViT adapter branch first encodes the input image into a set of semantic features through a trainable spatial prior module, and then extracts the general features of the ViT backbone network through the spatial feature extractor, and injects the learned vision-specific features into the backbone network via the spatial feature injector~\citep{chen2022vision}. 
ViT adapter method is used to fine-tune large-scale plain ViT models with a small number of trainable parameters and achieve promising accuracy~\citep{jie2023fact,li2022exploring,chen2022vision}. 
% Currently, this technology has been widely used in many fundamental computer vision tasks, such as image classification~\citep{jie2023fact}, object detection~\citep{li2022exploring}, forgery detection~\citep{luo2023forgery}, and dense predictions~\citep{chen2022vision}. 
Inspired by~\citep{he2023simplifying,jie2022convolutional,chen2022vision,liu2023efficientvit,mercea2024time}, we propose an efficient solution to enhance the memory efficiency and minimize memory time consumption of ViT adapters. We also provide theoretical analysis to demonstrate the superior generalization and adaptability of our method.

