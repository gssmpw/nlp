% ----------------------------------------------
\newpage
\section*{supplementary material}
In this supplementary material, we will provide a theoretical analysis to the proposed memory efficient Transformer adapter (META) in Section~\ref{secS1}, provide a detailed description of the experimental datasets in Section~\ref{secS2}, provide a detailed description of the experimental settings in Section~\ref{secS3},
provide more result comparisons under different pre-trained weights in Section~\ref{secS4},
provide more ablation study results in Section~\ref{secS5}, show class activation map comparisons of instance segmentation before and after adding the Conv branch in Section~\ref{secS6},qualitative visualizations of instance segmentation and semantic segmentation results in Section~\ref{secS7},  as well as the pseudo-code for when the stripe size is set to $2$ in Section~\ref{secS8}. 
% -------------------------------------------
\section{Theoretical Analysis of META}
\label{secS1}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~3 of the main paper.}}} In this section, we will prove that META exhibits superior generalization capability and stronger adaptability compared to existing ViT adapters. 
%
To achieve this goal, we will prove that the proposed memory efficient adapter (MEA) block possesses larger information entropy (IE) than the existing attention-based ViT adapters~\citep{hu2022lora,jie2023fact,chen2022vision,ma2024segment,luo2023forgery,shao2023deepfake}, which provides evidence that the MEA block has more comprehensive feature representations. Then, based on the maximum mean discrepancy (MMD) theory~\citep{cheng2021neural,arbel2019maximum,wang2021rethinking}, larger IE in the ViT adapter framework leads to superior generalization capability and stronger adaptability. The detailed theoretical analysis process is as follows:

\begin{lemma}
% ---------------------------------
In any case of mutual information, the MEA block will gain larger information entropy after fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$.
% ---------------------------------
\end{lemma}
% ---------------------------------
\begin{proof}
As introduced in Section~3.2 of the main paper, the proposed MEA block can be viewed as an operation that integrates the ViT features (\ie, the Attn branch and the FFN branch) and the convolution features (\ie, the Conv branch). Therefore, we begin by formalizing the obtained features into the following two basic elements: the ViT features and the convolution features. To formalize the learning setting, we express the ViT features as $\textbf{X}_{vit}$ and the convolution features as $\textbf{X}_{con}$. It is evident that if $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ are extracted from the same image, then $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ are not independently distributed, and there exists some mutual information between them~\citep{zhang2022graph,wu2021cvt,zhang2023cae,peng2021conformer}. Therefore, the IE of the fused feature of $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ within the MEA block can be expressed as:
% ---------------------------------------------------
\begin{equation}
\begin{split}
\label{eqs:1}
\textrm{H}(\textbf{X}_{vit}, \textbf{X}_{con}) = \textrm{H}(\textbf{X}_{vit}) + \textrm{H}(\textbf{X}_{con}) - \textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con}),
\end{split}
\end{equation}
% ---------------------------------------------------
where $\textrm{H}(\cdot)$ is utilized to calculate the IE of the given variate, which can be formulated as:
% ---------------------------------------------------
\begin{equation}
\begin{split}
\label{eqs:2}
\textrm{H}(\textbf{X}_{vit}) = -\sum P(\textbf{x}_{vit}) log(P(\textbf{x}_{vit})),\\
\textrm{H}(\textbf{X}_{con}) = -\sum P(\textbf{x}_{con}) log(P(\textbf{x}_{con})),
\end{split}
\end{equation}
% ---------------------------------------------------
where $P(\textbf{x}_{vit})$ represents the probability of $\textbf{X}_{vit}$ taking on the value of $\textbf{x}_{vit}$. The similar definition of $P(\textbf{x}_{con})$. $\textrm{I}(\cdot;\cdot)$ in Eq.~\eqref{eqs:1} is used to compute the mutual information between $\textbf{X}_{vit}$ and $\textbf{X}_{con}$, which can be expressed as:
% ---------------------------------------------------
\begin{equation}
\begin{split}
\label{eqs:3}
\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con}) = \sum\sum \textrm{P}(\textbf{X}_{vit}, \textbf{X}_{con}) \textrm{log}(\textrm{P}(\textbf{X}_{vit}, \textbf{X}_{con}) (\textrm{P}(\textbf{X}_{vit}), \textrm{P}(\textbf{X}_{con}))),
\end{split}
\end{equation}
% ---------------------------------------------------
where $\textrm{P}(\textbf{X}_{vit}, \textbf{X}_{con})$ is their joint probability distribution. 
%\textrm{P}(\textbf{X}_{vit})$ and $\textrm{P}(\textbf{X}_{con})$ are the marginal probability distributions of $\textbf{X}_{vit}$ and $\textbf{X}_{con}$, respectively. 
Since $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ is always non-negative, $\textrm{H}(\textbf{X}_{vit}, \textbf{X}_{con})$ may still be greater than $\textrm{H}(\textbf{X}_{vit})$ or $\textrm{H}(\textbf{X}_{con})$~\citep{paninski2003estimation,gabrie2018entropy}. This suggests that the IE of the features extracted by MEA is always greater than the feature representation extracted by either of them separately.

Specifically, if $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ is small, the IE gain after fusion may still be significant, which is beneficial for improving the generalization capability and adaptability of the block. However, when $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ is large, the IE gain after fusion may be reduced. This means that $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ may affect the IE improvement of the fused model. Next, we will discuss the impact of $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ on improving the IE of the adapter based on the size of $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$, which can be divided into the following three cases:

\begin{itemize}
% --------------------------
\item {{Small} $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$.} This is an ideal state. When the dependency between $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ is small, it indicates that $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ is small, that is, $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ respectively represent different information of the image. In this case, fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ can bring a significant increase in IE, which is beneficial to improving the adapter's generalization capability and adaptability.
% --------------------------
\item {{Medium} $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$.} When $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ is between small and large, it indicates that there is a certain degree of correlation between them. In this case, fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ may still bring some IE gain. The specific improvement effect depends on the degree of correlation between $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ and their complementarity in image representations. Fortunately~\citep{zhang2022graph,zhang2023cae,marouf2024mini,liu2023efficientvit}, a large amount of work has validated that ViT and convolutional layers can extract distinctive information from images. Therefore, in this case, fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ can still bring IE gains.
\item {{Large} \myparagraph{$\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$}.} When $\textrm{I}(\textbf{X}_{vit}; \textbf{X}_{con})$ between $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ is large, it indicates that there is a high correlation between them, \ie, global ViT and local convolution features may represent similar or overlapping information of the image. In this case, the IE gain brought by fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ may decrease because there is a lot of information overlap between them. However, in our case, the probability of such a scenario occurring is almost non-existent, fusing $\textbf{X}_{vit}$ and $\textbf{X}_{con}$ may still improve the performance of the model to some extent, because they may capture the detailed information of the image to varying degrees.
% --------------------------
\end{itemize}
% --------------------------

Based on the aforementioned theoretical analysis, we can conclude that the proposed MEA block has a larger IE than existing ViT adapters (which are primarily based on the attention mechanism) under any scenario. This provides evidence that the MEA block has more comprehensive feature representations. 
% ---------------------------------
\end{proof}
% ---------------------------------
As the MEA block includes a parallel convolutional branch, it can better capture local inductive biases compared to the traditional ViT adapter, which mainly uses self-attention~\citep{hu2022lora,jie2023fact,chen2022vision,ma2024segment,luo2023forgery,shao2023deepfake,mercea2024time}. 
%
Therefore, the MEA block's feature space should be more capable of distinguishing different samples, resulting in a larger MMD value. 
%
Our MEA block's feature space is obtained by combining the attention branch, the feed-forward network branch, and the local convolutional branch, enabling it to capture both local and global inductive biases of the given image. 
%
In contrast, the traditional ViT adapter's feature space is mainly obtained through self-attention and may not be able to capture local features well. Therefore, according to the MMD theory~\citep{cheng2021neural,arbel2019maximum,wang2021rethinking}, we can conclude that if the MEA block's feature space is more discriminative than the traditional ViT adapter's feature space, then the MEA block's feature space is more suitable for adapter feature space and can better improve the model's generalization capability and adaptability.

% -------------------------------------------
\section{Introduction of the Experimental Datasets}
\label{secS2}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~4.1 of the main paper.}}}
In our paper, two representative datasets are used to evaluate the effectiveness and efficiency of our method, including MS-COCO~\citep{caesar2018coco} for ODet and ISeg, and ADE20K~\citep{zhou2017scene} for SSeg. Below are the details of the used datasets:

% -------------------------------
\begin{itemize}
% -------------------------------
\item MS-COCO~\citep{caesar2018coco} is a representative yet challenging dataset for common scene IS and object detection, which consists of $118$k, $5$k and $20$k images for the \emph{training} set, the \emph{val} set and the \emph{test} set, respectively. In our experiments, the model is trained on the \emph{training} set and evaluated on the \emph{val} set.
% -------------------------------
\item ADE20K~\citep{zhou2017scene} is a scene parsing dataset with $20$k images and $150$ object categories. Each image has pixel-level annotations for SS of objects and regions within the scene. The dataset is divided into $20$k, $2$k, and $3$k images for \emph{training}, \emph{val} and \emph{test}, respectively. Our model is trained on the \emph{training} set and evaluated on the \emph{val} set.
% -------------------------------
\end{itemize}
% -------------------------------
For data augmentation, random horizontal flip, brightness jittering and random scaling within the range of $[0.5, 2]$ are used in training as in~\citep{chen2022vision,luo2023forgery,zhang2023cae,mercea2024time}. By default, the inference results are obtained at a single scale, unless explicitly specified otherwise.    


% -------------------------------------------
\section{Introduction of the Experimental Settings}
\label{secS3}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~4.2 of the main paper.}}} Experiments on object detection and instance segmentation are conducted using the open-source MMDetection framework~\citep{chen2019mmdetection}. The training batch size is set to $16$, and AdamW~\citep{loshchilov2017decoupled} is used as the optimizer with the initial learning rate of $1 \times 10^{-4}$ and the weight decay of $0.05$. The layer-wise learning rate decay is used and set to $0.9$, and the drop path rate is set to $0.4$. Following~\citep{xiong2024efficient,wang2021pyramid,chen2022vision,liu2022convnet}, to ensure a fair result comparison, we choose two training schedules, 1$\times$ (\ie, $12$ training epochs) and 3$\times$ (\ie, $36$ training epochs). For the 1$\times$ training schedule, images are resized to the shorter side of 800 pixels, with the longer side not exceeding $1,333$ pixels. In inference, the shorter side of images is consistently set to 800 pixels by default. For the 3$\times$ training schedule, the multi-scale training strategy is also used as in~\citep{chen2022vision}, and the shorter side is resized to $480$ to $800$ pixels, while the longer side remains capped at $1,333$ pixels.

{\color{red}{\emph{This supplementary is for Section~4.3 of the main paper.}}} Experiments on semantic segmentation are conducted using the MMSegmentation framework~\citep{mmseg2020}. The input images are cropped to a fix size of 512 $\times$ 512 pixels as in~\citep{xiong2024efficient,chen2022vision}. The training batch size is set to $16$, and AdamW~\citep{loshchilov2017decoupled} is used as the optimizer with the initial learning rate of $1 \times 10^{-5}$ and the weight decay of $0.05$. Following~\citep{li2022exploring,liu2021swin}, the layer-wise learning rate decay is set to $0.9$ and the drop path rate is set to $0.4$. We report the experimental results on both single scale training and multi-scale training strategies. 
% -------------------------------
\begin{table}[t]
\centering
\small
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{6pt}{
\begin{tabular}{r|r|ccl}
\hline \hline 
Methods & Pre-Trained & Params.$\downarrow$ & AP$^\textrm{m}$ $\uparrow$ \\
\hline 
Swin-B~\citep{liu2021swin} & ImageNet-1k~\citep{deng2009imagenet} & 107.1 &  43.3 \\
ViT-Adapter-B~\citep{chen2022vision} & ImageNet-1k~\citep{deng2009imagenet} & 120.2 & 43.6 \\
\cellcolor[gray]{.95}\textbf{META-B$_{{\textrm{(Ours)}}}$} & \cellcolor[gray]{.95}ImageNet-1k~\citep{deng2009imagenet} & \cellcolor[gray]{.95}115.3 & \cellcolor[gray]{.95}44.3$_{\color{red}{+0.7}}$ \\
\cdashline{1-4}[0.8pt/2pt]
Swin-B~\citep{liu2021swin} & ImageNet-22k~\citep{steiner2021train} & 107.1 & 44.3\\
ViT-Adapter-B~\citep{chen2022vision} & ImageNet-22k~\citep{steiner2021train} & 120.2 & 44.6 \\
\cellcolor[gray]{.95}\textbf{META-B$_{{\textrm{(Ours)}}}$} & \cellcolor[gray]{.95}ImageNet-22k~\citep{steiner2021train} & \cellcolor[gray]{.95}115.3  & \cellcolor[gray]{.95}45.2$_{\color{red}{+0.6}}$ \\
\cdashline{1-4}[0.8pt/2pt]
Swin-B~\citep{liu2021swin} & Multi-Modal~\citep{zhu2022uni} & 107.1 &   -- \\
ViT-Adapter-B~\citep{chen2022vision} & Multi-Modal~\citep{zhu2022uni} & 120.2  & 45.3 \\
\cellcolor[gray]{.95}\textbf{META-B$_{{\textrm{(Ours)}}}$} & \cellcolor[gray]{.95}Multi-Modal~\citep{zhu2022uni} & \cellcolor[gray]{.95}115.3  & \cellcolor[gray]{.95}45.9$_{\color{red}{+0.6}}$ \\
\hline \hline 
\end{tabular}
\caption{Result comparisons on Params. (\textbf{M}) and AP (\%) under different pre-trained weights with Mask R-CNN ($3 \times$ +MS schedule)~\citep{he2017mask} as the baseline model on the \emph{val} set of MS-COCO~\citep{caesar2018coco}. ``--'' denotes there is no such a result in its paper.}
\label{tab3}}
\end{table}
% -------------------------------

% -------------------------------------------
\section{Result Comparisons under Different Weights}
\label{secS4}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~4.2 of the main paper.}}} In this section, we present the experimental results of META on object detection and instance segmentation with different pre-trained weights and compare them with other state-of-the-art methods including SwinViT~\citep{liu2021swin} and ViT-Adapter~\citep{chen2022vision} as in~\citep{chen2022vision}. 
Mask R-CNN~\citep{he2017mask} is used as the baseline, and ViT-B~\citep{li2022exploring} is used as the backbone. The 3$\times$ training schedule with MS training strategy is used. The obtained experimental results are given in Table~\ref{tab3}.
%
From this table, we can observe that our method is applicable to different pre-trained weights (\ie, ImageNet-1k~\citep{deng2009imagenet}, ImageNet-22k~\citep{steiner2021train}, and Multi-Modal~\citep{zhu2022uni}), and achieves more accurate AP with fewer model parameters compared to ViT-Adapter~\citep{chen2022vision}, across different pre-trained weights.  

% -------------------------------------------
\section{More Ablation Study Results}
\label{secS5}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~4.4 of the main paper.}}} In our main paper, we present the experimental results of deploying adapters with Attn branch and FFN branch as components on ViT-B~\citep{li2022exploring}. It is noteworthy that the layer normalization operation has been shared between the Attn branch and the FFN branch to reduce the memory access costs associated with the normalization operations. In this section, we demonstrate a result comparison between the experimental results of using shared layer normalization operation and those of not using it in the traditional setting (\ie, the non-shared normalization). The obtained experimental results are shown in Table~\ref{tab:s1}. It can be observed that sharing layer normalization does not significantly improve the performance in terms of AP. However, compared to FPS, FLOPs, MC, our approach can achieve satisfactory performance gains.
% --------------------------
\begin{table*}[t]
\centering
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{1pt}{
\begin{tabular}{r|ccccc|ccccc}
\hline \hline 
Settings & ViT-B & Attn & FFN & Conv & Cascade & AP$^\textrm{m}$ $\uparrow$ & FPS$\uparrow$ & Params.$\downarrow$ & FLOPs$\downarrow$ & MC$\downarrow$ \\
\hline 
Baseline model & \cmark & \xmark & \xmark & \xmark & \xmark & 41.3 & 11.5 & 113.6\textbf{M} & 719\textbf{G} & NA\\
\cdashline{1-11}[0.8pt/2pt]
\cellcolor[gray]{.95}Shared normalization & \cmark & \cmark & \cmark & \xmark & \xmark & \cellcolor[gray]{.95}43.4 & \cellcolor[gray]{.95}11.3 & \cellcolor[gray]{.95}114.4\textbf{M} & \cellcolor[gray]{.95}719\textbf{G} & \cellcolor[gray]{.95}7.5\textbf{GB}\\
Non-shared normalization & \cmark & \cmark & \cmark & \xmark & \xmark & 43.2 & 10.5 & 114.4\textbf{M} & 737\textbf{G} & 8.8\textbf{GB}\\
\hline \hline 
\end{tabular}
\caption{Ablation study results on shared layer normalization.}
\label{tab:s1}}
\end{table*}
% --------------------------

% -------------------------------
{\color{red}{\emph{This supplementary is for Section~4.4 of the main paper.}}} META is proposed as a simple and fast ViT adapter by minimizing inefficient memory access operations. In this section, we compare META with other efficient attention methods and advanced adapter methods~\citep{marouf2024mini,xia2022vision,sung2022vl}. All methods are used with their default settings and the same settings as the injector and extractor in ViT-adapter~\citep{chen2022vision}. Following the same setup as in~\citep{chen2022vision}, the attention mechanism is utilized as the ViT-adapter layer. Therefore, during the experimental comparisons, we replace the attention mechanism in the ViT-adapter with alternative attention mechanisms to ensure a fair comparison. 
The obtained experimental results are given in Table~\ref{tab6}. We can observe that compared to these methods, META achieves new state-of-the-art performance in both accuracy and efficiency. We ultimately achieve an AP of $44.3\%$ with $115.3$\textbf{M} parameters, $720$\textbf{G} FLOPs, $17.4$ FPS, and 8.1 \textbf{GB} MC. 
% -------------------------------
\begin{table}[t]
\centering
\footnotesize
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{5pt}{
\begin{tabular}{r|ccccc}
\hline \hline 
Methods & AP$\uparrow$ & FPS$\uparrow$ & Params. (\textbf{M})$\downarrow$ & FLOPs (\textbf{G})$\downarrow$  & Momory (\textbf{GB})$\downarrow$ \\
\hline 
WindowAtt~\citep{liu2021swin} & 41.2 & 11.6 & 145.0 & 982 & 18.5 \\
PaleAttention~\citep{wu2022pale} & 42.8 & 14.4 & 155.2 & 1,029 & 16.7\\
Attention~\citep{vaswani2017attention} & 43.1 & 5.2 & 188.4 & 1,250 & 18.3 \\
CSWindow~\citep{dong2022cswin}& 43.1 & 13.7 & 144.6 & 990 & 12.9\\
SimplingAtte~\citep{he2023simplifying} & 43.3 & 12.2 & 126.3 & 994 & 17.1\\
DeformableAtt~\citep{xia2022vision} & 43.7 & 13.5 & 166.0 & 988 & 15.2 \\
\cdashline{1-6}[0.8pt/2pt]
MiniAdapters~\citep{marouf2024mini} & 41.9 & 15.0 & 131.8 & 995 & 12.2 \\
VL-Adapter~\citep{sung2022vl} & 42.7 & 14.5 & 167.2 & 993  & 14.0\\
\cellcolor[gray]{.95}\textbf{META-B$_{{\textrm{(Ours)}}}$} & \cellcolor[gray]{.95}44.3 & \cellcolor[gray]{.95}17.4 & \cellcolor[gray]{.95}115.3 & \cellcolor[gray]{.95}720 & \cellcolor[gray]{.95}8.1\\
\hline \hline 
\end{tabular}
\caption{Result comparisons with different adapters.}
\label{tab6}}
\end{table}
% -------------------------------

% -------------------------------------------
\section{Visualizations under the Conv branch}
\label{secS6}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~3.2 of the main paper.}}} In this section, to observe if the adapter has learned local inductive biases through the Conv branch, we visualize the model's class activation maps. The obtained visualizations are given in Figure~\ref{figs1}. From this figure, it can be observed that after adding the Conv branch, the model focuses more on the specific object area (\eg,`` the dog'' and ``the person'') rather than the surrounding area that may extend beyond the object itself, as was the case before adding the Conv branch. This indicates that our method effectively learns local inductive biases after incorporating the Conv branch.
% -------------------------------------------
\input{figures/figure2}
% -------------------------------------------

% -------------------------------------------
\section{Qualitative Visualization results}
\label{secS7}
% -------------------------------------------
{\color{red}{\emph{This supplementary is for Section~4.2 and~4.3 of the main paper.}}} In this section, we show qualitative results on both instance segmentation and semantic segmentation. To demonstrate the superiority of our method, we present visualization results of ablation studies on instance segmentation, as well as comparisons with state-of-the-art methods on both instance segmentation and semantic segmentation. 
% 
The obtained visualization results are shown in Figure~\ref{figs2}. From the results, it can be observed that compared to other methods, our method can achieve more accurate object masks that better fit the actual boundaries of the objects themselves.
% -------------------------------------------
\input{figures/figure3}
% -------------------------------------------

as well as the pseudo-code for when the stripe size is set to $2$ in Section~\ref{secS8}. 
% -------------------------------------------
\section{Pseudo-code fo stripe size = $2$}
\label{secS8}
% -------------------------------------------
In this code snippet, stripe size is set to 2, and relevant features are directly obtained using the gather function instead of reshaping them with img2windows. This operation can reduce unnecessary reshaping operations and improves the efficiency of the code.
% -------------------------------------------
\begin{python}
function cross_shaped_window_attention(x, num_heads, window_size):
    # x: given feature
    # num_heads: head number
    # window_size: window size

    # Get dimensions
    (batch_size, seq_length, d_model) = shape(x)

    # Split into multiple heads
    Q, K, V = split_heads(x, num_heads)

    # Initialize attention output
    attention_output = zeros(batch_size, seq_length, d_model)

    # Initialize previous head's output for cascaded attention
    previous_Q = zeros(batch_size, seq_length, d_model)
    previous_K = zeros(batch_size, seq_length, d_model)
    previous_V = zeros(batch_size, seq_length, d_model)

    # Calculate attention for each head
    for head in range(num_heads):
        for position in range(seq_length):
            # Get cross-shaped window indices
            window_indices = get_cross_shaped_window_indices(position, window_size)

            # Gather Q, K, V for the current window
            Q_window = gather(Q[head], window_indices)
            K_window = gather(K[head], window_indices)
            V_window = gather(V[head], window_indices)

            # Incorporate previous head's output for cascaded attention
            if head > 0:
                Q_window += previous_Q
                K_window += previous_K
                V_window += previous_V

            # Calculate attention scores
            attention_scores = softmax(Q_window * K_window^T / sqrt(d_k))

            # Compute the attention output for the current position
            attention_output[position] = attention_scores * V_window

        # Update previous head's output for the next head
        previous_Q = Q_window
        previous_K = K_window
        previous_V = V_window

    # Final linear transformation
    attention_output = linear_transform(attention_output)
    return attention_output

function feed_forward_network(x):
    # Feed Forward Network
    x = ReLU(linear(x))
    x = linear(x)
    return x
\end{python}

\begin{python}
def get_cross_shaped_window_indices(position, window_size, seq_length):
    # Initialize the list of indices
    indices = []

    # Add the current position
    indices.append(position)

    # Add vertical neighbors (up and down)
    for offset in range(-window_size, window_size + 1):
        if position + offset >= 0 and position + offset < seq_length:
            indices.append(position + offset)

    # Remove duplicates and sort the indices
    indices = list(set(indices))
    indices.sort()

    return indices
\end{python}
% ----------------------------------------------
