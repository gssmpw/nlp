\documentclass[twoside,11pt]{article}

\usepackage{blindtext}

% Acknowledgements: This template is almost identical to that of Journal of Machine Learning Research https://github.com/JmlrOrg/jmlr-style-file
% Any additional packages needed should be included after dmlr.
% Note that dmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.dmlr.org/format/natbib.pdf

% Available options for package dmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove DMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%
% \usepackage[abbrvbib, preprint]{dmlr2e}

\usepackage{dmlr2e}

\usepackage{hyperref}
\usepackage{url}
\usepackage{comment}
\usepackage{float}
%\usepackage{svg}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{dsfont}
\usepackage{amsmath}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}

%TODO
\usepackage{lastpage}
\dmlrheading{2}{2025}{1-\pageref{LastPage}}{1/25; Revised ?/25}{02/01}{21-0000}{Arndt, Isil, Detzel, Samek, Ma } % Insert the dates and author names. This is not relevant if it is yet being reviewed, i.e., \documentclass[twoside,11pt, preprint]{article}
%\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version
\def\openreview{ TBD } %TODO
% Short headings should be running head and authors last names

\ShortHeadings{Spatio-Temporal datasets on Graphs using PDEs}{Arndt, Isil, Detzel, Samek and Ma}
\firstpageno{1}

\begin{document}

%\title{Synthetic Spatio-Temporal Graph Datasets}

%  title at NeurIPS
%\title{Simulating Temporal Propagation on Graphs using PDEs} 

% title at ICLR
\title{Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs}

\author{\name Jost Arndt$^1$ \email jost.arndt@hhi.fraunhofer.de \\
       \AND
       \name Utku Isil$^1$ \email utku.isil@hhi.fraunhofer.de \\
       %\addr Department of AI\\
       %Fraunhofer HHI\\
       %Berlin, Germany
       \AND
       \name Michael Detzel$^1$ \email michael.detzel@hhi.fraunhofer.de \\
       \AND
       \name Wojciech Samek$^{1,2,3}$ \email wojciech.samek@hhi.fraunhofer.de \\
       \AND
       \name Jackie Ma$^1$ \email jackie.ma@hhi.fraunhofer.de \\
       \AND
       \addr $^1$ Department of Artifical Intelligence, Fraunhofer Heinrich Hertz Institut, Berlin \\
       $^2$ Department of Electrical Engineering and Computer Science, TU Berlin \\
       $^3$ BIFOLD – Berlin Institute for the Foundations of Learning and Data
       }

\editor{Currently Under review}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
Many physical processes can be expressed through partial differential equations (PDEs). Real-world measurements of such processes are often collected at irregularly distributed points in space, which can be effectively represented as graphs; however, there are currently only a few existing datasets. Our work aims to make advancements in the field of PDE-modeling accessible to the temporal graph machine learning community, while addressing the data scarcity problem, by creating and utilizing datasets based on PDEs.
In this work, we create and use synthetic datasets based on PDEs to support spatio-temporal graph modeling in machine learning for different applications. More precisely, we showcase three equations to model different types of disasters and hazards in the fields of epidemiology, atmospheric particles, and tsunami waves. Further, we show how such created datasets can be used by benchmarking several machine learning models on the epidemiological dataset. Additionally, we show how pre-training on this dataset can improve model performance on real-world epidemiological data. 
The presented methods enable others to create datasets and benchmarks customized to individual requirements. The source code for our methodology and the three created datasets can be found on \href{https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs}{github.com/github-usr-ano/Temporal\_Graph\_Data\_PDEs}.
\end{abstract}



\begin{keywords}
  Graph, Spatio-Temporal, PDE, Epidemiology
\end{keywords}

\section{Introduction}
Partial differential equations (PDEs) describe
numerous scientific processes, and  many of these processes are of spatio-temporal nature, for instance fluid dynamics, chemical diffusion, financial mathematics, climate modeling, biomechanics, or seismology. Over the past centuries, considerable research efforts have been dedicated to advancing mathematical modeling with PDEs. Although there exists a significant amount of work on PDEs and their solutions~\citep{evans}, the practical implementation of a PDE solver remains a demanding task. Prominent ML research focuses on the study of PDEs itself, such as ~\citet{RAISSI2019686, FNO}. To support this research, some public datasets for machine learning (ML) on PDE-generate data already exists~\citep{PDEBENCH}, although they have some limitations that we will address later.

Graphs are a powerful tool for describing complex geometric structures in real-world scenarios, such as surfaces, volumes, or sensor networks. Recently, there have been notable developments in specifically spatio-temporal graph modeling within the field of machine learning~\citep{GTSsurvey}. %  (TODO more sources?).%maybe add https://arxiv.org/abs/2006.10637v3
One big obstacle for benchmarking, comparing, and for further developments, is the availability of spatio-temporal data. 
Only few datasets for spatio-temporal graph machine learning can be found in literature, such as traffic data from the \textit{California PeMS system}~\citep{PEMS, Pems2, PEMS3} in different settings, or COVID-19 data~\citet{equivalence_temporal_graph}. For some benchmarking applications~\citet{TGB, TGB2} offer different datasets of interest. Although the existent datasets are valuable for specific tasks, they have limitations in quality (high noise), scope (few samples), application (topic), accessibility (unpublished), and adaptability (high individual pre-processing effort). Thus, we identify a lack of larger high-quality datasets for different tasks. These limitations hinder the development and comprehensive benchmarking of spatio-temporal graph architectures for specific applications and diverse tasks. Therefore, large and more diverse high-quality datasets that are freely accessible are crucial. 

%Despite the availability of some datasources are available for spatio-temporal graph machine learning, such as traffic data from the \textit{California PeMS system}~\citep{PEMS, Pems2, PEMS3} in different settings. Also, COVID-19 data has been used in e.g.~\citet{equivalence_temporal_graph}. Also, for some benchmarking applications~\citet{TGB, TGB2} offer different datasets of interest. Although the existent datasets are valuable for specific tasks, they have limitations in quality (high noise), scope (few samples), application (topic), accessibility (unpublished), and adaptability (high individual pre-processing effort). Thus, we identify a lack of larger high-quality datasets for different tasks. These limitations hinder the development and comprehensive benchmarking of spatio-temporal graph architectures for specific applications and diverse tasks. Therefore, large and more diverse high-quality datasets that are freely accessible are crucial. 

To overcome these limitations, synthetic data is a promising ressource.
The utilization of synthetic data allows the fast creation and adaption of datasets, while avoiding barriers regarding privacy or data availability. Furthermore, synthetic data is less error-prone regarding measurement flaws and easier to control, as all involved dynamics are known in contrast to real-world data. Some examples of (partially) synthetic datasets that find application in machine learning are extensively used weather datasets~\citep{ERA5}, turbulence data~\citep{Turbulence_dataset}, PDE data \citep{PDEBENCH} or ground motion data~\citep{HEMEW3D}. 

This paper builds on the idea to utilize synthetic data and makes the significant developments from past PDE research accessible to the spatio-temporal graph learning community. We outline a method for generating synthetic datasets based on PDEs, and provide three spatio-temporal datasets ready to use, alongside with code. To this end, we solve three exemplary PDEs related to different types of disaster, each describing the movement of entities over space and time. We solve these PDEs on an irregular domain, and evaluate them on irregularly distributed points on this domain, as this is the realistic scenario for the presented applications. 
Subsequently, we form time-dependent graphs from the obtained values in combination with the underlying spatial structure, to which we grant researchers direct access. To demonstrate the utility of our synthetic data, we lastly present a benchmarking of prominent spatio-temporal prediction models on the epidemiological data. We further underline the impact and importance of our synthetic data by successfully transferring knowledge onto real-world data.




%\subsection{Our Contributions}
Our key \textbf{contributions} can be summarized as follows: 
% For reference of design, checkout https://openreview.net/pdf?id=fU8H4lzkIm
\begin{itemize}
    \item This paper presents three datasets, which constitute the first collection of PDE-based spatio-temporal datasets specifically designed for graphs.
    \item Through the detailed publication of code along with our method, we strongly encourage other practitioners to adapt our method. To ensure flexibility regarding sampling rate, domain (region), graph, dynamic, boundary condition, and PDE, we utilize the finite element method (FEM). We facilitate other temporal graph ML researchers to integrate PDE knowledge into their specific field and application.
    \item To our knowledge, our work provides the first numerical solution of this (or similar) epidemiological PDE, connecting the well-known SIR-ODE with spatial diffusion.
    \item To our knowledge, the epidemiological dataset constitutes the largest high-quality public epidemiological spatio-temporal graph dataset of this kind. This dataset allows us to offer new reproducible benchmarking results and give other practitioners the opportunity to develop enhanced models on our publicly available high-quality datasets, generated through a controlled setting.
    \item First pre-training of epidemiological spatio-temporal graph models on synthetic data and transfer-learning onto real-world data. This marks a significant step towards foundational modeling. Performance increase of up to $45\%$ in our experimental setup.
\end{itemize}

%\item We propose the RNN-GNN-Fusion model, as the best performing model from our benchmarking.
Our work differs from the aforementioned~\citep{PDEBENCH} not only through the selected PDEs, but also by the fundamental technique, the FEM and adaptability. Specifically, the FEM allows for complex domains with complex boundary conditions. Also, our overall outcome is a temporal graph, not a grid. 
Further, we notably emphasize the interchangeability of the (complex) domain, dynamics, or underlying equation throughout our published code and method. Our work facilitates the adaption of our code for individual needs and applications.







\section{Temporal PDEs to Simulate Spatio-Temporal Movement}\label{sec:temp_pde}


To simulate propagations through space and time through three exemplary PDEs, we first generally outline the numerical approximation of the solution. In Section~\ref{sec:data_gen}, we describe the generations of the datasets more explicitly.

\paragraph{Overview of Our PDEs}
The\textbf{ first equation} we employ is inspired by the structure of real-world epidemiological data (e.g. COVID-19 incidences), which suggests the use of spatio-temporal graph machine learning~\citep{HySonLab, equivalence_temporal_graph}. We create a comparably large graph dataset containing long records of infectious data with different epidemiological settings but consistent measurements over time, absence of noise, and unconstrained accessibility to all determining information.
This or any similar epidemiological PDE, based on the SIR-ODE~\citep{SIR}, has never been solved numerically. Especially, there are no accessible simulations for other practitioners.
Furthermore, we want to enable epidemiological researchers, to use individual spatial domains (e.g. country) by adapting our published code to specifically train and test models on other geometries. Additionally, a broad collection of similar PDEs with slight adaptations can easily extend the modeling by features such as vaccination status~\citep{SIRV}, exposed~\citep{seir}, or disease vectors~\citep{malaria}.
Naturally, similar adaptations can also be applied to the following two equations.


The \textbf{second equation}, an advection-diffusion (or convection-diffusion) equation, models the movement of particles (e.g. dust, nuclear fallout, smoke) or other quantities in the atmosphere.
%Secondly, we model the movement of particles or other quantities (e.g. sand, nuclear fallout, smoke) in the atmosphere, with an Advection-diffusion (or Convection-diffusion) equation. 
The arrangement as a spatio-temporal graph as the underlying datatype is canonical, as real-life measurements of those quantities with a sensor network likewise have an irregular geometry. 
Besides a further benchmarking of temporal graphs, the creation of such a dataset could be used to pre-train either risk-prediction models or to pre-train or test specific parts of ML-based weather models such as ClimODE~\citep{ClimODE}.

As a \textbf{third PDE}, we choose the wave equation, which is very common in physics and describes, for example, water, sound, or electromagnetism.
In our case, we model a tsunami wave approaching some coastline, where an interesting task could be to predict the next few steps of the wave.



\paragraph{General setting}
We solve the PDEs on (different) 2D-domains $\Omega \subset \mathbb{R}^2$ for the three proposed scenarios. The PDEs are time-dependent on an interval $[0,T]$, and their solutions $\nu$ are of dimension $d\in\{1,2\}$, depending on the equation. Generally, our PDEs have the form $\frac{\partial \nu}{\partial t} = F(\nu, \nabla \nu, \Delta \nu)$ with some additional boundary conditions. $F$ is some (general) functional which will be specified later, $\Delta$ the Laplace operator and $\nabla $ the Gradient, both only along the spatial dimensions.

The solution $\nu: [0,T] \times \Omega \rightarrow \mathbb{R}^d$ will be evaluated on a set of points within the domain $\Omega$. These points mimic geographical regions and their administrative units, or any unevenly distributed network of sensors. We equip this set of points with adjacencies and inverse distances to create a spatio-temporal graph dataset.

Our method is applicable to an even broader class of PDEs than those described here (e.g., for \(d > 2\)). It not only facilitates the exchange and extension of the domain or parameters, but can also be seamlessly extended to PDEs for which the FEM is effective, such as elasticity-, (Navier-)Stokes-, Euler-, Schrödinger-, and Burgers equations, for which numerous implementations are available.

\paragraph{Numerical solution sketch}
In the following paragraphs, we describe the numerical approximation of different time-dependent PDEs. This part can be skipped by a reader looking for a spatio-temporal graph dataset but does not want to bother with the details of its generation.
However, to understand and adapt the published code and the data generation process, a short sketch of the methods is inevitable to us and important to a reader looking to expand our set of equations and parameters. Note, that although the outlined methodology can be applied to countless processes and PDEs, the complexity of the synthesized data is limited by the amount and complexity of given dynamics.

We solve the equations iteratively in the time domain and solve the occurring PDE for each time-step with the FEM, this order is called Rothe method.
We therefore first discretize alongside the temporal dimension with an Euler scheme, i.e. approximate the time-derivative with a difference quotient.
We use the Crank-Nicolson method, which is a mixture of the implicit and explicit Euler scheme for time-stepping. With $\theta = 0.5$, a small temporal Euler stepsize $h$, and $\nu = \nu (x,t) $ and $ \nu_+ = \nu (x, t+h)$  the approximation has the form 
\begin{equation}
    \frac{\nu_+ - \nu}{h } = \theta F(\nu, \nabla \nu, \Delta \nu) + (1-\theta) F(\nu_+, \nabla \nu_+, \Delta \nu_+).
\end{equation}
Since for the timestep $t$ the solution $\nu$ is known, we thereby obtain a PDE that is not time-dependent anymore. We solve this PDE for $\nu^+$ with the FEM, which is the most flexible, reliable, and therefore standard numerical approach on PDEs. We only give a sketch of the FEM here, as details can be found in numerical textbooks for PDEs. The FEM discretizes the domain $\Omega$ into a mesh, on which a set of test functions $\phi$ as well as trial functions $\tilde{\nu}$ (approximation of the solution $\nu$) are defined. Throughout this work, we use first-order (linear) Lagrange Elements as test and trial functions. 
To obtain the so-called weak formulation, we rearrange the PDE into an implicit and an explicit part, then multiply it from the left with the test functions and integrate over the full domain. This forms an inner product, which allows an underlying Sobolev space to have the structure of a Hilbert space. We will not dive deeper into this, but use the notation of an inner product $ \langle \phi , \nu  \rangle = \int_\Omega \phi \nu \,dx $ for a better readability. 

Replacing additionally $\nu , \nu_+$ with the corresponding trial functions $\tilde{\nu}, \tilde{\nu_{+}}$, we obtain the formulation:
\begin{equation*}
    \langle \phi  , \tilde{\nu}_+ \rangle  - h (1-\theta) \langle \phi , F(\tilde{\nu}_+, \nabla\tilde{\nu}_+, \Delta \tilde{\nu}_+) \rangle
= \langle \phi ,  \tilde{\nu} \rangle  + h \theta \langle \phi ,  F(\tilde{\nu}, \nabla \tilde{\nu}, \Delta \tilde{\nu}) \rangle.
\end{equation*}
The left-hand-side (implicit side) can be written as $A (\Tilde{\nu}_+)$, the right hand (explicit side) as $b$. We then seek to solve $A (\Tilde{\nu}_+) = b$ for (the parameters of) $\tilde{\nu}_+$.
Note, that technically $\phi$ is a vector of test functions, but we wanted to keep the notation simple. %We will be more explicit in the individual subsections. 

One often employed identity for compact $\phi$ is $\langle \phi, \Delta \nu \rangle = - \langle \nabla \phi, \nabla \nu \rangle$ due to integration by parts, which we apply when Dirichlet or zero-flow Neumann boundary values are imposed. Note that we omit plenty of information on which space this inner product is defined and details of such transformations for the sake of simplicity, but again refer to any textbook covering the FEM.



\subsection{SI-Diffusion Equation}

\begin{comment}
\end{comment}
To model the spatio-temporal spread of infectious diseases, we consider the following PDE from~\citet{murray2003}   


\begin{equation}
\label{eq:si}
\begin{split}
    \frac{\partial S}{\partial t} &= - r I S  + D\Delta S,
    \\
    \frac{\partial I}{\partial t} &= rI S - \alpha I + D \Delta I,
\end{split}
\end{equation}
where $\Delta$ denotes the Laplace operator and the functions $S(x,t),I(x,t)$ describe the densities of the susceptible and infected population over space and time. 

The functions $\alpha (x,t), r (x,t),$ and $D (x,t)$ in the equation represent dynamics rising from pathogens and the population. More precisely, $r$ describes the transmission rate of the disease,  $\alpha$ describes the duration of the disease, and $D$ describes the speed of the diffusion, i.e. movement of the population.
Note that by setting $D=0$ one receives the underlying SI compartment ODE, while setting $r=\alpha=0$ leads to two time-dependent heat equations. Many adaptions of the underlying SI compartment ODE~\citep{seir, malaria} can easily be integrated into our framework.

As boundary condition, we chose mixed boundary conditions: usually, we impose zero-flow Neumann boundary conditions. However, to start a wave of infections, we impose small positive Dirichlet boundary conditions for a small part of the boundary for a limited time.
Since this PDE is a system of two equations, the weak formulation utilizes two test functions $\phi_1, \phi_2$ from the same set of functions.
The left-hand-side of the weak formulation, named $A$ above, here takes the form 
\begin{align*}
    A(\Tilde{S}_+,\Tilde{I}_+) = \langle &\phi_1 , \Tilde{S}_+\rangle  + (1-\theta) h_k \langle \phi_1 , r \Tilde{S}_+\Tilde{I}_+ \rangle 
+ (1-\theta) h_k \langle \nabla \phi_1 , + D\nabla S_+ \rangle \\
&+  \langle \phi_2  , \Tilde{I}_+ \rangle  - (1-\theta) h_k \langle\phi_2 , r\Tilde{S}_+\Tilde{I}_+ - \alpha \Tilde{I}_+ \rangle
+ (1-\theta) h_k \langle \nabla \phi_2 , D\nabla  \Tilde{I}_+\rangle
\end{align*}
while the right-hand-side $b$ is defined by the following
\begin{align*}
    b = \langle& \phi_1 , \Tilde{S}\rangle  - \theta h_k \langle \phi_1 , r \Tilde{S}\Tilde{I} \rangle 
- \theta h_k \langle \nabla \phi_1 , + D\nabla \Tilde{S} \rangle \\
&+  \langle \phi_2  , \Tilde{I} \rangle  + \theta h_k \langle\phi_2 , r\Tilde{S}\Tilde{I} - \alpha \Tilde{I} \rangle
-\theta h_k \langle \nabla \phi_2 ,  D\nabla \Tilde{I}\rangle.
\end{align*}
Due to the product $\Tilde{S}_+,\Tilde{I}_+$, the equation $A(\Tilde{S}_+, \Tilde{I}_+) = b$ depends nonlinearly on (the parameters of) the trial functions $\Tilde{S}_+,\Tilde{I}_+$. To solve this equation for (the parameters of) $\Tilde{S}_+,\Tilde{I}_+$, the Newton method is a standard choice, which we used. The Newton method solves this system iteratively, i.e. starts with $\Tilde{S}_{+,1}, \Tilde{I}_{+,1}$ and calculates $\Tilde{S}_{+,2}, \Tilde{I}_{+,2}$ until a convergence criterion is reached. 
The updates $\delta_S, \delta_I$ to $\Tilde{S}_{+,i}, \Tilde{I}_{+,i}$ are calculated as the solution of the linear system $J_A(\Tilde{S}_{+,i}, \Tilde{I}_{+,i}) \delta = - A(\Tilde{S}_{+,i}, \Tilde{I}_{+,i})+b$,
where $J_A$ is the Jacobian matrix of $A(\Tilde{S}_{+,i}, \Tilde{I}_{+,i})$. The differentiation to set up $J_A$ is handled automatically.


\subsection{Advection-Diffusion Equation}

The Advection-diffusion equation takes the form
\begin{equation}\label{eq:ad}
\frac{\partial u}{\partial t} =  - \beta \cdot \nabla u+ \alpha \Delta u +s, 
\end{equation}
with $u = u(x,t) \in \mathbb{R}$ being a measurement of the quantities density and the vector field $\beta = \beta(x,t) \in \mathbb{R}^2$ being the velocity field of e.g. wind over our domain. The diffusion coefficient $\alpha = \alpha(x,t)$ controls the diffusive spread of $u$ and $s = s(x,t)$ is a source term, with $s_+ = s(x, t+h)$.

As boundary condition, we assume Dirichlet boundary conditions $u \vert _ {\partial \Omega } = 0 $.

The explicit part, using integration by parts, takes the form:
\begin{equation*}
    b = \langle \phi,\Tilde{u} \rangle + h\theta \langle \phi, \beta \cdot \nabla \Tilde{u} \rangle + h\theta \alpha \langle \nabla \phi, \nabla \Tilde{u} \rangle - h\theta \langle \phi, s\rangle -h(1-\theta)\langle \phi, s_+\rangle 
\end{equation*}
and the implicit part
\begin{equation*}
A (\Tilde{u}_+) = \langle \phi ,\Tilde{u}_+\rangle - h(1-\theta) \langle \phi , \beta \cdot \nabla \Tilde{u}_+\rangle  -h(1-\theta) \alpha \langle \nabla \phi, \nabla \Tilde{u}_+\rangle
\end{equation*}
that is linear in (the parameters of) $\Tilde{u}_+$, such that we can solve a linear system
\begin{equation*}
    A \Tilde{u}_+ = b.
\end{equation*}






\subsection{Wave Equation}

The wave equation is a prototype time-dependent PDE, in which we added a damping term to damp simulated waves over time. The wave equation we will use takes the following form
\begin{equation}
\label{eq:wave}
    \frac{\partial ^2 u}{\partial t ^2} + b \frac{\partial u}{\partial t} =  \Delta u,  \quad   u(x,0) = 0.
\end{equation}
%\begin{align*}
%    \frac{\partial ^2 u}{\partial t ^2} &=  \Delta u, \\
%    u(x,0) &= 0.
%\end{align*}
On the boundary of the domain we impose Robin boundary conditions $\alpha u + \beta \eta \cdot \nabla u = 0$, but with a local initial disturbance to start a wave that mimics an initial tsunami-wave. Due to the second derivative in the time domain, the equation doesn't suit our opening definition, but can easily be extended into a suitable system of PDEs:
\begin{equation*}
    \frac{\partial u}{\partial t} = v,  \quad \frac{\partial v}{\partial t} + b \frac{\partial u}{\partial t} = \Delta u.
\end{equation*}

By replacing the time-derivatives, using the trial functions $\Tilde{u}, \Tilde{v}$ in the first equation, we obtain an implicit and explicit side again 
\begin{equation*}
    \begin{split}
        (1 - (1-\theta )^2 h^2 \Delta +h (1-\Theta) b) \Tilde{u}_+ &= (1+ (1-\Theta) h b + \theta (1-\theta) h^2 \Delta \Tilde{u} + h \Tilde{v} \\
        \Tilde{v}_+ &= h \Delta ( (1-\theta) \Tilde{u}_+ + \theta \Tilde{u} ) +b \Tilde{u} - b \Tilde{u}_+ + \Tilde{v} + ,
    \end{split}
\end{equation*}


from which the weak formulation can be derived easily. We solve the first equation (linear) for (the parameters of) $\Tilde{u}_+$, and receive (the parameters of) $\Tilde{v}_+$ straightforward from the second equation.\footnote{The methodology we use here for an undamped ($b = 0$) wave equation, as well as similar numerical code, can be found in this tutorial of the used library \href{https://www.dealii.org/current/doxygen/deal.II/step_23.html}{https://www.dealii.org/current/doxygen/deal.II/step\_23.html}. We made only smaller adaptions to the code.}

% We impose Dirichlet Boundary conditions on the "coastline" $\Gamma_1 \subset \partial \Omega$ % of the domain
% $$
% u(x, t) = 0, x \in \Gamma_1
% $$
% to reflect the waves and keep the rest of the domains boundary (e.g. open water) unconstrained. (TODO well-defined?).







\section{Dataset Creation}\label{sec:data_gen}
To compute the solutions of the three presented PDEs, we generate a mesh of the domain $\Omega$ with the meshing software \textit{gmsh}~\citep{gmsh}. The implementation of the FEM is done with \textit{deal.ii}~\citep{dealII95}, which is written in \textit{C++} and abstracted the full implementation around the FEM, but this can be done with any other FEM library. 

As an underlying domain for both the epidemiological PDE and the advection-diffusion PDE, we use the shape of Germany. We evaluate the solutions of the PDEs on a set of points that have the coordinates of administrative regions in Germany: $400$ \textit{NUTS-3} regions. The intuition is the reporting of COVID-19 cases in the same spatial granularity. We additionally manually create an adjacency list of neighboring regions and weight them with the inverse of the distance of the centers of the regions,
More details on this can be found in the appendix~\ref{sec:detail:dataset}. We ultimately have created a graph
\begin{equation*}
    \mathcal{G} = ((V, E, X_i))_{i=1,..., N}
\end{equation*}
with $400=\vert V \vert $ nodes, $2088 = \vert E \vert$ edges and edge features $X_i \in \mathbb{R}^d$ for every time-step $i$. The mesh for the FEM computation and the resulting graph can be seen in Figure~\ref{fig:mesh_ger}. Note, that through the use of the FEM our method is \textbf{not} limited to fixed graphs. The values of $X_i$ are created by evaluating the solution $\nu(x,t)$ on the points of $V$. No changes to the PDE solver needs to be made, to evaluate the solution on different locations every time-step. However, to correspond to real world data, we chose a fix $V$. On our GitHub we provide further explanations on how to increase and change the size of the graph $V$ with minimal effort. Note that no recomputation of $\nu(x,t)$ is needed to create larger graphs, and the evaluation of $\nu$ can easily be parallelized, thus our method is suitable to create also very large graphs.

\begin{figure}[H]
  \centering
  %\includegraphics[width = 3cm]{img/grid-1.svg}
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \centering
    \subfloat{{\includegraphics[width=2.8cm]{img/germany_grid.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=2.8cm]{img/germany_net.png} }}%
    \qquad
    \subfloat{{\includegraphics[width=2.8cm]{img/germany_net_reshaped.png} }}%
    \caption{The mesh on the domain for the SI-diffusion equation and the Advection-diffusion equation (left). Independent of this mesh, we build a Graph using administrative regions (\textit{NUTS-3}), their adjacencies, and the distances of the centers (center). Note, that we used the underlying geo-coordinates to plot this  graph (center), while the later used and constructed graph has no trivial unique representation (right).}%
    \label{fig:mesh_ger}%
\end{figure}

For the wave equation, the domain $\Omega $ is an imaginary (similar to the German) coastline, and the set of points on which we evaluate the solution of the wave equation are chosen randomly. Their respective adjacencies are selected with a Delaunay triangulation, and equipped with inverse distances to form a graph $\mathcal{G}$ with $325=\vert V \vert $ nodes and  $1858=\vert E \vert $ edges.
The mesh and the resulting graph can be seen in Figure~\ref{fig:mesh_coast}.
\begin{figure}[H]
  \centering
  \centering
    \subfloat{{\includegraphics[width=3.5cm]{img/coastline_mesh.png} }}%
    \qquad    \subfloat{{\includegraphics[width=3.5cm]{img/coastline_grid.png} }}%
    \caption{The mesh on the domain for the Wave Equation. On the same domain we generate a set of $325$ random points and generate a Delaunay triangulation.}%
    \label{fig:mesh_coast}%
\end{figure}
%Note that on the domains, the points on which we evaluate the solutions are not identical to the mesh used for the FEM discretization.



\paragraph{SI-diffusion equation}
We set the parameter $\alpha = 0.22$, the time-weighting parameter $\Theta =0.5$ effectively resulting in a Crank-Nicolson scheme, the Euler stepsize $h_k = 1$, and a Newton stepsize $\eta = 0.3 $. The underlying domain $\Omega$ can be seen in Figure \ref{fig:mesh_ger}.

We generated $25$ scenarios, each with a length of $364$ time-steps. For every simulation we varied the parameters $r, D$, they can be found in the supplementary material in \ref{sec:detail:dataset} and Table~\ref{tab:exp_id}, leading to a dataset of shape $[9100, 400, 2]$ for [time-steps, nodes, features] in $5$:$27$ hours (wall clock) of computation. Note, that two features exist due to $2=d=\vert \{S,I\} \vert$.



\begin{figure}[H]
    \centering
    \includegraphics[width=13.5cm]{img/Graph-image-2.png}
    \caption{Plot of one scenario of an infection from the diffusion SI-Equation. The Infection wave propagates over the nodes.}
    \label{fig:SI_graph}
\end{figure}



\paragraph{Advection-diffusion equation}
For the Advection-diffusion equation, we simulated 54 different parameters right after one another, each with a length of $80$ time-steps. Throughout the simulation, we keep $\alpha=-1$ constant and also change $\beta (x,t)$ only every $80$ time-steps.
For the source term $s(x,t)$ we define a small constant support $\Omega_{s(x,t)} \subset \Omega $ for $s$, such that
\begin{equation*}
s(x,t) = \begin{cases}
      S(t) & \text{$x \in \Omega_{s(x,t)}$}\\
      0 & \text{$x \notin \Omega_{s(x,t)} $}
    \end{cases}
\end{equation*}
where $\Omega_{s(x,t)}$ is a rectangle within the domain. 
The choice of $h, T, S(t), \beta(x,t), \Omega_{s(x,t)}$ and further details can be found in the supplementary material~\ref{subsect:details_AD} Table~\ref{tab:exp_id_adv_diff}, resulting in a dataset of shape $[4320, 400, 1]$, for [time-steps, nodes, features] in $0$:$22$h. A  spatio-temporal visualization can be found in~\ref{fig:ad_graph}.


\begin{figure}[H]
    \centering
    \includegraphics[width=13.5cm]{img/graph_image_advection_diffusion.png}
    \caption{Plot of some advection-field on the left side and an exemplary plot of the solution of the advection-diffusion equation over 4 time-steps.}
    \label{fig:ad_graph}
\end{figure}



\paragraph{Wave equation}
For the wave equation, we simulated one consecutive simulation in which we simulated two tsunami waves from different starting points with an initial amplitude of $1$. 
We set $T=2700000$ and $h=100000 / 64$, resulting in a dataset of shape $[1858, 325,1]$, for [time-steps, nodes, features] in $0$:$20$h. Details can be found in the appendix~\ref{sec:supp_mat_wave}.

\begin{figure}[H]
    \centering
    \includegraphics[width=14.5cm]{img/graph_image_wave_equation.png}
    \caption{Plot of one scenario of an infection from the Wave-equation.}
    \label{fig:wave_graph}
\end{figure}
 
\paragraph{Real-world epidemiological datasets}
Additionally to the synthetic datasets we created above, we want to compare the epidemiological dataset to real-world data. To do we create three real world datasets:
German COVID-19, German Influenza, and Brazilian COVID-19.
The numbers of infected people were taken from public sources. The German numbers were connected with the graph $\mathcal{G}$ from the SI-diffusion equation, for the Brazilian data we created a new graph $\mathcal{G}$ analogously by using geographic data of the regions. Details can be found in the appendix, section~\ref{sec:appdx:real_world_dataset}.











\section{Application of Synthetic Data}\label{sec:examples}
%Example Applications on the Datasets
To showcase the utility of our created datasets, we conduct several machine learning experiments in this section.
We first define several interesting (spatio-) temporal machine learning architectures in section~\ref{sec:models}. In section~\ref{sec:benchmarking_definition} we define three different benchmarking scenarios, to compare the performance of the previously defined machine learning models. 
To demonstrate that the usage of our data exceeds pure theoretical studies, we additionally perform experiments of transfer-learning from our synthetic data onto three real-world datasets from epidemiology in section~\ref{sec:transfer_learning}. 



\subsection{(Spatio-) Temporal Models}\label{sec:models}
We want to test solely data-centric models and do not take any knowledge of the underlying PDE into account, such as its structure or parameters, as done by machine learning-based approaches to solve PDEs~\citep{FNO, RAISSI2019686}. 

We test both temporal and spatio-temporal models. More specific details can be found in the supplementary material~\ref{appendix:models} and the published material online.
The presented examples naturally do not take all possible models into account, but offer a good overview. In particular models specialized for traffic data such as GraphWaveNet~\citep{GraphWaveNet} or STG-ODE~\citep{stgode} are not taken into account.
Besides their specialization in traffic data, they have another drawback as they only operate on a single predefined graph, embedded in the architecture of the models. This makes transfer-learning or generalization onto unseen graphs (or domains) impossible, which will be carried out in section~\ref{sec:transfer_learning}.

\textbf{Repetition} The repetition model is a naive baseline that simply repeats the last given value.

\textbf{RNN}
Recurrent-Neural Networks based on GRUs~\citep{GRU} have proven to be successful for sequence prediction. During training, teacher forcing is applied. During validation forecasts are produced autoregressively.

\textbf{TST} Time Series Transformer (TST)  is a Transformer-based~\citep{vaswani2023attention} architecture designed for temporal forecasting. It utilizes encoder and decoder layers to capture temporal dependencies, and a positional encoding depending purely on the time-step. During training, teacher forcing is applied. During validation forecasts are produced autoregressively.

\textbf{MP-PDE}
Given that the underlying data is based on solutions of a PDE, we also test a model based on a Message-Passing PDE-solver (MP-PDE) from~\citet{brandstetter2022message}, but will not pass any information of the underlying PDE to our model. The model consists of an encoder, a processor, and a decoder. The encoder creates node-wise embeddings of the context data. The processor consists of an MP-GNN, operating on a single graph with embedded features. The decoder is a 1D convolution applied node-wise, and a special update rule, that propagates the last value through the next time-step.

\textbf{RNN-GNN-Fusion}
Motivated by the PDE itself, we aimed to separate the time from the space
dimensions by building an RNN (analogously to the abovementioned model) to encode the underlying ODE and an MP-GNN to emulate the diffusion. RNN and GNN run parallel, their outputs are combined in a convex combination.
During training, teacher-forcing is applied. During validation forecasts are produced autoregressively.
In the classification of~\citet{equivalence_temporal_graph} this would be classified as \textit{time-and-graph}.

\textbf{GraphEncoding}
We recreated a model from a recent contribution from the field of ML-based epidemiological forecasting~\citep{HySonLab}. This model encodes the contextual time-steps separately in a shared GNN. The encoded graphs are then propagated through an LSTM~\citep{LSTM} network. To achieve a forecast for multiple days, this network is applied autoregressively.
In the classification of~\citet{equivalence_temporal_graph} this would be classified as \textit{graph-then-time}.










\subsection{Epidemiological Benchmarking}\label{sec:benchmarking_definition}
%To showcase the usage of the datasets,
To benchmark the presented architectures from section~\ref{sec:models}, we define three tasks on the synthetic epidemiological dataset, created from the SI Eq.~\ref{eq:si}, that are motivated by forecasting tasks with real-world data.
While we wanted to present thorough experiments on this dataset, results with a similar experimental setup, but on the other two datasets from Eq.~\ref{eq:ad} and Eq.~\ref{eq:wave}, can be found in the appendix~\ref{sec:add_experiments}.

We will proceed with the epidemiological dataset which we split $80/20$ along the time axis into a train and a test dataset, and omit any information about the Susceptible $S$, as this reflects real-world data. On this data, we define the following tasks for machine learning models.

\textbf{Forecasting on clean data}
The most straightforward task is a simple forecast of the next $n$ timesteps, based on the last $m$ timesteps of inputs. We set $m=n=14$. The input data into the models therefore are $14$ consecutive graphs, sharing the same adjacency, or one graph with $14$ node features: $(V, E, X_{i,..,i+13})$.
The targets are simply $X_{i+14,..,i+27}$. 
As a test- and training loss we use the RMSE over all samples, nodes in $V$, and forecasted timesteps $m$.

%\begin{equation*}
%    RMSE(X,Y) = \sqrt{\dfrac{1}{\vert \textit{samples} \vert +\vert V \vert +m}\sum_{\textit{samples}}\sum_{V}\sum_{i=1}^m(X_i- Y_i)^2}
%\end{equation*}



\textbf{Stability: Noise on test data}
Since usually there is heavy noise on real-world input data, it is a highly relevant and interesting scenario to test the robustness of the tested machine learning models. We adopt the experimental setting from the previous forecasting benchmarking, but add noise on the node features $X_{i,..., i+13}$ of the test dataset, but not onto training data. This enables us to study some aspects of the models' robustness in a controlled setting. Further, this reflects real-world data, on which noise exists but is variable due to changed measurements, or delayed reporting. We studied two different types of noise:

\begin{itemize}
    \item We found the Gaussian noise with distribution $\mathcal{N}(0, 0.01)$ to be an interesting setting for the normalized dataset. A plot of the noisy data can be found in the appendix~\ref{sec:appdx_noise} Fig~\ref{fig:noise_diff}. 
    \item We also studied noise which reflects the failure of sensors/reports and therefore the occurrence of reported zeros instead of real values. Guided by this scenario, we replace $10 \%$ of the test data with zeros to further study the models' robustness.
\end{itemize}




\textbf{Denoising: noise on context data} 
We seek to study the abilities of different models to implicitly denoise input data, by both training and testing on noisy context data. We use the same Gaussian noise and dropout noise as in the previous experiments.



%\subsection{Results: Sample benchmarking}\label{section:results}
%The training was executed on a single \textit{NVIDIA A100-SXM4-40GB} GPU. The number of parameters and training time for each model can be found in Table~\ref{tab:computation} TODO . 
%The exact parameters can be found in the implementation on GitHub and were chosen as the optimal parameters for each model experimentally. 

We executed the benchmarking experiments each three times with different random initializations and trained all models until convergence. Details on the setup and computational aspects can be found in supplementary material~\ref{appendix:models} or on our given GitHub.
The resulting RMSEs over all samples from the test datasets for the different tasks can be found in Table~\ref{tab:results}. A visual display of the results can also be found in Fig~\ref{fig:appdx_forecast_rmse}, Fig~\ref{fig:appdx_stab} and Fig~\ref{fig:appdx_denoise}.






\begin{table}[H]
    \caption{
    {\bf Overview of Performance compared to different tasks.} We take the RMSEs over all samples and all forecasted time-steps from the test dataset. The $\pm$ indicates the standard deviation out of 3 training runs with random initialization respectively. We scaled all values up by the factor $100$ for readability.}
    \label{tab:results}
    \begin{center}
        \begin{tabular}{l|c|cc|cc}
        Model & Forecasting  & \multicolumn{2}{c|}{Stability} & \multicolumn{2}{|c}{Denoising} \\
        \hline
        &  & Gauss & Dropout & Gauss & Dropout
        \\ \hline
        Repetition  & $2.27$ & $2.74$ & $3.30$ & $2.74$ & $3.30 $ \\ 
        MP-PDE & $1.09 \pm 0.15$ & $2.2 \pm 0.2$ & $2.15 \pm 0.1$ & $1.37 \pm 0.05$ & $\mathbf{1.34 \pm 0.18}$ \\ 
        RNN-GNN-Fusion &   $\mathbf{0.97 \pm 0.10}$ & $\mathbf{1.3 \pm 0.1}$ & $\mathbf{1.80 \pm 0.1}$ & $\mathbf{1.33 \pm 0.06}$& $1.48 \pm 0.11$ \\ 
        RNN & $1.67 \pm 0.47$ & $2.3 \pm 0.3$ & $2.81 \pm 0.2$ & $1.76 \pm 0.08$ & $2.98 \pm 0.20$ \\ 
        GraphEncoding & $3.51 \pm 0.67$ & $3.5 \pm 0.7$ & $4.10 \pm 0.5$ & $3.81 \pm 0.66$ & $5.36 \pm 1.12$ \\ 
        TST & $1.14\pm 0.03$ & $2.6\pm 0.1$ & $2.5 \pm 0.02$ & $2.98 \pm 0.68$ & $2.72 \pm 0.02$\\
        \end{tabular}
    \end{center}
\end{table}

The RNN-GNN Fusion model, as well as the MP-PDE, generally exhibits strong performance, effectively leveraging the spatial aspect of the data. However, the performance of GraphEncoding underscores that simply incorporating a GNN does not necessarily boost performance. Therefore, meticulous benchmarking of different architectures on public datasets like ours is critical for the advancement of spatio-temporal graph machine learning.









\subsection{Transfer Learning to Real-World Data}\label{sec:transfer_learning}

While the benchmarking of different model architectures already provides valuable insights, we aim to further illustrate the utility of synthetic data in developing foundational forecasting models for epidemiology or pre-training models on tasks on which data is sparse. We, therefore, seek to demonstrate whether pre-training on our provided synthetic data can lead to improved performance on real-world tasks. 
To carry out this experiment we train and evaluate the models from the previous section on real-world epidemiological data from respectively German COVID-19, German Influenza, and Brazilian COVID-19. The prediction task is again a 14-day forecast based on 14 days of input.
Simultaneously, we pre-train the same models first on the synthetic dataset based on the SI-diffusion equation, and only then shortly retrain (fine-tune) them on the respective real-world data. We then compare the difference in performance.
Note, that the Brazilian data requires an additional knowledge transfer onto an unseen domain (i.e. Graph). Details on the real-world datasets can be found in section~\ref{sec:appdx:real_world_dataset}.






The outcomes of the transfer-learning experiments were highly favorable for pre-training on our dataset, underscoring the effectiveness and possible impact of our proposed method of data synthetization on real-world data and problems. The results can be found in Table~\ref{tab:results_transfer_learning}. A graphic representation, containing the actual RMSEs and comparing the models against each other can be found in the appendix, see fig.~\ref{fig:arrow_plot}.
\begin{table}[H]
    \caption{
    Change in validation-loss of the models in the transfer-learning tasks in percent. A negative number indicates improved performance through out method, i.e. by pre-training on synthetic data as opposed to training solely on the actual data.}
    \label{tab:results_transfer_learning}
    \begin{center}
    \begin{tabular}{lccccc}
        Model  & German COVID-19 & German Influenza & Brazilian COVID-19
        \\ \hline 
        MP-PDE & $-7.92\%$ & $6.96\%$ & $-16.88\%$ \\ 
        RNN-GNN-Fusion &   $-30.57\%$ & $-19.04\%$ & $-45.58\%$\\ 
        RNN & $-11.34\%$ & $-16.13\%$ & $-42.61\%$ \\ 
        GraphEncoding & $-8.39\%$ & $-6.91\%$ & $-35.45\%$ \\ 
        TST & $-11.73\%$ & $4.48\%$ & $-12.49\%$ \\
        \hline
        mean & $-13.99\%$ & $-6.13\%$ & $-30.60\%$
        \end{tabular}
    \end{center}
\end{table}
Only two out of 15 experiments exhibited a slightly decreased performance, while 13 showed an increased performance, frequently to a significant extent. Especially on the Brazilian COVID-19 dataset, some models showed an increase in performance up to $45\%$. Especially the previously already favorable RNN-GNN-Fusion model exhibits a significant boost in performance across all datasets. 



        
%\section{Access to code and data}\label{sec:access}
%The code and data can be found on \href{https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs}{github.com/github-usr-ano/Temporal\_Graph\_Data\_PDEs}. 
%The data can furthermore be found on kaggle upon acceptance.
%\footnote{\href{https://kaggle.com/datasets/7b0a33c38bd818611c47e31f06cc57dbe2f0b4233fbbf488e9aafc8515957d9f}{kaggle.com/datasets/7b0a33c38bd818611c47e31f06cc57dbe2f0b4233fbbf488e9aafc8515957d9f}.} 
% The code is published under the \textit{GNU LESSER GENERAL PUBLIC LICENSE v2.1}. %\footnote{\href{https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html#SEC1}{www.gnu.org/licenses/old-licenses/lgpl-2.1.html#SEC1}.}
%The created datasets are published under the \textit{CC BY 4.0} license. %\footnote{\href{https://creativecommons.org/licenses/by/4.0/legalcode.en}{creativecommons.org/licenses/by/4.0/legalcode.en}}


\section{Conclusions}\label{sec:conclusion}
We demonstrated how time-dependent PDEs can be used to create synthetic data for machine learning on graphs. In particular, we have created and published three exemplary datasets that can directly be used by other researchers for further research questions and benchmarking. %Notably, the numerical solution of any epidemiological PDE constitutes a novelty. 
The described method and code enables researchers to create synthetic temporal graph datasets for individual use cases to support the development of new machine learning methods, in particular, for use cases where data is scarce. 

We demonstrate through transfer-learning experiments that pre-training on our synthetic datasets can lead to drastic improvements in real-world data performance. Despite the underlying PDE being relatively limited in flexibility and parameters, these experiments showed a significantly increased performance for some models in our experimental setup.
Additionally, as an application of our datasets, we have demonstrated a benchmarking of various models, showcasing that rigorous testing of common architectures was a substantial research gap due to a lack of sufficient data.

Our datasets enables others to build, test, and compare new models for three different applications on large amounts high-quality data, stemming from a fully controllable simulation. A comparable epidemiological benchmarking with publicly accessible data is unknown to the authors, and especially conducting such a benchmarking with a comparable amount of data was not possible prior to our contribution.
We also strongly encourage others to adapt the code and methodology, such as geometry, parameters, scenarios, or PDEs. This not only enables researchers to fulfill individual needs, but also enables others to explore the capacity of machine learning models to simulate specific behaviors of PDEs.

In Summary this paper presents a method for generating individual PDE-based graph datasets, offers three readily available synthetic datasets, provides a benchmarking, and highlights significance through drastic improvements of performance on real-world data in exemplary applications. This work lays the foundation for the development of further datasets and research problems, inviting others to build upon our findings.

\paragraph{Limitations}
The adaptations of our code can become quite complex and more advanced changes can require considerable coding effort and expertise. While exchanging the domain, parameters, sampling rates, or graph structure can be done with little effort, extending or even replacing the PDE can be a very demanding task, requiring a deeper in-depth understanding of the PDE, the FEM, and the employed FEM software.
Also our method is constrained to PDEs that can be solved through the FEM, which reaches its computational limits in high-dimensional problems quickly.
While our epidemiological experiments demonstrates significant success on real-world data, other applications may need a much greater number of parameters to derive insights for machine learning models, which could also be limited by computational aspects.

 





\newpage

\impact{
%Recommended to read this \href{https://medium.com/@GovAI/a-guide-to-writing-the-neurips-impact-statement-4293b723f832}{medium}


Spatio-temporal machine learning models and PDEs have diverse applications, including epidemiology, weather forecasting, traffic prediction, and more. Our contribution could be used both for foundational research, as well as pre-training models in domains in which data is sparse.


Our datasets and the creation method leverage PDEs, which are controllable and well-structured. While our work has the potential to enhance data-driven methods for real-world applications and contribute to foundational research, we believe the societal implications are minimal. We aim for a significant improvement in prediction systems, and the authors do not anticipate any negative societal impacts. Our PDEs explicitly do not discriminate against different groups of society or individuals, thus we also see no potential of biased assumptions.


Given the absence of identified drawbacks in our datasets and methodology, we primarily see opportunities for other researchers to expand our datasets, further enhancing their detail and the performance of our PDEs.
}



\acks{
This work was supported by the Federal Ministry for Economic Affairs and Climate Action (BMWK) as grant DAKI-FWS (01MK21009A).
}

\vskip 0.2in
\bibliography{sample}




\appendix



\section{Details on: Dataset Creation}\label{sec:detail:dataset}
The datasets were created with different parameters of the dynamics of the PDEs. We described the key steps to numerically solve the presented equations in the main paper.
Further details on the selected parameters to extend section~\ref{sec:data_gen} might be valuable to understand the published code and data or to develop more suitable models and will be presented here.
The following computations are executed on two AMD EPYC 7543 32-Core Processors.

The evaluation points to create the Graph $\mathcal{G}$ were created by finding the centers of the German \textit{NUTS3} regions. Geographical data regarding the regions are publicly available. \footnote{
\href{https://mis.bkg.bund.de/trefferanzeige?docuuid=D38F5B40-9209-4DC0-82BC-57FB575D29D7}{mis.bkg.bund.de/trefferanzeige?docuuid=D38F5B40-9209-4DC0-82BC-57FB575D29D7},
which is published under the dl-de/by-2-0 license (\href{https://www.govdata.de/dl-de/by-2-0}{https://www.govdata.de/dl-de/by-2-0} ) } The data was processed with the use of \textit{GeoPandas}~\citep{Geopandas}, \textit{shapely} and \textit{SciPy}~\citep{SciPy} by finding centers of each region, adjacent regions and their distances. Some code regarding this can be found in a notebook on our GitHub under \href{https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs/blob/main/additional_resources/point_and_mesh_generation.ipynb}{/additional\_resources/point\_and\_mesh\_generation.ipynb}.
The underlying graph of the wave-equation dataset was created with the same geometry files. The evaluation points were chosen randomly, the adjacencies were created using a Delaunay triangulation.





\subsection{SI-Diffusion Equation}
The epidemiological dataset from the SI diffusion equation (\ref{eq:si}) was created by running $25$ simulations, each with different parameters, with a length of $364$ time-steps.
The parameters of the individual simulations can be found in Table~\ref{tab:exp_id}.



\begin{table}[H]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth 
    \centering
    \caption{
    {\bf Experiment Ids and their respective parameters.} The parameters $r$ and $D$ of the published 25 simulations. "Id." denotes the experiment identifier. The omitted experiment identifier corresponds to different initial values or boundary conditions and can be found in the published code for further simulations. $D$ was scaled in the table by the factor $10^{-8}$ for simplicity of the notation.}
    \label{tab:exp_id}
    \begin{tabular}{ccc}
         Id. & $r$ & $D \cdot 10^{-8}$  
         \\ \hline \\
         0 & $0.6$ & $2$  \\ 
         5 & $1+ (0.2 sin ( 5 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2)$ & $2$  \\ 
         10 & $0.7+ (0.6 sin ( 5 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2)$ & $2$   \\ 
         15 & $0.5+ (0.3 sin ( 2 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2)$ & $2$  \\ 
         20 & $1.1+ (0.1 sin ( 8 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2)$ & $2 $  \\ 
         25 - 45 & same as 0 - 20 & $ (1 + 0.1 $ $sin ( 5 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2))$ \\ 
         50 - 70 & same as 0 - 20 & $3 $ \\ 
         75 - 95 & same as 0 - 20 & $0.8$ \\ 
         100 - 120 & same as 0 - 20 & $ (1 + 0.2 sin ( 5 \mathrm{e}{-6} \pi x_1) sin(5 \mathrm{e}{-6} \pi x_2)) $ \\
    \end{tabular}
%\end{adjustwidth}
\end{table}

The resulting simulations were concatenated to a single large dataset. The concatenation is possible since Infected $I$ in the individual converges to zero at the end of every individual simulation. Although the concatenation is not smooth we consider the discontinuity as negligible. 
A plot of the $25$ different scenarios over time can be found in Figure~\ref{fig:si_over_time}.
\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{img/SI_over_time.png}
    \caption{The dataset generated from the SI-diffusion equation over time. There are 400 lines plotted, each representing the values of an individual node over time.}
    \label{fig:si_over_time}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=13cm]{img/simu_influenza_comparison.png}
    \caption{The SI-diffusion dataset(top) compared against the real-world German Influenza dataset(bottom). The step patterns in the Influenza dataset are presumably artefacts of the data collection/management pipeline.}
    \label{fig:simu_influenza_comparison}
\end{figure}




\subsection{Advection-Diffusion equation}\label{subsect:details_AD}
The dataset created from the advection-diffusion equation~(\ref{eq:ad}) consists of a single, connected, but long simulation.
We define $54$ different but consecutive sets of dynamics of the equation, each with a length of $80$ time-steps.
We set $h = \frac{10^{9}}{8}$, $T=54 \cdot 10^{10}$.


The support, necessary to describe the source term $s(x,t)$, $\Omega_{s(x,t)}$, is a rectangle $[a \cdot 10^6, 1.01 a \cdot 10^6]\times [b\cdot 10^6, 1.01b\cdot 10^6]$ on the domain.  

The source term $s(x,t)$ was specified in the main paper via $S(t)$ which can be recovered by scaling the temporal dimension $S(t)=\hat{s}(t\cdot 10^{-10}) \cdot  10^{-10}$ from Table~\ref{tab:exp_id_adv_diff} as scaled characteristic function, i.e.
\begin{equation*}
\mathds{1} _{I}(t) = \begin{cases}
      1 & \text{$t \in I$}\\
      0 & \text{$t \notin I $}.
    \end{cases},
\end{equation*}
Where $I \in \mathbb{R}$ is some interval.
Also $\beta (x,t ) $, and $a,b$ can be found in Table~\ref{tab:exp_id_adv_diff}.


\begin{table}[H]
    \centering
    \caption{
    {\bf Parameters of the advection-diffusion equation} The parameters of the simulation for the advection-diffusion equation."Id." represents 80 consecutive time-steps. The factor $10^4$ in the column defined $\beta$ was used for simplicity of the notation. Note that in the characteristic functions $\mathds{1}$ also a modulus operation is used, such that after Experiment Id 6, the same dynamics as in 1 starts, i.e. the intervals were shifted to the right.
    Ids. 7-12 does not match first columns}
    \label{tab:exp_id_adv_diff}
    \begin{tabular}{cccc}
         Id. & $\hat{s}(t)$ & $\Omega_s$ & $\beta (x) \cdot 10^{4}$
         \\ \hline \\
         1 & $-32 \mathds{1}_{[0,0.1]} -22 \mathds{1}_{[0.8,0.9]} $ & $a = 3.45, b=5.4$ & $(-0.5,  0.5)$ \\ 
         2 & $-27\mathds{1}_{[1.3,1.4]} $ & same as 1 & same as 1 \\ 
         3 & $-32 \mathds{1}_{[2.1, 2.2]} -42\mathds{1}_{[2.8, 2.9]}$ & same as 1 & same as 1 \\
         4 & $-28\mathds{1}_{[3.3, 3.4]}$ & same as 1 & same as 1 \\
         5 & $-38\mathds{1}_{[4.1, 4.2]}-33\mathds{1}_{[4.8, 4.9]}$ & same as 1 & same as 1 \\
         6 & $-30\mathds{1}_{[5.3, 5.4]}$ & same as 1 & same as 1 \\
         7-12 & same as 1-6 & $a = 3.6, b = 5.6$ & same as 1 \\ 
         13-18 & same as 1-6 & $a = 3.45, b = 5.8$ & same as 1 \\
         19-36 & same as 1-18 & same as 1-18 & $(0.6 , 55 - x_1 10^{-6} )$ \\
         37-54 & same as 1-18 & same as 1-18 & $(0.5, 2.5 x_1 10^{-6} -1.375 ) $ \\
    \end{tabular}
\end{table}




Similar to the source term $s$, we also initialize $u$ once on the support $\Omega_{s(x,0)}$  with

\begin{equation*}
u = \begin{cases}
      1 & \text{$x \in \Omega_{s(x,0)}$}\\
      0 & \text{$x \notin \Omega_{s(x,0)} $}.
    \end{cases}
\end{equation*}




\subsection{Wave Equation}\label{sec:supp_mat_wave}
The wave equation~(\ref{eq:wave}) also consists of a single consecutive simulation. There are two waves throughout the simulation, that start at the boundaries. The boundary conditions are usuallyRobin boundary conditions $\alpha u + \beta \eta \cdot \nabla u = 0$ three exceptions: first during the interval $0 < t < 50000$and on the boundary with $x_1 > 6200000, x_0 < 3400000$, second during the interval $1000000<t<1100000$, and on the boundary with $x_1 > 6200000, x_0 > 3800000$, and third during the interval $1800000<t<1900000$, and on the boundary with $x_1 > 6200000, 3500000 < x_0 < 3600000$.

On these intervals and regions, the Dirichlet boundary $g$ are respectively a constant factor ($0.8, 1, 0.9$) for the wave's amplitude, that can easily be adapted for further simulations.
we set $b = 0.000005$.
The first wave can be seen in Figure~\ref{fig:wave_graph}. 
\begin{comment}
    \begin{equation*}
g = \begin{cases}
      cos(t * 0.00004 * \pi ) * 0.00004 * \pi &\text{if $0<t<50000$ and $x$ and $y$}\\
      ? & \text{if $1000000<t<110000$ and $x$ and $y$}\\
      0 & \text{else}
    \end{cases}
\end{equation*}
\end{comment}



\subsection{Creation of Epidemiological Real-World Datasets}\label{sec:appdx:real_world_dataset}
The three real-world datasets from section~\ref{sec:data_gen} are based from epidemiological data, which can be found online. 
\begin{enumerate}
\item The Brazilian COVID-19 dataset has 27 nodes and 1093 time-steps spanning 2019-2022 after concatenation and linear interpolation for daily resolution and is publicly accessible.
\footnote{\href{https://sisaps.saude.gov.br/painelsaps/atendimento}{sisaps.saude.gov.br/painelsaps/atendimento}}


\item German COVID-19 data can be found at \href{https://github.com/robert-koch-institut/SARS-CoV-2-Infektionen_in_Deutschland}{github.com/robert-koch-institut/SARS-CoV-2-Infektionen\_in\_Deutschland}, with 1539 time-steps. 

\item German Influenza dataset can be found at \href{https://survstat.rki.de/}{survstat.rki.de/} with our curation having 5256 time-steps. 
\end{enumerate}


All 3 datasets were smoothed with a 7-day moving average. 

The graph describing the spatial connection of the \textit{NUTS3} regions in Germany was created above. 
The Brazilian geospatial data used for constructing the graph connectivity as described in section~\ref{sec:detail:dataset} can be accessed at 
\href{https://www.ibge.gov.br/en/geosciences/territorial-organization/territorial-meshes/18890-municipal-mesh.html}{https://www.ibge.gov.br/en/geosciences/territorial-organization/territorial-meshes/18890-municipal-mesh.html}.

A visualization can be found in Fig.~\ref{fig:covid_datasets}.

\begin{figure}[H]
  \centering
  \subfloat[]{{\includegraphics[width=6cm]{img/brazil_covid.png}}}
  \qquad
  \subfloat[]{{\includegraphics[width=6cm]{img/germany_covid.png}}}
  \caption{Left: Brazilian COVID dataset. Right: German COVID dataset. In both plots, each curve represents a region, and the vertical red line indicates the training/evaluation cutoff.}
  \label{fig:covid_datasets}
\end{figure}

These datasets will additionally be released upon acceptance.



\section{Details on: Applications of Synthetic Data}\label{appendix:models}
In our experiments for epidemiological applications from section~\ref{sec:examples}, we trained and evaluated the models on the data based on the SI-diffusion equation in 32-bit float representations. The employed training epochs for the different tasks and models can be found in Table~\ref{tab:computation_epoch}. 


\begin{comment}

\begin{table}[H]
%\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth 
    \centering
    \caption{
    {\bf Training Epochs for different models and tasks.}}
    \begin{tabular}{ccccc}
        \toprule
         Model & Task 1 & Task 4 & Task 5 & Task 6 \\
         \midrule 
         MP-PDE & $30$ &  $30$ & $30$ & $30$ \\ 
         RNN-GNN-Fusion  & $30$ &  $30$ & $30$ & $30$ \\
         RNN & $30$ &  $30$ & $30$ & $30$ \\
         GraphEncoding & $30$ &  $30$ & $30$ & $30$ \\ 
         TST & $50$ &  $50$ & $50$ & $50$ \\ 
         \bottomrule
    \end{tabular}
    \begin{flushleft} 
    The actual training times and epochs depend on the specific training run due to random initialization and early stopping.  In tasks 2 and 3, models trained for task 1 are used in inference time. 
    \end{flushleft}
    \label{tab:epochs}
%\end{adjustwidth}
\end{table}

The number of parameters and training time for each model can be found in Table~\ref{tab:computation}. The trainings were executed on a single \textit{NVIDIA A100-SXM4-40GB} GPU. 

\begin{table}[H]
    \centering
    \caption{
    {\bf Overview of Training Epochs and Computational Aspects for Different Models.}}
    \begin{tabular}{ccc}
        \toprule
         Model & \# Params & Training Time \\
         \midrule 
         MP-PDE  & $623$k & $57$m \\ 
         RNN-GNN-Fusion  & $70$k & $2$:$29$h  \\ 
         RNN  & $67$k & $1$:$16$h\\ 
         GraphEncoding  & $382$k & $3$:$42$h \\ 
         TST & $344$k & $1$:$51$h\\ 
         \bottomrule
    \end{tabular}
    \begin{flushleft} 
    Maximum training epochs for each task, number of parameters for each model, and their respective training times for Task 1.
    \end{flushleft}
    \label{tab:computation}
\end{table}
\end{comment}


\begin{table}[H]
    \centering
    \caption{
    {\bf Overview of Training Epochs and Computational Aspects for Different Models.} Number of parameters, their respective training times for the forecasting benchmarking, and maximum training epochs for all experiments from section~\ref{sec:examples} for each model.
    The exact training times and epochs depend on the specific training run due to random initialization and early stopping. }
    \label{tab:computation_epoch}
    \begin{tabular}{cccc}
         Model & \# Params & Train Time & \#Epochs 
         \\ \hline 
         MP-PDE  & $623$k & $57$m & $30$  \\ 
         RNN-GNN-Fusion  & $70$k & $2$:$29$h  & $30$ \\ 
         RNN  & $67$k & $1$:$16$h & $30$  \\ 
         GraphEncoding  & $382$k & $3$:$42$h & $30$  \\ 
         TST & $344$k & $1$:$51$h & $50$\\ 
    \end{tabular}
\end{table}




The exact hyperparameters can be found in the implementation on GitHub in the regarding parameter files such as \href{https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs/blob/main/ml/mp_pde/mp_pde.yml}{github.com/github-usr-ano/Temporal\_Graph\_Data\_PDEs/blob/main/ml/mp\_pde/mp\_pde.yml} for the MP-PDE model. The parameter files for other models can be found in their respective directories. The hyperparameters were chosen as optimal for each model experimentally. We trained all models for each task with the Adam optimizer until convergence (early stopping).









\section{Details on Epidemiological Benchmarking}
\label{sec:appdx_noise}


A visualization of some data with Gaussian noise from the stability and denoising experiments can be seen in Figure~\ref{fig:noise_diff}.
\begin{figure}[H]
  \centering
  %\includegraphics[width = 3cm]{img/grid-1.svg}
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \centering{{\includegraphics[trim={0 20.8cm 0 0},clip,width=13cm]{img/noise_difference.png} }}%

    \caption{ Illustration of the effect of additive noise sampled from the distribution $\mathcal{N}(0, 0.01)$ from task 2 and task 4. On the left is the time series from a random graph node; on the right is its respective noise-added time series. The plotted section is the first $1000$ time-steps of the training dataset.}
\label{fig:noise_diff}%
\end{figure}

A visual comparison of RMSEs for each time-step in the forecasting experiment for different models can be seen in Fig~\ref{fig:appdx_forecast_rmse}
\begin{figure}[H]
  \centering
  %\includegraphics[width = 3cm]{img/grid-1.svg}
  %\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
  \centering
  {\includegraphics[width=0.43\linewidth]{img/task_1_variance.png} }%
    \caption{The RMSE over all samples from the test dataset. % for different models for every forecasted time-step. 
    The less saturated colors represent the standard deviation for different initializations.}\label{fig:appdx_forecast_rmse}
\end{figure}
A visual comparison of RMSEs for each time-step in the stability experiments for different models can be seen in Fig~\ref{fig:appdx_stab}.
\begin{figure}[H]
  \centering
    \subfloat[Gaussian]{{\includegraphics[width=0.43\linewidth]{img/task_2_variance.png} }}%
    \qquad
    \subfloat[Dropout]{{\includegraphics[width=0.43\linewidth]{img/task_3_variance.png} }}%
    
    \caption{Plots of the RMSE of the presented models during the stability experiments for each predicted time-step. The models were trained on the synthetic dataset, and evaluated on a synthetic dataset with:
    \textbf{a)} additive Gaussian noise,
    \textbf{b)} dropout noise.
    The less saturated colors represent the standard deviation for different initializations.}
    \label{fig:appdx_stab}
\end{figure}

A visual comparison of RMSEs for each time-step in the denoising experiments for different models can be seen in Fig~\ref{fig:appdx_denoise}.
\begin{figure}[H]
  \centering

    \centering
    \subfloat[Gaussian]{{\includegraphics[width=0.43\linewidth]{img/task_4_variance.png} }}%
    %
    \subfloat[Dropout]{{\includegraphics[width=0.43\linewidth]{img/task_5_variance.png} }}%
    \caption{Plots of the RMSE of the presented models during the denoising experiments for each predicted time-step. The models were trained and evaluated on a synthetic dataset which. The models were trained to predict based on context data, with 
    \textbf{a)} additive Gaussian noise,
    \textbf{b)} dropout noise.
    The less saturated colors represent the standard deviation for different initializations.}
    \label{fig:appdx_denoise}
\end{figure}



\begin{comment}
    
\begin{table}[H]
    \caption{
    {\bf Overview of relative changes in performance compared to different tasks.} Relative change of losses from Table~\ref{tab:results}}
    \label{tab:results_relative}
    \begin{center}
        \begin{tabular}{l|c|cc|cc}
        Model & Forecasting  & \multicolumn{2}{c|}{Stability} & \multicolumn{2}{|c}{Denoising} \\
        \hline
        &  & Gauss & Dropout & Gauss & Dropout
        \\ \hline
        Repetition  & $2.27$ & $2.74$ & $3.30$ & $2.74$ & $3.30 $ \\ 
        MP-PDE & $1.09 \pm 0.15$ & $2.2 \pm 0.2$ & $2.15 \pm 0.1$ & $1.37 \pm 0.05$ & $\mathbf{1.34 \pm 0.18}$ \\ 
        RNN-GNN-Fusion &   $\mathbf{0.97 \pm 0.10}$ & $\mathbf{1.3 \pm 0.1}$ & $\mathbf{1.80 \pm 0.1}$ & $\mathbf{1.33 \pm 0.06}$& $1.48 \pm 0.11$ \\ 
        RNN & $1.67 \pm 0.47$ & $2.3 \pm 0.3$ & $2.81 \pm 0.2$ & $1.76 \pm 0.08$ & $2.98 \pm 0.20$ \\ 
        GraphEncoding & $3.51 \pm 0.67$ & $3.5 \pm 0.7$ & $4.10 \pm 0.5$ & $3.81 \pm 0.66$ & $5.36 \pm 1.12$ \\ 
        TST & $1.14\pm 0.03$ & $2.6\pm 0.1$ & $2.5 \pm 0.02$ & $2.98 \pm 0.68$ & $2.72 \pm 0.02$\\
        \end{tabular}
    \end{center}
\end{table}

\end{comment}


\section{Details on Transfer Learning to Real-World Data}
To have a better visual comparison between the predictive performances of the models and the degree of their improvement in the transfer learning experiment, we created Fig.~\ref{fig:arrow_plot}.
\begin{figure}[H]
  \centering
  {\includegraphics[width=0.49\linewidth]{img/simulation_pretraining_repetition2.png} }%
    \caption{%The RMSE of models trained directly on real-world data compared against models firstly pre-trained on our simulation dataset. %Rather than training directly on the COVID-19 datasets, pre-training on our simulation data yields a better validation performance for most of the models. 
    Plots of the RMSEs of the presented models during the transfer-learning experiments. The arrows point from non-pre-trained to pre-trained models.}
    \label{fig:arrow_plot}
\end{figure}



\subsection{Additional Experiments} \label{sec:add_experiments}

We repeated the Forecasting experiment from section~\ref{sec:examples}, more specifically section~\ref{sec:benchmarking_definition} on the two other created datasets, i.e. the Advection-diffusion dataset and the wave-equation dataset.


\begin{figure}[H]
  \centering
  \centering
  \subfloat[Advection-diffusion equation dataset]{{\includegraphics[width=0.49\linewidth]{img/experiments_on_advection.png} }}%
  \subfloat[Wave equation dataset]{{\includegraphics[width=0.49\linewidth]{img/experiments_on_wave.png} }}%
    \caption{
    \textbf{a)} The RMSE for 7-timestep forecasts on the dataset generated by Advection-Diffusion Equation. 
    \textbf{b)} The RMSE for 7-timestep forecasts on the dataset generated by Wave Equation. }
    \label{fig:results}
\end{figure}


\begin{table}[H]
    %\begin{adjustwidth}{-2.25in}{0in} % Comment out/remove adjustwidth 
    \centering
    \caption{
    {\bf Performance Comparison Across Models for Different Datasets}}
    \begin{tabular}{lcccccc}
        \hline
        Model & Advection-Diffusion Equation & Wave Equation \\
        \hline 
        Repetition & $0.0156 \pm 0$ & $0.0244 \pm 0$ \\ 
        MP-PDE & $\mathbf{0.0115 \pm 0.0001}$ & $\mathbf{0.0139 \pm 0.0003}$ \\ 
        RNN-GNN-Fusion & $0.0134 \pm 0.0009$ & $0.0213 \pm 0.0013$ \\ 
        RNN & $0.0135 \pm 0.0010$ & $0.0234 \pm 0.0008$ \\ 
        GraphEncoding & $0.0195 \pm 0.0012$ & $0.0383 \pm 0.0101$ \\ 
        TST & $0.0117 \pm 0.0006$ & $0.0256 \pm 0.0014$ \\
        \hline
    \end{tabular}
    %\end{adjustwidth}
\end{table}








\newpage
Supplementary Material for DMLR (seperate PDF)




\section{Our Datasheet}
Our datasheet from~\citet{datasheet} can be found below.

\subsection{Motivation}
\paragraph{For what purpose was the dataset created?}
\textit{Was there a specific task
in mind? Was there a specific gap that needed to be filled? Please provide a description.}

The Dataset was created to fill a research gap of large high-quality, flexible, easily accessible temporal graph datasets. The first use-case in mind was benchmarking temporal graph machine learning architectures from the field of epidemiology. We then wanted to extend the datasets and showcase the flexibility of our method.

\textbf{Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?}

All involved people are listed as authors of this paper. The authors are all employees of Fraunhofer Heinrich Hertz Institut, Berlin, Germany.

\textbf{Who funded the creation of the dataset?}
\textit{If there is an associated grant, please provide the name of the grantor and the grant name and number.}

This work was supported by the German Federal Ministry for Economic Affairs and Climate Action (BMWK) as grant DAKI-FWS (01MK21009A).

\textbf{Any other comments?}

No.




\subsection{Composition}
\paragraph{ What do the instances that comprise the dataset represent (e.g.,
documents, photos, people, countries)?}
\textit{Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.}

The instances represent measurement values of geographical administrative units (in our case German NUTS3 regions) or a sensor network. Additionally, the published dataset contains adjacencies and inverse distances of these points to form temporal graphs.


\paragraph{How many instances are there in total (of each type, if appropriate)?}

There are three different datasets, the dataset based on the SI-diffusion equation and advection-diffusion equation share the underlying graph, which has $400$ nodes and $2088$ adjacencies. 
The dataset based on the wave equation consists of $325$ nodes and $1858$ adjacencies.
The dataset based on the SI-diffusion equation consists of $9100$, the dataset based on the advection-diffusion equation consists of $4320$ time-steps and the dataset based on the wave equation consists of $960$ time-steps.

\paragraph{Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?} 
\textit{If the
dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).}

This is a synthetic dataset created from PDEs. The points on which we evaluated the PDEs solution are part of the assumptions. Therefore this is not part of a larger dataset. However, the dataset can be extended by additional points in the graph if necessary.

\paragraph{What data does each instance consist of?}
\textit{ “Raw” data (e.g., unprocessed text or images) or features? In either case, please provide a description.}

Each value on the dataset (besides the adjacencies) represents a different measurement. In the case of the SI-equation, the measurements describe an infectious density such as cases per capita. 
The wave equation and advection-diffusion equation also represent some measurement but are rather an abstract density or height, hence we do not give any unit of measurement, as this is a synthetic dataset.

\paragraph{Is there a label or target associated with each instance?}
\textit{ If so, please
provide a description.}

This is regression data, consisting of temporal graphs. Each value in the dataset corresponds to a value on a graph at a certain time and can be used as a target for regression. Possible applications can be found in Section~\ref{sec:examples}.

\paragraph{Is any information missing from individual instances?}
\textit{If so, please
provide a description, explaining why this information is missing (e.g.,
because it was unavailable). This does not include intentionally removed
information, but might include, e.g., redacted text.}

N/A as this is a synthetic dataset.


\paragraph{Are relationships between individual instances made explicit
(e.g., users’ movie ratings, social network links)?}
\textit{If so, please describe how these relationships are made explicit.}

Yes with the published adjacencies the relationships are made explicit. 


\paragraph{Are there recommended data splits (e.g., training, development/validation,
testing)?}
\textit{If so, please provide a description of these splits, explaining
the rationale behind them.}

No recommended data splits were given.


\paragraph{Are there any errors, sources of noise, or redundancies in the
dataset?}
\textit{If so, please provide a description.}

There might be numerical errors in the synthetic datasets stemming from the process of solving PDEs which we consider insignificant, as the FEM with known for its accuracy and the employed spatial resolution was relatively high.
In the dataset based on the SI-diffusion equation there are $25$ concatenated simulations, between these simulations there are also small errors as the simulations only converge relatively slowly to zero but the following simulations are initialized as zero.

\paragraph{Is the dataset self-contained, or does it link to or otherwise rely on
external resources (e.g., websites, tweets, other datasets)?}
\textit{If it links
to or relies on external resources, a) are there guarantees that they will
exist, and remain constant, over time; b) are there official archival versions
of the complete dataset (i.e., including the external resources as they
existed at the time the dataset was created); c) are there any restrictions
(e.g., licenses, fees) associated with any of the external resources that
might apply to a dataset consumer? Please provide descriptions of all
external resources and any restrictions associated with them, as well as
links or other access points, as appropriate.}

The data is self-contained.

\paragraph{Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor–patient confidentiality, data that includes the content of individuals’ non-public communications)?} 
\textit{ If so, please provide a description.}

No.


\paragraph{Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?}
\textit{If so, please describe why.}

No.


\underline{If the dataset does not relate to people, you may skip the remaining questions
in this section.}


\subsection{Collection Process}
\paragraph{How was the data associated with each instance acquired?}
\textit{Was the data directly observable (e.g., raw text, movie ratings), reported by
subjects (e.g., survey responses), or indirectly inferred/derived from other
data (e.g., part-of-speech tags, model-based guesses for age or language)?
If the data was reported by subjects or indirectly inferred/derived from
other data, was the data validated/verified? If so, please describe how.}

The data was acquired by numerically solving PDEs.


\paragraph{What mechanisms or procedures were used to collect the data
(e.g., hardware apparatuses or sensors, manual human curation,
software programs, software APIs)?}
\textit{ How were these mechanisms or procedures validated?}

The data was collected using software and can be recreated by other users. During the creation, a residual is given which can be seen as validation. 

\paragraph{If the dataset is a sample from a larger set, what was the sampling
strategy (e.g., deterministic, probabilistic with specific sampling
probabilities)?}
The dataset was not a sample from a larger dataset. However, it can be extended, as this is a synthetic dataset by adapting the simulation process.

\paragraph{Who was involved in the data collection process (e.g., students,
crowdworkers, contractors) and how were they compensated (e.g.,
how much were crowdworkers paid)?}
All involved people are listed as authors of this paper and receive compensation as they are employees of Fraunhofer Heinrich Hertz Institut. To list them here again: Jost Arndt, Utku Isil, Michael Detzel, Samek Wojciech, and Jackie Ma.

\paragraph{Over what timeframe was the data collected?}
\textit{Does this timeframe
match the creation timeframe of the data associated with the instances
(e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.}

This is a synthetic dataset stemming from numerical solutions of PDEs, the timeframe of their solution is irrelevant to their content. However, the simulations were run in 2024 but can be carried out again within a few hours.

\paragraph{Were any ethical review processes conducted (e.g., by an institutional review board)?}
\textit{If so, please provide a description of these review
processes, including the outcomes, as well as a link or other access point
to any supporting documentation.}

No.

\underline{If the dataset does not relate to people, you may skip the remaining questions
in this section.}



\subsection{Preprocessing/cleaning/labeling}
\paragraph{Was any preprocessing/cleaning/labeling of the data done (e.g.,
discretization or bucketing, tokenization, part-of-speech tagging,
SIFT feature extraction, removal of instances, processing of miss-
ing values)?}
\textit{If so, please provide a description. If not, you may skip the
remaining questions in this section.}

During the creation of the datasets, every time-step of the simulation resulted in a single file. These files were concatenated to create a single file. 
Also to create the adjacencies and distances between the points of evaluation, geographical data was processed once.
Additionally, no preprocessing, cleaning, or labeling was executed.

\paragraph{Was the “raw” data saved in addition to the preprocessed/cleaned/labeled
data (e.g., to support unanticipated future uses)?}
\textit{If so, please provide a link or other access point to the “raw” data.}

The individual files were not saved. The underlying geometries and files can be found online.

\paragraph{Is the software that was used to preprocess/clean/label the data
available?}
\textit{If so, please provide a link or other access point.}

The software to concatenate different CSV files or create adjacencies once was not released.

\paragraph{Any other comments?}

No.


\subsection{Uses}
\paragraph{Has the dataset been used for any tasks already?}
\textit{ If so, please provide
a description.}

The epidemiological part of the dataset has been used in the exemplary tasks described in Section~\ref{sec:examples}, which previously has in parts been presented during an ICLR workshop (no proceedings, dataset was not published). The others have not been used.

\paragraph{Is there a repository that links to any or all papers or systems that
use the dataset?}
\textit{If so, please provide a link or other access point.}

No, this dataset is new.

\paragraph{What (other) tasks could the dataset be used for?}

The dataset could be used for benchmarking, pretraining, experimenting with noise, experimenting with classification and many more tasks. However to extend the field of application to other PDEs, our method has to be adapted.

\paragraph{Is there anything about the composition of the dataset or the way
it was collected and preprocessed/cleaned/labeled that might impact future uses?}
\textit{For example, is there anything that a dataset consumer
might need to know to avoid uses that could result in unfair treatment of
individuals or groups (e.g., stereotyping, quality of service issues) or other
risks or harms (e.g., legal risks, financial harms)? If so, please provide a
description. Is there anything a dataset consumer could do to mitigate
these risks or harms?}

This is a synthetic dataset, it is based on the assumptions of the PDE and its parameters.

\paragraph{Are there tasks for which the dataset should not be used?}
\textit{If so,
please provide a description.}

No.

\paragraph{Any other comments?}

No.


\subsection{Distribution}
\paragraph{Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which
the dataset was created?}
\textit{If so, please provide a description.}

This dataset will be made available to the public through this paper.


\paragraph{How will the dataset will be distributed (e.g., tarball on website,
API, GitHub)?}
\textit{Does the dataset have a digital object identifier (DOI)?}

The datasets are available as on GitHub.

\paragraph{When will the dataset be distributed?}

Right after the acceptance of this paper.


\paragraph{Will the dataset be distributed under a copyright or other intel-
lectual property (IP) license, and/or under applicable terms of use
(ToU)?}
\textit{If so, please describe this license and/or ToU, and provide a link
or other access point to, or otherwise reproduce, any relevant licensing
terms or ToU, as well as any fees associated with these restrictions.}

The data is published under \textit{CC BY 4.0} license, the code it is synthesized under \textit{GNU LESSER GENERAL PUBLIC LICENSE v2.1}.

\paragraph{Have any third parties imposed IP-based or other restrictions on
the data associated with the instances?}
\textit{If so, please describe these
restrictions, and provide a link or other access point to, or otherwise
reproduce, any relevant licensing terms, as well as any fees associated
with these restrictions.}

No.

\paragraph{Do any export controls or other regulatory restrictions apply to
the dataset or to individual instances?}
\textit{If so, please describe these
restrictions, and provide a link or other access point to, or otherwise
reproduce, any supporting documentation.}

No.

\paragraph{ Any other comments?}

No.



\subsection{Maintenance}

\paragraph{Who will be supporting/hosting/maintaining the dataset?}

The dataset for now is hosted on GitHub which is owned by the Microsoft Corporation.

\paragraph{How can the owner/curator/manager of the dataset be contacted
(e.g., email address)?}

Via email, which is also given on the cover page, and via GitHub.

\paragraph{ Is there an erratum?}
\textit{If so, please provide a link or other access point.}

There is no currently known erratum, besides insignificant numerical errata. If any are discovered at a later point, updated versions of the dataset will be released available on the same platforms.

\paragraph{Will the dataset be updated (e.g., to correct labeling errors, add
new instances, delete instances)?}
\textit{If so, please describe how often, by
whom, and how updates will be communicated to dataset consumers
(e.g., mailing list, GitHub)?}

There is no plan to regularly update the dataset. However, if flaws are discovered, or additional PDEs are solved, the dataset might be expanded. As this is not planned we cannot give any frequency of updates. The updates can be found on the same platforms as the current datasets.

\paragraph{If the dataset relates to people, are there applicable limits on the
retention of the data associated with the instances (e.g., were the
individuals in question told that their data would be retained for
a fixed period of time and then deleted)?}
\textit{ If so, please describe these
limits and explain how they will be enforced.}

N/A

\paragraph{Will older versions of the dataset continue to be supported/hosted/maintained?}
\textit{If so, please describe how. If not, please describe how its obsolescence
will be communicated to dataset consumers.}

The versioning of the datasets will be available on the platforms in their respective versioning systems, e.g. on GitHub via the git version control. The descriptions can also be found on the platforms

\paragraph{If others want to extend/augment/build on/contribute to the
dataset, is there a mechanism for them to do so?}
\textit{If so, please
provide a description. Will these contributions be validated/verified? If
so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to dataset consumers? If so,
please provide a description.}

Contributions could be made via pull requests on GitHub and have to be reviewed manually. Also, contributions can made manually via email.

\paragraph{Any other comments?}


No.



\section{URL to our data and code}
\href{https://github.com/github-usr-ano/Temporal_Graph_Data_PDEs}{github.com/github-usr-ano/Temporal\_Graph\_Data\_PDEs}.


\section{Author Statement}
I, the undersigned author, hereby confirm that I bear full responsibility for any violations of rights, including copyright or intellectual property infringements, that may arise in connection with my submitted work. I affirm that I have obtained all necessary permissions and licenses for any data, images, or materials included in this work. Additionally, I confirm that I comply with the data license and acknowledge the terms under which the data is shared and used.

\section{Hosting plan:}
Both code and data are published on GitHub which is owned by the Microsoft Corporation.

The code is published under the \textit{GNU LESSER GENERAL PUBLIC LICENSE v2.1}. %\footnote{\href{https://www.gnu.org/licenses/old-licenses/lgpl-2.1.html#SEC1}{www.gnu.org/licenses/old-licenses/lgpl-2.1.html#SEC1}.}
The created datasets are published under the \textit{CC BY 4.0} license. %\footnote{\href{https://creativecommons.org/licenses/by/4.0/legalcode.en}{creativecommons.org/licenses/by/4.0/legalcode.en}}




\end{document}
