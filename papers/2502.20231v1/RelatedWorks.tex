A wealth of research exists looking at the effects of AI companions on humans, for example \citet{Brandtzaeg2022AIfriend, xie2022attachment}. Our paper instead focuses on evaluating the biases and stereotypes that chatbots perpetuate as it becomes increasingly important to mitigate their impacts.

Metrics play a crucial role in assessing {LLM}s, and a range of papers have produced quantitative evaluations of these models \citep{nangia-etal-2020-crows, dhamalabold2021, bellem2024are, wan2023biasasker}. Through the lens of gender, extensive work has been done on creating a metric for occupational bias \citep{kirk2024box, rudinger-etal-2018-wino}. \citet{bai2024measuring} is one of few papers that focus on more underlying gender biases in that it studies implicit (unintentional, automatic) rather than explicit (intentional, deliberate) bias. It does this by using the Implicit Association Test (IAT), commonly used for human biases, and modifies it to {LLM}s.

\subsection{Persona Bias in LLMs}

Research into {AI} personas find that, generally, the design and implementation of personas result in models reflecting existing human biases, as shown by \citet{cheng-etal-2023-marked}. They generated personas with different ethnicities and genders and then had the LLM describe itself in that personas voice. This output is compared to the unmarked default persona descriptions, i.e., White and Man, by finding words that statistically distinguish the two groups and comparing the generated descriptions to human-created ones. The results show that models positively stereotype and assume resilience in marked groups much more heavily than unmarked ones and much more often than humans do. \citet{wan-etal-2023-stochastic} aimed to categorise and measure ‘persona biases’ by creating a UniversalPersona dataset of generic and specific personas. These personas are measured against harmful expression (offensiveness, toxic continuation, and regard) and harmful agreement metrics (stereotype and toxic agreement). Findings show that models have fairness issues when taking on the role of a persona. This work is a continuation of that by \citet{deshpande-etal-2023-toxicity}, which shows that assigning a specific persona can increase toxicity up to six-fold. 

To uncover more implicit bias, \citet{gupta2024bias} evaluates the unintended effects of persona assignment by measuring the reasoning capability of different models on different tasks. The results are clear; although ChatGPT will unilaterally reply that there is no difference in the maths problem-solving skills between a physically-abled and disabled person, when adopting the identity of a physically-disabled person, it outputs that because of its disability, it is unable to perform calculations. The work by \citet{plaza2024angry} evaluates a more inferred bias that assumes women are more emotional than men, which {LLM}s seem to agree with; sadness is overwhelmingly linked with women, anger with men.

To date, no work has studied how assigning gendered personas to a model with an implied relationship with its user impacts model responses. Not acknowledging the user's role disregards the topic of sycophancy -- where {LLM}s may echo the opinions of the users they interact with. \citet{huang2024trustllm} and \citet{xu2024earthflatbecauseinvestigating} show that assigning the user a persona and then prompting the model with questions tends to have the model giving responses that would align with the user's persona. However, there is a research gap in how sycophancy may change when assigning a persona to the model system. The role of sycophancy is an essential question when focusing on {AI} companions, as the relationship between user and model is, at its core, intertwined \citep{sharma2023understandingsycophancylanguagemodels}.

