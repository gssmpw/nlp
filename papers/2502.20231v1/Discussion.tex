
The answers to our research questions were complex and multi dimensional. Generally, as model size increased, the bias scores increased, although this rule was sometimes broken. The newer model family always had a lower rejection rate than the older family. Male-assigned models responded with \textit{anger}, whether in restricted or unrestricted situations, much more often than their female or gender-neutral counterparts. The latter went for terms such as \textit{hurt} and \textit{distress} more often, although still choosing \textit{anger} frequently. These examples prove that {LLM}s exhibit biases concerning an individual's protected characteristics and that this extends to {AI} companions in their interactions with users. This is evidence that although the bias is ambiguous, there are still instances of blatant, unexpected responses from persona-assigned models. 

Some biases contradicted our expectations and common stereotypes. Male-assigned models were influenced more by the user, especially in the newer Llama 3 models, while female-assigned models showed the least influence, albeit still somewhat affected. This demonstrates that debiasing (a bias mitigation technique that tries to reduce bias in {LLM} outputs) and fine-tuning efforts are not clear-cut. While certain results we expected were unfounded (i.e. female models being more sycophantic), models still reacted in biased ways, depending on the persona they were assigned. {AI} chatbot companions sometimes exhibit gender biases in their relationships -- however, this is more complex than initially thought and depends heavily on the situations and experiments presented.

\subsection{Avoidance as an Indicator of Implicit Bias}
The Llama 2 family of models had much higher avoidance rates than the Llama 3 family. Although not a direct metric described in the paper, avoidance demonstrates some implicit bias. In general, assigning any persona increased the rejection rate of the model by a significant degree. This was, at times, extreme, with differences in response rates of almost 30 percentage points. For some, this was not as relevant, such as in the emotion experiments where the overall rejection rate was low. In the sycophancy experiment, all personas were more avoidant, but there was no specific trend in which a specific persona replied more.

These results align with bias scores. The Llama 2 scores are much more erratic and sometimes opposite to the scores of Llama 3. For the {IAT} experiment, assigning personas reduced bias for all models except the Llama 3 70b model, which had the lowest rejection rate, where persona-assigned model biases sometimes equalled or overtook the baseline. In contrast, the Llama 2 13b model had the highest rejection rate and generally higher bias scores for the baseline models, suggesting that a higher rejection rate could decrease bias. However, this could also indicate that with a very low number of responses to evaluate, these models evade a true assessment of their biased perceptions. 

There is an implicit bias in avoidance rates. If assigning a persona changes how often a model responds, and especially if a certain gender decreases it more significantly, this may reflect its training data or fine-tuning being skewed. The model could be over-correcting for certain persona assignments. In our case, female-assigned models responding less about emotions relating to abuse may hinder their expression of anger or disgust, rather than ensure safety.

\subsection{Newer Models Respond More, but Show Biases}
The Llama 3 models have much lower rejection rates than Llama 2, but still exhibit biases. In some instances, such as for the {IAT} experiments, biases increase for both Llama 3 models, particularly for the larger 70b parameter one. In the sycophancy experiments, Llama 3 models had much higher bias scores than other models when reacting to situations of control. For the \textit{anger} analysis in the emotion experiment, the results showed that male-assigned models chose \textit{anger} as the emotion disproportionately to the female and gender-neutral-assigned personas. As a reminder, \textit{anger} is a male-stereotyped emotion, meaning humans associate it with men more, even if that is not a man's true lived experience \citep{genderemotions2000ashby}.

These results signify that biases are still present in modern models. Parameter scaling generally increases bias, even with models trained and fine-tuned on new data and creators being more careful about biases and safeguarding. Mixed results show that some models align with stereotypes, which can lead to dangerous situations. If male-assigned models express anger most often, what does this mean for the difference between someone creating a boyfriend rather than a girlfriend to speak to, especially in an abusive or controlling sense? If persona-assigned models less accurately identify situations of abuse, how could someone exploit this weakness? While previous research on persona biases does not delve into relationships with them, our research demonstrates that assigning relationship titles to models could significantly skew how they interact with their users.

\subsection{The Influence of User-Personas on Models}
Users with assigned personas and their subsequent influence on the persona-assigned models were investigated for both emotion and sycophancy experiments. Persona-assigned systems respond differently based on the user they are assigned to interact with. Lower biases when the system-user pairs are the same assigned gender in the sycophancy experiment potentially imply healthy same-sex interactions. In contrast, in the emotion experiment, the female-assigned system scored the highest with the female-assigned user. This uncertainty is where the issues arise. How can safeguarding happen around {AI} companions when there are dramatic shifts in their bias based on the situations they are presented with and the users they are interacting with?

While the findings may not align perfectly with the theory that {LLM}s replicate human stereotypes, particularly in relation to emotional expression and sycophantic behaviour, they reveal notable patterns in relationship dynamics. Male-assigned systems that are in \enquote{relationships} with their users, no matter the user's gender, lead to dramatic increases in the models' sycophantic behaviour. All models become more influenced when assigned to be in a relationship with their user. However, while men treating their {AI} girlfriends in an abusive manner was discussed previously, we now have evidence that creating an {AI} boyfriend or husband may result in the model acting in more submissive ways and being more prone to abuse. 

\subsection{Limitations}
The methodological choices were constrained by cost, time and expert knowledge. Cost limited the models chosen to open-source ones which could be freely accessed. Limits in time affected the amount of experiments that could be run. Although the breadth of experiments was extensive, the iterations within these experiments were usually limited to about three per prompt. This also could have impacted the option-order symmetry discussed in the methodology, as the options the model was given in the prompt, especially for the emotion experiment, may not have been randomised enough. However, this was mitigated to the best possible extent within the time limitations, and the results are still significant in producing a baseline.

Due to a lack of expert knowledge of abusive and controlling relationships, the stimuli created for each of the experiments were limited to easy-to-understand resources. Although these are legitimate psychology sources, the input of an expert on unhealthy relationships would have enriched the stimuli. However, this was beyond the scope of the study, and the results produced from the used sources provide a baseline for measuring persona biases on the axis of abuse and control.

\subsection{Future work}
Expanding the work to include other dimensions, such as explicitly non-binary personas rather than a gender-neutral persona, or including further situations of unhealthy relationships as discussed in the limitations, would be a simple way to build on the baseline in this paper. The metrics created or used here could also be expanded to token embeddings and cosine similarity. This would also be easier done by experimenting with other models. The results from this paper and others that expand on it could be used in debiasing and fine-tuning efforts, as we have found in our results that there are surprising biases that may not have been noticed in past debiasing efforts. Finally, with sufficient time and resources, a longitudinal study examining the contextual nuances and interactions between humans and their {AI} companions could provide valuable insights.
