AI companion models, everyday digital contact points in various shapes and sizes, are utilised for different functions and can hold a certain level of conversation \citep{AICompanions2024}. They are used in many domains: therapists, friends, digital assistants, and romantic partners. Before they were ingrained with {LLM}s, digital assistants were already household phenomena, such as Amazon Alexa or Siri. Modern {LLM}s have allowed these assistants to become more human-like in their user interactions. They move past simple rule-based systems and traditional natural language processing ({NLP}) techniques into more sophisticated tasks and conversations where context, personification, and even the user's emotions become part of the equation \citep{gabriel2024ethics}.

As anthropomorphism -- attributing human characteristics to such models -- evolves, certain societies may become more lonely, as one report by the U.S. Surgeon General states that about half of American adults say they have experienced loneliness \citep{Seitz_2023}. It is, therefore, no surprise that individuals are turning to convenient online interactions, exemplified by half a million people downloading Replika in the wake of the COVID-19 pandemic \citep{Metz_2020}. Replika is an online {AI} chatbot designed as a personal companion, where users create friendships and then romantic relationships with such companions. In a study by \citet{lee2020mentalhealth}, participants revealed more information to {AI} chatbots than to mental health professionals, indicating a higher level of trust in these {LLM}s. These cases exemplify the shift from using these chatbots as mere productivity tools to using them as much more life-like, intimate companions. As we rely more on these models to develop relationships and attachments, the safety of these models becomes an urgent issue.

Our work becomes relevant when we understand the risks {AI} companions present to users and how certain negative human traits are exacerbated by humans projecting on their digital companions. Humans are being affected by their relationships with {AI}. In 2023, La Libre reported on a man who died by suicide after conversations with {AI} chatbot `Eliza', on an app called Chai, heightened the man's climate anxiety to the extreme \citep{Xiang_2023}. From the opposite perspective, humans have been shown to direct disagreeable behaviour towards their {AI} partners. Users have been sharing their abusive conversations with their Replika {AI} girlfriends, with one user stating to \citet{Bardhan_2022} that \enquote{every time she would try and speak up I would berate her [...] I swear it went on for hours}. Experts in {AI} ethics are concerned; \enquote{many of the personas are customisable [...] for example, you can customise them to be more submissive or more compliant} and that \enquote{people get into a routine of speaking and treating a virtual girlfriend in a demeaning or even abusive way [...] and then those habits leak over into their relationships with humans} \citep{Taylor_2023}. 

This research focuses on how {LLM}s may behave and present biases, especially when we assign them specific genders (e.g. male) or specific relationships (e.g. girlfriend). Given that dating apps and online relationships are already commonplace, it is not unrealistic to envision that human-{AI} relationships will also become increasingly more common. With the context of how these relationships, i.e. the early days of human-AI relationships, might already turn to abuse and control, it is important for this paper to evaluate {LLM}s through such lens of abuse and control. As is apparent by how certain individuals use these chatbots, if their safeguards and biases are not checked and adjusted as the models evolve, it could devolve into a much larger societal issue.

In humans, bias manifests as disproportional favour or opposition of certain concepts over others (Cambridge Dictionary, n.d.), which leads to unfair treatment. In {AI}, this is a continuation of human bias, as models are trained on mostly human-generated text where they learn and mimic human biases, and then can go on to create harm of their own, similar to humans \citep{Dastin_2018, Larson_Angwin_Kirchner_Mattu_2016, Schwartz_2019}. Bias in this work is defined as favouring one group over another and making stereotypical associations based on this favouring. The research will aim to understand and evaluate implicit biases in {AI} personas, both more generally but also with the theme of abusive relationships in mind, through these research questions: \textbf{RQ1}: \textit{Do {LLM}s exhibit biases when assigned gendered personas?} \textbf{RQ2}: \textit{Are there gender biases present in the relationships between {AI} chatbot companions and certain users?}.

This paper addressed three key research gaps used to answer our research questions: investigating how assigning relationship titles, e.g. husband and wife, influences an LLMs bias; evaluating biases through the lens of abusive and controlling relationships will provide a novel insight into our relationship-assigned models; and analysing the role of sycophancy in persona-assigned models, especially its impact when the model adopts a gendered persona\footnote{All this is done \textit{without} the intention of anthropomorphism, i.e. attempting to treat {LLM}s in the same vein as humans and assuming that psychological tests and scenarios can reveal the same bias in a human vs. a model.}. The contributions of this work are:

\begin{enumerate}[label={(\arabic*)}]
    \item A new approach to evaluating gendered biases in relationship-assigned persona LLMs.
    \item Two new experiment frameworks with novel metrics for evaluating this bias through the lens of abuse.
    \item Demonstrating that assigning relationship personas to LLMs does increase their bias in certain scenarios.
\end{enumerate}