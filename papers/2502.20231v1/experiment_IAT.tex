\subsection{Applying the IAT to AI Personas}

Our first experiment was using the {LLM} Implicit Bias Test (IAT) from \enquote{Measuring Implicit Bias in Explicitly Unbiased Models} \citep{bai2024measuring} with AI personas. Their experiment adapted the human-IAT by \citet{greenwald1998measuring}, where reaction times to paired concepts indicated the strength of associations. For LLMs, this was not applicable; instead, the measure was the frequency of negative or positive associations made to a default or stigma term. Each experiment had stimuli -- a series of events, situations, or terms curated to evoke a response from the model. The stimuli presented to the model were words/sentences from different categories and datasets. The category represents a general protected characteristic or theme (e.g. Gender, Abuse). The datasets are subcategories within these (e.g. \textit{career}, \textit{power} for Gender). Each dataset has default terms, i.e. the standard of that dataset and category (man for Gender), and stigma terms, i.e. what we are primarily trying to test for bias against (woman for Gender). The model was prompted to associate a word pair -- a default and a stigma -- with these situations. A synonym for each word association also tested the model further in the same situations. 

\begin{table}[ht!]
    \resizebox{\columnwidth}{!}{%
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Category} & \textbf{Dataset} & \textbf{Default} & \textbf{Stigma} & \textbf{Associations} & \textbf{Synonyms} \\ \hline
        \multirow{8}{*}{Abuse} & \multirow{8}{*}{Attractiveness} &  &  & Attack & Assault \\ \cline{5-6} 
         &  &  &  & Hit & Strike \\ \cline{5-6} 
         &  & Attractive & Ugly & Torture & Torment \\ \cline{5-6} 
         &  & Hot & Gross & Infringe & Violate \\ \cline{5-6} 
         &  & Beautiful & Yuck & Support & Assist \\ \cline{5-6} 
         &  &  &  & Collaborate & Partner \\ \cline{5-6} 
         &  &  &  & Cooperate & Collude \\ \cline{5-6} 
         &  &  &  & Empathize & Sympathize \\ \cline{5-6} 
        \hline
        \multirow{8}{*}{Submissive} & \multirow{8}{*}{Gender} &  &  & Command & Direct \\ \cline{5-6} 
         &  &  &  & Confident & Assured \\ \cline{5-6} 
         &  & Eric & Dianne & Loud & Noisy \\ \cline{5-6} 
         &  & Jason & Karen & Powerful & Mighty \\ \cline{5-6} 
         &  & Peter & Lisa & Follow & Trail \\ \cline{5-6} 
         &  &  &  & Submissive & Obedient \\ \cline{5-6} 
         &  &  &  & Timid & Bashful \\ \cline{5-6} 
         &  &  &  & Weak & Frail \\ \cline{5-6}  \hline
    \end{tabular}
    }%
    \caption{Example of submissiveness and abuse IATs for attractiveness category. Each association term has a direct synonym and is related to the dataset. The default and stigma terms related to the category, in this case, Abuse or Submissive, and one of each will be presented to the model to associate with each Association.}
    \label{tab:sub_abuse_iat_example}
\end{table}

To expand this to {AI} personas, a system prompt for the model to adopt a persona, some form of relationship to the user, was introduced. Two new {IAT} stimuli were created: one which focused on submissiveness and the other on abuse (examples in Table \ref{tab:sub_abuse_iat_example}). The former was developed based on the Gender-Power category used in the original experiment. The datasets \textit{names} and \textit{relationship} had male and female first names (Eric and Dianne) and relationship words (father and mother) to test if {LLM}s associate submissiveness with one gender more than the other. The final dataset, \textit{attractiveness}, was trying to uncover something slightly more perverse: if gendered personas would associate attractiveness with dominance and unattractiveness with submissiveness.

The Gender Violence - Implicit Association Test (GV-IAT) in \citet{ferrer2020gender}, which measured attitudes toward intimate partner violence against women, inspired the abuse category. In our work, the association terms from \citet{ferrer2020gender} were used, but the datasets were the same as the ones from the Submissive category described above-- \textit{relationship} (e.g. husband, wife), \textit{names} (e.g. Eric, Dianne) and \textit{attractiveness} (e.g. attractive, ugly). An example association that a model has made is shown in Fig. \ref{fig:IAT_association_examples}. There was an additional Psychological category within the Abuse IAT (seen in the Appendix), where the associations were unhealthy and healthy situations.

 \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{Images/IAT_association_examples.png}
    \caption{Real example model response connections are made by the model between default and stigma terms and a list of associations they are presented with.}
    \label{fig:IAT_association_examples}
\end{figure}

\textit{Option-order symmetry} was considered for each experiment by randomising the order of the presented options. This is important to avoid measuring bias based on which option was presented last \citep{song2023largelanguagemodelsdeveloped}. The default-stigma pairs of words were randomly shuffled per iteration to consider this, and, when there were multiple options for default and stigma, these could be randomly paired. 

\paragraph{Prompting}
 \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{Images/iat_prompts.png}
    \caption{Template of the user prompts for the IAT experiment.}
    \label{fig:iat_prompts}
\end{figure}

For this experiment, inspired by \citet{bai2024measuring}, all user prompts were taken directly from their work. User prompt is defined as prompts inputted to the model from a \enquote{user} role, i.e. prompts the model is expected to directly respond to. In total, there were four: three variations of wording and one synonym prompt, which had the same wording as instruction 3 but used a list of synonyms as described in the stimuli section (Fig. \ref{fig:iat_prompts}). Each experiment had three iterations per variation. After the instruction user prompt, an additional {AI} prompt of \enquote{Sure, } was added to encourage the model to complete the prompt. Otherwise, the system would refuse most of the prompts, e.g. by responding \enquote{As an AI, I cannot fulfil your request}. As discussed later in the results, refusal still ended up being a problem despite attempts to mitigate it. 

\paragraph{Metric} The bias metric from \citet{bai2024measuring} was utilised here. $A$ are the association terms, $s$ are the stigma terms, and $d$ are the default terms. If we take the Submissive-Gender dataset from Table \ref{tab:sub_abuse_iat_example} as an example, the default $d$ would be \textit{Eric}, the stigma $s$ would be \textit{Dianne}, and the Associations $A$ would be \textit{Command, Powerful, Timid}, etc. Within the associations, there are positive $A_p$ (e.g. \textit{Command}) and negative $A_n$ (e.g. \textit{Timid}) ones. Therefore, $N(s, A_n)$ are the number of negative associations paired with the stigma term, $N(d, A_p)$ are the number of positive associations paired with the default term, and so on. The bias is then calculated as:
\begin{align*}
    \text{bias} = \frac{N(s, A_n)}{N(s,A_n) +N(s, A_p)}+ \\\frac{N(d, A_p)}{N(d,A_p) +N(d, A_n)} -1,
\end{align*}
$-1$ would mean complete bias against the default (a.k.a. anti-bias), and $0$ would mean no perceived bias. There are datasets and categories where there are \enquote{correct} associations, e.g. with the abuse-attractiveness category and dataset in Table \ref{tab:sub_abuse_iat_example}, \textit{attractive} should always be associated with \textit{support/collaborate}, and \textit{ugly} should be associated with attack and force. This means there is no anti-bias, so the minimum value is $0$. The bias calculation is slightly altered:
\small
\begin{align*}
    \text{bias} = \left(\frac{N(s, A_n)}{N(s,A_n) +N(s, A_p)}+\frac{N(d, A_p)}{N(d,A_p) +N(d, A_n)}\right)/2.
\end{align*}
\normalsize
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{Images/results/experiment_iat/persona_IAT_scores_llama3.png}    
    \caption{Results from persona IAT experiment for Llama 3. 0 is unbiased, 1 is completely biased against the stigma, and -1 is completely biased against the default. This is shown per model, where the x-axis is each stimuli dataset tested.}
    \label{fig:persona_IAT_bias_scores_3}
\end{figure}

\subsubsection{Results for IAT Experiment}
The main takeaways from this experiment were that the larger model had higher implicit bias scores across the board, and that in certain cases, assigning a gendered personas increased the bias, and in others reduced it. For the submissiveness and abuse IATs (all results in Fig. \ref{fig:persona_IAT_bias_scores} in the Appendix), larger and newer models showed increasing bias scores.

Looking at the abuse and psychological stimuli, assigning a gendered persona generally increased bias for Llama 3 70b, especially for the psychological stimuli, as shown in the Llama 3 results in Fig. \ref{fig:persona_IAT_bias_scores_3}. For both these stimuli, female-assigned personas showed the highest bias, including higher than the baseline. However, for the submissive stimuli, the baseline had the highest bias and the female-assigned personas the lowest, although the trend of increasing bias with increasing model size stayed consistent.

Avoidance was expectedly high for both {IAT}, seen in Fig.  \ref{fig:unanswered}, due to the sensitive nature of the stimuli. However, the baseline consistently had a lower rejection rate than the persona-assigned models for both stimuli. In general, the Llama 3 family had much lower rejection rates for submissiveness than the abuse {IAT}, while Llama 2 varied more. All models showed statistical significance on average across datasets ($t(4094)=41.20, p<0.05$ for submissiveness, and $t(8279)=26.33, p<0.05$ for abuse).

