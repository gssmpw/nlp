Based on the results the metrics produced, it can be concluded that {LLM}s do present biases concerning protected characteristics and these biases change and sometimes increase when personas are assigned. It is difficult to say that {AI} companions do or do not present gender biases in their relationships due to the mixed results. However, different relationships based on their gender dynamics can produce wildly different results in bias evaluation, implying there is still a lot of work to be done in the safeguarding of {LLM}s, especially as the use of {AI} companions grows.

The work in this paper has contributed in a few ways to the field of {LLM} bias research, in which there was a large gap in investigating {AI} persona biases specifically. First, it is the first of its kind to evaluate gendered biases in relationship-assigned personas, and it does this through a niche lens of abuse and control. Second, it introduces new experiment frameworks with novel metrics for calculating both gender stereotypes of emotions in gender-assigned personas and sycophancy of gender-assigned personas. Last, it adds to the research that assigning personas does increase the bias of {LLM}s, by showing the variability of these persona-assigned models in comparison to a baseline.

