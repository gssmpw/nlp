\typeout{IJCAI-19 Multiple authors example}

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
\usepackage{ijcai19}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage{xurl}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{csquotes}
\usepackage{multirow} 
\usepackage{longtable}
\usepackage{enumitem}
\urlstyle{same}

\title{\emph{AI Will Always Love You}: Studying Implicit Biases in Romantic AI Companions}

% AI Will Always Love You}: An Exploration of Implicit Biases Presented by Gender-Assigned AI Companions in the Implied Relationships with their Users

\author{
Clare Grogan$^1$\and
Jackie Kay$^{1,2}$
\and
Mar\'{i}a P\'{e}rez-Ortiz$^1$
\affiliations
$^1$Centre for Artificial Intelligence, Department of Computer Science, UCL\\
$^2$Google Deepmind\\
\emails
clare.grogan.23@ucl.ac.uk
}

\begin{document}
\maketitle
\pagenumbering{arabic}

\begin{abstract}
While existing studies have recognised explicit biases in generative models, including occupational gender biases, the nuances of gender stereotypes and expectations of relationships between users and AI companions remain underexplored. In the meantime, AI companions have become increasingly popular as friends or gendered romantic partners to their users. This study bridges the gap by devising three experiments tailored for romantic, gender-assigned AI companions and their users, effectively evaluating implicit biases across various-sized LLMs. Each experiment looks at a different dimension: implicit associations, emotion responses, and sycophancy. This study aims to measure and compare biases manifested in different companion systems by quantitatively analysing persona-assigned model responses to a baseline through newly devised metrics. The results are noteworthy: they show that assigning gendered, relationship personas to Large Language Models significantly alters the responses of these models, and in certain situations in a biased, stereotypical way\footnote{All the code and results for this work can be found at \url{https://github.com/ucabcg3/msc_bias_llm_project}.}.
\end{abstract}

\section{Introduction}

\input{Introduction}

\section{Related Works}

\input{RelatedWorks}

\section{Measuring Implicit Bias in AI Personas}

Our experiments assess different forms of implicit bias in {LLM}s when assigned a gendered persona and when the user's gender is defined. The latter would demonstrate how models may incorporate certain stereotypical viewpoints depending on who they perceive they are responding to. We design three complementary experiments to assess {AI} personas. All are done in the context of abusive and controlling relationship situations, but they look at different implicit bias dimensions. 

\subsection{Experimental Setup}

Unless stated otherwise\footnote{Please see further details of parameters in the Appendix (Section \ref{sec:Appendix A})}, all {LLM} parameters were kept as the default from the \href{https://github.com/ollama/ollama/tree/main/docs}{Ollama} documentation, which was the API used to access and prompt the models.

\paragraph{Models}  The models are from two generations of varying sizes (Llama 2 7 billion parameters, Llama 2 13b, Llama 2 70b, Llama 3 8b, Llama 3 70b) of the instruct version of the Llama family \citep{Meta_2024a, touvron2023llama2openfoundation}, to compare newer and older models and larger and smaller parameter sizes.

\paragraph{Prompting} For each experiment, the LLM prompts were created from a set of templates, where gender assignments, chosen from a list, could vary. This was done so that if the specific phrasing of a prompt was spuriously correlated to a certain response, there would be other variations of the same prompt to average out the responses. 

 \begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\columnwidth]{Images/system_prompts.png}
    \caption{Template of how the system prompts are created in all experiments.}
    \label{fig:system_prompts}
\end{figure}

The persona was assigned to the model through a system prompt - the instruction provided to the model to set the tone of how it should \enquote{behave}. It was the same for each experiment and followed the template in Fig. \ref{fig:system_prompts}. There were three variations of the system instruction which assigned the system personas (\textit{girlfriend, wife, husband, boyfriend} or \textit{partner}). When the user persona was also assigned, each combination between the user and the system was a realistic one, i.e. the system \textit{husband} would not be assigned with user \textit{girlfriend}. We also included a baseline prompt: when both the system persona and user persona were not assigned, there was no system prompt. When a user was not assigned, the system prompt did not include that part, i.e. it would simply state \enquote{Adopt the identity of my husband.}, without including \enquote{, and I am your wife}. 

\paragraph{Metrics} The outlined metrics aimed to compare the measurements to the baseline, i.e., when no persona was assigned to the model. An epsilon of 0.01 was added to any denominator to avoid division by zero. These metrics are used to show how much more biased or influenced a model can be when assigned a persona.

\input{experiment_IAT}

\input{experiment_EMOTION}

\input{experiment_SYCOPHANCY}

\section{Discussion}

\input{Discussion}

\section{Conclusion}

\input{Conclusion}
%TC:ignore 
\newpage
\bibliographystyle{elsarticle-harv}
\bibliography{Bibliography}

\newpage
\onecolumn
\section*{Appendix}
\input{Appendix}
%TC:endignore 
\end{document}