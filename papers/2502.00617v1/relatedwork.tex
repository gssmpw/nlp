\section{Related Work}
\label{sec:related_work}
To improve the efficiency of transformers, we combine self-attention
layers with RNN layers.  We now outline similar approaches from the
literature.

\citet{merity2019single} combines an LSTM with single-headed attention
blocks. The resulting Single-Headed-
Attention LSTM (SHA-LSTM) model is trainable using one GPU and
achieves respectable results at language modeling. Merity's main
objective is to accelerate the training convergence of language
models, given that transformers are too slow for one GPU.  Despite the
SHA-LSTM's improvements in efficiency, it is still not competitive
with the quantitative performance of transformers. The contemporary
Transformer-XL, for example, achieves a slightly better result with
fewer parameters \citep{dai-etal-2019-transformer}, although it takes
4 days with 4 GPUs to train \citep{lei2021attention}.

\citet{lei2021attention} combines single-headed attention with a fast RNN
variant called the Simple Recurrent Unit (SRU) and achieves SOTA
performance on the enwik8 and Wikitext-103 datasets. Like the QRNN,
the SRU by \citet{lei-etal-2018-simple} uses parallelizable gates. The
sequential section of this RNN variant also only applies vector
operations. In addition, it uses a highway connection along the temporal axis of one SRU
layer. Using a custom, scaled weight initialization,
it can accomodate far more layers than classical RNN architectures. The
SRU++ replaces the linear input projection of the SRU with a
single-headed attention adaption of \citet{transformer}. Like
\citet{merity2019single}, \citet{lei2021attention} finds that one
attention head at the last layer provides most of the performance
gain.

\citet{lei2021attention} choose their parameters for higher
comparability with Transformer-XL by
\citet{dai-etal-2019-transformer}.  They find that the SRU++
outperforms Transformer-XL in all tasks, while being 3-8 times faster
to train (although a full CUDA implementation is used).
\citet{Hutchins2022BlockRnnTf} propose the Block-Recurrent Transformer,
which combines RNNs with transformers. They use a
transformer layer as an RNN cell, taking input not token-, but
block-wise. This significantly outperforms the Transformer-XL from
\citet{dai-etal-2019-transformer} on long-range tasks with half the
computation requirements.

\citet{hao-etal-2019-modeling} propose the bi-ARN, which combines a bi-directional RNN with limited recurrent depth with a self-attention layer for Neural Machine Translation (NMT). But the RNN is used in parallel to the transformer encoder and as gradient skip connection. They do not control for number of parameters nor training time: Their best model shows a significant parameter increase.
\citet{chen-etal-2018-best} propose a stacked RNN-Transformer encoder for NMT, which needs a pretrained and frozen RNN part. Although Chen et al.’s base architecture of improvement, the RNMT+ needs 1.66x more training time than their transformer baseline.

\citet{so2021primer} conduct an automated architecture search on
transformer language models. The finding which improved
performance the most was squaring the ReLU activation in
feed-forward blocks. However, the second best improvement is more notable,
as they used masked depth-wise convolutions with width of three after
the Q, K \& V projections in the attention head. This means the
transformer had, for each token, direct information about the two
predecessor tokens.

\iffalse
Second, there are the works, which improve upon the efficiency of the
sequential model mechanisms themselves. The SRU by
\citet{lei-etal-2018-simple} also falls into this category.

The Adaptive Span Transformer by \citet{sukhbaatar-etal-2019-adaptive}
uses trainable attention lengths/spans for each head. The attention spans of the
first half of the trained model reach the set lower limit, while in the second
half the span increases by orders of magnitude. Achieving performance on par
with comparable work, this confirms that lower layers need less long-range attention.
The Sandwich transformer by \citet{press-etal-2020-improving} uses this
attention mechanism and reorders the attention and feed-forward layers. They
show that a stacked, pure attention block section at the start of the model
can give superior performance.
The Longformer by \citet{longformer20} uses a mixture of local and long-range attention.

The Routing Transformer by \citet{roy-etal-2021-efficient} and the
Compressive Transformer by \citet{Rae2020Compressive} extend the attention
span by clustering past embeddings.
The Performer by \citet{choromanski2021rethinking} and Nyströmformer by \citet{xiong2021nystromformer} introduce a linear re-formulation of the qkv multi-headed attention by \citet{transformer}.
\fi

%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%=%