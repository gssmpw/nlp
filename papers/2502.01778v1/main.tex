%%%%%%%% ICML 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

% \documentclass[nohyperref]{article}
\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage[caption=false,
% font=normalsize,
% labelfont=sf
% ,textfont=sf
]{subfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2022} with \usepackage[nohyperref]{icml2022} above.
\usepackage{hyperref}

\newcommand{\soc}{\text{SoC}}
\newcommand{\cpo}{\text{cpo}}
\newcommand{\dep}{\text{dep}}
\newcommand{\arr}{\text{arr}}
\newcommand{\ch}{\text{ch}}
\newcommand{\dis}{\text{dis}}
\newcommand{\cs}{\text{cs}}
\newcommand{\ev}{\text{ev}}
\newcommand{\tr}{\text{tr}}
\newcommand{\set}{\text{set}}
\newcommand{\tot}{\text{tot}}
\newcommand{\pot}{\text{pot}}
\newcommand{\AC}{\text{AC}}
\newcommand{\DC}{\text{DC}}
\newcommand{\cyc}{\text{cyc}}
\newcommand{\lost}{\text{lost}}
\newcommand{\cale}{\text{cal}}
\newcommand{\total}{\text{tot}}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{tabularray}
\usepackage{rotating}
% \usepackage[margin=1in]{geometry}
\usepackage{graphicx}
% \usepackage{subcaption}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Transformer Based Deep Reinforcement Learning for EV Smart Charging}
% \icmltitlerunning{Impact of the training dataset in multiobjective optimization using Decision Transformers}
\icmltitlerunning{GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments}

\begin{document}

\twocolumn[
\icmltitle{GNN-DT: Graph Neural Network Enhanced Decision Transformer for Efficient Optimization in Dynamic Environments}
% \icmltitle{Improving the Sample Efficiency of Decision Transformers for Real-World Optimization Tasks}
% \icmltitle{Improving the Sample Efficiency of Decision Transformers for Real-World Optimization Tasks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2022
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Stavros~Orfanoudakis}{yyy}
\icmlauthor{Nanda Kishor Panda}{yyy}
\icmlauthor{Peter Palensky}{yyy}
\icmlauthor{Pedro~P.~Vergara}{yyy}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Intelligent Electrical Power Grids, Faculty of, Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Delft, The Netherlands}


\icmlcorrespondingauthor{Stavros Orfanoudakis}{s.orfanoudakis@tudelft.nl}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Electric Vehicle Charging, Graph Neural Networks, Large Language Models, Mathematical Optimization, Offline Reinforcement Learning}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.
%
\begin{abstract}
% Real-world sequential decision-making problems often involve dynamic state-action spaces, larger scale, and sparse rewards, posing significant challenges for conventional optimization and Reinforcement Learning (RL) methods.
Reinforcement Learning (RL) methods used for solving real-world optimization problems often involve dynamic state-action spaces, larger scale, and sparse rewards, leading to significant challenges in convergence, scalability, and efficient exploration of the solution space. This study introduces GNN-DT, a novel Decision Transformer (DT) architecture that integrates Graph Neural Network (GNN) embedders with a novel residual connection between input and output tokens crucial for handling dynamic environments. By learning from previously collected trajectories, GNN-DT reduces dependence on accurate simulators and tackles the sparse rewards limitations of online RL algorithms. We evaluate GNN-DT on the complex electric vehicle (EV) charging optimization problem and prove that its performance is superior and requires significantly fewer training trajectories, thus improving sample efficiency compared to existing DT baselines. Furthermore, GNN-DT exhibits robust generalization to unseen environments and larger action spaces, addressing a critical gap in prior DT-based approaches.
\end{abstract}

\section{Introduction}

% Solving optimization problems is critical for the efficient operation of a wide array of industries. From power systems control~\cite{ROALD2023108725} and logistics~\cite{konstantakopoulos_vehicle_2022} to portfolio management~\cite{gunjan2023brief} and advanced manufacturing processes~\cite{2020OptimizationOM}, real-time and effective decision making depends on solving these problems efficiently in real-time. However, many optimization problems encountered in practice are characterized by high complexity and large scale, often rendering conventional solvers impractical for these applications~\cite{8187196}.
% This is particularly evident in dynamic environments, where the optimization landscape continuously changes, demanding solutions in real-time.

Sequential decision-making problems are critical for efficiently operating a wide array of industries, such as power systems control~\cite{ROALD2023108725}, logistics optimization~\cite{konstantakopoulos_vehicle_2022}, portfolio management~\cite{gunjan2023brief}, and advanced manufacturing processes~\cite{2020OptimizationOM}. However, many practical problems, such as the electric vehicle (EV) charging optimization~\cite{n2024quantifying}, are large-scale, have temporal dependencies, and aggregated constraints, often making conventional methods impractical~\cite{8187196}. 
This is especially observed in dynamic environments, where the optimization landscape continuously evolves, requiring real-time solutions.

Reinforcement learning (RL)~\cite{SuttonReinforcementIntroduction} has been extensively studied for solving optimization problems due to its ability to manage uncertainty, adapt to dynamic environments, and enhance decision-making through trial-and-error~\cite{lan2023learning, ZHANG2023205}. In complex and large-scale scenarios, RL can provide high-quality solutions in real-time compared to traditional mathematical programming techniques that fail to do so~\cite{jaimungal_reinforcement_2022}. However, RL approaches face significant challenges, such as sparse reward signals that slow learning and hinder convergence to optimal policies~\cite{dulac-arnold_challenges_2021}. In addition, RL solutions struggle to generalize when deployed in environments different from the one they were trained in, limiting their applicability in real-world scenarios with constantly changing conditions~\cite{wang_domain_2024}.

Decision Transformers (DT)~\cite{chen2021decision} is an offline RL algorithm that reframes traditional RL problems as generative sequence modeling tasks conditioned on future rewards~\cite{Zhang2023}.
% DTs utilize previously collected trajectories to inform decision-making without requiring an environment simulator.
By learning from historical data, DTs effectively address the sparse reward issue inherent in online RL, relying on demonstrated successful outcomes instead of extensive trial-and-error exploration. However, the trajectory-stitching mechanism of DT often proves insufficient in dynamic real-world environments, leading to suboptimal policies. Although improved variants such as Q-regularized DT (Q-DT)~\cite{qdt} incorporate additional constraints for greater robustness, they still face significant challenges in generalizing across non-stationary tasks~\cite{10.5555/3600270.3603094}. Consequently, further architectural advances and training strategies are essential to ensure consistent performance in complex environments.



This study introduces GNN-DT\footnote{The code can be found at \url{https://github.com/StavrosOrf/DT4EVs}.}, a novel DT architecture that leverages the permutation-equivariant properties of Graph Neural Networks (GNNs) to handle dynamically changing state-action spaces (i.e., varying numbers of nodes over time) and improve generalization. By generating embeddings that remain consistent under node reordering, GNNs offer a powerful way to capture relational information in complex dynamic environments. Moreover, {GNN-DT} features a novel residual connection between input and output tokens, ensuring that action outputs are informed by the dynamically learned state embeddings for more robust decision-making. 
To demonstrate the superior performance of the proposed method, we conduct extensive experiments on the complex multi-objective EV charging optimization problem~\cite{10803908}, which encompasses sparse rewards, temporal dependencies, and aggregated constraints.
The main contributions are summarized as follows:
\begin{itemize}
    \item Introducing a novel DT architecture integrating GNN embeddings, resulting in enhanced sample efficiency, superior performance, robust generalization to unseen environments, and effective scalability to larger action spaces, demonstrating the critical role of GNN-based embeddings in the model's improvement.
    \item Demonstrating that online RL algorithms and offline DT baselines, even when trained on diverse datasets (Optimal, Random, Business-as-Usual) with varying sample sizes, perform inferior to GNN-DT when dealing with real-world optimization tasks.
    \item Proving that both the size and type of training dataset critically influence the learning process of DTs, highlighting the importance of dataset selection.
    \item Highlighting that strategically integrating high- and low-quality training data (Optimal \& Random datasets) significantly enhances policy learning, outperforming models trained exclusively on single-policy datasets.
\end{itemize}




\section{Related Work}

% Paragraph: Talk about DT
\paragraph{Advancements in Decision Transformers}
Classic DT encounters significant challenges, including limited trajectory stitching capabilities and difficulties in adapting to online environments. To address these issues, several enhancements have been proposed. The Q-DT~\cite{qdt} improves the ability to derive optimal policies from sub-optimal trajectories by relabeling return-to-go values in the training data. Elastic DT~\cite{10.5555/3666122.3666936} enhances classic DT by enabling trajectory stitching during action inference at test time, while Multi-Game DT~\cite{10.5555/3600270.3602295} advances its task generalization capabilities. The Online DT~\cite{online_dt,3056976d4116471a86ab3fa345b1695d} extends DTs to online settings by combining offline pretraining with online fine-tuning, facilitating continuous policy updates in dynamic environments. Additionally, adaptations for offline safe RL incorporate cost tokens alongside rewards~\cite{liu2023constrained, pmlr-v238-hong24a}. DT has also been effectively applied to real-world domains, such as healthcare~\cite{Zhang2023} and chip design~\cite{pmlr-v202-lai23c}, showcasing its versatility and practical utility.

\paragraph{RL for EV Smart Charging}
RL algorithms offer notable advantages for EV dispatch, including the ability to handle nonlinear models, robustly quantify uncertainty, and deliver faster computation than traditional mathematical programming~\cite{QIU2023113052}. Popular methods, such as DDPG~\cite{JIN2022120140}, SAC~\cite{9211734}, and batch RL~\cite{8727484}, show promise but often lack formal constraint satisfaction guarantees and struggle to scale with high-dimensional state-action spaces~\cite{isgt2024, 9465776}. Safe RL frameworks address these drawbacks by imposing constraints via constrained MDPs, but typically sacrifice performance and scalability~\cite{ZHANG2023121490, chen2022deep}. Multiagent RL techniques distribute complexity across multiple agents, e.g. charging points, stations, or aggregators~\cite{KAMRANI2025100620}, yet still encounter convergence challenges and may underperform in large-scale applications. 
To the best of our knowledge, no study has used DTs for solving the complex EV charging problem, despite DT's potential to handle sparse rewards effectively.

\section{Preliminaries}
In this section, an introduction to offline RL and the mathematical formulation of the EV charging optimization problem is presented as an example of what type of problems can be solved by the proposed GNN-DT methodology.

\subsection{Offline RL}

Offline RL aims to learn a policy \(\pi_\theta(a\!\mid\!s)\) that maximizes the expected discounted return \(\mathbb{E}\bigl[\sum_{t=0}^{\infty} \gamma^t R(s_t,a_t)\bigr]\)
without additional interactions with the environment~\cite{levine2020offline}. 
A Markov Decision Process (MDP) is defined by the tuple 
\((S, A, P, R, \gamma)\), 
where \(S\) is the state space, \(A\) the action space, \(P\) the transition function, 
\(R\) the reward function, \(\gamma \in (0,1]\) the discount factor~\cite{SuttonReinforcementIntroduction}.
In the offline setting, a static dataset 
\(\mathcal{D} = \{(s, a, r)\}\),
collected by a (potentially suboptimal) policy, is provided.
DTs leverage this dataset by treating
RL trajectories as sequences, learning to predict actions that
maximize returns based on previously collected experiences.
A key component in DTs is the 
\emph{return-to-go} (RTG), which for a time step \(t\) can be defined as:
\begin{equation}
    G_t = \sum_{\tau=t}^{T} \gamma^{\tau-t} \, r_\tau,
\end{equation}
representing the discounted cumulative reward from \(t\) 
until the terminal time \(T\). This formulation is particularly 
beneficial when real-time exploration is costly or impractical, 
while sufficient historical data remain available for training. 


\subsection{The EV Smart Charging Problem}
\label{sec:mip}
We consider a set of $\mathcal{I}$  charging stations indexed i, all assumed to be controlled by a charge point operator (CPO) over a time window $\mathcal{T}$, divided into $T$ non-overlapping length intervals $\Delta t>0$.  For a given time window, each charging station $i$ operates a set of  $\mathcal{J}$ non-overlapping charging sessions, denoted by $\mathcal{J}_i=\{j_{i,1}, \cdots, j_{i,J_i}\}$, where $J_{i,j}$ represents the $j^{th}$ charging event at the $i^{th}$ charging station and $J_i=|\mathcal{J}_i|$ is the total number of charging sessions seen by charging station i in an episode. A charging session is then represented as $j_{j,c}: \{t^a_{j,i},t^d_{j,i},\bar{p}_{j,i}, e^*_{j,i}\}, ~ \forall j,i$, where
% A mapping further represents each charging session as
% \begin{equation}
%     s_{cs}: \{(t^a, t^a_{in}), (t^d, t^d_{in}), (\bar{p}, \bar{p}_{in}), (\bar{e}, \bar{e}_{in})\}, ~ \forall i,n
% \end{equation}
$t^a, t^d, \bar{p}$ and $e^*$ represent the arrival time, departure time, maximum charging power, and the desired battery energy level at the departure time. The primary goal is to minimize the total energy cost given by:
\begin{equation}  
    f_1(p^+,p^-) = \sum_{t\in \mathcal{T}}\sum_{i \in \mathcal{I}} \Delta t (\Pi^{+}_tp_{i,t}^+ - \Pi^-_tp_{i,t}^-)
    \label{eq:mo1}
\end{equation}
$p^+_{i,t}$ and $p^-_{i,t}$ denote the charging or discharging power of the $i^{th}$ charging station during time interval $t$.
% , which is assumed to be constant within the time interval.
$\Pi^+_t$ and $\Pi^-_t$  are the charging and discharging costs, respectively. Along with minimizing the total energy costs 
% (maximizing $-f_1$)
, the CPO also wants the aggregate power of all the charging stations ($p^{\sum}_t=\sum_{i\in \mathcal{I}}p^+_{i,t}-p^-_{i,t}$) to remain below the set power limit $p_t^*$. By doing so, the CPO avoids paying penalties due to overuse of network capacity. As the set point keeps on updated based on external factors, we introduce the penalty:
\begin{equation}
    f_2(p^+,p^-) = \sum_{t\in \mathcal{T}}\max\{0, p_t^{\sum}-p^*_t\},
    % \quad \quad \forall t \in \mathcal{T} 
    \label{eq:mo2}
\end{equation}

Maintaining the desired battery charge at departure is important for EV user satisfaction, which we model as:
\begin{equation}
    f_3(p^+, p^-)\hspace{-0.5mm} = \hspace{-0.5mm} \sum_{i\in \mathcal{I}}\sum_{j\in \mathcal{J}_i} \left( 
    \sum_{t=t_{j,i}^a}^{{t_{j,i}^a}}
    (p_{i,t}^+-p_{i,t}^-)-e^*_{j,i}
    \right)^2
    \label{eq:mo3}
\end{equation}
Eq.~\eqref{eq:mo3} defines a sparse reward added at each EV departure based on its departure energy level. Building on these objectives, the EV charging problem is formulated as:
\begin{equation}
    \max_{p^+,p^-} \left(f_1(\cdot) - 100 \cdot f_2(\cdot) - 10 \cdot f_3(\cdot)\right)
        % \max_{p^+,p^-} \left(f_1(\cdot) - 100 \cdot f_2(p^+,p^-) - 10 \cdot f_3(p^+,p^-)\right)
    \label{eq:mo4}
\end{equation}
% \begin{equation}
% \begin{aligned}
% \text{subject to:}\quad & \mathbf{A}\,\mathbf{x} \le \mathbf{b},\quad \mathbf{C}\,\mathbf{x} = \mathbf{d},\quad \mathbf{l} \le \mathbf{x} \le \mathbf{u}.
% \end{aligned}
% \label{eq:mo5}
% \end{equation}
The multi-objective optimization function in Eq.\ref{eq:mo4} integrates Eqs.\ref{eq:mo1}–\ref{eq:mo3} using experimentally determined coefficients based on practical importance. This mixed integer programming problem is subject to lower-level operational constraints (e.g., EV battery, power levels) as detailed in Appendix\ref{app_1}.

\paragraph{EV Charging MDP}

The optimal EV charging problem can be framed as an MDP: $\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mathcal{P}, R)$.
% where $S$ is the state space, $A$ is the action space, $P$ is the transition probability function, and $R$ is the reward function.
At any time step $t$, the state $\boldsymbol{s}_t \in \mathcal{S}$ is represented by a graph $\mathcal{G}_t = (\mathcal{N}_t, \mathcal{E}_t)$, where $\mathcal{N}_t$ is the set of nodes and $\mathcal{E}_t$ is the set of edges. 
Each node $n \in \mathcal{N}_t$ has a feature vector $\boldsymbol{x}_n^t \in \mathbb{R}^d$, capturing node-dependent information such as power limits and prices. 
This graph structure~\cite{Orfanoudakis2024} efficiently models evolving relationships among EVs, chargers, and the grid infrastructure.
The action space $\mathbf{a}_t \in \mathcal{A}$ is represented by a dynamic\footnote{Note that the number of nodes in the state and action graph can vary in each step, because EVs arrive and depart.}  graph $\mathcal{G}^{\mathbf{a}}_t=(\mathcal{N}^{\mathbf{a}}_t,\mathcal{E}^{\mathbf{a}}_t)$, where nodes $\mathcal{N}^{\mathbf{a}}_t$ correspond to the decision variables of the optimization problem (e.g., EVs).  Each node $n \in \mathcal{N}^{\mathbf{a}}_t$ represents a single action $a_{i,t} \in \mathbf{a}_t$, scaled by the corresponding charger's maximum power limit.  For charging, $a_{i,t} \in [0,1]$, and for discharging, $a_{i,t} \in [-1,0)$. The transition function $\mathcal{P}(\boldsymbol{s}_{t+1} \mid \boldsymbol{s}_t, \mathbf{a}_t)$ accounts for uncertainties in EV arrivals, departures, energy demands, and grid fluctuations. 
Finally, the reward $R(\boldsymbol{s}_t, \mathbf{a}_t)$ aligns with the objective described in Eq.~\ref{eq:mo4}, guiding the policy to maximize cost savings, respect operational constraints, and meet EV driver requirements. While Eqs.\ref{eq:mo1} and \ref{eq:mo2} represent individual EV rewards and aggregated EV penalties, respectively, Eq.\ref{eq:mo3} introduces a sparse reward that activates only when an EV departs, thereby creating complex temporal dependencies.




% ####################################################################
% ####################################################################
% ####################################################################

\section{GNN-based Decision Transformer}

\begin{figure*}[!t]
\centering
\includegraphics[width=\linewidth]{Figures/DT4EVs_Overview.pdf}
\vspace{-5mm}
\caption{Overview of the GNN-DT architecture. The input sequence, comprising return-to-go, action, and state, is processed through specialized embedding modules. The action graph $\mathcal{G}^{\mathbf{a}}_t=(\mathcal{N}^{\mathbf{a}}_t,\mathcal{E}^{\mathbf{a}}_t )$, with nodes $\mathcal{N}^{\mathbf{a}}_t \subset \mathcal{N}_t$, and the state graph $\mathcal{G}_t=(\mathcal{N}_t,\mathcal{E}_t )$ are encoded using GNN-based embedders to produce embeddings of dimension $F_L$. These embeddings serve as inputs to a GPT-2–based causal transformer, which predicts the next action token. The predicted action token acts as a decoder, generating actions by multiplying with specific GNN state node embeddings. This architecture enables efficient decision-making in scenarios with dynamic states and actions.
}
\label{fig:gnndt_ow}
\end{figure*}

% GNN-DT is an innovative architecture designed to solve dynamic optimization problems in complex environments through three key processes (see Fig.~\ref{fig:gnndt_ow}):: 

% First, it employs specialized embedders to encode the past \( K \) steps of variable-sized actions, states, and Return-to-Go values. Second, a causal transformer processes these embedded tokens to generate action tokens for each step. Finally, the action tokens are integrated with state-node embeddings to determine the final actions for each node; supporting dynamic action-spaces.

The innovative GNN-DT architecture (Fig.~\ref{fig:gnndt_ow}) efficiently solves optimization problems in complex environments with dynamic state-action spaces by embedding past actions, states, and returns-to-go, using a causal transformer to generate action tokens, and integrating these with current state embeddings to determine final actions within the dynamically changing action space.

\subsection{Sequence Embeddings}

In GNN-DT, each input ``modality'' is processed by a specialized embedding network. The state graph passes through the \emph{State Embedder}, the action through the \emph{Action Embedder}, and the return-to-go value through a simple Multi-Layer Perceptron (MLP). Compared to standard MLP embedders, GNNs provide embeddings for states and actions invariant to the number of nodes by capturing the graph structure. This design makes GNN-DT more sample-efficient during training and better at generalizing to unseen environments.

In detail, the \textit{State Embedder} consists of $L$ consecutive Graph Convolutional Network (GCN)~\cite{kipf2016semisupervised} layers, which aggregate information from neighboring nodes as follows:
\begin{equation}
    \boldsymbol{x}_t^{(l+1)} = \sigma \Bigl(D^{-1/2} A_t D^{-1/2} \boldsymbol{x}_t^{(l)} W^{(l)}\Bigr),
\end{equation}
where $\boldsymbol{x}_t^{(l)} \in \mathbb{R}^{N_t \times F_l}$ denotes the node embeddings at layer $l$ with $N_t$ number of nodes, $W^{(l)} \in \mathbb{R}^{F_l \times F_{l+1}}$ are trainable weights, $\sigma(\cdot)$ is a nonlinear activation (ReLU), $A_t$ is the adjacency matrix of the state graph $\mathcal{G}_t$, and $D$ is the degree matrix for normalization. After the final layer, a mean-pooling operation produces a fixed-size state embedding:
\begin{equation}
    \boldsymbol{\widetilde{s}}_{t} = \frac{1}{\lvert \mathcal{N}_t \rvert} \sum_{n \in \mathcal{N}_t} \boldsymbol{x}_{n,t}^{(L)},
\end{equation}
where $\boldsymbol{x}_n^{(L)}$ is the embedding of node $n$ at the $L$-th layer. This pooling step ensures that the state embedding is invariant to the number of nodes in the graph, enabling the architecture to scale with any number of EVs or chargers.

Similarly, the \textit{Action Embedder} processes the action graph $\mathcal{G}^{\mathbf{a}}_t=(\mathcal{N}^{\mathbf{a}}_t,\mathcal{E}^{\mathbf{a}}_t)$ through $C$ GCN layers followed by mean pooling, producing the action embedding $\widetilde{\textbf{a}}_{t}$. All embedding vectors (states, actions, or the return-to-go value) have the same dimensions. 
This design leverages the dynamic and invariant nature of GCN-based embeddings, allowing the DT to handle variable-sized graphs.

\subsection{Decoding Actions}

Once the embedding sequence of length $K$ is constructed\footnote{During inference the action ($\mathbf{a}_t$) and RTG ($\boldsymbol{\widehat{R}}_t$) of the last step $t$ are filled with zeros as they are not known.}, it is passed through the causal transformer GPT-2~\cite{chen2021decision} to produce a fixed-size output vector 
$\boldsymbol{y}_t \in \mathbb{R}^{F_L}$ for each step. Because DT architectures inherently generate outputs of fixed dimensions, an additional mechanism is required to manage dynamic action spaces. To address this, GNN-DT implements a residual connection that merges the final GCN layer embeddings $\boldsymbol{x}_t^{(L)}$ with the transformer output $\boldsymbol{y}_t$ for every step of the sequence.

Specifically, 
% let \(\mathcal{G}^* = (\mathcal{N}^*, \mathcal{E}^*)\) be a fully connected action graph, and let \
for each node $n \in \mathcal{N}_t^{\mathbf{a}}$, we retrieve its corresponding state embedding $\boldsymbol{x}_{n,t}^{(L)} \in \mathbb{R}^{1\times F_L}$ and multiply it with the transformer output token $\boldsymbol{y}_t\in \mathbb{R}^{1\times F_L}$, yielding the final action for node $n$:
\begin{equation}  
    \hat{\mathrm{a}}_{n,t} =  \boldsymbol{y}_t ^\mathsf{T} \cdot \boldsymbol{x}_{n,t}^{(L)}.
    \label{eq:meth1}
\end{equation}
By repeating Eq.~\ref{eq:meth1} for every step $t$ and every node $n \in \mathcal{N}^{\mathbf{a}}_t$ the final action vector $\hat{\mathbf{a}}_t$ is generated.
This design allows the model to maintain a fixed-size output from the DT while dynamically adapting to any number of nodes (and hence actions). It effectively combines the high-level context learned by the transformer with the node-specific state information captured by the GNN, enabling robust, scalable decision-making even as the graph structure changes.

\begin{figure*}[!t]     
    \centering
    \includegraphics[width=1\textwidth]{Figures/DT4EVs_main_results.pdf}
    \vspace{-5mm}
        \caption{Training performance comparison of DT variations (classic DT, Q-DT, and GNN-DT) across 3 different datasets with 10.000 trajectories each and classic online RL algorithms for the same problem.
        }
        \label{fig:perf_comp}
\end{figure*}

% \begin{figure*}[!t]     
%      \hfill
%      \subfloat[Optimal Dataset\\ (K=10)]{
%          \centering
%          \includegraphics[width=0.252\textwidth]{Figures/plot_performance_optimal_10000.pdf}
%          \label{fig:pf_a}
%          }
%      \hspace{-5.25mm}     
%      \subfloat[Random Dataset\\ (K=10)]{
%          \centering
%          \includegraphics[width=0.255\textwidth]{Figures/plot_performance_random_10000.pdf}
%          \label{fig:pf_b}
%          }    
%     \hspace{-5.5mm}     
%      \subfloat[BaU Dataset\\ (K=10)]{
%          \centering
%          \includegraphics[width=0.255\textwidth]{Figures/plot_performance_bau_10000.pdf}
%          \label{fig:pf_c}
%          }    
%     \hspace{-5.5mm}     
%      \subfloat[Online RL]{
%          \centering
%          \includegraphics[width=0.255\textwidth]{Figures/plot_performance_classicRL.pdf}
%          \label{fig:pf_d}
%          }
%          % \hfill
%          % \vspace{-2mm}
%         \caption{Training performance comparison of DT variations (classic DT, Q-DT, and GNN-DT) across 3 different datasets with 10.000 trajectories each and classic online RL algorithms for the same problem.
%         }
%         \label{fig:perf_comp}
% \end{figure*}

\subsection{Action Masking and Loss Function}

In GNN-DT, the learning of infeasible actions, such as charging an unavailable EV, is avoided through action masking. At each time step $t$, a mask vector $\mathbf{m}_t$, which has the same dimension as $\mathbf{a}_t$, is generated with zeros marking invalid actions and ones marking valid actions. For example, an action is invalid when the $a_{i,t} \neq 0$ and no EV is connected at charger $i$.
The mean squared error between the predicted actions $\widehat{\mathbf{a}}_t$ and ground-truth actions $\mathbf{a}_t$ from expert or offline trajectories is employed as the loss function. For a window of length $K$ ending at time $t$, training loss is defined as:
\begin{equation}
    \mathcal{L} = \frac{1}{K} \sum_{\tau=t-K}^{t} \| \left(\widehat{\mathbf{a}}_{\tau} - \mathbf{a}_\tau \right) \circ \mathbf{m}_\tau\|^2.
\end{equation}
By incorporating the mask into the loss calculation (elementwise multiplication), a focus solely on valid actions is enforced, thereby preserving meaningful gradient updates and enhancing training stability.


% ####################################################################
% ####################################################################
% ####################################################################
\section{Experiments}
\label{sec:experiments}



In this section, a comprehensive set of experiments is presented to evaluate the proposed method’s performance, both during training and under varied test conditions. Different dataset types and sample sizes are examined to determine their impact on learning efficiency and convergence, while generalization to unseen environments is also assessed.

\paragraph{Experimental Setup}
The dataset generation and the evaluation experiments are conducted using the EV2Gym simulator~\cite{10803908}, which leverages real-world data distributions, including EV arrivals, EV specifications, electricity prices, etc. This setup ensures a realistic environment where the state and action spaces accurately reflect real charging stations’ operational complexity. A scenario with 25 chargers is chosen, allowing up to 25 EVs to be connected simultaneously. In this configuration, the action vector has up to 25 variables (one per EV), while the state vector contains around 150 variables describing EV statuses, charger conditions, power transformer constraints, and broader environmental factors.
Consequently, the resulting optimization problem is in the moderate-to-large scale range, reflecting the key complexities of real-world EV charging infrastructure. Each training procedure is repeated 10 times with distinct random seeds to ensure statistically robust findings. All reported rewards represent the average performance over 50 evaluation scenarios, each featuring different configurations (electricity prices, EV behavior, power limits, etc.). 
% This setup presents a robust test of both training efficiency and adaptive capacity in a dynamic environment.


\paragraph{Dataset Generation}
\label{subsec:dataset_generation}

Offline RL algorithms, including DTs, can learn optimal policies from trajectories without needing online interaction with the environment. Consequently, the quality of the gathered training trajectories has a substantial impact on the learning process. In this work, three distinct strategies were used to generate trajectories:
% \textbf{Random Actions}: Uniformly sampled actions in the range $[-1, 1]$ were applied to the simulator. \textbf{Business-as-Usual (BaU)}: A Round Robin charging policy commonly employed by CPOs, which sequentially allocates charging power among EVs to balance fairness and efficiency. \textbf{Optimal Policies}: Expert solutions derived from solving offline the mathematical formulation in Section~\ref{sec:mip} for random scenarios.
\begin{itemize}
    \item \textbf{Random Actions}: Uniformly sampled actions in the range $[-1, 1]$ were applied to the simulator.
    \item \textbf{Business-as-Usual (BaU)}: A Round Robin charging policy commonly employed by CPOs, which sequentially allocates charging power among EVs to balance fairness and efficiency.
    \item \textbf{Optimal Policies}: Optimal solutions derived from solving offline the mathematical problem described in Section~\ref{sec:mip} for randomly generated scenarios.
\end{itemize}
Each trajectory consists of 300 state-action-reward-action mask tuples, with each timestep representing a 15-minute interval, resulting in a total of three simulated days. This combination of random, typical, and expert data provides a comprehensive basis for evaluating how GNN-DT learns from diverse offline trajectories. 

% \subsection{Experimental Results}

\begin{table*}
\centering
\captionsetup{labelformat=empty}
\caption{Comparison of maximum episode rewards ($\times10^{5}$) achieved by baseline methods and GNN-DT across various datasets and context lengths ($K$). Rewards highlighted in \textbf{bold} indicate the highest value within each dataset and $K$ category.}
\label{tab:rewards_ov}
\resizebox{0.85\textwidth}{!}{
\begin{tblr}{
  row{even} = {c},
  row{3} = {c},
  row{5} = {c},
  row{7} = {c},
  row{9} = {c},
  row{11} = {c},
  cell{1}{1} = {r=2}{c},
  cell{1}{2} = {r=2}{},
  cell{1}{3} = {r=11}{},
  cell{1}{4} = {c=3}{c},
  cell{1}{7} = {c=3}{c},
  cell{3}{2} = {r=3}{},
  cell{6}{2} = {r=3}{},
  cell{9}{2} = {r=3}{},
  hline{1,12} = {-}{0.08em},
  hline{2} = {4-9}{0.03em},
  hline{3} = {1-2,4-9}{0.03em},
  hline{6,9} = {1-2,4-9}{dashed},
}
Dataset & {{Avg. Training}\\{Dataset Reward}} &  & K=2 &  &  & K=10 &  & \\
 &  &  & DT & Q-DT & GNN-DT (\textbf{Ours}) & DT & Q-DT & GNN-DT (\textbf{Ours})\\
Random $100$ & $-2.37$~\textcolor[rgb]{0.216,0.212,0.216}{±$0.39$} &  & $-1.91$ & $-1.97$ & $\mathbf{-0.82}$ & $-2.12$ & $-2.09$ & $-1.16$\\
Random $1000$ &  &  & $-1.93$ & $-2.04$ &$ -0.86$ & $-2.11$ & $-2.01$ & $-1.18$\\
Random $10000$ &  &  & $-1.76$ & $-2.04$ & $-1.25$ & $-1.81$ & $-1.98$ & $\mathbf{-0.98}$\\
BaU $100$ & $-0.67$~\textcolor[rgb]{0.216,0.212,0.216}{±$0.07$} &  & $-0.79$ & $-0.74$ & $\mathbf{-0.59}$ & $-0.79$ & $-0.72$ & $-0.56$\\
BaU $1000$ &  &  & $-0.71$ & $-0.66$ & $-0.65$ & $-0.64$ & $-0.71$ &$ -0.57$\\
BaU $10000$ &  &  & $-0.69$ & $-0.66$ & $-0.66$ & $\mathbf{-0.44}$ & $-0.74$ & $-0.53$\\
Optimal $100$ & $-0.01$~\textcolor[rgb]{0.216,0.212,0.216}{±$0.01$} &  & $-0.67$ & $-0.91 $& $-0.15$ & $-1.12$ & $-0.90$ & $-0.14$\\
Optimal $1000$ &  &  & $-0.63$ & $-0.67 $& $-0.10$ & $-0.87$ & $-0.86$ & $-0.09$\\
Optimal $10000$ &  &  & $-0.63$ & $-0.80 $& $\mathbf{-0.04}$ & $-0.72$ & $-0.90$ & $\mathbf{-0.07}$
\end{tblr}
}
\end{table*}


\subsection{Training Performance}

Fig.~\ref{fig:perf_comp} compares the proposed GNN-DT against two baselines, {classic DT}\footnote{\href{https://github.com/kzl/decision-transformer}{https://github.com/kzl/decision-transformer}} and {Q-DT}\footnote{\href{https://github.com/charleshsc/QT}{https://github.com/charleshsc/QT}}, which both rely on flattened state representations due to their inability to directly process graph-structured data. In these baseline methods, empty chargers and unavailable actions are replaced by zeros, so the action vector is always the same size. Several well-known online RL algorithms from the Stable-Baselines 3 framework~\cite{sb3} (SAC, DDPG, TD3, TRPO, PPO, and TQC) are also included to provide a performance benchmark for complex optimization tasks featuring both dense and sparse rewards. 
The offline RL algorithms (DT, Q-DT, and GNN-DT) are trained on three different datasets (Optimal, Random, and BaU), each comprising 10.000 trajectories. 
% The rightmost plot shows the training curves of the online RL algorithms for further comparison.
A red dotted line marks the \emph{oracle} reward, which represents the experimental maximum achievable reward obtained by completing a full simulation without uncertainty. This oracle reward serves as an upper bound and helps contextualize the relative performance of each method.

In Figs.~\ref{fig:perf_comp}.a-c, the offline DT-based approaches use a context length $K=10$ and learn from pre-collected trajectories. As expected, the \emph{Optimal} dataset provides the highest-quality information, enabling GNN-DT to converge rapidly toward near-oracle performance, while classic DT and Q-DT lag far behind, showcasing GNN-DTs improved sampling efficiency. With the \emph{Random} dataset, the limited quality of data leads all methods to plateau at lower reward values, although GNN-DT still surpasses the other baselines. An intriguing behavior is observed in the \emph{BaU} dataset, where classic DT initially experiences a substantial drop but later recovers to a final reward level exceeding that of Q-DT and GNN-DT. In contrast, the online RL algorithms displayed in Fig.~\ref{fig:perf_comp}.d struggle to achieve comparable improvements, suggesting that pure online exploration is insufficient for solving this complex EV charging optimization problem with sparse rewards. For completeness, Appendix~\ref{app:perf} contains the training curves for all algorithm-dataset-context length configurations used.

In Table~\ref{tab:rewards_ov}, the maximum episode reward is compared for small, medium, and large datasets (100, 1.000, and 10.000 trajectories), under two different context lengths ($K=2$ and $K=10$). The left side of Table~\ref{tab:rewards_ov} reports the dataset type, the number of trajectories, and the average reward in each dataset. All baselines achieve performance above the \emph{Random} dataset’s average reward. However, only GNN-DT consistently approaches the \emph{Optimal} dataset’s performance, reaching as close as $-0.04\times10^{5}$ compared to the $-0.01\times10^{5}$ optimal reward. This advantage becomes especially evident at the largest dataset size (10.000 trajectories), highlighting the benefits of the graph-based embedding layer. Overall, GNN-DT outperforms the baselines across all datasets and both context lengths, with the single exception of the \emph{BaU} dataset at $K=10$. Interestingly, a larger context window does not always translate into higher rewards, potentially due to the problem setting. Similarly, the dataset size appears to have minimal impact on Q-DT, whereas DT and GNN-DT generally improve with more trajectories. These findings underscore that both the {quality} and {quantity} of offline data, coupled with the GNN-DT architecture, are key to achieving superior performance. 



\paragraph{Enhancing Training Datasets}

\begin{table}[!t]
\centering
\captionsetup{labelformat=empty}
\caption{Maximum reward of GNN-DT trained on merged Optimal and Random datasets for $K\!=\!2$ and $K\!=\!10$. Notice that performance improves even though the average training dataset rewards are substantially lower, showing the importance of using diverse datasets in training DTs. Rewards highlighted in \textbf{bold} represent the highest values within each $K$ category.}

\label{tab:mixed_opt}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|cc} 
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Total \\Traj.\end{tabular}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Avg. Dataset\\Reward\end{tabular}}} & \multicolumn{2}{c}{GNN-DT Reward ($\times10^5$)}      \\ 
\cline{4-5}
                         &                                                                        & \multicolumn{1}{c}{}                                                                              & K=2               & K=10               \\ 
\hline
Random (Rnd.) 100\%      & $1000$                                                                 & $-2.37$ ±$0.39$                                                                                   & $-0.863$          & $-1.187$           \\
Opt. 25\% + Rnd. 75\%    & $1000$                                                                 & $-1.78$~±$1.07$                                                                                   & $-0.045$          & $\mathbf{-0.020}$  \\
Opt. 50\% + Rnd. 50\%    & $1000$                                                                 & $-1.18$~±$1.19$                                                                                   & $\mathbf{-0.021}$ & $-0.040$           \\
Opt. 75\% + Rnd. 25\%    & $1000$                                                                 & $-0.60~$±$1.03$                                                                                   & $-0.073$          & $-0.057$           \\
Optimal (Opt.) 100\%     & $1000$                                                                 & $-0.01$~±$0.01$                                                                                   & $-0.108$          & $-0.099$           \\
\bottomrule
\end{tabular}
}
\end{table}


The previous section highlighted that the {quality} of trajectories in the training dataset is the most influential factor for achieving high performance. In this section, we explore whether creating new datasets by mixing existing ones can further improve performance. Initially, the \emph{Optimal} and \emph{Random} datasets are combined in different proportions, as summarized in Table~\ref{tab:mixed_opt}. A noteworthy result is that supplementing the \emph{Optimal} dataset with theoretically less useful (\emph{Random}) trajectories {consistently} boosts performance. In particular, GNN-DT with $K=10$, trained on a mix of 250 \emph{Optimal} and 750 \emph{Random} trajectories, achieves near-oracle results, deviating by only $-0.001\times10^{5}$ from the optimal reward. A similar trend emerges when blending \emph{BaU} and \emph{Random} datasets (Table~\ref{tab:mixed_bau}), although the performance gains are not as significant. Overall, these findings indicate that carefully integrating high- and lower-quality data can enhance policy learning beyond what purely \emph{Optimal} or purely \emph{Random} datasets can provide. 


\begin{table}[!t]
\centering
\captionsetup{labelformat=empty}
\caption{Maximum reward of GNN-DT trained on merged BaU and Random datasets for $K=2$ and $K=10$.}
\label{tab:mixed_bau}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccc|cc} 
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Total \\Traj.\end{tabular}} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Avg. Dataset\\Reward\end{tabular}}} & \multicolumn{2}{c}{GNN-DT Reward ($\times10^5$)}      \\ 
\cline{4-5}
                         &                                                                        & \multicolumn{1}{c}{}                                                                              & K=2               & k=10               \\ 
\hline
Random (Rnd.) 100\%      & $1000$                                                                 & $-2.37$ ±$0.39 $                                                                                  & $-0.863$          & $-1.187$           \\
BaU~25\% + Rnd. 75\%     & $1000$                                                                 & $-1.93$ ±$0.80$                                                                                   & $-0.578$          & $-0.461$           \\
BaU~50\% + Rnd. 50\%     & $1000$                                                                 & $-1.51$ ±$0.87$                                                                                   & $-0.665$          & $\mathbf{-0.447}$  \\
BaU~75\% + Rnd. 25\%     & $1000$                                                                 & $-1.09$ ±$0.76$                                                                                   & $\mathbf{-0.421}$ & $-0.471$           \\
BaU~100\%                & $1000$                                                                 & $-0.01$~$±0.01$                                                                                   & $-0.654$          & $-0.572$           \\
\bottomrule
\end{tabular}
}
\end{table}

\paragraph{Impact of larger context lengths (K)}

\begin{figure}[!t]
\centering
\includegraphics[width=0.85\linewidth]{Figures/max_rewards_K.pdf}
\vspace{-5mm}
\caption{
GNN-DT performance for larger context lengths (K).}
\label{fig:K_comp}
\end{figure}

Fig.~\ref{fig:K_comp} demonstrates that the context length $K$ plays a key role in the performance of GNN-DT, with diminishing returns beyond a certain point. For high-quality datasets like \textit{Optimal}, moderate context lengths ($K=5$ to $K=10$) yield the best results, while larger $K$ values do not improve performance significantly. For suboptimal datasets like \textit{BaU} and \textit{Random}, the performance is lower overall, and longer context lengths seem to offer meaningful improvements, particularly when using the \textit{BaU} dataset. Thus, selecting an appropriate context length is crucial for achieving better performance, while the quality of the dataset remains the most influential factor.



\begin{figure}[!t]     
     \hfill
     \subfloat[EVs' battery level progression over time.]{
         \centering
         \includegraphics[width=1\linewidth]{Figures/EV_Energy_Level_single.pdf}
         \label{fig:comp_a}
         }
    \vspace{-3mm}
     \subfloat[Charger Actions]{
         \centering
         \includegraphics[width=1\linewidth]{Figures/CS_Power_scatter.pdf}
         \label{fig:comp_b}
         }
         % \hfill
         % \vspace{-2mm}
        \caption{Comparison of smart charging algorithms based on (a) State of Charge, and (b) Action Probability.
        }
        \label{fig:physics}
\end{figure}

\subsection{Illustrative Example of EV Charging}

With the models trained, we proceed to compare the behavior of the best baseline models trained (DT, Q-DT, GNN-DT) against the heuristic BaU algorithm in an EV charging scenario. Fig.~\ref{fig:comp_a} presents the SoC progress for three EVs connected one after the other to a single charger throughout the simulation, while Fig.~\ref{fig:comp_b} illustrates the charging and discharging actions of all chargers taken by each algorithm.

% At the beginning of the simulation, EVs arrive at the charging station with unknown initial SoCs. Upon connection, they communicate their departure times and desired SoC levels to the CPO. Leveraging this information, along with real-time electricity price signals and power constraints, each algorithm determines optimal charging and discharging actions.

In Fig.~\ref{fig:comp_a}, the heuristic {BaU} algorithm consistently overcharges the EVs, often exceeding the desired SoC levels. In contrast, both DT and Q-DT fail to fully satisfy the desired SoC, except for the last EV, resulting in suboptimal performance. Conversely, GNN-DT successfully achieves the desired SoC for all EVs, closely mirroring the behavior of the optimal algorithm. This demonstrates GNN-DT's ability to precisely control charging actions based on dynamic state information. Fig.~\ref{fig:comp_b} provides further insights into the actions taken by each algorithm. The optimal solution primarily employs maximum charging or discharging power, since it knows the future. In comparison, GNN-DT exhibits a more refined approach, modulating charging power within a range of -6 to 11 kW.  
On the other hand, baseline DT and Q-DT display a narrower range of actions, limiting their ability to optimize the charging schedules and adapt to varying conditions.
These results underscore the superior capability of GNN-DT in managing the complexities of EV charging dynamics. For a more detailed analysis of key performance metrics in EV charging scenarios, refer to Appendix~\ref{sec:res}.


\subsection{Evaluation of Generalization and Scalability}

\paragraph{Generalization Analysis}

\begin{figure}[!t]     
     \hfill
     \subfloat[Unseen state transition probabilities.]{
         \centering
         \includegraphics[width=1\linewidth]{Figures/generalization_eda.pdf}
         \label{fig:gen_a}
         }
    \vspace{-3mm}
     \subfloat[Evaluating on different size problems.]{
         \centering
         \includegraphics[width=1\linewidth]{Figures/generalization_eda_CS.pdf}
         \label{fig:gen_b}
         }
         % \hfill
         % \vspace{-2mm}
        \caption{Generalization performance of the proposed model, depicting the average rewards achieved across 100 randomly generated scenarios in previously unseen environments.
        }
        \label{fig:generalization}
\end{figure}

% Deploying RL models in real-world settings often requires retraining due to discrepancies in state transition dynamics between simulators and actual environments, as well as shifts in input data distributions over time. 
Evaluating the generalization of RL models across varying state transition probabilities is crucial for ensuring consistent performance under diverse conditions~\cite{wang2020statistical}. 
% Offline RL algorithms typically struggle with generalization, as they are not designed to handle distribution shifts~\cite{wang2020statistical}. 
In Fig.~\ref{fig:gen_a}, the generalization capabilities of GNN-DT and other baselines are assessed in environments with small, medium, and extreme variations in state transition probabilities (compared to the training environment). While the baseline methods experience significant performance drops as the evaluation environment deviates from the training setting, GNN-DT maintains strong performance across all scenarios. This highlights the critical role of GNN-based embeddings in improving model robustness and generalization.

A key advantage of the GNN-DT architecture, not present in classic DTs, is its invariance to problem size, i.e. the same network can be applied to both smaller and larger-scale environments. Fig.~\ref{fig:gen_b} illustrates the scalability and generalization performance of GNN-DT compared to the BaU algorithm and Optimal policy. GNN-DT, originally trained on a 25-charger environment, is evaluated in environments with 5, 50, 75, and 100 chargers. As expected, performance decreases as the number of chargers increases, since GNN-DT was not trained on larger-scale environments. Nevertheless, it still outperforms the heuristic BaU, demonstrating the model’s capability to handle problem size variation. In future work, training GNN-DT on a range of charger configurations simultaneously could further enhance its adaptability across a broader spectrum of environments.



\paragraph{Scalability Analysis}

\begin{table}[!t]
\centering
\captionsetup{labelformat=empty}
\caption{Maximum reward achieved by GNN-DT in a large-scale EV charging optimization task involving 250 chargers.}

\label{tab:250reward}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{ccc|c} 
\hline
        & \begin{tabular}[c]{@{}c@{}}Total\\Trajectories\end{tabular} & \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Avg. Dataset\\Reward\end{tabular}} & \begin{tabular}[c]{@{}c@{}}GNN-DT\\Reward\end{tabular}  \\ 
\hline
Random  & $3000$                                                      & $-22.39$~±$1.49$                                                                 & $-9.34$                                                 \\
BaU     & $3000$                                                      & $-6.67$~±$0.32$                                                                  & $-4.23$                                                 \\
Optimal & $3000$                                                      & $-0.08$~±$0.03$                                                                  & $\mathbf{-0.27}$                                                 \\
\hline
\end{tabular}
}
\end{table}

The scalability and effectiveness of GNN-DT were tested when trained on a significantly larger optimization problem involving 250 charging stations (CSs). In this scenario, the model must handle up to 250 action variables per step and over 1,000 state variables, which include critical information such as power limits and battery levels. The results presented in Table~\ref{tab:250reward} demonstrate that GNN-DT shows promise for addressing more complex optimization tasks. However, the model requires a substantial increase in both the number of training trajectories and memory resources to maintain efficiency, highlighting a well-known limitation of DT-based approaches.


\section{Conclusions}

In this work, we introduced a novel DT-based architecture, GNN-DT, which incorporates GNN embedders to significantly enhance sample efficiency and overall performance. Through extensive evaluation across various datasets, including expert, random, and BaU, we demonstrated that traditional DTs and online RL algorithms fail to generalize effectively in real-world settings without specialized embeddings. We further show that both the size and quality of input trajectories critically impact the training process, underscoring the importance of carefully selecting datasets for effective learning. Finally, by leveraging the power of GNN embeddings, GNN-DT improved the model’s ability to generalize in previously unseen environments and handle large, complex action spaces. These contributions demonstrate GNN-DT's potential to address complex dynamic optimization challenges beyond EV charging. They also underscore the critical roles of data quality and model architecture in enabling efficient real-world deployment.

% \clearpage

\section*{Acknowledgements}
The study was partially funded by the DriVe2X research and innovation project from the European Commission with grant numbers 101056934. The authors acknowledge the use of computational resources of the DelftBlue supercomputer, provided by Delft High Performance Computing Centre (https://www.tudelft.nl/dhpc). This work used the Dutch national e-infrastructure with the support of the SURF Cooperative using grant no. EINF-5716.


\bibliography{ref}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Appendix: EV Charging Model}

The first appendix Section provides the complete mixed integer programming (MIP) formulation of the optimal EV charging problem and presents detailed experimental results for key evaluation metrics.

\subsection{Complete EV charging MIP model} \label{app_1}

The optimal EV charging problem we investigate aims to maximize the CPO's profits while ensuring that the demands of EV users are fully satisfied. The CPO lacks prior knowledge of when EVs will arrive or the aggregated power constraints. However, it is assumed that when an EV arrives at charging station \(i\) initiating charging session $j$, it provides its departure time (\(t_{j,i}^{d}\)) and the desired battery capacity (\(e^*_{j,i}\)) at departure. Additionally, the battery capacity \(e_{i,t}\) for each EV is known while it is connected to the charger. These assumptions are standard in research, as this information can typically be retrieved through evolving charging communication protocols. 

The investigated EV charging problem expands over a simulation with $t$ timesteps, $t \in \mathcal{T}$ where a CPO decides the charging and discharging power ($p^+_{i,t}$ and $p^-_{i,t}$) for every charging station $i \in \mathcal{I}$. Since the chargers can be spread around the city, there are charger groups $w \in \mathcal{W}$, that can have a lower-level aggregated power limits representing connections to local power transformers.
Based on these factors, the overall optimization objective is defined as follows:
\begin{flalign}
    \max_{p^+,\omega^+, p^-, \omega^-} \sum_{t \in \mathcal{T}} \Bigg( &- 100 \cdot \max\{0, p_t^{\sum} - p^*_t\} \\
    &+ \sum_{i \in \mathcal{I}} \Bigg(
        \Delta t \left( \Pi^{+}_t p_{i,t}^+ \omega^+_{i,t} - \Pi^{-}_t p_{i,t}^- \omega^-_{i,t} \right) \\
        &\quad - 10 \cdot \sum_{j \in \mathcal{J}_i} \left( 
            \sum_{t=t_{j,i}^a}^{t_{j,i}^d} \left( p_{i,t}^+ \omega^+_{i,t} - p_{i,t}^- \omega^-_{i,t} \right) - e^*_{j,i}
        \right)^2
    \Bigg) 
    \Bigg)
% \label{eq:opt3.130}
\end{flalign}

Subject to:
\begin{flalign}
% Total Power
&p^{\sum}_t=\sum_{i\in \mathcal{I}}p^+_{i,t} \cdot \omega^+_{i,t} -p^-_{i,t} \cdot \omega^-_{i,t}
  & \forall  i , \;\forall t&
    \label{eq:opt3.0}
\end{flalign}
\vspace{-3mm}
\begin{flalign}
% Total Power
&\overline{p}_{w,t} \geq \sum_{i\in \mathcal{W}_i}p^+_{i,t} \cdot \omega^+_{i,t} -p^-_{i,t} \cdot \omega^-_{i,t}
  & \forall  i ,\;\forall w, \;\forall t &
    \label{eq:opt3.01}
\end{flalign}
\vspace{-5mm}
% \begin{flalign}
% % Power equation constraint
% & p^+_{i,t} = I^\ch_{i,t} \cdot V_{i,t} \cdot \sqrt{\phi_{i,t}} \cdot \eta^{\ch}_{i,t} \cdot \omega^\ch_{i,t}  &  \forall  i , \;\forall t&
%     \label{eq:opt3.1}
% \end{flalign}
% \vspace{-8mm}
% \begin{flalign}
% & P^\dis_{i,t} = I^\dis_{i,t} \cdot V_{i,t} \cdot \sqrt{\phi_{i,t}} \cdot \eta^{\dis}_{i,t} \cdot \omega^\dis_{i,t}  &  \forall  i , \;\forall t&
%     \label{eq:opt3.2}
% \end{flalign}
% \vspace{-8mm}
\begin{flalign}
%EV battery constraint
& \underline{e}_{j,i} \leq e_{j,i,t} \leq \overline{e}_{j,i} & \forall  j, \;\forall  i , \;\forall t&
    \label{eq:opt3.3} 
\end{flalign}
\vspace{-8mm}
\begin{flalign}
% EV battery level update constraint
& e_{j,i,t} = e_{j,i,t-1} + (p^+_{i,t} \cdot \omega^+_{i,t} + p^-_{i,t} \cdot \omega^-_{i,t}) \cdot \Delta t &  \forall  j, \;\forall  i , \;\forall t&
\label{eq:opt3.4}
\end{flalign}
\vspace{-8mm}
\begin{flalign}
 % EV energy at arrival
& e_{j,i,t} = e^{a}_{j,i,t} &  \forall  j, \;\forall  i , \;\forall t | \; t = {t}^a_{j,i,t}&
\label{eq:opt3.5}
\end{flalign}
\vspace{-8mm}
\begin{flalign}
% Current constraints
&\underline{p}^{+}_{j,i}\leq p^+_{i,t} \leq 
 \overline{p}^{+}_{j,i} &  \forall  j, \;\forall  i , \;\forall t&
\label{eq:opt3.6}
\end{flalign}
\vspace{-8mm}
\begin{flalign}
&\underline{p}^{-}_{j,i} \geq p^-_{i,t} \geq 
 \overline{p}^{-}_{j,i} &  \forall  j, \;\forall  i , \;\forall t&
\label{eq:opt3.7}
\end{flalign}
% \vspace{-8mm}
% \begin{flalign}
% % CS current def
% &p^+_{i,t} = P^+_{i,t} \cdot \omega^+_{i,t}
%   &  \forall  i , \;\forall t&
% \label{eq:opt3.8}
% \end{flalign}
% \vspace{-8mm}
% \begin{flalign}
% % CS current def
% &p^-_{i,t} = P^-_{i,t} \cdot \omega^-_{i,t}
%   &  \forall  i , \;\forall t&
% \label{eq:opt3.9}
% \end{flalign}
\vspace{-8mm}
\begin{flalign}
& \omega^+_{i,t} + \omega^-_{i,t} \leq 1 &  \forall  i , \;\forall t&
 \label{eq:opt3.11}
 \end{flalign}

The power of a single charger $i$ is modeled using four decision variables, $p^+ \cdot \omega^+$ and $p^- \cdot \omega^-$, where $\omega^+$ and $\omega^-$ are binary variables, to differentiate between charging and discharging behaviors and enable charging power to get values in ranges $0\cup [\underline{p}^+,\overline{p}^+]$, and discharging power in $[\underline{p}^-,\overline{p}^-]\cup0$.
Eq.~\ref{eq:opt3.01} defines the locally aggregated transformer power limits $\overline{p}$ for chargers belonging to groups $\mathcal{W}_i$.
Eqs.\eqref{eq:opt3.4} and\eqref{eq:opt3.5} address EV battery constraints during operation with a minimum and maximum capacity of $\underline{e}$, $\overline{e}$, and energy $e^{a}$ at time of arrival $t^{a}$. Equations \eqref{eq:opt3.6} and \eqref{eq:opt3.7} impose charging and discharging power limits for every charger-EV session combination.
To prevent simultaneous charging and discharging, the binary variables $\omega^\ch$ and $\omega^\dis$ are constrained by \eqref{eq:opt3.11}.

\subsection{Evaluation Metrics}
The following evaluation metrics are used in this study to assess the performance of the proposed algorithms:

\textbf{User Satisfaction [\%]}: This metric measures how closely the state of charge (\(\soc\)) of an EV at departure matches its target \(\soc^*\). For a set of EVs \(\mathcal{J}\), user satisfaction is given by:
\begin{equation}
\textbf{User Sat.} = \frac{1}{|\mathcal{J}|} \cdot \sum_{j \in \mathcal{J}} \left( \frac{e_{j,t^d}}{e^*_j} \right) \cdot 100 \%.
\end{equation}
This ensures that each EV is charged to its desired level by the end of the charging session.

\textbf{Energy Charged [kWh]}: This represents the total energy supplied to EVs over the entire charging period and is given by:
\begin{equation}
\textbf{Energy Charged} = \sum_{t \in \mathcal{T}} \sum_{i \in \mathcal{I}} p^+_{i,t} \cdot \omega^+_{i,t} \cdot \Delta t.
\end{equation}
This metric helps quantify the overall energy throughput for the system.

\textbf{Energy Discharged [kWh]}: This measures the amount of energy discharged back to the grid by the EVs. Discharging is typically done when electricity prices are high, and this metric is important for evaluating the system's ability to contribute to grid stability.
\begin{equation}
\textbf{Energy Discharged} = \sum_{t \in \mathcal{T}} \sum_{i \in \mathcal{I}} p^-_{i,t} \cdot \omega^-_{i,t} \cdot \Delta t.
\end{equation}

\textbf{Power Violation [kW]}: This metric tracks violations of operational constraints, such as exceeding the aggregated power limits at any given time. A violation occurs when the total power used exceeds the maximum allowed power:
\begin{equation}
\textbf{Power Violation} = \sum_{t\in \mathcal{T}}\max\{0, p_t^{\sum}-p^*_t\}\Delta t.
\end{equation}
Minimizing this metric ensures that the system remains within operational limits and avoids overloading the grid.

\textbf{Cost [€]}: This evaluates the financial cost of the charging operations, considering both charging and discharging costs based on electricity prices. The total cost over the simulation period is defined as:
\begin{equation}
\textbf{Cost} = \sum_{t\in \mathcal{T}}\sum_{i \in \mathcal{I}} \Delta t (\Pi^{+}_tp_{i,t}^+ - \Pi^-_tp_{i,t}^-),
\end{equation}
This metric helps assess the cost-effectiveness of the charging strategy.


\subsection{Complete Experimental Results of EV Charging.}
\label{sec:res}



\begin{figure*}[t]     

     \centering
     \subfloat[]{
         \centering
         \includegraphics[width=1\textwidth]{Figures/EV_Energy_Level.png}
         \label{fig:sim_a}
         }
     % \hspace{-5.5mm}   
     \hfill
     \subfloat[]{
         \centering
         \includegraphics[width=0.85\textwidth]{Figures/Actual_vs_Setpoint_Power.pdf}
         \label{fig:sim_b}
         }    
        \caption{Complete results for a single evaluation scenario:(a) illustrating individual EV battery trajectories across all 25 charging stations, and (b) presenting the actual aggregate power usage against the power limit for the entire EV fleet.
        }
        \label{fig:single_sim}
\end{figure*}


Table~\ref{tab:ev_exps_results} shows a comparison of key EV charging metrics for the 25-station problem after 100 evaluations, including heuristic algorithms, Charge As Fast as Possible (CAFAP)  and BaU, and DT variants with the optimal solution, which assumes future knowledge. GNN-DT shows remarkable performance, achieving a close approximation to the optimal solution, particularly in user satisfaction (99.3\% ± 0.03\%) and power violation (21.7 ± 22.8 kW). It outperforms both BaU and DT variants in terms of energy discharged, power violation, and costs. Notably, GNN-DT performs well even compared to Q-DT, while maintaining competitive execution time, albeit slightly slower than the simpler models. The results underscore the effectiveness of GNN-DT in managing complex EV charging tasks, demonstrating its potential for real-world applications where future knowledge is not available.

\begin{table*}[t]
\centering
\captionsetup{labelformat=empty}
\caption{Comparison of key EV charging metrics for the 25-station problem after 100 evaluations, for heuristic algorithms (CAFAP \& BaU) and DT variants with the optimal solution, which assumes future knowledge. }
\label{tab:ev_exps_results}
\resizebox{0.95\linewidth}{!}{
\begin{tblr}{
  cells = {c,t},
  vline{2} = {2-7}{0.05em},
  hline{1,8} = {-}{0.08em},
  hline{2} = {-}{0.05em},
}
Algorithm & {Energy\\~Charged\\~[MWh]} & {Energy\\~Discharged\\~[MWh]} & {User\\~Satisfaction\\~[\%]} & {Power\\~Violation\\~[kW]} & {Costs\\~[€]} & {Reward\\~[-$10^5$]} & {Exec. Time\\~[sec/step]}\\
CAFAP & $1.3$ ±$0.2$ & $0.00$ ±$0.00$ & $100.0$ ±$0.0$ & $1289.2$ ±$261.8$ & $-277$ ±$165$ & $-1.974$ ±$0.283$ & $0.001$\\
BaU & $1.3$ ±$0.2$ & $0.00$ ±$0.00$ & $99.9$ ±$0.2$ & $10.5$ ±$9.4$ & $-255$ ±$156$ & $-0.679$ ±$0.067$ & $0.001$\\
DT & $0.9$ ±$0.1$ & $0.03$ ±$0.01$ & $94.4$ ±$1.6$ & $58.7$ ±$28.3$ & $-173$ ±$104$ & $-0.462$ ±$0.093$ & $0.006$\\
Q-DT & $1.0$ ±$0.1$ & $0.00$ ±$0.00$ & $93.6$ ±$2.1$ & $20.1$ ±$21.4$ & $-187$ ±$113$ & $-0.665$ ±$0.135$ & $0.010$\\
\textbf{GNN-DT} (Ours) & $0.9$ ±$0.1$ & $0.19$ ±$0.03$ & $99.3$ ±$0.2$ & $21.7$ ±$22.8$ & $-142$ ±$89$ & $-0.027$ ±$0.023$ & $0.023$\\
Optimal (Offline) & $1.9$ ±$0.2$ & $1.08$ ±$0.19$ & $99.1$ ±$0.2$ & $2.0$ ±$4.6$ & $-119$ ±$84$ & $-0.020$ ±$0.015$ & -
\end{tblr}
}
\end{table*}


Fig.~\ref{fig:single_sim} presents the results from a single evaluation scenario, focusing on the performance of various charging strategies across all 25 charging stations. Fig.~\ref{fig:sim_a} illustrates the individual battery trajectories for each EV across the stations, showing how the actual SoC evolves over time. The desired SoC is compared against the results from different algorithms: BaU, GNN-DT, DT, Q-DT, and the Optimal (Offline) solution. It is evident that GNN-DT closely tracks the desired SoC across all stations, outperforming the other methods, particularly in terms of maintaining the target SoC. Fig.~\ref{fig:sim_b} provides insights into the aggregate power usage of the entire EV fleet, where the actual power used is compared to the setpoint power. GNN-DT closely aligns with the power setpoint, demonstrating effective power management, while other methods such as BaU and Q-DT show greater deviations, indicating less efficient power usage. These results underline the superior performance of GNN-DT in optimizing charging strategies while adhering to power constraints.


\clearpage

\section{Appendix: Training}

Here, we present the hyperparameter settings used for training DT, Q-DT, and GNN-DT, accompanied by detailed training curves that illustrate the convergence of each model.

\subsection{Training Hyperparameters}

\begin{table}[h]
\centering
\caption{Algorithm hyperparameters.}
\label{tab:my-table}
% \resizebox{\columnwidth}{!}{%
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Value} \\ \hline
Batch Size & $64$ (for large scale), $128$ (for small scale) \\ \hline
Learning Rate & $10^{-4}$ \\ \hline
Weight Decay & $10^{-4}$ \\ \hline
Number of Steps per Iteration & $1000$ \\ \hline
Number of Decoder Layers & $3$ \\ \hline
Number of Attention Heads & $4$ \\ \hline
Embedding Dimension & $256$ \\ \hline
GNN Embedder Feature Dimension & $16$ \\ \hline
GNN Hidden Dimension & $32$ (for small scale), $64$ (for large scale) \\ \hline
Number of GCN Layers & $3$ \\ \hline
Maximum Epochs & $250$ (for small scale), $400$ (for large scale) \\ \hline
Number of Steps per Iteration & $1000$ (for small scale), $3000$ (for large scale) \\ \hline
Embedding Dimension & $128$ (for small scale), $256$ (for large scale) \\ \hline
Memory per CPU (GB) & $8$ (for small scale), $16$ or $40$ (for large scale) \\ \hline
Time Limit (hours) & $10$ (for small scale), $20$ or $46$ (for large scale) \\ \hline
\end{tabular}%
% }
\end{table}

\clearpage

\subsection{Detailed Training Curves}
\label{app:perf}


Fig.~\ref{fig:all_1} provides a detailed comparison of training curves for various algorithm-dataset-context length (\(K\)) combinations, highlighting the significant impact of the training sample size on performance. The figure includes training results for DT, Q-DT, and GNN-DT across different datasets (Optimal, Random, and BaU) and context lengths (\(K=2\) and \(K=10\)), with each plot showing the performance for 100, 1,000, and 10,000 trajectory samples.

\begin{figure*}[!ht]
\centering
\includegraphics[width=1\textwidth]{Figures/performance_all.pdf}
\vspace{-5mm}
\caption{Complete comparison of training curves for combinations of algorithms-K-training dataset for different numbers of samples.}
\label{fig:all_1}
\end{figure*}


\clearpage
\subsection{Mixed Dataset Training Curves}

Fig.~\ref{fig:perf_comparison} presents the learning curves for the mixed datasets approach, comparing performance across various combinations of Optimal, Random, and BaU datasets for both \(K=2\) and \(K=10\). Fig.~\ref{fig:mixed_a} illustrates the results for mixed Optimal datasets, where different proportions of the Optimal and Random datasets (e.g., 50\% Optimal + 50\% Random) are used for training. The performance becomes more unstable as more Random data is included.
Interestingly, the performance for all the mixed datasets demonstrates better maximum reward reached compared to the Optimal-only dataset, highlighting the benefits of combining high-quality and lower-quality data. Fig.~\ref{fig:mixed_b} shows similar trends for the Mixed-BaU datasets. While the BaU dataset alone performs worse than the Optimal dataset, mixing it with Random data still yields improvements, with the 75\% BaU and 25\% Random combination showing the best results. The results underscore the potential of mixing datasets to improve training performance, especially when high-quality data (Optimal or BaU) is supplemented with lower-quality Random data.

\begin{figure*}[!h]     

     \centering
     \subfloat[Mixed-Optimal]{
         \centering
         \includegraphics[width=0.7\textwidth]{Figures/mixed_opt_performance.pdf}
         \label{fig:mixed_a}
         }
     % \hspace{-5.5mm}   
     \hfill
     \subfloat[Mixed-BaU]{
         \centering
         \includegraphics[width=0.7\textwidth]{Figures/mixed_bau_performance.pdf}
         \label{fig:mixed_b}
         }    
        \caption{Mixed datasets learning curves for optimal and BaU.
        }
        \label{fig:perf_comparison}
\end{figure*}

\end{document}
