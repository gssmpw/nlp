@inproceedings{10.5555/3600270.3602295,
author = {Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and Mordatch, Igor},
title = {Multi-game decision transformers},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A longstanding goal of the field of AI is a method for learning a highly capable, generalist agent from diverse experience. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2025},
numpages = {16},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@inproceedings{10.5555/3666122.3666936,
author = {Wu, Yueh-Hua and Wang, Xiaolong and Hamaya, Masashi},
title = {Elastic decision transformer},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: https://kristery.github.io/edt/.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {814},
numpages = {19},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{3056976d4116471a86ab3fa345b1695d,
title = "A Hybrid Online Off-Policy Reinforcement Learning Agent Framework Supported by Transformers",
keywords = "Reinforcement learning, self-attention, off-policy, Transformer, experience replay, General Medicine, Computer Networks and Communications",
author = "Villarrubia-Martin, {Enrique Adrian} and Luis Rodriguez-Benitez and Luis Jimenez-Linares and David Mu{\~n}oz-Valero and Jun Liu",
year = "2023",
month = oct,
day = "20",
doi = "10.1142/s012906572350065x",
language = "English",
volume = "33",
journal = "International Journal of Neural Systems",
issn = "0129-0657",
publisher = "World Scientific Publishing",
number = "12",
}

@ARTICLE{8727484,
  author={Sadeghianpourhamami, Nasrin and Deleu, Johannes and Develder, Chris},
  journal={IEEE Transactions on Smart Grid}, 
  title={Definition and Evaluation of Model-Free Coordination of Electrical Vehicle Charging With Reinforcement Learning}, 
  year={2020},
  volume={11},
  number={1},
  pages={203-214},
  keywords={Load modeling;Electric vehicle charging;Aggregates;Charging stations;Reinforcement learning;Markov processes;Data models;Demand response;electric vehicles;batch reinforcement learning},
  doi={10.1109/TSG.2019.2920320}}

@ARTICLE{9211734,
  author={Jin, Jiangliang and Xu, Yunjian},
  journal={IEEE Transactions on Smart Grid}, 
  title={Optimal Policy Characterization Enhanced Actor-Critic Approach for Electric Vehicle Charging Scheduling in a Power Distribution Network}, 
  year={2021},
  volume={12},
  number={2},
  pages={1416-1428},
  keywords={Electric vehicle charging;Optimal scheduling;Stochastic processes;Distribution networks;Reinforcement learning;Solar power generation;Dynamic programming;deep reinforcement learning;electric vehicle charging;actor-critic approach;power distribution network},
  doi={10.1109/TSG.2020.3028470}}

@ARTICLE{9465776,
  author={Li, Sichen and Hu, Weihao and Cao, Di and Dragičević, Tomislav and Huang, Qi and Chen, Zhe and Blaabjerg, Frede},
  journal={Journal of Modern Power Systems and Clean Energy}, 
  title={Electric Vehicle Charging Management Based on Deep Reinforcement Learning}, 
  year={2022},
  volume={10},
  number={3},
  pages={719-730},
  keywords={Artificial neural networks;Electric vehicle charging;Schedules;Reinforcement learning;Feature extraction;Optimization;Batteries;Deep reinforcement learning;data-driven control;uncertainty;electric vehicles (EVs)},
  doi={10.35833/MPCE.2020.000460}}

@article{JIN2022120140,
title = {Deep reinforcement learning-based strategy for charging station participating in demand response},
journal = {Applied Energy},
volume = {328},
pages = {120140},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120140},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922013976},
author = {Ruiyang Jin and Yuke Zhou and Chao Lu and Jie Song},
keywords = {Demand response, Charging station, Electric vehicle, Deep reinforcement learning},
abstract = {The trend of zero-carbonization has accelerated the prevalence of electric vehicles (EVs) owing to their advantages of low carbon emissions and high energy efficiency. The stochastic and high charging load of EVs results in a non-negligible challenge that may cause grid overload. A promising approach is the participation of charging stations in demand response as load aggregators by coordinating the charging power of electric vehicles. However, improper coordination of charging load may lead to unfulfilled charging demand, which would cause dissatisfaction on the demand side. In this study, the incentive-based and time-varying demand response mechanism is considered when charging stations coordinate charging of multiple EVs. A decentralized decision-making framework is innovatively applied to provide charging power of each EV. The charging process is modeled as a Markov decision process, and a virtual price is designed to help decide the charging power. Deep reinforcement learning algorithms such as deep deterministic policy gradient are applied to determine the charging strategy of multiple and heterogeneous EVs. Numerical experiments are performed to validate the effectiveness of the proposed method. A comparison with an optimal charging strategy and a heuristic rule-based method shows that the proposed method can trade off the revenue from demand response and user satisfaction, as well as reduce the peak load of the charging station. Furthermore, a test with inaccurate departure information indicates the robustness of the proposed method.}
}

@article{KAMRANI2025100620,
title = {Multi-agent deep reinforcement learning with online and fair optimal dispatch of EV aggregators},
journal = {Machine Learning with Applications},
pages = {100620},
year = {2025},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2025.100620},
url = {https://www.sciencedirect.com/science/article/pii/S2666827025000039},
author = {Arian Shah Kamrani and Anoosh Dini and Hanane Dagdougui and Keyhan Sheshyekani},
keywords = {Electric vehicle (EV), Reinforcement learning (RL), Energy management, Jain’s index, Distribution system operator (DSO)},
abstract = {The growing popularity of electric vehicles (EVs) and the unpredictable behavior of EV owners have attracted attention to real-time coordination of EVs charging management. This paper presents a hierarchical structure for charging management of EVs by integrating fairness and efficiency concepts within the operations of the distribution system operator (DSO) while utilizing a multi-agent deep reinforcement learning (MADRL) framework to tackle the complexities of energy purchasing and distribution among EV aggregators (EVAs). At the upper level, DSO calculates the maximum allowable power for each EVA based on power flow constraints to ensure grid safety. Then, it finds the optimal efficiency-jain tradeoff (EJT) point, where it sells the highest energy amount while ensuring equitable energy distribution. At the lower level, initially, each EVA acts as an agent employing a double deep Q-network (DDQN) with adaptive learning rates and prioritized experience replay to determine optimal energy purchases from the DSO. Then, the real-time smart dispatch (RSD) controller prioritizes EVs for energy dispatch based on relevant EVs information. Findings indicate the proposed enhanced DDQN outperforms deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO) in cumulative rewards and convergence speed. Finally, the framework’s performance is evaluated against uncontrolled charging and the first come first serve (FCFS) scenario using the 118-bus distribution system, demonstrating superior performance in maintaining safe operation of the grid while reducing charging costs for EVAs. Additionally, the framework’s integration with renewable energy sources (RESs), such as photovoltaic (PV), demonstrates its potential to enhance grid reliability.}
}

@article{QIU2023113052,
title = {Reinforcement learning for electric vehicle applications in power systems:A critical review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {173},
pages = {113052},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.113052},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122009339},
author = {Dawei Qiu and Yi Wang and Weiqi Hua and Goran Strbac},
keywords = {Electric vehicles, Vehicle-to-grid, Reinforcement learning, Power systems},
abstract = {Electric vehicles (EVs) are playing an important role in power systems due to their significant mobility and flexibility features. Nowadays, the increasing penetration of renewable energy resources has been observed in modern power systems, which brings many benefits for improving climate change and accelerating the low-carbon transition. However, the intermittent and unstable nature of renewable energy sources introduces new challenges to both the planning and operation of power systems. To address these issues, vehicle-to-grid (V2G) technology has been gradually recognized as a valid solution to provide various ancillary service provisions for power systems. Many studies have developed model-based optimization methods for EV dispatch problems. Nevertheless, this type of method cannot effectively handle the highly dynamic and stochastic environment due to the complexity of power systems. Reinforcement learning (RL), a model-free and online learning method, can capture various uncertainties through numerous interactions with the environment and adapt to various state conditions in real-time. As a result, using advanced RL algorithms to solve various EV dispatch problems has attracted a surge of attention in recent years, leading to many outstanding research papers and important findings. This paper provides a comprehensive review of popular RL algorithms categorized by single-agent RL and multi-agent RL, and summarizes how these advanced algorithms can be applied to various EV dispatch problems, including grid-to-vehicle (G2V), vehicle-to-home (V2H), and V2G. Finally, key challenges and important future research directions are discussed, which involve five aspects: (a) data quality and availability; (b) environment setup; (c) safety and robustness; (d) training performance; and (e) real-world deployment.}
}

@article{ZHANG2023121490,
title = {A safe reinforcement learning-based charging strategy for electric vehicles in residential microgrid},
journal = {Applied Energy},
volume = {348},
pages = {121490},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121490},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923008541},
author = {Shulei Zhang and Runda Jia and Hengxin Pan and Yankai Cao},
keywords = {Electric vehicles, Charging scheduling, Safe reinforcement learning, Constrained soft actor-critic, Residential microgrid, Nonlinear charging characteristics},
abstract = {With the growing popularity of electric vehicles (EVs), it is a new challenge for the residential microgrid system to conduct charging scheduling to meet the charging demands of EVs while maximizing its profit. In this work, a safe reinforcement learning (RL)-based charging scheduling strategy is proposed to meet this challenge. We construct a complete microgrid system equipped with a large charging station and consider different types of EVs, as well as the vehicle-to-grid (V2G) mode and nonlinear charging characteristics of EVs. Subsequently, the charging scheduling problem is formulated as a constrained Markov decision process (CMDP) due to the various limitations of power and demands. To effectively capture the uncertainties of the supply side and demand side of the microgrid, a model-free RL framework is employed. However, the curse of dimensionality of the action space is inevitable as EVs increase. To solve this problem, a charging and discharging strategy based on a general ladder electricity pricing scheme is designed. Different EVs are divided into different sets according to their states under this strategy, and the agent gives control signals of different sets instead of controlling each EV individually, which effectively reduces the dimension of the action space. Subsequently, a constrained soft actor-critic (CSAC) algorithm is designed to solve the established CMDP, and a safety filter is introduced to ensure safety. In the end, a numerical case is conducted to verify the effectiveness of the proposed method.}
}

@article{Zhang2023,
  title={Continuous-Time Decision Transformer for Healthcare Applications},
  author={Zhang, Zhiyue and Mei, Hongyuan and Xu, Yanxun},
  journal={Proceedings of Machine Learning Research},
  volume={206},
  pages={6245--6262},
  year={2023},
  month={April},

}

@misc{chen2022deep,
    title={A Deep Reinforcement Learning-Based Charging Scheduling Approach with Augmented Lagrangian for Electric Vehicle},
    author={Guibin. Chen and Xiaoying. Shi},
    year={2022},
    eprint={2209.09772},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@INPROCEEDINGS{isgt2024,
  author={Yılmaz, Yunus Emre and Orfanoudakis, Stavros and Vergara, Pedro P.},
  booktitle={2024 IEEE PES Innovative Smart Grid Technologies Europe (ISGT EUROPE)}, 
  title={Reinforcement Learning for Optimized EV Charging Through Power Setpoint Tracking},
  volume={},
  number={},
  pages={1-5},
  year={2024},
  doi={}
}

@misc{liu2023constrained,
    title={Constrained Decision Transformer for Offline Safe Reinforcement Learning},
    author={Zuxin Liu and Zijian Guo and Yihang Yao and Zhepeng Cen and Wenhao Yu and Tingnan Zhang and Ding Zhao},
    year={2023},
    eprint={2302.07351},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{online_dt,
  title = 	 {Online Decision Transformer},
  author =       {Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27042--27059},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22c/zheng22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22c.html},
  abstract = 	 {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via task-specific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.}
}

@InProceedings{pmlr-v202-lai23c,
  title = 	 {{C}hi{PF}ormer: Transferable Chip Placement via Offline Decision Transformer},
  author =       {Lai, Yao and Liu, Jinxin and Tang, Zhentao and Wang, Bin and Hao, Jianye and Luo, Ping},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {18346--18364},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/lai23c/lai23c.pdf},
  url = 	 {https://proceedings.mlr.press/v202/lai23c.html},
  abstract = 	 {Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing the runtime by 10x compared to recent state-of-the-art approaches in both public benchmarks and realistic industrial tasks. The deliverables are released at https://sites.google.com/view/chipformer/home.}
}

@InProceedings{pmlr-v238-hong24a,
  title = 	 {A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning},
  author =       {Hong, Kihyuk and Li, Yuhang and Tewari, Ambuj},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {280--288},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/hong24a/hong24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/hong24a.html},
  abstract = 	 {Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected cumulative cost using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate and the dual player acts greedily to minimize the Lagrangian estimate. We show that PDCA finds a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and a strong Bellman completeness assumption, PDCA only requires concentrability and realizability assumptions for sample-efficient learning.}
}

