
@inproceedings{edt,
author = {Wu, Yueh-Hua and Wang, Xiaolong and Hamaya, Masashi},
title = {Elastic decision transformer},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: https://kristery.github.io/edt/.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {814},
numpages = {19},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{gunjan2023brief,
  title={A brief review of portfolio optimization techniques},
  author={Gunjan, Abhishek and Bhattacharyya, Siddhartha},
  journal={Artificial Intelligence Review},
  volume={56},
  number={5},
  pages={3847--3886},
  year={2023},
  publisher={Springer}
}

@article{dulac-arnold_challenges_2021,
	title = {Challenges of real-world reinforcement learning: definitions, benchmarks and analysis},
	volume = {110},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-021-05961-4},
	doi = {10.1007/s10994-021-05961-4},
	abstract = {Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. In this work, we identify and formalize a series of independent challenges that embody the difficulties that must be addressed for RL to be commonly deployed in real-world systems. For each challenge, we define it formally in the context of a Markov Decision Process, analyze the effects of the challenge on state-of-the-art learning algorithms, and present some existing attempts at tackling it. We believe that an approach that addresses our set of proposed challenges would be readily deployable in a large number of real world problems. Our proposed challenges are implemented in a suite of continuous control environments called realworldrl-suite which we propose an as an open-source benchmark.},
	number = {9},
	journal = {Machine Learning},
	author = {Dulac-Arnold, Gabriel and Levine, Nir and Mankowitz, Daniel J. and Li, Jerry and Paduraru, Cosmin and Gowal, Sven and Hester, Todd},
	month = sep,
	year = {2021},
	pages = {2419--2468},
}


@article{ZHANG2023205,
title = {A survey for solving mixed integer programming via machine learning},
journal = {Neurocomputing},
volume = {519},
pages = {205-217},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
author = {Jiayi Zhang and Chang Liu and Xijun Li and Hui-Ling Zhen and Mingxuan Yuan and Yawen Li and Junchi Yan},
keywords = {Mixed integer programming, Machine learning, Combinatorial optimization},
abstract = {Machine learning (ML) has been recently introduced to solving optimization problems, especially for combinatorial optimization (CO) tasks. In this paper, we survey the trend of leveraging ML to solve the mixed-integer programming problem (MIP). Theoretically, MIP is an NP-hard problem, and most CO problems can be formulated as MIP. Like other CO problems, the human-designed heuristic algorithms for MIP rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve MIP since ML-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of MIP and representative traditional solvers. Then, we show the integration of machine learning and MIP with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based MIP solvers, the direction toward more combinatorial optimization problems beyond MIP, and the mutual embrace of traditional solvers and ML components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-SJTU/awesome-ml4co.}
}
@book{8187196,
  author={Bubeck, Sébastien},
  title={Convex Optimization: Algorithms and Complexity},
  year={2015},
  volume={},
  number={},
  pages={},
  keywords={Optimization},
publisher={Now Foundations and Trends},
  doi={10.1561/2200000050}}

@article{jaimungal_reinforcement_2022,
	title = {Reinforcement learning and stochastic optimisation},
	volume = {26},
	issn = {1432-1122},
	url = {https://doi.org/10.1007/s00780-021-00467-2},
	doi = {10.1007/s00780-021-00467-2},
	abstract = {At the heart of financial mathematics lie stochastic optimisation problems. Traditional approaches to solving such problems, while applicable to broad classes of models, require specifying a model to complete the analysis and obtain implementable results. Even then, the curse of dimensionality challenges the viability of conventional methods to settings of practical relevance. In contrast, machine learning, and reinforcement learning (RL) particularly, promises to learn from data and overcome the curse of dimensionality simultaneously. This article touches on several approaches in the extant literature that are well positioned to merge our traditional techniques with RL.},
	number = {1},
	journal = {Finance and Stochastics},
	author = {Jaimungal, Sebastian},
	month = jan,
	year = {2022},
	pages = {103--129},
}


@book{2020OptimizationOM,
  title={Optimization of Manufacturing Processes},
  editor={Gupta, Kapil and Gupta, Munish Kumar},
  publisher={Springer Series in Advanced Manufacturing},
  year={2020}
}

@article{konstantakopoulos_vehicle_2022,
	title = {Vehicle routing problem and related algorithms for logistics distribution: a literature review and classification},
	volume = {22},
	issn = {1866-1505},
	url = {https://doi.org/10.1007/s12351-020-00600-7},
	doi = {10.1007/s12351-020-00600-7},
	abstract = {The scheduling of deliveries and the routing of vehicles are of great importance for supply chain operations, as both determine to a great extent the distribution costs, as well as customer satisfaction. The fact that the distribution of goods is being affected by multiple factors, stemming from the demands of transportation companies, customers, and the external environment, has made the vehicle routing problem (VRP) among the most studied topics in operational research. These factors are transformed either to constraints or variables of the problem and finally lead to the creation of different variants of the VRP, formulated and studied by researchers. Moreover, the management of logistics and supply chain operations is being enhanced by the use of algorithms, integrated into information systems, enabling the optimization of real-life distribution cases. This paper presents a methodology for classifying the multiple VRP variants related to freight transportation, that most logistics and distribution companies face in their daily operations, as well as the algorithms solving the various problems. The application of the research methodology concluded to 334 papers, which were further sorted to 263 papers on the subject of freight transportation, aiming to identify the trends of the VRP variants and the applied algorithms, during the last decade. The correlation between the VRP variants and the applied algorithms is also identified. Finally, the paper presents the quantitative and qualitative results of the literature review and discusses the scientific publications with a significant impact on the research community.},
	number = {3},
	journal = {Operational Research},
	author = {Konstantakopoulos, Grigorios D. and Gayialis, Sotiris P. and Kechagias, Evripidis P.},
	month = jul,
	year = {2022},
	pages = {2033--2062},
}


@article{ROALD2023108725,
title = {Power systems optimization under uncertainty: A review of methods and applications},
journal = {Electric Power Systems Research},
volume = {214},
pages = {108725},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2022.108725},
url = {https://www.sciencedirect.com/science/article/pii/S0378779622007842},
author = {Line A. Roald and David Pozo and Anthony Papavasiliou and Daniel K. Molzahn and Jalal Kazempour and Antonio Conejo},
keywords = {Stochastic optimization, Robust optimization, Chance-constrained optimization, Electric power systems},
abstract = {Electric power systems and the companies and customers that interact with them are experiencing increasing levels of uncertainty due to factors such as renewable energy generation, market liberalization, and climate change. This raises the important question of how to make optimal decisions under uncertainty. This paper aims to provide an overview of existing methods for modeling and optimization of problems affected by uncertainty, targeted at researchers with a familiarity with power systems and optimization. We also review some important applications of optimization under uncertainty in power systems and provide an outlook to future directions of research.}
}

@book{SuttonReinforcementIntroduction,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@misc{lan2023learning,
    title={Learning to Optimize for Reinforcement Learning},
    author={Qingfeng Lan and A. Rupam Mahmood and Shuicheng Yan and Zhongwen Xu},
    year={2023},
    eprint={2302.01470},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{levine2020offline,
    title={Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems},
    author={Sergey Levine and Aviral Kumar and George Tucker and Justin Fu},
    year={2020},
    eprint={2005.01643},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{
qdt,
title={Q-value Regularized Transformer for Offline Reinforcement Learning},
author={Shengchao Hu and Ziqing Fan and Chaoqin Huang and Li Shen and Ya Zhang and Yanfeng Wang and Dacheng Tao},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=ojtddicekd}
}

@misc{chen2021decision,
    title={Decision Transformer: Reinforcement Learning via Sequence Modeling},
    author={Lili Chen and Kevin Lu and Aravind Rajeswaran and Kimin Lee and Aditya Grover and Michael Laskin and Pieter Abbeel and Aravind Srinivas and Igor Mordatch},
    year={2021},
    eprint={2106.01345},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Zhang2023,
  title={Continuous-Time Decision Transformer for Healthcare Applications},
  author={Zhang, Zhiyue and Mei, Hongyuan and Xu, Yanxun},
  journal={Proceedings of Machine Learning Research},
  volume={206},
  pages={6245--6262},
  year={2023},
  month={April},

}

@article{yuan_transformer_2024,
	title = {Transformer in reinforcement learning for decision-making: a survey},
	volume = {25},
	issn = {2095-9230},
	url = {https://doi.org/10.1631/FITEE.2300548},
	doi = {10.1631/FITEE.2300548},
	abstract = {Reinforcement learning (RL) has become a dominant decision-making paradigm and has achieved notable success in many real-world applications. Notably, deep neural networks play a crucial role in unlocking RL’s potential in large-scale decision-making tasks. Inspired by current major success of Transformer in natural language processing and computer vision, numerous bottlenecks have been overcome by combining Transformer with RL for decision-making. This paper presents a multiangle systematic survey of various Transformer-based RL (TransRL) models applied in decision-making tasks, including basic models, advanced algorithms, representative implementation instances, typical applications, and known challenges. Our work aims to provide insights into problems that inherently arise with the current RL approaches, and examines how we can address them with better TransRL models. To our knowledge, we are the first to present a comprehensive review of the recent Transformer research developments in RL for decision-making. We hope that this survey provides a comprehensive review of TransRL models and inspires the RL community in its pursuit of future directions. To keep track of the rapid TransRL developments in the decision-making domains, we summarize the latest papers and their open-source implementations at https://github.com/williamyuanv0/Transformer-in-Reinforcement-Learning-for-Decision-Making-A-Survey.},
	number = {6},
	journal = {Frontiers of Information Technology \& Electronic Engineering},
	author = {Yuan, Weilin and Chen, Jiaxing and Chen, Shaofei and Feng, Dawei and Hu, Zhenzhen and Li, Peng and Zhao, Weiwei},
	month = jun,
	year = {2024},
	pages = {763--790},
}

@inproceedings{10.5555/3600270.3602295,
author = {Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao and Lee, Lisa and Freeman, Daniel and Xu, Winnie and Guadarrama, Sergio and Fischer, Ian and Jang, Eric and Michalewski, Henryk and Mordatch, Igor},
title = {Multi-game decision transformers},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {A longstanding goal of the field of AI is a method for learning a highly capable, generalist agent from diverse experience. In the subfields of vision and language, this was largely achieved by scaling up transformer-based models and training them on large, diverse datasets. Motivated by this progress, we investigate whether the same strategy can be used to produce generalist reinforcement learning agents. Specifically, we show that a single transformer-based model - with a single set of weights - trained purely offline can play a suite of up to 46 Atari games simultaneously at close-to-human performance. When trained and evaluated appropriately, we find that the same trends observed in language and vision hold, including scaling of performance with model size and rapid adaptation to new games via fine-tuning. We compare several approaches in this multi-game setting, such as online and offline RL methods and behavioral cloning, and find that our Multi-Game Decision Transformer models offer the best scalability and performance. We release the pre-trained models and code to encourage further research in this direction.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2025},
numpages = {16},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}


@InProceedings{pmlr-v238-hong24a,
  title = 	 {A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning},
  author =       {Hong, Kihyuk and Li, Yuhang and Tewari, Ambuj},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {280--288},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/hong24a/hong24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/hong24a.html},
  abstract = 	 {Offline constrained reinforcement learning (RL) aims to learn a policy that maximizes the expected cumulative reward subject to constraints on expected cumulative cost using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm (PDCA), a novel algorithm for offline constrained RL with general function approximation. PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics. The primal player employs a no-regret policy optimization oracle to maximize the Lagrangian estimate and the dual player acts greedily to minimize the Lagrangian estimate. We show that PDCA finds a near saddle point of the Lagrangian, which is nearly optimal for the constrained RL problem. Unlike previous work that requires concentrability and a strong Bellman completeness assumption, PDCA only requires concentrability and realizability assumptions for sample-efficient learning.}
}


@article{3056976d4116471a86ab3fa345b1695d,
title = "A Hybrid Online Off-Policy Reinforcement Learning Agent Framework Supported by Transformers",
keywords = "Reinforcement learning, self-attention, off-policy, Transformer, experience replay, General Medicine, Computer Networks and Communications",
author = "Villarrubia-Martin, {Enrique Adrian} and Luis Rodriguez-Benitez and Luis Jimenez-Linares and David Mu{\~n}oz-Valero and Jun Liu",
year = "2023",
month = oct,
day = "20",
doi = "10.1142/s012906572350065x",
language = "English",
volume = "33",
journal = "International Journal of Neural Systems",
issn = "0129-0657",
publisher = "World Scientific Publishing",
number = "12",
}

@ARTICLE{9211734,
  author={Jin, Jiangliang and Xu, Yunjian},
  journal={IEEE Transactions on Smart Grid}, 
  title={Optimal Policy Characterization Enhanced Actor-Critic Approach for Electric Vehicle Charging Scheduling in a Power Distribution Network}, 
  year={2021},
  volume={12},
  number={2},
  pages={1416-1428},
  keywords={Electric vehicle charging;Optimal scheduling;Stochastic processes;Distribution networks;Reinforcement learning;Solar power generation;Dynamic programming;deep reinforcement learning;electric vehicle charging;actor-critic approach;power distribution network},
  doi={10.1109/TSG.2020.3028470}}

@inproceedings{10.5555/3600270.3603094,
author = {Paster, Keiran and McIlraith, Sheila A. and Ba, Jimmy},
title = {You can't count on luck: why decision transformers and RvS fail in stochastic environments},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, methods such as Decision Transformer [1] that reduce reinforcement learning to a prediction task and solve it via supervised learning (RvS) [2] have become popular due to their simplicity, robustness to hyperparameters, and strong overall performance on offline RL tasks. However, simply conditioning a probabilistic model on a desired return and taking the predicted action can fail dramatically in stochastic environments since trajectories that result in a return may have only achieved that return due to luck. In this work, we describe the limitations of RvS approaches in stochastic environments and propose a solution. Rather than simply conditioning on the return of a single trajectory as is standard practice, our proposed method, ESPER, learns to cluster trajectories and conditions on average cluster returns, which are independent from environment stochasticity. Doing so allows ESPER to achieve strong alignment between target return and expected performance in real environments. We demonstrate this in several challenging stochastic offline-RL tasks including the challenging puzzle game 2048, and Connect Four playing against a stochastic opponent. In all tested domains, ESPER achieves significantly better alignment between the target return and achieved return than simply conditioning on returns. ESPER also achieves higher maximum performance than even value-based baselines.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2824},
numpages = {14},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@InProceedings{pmlr-v202-lai23c,
  title = 	 {{C}hi{PF}ormer: Transferable Chip Placement via Offline Decision Transformer},
  author =       {Lai, Yao and Liu, Jinxin and Tang, Zhentao and Wang, Bin and Hao, Jianye and Luo, Ping},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {18346--18364},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/lai23c/lai23c.pdf},
  url = 	 {https://proceedings.mlr.press/v202/lai23c.html},
  abstract = 	 {Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing the runtime by 10x compared to recent state-of-the-art approaches in both public benchmarks and realistic industrial tasks. The deliverables are released at https://sites.google.com/view/chipformer/home.}
}


@ARTICLE{8727484,
  author={Sadeghianpourhamami, Nasrin and Deleu, Johannes and Develder, Chris},
  journal={IEEE Transactions on Smart Grid}, 
  title={Definition and Evaluation of Model-Free Coordination of Electrical Vehicle Charging With Reinforcement Learning}, 
  year={2020},
  volume={11},
  number={1},
  pages={203-214},
  keywords={Load modeling;Electric vehicle charging;Aggregates;Charging stations;Reinforcement learning;Markov processes;Data models;Demand response;electric vehicles;batch reinforcement learning},
  doi={10.1109/TSG.2019.2920320}}

@misc{chen2022deep,
    title={A Deep Reinforcement Learning-Based Charging Scheduling Approach with Augmented Lagrangian for Electric Vehicle},
    author={Guibin. Chen and Xiaoying. Shi},
    year={2022},
    eprint={2209.09772},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{sb3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{shojaeighadikolaei2024centralized,
    title={Centralized vs. Decentralized Multi-Agent Reinforcement Learning for Enhanced Control of Electric Vehicle Charging Networks},
    author={Amin Shojaeighadikolaei and Zsolt Talata and Morteza Hashemi},
    year={2024},
    eprint={2404.12520},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@article{ZHANG2023121490,
title = {A safe reinforcement learning-based charging strategy for electric vehicles in residential microgrid},
journal = {Applied Energy},
volume = {348},
pages = {121490},
year = {2023},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2023.121490},
url = {https://www.sciencedirect.com/science/article/pii/S0306261923008541},
author = {Shulei Zhang and Runda Jia and Hengxin Pan and Yankai Cao},
keywords = {Electric vehicles, Charging scheduling, Safe reinforcement learning, Constrained soft actor-critic, Residential microgrid, Nonlinear charging characteristics},
abstract = {With the growing popularity of electric vehicles (EVs), it is a new challenge for the residential microgrid system to conduct charging scheduling to meet the charging demands of EVs while maximizing its profit. In this work, a safe reinforcement learning (RL)-based charging scheduling strategy is proposed to meet this challenge. We construct a complete microgrid system equipped with a large charging station and consider different types of EVs, as well as the vehicle-to-grid (V2G) mode and nonlinear charging characteristics of EVs. Subsequently, the charging scheduling problem is formulated as a constrained Markov decision process (CMDP) due to the various limitations of power and demands. To effectively capture the uncertainties of the supply side and demand side of the microgrid, a model-free RL framework is employed. However, the curse of dimensionality of the action space is inevitable as EVs increase. To solve this problem, a charging and discharging strategy based on a general ladder electricity pricing scheme is designed. Different EVs are divided into different sets according to their states under this strategy, and the agent gives control signals of different sets instead of controlling each EV individually, which effectively reduces the dimension of the action space. Subsequently, a constrained soft actor-critic (CSAC) algorithm is designed to solve the established CMDP, and a safety filter is introduced to ensure safety. In the end, a numerical case is conducted to verify the effectiveness of the proposed method.}
}

@ARTICLE{9465776,
  author={Li, Sichen and Hu, Weihao and Cao, Di and Dragičević, Tomislav and Huang, Qi and Chen, Zhe and Blaabjerg, Frede},
  journal={Journal of Modern Power Systems and Clean Energy}, 
  title={Electric Vehicle Charging Management Based on Deep Reinforcement Learning}, 
  year={2022},
  volume={10},
  number={3},
  pages={719-730},
  keywords={Artificial neural networks;Electric vehicle charging;Schedules;Reinforcement learning;Feature extraction;Optimization;Batteries;Deep reinforcement learning;data-driven control;uncertainty;electric vehicles (EVs)},
  doi={10.35833/MPCE.2020.000460}}

@article{10.1609/aaai.v38i14.29499,
  author = {Wang, Y. and Yang, C. and Wen, Y. and Liu, Y. and Qiao, Y.},
  title = {Critic-guided decision transformer for offline reinforcement learning},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year = {2024},
  volume = {38},
  issue = {14},
  pages = {15706-15714},
  doi = {10.1609/aaai.v38i14.29499}
}

@INPROCEEDINGS{10774394,
  author={Li, Ziyue and Chen, Jiancheng and Xu, Jianzhi},
  booktitle={2024 IEEE 22nd International Conference on Industrial Informatics (INDIN)}, 
  title={EMDT: A Decision Transformer-Based Energy Management Strategy in Integrated Energy Systems}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Electric potential;Supply and demand;Electricity;Decision making;Virtual power plants;Transformers;Stability analysis;Informatics;Research and development;Integrated Energy System;Energy Management;Decision Transformer},
  doi={10.1109/INDIN58382.2024.10774394}}

@misc{wang2020statistical,
    title={What are the Statistical Limits of Offline RL with Linear Function Approximation?},
    author={Ruosong Wang and Dean P. Foster and Sham M. Kakade},
    year={2020},
    eprint={2010.11895},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{n2024quantifying,
    title={Quantifying the Aggregate Flexibility of EV Charging Stations for Dependable Congestion Management Products: A Dutch Case Study},
    author={Nanda Kishor Panda and Simon H. Tindemans},
    year={2024},
    eprint={2403.13367},
    archivePrefix={arXiv},
    primaryClass={eess.SY}
}

@article{JIN2022120140,
title = {Deep reinforcement learning-based strategy for charging station participating in demand response},
journal = {Applied Energy},
volume = {328},
pages = {120140},
year = {2022},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2022.120140},
url = {https://www.sciencedirect.com/science/article/pii/S0306261922013976},
author = {Ruiyang Jin and Yuke Zhou and Chao Lu and Jie Song},
keywords = {Demand response, Charging station, Electric vehicle, Deep reinforcement learning},
abstract = {The trend of zero-carbonization has accelerated the prevalence of electric vehicles (EVs) owing to their advantages of low carbon emissions and high energy efficiency. The stochastic and high charging load of EVs results in a non-negligible challenge that may cause grid overload. A promising approach is the participation of charging stations in demand response as load aggregators by coordinating the charging power of electric vehicles. However, improper coordination of charging load may lead to unfulfilled charging demand, which would cause dissatisfaction on the demand side. In this study, the incentive-based and time-varying demand response mechanism is considered when charging stations coordinate charging of multiple EVs. A decentralized decision-making framework is innovatively applied to provide charging power of each EV. The charging process is modeled as a Markov decision process, and a virtual price is designed to help decide the charging power. Deep reinforcement learning algorithms such as deep deterministic policy gradient are applied to determine the charging strategy of multiple and heterogeneous EVs. Numerical experiments are performed to validate the effectiveness of the proposed method. A comparison with an optimal charging strategy and a heuristic rule-based method shows that the proposed method can trade off the revenue from demand response and user satisfaction, as well as reduce the peak load of the charging station. Furthermore, a test with inaccurate departure information indicates the robustness of the proposed method.}
}

@ARTICLE{9371688,
  author={Abdullah, Heba M. and Gastli, Adel and Ben-Brahim, Lazhar},
  journal={IEEE Access}, 
  title={Reinforcement Learning Based EV Charging Management Systems–A Review}, 
  year={2021},
  volume={9},
  number={},
  pages={41506-41531},
  keywords={Electric vehicle charging;Uncertainty;Batteries;Reinforcement learning;Vehicle-to-grid;Optimization;Load modeling;Global warming;Artificial intelligence;electric vehicles;machine learning;management;smart grids},
  doi={10.1109/ACCESS.2021.3064354}}

@misc{kipf2016semisupervised,
    title={Semi-Supervised Classification with Graph Convolutional Networks},
    author={Thomas N. Kipf and Max Welling},
    year={2016},
    eprint={1609.02907},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{10.5555/3666122.3666936,
author = {Wu, Yueh-Hua and Wang, Xiaolong and Hamaya, Masashi},
title = {Elastic decision transformer},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This paper introduces Elastic Decision Transformer (EDT), a significant advancement over the existing Decision Transformer (DT) and its variants. Although DT purports to generate an optimal trajectory, empirical evidence suggests it struggles with trajectory stitching, a process involving the generation of an optimal or near-optimal trajectory from the best parts of a set of sub-optimal trajectories. The proposed EDT differentiates itself by facilitating trajectory stitching during action inference at test time, achieved by adjusting the history length maintained in DT. Further, the EDT optimizes the trajectory by retaining a longer history when the previous trajectory is optimal and a shorter one when it is sub-optimal, enabling it to "stitch" with a more optimal trajectory. Extensive experimentation demonstrates EDT's ability to bridge the performance gap between DT-based and Q Learning-based approaches. In particular, the EDT outperforms Q Learning-based methods in a multi-task regime on the D4RL locomotion benchmark and Atari games. Videos are available at: https://kristery.github.io/edt/.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {814},
numpages = {19},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@misc{liu2023constrained,
    title={Constrained Decision Transformer for Offline Safe Reinforcement Learning},
    author={Zuxin Liu and Zijian Guo and Yihang Yao and Zhepeng Cen and Wenhao Yu and Tingnan Zhang and Ding Zhao},
    year={2023},
    eprint={2302.07351},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{online_dt,
  title = 	 {Online Decision Transformer},
  author =       {Zheng, Qinqing and Zhang, Amy and Grover, Aditya},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27042--27059},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22c/zheng22c.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22c.html},
  abstract = 	 {Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via task-specific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.}
}



@article{wang_domain_2024,
	title = {Domain {Adaptation} in {Reinforcement} {Learning}: {Approaches}, {Limitations}, and {Future} {Directions}},
	volume = {105},
	issn = {2250-2114},
	url = {https://doi.org/10.1007/s40031-024-01049-4},
	doi = {10.1007/s40031-024-01049-4},
	abstract = {Reinforcement learning (RL) has demonstrated impressive results in various fields; however, its performance can be significantly hindered when the training and testing environments differ. Domain adaptation (DA) techniques aim to bridge this gap by moving knowledge between domains. This paper presents a thorough and systematic study of DA in RL. We review and categorize existing approaches for DA in RL, including model-free and model-based. We examine the drawbacks associated with each approach, such as sample inefficiency and generalization issues. Furthermore, we explore various strategies used in DA, such as feature adaptation, reward shaping, and data augmentation. We provide insights into the benefits and drawbacks of different techniques and propose future research directions for enhancing DA in RL. Through this study, the goal is to offer comprehensive insight into the current state of DA in RL and contribute to developing more robust and adaptable RL algorithms.},
	number = {5},
	journal = {Journal of The Institution of Engineers (India): Series B},
	author = {Wang, Bin},
	month = oct,
	year = {2024},
	pages = {1223--1240},
}
@ARTICLE{10803908,
  author={Orfanoudakis, Stavros and Diaz-Londono, Cesar and Yılmaz, Yunus Emre and Palensky, Peter and Vergara, Pedro P.},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and Benchmarking}, 
  year={2024},
  volume={},
  number={},
  pages={1-12},
  keywords={Vehicle-to-grid;Smart charging;Optimization;Benchmark testing;Batteries;Data models;Schedules;Reinforcement learning;Prediction algorithms;Power transformers;Electric vehicle optimization;gym environment;reinforcement learning;mathematical programming;model predictive control (MPC)},
  doi={10.1109/TITS.2024.3510945}}

@article{KAMRANI2025100620,
title = {Multi-agent deep reinforcement learning with online and fair optimal dispatch of EV aggregators},
journal = {Machine Learning with Applications},
pages = {100620},
year = {2025},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2025.100620},
url = {https://www.sciencedirect.com/science/article/pii/S2666827025000039},
author = {Arian Shah Kamrani and Anoosh Dini and Hanane Dagdougui and Keyhan Sheshyekani},
keywords = {Electric vehicle (EV), Reinforcement learning (RL), Energy management, Jain’s index, Distribution system operator (DSO)},
abstract = {The growing popularity of electric vehicles (EVs) and the unpredictable behavior of EV owners have attracted attention to real-time coordination of EVs charging management. This paper presents a hierarchical structure for charging management of EVs by integrating fairness and efficiency concepts within the operations of the distribution system operator (DSO) while utilizing a multi-agent deep reinforcement learning (MADRL) framework to tackle the complexities of energy purchasing and distribution among EV aggregators (EVAs). At the upper level, DSO calculates the maximum allowable power for each EVA based on power flow constraints to ensure grid safety. Then, it finds the optimal efficiency-jain tradeoff (EJT) point, where it sells the highest energy amount while ensuring equitable energy distribution. At the lower level, initially, each EVA acts as an agent employing a double deep Q-network (DDQN) with adaptive learning rates and prioritized experience replay to determine optimal energy purchases from the DSO. Then, the real-time smart dispatch (RSD) controller prioritizes EVs for energy dispatch based on relevant EVs information. Findings indicate the proposed enhanced DDQN outperforms deep deterministic policy gradient (DDPG) and proximal policy optimization (PPO) in cumulative rewards and convergence speed. Finally, the framework’s performance is evaluated against uncontrolled charging and the first come first serve (FCFS) scenario using the 118-bus distribution system, demonstrating superior performance in maintaining safe operation of the grid while reducing charging costs for EVAs. Additionally, the framework’s integration with renewable energy sources (RESs), such as photovoltaic (PV), demonstrates its potential to enhance grid reliability.}
}

@article{QIU2023113052,
title = {Reinforcement learning for electric vehicle applications in power systems:A critical review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {173},
pages = {113052},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.113052},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122009339},
author = {Dawei Qiu and Yi Wang and Weiqi Hua and Goran Strbac},
keywords = {Electric vehicles, Vehicle-to-grid, Reinforcement learning, Power systems},
abstract = {Electric vehicles (EVs) are playing an important role in power systems due to their significant mobility and flexibility features. Nowadays, the increasing penetration of renewable energy resources has been observed in modern power systems, which brings many benefits for improving climate change and accelerating the low-carbon transition. However, the intermittent and unstable nature of renewable energy sources introduces new challenges to both the planning and operation of power systems. To address these issues, vehicle-to-grid (V2G) technology has been gradually recognized as a valid solution to provide various ancillary service provisions for power systems. Many studies have developed model-based optimization methods for EV dispatch problems. Nevertheless, this type of method cannot effectively handle the highly dynamic and stochastic environment due to the complexity of power systems. Reinforcement learning (RL), a model-free and online learning method, can capture various uncertainties through numerous interactions with the environment and adapt to various state conditions in real-time. As a result, using advanced RL algorithms to solve various EV dispatch problems has attracted a surge of attention in recent years, leading to many outstanding research papers and important findings. This paper provides a comprehensive review of popular RL algorithms categorized by single-agent RL and multi-agent RL, and summarizes how these advanced algorithms can be applied to various EV dispatch problems, including grid-to-vehicle (G2V), vehicle-to-home (V2H), and V2G. Finally, key challenges and important future research directions are discussed, which involve five aspects: (a) data quality and availability; (b) environment setup; (c) safety and robustness; (d) training performance; and (e) real-world deployment.}
}

@INPROCEEDINGS{isgt2024,
  author={Yılmaz, Yunus Emre and Orfanoudakis, Stavros and Vergara, Pedro P.},
  booktitle={2024 IEEE PES Innovative Smart Grid Technologies Europe (ISGT EUROPE)}, 
  title={Reinforcement Learning for Optimized EV Charging Through Power Setpoint Tracking},
  volume={},
  number={},
  pages={1-5},
  year={2024},
  doi={}
}

@misc{Orfanoudakis2024,
  title        = {Scalable Reinforcement Learning for Large-Scale Coordination of Electric Vehicles Using Graph Neural Networks},
  author       = {Orfanoudakis, Stavros and Robu, Valentin and Salazar Duque, Edgar Mauricio and others},
  howpublished = {Preprint (Version 1) available at Research Square},
  year         = {2024},
  month        = dec,
  url          = {https://doi.org/10.21203/rs.3.rs-5504138/v1},
  note         = {Accessed on 19 December 2024}
}