\section{Related Work}
% Paragraph: Talk about DT
\paragraph{Advancements in Decision Transformers}
Classic DT encounters significant challenges, including limited trajectory stitching capabilities and difficulties in adapting to online environments. To address these issues, several enhancements have been proposed. The Q-DT____ improves the ability to derive optimal policies from sub-optimal trajectories by relabeling return-to-go values in the training data. Elastic DT____ enhances classic DT by enabling trajectory stitching during action inference at test time, while Multi-Game DT____ advances its task generalization capabilities. The Online DT____ extends DTs to online settings by combining offline pretraining with online fine-tuning, facilitating continuous policy updates in dynamic environments. Additionally, adaptations for offline safe RL incorporate cost tokens alongside rewards____. DT has also been effectively applied to real-world domains, such as healthcare____ and chip design____, showcasing its versatility and practical utility.

\paragraph{RL for EV Smart Charging}
RL algorithms offer notable advantages for EV dispatch, including the ability to handle nonlinear models, robustly quantify uncertainty, and deliver faster computation than traditional mathematical programming____. Popular methods, such as DDPG____, SAC____, and batch RL____, show promise but often lack formal constraint satisfaction guarantees and struggle to scale with high-dimensional state-action spaces____. Safe RL frameworks address these drawbacks by imposing constraints via constrained MDPs, but typically sacrifice performance and scalability____. Multiagent RL techniques distribute complexity across multiple agents, e.g. charging points, stations, or aggregators____, yet still encounter convergence challenges and may underperform in large-scale applications. 
To the best of our knowledge, no study has used DTs for solving the complex EV charging problem, despite DT's potential to handle sparse rewards effectively.