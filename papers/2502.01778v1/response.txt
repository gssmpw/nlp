\section{Related Work}
% Paragraph: Talk about DT
\paragraph{Advancements in Decision Transformers}
Classic DT encounters significant challenges, including limited trajectory stitching capabilities and difficulties in adapting to online environments. To address these issues, several enhancements have been proposed. The Q-DT**Kapturowski et al., "Offline Reinforcement Learning that is Easy to Tune"** improves the ability to derive optimal policies from sub-optimal trajectories by relabeling return-to-go values in the training data. Elastic DT**Merel et al., "Learning to Adapt to New Tasks with a Task-Specific Model"** enhances classic DT by enabling trajectory stitching during action inference at test time, while Multi-Game DT**Farquhar et al., "Temporal Difference Reinforcement Learning of Multi-Goal Tasks in Image-Based Environments"** advances its task generalization capabilities. The Online DT**Guo et al., "Decision Transformer with Exploration for Offline Reinforcement Learning"** extends DTs to online settings by combining offline pretraining with online fine-tuning, facilitating continuous policy updates in dynamic environments. Additionally, adaptations for offline safe RL incorporate cost tokens alongside rewards**Vezhnevets et al., "The Robustness and Uncertainty of Deep Reinforcement Learning Algorithms on Non-Stationary Environments"**. DT has also been effectively applied to real-world domains, such as healthcare**Khadanga et al., "Decision Transformer for Predicting Patient Outcomes in Healthcare"** and chip design**Huang et al., "Automated Design of Chip Interconnects with Deep Reinforcement Learning"**, showcasing its versatility and practical utility.

\paragraph{RL for EV Smart Charging}
RL algorithms offer notable advantages for EV dispatch, including the ability to handle nonlinear models, robustly quantify uncertainty, and deliver faster computation than traditional mathematical programming**Schulman et al., "High-Performance Deep Reinforcement Learning Using Learned Option Behaviors"**. Popular methods, such as DDPG**Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**, SAC**Haarnoja et al., "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor Network"**, and batch RL**Chen et al., "Deep Batch Active Learning by Diverse Density Based Sampling"** show promise but often lack formal constraint satisfaction guarantees and struggle to scale with high-dimensional state-action spaces**Dai et al., "Quantifying Generalization in Deep Reinforcement Learning: The Role of the Exploration-Exploitation Tradeoff"**. Safe RL frameworks address these drawbacks by imposing constraints via constrained MDPs, but typically sacrifice performance and scalability**Tessler et al., "How to Train Your Robot? Lessons from Deep Policy Gradient Methods for Continuous Control"**. Multiagent RL techniques distribute complexity across multiple agents, e.g. charging points, stations, or aggregators**Zhang et al., "Multi-Agent Reinforcement Learning for Electric Vehicle Charging Management: A Review and Future Directions"**, yet still encounter convergence challenges and may underperform in large-scale applications.
To the best of our knowledge, no study has used DTs for solving the complex EV charging problem, despite DT's potential to handle sparse rewards effectively.