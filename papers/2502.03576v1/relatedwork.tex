\section{Related Work}
Our work intersects with several bodies of literature; we focus hereafter on 
three of the most relevant topics.



\paragraph{Benchmark Aggregation and Measuring Task Similarity.}

In a recent line of work \cite{colombo_what_2022,himmi_towards_2023,rofin_votenrank_2023,tatiana_how_2021,zhang_inherent_2024}, multi-tasks benchmarking practices have been scrutinized through the lenses of social choice theory. In particular, these works question the usage of the arithmetical mean to aggregate scores of different tasks in popular benchmarks \cite{koh_wilds_2021,wang_superglue_2020}
%wang_glue_2019,
and investigate different aggregates such as the Pythagorean means \cite{tatiana_how_2021}, the Bradley-Terry model \cite{peyrard_better_2021}, or other classical voting rules \cite{colombo_what_2022,himmi_towards_2023,rofin_votenrank_2023}.

Contrary to usual voting scenarios where the equal treatment of voters is of utmost importance, there is apriori no requirement to treat each task equally in benchmark aggregation scenarios, and we may want to consider voting schemes with different weights, e.g., chosen arbitrarily by the benchmark creator.
These weights may however carry more information than arbitrary preference. In \cite{balduzzi_re-evaluating_2018}, researchers proposed to model the evaluation of agents on different tasks through a zero-sum meta-game played between an ``agent'' and a ``task'' player, each choosing a probability distribution over the corresponding set. Scores on different tasks are then aggregated with a weighted average, where the weights correspond to the probability of playing each task in the entropy-maximizing Nash Equilibrium. One of the desirable properties of this technique is that it is invariant under the addition of \emph{exact} copies of agents, a property which has been studied under the appellation \emph{false-name-proofness} in social choice theory \cite{conitzer_using_2010,nehama_manipulation-resistant_2022,todo_characterizing_2009}. Note however that this property is very brittle and only applies to purely adversarial scenarios, as we lose all guarantees whenever a vanishingly small amount of noise is added to one of the copies. 
Our work aims exactly at extending this property to near-copies, thus ensuring it remains usable in real-world scenarios. Calculating the weights for different tasks using the representation function introduced in this work ensures an automatic scaling of the benchmark, i.e., the evaluation always benefits from adding new although slightly redundant tasks.


 
Importantly, our framework assumes that practitioners provide a suitable distance metric, as its selection lies beyond the scope of this work.
Identifying an appropriate distance metric between tasks or datasets is a critical prerequisite for applying our approach effectively. 
Fortunately, this challenge has been extensively explored 
%in prior works 
\cite{alvarez-melis_geometric_2020,gretton_kernel_2012,liu_wasserstein_2022}, particularly within the transfer learning literature \cite{achille_task2vec_2019,peng_domain2vec_2020}. 
This existing body of research complements our work and provides valuable guidance for practitioners seeking to use our framework for benchmark aggregation.








\paragraph{Domain Adaptation and Samples Reweighting.}

In traditional learning setups, training and testing data are assumed to follow the same distribution. \emph{Domain adaptation} \cite{wang_deep_2018}, however, addresses scenarios where this assumption is violated, such as in the presence of class imbalance or label noise \cite{torralba_unbiased_2011}.

One approach to handle biases in training datasets involves assigning weights to individual samples and minimizing a weighted loss. Classical algorithms, such as AdaBoost \cite{freund_decision-theoretic_1997}, hard-negative mining \cite{chang_active_2018},
%,malisiewicz_ensemble_2011,lin_focal_2018
self-paced learning \cite{jiang_self-paced_2015}, adapt these weights dynamically during training based on the observed training loss. In contrast, the \emph{meta-learning} framework \cite{jamal_rethinking_2020,ren_learning_2019,shu_meta-weight-net_2019}
%
iteratively optimizes the weighting 
%weights 
to minimize loss on a small, unbiased validation dataset. Diverging from these methods, we consider a one-shot scenario, where the sample weighting is determined a priori and remains fixed.


In the context of imbalanced classification and long-tailed datasets, reweighting techniques have been explored extensively \cite{cao_learning_2019,dong_class_2017,gebru_fine-grained_2017}. These methods typically assign weights inversely proportional to the number of instances in each class. Recent approaches, such as those proposed in \cite{cui_class-balanced_2019}, go further by accounting for data overlap. They suggest weighting samples based on the effective number of samples, under the intuition that the marginal benefit of adding a new sample diminishes as the sample count grows. Specifically, they expand each data point to include its surrounding neighborhood and define the informativeness of a sample as the additional coverage %volume
it contributes compared to the scenario where the sample is excluded.
Our theoretical framework, in particular the locality axiom (see Axiom~\ref{axi:alpha_clone_locality}) and the volume-based construction 
%parametrized by $\alpha$ 
(see Section~\ref{sec:local_voting}), draws a close parallel to their total volume of sampled data, but extends this intuition beyond simple classification problems.


More generally, reweighting is a key technique in addressing \emph{covariate shift} within domain adaptation. Covariate shift occurs when the input distribution differs between training and evaluation datasets, i.e., $P_{train}(x) \neq P_{test}(x)$, but the conditional distribution $P_{train}(y \vert x) = P_{test}(y \vert x)$ remains consistent. Originating in importance sampling -- a technique commonly used to reduce variance in Monte Carlo estimation -- different methods \cite{y_covariate_2019} tackle covariate shift by reweighting samples with the ratio $P_{test} (x) / P_{train}(x).$ Approaches such as \emph{Kernel Density Estimation} \cite{hardle_nonparametric_2004} approximate these distributions using Gaussian kernels but suffer from the curse of dimensionality. Alternatively, \emph{Kernel Mean Matching} \cite{gretton_covariate_2009} minimizes the discrepancy between training and test distributions by aligning their means in a reproducing kernel Hilbert space, effectively estimating $P_{test} (x) / P_{train}(x) $ directly.
In contrast to these statistical methods, our approach does not rely on assumptions about the stochasticity of the sampling process. Instead, we adopt an axiomatic framework that provides robustness guarantees even when samples are adversarially selected. This makes our method more resilient to challenges like dataset poisoning \cite{carlini_poisoning_2024}.







\paragraph{Metric Learning and Hierarchical clustering.}

Metric learning and clustering techniques adopt fundamentally opposing philosophies. On the one hand, metric learning generally assumes access to ground-truth labels of similarity, e.g., whether points belong to the same class, and seeks to derive a distance metric, often within the class of generalized Mahalanobis metrics, that best separates dissimilar points while bringing similar ones closer \cite{ghojogh_spectral_2022}. 
On the other hand, clustering assumes some ground truth distance metric and aims to recover a notion of class by grouping similar points into clusters.
Our framework aligns more closely with clustering techniques through its shared starting assumption -- the availability of an informative distance metric. However, it diverges in its objective, focusing instead on the implications of similarity for 
%achieving 
unbiased weighting of 
%the 
data points.


Still, clustering techniques may represent a useful step toward this goal: one could first group points into clusters, assign clusters equal weights, and then share these weights uniformly within each cluster.
Such ``hard'' clusters may however lack the smooth properties we aim for. These limitations can be mitigated by adopting hierarchical clustering techniques, where points can belong to multiple clusters arranged in a tree structure (dendrogram) \cite{murtagh_algorithms_2017,ran_comprehensive_2023}. This approach enables contributions across different scales, akin to the role of the probability distribution $\nu$  in Theorem~\ref{thm:cont_convex_combi_gr}.

One may even consider more flexible structures, such as \emph{Fuzzy Hierarchical Clustering (FHC)} or \emph{Overlapping Hierarchical Clustering (OHC)}. 
FHC \cite{varshney_pifhc_2022} 
%varshney_improved_2020,
allows points to have partial membership in several clusters at once, with the sum of memberships normalized across clusters. 
OHC \cite{berthold_overlapping_2020}, on the other hand, constructs directed acyclic graphs of clusters (quasi-dendrograms) instead of traditional trees, enabling a soft merging process. This approach allows points to have full membership in multiple clusters simultaneously, letting clusters overlap without the need for fuzzy memberships.

In any case, transitioning from clusters of points to individual weights becomes a non-trivial task for ``soft'' clusters. In this work, we move away from the concept of clusters and allow for non-transitive similarity relations.