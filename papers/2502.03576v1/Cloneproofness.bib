
@misc{balduzzi_re-evaluating_2018,
	title = {Re-evaluating {Evaluation}},
	url = {http://arxiv.org/abs/1806.02643},
	doi = {10.48550/arXiv.1806.02643},
	abstract = {Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents.},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Balduzzi, David and Tuyls, Karl and Perolat, Julien and Graepel, Thore},
	month = oct,
	year = {2018},
	note = {arXiv:1806.02643}, 
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory},
	annote = {Comment: NIPS 2018, final version},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/WNQTUCBE/Balduzzi et al. - 2018 - Re-evaluating Evaluation.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/5RJWPNEQ/1806.html:text/html},
}

@inproceedings{rofin_votenrank_2023,
	address = {Dubrovnik, Croatia},
	title = {Vote'n'{Rank}: {Revision} of {Benchmarking} with {Social} {Choice} {Theory}},
	shorttitle = {Vote'n'{Rank}},
	url = {https://aclanthology.org/2023.eacl-main.48},
	doi = {10.18653/v1/2023.eacl-main.48},
	abstract = {The development of state-of-the-art systems in different applied areas of machine learning (ML) is driven by benchmarks, which have shaped the paradigm of evaluating generalisation capabilities from multiple perspectives. Although the paradigm is shifting towards more fine-grained evaluation across diverse tasks, the delicate question of how to aggregate the performances has received particular interest in the community. In general, benchmarks follow the unspoken utilitarian principles, where the systems are ranked based on their mean average score over task-specific metrics. Such aggregation procedure has been viewed as a sub-optimal evaluation protocol, which may have created the illusion of progress. This paper proposes Vote'n'Rank, a framework for ranking systems in multi-task benchmarks under the principles of the social choice theory. We demonstrate that our approach can be efficiently utilised to draw new insights on benchmarking in several ML sub-fields and identify the best-performing systems in research and development case studies. The Vote'n'Rank's procedures are more robust than the mean average while being able to handle missing performance scores and determine conditions under which the system becomes the winner.},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rofin, Mark and Mikhailov, Vladislav and Florinsky, Mikhail and Kravchenko, Andrey and Shavrina, Tatiana and Tutubalina, Elena and Karabekyan, Daniel and Artemova, Ekaterina},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {670--686},
	annote = {Take Benchmarks as voters, and considers different voting rules to aggregate preferences.Not so much theory, primarily measuring the changes on popular benchmarks
},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/CXWPIVVI/Rofin et al. - 2023 - Vote'n'Rank Revision of Benchmarking with Social .pdf:application/pdf},
}

@misc{zhang_inherent_2024,
	title = {Inherent {Trade}-{Offs} between {Diversity} and {Stability} in {Multi}-{Task} {Benchmarks}},
	url = {https://arxiv.org/abs/2405.01719v2},
	abstract = {We examine multi-task benchmarks in machine learning through the lens of social choice theory. We draw an analogy between benchmarks and electoral systems, where models are candidates and tasks are voters. This suggests a distinction between cardinal and ordinal benchmark systems. The former aggregate numerical scores into one model ranking; the latter aggregate rankings for each task. We apply Arrow's impossibility theorem to ordinal benchmarks to highlight the inherent limitations of ordinal systems, particularly their sensitivity to the inclusion of irrelevant models. Inspired by Arrow's theorem, we empirically demonstrate a strong trade-off between diversity and sensitivity to irrelevant changes in existing multi-task benchmarks. Our result is based on new quantitative measures of diversity and sensitivity that we introduce. Sensitivity quantifies the impact that irrelevant changes to tasks have on a benchmark. Diversity captures the degree of disagreement in model rankings across tasks. We develop efficient approximation algorithms for both measures, as exact computation is computationally challenging. Through extensive experiments on seven cardinal benchmarks and eleven ordinal benchmarks, we demonstrate a clear trade-off between diversity and stability: The more diverse a multi-task benchmark, the more sensitive to trivial changes it is. Additionally, we show that the aggregated rankings of existing benchmarks are highly unstable under irrelevant changes. The codes and data are available at https://socialfoundations.github.io/benchbench/.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Zhang, Guanhua and Hardt, Moritz},
	month = may,
	year = {2024},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/2VPWMBW8/Zhang and Hardt - 2024 - Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks.pdf:application/pdf},
}

@misc{tatiana_how_2021,
	title = {How not to {Lie} with a {Benchmark}: {Rearranging} {NLP} {Leaderboards}},
	shorttitle = {How not to {Lie} with a {Benchmark}},
	url = {http://arxiv.org/abs/2112.01342},
	doi = {10.48550/arXiv.2112.01342},
	abstract = {Comparison with a human is an essential requirement for a benchmark for it to be a reliable measurement of model capabilities. Nevertheless, the methods for model comparison could have a fundamental flaw - the arithmetic mean of separate metrics is used for all tasks of different complexity, different size of test and training sets. In this paper, we examine popular NLP benchmarks' overall scoring methods and rearrange the models by geometric and harmonic mean (appropriate for averaging rates) according to their reported results. We analyze several popular benchmarks including GLUE, SuperGLUE, XGLUE, and XTREME. The analysis shows that e.g. human level on SuperGLUE is still not reached, and there is still room for improvement for the current models.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Tatiana, Shavrina and Valentin, Malykh},
	month = dec,
	year = {2021},
	note = {arXiv:2112.01342 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, 68-06, 68T50, 68T01, G.3, I.2.7},
	annote = {Comment: Accepted to ICBINB Workshop, NeurIPS 2021},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/EQIHDDH3/Tatiana and Valentin - 2021 - How not to Lie with a Benchmark Rearranging NLP Leaderboards.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/26RYBY2A/2112.html:text/html},
}

@misc{colombo_what_2022,
	title = {What are the best systems? {New} perspectives on {NLP} {Benchmarking}},
	shorttitle = {What are the best systems?},
	url = {http://arxiv.org/abs/2202.03799},
	doi = {10.48550/arXiv.2202.03799},
	abstract = {In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in (i) assessing the progress of new methods along different axes and (ii) selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (e.g. GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Colombo, Pierre and Noiry, Nathan and Irurozki, Ekhine and Clemencon, Stephan},
	month = oct,
	year = {2022},
	note = {arXiv:2202.03799 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/3BC9E9KR/Colombo et al. - 2022 - What are the best systems New perspectives on NLP Benchmarking.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/BY2EHVQJ/2202.html:text/html},
}

@misc{himmi_towards_2023,
	title = {Towards {More} {Robust} {NLP} {System} {Evaluation}: {Handling} {Missing} {Scores} in {Benchmarks}},
	shorttitle = {Towards {More} {Robust} {NLP} {System} {Evaluation}},
	url = {https://arxiv.org/abs/2305.10284v1},
	abstract = {The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.},
	language = {en},
	urldate = {2024-09-12},
	journal = {arXiv.org},
	author = {Himmi, Anas and Irurozki, Ekhine and Noiry, Nathan and Clemencon, Stephan and Colombo, Pierre},
	month = may,
	year = {2023},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/C57HQVR3/Himmi et al. - 2023 - Towards More Robust NLP System Evaluation Handling Missing Scores in Benchmarks.pdf:application/pdf},
}

@article{christensen_measures_1970,
	title = {On {Some} {Measures} {Analogous} to {Haar} {Measure}.},
	volume = {26},
	copyright = {Copyright (c)},
	issn = {1903-1807},
	url = {https://www.mscand.dk/article/view/10969},
	doi = {10.7146/math.scand.a-10969},
	language = {en},
	urldate = {2024-11-04},
	journal = {MATHEMATICA SCANDINAVICA},
	author = {Christensen, Jens Peter Reus},
	month = jun,
	year = {1970},
	pages = {103--106},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/D6XFEJHW/Christensen - 1970 - On Some Measures Analogous to Haar Measure..pdf:application/pdf},
}

@article{dokmanic_euclidean_2015,
	title = {Euclidean {Distance} {Matrices}: {Essential} theory, algorithms, and applications},
	volume = {32},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1053-5888},
	shorttitle = {Euclidean {Distance} {Matrices}},
	url = {http://ieeexplore.ieee.org/document/7298562/},
	doi = {10.1109/MSP.2015.2398954},
	language = {en},
	number = {6},
	urldate = {2024-11-10},
	journal = {IEEE Signal Processing Magazine},
	author = {Dokmanic, Ivan and Parhizkar, Reza and Ranieri, Juri and Vetterli, Martin},
	month = nov,
	year = {2015},
	pages = {12--30},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/UBAAL4DE/Dokmanic et al. - 2015 - Euclidean Distance Matrices Essential theory, algorithms, and applications.pdf:application/pdf},
}

@book{horn_matrix_2012,
	address = {Cambridge ; New York},
	edition = {2nd ed},
	title = {Matrix analysis},
	isbn = {978-0-521-83940-2},
	abstract = {"The thoroughly revised and updated second edition of this acclaimed text has several new and expanded sections and more than 1,100 exercises"--},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Horn, Roger A. and Johnson, Charles R.},
	year = {2012},
	keywords = {MATHEMATICS / Algebra / Abstract, Matrices},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/6JMYP4S6/Horn and Johnson - 2012 - Matrix analysis.pdf:application/pdf},
}

@book{federer_geometric_1996,
	address = {Berlin, Heidelberg},
	series = {Classics in {Mathematics}},
	title = {Geometric {Measure} {Theory}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-540-60656-7 978-3-642-62010-2},
	url = {http://link.springer.com/10.1007/978-3-642-62010-2},
	urldate = {2024-11-12},
	publisher = {Springer},
	author = {Federer, Herbert},
	editor = {Eckmann, B. and Van Der Waerden, B. L.},
	year = {1996},
	doi = {10.1007/978-3-642-62010-2},
	keywords = {Geometric measure theory, calculus, calculus of variations, classical analysis, classical geometry, form, functional, homology, integration, integration theory, Lebesgue integration, measure theory, Multiplication, sets, Tensor},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/8EQMJRY8/Federer - 1996 - Geometric Measure Theory.pdf:application/pdf},
}

@techreport{mitchell_fast_2018,
	title = {Fast {Approximate} {Union} {Volume} in {High} {Dimensions} with {Line} {Samples}},
	url = {http://www.osti.gov/servlets/purl/1464880/},
	abstract = {The classical problem of calculating the volume of the union of d-dimensional balls is known as “UnionVolume.” We present line-sampling approximation algorithms for UnionVolume. Our methods may be extended to other Boolean operations, such as setminus; or to other shapes, such as hyper-rectangles. The deterministic, exact approaches for UnionVolume do not scale well to high dimensions. However, we adapt several of these exact approaches to approximation algorithms based on sampling. We perform local sampling within each ball using lines. We have several variations, depending on how the overlapping volume is partitioned, and depending on whether radial, axis-aligned, or other line patterns are used.},
	language = {en},
	number = {SAND--2018-8684, 1464880},
	urldate = {2024-11-13},
	author = {Mitchell, Scott A. and Awad, Muhammad A. and Ebeida, Mohamed S. and Swiler, Laura P.},
	month = aug,
	year = {2018},
	doi = {10.2172/1464880},
	pages = {SAND--2018--8684, 1464880},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/7FCTLUEA/Mitchell et al. - 2018 - Fast Approximate Union Volume in High Dimensions with Line Samples.pdf:application/pdf},
}

@misc{helfgott_graph_2017,
	title = {Graph isomorphisms in quasi-polynomial time},
	url = {http://arxiv.org/abs/1710.04574},
	doi = {10.48550/arXiv.1710.04574},
	abstract = {Let us be given two graphs \${\textbackslash}Gamma\_1\$, \${\textbackslash}Gamma\_2\$ of \$n\$ vertices. Are they isomorphic? If they are, the set of isomorphisms from \${\textbackslash}Gamma\_1\$ to \${\textbackslash}Gamma\_2\$ can be identified with a coset \$H{\textbackslash}cdot{\textbackslash}pi\$ inside the symmetric group on \$n\$ elements. How do we find \${\textbackslash}pi\$ and a set of generators of \$H\$? The challenge of giving an always efficient algorithm answering these questions remained open for a long time. Babai has recently shown how to solve these problems -- and others linked to them -- in quasi-polynomial time, i.e. in time \${\textbackslash}exp{\textbackslash}left(O({\textbackslash}log n){\textasciicircum}\{O(1)\}{\textbackslash}right)\$. His strategy is based in part on the algorithm by Luks (1980/82), who solved the case of graphs of bounded degree.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Helfgott, Harald Andrés and Bajpai, Jitendra and Dona, Daniele},
	month = oct,
	year = {2017},
	note = {arXiv:1710.04574},
	keywords = {Mathematics - Group Theory},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/R549CQM3/Helfgott et al. - 2017 - Graph isomorphisms in quasi-polynomial time.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/GJTTXFAI/1710.html:text/html},
}

@book{gallian_contemporary_2020,
	address = {New York},
	edition = {10},
	title = {Contemporary {Abstract} {Algebra}},
	isbn = {978-1-003-14233-1},
	abstract = {Contemporary Abstract Algebra, Tenth Edition
For more than three decades, this classic text has been widely appreciated by instructors and students alike. The book offers an enjoyable read and conveys and develops enthusiasm for the beauty of the topics presented. It is comprehensive, lively, and engaging.
The author presents the concepts and methodologies of contemporary abstract algebra as used by working mathematicians, computer scientists, physicists, and chemists. Students will learn how to do computations and to write proofs. A unique feature of the book are exercises that build the skill of generalizing, a skill that students should develop but rarely do. Applications are included to illustrate the utility of the abstract concepts.
Examples and exercises are the heart of the book. Examples elucidate the definitions, theorems, and proof techniques; exercises facilitate understanding, provide insight, and develop the ability to do proofs. The exercises often foreshadow definitions, concepts, and theorems to come.
Changes for the tenth edition include new exercises, new examples, new quotes, and a freshening of the discussion portions. The hallmark features of previous editions of the book are enhanced in this edition. These include:

A good mixture of approximately 1900 computational and theoretical exercises, including computer exercises, that synthesize concepts from multiple chapters
Approximately 300 worked-out examples from routine computations to the challenging
Many applications from scientific and computing fields and everyday life
Historical notes and biographies that spotlight people and events
Motivational and humorous quotations
Numerous connections to number theory and geometry
While many partial solutions and sketches for the odd-numbered exercises appear in the book, an Instructor’s Solutions Manual written by the author has comprehensive solutions for all exercises and some alternative solutions to develop a critical thought and deeper understanding. It is available from CRC Press only. The Student Solution Manual has comprehensive solutions for all odd-numbered exercises and many even-numbered exercises.},
	publisher = {Chapman and Hall/CRC},
	author = {Gallian, Joseph},
	month = dec,
	year = {2020},
	doi = {10.1201/9781003142331},
}

@article{bringmann_approximating_2010,
	title = {Approximating the volume of unions and intersections of high-dimensional geometric objects},
	volume = {43},
	issn = {0925-7721},
	url = {https://www.sciencedirect.com/science/article/pii/S0925772110000167},
	doi = {10.1016/j.comgeo.2010.03.004},
	abstract = {We consider the computation of the volume of the union of high-dimensional geometric objects. While showing that this problem is \#P-hard already for very simple bodies, we give a fast FPRAS for all objects where one can (1) test whether a given point lies inside the object, (2) sample a point uniformly, and (3) calculate the volume of the object in polynomial time. It suffices to be able to answer all three questions approximately. We show that this holds for a large class of objects. It implies that Klee's measure problem can be approximated efficiently even though it is \#P-hard and hence cannot be solved exactly in polynomial time in the number of dimensions unless P=NP. Our algorithm also allows to efficiently approximate the volume of the union of convex bodies given by weak membership oracles. For the analogous problem of the intersection of high-dimensional geometric objects we prove \#P-hardness for boxes and show that there is no multiplicative polynomial-time 2d1−ε-approximation for certain boxes unless NP=BPP, but give a simple additive polynomial-time ε-approximation.},
	number = {6},
	urldate = {2024-11-25},
	journal = {Computational Geometry},
	author = {Bringmann, Karl and Friedrich, Tobias},
	month = aug,
	year = {2010},
	keywords = {Approximation, Boxes, Intersection of geometric objects, Klee's measure problem, Measure, Union of geometric objects, Volume},
	pages = {601--610},
	file = {ScienceDirect Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/ZBMYTJGA/Bringmann and Friedrich - 2010 - Approximating the volume of unions and intersections of high-dimensional geometric objects.pdf:application/pdf;ScienceDirect Snapshot:/local/home/dberriaud/Software/Zotero/storage/BFA4UQVQ/S0925772110000167.html:text/html},
}

@article{cazals_computing_2011,
	title = {Computing the volume of a union of balls: {A} certified algorithm},
	volume = {38},
	issn = {0098-3500},
	shorttitle = {Computing the volume of a union of balls},
	url = {https://dl.acm.org/doi/10.1145/2049662.2049665},
	doi = {10.1145/2049662.2049665},
	abstract = {Balls and spheres are amongst the simplest 3D modeling primitives, and computing the volume of a union of balls is an elementary problem. Although a number of strategies addressing this problem have been investigated in several communities, we are not aware of any robust algorithm, and present the first such algorithm.Our calculation relies on the decomposition of the volume of the union into convex regions, namely the restrictions of the balls to their regions in the power diagram. Theoretically, we establish a formula for the volume of a restriction, based on Gauss' divergence theorem. The proof being constructive, we develop the associated algorithm. On the implementation side, we carefully analyse the predicates and constructions involved in the volume calculation, and present a certified implementation relying on interval arithmetic. The result is certified in the sense that the exact volume belongs to the interval computed.Experimental results are presented on hand-crafted models illustrating various difficulties, as well as on the 58,898 models found in the tenth of July 2009 release of the Protein Data Bank.},
	number = {1},
	urldate = {2024-11-25},
	journal = {ACM Trans. Math. Softw.},
	author = {Cazals, Frederic and Kanhere, Harshad and Loriot, Sébastien},
	month = dec,
	year = {2011},
	pages = {3:1--3:20},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/9KI252PS/Cazals et al. - 2011 - Computing the volume of a union of balls A certified algorithm.pdf:application/pdf},
}

@inproceedings{cheng_sybilproof_2005,
	address = {Philadelphia, Pennsylvania, USA},
	title = {Sybilproof reputation mechanisms},
	isbn = {978-1-59593-026-2},
	url = {http://portal.acm.org/citation.cfm?doid=1080192.1080202},
	doi = {10.1145/1080192.1080202},
	abstract = {Due to the open, anonymous nature of many P2P networks, new identities - or sybils - may be created cheaply and in large numbers. Given a reputation system, a peer may attempt to falsely raise its reputation by creating fake links between its sybils. Many existing reputation mechanisms are not resistant to these types of strategies.},
	language = {en},
	urldate = {2024-11-26},
	booktitle = {Proceeding of the 2005 {ACM} {SIGCOMM} workshop on {Economics} of peer-to-peer systems  - {P2PECON} '05},
	publisher = {ACM Press},
	author = {Cheng, Alice and Friedman, Eric},
	year = {2005},
	pages = {128},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/FE89HSP9/Cheng and Friedman - 2005 - Sybilproof reputation mechanisms.pdf:application/pdf},
}

@inproceedings{resnick_sybilproof_2009,
	address = {Stanford California USA},
	title = {Sybilproof transitive trust protocols},
	isbn = {978-1-60558-458-4},
	url = {https://dl.acm.org/doi/10.1145/1566374.1566423},
	doi = {10.1145/1566374.1566423},
	abstract = {We study protocols to enable one user (the principal) to make potentially proﬁtable but risky interactions with another user (the agent), in the absence of direct trust between the two parties. In such situations, it is possible to enable the interaction indirectly through a chain of credit or “trust” links. We introduce a model that provides insight into many disparate applications, including open currency systems, network trust aggregation systems, and manipulation-resistant recommender systems. Each party maintains a trust account for each other party. When a principal’s trust balance for an agent is high enough to cover potential losses from a bad interaction, direct trust is suﬃcient to enable the interaction. Allowing indirect trust opens up more interaction opportunities, but also expands the strategy space of an attacker seeking to exploit the community for its own ends. We show that with indirect trust exchange protocols, some friction is unavoidable: any protocol that satisﬁes a natural strategic safety property that we call sum-sybilproofness can sometimes lead to a reduction in expected overall trust balances even on interactions that are proﬁtable in expectation. Thus, for long-term growth of trust accounts, which are assets enabling risky but valuable interactions, it may be necessary to limit the use of indirect trust. We present the hedged-transitive protocol and show that it achieves the optimal rate of expected growth in trust accounts, among all protocols satisfying the sumsybilproofness condition.},
	language = {en},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 10th {ACM} conference on {Electronic} commerce},
	publisher = {ACM},
	author = {Resnick, Paul and Sami, Rahul},
	month = jul,
	year = {2009},
	pages = {345--354},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/CGF7KPC3/Resnick and Sami - 2009 - Sybilproof transitive trust protocols.pdf:application/pdf},
}

@inproceedings{stannat_achieving_2021,
	address = {Richland, SC},
	series = {{AAMAS} '21},
	title = {Achieving {Sybil}-{Proofness} in {Distributed} {Work} {Systems}},
	isbn = {978-1-4503-8307-3},
	abstract = {In a multi-agent system where agents provide quantifiable work for each other on a voluntary basis, reputation mechanisms are incorporated to induce cooperation. Hereby agents assign their peers numerical scores based on their reported transaction histories. In such systems, adversaries can launch an attack by creating fake identities called Sybils, who report counterfeit transactions among one another, with the aim of increasing their own scores in the eyes of others. This paper provides new results about the Sybil-proofness of reputation mechanisms. We revisit the impossibility result of Seuken and Parkes (2011), who show that strongly-beneficial Sybil attacks cannot be prevented on reputation mechanisms satisfying three particular requirements. We prove that, under a more rigorous set of definitions of Sybil attack benefit, this result no longer holds. We characterise properties under which reputation mechanisms are susceptible to strongly-beneficial Sybil attacks. Building on our results, we propose a minimal set of requirements for reputation mechanisms to achieve resistance to such attacks, which are stronger than the results by Cheng and Friedman (2005), who show Sybil-proofness of certain asymmetric reputation mechanisms.},
	urldate = {2024-11-26},
	booktitle = {Proceedings of the 20th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Stannat, Alexander and Umut Ileri, Can and Gijswijt, Dion and Pouwelse, Johan},
	month = may,
	year = {2021},
	pages = {1263--1271},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/PM47AIGD/Stannat et al. - 2021 - Achieving Sybil-Proofness in Distributed Work Systems.pdf:application/pdf},
}

@misc{wang_neuroback_2024,
	title = {{NeuroBack}: {Improving} {CDCL} {SAT} {Solving} using {Graph} {Neural} {Networks}},
	shorttitle = {{NeuroBack}},
	url = {http://arxiv.org/abs/2110.14053},
	doi = {10.48550/arXiv.2110.14053},
	abstract = {Propositional satisﬁability (SAT) is an NP-complete problem that impacts many research ﬁelds, such as planning, veriﬁcation, and security. Despite the remarkable success of modern SAT solvers, scalability still remains a challenge. Mainstream modern SAT solvers are based on the Conﬂict-Driven Clause Learning (CDCL) algorithm. Recent work aimed to enhance CDCL SAT solvers by improving its variable branching heuristics through predictions generated by Graph Neural Networks (GNNs). However, so far this approach either has not made solving more effective, or has required frequent online accesses to substantial GPU resources. Aiming to make GNN improvements practical, this paper proposes an approach called NeuroComb, which builds on two insights: (1) predictions of important variables and clauses can be combined with dynamic branching into a more effective hybrid branching strategy, and (2) it is sufﬁcient to query the neural model only once for the predictions before the SAT solving starts. Implemented as an enhancement to the classic MiniSat solver, NeuroComb allowed it to solve 18.5\% more problems on the recent SATCOMP-2020 competition problem set. NeuroComb is therefore a practical approach to improving SAT solving through modern machine learning.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Wang, Wenxi and Hu, Yang and Tiwari, Mohit and Khurshid, Sarfraz and McMillan, Kenneth and Miikkulainen, Risto},
	month = may,
	year = {2024},
	note = {arXiv:2110.14053 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Paper has been accepted by ICLR'24},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/LTH6YRCM/Wang et al. - 2024 - NeuroBack Improving CDCL SAT Solving using Graph Neural Networks.pdf:application/pdf},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	copyright = {2021 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	language = {en},
	number = {7873},
	urldate = {2024-11-27},
	journal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	month = aug,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational biophysics, Machine learning, Protein structure predictions, Structural biology},
	pages = {583--589},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/Q7TBBEUU/Jumper et al. - 2021 - Highly accurate protein structure prediction with AlphaFold.pdf:application/pdf},
}

@article{kochkov_neural_2024,
	title = {Neural general circulation models for weather and climate},
	volume = {632},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07744-y},
	doi = {10.1038/s41586-024-07744-y},
	abstract = {General circulation models (GCMs) are the foundation of weather and climate prediction1,2. GCMs are physics-based simulators that combine a numerical solver for large-scale dynamics with tuned representations for small-scale processes such as cloud formation. Recently, machine-learning models trained on reanalysis data have achieved comparable or better skill than GCMs for deterministic weather forecasting3,4. However, these models have not demonstrated improved ensemble forecasts, or shown sufficient stability for long-term weather and climate simulations. Here we present a GCM that combines a differentiable solver for atmospheric dynamics with machine-learning components and show that it can generate forecasts of deterministic weather, ensemble weather and climate on par with the best machine-learning and physics-based methods. NeuralGCM is competitive with machine-learning models for one- to ten-day forecasts, and with the European Centre for Medium-Range Weather Forecasts ensemble prediction for one- to fifteen-day forecasts. With prescribed sea surface temperature, NeuralGCM can accurately track climate metrics for multiple decades, and climate forecasts with 140-kilometre resolution show emergent phenomena such as realistic frequency and trajectories of tropical cyclones. For both weather and climate, our approach offers orders of magnitude computational savings over conventional GCMs, although our model does not extrapolate to substantially different future climates. Our results show that end-to-end deep learning is compatible with tasks performed by conventional GCMs and can enhance the large-scale physical simulations that are essential for understanding and predicting the Earth system.},
	language = {en},
	number = {8027},
	urldate = {2024-11-27},
	journal = {Nature},
	author = {Kochkov, Dmitrii and Yuval, Janni and Langmore, Ian and Norgaard, Peter and Smith, Jamie and Mooers, Griffin and Klöwer, Milan and Lottes, James and Rasp, Stephan and Düben, Peter and Hatfield, Sam and Battaglia, Peter and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Brenner, Michael P. and Hoyer, Stephan},
	month = aug,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Atmospheric dynamics, Climate and Earth system modelling},
	pages = {1060--1066},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/LVPWKXRZ/Kochkov et al. - 2024 - Neural general circulation models for weather and climate.pdf:application/pdf},
}

@article{eyring_pushing_2024,
	title = {Pushing the frontiers in climate modelling and analysis with machine learning},
	volume = {14},
	copyright = {2024 Springer Nature Limited},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/s41558-024-02095-y},
	doi = {10.1038/s41558-024-02095-y},
	abstract = {Climate modelling and analysis are facing new demands to enhance projections and climate information. Here we argue that now is the time to push the frontiers of machine learning beyond state-of-the-art approaches, not only by developing machine-learning-based Earth system models with greater fidelity, but also by providing new capabilities through emulators for extreme event projections with large ensembles, enhanced detection and attribution methods for extreme events, and advanced climate model analysis and benchmarking. Utilizing this potential requires key machine learning challenges to be addressed, in particular generalization, uncertainty quantification, explainable artificial intelligence and causality. This interdisciplinary effort requires bringing together machine learning and climate scientists, while also leveraging the private sector, to accelerate progress towards actionable climate science.},
	language = {en},
	number = {9},
	urldate = {2024-11-27},
	journal = {Nature Climate Change},
	author = {Eyring, Veronika and Collins, William D. and Gentine, Pierre and Barnes, Elizabeth A. and Barreiro, Marcelo and Beucler, Tom and Bocquet, Marc and Bretherton, Christopher S. and Christensen, Hannah M. and Dagon, Katherine and Gagne, David John and Hall, David and Hammerling, Dorit and Hoyer, Stephan and Iglesias-Suarez, Fernando and Lopez-Gomez, Ignacio and McGraw, Marie C. and Meehl, Gerald A. and Molina, Maria J. and Monteleoni, Claire and Mueller, Juliane and Pritchard, Michael S. and Rolnick, David and Runge, Jakob and Stier, Philip and Watt-Meyer, Oliver and Weigel, Katja and Yu, Rose and Zanna, Laure},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Climate change, Climate sciences},
	pages = {916--928},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/5TNY423N/Eyring et al. - 2024 - Pushing the frontiers in climate modelling and analysis with machine learning.pdf:application/pdf},
}

@article{banerjee_machine_2023,
	title = {Machine learning in rare disease},
	volume = {20},
	copyright = {2023 Springer Nature America, Inc.},
	issn = {1548-7105},
	url = {https://www.nature.com/articles/s41592-023-01886-z},
	doi = {10.1038/s41592-023-01886-z},
	abstract = {High-throughput profiling methods (such as genomics or imaging) have accelerated basic research and made deep molecular characterization of patient samples routine. These approaches provide a rich portrait of genes, molecular pathways and cell types involved in disease phenotypes. Machine learning (ML) can be a useful tool for extracting disease-relevant patterns from high-dimensional datasets. However, depending upon the complexity of the biological question, machine learning often requires many samples to identify recurrent and biologically meaningful patterns. Rare diseases are inherently limited in clinical cases, leading to few samples to study. In this Perspective, we outline the challenges and emerging solutions for using ML for small sample sets, specifically in rare diseases. Advances in ML methods for rare diseases are likely to be informative for applications beyond rare diseases for which few samples exist with high-dimensional data. We propose that the method community prioritize the development of ML techniques for rare disease research.},
	language = {en},
	number = {6},
	urldate = {2024-11-27},
	journal = {Nature Methods},
	author = {Banerjee, Jineta and Taroni, Jaclyn N. and Allaway, Robert J. and Prasad, Deepashree Venkatesh and Guinney, Justin and Greene, Casey},
	month = jun,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Machine learning, Data integration, Genetics research, Statistical methods},
	pages = {803--814},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/RF7RBIJF/Banerjee et al. - 2023 - Machine learning in rare disease.pdf:application/pdf},
}

@article{li_role_2024,
	title = {Role of {Artificial} {Intelligence} in {Medical} {Image} {Analysis}: {A} {Review} of {Current} {Trends} and {Future} {Directions}},
	volume = {44},
	issn = {2199-4757},
	shorttitle = {Role of {Artificial} {Intelligence} in {Medical} {Image} {Analysis}},
	url = {https://doi.org/10.1007/s40846-024-00863-x},
	doi = {10.1007/s40846-024-00863-x},
	abstract = {This review offers insight into AI’s current and future contributions to medical image analysis. The article highlights the challenges associated with manual image interpretation and introduces AI methodologies, including machine learning and deep learning. It explores AI’s applications in image segmentation, classification, registration, and reconstruction across various modalities like X-ray, computed tomography (CT), magnetic resonance imaging (MRI), and ultrasound.},
	language = {en},
	number = {2},
	urldate = {2024-11-27},
	journal = {Journal of Medical and Biological Engineering},
	author = {Li, Xin and Zhang, Lei and Yang, Jingsi and Teng, Fei},
	month = apr,
	year = {2024},
	keywords = {Deep learning, Artificial intelligence, Diagnostic imaging, Medical Imaging, Multi-modal image fusion, Personalized medicine},
	pages = {231--243},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/AVRV9XIJ/Li et al. - 2024 - Role of Artificial Intelligence in Medical Image Analysis A Review of Current Trends and Future Dir.pdf:application/pdf},
}

@article{motie_financial_2024,
	title = {Financial fraud detection using graph neural networks: {A} systematic review},
	volume = {240},
	issn = {0957-4174},
	shorttitle = {Financial fraud detection using graph neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417423026581},
	doi = {10.1016/j.eswa.2023.122156},
	abstract = {Financial fraud is a persistent problem in the finance industry that may have severe consequences for individuals, businesses, and economies. Graph Neural Networks (GNNs) are a class of deep learning models designed to operate on graph data structures that consist of nodes and edges connecting them. GNNs have emerged as a powerful tool for detecting fraudulent activities in complex financial systems because they can analyze the network structure of financial transactions, capturing patterns and anomalies that traditional rule-based and machine learning methods might miss. The objective of this systematic review is to provide a comprehensive overview of the current state-of-the-art technologies in using Graph Neural Networks (GNNs) for financial fraud detection, identify the gaps and limitations in the existing research, and suggest potential directions for future research. We searched five academic databases, including Web of Science, Scopus, IEEE Xplore, ACM, and science direct using specific keywords and search strings related to graph neural networks, financial areas, and anomaly detection to identify relevant publications, resulting in a total of 388 unique articles. We selected the relevant publications based on the inclusion, exclusion, and quality assessment criteria, and 33 articles were included in the review. In addition, forward snowballing was used to identify relevant papers that were not captured in the initial search. Data was extracted from the selected articles, then analyzed and summarized to identify current state, gaps, and trends in the literature. Our review presents a new taxonomy of GNNs applied in financial fraud detection and identifies potential research directions in this field. We find that GNNs applied to financial fraud detection have mostly been employed in a supervised or semi-supervised manner, with limited exploration of unsupervised approaches. In addition to financial areas, we explore the different types of graphs such as homogeneous, heterogenous, static, temporal, and dynamic graphs, and investigate the various learning mechanisms and anomaly types studied. We also note a lack of research on edge-level and graph-level anomaly detection commonly employed in financial domain.},
	urldate = {2024-11-27},
	journal = {Expert Systems with Applications},
	author = {Motie, Soroor and Raahemi, Bijan},
	month = apr,
	year = {2024},
	keywords = {Anomaly detection, Financial fraud detection, GNNs, Graph neural networks, Graph representation learning},
	pages = {122156},
	file = {ScienceDirect Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/VLBYA72Q/Motie and Raahemi - 2024 - Financial fraud detection using graph neural networks A systematic review.pdf:application/pdf;ScienceDirect Snapshot:/local/home/dberriaud/Software/Zotero/storage/9883UX2M/S0957417423026581.html:text/html},
}

@misc{magueresse_low-resource_2020,
	title = {Low-resource {Languages}: {A} {Review} of {Past} {Work} and {Future} {Challenges}},
	shorttitle = {Low-resource {Languages}},
	url = {http://arxiv.org/abs/2006.07264},
	doi = {10.48550/arXiv.2006.07264},
	abstract = {A current problem in NLP is massaging and processing low-resource languages which lack useful training attributes such as supervised data, number of native speakers or experts, etc. This review paper concisely summarizes previous groundbreaking achievements made towards resolving this problem, and analyzes potential improvements in the context of the overall future research direction.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Magueresse, Alexandre and Carles, Vincent and Heetderks, Evan},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07264},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/VXPRN3UB/Magueresse et al. - 2020 - Low-resource Languages A Review of Past Work and Future Challenges.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/FY58DAJ2/2006.html:text/html},
}

@inproceedings{hedderich_survey_2021,
	address = {Online},
	title = {A {Survey} on {Recent} {Approaches} for {Natural} {Language} {Processing} in {Low}-{Resource} {Scenarios}},
	url = {https://aclanthology.org/2021.naacl-main.201},
	doi = {10.18653/v1/2021.naacl-main.201},
	abstract = {Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and ﬁne-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a speciﬁc low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.},
	language = {en},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Hedderich, Michael A. and Lange, Lukas and Adel, Heike and Strötgen, Jannik and Klakow, Dietrich},
	year = {2021},
	pages = {2545--2568},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/8Q944AM8/Hedderich et al. - 2021 - A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios.pdf:application/pdf},
}

@misc{zhang_sybil-proof_2020,
	title = {Sybil-proof {Answer} {Querying} {Mechanism}},
	url = {http://arxiv.org/abs/2005.13224},
	doi = {10.48550/arXiv.2005.13224},
	abstract = {We study a question answering problem on a social network, where a requester is seeking an answer from the agents on the network. The goal is to design reward mechanisms to incentivize the agents to propagate the requester's query to their neighbours if they don't have the answer. Existing mechanisms are vulnerable to Sybil-attacks, i.e., an agent may get more reward by creating fake identities. Hence, we combat this problem by first proving some impossibility results to resolve Sybil-attacks and then characterizing a class of mechanisms which satisfy Sybil-proofness (prevents Sybil-attacks) as well as other desirable properties. Except for Sybil-proofness, we also consider cost minimization for the requester and agents' collusions.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Zhang, Yao and Zhang, Xiuzhen and Zhao, Dengji},
	month = may,
	year = {2020},
	note = {arXiv:2005.13224},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/YQVVWIUS/Zhang et al. - 2020 - Sybil-proof Answer Querying Mechanism.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/FNABTGS5/2005.html:text/html},
}

@article{chen_sybil-proof_2023,
	title = {Sybil-{Proof} {Diffusion} {Auction} in {Social} {Networks}},
	abstract = {A diffusion auction is a market to sell commodities over a social network, where the challenge is to incentivize existing buyers to invite their neighbors in the network to join the market. Existing mechanisms have been designed to solve the challenge in various settings, aiming at desirable properties such as non-deficiency, incentive compatibility and social welfare maximization. Since the mechanisms are employed in dynamic networks with ever-changing structures, buyers could easily generate fake nodes in the network to manipulate the mechanisms for their own benefits, which is commonly known as the Sybil attack. We observe that strategic agents may gain an unfair advantage in existing mechanisms through such attacks. To resist this potential attack, we propose two diffusion auction mechanisms, the Sybil tax mechanism (STM) and the Sybil cluster mechanism (SCM), to achieve both Sybil-proofness and incentive compatibility in the single-item setting. Our proposal provides the first mechanisms to protect the interests of buyers against Sybil attacks with a mild sacrifice of social welfare and revenue.},
	language = {en},
	author = {Chen, Hongyin},
	year = {2023},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/2BJ6DYKX/Chen - 2023 - Sybil-Proof Diffusion Auction in Social Networks.pdf:application/pdf},
}

@misc{babaioff_bitcoin_2016,
	title = {On {Bitcoin} and {Red} {Balloons}},
	url = {http://arxiv.org/abs/1111.2626},
	doi = {10.48550/arXiv.1111.2626},
	abstract = {Many large decentralized systems rely on information propagation to ensure their proper function. We examine a common scenario in which only participants that are aware of the information can compete for some reward, and thus informed participants have an incentive not to propagate information to others. One recent example in which such tension arises is the 2009 DARPA Network Challenge (finding red balloons). We focus on another prominent example: Bitcoin, a decentralized electronic currency system. Bitcoin represents a radical new approach to monetary systems. It has been getting a large amount of public attention over the last year, both in policy discussions and in the popular press. Its cryptographic fundamentals have largely held up even as its usage has become increasingly widespread. We find, however, that it exhibits a fundamental problem of a different nature, based on how its incentives are structured. We propose a modification to the protocol that can eliminate this problem. Bitcoin relies on a peer-to-peer network to track transactions that are performed with the currency. For this purpose, every transaction a node learns about should be transmitted to its neighbors in the network. The current implemented protocol provides an incentive to nodes to not broadcast transactions they are aware of. Our solution is to augment the protocol with a scheme that rewards information propagation. Since clones are easy to create in the Bitcoin system, an important feature of our scheme is Sybil-proofness. We show that our proposed scheme succeeds in setting the correct incentives, that it is Sybil-proof, and that it requires only a small payment overhead, all this is achieved with iterated elimination of dominated strategies. We complement this result by showing that there are no reward schemes in which information propagation and no self-cloning is a dominant strategy.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Babaioff, Moshe and Dobzinski, Shahar and Oren, Sigal and Zohar, Aviv},
	month = jun,
	year = {2016},
	note = {arXiv:1111.2626},
	keywords = {Computer Science - Computer Science and Game Theory},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/LXDLU4WS/Babaioff et al. - 2016 - On Bitcoin and Red Balloons.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/SX53TU79/1111.html:text/html},
}

@inproceedings{seuken_sybil-proofness_2011,
	title = {On the {Sybil}-{Proofness} of {Accounting} {Mechanisms}},
	url = {http://econcs.seas.harvard.edu/files/econcs/files/seuken_netecon11.pdf},
	abstract = {A common challenge in distributed work systems like P2P ﬁle-sharing communities, or ad-hoc routing networks, is to minimize the number of free-riders and incentivize contributions. Without any centralized monitoring it is diﬃcult to distinguish contributors from free-riders. One way to address this problem is via accounting mechanisms which rely on voluntary reports by individual agents and compute a score for each agent in the network. In Seuken et al. [11], we have recently proposed a mechanism which removes any incentive for a user to manipulate the mechanism via misreports. However, we left the existence of sybil-proof accounting mechanisms as an open question. In this paper, we settle this question, and show the striking impossibility result that under reasonable assumptions no sybil-proof accounting mechanism exists. We show, that a signiﬁcantly weaker form of K-sybil-proofness can be achieved against certain classes of sybil attacks. Finally, we explain how limited robustness to sybil manipulations can be achieved by using max-ﬂow algorithms in accounting mechanism design.},
	language = {en},
	booktitle = {Proceedings of the 6th {Workshop} on the {Economics} of {Networks}, {Systems}, and {Computation}},
	author = {Seuken, Sven and Parkes, David C},
	year = {2011},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/2VES3RW6/Seuken and Parkes - On the Sybil-Proofness of Accounting Mechanisms.pdf:application/pdf},
}

@inproceedings{seuken_sybil-proof_2014,
	address = {Richland, SC},
	series = {{AAMAS} '14},
	title = {Sybil-proof accounting mechanisms with transitive trust},
	isbn = {978-1-4503-2738-1},
	abstract = {For the design of distributed work systems like P2P file-sharing networks it is essential to provide incentives for agents to work for each other rather than free ride. Several mechanisms have been proposed to achieve this goal, including currency systems, credit networks, and accounting mechanisms. It has proven particularly challenging to provide robustness to sybil attacks, i.e., attacks where an agent creates and controls multiple false identities. In this paper, we consider accounting mechanisms for domains in which (1) transactions cannot be bound to reports, (2) transactions are bilateral and private, and (3) agents can only form trust links upon successful work interactions. Our results reveal the trade-off one must make in designing such mechanisms. We show that accounting mechanisms with a strong form of transitive trust cannot be robust against strongly beneficial sybil attacks. However, we also present a mechanism that strikes a balance, providing a weaker form of transitive trust while also being robust against the strongest form of sybil attacks. On the one hand, our results highlight the role of strong social ties in providing robustness against sybil attacks (such as those leveraged in credit networks using bilateral IOUs), and on the other hand our results show what kind of robustness properties are possible and impossible in domains where such pre-existing trust relations do not exist.},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 2014 international conference on {Autonomous} agents and multi-agent systems},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Seuken, Sven and Parkes, David C.},
	month = may,
	year = {2014},
	pages = {205--212},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/CDW974JR/Seuken and Parkes - 2014 - Sybil-proof accounting mechanisms with transitive trust.pdf:application/pdf},
}

@misc{ghojogh_spectral_2022,
	title = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}: {Tutorial} and {Survey}},
	shorttitle = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}},
	url = {http://arxiv.org/abs/2201.09267},
	doi = {10.48550/arXiv.2201.09267},
	abstract = {This is a tutorial and survey paper on metric learning. Algorithms are divided into spectral, probabilistic, and deep metric learning. We first start with the definition of distance metric, Mahalanobis distance, and generalized Mahalanobis distance. In spectral methods, we start with methods using scatters of data, including the first spectral metric learning, relevant methods to Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric learning, locally linear metric adaptation, and adversarial metric learning are covered. We also explain several kernel spectral methods for metric learning in the feature space. We also introduce geometric metric learning methods on the Riemannian manifolds. In probabilistic methods, we start with collapsing classes in both input and feature spaces and then explain the neighborhood component analysis methods, Bayesian metric learning, information theoretic methods, and empirical risk minimization in metric learning. In deep learning methods, we first introduce reconstruction autoencoders and supervised loss functions for metric learning. Then, Siamese networks and its various loss functions, triplet mining, and triplet sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning, geometric metric learning by neural networks, and few-shot metric learning.},
	language = {en},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	month = jan,
	year = {2022},
	note = {arXiv:2201.09267 [stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {metric learning: learns a distance metric or an embedding space for separation of dissimilar points and closeness of similar points.
Metric learning assumes that the metric space is incorrect and tries to find the metric that best fits the data
On the other hand, we assume that the distance metric is given/ correct, and deduce from this which weight to give to each point in order to tackle data imbalance. This can be used to build a metric/ a loss, but in a different space (judgement aggregation)
},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/5F6UJM5T/Ghojogh et al. - 2022 - Spectral, Probabilistic, and Deep Metric Learning Tutorial and Survey.pdf:application/pdf},
}

@misc{cao_learning_2019,
	title = {Learning {Imbalanced} {Datasets} with {Label}-{Distribution}-{Aware} {Margin} {Loss}},
	url = {http://arxiv.org/abs/1906.07413},
	doi = {10.48550/arXiv.1906.07413},
	abstract = {Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
	month = oct,
	year = {2019},
	note = {arXiv:1906.07413},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Classical reweigthing technique = 1 / class frequency
better method 1 / effective number of samples
Not so effective without regularization
deferred reweighting more effective than reweighting

In this paper: additional regularization to add to reweighting techinque
},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/UV6X5IZ6/Cao et al. - 2019 - Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/YY5DQ5W9/1906.html:text/html},
}

@misc{jamal_rethinking_2020,
	title = {Rethinking {Class}-{Balanced} {Methods} for {Long}-{Tailed} {Visual} {Recognition} from a {Domain} {Adaptation} {Perspective}},
	url = {http://arxiv.org/abs/2003.10780},
	doi = {10.48550/arXiv.2003.10780},
	abstract = {Object frequency in the real world often follows a power law, leading to a mismatch between datasets with long-tailed class distributions seen by a machine learning model and our expectation of the model to perform well on all classes. We analyze this mismatch from a domain adaptation point of view. First of all, we connect existing class-balanced methods for long-tailed classification to target shift, a well-studied scenario in domain adaptation. The connection reveals that these methods implicitly assume that the training data and test data share the same class-conditioned distribution, which does not hold in general and especially for the tail classes. While a head class could contain abundant and diverse training examples that well represent the expected data at inference time, the tail classes are often short of representative training data. To this end, we propose to augment the classic class-balanced learning by explicitly estimating the differences between the class-conditioned distributions with a meta-learning approach. We validate our approach with six benchmark datasets and three loss functions.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Jamal, Muhammad Abdullah and Brown, Matthew and Yang, Ming-Hsuan and Wang, Liqiang and Gong, Boqing},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10780},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/URZPHDTG/Jamal et al. - 2020 - Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspe.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/9IUVQXM8/2003.html:text/html},
}

@misc{shu_meta-weight-net_2019,
	title = {Meta-{Weight}-{Net}: {Learning} an {Explicit} {Mapping} {For} {Sample} {Weighting}},
	shorttitle = {Meta-{Weight}-{Net}},
	url = {http://arxiv.org/abs/1902.07379},
	doi = {10.48550/arXiv.1902.07379},
	abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
	month = sep,
	year = {2019},
	note = {arXiv:1902.07379},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/EHGDZZNH/Shu et al. - 2019 - Meta-Weight-Net Learning an Explicit Mapping For Sample Weighting.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/NLHU6Y2B/1902.html:text/html},
}

@misc{cui_class-balanced_2019,
	title = {Class-{Balanced} {Loss} {Based} on {Effective} {Number} of {Samples}},
	url = {http://arxiv.org/abs/1901.05555},
	doi = {10.48550/arXiv.1901.05555},
	abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula \$(1-{\textbackslash}beta{\textasciicircum}\{n\})/(1-{\textbackslash}beta)\$, where \$n\$ is the number of samples and \${\textbackslash}beta {\textbackslash}in [0,1)\$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
	month = jan,
	year = {2019},
	note = {arXiv:1901.05555},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {associating with each sample a small neighboring region instead of a point
},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/K3TNYIHP/Cui et al. - 2019 - Class-Balanced Loss Based on Effective Number of Samples.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/ITNUGGVW/1901.html:text/html},
}

@inproceedings{gebru_fine-grained_2017,
	title = {Fine-{Grained} {Recognition} in the {Wild}: {A} {Multi}-task {Domain} {Adaptation} {Approach}},
	shorttitle = {Fine-{Grained} {Recognition} in the {Wild}},
	url = {https://ieeexplore.ieee.org/document/8237413/?arnumber=8237413},
	doi = {10.1109/ICCV.2017.151},
	abstract = {While fine-grained object recognition is an important problem in computer vision, current models are unlikely to accurately classify objects in the wild. These fully supervised models need additional annotated images to classify objects in every new scenario, a task that is infeasible. However, sources such as e-commerce websites and field guides provide annotated images for many classes. In this work, we study fine-grained domain adaptation as a step towards overcoming the dataset shift between easily acquired annotated images and the real world. Adaptation has not been studied in the fine-grained setting where annotations such as attributes could be used to increase performance. Our work uses an attribute based multi-task adaptation loss to increase accuracy from a baseline of 4.1\% to 19.1\% in the semi-supervised adaptation case. Prior domain adaptation works have been benchmarked on small datasets such as [46] with a total of 795 images for some domains, or simplistic datasets such as [41] consisting of digits. We perform experiments on a subset of a new challenging fine-grained dataset consisting of 1, 095, 021 images of 2, 657 car categories drawn from e-commerce websites and Google Street View.},
	urldate = {2024-12-04},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Gebru, Timnit and Hoffman, Judy and Fei-Fei, Li},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Training, Automobiles, Computational modeling, Adaptation models, Computer vision, Image recognition},
	pages = {1358--1367},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/IFXGSL72/Gebru et al. - 2017 - Fine-Grained Recognition in the Wild A Multi-task Domain Adaptation Approach.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/RS8WZ6MR/8237413.html:text/html},
}

@article{wang_deep_2018,
	title = {Deep {Visual} {Domain} {Adaptation}: {A} {Survey}},
	volume = {312},
	issn = {09252312},
	shorttitle = {Deep {Visual} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1802.03601},
	doi = {10.1016/j.neucom.2018.05.083},
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that deﬁne how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare brieﬂy the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classiﬁcation, such as face recognition, semantic segmentation and object detection. Fourth, some potential deﬁciencies of current methods and several future directions are highlighted.},
	language = {en},
	urldate = {2024-12-04},
	journal = {Neurocomputing},
	author = {Wang, Mei and Deng, Weihong},
	month = oct,
	year = {2018},
	note = {arXiv:1802.03601 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {135--153},
	annote = {Comment: Manuscript accepted by Neurocomputing 2018},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/B39YQNK8/Wang and Deng - 2018 - Deep Visual Domain Adaptation A Survey.pdf:application/pdf},
}

@misc{ren_learning_2019,
	title = {Learning to {Reweight} {Examples} for {Robust} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1803.09050},
	doi = {10.48550/arXiv.1803.09050},
	abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
	month = may,
	year = {2019},
	note = {arXiv:1803.09050},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/XAM3KK5K/Ren et al. - 2019 - Learning to Reweight Examples for Robust Deep Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/RKXSDZGW/1803.html:text/html},
}

@misc{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	doi = {10.48550/arXiv.1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	language = {en},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {arXiv:1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/L3A3WGGM/Lin et al. - 2018 - Focal Loss for Dense Object Detection.pdf:application/pdf},
}

@article{ran_comprehensive_2023,
	title = {Comprehensive survey on hierarchical clustering algorithms and the recent developments},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10366-3},
	doi = {10.1007/s10462-022-10366-3},
	abstract = {Data clustering is a commonly used data processing technique in many fields, which divides objects into different clusters in terms of some similarity measure between data points. Comparing to partitioning clustering methods which give a flat partition of the data, hierarchical clustering methods can give multiple consistent partitions of the data at different levels for the same data without rerunning clustering, it can be used to better analyze the complex structure of the data. There are usually two kinds of hierarchical clustering methods: divisive and agglomerative. For the divisive clustering, the key issue is how to select a cluster for the next splitting procedure according to dissimilarity and how to divide the selected cluster. For agglomerative hierarchical clustering, the key issue is the similarity measure that is used to select the two most similar clusters for the next merge. Although both types of the methods produce the dendrogram of the data as output, the clustering results may be very different depending on the dissimilarity or similarity measure used in the clustering, and different types of methods should be selected according to different types of the data and different application scenarios. So, we have reviewed various hierarchical clustering methods comprehensively, especially the most recently developed methods, in this work. The similarity measure plays a crucial role during hierarchical clustering process, we have reviewed different types of the similarity measure along with the hierarchical clustering. More specifically, different types of hierarchical clustering methods are comprehensively reviewed from six aspects, and their advantages and drawbacks are analyzed. The application of some methods in real life is also discussed. Furthermore, we have also included some recent works in combining deep learning techniques and hierarchical clustering, which is worth serious attention and may improve the hierarchical clustering significantly in the future.},
	language = {en},
	number = {8},
	urldate = {2024-12-04},
	journal = {Artificial Intelligence Review},
	author = {Ran, Xingcheng and Xi, Yue and Lu, Yonggang and Wang, Xiangwen and Lu, Zhenyu},
	month = aug,
	year = {2023},
	keywords = {Agglomerative, Artificial Intelligence, Dissimilarity, Divisive, Hierarchical clustering, Similarity},
	pages = {8219--8264},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/P3SAGRG2/Ran et al. - 2023 - Comprehensive survey on hierarchical clustering algorithms and the recent developments.pdf:application/pdf},
}

@article{murtagh_algorithms_2017,
	title = {Algorithms for hierarchical clustering: an overview, {II}},
	volume = {7},
	copyright = {© 2017 Wiley Periodicals, Inc.},
	issn = {1942-4795},
	shorttitle = {Algorithms for hierarchical clustering},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1219},
	doi = {10.1002/widm.1219},
	abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86–97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219 This article is categorized under: Algorithmic Development {\textgreater} Hierarchies and Trees Technologies {\textgreater} Classification Technologies {\textgreater} Structure Discovery and Clustering},
	language = {en},
	number = {6},
	urldate = {2024-12-04},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Murtagh, Fionn and Contreras, Pedro},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1219},
	pages = {e1219},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/EKPF5UD6/Murtagh and Contreras - 2017 - Algorithms for hierarchical clustering an overview, II.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/SECH9PFC/widm.html:text/html},
}

@incollection{berthold_overlapping_2020,
	address = {Cham},
	title = {Overlapping {Hierarchical} {Clustering} ({OHC})},
	volume = {12080},
	isbn = {978-3-030-44583-6 978-3-030-44584-3},
	url = {http://link.springer.com/10.1007/978-3-030-44584-3_21},
	abstract = {Agglomerative clustering methods have been widely used by many research communities to cluster their data into hierarchical structures. These structures ease data exploration and are understandable even for non-specialists. But these methods necessarily result in a tree, since, at each agglomeration step, two clusters have to be merged. This may bias the data analysis process if, for example, a cluster is almost equally attracted by two others. In this paper we propose a new method that allows clusters to overlap until a strong cluster attraction is reached, based on a density criterion. The resulting hierarchical structure, called a quasi-dendrogram, is represented as a directed acyclic graph and combines the advantages of hierarchies with the precision of a less arbitrary clustering. We validate our work with extensive experiments on real data sets and compare it with existing tree-based methods, using a new measure of similarity between heterogeneous hierarchical structures.},
	language = {en},
	urldate = {2024-12-04},
	booktitle = {Advances in {Intelligent} {Data} {Analysis} {XVIII}},
	publisher = {Springer International Publishing},
	author = {Jeantet, Ian and Miklós, Zoltán and Gross-Amblard, David},
	editor = {Berthold, Michael R. and Feelders, Ad and Krempl, Georg},
	year = {2020},
	doi = {10.1007/978-3-030-44584-3_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {261--273},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/9IJYUHAX/Jeantet et al. - 2020 - Overlapping Hierarchical Clustering (OHC).pdf:application/pdf},
}

@inproceedings{varshney_improved_2020,
	title = {Improved {Probabilistic} {Intuitionistic} {Fuzzy} c-{Means} {Clustering} {Algorithm}: {Improved} {PIFCM}},
	shorttitle = {Improved {Probabilistic} {Intuitionistic} {Fuzzy} c-{Means} {Clustering} {Algorithm}},
	url = {https://ieeexplore.ieee.org/document/9177574},
	doi = {10.1109/FUZZ48607.2020.9177574},
	abstract = {Recently proposed Probabilistic Intuitionistic Fuzzy c-Means Algorithm (PIFCM) is a Probabilistic Euclidian distance measure (PEDM) based clustering technique, which incorporate computation of probabilistic intervals (Pij, Qij) for each of the data point. PIFCM algorithm employs a random membership function {\textbackslash}frac1łeft{\textbar} x {\textbackslash}right{\textbar} and discards a data point if its membership value is uniformly distributed in the clusters. Fuzzy clustering always gets affected by the choice of the membership function. Accordingly, in PIFCM algorithm, membership function changes the properties of the data limiting its capabilities in giving consistent clustering results. Moreover, PIFCM algorithm incorporates computation of redundant matrices while finding Pij and Qij. In this paper, we propose some novel changes in the existing PIFCM algorithm, and hence introduce our Improved PIFCM algorithm. The improved PIFCM algorithm considers the min-max normalization as membership function, and also removes the redundant matrix computation that was used to find the Pij and Qij in the original PIFCM. Results over various UCI datasets validates the superiority of our improved PIFCM algorithm over FCM algorithm, IFCM algorithm and PIFCM algorithm.},
	urldate = {2024-12-04},
	booktitle = {2020 {IEEE} {International} {Conference} on {Fuzzy} {Systems} ({FUZZ}-{IEEE})},
	author = {Varshney, Ayush K. and Danish Lohani, Q. M. and Muhuri, Pranab K.},
	month = jul,
	year = {2020},
	note = {ISSN: 1558-4739},
	keywords = {AIFS based clustering, Clustering algorithms, Conferences, Distributed databases, Fuzzy clustering, Fuzzy systems, IFCM, Limiting, PEDM, PIFCM, probabilistic interval, Probabilistic logic},
	pages = {1--6},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/HIBIIVBB/Varshney et al. - 2020 - Improved Probabilistic Intuitionistic Fuzzy c-Means Clustering Algorithm Improved PIFCM.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/6YP9R3RZ/9177574.html:text/html},
}

@article{varshney_pifhc_2022,
	title = {{PIFHC}: {The} {Probabilistic} {Intuitionistic} {Fuzzy} {Hierarchical} {Clustering} {Algorithm}},
	volume = {120},
	issn = {1568-4946},
	shorttitle = {{PIFHC}},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494622000977},
	doi = {10.1016/j.asoc.2022.108584},
	abstract = {Hierarchical clustering techniques help in building a tree-like structure called dendrogram from the data points which can be used to find the closest related data objects. This paper presents a novel hierarchical clustering technique which considers intuitionistic fuzzy sets to deal with the uncertainty present in the data. Instead of using traditional hamming distance or Euclidean distance measure to find the distance between the data points, it employs the probabilistic Euclidean distance measure to propose a novel clustering approach which we term as ‘Probabilistic Intuitionistic Fuzzy Hierarchical Clustering (PIFHC) Algorithm’. The proposed PIFHC algorithm considers probabilistic weights from the data to measure the distances between the data points. Clustering results over UCI datasets show that our proposed PIFHC algorithm gives better cluster accuracies than its existing counterparts. PIFHC efficiently provides improvements of 1\%–3.5\% in the clustering accuracy compared to other fuzzy hierarchical clustering algorithms for most of the datasets. We further provide experimental results with the real-world car dataset and the Listeria monocytogenes dataset for mouse susceptibility to demonstrate the practical efficacy of the proposed algorithm. For Listeria datasets as well, proposed PIFHC records 1.7\% improvement against the state-of-the-art methods The dendrograms formed by the proposed PIFHC algorithm exhibits high cophenetic correlation coefficient with an improvement of 0.75\% over others. We provide various AGNES methods to update the distance between merged clusters in the proposed PIFHC algorithm.},
	urldate = {2024-12-04},
	journal = {Applied Soft Computing},
	author = {Varshney, Ayush K. and Muhuri, Pranab K. and Danish Lohani, Q. M.},
	month = may,
	year = {2022},
	keywords = {Fuzzy clustering, Hierarchical clustering, Intuitionistic fuzzy sets, Probabilistic Euclidean distance measure, Probabilistic intuitionistic fuzzy hierarchical clustering algorithm, Probabilistic weights},
	pages = {108584},
	file = {ScienceDirect Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/CM8DK3Y7/Varshney et al. - 2022 - PIFHC The Probabilistic Intuitionistic Fuzzy Hierarchical Clustering Algorithm.pdf:application/pdf},
}

@article{conitzer_using_2010,
	title = {Using {Mechanism} {Design} to {Prevent} {False}‐{Name} {Manipulations}},
	volume = {31},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0738-4602, 2371-9621},
	url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v31i4.2315},
	doi = {10.1609/aimag.v31i4.2315},
	abstract = {When mechanisms such as auctions, rating systems, and elections are run in a highly anonymous environment such as the Internet, a key concern is that a single agent can participate multiple times by using false identiﬁers. Such false-name manipulations have traditionally not been considered in the theory of mechanism design. In this article, we review recent efforts to extend the theory to address this. We ﬁrst review results for the basic concept of false-name-proofness. Because some of these results are very negative, we also discuss alternative models that allow us to circumvent some of these negative results.},
	language = {en},
	number = {4},
	urldate = {2024-12-06},
	journal = {AI Magazine},
	author = {Conitzer, Vincent and Yokoo, Makoto},
	month = dec,
	year = {2010},
	pages = {65--78},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/I6C2ZDZ9/Conitzer and Yokoo - 2010 - Using Mechanism Design to Prevent False‐Name Manipulations.pdf:application/pdf},
}

@article{nehama_manipulation-resistant_2022,
	title = {Manipulation-resistant false-name-proof facility location mechanisms for complex graphs},
	volume = {36},
	issn = {1573-7454},
	url = {https://doi.org/10.1007/s10458-021-09535-5},
	doi = {10.1007/s10458-021-09535-5},
	abstract = {In many real-life scenarios, a group of agents needs to agree on a common action, e.g., on a location for a public facility, while there is some consistency between their preferences, e.g., all preferences are derived from a common metric space. The facility location problem models such scenarios and it is a well-studied problem in social choice. We study mechanisms for facility location on unweighted undirected graphs that are resistant to manipulations (strategy-proof, abstention-proof, and false-name-proof) by both individuals and coalitions on one hand and anonymous and efficient (Pareto-optimal) on the other. We define a new family of graphs, \$\$ZV\$\$-line graphs, and show a general facility location mechanism for these graphs that satisfies all these desired properties. This mechanism can also be computed in polynomial time and it can equivalently be defined as the first Pareto-optimal location according to some predefined order. Our main result, the \$\$ZV\$\$-line graphs family and the mechanism we present for it, unifies all works in the literature of false-name-proof facility location on discrete graphs including the preliminary (unpublished) works we are aware of. In particular, we show mechanisms for all graphs of at most five vertices, discrete trees, bicliques, and clique tree graphs. Finally, we discuss some generalizations and limitations of our result for facility location problems on other structures: Weighted graphs, large discrete cycles, infinite graphs; and for facility location problems concerning infinite societies.},
	language = {en},
	number = {1},
	urldate = {2024-12-06},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Nehama, Ilan and Todo, Taiki and Yokoo, Makoto},
	month = jan,
	year = {2022},
	keywords = {Strategy-proofness, Artificial Intelligence, Facility location, False-name-proofness, ZV
-line graphs},
	pages = {12},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/H5TE9THW/Nehama et al. - 2022 - Manipulation-resistant false-name-proof facility location mechanisms for complex graphs.pdf:application/pdf},
}

@article{todo_characterizing_2009,
	title = {Characterizing {False}-name-proof {Allocation} {Rules} in {Combinatorial} {Auctions}},
	abstract = {A combinatorial auction mechanism consists of an allocation rule that deﬁnes the allocation of goods for each agent, and a payment rule that deﬁnes the payment of each winner. There have been several studies on characterizing strategyproof allocation rules. In particular, a condition called weakmonotonicity has been identiﬁed as a full characterization of strategy-proof allocation rules. More speciﬁcally, for an allocation rule, there exists an appropriate payment rule so that the mechanism becomes strategy-proof if and only if it satisﬁes weak-monotonicity.},
	language = {en},
	author = {Todo, Taiki and Iwasaki, Atsushi and Yokoo, Makoto and Sakurai, Yuko},
	year = {2009},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/AFBY45TP/Todo et al. - 2009 - Characterizing False-name-proof Allocation Rules in Combinatorial Auctions.pdf:application/pdf},
}

@misc{alvarez-melis_geometric_2020,
	title = {Geometric {Dataset} {Distances} via {Optimal} {Transport}},
	url = {http://arxiv.org/abs/2002.02923},
	doi = {10.48550/arXiv.2002.02923},
	abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e.g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Alvarez-Melis, David and Fusi, Nicolò},
	month = feb,
	year = {2020},
	note = {arXiv:2002.02923 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/RPD4UIA3/Alvarez-Melis and Fusi - 2020 - Geometric Dataset Distances via Optimal Transport.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/WYXL4SYP/2002.html:text/html},
}

@misc{liu_wasserstein_2022,
	title = {Wasserstein {Task} {Embedding} for {Measuring} {Task} {Similarities}},
	url = {http://arxiv.org/abs/2208.11726},
	doi = {10.48550/arXiv.2208.11726},
	abstract = {Measuring similarities between different tasks is critical in a broad spectrum of machine learning problems, including transfer, multi-task, continual, and meta-learning. Most current approaches to measuring task similarities are architecture-dependent: 1) relying on pre-trained models, or 2) training networks on tasks and using forward transfer as a proxy for task similarity. In this paper, we leverage the optimal transport theory and define a novel task embedding for supervised classification that is model-agnostic, training-free, and capable of handling (partially) disjoint label sets. In short, given a dataset with ground-truth labels, we perform a label embedding through multi-dimensional scaling and concatenate dataset samples with their corresponding label embeddings. Then, we define the distance between two datasets as the 2-Wasserstein distance between their updated samples. Lastly, we leverage the 2-Wasserstein embedding framework to embed tasks into a vector space in which the Euclidean distance between the embedded points approximates the proposed 2-Wasserstein distance between tasks. We show that the proposed embedding leads to a significantly faster comparison of tasks compared to related approaches like the Optimal Transport Dataset Distance (OTDD). Furthermore, we demonstrate the effectiveness of our proposed embedding through various numerical experiments and show statistically significant correlations between our proposed distance and the forward and backward transfer between tasks.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Liu, Xinran and Bai, Yikun and Lu, Yuzhe and Soltoggio, Andrea and Kolouri, Soheil},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11726 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/BAAT6GL7/Liu et al. - 2022 - Wasserstein Task Embedding for Measuring Task Similarities.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/PYBXR5FI/2208.html:text/html},
}

@article{gretton_kernel_2012,
	title = {A kernel two-sample test},
	volume = {13},
	issn = {1532-4435},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	month = mar,
	year = {2012},
	pages = {723--773},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/DJS34MC4/Gretton et al. - 2012 - A kernel two-sample test.pdf:application/pdf},
}

@misc{achille_task2vec_2019,
	title = {{Task2Vec}: {Task} {Embedding} for {Meta}-{Learning}},
	shorttitle = {{Task2Vec}},
	url = {http://arxiv.org/abs/1902.03545},
	doi = {10.48550/arXiv.1902.03545},
	abstract = {We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a "probe network" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Soatto, Stefano and Perona, Pietro},
	month = feb,
	year = {2019},
	note = {arXiv:1902.03545 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/YBVZLUDX/Achille et al. - 2019 - Task2Vec Task Embedding for Meta-Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/HR2649HP/1902.html:text/html},
}

@misc{peng_domain2vec_2020,
	title = {{Domain2Vec}: {Domain} {Embedding} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {{Domain2Vec}},
	url = {http://arxiv.org/abs/2007.09257},
	doi = {10.48550/arXiv.2007.09257},
	abstract = {Conventional unsupervised domain adaptation (UDA) studies the knowledge transfer between a limited number of domains. This neglects the more practical scenario where data are distributed in numerous different domains in the real world. The domain similarity between those domains is critical for domain adaptation performance. To describe and learn relations between different domains, we propose a novel Domain2Vec model to provide vectorial representations of visual domains based on joint learning of feature disentanglement and Gram matrix. To evaluate the effectiveness of our Domain2Vec model, we create two large-scale cross-domain benchmarks. The first one is TinyDA, which contains 54 domains and about one million MNIST-style images. The second benchmark is DomainBank, which is collected from 56 existing vision datasets. We demonstrate that our embedding is capable of predicting domain similarities that match our intuition about visual relations between different domains. Extensive experiments are conducted to demonstrate the power of our new datasets in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Peng, Xingchao and Li, Yichen and Saenko, Kate},
	month = jul,
	year = {2020},
	note = {arXiv:2007.09257 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2020 paper},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/CY8KLMST/Peng et al. - 2020 - Domain2Vec Domain Embedding for Unsupervised Domain Adaptation.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/D622HJ2U/2007.html:text/html},
}

@article{gretton_covariate_2009,
	title = {Covariate {Shift} by {Kernel} {Mean} {Matching}},
	language = {en},
	author = {Gretton, Arthur and Smola, Alex and Huang, Jiayuan and Schmittfull, Marcel and Borgwardt, Karsten and Scholkopf, Bernhard},
	year = {2009},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/CMJKIX26/Gretton et al. - 8 Covariate Shift by Kernel Mean Matching.pdf:application/pdf},
}

@inproceedings{y_covariate_2019,
	title = {Covariate {Shift}: {A} {Review} and {Analysis} on {Classifiers}},
	shorttitle = {Covariate {Shift}},
	url = {https://ieeexplore.ieee.org/document/8978471},
	doi = {10.1109/GCAT47503.2019.8978471},
	abstract = {Training and testing are the two phases of a supervised machine learning model. When these models are trained, validated and tested, it is usually assumed that the test and train data points follow the same distribution. However, practical scenarios are much different; in real world, the joint distribution of inputs to the model and outputs of the model differs between training and test data, which is called dataset shift. A simpler case of dataset shift, where only the input distribution changes and the conditional distribution of the output for a given input remains unchanged is known as covariate shift. This article primarily provides an overview of the existing methods of covariate shift detection and adaption and their applications in the real world. It also gives an experimental analysis of the effect of various covariate shift adaptation techniques on the performance of classification algorithms for four datasets which include, synthetic and real-world data. Performance of machine learning models show significant improvement after covariate shift was handled using Importance Reweighting method and feature-dropping method. The review, experimental analysis, and observations of this work may serve as guidelines for researchers and developers of machine learning systems, to handle covariate shift problems efficiently.},
	urldate = {2024-12-11},
	booktitle = {2019 {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Y, Geeta Dharani. and Nair, Nimisha G and Satpathy, Pallavi and Christopher, Jabez},
	month = oct,
	year = {2019},
	keywords = {Classification algorithms, Covariate shift, Data Mining, Dataset shift, Machine Learning},
	pages = {1--6},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/PMZGVGZZ/Y et al. - 2019 - Covariate Shift A Review and Analysis on Classifiers.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/7294QI5A/8978471.html:text/html},
}

@misc{carlini_poisoning_2024,
	title = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},
	url = {http://arxiv.org/abs/2302.10149},
	doi = {10.48550/arXiv.2302.10149},
	abstract = {Deep learning models are often trained on distributed, web-scale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the LAION-400M or COYO-700M datasets for just \$60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
	month = may,
	year = {2024},
	note = {arXiv:2302.10149 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/26MT7RP2/Carlini et al. - 2024 - Poisoning Web-Scale Training Datasets is Practical.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/6CFEZ749/2302.html:text/html},
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2024-12-12},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {ScienceDirect Snapshot:/local/home/dberriaud/Software/Zotero/storage/AM5LTFIY/S002200009791504X.html:text/html},
}

@inproceedings{malisiewicz_ensemble_2011,
	address = {Barcelona, Spain},
	title = {Ensemble of exemplar-{SVMs} for object detection and beyond},
	isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
	url = {http://ieeexplore.ieee.org/document/6126229/},
	doi = {10.1109/ICCV.2011.6126229},
	abstract = {This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classiﬁer for every exemplar in the training set. Each of these ExemplarSVMs is thus deﬁned by a single positive instance and millions of negatives. While each detector is quite speciﬁc to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central beneﬁt of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding.},
	language = {en},
	urldate = {2024-12-12},
	booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei A.},
	month = nov,
	year = {2011},
	pages = {89--96},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/5WVQC9MX/Malisiewicz et al. - 2011 - Ensemble of exemplar-SVMs for object detection and beyond.pdf:application/pdf},
}

@misc{chang_active_2018,
	title = {Active {Bias}: {Training} {More} {Accurate} {Neural} {Networks} by {Emphasizing} {High} {Variance} {Samples}},
	shorttitle = {Active {Bias}},
	url = {http://arxiv.org/abs/1704.07433},
	doi = {10.48550/arXiv.1704.07433},
	abstract = {Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
	month = jan,
	year = {2018},
	note = {arXiv:1704.07433 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: camera-ready version for NIPS 2017},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/8N2PZSTQ/Chang et al. - 2018 - Active Bias Training More Accurate Neural Networks by Emphasizing High Variance Samples.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/624LZVXP/1704.html:text/html},
}

@inproceedings{jiang_self-paced_2015,
	address = {Austin, Texas},
	series = {{AAAI}'15},
	title = {Self-paced curriculum learning},
	isbn = {978-0-262-51129-2},
	abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to "instructor-student-collaborative" learning mode, as opposed to "instructor-driven" in CL or "student-driven" in SPL. Empirically, we show that the advantage of SPCL on two tasks.},
	urldate = {2024-12-12},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G.},
	month = jan,
	year = {2015},
	pages = {2694--2700},
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	url = {https://ieeexplore.ieee.org/document/5995347},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	urldate = {2024-12-12},
	booktitle = {{CVPR} 2011},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Training, Internet, Communities, Testing, Object recognition, Support vector machines, Visualization},
	pages = {1521--1528},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/XXTUNWPC/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/VJQ3MC4S/5995347.html:text/html},
}

@misc{dong_class_2017,
	title = {Class {Rectification} {Hard} {Mining} for {Imbalanced} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1712.03162},
	doi = {10.48550/arXiv.1712.03162},
	abstract = {Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
	month = dec,
	year = {2017},
	note = {arXiv:1712.03162 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/DKBMAQQU/Dong et al. - 2017 - Class Rectification Hard Mining for Imbalanced Deep Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/XNIZQIQM/1712.html:text/html},
}

@book{hardle_nonparametric_2004,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Statistics}},
	title = {Nonparametric and {Semiparametric} {Models}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-62076-8 978-3-642-17146-8},
	url = {http://link.springer.com/10.1007/978-3-642-17146-8},
	urldate = {2024-12-12},
	publisher = {Springer},
	author = {Härdle, Wolfgang and Werwatz, Axel and Müller, Marlene and Sperlich, Stefan},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {2004},
	doi = {10.1007/978-3-642-17146-8},
	keywords = {Additive Models, Density Estimation, econometrics, Generalized Partial Linear Models, modeling, Nonparametric Models, Regression, Semiparametric Model, Semiparametric Models, Single Index Models, Smoothing, statistics},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/IAC9687S/Härdle et al. - 2004 - Nonparametric and Semiparametric Models.pdf:application/pdf},
}

@inproceedings{peyrard_better_2021,
	title = {Better than {Average}: {Paired} {Evaluation} of {NLP} {Systems}},
	shorttitle = {Better than {Average}},
	url = {http://arxiv.org/abs/2110.10746},
	doi = {10.18653/v1/2021.acl-long.179},
	abstract = {Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instance-level pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30\% of the setups. To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing.},
	urldate = {2024-12-13},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	author = {Peyrard, Maxime and Zhao, Wei and Eger, Steffen and West, Robert},
	year = {2021},
	note = {arXiv:2110.10746 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {2301--2315},
	annote = {Comment: Published in ACL 2021 (long paper)},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/T5KZ8X8J/Peyrard et al. - 2021 - Better than Average Paired Evaluation of NLP Systems.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/4J58FE3R/2110.html:text/html},
}

@misc{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	doi = {10.48550/arXiv.1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv:1804.07461 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ICLR 2019; https://gluebenchmark.com/},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/PVP8UEJL/Wang et al. - 2019 - GLUE A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/7ZZSPB83/1804.html:text/html},
}

@misc{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	doi = {10.48550/arXiv.1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2019, super.gluebenchmark.com updating acknowledegments},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/FFR64RGZ/Wang et al. - 2020 - SuperGLUE A Stickier Benchmark for General-Purpose Language Understanding Systems.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/5N3ADG8G/1905.html:text/html},
}

@misc{koh_wilds_2021,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	url = {http://arxiv.org/abs/2012.07421},
	doi = {10.48550/arXiv.2012.07421},
	abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	month = jul,
	year = {2021},
	note = {arXiv:2012.07421 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/72PJ3HNW/Koh et al. - 2021 - WILDS A Benchmark of in-the-Wild Distribution Shifts.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/V54PP3XT/2012.html:text/html},
}

@misc{gehrmann_gem_2021,
	title = {The {GEM} {Benchmark}: {Natural} {Language} {Generation}, its {Evaluation} and {Metrics}},
	shorttitle = {The {GEM} {Benchmark}},
	url = {http://arxiv.org/abs/2102.01672},
	doi = {10.48550/arXiv.2102.01672},
	abstract = {We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for which we are organizing a shared task at our ACL 2021 Workshop and to which we invite the entire NLG community to participate.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D. and Du, Wanyu and Durmus, Esin and Dušek, Ondřej and Emezue, Chris and Gangal, Varun and Garbacea, Cristina and Hashimoto, Tatsunori and Hou, Yufang and Jernite, Yacine and Jhamtani, Harsh and Ji, Yangfeng and Jolly, Shailza and Kale, Mihir and Kumar, Dhruv and Ladhak, Faisal and Madaan, Aman and Maddela, Mounica and Mahajan, Khyati and Mahamood, Saad and Majumder, Bodhisattwa Prasad and Martins, Pedro Henrique and McMillan-Major, Angelina and Mille, Simon and Miltenburg, Emiel van and Nadeem, Moin and Narayan, Shashi and Nikolaev, Vitaly and Niyongabo, Rubungo Andre and Osei, Salomey and Parikh, Ankur and Perez-Beltrachini, Laura and Rao, Niranjan Ramesh and Raunak, Vikas and Rodriguez, Juan Diego and Santhanam, Sashank and Sedoc, João and Sellam, Thibault and Shaikh, Samira and Shimorina, Anastasia and Cabezudo, Marco Antonio Sobrevilla and Strobelt, Hendrik and Subramani, Nishant and Xu, Wei and Yang, Diyi and Yerukola, Akhila and Zhou, Jiawei},
	month = apr,
	year = {2021},
	note = {arXiv:2102.01672 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/3KGFCHVA/Gehrmann et al. - 2021 - The GEM Benchmark Natural Language Generation, its Evaluation and Metrics.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/DUPSWET3/2102.html:text/html},
}

@misc{hu_xtreme_2020,
	title = {{XTREME}: {A} {Massively} {Multilingual} {Multi}-task {Benchmark} for {Evaluating} {Cross}-lingual {Generalization}},
	shorttitle = {{XTREME}},
	url = {http://arxiv.org/abs/2003.11080},
	doi = {10.48550/arXiv.2003.11080},
	abstract = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
	month = sep,
	year = {2020},
	note = {arXiv:2003.11080 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: In Proceedings of the 37th International Conference on Machine Learning (ICML). July 2020},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/4R8JCXNN/Hu et al. - 2020 - XTREME A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/DUSI32EH/2003.html:text/html},
}
