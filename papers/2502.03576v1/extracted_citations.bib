@misc{achille_task2vec_2019,
	title = {{Task2Vec}: {Task} {Embedding} for {Meta}-{Learning}},
	shorttitle = {{Task2Vec}},
	url = {http://arxiv.org/abs/1902.03545},
	doi = {10.48550/arXiv.1902.03545},
	abstract = {We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a "probe network" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Achille, Alessandro and Lam, Michael and Tewari, Rahul and Ravichandran, Avinash and Maji, Subhransu and Fowlkes, Charless and Soatto, Stefano and Perona, Pietro},
	month = feb,
	year = {2019},
	note = {arXiv:1902.03545 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/YBVZLUDX/Achille et al. - 2019 - Task2Vec Task Embedding for Meta-Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/HR2649HP/1902.html:text/html},
}

@misc{alvarez-melis_geometric_2020,
	title = {Geometric {Dataset} {Distances} via {Optimal} {Transport}},
	url = {http://arxiv.org/abs/2002.02923},
	doi = {10.48550/arXiv.2002.02923},
	abstract = {The notion of task similarity is at the core of various machine learning paradigms, such as domain adaptation and meta-learning. Current methods to quantify it are often heuristic, make strong assumptions on the label sets across the tasks, and many are architecture-dependent, relying on task-specific optimal parameters (e.g., require training a model on each dataset). In this work we propose an alternative notion of distance between datasets that (i) is model-agnostic, (ii) does not involve training, (iii) can compare datasets even if their label sets are completely disjoint and (iv) has solid theoretical footing. This distance relies on optimal transport, which provides it with rich geometry awareness, interpretable correspondences and well-understood properties. Our results show that this novel distance provides meaningful comparison of datasets, and correlates well with transfer learning hardness across various experimental settings and datasets.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Alvarez-Melis, David and Fusi, Nicolò},
	month = feb,
	year = {2020},
	note = {arXiv:2002.02923 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/RPD4UIA3/Alvarez-Melis and Fusi - 2020 - Geometric Dataset Distances via Optimal Transport.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/WYXL4SYP/2002.html:text/html},
}

@misc{balduzzi_re-evaluating_2018,
	title = {Re-evaluating {Evaluation}},
	url = {http://arxiv.org/abs/1806.02643},
	doi = {10.48550/arXiv.1806.02643},
	abstract = {Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation -- since there is no harm (computational cost aside) from including all available tasks and agents.},
	urldate = {2024-08-06},
	publisher = {arXiv},
	author = {Balduzzi, David and Tuyls, Karl and Perolat, Julien and Graepel, Thore},
	month = oct,
	year = {2018},
	note = {arXiv:1806.02643}, 
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Science and Game Theory},
	annote = {Comment: NIPS 2018, final version},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/WNQTUCBE/Balduzzi et al. - 2018 - Re-evaluating Evaluation.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/5RJWPNEQ/1806.html:text/html},
}

@incollection{berthold_overlapping_2020,
	address = {Cham},
	title = {Overlapping {Hierarchical} {Clustering} ({OHC})},
	volume = {12080},
	isbn = {978-3-030-44583-6 978-3-030-44584-3},
	url = {http://link.springer.com/10.1007/978-3-030-44584-3_21},
	abstract = {Agglomerative clustering methods have been widely used by many research communities to cluster their data into hierarchical structures. These structures ease data exploration and are understandable even for non-specialists. But these methods necessarily result in a tree, since, at each agglomeration step, two clusters have to be merged. This may bias the data analysis process if, for example, a cluster is almost equally attracted by two others. In this paper we propose a new method that allows clusters to overlap until a strong cluster attraction is reached, based on a density criterion. The resulting hierarchical structure, called a quasi-dendrogram, is represented as a directed acyclic graph and combines the advantages of hierarchies with the precision of a less arbitrary clustering. We validate our work with extensive experiments on real data sets and compare it with existing tree-based methods, using a new measure of similarity between heterogeneous hierarchical structures.},
	language = {en},
	urldate = {2024-12-04},
	booktitle = {Advances in {Intelligent} {Data} {Analysis} {XVIII}},
	publisher = {Springer International Publishing},
	author = {Jeantet, Ian and Miklós, Zoltán and Gross-Amblard, David},
	editor = {Berthold, Michael R. and Feelders, Ad and Krempl, Georg},
	year = {2020},
	doi = {10.1007/978-3-030-44584-3_21},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {261--273},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/9IJYUHAX/Jeantet et al. - 2020 - Overlapping Hierarchical Clustering (OHC).pdf:application/pdf},
}

@misc{cao_learning_2019,
	title = {Learning {Imbalanced} {Datasets} with {Label}-{Distribution}-{Aware} {Margin} {Loss}},
	url = {http://arxiv.org/abs/1906.07413},
	doi = {10.48550/arXiv.1906.07413},
	abstract = {Deep learning algorithms can fare poorly when the training dataset suffers from heavy class-imbalance but the testing criterion requires good generalization on less frequent classes. We design two novel methods to improve performance in such scenarios. First, we propose a theoretically-principled label-distribution-aware margin (LDAM) loss motivated by minimizing a margin-based generalization bound. This loss replaces the standard cross-entropy objective during training and can be applied with prior strategies for training with class-imbalance such as re-weighting or re-sampling. Second, we propose a simple, yet effective, training schedule that defers re-weighting until after the initial stage, allowing the model to learn an initial representation while avoiding some of the complications associated with re-weighting or re-sampling. We test our methods on several benchmark vision tasks including the real-world imbalanced dataset iNaturalist 2018. Our experiments show that either of these methods alone can already improve over existing techniques and their combination achieves even better performance gains.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
	month = oct,
	year = {2019},
	note = {arXiv:1906.07413},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Classical reweigthing technique = 1 / class frequency
better method 1 / effective number of samples
Not so effective without regularization
deferred reweighting more effective than reweighting

In this paper: additional regularization to add to reweighting techinque
},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/UV6X5IZ6/Cao et al. - 2019 - Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/YY5DQ5W9/1906.html:text/html},
}

@misc{carlini_poisoning_2024,
	title = {Poisoning {Web}-{Scale} {Training} {Datasets} is {Practical}},
	url = {http://arxiv.org/abs/2302.10149},
	doi = {10.48550/arXiv.2302.10149},
	abstract = {Deep learning models are often trained on distributed, web-scale datasets crawled from the internet. In this paper, we introduce two new dataset poisoning attacks that intentionally introduce malicious examples to a model's performance. Our attacks are immediately practical and could, today, poison 10 popular datasets. Our first attack, split-view poisoning, exploits the mutable nature of internet content to ensure a dataset annotator's initial view of the dataset differs from the view downloaded by subsequent clients. By exploiting specific invalid trust assumptions, we show how we could have poisoned 0.01\% of the LAION-400M or COYO-700M datasets for just \$60 USD. Our second attack, frontrunning poisoning, targets web-scale datasets that periodically snapshot crowd-sourced content -- such as Wikipedia -- where an attacker only needs a time-limited window to inject malicious examples. In light of both attacks, we notify the maintainers of each affected dataset and recommended several low-overhead defenses.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
	month = may,
	year = {2024},
	note = {arXiv:2302.10149 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/26MT7RP2/Carlini et al. - 2024 - Poisoning Web-Scale Training Datasets is Practical.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/6CFEZ749/2302.html:text/html},
}

@misc{chang_active_2018,
	title = {Active {Bias}: {Training} {More} {Accurate} {Neural} {Networks} by {Emphasizing} {High} {Variance} {Samples}},
	shorttitle = {Active {Bias}},
	url = {http://arxiv.org/abs/1704.07433},
	doi = {10.48550/arXiv.1704.07433},
	abstract = {Self-paced learning and hard example mining re-weight training instances to improve learning accuracy. This paper presents two improved alternatives based on lightweight estimates of sample uncertainty in stochastic gradient descent (SGD): the variance in predicted probability of the correct class across iterations of mini-batch SGD, and the proximity of the correct class probability to the decision threshold. Extensive experimental results on six datasets show that our methods reliably improve accuracy in various network architectures, including additional gains on top of other popular training techniques, such as residual learning, momentum, ADAM, batch normalization, dropout, and distillation.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Chang, Haw-Shiuan and Learned-Miller, Erik and McCallum, Andrew},
	month = jan,
	year = {2018},
	note = {arXiv:1704.07433 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: camera-ready version for NIPS 2017},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/8N2PZSTQ/Chang et al. - 2018 - Active Bias Training More Accurate Neural Networks by Emphasizing High Variance Samples.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/624LZVXP/1704.html:text/html},
}

@misc{colombo_what_2022,
	title = {What are the best systems? {New} perspectives on {NLP} {Benchmarking}},
	shorttitle = {What are the best systems?},
	url = {http://arxiv.org/abs/2202.03799},
	doi = {10.48550/arXiv.2202.03799},
	abstract = {In Machine Learning, a benchmark refers to an ensemble of datasets associated with one or multiple metrics together with a way to aggregate different systems performances. They are instrumental in (i) assessing the progress of new methods along different axes and (ii) selecting the best systems for practical use. This is particularly the case for NLP with the development of large pre-trained models (e.g. GPT, BERT) that are expected to generalize well on a variety of tasks. While the community mainly focused on developing new datasets and metrics, there has been little interest in the aggregation procedure, which is often reduced to a simple average over various performance measures. However, this procedure can be problematic when the metrics are on a different scale, which may lead to spurious conclusions. This paper proposes a new procedure to rank systems based on their performance across different tasks. Motivated by the social choice theory, the final system ordering is obtained through aggregating the rankings induced by each task and is theoretically grounded. We conduct extensive numerical experiments (on over 270k scores) to assess the soundness of our approach both on synthetic and real scores (e.g. GLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method yields different conclusions on state-of-the-art systems than the mean-aggregation procedure while being both more reliable and robust.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Colombo, Pierre and Noiry, Nathan and Irurozki, Ekhine and Clemencon, Stephan},
	month = oct,
	year = {2022},
	note = {arXiv:2202.03799 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/3BC9E9KR/Colombo et al. - 2022 - What are the best systems New perspectives on NLP Benchmarking.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/BY2EHVQJ/2202.html:text/html},
}

@article{conitzer_using_2010,
	title = {Using {Mechanism} {Design} to {Prevent} {False}‐{Name} {Manipulations}},
	volume = {31},
	copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
	issn = {0738-4602, 2371-9621},
	url = {https://onlinelibrary.wiley.com/doi/10.1609/aimag.v31i4.2315},
	doi = {10.1609/aimag.v31i4.2315},
	abstract = {When mechanisms such as auctions, rating systems, and elections are run in a highly anonymous environment such as the Internet, a key concern is that a single agent can participate multiple times by using false identiﬁers. Such false-name manipulations have traditionally not been considered in the theory of mechanism design. In this article, we review recent efforts to extend the theory to address this. We ﬁrst review results for the basic concept of false-name-proofness. Because some of these results are very negative, we also discuss alternative models that allow us to circumvent some of these negative results.},
	language = {en},
	number = {4},
	urldate = {2024-12-06},
	journal = {AI Magazine},
	author = {Conitzer, Vincent and Yokoo, Makoto},
	month = dec,
	year = {2010},
	pages = {65--78},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/I6C2ZDZ9/Conitzer and Yokoo - 2010 - Using Mechanism Design to Prevent False‐Name Manipulations.pdf:application/pdf},
}

@misc{cui_class-balanced_2019,
	title = {Class-{Balanced} {Loss} {Based} on {Effective} {Number} of {Samples}},
	url = {http://arxiv.org/abs/1901.05555},
	doi = {10.48550/arXiv.1901.05555},
	abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula \$(1-{\textbackslash}beta{\textasciicircum}\{n\})/(1-{\textbackslash}beta)\$, where \$n\$ is the number of samples and \${\textbackslash}beta {\textbackslash}in [0,1)\$ is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
	month = jan,
	year = {2019},
	note = {arXiv:1901.05555},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {associating with each sample a small neighboring region instead of a point
},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/K3TNYIHP/Cui et al. - 2019 - Class-Balanced Loss Based on Effective Number of Samples.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/ITNUGGVW/1901.html:text/html},
}

@misc{dong_class_2017,
	title = {Class {Rectification} {Hard} {Mining} for {Imbalanced} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1712.03162},
	doi = {10.48550/arXiv.1712.03162},
	abstract = {Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale and extremely imbalanced among different attribute classes. To address this problem, we formulate a novel scheme for batch incremental hard sample mining of minority attribute classes from imbalanced large scale training data. We develop an end-to-end deep learning framework capable of avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. This is made possible by introducing a Class Rectification Loss (CRL) regularising algorithm. We demonstrate the advantages and scalability of CRL over existing state-of-the-art attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the CelebA facial attribute dataset and the X-Domain clothing attribute dataset.},
	urldate = {2024-12-12},
	publisher = {arXiv},
	author = {Dong, Qi and Gong, Shaogang and Zhu, Xiatian},
	month = dec,
	year = {2017},
	note = {arXiv:1712.03162 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/DKBMAQQU/Dong et al. - 2017 - Class Rectification Hard Mining for Imbalanced Deep Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/XNIZQIQM/1712.html:text/html},
}

@article{freund_decision-theoretic_1997,
	title = {A {Decision}-{Theoretic} {Generalization} of {On}-{Line} {Learning} and an {Application} to {Boosting}},
	volume = {55},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X},
	doi = {10.1006/jcss.1997.1504},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show how the resulting learning algorithm can be applied to a variety of problems, including gambling, multiple-outcome prediction, repeated games, and prediction of points in Rn. In the second part of the paper we apply the multiplicative weight-update technique to derive a new boosting algorithm. This boosting algorithm does not require any prior knowledge about the performance of the weak learning algorithm. We also study generalizations of the new boosting algorithm to the problem of learning functions whose range, rather than being binary, is an arbitrary finite set or a bounded segment of the real line.},
	number = {1},
	urldate = {2024-12-12},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	month = aug,
	year = {1997},
	pages = {119--139},
	file = {ScienceDirect Snapshot:/local/home/dberriaud/Software/Zotero/storage/AM5LTFIY/S002200009791504X.html:text/html},
}

@inproceedings{gebru_fine-grained_2017,
	title = {Fine-{Grained} {Recognition} in the {Wild}: {A} {Multi}-task {Domain} {Adaptation} {Approach}},
	shorttitle = {Fine-{Grained} {Recognition} in the {Wild}},
	url = {https://ieeexplore.ieee.org/document/8237413/?arnumber=8237413},
	doi = {10.1109/ICCV.2017.151},
	abstract = {While fine-grained object recognition is an important problem in computer vision, current models are unlikely to accurately classify objects in the wild. These fully supervised models need additional annotated images to classify objects in every new scenario, a task that is infeasible. However, sources such as e-commerce websites and field guides provide annotated images for many classes. In this work, we study fine-grained domain adaptation as a step towards overcoming the dataset shift between easily acquired annotated images and the real world. Adaptation has not been studied in the fine-grained setting where annotations such as attributes could be used to increase performance. Our work uses an attribute based multi-task adaptation loss to increase accuracy from a baseline of 4.1\% to 19.1\% in the semi-supervised adaptation case. Prior domain adaptation works have been benchmarked on small datasets such as [46] with a total of 795 images for some domains, or simplistic datasets such as [41] consisting of digits. We perform experiments on a subset of a new challenging fine-grained dataset consisting of 1, 095, 021 images of 2, 657 car categories drawn from e-commerce websites and Google Street View.},
	urldate = {2024-12-04},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Gebru, Timnit and Hoffman, Judy and Fei-Fei, Li},
	month = oct,
	year = {2017},
	note = {ISSN: 2380-7504},
	keywords = {Training, Automobiles, Computational modeling, Adaptation models, Computer vision, Image recognition},
	pages = {1358--1367},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/IFXGSL72/Gebru et al. - 2017 - Fine-Grained Recognition in the Wild A Multi-task Domain Adaptation Approach.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/RS8WZ6MR/8237413.html:text/html},
}

@misc{ghojogh_spectral_2022,
	title = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}: {Tutorial} and {Survey}},
	shorttitle = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}},
	url = {http://arxiv.org/abs/2201.09267},
	doi = {10.48550/arXiv.2201.09267},
	abstract = {This is a tutorial and survey paper on metric learning. Algorithms are divided into spectral, probabilistic, and deep metric learning. We first start with the definition of distance metric, Mahalanobis distance, and generalized Mahalanobis distance. In spectral methods, we start with methods using scatters of data, including the first spectral metric learning, relevant methods to Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric learning, locally linear metric adaptation, and adversarial metric learning are covered. We also explain several kernel spectral methods for metric learning in the feature space. We also introduce geometric metric learning methods on the Riemannian manifolds. In probabilistic methods, we start with collapsing classes in both input and feature spaces and then explain the neighborhood component analysis methods, Bayesian metric learning, information theoretic methods, and empirical risk minimization in metric learning. In deep learning methods, we first introduce reconstruction autoencoders and supervised loss functions for metric learning. Then, Siamese networks and its various loss functions, triplet mining, and triplet sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning, geometric metric learning by neural networks, and few-shot metric learning.},
	language = {en},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	month = jan,
	year = {2022},
	note = {arXiv:2201.09267 [stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {metric learning: learns a distance metric or an embedding space for separation of dissimilar points and closeness of similar points.
Metric learning assumes that the metric space is incorrect and tries to find the metric that best fits the data
On the other hand, we assume that the distance metric is given/ correct, and deduce from this which weight to give to each point in order to tackle data imbalance. This can be used to build a metric/ a loss, but in a different space (judgement aggregation)
},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/5F6UJM5T/Ghojogh et al. - 2022 - Spectral, Probabilistic, and Deep Metric Learning Tutorial and Survey.pdf:application/pdf},
}

@article{gretton_covariate_2009,
	title = {Covariate {Shift} by {Kernel} {Mean} {Matching}},
	language = {en},
	author = {Gretton, Arthur and Smola, Alex and Huang, Jiayuan and Schmittfull, Marcel and Borgwardt, Karsten and Scholkopf, Bernhard},
	year = {2009},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/CMJKIX26/Gretton et al. - 8 Covariate Shift by Kernel Mean Matching.pdf:application/pdf},
}

@article{gretton_kernel_2012,
	title = {A kernel two-sample test},
	volume = {13},
	issn = {1532-4435},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {null},
	journal = {J. Mach. Learn. Res.},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	month = mar,
	year = {2012},
	pages = {723--773},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/DJS34MC4/Gretton et al. - 2012 - A kernel two-sample test.pdf:application/pdf},
}

@book{hardle_nonparametric_2004,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Statistics}},
	title = {Nonparametric and {Semiparametric} {Models}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-62076-8 978-3-642-17146-8},
	url = {http://link.springer.com/10.1007/978-3-642-17146-8},
	urldate = {2024-12-12},
	publisher = {Springer},
	author = {Härdle, Wolfgang and Werwatz, Axel and Müller, Marlene and Sperlich, Stefan},
	editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
	year = {2004},
	doi = {10.1007/978-3-642-17146-8},
	keywords = {Additive Models, Density Estimation, econometrics, Generalized Partial Linear Models, modeling, Nonparametric Models, Regression, Semiparametric Model, Semiparametric Models, Single Index Models, Smoothing, statistics},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/IAC9687S/Härdle et al. - 2004 - Nonparametric and Semiparametric Models.pdf:application/pdf},
}

@misc{himmi_towards_2023,
	title = {Towards {More} {Robust} {NLP} {System} {Evaluation}: {Handling} {Missing} {Scores} in {Benchmarks}},
	shorttitle = {Towards {More} {Robust} {NLP} {System} {Evaluation}},
	url = {https://arxiv.org/abs/2305.10284v1},
	abstract = {The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.},
	language = {en},
	urldate = {2024-09-12},
	journal = {arXiv.org},
	author = {Himmi, Anas and Irurozki, Ekhine and Noiry, Nathan and Clemencon, Stephan and Colombo, Pierre},
	month = may,
	year = {2023},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/C57HQVR3/Himmi et al. - 2023 - Towards More Robust NLP System Evaluation Handling Missing Scores in Benchmarks.pdf:application/pdf},
}

@misc{jamal_rethinking_2020,
	title = {Rethinking {Class}-{Balanced} {Methods} for {Long}-{Tailed} {Visual} {Recognition} from a {Domain} {Adaptation} {Perspective}},
	url = {http://arxiv.org/abs/2003.10780},
	doi = {10.48550/arXiv.2003.10780},
	abstract = {Object frequency in the real world often follows a power law, leading to a mismatch between datasets with long-tailed class distributions seen by a machine learning model and our expectation of the model to perform well on all classes. We analyze this mismatch from a domain adaptation point of view. First of all, we connect existing class-balanced methods for long-tailed classification to target shift, a well-studied scenario in domain adaptation. The connection reveals that these methods implicitly assume that the training data and test data share the same class-conditioned distribution, which does not hold in general and especially for the tail classes. While a head class could contain abundant and diverse training examples that well represent the expected data at inference time, the tail classes are often short of representative training data. To this end, we propose to augment the classic class-balanced learning by explicitly estimating the differences between the class-conditioned distributions with a meta-learning approach. We validate our approach with six benchmark datasets and three loss functions.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Jamal, Muhammad Abdullah and Brown, Matthew and Yang, Ming-Hsuan and Wang, Liqiang and Gong, Boqing},
	month = mar,
	year = {2020},
	note = {arXiv:2003.10780},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/URZPHDTG/Jamal et al. - 2020 - Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition from a Domain Adaptation Perspe.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/9IUVQXM8/2003.html:text/html},
}

@inproceedings{jiang_self-paced_2015,
	address = {Austin, Texas},
	series = {{AAAI}'15},
	title = {Self-paced curriculum learning},
	isbn = {978-0-262-51129-2},
	abstract = {Curriculum learning (CL) or self-paced learning (SPL) represents a recently proposed learning regime inspired by the learning process of humans and animals that gradually proceeds from easy to more complex samples in training. The two methods share a similar conceptual learning paradigm, but differ in specific learning schemes. In CL, the curriculum is predetermined by prior knowledge, and remain fixed thereafter. Therefore, this type of method heavily relies on the quality of prior knowledge while ignoring feedback about the learner. In SPL, the curriculum is dynamically determined to adjust to the learning pace of the leaner. However, SPL is unable to deal with prior knowledge, rendering it prone to overfitting. In this paper, we discover the missing link between CL and SPL, and propose a unified framework named self-paced curriculum leaning (SPCL). SPCL is formulated as a concise optimization problem that takes into account both prior knowledge known before training and the learning progress during training. In comparison to human education, SPCL is analogous to "instructor-student-collaborative" learning mode, as opposed to "instructor-driven" in CL or "student-driven" in SPL. Empirically, we show that the advantage of SPCL on two tasks.},
	urldate = {2024-12-12},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Jiang, Lu and Meng, Deyu and Zhao, Qian and Shan, Shiguang and Hauptmann, Alexander G.},
	month = jan,
	year = {2015},
	pages = {2694--2700},
}

@misc{koh_wilds_2021,
	title = {{WILDS}: {A} {Benchmark} of in-the-{Wild} {Distribution} {Shifts}},
	shorttitle = {{WILDS}},
	url = {http://arxiv.org/abs/2012.07421},
	doi = {10.48550/arXiv.2012.07421},
	abstract = {Distribution shifts -- where the training distribution differs from the test distribution -- can substantially degrade the accuracy of machine learning (ML) systems deployed in the wild. Despite their ubiquity in the real-world deployments, these distribution shifts are under-represented in the datasets widely used in the ML community today. To address this gap, we present WILDS, a curated benchmark of 10 datasets reflecting a diverse range of distribution shifts that naturally arise in real-world applications, such as shifts across hospitals for tumor identification; across camera traps for wildlife monitoring; and across time and location in satellite imaging and poverty mapping. On each dataset, we show that standard training yields substantially lower out-of-distribution than in-distribution performance. This gap remains even with models trained by existing methods for tackling distribution shifts, underscoring the need for new methods for training models that are more robust to the types of distribution shifts that arise in practice. To facilitate method development, we provide an open-source package that automates dataset loading, contains default model architectures and hyperparameters, and standardizes evaluations. Code and leaderboards are available at https://wilds.stanford.edu.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton A. and Haque, Imran S. and Beery, Sara and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	month = jul,
	year = {2021},
	note = {arXiv:2012.07421 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/72PJ3HNW/Koh et al. - 2021 - WILDS A Benchmark of in-the-Wild Distribution Shifts.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/V54PP3XT/2012.html:text/html},
}

@misc{liu_wasserstein_2022,
	title = {Wasserstein {Task} {Embedding} for {Measuring} {Task} {Similarities}},
	url = {http://arxiv.org/abs/2208.11726},
	doi = {10.48550/arXiv.2208.11726},
	abstract = {Measuring similarities between different tasks is critical in a broad spectrum of machine learning problems, including transfer, multi-task, continual, and meta-learning. Most current approaches to measuring task similarities are architecture-dependent: 1) relying on pre-trained models, or 2) training networks on tasks and using forward transfer as a proxy for task similarity. In this paper, we leverage the optimal transport theory and define a novel task embedding for supervised classification that is model-agnostic, training-free, and capable of handling (partially) disjoint label sets. In short, given a dataset with ground-truth labels, we perform a label embedding through multi-dimensional scaling and concatenate dataset samples with their corresponding label embeddings. Then, we define the distance between two datasets as the 2-Wasserstein distance between their updated samples. Lastly, we leverage the 2-Wasserstein embedding framework to embed tasks into a vector space in which the Euclidean distance between the embedded points approximates the proposed 2-Wasserstein distance between tasks. We show that the proposed embedding leads to a significantly faster comparison of tasks compared to related approaches like the Optimal Transport Dataset Distance (OTDD). Furthermore, we demonstrate the effectiveness of our proposed embedding through various numerical experiments and show statistically significant correlations between our proposed distance and the forward and backward transfer between tasks.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Liu, Xinran and Bai, Yikun and Lu, Yuzhe and Soltoggio, Andrea and Kolouri, Soheil},
	month = aug,
	year = {2022},
	note = {arXiv:2208.11726 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/BAAT6GL7/Liu et al. - 2022 - Wasserstein Task Embedding for Measuring Task Similarities.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/PYBXR5FI/2208.html:text/html},
}

@article{murtagh_algorithms_2017,
	title = {Algorithms for hierarchical clustering: an overview, {II}},
	volume = {7},
	copyright = {© 2017 Wiley Periodicals, Inc.},
	issn = {1942-4795},
	shorttitle = {Algorithms for hierarchical clustering},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1219},
	doi = {10.1002/widm.1219},
	abstract = {We survey agglomerative hierarchical clustering algorithms and discuss efficient implementations that are available in R and other software environments. We look at hierarchical self-organizing maps and mixture models. We review grid-based clustering, focusing on hierarchical density-based approaches. Finally, we describe a recently developed very efficient (linear time) hierarchical clustering algorithm, which can also be viewed as a hierarchical grid-based algorithm. This review adds to the earlier version, Murtagh F, Contreras P. Algorithms for hierarchical clustering: an overview, Wiley Interdiscip Rev: Data Mining Knowl Discov 2012, 2, 86–97. WIREs Data Mining Knowl Discov 2017, 7:e1219. doi: 10.1002/widm.1219 This article is categorized under: Algorithmic Development {\textgreater} Hierarchies and Trees Technologies {\textgreater} Classification Technologies {\textgreater} Structure Discovery and Clustering},
	language = {en},
	number = {6},
	urldate = {2024-12-04},
	journal = {WIREs Data Mining and Knowledge Discovery},
	author = {Murtagh, Fionn and Contreras, Pedro},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1219},
	pages = {e1219},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/EKPF5UD6/Murtagh and Contreras - 2017 - Algorithms for hierarchical clustering an overview, II.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/SECH9PFC/widm.html:text/html},
}

@article{nehama_manipulation-resistant_2022,
	title = {Manipulation-resistant false-name-proof facility location mechanisms for complex graphs},
	volume = {36},
	issn = {1573-7454},
	url = {https://doi.org/10.1007/s10458-021-09535-5},
	doi = {10.1007/s10458-021-09535-5},
	abstract = {In many real-life scenarios, a group of agents needs to agree on a common action, e.g., on a location for a public facility, while there is some consistency between their preferences, e.g., all preferences are derived from a common metric space. The facility location problem models such scenarios and it is a well-studied problem in social choice. We study mechanisms for facility location on unweighted undirected graphs that are resistant to manipulations (strategy-proof, abstention-proof, and false-name-proof) by both individuals and coalitions on one hand and anonymous and efficient (Pareto-optimal) on the other. We define a new family of graphs, \$\$ZV\$\$-line graphs, and show a general facility location mechanism for these graphs that satisfies all these desired properties. This mechanism can also be computed in polynomial time and it can equivalently be defined as the first Pareto-optimal location according to some predefined order. Our main result, the \$\$ZV\$\$-line graphs family and the mechanism we present for it, unifies all works in the literature of false-name-proof facility location on discrete graphs including the preliminary (unpublished) works we are aware of. In particular, we show mechanisms for all graphs of at most five vertices, discrete trees, bicliques, and clique tree graphs. Finally, we discuss some generalizations and limitations of our result for facility location problems on other structures: Weighted graphs, large discrete cycles, infinite graphs; and for facility location problems concerning infinite societies.},
	language = {en},
	number = {1},
	urldate = {2024-12-06},
	journal = {Autonomous Agents and Multi-Agent Systems},
	author = {Nehama, Ilan and Todo, Taiki and Yokoo, Makoto},
	month = jan,
	year = {2022},
	keywords = {Strategy-proofness, Artificial Intelligence, Facility location, False-name-proofness, ZV
-line graphs},
	pages = {12},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/H5TE9THW/Nehama et al. - 2022 - Manipulation-resistant false-name-proof facility location mechanisms for complex graphs.pdf:application/pdf},
}

@misc{peng_domain2vec_2020,
	title = {{Domain2Vec}: {Domain} {Embedding} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {{Domain2Vec}},
	url = {http://arxiv.org/abs/2007.09257},
	doi = {10.48550/arXiv.2007.09257},
	abstract = {Conventional unsupervised domain adaptation (UDA) studies the knowledge transfer between a limited number of domains. This neglects the more practical scenario where data are distributed in numerous different domains in the real world. The domain similarity between those domains is critical for domain adaptation performance. To describe and learn relations between different domains, we propose a novel Domain2Vec model to provide vectorial representations of visual domains based on joint learning of feature disentanglement and Gram matrix. To evaluate the effectiveness of our Domain2Vec model, we create two large-scale cross-domain benchmarks. The first one is TinyDA, which contains 54 domains and about one million MNIST-style images. The second benchmark is DomainBank, which is collected from 56 existing vision datasets. We demonstrate that our embedding is capable of predicting domain similarities that match our intuition about visual relations between different domains. Extensive experiments are conducted to demonstrate the power of our new datasets in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model.},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Peng, Xingchao and Li, Yichen and Saenko, Kate},
	month = jul,
	year = {2020},
	note = {arXiv:2007.09257 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ECCV 2020 paper},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/CY8KLMST/Peng et al. - 2020 - Domain2Vec Domain Embedding for Unsupervised Domain Adaptation.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/D622HJ2U/2007.html:text/html},
}

@inproceedings{peyrard_better_2021,
	title = {Better than {Average}: {Paired} {Evaluation} of {NLP} {Systems}},
	shorttitle = {Better than {Average}},
	url = {http://arxiv.org/abs/2110.10746},
	doi = {10.18653/v1/2021.acl-long.179},
	abstract = {Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances. In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances. We illustrate the importance of taking the instance-level pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley-Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set. By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30\% of the setups. To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing.},
	urldate = {2024-12-13},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	author = {Peyrard, Maxime and Zhao, Wei and Eger, Steffen and West, Robert},
	year = {2021},
	note = {arXiv:2110.10746 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {2301--2315},
	annote = {Comment: Published in ACL 2021 (long paper)},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/T5KZ8X8J/Peyrard et al. - 2021 - Better than Average Paired Evaluation of NLP Systems.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/4J58FE3R/2110.html:text/html},
}

@article{ran_comprehensive_2023,
	title = {Comprehensive survey on hierarchical clustering algorithms and the recent developments},
	volume = {56},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10366-3},
	doi = {10.1007/s10462-022-10366-3},
	abstract = {Data clustering is a commonly used data processing technique in many fields, which divides objects into different clusters in terms of some similarity measure between data points. Comparing to partitioning clustering methods which give a flat partition of the data, hierarchical clustering methods can give multiple consistent partitions of the data at different levels for the same data without rerunning clustering, it can be used to better analyze the complex structure of the data. There are usually two kinds of hierarchical clustering methods: divisive and agglomerative. For the divisive clustering, the key issue is how to select a cluster for the next splitting procedure according to dissimilarity and how to divide the selected cluster. For agglomerative hierarchical clustering, the key issue is the similarity measure that is used to select the two most similar clusters for the next merge. Although both types of the methods produce the dendrogram of the data as output, the clustering results may be very different depending on the dissimilarity or similarity measure used in the clustering, and different types of methods should be selected according to different types of the data and different application scenarios. So, we have reviewed various hierarchical clustering methods comprehensively, especially the most recently developed methods, in this work. The similarity measure plays a crucial role during hierarchical clustering process, we have reviewed different types of the similarity measure along with the hierarchical clustering. More specifically, different types of hierarchical clustering methods are comprehensively reviewed from six aspects, and their advantages and drawbacks are analyzed. The application of some methods in real life is also discussed. Furthermore, we have also included some recent works in combining deep learning techniques and hierarchical clustering, which is worth serious attention and may improve the hierarchical clustering significantly in the future.},
	language = {en},
	number = {8},
	urldate = {2024-12-04},
	journal = {Artificial Intelligence Review},
	author = {Ran, Xingcheng and Xi, Yue and Lu, Yonggang and Wang, Xiangwen and Lu, Zhenyu},
	month = aug,
	year = {2023},
	keywords = {Agglomerative, Artificial Intelligence, Dissimilarity, Divisive, Hierarchical clustering, Similarity},
	pages = {8219--8264},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/P3SAGRG2/Ran et al. - 2023 - Comprehensive survey on hierarchical clustering algorithms and the recent developments.pdf:application/pdf},
}

@misc{ren_learning_2019,
	title = {Learning to {Reweight} {Examples} for {Robust} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1803.09050},
	doi = {10.48550/arXiv.1803.09050},
	abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
	urldate = {2024-12-04},
	publisher = {arXiv},
	author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
	month = may,
	year = {2019},
	note = {arXiv:1803.09050},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/XAM3KK5K/Ren et al. - 2019 - Learning to Reweight Examples for Robust Deep Learning.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/RKXSDZGW/1803.html:text/html},
}

@inproceedings{rofin_votenrank_2023,
	address = {Dubrovnik, Croatia},
	title = {Vote'n'{Rank}: {Revision} of {Benchmarking} with {Social} {Choice} {Theory}},
	shorttitle = {Vote'n'{Rank}},
	url = {https://aclanthology.org/2023.eacl-main.48},
	doi = {10.18653/v1/2023.eacl-main.48},
	abstract = {The development of state-of-the-art systems in different applied areas of machine learning (ML) is driven by benchmarks, which have shaped the paradigm of evaluating generalisation capabilities from multiple perspectives. Although the paradigm is shifting towards more fine-grained evaluation across diverse tasks, the delicate question of how to aggregate the performances has received particular interest in the community. In general, benchmarks follow the unspoken utilitarian principles, where the systems are ranked based on their mean average score over task-specific metrics. Such aggregation procedure has been viewed as a sub-optimal evaluation protocol, which may have created the illusion of progress. This paper proposes Vote'n'Rank, a framework for ranking systems in multi-task benchmarks under the principles of the social choice theory. We demonstrate that our approach can be efficiently utilised to draw new insights on benchmarking in several ML sub-fields and identify the best-performing systems in research and development case studies. The Vote'n'Rank's procedures are more robust than the mean average while being able to handle missing performance scores and determine conditions under which the system becomes the winner.},
	urldate = {2024-09-05},
	booktitle = {Proceedings of the 17th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rofin, Mark and Mikhailov, Vladislav and Florinsky, Mikhail and Kravchenko, Andrey and Shavrina, Tatiana and Tutubalina, Elena and Karabekyan, Daniel and Artemova, Ekaterina},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	month = may,
	year = {2023},
	pages = {670--686},
	annote = {Take Benchmarks as voters, and considers different voting rules to aggregate preferences.Not so much theory, primarily measuring the changes on popular benchmarks
},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/CXWPIVVI/Rofin et al. - 2023 - Vote'n'Rank Revision of Benchmarking with Social .pdf:application/pdf},
}

@misc{shu_meta-weight-net_2019,
	title = {Meta-{Weight}-{Net}: {Learning} an {Explicit} {Mapping} {For} {Sample} {Weighting}},
	shorttitle = {Meta-{Weight}-{Net}},
	url = {http://arxiv.org/abs/1902.07379},
	doi = {10.48550/arXiv.1902.07379},
	abstract = {Current deep neural networks (DNNs) can easily overfit to biased training data with corrupted labels or class imbalance. Sample re-weighting strategy is commonly used to alleviate this issue by designing a weighting function mapping from training loss to sample weight, and then iterating between weight recalculating and classifier updating. Current approaches, however, need manually pre-specify the weighting function as well as its additional hyper-parameters. It makes them fairly hard to be generally applied in practice due to the significant variation of proper weighting schemes relying on the investigated problem and training data. To address this issue, we propose a method capable of adaptively learning an explicit weighting function directly from data. The weighting function is an MLP with one hidden layer, constituting a universal approximator to almost any continuous functions, making the method able to fit a wide range of weighting functions including those assumed in conventional research. Guided by a small amount of unbiased meta-data, the parameters of the weighting function can be finely updated simultaneously with the learning process of the classifiers. Synthetic and real experiments substantiate the capability of our method for achieving proper weighting functions in class imbalance and noisy label cases, fully complying with the common settings in traditional methods, and more complicated scenarios beyond conventional cases. This naturally leads to its better accuracy than other state-of-the-art methods.},
	urldate = {2024-12-03},
	publisher = {arXiv},
	author = {Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
	month = sep,
	year = {2019},
	note = {arXiv:1902.07379},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/EHGDZZNH/Shu et al. - 2019 - Meta-Weight-Net Learning an Explicit Mapping For Sample Weighting.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/NLHU6Y2B/1902.html:text/html},
}

@misc{tatiana_how_2021,
	title = {How not to {Lie} with a {Benchmark}: {Rearranging} {NLP} {Leaderboards}},
	shorttitle = {How not to {Lie} with a {Benchmark}},
	url = {http://arxiv.org/abs/2112.01342},
	doi = {10.48550/arXiv.2112.01342},
	abstract = {Comparison with a human is an essential requirement for a benchmark for it to be a reliable measurement of model capabilities. Nevertheless, the methods for model comparison could have a fundamental flaw - the arithmetic mean of separate metrics is used for all tasks of different complexity, different size of test and training sets. In this paper, we examine popular NLP benchmarks' overall scoring methods and rearrange the models by geometric and harmonic mean (appropriate for averaging rates) according to their reported results. We analyze several popular benchmarks including GLUE, SuperGLUE, XGLUE, and XTREME. The analysis shows that e.g. human level on SuperGLUE is still not reached, and there is still room for improvement for the current models.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Tatiana, Shavrina and Valentin, Malykh},
	month = dec,
	year = {2021},
	note = {arXiv:2112.01342 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, 68-06, 68T50, 68T01, G.3, I.2.7},
	annote = {Comment: Accepted to ICBINB Workshop, NeurIPS 2021},
	file = {arXiv Fulltext PDF:/local/home/dberriaud/Software/Zotero/storage/EQIHDDH3/Tatiana and Valentin - 2021 - How not to Lie with a Benchmark Rearranging NLP Leaderboards.pdf:application/pdf;arXiv.org Snapshot:/local/home/dberriaud/Software/Zotero/storage/26RYBY2A/2112.html:text/html},
}

@article{todo_characterizing_2009,
	title = {Characterizing {False}-name-proof {Allocation} {Rules} in {Combinatorial} {Auctions}},
	abstract = {A combinatorial auction mechanism consists of an allocation rule that deﬁnes the allocation of goods for each agent, and a payment rule that deﬁnes the payment of each winner. There have been several studies on characterizing strategyproof allocation rules. In particular, a condition called weakmonotonicity has been identiﬁed as a full characterization of strategy-proof allocation rules. More speciﬁcally, for an allocation rule, there exists an appropriate payment rule so that the mechanism becomes strategy-proof if and only if it satisﬁes weak-monotonicity.},
	language = {en},
	author = {Todo, Taiki and Iwasaki, Atsushi and Yokoo, Makoto and Sakurai, Yuko},
	year = {2009},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/AFBY45TP/Todo et al. - 2009 - Characterizing False-name-proof Allocation Rules in Combinatorial Auctions.pdf:application/pdf},
}

@inproceedings{torralba_unbiased_2011,
	title = {Unbiased look at dataset bias},
	url = {https://ieeexplore.ieee.org/document/5995347},
	doi = {10.1109/CVPR.2011.5995347},
	abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
	urldate = {2024-12-12},
	booktitle = {{CVPR} 2011},
	author = {Torralba, Antonio and Efros, Alexei A.},
	month = jun,
	year = {2011},
	note = {ISSN: 1063-6919},
	keywords = {Training, Internet, Communities, Testing, Object recognition, Support vector machines, Visualization},
	pages = {1521--1528},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/XXTUNWPC/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/VJQ3MC4S/5995347.html:text/html},
}

@article{varshney_pifhc_2022,
	title = {{PIFHC}: {The} {Probabilistic} {Intuitionistic} {Fuzzy} {Hierarchical} {Clustering} {Algorithm}},
	volume = {120},
	issn = {1568-4946},
	shorttitle = {{PIFHC}},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494622000977},
	doi = {10.1016/j.asoc.2022.108584},
	abstract = {Hierarchical clustering techniques help in building a tree-like structure called dendrogram from the data points which can be used to find the closest related data objects. This paper presents a novel hierarchical clustering technique which considers intuitionistic fuzzy sets to deal with the uncertainty present in the data. Instead of using traditional hamming distance or Euclidean distance measure to find the distance between the data points, it employs the probabilistic Euclidean distance measure to propose a novel clustering approach which we term as ‘Probabilistic Intuitionistic Fuzzy Hierarchical Clustering (PIFHC) Algorithm’. The proposed PIFHC algorithm considers probabilistic weights from the data to measure the distances between the data points. Clustering results over UCI datasets show that our proposed PIFHC algorithm gives better cluster accuracies than its existing counterparts. PIFHC efficiently provides improvements of 1\%–3.5\% in the clustering accuracy compared to other fuzzy hierarchical clustering algorithms for most of the datasets. We further provide experimental results with the real-world car dataset and the Listeria monocytogenes dataset for mouse susceptibility to demonstrate the practical efficacy of the proposed algorithm. For Listeria datasets as well, proposed PIFHC records 1.7\% improvement against the state-of-the-art methods The dendrograms formed by the proposed PIFHC algorithm exhibits high cophenetic correlation coefficient with an improvement of 0.75\% over others. We provide various AGNES methods to update the distance between merged clusters in the proposed PIFHC algorithm.},
	urldate = {2024-12-04},
	journal = {Applied Soft Computing},
	author = {Varshney, Ayush K. and Muhuri, Pranab K. and Danish Lohani, Q. M.},
	month = may,
	year = {2022},
	keywords = {Fuzzy clustering, Hierarchical clustering, Intuitionistic fuzzy sets, Probabilistic Euclidean distance measure, Probabilistic intuitionistic fuzzy hierarchical clustering algorithm, Probabilistic weights},
	pages = {108584},
	file = {ScienceDirect Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/CM8DK3Y7/Varshney et al. - 2022 - PIFHC The Probabilistic Intuitionistic Fuzzy Hierarchical Clustering Algorithm.pdf:application/pdf},
}

@article{wang_deep_2018,
	title = {Deep {Visual} {Domain} {Adaptation}: {A} {Survey}},
	volume = {312},
	issn = {09252312},
	shorttitle = {Deep {Visual} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1802.03601},
	doi = {10.1016/j.neucom.2018.05.083},
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that deﬁne how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare brieﬂy the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classiﬁcation, such as face recognition, semantic segmentation and object detection. Fourth, some potential deﬁciencies of current methods and several future directions are highlighted.},
	language = {en},
	urldate = {2024-12-04},
	journal = {Neurocomputing},
	author = {Wang, Mei and Deng, Weihong},
	month = oct,
	year = {2018},
	note = {arXiv:1802.03601 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {135--153},
	annote = {Comment: Manuscript accepted by Neurocomputing 2018},
	file = {PDF:/local/home/dberriaud/Software/Zotero/storage/B39YQNK8/Wang and Deng - 2018 - Deep Visual Domain Adaptation A Survey.pdf:application/pdf},
}

@misc{wang_superglue_2020,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	doi = {10.48550/arXiv.1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2024-12-19},
	publisher = {arXiv},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv:1905.00537 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2019, super.gluebenchmark.com updating acknowledegments},
	file = {Preprint PDF:/local/home/dberriaud/Software/Zotero/storage/FFR64RGZ/Wang et al. - 2020 - SuperGLUE A Stickier Benchmark for General-Purpose Language Understanding Systems.pdf:application/pdf;Snapshot:/local/home/dberriaud/Software/Zotero/storage/5N3ADG8G/1905.html:text/html},
}

@inproceedings{y_covariate_2019,
	title = {Covariate {Shift}: {A} {Review} and {Analysis} on {Classifiers}},
	shorttitle = {Covariate {Shift}},
	url = {https://ieeexplore.ieee.org/document/8978471},
	doi = {10.1109/GCAT47503.2019.8978471},
	abstract = {Training and testing are the two phases of a supervised machine learning model. When these models are trained, validated and tested, it is usually assumed that the test and train data points follow the same distribution. However, practical scenarios are much different; in real world, the joint distribution of inputs to the model and outputs of the model differs between training and test data, which is called dataset shift. A simpler case of dataset shift, where only the input distribution changes and the conditional distribution of the output for a given input remains unchanged is known as covariate shift. This article primarily provides an overview of the existing methods of covariate shift detection and adaption and their applications in the real world. It also gives an experimental analysis of the effect of various covariate shift adaptation techniques on the performance of classification algorithms for four datasets which include, synthetic and real-world data. Performance of machine learning models show significant improvement after covariate shift was handled using Importance Reweighting method and feature-dropping method. The review, experimental analysis, and observations of this work may serve as guidelines for researchers and developers of machine learning systems, to handle covariate shift problems efficiently.},
	urldate = {2024-12-11},
	booktitle = {2019 {Global} {Conference} for {Advancement} in {Technology} ({GCAT})},
	author = {Y, Geeta Dharani. and Nair, Nimisha G and Satpathy, Pallavi and Christopher, Jabez},
	month = oct,
	year = {2019},
	keywords = {Classification algorithms, Covariate shift, Data Mining, Dataset shift, Machine Learning},
	pages = {1--6},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/PMZGVGZZ/Y et al. - 2019 - Covariate Shift A Review and Analysis on Classifiers.pdf:application/pdf;IEEE Xplore Abstract Record:/local/home/dberriaud/Software/Zotero/storage/7294QI5A/8978471.html:text/html},
}

@misc{zhang_inherent_2024,
	title = {Inherent {Trade}-{Offs} between {Diversity} and {Stability} in {Multi}-{Task} {Benchmarks}},
	url = {https://arxiv.org/abs/2405.01719v2},
	abstract = {We examine multi-task benchmarks in machine learning through the lens of social choice theory. We draw an analogy between benchmarks and electoral systems, where models are candidates and tasks are voters. This suggests a distinction between cardinal and ordinal benchmark systems. The former aggregate numerical scores into one model ranking; the latter aggregate rankings for each task. We apply Arrow's impossibility theorem to ordinal benchmarks to highlight the inherent limitations of ordinal systems, particularly their sensitivity to the inclusion of irrelevant models. Inspired by Arrow's theorem, we empirically demonstrate a strong trade-off between diversity and sensitivity to irrelevant changes in existing multi-task benchmarks. Our result is based on new quantitative measures of diversity and sensitivity that we introduce. Sensitivity quantifies the impact that irrelevant changes to tasks have on a benchmark. Diversity captures the degree of disagreement in model rankings across tasks. We develop efficient approximation algorithms for both measures, as exact computation is computationally challenging. Through extensive experiments on seven cardinal benchmarks and eleven ordinal benchmarks, we demonstrate a clear trade-off between diversity and stability: The more diverse a multi-task benchmark, the more sensitive to trivial changes it is. Additionally, we show that the aggregated rankings of existing benchmarks are highly unstable under irrelevant changes. The codes and data are available at https://socialfoundations.github.io/benchbench/.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Zhang, Guanhua and Hardt, Moritz},
	month = may,
	year = {2024},
	file = {Full Text PDF:/local/home/dberriaud/Software/Zotero/storage/2VPWMBW8/Zhang and Hardt - 2024 - Inherent Trade-Offs between Diversity and Stability in Multi-Task Benchmarks.pdf:application/pdf},
}

