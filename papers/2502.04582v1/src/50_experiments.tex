The Mini Wheelbot involves a number of challenges, making it an interesting testbed for data-driven, learning-based control algorithms.
We showcase two of such algorithms.
In Sec.~\ref{sec:bo}, we use BO which has become popular for controller tuning in the last decade~\cite{chatzilygeroudis2019survey,paulson2023tutorial, marco2016automatic, calandra2016bayesian, berkenkamp2016safe} and allows us to automatically find excellent balancing controller gains in automated hardware experiments.
In Sec.~\ref{sec:ampc}, we use imitation learning from an expert MPC, also called approximate MPC (see~\cite{gonzalez2023neural} for a recent survey).
Approximate MPC avoids slow online optimization which enables sophisticated nonlinear MPC in fast feedback loops onboard robots~\cite{carius2020mpc,nubert2020safe} even on low-cost hardware~\cite{hose2024parameter}.

\subsection{Tuning Stand-up \& Balancing via Bayesian Optimization}
\label{sec:bo}
BO describes a family of black-box optimization algorithms that can be used for controller tuning based on a few interactions with the real world system~\cite{paulson2023tutorial, marco2016automatic, calandra2016bayesian, berkenkamp2016safe,chatzilygeroudis2019survey}. 
Instead of LQR in~\cite{geist2022wheelbot}, we use BO to tune the gains of the state-feedback controller~(\ref{eqn:feedbackgain}) in a direct, data-driven approach based on rewards collected in real-world experiments.
This is practically motivated: Tuning LQR cost matrices can be unintuitive and for some gain combinations, unmodeled high-frequency oscillations occur that are difficult to avoid through the choice of LQR cost.
In addition to finding excellent balancing controller gains, BO illustrates the advantage of automatic environment resets through the Mini Wheelbot's stand-up maneuvers for learning on episodic tasks.


With BO, we find
\begin{align}\label{eqn:bo}
K^* = \argmax_{K} V(K)
\end{align}
based on (noisy) real-world evaluations of the true objective function~$V(K)$.
We define the objective function to be
\begin{align}
    \label{eqn:bo:cost}
    V(K) = 
        \begin{cases}
        -J_\text{c}, & \text{if crash}, \\
        -\frac{1}{T_\text{BO}}\sum_{t=0}^{T_\text{BO}} \|x(t)\|_{Q_\text{BO}}^2 - w_\text{vib}\cdot J_{\text{vib}}, & \text{else},
        \end{cases}
\end{align}
where~$T_\text{BO}$ is the experiments time horizon. The crash penalty~$J_\text{c}$ is empirically chosen slightly worse than a barely successfull experiment (e.g.,~$J_\text{c}=30$ for the experiment shown in Fig.~\ref{fig:boperformanceimprovement}).
For successful experiments, the objective value consists of the mean squared error over the state deviation from the equilibrium with diagonal weights~$Q_\text{BO}=\text{diag}([0,50,200,0,0.4,0.1,0,\dots]^\top)$.
The additional penalty~$J_\text{vib}$ weighted by~$w_\text{vib}=10^{-5}$ is
\begin{align}
    \label{eqn:bo:costvib}
    J_{vib} 
    = \sum_{t=1}^{T_\text{BO}} 
    ( 
        &|\Delta\dot{\phi}(t)|,
        \text{if}\; |\Delta\dot{\phi}(t)| \geq \alpha, \;
        \text{else}\; 0
    ),
\end{align}
with $\Delta\dot{\phi}(t)=\dot{\phi}(t)-\dot{\phi}(t-1)$, which effectively suppresses high frequency vibrations.

For each episode of the optimization, the Wheelbot has to perform a stand-up maneuver and balance with a setpoint change in the drive-wheel position.
An excerpt of the closed loop trajectories from our experiments when the controller stabilizes the robot right after the stand-up is shown in Fig.~\ref{fig:boclosedloop}.
We use GPyTorch~\cite{gardner2018gpytorch} to solve (\ref{eqn:bo}).
For comparison, we provide a pseudo-random baseline based on space-filling Sobol sampling.
The performance improvement over the episodes for five random seeds is plotted in Fig.~\ref{fig:boperformanceimprovement}.
Even though, pseudo random sampling finds acceptable controllers eventually, BO has the better overall performance or requires less experiments with less failures.

\begin{figure}[tb]
    \centering%
    \include{figures/bo_results/4d_roll_angle_comparison.tex}
    \caption{State evolution for a hand-tuned controller ({\color{red}red}), several episodes of BO ({\color{gray}gray}), and optimized controller ({\color{blue}blue}). The task is stabilizing after roll stand-up.}
    \label{fig:boclosedloop}
\end{figure}%
\begin{figure}[tb]
    \centering%
    \include{figures/bo_results/performance_improvement.tex}
    \caption{Performance of the best controller found by BO ({\color{blue}blue}) for a given number of trials (episodes) compared to a pseudo-random baseline ({\color{orange}orange}). Intervals are the standard deviation over five random seeds.}
    \label{fig:boperformanceimprovement}
\end{figure}


\subsection{Fast Neural Network Approximate MPC}
\label{sec:ampc}
Despite great engineering efforts, nonlinear solvers still struggle to solve MPC optimization problems in real-time on embedded CPUs.
Instead, we use imitation learning to find an explicit mapping from states to actions in the form of a neural network that approximates the MPC described in Section~\ref{sec:nonlinearmpc}.
The approximate MPC is several orders of magnitude faster (onboard inference in less than~\SI{300}{\micro\second}) compared to solving the optimization problem~\eqref{eqn:mpc}, which takes several hundred milliseconds on a desktop CPU.
It thus allows a sophisticated nonlinear MPC with convoluted system dynamics, fine discretization, and a long prediction horizon to run onboard the robot.
With this, we achieve -- for the first time -- articulated driving following high level yaw and velocity setpoints, e.g., provided by keyboard.

We imitate the optimal solution to~(\ref{eqn:mpc}) by first sampling a large dataset of random states~$x$ and optimal solutions~$u^*(x)$ containing \num{3.5} million points.
We use a multi-layer perceptron with \num{4} layers, \num{100} neurons per layer, and a mixture of tangent hyperbolic and rectified linear activations as function approximator, which is trained in Jax.
To achieve fast control, we implement inference in C++ using Eigen onboard the Mini Wheelbot.
The final controller on the Mini Wheelbot hardware driving around based on keyboard heading and velocity commands is shown in Fig.~\ref{fig:mpcexperiments} and the supplementary video.
This is the first time that controlled driving along specified directions is achieved for a reaction wheel balancing unicycle of this kind.

\begin{figure}[tb]
    \newcommand{\photoheight}{1.2in}
    \newcommand{\plotwidth}{1.65in}
    \newcommand{\plotheight}{1.4in}
    \centering
    \begin{subfigure}[t]{1.6in}
        \centering
        \includegraphics[height=\photoheight,trim=0.5in 0 0.5in 0, clip]{figures/yaw-control-plot/overlay_driving_3.jpg}
    \end{subfigure}
    \begin{subfigure}[t]{1.6in}
        \centering
        \input{figures/yaw-control-plot/yaw_step_photo.tex}
    \end{subfigure}
    \par\medskip %
    \begin{subfigure}[t]{1.4in}
        \centering
        \input{figures/yaw-control-plot/plot.tex}
    \end{subfigure}\hspace{0.2in}
    \begin{subfigure}[t]{1.6in}
        \input{figures/yaw-control-plot/yaw_step_plot.tex}
    \end{subfigure}
    \caption{Approximate MPC experiments: Driving around based on keyboard heading and velocity commands (left) and yaw reference step response (right).
    The controller runs onboard the Mini Wheelbot.
    }
    \label{fig:mpcexperiments}
\end{figure}
