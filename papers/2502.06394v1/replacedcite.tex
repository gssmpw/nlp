\section{Background and Related Work}
\subsection{Text Style Transfer}

Text Style Transfer (TST), the task of rewriting the text in a target style while preserving its semantic content and fluency, has garnered significant attention in the natural language processing community due to its potential applications in text generation____. TST encompasses various subtasks, including formality style transfer____, sentiment style transfer____, authorship style transfer____, and detoxification____. 

With the advent of Large Language Models (LLMs), in-context learning methods have increasingly been utilized for TST and detoxification tasks. ____ proposed a novel approach to TST by prompting LLMs and then reranking the generated texts based on three TST metrics: text similarity, target style strength, and fluency. Similarly, ____ demonstrated the effectiveness of prompting GPT-3, a state-of-the-art LLM at the time, to rewrite texts in a desired style.

\subsection{Text Detoxification}

Text Detoxification, a subtask of Text Style Transfer (TST), involves transforming an input text $x_i$, identified as toxic through toxicity estimation models, into a text $y_i$ that is non-toxic in style while maintaining semantic similarity and fluency. In this context, toxicity refers to language that is harmful, offensive, or inappropriate.

% Due to the lack of parallel training data, early research focused on unsupervised detoxification methods____. For instance,____ proposed a cycle consistency loss for translating toxic texts into a non-toxic style in an unsupervised manner. More recently, the introduction of parallel detoxification corpora, such as ParaDetox____ and APPDIA____, has enabled the training of sequence-to-sequence models____ that outperform most unsupervised approaches in terms of rewritten toxicity, fluency, and semantic similarity. Finally, ____ used activation patched LLMs to generate synthetic parallel detoxification data for English based on ParaDetox and showed that training on this data yields better detoxification models.

Due to the lack of parallel training data, early research focused on unsupervised detoxification methods____. For instance,____ and APPDIA____, has enabled the training of sequence-to-sequence models____ that outperform most unsupervised approaches in terms of rewritten toxicity, fluency, and semantic similarity. In parallel, ____ explored the use of activation patching in LLMs to generate synthetic parallel detoxification data for English. Their results demonstrated that training detoxification models on this data yields performance comparable to models trained on manually annotated datasets in automatic evaluations, while achieving superior quality in human assessments.

\subsection{Multilingual Text Style Transfer}

The scarcity of high-quality parallel multilingual detoxification data remains a major challenge in the field. Recently, new non-English parallel datasets have been introduced for various TST tasks, including a Bangla language parallel sentiment style transfer dataset____ and the extension of the GYAFC dataset to Portuguese, French, and Italian, resulting in XFORMAL____. Following the crowdsourcing pipeline introduced by____, a parallel text detoxification dataset for Russian was collected____. Later, using the similar data annotation pipeline,____ collected 1000 sentence pairs across nine languages, resulting in the MultiParaDetox dataset for a corresponding shared task on multilingual text detoxification. Furthermore, in a more recent work____, provide an in-depth analysis of toxicity characteristics across languages, exploring descriptive linguistic features that influence detoxification quality.

Nevertheless, the size of MultiParaDetox is far from satisfactory with 1000 sentence pairs per language, only 400 of which are publicly available. The remaining 600 pairs comprised the test set for the multilingual text detoxification shared task____. Such relatively small dataset may be insufficient for training big multilingual language models for multilingual text detoxification.

To bridge this gap, we present \dataset~- a synthetic parallel detoxification corpus for four European languages, namely, German, Spanish, French, and Russian, with 4000 samples for each language. The dataset creationg pipeline presented in our work can easily be transferred to other languages as well, drastically reducing the cost of annotation for parallel detoxification datasets.