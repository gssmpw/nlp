% As in the pure-NN case, the hardness of exact bound propagation motivates the use of abstraction-based techinques. We now turn to how these techniques can be applied to the neurosymbolic setting.

The NP-hardness of exact bound computation through the compiled symbolic component motivates the use of relaxation-based techniques. We now show how these can be extended to the NeSy setting in order to provide a scalable solution to Equation \ref{eq:goal}. 

Compositional probabilistic NeSy systems can be viewed as a single computational graph, by providing the outputs of the neural network as the inputs of the symbolic probabilistic circuit. In the case of the running example of Figure \ref{fig:system}, the outputs of each of the two networks are concatenated into a single vector and used as input to the arithmetic circuit which represents the constraints. Hence, a NeSy system can be seen as an end-to-end differentiable algebraic computational graph, which accepts an input, an image in this case, and outputs a vector of probabilities. These characteristics allow one to construct the NeSy system as a single module comprising an arbitrary number of neural networks and a single arithmetic circuit. Such a module can be constructed in a machine learning library, such as Pytorch, and subsequently exported as an Open Neural Network Exchange (ONNX) graph \cite{onnxruntime}. Figure \ref{fig:onnx} depicts the ONNX representation of the NeSy system of the running example.

% Importantly, the proposed method is not confined to a particular representation language, such as SDD's, since all tractable representation under a probabilistic semantics can be transformed into arithmetic circuits for inference.


% \begin{wrapfigure}{o}{0.6\linewidth}
%     \includegraphics[width=\linewidth]{figures/3. road_r_onnx_small.pdf}
%   \caption{Unified ONNX representation of the NeSy system from the running example. The input image is passed through the two NNs (left branch is action selection, right branch is object detection) and then through the arithmetic circuit. The NNs are stripped down to 1 convolutional and 1 dense layer for conciseness. The operators in the circuit besides Add, Sub, and Mul, are created by other Python operations such as tensor indexing, concatenation, etc.}
% \end{wrapfigure}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/3_road_r_onnx_small.pdf}
    \caption{Unified ONNX representation of the NeSy system of the running example. The input image is processed by the two NNs (left branch is action selection, right branch is object detection) and then through the arithmetic circuit. The NNs are stripped down to one convolutional layer (Conv + MaxPool + ReLU) and one dense layer (Reshape + Gemm + Softmax/Sigmoid) for conciseness. The operators in the circuit, besides Add, Sub, and Mul, are created by Python operations, such as tensor indexing and concatenation.}
    \vspace{0.1cm}
    % \small\textsuperscript{a} General Matrix Multiplication.
    \label{fig:onnx}
\end{figure}

ONNX is a widespread NN representation, and is the stadard input format for NN verifiers \cite{vnnlib2024}. This includes both solver-based verification tools, such as Marabou \cite{katz2019marabou}, and relaxation-based ones, such as auto\_LiRPA \cite{autolirpaXu2020} and VeriNet \cite{henriksen2020efficient}. Thus, by representing a NeSy system as an end-to-end computational graph and exporting it to this format, it is possible to utilize state-of-the-art tools to perform verification in an almost ``out-of-the-box'' fashion. While our proposed framework is, in principle, compatible with all the aforementioned tools, we focus on relaxation-bsed verifiers, in order to showcase scalable probabilistic NeSy verification. Such verifiers allow us to perturb the input and compute bounds directly on the output of the NeSy system, that is, without computing intermediate bounds on the NN outputs.
