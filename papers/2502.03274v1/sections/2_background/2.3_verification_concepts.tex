% Objectives
% \begin{itemize}
%     \item \textbf{1 paragraph} Introduce abstract/deductive and complete/sound verification. Mention that exact NN verification is NP-hard.
%     \item \textbf{1-2 paragraphs} Abstraction-based NN verification techniques: IBP/SIP/RSIP etc. Somewhere here we need to clearly say that these methods allow us to obtain probability \textit{ranges} at the NN outputs rather than probability \textit{values}. Also, we need to note how these bounds are used to calculate robustness.
% \end{itemize}

% 1. What is verification
% 2. Complete
% 3. Incomplete

\paragraph{NN Robustness.}
Verifying the robustness of NN classifiers amounts to proving that the network's correct predictions remain unchanged if the corresponding input is perturbed within a given range $\epsilon$ \cite{Wong+18}. Contrary to empirical machine learning evaluation techniques, NN verification methods reason over infinitely-many inputs to derive certificates for the robustness condition. For a given network $f$, this is formalized as follows: for all inputs $\boldsymbol{x}$, such that $f(\boldsymbol{x})$ is a correct prediction, and for all $\boldsymbol{x}'$, such that $\left\| \boldsymbol{x} - \boldsymbol{x}' \right\| \leq \epsilon$, it holds that $f(\boldsymbol{x}) = f(\boldsymbol{x}')$. 
    
Checking if the robustness condition holds for some $\epsilon$ can be achieved by reasoning over the relations between the network's un-normalized predictions (logits) at the NN's output layer. In particular, it can be seen that if for any $\boldsymbol{x}'$ in an $\epsilon$-ball of $\boldsymbol{x}$, it holds that $\mathit{y_{true} - y_{i}} > 0$, for all $y_i \neq y_{true}$, then the network is robust for $\epsilon$~\cite{gowal2018ibp}. Here $\mathit{y_{true}}$ is the logit corresponding to the correct class and $y_i$ are the logits corresponding to all other labels. This condition can be checked by computing the minimum differences of the predictions for all points in the $\epsilon$-ball. If that minimum is positive, the robustness condition is satisfied. However, finding that minimum is NP-hard \cite{katz2017reluplex}. 

% The area of Verification of Neural Networks~\cite{liu2020algorithms} aims to provide determininstic guarantees of  a network output with respect to a given specification, e.g., robustness to noise, colour changes, etc. Formally, a neural network with parameters $\theta$, denoted by $f_\theta$ is said to be (locally) robust at an input point $x_0$ if no point (or adversarial example) $x_0'$ exists in the neighbourhood causing the prediction of the network to change from its target label \cite{gowal2018ibp-not-the-correct-ref}. For classification problems, this amounts to ensuring that the output class predicted by a network does not change for small input perturbations within a budget denoted by $\epsilon$ \cite{gowal2018ibp}. Input perturbations are usually defined in the $\mathcal{L}_p$-norm function space. Formally, for all inputs set $x$ and perturbed input set $x'$ this is expressed as:
% \begin{equation}
% 	f_\theta(x) = f_\theta(x') \forall x, \norm{x-x'} \leq \epsilon
% \end{equation}
% Robustness verification with respect to input perturbations involves analyzing the unnormalized final layer predictions of the network. Specifically, for all perturbed inputs \(x_i'\) within the perturbation set \(x'\), the network is considered robust if \(y_{\text{true}} - y_j > 0\) holds, where \(y_{\text{true}}\) represents the correct class prediction, and \(y_j\) corresponds to the output for all other classes \cite{gowal2018ibp} and is known for being NP-Hard~\cite{katz2017reluplex}.
%Robustness verification w.r.t.~perturbations over $\mathcal{L}_p$-norm %space within an $\epsilon$ budget entails the inspection of the un-%normalized network's final layer predictions. In other words, for all %perturbed inputs $x_i'$ in the perturbed input set $x'$, if $y_true - %y_j > 0$, where $y_true$ denotes the prediction of the correct class and %$y_j$ denotes the network output for all other classes, then the network %is robust for $\epsilon$ \cite{gowal2018ibp}. This check can be achieved %by computing the minimum differences of the predictions for all points %within the $\epsilon$ region and ensuring that these minimum values are %positive to satisfy the robustness condition. However, finding this %minimum is an NP-hard problem \cite{katz2017reluplex}.

%AL: This requires major re-writing. Alternatively we can just say.
% Robustness verification is known for being NP-Hard~\cite{katz2017reluplex}.


%AL: Imperial MILP references to ad??
%and Semidefinite Programming (SDP), 
%AL: SDP is NOT complete
%AL: MILP is invented at Imperial - shall we use those? Reulplex does not use MILP but SMT.
% Add SMT?


%     Verifying the robustness of NN classifiers amounts to proving that the network's correct predictions remain unchanged if the corresponding input is perturbed within a given range $\epsilon$ \cite{wong2018provable}. Contrary to empirical machine learning evaluation techniques, such as cross-validation, NN verification methods reason over infinitely-many inputs, to derive certificates for the robustness condition, which, for a given network $f$ can be formalized as follows: for all inputs $\boldsymbol{x}$, such that $f(\boldsymbol{x})$ is a correct prediction and for all $\boldsymbol{x}'$ such that $\left\| \boldsymbol{x} - \boldsymbol{x}' \right\| \leq \epsilon$, it holds that $f(\boldsymbol{x}) = f(\boldsymbol{x}')$. 
    
    % Checking if the robustness condition holds for some $\epsilon$ can be achieved by reasoning over the relations between the network's un-normalized predictions (logits) at the NN's output layer. In particular, it can be seen \cite{gowal2018ibp} that if for any $\boldsymbol{x}'$ in an $\epsilon$-ball of $\boldsymbol{x}$, it holds that $\mathit{y_{true} - y_{i}} > 0$, for all $y_i \neq y_{true}$, then the network is robust for $\epsilon$. Here $\mathit{y_{true}}$ is the prediction corresponding to the correct class and the $y_i$'s are the other labels' predictions. In turn, this last condition can be checked by computing the minimum of the predictions' differences for all points in the $\epsilon$-ball and checking if that minimum is positive, in which case the robustness condition is satisfied. However, finding that minimum is NP-hard \cite{Katz+17}. 

% \paragraph{Complete/incomplete Verification.}
% Verification approaches are categorized as either complete or incomplete methods. Complete methods are those that return a definite answer as to whether the property in question is satisfied by reasoning over the exact verification problem. In contrast, incomplete approaches do not reason over an exact formulation of the verification problem and hence are not theoretically guaranteed to solve a problem, and may fail to solve a subset of queries. In return, they can be far less computationally expensive.

% Verification algorithms for neural network robustness are classified into complete and incomplete methods. Complete verification techniques guarantee to find a proof that the specification is true or provide a counterexample if the specification is incorrect. This requires exhaustive search in the worst case \cite{qin2019verification}.

\paragraph{Solver-Based Verification.}
Early verification approaches include Mixed Integer Linear Programming (MILP) \cite{lomuscio2017approach,tjeng2018evaluating,henriksen2020efficient} and Satisfiability Modulo Theories (SMT) \cite{ehlers2017formal,katz2017reluplex}. MILP approaches encode the verification problem as an optimization task over linear constraints, which can be solved by off-the-shelf MILP-solvers. SMT-based verifiers translate the NN operations and the verification query into an SMT formula and use SMT solvers to check for satisfiability. Although these methods are precise and provide exact verification results, they do not scale to large, deep networks, due to their high computational complexity. As such, they are impractical for real-world applications with high-dimensional inputs like images or videos.

\paragraph{Relaxation-Based Verification.}
As the verification problem is NP-hard~\cite{katz2017reluplex}, incomplete techniques that do not reason over an exact formulation of the verification problem, but rather an over-approximating relaxation, are used for efficiency. A salient method that is commonly used is Interval Bound Propagation (IBP), a technique which uses interval arithmetic~\cite{intervalArithmeticSunaga1958} to propagate the input bounds through all the layers of a NN~\cite{gowal2018ibp}. As a non-exact approach to verification, it is not theoretically guaranteed to solve a problem. However, the approach is sound, in that if the lower bound is shown to be positive the network is robust. Therefore, once the bounds of the output layer are obtained, an instance is safe if the lower bound of the logit corresponding to the correct class is greater than the upper bounds of the rest of the logits, since this ensures a correct prediction, even in the worst case.

% In contrast, incomplete verification techniques~\cite{gowal2018ibp} work by approximating the verification queries. They may be able to establish the local robustness of a model, but may be unable to solve the query, returning undefined. Interval Bound Propagation (IBP) is an approximate technique that computes bounds on neuron activations, by propagating intervals through the network layers \cite{gowal2018ibp}. 

% NB EdS: Do we need to mention SDP?
% Semi Definite Programming (SDP) approaches model the verification as an optimisation problem and express the dual representation as a maximum eigenvalue problem \cite{kolter2017provabledefenses,wong2018scalingpa,dvijotham2018ada,bunel2020lagrangiandf}.

% Symbolic Interval Propagation (SIP) refines IBP by using symbolic representations to track dependencies between inputs and intermediate layer activations, improving the tightness of bounds \cite{wang2021beta}. Reverse Symbolic Interval Propagation (RSIP) further improves this by propagating constraints backward from the output to the input, enabling tighter bounds while maintaining efficiency~\cite{gehr2018ai2}. Branch and bound (BaB) based complete verifiers may use an incomplete verifier as a sub-procedure to perform branching on the model input \cite{bunel2018unifiedviewpiecewiselinear} or the ReLU neurons \cite{bunel2018unifiedviewpiecewiselinear,botoeva2020efficient}. IBP is the fastest of the methods above but is the least precise. SIP and RSIP methods enable tighter approximations but, due to their complexity, may be unable to solve verification queries for large models. 

% While complete verifiers provide precise approximations they are computationally expensive to scale and incomplete verifiers scale to a certain extent at the cost of loose approximations.