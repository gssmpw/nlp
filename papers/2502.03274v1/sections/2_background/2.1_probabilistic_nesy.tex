Probabilistic NeSy AI aims to combine perception with probabilistic logical reasoning. We provide a brief overview of the operation of such a system based on \cite{marconato2024bears}. Given input $\boldsymbol{x} \in \mathbb{R}^n$, the system utilizes a NN, as well as symbolic knowledge $\mathrm{K}$, to infer a (multi-)label output $\boldsymbol{y} \in \{0, 1\}^m$. In particular, the system computes $p_\theta(\boldsymbol{y} \,\vert\, \boldsymbol{x}; \mathrm{K})$, where $\theta$ refers to the trainable parameters of the NN. This is achieved in a two-step process. First, the system extracts a set of \textit{latent concepts} $\boldsymbol{c} \in \{0, 1\}^k$, through the use of a parameterized neural model $p_\theta(\boldsymbol{c} \,\vert\, \boldsymbol{x})$. These latent concept predictions are then used as input to a reasoning layer, in conjunction with knowledge $\mathrm{K}$, to infer $p(\boldsymbol{y} \,\vert\, \boldsymbol{c}; \mathrm{K})$.

The setting is straightforward to extend to multiple NNs. In that case, the $i^{\text{th}}$ network from a set $\mathrm{E}$ would predict $p_\theta^i(\boldsymbol{c^i} \,\vert\, \boldsymbol{x})$, with $\bigcup_{i \in \mathrm{E}} \boldsymbol{c}^i = \boldsymbol{c}$. Consider the running example of Figure \ref{fig:system}, where two NNs accept the same image as input and output two disjoint sets of latent concepts. These are then combined to form the input to the reasoning layer in order to output the target $\boldsymbol{y}$.