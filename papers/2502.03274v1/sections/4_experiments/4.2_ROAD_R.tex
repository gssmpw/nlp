In this experiment we apply our proposed approach to a real-world dataset from the autonomous driving domain. The purpose of the experiment is to assess the robustness of a neural autonomous driving system with respect to the safety and common-sense properties of Figure \ref{fig:system}, i.e., to evaluate whether input perturbations cause the neural systems to violate the constraints that they previously satisfied.

\paragraph{Dataset.} To that end, we use the ROad event Awareness Dataset with logical Requirements (ROAD-R) \cite{giunchiglia2023road}. ROAD-R consists of 22 videos of dashcam footage from the point of view of an autonomous vehicle (AV), and is annotated at frame-level with bounding boxes. Each bounding box represents an \textit{agent} (e.g. a pedestrian, vehicles of different types, etc.) performing an \textit{action} (e.g. moving towards the AV, turning, etc.) at a specific \textit{location} (e.g. right pavement, incoming lane, etc.). 


\paragraph{Experimental Setting.} 
We focus on a subset of the dataset that is relevant to the symbolic constraints of Figure \ref{fig:system}. Consequently, we select a subset of frames which adhere to these constraints. Specifically, either the AV is moving forward, there is no red traffic light in the frame, and no car stopped in front of the AV, or the AV is stopped, and there is either a red traffic light or a car stopped in front. By sampling the videos every 2 seconds, we obtain a dataset of 3143 examples, where each example contains a $3 \times 240 \times 320$ image, and four binary labels: red light, car in front, stop, move forward.

% ROAD-R also comes with 243 common-sense logical requirements (e.g. an agent cannot move away and towards the vehicle at the same time).

The neural part of the system comprises two 6-layer CNNs\footnote{The CNNs have 4 convolutional layers with max pooling and 2 linear ones. The object detection network has a sigmoid activation at the output, while the action selection network has a softmax.}, responsible for object detection and action selection respectively. The two networks are trained in a standard supervised fashion using an 80/20 train/test split over the selected frames. The object detection and action selection networks achieve accuracies of $97.2\%$ and $96.3\%$ on the respective test sets. We add $\mathcal{L}_{\infty}$-norm perturbations to the test input images  for five values of perturbation size $\epsilon$: $\{10^{-5}, 5 \cdot 10^{-5}, 10^{-4}, 5 \cdot 10^{-4}, 10^{-3}\}$. 

\begin{table*}[!t]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{cccccc}
        \hline \hline
        \multirow{2}{*}{\textbf{Metric}} & \multicolumn{5}{c}{\textbf{Epsilon}} \\ 
        & 1e-5      & 5e-5      & 1e-4      & 5e-4     & 1e-3 \\ \hline
        Robustness (\%) & $96.82\%$ & $92.68\%$ & $82.64\%$ & $6.21\%$ & $0.00\%$ \\
        Runtime per Sample (s) & $0.091$ & $0.092$ & $0.091$ & $0.092$ & $0.092$ \\
        \hline \hline
    \end{tabular}
    \caption{Autonomous driving experiment results, indicating robustness and verification runtime for five values of the $\epsilon$-perturbation.}
    %Standard (unperturbed input) accuracy: $97.2\%$.}
    \label{tab:road-r-robustness}
\end{table*}

Table \ref{tab:road-r-robustness} presents the results. We report robustness, i.e., the fraction of robust instances over the total number of instances in the test set, and verification runtime for \textsc{E2E-R}. Since this task consists of a small arithmetic circuit and a significantly larger neural component, it is the latter that predominantly affects both the computational overhead and the accumulated errors of bound propagation. Therefore, \textsc{E2E-R} and \textsc{R+SLV}, which differ only in the symbolic component, provide nearly identical results that are omitted.

As expected, robust accuracy falls as the perturbation size increases. Regarding the verification runtime, this experiment reinforces our results from Section \ref{mnist_addition}, by demonstrating that the runtime of our approach remains largely unaffected by changes in the value of the perturbation size $\epsilon$.

% Further, given a standard accuracy (unperturbed input) of $97.2\%$, we see that robust accuracy is almost equal to the standard accuracy for the smallest $\epsilon$ and reducing to zero for the largest $\epsilon$.

% We omit \textsc{R+SLV} as it would provide almost identical results with \textsc{E2E-R}. 

% This is because the two approaches differ only in their solution regarding the symbolic component, and thus, given a task where the majority  would be very similar.

% As this NeSy task is characterized by larger neural networks and a small circuit. Thus, the majority of (1) the computational overhead and (2) the accumulated errors of bound propagation lie on the neural component of the system. 

% Thus, the computational overhead in obtaining output bounds lies on the neural side. 

% Contrary to the MNIST addition experiment this task inclu, the majority of the computational overhead lies in propagating bounds through the large neural networks. where the NNs are larger and the circuit. Consequently, both the robustness and the runtime would be almost identical with the results obtained by \textsc{R+SLV}, overheadddddd, errors accumulate etc.

% since the runtimes are almost identical to the those \textsc{E2E-R} -- note that the symbolic part in this case is a relative small circuit, which is easy for Gurobi to handle -- and the robustness of   