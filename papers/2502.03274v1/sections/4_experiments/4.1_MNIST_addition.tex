In this experiment we evaluate the scalability of our approach as the complexity of the probabilistic reasoning component increases. Specifically, we explore how the approximate nature of our method enhances scalability, while also considering the corresponding trade-off in the quality of verification results. 

\noindent To this end, we compare the following approaches:

\begin{enumerate}
    \item \textbf{End-to-End relaxation-based verification \big(\textsc{E2E-R}\big)} \\
    An implementation of our method in \lirpa, a state-of-the-art relaxation-based verification tool. The input to \lirpa is the NeSy system under verification, which is translated internally into an ONNX graph. The verification method used is IBP, as implemented in \lirpa.
    \item \textbf{Hybrid verification \big(\textsc{R+SLV}\big)} \\
    A hybrid approach consisting of relaxation-based verification for the neural part of the NeSy system and solver-based bound propagation through the symbolic part. The former is implemented in \lirpa using IBP. The latter is achieved by transforming the circuit into a polynomial (see Section \ref{exact_solution}), and solving a constrained optimization problem with the \gurobi solver. The purpose of comparing to this baseline is to assess the trade-off between scalability and quality of results, when using exact vs approximate bound propagation through the symbolic component. 
    \item \textbf{Solver-based verification \big(\textsc{Marabou}\big)} \\
    Exact verification using Marabou, a state-of-the-art SMT-based verification tool, also used as a backend by most NeSy verification works in the literature \cite{xie2022neuro,daggitt2024vehiclebridgingembeddinggap}. Marabou is unable to run on the full NeSy architecture, as the current implementation\footnote{https://github.com/NeuralNetworkVerification/Marabou} does not support several operators, such as Softmax and tensor indexing. To obtain an indication of Marabou's performance, we use it to verify only the neural part of the NeSy system, a subtask of NeSy verification. Specifically, we verify the classification robustness of the CNN performing MNIST digit recognition. 
\end{enumerate}

\paragraph{Dataset.} 
We use a synthetic task, where we can controllably increase the size of the symbolic component, while keeping the neural part constant. In particular, we create a variant of multi-digit MNIST addition \cite{manhaeve2018deepproblog}, where each instance consists of multiple MNIST digit images, and is labelled by the sum of all digits. We can then control the number of MNIST digits per sample, e.g. for 3-digit addition, an instane would be $\big( \, \big( \raisebox{-0.2\height} {\includegraphics[width=0.4cm]{figures/mnist_4.png}}, \raisebox{-0.2\height}{\includegraphics[width=0.4cm]{figures/mnist_7.png}}, \raisebox{-0.2\height}{\includegraphics[width=0.4cm]{figures/mnist_2.png}} \big), \, 13 \big)$. We construct the verification dataset from the $10\mathrm{K}$ samples of the MNIST test set, using each image only once. Thus, for a given $\#\text{digits}$ the verification set contains $10\mathrm{K} / \#\text{digits}$ test instances.

\paragraph{Experimental setting.} 
The NN is a convolutional neural network\footnote{The CNN comprises 2 convolutional layers with max pooling and 2 linear layers, with a final softmax activation.} tasked to recognize single MNIST digits. The CNN is trained in a standard supervised fashion on the MNIST train dataset, consisting of 60K images, and achieves an accuracy of $98\%$ on the test set. The symbolic part consists of the rules of multi-digit addition. It accepts the CNN predictions for the input images and computes a probability for each sum. As the number of summand digits increases, so does the size of the reasoning circuit, since there are more ways to construct a given sum using more digits (e.g. consider the ways in which 2 and 5 digits can sum to 17). We vary the number of digits as well as the size of $\mathcal{L}_{\infty}$-norm perturbations added to the input images. We consider five values for $\#$digits: $\{2, 3, 4, 5, 6\}$ and three values for the perturbation size $\epsilon$: $\{10^{-2}, 10^{-3}, 10^{-4}\}$, resulting in 15 distinct experiments. For each experiment, i.e., combination of $\#$digits and $\epsilon$ values, we use a timeout of 72 hours. \textsc{E2E-R} runs on a single thread, while the Gurobi solver in \textsc{R+SLV} dynamically allocates up to 1024 threads.


% Figure~\ref{fig:mnist-timing} we examine how increasing the number of digits affects the runtime of each method. The metric reported for each method is the time required to verify the robustness of the NeSy system on a single sample, averaged across the entire MNIST test dataset. We run the experiment for three values of epsilon to investigate how the size of the robustness guarantee affects the runtime of each approach. The digit recognition CNN is trained in a standard supervised fashion, achieving an accuracy of $98\%$ on the MNIST test dataset. We use a timeout of 3 days for each verification query, i.e. one number of digits and one epsilon. 

\paragraph{Scalability.} Figure \ref{fig:mnist-timing} presents a scalability comparison between the methods. The figure illustrates the time required to verify the robustness of the NeSy system for a single sample, averaged across the test dataset. All experiments terminate within the timeout limit, with the exception of two configurations for \textsc{R+SLV}. For $\langle \epsilon=10^{-2}, \#\text{digits}=5, 6 \rangle$, \textsc{R+SLV} was not able to verify any instance within the timeout (which is why the lines for $\epsilon=10^{-2}$ stop at 4 digits in Figure \ref{fig:mnist-timing}). For $\langle \epsilon=10^{-3}, \#\text{digits}=6 \rangle$, \textsc{R+SLV} verifies less than 5\% of the examples within the timeout. The reported values in Figure \ref{fig:mnist-timing} are the average runtime for this subset. 

% Explain through bound looseness in a branch-and-bound-based solver.
% This can explain the fact that the orange line for our method doesn't change between 5 \& 6 digits.

\begin{table*}[t]  
    \renewcommand{\arraystretch}{1.1}
    \centering
    \begin{tabular}{cccccc}
         \hline \hline
         \multirow{2}{*}{\textbf{Verification Method}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{4}{c}{\textbf{\#MNIST digits}} \\
         % \cline{2-13}
         & & 2 & 3 & 4 & 5 \\
         \hline
         % Average circuit size       & \multicolumn{2}{c|}{s2} & \multicolumn{2}{c|}{s3} & \multicolumn{2}{c|}{s4} & \multicolumn{2}{c|}{s5} \\
         % \hline
         \multirow{2}{*}{\textsc{R+SLV}} & Lower/Upper Bound
                                               & $0.871-0.981$ & $0.815-0.972$ & $0.764-0.962$ & $0.731-0.928$ \\
                                               & Robustness (\%)         & $90.60$ & $86.17$ & $81.33$ & $78.31$ \\\hline
         \multirow{2}{*}{\textsc{E2E-R}}  & Lower/Upper Bound
                                               & $0.871-0.982$ & $0.815-0.974$ & $0.763-0.965$ & $0.716-0.958$ \\
                                               & Robustness (\%)         & $90.60$ & $86.11$ & $81.21$ & $76.67$ \\ \hline \hline
    \end{tabular}
    \caption{Comparison of performance between the proposed approach and the baseline with respect to the size of the symbolic component. We report one metric for bound tightness and one metric for the robustness of the system, according to each method.}
    \label{tab:mnist-perf}
\end{table*}

\begin{figure}[!h]
    \centering
    \includegraphics[width=\linewidth]{figures/4_MNIST_scaling.pdf}
    \caption{Comparison of verification runtime between three methods, with respect to the size of the symbolic component. We report the time required to verify the robustness of the NeSy system on a single sample, averaged across the MNIST test dataset, and repeat the experiment for three values of the perturbation size $\epsilon$.}
    \label{fig:mnist-timing}
\end{figure}

As Figure \ref{fig:mnist-timing} illustrates, \textsc{E2E-R} scales exponentially better than \textsc{R+SLV} -- note that runtimes are in log-scale. This is due to the computational complexity of exact bound propagation through the probabilistic reasoning component, as shown in Section~\ref{exact_solution}. In the surrogate task of verifying the robustness of the CNN only, \textsc{Marabou}'s runtime is 314 seconds per sample, averaged across 100 MNIST test images. It is thus several orders of magnitude slower than our approach, in performing a subtask of NeSy verification. %Consider, for example, that for $\#\text{digits} = 6$, \textsc{E2E-R} verifies 6 copies of the CNN along with a large circuit 3 orders of magnitude faster than \textsc{Marabou} verifies 1 copy of the CNN. 
This indicative performance for Marabou aligns with theoretical~\cite{crownZhang2018} and empirical evidence~\cite{vnnlib2024} on the poor scalability of SMT-based approaches. Our results suggest that this trade-off between completeness and scalability is favourable in the NeSy setting, where the verification task may involve multiple NNs and complex reasoning components.
% While we do not compare Marabou on the actual task, we believe that this preliminary result, along with the well-established view that SMT-based approaches are bad [ref, ref], effectively highlights the strengths of our approach.


% \begin{itemize}
%     \item Our approach scales exponentially better than the abstract+solver baseline. We see that exact bound propagation in the symbolic component is computationally prohibitive, as reinforced by the theoretical results made in Section \ref{exact_solution}.
%     \item All experiments finish within the timeout besides 5 and 6 digits for epsilon $\epsilon = 0.01$ and for 6 digits for epsilon $\epsilon = 0.001$. In the former, the solver-based baseline doesn't finish any example within the timeout (hence the lines stop at 4 digits). Explain through bound looseness in a branch-and-bound-based solver. In the latter, the solver-based baseline runs for under 5\% of the examples. The reported values are the average runtime for the examples that ran. This can explain the fact that the orange line for our method doesn't change between 5 \& 6 digits.
%     \item We run Marabou on 100 MNIST images and takes 314 seconds to verify the CNN, and is thus is several orders of magnitude slower than our approach. Consider for example, that at 6 digits, our approach verifies 6 copies of the CNN along with a large circuit 3 orders of magnitude faster than Marabou verifies 1 copy of the CNN. While we do not manage to compare Marabou on the actual task, we believe that this preliminary result, along with the well-established view that SMT-based approaches are bad [ref, ref], effectively highlights the strengths of our approach.
% \end{itemize}


\paragraph{Quality of verification results.} We next investigate how the complexity of the reasoning component affects the quality of the verification results. In Table \ref{tab:mnist-perf} we report, for $\epsilon=0.001$: (a) the tightness of the output bounds, in the form of lower/upper bound intervals for the probability of the \textit{correct} sum for each sample, averaged across the test set; (b) the robustness of the NeSy system, defined as the the number of robust samples divided by the total samples in the test set.\footnote{We don't report metrics for 6 digits since the full experiment exceeds the timeout.}

As expected, \textsc{R+SLV} outputs strictly tighter bounds than \text{E2E-R} for all configurations. We further observe that the quality of the bounds obtained by \text{E2E-R} degrades as the size of the reasoning circuits increases. This is also expected, since errors compound and accumulate over the larger network. However, the differences between \textsc{R+SLV} and \text{E2E-R} are minimal, especially in terms of robustness.

% the intervals decrease as the number of digits increases because there are more output classes, so the probability of the correct class decreases.

% \begin{itemize}
%     \item In Table \ref{tab:mnist-perf} we see that, as expected, the abstract+solver baseline gives strictly tighter bounds for all digits. Further, we see that the quality of the bounds obtained by our method degrades as the circuit sizes increase. However, the differences with the solver-based baseline are minor, and especially so when comparing the robustness metric. This makes sense.
% \end{itemize}

% Second, we examine how increasing the number of digits affects the performance of each method, which we quantify via two proxy metrics. The first relates to the ``tightness'' of the computed output bounds, i.e. how close the lower/upper bounds are for the output layer of the system. We estimate this by recording the lower and upper bound for the probability of the \textit{correct} sum for each sample, and averaging across the test dataset. This allows us to compare the accuracy of each method with fine granularity. The second metric estimates the robustness of the system under a given method. The soundness of the methods tested, i.e. that they never predict a non-robust system to be robust, implies that reported robustness is an indicator of performance. We estimate this by recording whether the lower bound of the probability of the \textit{correct} sum is greater than 0.5 \footnote{To see that this underapproximates actual robustness, consider that a lower bound $> 0.5$ for the correct class entails that there is no perturbation for which any other class has probability larger than that of the correct class.}. We report a percentage equal to the number of ``robust'' samples divided by the total number of examples. The perturbation size used is $\epsilon=0.001$. The results are shown in Table \ref{tab:mnist-perf} \footnote{the bounds for 6 digits are not shown because the experiment exceeds the timeout.}.

% Naturally, bound tightness is a desirable characteristic for an accurate and reliable verification method.

% Reported robustness can serve as an indicator of performance due to the soundness of the methods tested.

% The soundness of the methods tested implies that higher reported robustness is better. 

% As all methods tested are sound, reported robustness serves as an indicator of performance, since disagreement on a given sample means that one method was unable to prove the robustness of a robust system.

% Since all methods tested are sound, the lack of false positives implies that higher reported robustness is better.

% For a given sample, we record (1) the lower and upper bound for the probability of the \textit{correct} sum and (2) whether that lower bound is greater than 0.5. 

% Regarding the former, we are concerned with the "tightness" of the bounds, i.e. how close the lower/upper bounds are. Naturally, bound tightness is a desirable characteristic for an accurate and reliable verification method. By averaging the lower/upper bound values across the test dataset, we obtain a proxy metric for this type of accuracy. 

% The latter is used as a proxy for the robustness of the system, 

% We report the average value of the lower and upper bounds across the test dataset. This metric thus serves as a proxy for a verification method's accuracy and reliability.

% The former allows us to quantify the "tightness" of the bounds, i.e. how close the lower/upper bounds are.

% Regarding the former, ideally we desire that these bounds are (1) high in magnitude, indicating high classification confidence, and (2) "tight", i.e. the lower/upper bounds are close, indicating a robust system. By averaging these bounds across the test dataset, we obtain a metric serving as a proxy for a method's accuracy of bound computation.

% Further, we record whether the lower bound of the correct sum is greater than 0.5. Note that this is an overapproximation of robustness, since a $> 0.5$ lower bound entails that there is no perturbation for which any other class has probability $> 0.5$. 
