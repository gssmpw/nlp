\onecolumn

% \twocolumn[
\begin{center}
    {\Large \textbf{Appendices}}
\end{center}
% \vspace{0.4cm}
% ]


% \section{Additional Related Work}

% \paratitle{Sequential recommendation.}

% \paratitle{Tokenization for non-text data.}

\begin{table*}[!t] %
    \small
	\caption{Notations and explanations.}
	\label{tab:notation}
	\vskip 0.1in
	\resizebox{\columnwidth}{!}{
	\begin{tabular}{cl}
		\toprule
		\textbf{Notation} & \textbf{Explaination}\\
		\midrule
		$i$, $i_1$, $i_j$ & item, item identifier, item ID\\
		$t$ & the number of actions in the input action sequence; the timestamp when the model makes a prediction\\
		$i_{t+1}$ & the ground-truth next item \\
		$\hat{i}_{t+1}$ & the predicted next item \\
		$S = \{i_1,i_2,\ldots,i_t\}$ & the action sequence where each action is represented with the interacted item ID \\
		$\mathcal{A}$, $\mathcal{A}_1$, $\mathcal{A}_j$ & a set of item features or tokens \\
		$m = |\mathcal{A}_j|$ & the number of features associated with each item \\
		$f_{j,k}$ & the $k$-th feature of item $i_j$ \\
		$\mathcal{F}_k$ & the collection of all possible choices for the $k$-th feature \\
		$S' = \{\mathcal{A}_1, \mathcal{A}_2, \ldots, \mathcal{A}_t\}$ & the action sequence where each action is represented with a set of item features \\
		$c$, $c_1$, $c_j$ & input \& generated tokens \\
		$l$ & the number of tokens in the token sequence \\
		$C = \{c_1, c_2, \ldots, c_l\}$ & the token sequence tokenized from the input action sequence $S'$ \\
		$\{c_{l+1}, \ldots, c_q\}$ & the tokens generated by the GR model \\
		$\mathcal{V}$ & vocabulary of ActionPiece tokenizer \\
		$\mathcal{R}$ & merge rules of ActionPiece tokenizer \\
		$\{(c_u, c_v) \to c_{\text{new}}\}$ & one merge rule indicating two adjacent tokens $c_u$ and $c_v$ can be replaced by a token $c_{\text{new}}$ \\
		$Q = |\mathcal{V}|$ & size of ActionPiece vocabulary \\
		$P(c, c')$ & probability that tokens $c$ and $c'$ are adjacent when flattening a sequence of sets into a token sequence \\
		$N$ & the number of action sequences in the training corpus \\
		$L$ & the average length of action sequences in the training corpus \\
		$H$ & Maximal heap size, $O(NLm)$\\
% 		$D$ & Average time a token appears in the training corpus, $O(\frac{NLm}{Q})$\\
		$q$ & The number of segmentations produced using set permutation regularization during inference\\
		\bottomrule
	\end{tabular}
	}
\end{table*}

\section{Notations}

We summarize the notations used in this paper in~\Cref{tab:notation}.

% Belows are unpolished by LLMs

\section{Algorithmic Details}

In this section, we provide detailed algorithms for vocabulary construction and segmentation.

\subsection{Vocabulary Construction Algorithm}


\begin{algorithm}[!t]
  \caption{ActionPiece Vocabulary Construction -- Count (\Cref{fig:weight})}
  \label{alg:vocab_construction_count}
\begin{algorithmic}[1]
\INPUT Action sequence corpus $\mathcal{S}'$, current vocabulary $\mathcal{V}$
   \OUTPUT Accumulated weighted token co-occurrences\ \ $\text{count}(\cdot, \cdot)$
   \FOR{$i \gets 0$ \textbf{to} $|\mathcal{V}|, j \gets 0$ \textbf{to} $|\mathcal{V}|$}
    \STATE $\text{count}(c_i, c_j) \gets 0$
   \ENDFOR
    \FORALL{sequence $S' \in \mathcal{S}'$}
    \STATE $t \gets \text{length}(S')$ \COMMENT{number of action nodes in sequence}
    \FOR{$k \gets 0$ \textbf{to} $t-1$}
        \STATE $\mathcal{A}_k \gets S'[k]$ \COMMENT{current action node}
        \STATE \COMMENT{Process all unordered token pairs within $\mathcal{A}_k$}
        \FORALL{$c_i, c_j \in \mathcal{A}_k, i\neq j$}
                \STATE $\text{count}(c_i, c_j) \gets \text{count}(c_i, c_j) + 2 / |\mathcal{A}_k|$ \COMMENT{weight of tokens within a single set (\Cref{eq:p_one_set})}
                \STATE $\text{count}(c_j, c_i) \gets \text{count}(c_j, c_i) + 2 / |\mathcal{A}_k|$ \COMMENT{symmetric update}
        \ENDFOR
        
        \COMMENT{Process all ordered token pairs between $A_k$ and $A_{k+1}$}
        \IF{$k < t-1$}
            \STATE $\mathcal{A}_{k+1} \gets S'[k+1]$
            \FORALL{$c_i \in \mathcal{A}_k, c_j \in \mathcal{A}_{k+1}$}
                \STATE $\text{count}(c_i, c_j) \gets \text{count}(c_i, c_j) + 1 / (|\mathcal{A}_k| \times |\mathcal{A}_{k+1}|)$ \COMMENT{weight of tokens from two adjacent sets (\Cref{eq:p_two_sets})}
            \ENDFOR
        \ENDIF
    \ENDFOR
\ENDFOR
\item[\textbf{return} $\text{count}(\cdot, \cdot)$]
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!t]
  \caption{ActionPiece Vocabulary Construction -- Update (\Cref{fig:update})}
  \label{alg:vocab_construction_update}
\begin{algorithmic}[1]
\INPUT Action sequence corpus $\mathcal{S}'$ before updating, current merge rule $\{(c_u, c_v) \to c_{\text{new}}\}$
\OUTPUT Updated action sequence corpus $\mathcal{S}'$
   \FORALL{sequence $S' \in \mathcal{S}'$}
        \STATE $t \gets \text{length}(S')$
        \FOR{$k \gets 0$ \textbf{to} $t-1$}
        \STATE $\mathcal{A}_k \gets S'[k]$
        \STATE \COMMENT{Merge tokens in one action node}
        \IF{$c_u \in \mathcal{A}_k$ \AND $c_v \in \mathcal{A}_k$}
          \STATE Replace $c_u$ and $c_v$ in $\mathcal{A}_k$ with $c_{\text{new}}$
        \ENDIF
        
        \STATE \COMMENT{Merge tokens from two adjacent nodes}
        \IF{$k < t-1$}
          \STATE $\mathcal{A}_{k+1} \gets S'[k+1]$
          \IF{$c_u \in \mathcal{A}_k$ \AND $c_v \in \mathcal{A}_{k+1}$}
            \IF{$\mathcal{A}_k, \mathcal{A}_{k+1}$ are both action nodes}
                \STATE Create intermediate node $M$ between $\mathcal{A}_k$ and $\mathcal{A}_{k+1}$
                \STATE $M \gets \{c_{\text{new}}\}$ \COMMENT{linked list: $\mathcal{A}_k \to M \to \mathcal{A}_{k+1}$}
                \STATE $\mathcal{A}_k \gets \mathcal{A}_k \setminus c_u$
                \STATE $\mathcal{A}_{k+1} \gets \mathcal{A}_{k+1} \setminus c_v$
            \ELSIF{$\mathcal{A}_k$ is intermediate node}
                \STATE $\mathcal{A}_k \gets \{c_{\text{new}}\}$
                \STATE $\mathcal{A}_{k+1} \gets \mathcal{A}_{k+1} \setminus c_v$
            \ELSIF{$\mathcal{A}_{k+1}$ is intermediate node}
                \STATE $\mathcal{A}_k \gets \mathcal{A}_k \setminus c_u$
                \STATE $\mathcal{A}_{k+1} \gets \{c_{\text{new}}\}$
            \ENDIF
          \ENDIF
        \ENDIF
        \ENDFOR
    \ENDFOR
\item[\textbf{return} $\mathcal{S}'$]
\end{algorithmic}
\end{algorithm}

The overall procedure for vocabulary construction is illustrated in~\Cref{alg:vocab_construction_overall}. As described in~\Cref{subsubsec:vocab_construct}, this process involves iterative \textbf{Count}~(\Cref{alg:vocab_construction_count}) and \textbf{Update}~(\Cref{alg:vocab_construction_update}) operations.


\subsection{Segmentation with Set Permutation Regularization Algorithm}

\begin{algorithm}[!t]
\caption{Segmentation via Set Permutation Regularization (SPR) (\Cref{subsubsec:segmentation})}
\label{alg:spr}
\begin{algorithmic}[1]
\INPUT Action sequence $S$, merge rules $\mathcal{R}$
\OUTPUT Segmented token sequences $C$
    \STATE $C \gets [\;]$ \COMMENT{initialize permuted initial token sequence}
    \FORALL{token set $\mathcal{A}_i \in S$}
        \STATE Generate random permutation of $\mathcal{A}_i$ as $[c_1, c_2, \dots, c_{|\mathcal{A}_i|}]$
        \STATE Extend $C$ with $[c_1, c_2, \dots, c_{|\mathcal{A}_i|}]$ \COMMENT{concatenate permutations}
    \ENDFOR
    \STATE
    \STATE \COMMENT{Apply BPE~\cite{sennrich2016bpe} segmentation on permuted sequence}
    \REPEAT
        \STATE $\mathcal{R}' \gets \emptyset$ \COMMENT{candidate merge rules}
        \FOR{$i \gets 0$ \textbf{to} $|C| - 1$}
            \IF{$\{(c_i, c_{i+1}) \to c'\} \in \mathcal{R}$}
                \STATE $\mathcal{R}' \gets \mathcal{R}' \cup \{(c_i, c_{i+1}) \to c'\}$
            \ENDIF
        \ENDFOR
        \STATE Select $\{(c_k, c_{k+1}) \to c'\} \in \mathcal{R}'$ with the smallest index among all merge rules $\mathcal{R}$
        \STATE $C\gets [c_1, \dots, c_{k-1}, c', c_{k+2}, \dots]$ \COMMENT{replace $(c_k, c_{k+1})$ with a new token $c'$}
    \UNTIL{$\mathcal{R}'$ is $\emptyset$}
\item[\textbf{return} $C$]
\end{algorithmic}
\end{algorithm}

The detailed algorithm for segmenting action sequences into token sequences using set permutation regularization (SPR) is shown in~\Cref{alg:spr}. In practice, we often run~\Cref{alg:spr} multiple times to augment the training corpus or ensemble recommendation outputs, as described in~\Cref{subsubsec:training,subsubsec:inference}.


\section{Efficient Vocabulary Construction Implementation}\label{sec:vocab_construct_algo}

\begin{figure}[!t]
\lstinputlisting[language=Python]{fig/train_step.py}
\caption{Pseudocode for a single iteration of the efficient vocabulary construction algorithm, illustrating how a max-heap with lazy updates is used to track and merge frequent token pairs.} 
\label{fig:torch_implementation}
\end{figure}

To efficiently construct the ActionPiece vocabulary, we propose using data structures such as heaps with a lazy update trick, linked lists, and inverted indices to speed up each iteration of the construction process. The key idea is to avoid recalculating token co-occurrences in every iteration and instead update the data structures. The pseudocode is shown in~\Cref{fig:torch_implementation}.

\subsection{Data Structures}

The data structures used in the proposed algorithm are carefully designed to optimize the efficiency of vocabulary construction. Here is a detailed discussion of their roles and implementations:
\begin{itemize}
    \item \textbf{Linked list:}  
    Each action sequence in the training corpus is stored as a linked list. This allows efficient local updates during token merging. When a token pair $(c_u, c_v)$ is replaced by a new token $c_{new}$, only the affected nodes and their neighbors in the linked list need to be modified (as shown in~\Cref{alg:vocab_construction_update,fig:update}).
    \item \textbf{Heap with lazy update trick:}  
    A max-heap prioritizes token pairs by their co-occurrences. Instead of recalculating the heap entirely in each iteration, a ``lazy update'' strategy is employed: outdated entries (with mismatched co-occurrence counts) are retained but skipped during extraction. In the pseudocode, the loop checks if the top element is outdated via \texttt{is\_outdated}. Invalid entries are discarded, and only valid ones are processed. Updated co-occurrences are pushed as new entries (with negative counts for max-heap emulation).
    \item \textbf{Inverted indices:}  
    The \texttt{pair2head} dictionary maps token pairs to the sequences containing them. When a pair $(c_u, c_v)$ is merged, the algorithm directly retrieves affected sequence IDs via \texttt{pair2head[(c\_u, c\_v)]}, avoiding a full corpus scan. After merging, the inverted indices are incrementally updated: new token pairs (\eg $(c_{prev}, c_{new})$ and $(c_{new}, c_{next})$) are added to \texttt{pair2head}, while obsolete pairs are removed. This enables targeted updates and ensures subsequent iterations efficiently access relevant sequences.
\end{itemize}
These structures collectively reduce time complexity by focusing computation on dynamically changing parts of the corpus and avoiding redundant global operations. The linked list enables localized edits, the heap minimizes priority recalculation, and the inverted indices eliminate brute-force searches, making the algorithm scalable to large corpora.

\subsection{Time Complexity}

The time complexity of the efficient vocabulary construction algorithm can be analyzed through two main components: \textbf{initialization} and \textbf{iterative merging}.

\begin{itemize}  
   \item \textbf{Initialization phase}  
   involves building the initial max-heap to track co-occurrence frequencies. Given \(N\) input sequences (each with an average length of \(L\)), we count co-occurrences for all \(O(m^2)\) token pairs within each set of size \(m\). This requires \(O(NLm^2)\) time.  

   \item \textbf{Iterative merging phase}
   dynamically processes the involved sequences. The total number of such sequences across all iterations is approximately  
   \[
   O\left(\frac{N}{|\mathcal{V}_0|}\right) + O\left(\frac{N}{|\mathcal{V}_0| + 1}\right) + \dots + O\left(\frac{N}{Q}\right) \simeq O(\log{Q}N).
   \]  
   For each sequence, updating the linked list requires \(O(Lm)\) time, counting co-occurrences takes \(O(Lm^2)\) time, and inserting co-occurrences into the max-heap requires at most \(O(Lm^2\log{H})\) time. Here, \(H\) represents the heap size, which is at most \(O(NLm)\). Thus, the overall time complexity for iterative merging is  
   \[
   O(\log{Q}N(Lm + Lm^2 + Lm^2\log{H})) = O(\log{Q}\log{H} \cdot NLm^2).
   \]  
\end{itemize}  

Therefore, the overall time complexity of our proposed vocabulary construction algorithm is \(O(\log{Q}\log{H} \cdot NLm^2)\), where the iterative merging phase dominates. This complexity is significantly better than the naive vocabulary construction complexity of \(O(QNLm^2)\).  

\section{Discussion: Comparison Between ActionPiece and BPE}

\begin{table}[!t]
\small
\centering
\caption{Comparison between ActionPiece and BPE.}
\vskip 0.1in
\label{tab:comparison}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{BPE} & \textbf{ActionPiece} \\
\midrule
\textbf{Data Type} & text sequences & action (unordered feature set) sequences \\
\textbf{Token} & a byte sequence & a feature set \\
\textbf{Initial Vocabulary} & single bytes & single-feature sets \\
\textbf{Merging Unit} & adjacent byte pairs & feature pairs within one set or between adjacent sets \\
% \textbf{Context Handling} & Local character adjacency & Explicit cross-action context modeling \\
\textbf{Co-occurrence Weighting} & raw frequency counting & probabilistic weighting~(\Cref{fig:weight}) \\
\textbf{Segmentation Strategy} & greedy fixed-order merging & set permutation regularization~(\Cref{alg:spr}) \\
\textbf{Intermediate Structures} & N/A & intermediate nodes for cross-action merges \\
% \textbf{Time Complexity} & O(NL) with full recomputing & O(logQ logH·NLm²) with incremental updates \\
% \textbf{Set Unorderedness} & N/A (ordered text) & Explicit set permutation invariance \\
\bottomrule
\end{tabular}
\end{table}

While ActionPiece follows a similar algorithmic framework as BPE, its design is fundamentally different because it is tailored for tokenizing action sequences. To clarify, we summarize the key differences in~\Cref{tab:comparison}.

\section{Datasets}\label{app:datasets}

\begin{table*}[!t] %
    \small
    \centering
	\caption{Statistics of the processed datasets. ``Avg.~$t$'' denotes the average number of actions in an action sequence.}
	\label{tab:dataset}
	\vskip 0.1in
% 	\resizebox{\columnwidth}{!}{
	\begin{tabular}{c@{\hspace{0.5in}}r@{\hspace{0.5in}}r@{\hspace{0.5in}}r@{\hspace{0.5in}}r}
		\toprule
		\textbf{Datasets} & \textbf{\#Users} & \textbf{\#Items} & \textbf{\#Actions} & \textbf{Avg.~$t$}\\
		\midrule
		\textbf{Sports}  & 18,357            & 35,598           & 260,739            & 8.32 \\
		\textbf{Beauty}  & 22,363            & 12,101           & 176,139            & 8.87 \\
% 		\textbf{Toys}    & 19,412            & 11,924           & 148,185            & 8.63 \\
		\textbf{CDs}     & 75,258            & 64,443           & 1,022,334          & 14.58 \\
% 		\textbf{ML-1M} \\
		\bottomrule
	\end{tabular}
% 	}
    % \vskip -0.3in
\end{table*}

\textbf{Categories.} Among all the datasets, ``Sports'' and ``Beauty'' are two widely used benchmarks for evaluating generative recommendation models~\cite{rajput2023tiger,jin2024lmindexer,hua2023p5cid}. We conduct experiments on these benchmarks to ensure fair comparisons with existing results. Additionally, we introduce ``CDs'', which contains about $4\times$ more interactions than ``Sports'', making it a larger dataset for evaluating the scalability of GR models. For ``CDs'', we apply the same data processing strategy as the public benchmarks. The statistics of the processed datasets are shown in~\Cref{tab:dataset}.

\textbf{Sequence truncation length.} Following \citet{rajput2023tiger}, we filter out users with fewer than $5$ reviews and truncate action sequences to a maximum length of $20$ for ``Generative'' methods, including ActionPiece. For ``ID-based'' and ``Feature + ID'' baselines, we set the maximum length to $50$, as suggested in their original papers.

\textbf{Item text features.} Following~\citet{rajput2023tiger,zheng2024lcrec,sheng2025alpharec}, the first step for feature engineering is to combine multiple raw text features into a single sentence for each item. Then, we use a pretrained sentence embedding model to encode this sentence into a vector representation. In all our implementations, we concatenate \emph{title}, \emph{price}, \emph{brand}, \emph{feature}, \emph{categories}, and \emph{description}, and use \texttt{sentence-t5-base}~\cite{ni2022sentencet5} as the sentence embedding model.
\begin{itemize}
\item The encoded sentence embeddings of $768$ dimension are directly used as textual item representations for UniSRec.
\item We quantize the sentence embeddings using residual quantization (RQ)~\cite{rajput2023tiger,zeghidour2021rqvae,zheng2024enhancing} into three codes, each with $256$ candidates. To prevent conflicts, we add an extra identification code. These four codes together serve as the RQ-based semantic IDs for TIGER and SPM-SID.
\item For other baselines that require item features, such as FDSA, S$^3$-Rec, VQ-Rec, HSTU, and our method, we follow~\citet{hou2023vqrec} and quantize the sentence embeddings using optimized product quantization (OPQ)~\cite{ge2013opq}. Except for VQ-Rec, where the sentence embeddings are quantized into $32$ codes as suggested in the original paper, we quantize the sentence embeddings into $4$ codes for all other methods to ensure a fair comparison. The codebook size is $256$ for each digit of code. For generative methods HSTU and ActionPiece, we also include an additional identification code to prevent conflicts. Note that, unlike RQ-based semantic IDs, features produced by product/vector quantization do not require a specific order.
\end{itemize}

\section{Baselines}\label{appendix:baselines}

We compare ActionPiece with the following representative baselines:

\subsection{ID-Based Sequential Recommendation Methods}

\begin{itemize}
    \item \textbf{SASRec}~\cite{kang2018sasrec} represents each item using its unique item ID. It encodes item ID sequences with a self-attentive Transformer decoder. The model is trained by optimizing a binary cross-entropy objective.
    \item \textbf{BERT4Rec}~\cite{sun2019bert4rec} also represents each item using its unique item ID. Unlike SASRec, it encodes sequences of item IDs with a bidirectional Transformer encoder. The model is trained using a masked prediction objective.
\end{itemize}

\subsection{Feature-Enhanced Sequential Recommendation Methods}

\begin{itemize}
    \item \textbf{FDSA}~\cite{zhang2019fdsa} integrates item feature embeddings with vanilla attention layers to obtain feature representations. It then processes item ID sequences and feature sequences separately through self-attention blocks.
    \item \textbf{S$^3$-Rec}~\cite{zhou2020s3} first employs self-supervised pre-training to capture the correlations between item features and item IDs. Then the checkpoints are loaded and fine-tuned for next-item prediction, using only item IDs.
    \item \textbf{VQ-Rec}~\cite{hou2023vqrec} encodes text features into dense vectors using pre-trained language models. It then applies product quantization to convert these dense vectors into semantic IDs. The semantic ID embeddings are pooled together to represent each item. Since the experiments are not performed in a transfer learning setting, we omit the two-stage training strategy outlined in the original paper. Instead, we reuse the model architecture and train it from scratch using an in-batch contrastive loss with a batch size of $256$.
\end{itemize}

\subsection{Generative Recommendation Methods}

Each generative recommendation baseline corresponds to an action tokenization method described in~\Cref{tab:act_tokenization}.

\begin{itemize}
    \item \textbf{P5-CID}~\cite{hua2023p5cid} is an extension of P5~\cite{geng2022p5}, which formulates recommendation tasks in a text-to-text format. Building on P5, the authors explored several tokenization methods to index items for better recommendations. In this study, we use P5-CID as a representative hierarchical clustering-based action tokenization method. It organizes the eigenvectors of the Laplacian matrix of user-item interactions into a hierarchy and assigns cluster IDs at each level as item indices. When implementing this baseline method, we adopt the same model backbone as ActionPiece (encoder-decoder Transformers trained from scratch) and use the indices  produced by P5-CID.
    \item \textbf{TIGER}~\cite{rajput2023tiger} encodes text features similarly to VQ-Rec but quantizes them into semantic IDs using RQ-VAE. The model is then trained to autoregressively predict the next semantic ID and employs beam search for inference. We use a beam size of $50$ in beam search to generate the top-$K$ recommendations.
    \item \textbf{LMIndexer}~\cite{jin2024lmindexer} takes text as input and predicts semantic IDs. The text description of each item is first tokenized using a text tokenizer. The resulting text tokens are then concatenated to form input action sequences. The model is trained with self-supervised objectives to learn the semantic IDs of target items. The reported results in~\Cref{tab:performance} are taken from the original paper. We do not report the results of LMIndexer on the large dataset ``CDs'' because it does not converge under similar computing budget as the other methods.
    \item \textbf{HSTU}~\cite{zhai2024hstu} discretizes raw item features into tokens, treating them as input tokens for generative recommendation. The authors also propose a lightweight Transformer layer that improves both performance and efficiency. For action tokenization, we use the same item features as our method and arrange them in a specific order to form the tokenized tokens of each item.
    \item \textbf{SPM-SID}~\cite{singh2024spmsid} first tokenizes each item into semantic IDs. It then uses the SentencePiece model (SPM)~\cite{kudo2018sentencepiece} to merge important semantic ID patterns within each item into new tokens in the vocabulary. While the original paper introduces this method for ranking models, we adapt it for the generative recommendation task. Specifically, we concatenate the SPM tokens as inputs, feed them into the T5 model, and autoregressively generate SPM tokens as recommendations.
\end{itemize}

\begin{table}[!t]
\centering
\small
\caption{Hyperparameter settings of ActionPiece for each dataset.}
\label{tab:reproduction}
\vskip 0.1in
\begin{tabular}{l@{\hspace{0.5in}}c@{\hspace{0.5in}}c@{\hspace{0.5in}}c}
\toprule
\textbf{Hyperparameter} & \textbf{Sports} & \textbf{Beauty} & \textbf{CDs} \\
\midrule
learning\_rate & 0.005 & 0.001 & 0.001 \\
warmup\_steps & 10,000 & 10,000 & 10,000 \\
dropout\_rate & 0.1 & 0.1 & 0.1 \\
weight\_decay & 0.15 & 0.15 & 0.07 \\
vocabulary\_size & 40,000 & 40,000 & 40,000 \\
n\_inference\_segments & 5 & 5 & 5 \\
beam\_size & 50 & 50 & 50 \\
num\_layers & 4 & 4 & 4 \\
d\_model & 128 & 128 & 256 \\
d\_ff & 1,024 & 1,024 & 2,048 \\
num\_heads & 6 & 6 & 6 \\
d\_kv & 64 & 64 & 64 \\
optimizer & adamw & adamw & adamw \\
lr\_scheduler & cosine & cosine & cosine \\
train\_batch\_size & 256 & 256 & 256 \\
max\_epochs & 200 & 200 & 200 \\
early\_stop\_patience & 20 & 20 & 20 \\
\bottomrule
\end{tabular}
\end{table}

\section{Implementation Details}\label{appendix:implementation}

\textbf{Baselines.} The results of BERT4Rec, SASRec, FDSA, S$^3$-Rec, TIGER, and LMIndexer on the ``Sports'' and ``Beauty'' benchmarks are taken directly from existing papers~\cite{zhou2020s3,rajput2023tiger,jin2024lmindexer}. For other results, we carefully implement the baselines and tune hyperparameters according to the suggestions in their original papers. We implement BERT4Rec, SASRec, FDSA, and S$^3$-Rec using the open-source recommendation 
library RecBole~\cite{zhao2021recbole}. For other methods, we implement them ourselves with HuggingFace Transformers~\cite{wolf2020transformers} and PyTorch~\cite{paszke2019pytorch}. We use FAISS~\cite{douze2024faiss} to quantize sentence representations.
% By adjusting embedding dimensions, linear layer shapes, \emph{etc.}, we aim to keep the model parameters of all compared methods similar. This ensures a fair comparison from the perspective of scaling laws~\cite{kaplan2020scaling}.

\textbf{ActionPiece.} We use an encoder-decoder Transformer architecture similar to T5~\cite{raffel2020t5}. We use four layers for both the encoder and decoder. The multi-head attention module has six heads, each with a dimension of $64$. For the public benchmarks ``Sports'' and ``Beauty'', we follow~\citet{rajput2023tiger} and set the token embedding dimension to $128$ and the intermediate feed-forward layer dimension to $1024$. This results in a total of 4.46M non-embedding parameters. For the larger ``CDs’’ dataset, we use a token embedding dimension of $256$ and an intermediate feed-forward layer dimension of $2048$, leading to 13.11M non-embedding parameters. For model inference, we use beam search with a beam size of $50$. Note that the baselines P5-CID, TIGER, and SPM-SID use the same model architecture, differing only in their action tokenization methods. For ActionPiece-specific hyperparameters, we set the number of segmentations produced using set permutation regularization during inference to $q = 5$. We tune the vocabulary size in $\{5k, 10k, 20k, 30k, 40k\}$.

\textbf{Training.} We train the GR models from scratch for up to $200$ epochs, using early stopping if the model does not achieve a better NDCG@$10$ on the validation set for $20$ consecutive epochs. The training batch size is set to $256$. The learning rate is selected from $\{1\times 10^{-3}, 3\times 10^{-3}, 5\times 10^{-3}\}$ with a warmup step of $10{,}000$. We use a dropout rate of $0.1$ and tune the weight decay from $\{0.07, 0.1, 0.15, 0.2\}$. For all methods implemented by us, we conduct five repeated experiments using random seeds $\{2024, 2025, 2026, 2027, 2028\}$. The model checkpoints with the best average NDCG@$10$ on the validation set are selected for evaluation on the test set, and we report these results. Each model is trained on a single $40$G NVIDIA A100 GPU.


\section{Reproduction}

% \textbf{We will release the code to reproduce our results after the review phase.} 
To improve reproducibility, we provide the algorithms of vocabulary construction and segmentation processes in~\Cref{alg:vocab_construction_overall,alg:vocab_construction_count,alg:vocab_construction_update,alg:spr}. We also provide the pseudocode for the efficient vocabulary construction implementation in~\Cref{fig:torch_implementation}. In addition, we provide the best hyperparameters of ActionPiece for all experimental datasets in~\Cref{tab:reproduction}.




