\section{Related Work}

\textbf{Generative recommendation.} Conventional sequential recommendation models 
% are typically designed as retrieval models~\cite{hidasi2016gru4rec,kang2018sasrec}. 
% Given a sequence of interacted items (also referred to as actions), the models retrieve item representations closest to the sequence representations. 
% However, this approach 
often relies on large embedding tables to store representations for all items, leading to significant engineering and optimization challenges~\cite{hidasi2016gru4rec,kang2018sasrec}.
% , a recently emerging paradigm, 
Generative recommendation~\cite{deldjoo2024review,rajput2023tiger,zheng2024lcrec} addresses these issues by tokenizing each item as tokens from a shared table. 
% Given a sequence of tokens representing the historical actions of one user, models autoregressively generate the next tokens as recommendations,
% . This approach allows recommender systems to leverage advancements in generative models,
By autoregressively generating the next tokens as recommendations, this generative paradigm offers benefits such as memory efficiency~\cite{rajput2023tiger,yang2024liger,ding2024specgr}, scalability~\cite{zhai2024hstu,liu2024mbgen}, and easier alignment with LLMs~\cite{zheng2024lcrec,jin2024lmindexer,tan2024idgenrec,li2025semantic}.
Existing research has developed different action tokenization techniques, such as hierarchical clustering~\cite{hua2023p5cid,si2024seater}, quantization~\cite{rajput2023tiger,wang2024letter,zhu2024cost}, or jointly training with recommendation models~\cite{liu2024etegrec}. Other works incorporate additional modalities like collaborative filtering~\cite{petrov2023gptrec,wang2024eager,wang2024colarec,liu2024mbgen}, text~\cite{zheng2024lcrec,jin2024lmindexer,hou2024llmrank,zhang2025instructrec}, and vision signals~\cite{liu2024mmgrec}. However, current methods tokenize each action independently, ignoring the surrounding context.
In this work, we propose the first context-aware action tokenization method, where the same actions are tokenized differently in different action sequences.
% This explicitly incorporates contextual information into the tokenization process.

\textbf{Tokenization for language modeling.} Tokenization is the process of transforming raw text into discrete token sequences~\cite{kudo2018sentencepiece}.
% While it is feasible to train byte-level language models~\cite{xue2022byt5,wang2024mambabyte}, representing multiple bytes with a single token achieves better compression. 
Early word-level methods are context-independent and struggle to tokenize out-of-vocabulary words~\cite{sutskever2014seq2seq,bahdanau2015word}. Consequently, subword-level tokenization has gradually become the more mainstream choice.
% methods have become the standard approach. 
The vocabularies of these subword-level tokenizers are constructed iteratively, either bottom-up (starting with a small vocabulary and merging commonly occurring token pairs as new tokens)~\cite{wu2016wordpiece,sennrich2016bpe}, or top-down (starting with a large vocabulary and pruning tokens to minimize likelihood decrease)~\cite{kudo2018unigram,yehezkel2023sage}. Once the vocabulary is built, the text can be segmented either using the same method employed during vocabulary construction or based on additional objectives~\cite{he2020dpe,provilkov2020bpedropout,hofmann2022flota,schmidt2024pathpiece}. As an analogy, existing action tokenizers are context-independent and function like ``word-level'' language tokenizers. In this work, we take the first step toward context-aware subaction-level action tokenizer.
% ation for action sequences.
