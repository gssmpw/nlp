\section{Introduction}

Generative recommendation (GR)~\cite{geng2022p5,rajput2023tiger,zheng2024lcrec,zhai2024hstu} is an emerging paradigm for the sequential recommendation task~\cite{hidasi2016gru4rec,kang2018sasrec}. By tokenizing the user actions (typically represented by the interacted items) into discrete tokens, GR models learn to autoregressively generate tokens, which are then parsed into recommended items.
These tokens share a compact vocabulary that does not scale with the item pool size, improving model scalability, memory efficiency, and recommendation performance~\cite{rajput2023tiger,zhai2024hstu}.
% Since the tokens share a compact dictionary that is not proportional to the size of the item pool, GR models can easily scale up and deliver strong performance~\cite{}.
% Evolved from generative retrieval~\cite{}, generative recommendation differs by taking \emph{action sequences} as input. 
The input action sequence is vital in understanding user intentions~\cite{li2017narm,kang2018sasrec}, which organizes a user's historical interactions in chronological order. The same action (\eg purchasing the same item) may have different meanings in different action sequences. Evidence of taking a certain action can be found in the context, such as whether other items in the sequence share the same brand, color tone, or price range~\cite{zhang2019fdsa,zhou2020s3,hou2022unisrec,hou2023vqrec,yuan2023go}.

\begin{figure}[!t]
    \begin{center}
    \includegraphics[width=\columnwidth]{fig/case.pdf}
    % \vspace{-3cm}
    \vskip -0.05in
    \caption{Illustration of the tokenization process of ActionPiece. Each action is represented as an unordered feature set. 
    This figure presents two possible tokenized sequences. The same action can be tokenized into different tokens depending on the surrounding context. A detailed case study can be found in~\Cref{subsec:case}.
    % when applying set permutation regularization (Segment 1 and 2), along with their corresponding token sequences (Token Sequence 1 and 2).
    }
    \label{fig:case}
    \end{center}
    \vskip -0.2in
\end{figure}

% Existing methods model such contextual relations among actions by training a sequence encoder, hoping it generalizes well on new action sequences.
% model parameters, such as self-attention mechanism~\cite{},
% because the data itself contains no contextual information.
Despite the importance of contextual relations among actions, existing methods tokenize each action independently of its context (summarized in ~\Cref{tab:act_tokenization}).
The typical pipeline for tokenizing action sequences involves two steps: (1) Tokenizing each action/item individually into a pattern of tokens; (2) Replacing each action in the input sequence with its corresponding token pattern.
In this way, the tokens do not explicitly contain the context. Instead, they solely rely on the autoregressive model's parameters being well-trained to generalize effectively in understanding the context,
% of unseen inputs, 
which challenges the capabilities of GR models.
As a comparison, tokenization in language modeling also originates from context-independent methods, such as word-level tokenization~\cite{sutskever2014seq2seq,bahdanau2015word}. A decade of progress has led to most tokenization methods for modern large language models (LLMs)~\cite{openai2022chatgpt,google2023gemini,touvron2023llama,zhao2023survey} adopting context-aware approaches, including BPE~\cite{sennrich2016bpe} and Unigram tokenization~\cite{kudo2018unigram}, which tokenize the same characters along with their adjacent context into different tokens.
% As a comparison, popular tokenization techniques in language modeling, such as unigram tokenization~\cite{kudo2018unigram} and BPE~\cite{sennrich2016bpe}, tokenize the same characters along with their adjacent context into different tokens. Recently, we have also observed a trend towards memorizing more character contexts into tokens (\eg a vocabulary of 128K in LLaMA 3~\cite{meta2024llama3}), which contributes to the success of large language models~\cite{openai2022chatgpt,zhao2023survey,touvron2023llama,rae2021gopher,hoffmann2022chinchilla,google2023gemini}.

In this work, we aim to make the first step towards context-aware tokenization for modeling \emph{action sequences}. In analogy to how characters or bytes serve as the basic units in language modeling, we consider the associated features of an item as initial tokens.
% the most fundamental units/tokens. 
The idea is to iteratively find the most commonly co-occurring pairs of tokens among the training action sequences, then merge them into new tokens to represent segments of context. However, it's non-trivial to achieve this. Unlike text, where characters naturally form a sequence, the features associated with an action form an unordered set~\cite{zhang2019fdsa,zhou2020s3}. Thus, the proposed tokenization algorithm should be applied on \emph{sequences of token sets}.
% There are two types of co-occurring relationships between tokens. The first occurs between two different features of one item, and the second occurs between the features of two adjacent items in an action sequence. 
We need to carefully consider which pairs of tokens should be counted, whether within a single set or between two adjacent sets, and how much weight should be given to these different types of relationships.
% , how to account for these two types of relationships. 

% To this end, we propose \textbf{ActionPiece} tokenizer, by applying which, the same actions can be tokenized into different tokens based on the surrounding context (adjacent actions). 
To this end, we propose \textbf{ActionPiece}, which enables the same actions to be tokenized into different tokens based on their surrounding context.
% , a context-aware action sequence tokenization approach.
% which stands for \textbf{C}ontextual \textbf{A}ction \textbf{T}okenization. 
% To train an ActionPiece tokenizer,
% on a corpus of action sequences, 
\textbf{(1) Vocabulary construction}:
We first initialize the vocabulary to include every unique feature
% present in the item pool 
as initial tokens. The vocabulary is then constructed by iteratively learning merge rules. Each merge rule specifies that a pair of tokens can be merged into a new token.
% with a certain priority. 
In each iteration, we enumerate the training corpus to count the co-occurrence of existing tokens. 
Considering the structural differences between token pairs, \eg whether they occur within a single set or between two adjacent sets, we assign different weights to different pairs during the counting process.
% Considering the structure of the action sequences, Every pair of features, whether within a single set or between two adjacent sets, is assigned a weighted value based on the expected transition probabilities.
% , but with different weights. The assigned weight is determined by the expected transition probability between two tokens when we 
% that the pair will appear when we randomly flatten each set in the action sequence into a token sequence. We refer to this random flattening operation as \emph{random walk augmentation}.
% which can be applied to tokenize action sequences for model training and inference when we have already trained a CAT tokenizer. 
% The augmentation can be applied during both GR model training and inference, once the CAT tokenizer is trained.
% It improves the utilization rate of infrequently used tokens, enabling better generalization of generative recommendation models.
% By applying ActionPiece, the same actions can be tokenized into different tokens based on the surrounding context (adjacent actions).
% In addition to explicitly considering context during tokenization, ActionPiece offers other two merits: (1) Item features are used in an order-agnostic manner, treated as a set. In contrast, existing action tokenization methods like RQ-VAE~\cite{rajput2023tiger,zheng2024lcrec,liu2024mbgen} require features to be arranged in a specific order before being fed into the text encoder; (2) CAT operates directly on raw features. In contrast, existing methods first convert item features into text or embedding vectors, which leads to unavoidable information loss. 
% Another merit of ActionPiece is its ability to use item features in an order-agnostic manner. 
\textbf{(2) Segmentation}:
% Once the ActionPiece tokenizer is well-trained, 
The next step is to segment action sequences into token sequences for GR model training and inference. To fully exploit the unordered nature of the feature set for each action, we introduce set permutation regularization. By randomly permuting the features within each set, 
% the same actions can be tokenized into different tokens depending on the surrounding context. Consequently,
we can produce multiple token sequences of a single action sequence that preserve the same semantics. These variations act as natural augmentations for training data and enable inherent ensembling during model inference.

% Extensive experiments on public datasets show that ActionPiece consistently outperforms existing action tokenization methods. By adjusting the vocabulary size, ActionPiece effectively balances memory efficiency and recommendation performance~(\cref{fig:intro}).

\begin{table}[!t]
    \small
    \centering
    \caption{Comparison of different action tokenization methods for generative recommendation.
    % ``Action level'' denotes that each action is individually tokenized. ``Sequence level'' denotes that each action sequence is treated as a whole for tokenization. 
    ``Contextual'' denotes whether the same actions can be tokenized into different tokens based on the surrounding context.
    ``Unordered'' denotes whether the item features or semantic IDs are used in an order-agnostic manner.}
    \label{tab:act_tokenization}
    \vskip 0.1in
    \resizebox{\columnwidth}{!}{
    \setlength{\tabcolsep}{0.8mm}{
    \begin{tabular}{cccc}
        \toprule
        \textbf{Action Tokenization} & \textbf{Example} & \textbf{Contextual} & \textbf{Unordered} \\
        \midrule
        % Text Tokenizer & P5~\cite{geng2022p5} & Sequence & \XSolidBrush \\
        Product Quantization & VQ-Rec~\cite{hou2023vqrec} & \XSolidBrush & \CheckmarkBold \\
        Hierarchical Clustering & P5-CID~\cite{hua2023p5cid} & \XSolidBrush & \XSolidBrush \\
        Residual Quantization & TIGER~\cite{rajput2023tiger} & \XSolidBrush & \XSolidBrush \\
        Text Tokenization & LMIndexer~\cite{jin2024lmindexer} & \XSolidBrush & \XSolidBrush \\
        Raw Features & HSTU~\cite{zhai2024hstu} & \XSolidBrush & \XSolidBrush \\
        SentencePiece & SPM-SID~\cite{singh2024spmsid} & \XSolidBrush & \XSolidBrush \\
        \midrule
        ActionPiece & Ours & \CheckmarkBold & \CheckmarkBold \\
        \bottomrule
    \end{tabular}
    }}
    % \vskip -0.3in
    \vspace{-0.3in}
\end{table}

% To demonstrate the effectiveness of ActionPiece, we conduct extensive experiments on three public datasets, including two benchmarks and one large-scale dataset.
% Empirical results show that ActionPiece achieves the best among the compared methods for both performance and efficiency.

% , and one dataset that favors longer action sequences as inputs. 
% By controlling the vocabulary size, one can also balance efficiency and effectiveness.
% , as well as the trade-off between generalization and memorization.
