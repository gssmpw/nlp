\section{Experiments}

\subsection{Experimental Setup}

\textbf{Datasets.} We use three categories from the Amazon Reviews dataset~\cite{mcauley2015image} for our experiments: ``Sports and Outdoors'' (\textbf{Sports}), ``Beauty'' (\textbf{Beauty}), and ``CDs and Vinyl'' (\textbf{CDs}). Each user’s historical reviews are considered ``actions'' and are sorted chronologically as action sequences, with earlier reviews appearing first. To evaluate the models, we adopt the widely used leave-last-out protocol~\cite{kang2018sasrec,zhao2022revisiting,rajput2023tiger}, where the last item and second-to-last item in each action sequence are used for testing and validation, respectively. More details about the datasets can be found in~\Cref{app:datasets}.

\textbf{Compared methods.} We compare the performance of ActionPiece with the following methods: (1)~ID-based sequential recommendation methods, including BERT4Rec~\cite{sun2019bert4rec}, and SASRec~\cite{kang2018sasrec}; (2)~feature-enhanced sequential recommendation methods, such as FDSA~\cite{zhang2019fdsa}, S$^3$-Rec~\cite{zhou2020s3}, and VQ-Rec~\cite{hou2023vqrec}; and (3)~generative recommendation methods, including P5-CID~\cite{hua2023p5cid}, TIGER~\cite{rajput2023tiger}, LMIndexer~\cite{jin2024lmindexer}, HSTU~\cite{zhai2024hstu}, and SPM-SID~\cite{singh2024spmsid}, each representing a different action tokenization method (\Cref{tab:act_tokenization}). A detailed description of these baselines is provided in~\Cref{appendix:baselines}.

\textbf{Evaluation settings.} Following~\citet{rajput2023tiger}, we use Recall@$K$ and NDCG@$K$ as metrics to evaluate the methods, where $K \in \{5, 10\}$. Model checkpoints with the best performance on the validation set are used for evaluation on the test set. We run the experiments with five random seeds and report the average metrics.

\textbf{Implementation details.} Please refer to~\Cref{appendix:implementation} for detailed implementation and hyperparameter settings.


\subsection{Overall Performance}

We compare ActionPiece with sequential recommendation and generative recommendation baselines, which use various action tokenization methods, across three public datasets. The results are shown in~\Cref{tab:performance}. 

For the compared methods, we observe that those using item features generally outperform item ID-only methods. This indicates that incorporating features enhances recommendation performance. Among the methods leveraging item features (``Feature + ID'' and ``Generative''), generative recommendation models achieve better performance. These results further confirm that injecting semantics into item indexing and optimizing at a sub-item level enables generative models to better use semantic information and improve recommendation performance. Among all the baselines, SPM-SID achieves the best results. By incorporating the SentencePiece model~\cite{kudo2018sentencepiece}, SPM-SID replaces popular semantic ID patterns within each item with new tokens, benefiting from a larger vocabulary.

\begin{table}[t!]
    \small
    \centering
	\caption{Ablation analysis of ActionPiece. The recommendation performance is measured using NDCG@$10$. The best performance is denoted in \textbf{bold} fonts.}
	\label{tab:ablation}
	\vskip 0.1in
% 	\setlength{\tabcolsep}{1mm}{
% \resizebox{2.1\columnwidth}{!}{
    \begin{tabular}{lccc}
	\toprule
	\multicolumn{1}{c}{\textbf{Variants}} & \textbf{Sports} & \textbf{Beauty} & \textbf{CDs} \\
	\midrule
	\midrule
    \multicolumn{4}{@{}c}{\textit{TIGER with larger vocabularies}} \\
    \midrule
    (1.1) TIGER\ -\ 1k ($4 \times 2^8$) & 0.0225 & 0.0384 & 0.0411 \\
    (1.2) TIGER-49k ($6 \times 2^{13}$) & 0.0162 & 0.0317 & 0.0338 \\
    (1.3) TIGER-66k ($4 \times 2^{14}$) & 0.0194 & N/A$^\dag$ & 0.0319 \\
    \midrule
    \multicolumn{4}{@{}c}{\textit{Vocabulary construction}} \\
    \midrule
    (2.1) \emph{w/o} tokenization & 0.0215 & 0.0389 & 0.0346 \\
    (2.2) \emph{w/o} context-aware & 0.0258 & 0.0416 & 0.0429 \\
    (2.3) \emph{w/o} weighted counting & 0.0257 & 0.0412 & 0.0435 \\
    \midrule
    \multicolumn{4}{@{}c}{\textit{Set permutation regularization}} \\
    \midrule
    (3.1) only for inference & 0.0192 & 0.0316 & 0.0329 \\
    (3.2) only for training & 0.0244 & 0.0387 & 0.0422 \\
    \midrule
    ActionPiece (40k) & \textbf{0.0264} & \textbf{0.0424} & \textbf{0.0451} \\
    \bottomrule
	\end{tabular}
	\vspace{0.05cm}
	\begin{flushleft}
        $^\dag$ not applicable as $2^{14}$ is larger than \#items in Beauty.
    \end{flushleft}
% 	}}
    \vskip -0.2in
\end{table}

\begin{figure*}[t!]
    \begin{center}
    \includegraphics[width=\linewidth]{fig/vocab_size.pdf}
    \vskip -0.1in
    \caption{Analysis of recommendation performance (NDCG@10, $\uparrow$) and average tokenized sequence length (NSL, $\downarrow$) \wrt vocabulary size across three datasets.
    % NSL refers to the normalized sequence length, calculated relative to the number of initial tokens.
    ``N/A’’ indicates that ActionPiece is not applied, \ie action sequences are represented solely by initial tokens.}
    \label{fig:vocab_size}
    \end{center}
    \vskip -0.2in
\end{figure*}

Our proposed ActionPiece consistently outperforms all baselines across three datasets, achieving a significant improvement in NDCG@$10$. It surpasses the best-performing baseline method by $6.00\%$ to $12.82\%$. Unlike existing methods, ActionPiece is the first context-aware action sequence tokenizer, \ie the same action can be tokenized into different tokens depending on its surrounding context. This allows ActionPiece to capture important sequence-level feature patterns that enhance recommendation performance.

% \begin{figure}[t]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.85\columnwidth]{fig/ndcg_vs_vocab_size.pdf}}
% \end{center}
% % \vskip -0.3in
% \vspace{-0.3in}
% \caption{Comparison of performance and vocabulary size (\#token for TIGER, SPM-SID, and ActionPiece; \#item for SASRec; and \#item+\#attribute for S$^3$-Rec) on ``Sports'' dataset.
% % By adjusting the vocabulary size, ActionPiece is shown to balance memory efficiency and recommendation performance.
% }
% \label{fig:intro}
% % \vskip -0.2in
% \vspace{-0.1in}
% \end{figure}


\subsection{Ablation Study}\label{sec:ablation}

We conduct ablation analyses in~\Cref{tab:ablation} to study how each proposed technique contributes to ActionPiece.\\
\hspace*{3mm} (1)~We increase the vocabulary size of TIGER, to determine whether the performance gain of ActionPiece is solely due to scaling up the number of tokens in the vocabulary. By increasing the number of semantic ID digits per item~($4 \rightarrow 6$) and the number of candidate semantic IDs per digit~($2^8 \rightarrow 2^{13}\ \text{or}\ 2^{14}$), we create two variants with vocabularies larger than ActionPiece. However, these TIGER variants perform worse than ActionPiece, and even the original TIGER with only $1024$ tokens. The experimental results suggest that scaling up the vocabulary size for generative recommendation models is challenging, consistent with the observations from~\citet{zhang2024moc}.\\
\hspace*{3mm} (2)~To evaluate the effectiveness of the proposed vocabulary construction techniques, we introduce the following variants: \emph{(2.1)~w/o tokenization}, which skips vocabulary construction, using item features directly as tokens; \emph{(2.2)~w/o context-aware}, which only considers co-occurrences and merges tokens within each action during vocabulary construction and segmentation; and \emph{(2.3)~w/o weighted counting}, which treats all token pairs equally rather than using the weights defined in~\Cref{eq:p_one_set,eq:p_two_sets}. The results indicate that removing any of these techniques reduces performance, demonstrating the importance of these methods for building a context-aware tokenizer.\\
\hspace*{3mm} (3)~To evaluate the effectiveness of SPR, we revert to naive segmentation, as described in~\Cref{subsubsec:segmentation}, during model training and inference, respectively. The results show that replacing SPR with naive segmentation in either training or inference degrades performance.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=0.95\columnwidth]{fig/token_util.pdf}
    \vskip -0.1in
    \caption{Analysis of token utilization rate (\%) during model training \wrt segmentation strategy.
    % ``SPR'' denotes set permutation regularization.
    }
    \label{fig:token_util}
    \end{center}
    % \vskip -0.3in
    \vskip -0.3in
\end{figure}

\subsection{Further Analysis}

% In this section, we analyze the impact of key hyperparameters in vocabulary construction and segmentation.

\subsubsection{Performance and Efficiency \wrt Vocabulary Size}

Vocabulary size is a key hyperparameter for language tokenizers~\cite{meta2024llama3,dagan2024getting}. In this study, we investigate how adjusting vocabulary size affects the generative recommendation models. We use the normalized sequence length (NSL)~\cite{dagan2024getting} to measure the length of tokenized sequences, where a smaller NSL indicates fewer tokens per tokenized sequence. We experiment with vocabulary sizes in \{N/A, 5k, 10k, 20k, 30k, 40k\}, where ``N/A'' represents the direct use of item features as tokens. As shown in~\Cref{fig:vocab_size}, increasing the vocabulary size improves recommendation performance and reduces the tokenized sequence length. Conversely, reducing the vocabulary size lowers the number of model parameters, improving memory efficiency. This analysis demonstrates that adjusting vocabulary size enables a trade-off between model performance, sequence length, and memory efficiency.

\subsubsection{Token Utilization Rate \wrt Segmentation Strategy}\label{sec:token_utilization}

As described in~\Cref{subsubsec:training}, applying SPR augments the training corpus by producing multiple token sequences that share the same semantics. In~\Cref{tab:ablation}, we observe that incorporating SPR significantly improves recommendation performance. One possible reason is that SPR increases token utilization rates. To validate this assumption, we segment the action sequences in each training epoch using two strategies: naive segmentation and SPR. As shown in~\Cref{fig:token_util}, naive segmentation uses only $56.89\%$ of tokens for model training, limiting the model's ability to generalize to unseen action sequences. In contrast, SPR achieves a token utilization rate of $87.01\%$ after the first training epoch, with further increases as training progresses. These results demonstrate that the proposed SPR segmentation strategy improves the utilization of ActionPiece tokens, enabling better generalization and enhanced performance.


\subsubsection{Performance \wrt Inference-Time Ensembles}\label{sec:inference_time_ensemble}

As described in~\Cref{subsubsec:inference}, ActionPiece supports inference-time ensembling by using SPR segmentation. We vary the number of ensembled segments, $q$, in \{N/A, 1, 3, 5, 7\}, where ``N/A'' indicates using naive segmentation during model inference. As shown in~\Cref{fig:ensemble}, ensembling more tokenized sequences improves ActionPiece's recommendation performance. However, the performance gains slow down as $q$ increases to $5$ and $7$. Since a higher $q$ also increases the computational cost of inference, this creates a trade-off between performance and computational budget in practice.

\begin{figure}[t!]
    \begin{center}
    \includegraphics[width=\columnwidth]{fig/ensemble.pdf}
    \vskip -0.15in
    \caption{Analysis of performance (NDCG@10, $\uparrow$) \wrt the number of ensembled segments $q$ during model inference.}
    \label{fig:ensemble}
    \end{center}
    \vskip -0.25in
\end{figure}

\subsection{Case Study}\label{subsec:case}

To understand how GR models benefit from the unordered feature setting and context-aware action sequence tokenization, we present an illustrative example in~\Cref{fig:case}.

Each item in the action sequence is represented as a feature set, with each item consisting of five features. The features within an item do not require a specific order. The first step of tokenization leverages the unordered nature of the feature set and applies set permutation regularization~(\Cref{subsubsec:segmentation}). This process arranges each feature set into a specific permutation and iteratively groups features based on the constructed vocabulary~(\Cref{subsubsec:vocab_construct}). This results in different segments that convey the same semantics. Each segment is represented as a sequence of sets, where each set corresponds to a token in the vocabulary.

By examining the segments and their corresponding token sequences, we identify four types of tokens, as annotated in~\Cref{fig:case}: (1) a subset of features from a single item (token {\setlength{\fboxsep}{0pt}\colorbox{myblue}{14844}} corresponds to features {\setlength{\fboxsep}{0pt}\colorbox{myblue}{747}} and {\setlength{\fboxsep}{0pt}\colorbox{myblue}{923}} of the T-shirt); (2) a set containing a single feature (feature {\setlength{\fboxsep}{0pt}\colorbox{mygreen}{76}} of the socks); (3) all features of a single item (token {\setlength{\fboxsep}{0pt}\colorbox{myyellow}{7995}} corresponds to all features of the shorts); and (4) features from multiple items (\eg token {\setlength{\fboxsep}{0pt}\colorbox{myblue}{83}\colorbox{mygreen}{16}} includes feature {\setlength{\fboxsep}{0pt}\colorbox{myblue}{923}} from the T-shirt and feature {\setlength{\fboxsep}{0pt}\colorbox{mygreen}{679}} from the socks, while token {\setlength{\fboxsep}{0pt}\colorbox{mygreen}{19}\colorbox{myyellow}{895}} includes feature {\setlength{\fboxsep}{0pt}\colorbox{mygreen}{1100}} from the socks as well as features {\setlength{\fboxsep}{0pt}\colorbox{myyellow}{560}} and {\setlength{\fboxsep}{0pt}\colorbox{myyellow}{943}} from the shorts). Notably, the fourth type of token demonstrates that the features of one action can be segmented and grouped with features from adjacent actions. This results in different tokens for the same action depending on the surrounding context, showcasing the context-aware tokenization process of ActionPiece.

