\section{Method}

In this section, we present \textbf{ActionPiece}, a context-aware method for tokenizing action sequences in generative recommendation. First, we formulate the task in~\Cref{subsec:problem}. Then, we introduce the proposed tokenizer, covering vocabulary construction and sequence segmentation,
% and how to segment action sequences of modeling 
in~\Cref{subsec:tokenizer}. Finally, we describe the model training and inference process using ActionPiece-tokenized sequences
% of training generative recommendation models using ActionPiece-tokenized sequences 
in~\Cref{subsec:model}.

\subsection{Problem Formulation}\label{subsec:problem}

% input action sequence, output probability on candidate items

Given a user's historical actions $S = \{i_1, i_2, \ldots, i_t\}$, organized sequentially by their timestamps, the task is to predict the next item $i_{t+1}$ the user will interact with.

\textbf{Action as an unordered feature set.}
% what is action: interacted item, a set of features
% what is feature: discrete (one-hot), continuous, or multimodal (text, image, video)
In the development of modern recommender systems, each item $i_j$ is usually associated with a set of features $\mathcal{A}_j$~\cite{zhang2019fdsa,zhou2020s3,cheng2016wd}. Assuming there are $m$ features per item, the $k$-th feature of item $i_j$ is denoted as $f_{j,k} \in \mathcal{F}_k$, where $\mathcal{F}_k$ is the collection of all possible choices for the $k$-th feature.
% The unordered set setting covers most feature types. Discrete features, such as \emph{category} and \emph{brand}, are naturally represented as characteristics~\cite{pazzani2007content,zhang2019fdsa,zhou2020s3}; Numeric features, like \emph{price} and \emph{popularity ranking}, can be discretized into buckets, with the corresponding bucket IDs treated as characteristics~\cite{juan2016field}; Features with rich modalities, such as text (\emph{title}, \emph{description}), image, or videos, can be encoded into dense representations using pretrained models~\cite{Devlin2019bert,ni2022gtr,ni2022sentencet5,radford2021clip} and then quantized into semantic IDs to be included in the feature set~\cite{hou2023vqrec,rajput2023tiger,jin2024lmindexer,singh2024spmsid}.
Compared to representing actions using ordered semantic IDs (\eg those produced by RQ-VAE~\cite{rajput2023tiger,singh2024spmsid}), the unordered set setting offers two key advantages: (1) It does not require a specific order among features, which aligns better with how items or actions are represented in most recommender systems; (2) It enables the inclusion of more general discrete and numeric features, such as \emph{category}, \emph{brand}, and \emph{price}~\cite{pazzani2007content,juan2016field}.
% widely used in existing recommender systems.
% and (3) It represents the data in a more fundamental way, whereas semantic IDs unavoidably involve information loss due to the encoding process by pretrained models.

\textbf{Action sequence as a sequence of sets.}
% what is action sequence: sequence of sets
% features within a set are unordered, sets within a sequence are ordered
Representing each item as an unordered set, the input action sequence can be written as $S'=\{\mathcal{A}_1, \mathcal{A}_2, \ldots, \mathcal{A}_t\}$, which is a chronologically ordered sequence of sets. There is no order within each set,
% as the features of an item contribute in parallel. However,
but there are orders between the features from different sets. The tokenizer design should account for the ordered and unordered relationships among features.

\textbf{Generative recommendation task.} In this work, we aim to design a tokenizer that maps an input action sequence $S'$ to a token sequence $C = \{c_1, c_2, \ldots, c_l\}$, where $l$ denotes the number of tokens in the sequence. Note that $l$ is typically greater than the number of actions $t$. Next, we train a GR model to autoregressively generate tokens $\{c_{l+1}, \ldots, c_q\}$, which can be parsed as next-item predictions $\hat{i}_{t+1}$.

\subsection{Contextual Action Sequence Tokenizer}\label{subsec:tokenizer}

The proposed tokenizer is designed to transform action sequences (represented as sequences of feature sets) into token sequences. In the ActionPiece-tokenized sequences, each token corresponds to a set containing varying numbers of features. For example, a token can represent: (1) a subset of features from one item; (2) a set with a single feature; (3) all features of one item; or (4) features from multiple items.
% \todo{A Figure  describing all types of token<>feature relations.}
We also label these four types of tokens in~\Cref{fig:case}.
Below, we first describe how to construct the ActionPiece tokenizerâ€™s vocabulary given a corpus of action sequences (\cref{subsubsec:vocab_construct}). Then, we introduce how to segment action sequences into a new sequence of sets, where each set corresponds to a token from the constructed vocabulary (\cref{subsubsec:segmentation}).

% token -> set
% Initial tokens, merged tokens



\begin{algorithm}[!t]
\small
   \caption{ActionPiece Vocabulary Construction}
   \label{alg:vocab_construction_overall}
\begin{algorithmic}[1]
   \INPUT Sequence corpus $\mathcal{S}'$, initial tokens $\mathcal{V}_0$, target size $Q$
   \OUTPUT Merge rules $\mathcal{R}$, constructed vocabulary $\mathcal{V}$
   \STATE Initialize vocabulary $\mathcal{V} \gets \mathcal{V}_0$ \COMMENT{each initial token corresponds to one unique item feature}
   \STATE $\mathcal{R} \gets \emptyset$
   \WHILE{$|\mathcal{V}| < Q$}
    %   \STATE \COMMENT{\textbf{\emph{Count:}} accumulate weighted co-occurrences}
    %   \FOR{each sequence $S' \in \mathcal{S}'$}
    %      \FOR{each adjacent/within-set token pair $(c_i, c_j)$ in $S'$}
    %          \IF{$c_i, c_j$ in same set $\mathcal{A}_k$}
    %              \STATE $w \gets 2 / |\mathcal{A}_k|$ \COMMENT{\Cref{eq:p_one_set}}
    %          \ELSE
    %              \STATE $w \gets 1/(|\mathcal{A}_k| \times |\mathcal{A}_{k+1}|)$ \COMMENT{\Cref{eq:p_two_sets}}
    %          \ENDIF
    %          \STATE $\text{count}(c_i, c_j) \mathrel{+}= w$ \COMMENT{update co-occurrence count}
    %      \ENDFOR
    %   \ENDFOR
    \STATE \COMMENT{\textbf{\emph{Count:}} accumulate weighted token co-occurrences}
    \STATE $\text{count}(\cdot, \cdot) \gets \text{Count}(\mathcal{S'}, \mathcal{V})$ \COMMENT{\Cref{alg:vocab_construction_count}}
      \STATE \COMMENT{\textbf{\emph{Update:}} Merge a frequent token pair into a new token}
      \STATE Select $(c_u, c_v) \gets \argmax_{(c_i,c_j)} \text{count}(c_i,c_j)$
      \STATE $\mathcal{S}' \gets \text{Update}(\mathcal{S}', \{(c_u, c_v) \to c_{\text{new}}\})$ \COMMENT{\Cref{alg:vocab_construction_update}}
      \STATE $\mathcal{R} \gets \mathcal{R} \cup \{(c_u, c_v) \to c_{\text{new}}\}$ \COMMENT{new merge rule}
      \STATE $\mathcal{V} \gets \mathcal{V} \cup \{c_{\text{new}}\}$ \COMMENT{add new token to the vocabulary}
    %   \IF{$c_u$ and $c_v$ in same action node}
    %      \STATE Merge into new token $c_{\text{new}} = c_u \cup c_v$
    %      \STATE Update action node: replace $c_u,c_v$ with $c_{\text{new}}$
    %   \ELSE
    %      \STATE Create intermediate node between action nodes
    %      \STATE $c_{\text{new}} \gets \text{Merge}(c_u, c_v)$
    %      \STATE Update linked list structure
    %   \ENDIF
   \ENDWHILE
   \item[\textbf{return} $\mathcal{R}, \mathcal{V}$]
\end{algorithmic}
\end{algorithm}
% \vskip -0.2in

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\linewidth]{fig/weight.pdf}}
\end{center}
\vskip -0.5in
\caption{Illustration of how weights of co-occurring token pairs are counted during vocabulary construction. In this example, two adjacent sets in the sequence are considered: one with $4$ tokens (represented as $\bigcirc$) and another with $3$ tokens (represented as $\square$). Token pairs are counted within a single set ($<\bigcirc, \bigcirc>$ and $<\square, \square>$) and across the two adjacent sets ($<\bigcirc, \square>$).}
\label{fig:weight}
\vskip -0.1in
\end{figure*}


\subsubsection{Vocabulary Construction on Action Sequence Corpus}\label{subsubsec:vocab_construct}

Given a corpus of action sequences $\mathcal{S}'$, the goal of vocabulary construction is to create a vocabulary $\mathcal{V}$ of $Q$ tokens. Each token represents a combination of features that frequently occur in the corpus. Similar to BPE~\cite{sennrich2016bpe}, we construct the vocabulary using a bottom-up approach. The process starts with an \textbf{initial vocabulary} of tokens $\mathcal{V}_0$. The construction proceeds iteratively, adding one new token to the vocabulary at each iteration until the predefined target size is reached. Each iteration consists of two consecutive steps: \textbf{count}, where the most frequently occurring token pair is identified, and \textbf{update}, where the corpus is modified by merging the selected pair into a new token. An algorithmic  workflow is illustrated in~\Cref{alg:vocab_construction_overall}.
% In what follows, we first describe the initialization of the vocabulary. Next, we explain the two main steps \emph{Count} and \emph{Update} for each iteration. Finally, we present the data structures and algorithm used to speed up the vocabulary construction process.

% input: action sequence corpus, initial tokens, target vocab size
% output: merge rules
% logic:
%   initial tokens
%   add a new merge rule after each iteration, until meet the target vocab size
%   each iteration: count -> update

\textbf{Vocabulary initialization.} In BPE, each token represents a sequence of bytes. Thus, the most fundamental units--the initial tokens--are single bytes, which form the initial vocabulary of BPE. Similarly, each token in ActionPiece represents a set of features. Therefore, we initialize ActionPiece with a vocabulary in which each token represents a set containing one unique item feature. Formally, we denote the initial vocabulary as $\mathcal{V}_{0} = \left\{c = \{f\} | f \in \mathcal{F}_1 \cup \ldots \cup \mathcal{F}_m\right\}$. After initializing the vocabulary, each action sequence (of feature sets) can be represented as a sequence of token sets.

\textbf{Count: context-aware token co-occurrence counting.} In each iteration of vocabulary construction, the first step is to count the co-occurrence of token pairs in the corpus. These pairs capture important feature combinations, which are encoded by creating new tokens. There are two types of token co-occurrence within a sequence of sets: (1) two tokens exist within the same set, or (2) two tokens exist in adjacent sets in the sequence. Notably, the second type allows ActionPiece to explicitly include context information.
% , \ie tokens that represent features from adjacent actions.

% It is intuitive to count token pairs based on their co-occurrence types (\ie whether the tokens belong to the same set or adjacent sets) and the sizes of the sets. For reference, i
\emph{Weighted co-occurrence counting.} In one-dimensional token sequences (\eg text), all token pairs are typically treated equally. However, in sequences of token sets, token pairs vary based on their types and the sizes of their respective sets. To account for these differences, we propose assigning different weights to token pairs. To determine the weight for each token pair, we relate sequences of token sets to token sequences by randomly permuting the tokens within each set and flattening them into a single token sequence. Let $P(c, c')$ represent the expected probability that tokens \(c\) and \(c'\) are adjacent in the flattened sequence. For two tokens from the same set, we have:
\begin{equation}
    P(c_1, c_2) = P(c_2, c_1) = \frac{|\mathcal{A}_i| - 1}{\tbinom{|\mathcal{A}_i|}{2}} = \frac{2}{|\mathcal{A}_i|}, \quad c_1, c_2 \in \mathcal{A}_i, \label{eq:p_one_set}
\end{equation}
and for two tokens from adjacent sets, we have:
\begin{equation}
    P(c_1, c_3) = \frac{1}{|\mathcal{A}_i| \times |\mathcal{A}_{i + 1}|}, \quad c_1 \in \mathcal{A}_i, \; c_3 \in \mathcal{A}_{i+1}. \label{eq:p_two_sets}
\end{equation}
By considering the probabilities of all adjacent token pairs in the flattened sequence as \(1\), the weights for token pairs in the original sequence of token sets correspond to the probabilities given in~\Cref{eq:p_one_set,eq:p_two_sets}. An illustration is shown in~\Cref{fig:weight}.
% \todo{Figure on probability calculation.} 

\emph{Accumulating co-occurrence weights.} The weights described above are calculated based solely on the co-occurrence type and the set size. They do not take into account the specific tokens being analyzed. Tokens $c_i$ and $c_j$ might appear in the same set in one sequence but in two adjacent sets in another sequence. By iterating through the corpus, we sum up the weights for each token pair whenever they appear together multiple times.

% which pairs to count: token within one single set, token between two adjacent sets
% weight:
%   why: imagine a gradually prediction process
%   how


\begin{figure*}[!t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{fig/update.pdf}}
\end{center}
\vskip -0.3in
\caption{Illustration of how the linked list, which maintains the action sequence, is updated when merging two tokens into a new token. Three cases are considered: (1) both tokens are in the same action node; (2) the tokens are in two adjacent action nodes; (3) one token is in an action node, while the other is in an intermediate node.}
\label{fig:update}
\vskip -0.1in
\end{figure*}


\textbf{Update: corpus updating with action-intermediate nodes.} The next step in each iteration is to merge the token pair with the highest accumulated co-occurrence weight. Since token merging may change the set size, we use a double-ended linked list~\cite{zouhar2023formal} to maintain the action sequences, where each node represents a set of tokens. Merging tokens within the same set is straightforward, \ie replacing the two tokens with a new one. However, merging tokens from two adjacent sets is more complex, \eg determining which set should include the new token.

\emph{Intermediate Node.} We introduce the concept of ``intermediate node'' to handle tokens that combine features from multiple sets. Initially, all nodes in the maintained linked list contain features specific to their corresponding actions. These nodes are referred to as ``action nodes.''\\
\hspace*{3mm} (1) When tokens from two adjacent action nodes are being merged, we insert a new intermediate node between the two action nodes. The new token is stored in the intermediate node, and the merged tokens are removed from their respective action nodes;\\
\hspace*{3mm} (2) When merging tokens from an action node and an intermediate node, the new token replaces the original token in the intermediate node. The reason is that this new token also combines features from multiple actions. After the merge, the token from the action node is removed.\\
\hspace*{3mm} Following the above update rules ensures that there is at most one intermediate node between any two action nodes, and each intermediate node contains no more than one token. When calculating co-occurrence weights involving an intermediate node, it can simply be treated as a set of size $1$.
% \todo{Figure about corpus updating.}

% For a sequence of $t$ actions, the corresponding linked list has $2t - 1$ nodes, in which there are $t$ ``action nodes'' with tokens representing features solely associated with one action, and $t-1$ ``intermediate nodes'' placing interleaved between action nodes. At the very beginning, all the intermediate nodes are empty. An intermediate node has its first token when the neighbor

% maintain the corpus using linked list
% what if two tokens from different sets are merged?
% action-intermediate slots



\textbf{Efficient implementation.} Naively counting and updating the corpus requires a total time complexity of $O(QNLm^2)$, where $Q$ is the target vocabulary size, $N$ is the number of action sequences in the training corpus, and $L$ is the average length of these sequences. However, it is unnecessary to count co-occurrences from scratch in each iteration. This is because only a small portion of the maintained linked lists is modified compared to the previous iteration.

\emph{Data structures.} To address this, we propose maintaining a hash table to store the accumulated co-occurrences for each token pair. Additionally, we use inverted indices to map token pairs to all the linked lists that contain them. A global heap is maintained to return the token pair with the highest accumulated co-occurrence. The key challenge lies in updating these data structures. We carefully compute the changes in accumulated co-occurrences and update both the hash table and the inverted indices. For the heap, we employ a lazy-update strategy. We insert the latest weights with a tag. When fetching a value from the heap, we check the tag to verify if the value is up-to-date. If it is not, we discard the value and fetch the next one.

\emph{Time complexity.} Let $H = O(NLm)$ represent the maximal heap size.
% , and $D = O(\frac{NLm}{Q})$ denote the average time a token appears in the training corpus. 
Using the proposed algorithm, we successfully reduce the original time complexity to $O(\log{Q}\log{H}\cdot NLm^2)$, achieving efficient vocabulary construction. In practice, the later iterations take significantly less time than the initial ones. This is expected and because tokens with higher accumulated co-occurrence weights typically appear frequently in the early stages. However, the overall construction time benefits from the reduced amortized complexity. Further details about the vocabulary construction algorithm are provided in~\cref{sec:vocab_construct_algo}.


% double-ended linked list, inverted index, heap with lazy update


\begin{table*}[!th]
    \small
    \centering
	\caption{Performance comparison of different methods on the Amazon Reviews dataset~\cite{mcauley2015image}. The best and second-best performance is denoted in \textbf{bold} and \underline{underlined} fonts. ``R@K'' and ``N@K'' are short for ``Recall@K'' and ``NDCG@K'', respectively. ``Improv.'' denotes the percentage improvement of our method compared to the strongest baseline method.}
	\label{tab:performance}
	\vskip 0.1in
	\setlength{\tabcolsep}{1mm}{
\resizebox{2.05\columnwidth}{!}{
	\begin{tabular}{llc@{\hspace{0.8mm}}c@{\hspace{3.5mm}}c@{\hspace{3.5mm}}ccc@{\hspace{3mm}}cccccr}
	\toprule
		\multirow{2.5}{*}{\textbf{Datasets}} & \multirow{2.5}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{ID-based}} & \multicolumn{3}{c}{\textbf{Feature + ID}} & \multicolumn{6}{c}{\textbf{Generative}} & \multirow{2.5}{*}{\textbf{Improv.}} \\
		\cmidrule(lr){3-4} \cmidrule(lr){5-7} \cmidrule(lr){8-13}
		& & BERT4Rec& SASRec &FDSA & S$^3$-Rec &VQ-Rec &P5-CID &TIGER &LMIndexer & HSTU &SPM-SID & ActionPiece &  \\
	\midrule
	\midrule
\multirow{4} * {Sports}
 & R@5 &	0.0115 &	0.0233 &	0.0182 &	0.0251 & 0.0181 & 0.0287 & 0.0264 &	0.0222 & 0.0258 & \underline{0.0280} & \textbf{0.0316}\textsubscript{ $\pm$ 0.0005} & +12.86\% \\
 & N@5 &	0.0075 &	0.0154 &	0.0122 &	0.0161 & 0.0132 & 0.0179 & \underline{0.0181} &	0.0142 & 0.0165 & 0.0180 & \textbf{0.0205}\textsubscript{ $\pm$ 0.0002} & +11.71\% \\
 & R@10 &	0.0191 &	0.0350 &	0.0288 &	0.0385 & 0.0251 & 0.0426 & 0.0400 &	--- & 0.0414 & \underline{0.0446} & \textbf{0.0500}\textsubscript{ $\pm$ 0.0007} & +12.11\% \\
 & N@10 &	0.0099 &	0.0192 &	0.0156 &	0.0204 & 0.0154 & 0.0224 & 0.0225 &	--- & 0.0215 & \underline{0.0234} & \textbf{0.0264}\textsubscript{ $\pm$ 0.0003} & +12.82\% \\
\midrule
\multirow{4} * {Beauty}
 & R@5 &	0.0203 &	0.0387 &	0.0267 &	0.0387 & 0.0434	&	0.0468 &		0.0454 &	0.0415	& 0.0469 & \underline{0.0475} & \textbf{0.0511}\textsubscript{ $\pm$ 0.0014} & +7.58\% \\
 & N@5 &	0.0124 &	0.0249 &	0.0163 &	0.0244 & 0.0311	&	0.0315 &		0.0321 &	0.0262	& 0.0314 & \underline{0.0321} & \textbf{0.0340}\textsubscript{ $\pm$ 0.0011} & +5.92\% \\
 & R@10 &	0.0347 &	0.0605 &	0.0407 &	0.0647 & \underline{0.0741}	&	0.0701 &		0.0648 &	--- & 0.0704 & 0.0714 & \textbf{0.0775}\textsubscript{ $\pm$ 0.0017} & +4.59\% \\
 & N@10 &	0.0170 &	0.0318 &	0.0208 &	0.0327 & 0.0372	&	\underline{0.0400} &		0.0384 &	--- & 0.0389 & 0.0399 & \textbf{0.0424}\textsubscript{ $\pm$ 0.0011} & +6.00\% \\
 \midrule
 \multirow{4} * {CDs}
 & R@5 & 0.0326 & 0.0351 & 0.0226 & 0.0213 & 0.0314 & 0.0505 & 0.0492 & --- & 0.0417 & \underline{0.0509} & \textbf{0.0544}\textsubscript{ $\pm$ 0.0005} & +6.88\% \\
 & N@5 & 0.0201 & 0.0177 & 0.0137 & 0.0130 & 0.0209 & 0.0326 & 0.0329 & --- & 0.0275 & \underline{0.0337} & \textbf{0.0359}\textsubscript{ $\pm$ 0.0004} & +6.53\% \\
 & R@10 & 0.0547 & 0.0619 & 0.0378 & 0.0375 & 0.0485 & \underline{0.0785} & 0.0748 & --- & 0.0638 & 0.0778 & \textbf{0.0830}\textsubscript{ $\pm$ 0.0008} & +5.73\% \\
 & N@10 & 0.0271 & 0.0263 & 0.0186 & 0.0182 & 0.0264 & 0.0416 & 0.0411 & --- & 0.0346 & \underline{0.0424} & \textbf{0.0451}\textsubscript{ $\pm$ 0.0005} & +6.37\% \\
\bottomrule
	\end{tabular}
	}}
	\vskip -0.1in
\end{table*}


\subsubsection{Segmentation by Set Permutation Regularization}\label{subsubsec:segmentation}

Segmentation is to convert original action sequences into a sequence of feature sets. Each set in the segmented sequence corresponds to a token in the vocabulary.

\textbf{Naive segmentation.} One segmentation strategy in ActionPiece involves applying the same technique used to construct the vocabulary. Specifically, this technique iteratively identifies token pairs with high priorities (represented by the IDs of tokens, where tokens added earlier may have higher priority). However, we observed that this strategy can lead to a bias, where only a subset of tokens in the vocabulary is frequently used (as shown empirically in~\Cref{sec:token_utilization}).

\textbf{Set permutation regularization (SPR).} To address this issue and account for the unordered nature of sets, we propose \emph{set permutation regularization}, which generates multiple segmentations for each action sequence. The key idea is to avoid enumerating all possible pairs between tokens in a set or adjacent sets. Instead, we generate a random permutation of each set and treat it as a one-dimensional sequence. By concatenating all the permutations, we create a long token sequence. This sequence can then be segmented using traditional BPE segmentation methods~\cite{sennrich2016bpe}. In this approach, different permutations can produce distinct segmented token sequences with the same semantics. These sequences serve as natural augmentations for model training (\Cref{subsubsec:training}) and enable inherent ensembling during model inference (\Cref{subsubsec:inference}).


\subsection{Generative Recommendation Models}\label{subsec:model}

% In this section, we present the training and inference processes of the GR models.
% how to train a generative recommendation model and perform inference using ActionPiece-tokenized sequences.

\subsubsection{Training on Augmented Token Sequences}\label{subsubsec:training}

For an action sequence and its ground-truth next action in the training corpus, we tokenize them into token sequences $C_{\text{in}}$ and $C_{\text{out}}$, respectively. Taking $C_{\text{in}}$ as input, we train a Transformer encoder-decoder module~\cite{raffel2020t5} to autoregressively generate $C_{\text{out}}$ (\eg next-token prediction objective~\cite{rajput2023tiger}). During training, we tokenize the action sequence using the set permutation regularization described in~\Cref{subsubsec:segmentation} in each epoch. This approach naturally augments the training sequences, which empirically improves model performance, as shown in~\Cref{sec:ablation}.

\subsubsection{Inference-Time Ensembling}\label{subsubsec:inference}

During model inference, we tokenize each action sequence $q$ times using set permutation regularization. By passing these $q$ tokenized sequences through the model, we obtain $q$ output ranking lists (\eg using beam search for inference when TIGER~\cite{rajput2023tiger} is the GR backbone). We then combine these ranking lists by averaging the scores of each predicted item. This approach applies data-level ensembling, which has been shown to enhance recommendation performance, as discussed in~\Cref{sec:inference_time_ensemble}.
