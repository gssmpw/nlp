%\subsection{Binary Iterative Hard Thresholding (BIHT) Algorithm}
%\label{outline:notations|>recovery-algs}

%The discussion now returns to the inverse problem in \onebitcs---specifically, an algorithm empirically and theoretically known to perform well both statistically and computationally for this problem.
For reconstruction in \onebitcs, \cite{jacques2013robust} propose the \emph{binary iterative hard thresholding (BIHT)} algorithm, inspired by the existing {\em iterative hard thresholding} algorithm for compressed sensing~\cite{blumensath2009iterative}. BIHT is a projected, (sub)gradient descent algorithm on the (negative) ReLU loss, given for
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \thetaX \in \R^{\n}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
by
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:notations:objective-function}
  \mathcal{J}( \thetaX )
  =
  \| [ \Diag( \RespV ) \CovM \thetaX ]_{-} \|_{1}
,\end{gather}
%|<<|===========================================================================================|<<|
where
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV \defeq \Sign( \CovM \thetaStar ) \in \{ -1,1 \}^{\m}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
is the vector of binary responses and where
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  ( [ \Vec{v} ]_{-} )_{\iIx} = [ \Vec*{v}_{\iIx} ]_{-} = \min \{ \Vec*{v}_{\iIx}, 0 \}  \),
\(  \iIx \in [\m]  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
for a vector \(  \Vec{v} \in \R^{\m}  \).
\cite{jacques2013robust} shows that
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:notations:grad-objective-function}
  \CovM^{\T} \sep \frac{1}{2} ( \Sign( \CovM \thetaX ) - \RespV ) \in \nabla_{\thetaX} \mathcal{J}( \thetaX )
,\end{gather}
%|<<|===========================================================================================|<<|
leading to their BIHT algorithm.
This present work studies the normalized variant of the BIHT algorithm of \cite{jacques2013robust}, presented in \ALGORITHM \ref{alg:biht:normalized}, which iteratively performs
%|>>|×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××|>>|
\Enum[{\label{step:alg:biht:normalized:i}}]{i} first a (sub)gradient descent step on the objective function \(  \objectiveFn  \) in \EQUATION \eqref{eqn:notations:objective-function}, %(\see \LINE \ref{alg:biht:normalized:intermediate} of \ALGORITHM \ref{alg:biht:normalized}),
%×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××
followed by
\Enum[{\label{step:alg:biht:normalized:ii}}]{ii} a projection onto \(  \ParamSpace  \),
%via \topk-thresholding and normalization
%(\see \LINE \ref{alg:biht:normalized:projection} of \ALGORITHM \ref{alg:biht:normalized}).
%|<<|×××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××××|<<|
%\STEP \ref{step:alg:biht:normalized:i} of the algorithm effectively ``tries to'' minimize the objective function in \EQUATION \eqref{eqn:notations:objective-function}
%\ref{step:alg:biht:normalized:i} uses the subgradient in \EQUATION \eqref{eqn:notations:grad-objective-function} for the direction of descent.
%Meanwhile, \STEP \ref{step:alg:biht:normalized:ii} projects the approximation obtained from the gradient step, \STEP \ref{step:alg:biht:normalized:i}, onto the parameter space, \(  \ParamSpace  \), 
by performing the \topk thresholding operation and then normalizing.
Note that in the dense regime, when \(  \k=\n  \), the algorithm applies no thresholding in the second step but still normalizes the approximation.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%The original algorithm of \cite{jacques2013robust} takes the quantization function, \(  \fFn  \), to simply be the sign function.
%, whereas this work considers a more general choice of \(  \fFn  \) that extends the algorithm to GLMs.
%In the noiseless setting, when \(  \fFn  \) is the sign function, theoretical guarantees for the BIHT algorithm have been obtained and corroborated by empirical results.
For this noiseless response setting in \onebitcs,
%when \(  \fFn  \) is the sign function,
\cite{friedlander2021nbiht} shows that under the Gaussian design, the BIHT algorithm converges to the correct solutions with a suboptimal sample complexity. % of
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
% \(  \BigO( \max \{ \k^{10} \log^{10}( \frac{\n}{\k} ), \frac{1}{\epsilonX} \k^{7/2} \log^{7/2}( \frac{\n}{\k} ), 24^{48} \} )  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
Subsequently, \cite{matsumoto2022binary} improves the sample complexity to the theoretically \orderwise optimal (up to logarithmic factors) sample complexity:
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \BigO'( \frac{\k}{\epsilonX} \log( \frac{\n}{\k} ) \sqrt{\log( \smash[b]{\frac{1}{\epsilonX}} )} + \frac{\k}{\epsilonX} \log^{3/2}( \frac{1}{\epsilonX} ) )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
matching lower bound on the sample complexity for recovery in \onebitcs established by \cite{jacques2013robust}.
One limitation of \cite{friedlander2021nbiht,matsumoto2022binary} is the inability to immediately extend the convergence result to \nonseparable data and settings with noise.
This was partially addressed in later work, in which \cite{matsumoto2024robust} shows robustness properties of BIHT when \(  \fFn  \) incorporates adversarial noise: convergence of BIHT is possible with a similar sample complexity to that stated for the noiseless (or \nonseparable) setting.
This begs the question: what convergence guarantees are possible in other models of noise or \nonseparable data?
This work seeks an answer to this question for one such class of models: binary GLMs.

%%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
%%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
%%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
%\begin{algorithm}
%\label{alg:gd:normalized}
%\caption{Gradient descent algorithm with normalized projections}
%%
%\Given{\(  \fFn{} \big( \CovM \thetaStar \big)  \)}
%\(
%  \thetaHat^{(0)}
%  \sim
%  \Sphere{\n}
%\)\;
%%
%\For
%{\( \Iter = 1, 2, 3, \dots \)}
%{
%  \(
%    \thetaHatX[\Iter]
%    \gets
%    \thetaHat^{(\Iter-1)}
%    +
%    \frac{\sqrt{2\pi}}{\m}
%    \sep
%    \CovM^{\T}
%    \sep
%    \frac{1}{2}
%    \left(
%      \fFn{} \big( \CovM \thetaStar \big) - \Sign{} \big( \CovM \thetaHat[\Iter-1] \big)
%    \right)
%  \)\;
%  \(
%    \thetaHat[\Iter]
%    \gets
%    \frac
%    {\thetaHatX[\Iter]}
%    {\| \thetaHatX[\Iter] \|_{2}}
%  \)\;
%}
%\end{algorithm}
%%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|
%%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|
%%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|

%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
%|>>|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|>>|
\begin{algorithm}
\caption{Normalized binary iterative hard thresholding (BIHT)\label{alg:biht:normalized}}
%
\Given{\(  \RespV, \CovM  \)}
\(
  \thetaHat^{(0)}
  \sim
  \SparseSphereSubspace{\k}{\n}
\)\;

\For
{\( \Iter = 1, 2, 3, \dots \)}
{
  \(
    \thetaHatX[\Iter]
    \gets
    \thetaHat^{(\Iter-1)}
    +
    \frac{\sqrt{2\pi}}{\m}
    \sep
    \CovM^{\T}
    \sep
    \frac{1}{2}
    \left(
      %\fFn{} \big( \CovM \thetaStar \big) - \Sign{} \big( \CovM \thetaHat[\Iter-1] \big)
      \RespV - \Sign{} \big( \CovM \thetaHat[\Iter-1] \big)
    \right)
  \)\; \label{alg:biht:normalized:intermediate}

  \(
    \thetaHat[\Iter]
    \gets
    \frac
    {\Threshold{\k}{}(\thetaHatX[\Iter])}
    {\| \Threshold{\k}{}(\thetaHatX[\Iter]) \|_{2}}
  \)\; \label{alg:biht:normalized:projection}
}
\end{algorithm}
%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|
%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|
%|<<|+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++|<<|