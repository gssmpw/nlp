\section{Comparison to Prior Works}
\label{outline:intro|>contributions|>comparison}

A special case of the projected gradient decent algorithm studied by \cite{bahmani2016learning} offers an alternative gradient-based method to BIHT for parameter estimation in some classes of GLMs, including those with nondecreasing, Lipschitz transfer functions, \(  \linkFn^{-1}  \).
Note that this class  encompasses GLMs whose responses
%come from
follow
an exponential distribution, which is one of the most widely studied families of GLMs.
Like BIHT, this algorithm projects its approximations onto the set of \(  \k  \)-sparse vectors, where the sparsity can be taken as \(  \k=\n  \) in the dense parameter regime so as to effectively eliminate the projection step of the algorithm.
In the dense parameter regime, this algorithm becomes the perceptron-like \emph{\GLMtron} algorithm of the earlier work, \cite{kakade2011efficient}, which learns GLMs with (possibly nonstrictly) monotonically increasing, Lipschitz transfer functions.
%For concise nomenclature, we will borrow the name of ``\GLMtronX'' to refer to the specialization of the projected gradient descent algorithm analyzed in \cite{bahmani2016learning} to the original \GLMtron algorithm with the addition of a sparse projection.
%%with nondecreasing, Lipschitz transfer functions.
For concise nomenclature, we will borrow the name of ``\GLMtronX'' to refer to the original \GLMtron algorithm with the addition of a sparse projection, as analyzed in \cite{bahmani2016learning}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
BIHT and \GLMtronX have a few key differences.
While other covariate designs are possible, the analysis in \cite{bahmani2016learning} assumes that the norm of each covariate is almost surely at most \(  1  \), in contrast to the Gaussian covariate design considered in this present work.
As another distinction
%difference
between BIHT and \GLMtronX, BIHT requires that the GLM has binary outcomes with a mild condition on the link function, \(  \linkFn  \), but otherwise need not know the specific choice of link function, while \GLMtron can learn a larger class of GLMs that only necessitates that the transfer function, \(  \linkFn^{-1}  \), satisfies a certain derivative condition (which indeed holds when the transfer function is nondecreasing and Lipschitz).
%while \GLMtron only necessitates that the transfer function, \(  \linkFn^{-1}  \), satisfies a certain derivative condition (which indeed holds when the transfer function is nondecreasing and Lipschitz), and hence applies to a larger class of GLMs than BIHT.
However, unlike BIHT, \GLMtronX requires knowledge of the specific choice of link function.
(Note that \cite{kakade2011efficient} proposes a second algorithm for learning single-index models which estimates an unknown link function, but this is outside the scope of this work.)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Most significantly, BIHT and \GLMtronX ``try to'' minimize different objective functions.
Whereas BIHT performs gradient descent on the (negative) ReLU loss,
%|>>|===========================================================================================|>>|
\begin{gather*}
  \Jbiht( \thetaX )
  =
  \sum_{\iIx=1}^{\m}
  | [ \RespV*_{\iIx} \langle \CovV\VIx{\iIx}, \thetaX \rangle ]_{-} |
,\end{gather*}
%|<<|===========================================================================================|<<|
by taking gradient steps in the negated direction of
%|>>|===========================================================================================|>>|
\begin{gather*}
  \nabla_{\thetaX}
  \Jbiht( \thetaX )
  %\propto
  \ni
  %\frac{\sqrt{2\pi}}{\m}
  %\sep
  -\CovM^{\T}
  \sep
  \frac{1}{2}
  \left(
    %\fFn{} \big( \CovM \thetaStar \big) - \Sign{} \big( \CovM \thetaHat[\Iter-1] \big)
    \RespV - \Sign{} \big( \CovM \thetaX \big)
  \right)
,\end{gather*}
%|<<|===========================================================================================|<<|
\GLMtronX is a gradient descent procedure on the loss
%|>>|===========================================================================================|>>|
\begin{gather*}
  \Jglmtron( \thetaX )
  =
  \sum_{\iIx=1}^{\m}
  \intlinkFn( \langle \CovV\VIx{\iIx}, \thetaX \rangle )
  -
  \RespV*_{\iIx}
  \langle \CovV\VIx{\iIx}, \thetaX \rangle
\end{gather*}
%|<<|===========================================================================================|<<|
with gradient steps in the negated direction of
%|>>|===========================================================================================|>>|
\begin{gather*}
  \nabla_{\thetaX}
  \Jglmtron( \thetaX )
  =
  %\propto
  %\frac{1}{\m}
  %\sep
  -\CovM^{\T}
  \left(
    %\fFn{} \big( \CovM \thetaStar \big) - \Sign{} \big( \CovM \thetaHat[\Iter-1] \big)
    \RespV - \linkFn^{-1} \big( \CovM \thetaX \big)
  \right)
,\end{gather*}
%|<<|===========================================================================================|<<|
where the function, \(  \intlinkFn  \), is defined such that
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \linkFn^{-1} = \partial \intlinkFn  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
When the responses,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV*_{\iIx} \Mid| \CovV\VIx{\iIx}  \),
\(  \iIx \in [\m]  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
follow an exponential distribution, \(  \Jglmtron  \) becomes the negative log-likelihood function, and hence, roughly speaking, \GLMtronX essentially ``tries to'' compute the MLE in this case.
The difference in objective functions is fundamental:
it precludes the application of the analysis for BIHT in this work to \GLMtronX.
Conversely, adapting the approach in \cite{bahmani2016learning} is insufficient to achieve the sample complexity established for BIHT here.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Assuming that the responses, \(  \RespV*_{\iIx}  \), \(  \iIx \in [\m]  \), are bounded, as is the case for binary GLMs, \cite{bahmani2016learning} shows that \GLMtronX achieves an \errorrate of \(  \epsilonX  \) provided the number of covariates, \(  \m  \), is at least
%\?{@Arya -- Could you please double-check this sample complexity?}
%|>>|===========================================================================================|>>|
\begin{gather*}
  \m = \BigO'( \max \left\{ \frac{1}{\epsilonX^{4}}, \k \log \left( \frac{\n}{\k} \right) \right\} )
,\end{gather*}
%|<<|===========================================================================================|<<|
where this hides some terms.
Notice that the dependency on the \errorrate, \(  \epsilonX  \), is \(  \epsilonX^{-4}  \) compared to \(  \epsilonX^{-2}  \) obtained in this work for BIHT, though in fairness, neither result should be considered ``superior'' to the other since, although this work obtains a smaller dependence on \(  \epsilonX  \) and need not know the link function, the analysis \cite{bahmani2016learning} applies to a larger class of GLMs.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
A generalized version of the popular ``LASSO'' algorithm has been proposed for GLMs in \cite{plan2016generalized}. Note that, for estimation with generalized LASSO, one needs the knowledge of the link function of the generative model. Nonetheless, our results for BIHT are analogous to their results for parameter estimation with LASSO; in particular their sample complexity is \(  \BigO'( \frac{\k {\log( \frac{\n}{\k} )}}{\epsilonX^{2}} ) \); and their results also depends on properties of the link function, namely a scaling factor and noise variance, the former being same as the quantity
 \(  \gammaX  \) as defined in Equation \eqref{eqn:notations:gamma:def}. Subsequently, a very precise error-analysis for generalized LASSO has been performed in \cite{thrampoulidis2015lasso}. To  compare with \THEOREM \ref{thm:approx-error:sparse}, their sample complexity is \(  \BigO'( \frac{( 1-\gammaX^{2} ) \k {\log( \frac{\n}{\k} )}}{\epsilonX^{2}} ) \) (see, \cite[Equation (8)]{thrampoulidis2015lasso}); however, due to the differences in assumption and applicability as explained above, one should exercise caution in such comparisons.
%Our results  parallels theirs, except we show these sample complexities for the case of BIHT algorithm, as opposed to ``generalized LASSO,'' and the generalized las
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
A few other prior works are worth remarking on.
In the following discussion, the parameter, \(  \thetaStar  \), is assumed to have unit norm, but the models incorporate SNR denoted by \(  \betaX \GTR 0  \).
Formulating the (sparse) estimation problem as a convex program, \cite{plan2012robust} shows that the estimation of \(  \thetaStar  \) from binary responses is possible with
%|>>|===========================================================================================|>>|
%\begin{gather*}
\(  \BigO'( \frac{\k {\log( \frac{\n}{\k} )}}{\min \{ \betaX^{2}, 1 \} \epsilonX^{4}} ) \)
%\end{gather*}
%|<<|===========================================================================================|<<|
samples under the Gaussian covariate design.
%\cite{plan2017high} improved on this sample complexity by showing that the ``Average'' algorithm of \cite{servedio1999pac} can estimate the parameter using
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \BigO'( \frac{\k \log( \frac{\n}{\k} )}{\min \{ \| \thetaX \|_{2}^{2}, 1 \} \epsilonX^{2}} )  \)
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%samples.
\cite{plan2017high} improves this sample complexity to
%|>>|===========================================================================================|>>|
%\begin{gather*}
\( \BigO'( \frac{\k {\log( \frac{\n}{\k} )}}{\min \{ \betaX^{2}, 1 \} \epsilonX^{2}} )\)
%\end{gather*}
%|<<|===========================================================================================|<<|
using a method that effectively amounts to the ``Average'' algorithm of \cite{servedio1999pac}.
%, as observed by \cite{hsu2024sample}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Subsequently, in the case of logistic regression with Gaussian covariates, maximum likelihood estimators and their regularized versions have  recently received renewed attention. In this regard, \cite{sur2019modern} and \cite{salehi2019impact} are notable; however their precise asymptotic results are  given in terms of solutions of a system of equations, and are not directly comparable to our sample complexity results. On the other hand, recently
\cite{hsu2024sample} established a lower bound on sample complexity for this case via a variant of Fano's inequality that is \orderwise tight (up to logarithmic factors) when \(  \betaX \leq 1  \).
\cite{hsu2024sample} additionally obtains \orderwise tight (again, up to logarithmic factors) bounds on the sample complexity in logistic regression with the dense parameter space (when \(  \k=\n  \)) for any \(  \betaX  \), summarized in Equation~\eqref{eqn:corollary:approx-error:logistic-regression:m:sparse}.
%as the following bounds on the number of samples:
%|>>|===========================================================================================|>>|
% \begin{align*}
%   \m
%   =
%   \begin{cases}
%   \BigTheta'(\frac{\n}{\betaX^{2} \epsilonX^{2}}) ,&\cIf \betaX \leq 1, \\
%   \BigTheta'(\frac{\n}{\betaX \epsilonX^{2}})     ,&\cIf 1 < \betaX \leq \frac{1}{\epsilonX}, \\
%   \BigTheta'(\frac{\n}{\epsilonX})                ,&\cIf \betaX > \frac{1}{\epsilonX}.
%   \end{cases}
%  &\BigTheta'(\frac{\n}{\betaX^{2} \epsilonX^{2}}) ,&& \cIf \betaX \leq 1, \\
%  &\BigTheta'(\frac{\n}{\betaX \epsilonX^{2}}) ,&& \cIf 1 < \betaX \leq \frac{1}{\epsilonX}, \\
%  &\BigTheta'(\frac{\n}{\epsilonX}) ,&& \cIf \betaX > \frac{1}{\epsilonX}.
%\end{align*}
%|<<|===========================================================================================|<<|
%However, while efficient algorithms were known to achieved the optimal sample complexity when \(  \| \thetaX \|_{2} \leq 1  \) (a convex program can be efficiently solved using standard procedures),
Very recently, ~\cite{chardon2024finite} show that for the $\k= \n$ case MLE achieves optimal sample complexity for $\betaX = \Omega(1).$
While \cite{plan2012robust,plan2017high}
%accompany
%complement
pair
their sample complexity bounds with efficient algorithms for the Gaussian design, \polytime algorithms achieving the optimal sample complexity for logistic regression in the \(  \betaX > 1  \) regimes were not known in general. %, as discussed by \cite{hsu2024sample}.
This work settles this question by proving that BIHT is a computationally efficient algorithm that in fact simultaneously achieves the \orderwise optimal sample complexity (up to logarithmic factors) for all choices of \(  \betaX  \) even with the sparsity constraint.
Here, it is worth remarking that, although we are not aware of a lower bound on the sample complexity for probit regression in the literature, our result for probit model shares the same order sample complexity (which we believe to be tight) as our result for logistic regression.
It should be noted that, for the probit model in the the non-sparse $k=d$ case, when restricted to \(  \betaX > 1  \),
the same sample complexity (up to log factors)  is also achieved by \cite{kuchelmeister2024finite}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\BEGINMARK
\subsection{Comparison to \cite{matsumoto2022binary,matsumoto2024robust}}
\label{outline:intro|>contributions|>comparison-biht}

Although numerous prior works have studied BIHT, \eg \cite{friedlander2021nbiht,jacques2013quantized,jacques2013robust,liu2019one,plan2017high}, the works  most closely aligned with the analysis in this manuscript are \cite{matsumoto2022binary,matsumoto2024robust}, and indeed, some elements of the approach in this work are analogous to components of the analyses in \cite{matsumoto2022binary,matsumoto2024robust}.
However, handling the GLM's randomness---introduced into the model through the function \(  \fFn  \)---requires a novel approach.
It turns out that the normalization step in each iteration of BIHT is
crucial
%essential
to obtain our bound on the approximation error, which distinguishes this work from \cite{matsumoto2022binary,matsumoto2024robust}, even despite the fact that \cite{matsumoto2024robust} considers an alternative (adversarially) noisy setting.
As discussed in Section \ref{outline:intro|>contributions|>techniques}, the analysis in this work largely centers around an invertibility condition that uniformly bounds an expression of the form
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:contributions:3}
  \left\|
    \thetaStar
    -
    \frac{\thetaXX + \hfFn[\JCoords]( \thetaStar, \thetaXX )}{\| \thetaXX + \hfFn[\JCoords]( \thetaStar, \thetaXX ) \|_{2}}
  \right\|_{2}
\end{gather}
%|<<|===========================================================================================|<<|
for all \(  \thetaXX \in \ParamSpace  \) and all \(  \JCoords \subseteq [\n]  \), \(  | \JCoords | \leq \k  \).
In contrast, \cite{matsumoto2022binary,matsumoto2024robust} consider invertibility conditions that bounds expressions of the respective forms
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:contributions:4}
  \|
    \thetaStar
    -
    \thetaXX - \hFn[\Sign;\JCoords]( \thetaStar, \thetaXX )
  \|_{2}
  ,\qquad
  \|
    \thetaStar
    -
    \thetaXX - \hFn[f_{\mathrm{adv}};\JCoords]( \thetaStar, \thetaXX )
  \|_{2}
%  ,\\
%\label{eqn:contributions:5}
%  \left\|
%    \thetaStar
%    -
%    \thetaXX - \hfFn[\JCoords]( \thetaStar, \thetaXX )
%  \right\|_{2}
%  \leq
%  \left\|
%    \thetaStar
%    -
%    \thetaXX - \hFn[\Sign;\JCoords]( \thetaStar, \thetaXX )
%  \right\|_{2}
%  +
%  \left\|
%    \hfFn[\JCoords]( \thetaStar, \thetaStar )
%  \right\|_{2}
,\end{gather}
%|<<|===========================================================================================|<<|
where the parameterizations by the \(  \Sign  \) and \(  f_{\mathrm{adv}}  \) functions can be thought of as the noiseless and adversarially noisy analogs, respectively, to \(  \fFn  \) in our setting for GLMs.
%, and in the latter expression, the function \(  f_{\mathrm{adv}}  \) can be viewed as a function introducing adversarial noise.
Notice that both expressions in \eqref{eqn:contributions:4} omit the sort of normalization that appears in \eqref{eqn:contributions:3}.
But following \cite{matsumoto2022binary,matsumoto2024robust} in this way turns out to be problematic when applying the analysis to GLMs:
in ``lower'' SNR regimes, it would effectively lead to an \(  \Omega(1)  \) additive term in the bound, meaning that the bound on the \errorrate for BIHT would become \(  \Omega(1)  \), rather than \(  \epsilonX  \), regardless of the number of covariates (\ie the sample complexity).
However, accounting for the normalization mitigates this issue to give the desired \(  \epsilonX  \)-\errorrate.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
The intuition behind this is the following.
In expectation, the vector
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \thetaXX + \hfFn[\JCoords]( \thetaStar, \thetaXX )  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
is aligned with \(  \thetaStar  \), as is the noise introduced by the GLM's randomness.
Therefore, the normalization essentially eliminates (or at least reduces) this noise, leading to the desired \errorrate.
Note that, on the other hand, if the noise was instead adversarial, as in \cite{matsumoto2024robust}, it is unlikely that accounting for such normalization in this way would help as the noise can be (adversarially) chosen to be in more or less any direction.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
In fact, an empirical study with logistic regression (\see \FIGURE \ref{fig:error-decay-nbiht-biht-comparison-beta=1-plot:logistic-regression}) corroborates these observations and suggests that BIHT may exhibit different convergence and stability behaviors when it does or does not normalize its approximations, at least at ``higher'' noise levels, which is a notable distinction from observations made in the ``noiseless'' setting, where BIHT has been empirically seen to converge well (and potentially even less brittlely) when the algorithm's approximations are not normalized (\see \FIGURE \ref{fig:error-decay-nbiht-biht-comparison-beta=1-plot:sign}).
Similar empirical behavior is exhibited with probit regression, as well, but such empirical results have been omitted to avoid redundancy.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
All in all, this key observation and distinguishing approach are essential for our analysis of and convergence guarantees for learning GLMs with BIHT, and may potentially even be less an artifact of our analysis and
more an inherent feature of
%more something intrinsic to
the algorithm itself.
%\ENDMARK
\subsection{Other Related Work}
Stochastic gradient descent (SGD) \citep{robbins1951stochastic,sakrison1965efficient} offers an alternative gradient-based method for parameter estimation in GLMs.
\cite{toulis2014statistical} studies the statistical properties of SGD estimates in GLMs when updates are both explicit and implicit.
However, such SGD estimates are asymptotically sub-optimal compared to the maximum likelihood estimates, as noted by \cite{toulis2014statistical}.
As another common approach, approximate message passing (AMP) and its extensions have also been heavily applied to parameter estimation in GLMs, \eg \cite{mondelli2021approximate,venkataramanan2022estimation,zhang2024spectral,barbier2019optimal,zhu2018amp,schniter2016vector,zhao2024vector}.
This includes generalized approximate message passing (GAMP), an algorithm first proposed by \cite{rangan2011generalized}.
%\citep{donoho2009message,rangan2011generalized,mezard1989space,barbier2019optimal}
%The \informationtheoretical optimality of GAMP for specific GLMs was established---for example, linear GLMs with additive Gaussian noise \cite{todo}.
%\ToDo{Maybe see if there are any other examples to add.}
%However,
%%in general,
%more broadly,
While the \errorrate of GAMP is \informationtheoretically optimal for some GLM's, it falls short of
%not achieve
%fail to achieve
the \informationtheoretical optimum for GLMs in some paradigms \citep{barbier2019optimal}.
%Indeed,
In fact,
\cite{barbier2019optimal} characterizes the regions of the parameter space in which GAMP achieves the \informationtheoretical optimal \errorrate or is \informationtheoretically sub-optimal for GLMs.
As a relative to GAMP, another variant of approximation passing called vector approximate message passing (VAMP), introduced by \cite{rangan2019vector}, has been used for estimation in GLMs, initially by \cite{schniter2016vector} and subsequently by, \eg \cite{zhao2024vector}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\subsection{Contributions}
\label{outline:intro|>contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Comparison to Prior Works}
\label{outline:intro|>contributions|>comparison}

\ToDo{Not sure where to put this ..}

Formulating the (sparse) estimation problem as a convex program, \cite{plan2012robust} showed that the estimation of \(  \frac{\thetaStar}{\| \thetaStar \|_{2}}  \) from binary responses is possible with
%|>>|===========================================================================================|>>|
\begin{gather*}
  \BigO'( \frac{\k {\log( \frac{\n}{\k} )}}{\min \{ \| \thetaX \|_{2}^{2}, 1 \} \epsilonX^{4}} )
\end{gather*}
%|<<|===========================================================================================|<<|
samples under the Gaussian covariate design.
%\cite{plan2017high} improved on this sample complexity by showing that the ``Average'' algorithm of \cite{servedio1999pac} can estimate the parameter using
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \BigO'( \frac{\k \log( \frac{\n}{\k} )}{\min \{ \| \thetaX \|_{2}^{2}, 1 \} \epsilonX^{2}} )  \)
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%samples.
\cite{plan2017high} improved this sample complexity to
%|>>|===========================================================================================|>>|
\begin{gather*}
  \BigO'( \frac{\k {\log( \frac{\n}{\k} )}}{\min \{ \| \thetaX \|_{2}^{2}, 1 \} \epsilonX^{2}} )
\end{gather*}
%|<<|===========================================================================================|<<|
using what effectively amounts to the ``Average'' algorithm of \cite{servedio1999pac}.
Subsequently, in the special of logistic regression with Gaussian covariates, \cite{hsu2024sample} established via a variant of Fano's inequality that this is \orderwise tight (up to logarithmic factors) when \(  \| \thetaX \|_{2} \leq 1  \).
\cite{hsu2024sample} additionally obtained \orderwise tight (again, up to logarithmic factors) bounds on the sample complexity in logistic regression with the dense parameter space (when \(  \k=\n  \)) for any \(  \| \thetaX \|_{2}  \), summarized as the following bounds on the number of samples:
%|>>|===========================================================================================|>>|
\begin{align*}
  &\BigTheta'(\frac{\n}{\| \thetaX \|_{2}^{2} \epsilonX^{2}}) ,&& \cIf \| \thetaX \|_{2} \leq 1, \\
  &\BigTheta'(\frac{\n}{\| \thetaX \|_{2} \epsilonX^{2}}) ,&& \cIf 1 < \| \thetaX \|_{2} \leq \frac{1}{\epsilonX}, \\
  &\BigTheta'(\frac{\n}{\epsilonX}) ,&& \cIf \| \thetaX \|_{2} > \frac{1}{\epsilonX}.
\end{align*}
%|<<|===========================================================================================|<<|
%However, while efficient algorithms were known to achieved the optimal sample complexity when \(  \| \thetaX \|_{2} \leq 1  \) (a convex program can be efficiently solved using standard procedures),
While \cite{plan2012robust,plan2017high}
%accompany
%complement
pair
their sample complexity bounds with efficient algorithms for the Gaussian design, \polytime algorithms achieving the optimal sample complexity for logistic regression in the \(  \| \thetaX \|_{2} \geq 1  \) regime were not known, as discussed by \cite{hsu2024sample}.

\ToDo{\cite{bahmani2016learning} specialized to logistic regression.}

\ToDo{Define \errorrate, \(  \epsilonX  \), and \(  \k,\n  \).}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\ToDo{Discussion of 1-bit compressed sensing.}
%Returning to the
%idea
%%notion
%of sparse models, \ldots
\ToDo{Connection of sparse binary GLMs with 1-bit compressed sensing.}
\ToDo{Overview of 1-bit compressed sensing.}

\ToDo{When we compare our results with GLM-tron, we should also discuss \cite{bahmani2016learning}.}
\end{comment}