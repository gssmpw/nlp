\section{Introduction}
\label{outline:intro}

\subsection{Generalized Linear Models}
\label{outline:intro|>glm}

Generalized linear models (GLMs) are a popular statistical paradigm that has been extensively studied since their introduction by \cite{nelder1972generalized} as a generalization
%of
and unifying framework encompassing several
common
%prevalent
statistical models.
%\cite{mccullagh2019generalized,dobson2018introduction,fahrmeir1994multivariate} serve as just a few references for background and surveys on GLMs.
In a GLM, each response (random) variable, \(  \RespV* \in \R \), has distribution with a parameter, \(  \thetaStar \in \ParamSpace  \), taken from a parameter space, \(  \ParamSpace \subseteq \R^{\n}  \), and dependent on a covariate, \(  \CovV  \in \R^{\n} \), such that,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \E[\RespV* \Mid| \CovV] = \linkFn^{-1}( \langle \CovV, \thetaStar \rangle )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
where \(  \linkFn  \) is the \emph{link function} that ``links'' the linear combination, \(  \langle \CovV, \thetaStar \rangle  \), to the conditional expectation of the response, \(  \RespV* \Mid| \CovV  \).
%In alternative terminology, the inverse of the link function, \(  \linkFn^{-1}  \), is referred to as the \emph{transfer function} of the GLM.
This
framework
%model
offers a flexible extension of the popular linear regression model, %---%which supposes a linear relationship,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \E[\RespV* \Mid| \CovV] = \langle \CovV, \thetaStar \rangle  \)
---to allow for nonlinearities.
The reader is referred to \cite{mccullagh2019generalized,dobson2018introduction,fahrmeir1994multivariate,hardin2007generalized} for background on GLMs.

A fundamental problem in GLMs is parameter estimation---that is, the estimation of the parameter, \(  \thetaStar \in \ParamSpace  \), when given \(  \m  \)  i.i.d. samples,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  ( \CovV\VIx{1}, \RespV*_{1} ), \dots, ( \CovV\VIx{\m}, \RespV*_{\m} )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
where the observed responses,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV*_{1}, \dots, \RespV*_{\m}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
and known covariates,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \CovV\VIx{1}, \dots, \CovV\VIx{\m}  \),
are related
%through the link function, \(  g  \), and the conditional expectations of
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \RespV*_{\iIx} \Mid| \CovV\VIx{\iIx}  \), \(  \iIx \in [\m]  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
in the manner stated earlier.
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%are related through the link function, \(  g  \), and the conditional expectations,
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \E[ \RespV*_{\iIx} \Mid| \CovV\VIx{\iIx} ] = \linkFn^{-1}( \langle \CovV\VIx{\iIx}, \thetaX \rangle )  \),
%\(  \iIx \in [\m]  \),
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%the goal is to estimate the parameter, \(  \thetaX \in \ParamSpace  \), where \(  \ParamSpace \subseteq \R^{\n}  \) denotes the parameter space.
%Typically, the covariates, \(  \CovV\VIx{\iIx}  \), \(  \iIx \in [\m]  \), are assumed to be \iid[]
%However, some extensions of the GLM framework, \eg generalized linear mixed models (GLMMs) \citep{stroup2012generalized}, which are outside the scope of this work, account for dependencies between the covariates. %\textcolor{red}{I think you mean $\CovV$ has i.i.d. features, right?}.
%This paper is exclusively about the \iid covariates case.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{comment}
Maximum likelihood estimation (MLE) \citep{myung2003tutorial,richards1961method,gallant1987semi,wald1949note,aitchison1960maximum} is a predominant approach to parameter estimation in GLMs \citep{mcculloch1997maximum,hardin2007generalized}, where the estimates can be obtained through techniques such as iterative weighted least-squares methods \citep{nelder1972generalized,firth1992generalized,hardin2007generalized}, the Newton-Raphson method \citep{jin2022statistical,hardin2007generalized}, and the Gauss-Newton method \citep{wedderburn1974quasi}.
%, and gradient-based methods \citep{kakade2011efficient,bahmani2016learning,todo}.
Gradient descent can also compute maximum likelihood estimates for parameter estimation in GLMs.
%As a
%%prevalent
%%common
%well-studied
%alternative to the above methods, gradient decent on various loss functions has seen some success, \eg \cite{kalai2009isotron,kakade2011efficient,wu2023finite,bahmani2016learning,klivans2017learning}.
%Gradient-based methods have seen some success in GLMs, both when computing the MLE and when minimizing other loss functions.
%, \eg \cite{kalai2009isotron,kakade2011efficient,wu2023finite,bahmani2016learning,klivans2017learning}.
%One such line of work has studied perceptron-type algorithms on the objective
One such line of work has studied gradient descent on the objective,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \objectiveFn( \thetaX ) = \sum_{\iIx} \intlinkFn( \langle \CovV\VIx{\iIx}, \thetaX \rangle ) - \RespV*_{\iIx} \langle \CovV\VIx{\iIx}, \thetaX \rangle  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
where \(  \intlinkFn  \) is defined such that
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \linkFn^{-1}( z ) = \frac{d}{dz} \intlinkFn( z )  \).
\(  \linkFn^{-1} = \partial \intlinkFn  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
\cite{kakade2011efficient} propose a perceptron-like algorithm, \emph{\GLMtron}, for learning %(dense parameter)
GLMs by performing gradient descent on this loss function.
\cite{bahmani2016learning} study a similar gradient decent algorithm which incorporates (sparse) projections.
When the outcomes, \(  \RespV*_{\iIx}  \), are \iid and follow an exponential distribution, \(  \objectiveFn  \) is the negative log-likelihood function, and therefore, under these conditions, the minimizer of \(  \objectiveFn  \) is the maximum likelihood estimate.
Note, however, that \cite{kakade2011efficient,bahmani2016learning} consider more general classes of GLMs, % that encompass those whose outcomes have exponential distributions, 
meaning that their results extend beyond MLE in some cases.
%While \cite{kakade2011efficient} focuses on prediction error, \cite{bahmani2016learning} analyzes the parameter error of what essentially amounts to the same algorithm when applied to GLMs from exponential families.
These works will be discussed further and compared to the contributions of this work in Appendix \ref{outline:intro|>contributions|>comparison}.
%\ToDo{\cite{kalai2009isotron,kakade2011efficient,klivans2017learning,wu2023finite,barbier2019optimal,bahmani2016learning}}
Of note, the \emph{Sparsitron} algorithm of \cite{klivans2017learning}, a multiplicative-weights update method, improves on the error of \GLMtron. % obtained by \cite{kakade2011efficient}.

One consideration in learning GLMs is whether the link function is known.
In contrast to many other works, \eg \cite{nelder1972generalized,kakade2011efficient,bahmani2016learning,barbier2019optimal,plan2016generalized,thrampoulidis2015lasso}, the algorithm studied in this work does not require knowledge of the specific link function. %, meaning that the algorithm can simply be run to learn any
%binary GLM (subject to the mild constraint on the link function)
%GLM within the class considered here,
%without needing to tailor the algorithm to a particular model.
%This generality of the algorithm is convenient and allows modeling of data without stronger distributional assumptions on the data that would arise through imposing a specific link function.
%\MARK{Some other works also consider learning GLMs without access to the link function.
Some other works do consider learning a class of single-index models (SIMs) agnostically, without access to the link function, 
e.g. \cite{gollakota2024agnostically} via omnipredictors,  which are predictors that optimize over all loss functions in a collection. The setup and guarantees of this line of work is quite different than ours.


While the discussion thus far has not constrained the outcomes, \(  \RespV*  \), GLMs with binary outcomes are popular for classification and particularly useful when data is \nonseparable.
For brevity, this class of GLMs will be referred to as \emph{binary GLMs} throughout this manuscript.
%encompass
%include
They contain several important families of models, such as a subset of the exponential family that includes the ubiquitous logistic and probit regression models.
%These include the ubiquitous logistic and probit regression models \citep{todo}.
%As noted by \cite{todo}, the vast majority of the literature developed around binary GLMs has focused on the ubiquitous logistic and probit regression models \citep{todo}.
%As one of the most common frameworks for non-separable data, logistic regression has accumulated an extensive body of work \citep{todo}.
%One line of work has developed around parameter estimation in logistic regression with (standard) Gaussian covariates.
%\cite{plan2012robust} showed that parameter estimation , which was improved by \cite{plan2017high} to
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \BigO'(  )  \)
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%Logistic regression is perhaps the most widely studied binary GLMs.
This work is concerned with binary GLMs with a mild assumption on their link functions, which indeed holds for logistic and probit regression.
%\MARK{In contrast to many other works, \eg \cite{kakade2011efficient,bahmani2016learning}, the algorithm studied in this work does not require knowledge of the specific link function, meaning that the algorithm can simply be run to learn any binary GLM (subject to the mild constraint on the link function) without needing to tailor the algorithm to a particular model.
%This generality of the algorithm is convenient and allows modeling of data without stronger distributional assumptions on the data that would arise through imposing a specific link function.}
Appendix \ref{outline:intro|>contributions|>comparison} briefly surveys some relevant prior works on parameter estimation in binary GLMs.
 
%The imposition of structure on
%Imposed structure of
%the parameter space, \(  \ParamSpace  \), can arise naturally in GLMs.
%For example, 
In treating binary GLMs as (linear) classifiers, where \(  \thetaX  \) becomes a feature vector, the assumption of sparsity in a high-dimensional parameter space---or in
%this viewpoint,
this analogy
the feature space---is a commonplace paradigm in machine learning.
Moreover, interpreting the parameter \(  \thetaX  \) as a signal or data vector, parameter estimation in GLMs can be framed as
the inverse problem of
signal reconstruction from noisy measurements.
In this regard, requiring high-dimensional parameters---or signals in this perspective---to be contained in a sparse subspace is again a frequent consideration. 
%tying GLMs to compressed sensing, a means of acquisition and reconstruction of compressible signals that has been highly successful since the seminal work of \cite{candes2006robust,donoho2006compressed}.
In fact, this present work is motivated by the connection of binary GLMs to \onebitcs, a topic within compressed sensing where the entries of the compressed signal representations are quantized to single bits: the \(  \pm  \) signs of the unquantized values.
The next section, \SECTION \ref{outline:intro|>1bcs}, briefly introduces \onebitcs and explores this connection.
%Subsequently, after formally defining some notations in \SECTION \ref{outline:notations}, \SECTION \ref{outline:notations|>recovery-algs} discusses a reconstruction algorithm for \onebitcs which is at the center of this work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{1-Bit Compressed Sensing and Binary Iterative Hard Thresholding}
\label{outline:intro|>1bcs}

% Since the breakthrough
% %by
% of
% \cite{candes2006robust,donoho2006compressed}, compressed sensing, a method for signal acquisition, compression, and recovery for compressible data,
% %has been highly successful,
% %been highly productive,
% %has flourished,
% has been studied extensively.
% \See \cite{candes2008introduction} for a concise introduction.
% While many works in compressed sensing assume the compressed representations of signals have infinite precision, in practice, acquiring and storing the signals always requires some quantization, the most extreme of which is quantization to single bits: the \(  \pm  \) signs of the responses.
% Beginning with the seminal work of \cite{boufounos20081}, \emph{\onebitcs} has addressed this 1-bit quantization paradigm \citep{boufounos20081,jacques2013robust,jacques2013quantized,plan2013one,plan2012robust,plan2016generalized,plan2017high}.
% \cite{li2018survey} surveys this topic and several algorithms developed for reconstruction under this framework.
%Next, the inverse problem in \onebitcs will be overviewed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\ToDo{This might need to be more high-level.}
%The most basic model in \onebitcs is the noiseless setting with binarized linear responses.
%We will introduce the basic ideas behind \onebitcs under this noiseless setting, and subsequently adjust the definitions to relate binary GLMs to a noisy version of the framework.
%\Onebitcs considers the following inverse problem.
%Fixing \(  \m  \) measurements,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \MeasV\VIx{1}, \dots, \MeasV\VIx{\m} \in \R^{\n}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%and fixing
%For an unknown
%\(  \k  \)-sparse (having at most \(  \k  \) nonzero entries)
%compressible   
%vector
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \thetaStar  \), %\in \SparseSphereSubspace{\k}{\n}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
The \onebitcs problem~\cite{boufounos20081} seeks to recover an unknown vector \(  \thetaStar  \) when given \(  \m  \) responses,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV*_{\iIx} = \Sign(\langle \MeasV\VIx{\iIx}, \thetaStar \rangle )  \),
%\(  \iIx \in [\m]  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
where
\(  \MeasV\VIx{1}, \dots, \MeasV\VIx{\m} \in \R^{\n}  \) are measurement vectors, and only $\pm$ signs of the linear measurements are being kept.
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \fFn : \R \to \{ -1,1 \}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%is a function which individually binarizes the linear responses
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \langle \MeasV\VIx{\iIx}, \thetaStar \rangle  \), \(  \iIx \in [\m]  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
Letting
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \MeasM = ( \MeasV\VIx{1} \;\cdots\; \MeasV\VIx{\m} )^{\T}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
and extending the notation of signs
%\(  \fFn  \)
to vectors by applying it \coordinatewise,
this representation can be written concisely as
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV = ( \RespV*_{1}, \dots, \RespV*_{\m} ) = \Sign( \MeasM \thetaStar )  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
Typically, it is assumed that \(  \m \ll \n  \), and thus,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \RespV = ( \RespV*_{1}, \dots, \RespV*_{\m} )  \)
\(  \RespV  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
is a compressed representation of the original signal vector, \(  \thetaStar  \).
The compressibility of \(  \thetaStar  \) is often incorporated by a notion of sparsity.
In this work, it is assumed that \(  \thetaStar  \) is \emph{\(  \k  \)-sparse}---that is, the vector is supported on at most \(  \k \leq \n  \) nonzero entries.
Additionally, one of the most common choices of measurements is \iid standard Gaussian random vectors, as is the design studied in this work.
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \CovV\VIx{1}, \dots, \CovV\VIx{\m} \simiid \N( \Vec{0}, \Id{\n} )  \),
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%where \(  \N( \Vec{0}, \Id{\n} )  \) denotes the standard multivariate Gaussian distribution (\see formal definition of the notations in \SECTION \ref{outline:notations}).
Most often, in \onebitcs, the unknown vector, \(  \thetaStar  \), is assumed to have unit norm since information about the norm is lost by the binarization of the responses.
%though norm reconstruction is possible via techniques such as dithering, as in \cite{knudson2016one}.
%In the most basic version of \onebitcs, in which responses are noiseless,
%(or from a classification viewpoint, when data is separable),
%the quantization function, \(  \fFn  \), is simply the sign function, which maps values to their \(  \pm  \) signs deterministically.
In relating \onebitcs to binary classification, this noiseless model corresponds with classification of separable data.
Alternatively, in connecting \onebitcs to binary GLMs, the sign function can be replaced by a random function \(  \fFn  \), to incorporate noise,   defined such that each \(  \iIx\Th  \) response takes the value \(  1  \) with probability \(  \pFn( \langle \MeasV\VIx{\iIx}, \thetaStar \rangle )  \) and the value \(  -1  \) with probability \(  1-\pFn( \langle \MeasV\VIx{\iIx}, \thetaStar \rangle )  \) for some function, \(  \pFn : \R \to [0,1]  \).
This setting, which can also be interpreted as binary classification of non-separable data, is studied in this work.

%Note that although the noiseless response setting is the simplest model to formulate in \onebitcs, introducing noise can lead to easier analysis through maximum likelihood techniques, as observed by \cite{plan2016generalized,friedlander2021nbiht}.
%However, while this work introduces noise into the responses, it does not take advantage of maximum likelihood analysis.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%As one means of reconstruction for this inverse problem in \onebitcs, \cite{jacques2013robust} proposed the \emph{binary iterative hard thresholding (BIHT)} algorithm, which is a projective, (sub)gradient descent algorithm on the ReLU loss:
%%|>>|===========================================================================================|>>|
%\begin{gather*}
%  \objectiveFn( \thetaX )
%  =
%  \|  \|_{2}
%\end{gather*}
%%|<<|===========================================================================================|<<|

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\BEGINGRAYOUT
Generalized linear models (GLMs) (\seeeg \cite{mccullagh2019generalized,dobson2018introduction,fahrmeir1994multivariate} as surveys) have received extensive study in both theoretical \cite{todo} and applied works \cite{todo} since their introduction by \cite{nelder1972generalized} as a unifying framework.
In a GLM, each response (random) variable, \(  \RespV*  \), is assumed to be generated from a distribution parameterized by a parameter, \(  \thetaX  \), and dependent on a covariate, \(  \CovV  \), such that in expectation,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \E[\RespV* \Mid| \CovV] = \linkFn^{-1}( \langle \CovV, \thetaX \rangle )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
where \(  \linkFn  \) is the \emph{link} function that ``links'' the linear combination, \(  \langle \CovV, \thetaX \rangle  \), to the conditional expectation of the response, \(  \RespV* \Mid| \CovV  \).
This offers a flexible extension of the popular linear regression model---which supposes a linear relationship,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \E[\RespV* \Mid| \CovV] = \langle \CovV, \thetaX \rangle  \)%
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
---to allow for nonlinearities.
This work
studies
%focuses on
GLMs with binary responses, where the response variables, \(  \RespV*  \), take values in \(  \{ -1,1 \}  \).
These include the well-studied logistic and probit regression models, the former of which is especially the subject of extensive theoretical and empirical study \cite{todo}, and has seen deployment in many applications \cite{todo}.

This work is concerned with the problem of parameter estimation in sparse binary GLMs under a Gaussian design---that is, given \(  \m  \) independent samples,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  ( \CovV\VIx{1}, \RespV*_{1} ), \dots, ( \CovV\VIx{\m}, \RespV*_{\m} ) \in \R^{\n} \times \{ -1,1 \}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
with observed binary responses,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \RespV*_{1}, \dots, \RespV*_{\m} \in \{ -1,1 \}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
and with known \iid standard Gaussian covariates,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \CovV\VIx{1}, \dots, \CovV\VIx{\m} \sim \N( \Vec{0}, \Id{\n} )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
the goal is to estimate the parameter, \(  \thetaX \in \ParamSpace  \), where the parameter space under consideration in this manuscript is the \(  \k  \)-sparse unit sphere, \(  \ParamSpace = \SparseSphereSubspace{\k}{\n}  \).
Here, \(  \N( \Vec{0}, \Id{\n} )  \) denotes the \(  \n  \)-variate standard Gaussian distribution, and \(  \Sphere{\n} \subset \R^{\n}  \) is the origin-centered unit sphere in \(  \R^{\n}  \), while \(  \SparseSubspace{\k}{\n} \subset \R^{\n}  \) is the subset of \(  \n  \)-dimensional, \realvalued vectors whose support sets (nonzero entries) have cardinality at most \(  \k \leq \n  \) (\see \SECTION \ref{outline:notations} for formal definitions).
While this manuscript %primarily
%frames
formulates
%presents
the analysis in terms of sparse parameters, \(  \thetaX  \), the results extend to the dense parameter regime, where \(  \k=\n  \).

Parameter estimation in GLMs has received much
\ENDGRAYOUT
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%