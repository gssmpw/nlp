\section{Conclusion}
In this paper, we  made a case for binary iterative hard thresholding, (projected) gradient descent on the ReLU loss, as a universal learning algorithm for classification tasks. Under very general models of nonseparable (and separable) data, that include logistic, probit, and random classification noise models, BIHT is statistically optimal in parameter estimation. We observe this in practice as well. %simulations as well, as illustrated in Figures \ref{fig:error-decay-plot:logistic-regression}--\ref{fig:m-vs-error-plot:probit}.

We have restricted ourselves to Gaussian covariates, for which our results are tight. However it will be worth exploring the performance of BIHT for more general classes of distributions. We also note an observation that contrasts the noiseless case from generalized linear models, as far as the dynamics of BIHT is concerned. It is known that the normalization step in BIHT, though essential for the convergence proof of \cite{friedlander2021nbiht,matsumoto2022binary}, can be redundant in practice for the noiseless case, see Figure~\ref{fig:error-decay-nbiht-biht-comparison-beta=1-plot:sign} in the appendix. On the other hand, for the setting of this paper, the normalization seems to be crucial for the stability of the algorithm especially in the high-noise regime (as can be seen in Figure~\ref{fig:error-decay-nbiht-biht-comparison-beta=1-plot:logistic-regression} in the appendix). 

Finally, it will  be interesting to analyze BIHT (and a stochastic perceptron-like version of it) from a learning theory perspective, especially in the agnostic setting, where data not necessarily comes from a GLM. Other noise models, such as Massart noise, can also be interesting. 