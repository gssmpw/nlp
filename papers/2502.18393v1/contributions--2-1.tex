\subsection{Main Contributions}
\label{outline:intro|>contributions}

%\subsubsection{Main Results}

One realization that leads to this work is that the BIHT algorithm (Algorithm~\ref{alg:biht:normalized}) can be applicable to any possibly randomized $1$-bit  quantization function, \(  \fFn  \), without any change, and not restricted to the sign function. Therefore, in particular,  Algorithm~\ref{alg:biht:normalized} is a perfect candidate for efficient parameter estimation in binary GLMs, and can be advantageous over some existing estimation methods as it is oblivious of the link function.
%, whereas this work considers a more general choice of \(  \fFn  \) that extends the algorithm to GLMs.


 This work establishes that, when applied to binary GLMs where the link function satisfies a reasonable property (\ie \ASSUMPTION \ref{assumption:p}), the BIHT algorithm iteratively produces approximations that converge to the \(  \epsilonX  \)-ball around the true parameter, \(  \thetaStar  \), with high probability under the Gaussian covariates design with a sample complexity of
%|>>|===========================================================================================|>>|
\begin{gather*}
  % \m = \BigO'(\max\Big[\frac{\max(\alpha,\epsilonX)}{\gammaX^2\epsilonX^{2}}, \frac{1}{\gammaX\epsilonX}\Big] \k \log \left( \frac{\n}{\epsilonX \k} \right) )
  \m = \BigO'(\max\Bigg[\frac{\max(\alpha,\epsilonX)}{\gammaX^2\epsilonX^{2}} \k \log \left( \frac{\n}{\epsilonX \k} \right), \frac{k}{\epsilonX} \log^{3/2} \left( \frac{1}{\epsilonX} \right) \Bigg] )
,\end{gather*}
%|<<|===========================================================================================|<<|
where 
the big-O term hides  some less-significant factors for the sake of readability, and
%the dependencies on the link function of the GLM (in particular, these dependencies are through
\(  \alphaX  \) and \(  \gammaX  \) are  properties of the link function defined in Equations~\eqref{eqn:notations:alpha:def} and \eqref{eqn:notations:gamma:def}
which can be explicitly computed for standard GLMs (leading to  tighter sample complexity \(\BigO'(\k/\epsilonX)\) in some regimes - equivalent to {\em optimistic rate} in learning theory). In addition the estimation error decays at an exponential rate with respect to the number of iterations, \(  \Iter \in \Z_{\geq 0}  \), of the algorithm, bounded from above by
%|>>|===========================================================================================|>>|
\begin{gather*}
  \| \thetaStar - \thetaHat[\Iter] \|_{2}
  =
  \BigO( \epsilon^{1-2^{-\Iter}} )
,\end{gather*}
%|<<|===========================================================================================|<<|
and hence, asymptotically approaches the \errorrate of \(  \epsilonX  \) as \(  \Iter \to \infty  \).
%Due to lower bounds obtained by \cite{plan2017high}, the dependency on the \errorrate, \(  \epsilonX  \), sparsity, \(  \k  \), and dimension, \(  \n  \), is optimal up to logarithmic factors.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%Moreover,
When specialized to two of the most popular binary GLMs, logistic and probit regressions, convergence of BIHT can be shown and explicit expressions
%---in the sense that closed-form bounds on \(  \alphaX  \) and \(  \gammaX  \) are obtained---
can derived for the sample complexities (via closed-form bounds on \(  \alphaX  \) and \(  \gammaX  \)).
%While the dependencies on \(  \alphaX  \) and \(  \gammaX  \) are omitted in the sample complexity stated earlier in this section, they are important in the applications of the main result to
For logistic and probit regressions these
%give
lead to
the optimal scaling with the ``signal-to-noise ratio'' SNR, \(  \betaX  \), and \errorrate, \(  \epsilonX  \), as \(  \betaX  \) is varied.
Notably, as \(  \betaX \to \infty  \), the dependence of the sample complexity on the \errorrate, reduces to \(  \frac{1}{\epsilonX}  \).
%lead to optimal sample complexities as the SNR, \(  \betaX  \), is varied.
%In particular, they give the optimal scaling with \(  \betaX  \), and additionally, as \(  \betaX \to \infty  \), the dependence of the sample complexity on the \errorrate, \(  \epsilonX  \), reduces to \(  \frac{1}{\epsilonX}  \).
%For these models, the sample complexity is split up into three regimes according to the \betaXnamelr or \betaXnamepr, \(  \betaX  \), (recall the incorporation of the \betaXnamelr and \betaXnamepr in \DEFINITIONS \ref{def:p:logistic-regression} and \ref{def:p:probit}, respectively): when
%\Enum[{\label{enum:contributions:beta:i}}]{i} \(  \betaX = \BigO( 1 )  \),
%\Enum[{\label{enum:contributions:beta:ii}}]{ii} \(  \betaX = \BigOmega( 1 ), \BigO( \frac{1}{\epsilonX} )  \), and
%\Enum[{\label{enum:contributions:beta:iii}}]{iii} \(  \betaX = \BigOmega( \frac{1}{\epsilonX} )  \).
Due to lower bounds of \cite{hsu2024sample},
%for logistic regression,
the resultant sample complexity for logistic regression is optimal up to logarithmic factors in all %\betaXnamelr
regimes.
%and an adaptation of their result and proof for probit regression, these sample complexities for logistic and probit regression are optimal up to logarithmic factors in all \betaXname regimes.
As discussed in \cite{hsu2024sample}, for logistic regression with Gaussian covariates,  computationally efficient algorithms  with optimal sample complexity were known only for the high noise regime, %\ref{enum:contributions:beta:i},
e.g., \cite{plan2017high}, and %when \(  \betaX = \infty  \), e.g., \cite{todo},
the noiseless regime~\cite{matsumoto2022binary}, while no such optimal algorithms were known for the intermediate regimes\footnote{However, for $\k=\n$, MLE achieves optimal samples for low noise case, by recent work of~ \cite{chardon2024finite}.}.
%, \ref{enum:contributions:beta:ii} and \ref{enum:contributions:beta:iii}.
Thus, to the best of our knowledge, BIHT is the first computationally efficient algorithm with the optimal sample complexity in all noise regimes
%for parameter estimation
for logistic regression under the Gaussian covariate design.

\paragraph{Organization}
%The remainder of this manuscript is organized as follows.
%This manuscript proceeds as follows. 
In \SECTION \ref{outline:intro|>contributions|>techniques} we give an overview of how our results are obtained. In \SECTION \ref{outline:notations}, we provide the notations used throughout the paper, and formally define binary GLMs and related quantities and assumptions.
In \SECTION \ref{outline:main-result}, the main theorems of this work, which establish the convergence of BIHT to the correct solution for parameter estimation in binary GLMs, are formally stated.
\SECTION \ref{outline:main-result|outline-of-pf} outlines the key steps in the proof of the main results.
\SECTION \ref{outline:main-technical-result} presents the main technical theorem: the invertibility condition for Gaussian covariates. %, whose proof is overviewed in \SECTION \ref{outline:main-technical-result|outline-of-pf}. 
In \APPENDIX \ref{outline:intro|>contributions|>comparison} we compare our techniques with existing results, notably with~\cite{matsumoto2022binary}. 
The main theorems are formally proved in \APPENDIX \ref{outline:pf-main-result}, while formal proofs of the main technical theorems are in \APPENDIX \ref{outline:pf-main-technical-result}.
Lastly, \APPENDIX \ref{outline:concentration-ineq} derives some concentration inequalities that are needed in the proof of the main technical theorems.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\par %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%To prove the convergence of the BIHT approximations in the general case, \ie for any model satisfying \ASSUMPTION \ref{assumption:p}, a property similar to the restricted approximate invertibility conditions of \cite{matsumoto2022binary} and \cite{friedlander2021nbiht} is developed for Gaussian matrices, which is the main technical contribution of this work.
%and shown to hold for Gaussian covariate matrices.
%The establishment of this property follows a similar argument to that which appears in \cite{matsumoto2022binary}.

