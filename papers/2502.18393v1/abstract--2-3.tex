\begin{abstract}

%%In statistical modeling, linear regression is among the most common frameworks to represent data.
%%However, this model
%%enforces
%%%assumes
%%a linear relationship between the data's independent variables, or covariates, and the dependent variables, or outcomes, which limits the types of data that can be captured well by the model.
%Generalized linear models (GLMs) are therefore a more powerful means to represent data by incorporating potential nonlinear dependencies of the outcomes on the covariates.
In statistics, generalized linear models (GLMs) are widely used for modeling
data and can expressively capture 
potential nonlinear dependence of the model's outcomes on its covariates.
Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as  binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called \emph{binary iterative hard thresholding (BIHT)}, for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in  a general class of sparse binary GLMs.
Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron \citep{kakade2011efficient,bahmani2016learning}, BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs.
As two applications, logistic and probit regression are additionally studied.
In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously.
To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression. 
% without enforcing the strong distributional assumption that would otherwise come with tailoring the algorithm to specific link functions.
\end{abstract}