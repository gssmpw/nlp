\section{Main Results\label{outline:main-result}}
The main result for the convergence of \ALGORITHM \ref{alg:biht:normalized} to the correct solution is stated below as \THEOREM \ref{thm:approx-error:sparse}, whose proof is overviewed in \SECTION \ref{outline:main-result|outline-of-pf} and presented in full in \APPENDIX \ref{outline:pf-main-result}.
Its analog for the dense parameter regime, when \(  \k = \n  \) and \(  \ParamSpace = \Sphere{\n}  \), is provided as \COROLLARY \ref{corollary:approx-error:dense}.
Additionally, the specializations of the main result to logistic and probit regression are presented below in \COROLLARY \ref{corollary:approx-error:logistic-regression} %and \ref{corollary:approx-error:probit}, respectively,
which is also proved in \APPENDIX \ref{outline:pf-main-result}.
Essentially, these theorems say that for a sufficiently large number of samples, \(  \m  \), the approximations produced by \ALGORITHM \ref{alg:biht:normalized} converge to the \(  \epsilonX  \)-ball around the true parameter, \(  \thetaStar \in \ParamSpace  \).

%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
\begin{theorem}
\label{thm:approx-error:sparse}
%
% Let
% %|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
% \(  \ConstbLD, \ConstdSD, \Constc > 0  \)
% %|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
% be absolute constants as specified in \DEFINITION \ref{def:univ-const}.
Fix
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \n, \k, \m \in \Z_{+}  \), \(  \k \leq \n  \),
and
\(  \epsilonX, \rhoX \in (0,1)  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
%where
%|>>|===========================================================================================|>>|
% \begin{gather}
%   \deltaX \defeq \frac{\epsilonX}{\frac{3}{2} ( 5+\sqrt{21} )}
%   %\frac{\epsilonX}{\frac{1}{2} ( 3+\sqrt{5} )}
% .\end{gather}
%|<<|===========================================================================================|<<|
%Let \(  \nuX = \nuX( \deltaX ), \tauX = \tauX( \deltaX ) > 0  \) 
%such that
%|>>|===========================================================================================|>>|
% \begin{gather}
%   \nuX
%   =
%   \nuXEXPR
% ,\end{gather}
% %|<<|===========================================================================================|<<|
% and
% %|>>|===========================================================================================|>>|
% \begin{gather}
%   \tauX \defeq \tauXEXPR
% ,\end{gather}
%|<<|===========================================================================================|<<|
%as in \DEFINITION \ref{def:nu-and-tau}.
Write
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \alphaO  \defeq \max \{ \alphaX, \frac{\epsilonX}{\frac{3}2(5+\sqrt{21})} \}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
as in \EQUATION \eqref{eqn:notations:alpha_0:def}.
Let
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \ParamSpace = \SparseSphereSubspace{\k}{\n}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
and fix
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \thetaStar \in \ParamSpace  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
as the unknown parameter.
Fix \(  \m  \) \iid standard Gaussian covariates,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \CovV\VIx{1}, \dots, \CovV\VIx{\m} \sim \N( \Vec{0}, \Id{\n} )  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
and let
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \CovM = ( \CovV\VIx{1} \, \cdots \, \CovV\VIx{\m} )^{\T}  \)
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
be the covariate matrix.
For a number of samples
%|>>|===========================================================================================|>>|
\begin{align}
\nonumber
  \m
  % &\geq
  % \mEXPR[s]
  % \\ \nonumber
  &=
  \mOEXPRS{\epsilonX}[,]
  \\
\label{eqn:approx-error:sparse:m}
.\end{align}
%|<<|===========================================================================================|<<|
of the model specified in \EQUATION \eqref{eqn:notations:f:def} and under \ASSUMPTION \ref{assumption:p}, with probability at least \(  1-\rhoX  \), the sequence of approximations,
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \{ \thetaHat[\Iter] \in \ParamSpace \}_{\Iter \in \Z_{\geq 0}}  \),
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
produced by \ALGORITHM \ref{alg:biht:normalized} with the covariate matrix \(  \CovM  \) converges to the \(  \epsilonX  \)-ball around \(  \thetaStar  \) such that
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:approx-error:sparse:asymptotic}
  \lim_{\Iter \to \infty} \| \thetaStar-\thetaHat[\Iter] \|_{2}
  \leq
  \epsilonX
,\end{gather}
%|<<|===========================================================================================|<<|
with the rate of convergence upper bounded at each \(  \Iter\Th  \) iteration, \(  \Iter \in \Z_{\geq 0}  \) by
%|>>|===========================================================================================|>>|
\begin{gather}
\label{eqn:approx-error:sparse:iterative}
  \| \thetaStar-\thetaHat[\Iter] \|_{2}
  \leq
  2^{2^{-\Iter}}
  \epsilonX^{1-2^{-\Iter}}
.\end{gather}
%|<<|===========================================================================================|<<|
\end{theorem}
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|

There are a few special cases of interest, formalized below as \COROLLARIES \ref{corollary:approx-error:dense}--\ref{corollary:approx-error:logistic-regression}.
Unless stated otherwise, the following corollaries to \THEOREM \ref{thm:approx-error:sparse} use notations consistent with the theorem.
As the first of these, \COROLLARY \ref{corollary:approx-error:dense} takes a look at the dense parameter regime.

%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
\begin{corollary}
\label{corollary:approx-error:dense}
%
Let
%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
\(  \ParamSpace = \Sphere{\n}  \).
%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
Then, under \ASSUMPTION \ref{assumption:p}, the convergence guarantees for \ALGORITHM \ref{alg:biht:normalized} stated in \EQUATIONS \eqref{eqn:approx-error:sparse:asymptotic} and \eqref{eqn:approx-error:sparse:iterative} of \THEOREM \ref{thm:approx-error:sparse} hold for a number of samples
%|>>|===========================================================================================|>>|
\begin{align}
\nonumber
  \m
  &=
\mOEXPRD{\epsilonX}[.]
 % \\
%\label{eqn:approx-error:dense:m}
\end{align}
%|<<|===========================================================================================|<<|
\end{corollary}
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|

%|>>|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|>>|
%|>>|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|>>|
%|>>|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|>>|
% \begin{proof}
% {\COROLLARY \ref{corollary:approx-error:dense}}
% %
% Under the assumed correctness of \THEOREM \ref{thm:approx-error:sparse}, this corollary directly follows by replacing \(  \k  \) with \(  \n  \) in the sample complexity in \EQUATION \eqref{eqn:approx-error:sparse:m} of \THEOREM \ref{thm:approx-error:sparse}. 
% %, and then simplifying the resulting expression to obtain the sample complexity in \EQUATION \eqref{eqn:approx-error:dense:m} of \COROLLARY \ref{corollary:approx-error:dense}.
% % The primary work towards this is computing
% % %|>>|===========================================================================================|>>|
% % \begin{gather*}
% %   \left.
% %   \sum_{\ell=0}^{\k} \binom{\n}{\ell} \sum_{\ell'=0}^{\k} \binom{\n}{\ell'} \left( \frac{3}{\tauX} \right)^{\ell'}
% %   \right|_{\k=\n}
% % ,\end{gather*}
% % %|<<|===========================================================================================|<<|
% % which is calculated as follows:
% % %|>>|===========================================================================================|>>|
% % \begin{align*}
% %   \left.
% %   \sum_{\ell=0}^{\k} \binom{\n}{\ell} \sum_{\ell'=0}^{\k} \binom{\n}{\ell'} \left( \frac{3}{\tauX} \right)^{\ell'}
% %   \right|_{\k=\n}
% %   &=
% %   \sum_{\ell=0}^{\n} \binom{\n}{\ell} \sum_{\ell'=0}^{\n} \binom{\n}{\ell'} \left( \frac{3}{\tauX} \right)^{\ell'}
% %   \\
% %   &=
% %   \left( \sum_{\ell=0}^{\n} \binom{\n}{\ell} \right)
% %   \left( \sum_{\ell=0}^{\n} \binom{\n}{\ell} \left( \frac{3}{\tauX} \right)^{\ell} \right)
% %   \\
% %   &=
% %   \left( \sum_{\ell=0}^{\n} \binom{\n}{\ell} 1^{\ell} 1^{\n-\ell} \right)
% %   \left( \sum_{\ell=0}^{\n} \binom{\n}{\ell} \left( \frac{3}{\tauX} \right)^{\ell} 1^{\n-\ell} \right)
% %   \\
% %   &=
% %   2^{\n}
% %   \left( \frac{3}{\tauX} + 1 \right)^{\n}
% %   \\
% %   &\dCmt{by the binomial theorem (applied twice)}
% %   \\
% %   &=
% %   \left( \frac{6}{\tauX} + 2 \right)^{\n}
% % .\end{align*}
% % %|<<|===========================================================================================|<<|
% % The other calculations are omitted here as they are
% % %simple and immediate
% % %practically instantaneous
% % almost instantaneous.
% %to carry out.
% \end{proof}
%|<<|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|<<|
%|<<|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|<<|
%|<<|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|<<|

We now proceed to applications of the main result to two well-studied GLMs: logistic and probit regression models.
For these models, not only can \THEOREM \ref{thm:approx-error:sparse} be shown to be valid, but also closed-form bounds on the sample complexity in the theorem can be obtained, as per \COROLLARY \ref{corollary:approx-error:logistic-regression} % and \ref{corollary:approx-error:probit}, 
below.
For conciseness, only \orderwise sample complexities are presented in the corollary's statement, while precise bounds on the sample complexity are specified in its proof in \APPENDIX \ref{outline:pf-main-result|pf-main-corollaries}.
%The result for logistic and probit models are presented next.

%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
\begin{corollary}
\label{corollary:approx-error:logistic-regression}
%
%Let \(  \cO, \bO > 0  \) be positive constants, such that
%%|>>|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|>>|
%\(  \cO \defeq \frac{\sqrt{\hfrac{8}{\pi}}}{1+\bO} \geq 1  \).
%%|<<|:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::|<<|
When \(  \pFn  \) is the logistic function with \betaXnamelr \(  \betaX \geq 0  \), as in \DEFINITION \ref{def:p:logistic-regression} (or the probit regression with \betaXnamepr \(  \betaX \geq 0  \) defined as in \DEFINITION \ref{def:p:probit}), there exist positive constants, \(  b_{1}, b_{2} > 0  \), such that the convergence guarantees for \ALGORITHM \ref{alg:biht:normalized} stated in \EQUATIONS \eqref{eqn:approx-error:sparse:asymptotic} and \eqref{eqn:approx-error:sparse:iterative} of \THEOREM \ref{thm:approx-error:sparse} hold if
%|>>|===========================================================================================|>>|
\begin{align}
\label{eqn:corollary:approx-error:logistic-regression:m:sparse}
   \m &= \mOEXPRLR[s]{\epsilonX}[.]
\end{align}
%|<<|===========================================================================================|<<|
Note that the constants \(  b_{1}, b_{2} > 0  \) can be different for the logistic and probit cases.
\end{corollary}
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|

%The final main result established in this work, formalized below as \COROLLARY \ref{corollary:approx-error:probit}, applies \THEOREM \ref{thm:approx-error:sparse} to probit regression.

%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
%|>>|*******************************************************************************************|>>|
% \begin{corollary}
% \label{corollary:approx-error:probit}
% %
% When the function \(  \pFn  \) is defined as in \DEFINITION \ref{def:p:probit} for probit regression with \betaXnamepr \(  \betaX \geq 0  \), there exists a positive constant, \(  b_3,b_4 > 0  \), such that the convergence guarantees for \ALGORITHM \ref{alg:biht:normalized} stated in \EQUATIONS \eqref{eqn:approx-error:sparse:asymptotic} and \eqref{eqn:approx-error:sparse:iterative} of \THEOREM \ref{thm:approx-error:sparse} hold if
% %|>>|===========================================================================================|>>|
% \begin{align}
% \label{eqn:corollary:approx-error:probit:m:sparse}
%    \m &= \mOEXPRPR[s]{\epsilonX}[.]
% \end{align}
% %|<<|===========================================================================================|<<|
% \end{corollary}
% %|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|
%|<<|*******************************************************************************************|<<|

%|>>|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|>>|
%|>>|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|>>|
%|>>|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|>>|
\begin{remark}
\label{remark:}
%
%\ToDo{Check this.}
In \COROLLARY \ref{corollary:approx-error:logistic-regression}, \ALGORITHM \ref{alg:biht:normalized} achieves the \orderwise optimal sample complexity (up to logarithmic factors) for parameter estimation in logistic regression under the Gaussian design.
\See \cite{hsu2024sample} for the establishment of the optimal sample complexity.
\end{remark}
%|<<|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|<<|
%|<<|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|<<|
%|<<|♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢♢|<<|

%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
