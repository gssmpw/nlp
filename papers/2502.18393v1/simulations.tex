
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% \begin{figure}
% %
% %\begin{subfigure}{0.66\textwidth}
% %
% \includegraphics[width=\textwidth]{errors-and-inconsistent--logistic--nbiht--beta=1.png}
% \caption{\label{fig:error-decay-plot:logistic-regression}
% %This experiment was run under the logistic regression model with \betaXnamelr \(  \betaX = 1  \).
% %The \LHS shows the error decay of BIHT approximations empirically and theoretically.
% %%The empirical results were obtained by running \(  100  \) trials of \(  50  \) iterations of the normalized BIHT algorithm when recovering random \(  k  \)-sparse unit vectors.
% %The \RHS displays the fraction of covariates which fall onto opposite sides of
% %the hyperplanes associated with
% %%the hyperplane whose normal vector is
% %the true parameter, \(  \thetaStar  \), and the approximations.
% %The empirical results were obtained by running \(  100  \) trials of recovering random \(  k  \)-sparse unit vectors via the normalized BIHT algorithm for \(  25  \) iterations.
% %The parameters were set as: \(  d = 2000  \), \(  k = 5  \), \(  n = 3000  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
% This experiment demonstrates the convergence behavior of the normalized BIHT algorithm under logistic regression with \betaXnamelr \(  \betaX = 1  \).
% The left plot shows the empirical and theoretical error decay of the BIHT approximations.
% The right plot shows the fraction of covariates which fall onto opposite sides of
% the hyperplanes associated with
% %the hyperplane whose normal vector is
% the true parameter, \(  \thetaStar  \), and the approximations.
% The experiment ran \(  100  \) trials of recovery for \(  25  \) iterations with parameters: \(  d = 2000  \), \(  k = 5  \), \(  n = 3000  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
% }
% %
% %\Description[Two box plots showing the rates of decay of (1) the error, and (2) the fraction of mismatches in a numerical experiment.]{Two side-by-side box plots displaying the number of iterations of BIHT versus (1) the error, and (2) the fraction of mismatches in a numerical experiment, which are seen to track with the theoretically predicted rates of decay. Results are shown for each of the first 25 iterations.}
% %
% %\end{subfigure}
% \end{figure}
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% \begin{figure}
% %\begin{subfigure}{0.33\textwidth}
% %
% \centering
% \includegraphics[width=0.85\textwidth]{m-vs-errors--logistic--nbiht--beta=1.png}
% \caption{\label{fig:m-vs-error-plot:logistic-regression}%
% %This plot shows the (roughly linear) relationship between the number of covariates, \(  d  \), (\(  x  \)-axis) and the reciprocal of the squared error (\(  y  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the normalized BIHT algorithm under the logistic regression model with \betaXnamelr \(  \betaX = 1  \).
% %The sparsity and dimension parameters were set, respectively, as: \(  d = 2000  \) and \(  k = 5  \).%
% This experiment studies the relationship between the \errorrate and sample complexity for the normalized BIHT algorithm under logistic regression with \betaXnamelr \(  \betaX = 1  \).
% This plot shows the reciprocal of the squared error (\(  y  \)-axis) scaling roughly linearly with the number of covariates, \(  d  \), (\(  x  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the algorithm.
% The experiment ran 100 trials of recovery with dimension and sparsity parameters: \(  d = 2000  \) and \(  k = 5  \).%
% }
% %
% %\Description[A box plot of the relationship between the number of measurements and reciprocal of the error in a numerical experiment.]{A box plot displaying the approximately linear scaling of the number of measurements versus the reciprocal of the error in a numerical experiment, where the number of measurements is varied between 100 and 2000 at increments of 100.}
% %\end{subfigure}
% \end{figure}
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|

% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% \begin{figure}
% %
% %\begin{subfigure}{0.66\textwidth}
% %
% \includegraphics[width=\textwidth]{errors-and-inconsistent--probit--nbiht--beta=1.png}
% \caption{\label{fig:error-decay-plot:probit}
% %This experiment was run under the probit regression model with \betaXnamepr \(  \betaX = 1  \).
% %The \LHS shows the empirical and theoretical error decay of the BIHT approximations.
% %The \RHS shows the fraction of covariates which fall onto opposite sides of
% %the hyperplanes associated with
% %%the hyperplane whose normal vector is
% %the true parameter, \(  \thetaStar  \), and the approximations.
% %The experiment ran \(  100  \) trials of recovery with the normalized BIHT algorithm for \(  25  \) iterations under the probit regression model with \betaXnamepr \(  \betaX = 1  \) with the parameters: \(  d = 2000  \), \(  k = 5  \), \(  n = 3000  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
% This experiment demonstrates the convergence behavior of the normalized BIHT algorithm under probit regression with \betaXnamepr \(  \betaX = 1  \).
% The left plot shows the empirical and theoretical error decay of the BIHT approximations.
% The right plot shows the fraction of covariates which fall onto opposite sides of
% the hyperplanes associated with
% %the hyperplane whose normal vector is
% the true parameter, \(  \thetaStar  \), and the approximations.
% The experiment ran \(  100  \) trials of recovery for \(  25  \) iterations with parameters: \(  d = 2000  \), \(  k = 5  \), \(  n = 3000  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
% }
% %
% %\Description[Two box plots showing the rates of decay of (1) the error, and (2) the fraction of mismatches in a numerical experiment.]{Two side-by-side box plots displaying the number of iterations of BIHT versus (1) the error, and (2) the fraction of mismatches in a numerical experiment, which are seen to track with the theoretically predicted rates of decay. Results are shown for each of the first 25 iterations.}
% %
% %\end{subfigure}
% \end{figure}
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% \begin{figure}
% %\begin{subfigure}{0.33\textwidth}
% %
% \centering
% \includegraphics[width=0.85\textwidth]{m-vs-errors--probit--nbiht--beta=1.png}
% \caption{\label{fig:m-vs-error-plot:probit}%
% %This plot shows the (roughly linear) relationship between the number of covariates, \(  d  \), (\(  x  \)-axis) and the reciprocal of the squared error (\(  y  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the normalized BIHT algorithm under the probit regression model with \betaXnamepr \(  \betaX = 1  \).
% %The sparsity and dimension parameters were set, respectively, as: \(  d = 2000  \) and \(  k = 5  \).%
% This experiment studies the relationship between the \errorrate and sample complexity for the normalized BIHT algorithm under probit regression with \betaXnamepr \(  \betaX = 1  \).
% This plot shows the reciprocal of the squared error (\(  y  \)-axis) scaling roughly linearly with the number of covariates, \(  d  \), (\(  x  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the algorithm.
% The experiment ran 100 trials of recovery with dimension and sparsity parameters: \(  d = 2000  \) and \(  k = 5  \).%
% }
% %
% %\Description[A box plot of the relationship between the number of measurements and reciprocal of the error in a numerical experiment.]{A box plot displaying the approximately linear scaling of the number of measurements versus the reciprocal of the error in a numerical experiment, where the number of measurements is varied between 100 and 2000 at increments of 100.}
% %\end{subfigure}
% \end{figure}
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
% %|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|

% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
% %|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
\begin{figure}
%
%\begin{subfigure}{0.66\textwidth}
%
\includegraphics[width=\textwidth]{errors--logistic--nbiht-beta=1.png}
\includegraphics[width=\textwidth]{errors--logistic--biht-beta=1.png}
\caption{\label{fig:error-decay-nbiht-biht-comparison-beta=1-plot:logistic-regression}
This experiment compares the iterative approximation errors for BIHT with (top plot) and without (bottom plot) the normalization step of the algorithm under logistic regression with \betaXnamelr \(  \betaX = 1  \).
The error is the \(  \lnorm{2}  \)-distance between the normalized approximation and the true parameter.
In both plots, the theoretical error decay for the normalized version of BIHT with logistic regression is displayed for reference.
The experiment ran \(  100  \) trials of recovery for \(  30  \) iterations with parameters: \(  d = 2000  \), \(  k = 5  \), \(  n = 3000  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
}
%
%\end{subfigure}
\end{figure}
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
\begin{figure}
%
%\begin{subfigure}{0.66\textwidth}
%
\includegraphics[width=\textwidth]{errors--sign--nbiht.png}
\includegraphics[width=\textwidth]{errors--sign--biht.png}
\caption{\label{fig:error-decay-nbiht-biht-comparison-beta=1-plot:sign}
This experiment compares the iterative approximation errors for BIHT with (top plot) and without (bottom plot) the normalization step of the algorithm under the noiseless model.
The error is the \(  \lnorm{2}  \)-distance between the normalized approximation and the true parameter.
In both plots, the theoretical error decay for the normalized version of BIHT in the noiseless setting---established by \cite{matsumoto2022binary}---is displayed for reference.
The experiment ran \(  100  \) trials of recovery for \(  30  \) iterations with parameters: \(  d = 2000  \), \(  k = 5  \), \(  n = 700  \), \(  \epsilonX = 0.25  \), and \(  \rhoX = 0.25  \).%
}
%
%\end{subfigure}
\end{figure}
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

\begin{comment}

%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
\begin{figure}
%
%\begin{subfigure}{0.66\textwidth}
%
\includegraphics[width=\textwidth]{lr-nbiht-errors-and-inconsistent-n=2000-k=5-m=1000-epsilon=0.05-rho=0.05-num_trials=100-figsize=(28, 6)--2-1--cropped.png}%{lr-nbiht-errors-and-inconsistent-n=2000-k=5-m=1000-epsilon=0.05-rho=0.05-num_trials=100--2-1b.png}
\caption{\label{fig:error-decay-plot:logistic-regression}
This experiment was run under the logistic regression model with \betaXnamelr \(  \betaX = 1  \).
The \LHS shows the error decay of BIHT approximations empirically and theoretically.
%The empirical results were obtained by running \(  100  \) trials of \(  50  \) iterations of the normalized BIHT algorithm when recovering random \(  k  \)-sparse unit vectors.
The \RHS displays the fraction of covariates which fall onto opposite sides of
the hyperplanes associated with
%the hyperplane whose normal vector is
the true parameter, \(  \thetaStar  \), and the approximations.
The empirical results were obtained by running \(  100  \) trials of recovering random \(  k  \)-sparse unit vectors via the normalized BIHT algorithm for \(  25  \) iterations.
The parameters were set as: \(  d = 2000  \), \(  k = 5  \), \(  n = 1000  \), \(  \epsilonX = 0.05  \), and \(  \rhoX = 0.05  \).%
}
%
%\Description[Two box plots showing the rates of decay of (1) the error, and (2) the fraction of mismatches in a numerical experiment.]{Two side-by-side box plots displaying the number of iterations of BIHT versus (1) the error, and (2) the fraction of mismatches in a numerical experiment, which are seen to track with the theoretically predicted rates of decay. Results are shown for each of the first 25 iterations.}
%
%\end{subfigure}
\end{figure}
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
\begin{figure}
%\begin{subfigure}{0.33\textwidth}
%
\centering
\includegraphics[width=0.85\textwidth]{lr-nbiht-m-vs-errors--n=2000-k=5-m=[100-2000]-epsilon=0.1-rho=0.05-num_trials=100-figsize=(24, 6)--2-1--cropped.png}%{lr-nbiht-m-vs-errors--n=2000-k=5-m=[100-2000]-epsilon=0.1-rho=0.05-num_trials=100--2-1b.png}
\caption{\label{fig:m-vs-error-plot:logistic-regression}%
This plot shows the (roughly linear) relationship between the number of covariates, \(  d  \), (\(  x  \)-axis) and the reciprocal of the squared error (\(  y  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the normalized BIHT algorithm under the logistic regression model with \betaXnamelr \(  \betaX = 1  \).
The sparsity and dimension parameters were set, respectively, as: \(  d = 2000  \) and \(  k = 5  \).%
}
%
%\Description[A box plot of the relationship between the number of measurements and reciprocal of the error in a numerical experiment.]{A box plot displaying the approximately linear scaling of the number of measurements versus the reciprocal of the error in a numerical experiment, where the number of measurements is varied between 100 and 2000 at increments of 100.}
%\end{subfigure}
\end{figure}
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|

%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
\begin{figure}
%
%\begin{subfigure}{0.66\textwidth}
%
\includegraphics[width=\textwidth]{pr-nbiht-errors-and-inconsistent-n=2000-k=5-m=1000-epsilon=0.05-rho=0.05-num_trials=100-figsize=(28, 6)--2-1--cropped.png}%{pr-nbiht-errors-and-inconsistent-n=2000-k=5-m=1000-epsilon=0.05-rho=0.05-num_trials=100--2-1c.png}
\caption{\label{fig:error-decay-plot:probit}
This experiment was run under the probit regression model with \betaXnamepr \(  \betaX = 1  \).
The \LHS shows the error decay of BIHT approximations empirically and theoretically.
%The empirical results were obtained by running \(  100  \) trials of \(  50  \) iterations of the normalized BIHT algorithm when recovering random \(  k  \)-sparse unit vectors.
The \RHS displays the fraction of covariates which fall onto opposite sides of
the hyperplanes associated with
%the hyperplane whose normal vector is
the true parameter, \(  \thetaStar  \), and the approximations.
The empirical results were obtained by running \(  100  \) trials of recovering random \(  k  \)-sparse unit vectors via the normalized BIHT algorithm for \(  25  \) iterations.
The parameters were set as: \(  d = 2000  \), \(  k = 5  \), \(  n = 1000  \), \(  \epsilonX = 0.05  \), and \(  \rhoX = 0.05  \).%
}
%
%\Description[Two box plots showing the rates of decay of (1) the error, and (2) the fraction of mismatches in a numerical experiment.]{Two side-by-side box plots displaying the number of iterations of BIHT versus (1) the error, and (2) the fraction of mismatches in a numerical experiment, which are seen to track with the theoretically predicted rates of decay. Results are shown for each of the first 25 iterations.}
%
%\end{subfigure}
\end{figure}
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|
%|>>|///////////////////////////////////////////////////////////////////////////////////////////|>>|

%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
\begin{figure}
%\begin{subfigure}{0.33\textwidth}
%
\centering
\includegraphics[width=0.85\textwidth]{pr-nbiht-m-vs-errors--n=2000-k=5-m=[100-2000]-epsilon=0.1-rho=0.05-num_trials=100-figsize=(24, 6)--2-1--cropped.png}%{pr-nbiht-m-vs-errors--n=2000-k=5-m=[100-2000]-epsilon=0.1-rho=0.05-num_trials=100--2-1f.png}
\caption{\label{fig:m-vs-error-plot:probit}%
This plot shows the (roughly linear) relationship between the number of covariates, \(  d  \), (\(  x  \)-axis) and the reciprocal of the squared error (\(  y  \)-axis), where the error is the \(  \lnorm{2}  \)-distance between the true parameter and the approximation obtained after \(  25  \) iterations of the normalized BIHT algorithm under the probit regression model with \betaXnamepr \(  \betaX = 1  \).
The sparsity and dimension parameters were set, respectively, as: \(  d = 2000  \) and \(  k = 5  \).%
}
%
%\Description[A box plot of the relationship between the number of measurements and reciprocal of the error in a numerical experiment.]{A box plot displaying the approximately linear scaling of the number of measurements versus the reciprocal of the error in a numerical experiment, where the number of measurements is varied between 100 and 2000 at increments of 100.}
%\end{subfigure}
\end{figure}
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|
%|<<|\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\|<<|

\end{comment}