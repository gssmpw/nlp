\begin{table*}[t]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l|cccc} % Begin the table with specified column alignments
\toprule
Method & Training-free & General Policy & Smaller LM Guidance & Implicit Preference \\ 
\midrule
MORLHF~\citep{li2020deep}        & \textcolor{red}{$\times$}  & \textcolor{green}{$\checkmark$} & - & \textcolor{green}{$\checkmark$}  \\
MODPO~\citep{zhou2024beyond}   & \textcolor{red}{$\times$}      & \textcolor{green}{$\checkmark$} & -   & \textcolor{green}{$\checkmark$}  \\
Personalized soups~\citep{jang2023personalized}  & \textcolor{red}{$\times$}& \textcolor{red}{$\times$}  & \textcolor{red}{$\times$}  & \textcolor{red}{$\times$} \\
Preference Prompting~\citep{jang2023personalized} &  \textcolor{green}{$\checkmark$}  &\textcolor{green}{$\checkmark$}  & -  & \textcolor{red}{$\times$} \\
Rewarded soups~\citep{rame2024rewarded}   & \textcolor{red}{$\times$} & \textcolor{red}{$\times$}  & \textcolor{red}{$\times$}  & \textcolor{red}{$\times$}  \\
RiC~\citep{yang2024rewards}    & \textcolor{red}{$\times$}       & -  & \textcolor{red}{$\times$} & \textcolor{red}{$\times$}  \\
DPA~\citep{DPA}     & \textcolor{red}{$\times$}      & \textcolor{green}{$\checkmark$} & - & \textcolor{red}{$\times$}  \\
ARGS~\citep{khanov2024args}   & \textcolor{green}{$\checkmark$}       & \textcolor{green}{$\checkmark$}  & \textcolor{green}{$\checkmark$} & \textcolor{green}{$\checkmark$}  \\
MOD~\citep{shi2406decoding}    & \textcolor{green}{$\checkmark$}       & \textcolor{red}{$\times$}  & \textcolor{red}{$\times$}   & \textcolor{red}{$\times$}  \\
MetaAligner~\citep{yang2024metaaligner}  & \textcolor{green}{$\checkmark$}  & \textcolor{green}{$\checkmark$}  & \textcolor{green}{$\checkmark$}   & \textcolor{red}{$\times$} \\
PAD~\citep{chen2024pad}  & \textcolor{green}{$\checkmark$}  & \textcolor{green}{$\checkmark$}  & \textcolor{red}{$\times$}   & \textcolor{red}{$\times$} \\
\midrule
\includegraphics[height=1.8ex]{figs/racer.png} Drift~(Ours)   & \textcolor{green}{$\checkmark$}         & \textcolor{green}{$\checkmark$} & \textcolor{green}{$\checkmark$} & \textcolor{green}{$\checkmark$} \\
\bottomrule
\end{tabular}
}
\caption{Key characteristics of previous methods and Drift.} % Table caption
\label{tab:method_comparison}
\end{table*}

\section{Proof}

\subsection{Derivation of the RL Closed-Form Solution}
\label{proof-rl}

We want to solve the following optimization problem (for a single variable \(x\)):
\[
\max_{\theta} \Bigl[\,r(x)\;-\;\beta \,\log \tfrac{\pi_{\theta}(x)}{\pi_{base}(x)}\Bigr].
\]
Define \(\pi(x) = \pi_{\theta}(x)\). The quantity we want to maximize can be thought of as an expectation under \(\pi(x)\):
\[
\max_{\pi} \int \pi(x) \Bigl[r(x) - \beta\,\log \tfrac{\pi(x)}{\pi_{base}(x)}\Bigr] \, dx,
\]
subject to
\[
\int \pi(x)\, dx = 1 \quad\text{and}\quad \pi(x) \ge 0.
\]

Introduce a Lagrange multiplier \(\lambda\) to enforce the normalization constraint \(\int \pi(x)\,dx = 1\). The Lagrangian is
\begin{equation}
\begin{split}
    \mathcal{L}[\pi, \lambda] 
= &\int \pi(x)\,\Bigl[r(x) - \beta\,\log \tfrac{\pi(x)}{\pi_{base}(x)}\Bigr]\,dx 
\;\\- &\;\lambda\!\Bigl(\int \pi(x)\,dx - 1\Bigr). \nonumber
\end{split}
\end{equation}

We now take the functional derivative of \(\mathcal{L}\) w.r.t.\ \(\pi(x)\) and set it to zero for optimality:
\[
\frac{\delta \mathcal{L}}{\delta \pi(x)} 
= r(x) 
- \beta \Bigl[\log \tfrac{\pi(x)}{\pi_{base}(x)} + 1 \Bigr]
- \lambda 
= 0.
\]

Rearranging:
\[
r(x) - \beta \,\log \tfrac{\pi(x)}{\pi_{base}(x)} - \beta - \lambda = 0,
\]
which implies
\[
\beta \,\log \tfrac{\pi(x)}{\pi_{base}(x)} = r(x) - \beta - \lambda.
\]

Exponentiate both sides:
\[
\tfrac{\pi(x)}{\pi_{base}(x)} 
= \exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr) \;\exp\!\Bigl(- 1 - \tfrac{\lambda}{\beta}\Bigr).
\]
So
\[
\pi(x) 
= \pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr)\,\exp\!\Bigl(-1 - \tfrac{\lambda}{\beta}\Bigr).
\]
Let $C = \exp\!\Bigl(-1 - \tfrac{\lambda}{\beta}\Bigr)$.
Hence
\[
\pi(x) = C\,\pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr).
\]

We find \(C\) by imposing the constraint \(\int \pi(x)\,dx = 1\):
\[
1 
= \int \pi(x)\,dx 
= C \int \pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr)\,dx.
\]
Therefore
\[
C 
= \biggl[\int \pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr)\,dx \biggr]^{-1}.
\]

Putting it all together, the optimal distribution \(\pi^{*}(x)\) is
\[
\pi^{*}(x) 
= \frac{\pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr)}
       {\displaystyle\int \pi_{base}(x)\,\exp\!\Bigl(\tfrac{r(x)}{\beta}\Bigr)\,dx}.
\]

This shows that the optimal solution is a \emph{Boltzmann-like} (or \emph{softmax}) distribution given by weighting the reference distribution \(\pi_{base}(x)\) with the exponential of the scaled reward \(r(x)/\beta\).

\subsection{Expanded Explanation for Drift Decoding}
\label{appendix:drift-decoding-proof}
In Section~\ref{sec:drift-decoding}, we introduced the following target distribution for Drift Decoding:
\begin{equation}
    \label{eq:drift-decoding-target-dist-appx}
    \tilde{\pi}(w) \;\propto\; \pi_{\text{LLM}}(w) 
    \;\prod_{i=1}^{k} \Bigl(\tfrac{\pi_i(w)}{\pi_{\text{base}}(w)}\Bigr)^{\frac{p_i}{\beta}},
\end{equation}
where \(\pi_{\text{LLM}}(w)\) is the probability of token \(w\) under the LLM, \(\pi_i(w)\) is the probability of token \(w\) under an attribute-specific prompt (i.e., \(\pi(\cdot \mid s_i)\)), \(\pi_{\text{base}}(w)\) is the probability under a base prompt, and \(p_i\) is the weight for the \(i\)-th attribute estimated by \textit{Drift Approximation}. The hyperparameter \(\beta\) controls the strength of personalization via KL regularization.
Then Eq~\eqref{eq:drift-decoding-target-dist-appx} can be equivalently written in \emph{logit space} as
\begin{equation}
\begin{split}
  \tilde{\pi}(w) 
  \;&=\; 
  \text{softmax}\!\Bigl[
     h^{\text{LLM}}(w)
     \;\\ \nonumber &+\;
     \frac{1}{\beta}\sum_{i=1}^{k} 
        p_i\,\bigl(h^i(w) \;-\; h^{\text{base}}(w)\bigr)
  \Bigr], 
\end{split}
\end{equation}
where \(h^\text{LLM}\), \(h^i\), and \(h^\text{base}\) are the \emph{logits} (i.e., \(\log\)-probabilities) of \(\pi_{\text{LLM}}\), \(\pi_i\), and \(\pi_{\text{base}}\), respectively.
By definition of the logits, let $h^{\text{LLM}}(w) = \log \pi_{\text{LLM}}(w)$, $h^i(w) = \log \pi_i(w)$,  $h^{\text{base}}(w) = \log \pi_{\text{base}}(w)$.
Then Eq~\eqref{eq:drift-decoding-target-dist-appx} can be rewritten as
\begin{equation}
\begin{split}
  \tilde{\pi}(w)
  \;&\propto\;
  \exp\bigl(h^{\text{LLM}}(w)\bigr)
  \;\\ \nonumber &\prod_{i=1}^{k} 
    \exp\!\Bigl(
      \frac{p_i}{\beta}\,\bigl[
         h^i(w) \;-\; h^{\text{base}}(w)
      \bigr]
    \Bigr).
\end{split}
\end{equation}
Combining the exponential terms, we get
\begin{equation}
\begin{split}
  \tilde{\pi}(w)
  \;&\propto\;
  \exp\!\Bigl[
    h^{\text{LLM}}(w)
    \;\\ \nonumber &+\;
    \frac{1}{\beta}\sum_{i=1}^k p_i\bigl(h^i(w) \;-\; h^{\text{base}}(w)\bigr)
  \Bigr].
\end{split}
\end{equation}
Since the \(\mathrm{softmax}\) operation normalizes these exponentials to sum to 1 over all possible tokens \(w\), it follows that
{\tiny
\[
  \tilde{\pi}(w)
  \;=\;
  \frac{
    \exp\!\Bigl[
      h^{\text{LLM}}(w)
      + \frac{1}{\beta}\sum_{i=1}^k p_i\bigl(h^i(w) - h^{\text{base}}(w)\bigr)
    \Bigr]
  }{
    \sum_{w'} 
      \exp\!\Bigl[
        h^{\text{LLM}}(w')
        + \frac{1}{\beta}\sum_{i=1}^k p_i\bigl(h^i(w') - h^{\text{base}}(w')\bigr)
      \Bigr]
  }
\]
}
\[
  \;=\;
  \text{softmax}\Bigl[
     h^{\text{LLM}} 
     + \frac{1}{\beta}\sum_{i=1}^k p_i\,\bigl(h^i - h^{\text{base}}\bigr)
  \Bigr][w].
\]
% \[
%   \tilde{\pi}(w)
%   \;=\;
%   \frac{
%     \exp\!\Bigl[
%       h^{\text{LLM}}(w)
%       + \frac{1}{\beta}\sum_{i=1}^k p_i\bigl(h^i(w) - h^{\text{base}}(w)\bigr)
%     \Bigr]
%   }{
%     \sum_{w'} 
%       \exp\!\Bigl[
%         h^{\text{LLM}}(w')
%         + \frac{1}{\beta}\sum_{i=1}^k p_i\bigl(h^i(w') - h^{\text{base}}(w')\bigr)
%       \Bigr]
%   }
%   \;=\;
%   \text{softmax}\Bigl[
%      h^{\text{LLM}} 
%      + \frac{1}{\beta}\sum_{i=1}^k p_i\,\bigl(h^i - h^{\text{base}}\bigr)
%   \Bigr][w].
% \]
This completes the proof.

\section{Details of Perspective Dataset}
\label{sec:perspective-details}
In this section, we describe the principles underlying the design of our Perspective dataset. To evaluate personal preferences accurately, the evaluation must adhere exactly to the individual criteria used during the annotation of the training data. In other words, the data construction process and evaluation pipeline must be identical, which makes evaluations based on actual human responses challenging. Therefore, our primary objective is to enable reliable evaluation even using virtual personas.

\subsection{Dataset Construction}

For constructing the dataset, a diverse set of well-defined persona concepts was essential. To this end, we leveraged the Multifacet~\citep{multifacet} dataset, which defines various dimensions that can be combined to create a wide range of persona concepts. In the Multifacet dataset, each persona is associated with one question and three answers. However, our methodology required a substantial number of question–preference pairs per persona. To achieve this, we followed these steps:
\begin{enumerate}
    \item \textbf{Collection:} Gather ten distinct, non-overlapping personas from diverse domains within the Multifacet dataset.
    \item \textbf{Question Selection:} For each persona, select related questions based on specific sub-dimensions.
    \item \textbf{Evaluation:} Instruct GPT-4 to evaluate the triplets consisting of one question and three answers (\(\{Q, A, A, A\}\)) using system prompts tailored to each persona. \texttt{gpt-4-turbo} assigns scores to each QA pair, thereby determining the preferred and less preferred answers.
\end{enumerate}
\noindent
During the creation process, \texttt{gpt-4-turbo} evaluated the answers using an explicitly defined persona. This same approach can later be adopted to assess generation results, ensuring a reliable evaluation procedure. As a result, we generated an average of 7,642 questions and 15,284 answers per persona. Below shows an example instance from the dataset, featuring a specific persona along with its corresponding QAA triplet and associated scores.

\begin{Verbatim}[fontsize=\small]
'gold_persona': "Assume the role of a seasoned
consultant with advanced expertise in the 
construction and engineering sectors ...,
'prompt': 'In Python, I have encountered ...,
'win': 'Certainly! The header `# -*- ...,
'lose': "Certainly, diving into the `# -*- ...,
 'win_score': 5,
 'lose_score': 4
\end{Verbatim}


\subsection{Comparison to the PRISM Dataset Instance}

The PRISM dataset provides user personal information and self-introductions as shown below:

\begin{Verbatim}[fontsize=\small]
'user_id': 'user1008',
'lm_familiarity': 'Somewhat familiar',
'lm_indirect_use': 'Yes',
'lm_direct_use': 'Yes',
'lm_frequency_use': 'Every day',
'self_description': "The importance in my
life right now is having ...",
'age': '45-54 years old',
'gender': 'Female',
'employment_status': 'Working full-time',
'education': 'Some University but no degree',
'marital_status': 'Divorced / Separated',
'english_proficiency': 'Native speaker',
'study_locale': 'us',
'religion': {'self_described': 'christianity',
              'categorised': 'Christian',
              'simplified': 'Christian'},
'ethnicity': {'self_described': 'white',
              'categorised': 'White',
              'simplified': 'White'},
'location': {'birth_country': 'Australia',
             'birth_countryISO': 'AUS',
             'birth_region': 'Oceania',
             'birth_subregion': 'Australia ...',
             'reside_country': 'United States',
             'reside_region': 'Americas',
             'reside_subregion': 'Northern ...',
             'reside_countryISO': 'USA',
             'same_birth_reside_country': 'No'},
'lm_usecases': {'homework_assistance': 0,
                'research': 1,
                'source_suggestions': 0,
                'professional_work': 0,
                'creative_writing': 1,
                'casual_conversation': 1,
                'personal_recommendations': 1,
                'daily_productivity': 0,
                'technical_...': 0,
                'travel_guidance': 0,
                'lifestyle_and_hobbies': 1,
                'well-being_guidance': 1,
                'medical_guidance': 1,
                'financial_guidance': 0,
                'games': 1,
                'historical_or_news_insight': 1,
                'relationship_advice': 1,
                'language_learning': 1,
                'other': 0,
                'other_text': None}
\end{Verbatim}

Although the PRISM dataset also provides explicit persona information through user profiles, there is no guarantee that these explicit personas align with the implicit personas used during annotation. Consequently, unlike the Perspective dataset—where the explicit persona is directly distilled into the implicit persona—the PRISM dataset does not support the same evaluation methodology. Moreover, since each user contributes at most 50 instances, it is not feasible to construct a gold-standard reward model from the PRISM dataset. For these reasons, PRISM is used only as a qualitative benchmark in preference modeling experiments.

\subsection{Misalignments between \textit{Explicit} and \textit{Implicit} preferences}

In the psychology domain, there has been discussion about the difficulty of fully expressing one's deep, complex, hidden preferences through language~\citep{nisbett1977telling, pronin2001you}. Recent studies~\citep{jin2024implicit} have also discussed how these \textit{implicit} values are intricately intertwined among various factors. The PRISM dataset contains user self-introductions describing their preferences and stated preferences regarding LLM usage. When we provided this information to \texttt{gpt-4-turbo} to predict individual user preferences, it achieved an accuracy of approximately 57\%. While this doesn't represent a comprehensive explicit preference analysis, considering the general preference aspects used in prediction, it suggests that explicit preferences alone may be insufficient to explain complex implicit preferences, or there may be mismatches between them. However, as mentioned in the Limitations section, due to the absence of online human evaluation benchmarks, extensive analysis is not possible, and we leave this as an intriguing interpretation for future researchers.

\section{Details of Drift Implementation}

\subsection{Used Differential System Prompts for Zero-shot Rewarding}
\input{source/system-prompts}
In our experiments, we use the system prompts for each attribute as shown in Table~\ref{tab:system_prompts}. Although minor performance variations may occur due to changes in the basic template, we employ the most fundamental system prompt template in this paper to serve as a baseline for future research.

\subsection{Detailed Hyperparameters and Models}

Table~\ref{app:hyperparameters} shows the hyperparameters used in our experiments. Since the overall algorithm does not perform gradient computations, the hyperparameter space is limited. In the Drift Approximation stage, the number and definition of attributes determine everything, as detailed in Table~\ref{tab:system_prompts}. Similarly, in Drift Decoding, the logit-level computations are deterministic, so the only variable is the choice of samplers.


\begin{table}[htbp]
  \centering\resizebox{\columnwidth}{!}{
  \begin{tabular}{ll}
    \toprule
    Hyperparameter & value \\
    \midrule
    % \specialrule{.1em}{.05em}{.05em} 
    Frozen LLM & Llama-8B\tablefootnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}, Gemma-9B\tablefootnote{\url{https://huggingface.co/google/gemma-2-9b-it}} \\
    \midrule
    Small LM for RM & Llama-1B\tablefootnote{\url{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}}, Gemma-2B\tablefootnote{\url{https://huggingface.co/google/gemma-2-2b-it}} \\
    \midrule
    LoRA~\citep{hu2021lora} $r$ for RM & 8 \\
    \midrule
    LoRA $\alpha$ for RM & 32 \\
    \midrule
    LoRA training epochs for RM & 5 \\
    \midrule
    top-p for generation & 0.9 \\
    \midrule
    $\beta$ for generation & 0.5 \\
    \midrule
    text\_length & 500 \\
    \midrule
    attributes\_num for generation & 7 \\
    \bottomrule
  \end{tabular}}
\caption{Hyperparameters used for the experiments.}
\label{app:hyperparameters}
  \vspace{-3 mm}
\end{table}

\section{Expanded Analysis}

\subsection{Activated Attributes for Each User}
\label{app:attributes-activation}
This section interprets and analyzes PRISM's actual personal preferences. Looking at Figure~\ref{fig:activated-attributes-prism}, we can see that the activated attributes vary significantly between individuals. In particular, PRISM's actual users show dynamic patterns compared to each other user.


\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{figs/activation-user-expanded.pdf}
\caption{For each user in PRISM, there is a $W-L$ (Win-Loss) value for each attribute. The higher this value is, the more that user can be interpreted as preferring that attribute.
}
\label{fig:activated-attributes-prism}
\vspace{-3mm}
\end{figure*}



% \begin{figure*}[ht]
% \centering
% \includegraphics[width=\textwidth]{figs/activation-expanded.pdf}
% \caption{For each persona in Perspective, there is a $W-L$ (Win-Loss) value for each attribute. The higher this value is, the more that user can be interpreted as preferring that attribute.}
% \label{fig:activated-attributes-perspective}
% \vspace{-3mm}
% \end{figure*}

\subsection{Expanded Case study for Personalized Generation in PRISM}
\label{app:prism_analysis_section}

\input{source/case_studies}

In this section, we present a personalized generation case study by examining the complete set of generated outputs. Table~\ref{tab:case-study-appendix1} shows the full version of the main paper, while Table~\ref{tab:case-study-appendix2} and \ref{tab:case-study-appendix3} provide additional analysis. The characteristics shown in the main paper are also evident in the full text version. While Llama-8B's pure generation attempts to provide neutral, fact-based answers like the lose response, Drift tries to provide responses from various angles like the win response. This tendency can also be observed in Table~\ref{tab:case-study-appendix2}, where \texttt{user1280} asked a question regarding the possibility of UFOs existing, and among the responses—one neutral and one open to the possibility—they selected the latter. While Llama-8B tends to focus on a neutral perspective, the output generated via Drift maintains the overall response structure while offering a more open stance on the possibility. In Table~\ref{tab:case-study-appendix3}, \texttt{user1247} poses a philosophical question about belief in existence. While the lose response and LLM pure output suggest the possibility of building understanding through dialogue and data accumulation, Drift, like the win response, definitively argues that this transcends the realm of logic and that AI's belief in God's existence is impossible. These examples of win-lose responses suggest that Drift's approximation effectively captures user preference characteristics and demonstrates sufficient ability to generate responses that users are likely to prefer during the decoding phase.



