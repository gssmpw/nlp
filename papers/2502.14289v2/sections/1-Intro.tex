
\section{Introduction}

Large language models (LLMs) have rapidly become integral to a wide range of applications, driven by advances in \textit{Reinforcement Learning from Human Feedback} (RLHF)~\citep{ziegler2020finetune, rafailov2024direct}. Traditionally, RLHF aligns LLMs with \textbf{general preferences} by leveraging large-scale annotations from diverse users. Building on these successes, an important question naturally arises: \textit{Can we align LLMs with individual users’ \textbf{personal preferences?}}

LLM personalization, however, presents several challenges. First, collecting extensive, user-specific annotations is prohibitively expensive and impractical. Second, training and maintaining separate LLMs per user is computationally infeasible, which motivates the need for a \textit{training-free} approach. Third, while user-specific system prompts have been proposed as an alternative~\citep{hwang2023aligninglanguagemodelsuser, multifacet}, most users struggle to \textit{explicitly} articulate their complex preferences~\citep{nisbett1977telling, pronin2001you}. This often leads to misalignment between stated and actual \textit{implicit} preferences.

\begin{figure}[t!]
\centering
\includegraphics[trim=7 8 2 2, clip, width=0.78\columnwidth]{figs/Intro.pdf}
\caption{Few-shot preference modeling for \texttt{user1008} in the PRISM dataset. 
% Shading means the standard deviation. 
Traditional reward models fail to generalize with scarce data, whereas Drift achieves strong prediction with only 50 examples.}
\label{fig:Intro}
\vspace{-5mm}
\end{figure}

To address these challenges, we propose \textbf{Drift}, an algorithm for few-shot personalization of LLMs that requires no gradient updates. Our key contributions are as follows:

\noindent
\textit{Drift Approximation:} We first decompose complex personal preferences into a composition of simpler and predefined attributes (e.g., ``emotional,'' ``concise,'' ``technical''). In this process, we theoretically demonstrate how to convert the RLHF objective~\citep{rafailov2024direct} into a Drift optimization problem, enabling efficient preference modeling with minimal data. 

\noindent \textit{Zero-shot Rewarding:} We leverage a \textit{differential prompting} approach to reward various attributes in a zero-shot manner. Specifically, we modify a base system prompt with attribute cues and compute the difference in log likelihood between the modified and base prompts. This differential signal acts as a surrogate reward, eliminating the need for specialized datasets or attribute-specific training.

\noindent
\textit{Drift Decoding:} Finally, using the composite preferences obtained via Drift Approximation, we derive a principled method to steer the decoding process of a frozen LLM. We prove that by integrating weighted, attribute-specific rewards into the logit space, we can achieve personalized generation without any model updates or gradient computations. 

We evaluate Drift on two fronts: (i) efficient few-shot preference modeling and (ii) personalized generation. As shown in Figure~\ref{fig:Intro}, unlike traditional reward models, Drift reaches a test-set accuracy of 70\% with only 50 examples and even outperforms a reward model trained on 500 examples. By aligning this effective preference model at decoding-time, Drift consistently produces outputs that better reflect individual users’ implicit preferences.
Extensive analysis and discussion further validate the robustness and practical benefits of Drift.

% \paragraph{Contributions.}
% 1) We propose the first \textit{few-shot implicit personalization} and \textit{training-free} framework for LLMs that decomposes implicit preferences into diverse, interpretable attributes and aligns their composition at decoding-time. 2) We introduce a novel zero-shot rewarding mechanism based on differential prompting to capture numerous aspects of personal preferences without the need for dedicated dataset construction. 3) We empirically demonstrate that Drift achieves robust few-shot personalized preference modeling and generation on both synthetic and real-world datasets, offering significant practical benefits.


\paragraph{Contributions.}
1) We propose the first \textit{few-shot implicit personalization} and \textit{training-free} algorithm for LLMs that decomposes implicit preferences into diverse, interpretable attributes and aligns their composition at decoding-time. 2) We introduce the differential prompting technique to capture numerous aspects of personal preferences, demonstrating strong empirical results without the need for dedicated dataset construction. 3) We empirically show that Drift achieves robust few-shot preference modeling and personalized generation on both synthetic and real-world datasets, providing significant practical benefits with extensive analysis.