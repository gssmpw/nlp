\section{Experiments}

We evaluate Drift on two fronts: (i) efficient few-shot preference modeling and (ii) personalized generation. Our experiments are conducted on two datasets: \textbf{Perspective} (a synthetic persona dataset) and \textbf{PRISM} (an actual human-annotated dataset).

\begin{table}[ht]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \textbf{Explicit} & \textbf{Implicit persona} & \textbf{Avg. Size} \\
& \textbf{persona} & \textbf{ \textit{(Annotators)}} & \textbf{per user} \\
\midrule
PersonalLLM~\citep{personallm} & \ding{55} & Open-sourced RMs & 9,402 \\
PersonalSum~\citep{zhang2024personalsum} & \ding{51} & Human & 2.7 \\
PRISM~\citep{kirk2024prism} & \ding{51} & Human & 19.5 \\
Multifacet~\citep{multifacet} & \ding{51} & GPT-4 with persona & 3 \\
Perspective (Ours) & \ding{51} & GPT-4 with persona & 7,645 \\
\bottomrule
\end{tabular}}
\caption{Comparison of personal preference datasets. Perspective offers both scale and explicit persona information, enabling comprehensive evaluation.}
\label{tab:dataset-comparison}
\vspace{-5mm}
\end{table}

\subsection{Datasets and Evaluations.}
While actual user preferences are invaluable, collecting large-scale human preference data presents significant challenges. As shown in Table~\ref{tab:dataset-comparison}, existing human-annotated datasets typically contain only a few examples per user, making it difficult to train reliable reward models for evaluation. Moreover, when evaluating generation tasks, it's practically impossible to find annotators to evaluate newly generated outputs again. To address these issues, we first experiment with preference modeling and personalized generation on synthetic personas and then validate our findings using real-world data.

\paragraph{Perspective.}~\label{sec:perspective-advantages}
We introduce \textit{Perspective}, a large-scale dataset that employs synthetic personas with diverse viewpoints for reliable evaluation. Personas are selected from the Multifacet dataset~\citep{multifacet}, and we curate corresponding QA pairs that GPT-4 annotates according to each persona (details in Appendix~\ref{sec:perspective-details}). With an average of 7,645 examples per persona, Perspective offers two key advantages:
\begin{enumerate} 
\item The abundance of examples per persona allows us to train gold reward models that serve as dependable evaluation metrics for both approximation and generation tasks. 
\item The explicit persona information and consistent annotation process facilitate controlled and repeatable evaluations of generation tasks. 
\end{enumerate}

\paragraph{PRISM.}
In contrast, the PRISM dataset comprises human-annotated preferences, averaging 19.5 examples per user. We use PRISM to validate Drift’s performance in real-world scenarios, particularly under conditions of limited user data. For our experiments, we selected six users with more than 50 annotated pairs each and conducted few-shot personalization experiments to assess Drift's practical effectiveness.

\subsection{Few-shot Preference Modeling}

\begin{figure*}[th]
    \centering
        \begin{subfigure}{0.24\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/RM-per-llama.pdf}
            % \caption*{1. Perspective - Llama 1B}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/RM-per-gemma.pdf}
            % \caption*{2. Perspective - Gemma 2B}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/RM-prism-llama.pdf}
            % \caption*{3. PRISM - Llama 1B}
        \end{subfigure}
        \begin{subfigure}{0.24\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figs/RM-prism-gemma.pdf}
            % \caption*{4. PRISM - Gemma 2B}
        \end{subfigure}
    \caption{
    Average $k$-shot preference modeling results across personas in the Perspective and PRISM datasets. The two figures on the left show the results for Perspective using Llama 1B and Gemma 2B; the two on the right for PRISM using Llama 1B and Gemma 2B.
    }
    \label{fig:rm-results}
\vspace{-5mm}
\end{figure*}

\paragraph{Experimental setting.}
We evaluate the efficiency of the Drift approximation on both datasets. For Perspective, we vary the training set size from 10 to 500 examples; for PRISM, from 10 to 40 examples. Drift is compared against traditional reward models (RMs) implemented using Llama-1B~\citep{llama} and Gemma-2B~\citep{gemma}. A Llama-8B model trained on the full dataset (Gold RM) serves as an upper bound. Additionally, to assess the benefits of \textit{differential prompting}, we conduct experiments using the Drift approximation on Helpsteer2~\citep{helpsteer}, which provides well-defined attributes through specifically constructed datasets—"helpfulness," "correctness," "coherence," "complexity," and "verbosity."

\paragraph{Results.}
Figure~\ref{fig:rm-results} demonstrates that the Gold RM achieves nearly 80\% accuracy on the test set (with 8B exceeding 85\%) when trained on extensive data, while the performance of standard RMs drops below 60\% when fewer than 500 examples are available. In contrast, Drift achieves superior performance using only 50 samples, outperforming an RM trained on 500 examples with lower variance. Performance improves sharply with 100 examples and plateaus thereafter, although predictive stability continues to increase. Moreover, in PRISM involving actual users, RMs perform nearly at random, whereas Drift maintains robust approximation capabilities with just 40 samples. 
Furthermore, while Helpsteer2 offers precise reward signals for its five well-defined attributes, its limited scope fails to represent the richness of individual user preferences. Consequently, Helpsteer2 fails to generalize as effectively as our zero-shot rewarding approach, leading to lower performance compared to Drift.
These results underscore Drift’s strong \textit{few-shot personalization} capabilities, demonstrating that decomposing implicit personal preferences into multiple attributes via our \textit{differential prompting} yields robust modeling even under data scarcity.

\paragraph{Attribute Reduction for Decoding.}
\label{sec:practical-1}
Although Drift initially employs a large pool of candidate attributes (e.g., 40), only a subset is used during decoding. Figure~\ref{fig:attributes_num} shows that reducing the attribute count from 40 to 10 incurs only a modest drop in performance. Even with five attributes, the performance is significantly better than that of HelpSteer2. This suggests that a few core attributes selected by zero-shot rewarding suffice to capture personal preferences effectively. By evaluating a wide variety of attributes during the cost-efficient approximation stage, Drift identifies the most informative attributes for efficient decoding without compromising overall performance.


\begin{figure}[ht]
\centering
\includegraphics[trim=7 8 2 2, clip, width=0.7\columnwidth]{figs/attributes.pdf}
\caption{Performance variation when reducing the number of attributes during Drift Approximation with 40 samples. The performance decline is slightly more pronounced in the PRISM dataset, suggesting that real users’ implicit preferences are more complex than those of synthetic personas.}

\label{fig:attributes_num}
\vspace{-5mm}
\end{figure}

\paragraph{Interpretability.}
During the Drift Approximation process, we compute the average reward $W$ assigned to win responses and $L$ assigned to lose responses for each attribute. The difference $W-L$ for each attribute can be interpreted as \textbf{Unit implicit preference}, a measure of how strongly an individual implicitly prioritizes each attribute. For example, in PRISM, \texttt{user1280} introduced themselves as someone who \textit{uses an LLM exclusively for language learning}. The activated attributes reveal that ``concise" is highly prioritized with a value of 1.46, while attributes such as ``old-fashioned," ``exclamatory," and ``proverb"—which could potentially hinder language learning—are least preferred, scoring -1.19, -1.10, and -1.09, respectively. Thus, Drift not only delivers effective preference modeling with a few dozen examples but also provides valuable interpretability at the user level (additional analysis is provided in Appendix~\ref{app:attributes-activation}).

\subsection{Personalized Generation}

Next, we validate how Drift's robust preference model can be effectively integrated into personalized generation.

\paragraph{Experimental setting.}
We evaluate personalized generation primarily on the Perspective dataset, which offers reliable persona-specific evaluation metrics via a Gold RM and a GPT-based judge (see Section~\ref{sec:perspective-advantages}). Our evaluation compares the win rate of each baseline output against pure LLM outputs using these metrics. Under a few-shot setting with 100 training examples, Drift decoding is compared against several baselines:
\begin{enumerate}
    \item Training-base: PPO~\citep{schulman2017proximal}, DPO~\citep{rafailov2024direct}, DPA~\footnote{DPA training uses the weight from Drift on Helpsteer2.}~\citep{DPA}
    \item Training-free (as Drift): ARGS~\citep{khanov2024args}, Best-of-$N$ sampling ($N$=10)
\end{enumerate}
We use Llama-8B (controlled by Llama-1B as the RM) and Gemma-9B (controlled by Gemma-2B) for model configurations. Due to PRISM’s limited evaluation capabilities, we complement our quantitative results with qualitative case studies involving actual users in PRISM.

\input{source/personal-generation-results}

\paragraph{Results.}
Table~\ref{tab:pg-results} summarizes the few-shot personalized generation results with 100 training sets from the Perspective dataset. As observed in the previous section, the RM exhibits significant shortcomings with limited data. Consequently, PPO—which relies on the reward—experiences a performance drop. In contrast, DPO, which does not depend directly on the reward signal, avoids this decline; however, its performance improvement remains marginal. Although DPA leverages a robust preference model with HelpSteer2, its effect is also limited.
Among the training-free methods, both ARGS and Best-of-$N$ sampling are more robust than the PPO because they avoid unstable training with noisy RM. Nonetheless, since they rely on the RM signal for search, their performance gains are limited.
In contrast, Drift decodes by leveraging efficient and robust approximations, resulting in significant performance improvements across all models and evaluation metrics. These findings confirm that effective \textit{few-shot personalized generation} can be achieved with as few as 100 training examples using Drift in a training-free manner.

\paragraph{Case Study in PRISM.}
We compare the user-preferred and rejected responses from PRISM with the Drift outputs. In Table~\ref{tab:case-study}, \texttt{user1008} prefers responses that present diverse opposing viewpoints rather than neutral opinions on sensitive topics. Drift effectively guides the LLM in generating responses aligned with this user preference, moving away from generic neutral responses. This demonstrates that Drift achieves robust personalization even in few-shot scenarios. The full version and additional case studies are provided in Appendix~\ref{app:prism_analysis_section}.


\begin{table}[t]
\centering
\small
\begin{tabular}{p{7cm}}
\toprule
\textbf{User Query:} \textit{"I need information about the Civil War, a topic that has caused much disagreement."} \\
\midrule
\textbf{Win Response} \\
"Sure! I can provide some basic information about the Civil War, which may help clarify the 
\textbf{\textcolor{red2}{differences of opinion people may have}}... " \\
\midrule
\textbf{Lose Response} \\
"Sure, I'd be happy to help you ... As a language model, \textbf{\textcolor{blue2}{I do not have personal opinions or biases towards any topics or discussions}}, including the Civil War." \\
\midrule
\textbf{Llama-8B with Drift Response:} \\
"A sensitive and complex topic indeed. The American Civil War, fought from ... \textbf{\textcolor{red2}{What specific aspects of the Civil War would you like to know more about?}}" \\
\midrule
\textbf{Llama-8B Pure Response:} \\
"... \textbf{\textcolor{blue2}{I'll try to provide a neutral and fact-based overview of the topic.}}" \\
\bottomrule
\end{tabular}
\caption{This table contains the \texttt{user1008} question from PRISM, with the \textbf{\textcolor{red2}{selected}} and \textbf{\textcolor{blue2}{rejected}} answers and the responses from Llama-8B w/ and w/o Drift decoding.}
\label{tab:case-study}
\vspace{-3mm}
\end{table}


\paragraph{Inference cost.}

\begin{table}[htbp]
  \centering\resizebox{0.7\columnwidth}{!}{
  \begin{tabular}{ll}
    \toprule
    Method & Time complexity \\
    \midrule
    Best-of-$N$ & $T(N\cdot C + N\cdot c)$ \\
    ARGS (top-$N$ tokens) & $T(C + N\cdot c)$ \\
    Drift ($N$ attributes) & $T(C + N\cdot c)$ \\
    \bottomrule
  \end{tabular}}
  \caption{Time complexity of each training-free method. Here, $C$ represents the LLM inference cost, $c$ denotes the sLM inference cost, and $N$ is the key hyperparameter for each method.}
  \label{tab:time_complexity}
  \vspace{-5mm}
\end{table}

Table~\ref{tab:time_complexity} summarizes the time complexity of each training-free baseline. 
Best-of-$N$ involves sampling from the LLM $N$ times and evaluating each sample with the sLM, resulting in a total complexity of $T(N\cdot C + N\cdot c)$. This method is the most computationally expensive. Both ARGS and Drift steer the LLM's next-token distribution using sLM. However, while ARGS sequentially samples from LLM and evaluates top-$N$ tokens, Drift samples the next-token distributions in parallel both from the LLM and sLM and then combines them. This flexibility offers improved efficiency over ARGS under the same memory size.
