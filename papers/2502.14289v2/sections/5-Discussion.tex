\section{Discussion}

\paragraph{Quadratic Programming vs. Logistic Regression.}  
Our formulation estimates the attribute weights $\mathbf{p}$ by transforming the Bradley-Terry loss into a quadratic program. An alternative approach based on logistic regression—which assigns absolute labels of 1 and 0 to win/lose responses—can also be used, as demonstrated by \citep{go2023compositional}. 
We compared these two formulations using Drift attributes in Table~\ref{fig:discussion}. The logistic regression approach proves highly unstable and shows lower performance when training examples are limited. We interpret this instability as follows: preference judgments are inherently relative—what constitutes a winning response in one context might be considered a losing response when compared to an even better alternative. Thus, imposing absolute labels through regression can lead to overfitting, particularly when data are scarce. Our results suggest that approaching preference problems from a relative perspective is crucial for effective preference modeling.
\begin{figure}[ht]
\centering
\includegraphics[trim=7 8 2 2, clip, width=0.65\columnwidth]{figs/discussion.pdf}
\caption{Few-shot preference modeling results for \texttt{user1008} in the PRISM with quadratic programming (QP) and logistic regression (LQ).}
\label{fig:discussion}
\vspace{-5mm}
\end{figure}


\paragraph{Compatible with samplers.}
\label{sec:practical-2}
Autoregressive sampling in LLMs has various decoding strategies at the token-level distribution. Drift steers distributions at the logit level—applying its computations before the softmax—making it compatible with a wide range of sampling methods tailored to different objectives~\citep{vijayakumar2016diverse, fan2018hierarchical, holtzman2019curious}. our analysis indicates that the backbone LLM exhibits an average next-token entropy of about 0.27 bits, which increases to approximately 0.63 bits after applying Drift. While this boost in entropy can substantially enhance generation diversity, it may also increase the likelihood of selecting unreliable tokens. Therefore, we recommend combining Drift with top-p or top-k sampling strategies to control an optimal balance between diversity and reliability.

\paragraph{Practical Implications.}
While traditional RLHF methods may eventually surpass Drift when user data becomes abundant, Drift offers several advantages in practical settings. 
First, conventional reward models struggle with \textit{continual learning}; retraining on an ever-expanding user dataset is impractical. In contrast, Drift can be updated instantly by simply appending new instances to the $W-L$—no retraining required. 
Second, personal preferences often \textit{change more rapidly than general preferences}. Drift’s interpretability allows real-time tracking of preference shifts, enabling dynamic adjustments for improved personalization. 
Third, when collecting additional user annotations, the variance observed in each attribute can inform an \textit{active learning} strategy~\citep{miller2020active} for efficient data collection. These benefits make Drift an attractive complement to existing RLHF pipelines in personalized applications.
