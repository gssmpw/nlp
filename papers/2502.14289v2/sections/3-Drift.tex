\section{{\includegraphics[height=1.6ex]{figs/racer.png}} Drift Algorithms}

Drift overcomes data scarcity and computational inefficiency by decomposing a user’s complex personal preferences as a linear combination of simpler attributes. As Figure~\ref{fig:main}, we describe two key components: \textit{Drift Approximation}, which efficiently estimates attribute weights from a few dozen examples, and \textit{Drift Decoding}, which integrates these weights into the LLM’s decoding process.

\subsection{Drift Approximation}
\paragraph{Problem Setup.} 
Assume we have a personalized preference dataset $\mathcal{D}$, a frozen LLM $\pi_\text{LLM}$, and a set of $k$ attribute-specific small LMs $\{\pi_i^*\}_{i=1}^k$ (with corresponding base model $\pi$). We model the overall personalized reward as
\begin{equation}
    R_{\mathcal{D}}(y \mid x) = \sum_{i=1}^k p_i \, r_i(y \mid x),
\end{equation}
where $p_i$ indicates the importance of the $i$th attribute. Under the KL-regularized framework in Eq.~\ref{eq:ideal_distributions}, the target distribution $\tilde{\pi}$ becomes:

{\small
\begin{equation}
\begin{split}
    \tilde{\pi}(y \mid x) &\propto \pi_\text{LLM}(y \mid x) \exp\!\Bigl(\beta^{-1}R_{\mathcal{D}}(y \mid x)\Bigr)\\[1mm]
    &= \pi_\text{LLM}(y \mid x) \prod_{i=1}^k \exp\!\left(\frac{p_i}{\beta} \, r_i(y \mid x)\right).
    \label{eq:combined_distribution}
\end{split}
\end{equation}}
\noindent
Each reward is expressed in a generative form:
\begin{equation}
    r_i(y \mid x) = \log\!\frac{\pi_i^*(y \mid x)}{\pi(y \mid x)} + \log Z_i(x),
    \label{eq:attribute_reward}
\end{equation}
with the partition term $Z_i(x)$ canceling out in pairwise comparisons.

\paragraph{From Bradley-Terry to Drift.} 
To estimate the attributes weights $\mathbf{p} = [p_1, \dots, p_k]$, we initiate the Bradley-Terry formulation as \citet{rafailov2024direct}. For a given pair $(y_w, y_l)$ (where $y_w$ is preferred over $y_l$), we have:
\begin{align}
    &\max_{\theta} \ p(y_w > y_l \mid x) = \nonumber \\
    &\frac{1}{1 + \exp\left(\beta\left(\log\frac{\pi_\text{LLM}^{\theta}(y_l \mid x)}{\pi_\text{LLM}^{\text{ref}}(y_l \mid x)} - \log\frac{\pi_\text{LLM}^{\theta}(y_w \mid x)}{\pi_\text{LLM}^{\text{ref}}(y_w \mid x)}\right)\right)}\nonumber
\end{align}
as in DPO~\citep{rafailov2024direct}. 
Substituting Eqs.~\ref{eq:combined_distribution} and \ref{eq:attribute_reward} simplifies this optimization to:
% \begin{align}
% \max_{\mathbf{p}}\; &\frac{1}{1 + \exp\Bigl(\beta \Bigl(
% \sum\limits_{i=1}^k p_i \log\frac{\pi^*_i(y_l \mid x)}{\pi(y_l \mid x)} \nonumber\\[1mm]
% &\quad\quad - \sum_{i=1}^k p_i \log\frac{\pi^*_i(y_w \mid x)}{\pi(y_w \mid x)}
% \Bigr)\Bigr)}.
% \end{align}
{\tiny
\begin{align}
\max_{\mathbf{p}}\; &\frac{1}{1 + \exp\Bigl(\beta \Bigl(\sum\limits_{i=1}^k p_i \log\frac{\pi^*_i(y_l \mid x)}{\pi(y_l \mid x)} \nonumber - \sum_{i=1}^k p_i \log\frac{\pi^*_i(y_w \mid x)}{\pi(y_w \mid x)}\Bigr)\Bigr)}.
\end{align}}
By monotonicity of $x \mapsto \frac{1}{1 + \exp(-\beta x)}$, reducing the problem to a simpler optimization task:
\begin{equation}
    \max_{\mathbf{p}} \ \sum_{i=1}^k p_i \left(\log\frac{\pi_i^*(y_w \mid x)}{\pi(y_w \mid x)} - \log\frac{\pi_i^*(y_l \mid x)}{\pi(y_l \mid x)}\right).\nonumber
\end{equation}
To avoid an unbounded solution, we constrain $\mathbf{p}$ to lie on the unit $\ell_2$ sphere:
\begin{equation}
    \max_{\mathbf{p}} \ \left(\mathbf{W} - \mathbf{L}\right)^T \mathbf{p}, \quad \text{subject to } \|\mathbf{p}\|_2 = 1,
\end{equation}
where $\mathbf{W}$ and $\mathbf{L}$ aggregate the log-ratio differences for the preferred $y_w$ and less preferred $y_l$ outputs over $\mathcal{D}$, respectively. 
Notably, this approximation is completely gradient-free and thus highly efficient compared to traditional preference modeling.

\paragraph{Zero-Shot Rewarding via Differential Prompts.}  
Drift Approximation computes $r_i$ for each instance $y$ as $\log\frac{\pi_i^*(y \mid x)}{\pi(y \mid x)}$. However, training an attribute-specific model $\pi^*_i$ for every possible attribute is infeasible. Instead, we reward each attribute in a zero-shot manner using differential prompts. 

Starting from a base prompt $s_0$ (e.g., \textit{"You are an AI assistant."}), we compute the log-probability $\log \pi(y | x, s_0)$. For each attribute (e.g., \textit{\textcolor{blue2}{emotion}}), we modify the base prompt by adding a corresponding cue (e.g., \textit{"You are an \textcolor{blue2}{emotional} AI assistant."}) to obtain $s_i$ and compute $\log \pi_i^*(y | x)=\log \pi(y | x, s_i)$. Their difference $\log\frac{\pi(y \mid x, s_i)}{\pi(y \mid x, s_0)}$ captures the differential impact of the attribute cue, serving as a surrogate reward signal that measures how well the response $y$ aligns with the attribute. 
This approach is: 1) \textbf{Training-free:} No additional fine-tuning is needed, 2) \textbf{Flexible:} New attributes can be integrated on the fly, 3) \textbf{Memory efficient:} It avoids the need to maintain multiple LLMs.

Algorithm~\ref{alg:drift-approximation} summarizes the Drift Approximation procedure.

\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}
\begin{algorithm}[t]
\caption{Drift Approximation}
\label{alg:drift-approximation}
\begin{algorithmic}[1]
\Require Dataset $\mathcal{D} = \{(y^j_w, y^j_l, x^j)\}_{j=1}^n$, sLM $\pi$, base prompt $s_0$, attribute prompts $\{s_i\}_{i=1}^k$
\Ensure Attribute weights $\mathbf{p} = \{p_1, p_2, \dots, p_k\}$
\For{$j = 1$ to $n$} \Comment{Over each data point}
    \For{$i = 1$ to $k$} \Comment{For each attribute}
        \State $\mathbf{W}_{j,i} \gets \log \frac{\pi(y^j_w\mid x^j, s_i)}{\pi(y^j_w\mid x^j, s_0)}$
        \State $\mathbf{L}_{j,i} \gets \log \frac{\pi(y^j_l\mid x^j, s_i)}{\pi(y^j_l\mid x^j, s_0)}$
    \EndFor
\EndFor
\State $\mathbf{p} \gets \arg\max_{\mathbf{p}:\|\mathbf{p}\|_2=1} \;  (\mathbf{W} - \mathbf{L})^T\mathbf{p}$
\State \Return $\mathbf{p}$
\end{algorithmic}
\end{algorithm}

\subsection{Drift Decoding}
\label{sec:drift-decoding}
Once the attribute weights $\mathbf{p}$ are obtained, Drift enables personalized generation by sampling directly from a composite distribution that adjusts the frozen LLM’s logits.
\paragraph{Composite Distribution.}  
Let $\pi_{\text{LLM}}$ denote the frozen LLM and $\{\pi_i\}_{i=1}^k$ the distributions obtained by prompting with $s_i$. Denote their respective logits by $h^{\text{LLM}}$, $h^i$, and let $h^{\text{base}}$ correspond to the base prompt $s_0$. The composite distribution $\tilde{\pi}$ of next token candidates $w$ is defined as:
\begin{equation}
    \tilde{\pi}(w) \propto \pi_{\text{LLM}}(w) \prod_{i=1}^k \left(\frac{\pi_i(w)}{\pi_{\text{base}}(w)}\right)^{\frac{p_i}{\beta}},
\end{equation}
where $\beta$ is the KL regularization hyperparameter that controls the strength of personalization.
Converting probabilities to logits (recall $\pi(w)=\text{softmax}(h[w])$ for all $w$), we obtain:
\begin{equation}
\begin{split}
    \log \tilde{\pi}(w) &= h^{\text{LLM}}[w] + \\ &\sum_{i=1}^k \frac{p_i}{\beta} \big(h^i[w] - h^{\text{base}}[w]\big) + C,
    \end{split}
\end{equation}
where $C$ is a constant independent of $w$ and will be ignored after $\text{softmax}$. Thus, sampling from $\tilde{\pi}$ amounts to:
{\small
\begin{equation}
    \tilde{\pi}(w) = \text{softmax}\Big( h^{\text{LLM}} + \sum_{i=1}^k \frac{p_i}{\beta} (h^i - h^{\text{base}}) \Big)[w].
\end{equation}}
Thus, sampling from $\tilde{\pi}$ amounts to adjusting the LLM's logits using the weighted attribute differences.
For a more detailed derivation, see Appendix~\ref{appendix:drift-decoding-proof}.

Algorithm~\ref{alg:drift-decoding} describes the complete autoregressive decoding procedure.


\begin{algorithm}[t]
\caption{Drift Decoding}
\label{alg:drift-decoding}
\begin{algorithmic}[1]
\Require Input context $x$, LLM $\pi_{\text{LLM}}$, sLM  $\pi$, base prompt $s_0$, attribute-specific prompts $\{s_i\}_{i=1}^k$, personal weights $\{p_i\}_{i=1}^k$ and strength $\beta$
\Ensure Generated sequence $y$
\State $y \gets \emptyset$
\While{not end of sequence}
    \State Compute $h^{\text{LLM}}_t \gets \pi_{\text{LLM}}(\cdot \mid x,y)$
    \State Compute $h^{\text{base}}_t \gets \pi(\cdot \mid x,y, s_0)$
    \For{$i = 1$ to $k$}
        \State Compute $h^i_t \gets \pi(\cdot \mid x,y, s_i)$
    \EndFor
    \State $h^{\text{drift}}_t \gets h^{\text{LLM}}_t + \frac{1}{\beta}\sum_{i=1}^k p_i (h^i_t - h^{\text{base}}_t)$
    \State Sample token $w_t \sim \text{softmax}(h^{\text{drift}}_t)$
    \State Append $w_t$ to $y$
\EndWhile
\State \Return $y$
\end{algorithmic}
\end{algorithm}


\paragraph{Practical Considerations.}
For Drift Approximation, a zero-shot rewarding mechanism can consider an unlimited number of candidate attributes with gradient-free computational cost. It is advantageous to evaluate as many attributes as possible, thereby increasing the likelihood that even a small, carefully selected subset will capture the full nuances of a user's preferences. In practice, we perform the approximation using a large pool of attributes (e.g., 41 candidates as detailed in Table~\ref{tab:system_prompts}) and then select a subset with the highest absolute weights $|p_i|$ for the final decoding process—our experiments ultimately use seven representative attributes. We will further discuss this in Section~\ref{sec:practical-1}.

% Moreover, since Drift performs computations at the logit level, it is compatible with most sampling strategies (e.g., top-$p$, top-$k$). However, combining outputs from multiple language models can increase the entropy during next-token prediction, potentially raising the probability of selecting unreliable tokens. To mitigate this, we set top-$k=10$, top-$p=0.9$, and $\beta=0.5$ in our experiments.

% These two practical considerations are further discussed in Section~\ref{sec:practical-1} and \ref{sec:practical-2}.
