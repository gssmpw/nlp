\section{Related Works}
\paragraph{Explicit Personalization.}
As humans express their own preferences, recent works explored aligning LLMs with individual values through explicit cues. Multifacet~\citep{multifacet} has focused on designing diverse and detailed system prompts for LLM control. PAD~\citep{chen2024pad} and MetaAligner~\citep{yang2024metaaligner} have leveraged fine-grained RM—such as HelpSteer~\citep{helpsteer}—to construct specific policies and guide model behavior toward system prompts. Others allow users to directly specify attribute importance weights, either for training~\citep{yang2024rewards, DPA} or decoding-time alignments~\citep{dekoninck2023controlled, shi2406decoding}. 

\paragraph{Implicit Personalization.}
While they have advanced explicit preferences, implicit preferences behind users' behaviors remain understudied, as Table~\ref{tab:method_comparison}. \citet{jin2024implicit} has shown that these values arise from complex interactions between factors like experiences, education, lifestyle, and even dietary habits, leading to misalignment with explicitly stated preferences~\citep{nisbett1977telling}
To address this gap, several works proposed implicit personalization tasks - from title generation~\citep{ao-etal-2021-pens}, movie tagging~\citep{salemi2023lamp} to summarization~\citep{zhang2024personalsum}.
Notably, PRISM~\citep{kirk2024prism} made notable progress by collecting preference annotations from conversations with over a thousand users, though its effectiveness was limited by the small number of annotations per user, making traditional RLHF approaches challenging.

Our work advances this field in two key ways: First, we introduce the Perspective dataset, which enables more reliable evaluation. Second, we propose Drift, \textit{decoding-time few-shot personalization}. 
By addressing the challenges of implicit preferences, our approach represents a significant step forward in implicit personalized alignments.