\section{Preliminaries}

Before describing Drift in detail, we review the standard RLHF pipeline and recent decoding-time alignment methods that motivate our approach.

\subsection{RLHF}

RLHF aims to align a base model $\pi_{\text{base}}$ with human preferences by transforming human feedback into a reward function. The typical RLHF pipeline comprises three main steps: (1) Preference data collection, (2) Preference modeling, and (3) RL training.


\paragraph{Preference data collection.} 
Given a prompt $x$, $\pi_{\text{base}}$ generates responses $(y_1, y_2, \ldots, y_t) \sim \pi_{\text{base}}(\cdot \mid x)$. Human annotators then evaluate these responses by expressing pairwise preferences, denoted as $y_w \succ y_l \mid x$, where $y_w$ is the preferred response and $y_l$ is the less preferred one. These annotated pairs form the dataset $\mathcal{D}$.


\paragraph{Preference Modeling.} The preference model (also referred to as the reward model) $r(x, y)$ is trained to capture human preferences. This is typically achieved using a Bradley-Terry loss function~\citep{bradley1952rank}:
\begin{equation}
    \max_{r} \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma\big(r(x, y_w) - r(x, y_l)\big) \right],\nonumber
\end{equation}
where $\sigma$ represents the logistic function. Through this training process, $r(x, y)$ learns to quantify the human preferences encoded in $\mathcal{D}$.

\paragraph{KL-Regularized RL.} To align the base model $\pi_{\text{base}}$ with human preferences, the objective is to maximize the reward while minimizing the KL divergence $D_{\text{KL}}$ from the base model~\citep{schulman2017proximal}, as follows:
\begin{equation}
    \max_{\pi_{\theta}} \ \mathbb{E}_{y \sim \pi_{\theta}(y \mid x)} \left[ r(x, y)  - \beta D_{\text{KL}}\big(\pi_{\theta} \| \pi_{\text{base}}\big)\right],\nonumber
\end{equation}
where $\beta$ controls the deviation ($\beta>0$).


\begin{figure*}[th]
    \centering
    \includegraphics[trim=85 62 25 66, clip, width=0.95\textwidth]{figs/main2.pdf}
    \caption{Overview of the total Drift Algorithms. (a) Drift Approximation: Decomposes a user’s implicit preferences into a weighted combination of various attributes. (b) Drift Decoding: Integrates this attribute composition into the decoding process without retraining the LLM.
    }
    \label{fig:main}
\vspace{-5mm}
\end{figure*}


\subsection{Decoding from RLHF Objective}

\paragraph{RL Closed-Form Solution.} The KL-regularized RL problem has a closed-form solution~\citep{korbak2022rl}:

{
\small
\begin{equation}
    \pi^*(y \mid x) = \frac{1}{Z(x)} \pi_{\text{base}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right),
    \label{eq:ideal_distributions}
\end{equation}
}
where, $Z(x) = \sum_y \pi_{\text{base}}(y \mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$ is the partition function (Proof in Appendix~\ref{proof-rl}). While this solution implies the possibility of training-free alignment of $\pi_{\text{base}}$ using only $r$, in most cases, $Z(x)$ is often intractable~\citep{lin2022uncomputability}.

\paragraph{Decoding-time alignments.}
Recently, \citet{liu2024tuning, xu2024genarm, liu2024decoding} have addressed this challenge through collaborative decoding between the LLM and a smaller language model (sLM). By training a sLM $\pi_{r}$ on $\mathcal{D}$ to create an aligned model $\pi^*_{r}$, the relationship
\begin{equation}
r(x, y) = \beta\log\frac{\pi^*_r(y \mid x)}{\pi_{r}(y \mid x)} + \beta\log Z_r
\label{eq:generative_reward}
\end{equation}
is established. Although $Z_r$ remains intractable, calculations performed at the logit level yield:
\begin{equation}
\begin{split}
\pi^*(\cdot \mid x) =\; &\text{softmax}\Big( h_{\pi}(\cdot \mid x) \\
&\quad + \beta^{-1} \Bigl( h_{\pi^*_r}(\cdot \mid x) - h_{\pi_{r}}(\cdot \mid x) \Bigr) \Big).\nonumber
\end{split}
\label{eq:basic_decoding}
\end{equation}
Bypassing the computation of $Z_r$, this formulation allows practical decoding-time alignment without expensive fine-tuning.


\paragraph{Challenges for personalized alignments.}

Despite these advances, training a robust reward model typically requires large amounts of data—an impractical requirement for individual user personalization. Drift is designed to overcome this limitation via few-shot preference modeling with a weighted linear combination of various attribute-specific reward signals, which can be directly applied to decoding-time alignment.
