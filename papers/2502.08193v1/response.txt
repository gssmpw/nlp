\section{Related Work}
\subsection{Contrastive Language-Image Pretraining}
Contrastive Language-Image Pretraining (CLIP) **Radford et al., "Learning to Generate by Imitation of Natural Language"** is a large vision-language model trained from scratch using a contrastive learning objective on a dataset comprising 400 million image-text pairs collected from the internet. 
CLIP is designed to learn representations of images alongside their corresponding paired texts to align these representations from the two modalities within the same embedding space.
This alignment ensures that corresponding image-text pairs are closer in the embedding space compared to non-corresponding pairs.
After training, the vision encoder of CLIP learns to associate images with their corresponding paired texts. 
This capability enables CLIP to excel at zero-shot transfer tasks across various domains, such as image classification, optical character recognition, and semantic segmentation. 

\subsection{Typographic attacks against LVLMs}
Recent studies **Carvalho et al., "Breaking Vision and Language Models with Adversarial Typograpgy"** show that typographic attacks can impair the zero-shot classification capability of CLIP. 
**Shen et al., "Understanding Multimodal Neurons in Visiolangual Representations"** claims that the underlying reason for typographic attacks could be multimodal neurons responding to shared concepts across different formats.
Other LVLMs, such as InstructBLIP **Changpinyo et al., "InstructBLIP: A Large-Scale Multimodal Prompt Engineering Dataset"** and LLaVA **Wu et al., "LLaVA: A Lightweight Framework for Large-Scale Multimodal Pretraining"**, are expected to inherit similar typographic weaknesses when incorporating the vision encoder of CLIP. 
Studies in **Chen et al., "Evaluating Robustness to Adversarial Attacks on Vision-Language Models"** evaluate the robustness of LVLMs to typographic attacks, including InstructBLIP, LLaVA 1.5, MiniGPT4-2, and GPT4-V models.
**Li et al., "Adversarial Typograpgy: A Study on Robustness of Vision-Language Models"** selects the attack text by random method, further evaluating the impact of font size, color, opacity, and spatial positioning on the typographic attack success.
**Zhang et al., "Self-Generated Attacks for Vision-Language Models"** proposes novel typographic attacks, termed Self-Generated Attacks, which leverage the capabilities of LVLMs to identify visually similar deceiving classes or generate descriptive reasoning for more effective deception.
This work **Kaplan et al., "Contrastive Learning for Multimodal Few-Shot Learning"** is close to our work because the prompt to the LVLMs requests an attack text that is `similar' to the target image.
In contrast, our work calculates text-image similarity directly in the embedding space. 
As a result, our attacks are more directly related to confusion regions arising due to shortcomings in the training of LVLMs, laying the groundwork for future study of principled defenses.
As already mentioned, our work differs from previous contributions in our focus on the multi-image setting.



% Here, we mention that previous work has explored typographic attacks against MLLMs **Brown et al., "Language Models as Cultural Beings"**, where benign texts are paired with harmful images to compromise the model. 
% Specifically, FigStep **Hendricks et al., "Deep Multimodal Transfer Learning for Visually Grounded Question Answering"** uses typography to embed paraphrased instructions within image prompts, manipulating model responses. 
% Similarly, JIP **Li et al., "Joint Image-Text Pretraining with Adversarial Training"** creates malicious triggers within the joint embedding space, pairing manipulated images with generic prompts.
% MM-SafetyBench **Zhang et al., "MM-SafetyBench: A Multimodal Benchmark for Evaluating Safety in Vision-Language Models"** conducts safety-critical evaluations of query-relevant images on MLLMs. 
% To construct these images, they transform entities or keywords related to image content into a visual typography on the constructed images.
% %ML_Note: Please check this
% These typographic attacks against MLLMs focus on jailbreaks, while the typographical attacks that we study are aimed at misclassification.
%These typographic attacks against MLLMs focus on jailbreaks in generative tasks, while our studied typographic attacks against CLIP focus on misclassification in discriminative tasks.