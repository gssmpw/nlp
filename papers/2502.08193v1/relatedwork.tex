\section{Related Work}
\subsection{Contrastive Language-Image Pretraining}
Contrastive Language-Image Pretraining (CLIP)~\citep{clip} is a large vision-language model trained from scratch using a contrastive learning objective on a dataset comprising 400 million image-text pairs collected from the internet. 
CLIP is designed to learn representations of images alongside their corresponding paired texts to align these representations from the two modalities within the same embedding space.
This alignment ensures that corresponding image-text pairs are closer in the embedding space compared to non-corresponding pairs.
After training, the vision encoder of CLIP learns to associate images with their corresponding paired texts. 
This capability enables CLIP to excel at zero-shot transfer tasks across various domains, such as image classification, optical character recognition, and semantic segmentation. 

\subsection{Typographic attacks against LVLMs}
Recent studies~\citep{multi_neurons, azuma2023defense, noever2021reading} show that typographic attacks can impair the zero-shot classification capability of CLIP. 
~\citet{multi_neurons} claims that the underlying reason for typographic attacks could be multimodal neurons responding to shared concepts across different formats.
Other LVLMs, such as InstructBLIP~\citep{dai2023instructblip} and LLaVA~\citep{llava}, are expected to inherit similar typographic weaknesses when incorporating the vision encoder of CLIP. 
Studies in~\citet{self_typo, typo_mllms} evaluate the robustness of LVLMs to typographic attacks, including InstructBLIP, LLaVA 1.5, MiniGPT4-2, and GPT4-V models.
~\citet{typo_mllms} selects the attack text by random method, further evaluating the impact of font size, color, opacity, and spatial positioning on the typographic attack success.
~\citet{self_typo} proposes novel typographic attacks, termed Self-Generated Attacks, which leverage the capabilities of LVLMs to identify visually similar deceiving classes or generate descriptive reasoning for more effective deception.
This work~\citep{self_typo} is close to our work because the prompt to the LVLMs requests an attack text that is `similar' to the target image.
In contrast, our work calculates text-image similarity directly in the embedding space. 
As a result, our attacks are more directly related to confusion regions arising due to shortcomings in the training of LVLMs, laying the groundwork for future study of principled defenses.
As already mentioned, our work differs from previous contributions in our focus on the multi-image setting.



% Here, we mention that previous work has explored typographic attacks against MLLMs~\citep{figstep,jailbreakinpieces,safetybench}, where benign texts are paired with harmful images to compromise the model. 
% Specifically, FigStep~\citep{figstep} uses typography to embed paraphrased instructions within image prompts, manipulating model responses. 
% Similarly, JIP~\citep{jailbreakinpieces} creates malicious triggers within the joint embedding space, pairing manipulated images with generic prompts.
% MM-SafetyBench~\citep{safetybench} conducts safety-critical evaluations of query-relevant images on MLLMs. 
% To construct these images, they transform entities or keywords related to image content into a visual typography on the constructed images.
% %ML_Note: Please check this
% These typographic attacks against MLLMs focus on jailbreaks, while the typographical attacks that we study are aimed at misclassification.
%These typographic attacks against MLLMs focus on jailbreaks in generative tasks, while our studied typographic attacks against CLIP focus on misclassification in discriminative tasks.