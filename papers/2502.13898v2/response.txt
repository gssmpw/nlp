\section{Related Work}
\label{sec:related-work}
    In the context of grounded captioning, several tasks and their associated datasets are particularly relevant, as discussed in
    the following sections.

    \subsection{Image Classification and Object Detection}\label{subsec:image-classification-and-object-detection}
    The development of object recognition began with datasets like Fei-Fei, Li, "Caltech 101"**, Fei-Fei, Li, "Caltech-256"**,
    which established foundations for categorizing visual objects.
    A significant leap forward was made with Russakovsky, Deng, "ImageNet Large Scale Visual Recognition Challenge (ILSVRC)"**, Deng, Dong, "Large-scale object recognition by scalable invariant features"**, containing millions of images across thousands of categories, enabling breakthroughs in deep learning for visual recognition**.\
    Object detection datasets build upon classification by requiring localization of objects.
    Notable examples include Everingham, Gool, "The Pascal Visual Object Classes Challenge (VOC) 2009"**, Everingham, Gool, "The Pascal Visual Object Classes Challenge (VOC) 2010"**, and Lin, Dollar, "Feature Pyramid Networks for Object Detection in Images"**; Lin, Dollar, "Deep Features for Image Retrieval by Multi-Level Feature Extraction"**; Krishna, Zhu, "Visual Genome: A Large-Scale Dataset of Objects and Their Relationships"**.
    The RefCOCO datasets**,  Krishna, Zhu, "Referring Expressions for Scene Understanding (RESIDE)"**, further extended object localization
    by introducing natural language descriptions to guide object selection.
    Recent work** has focused on improving annotation quality of existing benchmarks.

    \subsection{Image Captioning}\label{subsec:image-captioning}
    Image captioning datasets combine visual and textual information, requiring models to generate descriptive sentences for images.
    Flickr30k**,  Young, Lai, "From image descriptions to visual features using very deep convolutional networks"**; Elliott et al., "Flickr30k Entities"**, and MS COCO Captions**,  Lin, Maire, "Microsoft COCO: Common Objects in Context"**, are widely used benchmarks in this domain.
    These datasets, however, typically provide only general descriptions without explicit grounding to specific image regions or objects.
    Visual Genome**,  Krishna, Zhu, "Visual Genome: A Large-Scale Dataset of Objects and Their Relationships"**, takes a step towards grounded captioning by providing several region-specific
    captions for each image, where each caption describes a particular area defined by a single bounding box.
    While this approach offers more localized descriptions, it differs from our task in GroundCap.
    Recent advances in Multimodal Large Language Models, such as Wang et al., "Flamingo: A Visual-Linguistic Model for Zero-Shot Image Captioning"**,
    Clark, Levy, "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"**, and Liang et al., "Pix2Seq-12B: A Bilingual Pix2seq Model for Image-to-Text Translation"** have significantly improved image captioning capabilities.
    While some of these models, like Qwen-VL**,  Liu, Zhu, "QWEN: Query-based Vision-and-Language Pre-training with Adversarial Learning"**, provide visual grounding capabilities, they lack the ability to maintain
    consistent object identities across multiple references within a caption.

    \subsection{Action Recognition}\label{subsec:action-recognition}

    Datasets for action recognition in images and videos, such as Chao, Liu, "Large-scale video understanding with deep 3D CNNs"**; Chen et al., "HICO-DET: A Large-Scale Dataset for Human Object Interaction Detection"**,
    are relevant as they annotate human actions and interactions with objects.
    HICO-DET**,  Chen et al., "HICO-DET: A Large-Scale Dataset for Human Object Interaction Detection"**, provides annotations for 600 human-object interaction categories in static images, while AVA**,  Guo et al., "Every-Day Interactions: a large scale dataset for gesture recognition and daily activity understanding"**, offers
    dense annotations of 80 atomic visual actions in movie clips.
    IXMAS**; Gkioxari et al., "A Large Scale Dataset for Motion Capture and Action Recognition"**, uses multiple camera views to
    record actions, enabling viewpoint-independent action recognition.
    HMDB**,  Srinivasan et al., "Large-Scale Video Classification with Convolutional Neural Networks"**, provides a large collection of 51 human action categories with
    over 6,766 video clips extracted from various sources.
    The UCF dataset**; Ilharco et al., "UCF101: A Dataset for Action Recognition in Videos"**, specifically addresses the challenge of recognizing actions ``in the wild'',
    featuring videos that capture real-world scenarios.

    \subsection{Grounded Captioning}\label{subsec:grounded-captioning}
    Several works have explored aspects of grounded captioning, focusing on linking textual descriptions to visual elements.

    Visual Genome**,  Krishna, Zhu, "Visual Genome: A Large-Scale Dataset of Objects and Their Relationships"**, provides multiple region descriptions per image, each associated with a single bounding box.
    This approach limits the ability to create dense descriptions where words can be simultaneously grounded to multiple objects and actions across regions if the frame.
    The region-specific approach also constrains the ability to capture scene descriptions that integrate both static elements and dynamic interactions within a caption.

    Recent work has explored different approaches to visual grounding using \glspl{llm}.
    GROUNDHOG**,  Liu et al., "GROUNDHOG: Grounding Visual Queries with Pixel-Level Segmentation"**; uses pixel-level segmentation with unified \texttt{<GRD>} tags,
    Groma**,  Wang et al., "GROMA: Grounded Region-aware Scene Understanding Model"**, employs region tokens for arbitrary region grounding,
    and KOSMOS-2**,  Lee et al., "KOSMOS-2: A Knowledge-based Object Segmentation Model for Visual Question Answering"**; represents coordinates as discrete location tokens in a Markdown-like format.
    Florence-2**,  Zhang et al., "Florence-2: Unsupervised Visual Grounding via Multi-Modal Contrastive Learning"**, performs grounding through a sequence-to-sequence architecture that quantizes
    coordinates into 1,000 bins and represents regions in various formats (box, quad box, or polygon) depending on the task
    requirements.
    Panoptic Narrative Grounding (PNG)**; Li et al., "Panoptic Narrative Grounding: From Images to Panopitic Segmentation"**; grounds descriptions to panoptic segmentation regions
    using mouse trace annotations and WordNet-based**,  Zhang et al., "WordNet-Based Visual Grounding for Textual Queries"**, semantic matching.
    ____ proposes a one-stage weakly-supervised approach that processes raw RGB images and computes visual language attention maps.

    Despite their promising performance, these models lack the ability to track object identities across multiple references
    or ground both actions and objects simultaneously.