\section{Related Work}
\label{sec:related-work}
    In the context of grounded captioning, several tasks and their associated datasets are particularly relevant, as discussed in
    the following sections.

    \subsection{Image Classification and Object Detection}\label{subsec:image-classification-and-object-detection}
    The development of object recognition began with datasets like Caltech 101~\cite{fei2004learning},
    which established foundations for categorizing visual objects.
    A significant leap forward was made with ImageNet~\cite{deng2009imagenet}, containing millions of images across thousands of categories, enabling breakthroughs in deep learning for visual recognition~\cite{krizhevsky2012imagenet}.
    Object detection datasets build upon classification by requiring localization of objects.
    Notable examples include PASCAL VOC~\cite{everingham2010pascal} and MS COCO~\cite{lin2014microsoft}, which provide bounding
    box annotations for multiple object categories in natural images.
    The RefCOCO datasets~\cite{refitgame2014, yu2016modeling} further extended object localization
    by introducing natural language descriptions to guide object selection.
    Recent work~\cite{tong2023rethinking} has focused on improving annotation quality of existing benchmarks.

    \subsection{Image Captioning}\label{subsec:image-captioning}
    Image captioning datasets combine visual and textual information, requiring models to generate descriptive sentences for images.
    Flickr30k~\cite{young2014image} and MS COCO Captions~\cite{lin2014microsoft} are widely used benchmarks in this domain.
    These datasets, however, typically provide only general descriptions without explicit grounding to specific image regions or objects.
    Visual Genome~\cite{krishna2017visual} takes a step towards grounded captioning by providing several region-specific
    captions for each image, where each caption describes a particular area defined by a single bounding box.
    While this approach offers more localized descriptions, it differs from our task in GroundCap.
    Recent advances in Multimodal Large Language Models, such as Flamingo~\cite{alayrac2022flamingo},
    LLaVA~\cite{liu2023visual} and Pixtral-12B~\cite{pixtral12B} have significantly improved image captioning capabilities.
    While some of these models, like Qwen-VL, provide visual grounding capabilities, they lack the ability to maintain
    consistent object identities across multiple references within a caption.

    \subsection{Action Recognition}\label{subsec:action-recognition}

    Datasets for action recognition in images and videos, such as HICO-DET~\cite{chao2018learning} and AVA~\cite{gu2018ava},
    are relevant as they annotate human actions and interactions with objects.
    HICO-DET provides annotations for 600 human-object interaction categories in static images, while AVA offers
    dense annotations of 80 atomic visual actions in movie clips.
    IXMAS~\cite{weinland2006free} uses multiple camera views to
    record actions, enabling viewpoint-independent action recognition.
    HMDB~\cite{hilde2011hmdb} provides a large collection of 51 human action categories with
    over 6,766 video clips extracted from various sources.
    The UCF dataset~\cite{liu2009recognizing} specifically addresses the challenge of recognizing actions ``in the wild'',
    featuring videos that capture real-world scenarios.

    \subsection{Grounded Captioning}\label{subsec:grounded-captioning}
    Several works have explored aspects of grounded captioning, focusing on linking textual descriptions to visual elements.

    Visual Genome~\cite{krishna2017visual}, provides multiple region descriptions per image, each associated with a single bounding box.
    This approach limits the ability to create dense descriptions where words can be simultaneously grounded to multiple objects and actions across regions if the frame.
    The region-specific approach also constrains the ability to capture scene descriptions that integrate both static elements and dynamic interactions within a caption.

    Recent work has explored different approaches to visual grounding using \glspl{llm}.
    GROUNDHOG~\cite{zhang2024groundhog} uses pixel-level segmentation with unified \texttt{<GRD>} tags,
    Groma~\cite{ma2024groma} employs region tokens for arbitrary region grounding,
    and KOSMOS-2~\cite{peng2023kosmos} represents coordinates as discrete location tokens in a Markdown-like format.
    Florence-2~\cite{xiao2023Florence2AA} performs grounding through a sequence-to-sequence architecture that quantizes
    coordinates into 1,000 bins and represents regions in various formats (box, quad box, or polygon) depending on the task
    requirements.
    Panoptic Narrative Grounding (PNG)~\cite{gonzalez2021panoptic} grounds descriptions to panoptic segmentation regions
    using mouse trace annotations and WordNet-based~\cite{wordnet1994} semantic matching.
    \cite{cai2024top} proposes a one-stage weakly-supervised approach that processes raw RGB images and computes visual language attention maps.

    Despite their promising performance, these models lack the ability to track object identities across multiple references
    or ground both actions and objects simultaneously.