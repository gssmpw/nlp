@inproceedings{alayrac2022flamingo,
    title = {Flamingo: a Visual Language Model for Few-Shot Learning},
    author = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year = {2022},
}

@article{cai2024top,
    title = {Top-down framework for weakly-supervised grounded image captioning},
    journal = {Knowledge-Based Systems},
    volume = {287},
    pages = {111433},
    year = {2024},
    issn = {0950-7051},
    author = {Chen Cai and Suchen Wang and Kim-Hui Yap and Yi Wang},
    keywords = {Grounded image captioning, Weakly-supervised learning, Object grounding, Relation understanding},
    abstract = {Weakly-supervised grounded image captioning (WSGIC) aims to generate the caption and ground (localize) predicted object words in the input image without using bounding box supervision. Recent two-stage solutions mostly apply a bottom-up pipeline: (1) encode the input image into multiple region features using an object detector; (2) leverage region features for captioning and grounding. However, utilizing independent proposals produced by object detectors tends to make the subsequent grounded captioner overfitted in finding the correct object words, overlooking the relation between objects, and selecting incompatible proposal regions for grounding. To address these issues, we propose a one-stage weakly-supervised grounded captioner that directly takes the RGB image as input to perform captioning and grounding at the top-down image level. Specifically, we encode the image into visual token representations and propose a Recurrent Grounding Module (RGM) in the decoder to obtain precise Visual Language Attention Maps (VLAMs), which recognize the spatial locations of the objects. In addition, we explicitly inject a relation module into our one-stage framework to encourage relation understanding through multi-label classification. This relation semantics served as contextual information facilitating the prediction of relation and object words in the caption. We observe that the relation semantic not only assists the grounded captioner in generating a more accurate caption but also improves the grounding performance. We validate the effectiveness of our proposed method on two challenging datasets (Flick30k Entities captioning and MSCOCO captioning). The experimental results demonstrate that our method achieves state-of-the-art grounding performance. We made the code publicly available.}
}

@article{chao2018learning,
    title = {Learning to Detect Human-Object Interactions},
    author = {Yu-Wei Chao and Yunfan Liu and Michael Xieyang Liu and Huayi Zeng and Jia Deng},
    journal = {IEEE Winter Conf. on Applications of Computer Vision},
    year = {2017},
    pages = {381-389},
}

@article{deng2009imagenet,
    title = {ImageNet: A large-scale hierarchical image database},
    author = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and K. Li and Li Fei-Fei},
    journal = {IEEE Conf. on Computer Vision and Pattern Recognition},
    year = {2009},
    pages = {248-255},
}

@article{everingham2010pascal,
    author = {Everingham, Mark and Van Gool, Luc and Williams, Christopher and Winn, John and Zisserman, Andrew},
    year = {2010},
    month = {06},
    pages = {303-338},
    title = {The {P}ascal {V}isual {O}bject {C}lasses {(VOC)} challenge},
    volume = {88},
    journal = {Intl. Journal of Computer Vision},
}

@article{fei2004learning,
    title = {Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories},
    journal = {Computer Vision and Image Understanding},
    volume = {106},
    number = {1},
    pages = {59-70},
    year = {2007},
    note = {Special issue on Generative Model Based Vision},
    issn = {1077-3142},
    author = {Li Fei-Fei and Rob Fergus and Pietro Perona},
    keywords = {Object recognition, Categorization, Generative model, Incremental learning, Bayesian model},
}

@article{gonzalez2021panoptic,
    title = {Panoptic Narrative Grounding},
    author = {Cristina Gonz{\'a}lez and Nicol{\'a}s Ayobi and Isabela Hern{\'a}ndez and Jos{\'e} Hern{\'a}ndez and Jordi Pont-Tuset and Pablo Arbel{\'a}ez},
    journal = {IEEE Intl. Conf. on Computer Vision},
    year = {2021},
    pages = {1344-1353},
}

@article{gu2018ava,
    title = {AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions},
    author = {Chunhui Gu and Chen Sun and Sudheendra Vijayanarasimhan and Caroline Pantofaru and David A. Ross and George Toderici and Yeqing Li and Susanna Ricco and Rahul Sukthankar and Cordelia Schmid and Jitendra Malik},
    journal = {IEEE Conf. on Computer Vision and Pattern Recognition},
    year = {2017},
    pages = {6047-6056},
}

@article{hilde2011hmdb,
    title = {{HMDB}: A large video database for human motion recognition},
    author = {Hilde Kuehne and Hueihan Jhuang and Est{\'i}baliz Garrote and Tomaso A. Poggio and Thomas Serre},
    journal = {IEEE Intl. Conf. on Computer Vision},
    year = {2011},
    pages = {2556-2563},
}

@article{krishna2017visual,
    abstract = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked ``What vehicle is the person riding?'', computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) to answer correctly that ``the person is riding a horse-drawn carriage.''In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 108K images where each image has an average of {\$}{\$}35{\$}{\$}objects, {\$}{\$}26{\$}{\$}attributes, and {\$}{\$}21{\$}{\$}pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answer pairs.},
    author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Fei-Fei, Li},
    date = {2017/05/01},
    date-added = {2025-03-24 16:14:37 +0000},
    date-modified = {2025-03-24 16:14:37 +0000},
    doi = {10.1007/s11263-016-0981-7},
    id = {Krishna2017},
    isbn = {1573-1405},
    journal = {Intl. Journal of Computer Vision},
    number = {1},
    pages = {32--73},
    title = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
    volume = {123},
    year = {2017},
}

@article{krizhevsky2012imagenet,
    title = {ImageNet classification with deep convolutional neural networks},
    author = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    journal = {Comm. of the ACM},
    year = {2012},
    volume = {60},
    pages = {84 - 90},
}

@inproceedings{lin2014microsoft,
    title = {Microsoft COCO: Common Objects in Context},
    author = {Tsung-Yi Lin and Michael Maire and Serge J. Belongie and James Hays and Pietro Perona and Deva Ramanan and Piotr Doll{\'a}r and C. Lawrence Zitnick},
    booktitle = {European Conf. on Computer Vision},
    year = {2014},
}

@inproceedings{liu2009recognizing,
    author = {Liu, Jingen and Jiebo Luo and Shah, Mubarak},
    booktitle = {IEEE Conf. on Computer Vision and Pattern Recognition},
    title = {Recognizing realistic actions from videos “in the wild”},
    year = {2009},
    volume = {},
    number = {},
    pages = {1996-2003},
    keywords = {Videos;Cameras;YouTube;Humans;Feature extraction;Motion pictures;Shape;Spatiotemporal phenomena;Computer vision;Vocabulary},
}

@inproceedings{liu2023visual,
    author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
    pages = {34892--34916},
    publisher = {Curran Associates, Inc.},
    title = {Visual Instruction Tuning},
    volume = {36},
    year = {2023}
}

@inproceedings{ma2024groma,
    author = "Ma, Chuofan and Jiang, Yi and Wu, Jiannan and Yuan, Zehuan and Qi, Xiaojuan",
    editor = "Leonardis, Ale{\v{s}} and Ricci, Elisa and Roth, Stefan and Russakovsky, Olga and Sattler, Torsten and Varol, G{\"u}l",
    title = "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
    booktitle = "Computer Vision -- ECCV 2024",
    year = "2025",
    publisher = "Springer Nature Switzerland",
    address = "Cham",
    pages = "417--435",
    isbn = "978-3-031-72658-3"
}

@inproceedings{peng2023kosmos,
    title = {Kosmos-2: Grounding Multimodal Large Language Models to the World},
    author = {Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},
    booktitle = {The Twelfth Intl. Conf. on Learning Representations},
    year = {2024},
}

@article{pixtral12B,
    title = {Pixtral 12B},
    author = {Pravesh Agrawal and Szymon Antoniak and Emma Bou Hanna and Devendra Singh Chaplot and Jessica Chudnovsky and Saurabh Garg and Th{\'e}ophile Gervet and Soham Ghosh and Am'elie H'eliou and Paul Jacob and Albert Q. Jiang and Timoth{\'e}e Lacroix and others},
    journal = {ArXiv},
    year = {2024},
    volume = {2410.07073},
}

@inproceedings{refitgame2014,
    title = {{R}efer{I}t{G}ame: Referring to Objects in Photographs of Natural Scenes},
    author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
    editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
    booktitle = {Empirical Methods in Natural Language Processing},
    month = oct,
    year = {2014},
    address = {Doha, Qatar},
    publisher = {ACL},
    pages = {787--798},
}

@article{tong2023rethinking,
    title = {Rethinking PASCAL-VOC and MS-COCO dataset for small object detection},
    journal = {Journal of Visual Communication and Image Representation},
    volume = {93},
    pages = {103830},
    year = {2023},
    issn = {1047-3203},
    author = {Kang Tong and Yiquan Wu},
    keywords = {Data annotation, Small object detection, SDOD, Mini6K, Mini2022, Mini6KClean},
    abstract = {The data and the algorithm are critical to deep learning-based small object detectors. In this paper, we rethink the PASCAL-VOC and MS-COCO dataset for small object detection. By visual analysis of the original annotations, we find that there are different labeling errors in these two datasets. To solve these problems, we build specific datasets, including SDOD, Mini6K, Mini2022 and Mini6KClean. The experimental results of several typical algorithms (e.g. SSD, YOLOv5, Faster RCNN and Deformable DETR) on the datasets show that data labeling errors (such as missing labels, category label errors, inappropriate labels) are another factor that affects the detection performance of small objects.}
}

@article{weinland2006free,
    title = {Free viewpoint action recognition using motion history volumes},
    journal = {Computer Vision and Image Understanding},
    volume = {104},
    number = {2},
    pages = {249-257},
    year = {2006},
    issn = {1077-3142},
    author = {Daniel Weinland and Remi Ronfard and Edmond Boyer},
    keywords = {Action recognition, View invariance, Volumetric reconstruction},
    abstract = {Action recognition is an important and challenging topic in computer vision, with many important applications including video surveillance, automated cinematography and understanding of social interaction. Yet, most current work in gesture or action interpretation remains rooted in view-dependent representations. This paper introduces Motion History Volumes (MHV) as a free-viewpoint representation for human actions in the case of multiple calibrated, and background-subtracted, video cameras. We present algorithms for computing, aligning and comparing MHVs of different actions performed by different people in a variety of viewpoints. Alignment and comparisons are performed efficiently using Fourier transforms in cylindrical coordinates around the vertical axis. Results indicate that this representation can be used to learn and recognize basic human action classes, independently of gender, body size and viewpoint.}
}

@inproceedings{wordnet1994,
    title = {{W}ord{N}et: A Lexical Database for {E}nglish},
    author = {Miller, George A.},
    booktitle = {{H}uman {L}anguage {T}echnology: Proc. of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994},
    year = {1994},
}

@article{xiao2023Florence2AA,
    title = {Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks},
    author = {Bin Xiao and Haiping Wu and Weijian Xu and Xiyang Dai and Houdong Hu and Yumao Lu and Michael Zeng and Ce Liu and Lu Yuan},
    journal = {IEEE Conf. on Computer Vision and Pattern Recognition},
    year = {2023},
    pages = {4818-4829},
}

@article{young2014image,
    title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
    author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
    editor = {Lin, Dekang and Collins, Michael and Lee, Lillian},
    journal = {Trans. of the ACL},
    volume = {2},
    year = {2014},
    address = {Cambridge, MA},
    publisher = {MIT Press},
    pages = {67--78},
    abstract = {We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.},
}

@inproceedings{yu2016modeling,
    author = {Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C. and Berg, Tamara L.},
    editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
    title = {Modeling Context in Referring Expressions},
    booktitle = {Computer Vision -- ECCV 2016},
    year = {2016},
    publisher = {Springer},
    address = {Cham},
    pages = {69--85},
    abstract = {Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension.},
    isbn = {978-3-319-46475-6}
}

@inproceedings{zhang2024groundhog,
    author = {Zhang, Yichi and Ma, Ziqiao and Gao, Xiaofeng and Shakiah, Suhaila and Gao, Qiaozi and Chai, Joyce},
    title = {GROUNDHOG: Grounding Large Language Models to Holistic Segmentation},
    booktitle = {IEEE Conf. on Computer Vision and Pattern Recognition},
    month = {June},
    year = {2024},
    pages = {14227-14238}
}

