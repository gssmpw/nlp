\section{Related Work}
\label{sec:related-work}
    In the context of grounded captioning, several tasks and their associated datasets are particularly relevant, as discussed in
    the following sections.

    \subsection{Image Classification and Object Detection}\label{subsec:image-classification-and-object-detection}
    The development of object recognition began with datasets like Caltech 101____,
    which established foundations for categorizing visual objects.
    A significant leap forward was made with ImageNet____, containing millions of images across thousands of categories, enabling breakthroughs in deep learning for visual recognition____.
    Object detection datasets build upon classification by requiring localization of objects.
    Notable examples include PASCAL VOC____ and MS COCO____, which provide bounding
    box annotations for multiple object categories in natural images.
    The RefCOCO datasets____ further extended object localization
    by introducing natural language descriptions to guide object selection.
    Recent work____ has focused on improving annotation quality of existing benchmarks.

    \subsection{Image Captioning}\label{subsec:image-captioning}
    Image captioning datasets combine visual and textual information, requiring models to generate descriptive sentences for images.
    Flickr30k____ and MS COCO Captions____ are widely used benchmarks in this domain.
    These datasets, however, typically provide only general descriptions without explicit grounding to specific image regions or objects.
    Visual Genome____ takes a step towards grounded captioning by providing several region-specific
    captions for each image, where each caption describes a particular area defined by a single bounding box.
    While this approach offers more localized descriptions, it differs from our task in GroundCap.
    Recent advances in Multimodal Large Language Models, such as Flamingo____,
    LLaVA____ and Pixtral-12B____ have significantly improved image captioning capabilities.
    While some of these models, like Qwen-VL, provide visual grounding capabilities, they lack the ability to maintain
    consistent object identities across multiple references within a caption.

    \subsection{Action Recognition}\label{subsec:action-recognition}

    Datasets for action recognition in images and videos, such as HICO-DET____ and AVA____,
    are relevant as they annotate human actions and interactions with objects.
    HICO-DET provides annotations for 600 human-object interaction categories in static images, while AVA offers
    dense annotations of 80 atomic visual actions in movie clips.
    IXMAS____ uses multiple camera views to
    record actions, enabling viewpoint-independent action recognition.
    HMDB____ provides a large collection of 51 human action categories with
    over 6,766 video clips extracted from various sources.
    The UCF dataset____ specifically addresses the challenge of recognizing actions ``in the wild'',
    featuring videos that capture real-world scenarios.

    \subsection{Grounded Captioning}\label{subsec:grounded-captioning}
    Several works have explored aspects of grounded captioning, focusing on linking textual descriptions to visual elements.

    Visual Genome____, provides multiple region descriptions per image, each associated with a single bounding box.
    This approach limits the ability to create dense descriptions where words can be simultaneously grounded to multiple objects and actions across regions if the frame.
    The region-specific approach also constrains the ability to capture scene descriptions that integrate both static elements and dynamic interactions within a caption.

    Recent work has explored different approaches to visual grounding using \glspl{llm}.
    GROUNDHOG____ uses pixel-level segmentation with unified \texttt{<GRD>} tags,
    Groma____ employs region tokens for arbitrary region grounding,
    and KOSMOS-2____ represents coordinates as discrete location tokens in a Markdown-like format.
    Florence-2____ performs grounding through a sequence-to-sequence architecture that quantizes
    coordinates into 1,000 bins and represents regions in various formats (box, quad box, or polygon) depending on the task
    requirements.
    Panoptic Narrative Grounding (PNG)____ grounds descriptions to panoptic segmentation regions
    using mouse trace annotations and WordNet-based____ semantic matching.
    ____ proposes a one-stage weakly-supervised approach that processes raw RGB images and computes visual language attention maps.

    Despite their promising performance, these models lack the ability to track object identities across multiple references
    or ground both actions and objects simultaneously.