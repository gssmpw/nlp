% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% add
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{listings}
\usepackage{threeparttable}
\usepackage[utf8]{inputenc}    % 支持UTF-8编码
\usepackage{listingsutf8}      % 支持UTF-8编码下的lstlisting


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation}


% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}
\author {
    % Authors
    Xiang Geng\textsuperscript{\rm 1}\footnotemark[1] \quad 
    Zhejian Lai\textsuperscript{\rm 1}\footnotemark[1] \quad
    {\bf Jiajun Chen}\textsuperscript{\rm 1} \quad
    {\bf Hao Yang}\textsuperscript{\rm 2} \quad
    {\bf Shujian Huang}\textsuperscript{\rm 1} \footnotemark[2] \\
    % \Affiliation
        \textsuperscript{\rm 1} National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China\\
        \textsuperscript{\rm 2} Huawei Translation Services Center, Beijing, China\\
        \texttt{\{gx,laizj\}@smail.nju.edu.cn} \\
        \texttt{\{yanghao30\}@huawei.com,\{chenjj,huangsj\}@nju.edu.cn}
        % \emails
        % \{first, second\}@example.com,
        % third@other.example.com,
        % fourth@example.com
}



\begin{document}
\maketitle

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution, in random order.}
\footnotetext[2]{Corresponding author.}
\renewcommand{\thefootnote}{\arabic{footnote}}


\begin{abstract}
Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task.
Due to the data scarcity, synthetic data generation has emerged as a promising solution.
However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences.
To tackle this issue, we introduce ADSQE, a novel framework for alleviating distribution shift in synthetic QE data.
To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models.
ADSQE uses references—i.e., translation supervision signals—to guide both the generation and annotation processes, enhancing the quality of token-level labels.
ADSE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels.
Specially, we underscore that the translation model can not annotate translations of itself accurately.
Extensive experiments demonstrate that ADSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings.
Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks.
\end{abstract}


\section{Introduction}
Unlike machine translation (MT) metrics such as BLEU~\cite{BLEU} and Comet~\cite{comet}, which rely on reference translations to evaluate quality, quality estimation (QE) assesses translation quality without any reference~\cite{mtqe}.
QE plays a crucial role in post-editing workflows by reducing human effort through filtering low-quality translations and identifying incorrect segments~\cite{effort}.
From the perspective of large language models (LLMs), QE models can function as reward models~\cite{RLHF} for machine translation.
\citet{QE-as-RM} explores the use of quality estimation to align translation models with human feedback, achieving significant improvements in translation performance.
LLMRefine~\cite{llmrefine} uses LLMs to refine translations according to fine-grained QE feedback.

\begin{figure}[t] 
    \centering 
    \includegraphics[width=\columnwidth]{fig/process.pdf} 
    \caption{
    We explore ways to enhance the quality of synthetic QE data by leveraging supervision signals and increasing model diversity.
    The histogram represents the generation probabilities of translation models.
    } 
    \label{fig:main} 
    \vspace{-20pt}
\end{figure}

The Multidimensional Quality Metrics (MQM)~\cite{MQM_old} annotations have become the primary standard for QE in recent years, as MQM scores are more reliable than the earlier Direct Assessment ~\cite{DA} scores ~\cite{MQM}.
As shown in Table~\ref{tab:example}, the MQM annotations are both fine-grained and explainable, offering not only error spans but also the severity of each error span.
Acquiring these fine-grained MQM annotations is labor-intensive. Consequently, their datasets are typically small and restricted to specific language pairs~\cite{wmt-2023-mt-findings}.


\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}
{
\begin{tabular}{l|l|l}
\hline
\textbf{SRC} & \multicolumn{2}{l}{Echidna with amethyst and magenta spikes .} \\
\textbf{MT} & \multicolumn{2}{l}{\textcolor{red}{Die} Echidna mit \textcolor{red}{Amethyst und Magenta-Spitzen} . } \\
\textbf{REF} & \multicolumn{2}{l}{Das Echidna mit Amethyst- und Magenta-Stacheln .} \\
\hline
\hline
\textbf{ID} & \textbf{Error Span}  & \textbf{Severity} \\
\textbf{No. 1} & \textcolor{red}{Die} & MINOR \\
\textbf{No. 2} & \textcolor{red}{Amethyst und Magenta-Spitzen} & CRITICAL \\
\hline
\hline
\textbf{Labels} & \multicolumn{2}{l}{\textcolor{red}{BAD} OK OK \textcolor{red}{BAD} \textcolor{red}{BAD} \textcolor{red}{BAD} \textcolor{red}{BAD} OK}\\
\textbf{Score} & \multicolumn{2}{l}{-0.375} \\
\hline

\end{tabular}
}
\caption{
At test time, only the source (SRC) and its translation (MT) are available; the reference (REF) is not accessible.
}
\label{tab:example}
\vspace{-15pt}
\end{table}


Therefore, existing studies aim to generate synthetic MQM data for QE from parallel sentences. For instance, MQMQE~\cite{wmt2023} randomly masks spans in the references, replaces these spans with tokens sampled negatively from a translation model, and annotates the severity using the generation probabilities from the model itself.
InstructScore~\cite{instructscore} instructs the GPT-4~\cite{gpt4} to generate errors with specific severities based on references.
 
Although these methods achieve competitive performance, the distribution of their synthetic data may differ significantly from that of real data.
The distribution shift problem not only causes a decrease in QE performance but also in downstream human preference optimization ~\cite{bpo}.
Specifically, the negative sampling strategy renders the synthetic translations of MQMQE less fluent. 
Moreover, the synthetic labels of MQMQE do not align with human preferences, as randomly masked spans often disrupt entire phrases, and the translation model tends to be overly confident in its own outputs.
While InstructScore produces fluent synthetic translations and accurate synthetic labels, the generated errors appear unnatural, unlike those that advanced translation models would typically produce.
More importantly, utilizing powerful closed-source LLMs requires substantial time and financial resources.

In this paper, we propose a framework, ADSQE (as shown in Figure~\ref{fig:main}), to \textbf{A}lleviate the \textbf{D}istribution \textbf{S}hift problem when generating synthetic \textbf{QE} data.
Specifically, we first train two translation models as Generator and Annotator respectively.
We then use the Generator to generate synthetic translations with constrained beam search (CBS)~\cite{cbsqe}, which preserves the main structure of references while maximizing generation probabilities. 
This allows us to treat matched tokens as correct with high accuracy using TER~\cite{hter} tool.
For the mismatched part, we utilize generation probabilities provided by the Annotator to rejudge their fine-grained severities.
Since human annotators typically prefer annotating entire phrases as spans, we propose an algorithm, which helps aggregate token-level labels into phrase-level labels.


Extensive experiments across three language directions (English-German, Chinese-English, and Hebrew-English) demonstrate that ADSQE achieves new (SOTA) results in both supervised and unsupervised settings. 
Further analysis offers insights into synthetic data generation that may benefit general reward models: 
(1) distribution shift problem is crucial in synthetic data methods;
(2) diversity between annotation and generation models enhances annotation accuracy;
(3) diversity in generation models provides further improvements;
(4) the capacity of the generation model must be balanced;
(5) enhancing the capacity of the annotation model with supervision signals is helpful.

To promote further research in this field, we will provide open access to our code and datasets (please refer to supplementary materials).

\section{Background}


\textbf{Quality estimation.}
The quality estimation task assesses the quality of translation without access to references.
Given a source sentence $\mathbf{x}$ and its machine translation $\hat{\mathbf{y}}= \{y_1, y_2, \dots, y_n\}$ with $n$ words, we aim to predict the following quality labels: 
(1) Span-level MQM labels $\mathbf{h}=\{h_1, h_2, \dots, h_n\}$, the label $h_i$ is categorically annotated with error severity (MINOR, MAJOR, or CRITICAL) through professional human annotation. 
(2) Word-level labels $\mathbf{g}=\{g_1, g_2, \dots, g_n\}$, the label $g_i$ is usually a binary label (OK or BAD), which is derived from the span-level label. 
Words within error spans are marked as ``BAD'', and vice versa. 
(3) Sentence-level MQM scores $s$, which is derived from the span-level labels. 
The scores can be calculated as 
\begin{align}\label{eq:mqm}
    s = 1 - \frac{n_{\text{MINOR}}+5\times n_{\text{MAJOR}}+10\times n_{\text{CRITICAL}}}{n},
\end{align}
where $n_{\text{severity}}$ denotes the number of each error severity and $n$ denotes the translation length.

\textbf{Model architecture.} Previous works, e.g. CometKiwi~\cite{CometKiwi2023} and MQMQE~\cite{wmt2023}, usually adopt multilingual pre-training language model, typically XLM-R~\cite{xlmr}, as the backbone architecture for the QE model.
The outputs from the final layer of the model are used to derive representations for each token.
Word-level representations are obtained by averaging the representations of all tokens within a word. 
Similarly, the regression score representation is computed by averaging the representations of all target tokens. 
These representations are subsequently fed into linear layers to predict word-level quality labels and regression scores, respectively.


\textbf{Training.} As noted in~\cite{wmt2023}, the span-level task can be regarded as a word-level task.
Thus, the overall objective only combines the sentence-level and word-level tasks.
The sentence-level task is treated as a regression problem optimized with MSE loss, while the word-level task is framed as a sequence labeling problem using cross-entropy loss.
In a supervised setting, the model is firstly pre-trained on synthetic data and then fine-tuned on real data; while unsupervised setting only trained using synthetic data.



\textbf{Inference.} The error severity can be determined by comparing the probability of ``OK'' against various thresholds.
For the span-level task, consecutive ``BAD'' tokens are identified as a span, with their error severity determined by the worst error severity within the span. 


\section{Method}


In this section, we first describe the process of generating realistic synthetic translations. Following this, we introduce methods for annotating synthetic labels to ensure they accurately align with human preferences.

\subsection{Synthetic Translations}



To produce synthetic translations that closely resemble real machine translations, we directly generate synthetic translations using the translation model with the constrained beam search (CBS) algorithm~\cite{cbsqe}.
Similar to the standard beam search (BS) algorithm, the CBS algorithm also seeks to generate translations with high generation probabilities, thereby producing natural translation errors.
However, the standard BS algorithm typically generates synonyms of the references, making it difficult to obtain accurate synthetic labels. 
In contrast, the CBS algorithm is designed to preserve reference tokens when their generation probabilities exceed a specified threshold, reducing the risk of generating synonyms.


In most QE applications, the target translation model is not accessible. 
As a result, existing studies train a surrogate translation model, referred to as the Generator, training on parallel corpora.
We assume that increasing the diversity of synthetic translations enhances the likelihood of generating translations that are more similar to the target.
However, as a variation of the BS algorithm, the CBS algorithm also struggles to generate diverse translations.
In preliminary experiments, we attempt to perturb Generators using dropout~\cite{dropout}, as suggested by~\cite{unsupervised_qe}, to produce diverse translations for a single source sentence. 
However this approach does not enhance the diversity of synthetic translations, nor does it improve QE performance.
Therefore, we further explore enhancing the diversity of Generators by training them on different parallel subsets in Section~\ref{sec:diversity_analysis}.


\begin{figure*}[ht] 
    \centering 
    \includegraphics[width=\textwidth]{fig/fig_SPCE.pdf}
    \caption{The illustration of the SPCE algorithm. 
    } 
    \label{fig:SPCE}
    \vspace{-15pt}
\end{figure*}

\subsection{Synthetic Labels}

\textbf{Coarse-grained labels.}
CBS allows us to preserve the main structure of references, thereby obtaining accurate labels using the exact match.
We use the TER tool to perform the word-level alignment between synthetic translation and the corresponding reference.
The match part is regarded as “OK”, and vice versa.
After generating synthetic translations, we need to annotate them to align with human preference.


\textbf{Refined labels.}
To assign fine-grained severities to each ``BAD'' token and correct false-negative labels, we use a translation model as the Annotator to rejudge the previous labels.
As demonstrated in~\cite{unsupervised_qe} and~\cite{ssqe}, the confidence of the translation model, i.e. the generation probabilities, serves as a reliable indicator for assessing translation quality.
Therefore, we utilize the generation probability $p_i$ of each token to determine its error severity.
Specifically, the severity label $\hat{h_i}$ is assigned as follows:
\begin{align}
\label{eq:get_severity}
\hat{h}_i&=\begin{cases}
\text{CRITICAL} &0 \le p_i < t_{\text{CRITICAL}} \\
\text{MAJOR} & t_{\text{CRITICAL}} \le p_i < t_{\text{MAJOR}} \\
\text{MINOR} & t_{\text{MAJOR}} \le p_i < t_{\text{MINOR}} \\
\text{OK} & {t_{\text{MINOR}} \le p_i \le 1} \\
\end{cases},
\end{align}
where $t_{\text{MINOR}} < t_{\text{MAJOR}} <t_{\text{CRITICAL}}$ are three ordered thresholds that can be determined using a validation dataset.


\label{sec:syn_labels}
\textbf{Leveraging supervision signals.}
When the model lacks the necessary translation knowledge for the given input, its generation probabilities become speculative and fail to meaningfully correlate with the severity of errors.
To address this issue, we ensure that the translation model is trained on parallel pairs that are also used for generating synthetic data.
Therefore, the Annotator becomes ``professional'' when working with the given parallel pairs, rather than ``amateur''.



\textbf{Differentiate the Annotator from the Generator.}
MQMQE~\cite{wmt2023} utilizes the same translation model to annotate its own outputs. However, this approach may lead to overconfidence in the model's predictions, resulting in inaccurate annotations.
To investigate this problem, we introduce another different, well-trained translation model to serve as the Annotator.


\textbf{Combine error tokens into human-readable phrases.}
Although we possess token-level labels, human annotators tend to prefer annotating entire phrases that contain translation errors to ensure the clarity and interpretability of the annotations.
At the same time, annotators strive to make the phrase as short as possible, ensuring it covers all consecutive error tokens without including any unnecessary ones.
Therefore, we propose the Shortest Phrase Covering Errors (SPCE) algorithm to combine error tokens into human-readable phrases.
As illustrated in Figure~\ref{fig:SPCE}, the SPCE algorithm operates as follows:

First, we parse the synthetic translation to obtain its dependency tree. 
Consecutive ``BAD'' tokens are added to the candidate set, which serves as the initial phrase (i.e. ``action with his'').
Following this, we iteratively execute the following steps:
\begin{itemize}
    \item Step a. We employ the Lowest Common Ancestor (LCA) algorithm~\cite{lca} to find the common ancestor node of the candidate tokens (i.e. word ``take''). 
    This step locates the smallest sub-tree that encompasses all errors.
    \vspace{-5pt}
    \item Step b. To ensure syntactic coherence, we add all tokens along the path from the LCA into the candidate set (i.e. the word ``consent'' is added as it lies on the path from ``his'' to `` take'').
    \vspace{-5pt}
    \item Step c. To ensure the candidate tokens are consecutive, we add all tokens, which are located between the leftmost and rightmost candidate tokens, into the candidate set (i.e. the word ``some'' is added as it appears  within the phrase ``take some action with his consent'').
\end{itemize}
The iteration terminates until no additional tokens need to be added to the candidate set.
We provide the detailed algorithm in Appendix~\ref{sec:spce_details}.

To assign a representative fine-grained label to each phrase, we determine its severity by selecting the most severe error label among the candidate tokens.


\section{Experiment}
\label{sec:exp}

In the experiment section, we aim to address the following research questions regarding ADSQE:
(1) How does ADSQE perform across supervised and unsupervised settings for various language pairs?
(2) To align synthetic labels with human preferences, we introduce the Annotator and the SPCE algorithm to generate  MQM data. To what extent are these techniques effective?
(3) Can the Annotator and the Generator be the same model? In other words, can a model fairly annotate its own output?
(4) Does increasing the diversity of Generators lead to enhanced QE performance?
(5) How does the translation performance of the Generator and the Annotator influence the quality of the synthetic data?
(6) Does ADSQE demonstrate advantages in terms of generation cost and convergence efficiency?

To address \textbf{Q1}, we evaluate ADSQE in both supervised and unsupervised settings across multiple language pairs (EN-DE, ZH-EN, HE-EN).
For \textbf{Q2}, we conduct ablation studies to quantify the individual contributions of the Annotator and the SPCE algorithm in improving the alignment of synthetic labels with human preferences.
Regarding \textbf{Q3}, we investigate the feasibility of employing a single model to simultaneously serve as both the Generator and the Annotator.
To explore \textbf{Q4}, we enhance the diversity of Generators by training them on distinct parallel subsets.
To address \textbf{Q5}, we control the translation performance of the Generator and the Annotator by training them on corpora of varying sizes.
Finally, for \textbf{Q6}, we compare ADSQE against other synthetic data approaches in terms of generation cost and convergence efficiency.


\subsection{Experiment Setup}
\paragraph{Datasets.}


In our experiment, we utilized the dataset provided by the Workshop on Machine Translation (WMT) QE Shared Task~\cite{wmt2023-findings}. 
The dataset comprises two types of data: \textbf{parallel data}\footnote{\url{https://www2.statmt.org/wmt23/translation-task.html\#training}} and \textbf{MQM data}\footnote{\url{https://wmt-qe-task.github.io/wmt-qe-2023/subtasks/resources}}.
Data statistics are given in Appendix~\ref{sec:data_statics}.

We employed parallel datasets for three language pairs: English-German (EN-DE), Chinese-English (ZH-EN), and Hebrew-English (HE-EN). 
Parallel data is widely used in the QE community including data synthesis, enhancement of translation knowledge, etc.
Unless specified, the parallel sentences employed for generating synthetic data and the training set for the Annotator are kept disjoint and the parallel sentences used for generation are derived from a subset of the Annotator's training set. 

We utilize the MQM training set from WMT2023 for EN-DE and ZH-EN language pairs.
For evaluation, we employ the MQM test set from WMT2022, which includes EN-DE and ZH-EN; the MQM test set from WMT2023, which includes EN-DE, ZH-EN, and HE-EN.
We exclude the WMT2022 test set from the supervised setting due to its overlap with the WMT2023 training set.
Furthermore, span-level evaluation is not performed on the WMT2022 test set, as it lacks the necessary span-level annotations.


\paragraph{Baselines.}
We incorporated top-performing baselines in our experiments:
\textbf{CometKiwi}~\cite{CometKiwi2023} stands out as the SOTA QE model, widely adopted in translation studies~\cite{wmt-2023-mt-findings}.
CometKiwi enhances its generalization capabilities by leveraging labeled QE datasets with various annotations\footnote{\url{https://github.com/sheffieldnlp/mlqe-pe}}, across multiple language pairs. 
Additionally, CometKiwi is built on the XLMR-XL model~\cite{xlmr}, which has seven times more parameters than ours.
\textbf{GEMBA-MQM}~\cite{gemba-mqm} employs few-shot prompts to guide GPT-4 in generating MQM predictions.
\textbf{InstructScore}~\cite{instructscore} and \textbf{MQMQE}~\cite{wmt2023} are two representative synthetic data approaches.
InstructScore generates MQM data by prompting GPT-4, whereas MQMQE employs a translation model combined with negative sampling to produce synthetic data. 


\begin{table*}[t]
\centering
\footnotesize
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Setting}} & \multirow{2}{*}{\textbf{Lang}} & \multirow{2}{*}{\textbf{Model Name}} & \multicolumn{2}{c}{\textbf{Sentence-level}} & \multicolumn{2}{c}{\textbf{Word-level}} & \multicolumn{3}{c}{\textbf{Span-level}} \\
\cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-10}
& & & \textbf{Spearman} & \textbf{Pearson} & \textbf{MCC} & \textbf{F1} & \textbf{F1} & \textbf{Prec} & \textbf{Recall} \\
\midrule
\multirow{10}{*}{{\rotatebox{90}{Supervised}}} & \multirow{5}{*}{23 EN-DE} & CometKiwi & 40.47 & 40.97 & 21.50 & - & 23.50 & - & - \\
 & & GEMBA & 40.06 & 35.12 & 13.21 & 18.17 & \hspace{5pt}5.40 & \hspace{5pt}7.50 & \hspace{5pt}4.22 \\
& & InstructScore & 35.03 & 32.85 & 22.54 & 26.60 & 23.77 & 18.19 & 34.46 \\
& & MQMQE & 37.88 & 25.07 & 22.84 & 25.22 & 21.14 & 17.13 & 27.62 \\
& & ADSQE & \textbf{43.17} & 41.64 & \textbf{27.11} & 30.61 & \textbf{25.89} & 21.20 & 33.26 \\
\cmidrule(lr){2-10}
&\multirow{5}{*}{23 ZH-EN} & CometKiwi & 40.35 & 35.53 & 26.90 & - & 27.20 & - & - \\
& & GEMBA & 33.80 & 32.56 & 16.11 & 18.21 & \hspace{5pt}9.23 & \hspace{5pt}8.41 & 10.22 \\
& & InstructScore & 36.40 & 28.00 & 26.05 & 29.54  & 26.56 & 23.87 & 29.94 \\
& & MQMQE & 39.26 & 22.94 & 23.53 & 26.49 & 22.01 & 22.65 & 21.40 \\
& & ADSQE & \textbf{46.41} & 37.55 & \textbf{28.12} & 28.61 & \textbf{27.71} & 22.19 & 36.88 \\
\midrule
\multirow{17}{*}{{\rotatebox{90}{Unsupervised}}} & \multirow{3}{*}{23 HE-EN} & CometKiwi & 55.00 & 44.15 & 33.40 & - & 10.50 & - & - \\
& & GEMBA & 54.63 & 35.27 & 21.79 & 28.39 & 12.12 & 14.87 & 10.23 \\
& & InstructScore & 31.72 & 33.18 & 32.39 & 37.25 & 36.29 & 30.84 & 44.08 \\
& & MQMQE & 25.90 & 15.66 & 16.19 & \hspace{5pt}8.39 & 23.78 & 23.57 & 23.99 \\
& & ADSQE & \textbf{56.46} & 45.06 & \textbf{36.34} & 38.28 & \textbf{39.51} & 42.25 & 37.10 \\
\cmidrule(lr){2-10}
& \multirow{4}{*}{23 EN-DE} & InstructScore & 12.08 & 20.16 & \textbf{18.97} & 22.59 & 19.70 & 14.94 & 28.95 \\
& & MQMQE & 24.11 & 20.99 & \hspace{5pt}7.49 & \hspace{5pt}2.39 & 16.79 & 19.80 & 14.58  \\
& & ADSQE & \textbf{35.78} & 37.19 & 18.00 & 21.91 & \textbf{20.15} & 16.27 & 26.46 \\
\cmidrule(lr){2-10}
& \multirow{3}{*}{23 ZH-EN} & InstructScore & 30.46 & 30.11 & 21.71 & 23.88 & 21.60 & 14.93 & 39.34 \\
& & MQMQE & \hspace{5pt}6.52 & 19.35 & \hspace{5pt}3.07 & \hspace{5pt}1.13 & 14.05 & 19.80 & 10.89 \\
& & ADSQE & \textbf{37.54} & 28.04 & \textbf{23.41} & 26.45 & \textbf{23.67} & 20.52 & 27.98 \\
\cmidrule(lr){2-10}
& \multirow{4}{*}{22 EN-DE} & InstructScore & 24.00 & 35.07 & 22.32 & 22.05 & - & - & - \\
& & MQMQE & 40.22 & 36.15 & 12.07 & \hspace{5pt}4.92 & - & - & - \\
& & ADSQE & \textbf{41.27} & 40.88 & \textbf{24.36} & 25.44 & - & - \\
\cmidrule(lr){2-10}
& \multirow{3}{*}{22 ZH-EN} & InstructScore & 13.53 & 25.75 & \hspace{5pt}5.53 & \hspace{5pt}6.58 & - & - & - \\
& & MQMQE & \hspace{1pt}-0.88 & 33.99 & -0.35 & \hspace{5pt}0.00 & - & - & - \\
& & ADSQE & \textbf{26.27} & 44.04 & \textbf{10.27} & 11.57 & - &  - & - \\
\bottomrule
\end{tabular}
\caption{Main results on different QE test sets. We follow the setting in GEMBA-MQM~\cite{gemba-mqm}, which utilizes a few-shot prompt containing examples for EN-DE and ZH-EN language pairs. 
However, since the prompt does not include examples for HE-EN, GEMBA-MQM is treated as a supervised method for EN-DE and ZH-EN, while an unsupervised one for HE-EN.
}
\vspace{-10pt}
\label{tab:main_result}
\end{table*}



\paragraph{Implementation Details.}
The Generator and the Annotator for synthesizing pseudo MQM data are based on the Transformer-Large~\cite{transformer} architecture with a shared decoder input-output embedding.
We use the TER tool called TERCOM\footnote{\url{https://www.cs.umd.edu/~snover/tercom/}} to annotate the synthetic translations generated by the Generator.
We train the QE model using the XLMR-L backbone~\cite{xlmr} for all synthetic data approaches. 
The experiments are implemented using the open-source Fairseq toolkit~\cite{fairseq} and conducted on NVIDIA V100 GPUs.



Since CometKiwi does not provide sentence-level results for the individual model, we reproduce sentence-level results using the released model\footnote{\url{https://huggingface.co/Unbabel/wmt23-cometkiwi-da-xl}}. 
For word- and span-level results, we directly utilize the results provided in the CometKiwi report~\cite{CometKiwi2023}, as the corresponding word- and span-level models have not been released.
InstructScore is implemented using 10K synthetic data released by~\citet{instructscore}. 
For ADSQE and MQMQE, we generate 500K synthetic data in each language pair.
In Section~\ref{sec:efficiency}, we demonstrate that ADSQE remains competitive with InstructScore when trained on 10K synthetic data while requiring significantly lower costs.
Additional implementation details are provided in Appendix~\ref{sec:appendix oid}.




\paragraph{Evaluations.}
Following WMT23 QE shared tasks~\cite{wmt2023-findings}, sentence-level evaluation utilizes Spearman's rank correlation coefficient as the primary metric, complemented by Pearson's correlation coefficient.
For word-level evaluation, the Matthews Correlation Coefficient (MCC) serves as the primary metric, complemented by F1 score.  
For span-level evaluation, the primary metric is the weighted F1 score, which accounts for all error severities.
We mark the results with \textbf{bold} if the results are statistically significant ($p<0.05$) under Williams significance test~\cite{williams_test}.



\subsection{Main Results}
\paragraph{Supervised setting.}
As demonstrated in Table~\ref{tab:main_result}, ADSQE substantially outperforms CometKiwi despite utilizing fewer parameters, achieving notable improvements with average gains of 4.38 in Spearman, 3.41 in MCC, and 1.45 in F1 score.
Moreover, ADSQE significantly outperforms GEMBA-MQM, which is based on the advanced LLM, GPT-4. 
Compared to other synthetic methods, i.e., MQMQE and InstructScore, ADSQE demonstrates consistent superiority, indicating that our synthetic data is of higher quality.


\paragraph{Unsupervised Setting.}
As shown in Table~\ref{tab:main_result}, both MQMQE and InstructScore demonstrate significant performance declines compared to their supervised counterparts, with reductions of 15.74 and 7.64 on average, respectively.
That implies the distribution shift problem in previous synthetic QE data. 
In contrast, our proposed method achieves superior robustness, incurring a smaller average reduction of 6.64 points. 
Moreover, ADSQE outperforms CometKiwi, which relies on labeled datasets from other language pairs, on HE-EN. This indicates that our synthetic data provides more relevant QE knowledge for HE-EN compared to datasets from different language pairs.

\subsection{Ablation Study}


The ADSQE framework introduces two techniques to align the synthetic labels with human preferences: 
(1) leveraging the Annotator to rejudge tokens initially labeled as BAD by the TER tool, 
and (2) applying the SPCE Algorithm to aggregate token-level annotations into span-level annotations.
The effectiveness of these techniques within the ADSQE framework is empirically validated in Table~\ref{tab:ablation_study}. 
The results demonstrate that the Annotator effectively corrects errors in coarse-grained labels.
Furthermore, the SPCE algorithm successfully identifies phrase spans, thereby achieving better alignment with human preferences.

\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Spearman}$\uparrow$ & \textbf{MCC}$\uparrow$ \\
\midrule
ADSQE & 35.78 & 18.00 \\
\hspace{5pt} -SPCE & 30.99 & 15.70 \\
\hspace{5pt} -SPCE \& Annotator & 11.24 & 11.17 \\
\bottomrule
\end{tabular}
\caption{Ablation studies on the WMT23 EN-DE test set.}
\label{tab:ablation_study}
\vspace{-10pt}
\end{table}



\subsection{Analysis}

In this subsection, we aim to investigate the contributions of the Generator and the Annotator to the performance of ADSQE. 
We create translation models with varying levels of performance by training them on datasets of different sizes.
We train three translation models—$S$, $M$, and $L$—using 1M, 5M, and 20M distinct sentence pairs, respectively. 
To enhance diversity while maintaining similar performance, we then train three additional models—$S'$, $M'$, and $L'$—on the remaining 1M, 5M, and 20M sentence pairs.
All subsequent analyses are conducted on the WMT23 EN-DE test set in the unsupervised setting.



\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{ccccc}
\toprule
\textbf{Generator} & \textbf{Annotator} & \textbf{Error Rate (\%)} & \textbf{Spearman}$\uparrow$ & \textbf{MCC}$\uparrow$ \\
\midrule
$M$& $M$ & \hspace{5pt}1.60 & 25.91 & 10.36 \\
$L$& $L$& \hspace{5pt}0.11 & 29.00 & 11.61 \\
$M$& $L$ & 19.23 & 35.78 & 18.00 \\
\bottomrule
\end{tabular}}
\caption{Analysis comparing individual and collaborative results of Generator $M$ and Annotator $L'$.}
\label{tab:calibrate_study}
\vspace{-10pt}
\end{table}

\textbf{The model cannot fairly annotate its outputs.}
As discussed in Section~\ref{sec:syn_labels}, the translation model may exhibit overconfidence in its output, leading to an increased number of ``OK'' labels.
To investigate that, we calculate the error rate across different settings.
Table~\ref{tab:calibrate_study} shows that translation models tend to consistently assign more ``OK'' labels to their own output, regardless of the model's performance.
The poor QE performance implies that most of the ``OK'' labels are false-negative.



\textbf{Diversity of Generators enhances QE performance.}
\label{sec:diversity_analysis}
To investigate the impact of the Generator's diversity on the QE performance, we employ $L$ and $L'$ to generate synthetic data.
We quantify the diversity between $L$ and $L'$ by calculating the BLEU score between their output for the same source.
The average BLEU score on Flores-200\footnote{\url{https://github.com/facebookresearch/flores/blob/main/flores200/README.md}} is 80.06.
The diversity result in distinct translation errors, which are more likely to comprehensively cover realistic errors.
As a result, as shown in Table~\ref{tab:generator_diversity}, generating diverse synthetic data for the identical parallel pair also enhances the QE performance.


\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c c c c}
        \toprule
        \textbf{Generator} & \textbf{Annotator} & \textbf{Spearman}$\uparrow$ & \textbf{MCC}$\uparrow$ \\
        \midrule
        $L$ & $M$ & 31.19 & 10.08 \\ 
        $L+L'$ & $M$ & 32.19 & 11.05 \\
        \bottomrule
    \end{tabular}
    \caption{The impact of diversity in Generators.}
    \label{tab:generator_diversity}
    \vspace{-10pt}
\end{table}


\textbf{The capacity of the Generators must be balanced.}
If the Generator is too strong, the generated data will have few errors, limiting learning opportunities; if the Generator is too weak, the data will be too noisy or unrealistic.
To investigate this point, we measure the error rate and similarity between synthetic translation and the translation across different Generators in Table~\ref{tab:generator_info}.
To be specific, the similarity is measured by the BLEU score, which calculates between synthetic translation and the ``real'' translation from the WMT2023 QE validation set.



\begin{table}[t]
    \centering
    \footnotesize
    \begin{tabular}{c c c}
        \toprule
        \textbf{Generator} & \textbf{Error Rate (\%)} & \textbf{BLEU} \\
        \midrule
        $S$ & 32.68 & 31.02 \\ 
        $M$ & 27.02 & 43.08 \\ 
        $L$ & \hspace{5pt}0.58 & 51.13 \\
        \bottomrule
    \end{tabular}
    \caption{Metrics on Generators.}
    \label{tab:generator_info}
    \vspace{-10pt}
\end{table}

To examine the influence of error rate, we fixes Generator $M$ for generation and downsamples its synthetic data to match the error rates corresponding to each Generator.
Likewise, to examine the influence of similarity, we fix the error rate of Generator $M$ for downsampling and utilize different Generators for generation.
As shown in Figure~\ref{fig:generator_analysis}, when the similarity is fixed, the performance degradation as the error rate decreases. 
Similarly, when the error rate is fixed, the performance improves with increasing similarity. 
That explains why ADSQE achieves the highest performance using Generator $M$. 

\begin{figure}[t] % 使用 [h] 参数让图片尽可能接近插入位置
    \centering % 图片居中
    \includegraphics[width=\columnwidth]{fig/fig_generator_analysis.pdf} % 设置图片宽度为文本宽度的50%
    \caption{Impact of Generator metrics on synthetic data quality in sentence-level and word-level tasks.} % 添加标题
    % size指代不清 
    \label{fig:generator_analysis} % 设置标签，用于引用
    \vspace{-5pt}
\end{figure}


\textbf{Enhancing the capacity of annotation model with supervision signals is helpful.}
\label{sec:enhance_annotator}
There are two possible solutions to enrich the Annotator's translation knowledge: (1) leveraging the parallel data from the Annotator's training set for synthetic data generation, and (2) expansion of the training corpus.
To evaluate their effectiveness, we conduct a comparative analysis of performance across different Annotators in Table~\ref{tab:annotator_study}.
The $M_\text{amateur}$ denotes the Annotator is not trained on the parallel pair for the generation.
The results demonstrates that both solutions enhance the capacity of the annotation model resulting in better QE performance.
The case study (Table~\ref{tab:annotator_case_study} see in Appendix) demonstrates a similar conclusion.





\begin{table}[t]
\centering
\footnotesize
\begin{tabular}{cccc}
\toprule
 \textbf{Generator} & \textbf{Annotator} & \textbf{Spearman}$\uparrow$ & \textbf{MCC}$\uparrow$ \\
\midrule
$M$ & $M_\text{amateur}$ & 29.12 & 13.84 \\
$M$ & $M'$ & 30.43 & 15.01 \\
$M$ & $L$ & 35.78 & 18.00 \\
\bottomrule
\end{tabular}
\caption{Comparison among different Annotators.}
\label{tab:annotator_study}
\vspace{-10pt}
\end{table}

\subsection{Generation Cost}
\label{sec:efficiency}
We measure the generation cost for various synthetic data approaches using 10K samples.
As shown in Table~\ref{tab:generation_time}, ADSQE generates synthetic data 14.29$\times$ faster than InstructScore.
Meanwhile, with the same amount of synthetic data, ADSQE also achieves a notable improvement of 14.29 in the Spearman score with slight decline of 1.76 in MCC.
Despite introducing rejudge and SPCE algorithm, ADSQE does not substantially increase computational complexity, requiring only 3.12$\times$ the timelapse of MQMQE.
Further analysis of ADSQE time overhead is provided in Section~\ref{sec:time_analysis}.

\begin{table}[h]
\centering
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{c c c}
\toprule
\textbf{Method} &  \textbf{Generation Time (ms)} $\downarrow$ & \textbf{Speed} $\uparrow$ \\
\midrule
MQMQE & \hspace{10pt}32.76 & $\text{44.65}\times$ \\
InstructScore & 1462.64 & $\text{1}\times$ \\
ADSQE & \hspace{5pt}102.36 & $\text{14.29}\times$ \\
\bottomrule
\end{tabular}}
\caption{
Average generation time per sample of different pseudo data methods on a single V100 GPU.
}
\vspace{-10pt}
\label{tab:generation_time}
\end{table}


\section{Related Works}
Early QE approaches predominantly relied on handcrafted features (e.g. alignment-based confidence~\cite{quest}, source complexity~\cite{scarton2015ushef,shah2015investigating}) designed to capture linguistic and statistical indicators.
Recently, \citet{unsupervised} and \citet{ssqe} regard generation probability of neural models as QE features.

CometKiwi~\cite{CometKiwi2023} aims to transfer QE knowledge by leveraging labeled datasets across diverse annotations and languages.
However, this idea still faces the pitfall of distribution shift, as highlighted in \cite{zouhar2024pitfalls}, primarily due to the scarcity of QE data.
To address this limitation, synthetic data generation methods have emerged.
(1) Negative sampling-based approaches like DirectQE~\cite{directqe} and MQMQE~\cite{wmt2023}, utilize translation models to generate synthetic errors through negative sampling, then directly regard error rate as sentence-level scores; (2) BSQE~\cite{nmt_ter} and CBSQE~\cite{cbsqe}, which generate synthetic translations based on search algorithms and derive labels by matching synthetic translations with references.
In this work, we aim to alleviate the distribution shift problem by leveraging supervision signals and
increasing model diversity.


\section{Conclusion}
The distribution shift of synthetic data poses a persistent challenge in the QE field.
To tackle this challenge, we introduce the ADSQE framework, which mitigates distribution shifts by utilizing translation references (a form of supervision signal) and enhancing the diversity among translation models.
Experiments show that ADSQE achieves SOTA results in both supervised and unsupervised settings. 
Furthermore, our analysis underscores some insights for synthetic data generation, which could benefit synthetic data methods for general reward models.


\section*{Limitations}
Our framework is subject to several limitations. 
Firstly, while synthetic data quality correlates with the translation performance of the Annotator, where higher-performance synthesized data is more aligned with human preferences, we did not explore LLMs as Annotators due to computational constraints. 
Secondly, although our method proves effective in high-resource settings, its robustness in extreme data scarcity scenarios (i.e. even parallel datasets are unavailable) needs further validation. 
Thirdly, our insights for synthetic QE data could be applied to general reward models, which need further exploration.


\bibliography{acl2025}

\appendix

\section{Experiment Details}
\label{sec:appendix oid}

\subsection{Data statistics}
\label{sec:data_statics}
\begin{table}[h] 
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{cccc}
\toprule
\textbf{Type} & \textbf{Dataset} & \textbf{Split} & \textbf{Size} \\
\midrule
\multirow{3}{*}{Parallel data} & WMT23 EN-DE & Train & 45M \\
& WMT23 ZH-EN & Train & 30M \\
& WMT23 HE-EN & Train & 35M \\
\midrule
\multirow{7}{*}{MQM data} & WMT23 EN-DE & Train \& Valid & 150K \\
& WMT23 ZH-EN & Train \& Valid & 200K \\
& WMT23 EN-DE & Test & 1887 \\
& WMT23 ZH-EN & Test & 1664 \\
& WMT23 HE-EN & Test & 1134 \\
& WMT22 EN-DE & Test & 511 \\
& WMT22 ZH-EN & Test & 505 \\
\bottomrule
\end{tabular}}
\caption{Statistics of the datasets.}
\label{tab:dataset_info}
\end{table}


\subsection{Generator \& Annotator}
The model parameters are optimized using the Adam optimizer, configured with  $\beta_1 = 0.9$ and $\beta_2 = 0.98$.
The initial learning rate is set to 5e-4, and the inverse square root learning rate scheduler is employed with 6000 warm-up steps.
Model parameters update every 20 batches and the batch is configured to  13,650.
The dropout rate is set to 0.3, and the weight decay is set to 1e-4.
The training objective employs the label-smoothed cross-entropy criterion with $eps=0.1$. 
The training stops early if there are no improvements in validation performance for the last 15 epochs.

We use the TER tool called TERCOM\footnote{\url{http://www.cs.umd.edu/~snover/tercom/}} to annotate the synthetic translations generated by the Generator.


\subsection{InstructScore}
By providing the references only, Instructscore prompts the GPT-4 to generate German and English synthetic translations along with explainable texts. 
We get these data from their public repository\footnote{\url{https://github.com/xu1998hz/InstructScore_SEScore3}}.
To construct the synthetic sources, we translated the German sentences into English and translated the English sentences into Chinese and Hebrew using Google Translate.
Following this, regular expressions are employed to extract MQM labels from the explainable texts.
These labels are then systematically organized alongside the corresponding sources and translations, resulting in a unified MQM dataset structured for analysis.

\subsection{GEMBA-MQM}
We adopt the same configuration as GEMBA-MQM~\cite{gemba-mqm}, but utilize the \textit{gpt-4-turbo-preview} model, and structured QE data is extracted from the regular expression.

\subsection{Preprocess}
We use the sacremoses tool kit\footnote{\url{https://github.com/alvations/sacremoses}} to normalize and tokenize the parallel sentences. For the Chinese sentences, we utilize pkuseg~\cite{pkuseg} for Chinese word segmentation.

\subsection{Pretraining and Finetuning}
For unsupervised experiments, 4 NVIDIA V100 GPUs are used to train the QE models. 
The learning rate is set to 1e-6 for the EN-DE direction and 1e-5 for the ZH-EN \& HE-EN direction.
The model parameters are optimized using the Adam optimizer, configured with  $\beta_1 = 0.9$ and $\beta_2 = 0.999$, and the clip norm is set to 1.0
During training, we set the maximum number of sentences in a batch to 15, and the update frequency is set to 20. 
The word-level ``OK'' label weight is twice the ratio of ``BAD'' to ``OK'' labels in the synthetic data while the ``BAD'' label weight is fixed to 2.0.
The training stops early if there are no improvements in validation performance for the last 15 epochs.

For supervised experiments, one NVIDIA V100 GPU is used to train the models.
The model parameters are optimized using the Adam optimizer, configured with  $\beta_1 = 0.9$ and $\beta_2 = 0.999$, and the clip norm is set to 1.0. 
The learning rate is set to 1e-6 for the EN-DE direction and 7e-6 for ZH-EN direction.
During training, we set the maximum number of sentences in a batch to 15, and the update frequency is set to 20. 
The training stops early if there are no improvements in validation performance for the last 15 epochs.

\subsection{Inference}

For the span-level task, we performed a greedy search to optimize thresholds for categorizing MINOR, MAJOR, and CRITICAL severity for each language direction. This optimization is conducted on the WMT2023 QE validation set for EN-DE and ZH-EN, and on a subset of the test set with 100 entries for HE-EN.
The t
For the sentence-level task, we calculated scores by averaging the regression score and the MQM score derived from the span-level results.

\section{Details of SPCE algorithm}
\label{sec:spce_details}
We utilize the Stanza toolkit~\cite{qi2020stanzapythonnaturallanguage} to perform dependency tree parsing on the synthetic translations. 
The pseudo-code for the implementation of the SPCE algorithm is presented in Algorithm~\ref{alg:qe}.

\begin{algorithm*}[h]
    \caption{Shortest Phrase Covering Errors.}
    \label{alg:qe}
    \textbf{Input}: {The error interval $\{l, \dots, r\}$, the dependency tree $\mathcal{T}$}\\%输入参数
    \textbf{Output}: {The shortest phrase covering errors $\{\hat l, \dots,  \hat r\}$.}%输出
    \begin{algorithmic}[1]
    \STATE $\mathcal{P}_{cur}\longleftarrow\{l, \dots, r\}$,  $\mathcal{P}_{last}\longleftarrow\emptyset$
    \WHILE{$\mathcal{P}_{last}\neq \mathcal{P}_{cur}$}
    \STATE $\mathcal{P}_{last} \longleftarrow \mathcal{P}_{cur}$\; 
    \STATE \# Least common ancestor of selected words in the dependency tree $\mathcal{T}$.
    \STATE $a := \text{LCA}(\mathcal{T},\mathcal{P}_{cur})$\; 
    \STATE \# Make the selected words form a dependency subtree.
    \FORALL{$p\in\mathcal{P}_{cur}$}
        \WHILE{$p$ is not $a$} 
                \STATE $p :=\text{get\_parent}(\mathcal{T}, p)$\;
                \STATE $\mathcal{P}_{cur} \longleftarrow \mathcal{P}_{cur} \cup \{ p \}$\; 
        \ENDWHILE
    \ENDFOR
    \STATE \# Make the selected words that make up the phrase consecutive.
    \STATE $\hat{l}:=\min(\mathcal{P}_{cur})$, $\hat{r}:=\max(\mathcal{P}_{cur})$
    \FORALL{$i \in \{\hat l, \dots,  \hat r\}$} %
        \STATE $\mathcal{P}_{cur} \longleftarrow \mathcal{P}_{cur} \cup \{i\}$\;  
    \ENDFOR\;
    \ENDWHILE
    \RETURN $\{\hat l, \dots,  \hat r\}$
    \end{algorithmic}
\end{algorithm*}




\section{Case Study}
\begin{table}[h]
\centering
\resizebox{\columnwidth}{!}{
\begin{tabular}{l|l|l}
\hline
\textbf{Source} & \multicolumn{2}{l}{Several anecdotes are included .} \\
\textbf{Translation} & \multicolumn{2}{l}{Es werden mehrere Anträge aufgenommen .} \\
\textbf{Reference} & \multicolumn{2}{l}{Viele Anekdoten sind darüber überliefert .} \\
\hline
\hline
\textbf{Annotator $M_\text{amateur}$} & \multicolumn{2}{l}{OK OK OK \textcolor{red}{MAJOR} OK OK}\\
\textbf{Annotator $M'$} & \multicolumn{2}{l}{OK OK OK \textcolor{red}{CRITICAL} \textcolor{red}{CRITICAL} OK}\\
\textbf{Annotator $L'$} & \multicolumn{2}{l}{OK OK OK \textcolor{red}{CRITICAL} \textcolor{red}{MINOR} OK}\\
\hline

\end{tabular}
}
\caption{A Case Study illustrating the distinction between Annotator $M_\text{amateur}$, $M'$ and $L$ with Generator $M$.}
\label{tab:annotator_case_study}
\end{table}

\section{Time Analysis}
\label{sec:time_analysis}

ADSQE can be divided into four stages: (1) Synthesize Translations with the Generator, (2) Synthesize coarse-grained labels with TER tool, (3) Refining labels with the Annotator, (4) Aggregate the results with the SPCE algorithm.
We counted the time spent on each stage in Table~\ref{tab:time_analysis}.

\begin{table}[htbp]
\centering
\resizebox{\columnwidth}{!}{\begin{tabular}{cccc}
\toprule
 \textbf{Slabele} & \textbf{Device} & \textbf{Calculation Time (ms)} & \textbf{Proportion (\%)} \\
\midrule
1 & GPU & 10.37 & 10.13 \\
2 & CPU & \hspace{5pt}0.24 & \hspace{5pt}0.23 \\
3 & GPU & \hspace{5pt}4.11 & \hspace{5pt}4.02 \\
4 & CPU \& GPU & 87.64 & 85.62 \\
\bottomrule
\end{tabular}}
\caption{Time Analysis of ADSQE.}
\label{tab:time_analysis}
\end{table}



\section{Convergence Efficiency.}
Higher-quality pseudo-data can accelerate training convergence. 
To evaluate the convergence efficiency of different methods, we plot the learning curve on the WMT2022 validation set in Figure~\ref{fig:generation_convergence}.
For both sentence-level and word-level tasks, CBSQE demonstrates faster convergence compared to MQMQE, ultimately achieving the highest performance.

\begin{figure}[h] % 使用 [h] 参数让图片尽可能接近插入位置
    \centering % 图片居中
    \includegraphics[width=\columnwidth]{fig/fig_generator_convergence.pdf} % 设置图片宽度为文本宽度的50%
    \caption{Training steps vs. Spearman or MCC score of different methods on the different task in WMT23 EN-DE QE validation set.} % 添加标题
    \label{fig:generation_convergence} % 设置标签，用于引用
\end{figure}


\section{Amount of synthetic Data}

We examine the impact of data size on the performance of ADSQE.
To this end, we conduct a series of experiments utilizing varying amounts of parallel sentence pairs from the WMT23 EN-DE dataset.
As illustrated in Figure~\ref{fig:data_scale}, for all the tasks, pseudo-data methods demonstrate significant performance improvements as the data size increases.
Notably, this upward trend persists and does not exhibit convergence until reaching a scale of 5M parallel pairs.

\begin{figure}[h] % 使用 [h] 参数让图片尽可能接近插入位置
    \centering % 图片居中
    \includegraphics[width=\columnwidth]{fig/fig_data_scale.pdf} % 设置图片宽度为文本宽度的50%
    \caption{Spearson or MCC score on the WMT23 EN-DE QE test set using different amounts of parallel pairs.} % 添加标题
    \label{fig:data_scale} % 设置标签，用于引用
\end{figure}

\end{document}
