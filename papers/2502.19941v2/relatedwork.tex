\section{Related Works}
Early QE approaches predominantly relied on handcrafted features (e.g. alignment-based confidence~\cite{quest}, source complexity~\cite{scarton2015ushef,shah2015investigating}) designed to capture linguistic and statistical indicators.
Recently, \citet{unsupervised} and \citet{ssqe} regard generation probability of neural models as QE features.

CometKiwi~\cite{CometKiwi2023} aims to transfer QE knowledge by leveraging labeled datasets across diverse annotations and languages.
However, this idea still faces the pitfall of distribution shift, as highlighted in \cite{zouhar2024pitfalls}, primarily due to the scarcity of QE data.
To address this limitation, synthetic data generation methods have emerged.
(1) Negative sampling-based approaches like DirectQE~\cite{directqe} and MQMQE~\cite{wmt2023}, utilize translation models to generate synthetic errors through negative sampling, then directly regard error rate as sentence-level scores; (2) BSQE~\cite{nmt_ter} and CBSQE~\cite{cbsqe}, which generate synthetic translations based on search algorithms and derive labels by matching synthetic translations with references.
In this work, we aim to alleviate the distribution shift problem by leveraging supervision signals and
increasing model diversity.