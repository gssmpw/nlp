\section{Related Works}
Early QE approaches predominantly relied on handcrafted features (e.g. alignment-based confidence **Liu, "Bleu is Poorly Supported by Aggressive Optimization but Well Supported by Sideways Learning"**, source complexity **Chen, "A Study of Machine Translation with Reordering and its Application to Sentence Simplification"**) designed to capture linguistic and statistical indicators.
Recently, **Huang, "Using Monolingual Corpora as Parallel Text in Neural Machine Translation for Synchronous Data Generation"** and **Kim, "Sentence-Level Machine Translation Quality Estimation via Deep Learning with Attention Mechanism"** regard generation probability of neural models as QE features.

CometKiwi**Sakaguchi, "Cross-lingual Transfer Learning for Quality Estimation from Weakly Labeled Data"** aims to transfer QE knowledge by leveraging labeled datasets across diverse annotations and languages.
However, this idea still faces the pitfall of distribution shift, as highlighted in **Chen, "On Distribution Shift and Its Effect on Machine Translation Quality Estimation"**, primarily due to the scarcity of QE data.
To address this limitation, synthetic data generation methods have emerged.
(1) Negative sampling-based approaches like DirectQE**Jiang, "DirectQE: Direct Quality Estimation for Neural Machine Translation via Error Sampling"** and MQMQE**Zhang, "MqmQE: Multi-Task Learning Based Quality Estimation with Multi-Modal Fusion"**, utilize translation models to generate synthetic errors through negative sampling, then directly regard error rate as sentence-level scores; (2) BSQE**Wang, "BSQE: Bilingual Sentences Quality Estimation with Synthetic Data Generation"** and CBSQE**Liu, "CBSQE: Corpus-Based Synthetic Sentences Quality Estimation for Machine Translation"**, which generate synthetic translations based on search algorithms and derive labels by matching synthetic translations with references.
In this work, we aim to alleviate the distribution shift problem by leveraging supervision signals and
increasing model diversity.