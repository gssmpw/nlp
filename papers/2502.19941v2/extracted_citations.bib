@inproceedings{cbsqe,
    title = "Improved Pseudo Data for Machine Translation Quality Estimation with Constrained Beam Search",
    author = "Geng, Xiang  and
      Zhang, Yu  and
      Lai, Zhejian  and
      She, Shuaijie  and
      Zou, Wei  and
      Tao, Shimin  and
      Yang, Hao  and
      Chen, Jiajun  and
      Huang, Shujian",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.764",
    doi = "10.18653/v1/2023.emnlp-main.764",
    pages = "12434--12447",
    abstract = "Machine translation (MT) quality estimation (QE) is a crucial task to estimate the quality of MT outputs when reference translations are unavailable. Many studies focus on generating pseudo data using large parallel corpus and achieve remarkable success in the supervised setting. However, pseudo data solutions are less satisfying in unsupervised scenarios because the pseudo labels are inaccurate or the pseudo translations differ from the real ones. To address these problems, we propose to generate pseudo data using the MT model with constrained beam search (CBSQE). CBSQE preserves the reference parts with high MT probabilities as correct translations, while the rest parts as the wrong ones for MT generation. Therefore, CBSQE can reduce the false negative labels caused by synonyms. Overall, beam search will prefer a more real hypothesis with a higher MT generation likelihood. Extensive experiments demonstrate that CBSQE outperforms strong baselines in both supervised and unsupervised settings. Analyses further show the superiority of CBSQE. The code is available at https://github.com/NJUNLP/njuqe.",
}

@inproceedings{directqe,
  title={Directqe: Direct pretraining for machine translation quality estimation},
  author={Cui, Qu and Huang, Shujian and Li, Jiahuan and Geng, Xiang and Zheng, Zaixiang and Huang, Guoping and Chen, Jiajun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12719--12727},
  year={2021}
}

@inproceedings{nmt_ter,
    title = "Quality Estimation without Human-labeled Data",
    author = "Tuan, Yi-Lin  and
      El-Kishky, Ahmed  and
      Renduchintala, Adithya  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Specia, Lucia",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.50",
    doi = "10.18653/v1/2021.eacl-main.50",
    pages = "619--625",
    abstract = "Quality estimation aims to measure the quality of translated content without access to a reference translation. This is crucial for machine translation systems in real-world scenarios where high-quality translation is needed. While many approaches exist for quality estimation, they are based on supervised machine learning requiring costly human labelled data. As an alternative, we propose a technique that does not rely on examples from human-annotators and instead uses synthetic training data. We train off-the-shelf architectures for supervised quality estimation on our synthetic data and show that the resulting models achieve comparable performance to models trained on human-annotated data, both for sentence and word-level prediction.",
}

@inproceedings{quest,
  title={QuEst-A translation quality estimation framework},
  author={Specia, Lucia and Shah, Kashif and De Souza, Jos{\'e} GC and Cohn, Trevor},
  booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={79--84},
  year={2013}
}

@inproceedings{scarton2015ushef,
  title={USHEF and USAAR-USHEF participation in the WMT15 QE shared task},
  author={Scarton, Carolina and Tan, Liling and Specia, Lucia},
  booktitle={Proceedings of the Tenth Workshop on Statistical Machine Translation},
  pages={336--341},
  year={2015}
}

@inproceedings{shah2015investigating,
  title={Investigating continuous space language models for machine translation quality estimation},
  author={Shah, Kashif and Ng, Raymond WM and Bougares, Fethi and Specia, Lucia},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={1073--1078},
  year={2015}
}

@inproceedings{ssqe,
    title = "Self-Supervised Quality Estimation for Machine Translation",
    author = "Zheng, Yuanhang  and
      Tan, Zhixing  and
      Zhang, Meng  and
      Maimaiti, Mieradilijiang  and
      Luan, Huanbo  and
      Sun, Maosong  and
      Liu, Qun  and
      Liu, Yang",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.267",
    doi = "10.18653/v1/2021.emnlp-main.267",
    pages = "3322--3334",
    abstract = "Quality estimation (QE) of machine translation (MT) aims to evaluate the quality of machine-translated sentences without references and is important in practical applications of MT. Training QE models require massive parallel data with hand-crafted quality annotations, which are time-consuming and labor-intensive to obtain. To address the issue of the absence of annotated training data, previous studies attempt to develop unsupervised QE methods. However, very few of them can be applied to both sentence- and word-level QE tasks, and they may suffer from noises in the synthetic data. To reduce the negative impact of noises, we propose a self-supervised method for both sentence- and word-level QE, which performs quality estimation by recovering the masked target words. Experimental results show that our method outperforms previous unsupervised methods on several QE tasks in different language pairs and domains.",
}

@article{unsupervised,
    title = "Unsupervised Quality Estimation for Neural Machine Translation",
    author = "Fomicheva, Marina  and
      Sun, Shuo  and
      Yankovskaya, Lisa  and
      Blain, Fr{\'e}d{\'e}ric  and
      Guzm{\'a}n, Francisco  and
      Fishel, Mark  and
      Aletras, Nikolaos  and
      Chaudhary, Vishrav  and
      Specia, Lucia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2020.tacl-1.35",
    doi = "10.1162/tacl_a_00330",
    pages = "539--555",
}

@inproceedings{wmt2023,
  title={Unify word-level and span-level tasks: NJUNLPâ€™s Participation for the WMT2023 Quality Estimation Shared Task},
  author={Geng, Xiang and Lai, Zhejian and Zhang, Yu and Tao, Shimin and Yang, Hao and Chen, Jiajun and Huang, Shujian},
  booktitle={Proceedings of the Eighth Conference on Machine Translation},
  pages={829--834},
  year={2023}
}

