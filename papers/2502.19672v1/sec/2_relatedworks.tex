\section{Related Work}
In this section, we will provide a brief overview of the transferability of adversarial examples, Multimodal Large Language Models (MLLMs) and the existing adversarial attack on MLLMs.

\textbf{Transferable Adversarial Attacks.} There are mainly two categories to improve the transferability of adversarial examples, input transformation based method and optimization based method. Input transformation based method transforms the input to get more diverse inputs. DI~\citep{xie2019improving} adds padding to a randomly resized image for a fixed size. TI~\citep{dong2019evading} adds a set of translations to the input and averages their gradient, which is further approximated by convoluting the gradient of the original input with a Gaussian kernel. SI~\citep{Lin2020SIM} scales the images with different scale factors and averages their gradients.  Spectrum simulation attack (SSA)~\citep{long2022frequency} transforms the input in the frequency domain, which could be considered as a model augmentation. SIT~\citep{wang2023structure} applies several transformations on blocks of input to craft more diverse inputs. Optimization based methods craft transferable adversarial examples by improving the optimization. MI~\citep{dong2018boosting}, NI~\citep{Lin2020SIM} introduce momentum and Nesterov accelerated gradient to the optimization progress. VMI~\citep{wang2021enhancing} attempts to reduce the variance of the gradient. Unlike attacks on the uni-modality vision model, this work delves into transfer attacks on MLLMs, which align two or more modalities, making traditional transfer attacks ineffective.

\textbf{Multimodal Large Language Models.} Benefiting from the success of LLMs, such as GPTs~\citep{brown2020language}, PaLM~\citep{anil2023palm}, LLaMA~\citep{touvron2023llama, touvron2023llama2, dubey2024llama}, recent MLLMs achieved an enhanced zero-shot performance in various complex tasks. These MLLMs built upon the achievement of LLMs train a modality connecter to align the vision space and text space. Concretely, BLIP~\citep{li2022blip} introduces a unified vision-language pre-training framework. BLIP2~\citep{li2023blip2} extends BLIP by connecting the vision encoder with a frozen OPT~\citep{zhang2022opt} or FlanT5~\citep{chung2024scaling}, aligning vision and language modality with a Query-Transformer. MiniGPT4~\citep{zhu2023minigpt} uses the same architecture with an additional linear projection matrix, further improving the performance with more powerful LLM Vicuna~\citep{vicuna2023} and high-quality data. LLaVA~\citep{liu2024visual} applies visual instruction tuning and aligns a vision encoder with LLaMA~\citep{touvron2023llama, touvron2023llama2, dubey2024llama} using a linear projection matrix. InstructBLIP~\citep{dai2024instructblip} proposes an instruction-aware Query-Transformer to extract visual features more related to the text. However, in this work, we demonstrate that even state-of-the-art MLLMs can fail when presented with inputs specifically crafted by humans.

\textbf{Adversarial Attacks on Multimodal Large Language Models.} Several recent researches have explored the robustness of MLLMs. These researches are mostly under untargeted settings, or try to mislead the content of the input image.~\citet{zhao2024evaluating} explore the robustness of VLMs under black-box setting by using transfer-based and query-based methods to craft adversarial examples.~\citet{qi2024visual} craft visual adversarial examples to jailbreak VLMs.~\citet{dong2023robust} use an ensemble-based method to mislead Google Bard.~\citet{tu2023many} build a benchmark for the safety issue of the VLMs.~\citet{wang2024stop} explore the influence of visual adversarial examples for VLMs with chain-of-thought reasoning.~\citet{gao2024inducing} craft visual adversarial examples to cause the VLMs to generate long content, leading to high energy latency.~\citet{wang2023instructta} propose an instruct-tuned method for targeted attack on VLMs.~\citet{luo2024image} explore the transferability of targeted adversarial examples across different prompts, and point out the low transferability of adversarial examples across models. Instead of cross-prompt transferability, this work explores the transferability across models. We also consider vision-language modality alignment to deploy an end-to-end attack, rather than targeting only the vision encoder.
\vspace{-2mm}