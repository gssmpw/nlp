\section{Experiments}

\subsection{Experimental Settings}

\textbf{Datasets.} We follow the previous work~\citep{luo2024image} to prepare the data. The images are collected from the validation set of MS-COCO~\citep{lin2014microsoft} dataset, and 1000 samples are randomly selected to run our attack. The image-specific VQA prompts are taken from the VQA-v2~\citep{goyal2016making}, and the classification, captioning, and image-agnostic VQA prompts are collected from the previous work~\citep{luo2024image} and we randomly select one prompt from each task for each image. All prompts used in the experiment are listed in the supplementary material.

\textbf{Models.} 
Four types of open-sourced models are employed as both surrogate and target models, including BLIP2~\citep{li2023blip2}, InstructBLIP~\citep{dai2024instructblip}, MiniGPT4~\citep{zhu2023minigpt}, and LLaVA~\citep{liu2024visual}.
Among them, BLIP2, InstructBLIP and MiniGPT4 use EVA-CLIP-ViT-G~\citep{sun2023evaclip} as vision encoder and LLaVA uses OpenA
I-CLIP-ViT-L~\citep{radford2021learning}.
For each of them, we select several versions based on different language models.
Specifically, for BLIP2, we use four versions built on the language models: OPT-2.6B, OPT-6.7B, FlanT5-xl and FlanT5-xxl(short as B-O2.7B, B-O6.7B, B-T5xl, B-T5xxl).
For InstructBLIP, we choose versions based on FlanT5-xl, FlanT5-xxl, Vicuna-7b and Vicuna-13B(short as IB-T5xl, IB-T5xxl, IB-V7B, IB-V13B).
Both LLaVA and MiniGPT4 are available in versions built on Vicuna-7B and Vicuna-13B versions.
Note that Vicuna-based models, such as InstructBLIP, LLaVA, and MiniGPT4, each use different versions of Vicuna, resulting in differences in their weights. The detail of all models we used in our experiments can be found in supplementary material. 

\textbf{Metric.} We employ the Attack Success Rate (ASR) as the metric for evaluating the adversarial robustness and transferability. An attack is successful only if the output of the model matches the target text exactly. For MiniGPT4, we consider the attack successful if the first sentence of output matches the target because the MiniGPT4 model always generates long content.
We evaluate the ASR of the adversarial example using the same prompt used to generate it.


\textbf{Baselines.} We compare our DynVLA Attack with PGD~\citep{madry2017towards} and three competitive attacks, namely DI~\citep{xie2019improving}, TI~\citep{dong2019evading}, SIT~\citep{wang2023structure}. 

\textbf{Implementation details.} All our experiments are under perturbation budget $ \epsilon = 16/255$, step size $ \alpha = 1/255$ and iteration steps $T = 2000$. In our DynVLA, both the size and strength of the Gaussian kernel are set randomly from 3 to 5. For most of our experiments, we use ``unknown" as our target output, following the setting of~\citep{luo2024image}. Additionally, we provide a detailed analysis of the results obtained using different target outputs in Section \ref{subsec:target}.

\subsection{Experimental Results}
To demonstrate the effectiveness of DynVLA, adversarial examples are crafted using all aforementioned models with classification prompts as text input, such as ``Identify the primary theme of this image in one word.", and evaluate their ASR when transferred to other models.
We select ``unknown" as the target output because it's not a typical output of MLLMs.
And all reported ASRs are averaged over 3 runs.
Table \ref{table:1} presents the results of our method compared to the baseline across all target models. 
The results indicate that our proposed DynVLA can significantly enhance the attack success rate for most of the models.
Specially, the highest ASR can be more than $70\%$ on BLIP2 models, while the ASR of the baseline method is around $10\%$.
The $70\%$ ASR is even close to the ASR directly attacking the target model under white-box setting.

% Another observation from the experimental results is the choice of surrogate models matters for transfer attacks. Even when the weights in LMMs are completely different, InstructBLIP-Vicuna13B is more vulnerable to adversarial examples generated by InstrcutBLIP-Vicuna7B, achieving an ASR of 56\%, which is significantly higher than other surrogate models. This trend is consistent across other models, indicating that higher ASR can consistently be achieved when the surrogate model shares a similar LLMs architecture and vision-language connector architecture with the target model. Additionally, as shown in the table, when transferring to BLIP2-OPT2.7B, InstructBLIP-Flant5xl achieves an ASR of 48.2\%, while InstructBLIP-Flant5xxl reaches 71.2\%. This suggests that larger LLM sizes in surrogate models generally result in improved ASR, excluding Vicuna-based model. For every pair of MLLMs using the same LLMs architecture, the MLLM with a larger LLM can always generate adversarial examples with higher ASR. If this trend holds true for most of the MLLMs, it may become easier to craft transferable adversarial examples using larger surrogate models. Due to the resource limitations, we will explore this further in future work.
\input{table/table1}

% \subsection{Comparison with Existing Transfer Attacks}
% \begin{figure}[h!]
%   \centering
%   \includegraphics[width=0.45\textwidth]{figure/compare_results_vert.pdf}
%   \vspace{-1mm}
%   \caption{\textbf{DynVLA can outperform all other existing transfer attack methods.}
%   The left figure uses InstructBLIP FlanT5xl version as the surrogate model, and the right figure uses InstructBLIP Vicuna7B version as the surrogate model.
%   The results show the ASR ($\%$) on the other seven target models.
%   Some existing input-transform based trasfer attacks can also improve the ASR,
%   however, these pixel-level augmentations are limited, while our method can augment the alignment of the vision-language modality.}
%   \label{fig:compare}
%   \vspace{-5mm}
% \end{figure}

\subsection{Comparison with Existing Transfer Attacks}
\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{figure/compare_results.pdf}
  \vspace{-1mm}
  \caption{\textbf{DynVLA can outperform all other existing transfer attack methods.}
  The left figure uses InstructBLIP FlanT5xl version as the surrogate model, and the right figure uses InstructBLIP Vicuna7B version as the surrogate model.
  The results show the ASR ($\%$) on the other seven target models.
  Some existing input-transform based trasfer attacks can also improve the ASR,
  however, these pixel-level augmentations are limited, while our method can augment the alignment of the vision-language modality.}
  \label{fig:compare}
  \vspace{-3mm}
\end{figure*}
There are few transfer attack methods in MLLMs scenario, we compare our DynVLA with other existing traditional transfer attack method and their combinations.
Specially, we compare our method with MI~\citep{dong2018boosting}, DI~\citep{xie2019improving}, TI~\citep{dong2019evading}, SIT~\citep{wang2023structure}.
We observe that optimization-based methods such as MI~\citep{dong2018boosting}, NI~\citep{Lin2020SIM} do not improve the transferability in the MLLMs scenario, but some data augmentation based methods can have improvement, like DI, SIT. These data augmentation based methods augment data at the pixel level, while our method augments the data at the vision-language modality alignment level, which can be more effective in the MLLMs scenario.
As illustrated in Figure \ref{fig:compare}, our method outperforms all other transfer attack methods across all target models. 

\subsection{DynVLA on Different Tasks}
\input{table/table2}
\input{table/table3}
In this section, we show that DynVLA is not limited to a specific type of prompt, but can be effective across various prompts.
Table \ref{table:2} and Table \ref{table:3} show the ASR on captioning prompts and image-specific VQA prompts, respectively.
Although the ASR for captioning prompts and VQA prompts is lower than the classification prompts, DynVLA can still significantly improve the ASR compared to the baseline.
We argue that the prompt is also an important factor that can influence the transferability of adversarial examples. classification prompts and captioning prompts will focus more on the high-level semantic information of an image while some VQA prompts focus on local information.
DynVLA forces the MLLMs to focus on different parts of the image when crafting the adversarial example, thus misleading both the global and local information of an image.
% \vspace{-2mm}
\subsection{DynVLA with Different Targets}
\label{subsec:target}
\vspace{-2mm}
In practice, an adversary may seek to force the MLLMs to generate various specific outputs, it could be a word, a sentence or even a harmful output.
We investigate the effectiveness of DynVLA on different target outputs, and demonstrate its high generalizability to various outputs.
In our experiments, we select two sentences ``I am sorry" and ``I don't know", and a common object ``cat" as the target output and craft the adversarial examples using InstructBLIP-Vicuna7B.
Figure \ref{fig:target} shows the results of the ASR on seven target models.
It can be observed that the ASR of the sentences is lower than the ``unknown" target, but our method can still significantly improve the ASR.
We can also observe that the ASR of the target text ``cat" is significantly higher than ``unknown" or sentences like ``I am sorry", because cat is a common object in the image, while MLLMs may not generate ``unknown" in the normal situation.
The ASR of the target text ``cat" can be almost $98\%$ in some cases. Among all four target texts, our method consistently outperforms the baseline method.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/target_results.pdf}
  \vspace{-3mm}
  \caption{\textbf{Our method DynVLA is effective on different target outputs.}
            In addition to the word ``unknown", DynVLA can also significantly improve the ASR with target sentences such as ``I don't know" and ``I am sorry",
            as well as a common object ``cat".
            Specifically, for target output ``cat", our method achieves more than 80\% ASR across all target models.}
  \label{fig:target}
  \vspace{-3mm}
\end{figure*}


\subsection{DynVLA on other Multimodal Large Language Models}
Since BLIP2, InstructBLIP and MiniGPT4 use similar architecture Q-Former to extract text-related information, we evaluate our method on other MLLMs that vary in vision encoders and vision-language connectors, as well as other closed-source models.

\textbf{LLaVA.}
LLaVA uses a linear projection to align the vision and language modality, which is different from the Q-Former used in BLIP2, InstructBLIP and MiniGPT4.
Given that the LLaVA model typically uses an input resolution of $336 \times 336$, compared to the $224 \times 224$ resolution used by other models, adversarial examples generated from different resolutions are challenging to transfer. Therefore, we conduct experiments with LLaVA models as both the surrogate and target models.
In the experiment, the adversarial examples are craft using LLaVA-v1.5-Vicuna7B, and evaluate them on LLaVA-v1.5-Vicuna13B, LLaVA-v1.6-Vicuna13B, LLaVA-v1.6-Mistral7B, as well as LLaVA-LLaMA3.
The results in Table \ref{table:4} indicate that our method can also attack successfully on LLaVA based on LLaMA3.
\input{table/table4}

\textbf{Other state-of-the-art models.}
Gemini is a popular closed-source model that accepts image and text as input.
We evaluate the adversarial examples generated by InstructBLIP models on Gemini, as well as InternVL, Qwen-VL and Llama-3.2-Vision, three state-of-the-art open-source MLLMs.
We found that it is hard to generate exactly the same output on these models, but some samples can generate text containing the target output. To the best of our knowledge, these samples on closed-source models like Gemini have never been reported by other works. 
% To better show the effectiveness of our methods, in Table \ref{table:5}, we report the CLIPScore of the output text and the target text, 
Some successful adversarial examples on Gemini are shown in Figure \ref{fig:gemini}. More adversarial examples of these models can be found in supplementary material.
% \input{table/table5}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figure/gemini.pdf}
  \caption{Successful adversarial examples on Gemini.
            }
  \label{fig:gemini}
% \vspace{-3mm}
\end{figure}

\subsection{Ablation Study}
To systematically investigate the impact of DynVLA, we ablate the size and strength of the Gaussian kernel added to the attention map, as well as the perturbation bounds. All these experiments use InstrcutBLIP-Vicuna7B as surrogate model and evaluate on all other seven models.

% \vspace{-1mm}
\textbf{Noise Size and Noise Strength}
We conduct experiments to show the impact of the noise size and noise strength on the transferability of adversarial examples.
Figure \ref{fig:noise_size} and Figure \ref{fig:noise_str} show the ASR of adversarial examples crafted with different noise sizes and noise strengths.
The result indicates that the strength of the noise doesn't have a significant impact on the transferability of adversarial examples.
And the best size of the Gaussian kernel is $5 \times 5$, while $3 \times 3$ and $4 \times 4$ have similar performance.
So in our main experiments, we randomly select strength from $3$ to $5$ and size from $3 \times 3$ to $5 \times 5$.

\begin{figure}[!h]
\centering
  \begin{subfigure}{.15\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/ablation_size.pdf}
    \caption{Noise Size}
    \label{fig:noise_size}
  \end{subfigure}
  \begin{subfigure}{.15\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/ablation_strength.pdf}
    \caption{Noise Strength}
    \label{fig:noise_str}
  \end{subfigure}
  \begin{subfigure}{0.15\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figure/ablation_eps.pdf}
    \caption{Perturbation bound}
    \label{fig:eps}
  \end{subfigure}
  \caption{Ablation study of noise size, noise strength and perturbation bound. The left two sub-figures show the ASR ($\%$) under different noise sizes and strengths, and the right sub-figure shows the ASR ($\%$) of our methods and baseline under various perturbation bounds.}
  \label{fig:ablation_1}
  \vspace{-3mm}
\end{figure}

% \begin{figure*}[!h]
% \centering
%   \begin{subfigure}{.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figure/ablation_size.pdf}
%     \caption{Noise Size}
%     \label{fig:noise_size}
%   \end{subfigure}
%   \begin{subfigure}{.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figure/ablation_strength.pdf}
%     \caption{Noise Strength}
%     \label{fig:noise_str}
%   \end{subfigure}
%   \begin{subfigure}{0.3\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{figure/ablation_eps.pdf}
%     \caption{Perturbation bound}
%     \label{fig:eps}
%   \end{subfigure}
%   \caption{Ablation study of noise size, noise strength and perturbation bound. The left two sub-figures show the ASR ($\%$) under different noise sizes and strengths, and the right sub-figure shows the ASR ($\%$) of our methods and baseline under various perturbation bounds.}
%   \label{fig:ablation_1}
%   \vspace{-3mm}
% \end{figure*}

% \vspace{-2mm}
\textbf{Perturbation Bound}
Figure \ref{fig:eps} shows the impact of perturbation bound on the transferability of adversarial examples.
The baseline method's transferability won't increase when the perturbation bound is larger than $8/255$, 
which may be due to the adversarial examples overfitting to the surrogate model.
With our DynVLA Attack, the transferability keeps increasing when the perturbation bound is larger.

\textbf{Attack Steps}
Figure \ref{fig:ablation_steps} shows the ASR over attack steps from 200 to 2000 every 200 iterations. Our DynVLA has a significant improvement compared to the baseline method when attack step $T$ is large.
% \vspace{-3mm}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figure/ablation_steps.pdf}
    \caption{ASR over attack steps on average of other 7 models with InstructBLIP-Vicuna7B as the surrogate model.}
    \label{fig:ablation_steps}
    \vspace{-3mm}
\end{figure}