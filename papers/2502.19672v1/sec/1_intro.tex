\section{Introduction}
\label{sec:intro}

Multimodal Large Language Models (MLLMs)~\citep{liu2024visual, li2023blip2, dai2024instructblip, zhu2023minigpt, chen2024far, Qwen-VL, team2023gemini} built upon Large Language Models (LLMs)~\citep{brown2020language, anil2023palm, touvron2023llama, touvron2023llama2, dubey2024llama, vicuna2023} have achieved great success in addressing intricate vision-language tasks, such as image captioning~\citep{lin2014microsoft} and visual question answering~\citep{goyal2016making}. By aligning visual and language modalities, these models excel in generating coherent language responses to visual input, demonstrating exceptional capabilities in both visual comprehension and language generation. 

Despite the remarkable advancements in Multimodal Large Language Models (MLLMs), they remain susceptible to adversarial attacks~\citep{szegedy2013intriguing, goodfellow2014explaining, madry2017towards, xie2019improving, long2022frequency, wang2023structure}, where carefully designed inputs can deceive the models into producing incorrect or misleading outputs. In addition, recent works~\citep{zhao2024evaluating, dong2023robust, cheng2024typography, schaeffer2024universal}  have shown that MLLMs can be misled by transferable adversarial examples~\citep{gu2023survey, tramer2017space, papernot2016transferability, liu2016delving}, where adversarial examples that are generated to fool one MLLM can also successfully deceive others. For example, ~\citet{zhao2024evaluating} matches the visual representation of the adversarial input with the representation of the target image generated by the target text. ~\citet{dong2023robust} utilize ensemble of a set of vision encoders when attack. ~\citet{cheng2024typography} improve the transferability by typography-based input transformation.
One of the critical challenges in this space is the limited transferability of these adversarial examples across different MLLMs, especially under targeted attack scenarios. 
% ~\citet{luo2024image} have explored the transferability across prompts of adversarial examples to generate targeted adversarial attacks but the transfereability is still limited. 
We hypothesize that most prior work in transfer-based attacks has primarily focused on the visual components of MLLMs, such as visual representation matching~\citep{zhao2024evaluating} and pixel-level augmentations~\citep{cheng2024typography}, without considering the diversity in vision-language modality alignment in MLLMs caused by the different base language models.





% Improving the transferability of adversarial examples is essential not only for identifying vulnerabilities in MLLMs but also for developing more robust and secure AI systems in practical applications.

 % Some works also explore making MLLMs generate specific target output, which means that the adversary can fully control the generated content by only modifying the input image. ~\citet{luo2024image} have explored the transferability across prompts of adversarial examples under this setting, but crafting high-transferability adversarial examples across models is still a challenging and open question. 

% Recent research~\citep{zhao2024evaluating} has demonstrated that adversarial attacks can mislead MLLMs into generating content unrelated to the image, underscoring the potential for transferable attacks. Furthermore, some studies have explored controlling the generated output, enabling adversaries to fully dictate the content by modifying the input image~\citep{luo2024image}. While work has been done to improve the transferability of adversarial examples across prompts, crafting adversarial examples that consistently transfer across different models remains an open and challenging problem. We posit that most existing transfer-based attack methods have focused predominantly on the visual components of MLLMs, such as visual representation matching~\citep{zhao2024evaluating} or pixel-level augmentation~\citep{cheng2024typography}, neglecting the critical role of vision-language modality alignment, which varies significantly depending on the underlying language model.

% However, while these MLLMs are known to be vulnerable to adversarial attacks~\citep{szegedy2013intriguing, goodfellow2014explaining}, the transferability~\citep{gu2023survey, tramer2017space, papernot2016transferability, liu2016delving} of these attacks across various MLLMs remains limited, particularly in targeted attack scenarios.

% Recent work~\citep{zhao2024evaluating} shows that MLLMs can be misled by transferable adversarial examples to generate content unrelated to the image. Some works also explore making MLLMs generate specific target output, which means that the adversary can fully control the generated content by only modifying the input image. ~\citet{luo2024image} have explored the transferability across prompts of adversarial examples under this setting, but crafting high-transferability adversarial examples across models is still a challenging and open question. We argue that most prior transfer-based attack works mainly focus on the visual components of the MLLMs, like visual representation matching~\citep{zhao2024evaluating} and pixel-level augmentation~\citep{cheng2024typography} while ignoring the fact diversity of vision-language modality alignment led by different base language model.
To this end, we propose \textbf{Dyn}amic \textbf{V}ision-\textbf{L}anguage \textbf{A}lignment (DynVLA) attack to dynamically perturb vision-language modality alignment in MLLMs.
In MLLMs, the alignment between vision and language modalities is achieved through vision-language connectors that map visual representations to textual space. The various LLM backbone have different vision-language alignments, leading to diverse interactions between visual and textual information. Unlike existing methods that use an end-to-end optimization approach based on a single vision-language alignment, DynVLA dynamically perturbs the attention mechanisms responsible for vision-language interaction within the vision-language connector, thereby incorporating diverse vision-language modality alignments. Specifically, DynVLA introduces a Gaussian kernel to the attention map within the vision-language connector, shifting the modelâ€™s attention to different regions of the image and thus achieving diverse vision-language alignment, as shown in Figure~\ref{fig:pipeline}.
The success of our method indicates that the variance in vision-language alignment among different MLLMs also diminishes the transferability of adversarial examples across MLLMs. 


In our experiments, we show DynVLA can improve the transferability of adversarial examples on four existing open-source MLLMs, including BLIP2~\citep{li2023blip2}, InstructBLIP~\citep{dai2024instructblip}, MiniGPT4~\citep{zhu2023minigpt} and LLaVA~\citep{liu2024visual}. And we also demonstrate that our method can significantly outperform other traditional attack methods, such as DIM~\citep{xie2019improving} and SIA~\citep{wang2023structure}. Our contribution can be summarized as follows:
\begin{itemize}
\item We introduce Dynamic Vision-Language Alignment (DynVLA) Attack, which incorporate diverse vision-language alignment by perturbing the attention component with Gaussian kernel in the vision-language connector.
\item 
Extensive experiments demonstrate the higher transferability of our method over baselines across four Multimodal Large Language Models and three tasks, posing significant risks to state-of-the-art MLLMs, as DynVLA requires no or little prior knowledge of the model, potentially leading to real-world security threats.
\item Detailed analysis of our experimental results indicate that both the architecture of vision-language connector and the LLMs, as well as the size of LLM, play crucial roles in selecting an effective surrogate model for adversarial attacks. In addition, similar architecture and larger LLMs sizes lead to better transferability.
\end{itemize}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{figure/pipeline.pdf}
  \vspace{-1mm}
  \caption{Overview of the framework of our proposed DynVLA attack.
DynVLA modifies the attention mechanism in the vision-language connector during the forward pass, forcing the model to focus on different parts of the image. Specifically, DynVLA adds a Gaussian kernel to the attention map to create a smooth attention shift. With the perturbed attention map, the generated adversarial attacks dynamically cover diverse vision-language modality alignments, significantly enhancing the transferability of DynVLA in attacking MLLMs.}
  \label{fig:pipeline}
  \vspace{-3mm}
\end{figure*}