\section{Methodology}

\subsection{Threat Model}
Our work focuses on targeted attack on Multimodal Large Language Models.
Let $f_v$ represent the vision encoder, $f_l$ the language model, $f_c$ the vision-language connector, and $(i, t)$ the input image-text pair, with $T$ as the target output.
An MLLM usually uses an existing vision encoder $f_v$, and trains a vision-language connector $f_c$ to align the vision and language modality.

The adversary aims to craft an adversarial example $i + \delta$ that can mislead the model to generate the targeted output $T$, where $\delta$ is the $l_p$-bounded perturbation.
The objective of targeted attack is to find an optimal $\delta$ that minimizes the language loss $\mathcal{L}$, which can be formulated as:
\begin{equation}
 \min_{\delta} \mathcal{L}(f_l(f_c(f_v(\boldsymbol{i}+\boldsymbol{\delta})), \boldsymbol{t}), \boldsymbol{T})
\end{equation}
The PGD attack~\citep{madry2017towards} is a widely used iterable optimization-based method to solve this problem, each iteration of PGD can be formulated as:
\begin{equation}
  \boldsymbol{\delta}\leftarrow\operatorname{clip}_\epsilon(\boldsymbol{\delta}+\alpha\cdot\operatorname{sign}(\nabla_\delta\mathcal{L}(f_l(f_c(f_v(\boldsymbol{i}+\boldsymbol{\delta})), \boldsymbol{t}),\boldsymbol{T})))
  \label{equ:pgd}
\end{equation}
Where $\epsilon$ is the perturbation budget.
For MLLMs, the function of $f_l(f_c(f_v(\boldsymbol{i}+\boldsymbol{\delta})), \boldsymbol{t})$ is very complex.
Thus our method tries to only augment the vision-language alignment in the vision-language connector $f_c$ to improve the transferability of adversarial examples.

\subsection{Vision-Language Modality Alignment in MLLMs}
The vision-language connector, denoted as $f_c$ , plays a crucial role in MLLMs by mapping visual representations, extracted from vision encoders, to textual space. Typically, an MLLM' architecture resembles the structure shown at the top of Figure \ref{fig:pipeline}, it accepts an image input, uses an existing vision encoder to get visual representation, and then the vision-language connector maps the visual representation to text tokens, which are then concatenated with text input, and subsequently fed into the LLM. During training, the parameters of the vision-language connector are updated to align the visual representations with the textual space. This alignment varies based on the specific LLM backbone used, resulting in different vision-language mappings. Broadly, there are two types of architectures used to align the vision and language modalities: cross-attention architectures, such as Qformer \citep{li2023blip2} and Resampler \citep{alayrac2022flamingo}, and MLP projection architectures \citep{liu2024visual}. In cross-attention architectures, cross-attention layers extract visual information from the visual representations to special query tokens, then these tokens are concatenated with text input and aligned to the textual space. In MLP projection architectures, MLP directly projects visual representations into the textual space, which are then concatenated with the text input and fed into the LLM. Here, the MLP and shadow layers of the LLM act as the vision-language alignment component like Q-former, facilitating interaction with textual tokens within the LLMâ€™s self-attention layers.
\vspace{-1mm}

\subsection{Dynamic Vision-Language Alignment Attack}
The varying alignments between vision and language modalities result in different interactions between visual and textual tokens. Baseline attacks~\citep{madry2017towards, xie2019improving, dong2018boosting, wang2023structure} use an end-to-end optimization approach on a specific MLLM, which limits the adversarial examples to a single type of vision-language alignment and results in low transferability. To address this limitation, we propose \textbf{Dyn}amic \textbf{V}ision-\textbf{L}anguage \textbf{A}ttack (DynVLA)  to dynamically perturb the interactions between visual and textual information, thereby incorporating diverse vision-language modality alignments.
Specifically, DynVLA focuses on dynamically perturbing the attention mechanism applied to visual tokens, changing how textual tokens extract visual information without directly modifying the visual content itself. Instead of applying random noise across the entire attention map, we force the model to focus on a specific region of the image, which adjusts the alignment of the vision-language modality without changing the visual information.

To avoid fragmented or inconsistent changes when directly modifying attention on individual visual tokens, we introduce smooth perturbations. 
Specially, we employ a Gaussian kernel to introduce smoother transitions and shift the model's attention to a new region of the image.
Basically, We follow PGD~\citep{madry2017towards} attack to generate the adversarial perturbation.
During each forward pass of attack iteration, we randomly select a visual token from $ n \times n$ visual tokens as the center of the Gaussian kernel, then add a 2D Gaussian kernel to that region. The 2D Gaussian kernel is defined as:
\begin{equation}
    \mathcal{N}(x, y; \mu_1, \mu_2, \sigma) = \frac{1}{2\pi\sigma^2}e^{-\frac{(x-\mu_1)^2+(y-\mu_2)^2}{2\sigma^2}}
\end{equation}
Here $(\mu_1, \mu_2)$ denotes the center of the kernel, and $(x, y)$ represents the position of the visual token in the $ n \times n$ attention map. We also clip the kernel to a size of $m \times m$ around the center, with $m$ set to $3$ or $5$ in our experiments.
After adding the Gaussian kernel, we will normalize the attention map to make sure the sum of the attention weights remains $1$. 
We then adopt a standard PGD attack step by computing the language modeling loss $\mathcal{L}$ and updating the adversarial perturbation according to Equation \ref{equ:pgd}.

For MLLMs with a cross-attention mechanism in their vision-language connector, we perturb the cross-attention map. For MLLMs with only an MLP in their vision-language connector, we perturb the self-attention map within the language model. 
% Algorithm \Ref{alg:1} details the process of our method.
Algorithm \Ref{alg:1} shows the detailed process of our method.


% For MLLMs with a cross-attention component in its vision-language connector, we add noise to the attention map of the cross-attention in the vision-language connector.
% For MLLMs with only an MLP in their vision-language connector, we add noise to the self-attention map of the language model.

\begin{algorithm}[t]
\caption{Dynamic Vision-Language Alignment Attack}
\label{alg:1}
\KwIn{Image $i$, Target $t$, Vision Encoder $f_v$, Language Model $f_l$, Vision-Language Connector $f_c$, Targeted Output $T$, Perturbation Budget $\epsilon$, Step Size $\alpha$, Iteration Steps $S$, Kernel Size $m$, kernel variance $\sigma$}
\KwOut{Adversarial Example $\delta$}
\BlankLine
Initialize $\delta$ as Uniform($-\epsilon$, $\epsilon$)\;
\For{each iteration $s=1$ to $S$}{
    $Z_v = f_v(i+\delta)$\;
    randomly select a token $[\mu_1, \mu_2]$ from $n \times n$ image tokens, generate a $m \times m$ Gaussian kernel $\mathcal{G}$ with variance $\sigma$\ and mean $[\mu_1, \mu_2]$\;
    $Z_c = f_c(Z_v, \mathcal{G})$, add the Gaussian kernel to the attention map of the cross-attention in the vision-language connector\;
    Compute the loss $\mathcal{L} = \mathcal{L}(f_l(Z_c), T)$\;
    $\delta \leftarrow \operatorname{clip}_\epsilon(\delta+\alpha\cdot\operatorname{sign}(\nabla_\delta\mathcal{L}))$\;
}
% \vspace{-3mm}
\end{algorithm}
