\section{Conclusion}
\vspace{-1mm}
In this paper, we propose the Dynamic Vision-Language Alignment (DynVLA) Attack, a novel approach designed to enhance the transferability of adversarial examples across Multimodal Large Language Models (MLLMs). By dynamically adjusting the vision-language alignment, DynVLA effectively encourages the model to focus on different regions of the input image, utilizing a Gaussian kernel to achieve smoother and more coherent changes. Our extensive experiments demonstrate that DynVLA significantly outperforms baseline methods, which struggle to transfer adversarial examples effectively across different models.
This poses new challenges for improving the robustness and security of MLLMs in real-world applications. We hope that this research not only sheds light on these vulnerabilities but also provides a foundation for future exploration of defense mechanisms and more secure AI systems.

Similar to the findings in~\citet{schaeffer2024universal}, our adversarial examples face challenges when attacking target models with architectures significantly different from the surrogate model. This indicates that while DynVLA performs well within a family of models with comparable vision-language connectors or LLM backbones, its ability to generalize across fundamentally different architectures is limited.
Moreover, our experiments reveal that attacking state-of-the-art closed-source models remains challenging, especially under our exactly-matching targeted attack scenario, which presents a promising area for future research.