\section{Related work}
\subsection{Low-light Image enhancement}
Low-light image enhancement techniques can be generally classified into two categories: traditional image enhancement methods and deep learning-based approaches. Traditional methods \cite{article1}\cite{article2}\cite{article3}, such as histogram equalization, Retinex-based algorithms, grayscale transformation techniques, and digital image signal processing algorithms applied in-camera (e.g., gamma correction, white balance, and color matrix adjustments), primarily focus on the original image. While these methods offer the benefit of being computationally lightweight, they often require extensive parameter tuning to accommodate varying scene conditions, leading to increased labor costs.
\par In recent years, the number of deep learning-based low-light image enhancement methods has significantly increased alongside the advancements in deep learning. LLNet \cite{article4}, as the pioneering deep learning approach for low-light image enhancement, enhances light intensity in low-light images through encoder stacking, yielding promising results. GLADNet \cite{article13} segments the enhancement process into two stages: light intensity evaluation and detail restoration, effectively improving low-light images through this dual-phase approach. MSRNet \cite{article5} enhances low-light images by learning an end-to-end mapping, while RetinexNet \cite{article7}, based on Retinex theory, decomposes images to achieve both enhancement and denoising, though it is prone to image distortion. EnlightenGAN \cite{article10}, employing perceptual loss functions and network attention to light intensity, generates images resembling those captured in normal lighting conditions. It is also the first low-light enhancement model to adopt unsupervised training methods. ZeroDCE \cite{article14}, another unsupervised deep learning-based enhancement network, adjusts image pixel values to achieve effective low-light image enhancement. The URetinexNet model \cite{article11} introduces a network with three learning-based modules dedicated to data-dependent initialization, efficient unfolding optimization, and user-specified illumination enhancement to improve brightness and detail. Other notable enhancement methods include those based on virtual exposure strategies \cite{article33}, recognition-integrated enhancement \cite{article34}, algorithms combining Retinex theory with Transformers \cite{article35}, implicit neural representation techniques \cite{article36}, and dual-input methods to restore detail in low-light images \cite{article37}. Collectively, these approaches have demonstrated substantial improvements in low-light image quality.
\par CLIP-LIT \cite{article15} is an unsupervised image enhancement algorithm that leverages the unsupervised learning capabilities of CLIP. While this approach mitigates the challenge of acquiring paired image datasets, it is susceptible to generating artifacts due to the absence of labeled supervision. Furthermore, the feature-awareness of current algorithms is generally constrained, limiting their ability to effectively capture local image details and address issues of localized illumination inhomogeneity.
\par We propose a novel luminance enhancement algorithm based on the CLIP contrastive learning framework, capable of unsupervised learning. This algorithm captures and adaptively enhances luminance features in heterogeneous regions without altering the original semantic information, thereby effectively addressing luminance inhomogeneity and achieving significant improvements in image luminance enhancement.
\subsection{CLIP-based visual works}
CLIP (Contrastive Language-Image Pre-Training) is a multimodal pre-training model grounded in contrastive learning. By leveraging 400 million image-text pairs, CLIP learns generalized visual and linguistic features, enabling zero-shot image classification based on detailed input cues. Its robust feature-awareness through contrastive learning and outstanding zero-shot generalization capabilities have made CLIP a popular choice in various computer vision tasks, consistently delivering impressive results.
\par A study in the literature \cite{article16} frames video motion recognition as video-text retrieval by encoding both the generated text and keyframe images using CLIP's pre-trained text and image encoders, and then computing the similarity between the two. This method effectively captures the semantic information of labels and facilitates zero-shot knowledge transfer. Another study \cite{article17} improved text encoding performance by replacing the HERO model's text encoder with CLIP's, achieving superior results on the video language understanding task benchmark. Additionally, research \cite{article18} leveraged CLIP-based models for pre-training video-text illustration logic, learning image features related to text semantics, and fine-tuning video subtitle generation based on these features, yielding excellent results. In \cite{article15}, CLIP was applied to the domain of low-light image enhancement, where a frozen CLIP encoder was used to generate cues related to normal-light images, guiding the enhancement network. While this approach effectively addresses defective features in localized regions, it is limited in the extent of enhancement and prone to artifact generation.
\par To tackle these challenges, we propose a CLIP-based low-light image enhancement algorithm. This algorithm leverages CLIP's robust feature perception capabilities to discern varying luminance levels in local detail regions. Furthermore, we introduce a novel luminance enhancement unit designed to adaptively perform pixel-level enhancement without introducing semantic inaccuracies, thereby effectively addressing the issue of uneven luminance.