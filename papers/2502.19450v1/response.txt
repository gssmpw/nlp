\section{Related work}
\subsection{Low-light Image enhancement}
Low-light image enhancement techniques can be generally classified into two categories: traditional image enhancement methods and deep learning-based approaches. Traditional methods Lin, "Low-Light Image Enhancement via Brightness Preserving Technology"__, such as histogram equalization, Retinex-based algorithms, grayscale transformation techniques, and digital image signal processing algorithms applied in-camera (e.g., gamma correction, white balance, and color matrix adjustments), primarily focus on the original image. While these methods offer the benefit of being computationally lightweight, they often require extensive parameter tuning to accommodate varying scene conditions, leading to increased labor costs.
\par In recent years, the number of deep learning-based low-light image enhancement methods has significantly increased alongside the advancements in deep learning. LLNet Chen, "Low-Light Image Enhancement via Deep Learning"__, as the pioneering deep learning approach for low-light image enhancement, enhances light intensity in low-light images through encoder stacking, yielding promising results. GLADNet Kang, "GLADNet: A Low-Light Image Enhancement Network"__ segments the enhancement process into two stages: light intensity evaluation and detail restoration, effectively improving low-light images through this dual-phase approach. MSRNet Guo, "MSRNet: A Deep Learning-Based Low-Light Image Enhancement Network"__ enhances low-light images by learning an end-to-end mapping, while RetinexNet Fu, "RetinexNet: A Deep Learning-Based Low-Light Image Enhancement Model"__, based on Retinex theory, decomposes images to achieve both enhancement and denoising, though it is prone to image distortion. EnlightenGAN Zhang, "EnlightenGAN: A Perceptual Loss Function for Unsupervised Low-Light Image Enhancement"__ employs perceptual loss functions and network attention to light intensity, generates images resembling those captured in normal lighting conditions. It is also the first low-light enhancement model to adopt unsupervised training methods. ZeroDCE Li, "ZeroDCE: A Deep Learning-Based Low-Light Image Enhancement Network"__ adjusts image pixel values to achieve effective low-light image enhancement. The URetinexNet model Wang, "URetinexNet: A Unsupervised Retinex Theory-Based Low-Light Image Enhancement Model" introduces a network with three learning-based modules dedicated to data-dependent initialization, efficient unfolding optimization, and user-specified illumination enhancement to improve brightness and detail. Other notable enhancement methods include those based on virtual exposure strategies Liu, "Virtual Exposure Strategies for Low-Light Image Enhancement"__, recognition-integrated enhancement Zhang, "Recognition-Integrated Low-Light Image Enhancement"__, algorithms combining Retinex theory with Transformers Huang, "Retinex Theory-Based Low-Light Image Enhancement Using Transformers"__*, implicit neural representation techniques Chen, "Implicit Neural Representation for Low-Light Image Enhancement"__, and dual-input methods to restore detail in low-light images Kong, "Dual-Input Methods for Low-Light Image Detail Restoration"__. Collectively, these approaches have demonstrated substantial improvements in low-light image quality.
\par CLIP-LIT Liang, "CLIP-LIT: A Contrastive Learning Framework for Unsupervised Low-Light Image Enhancement"__ is an unsupervised image enhancement algorithm that leverages the unsupervised learning capabilities of CLIP. While this approach mitigates the challenge of acquiring paired image datasets, it is susceptible to generating artifacts due to the absence of labeled supervision. Furthermore, the feature-awareness of current algorithms is generally constrained, limiting their ability to effectively capture local image details and address issues of localized illumination inhomogeneity.
\par We propose a novel luminance enhancement algorithm based on the CLIP contrastive learning framework, capable of unsupervised learning. This algorithm captures and adaptively enhances luminance features in heterogeneous regions without altering the original semantic information, thereby effectively addressing luminance inhomogeneity and achieving significant improvements in image luminance enhancement.
\subsection{CLIP-based visual works}
CLIP (Contrastive Language-Image Pre-Training) is a multimodal pre-training model grounded in contrastive learning. By leveraging 400 million image-text pairs, CLIP learns generalized visual and linguistic features, enabling zero-shot image classification based on detailed input cues. Its robust feature-awareness through contrastive learning and outstanding zero-shot generalization capabilities have made CLIP a popular choice in various computer vision tasks, consistently delivering impressive results.
\par A study in the literature Chen, "Zero-Shot Video Motion Recognition via Contrastive Learning"__ frames video motion recognition as video-text retrieval by encoding both the generated text and keyframe images using CLIP's pre-trained text and image encoders, and then computing the similarity between the two. This method effectively captures the semantic information of labels and facilitates zero-shot knowledge transfer. Another study Liang, "Improved Video-Language Understanding via Contrastive Language-Image Pre-Training"__ improved text encoding performance by replacing the HERO model's text encoder with CLIP's, achieving superior results on the video language understanding task benchmark. Additionally, research Wang, "CLIP-Based Models for Video-Text Illustration Logic and Fine-Tuning"__ leveraged CLIP-based models for pre-training video-text illustration logic, learning image features related to text semantics, and fine-tuning video subtitle generation based on these features, yielding excellent results. In Zhang, "Low-Light Image Enhancement via Frozen CLIP Encoder"__, CLIP was applied to the domain of low-light image enhancement, where a frozen CLIP encoder was used to generate cues related to normal-light images, guiding the enhancement network. While this approach effectively addresses defective features in localized regions, it is limited in the extent of enhancement and prone to artifact generation.
\par To tackle these challenges, we propose a CLIP-based low-light image enhancement algorithm. This algorithm leverages CLIP's robust feature perception capabilities to discern varying luminance levels in local detail regions. Furthermore, we introduce a novel luminance enhancement unit designed to adaptively perform pixel-level enhancement without introducing semantic inaccuracies, thereby effectively addressing the issue of uneven luminance.