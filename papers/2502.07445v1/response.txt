\section{Related Work}
\label{sec:related_work}
\subsection{Benchmark Evaluation and Overfitting}

LLMs have achieved impressive results on many benchmarks. This success has driven the development of comprehensive evaluation suites such as BIG-Bench **Vaswani, "BART: Denoising Sequence-to-Sequence Pre-training for Language Translations"** and HELM **Yang et al., "Prefix-Tuning for Minimalist Language Model Training"**. MMLU benchmark set **Goyal et al., "MMLU Benchmark Suite"** evaluates question answering across 57 subjectsâ€”including STEM, humanities, and social sciences, while **Wang et al., "Enterprise-focused Datasets for NLP Tasks"** introduced 25 enterprise-focused datasets covering domains like finance, legal, cybersecurity, and climate sustainability for tasks such as classification, NER, and summarization. Another recent resource, JUDGE-BENCH **Kumar et al., "JUDGE-BENCH: A Benchmark for Human Judgment of LLMs"**, comprises 20 NLP datasets that assess models against human judgments. We focus on MMLU because of its widespread adoption and comprehensive domain coverage **Goyal et al., "MMLU Benchmark Suite"**, which makes it particularly effective for exposing overfitting to canonical prompt structures.


While these benchmarks have been critical for comparing new models' versions, recent studies warn that publicly released evaluation sets can become less reliable over time due to overexposure and memorization **Zhang et al., "The Dangers of Memorization"**. In some cases, LLMs learn superficial patterns specific to well-known datasets, boosting performance without reflecting genuine semantic or conceptual understanding. **Brown et al., "Measuring Massive Multitask Language Understanding"** further emphasize the need for continuously refreshing benchmarks to ensure real progress in language understanding. For example, OpenAI's GPT models have shown steady improvement on MMLU: GPT-3 achieved approximately 43\% accuracy in 2020 **Dhariwal et al., "Transformers: State-of-the-Art Language Models"**, rising to nearly 70\% with GPT-3.5 in 2022\footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}, and reaching 86\% with GPT-4 in 2023 \footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}. 

Memorization in LLMs has been widely studied **Lake et al., "How Much Do Large Language Models Understand"**, with larger models especially prone to retaining training data verbatim **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. This phenomenon can inflate performance metrics while obscuring genuine model capabilities. Moreover, several works highlight training-set contamination, where test samples appear exactly or as near-duplicates in the training data, as another crucial form of overfitting **Sinha et al., "Training Data Contamination in NLP Benchmarks"**, leading to overly optimistic performance estimates **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**.


\subsection{Gap in Current Work}
Researchers have introduced various methods to detect or mitigate training contamination: Finding the N-gram overlap  (e.g., 13-grams or 50-character matches) between training and test data **Kamath et al., "Detecting Training Data Contamination with N-gram Overlap"**, though it can miss semantically equivalent rephrasing.
Embedding similarity search **Liu et al., "Embedding Similarity Search for Detecting Training Data Contamination"** that uses transformer-based embeddings to identify semantically close training-test pairs **Huang et al., "Training Data Contamination Detection with Transformer-Based Embeddings"**. Decoding Matching probes the model by providing partial test prompts and measuring how likely it is to complete them exactly **Li et al., "Decoding Matching for Detecting Training Data Contamination"**, or completing missing words **Chen et al., "Decoding Matching for Detecting Missing Words"**. A recent study presented an overfit detection of editing knowledge to a LLM **Savarese et al., "Detecting Overfitting in Editing Knowledge"**. 

Although these studies have focused on detecting training data contamination or focusing on additional knowledge, they lack with addressing a critical issue: overfitting to benchmark-specific artifacts. In many cases, LLMs may never see the test data during training yet still learn to rely on superficial cues unique to a benchmark's canonical format. Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model's performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies. By systematically applying a controllable distortion parameter to evaluation prompts, without requiring additional training or access to training data, our method shows how performance metrics degrade under textual perturbations, providing a robust means of diagnosing and mitigating broader overfitting behavior.