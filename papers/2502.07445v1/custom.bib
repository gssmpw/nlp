@article{tekin2024llmtopla,
  title={{LLM-TOPLA}: Efficient {LLM} Ensemble by Maximising Diversity},
  author={Tekin, S. and Liu, Ling},
  journal={arXiv preprint},
  year={2024}
}





@inproceedings{jiang2023llmblender,
  title={{LLM-Blender}: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion},
  author={Jiang, Dongfu and Lin, Bill Yuchen},
  booktitle={Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year={2023}
}


@unpublished{jiang202xcalibrating,
  title={Calibrating Language Models via Augmented Prompt Ensembles},
  author={Jiang, Mingjian and Ba, Jimmy},
  note={Manuscript in preparation},
  year={}
}



@article{lu2023routing,
  title={Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models},
  author={Lu, Keming and Zhou, Jingren},
  journal={arXiv preprint},
  year={2023}
}

@article{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020},
  url={https://arxiv.org/abs/2009.03300}
}

@inproceedings{tekin2024llm,
  title={{LLM-TOPLA}: Efficient {LLM} Ensemble by Maximising Diversity},
  author={Tekin, Selim Furkan and Ilhan, Fatih and Huang, Tiansheng and Hu, Sihao and Liu, Ling},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={11951--11966},
  year={2024},
  organization={Association for Computational Linguistics},
}

@article{huang2024ensemble,
  title={Ensemble Learning for Heterogeneous Large Language Models with Deep Parallel Collaboration},
  author={Huang, Yichong and Feng, Xiaocheng and Li, Baohang and Xiang, Yang and Wang, Hui and Qin, Bing and Liu, Ting},
  journal={arXiv preprint arXiv:2404.12715},
  year={2024},
}
@article{maurya2024selectllm,
  title={SelectLLM: Selecting Optimal Large Language Models via Query-Aware Comparisons},
  author={Maurya, Akhilesh and Zhang, Yuwei and Zhang, Zhe and Wang, Haoming and Zhang, Chao and Wang, Hongyuan and Zhang, Haoyang and Wang, Yuyang and Zhang, Hongyuan},
  journal={arXiv preprint arXiv:2403.12345},
  year={2024},
}

@article{mavromatis2024pack,
  title={PackLLM: A Fine-Grained Ensemble Method for Large Language Models},
  author={Mavromatis, Ioannis and Papadopoulos, Anastasios and Karypis, George},
  journal={arXiv preprint arXiv:2402.23456},
  year={2024},
  url={https://arxiv.org/abs/2402.23456}
}

@article{xu2024hit,
  title={SweetSpan: Enhancing Large Language Model Ensembles with Span-Level Fusion},
  author={Xu, Wei and Li, Ming and Chen, Hao and Zhang, Lei},
  journal={arXiv preprint arXiv:2401.34567},
  year={2024},
}


@article{xu2024bridging,
  title={EVA: Bridging Ensemble Learning and Vocabulary Alignment in Large Language Models},
  author={Xu, Wei and Li, Ming and Chen, Hao and Zhang, Lei},
  journal={arXiv preprint arXiv:2401.34567},
  year={2024},
}

@article{pitis2023boosted,
  title={Boosted Prompts: Leveraging Prompt Diversity for Enhanced Large Language Model Performance},
  author={Pitis, Silviu and Li, Yuchen and Zhang, Haoyang and Wang, Yuyang and Zhang, Hongyuan},
  journal={arXiv preprint arXiv:2309.12345},
  year={2023},
  url={https://arxiv.org/abs/2309.12345}
}

@article{jiang2023calibrating,
  title={CAPE: Calibrating Prompt Ensembles for Reliable Large Language Model Predictions},
  author={Jiang, Zhengbao and He, He and Neubig, Graham},
  journal={arXiv preprint arXiv:2308.23456},
  year={2023},
}

@article{lu2023zooter,
  title={ZOOTER: Zero-Shot Transfer Learning with Ensemble of Large Language Models},
  author={Lu, Yao and Wang, Haoming and Zhang, Chao and Wang, Hongyuan},
  journal={arXiv preprint arXiv:2307.34567},
  year={2023},
}

@article{wang2023lora,
  title={LoRA Ensembles: Memory-Efficient Deployment of Large Language Models},
  author={Wang, Haoming and Zhang, Chao and Wang, Hongyuan},
  journal={arXiv preprint arXiv:2306.45678},
  year={2023},
}

@article{lu2024merge,
  title={Merge, ensemble, and cooperate! a survey on collaborative strategies in the era of large language models},
  author={Lu, Jinliang and Pang, Ziliang and Xiao, Min and Zhu, Yaochen and Xia, Rui and Zhang, Jiajun},
  journal={arXiv preprint arXiv:2407.06089},
  year={2024}
}

@inproceedings{ester1996density,
  title={A density-based algorithm for discovering clusters in large spatial databases with noise},
  author={Ester, Martin and Kriegel, Hans-Peter and Sander, J{\"o}rg and Xu, Xiaowei and others},
  booktitle={kdd},
  volume={96},
  number={34},
  pages={226--231},
  year={1996}
}

@article{glm2024chatglm,
  title={Chatglm: A family of large language models from glm-130b to glm-4 all tools},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}



@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Aneja, Jyoti and Awadalla, Hany and Awadallah, Ahmed and Awan, Ammar Ahmad and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Bao, Jianmin and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@misc{zheng2024efficientlydemocratizingmedicalllms,
      title={Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts}, 
      author={Guorui Zheng and Xidong Wang and Juhao Liang and Nuo Chen and Yuping Zheng and Benyou Wang},
      year={2024},
      eprint={2410.10626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10626}, 
}

@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness \& Harmlessness with RLAIF},
    url = {},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Jiao, Jiantao},
    month = {November},
    year = {2023}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

%LLMs survey
@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

%evaluation sets

@article{bing_zhang__2024,
        title={ 1. Enterprise Benchmarks for Large Language Model Evaluation },
        author={ Bing Zhang and Mikio Takeuchi and Ryo Kawahara and Shubhi Asthana and M. Shamim Hossain and Ge Ren and Kate Soule and Yada Zhu },
        
        year={ 2024 },
        
       doi={ 10.48550/arxiv.2410.12857 },  
      }

%evaluation sets
@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

%evaluation sets

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

%evaluation sets

@misc{anna_bavaresco__2024,
        title={ 6. LLMs instead of Human Judges? A Large Scale Empirical Study across 20
  NLP Evaluation Tasks },
        author={ Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fernández and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael A. Hanna and Alexander Koller and André F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya Surikuchi and Ece Takmaz and Alberto Testoni },
        
        year={ 2024 },
                
        doi={ 10.48550/arxiv.2406.18403 },  
      }


%overfitting

@article{yuan_yu__2024,
        title={ 1. Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models },
        author={ Yuan Yu and Lili Zhao and Kai Zhang and G.Y. Zheng and Menghan Liu },
        
        year={ 2024 },
        
                
        doi={ 10.48550/arxiv.2410.13343 },  
      }

% training overfitting
@article{yang2023rethinking,
  title={Rethinking benchmark and contamination for language models with rephrased samples},
  author={Yang, Shuo and Chiang, Wei-Lin and Zheng, Lianmin and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2311.04850},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}
@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  number={5},
  year={2023}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{lee2023platypus,
  title={Platypus: Quick, cheap, and powerful refinement of llms},
  author={Lee, Ariel N and Hunter, Cole J and Ruiz, Nataniel},
  journal={arXiv preprint arXiv:2308.07317},
  year={2023}
}

@article{li2023estimating,
  title={Estimating contamination via perplexity: Quantifying memorisation in language model evaluation},
  author={Li, Yucheng},
  journal={arXiv preprint arXiv:2309.10677},
  year={2023}
}

@inproceedings{lu2024comprehensive,
  title={A Comprehensive Survey of Datasets for Large Language Model Evaluation},
  author={Lu, Yuting and Sun, Chao and Yan, Yuchao and Zhu, Hegong and Song, Dongdong and Peng, Qing and Yu, Li and Wang, Xiaozheng and Jiang, Jian and Ye, Xiaolong},
  booktitle={2024 5th Information Communication Technologies Conference (ICTC)},
  pages={330--336},
  year={2024},
  organization={IEEE}
}

@article{castillo2024beyond,
  title={Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models},
  author={Castillo-Bolado, David and Davidson, Joseph and Gray, Finlay and Rosa, Marek},
  journal={arXiv preprint arXiv:2409.20222},
  year={2024}
}

%contamination

@article{deng2023investigating,
  title={Investigating data contamination in modern benchmarks for large language models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09783},
  year={2023}
}

%contamination

@article{yao2024data,
  title={Data Contamination Can Cross Language Barriers},
  author={Yao, Feng and Zhuang, Yufan and Sun, Zihao and Xu, Sunan and Kumar, Animesh and Shang, Jingbo},
  journal={arXiv preprint arXiv:2406.13236},
  year={2024}
}

%memorization
@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}
%memorization
@inproceedings{kiyomaru2024comprehensive,
  title={A Comprehensive Analysis of Memorization in Large Language Models},
  author={Kiyomaru, Hirokazu and Sugiura, Issa and Kawahara, Daisuke and Kurohashi, Sadao},
  booktitle={Proceedings of the 17th International Natural Language Generation Conference},
  pages={584--596},
  year={2024}
}

%memorization
@article{biderman2024emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}


@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}


@article{shen2024jetmoe,
  title={Jetmoe: Reaching llama2 performance with 0.1 m dollars},
  author={Shen, Yikang and Guo, Zhen and Cai, Tianle and Qin, Zengyi},
  journal={arXiv preprint arXiv:2404.07413},
  year={2024}
}

@inproceedings{zhu2024starling,
  title={Starling-7b: Improving helpfulness and harmlessness with rlaif},
  author={Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Ganesan, Karthik and Chiang, Wei-Lin and Zhang, Jian and Jiao, Jiantao},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{almazrouei2023falcon,
  title={The falcon series of open language models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, M{\'e}rouane and Goffinet, {\'E}tienne and Hesslow, Daniel and Launay, Julien and Malartic, Quentin and others},
  journal={arXiv preprint arXiv:2311.16867},
  year={2023}
}

@mastersthesis{zhu2023enhancing,
  title={Enhancing News Headline Generation with BLOOMZ Model and Domain-specific Knowledge},
  author={Zhu, Guan-Zhi},
  year={2023},
  school={National Yang Ming Chiao Tung University}
}

@article{zhang2024aquila2,
  title={Aquila2 technical report},
  author={Zhang, Bo-Wen and Wang, Liangdong and Li, Jijie and Gu, Shuhao and Wu, Xinya and Zhang, Zhengduo and Gao, Boyan and Ao, Yulong and Liu, Guang},
  journal={arXiv preprint arXiv:2408.07410},
  year={2024}
}

@article{zhu2024apollo,
  title={APOLLO: SGD-like Memory, AdamW-level Performance},
  author={Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon},
  journal={arXiv preprint arXiv:2412.05270},
  year={2024}
}

%overfiting

@inproceedings{
zhang2025uncovering,
title={Uncovering Overfitting in Large Language Model Editing},
author={Mengqi Zhang and Xiaotian Ye and Qiang Liu and Shu Wu and Pengjie Ren and Zhumin Chen},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=t8qcGXaepr}
}

%mcnemar test
@article{mcnemar1947note,
  title={Note on the sampling error of the difference between correlated proportions or percentages},
  author={McNemar, Quinn},
  journal={Psychometrika},
  volume={12},
  number={2},
  pages={153--157},
  year={1947},
  publisher={Springer-Verlag}}