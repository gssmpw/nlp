\section{Related Work}
\label{sec:related_work}
\subsection{Benchmark Evaluation and Overfitting}

LLMs have achieved impressive results on many benchmarks. This success has driven the development of comprehensive evaluation suites such as BIG-Bench \cite{srivastava2022beyond} and HELM \cite{liang2022holistic}. MMLU benchmark set \cite{hendrycks2020measuring} evaluates question answering across 57 subjectsâ€”including STEM, humanities, and social sciences, while \cite{bing_zhang__2024} introduced 25 enterprise-focused datasets covering domains like finance, legal, cybersecurity, and climate sustainability for tasks such as classification, NER, and summarization. Another recent resource, JUDGE-BENCH \cite{anna_bavaresco__2024}, comprises 20 NLP datasets that assess models against human judgments. We focus on MMLU because of its widespread adoption and comprehensive domain coverage \cite{wang2024mmlu}, which makes it particularly effective for exposing overfitting to canonical prompt structures.


While these benchmarks have been critical for comparing new models' versions, recent studies warn that publicly released evaluation sets can become less reliable over time due to overexposure and memorization \cite{yuan_yu__2024, chang2024survey}. In some cases, LLMs learn superficial patterns specific to well-known datasets, boosting performance without reflecting genuine semantic or conceptual understanding. \cite{kiela2021dynabench} further emphasize the need for continuously refreshing benchmarks to ensure real progress in language understanding. For example, OpenAI's GPT models have shown steady improvement on MMLU: GPT-3 achieved approximately 43\% accuracy in 2020 \cite{brown2020language}, rising to nearly 70\% with GPT-3.5 in 2022\footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}, and reaching 86\% with GPT-4 in 2023 \footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}. 

Memorization in LLMs has been widely studied \cite{kiyomaru2024comprehensive, biderman2024emergent}, with larger models especially prone to retaining training data verbatim \cite{carlini2022quantifying}. This phenomenon can inflate performance metrics while obscuring genuine model capabilities. Moreover, several works highlight training-set contamination, where test samples appear exactly or as near-duplicates in the training data, as another crucial form of overfitting \cite{deng2023investigating, yao2024data}, leading to overly optimistic performance estimates \cite{yang2023rethinking}.


\subsection{Gap in Current Work}
Researchers have introduced various methods to detect or mitigate training contamination: Finding the N-gram overlap  (e.g., 13-grams or 50-character matches) between training and test data \cite{brown2020language,openai2023gpt}, though it can miss semantically equivalent rephrasing.
Embedding similarity search \cite{reimers2019sentence} that uses transformer-based embeddings to identify semantically close training-test pairs \cite{lee2023platypus}. Decoding Matching probes the model by providing partial test prompts and measuring how likely it is to complete them exactly \cite{li2023estimating} or completing missing words \cite{deng2023investigating}. A recent study presented an overfit detection of editing knowledge to a LLM \cite{zhang2025uncovering}. 

Although these studies have focused on detecting training data contamination or focusing on additional knowledge, they lack with addressing a critical issue: overfitting to benchmark-specific artifacts. In many cases, LLMs may never see the test data during training yet still learn to rely on superficial cues unique to a benchmark's canonical format. Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model's performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies. By systematically applying a controllable distortion parameter to evaluation prompts, without requiring additional training or access to training data, our method shows how performance metrics degrade under textual perturbations, providing a robust means of diagnosing and mitigating broader overfitting behavior.