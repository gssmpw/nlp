% This must be in the first 5 lines to tell arXiv to use pdfLaTeX.
\pdfoutput=1

\documentclass[11pt]{article}

% Change "review" to "final"
\usepackage[final]{acl}
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage{float}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{makecell}
\usepackage{setspace}
\setstretch{0.926}
% \newcommand{\seffi}[1]{\textcolor{purple}{[Seffi]~#1}}
% \newcommand{\nurit}[1]{\textcolor{blue}{[Nurit]~#1}}
% \newcommand{\yoni}[1]{\textcolor{red}{[Yoni]~#1}}
% \newcommand{\bracha}[1]{\textcolor{yellow}
% {[Bracha]~#1}}

\title{Forget What You Know about LLMs Evaluations - LLMs are Like a Chameleon}

\author{
  Nurit Cohen-Inger\textsuperscript{1}, 
  Yehonatan Elisha\textsuperscript{2}, 
  Bracha Shapira\textsuperscript{1}, 
  Lior Rokach\textsuperscript{1}, 
  Seffi Cohen\textsuperscript{1} \\
  \\
  \textsuperscript{1}Ben Gurion University \quad
  \textsuperscript{2}Tel Aviv University
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the \textbf{Chameleon Benchmark Overfit Detector (C-BOD)}, a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model’s performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15\% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD’s dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.
\end{abstract}






\section{Introduction}

Large Language Models (LLMs) have achieved impressive results on a wide range of NLP tasks \cite{chang2024survey}. Consequently, hundreds of benchmarks have been established to track progress and evaluate model capabilities \cite{lu2024comprehensive, liang2022holistic}. However, the rapid proliferation of LLMs and the frequent use of public leaderboards raise concerns about the robustness of these evaluation practices \cite{castillo2024beyond}. Specifically, as benchmark data becomes more widely recognized, models may learn to exploit surface patterns or spurious correlations, rather than exhibit genuine language understanding. This issue can lead to deceptively high scores that do not reflect true progress.

In this paper, we examine whether LLMs rely excessively on benchmark-specific cues potentially overfitting to the patterns inherent in widely published evaluation benchmarks and explore systematic methods to detect and mitigate this behavior. In other words, are LLMs prone to overfitting on popular benchmarks, and what underlying factors contribute to this phenomenon?

%We pose two key research questions:

%\begin{itemize}
%    \item \textbf{RQ1:} Are LLMs prone to overfitting on widely published evaluation benchmarks?
%    \item \textbf{RQ2:} Can we design a general-purpose evaluation framework that both detects and mitigates this overfitting in LLMs?
%\end{itemize}

To answer this question, we introduce Chameleon Benchmark Overfit Detector (C-BOD), a framework that reveals how heavily a model depends on the exact wording or structure of a test set. By introducing controlled textual distortions to benchmark prompts at varying intensities (defined by a distortion parameter $\mu$), as demonstrated in Figure \ref{fig:mu_transform_examples}, our method reveals whether high performance is built on superficial patterns. Notably, our framework requires only the evaluation set, without accessing the model’s training data or architecture. Unlike conventional leaderboards that solely track performance, our meta-evaluation framework acts as a safeguard ensuring that high scores do not stem from superficial memorization of benchmark cues.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.95\linewidth]{motivation.png} % 
    \caption{An example demonstrating the C-BOD method. The original question (top) is perturbed (bottom) while preserving the semantic meaning and correct answer options.  The model correctly answers the original question but fails on the perturbed version, suggesting potential overfitting. Changes in the perturbed question are highlighted in bold.}
    \label{fig:mu_transform_examples}
\end{figure}

\paragraph{Our Contributions:}
\begin{enumerate}[leftmargin=1.2em]
    \item \textbf{Robust Overfitting Detection with Statistical Significance.} 
    Our framework computes the performance difference $\Delta_{\mu}$ between original and perturbed prompts and confirms its statistical significance, ensuring that observed differences indeed indicate overfitting rather than chance variations.

    \item \textbf{New Findings For LLM Community} 
    Our extensive analysis reveals new trends regarding how LLMs function with respect to their number of parameters and baseline performance. 
    
    \item \textbf{Extensive Empirical Validation.} 
    We apply our method to multiple LLM families of various architectures and parameter sizes. Even modest textual distortions cause significant performance differences in most models, providing strong empirical evidence that overfitting is widespread.

    \item \textbf{Publicly Available Benchmarks and Code.} 
    We release rephrased versions of the widely used MMLU evaluation set under different distortion levels ($\mu$). These resources enable the community to adopt more robust, surface-invariant tests for reliable LLM assessment with our method reproducible code.

    \item \textbf{Blueprint for Iterative Overfit Mitigation.} 
    Beyond detection, these $\mu$-based rephrasings can be integrated into model training or fine-tuning pipelines. Regularly exposing models to diverse prompt variations helps reduce reliance on benchmark-specific phrasing, thus promoting more generalizable language understanding.
\end{enumerate}


\section{Related Work}
\label{sec:related_work}
\subsection{Benchmark Evaluation and Overfitting}

LLMs have achieved impressive results on many benchmarks. This success has driven the development of comprehensive evaluation suites such as BIG-Bench \cite{srivastava2022beyond} and HELM \cite{liang2022holistic}. MMLU benchmark set \cite{hendrycks2020measuring} evaluates question answering across 57 subjects—including STEM, humanities, and social sciences, while \cite{bing_zhang__2024} introduced 25 enterprise-focused datasets covering domains like finance, legal, cybersecurity, and climate sustainability for tasks such as classification, NER, and summarization. Another recent resource, JUDGE-BENCH \cite{anna_bavaresco__2024}, comprises 20 NLP datasets that assess models against human judgments. We focus on MMLU because of its widespread adoption and comprehensive domain coverage \cite{wang2024mmlu}, which makes it particularly effective for exposing overfitting to canonical prompt structures.


While these benchmarks have been critical for comparing new models' versions, recent studies warn that publicly released evaluation sets can become less reliable over time due to overexposure and memorization \cite{yuan_yu__2024, chang2024survey}. In some cases, LLMs learn superficial patterns specific to well-known datasets, boosting performance without reflecting genuine semantic or conceptual understanding. \cite{kiela2021dynabench} further emphasize the need for continuously refreshing benchmarks to ensure real progress in language understanding. For example, OpenAI's GPT models have shown steady improvement on MMLU: GPT-3 achieved approximately 43\% accuracy in 2020 \cite{brown2020language}, rising to nearly 70\% with GPT-3.5 in 2022\footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}, and reaching 86\% with GPT-4 in 2023 \footnote{\url{https://cdn.openai.com/papers/GPT-4-Technical-Report.pdf}}. 

Memorization in LLMs has been widely studied \cite{kiyomaru2024comprehensive, biderman2024emergent}, with larger models especially prone to retaining training data verbatim \cite{carlini2022quantifying}. This phenomenon can inflate performance metrics while obscuring genuine model capabilities. Moreover, several works highlight training-set contamination, where test samples appear exactly or as near-duplicates in the training data, as another crucial form of overfitting \cite{deng2023investigating, yao2024data}, leading to overly optimistic performance estimates \cite{yang2023rethinking}.


\subsection{Gap in Current Work}
Researchers have introduced various methods to detect or mitigate training contamination: Finding the N-gram overlap  (e.g., 13-grams or 50-character matches) between training and test data \cite{brown2020language,openai2023gpt}, though it can miss semantically equivalent rephrasing.
Embedding similarity search \cite{reimers2019sentence} that uses transformer-based embeddings to identify semantically close training-test pairs \cite{lee2023platypus}. Decoding Matching probes the model by providing partial test prompts and measuring how likely it is to complete them exactly \cite{li2023estimating} or completing missing words \cite{deng2023investigating}. A recent study presented an overfit detection of editing knowledge to a LLM \cite{zhang2025uncovering}. 

Although these studies have focused on detecting training data contamination or focusing on additional knowledge, they lack with addressing a critical issue: overfitting to benchmark-specific artifacts. In many cases, LLMs may never see the test data during training yet still learn to rely on superficial cues unique to a benchmark's canonical format. Existing techniques such as n-gram overlap and embedding similarity fail to capture this subtle form of overfitting. In contrast, our approach explicitly quantifies the dependence of a model's performance on the precise phrasing and structure of evaluation prompts, thereby filling this gap in current evaluation methodologies. By systematically applying a controllable distortion parameter to evaluation prompts, without requiring additional training or access to training data, our method shows how performance metrics degrade under textual perturbations, providing a robust means of diagnosing and mitigating broader overfitting behavior.



\section{Method}
\label{sec:method}
Let \(\mathcal{D}\) denote a benchmark dataset with N samples, and \(\mathcal{E}\) a LLM to be evaluated with respect to a given performance function \(\mathcal{M}\).
Our goal is to detect whether \(\mathcal{E}\) exhibits overfitting to \(\mathcal{D}\).
% Our method, Chameleon Benchmark Overfit Detector (C-BOD), uses a rephrasing tool to generate a new perturbed dataset from \(\mathcal{D}\). Then, evaluates \(\mathcal{E}\) using both the original and the perturbed dataset. Finally, C-BOD uses McNemar’s test to check the significance in the difference between the perforamce across the two datasets and detects wether \(\mathcal{E}\) is overfitting \(\mathcal{D}\).
Figure~\ref{fig:method_flow} provides an overview of our proposed method, Chameleon Benchmark Overfit Detector (C-BOD).
C-BOD employs a rephrasing transformation to generate a perturbed dataset from \(\mathcal{D}\), evaluates on both the original and perturbed datasets, and applies a statistical test to assess whether performance discrepancies indicate overfitting.
The following subsections detail each component of C-BOD.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{MMLU_Eval.png}
    \caption{High-level pipeline of our parametric approach. The original dataset \(\mathcal{D}\) is passed through the distortion operator \(T_{\mu}\) to form \(\mathcal{D}_{\mu}\). Both sets are evaluated by a LLM, and differences in performance are used to quantify overfitting.}
    \label{fig:method_flow}
\end{figure}

\subsection{C-BOD rephrased dataset generation}
To systematically introduce textual variations, C-BOD utilizes a rephrasing tool, denoted as \({T}\), which uses as a distortion operator to generate a perturbed dataset \(\mathcal{D}_{\mu}\) from \(\mathcal{D}\). This operator is parameterized by \(\mu\) (temperature), which controls the extent of textual modification, ranging from low (e.g., 0.1 for minimal changes like synonym substitution) to moderate (e.g., 1.0 for rewording and sentence fragment reordering) and high (e.g., 1.5 for aggressive modifications such as question reformulation). We define:

\[
T_{\mu} : \mathcal{X} \rightarrow \mathcal{X'}
\]

Given a prompt \(x_i\), the distortion operator produces a perturbed prompt \(x_i' = T_{\mu}(x_i)\). The perturbed dataset is then constructed as:

\[
\mathcal{D}_{\mu} \;=\; \left\{\,\bigl(x_i',\,y_i\bigr)\;\middle|\;(x_i,y_i)\in \mathcal{D}\right\}
\]

Although each pair \((x_i', y_i)\) in the perutbed dataset remains semantically equivalent to \((x_i, y_i)\) in the original dataset, the textual variations introduced by \(T_{\mu}\) can disrupt purely memorized mappings from surface patterns to correct labels.
This step presented in Lines 5-6 of Algorithm~\ref{alg:cbod}.

\subsection{Evaluating the Impact of Distortion}  
To assess the impact of distortion, we evaluate \(\mathcal{E}\) using a performance function, \(\mathcal{M}\). This function evaluates \(\mathcal{E}\) based on a given ground truth or prompt \( y_i \), considering two versions of an input: the original \( x_i \in \mathcal{D} \) and the perturbed version \( x_i' \in \mathcal{D}_{\mu} \), where \( i \) denotes the index of a sample in the dataset. Specifically, \(\mathcal{M}\) is a boolean function that takes as input the model \(\mathcal{E}\) and two data pairs, \((x_i, y_i)\) and \((x_i', y_i)\), and returns whether the model performs better on the original input than on the perturbed one. The function is formulated as follows:  
% \[
% \mathcal{M}(\mathcal{E}, (x_i, y_i), (x'_i, y_i)) =
% \begin{cases}
% 1, & \text{if } P(\mathcal{E}, x_i, y_i) > P(\mathcal{E}, x'_i, y_i), \\
% 0, & \text{otherwise,}
% \end{cases}
% \]
\[
\mathcal{M}(\mathcal{E}, (x_i, y_i), (x'_i, y_i) =
\begin{cases}
    1, & \text{if } P(\mathcal{E}, x_i, y_i) \\
       & \quad > P(\mathcal{E}, (x'_i, y_i), \\
    0, & \text{otherwise.}
\end{cases}
\]


% \(\mathcal{M}(\mathcal{E}, (x_i, y_i), (x_i', y_i)) = \mathbf{1} \left( \text{score}(\mathcal{E}, x_i', y_i) > \text{score}(\mathcal{E}, x_i, y_i) \right)\)
where \( P(\mathcal{E}, x, y) \) represents the performance score of model \(\mathcal{E}\) on input \( x \) with reference to ground truth \( y \). This formulation is designed to be generalizable across different evaluation metrics and natural language understanding (NLU) tasks.  
The performance difference between the original set and the perturbed set is then calculated as:
\begin{equation}
\Delta_{\mu}, b =\sum_{i=0}^{N} \mathcal{M}(\mathcal{E}, (x_i, y_i), (x_i', y_i))
\end{equation}
The performance difference between the perturbed set and the original set is then calculated as:
\begin{equation}
c =\sum_{i=0}^{N} \mathcal{M}(\mathcal{E}, (x_i', y_i), (x_i, y_i))
\end{equation}

% \begin{equation}
% c =\sum_{i=0}^{N} \mathcal{M}(\mathcal{E}, (x_i', y_i), (x_i, y_i))
% \end{equation}

A large positive \(\Delta_{\mu}\) indicates a significant performance decline due to textual perturbations, suggesting that \(\mathcal{E}\) may be overly reliant on surface-level patterns rather than exhibiting robust generalization. Notably, this approach remains metric-agnostic, making it applicable to a wide range of evaluation measures. This step presented in Lines 7-8 of Algorithm~\ref{alg:cbod}.
\subsection{Statistical Validation}
To assess the statistical significance of performance differences, we employ McNemar’s test \cite{mcnemar1947note}, which is specifically designed for paired data. This test evaluates whether the discrepancies between two related sets of classification outcomes, correct and incorrect predictions, are significant. In our context, McNemar’s test is well-suited for comparing each pair of samples \((x_i, y_i) \in D\) and \((x_i', y_i) \in D_\mu\), we record whether \(\mathcal{E}\) classifies them correctly and aggregate into b (original is better) and c (perturbed is better) as presented in Equation 1, Equation 2.
%\begin{itemize}[leftmargin=*]
%    \item{\(b\): The number of samples correctly classified on \(\mathcal{D}\) but incorrectly classified on \(\mathcal{D}_{\mu}\). Calculated by Equation 1.}
%    \item{\(c\): The number of samples correctly classified on \(\mathcal{D}\) but incorrectly classified on \(\mathcal{D}_{\mu}\). Calculated by Equation 2.}
%\end{itemize}
The McNemar statistic is then calculated as: 
\begin{equation}    
\chi^2 = \frac{(b - c)^2}{b + c} 
\end{equation}

We derive a \(p\)-value from the chi-squared distribution (with df=1, i.e., one degree of freedom), rejecting the null hypothesis if \(p < \alpha\). A significant result with \(b>c\) indicates a genuine performance difference due to the transformation, suggesting evidence of overfitting.
This step presented in Lines 10-19 of Algorithm~\ref{alg:cbod}.



\begin{algorithm}[ht!]
\caption{Chameleon Benchmark Overfit Detector}
\label{alg:cbod}
\begin{algorithmic}[1]
\Require 
\Statex $\mathcal{D}$: Original benchmark dataset of size $N$, 
\Statex $\mathcal{E}$: LLM,
\Statex $\mu$: Distortion parameter,
\Statex $T_{\mu}$: Transformation operator,
\Statex $\mathcal{M}$: Performance function (returns 1 if the first input is better, 0 otherwise),
\Statex $\alpha$: Significance level.

\State \textbf{C-BOD Computation:}

% \Statex \Comment{Initialize counters}
\State $b,c \leftarrow 0$
% \State $c \leftarrow 0$
\State \(D_\mu \leftarrow \{\}\)
\For{each \(x_i \in \mathcal{D}\)}
    \State \(x'_i \leftarrow T_{\mu}(x_i)\)
    \State \(D_\mu \leftarrow D_\mu \cup\,   x_i'\)
    \State \(b \leftarrow b + \mathcal{M}(\mathcal{E}, (x_i, y_i), (x_i', y_i))\)
    \State \(c \leftarrow c + \mathcal{M}(\mathcal{E}, (x_i', y_i), (x_i, y_i))\)
\EndFor


\State \(\chi^2 \leftarrow \dfrac{(b-c)^2}{b+c}\)
\State \(p \leftarrow \text{p-value}(\chi^2, \text{df}=1)\)

% \State $\Delta_{\mu} \leftarrow b$

\If{\(p < \alpha\)}
    \If{\(b > c\)}
        \State \(Overfit\_Flag \leftarrow True \)
    \Else
        \State \(Overfit\_Flag \leftarrow False \)
    \EndIf
\Else
    \State \(Overfit\_Flag \leftarrow False \)
\EndIf

\State \Return \(Overfit\_Flag\)
\end{algorithmic}
\end{algorithm}

\section{Experimental Setting}
\label{sec:experiments}

In this section, we describe the experimental setup used to evaluate our overfitting detection framework. We detail the benchmark dataset, the procedure for generating perturbed inputs, the LLMs under evaluation, implementation specifics, and the evaluation metrics.

\subsection{Dataset and Rephrasing Process}

Our experiments use the MMLU benchmark~\cite{hendrycks2020measuring}, which comprises multiple-choice questions spanning 57 subjects. The broad coverage and public availability of MMLU make it an ideal candidate for assessing general knowledge and the degree to which LLMs overfit canonical prompt formats.
The MMLU dataset is distributed under the MIT License, which allows for free use, modification, and distribution as long as the original copyright notice and license terms are maintained.
We generate a perturbed versions of the original dataset to probe overfitting, following the methodology described in Section~\ref{sec:method}. We used \texttt{DeepSeek~3} to create the transformed version of each question. We generate the perturbed dataset using \(\mu = 1.0\) (the default temperature parameter).

These perturbations include synonym substitutions, sentence reordering, and the insertion of distractor phrases, while preserving the original semantic meaning and correct answers. Automated formatting checks and manual audits (performed on approximately 10\% of the samples) ensure that the integrity of the questions is maintained. For example, an original question:\textit{``The coronal suture joins the?''} is rephrased as:\textit{``Which bones does the coronal suture connect?''}. The perturbed dataset, denoted by \(\mathcal{D}_{\mu}\), is released alongside our code for reproducibility \footnote{\url{https://github.com/SeffiCohen/CBOD}}.
\subsection{Models Under Evaluation}
\label{subsec:models_eval}

Table~\ref{tab:models_overview} provides an overview of the LLMs evaluated in our experiments. Our study covers a diverse set of architectures and parameter scales ranging from 1B to 236B parameters. This broad selection enables an in-depth analysis of how both architectural choices and model scale affect robustness to prompt perturbations.

\begin{table}[ht]
\centering
\caption{Overview of the evaluated LLMs. Models are grouped by family, model version, and the number of parameters (in billions).}
\small
\begin{tabular}{lll}
\toprule
\textbf{Family} & \textbf{Version}         & \textbf{Params} \\
\midrule
\textbf{Qwen} 
         & Qwen2.5 1.5B \cite{yang2024qwen2}            & 1.5                     \\
                      & Qwen2.5 3B               & 3                       \\
                      & Qwen2.5 7B               & 7                       \\
                      & Qwen2.5 32B              & 32                      \\
                      & Qwen2.5 72B              & 72                      \\
\midrule
\textbf{Llama 3}  
       & Llama 3.2 1B \cite{dubey2024llama}            & 1                       \\
                      & Llama 3.2 3B             & 3                       \\
                      & Llama 3.1 8B             & 8                       \\
\midrule
\textbf{Gemma} 
          & Gemma 2 2B \cite{team2024gemma}              & 2                       \\
                      & Gemma 7B                 & 7                       \\
                      & Gemma 27B                & 27                      \\
\midrule
\textbf{Phi} 
         & Phi 3.5 4B \cite{abdin2024phi}               & 4                       \\
                      & Phi 4 15B                & 15                      \\   
\midrule
\textbf{DeepSeek} 
    & DeepSeek 7B \cite{bi2024deepseek}             & 7                       \\
                      & DeepSeek V2 16B          & 16                      \\
                      & DeepSeek 236B            & 236                     \\
\midrule
\textbf{Yi} 
   & Yi 6B \cite{young2024yi}              & 6                       \\
                      & Yi 9B           & 9                      \\

\midrule
\textbf{Others}       & Apollo2 7B \cite{zhu2024apollo}              & 7                       \\
                      & Aquila 7B \cite{zhang2024aquila2}               & 7                       \\
                      & Bloomz 7B \cite{zhu2023enhancing}               & 7                       \\
                      & Falcon 7B  \cite{almazrouei2023falcon}              & 7                       \\
                      & Starling 7B \cite{zhu2024starling}             & 7                       \\
                      & Jetmoe 8B \cite{shen2024jetmoe}                & 8                       \\
                      & GLM 4 9B \cite{glm2024chatglm}                & 9                       \\
                      & Mistral 8B \cite {jiang2023mistral}              & 8                       \\
\bottomrule
\end{tabular}

\label{tab:models_overview}
\end{table}


\subsection{Implementation Details}

All experiments were executed under standardized conditions to ensure reproducibility and fair comparisons:
\begin{enumerate}[nosep, label=(\arabic*)]
    \item \textbf{Inference Environment:} Most models were accessed via the HuggingFace transformers library using RTX 6000 GPU. DeepSeek 236B model was evaluated using the official API.
    \item \textbf{Dataset Rephrasing Prompt:}  
    We instruct the rephrasing tool using the following prompt to generate an alternative version of each question while preserving its original meaning and correct answer:  “Rephrase the following question without changing its context or the correct answer: \{question\}”
    
    \item \textbf{Query Prompt:}  
    For every query, we construct a standardized input by prepending a fixed instruction to the original MMLU question. Importantly, the multiple-choice options remain identical between the original and the rephrased forms. The fixed instruction is: \\ “Select the best answer from the given options. Respond with only the letter corresponding to the correct choice.  
    \\[1mm]Question: \{question\}”
    

    
    %\item \textbf{Decoding Strategy:} We employed greedy decoding or a short beam-search strategy to focus responses on selecting the appropriate option. A mild repetition penalty (e.g., 1.2) was applied where supported.
    %\item \textbf{Runtime Constraints:} Each query was limited to a maximum token length of 256 tokens, ensuring that responses remained within the expected range for MMLU questions.
\end{enumerate}

\subsection{Evaluation Metrics}

We assess model performance by comparing the original dataset, \(\mathcal{D}\), with its perturbed counterpart, \(\mathcal{D}_{1.0}\), using the following metrics:

\textbf{Correct Predictions and Accuracy:} For each dataset, we report the number of correct answers and the corresponding accuracy, defined as
    \[
    \text{Accuracy} = \frac{\#\text{Correct Predictions}}{\#\text{Total Samples}}.
    \]
    
\textbf{Absolute and Percentage Performance Difference:} The absolute difference in the number of correct answers between \(\mathcal{D}\) and \(\mathcal{D}_{1.0}\) is denoted by \(\Delta_{1.0}\); we also report the relative difference.
 \textbf{Statistical Significance:} McNemar’s test is applied on the paired predictions to determine whether the performance gap is statistically significant (\(p < 0.05\))


\subsection{Reproducibility}
C-BOD source code and datasets, including scripts for data pre-processing, perturbation generation, perturbed datasets, model evaluation, and statistical analysis, are publicly available\footnote{\url{https://github.com/SeffiCohen/CBOD}}. This ensures that our experiments can be independently replicated and verified.



\section{Results}
\label{sec:results}

\subsection{Overall Performance}
As shown in Table~\ref{tab:results}, most models (20 out of 26) exhibit a noticeable drop in performance on the rephrased test set compared to the original, reinforcing our motivation that these LLMs overfit to the standard MMLU format. Notably, the \texttt{Llama 1B, Llama 3B} models maintain relatively stable accuracy, suggesting they are less susceptible to overfitting. We also observed that \texttt{Falcon 7B}, \texttt{DeepSeek 7B}, \texttt{Qwen 2.5 3B} and \texttt{Jetmoe 8B} show statistically insignificant differences, likely due to their lower baseline accuracy. McNemar's test confirms that the performance declines observed in most models are statistically significant. Notably, no model shows a significant improvement when inputs are rephrased. This indicates that the C-BOD method reliably uncovers model vulnerabilities rather than occasionally yielding unintentional performance gains.

Across all evaluated models, the average drop in accuracy was 2.15\%, and when considering only the models with statistically significant differences, this drop increased to 2.72\%.

\begin{table*}[htbp]
\centering
\caption{Comparison of LLM performance on the original and perturbed MMLU datasets ($\mu=1.0$). The table shows the number of correct answers on each dataset, accuracy, the absolute and percentage performance difference ($\Delta_{1.0}$), statistical significance ($p<0.05$), and whether the model performed better on the original or perturbed dataset.  Models are sorted by parameter count (ascending).}
\footnotesize
\begin{tabular}{
    l      % Model Name
    l      % Model Family
    c      % #Params (B)
    c      % Orig. Correct
    c      % Orig. Acc.
    c      % Rephr. Correct
    c      % Rephr. Acc.
    c      % Direct Diff
    c      % % Diff
    l      % Sig. Ind (Yes/No)
    l      % Better Perf.
}
\toprule
\makecell{\textbf{Model} \\ \textbf{Name}} &
\makecell{\textbf{Model} \\ \textbf{Family}} &
\makecell{\textbf{Par} \\ \textbf{(B)}} &
\makecell{\textbf{$\mathcal{D}$} \\ \textbf{Correct}} & 
\makecell{\textbf{$\mathcal{D}$} \\ \textbf{Accuracy}} & 
\makecell{\textbf{$\mathcal{D}_{1.0}$} \\ \textbf{Correct}} & 
\makecell{\textbf{$\mathcal{D}_{1.0}$} \\ \textbf{Accuracy}} &
\makecell{\textbf{\#} \\ \textbf{\(\Delta_{1.0}\)}} &
\makecell{\textbf{\%} \\ \textbf{\(\Delta_{1.0}\)}} &
\makecell{\textbf{Sig.}} &
\makecell{\textbf{Better} \\ \textbf{Perf.}} \\
\midrule
Llama 3.2 1B            & Llama    & 1    & 3799  & 26.98 & 3802  & 27.00 & -3   & -0.08 & No  & Not Sig \\
Gemma 2 2B              & Gemma    & 2    & 6656  & 47.28 & 6552  & 46.54 & 104  & 1.56  & Yes & Original \\
Qwen 2.5 3B             & Qwen     & 3    & 5836  & 41.45 & 5739  & 40.76 & 97   & 1.66  & No  & Not Sig \\
Llama 3.2 3B            & Llama    & 3    & 7940  & 56.40 & 7989  & 56.74 & -49  & -0.62 & No  & Not Sig \\
Phi 3.5 4B              & Phi      & 4    & 9547  & 67.81 & 9325  & 66.23 & 222  & 2.33  & Yes & Original \\
Qwen 2.5 1.5B           & Qwen     & 5    & 5137  & 36.49 & 4986  & 35.41 & 151  & 2.94  & Yes & Original \\
Yi 1.5 6B               & Yi       & 6    & 8899  & 63.21 & 8525  & 60.55 & 374  & 4.20  & Yes & Original \\
Qwen 2.5 7B             & Qwen     & 7    & 6990  & 49.65 & 6810  & 48.37 & 180  & 2.58  & Yes & Original \\
Gemma 7B                & Gemma    & 7    & 8173  & 58.05 & 8050  & 57.18 & 123  & 1.50  & Yes & Original \\
Apollo2 7B              & Apollo2        & 7    & 9547  & 67.81 & 9146  & 64.96 & 401  & 4.20  & Yes & Original \\
Aquila 7B               & Aquila        & 7    & 4586  & 32.57 & 4475  & 31.78 & 111  & 2.42  & Yes & Original \\
Bloomz 7B               & Bloomz        & 7    & 6251  & 44.40 & 6133  & 43.56 & 118  & 1.89  & Yes & Original \\
Falcon 7B               & Falcon        & 7    & 3733  & 26.51 & 3718  & 26.41 & 15   & 0.40  & No  & Not Sig \\
Starling 7B             & Starling        & 7    & 8364  & 59.41 & 8179  & 58.09 & 185  & 2.21  & Yes & Original \\
DeepSeek 7B             & DeepSeek & 7    & 6049  & 42.96 & 6077  & 43.16 & -28  & -0.46 & No  & Not Sig \\
Llama 3.1 8B            & Llama    & 8    & 6290  & 44.68 & 6169  & 43.82 & 121  & 1.92  & Yes & Original \\
Mistral 8B              & Mistral  & 8    & 6928  & 49.21 & 6691  & 47.52 & 237  & 3.42  & Yes & Original \\
Jetmoe 8B               & Jetmoe        & 8    & 6162  & 43.77 & 6132  & 43.55 & 30   & 0.49  & No  & Not Sig \\
GLM 4 9B                & GLM      & 9    & 9674  & 68.71 & 9346  & 66.38 & 328  & 3.39  & Yes & Original \\
Yi 9B                   & Yi       & 9    & 9345  & 66.38 & 9180  & 65.20 & 165  & 1.77  & Yes & Original \\
Phi 4 15B               & Phi      & 15   & 10776 & 76.54 & 10530 & 74.79 & 246  & 2.28  & Yes & Original \\
DeepSeek V2 16B         & DeepSeek & 16   & 7466  & 53.03 & 7358  & 52.26 & 108  & 1.45  & Yes & Original \\
Gemma 27B               & Gemma    & 27   & 10300 & 73.16 & 9903  & 70.34 & 397  & 3.85  & Yes & Original \\
Qwen 2.5 32B            & Qwen     & 32   & 11262 & 80.00 & 10816 & 76.82 & 446  & 3.96  & Yes & Original \\
Qwen 2.5 72B            & Qwen     & 72   & 11456 & 81.37 & 11081 & 78.71 & 375  & 3.27  & Yes & Original \\
DeepSeek 236B           & DeepSeek & 236  & 10648 & 75.63 & 10292 & 73.10 & 356  & 3.34  & Yes & Original \\
\bottomrule
\end{tabular}

\label{tab:results}
\end{table*}





\subsection{Relationship Between Model Size and Overfit Detection}
Figure \ref{fig:scatter_log_fit} illustrates the scatter plot of the percentage performance difference versus the number of parameters, with a red dashed line representing the logarithmic fit (\(\Delta_{1.0} = 0.6318 \cdot \ln\bigl(\text{\# Params}\bigr) + 0.7920\)). The significant log-linear relationship indicates that the performance difference increases with model size in a logarithmic fashion, suggesting diminishing returns as the number of parameters grows.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.42\textwidth]{logparams.png}
    \caption{Scatter plot of the performance difference ($\Delta_{1.0}$) versus the number of model parameters (log scale). A logarithmic trendline is shown. Different colors represent different model families, highlighting how scaling affects robustness to perturbations.}
    \label{fig:scatter_log_fit}
\end{figure}
Figure~\ref{fig:scatter_diff_params} plots the percentage performance difference (\(\Delta_{1.0}\)) for $\mu=1.0$ against the number of model parameters, with separate plots for models below and above 10B parameters (different x-axis scales). The data reveals a positive trend: larger models tend to exhibit greater performance degradation under textual perturbations. For example, models in the Gemma family show a progressive increase in  \(\Delta_{1.0}\) with higher parameter counts, while Llama models maintain low \(\Delta_{1.0}\) values across scales. The dotted trend line further highlights this relationship.


\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.8\textwidth]{diffvsparams.png}
    \caption{Performance difference ($\Delta_{1.0}$) versus the number of model parameters for models with (a) fewer than and (b) more than 10 billion parameters.  A trendline is shown, and different colors represent different model families, illustrating how model scale within each family relates to the impact of perturbations.}
    \label{fig:scatter_diff_params}
\end{figure*}



\subsection{Relationship Between Model Accuracy and Overfit Detection}

Figure~\ref{fig:scatter_diff_acc} examines the relationship between baseline accuracy on the original prompts and the corresponding percentage difference in performance when evaluated on rephrased inputs. The plot clearly indicates that models with higher original accuracy tend to experience larger declines when exposed to prompt perturbations. For example, a model achieving over 80\% accuracy on the original set shows one of the largest  \(\Delta_{1.0}\) values, while models with lower baseline accuracy exhibit only minor, often statistically insignificant, differences.

This observation highlights a paradox in current LLM evaluation: models that perform exceptionally well on standard benchmarks may be capitalizing on dataset-specific cues rather than demonstrating robust language understanding. The positive correlation between original accuracy and  \(\Delta_{1.0}\) underscores the need to carefully interpret high benchmark scores, as they might mask underlying vulnerabilities to prompt variations.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.42\textwidth]{difvsacc.png}
    \caption{Scatter plot showing  \(\Delta_{1.0}\) for $\mu=1.0$ against the original accuracy of the model. Models within the same family are marked with the same color.}
    \label{fig:scatter_diff_acc}
\end{figure}

\begin{table*}[ht]
\centering
\caption{Examples of how rephrasing affects LLM performance, illustrating potential overfitting to specific phrasing in the original MMLU dataset. The table shows original and rephrased questions, along with an explanation of why the model's prediction changed. The examples are from Qwen2.5 (32B parameters).}
\small % Set font size to small for compactness
\renewcommand{\arraystretch}{1.1} % Increase row height for readability
\begin{tabular}{p{1.6cm}|p{3.4cm}|p{3.4cm}|p{6.0cm}}
\hline
\textbf{Subject} & \textbf{Original Question} & \textbf{Rephrased Question} & \textbf{Why the Model Was Wrong?} \\
\hline

Professional Law &
“If the defendant is \textbf{prosecuted} for the man’s murder, he will most \textbf{likely be found}...” &
“If the defendant is \textbf{charged} with the man’s murder, what is the \textbf{most probable} outcome?” &
In legal contexts, terms like “prosecuted” and “found guilty/not guilty” are tied to specific legal standards. The rephrased question is more open-ended, leading the model to discuss outcomes like plea bargaining instead of focusing on the legal verdict. \\
\hline

Moral Disputes &
“Of the following social \textbf{problems} that could result from a genetic supermarket, which does Singer think is the least \textbf{serious}?” &
“Which of the following social \textbf{issues} arising from a genetic supermarket does Singer consider to be the least \textbf{concerning}?” &
The word “problems” was changed to “issues,” altering the model’s interpretation. “Issues” can broaden the context of "problems", causing the model to incorrectly interpret which concerns are least serious. \\
\hline

College Chemistry &
“Which of the following statements is not a \textbf{reason} why tetramethylsilane is used as a 1H chemical shift reference?” &
“Which of the following statements does not \textbf{explain} why tetramethylsilane is used as a reference for 1H chemical shifts?” &
The model may have overfit to the structure of the original question, particularly the phrase “is not a reason why,” as it directly signals the correct retrieval path. The rephrased version, with slight syntactic adjustments disrupts this memorization, leading to incorrect retrieval. \\
\hline

World Religions &
“\textbf{When} did the first Jaina temples \textbf{appear}?.” &
“\textbf{At what point in time} were the initial Jaina temples \textbf{established}?” &
The rephrased question shifts key terms (“When” to “At what point in time”), obscuring historical framing. The LLM fails to map this modified phrasing to the original temporal context. \\
\hline

\end{tabular}
\label{tab:examples}
\end{table*}

%\subsection{Summary of Results}
%In summary, our results indicate that:
%\begin{itemize}
%    \item Most LLMs show statistically significant performance drops when benchmark prompts are rephrased, confirming that many models overfit to the canonical MMLU format.
%    \item Larger models are generally more sensitive to such perturbations, suggesting that increased model complexity may come with an increased reliance on surface-level cues.
%    \item Models with higher baseline accuracy tend to exhibit greater degradation under rephrasing, revealing that exceptional performance on standard benchmarks may partially result from memorization of specific prompt structures.
%\end{itemize}

These findings underscore the importance of evaluating LLMs under varied prompt formulations to ensure that improvements in benchmark performance reflect genuine advances in language understanding rather than overfitting.

\section{Discussion}
\label{sec:discussion}

\subsection{Why Do LLMs Overfit?}

Table \ref{tab:examples}  highlights cases where LLMs answer the original questions correctly but fail on the rephrased versions. The failures suggest potential overfitting, where models overly rely on surface-level cues, memorized patterns, or specific terminologies. Overfitting in this context occurs because the model tends to associate certain question formats or keywords directly with answers instead of generalizing underlying concepts. Common root causes include shifts in terminology, subtle changes in phrasing that alter the semantic scope, and dependence on memorized patterns from training data.

\subsection{Forget What You Know About LLM Evaluation}
Ideally, LLMs should exhibit resilience when faced with variations in prompt wording and structure. In other words, robust LLMs are expected to maintain their performance regardless of how a question is phrased, thereby reflecting true language understanding rather than mere memorization. However, our experiments reveal a contrary trend: models that score highly on standard benchmarks often display heightened sensitivity to even minor alterations in prompt formulation. This behavior suggests that such models have implicitly overfitted to the specific linguistic patterns and structures characteristic of these benchmarks. As a result, when these surface-level cues are modified, performance declines, a phenomenon that underscores the paradox between high benchmark accuracy and genuine generalization. %Our findings advocate for evaluation methodologies that compel models to rely on deeper semantic understanding, effectively encouraging them to "forget" the memorized prompt patterns in favor of more flexible reasoning.






\paragraph{Agnosticism to Benchmark Set.}
Although we used MMLU as a demonstration, our approach is inherently dataset-agnostic. It can be applied to any benchmark by simply adapting the performance metric used to compare the original samples with their rephrased counterparts. 


\section{Conclusion}
\label{sec:conclusion}

In this paper, we introduced a novel approach for detecting overfit to benchmarks datasets in LLMs by applying parametric transformations to these datasets. Our method revealed that many models rely heavily on surface features of public test sets, leading to significant performance drops when these features are altered. This finding underscores a critical insight: what appears to be robust performance may, in fact, be largely driven by memorization rather than true generalization.

We demonstrated the effectiveness of our approach across multiple LLM families. Notably, larger models tend to exhibit more pronounced performance declines under perturbation, while certain models (such as Llama) show greater stability. These observations suggest that training strategies and architectural choices play a significant role in mitigating overfitting, prompting a necessary rethinking of how we evaluate and benchmark LLMs.

By providing a practical, dataset-agnostic framework, our work equips the community with a powerful tool to uncover overfitting and to drive the development of benchmarks that better capture genuine generalization. Incorporating these parametric transformations into the evaluation process not only exposes hidden vulnerabilities in current LLMs but also suggests a way for the creation of more resilient models that can adapt to the evolving challenges of language tasks.

\section{Limitations}

While C-BOD serves as a promising framework for detecting overfitting in LLMs and has successfully identified overfitting in most evaluated models, it remains subject to several limitations.
First, our approach primarily targets textual rephrasings that preserve semantic content. Consequently, it may overlook deeper forms of overfitting, such as factual inaccuracies or logical inconsistencies, which may require more specialized probing techniques.
Moreover, incorporating \(\mu\)-based transformations into the training or fine-tuning loop can significantly increase computational cost. Iteratively rephrasing large datasets and retraining with multiple \(\mu\) values imposes a heavy resource burden, which may not be feasible for LLMs or under restricted computational budgets. Future work should investigate more lightweight or partial-integration strategies.
In summary, while C-BOD provides an effective means of detecting surface-level overfitting, further advancements are necessary to enhance its efficiency, scalability, and ability to capture more nuanced forms of model overfitting.




\section*{Acknowledgements}
We used ChatGPT-4o for editing the language and refining the presentation of the text in this paper. The authors affirm that all research content and ideas are their own, and they take full responsibility for the final submitted manuscript.


\bibliography{custom}

\end{document}


old table with few miu

\begin{table*}[h!]
\centering
\caption{Comparison of LLM performance at two levels of text adjustment $\mu \in \{0.5, 1.0\}$. 
Columns for each $\mu$ include the number of correct answers on the rephrased/attacked test set, 
the percentage difference, p-value of McNemer statistical test, and the better-performing dataset (Original, Rephrased or Not Significance between them).}
\label{tab:multi-mu-results}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l r r
                r r r r
                r r r r}
\toprule
\textbf{Model Name} & \textbf{Family} & \textbf{Params} 
& \textbf{Original Correct} 
& \multicolumn{4}{c}{$\mu = 0.5$}
& \multicolumn{4}{c}{$\mu = 1.0$} \\
\cmidrule(lr){5-8} 
\cmidrule(lr){9-12}
 & & & 
 & \textbf{Rephrased Correct} & \textbf{\%Diff} & \textbf{p-value} & \textbf{Better Performance}
 & \textbf{Rephrased Correct} & \textbf{\%Diff} & \textbf{p-value} & \textbf{Better Performance} \\
\midrule
meta-llama\_Llama-3.2-3B-Instruct           & Llama  & 3   & 8535  & 8039  & 5.81  & $8.84 \times 10^{-27}$ & Original & 7989  & -0.62 & $3.35 \times 10^{-1}$ & Not Significant Difference \\
Qwen\_Qwen2.5-3B-Instruct                   & Qwen   & 3   & 9003  & 8638  & 4.05  & $3.13 \times 10^{-16}$ & Original & 5739  & 1.66  & $1.46 \times 10^{-1}$ & Not Significant Difference \\
meta-llama\_Llama-3.2-1B-Instruct           & Llama  & 1   & 6674  & 6551  & 1.84  & $9.55 \times 10^{-3}$ & Original & 3802  & -0.08 & $9.34 \times 10^{-1}$ & Not Significant Difference \\
google\_gemma-7b                            & Gemma  & 7   & 7924  & 7751  & 2.18  & $4.60 \times 10^{-4}$ & Original & 8050  & 1.50  & $5.46 \times 10^{-3}$ & Original \\
meta-llama\_Llama-3.1-8B                    & Llama  & 8   & 8840  & 8494  & 3.91  & $2.63 \times 10^{-15}$ & Original & 6169  & 1.92  & $3.33 \times 10^{-3}$ & Original \\
microsoft\_phi-4-15B                        & Phi    & 15   & 10776 & 10171 & 5.61  & $2.25 \times 10^{-50}$ & Original & 10530 & 2.28  & $3.46 \times 10^{-12}$ & Original \\
microsoft\_Phi-3.5-mini-4B                  & Phi    & 4   & 9547  & 9072  & 4.98  & $1.81 \times 10^{-27}$ & Original & 9325  & 2.33  & $3.39 \times 10^{-8}$ & Original \\
Qwen\_Qwen2.5-1.5B-Instruct                 & Qwen   & 5   & 8238  & 7876  & 4.39  & $1.18 \times 10^{-15}$ & Original & 4986  & 2.94  & $8.94 \times 10^{-3}$ & Original \\
nvidia\_Mistral-NeMo-Minitron-8B-Instruct   & Mistral   & 8   & 9582  & 9130  & 4.72  & $6.50 \times 10^{-25}$ & Original & 6691  & 3.42  & $4.72 \times 10^{-7}$ & Original \\
Qwen\_Qwen2.5-7B-Instruct                   & Qwen   & 7   & 9878  & 9349  & 5.36  & $5.71 \times 10^{-35}$ & Original & 6810  & 2.58  & $4.36 \times 10^{-3}$ & Original \\
Qwen\_Qwen2.5-32B-Instruct-GPTQ-Int4        & Qwen   & 32  & 11262 & \textit{Y0.5} & \textit{\%0.5} & Placeholder & Placeholder & 10816 & 3.96  & $3.26 \times 10^{-37}$ & Original \\
Qwen\_Qwen2.5-72B-Instruct-GPTQ-Int4        & Qwen   & 72  & 11456 & \textit{Y0.5} & \textit{\%0.5} & Placeholder & Placeholder & 11081 & 3.27  & $2.75 \times 10^{-32}$ & Original \\
google\_gemma-2-27b-it                      & Gemma  & 27  & 10300 & \textit{Y0.5} & \textit{\%0.5} & Placeholder & Placeholder & 9903  & 3.85  & $3.31 \times 10^{-27}$ & Original \\
google\_gemma-2-2b                          & Gemma  & 2   & 6656  & \textit{Y0.5} & \textit{\%0.5} & Placeholder & Placeholder & 6552  & 1.56  & $1.54 \times 10^{-2}$ & Original \\
Deepseek-2.5                                  & DeepSeek   & 236   & 10648 & \textit{Y0.5} & \textit{\%0.5} & Placeholder & Placeholder & 10292 & 3.34  & $2.44 \times 10^{-21}$ & Original \\
\bottomrule
\end{tabular}
} % end of resizebox
\end{table*}





\begin{table*}[ht]
\centering
\caption{
\textbf{Evolution of MMLU Performance (2020--2023).}
Numbers represent reported or well-documented overall accuracy (\%) on the MMLU benchmark.
Parameter sizes (B = billions) are approximate.
Scores can vary depending on the specific prompt or evaluation setting. \yoni{I think we should mention it somewhere in a sentence and maybe bold it}
}
\label{tab:mmlu_evolution}
\begin{tabular}{llccl}
\toprule
\textbf{Year} & \textbf{Model (Version)} & \textbf{Params} & \textbf{MMLU (\%)} & \textbf{Reference} \\
\midrule
\textbf{2020} 
 & GPT-3                
 & 175B 
 & 43.9  
 & \citet{brown2020language,hendrycks2020measuring} \\

\textbf{2021} 
 & Gopher               
 & 280B 
 & 60.0  
 & \citet{rae2021scaling} \\

\textbf{2022} 
 & PaLM                 
 & 540B 
 & 56.8  
 & \citet{chowdhery2022palm} \\

\textbf{2022} 
 & Chinchilla           
 & 70B  
 & 67.6  
 & \citet{hoffmann2022chinchilla} \\

\textbf{2022} 
 & GPT-3.5 / ChatGPT    
 & --   
 & 65  
 & \citet{openai2023gpt} \\

\textbf{2023} 
 & PaLM 2               
 & --   
 & 68.9  
 & \citet{anil2023palm} \\

\textbf{2023} 
 & LLaMA                
 & 65B  
 & 65.4  
 & \citet{touvron2023llama1} \\

\textbf{2023} 
 & LLaMA 2              
 & 70B  
 & 71.2  
 & \citet{touvron2023llama} \\

\textbf{2023} 
 & Mistral (7B)         
 & 7B   
 & 70--71 
 & \citet{mistralblog2023} \\

\textbf{2023} 
 & Claude (v1)          
 & --   
 & \(\sim 74\)  
 & \citet{anthropicclaude2023,li2023comparative} \\

\textbf{2023} 
 & Claude 2             
 & --   
 & 76.5  
 & \citet{anthropicclaude2023} \\

\textbf{2023} 
 & GPT-4                
 & --   
 & 86.4  
 & \citet{openai2023gpt4} \\

\bottomrule
\end{tabular}
\end{table*}

Old version of the algorithm
\begin{algorithm}[h]
\caption{Chameleon Benchmark Overfit Detector using $\mu$-Based Transformation}
\label{alg:cbod}
\begin{algorithmic}[1]
\Require 
\Statex $\mathcal{D}$: Original benchmark dataset of size $N$, 
\Statex $\mathcal{E}$: LLM,
\Statex $\mu$: Distortion parameter,
% \Statex $\{\mu_1, \dots, \mu_k\}$: Set of distortion levels,
\Statex $T_{\mu}$: Transformation operator,
\Statex $\mathcal{M}$: Performance metric,
\Statex $\alpha$: Significance level.

\State \textbf{C-BOD computation:}

\State $P \leftarrow \mathcal{M}\bigl(\mathcal{E}, \mathcal{D}\bigr)$


\State $\mathcal{D}_{\mu} \leftarrow \{(T_{\mu}(x_i),\,y_i) : (x_i,y_i)\in \mathcal{D}\}$
\State $P_{\mu} \leftarrow \mathcal{M}\bigl(\mathcal{E}, \mathcal{D}_{\mu}\bigr)$
\State $\Delta_{\mu} \leftarrow P - P_{\mu}$
\State $\chi^2 \leftarrow \frac{(b-c)^2}{b+c}$
\State $p \leftarrow \text{p-value}(\chi^2,\text{df}=1)$
\If{$p<\alpha$}
    \If{$b>c$}
        \State \textbf{Overfitting Detected}
    \Else
        \State \textbf{No Overfitting}
    \EndIf
\Else
    \State \textbf{No Significant Difference}
% \State \Return $\{\Delta_{\mu}\}$ for all $\mu$, \text{statistical significance results.}
\EndIf
\end{algorithmic}
\end{algorithm}

