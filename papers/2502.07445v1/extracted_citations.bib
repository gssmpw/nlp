@misc{anna_bavaresco__2024,
        title={ 6. LLMs instead of Human Judges? A Large Scale Empirical Study across 20
  NLP Evaluation Tasks },
        author={ Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fernández and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael A. Hanna and Alexander Koller and André F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya Surikuchi and Ece Takmaz and Alberto Testoni },
        
        year={ 2024 },
                
        doi={ 10.48550/arxiv.2406.18403 },  
      }

@article{biderman2024emergent,
  title={Emergent and predictable memorization in large language models},
  author={Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bing_zhang__2024,
        title={ 1. Enterprise Benchmarks for Large Language Model Evaluation },
        author={ Bing Zhang and Mikio Takeuchi and Ryo Kawahara and Shubhi Asthana and M. Shamim Hossain and Ge Ren and Kate Soule and Yada Zhu },
        
        year={ 2024 },
        
       doi={ 10.48550/arxiv.2410.12857 },  
      }

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{deng2023investigating,
  title={Investigating data contamination in modern benchmarks for large language models},
  author={Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman},
  journal={arXiv preprint arXiv:2311.09783},
  year={2023}
}

@article{hendrycks2020measuring,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020},
  url={https://arxiv.org/abs/2009.03300}
}

@article{kiela2021dynabench,
  title={Dynabench: Rethinking benchmarking in NLP},
  author={Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others},
  journal={arXiv preprint arXiv:2104.14337},
  year={2021}
}

@inproceedings{kiyomaru2024comprehensive,
  title={A Comprehensive Analysis of Memorization in Large Language Models},
  author={Kiyomaru, Hirokazu and Sugiura, Issa and Kawahara, Daisuke and Kurohashi, Sadao},
  booktitle={Proceedings of the 17th International Natural Language Generation Conference},
  pages={584--596},
  year={2024}
}

@article{lee2023platypus,
  title={Platypus: Quick, cheap, and powerful refinement of llms},
  author={Lee, Ariel N and Hunter, Cole J and Ruiz, Nataniel},
  journal={arXiv preprint arXiv:2308.07317},
  year={2023}
}

@article{li2023estimating,
  title={Estimating contamination via perplexity: Quantifying memorisation in language model evaluation},
  author={Li, Yucheng},
  journal={arXiv preprint arXiv:2309.10677},
  year={2023}
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  number={5},
  year={2023}
}

@article{reimers2019sentence,
  title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
  author={Reimers, N},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={arXiv preprint arXiv:2406.01574},
  year={2024}
}

@article{yang2023rethinking,
  title={Rethinking benchmark and contamination for language models with rephrased samples},
  author={Yang, Shuo and Chiang, Wei-Lin and Zheng, Lianmin and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2311.04850},
  year={2023}
}

@article{yao2024data,
  title={Data Contamination Can Cross Language Barriers},
  author={Yao, Feng and Zhuang, Yufan and Sun, Zihao and Xu, Sunan and Kumar, Animesh and Shang, Jingbo},
  journal={arXiv preprint arXiv:2406.13236},
  year={2024}
}

@article{yuan_yu__2024,
        title={ 1. Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models },
        author={ Yuan Yu and Lili Zhao and Kai Zhang and G.Y. Zheng and Menghan Liu },
        
        year={ 2024 },
        
                
        doi={ 10.48550/arxiv.2410.13343 },  
      }

