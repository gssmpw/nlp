[
  {
    "index": 0,
    "papers": [
      {
        "key": "srivastava2022beyond",
        "author": "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\\`a} and others",
        "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liang2022holistic",
        "author": "Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others",
        "title": "Holistic evaluation of language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hendrycks2020measuring",
        "author": "Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob",
        "title": "Measuring Massive Multitask Language Understanding"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "bing_zhang__2024",
        "author": " Bing Zhang and Mikio Takeuchi and Ryo Kawahara and Shubhi Asthana and M. Shamim Hossain and Ge Ren and Kate Soule and Yada Zhu ",
        "title": " 1. Enterprise Benchmarks for Large Language Model Evaluation "
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "anna_bavaresco__2024",
        "author": " Anna Bavaresco and Raffaella Bernardi and Leonardo Bertolazzi and Desmond Elliott and Raquel Fern\u00e1ndez and Albert Gatt and Esam Ghaleb and Mario Giulianelli and Michael A. Hanna and Alexander Koller and Andr\u00e9 F. T. Martins and Philipp Mondorf and Vera Neplenbroek and Sandro Pezzelle and Barbara Plank and David Schlangen and Alessandro Suglia and Aditya Surikuchi and Ece Takmaz and Alberto Testoni ",
        "title": " 6. LLMs instead of Human Judges? A Large Scale Empirical Study across 20\nNLP Evaluation Tasks "
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wang2024mmlu",
        "author": "Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others",
        "title": "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "yuan_yu__2024",
        "author": " Yuan Yu and Lili Zhao and Kai Zhang and G.Y. Zheng and Menghan Liu ",
        "title": " 1. Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges\nin Large Language Models "
      },
      {
        "key": "chang2024survey",
        "author": "Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others",
        "title": "A survey on evaluation of large language models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "kiela2021dynabench",
        "author": "Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and others",
        "title": "Dynabench: Rethinking benchmarking in NLP"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kiyomaru2024comprehensive",
        "author": "Kiyomaru, Hirokazu and Sugiura, Issa and Kawahara, Daisuke and Kurohashi, Sadao",
        "title": "A Comprehensive Analysis of Memorization in Large Language Models"
      },
      {
        "key": "biderman2024emergent",
        "author": "Biderman, Stella and Prashanth, Usvsn and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward",
        "title": "Emergent and predictable memorization in large language models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "carlini2022quantifying",
        "author": "Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan",
        "title": "Quantifying memorization across neural language models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "deng2023investigating",
        "author": "Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman",
        "title": "Investigating data contamination in modern benchmarks for large language models"
      },
      {
        "key": "yao2024data",
        "author": "Yao, Feng and Zhuang, Yufan and Sun, Zihao and Xu, Sunan and Kumar, Animesh and Shang, Jingbo",
        "title": "Data Contamination Can Cross Language Barriers"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "yang2023rethinking",
        "author": "Yang, Shuo and Chiang, Wei-Lin and Zheng, Lianmin and Gonzalez, Joseph E and Stoica, Ion",
        "title": "Rethinking benchmark and contamination for language models with rephrased samples"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "openai2023gpt",
        "author": "OpenAI, R",
        "title": "Gpt-4 technical report. arxiv 2303.08774"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "reimers2019sentence",
        "author": "Reimers, N",
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lee2023platypus",
        "author": "Lee, Ariel N and Hunter, Cole J and Ruiz, Nataniel",
        "title": "Platypus: Quick, cheap, and powerful refinement of llms"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2023estimating",
        "author": "Li, Yucheng",
        "title": "Estimating contamination via perplexity: Quantifying memorisation in language model evaluation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "deng2023investigating",
        "author": "Deng, Chunyuan and Zhao, Yilun and Tang, Xiangru and Gerstein, Mark and Cohan, Arman",
        "title": "Investigating data contamination in modern benchmarks for large language models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhang2025uncovering",
        "author": "Mengqi Zhang and Xiaotian Ye and Qiang Liu and Shu Wu and Pengjie Ren and Zhumin Chen",
        "title": "Uncovering Overfitting in Large Language Model Editing"
      }
    ]
  }
]