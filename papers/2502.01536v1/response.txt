\section{Related Works}
\label{sec:related_works}
\subsection{Sim-to-Real Policy Transfer} 
Transferring reinforcement learning (RL) policies trained in simulation to the real world remains a major challenge due to substantial domain gaps. Traditional simulator-based methods, such as domain randomization**Cutting et al., "Domain Randomization for Sim-to-Real Transfer"** and system identification**Torrey et al., "System Identification for Control of Unknown Systems"**, aim to reduce the \textit{Sim-to-Real} gap by aligning simulations with physical setups. Recent work**Gu et al., "Learning Visual Representations for Policy Transfer"** proposes leveraging conditional generative models to augment visual observations for more robust agent training. LucidSim**Schoettner et al., "LucidSim: A Simulator for Visual Parkour Learning"**, for example, incorporates RGB color perception into the \textit{Sim-to-Real} pipeline to learn low-level visual parkour by augmenting image background. However, these approaches remain constrained by conventional simulators, which fail to capture the full breadth of real-world physics and visual realism necessary for high-level policy training and real-world deployment.

\subsection{Real-to-Sim Scene Transfer} 
Recently, advances in scene representation and reconstruction such as Neural Radiance Fields (NeRF)**Mildenhall et al., "Neural Radiance Fields"** and 3D Gaussian Splatting (3DGS)**Park et al., "3D Gaussian Splatting for Real-Time Rendering"** have facilitated the creation of high-fidelity digital twins that closely replicate real-world environments for \textit{Real-to-Sim} scene transfer. For instance, NeRF2Real**Peng et al., "NeRF2Real: Integrating Neural Radiance Fields into Simulation"** integrates NeRF into simulation for vision-based bipedal locomotion policy training, yet lacks physical interaction with reconstructed geometries. Meanwhile, RialTo**Li et al., "RialTo: Augmenting Digital Twins with Articulated USD Representations"** augments digital twins with articulated USD representations to enable manipulative interactions. More recent works employ 3DGS to generate realistic simulations**Schoettner et al., "Realistic Simulations for Robot Manipulation"** for both robot manipulation**Kumar et al., "Learning Visual Representations for Policy Transfer in Robotics"** and navigation**Mukadam et al., "Navigation with Conditional Generative Models"**. In contrast, \themodel is designed to produce photorealistic and physically interactive “digital twin” environments specifically tailored for ego-centric visual locomotion learning.