\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xspace}
\usepackage{graphicx} 
\usepackage[bookmarks=true]{hyperref}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{float}
\usepackage{stfloats}
\usepackage{balance}

% \pdfinfo{
%    /Author (Homer Simpson)
%    /Title  (Robots: Our new overlords)
%    /CreationDate (D:20101201120000)
%    /Subject (Robots)
%    /Keywords (Robots;Overlords)
% }


\begin{document}
% paper title
\def\themodel{{{VR-Robo}}\xspace} 

\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother

\title{\themodel: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion}

% You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Author Names Omitted for Anonymous Review. Paper-ID [45]}

% \author{\authorblockN{Shaoting Zhu}
% \authorblockA{IIIS, Tsinghua University\\
% Email: zhust24@mails.tsinghua.edu.cn}
% \and
% \authorblockN{Homer Simpson}
% \authorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \authorblockN{James Kirk\\ and Montgomery Scott}
% \authorblockA{Starfleet Academy\\
% San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}

\author{Shaoting Zhu$^{13*}$\quad Linzhan Mou$^{23*}$\quad Derun Li$^{34}$\quad Baijun Ye$^{12}$\quad Runhan Huang$^{13}$\quad Hang Zhao$^{123}$\vspace{0.03in}\\

$^1$IIIS, Tsinghua University\quad$^2$Galaxea AI\quad$^3$Shanghai Qi Zhi Institute\quad$^4$Shanghai Jiao Tong University\\\quad$^*$Equal contribution\vspace{0.1in}\\
\href{https://vr-robo.github.io/}{\textbf{VR-Robo.github.io}\xspace}\vspace{-0.1in}}

% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\authorblockN{Michael Shell\authorrefmark{1},
%Homer Simpson\authorrefmark{2},
%James Kirk\authorrefmark{3}, 
%Montgomery Scott\authorrefmark{3} and
%Eldon Tyrell\authorrefmark{4}}
%\authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
%\authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

% \maketitle
\twocolumn[{
\renewcommand\twocolumn[1][]{#1}
    \maketitle
    \centering
    \vspace{-5.0mm}
    \includegraphics[width=0.96\linewidth]{figures/teaser-new.pdf}
    % \vspace{-1mm}
    \captionof{figure}{\textbf{Our {\themodel} introduces a \textit{Real-to-Sim-to-Real} framework.} We generate photorealistic and physically interactive ``digital twin'' simulation environments for ego-centric visual navigation and locomotion. \themodel facilitates the RL policy training of legged agents with exploration capability in complex scenarios. Notably, our framework achieves RGB-only zero-shot sim-to-real policy transfer to the real world. \textbf{Please refer to the supplementary video for simulation and real-world experiment results}.}
    \vspace{3mm}
}]

% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=0.98\linewidth]{figuresteaser.pdf}
%     \vspace{-3mm}
%     \label{fig:teaser}
% \end{figure*}

\begin{abstract}
Recent success in legged robot locomotion is attributed to the integration of reinforcement learning and physical simulators.  
However, these policies often encounter challenges when deployed in real-world environments due to sim-to-real gaps, as simulators typically fail to replicate visual realism and complex real-world geometry. Moreover, the lack of realistic visual rendering limits the ability of these policies to support high-level tasks requiring RGB-based perception like ego-centric navigation. This paper presents a \textit{Real-to-Sim-to-Real} framework that generates photorealistic and physically interactive ``digital twin'' simulation environments for visual navigation and locomotion learning. Our approach leverages 3D Gaussian Splatting (3DGS) based scene reconstruction from multi-view images and integrates these environments into simulations that support ego-centric visual perception and mesh-based physical interactions. To demonstrate its effectiveness, we train a reinforcement learning policy within the simulator to perform a visual goal-tracking task. Extensive experiments show that our framework achieves RGB-only sim-to-real policy transfer. Additionally, our framework facilitates the rapid adaptation of robot policies with effective exploration capability in complex new environments, highlighting its potential for applications in households and factories.

% Bridging the sim-to-real gap in robot navigation and locomotion remains a critical challenge, particularly for high-level tasks that rely on RGB perception. Existing locomotion methods predominantly focus on low-level control, constrained by traditional simulators that fail to capture visual realism and real-world geometry. This paper presents a \textit{Real-to-Sim-to-Real} system that generates photorealistic and physically interactive ``digital twin'' simulation environments for visual navigation and locomotion learning. To facilitate effective sim-to-real transfer, we reconstruct high-fidelity 3D environments from multi-view images and integrate them into simulations that support ego-centric visual observation and mesh-based physical interaction. Reinforcement learning is employed in our system to perform the \textit{visual goal tracking task} while interacting with terrain mesh. Extensive experiments in both simulated and real-world scenarios demonstrate the effectiveness of our system, achieving zero-shot transfer to real-world onboard deployments with RGB-only observations. This framework offers significant potential for real-world applications, including household robotics and industrial automation.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

% 1. Application in house and factory -> need Locomotion sim-to-real in RGB images. -> challenge both physics and photo realistic. 
% 2. previous only photo realistic / only physics interactive. LucidSim (only low-level), behaviour 1k (draw usd expensive), 
% 3. We propose a RSR system both physics and photo realistic.
% 4. contribution:
% 1) system
% 2) real2sim: camera take, coordinate transfer, randomization.
% 3) sim2real deployment.

% Exploring, perceiving, and interacting with the physical real world is crucial for legged robotics, which supports many applications such as household service and industrial automation. Real-world experimentation is often challenging due to safety risks and efficiency constraints. In recent years, training in simulations~\cite{mujoco,Isaac,habitat,orbit,igibson,sapien,threedworld} has become an effective alternative to sample diverse environmental conditions for robots where they can safely explore failure cases and learn directly from their actions. However, transferring the policies trained in simulated environments to the real world remains challenging due to the  \textit{Sim-to-Real} reality gap~\cite{behavior1k,maniskill3,mujoco}.
Exploring, perceiving, and interacting with the physical real world is crucial for legged robotics, which supports many applications such as household service and industrial automation. However, conducting real-world experiments poses considerable challenges due to safety risks and efficiency constraints. Consequently, training in simulation~\cite{mujoco,Isaac,habitat,orbit,igibson,sapien,threedworld} has emerged as an effective alternative, allowing robots to experience diverse environmental conditions, safely explore failure cases, and learn directly from their actions. Despite these advantages, transferring policies learned in simulated environments to real-world remains challenging, owing to the significant \textit{Sim-to-Real} reality gap~\cite{behavior1k,maniskill3,mujoco}.

% Substantial efforts have been made to bridge this gap. Previous works in locomotion~\cite{parkour,cheng2024extreme,hoeller2024anymal,wu2023learning,humanoid} rely on cross-domain depth images as input for agent training in simulation and achieve impressive zero-shot \textit{Sim-to-Real} policy transfer in quadrupeds and humanoid robots. To further incorporate RGB-color perception into the \textit{Sim-to-Real} pipeline, LucidSim~\cite{lucidsim} uses generative models as the training data in simulation~\cite{mujoco} for visual parkour. However, both depth-based and generation-based-visual policies are constrained on low-level locomotion tasks mainly because the simulation struggles to replicate visual realism and complex real-world geometry. Recent advances in neural reconstruction techniques like NeRF~\cite{nerf} and 3DGS~\cite{3dgs} emerge as a potential solution by reconstructing \textit{Real-to-Sim} "digital twins" simulation environments from real-world data. However, most works~\cite{nerf2real,quach2024gaussian,robogs,robogsim,splatsim,rl-gsbrige} focus on enhancing simulation photorealism and limit to the physical interaction. Moreover, the agent training in these simulations often lacks environment exploration capability, limiting their deployment in real-world scenarios with dynamic interaction.
Substantial efforts have been made to narrow the \textit{Sim-to-Real} gap in legged locomotion. Previous studies have employed cross-domain depth images to train agents in simulation, achieving impressive zero-shot policy transfer for both quadruped~\cite{parkour,cheng2024extreme,hoeller2024anymal,wu2023learning} and humanoid~\cite{humanoid, long2024learning} robots. To further integrate RGB perception into the \textit{Sim-to-Real} pipeline, LucidSim~\cite{lucidsim} leverages generative models within simulation~\cite{mujoco} for visual parkour. However, these depth-based and generation-based visual policies are predominantly constrained to low-level locomotion tasks, primarily because standard simulators struggle to replicate the visual fidelity and complex geometry of real-world environments.

Recently, advanced neural reconstruction techniques such as Neural Radiance Fields (NeRF)~\cite{nerf} and 3D Gaussian Splatting (3DGS)~\cite{3dgs} have emerged as promising solutions for creating \textit{Real-to-Sim} ``digital twins'' from real-world data. Nonetheless, most existing approaches~\cite{nerf2real,quach2024gaussian,robogs,robogsim,splatsim,rl-gsbrige,vid2sim} focus on enhancing photorealism, offering limited support for physical interaction with complex terrains. Moreover, the simulations in these works often lack mechanisms for robust environment exploration, thus constraining their deployment in complex real-world scenarios that demand dynamic interaction.

% To tackle these issues, in this work, we present \themodel, a novel \textit{Real-to-Sim-to-Real} system that supports both realistic and interactive \textit{Real-to-Sim} simulation reconstruction, and RL policy training of legged robot navigation and locomotion with minimal \textit{Sim-to-Real} gap. Given the multi-view images, we first leverage planar-based~\cite{pgsr,2dgs,neusg} 3DGS~\cite{3dgs} representation and foundation model prior~\cite{da2} regularization to reconstruct geometry-consistent scenes and objects. We then propose a GS-mesh hybrid representation and create a "digital twins" simulation environment in Issac Sim with coordinate alignment, which enables ego-centric visual perception and mesh-based physical interactions. To enable robust policy training, we introduce an agent-object randomization and occlusion-aware scene composition strategy, as illuminated in \autoref{fig:random}.
To address these challenges, we introduce \themodel, a novel \textit{Real-to-Sim-to-Real} framework that enables realistic, interactive \textit{Real-to-Sim} simulation and reinforcement learning (RL) policy training for legged robot navigation and locomotion, minimizing the \textit{Sim-to-Real} gap. Given the multi-view images, we employ planar-based~\cite{pgsr,2dgs,neusg} 3DGS~\cite{3dgs} and foundation-model priors~\cite{da2} to reconstruct geometry-consistent scenes and objects. We then propose a GS-mesh hybrid representation with coordinate alignment to create a ``digital twin'' simulation environment in Isaac Sim, which supports ego-centric visual perception and mesh-based physical interactions. To enable robust policy training, we introduce an agent-object randomization and occlusion-aware scene composition strategy, as illustrated in \autoref{fig:random}.

% With the reconstructed simulation, we train the visual RL policy with a two-level asynchronous control strategy: the first step is to train the low-level locomotion policy to teach the robot to recognize and utilize terrains like slopes or stairs to reach the high platform. The policy can not handle high-level navigation tasks using RGB perception after this pre-training step. The next high-level policy training step involves exploring the environment, recognizing the target goals, and planning a reasonable path to reach the goal. The agent leverages ego-view visual observation from GS rendering, interacts with mesh in Issac Sim, and updates its policy using PPO, as shown in \autoref{fig:rl}. This policy is sufficiently robust to transfer zero-shot to RGB-only observations in the real world throughout our extensive experiments.
With the reconstructed simulation, we adopt a two-stage training strategy for the visual RL policy. In the first stage, we train a low-level locomotion policy that enables the robot to perceive and utilize terrain like slopes and stairs to reach the high platforms. However, this policy alone does not address high-level ego-centric navigation tasks with RGB inputs. In the second stage, we train a high-level policy to explore the environment, identify target goals, and plan feasible paths. The agent utilizes ego-centric visual observations rendered via GS, interacts with the mesh in Isaac Sim, and updates its policy using rewards, as depicted in \autoref{fig:rl}. Through our extensive experiments, the trained policy demonstrates sufficient robustness to be transferred zero-shot to real-world scenarios using only RGB observations.

We summarize our contributions as follows:
\begin{enumerate}
    \item \textbf{A \textit{Real-to-Sim-to-Real} framework for robot navigation and locomotion.} We propose to reduce the \textit{Sim-to-Real} gap by training RL policies within a realistic and interactive simulation environment reconstructed directly from real-world scenarios.
    \item \textbf{Photorealistic and physically interactive \textit{Real-to-Sim} environment reconstruction.} We introduce a novel pipeline for transferring real-world environments into simulation using a GS-mesh hybrid representation with coordinate alignment. We further incorporate object randomization and occlusion-aware scene composition for robust and efficient RL policy training.
    
    \item \textbf{Zero-shot \textit{Sim-to-Real} RL policy transfer.} Through extensive experimental evaluations, we demonstrate that \themodel produces effective navigation and locomotion policies in complex, real-world scenarios using RGB-only observations.
\end{enumerate}

% Developing robust robot locomotion policies that can seamlessly transition from simulation to real-world deployment has long been a cornerstone of robotic learning. Traditional sim-to-real approaches rely heavily on domain randomization and system identification to bridge the inherent discrepancies between simulated environments and their physical counterparts. While effective for certain low-level control tasks, these methods often fall short in addressing the complexities of high-level tasks involving rich visual observations and dynamic interactions.

% In this paper, we address this challenge by proposing a real-to-sim-to-real (RSR) framework for ego-centric robot locomotion. Unlike conventional sim-to-real methods, our approach reverses the pipeline by constructing photorealistic and physically interactive simulations from real-world data. Leveraging advancements in 3D Gaussian Splatting (3DGS) and hybrid GS-mesh representations, we create simulations that faithfully replicate real-world geometry and texture. This enables the generation of diverse and on-policy training data, allowing robots to learn policies in scenarios that are otherwise difficult or unsafe to replicate in physical environments.

% Our method consists of three key components. First, we use multi-view imagery to reconstruct high-fidelity 3D environments with photorealistic rendering and accurate physical interactions. Second, we integrate these environments into a scalable simulation framework, incorporating domain randomization and occlusion-aware scene composition to improve policy robustness. Finally, we deploy the trained policies on real-world robots, demonstrating zero-shot transfer capabilities in challenging locomotion tasks.

% Extensive experiments on both simulated and real-world environments validate our system's ability to minimize the sim-to-real gap. We showcase the successful training of locomotion policies for high-level tasks, such as navigating complex terrains and interacting with dynamic objects. Our contributions not only advance the state-of-the-art in robot locomotion training but also pave the way for broader applications in autonomous robotics. 

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/framework-new.pdf}
    \caption{\textbf{Our \textit{Real-to-Sim-to-Real} Framework.} We first reconstruct the geometry-consistent scenarios from the captured images with foundation model constraints. Then we build a realistic and interactive simulation environment with GS-mesh hybrid representation and occlusion-aware composition and randomization for policy training. Finally, we zero-shot transfer the RL policy trained in simulation into the real robot for ego-centric navigation and visual locomotion.}
    \vspace{-5mm}
    \label{fig:framework}
\end{figure*}

\section{Related Works}
\label{sec:related_works}
\subsection{Sim-to-Real Policy Transfer} 
Transferring reinforcement learning (RL) policies trained in simulation to the real world remains a major challenge due to substantial domain gaps. Traditional simulator-based methods, such as domain randomization~\cite{gensim, dynamic_random, domain, dynamics_revisit, cad2rl, synthetic} and system identification~\cite{dynamic_motor, agile_loco, dexterous, closing}, aim to reduce the \textit{Sim-to-Real} gap by aligning simulations with physical setups. Recent work~\cite{layout, drivedreamer, semantically, scaling} proposes leveraging conditional generative models to augment visual observations for more robust agent training. LucidSim~\cite{lucidsim}, for example, incorporates RGB color perception into the \textit{Sim-to-Real} pipeline to learn low-level visual parkour by augmenting image background. However, these approaches remain constrained by conventional simulators, which fail to capture the full breadth of real-world physics and visual realism necessary for high-level policy training and real-world deployment.

\subsection{Real-to-Sim Scene Transfer} 
Recently, advances in scene representation and reconstruction such as Neural Radiance Fields (NeRF)~\cite{nerf} and 3D Gaussian Splatting (3DGS)~\cite{3dgs} have facilitated the creation of high-fidelity digital twins that closely replicate real-world environments for \textit{Real-to-Sim} scene transfer. For instance, NeRF2Real~\cite{nerf2real} integrates NeRF into simulation for vision-based bipedal locomotion policy training, yet lacks physical interaction with reconstructed geometries. Meanwhile, RialTo~\cite{reconciling} augments digital twins with articulated USD representations to enable manipulative interactions. More recent works employ 3DGS to generate realistic simulations~\cite{discoverse2024} for both robot manipulation~\cite{robogs, robogsim, splatsim, rl-gsbrige} and navigation~\cite{quach2024gaussian,vid2sim}. In contrast, \themodel is designed to produce photorealistic and physically interactive “digital twin” environments specifically tailored for ego-centric visual locomotion learning.


\section{Task definition}
\begin{figure}[ht]
    \centering
    % \vspace{-2mm}
    \includegraphics[width=0.98\linewidth]{figures/task_def.pdf}
    \vspace{-2mm}
    \caption{\textbf{Task Definition.} The robot needs to reach the target cone of a specified color, relying on RGB observations and proprioception.}
    \vspace{-5mm}
    \label{fig:task_def}
\end{figure}
\label{sec:task}

% As shown in ~\autoref{fig:task_def}, the task is defined as reaching the target cone of a specified color within a limited time horizon. We design two different scenes, each featuring a terrain in the middle, consisting of a high table and a slope connecting the ground to the platform. The positions of the robot and cones are randomized at the start of each test. The robot starts at $\mathbf{p}_\text{robot} \sim \mathcal{U}(\mathbb{P}_\text{robot})$ with a random orientation $\theta_\text{robot} \sim \mathcal{U}(\mathbb{\theta}_\text{robot})$. Each scene contains three cones with identical shape and texture but different colors $\mathcal{C} = \{\text{red}, \text{green}, \text{blue}\}$. The cones are placed in three regions: left, middle, and right, with one cone per region at a random position $\mathbf{p}_\text{cone} \sim \mathcal{U}(\mathbb{P}_\text{cone}^\text{region})$. At the beginning, the robot may not face the target cone, requiring exploration to locate it. Also, if the target cone is on the table, the robot must find a correct path to get onto the table.
Our designed task requires both high-level understanding and navigation, as well as low-level motion control. Our framework is applicable to tasks that demand both high-level decision-making and low-level execution. As shown in~\autoref{fig:task_def}, the task is defined as reaching the target cone of a specified color within a limited time horizon. We consider two distinct scenes, each featuring a terrain in the center composed of a high table and a slope connecting the ground to the platform.  
At the start of each trial, the robot's initial position is sampled from $\mathbf{p}_\text{robot} \sim \mathcal{U}(\mathbb{P}_\text{robot})$, and its orientation is similarly randomized as $\theta_\text{robot} \sim \mathcal{U}(\mathbb{\theta}_\text{robot})$. Each scene contains three cones with identical shapes and textures but different colors $\mathcal{C}=\{\text{red, green, blue}\}$. These cones are placed in three designated regions: left, middle, and right, with one cone per region at a random position $\mathbf{p}_\text{cone} \sim \mathcal{U}(\mathbb{P}_\text{cone}^\text{region})$.  
Since the robot may not initially face the target cone, it must explore the environment to identify and locate the correct cone. Additionally, if the target cone is placed atop the table, the robot must navigate up the slope to access the platform.  

\section{A Real-to-Sim-to-Real System for
Ego-Centric Robot Locomotion}
\label{sec:real2sim2real}
\subsection{Geometry-Consistent Reconstruction}
\label{subsec:real2sim}
Given multi-view RGB images of a scene with corresponding camera poses from COLMAP~\cite{colmap}, we aim to generate a photorealistic and physically interactive simulation environment for agent policy training, thereby minimizing the sim-to-real gap. To this end, we first reconstruct a high-quality, geometry-aware 3D environment using planar-based 3D Gaussian Splatting (GS)~\cite{3dgs} with geometric regularization.

\vspace{1mm}
\noindent \textbf{Gaussian-based 3D Scene Representation.} 3D Gaussian Splatting (3DGS) represents the scene as a set of Gaussian primitives. In particular, each Gaussian splat $\mathcal{G}_i(\mathbf{x})$ is parameterized by its mean $\mu_i \in \mathbb{R}^3$, a 3D covariance matrix $\Sigma_i \in \mathbb{R}^{3 \times 3}$, an opacity term $o_i$, and a color term $c_i$, as follows:
\begin{equation}
    \mathcal{G}_i(\mathbf{x})=\exp \left(-\frac{1}{2}\left(\mathbf{x}-\mu_i\right)^T \Sigma_i^{-1}\left(\mathbf{x}-\mu_i\right)\right),
\end{equation}
During optimization, $\Sigma_i$ is reparametrized using a scaling matrix $S_i \in \mathbb{R}^3$ and a rotation matrix $R_i \in \mathbb{R}^{3 \times 3}$ as $\Sigma_i=R_i S_i S_i^T R_i^T$. The color value of a pixel $\mathbf{c}(x)$ can be rendered through a volumetric alpha-blending~\cite{3dgs} process:
\begin{equation}
    \mathbf{c}(x)=\sum_{i \in N} T_i c_i \alpha_i(\mathbf{x}), \quad T_i=\prod_{i=1}^{i-1}\left(1-\alpha_i(\mathbf{x})\right).
\end{equation}
where $\alpha_i(\mathbf{x})=o_i \mathcal{G}_i(\mathbf{x})$ denotes the alpha of the Gaussian $\mathcal{G}_i$ at $\mathbf{x} \in \mathbb{R}^3$. Meanwhile, the view-dependent color $c_i$ of $\mathcal{G}_i$ is represented by its spherical harmonics $(\mathrm{SH})$~\cite{pulsar} coefficients.

\vspace{1mm}
\noindent \textbf{Flattening 3D Gaussians for Geometric Modeling.} Inspired by~\cite{2dgs, neusg}, we flatten the 3D Gaussian ellipsoid with covariance $\Sigma_i=R_i S_i S_i^T R_i^T$ into 2D flat planes to enhance the scene geometry modeling by minimizing its shortest axis scale $S_i=$ $\operatorname{diag}\left(s_1, s_2, s_3\right)$:
\begin{equation}
    \mathcal{L}_{\mathtt{scale}}=\frac{1}{N} \sum_{i \in N}\left\|\min \left(s_1, s_2, s_3\right)\right\|,
\end{equation}
where $N$ denotes the total number of planar-based Gaussian splats. Following PGSR~\cite{pgsr}, we utilize the plane representation to render both the plane-to-camera distance map $\mathcal{D}$ and the normal map $\mathcal{N}$, then convert them into unbiased depth maps by intersecting rays with the corresponding planes as:
\begin{equation}
    \boldsymbol{D}(p)_\mathtt{render}=\frac{\mathcal{D}}{\mathcal{N}(p)K^{-1} \tilde{p}}.
\end{equation}
where $p$ is the 2D position on the image plane. $\tilde{p}$ denotes the homogeneous coordinate of $p$, and $K$ is the camera intrinsic.

\vspace{1mm}
\noindent \textbf{Foundation Model Geometric Prior Constraints.} In textureless regions (e.g., ground and walls), photometric loss tend to be insufficient. To improve surface reconstruction in these areas, we employ an off-the-shelf monocular depth estimator~\cite{da2} to provide dense depth prior. We address the inherent scale ambiguity between the estimated depths and the actual scene geometry by comparing them to sparse Structure-from-Motion (SfM) points, following~\cite{dn-splatter, depth-reg, monosdf}. Specifically, we align the scale of the monocular depth map $\boldsymbol{D}_\mathtt{mono}$ with the SfM points projected depth map $\boldsymbol{D}_\mathtt{sfm}$ by solving the per-image scale $\boldsymbol{s}$ and shift $\boldsymbol{t}$ parameters using linear regression:
\begin{equation}
    \hat{\boldsymbol{s}}, \hat{\boldsymbol{t}}=\underset{\boldsymbol{s}, \boldsymbol{t}}{\arg \min } \sum_{p \in \boldsymbol{D}_\mathtt{sfm}}\left\|\left(\boldsymbol{s} * \boldsymbol{D}(p)_{\mathtt{mono}}+ \boldsymbol{t} \right)-\boldsymbol{D}(p)_{\mathtt{sfm}}\right\|_2^2.
\end{equation}
Finally, we use the re-scaled monocular depth map $\hat{\boldsymbol{s}} * \boldsymbol{D}(p)_{\mathtt{mono}} + \hat{\boldsymbol{t}}$ to regularize the rendered depth map $\boldsymbol{D}(p)_\mathtt{render}$. Similarly, we adopt an off-the-shelf monocular normal estimator~\cite{dsine} to provide dense normal regularization to the rendered normal map $\mathcal{N}_\mathtt{render}$ for accurate geometric modeling.

\vspace{1mm}
\noindent \textbf{Multi-view Consistency Constraints.} Inspired by~\cite{massively,geo-neus}, a patch-based normalized cross-correlation (NCC) loss is applied between two gray renders $\{\mathbf{I_\mathtt{render}}, \hat{\mathbf{I}}_\mathtt{render}\}$ to force the multi-view photometric consistency: 
\begin{equation}
    \mathcal{L}_{\mathtt{mv}} = \frac{1}{\|\mathcal{P}\|} \sum_{\mathbf{p} \in \mathcal{P}} \sum_{p \in \mathbf{p}} ( 1 - NCC(\hat{\mathbf{I}}(\mathcal{H} p)_\mathtt{render}, \mathbf{I}(p)_\mathtt{render})).
\end{equation}
where $\mathcal{P}$ is the set of all patches extracted from $\mathbf{I}_\mathtt{render}$ and $\mathcal{H}$ is the homography matrix bwteen the two frames.

\subsection{Building Realistic and Interactive Simulation}
\label{sec:hybrid-simulation}
To enable the agent-environment interaction, we integrate a GS-mesh hybrid scene representation into the Isaac Sim with coordinate alignment. We further leverage agent-object randomization and occlusion-aware scene composition to advance robust visual policy training.

\vspace{1mm}
\noindent \textbf{GS-mesh Hybrid Representation.} To enhance the realism and interactivity of our simulation environment, we introduce a hybrid scene representation that combines our Gaussian-based model with its corresponding scene mesh primitives. 

\noindent 1) The \textbf{\textit{Gaussian}} representation generates photorealistic visual observations from the robot’s ego-centric viewpoints. Because the robot camera differs from the camera used for 3D scene reconstruction, we first align the intrinsic parameters between \textit{Sim} and \textit{Real} by calibrating the robot camera’s focal length and distortion parameters. We then obtain the camera extrinsic within the simulation coordinate system by retrieving the ego-view position and quaternion-based orientation from Issac.

\noindent 2) The \textbf{\textit{Mesh}} representation facilitates physical interaction and precise collision detection. We render the depth for each input view and leverage a Truncated Signed Distance Function (TSDF)~\cite{tsdf} fusion algorithm to build the corresponding TSDF field. We then extract the mesh from this TSDF field to enable physically accurate interactions within the simulation.

\vspace{1mm}
\noindent \textbf{Coordinate Alignment.} To align the reconstructed COLMAP coordinate system with the Isaac Sim environment, we first compute the homogeneous transformation matrix \( T_{\mathtt{homo}} \in \mathbb{R}^{4 \times 4} \) by manually matching four non-coplanar points as correspondence. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/trans_eg.pdf}
    \vspace{-2mm}
    \caption{\textbf{Coordinate Alignment.} We first compute the transformation for a regular reference object and then apply this transformation to the irregular object of interest. }
    \vspace{-3mm}
    \label{fig:4pcs}
\end{figure}

Specifically, as shown in \autoref{fig:4pcs}, we use objects with regular shapes to transform coordinate systems. For instance, to obtain the transformation matrix for the blue cone, we first manually define a rectangular block \( \mathcal{B} \) with the correct coordinate system and size as the reference object. Then, using the four-point registration method, we align the block's coordinates with the reconstructed COLMAP system. The transformation matrix \( T_{\text{block}} \) is determined as:
\begin{equation}
    T_{\text{block}} = \arg\min_T \sum_{i=1}^{4} \| T \cdot \mathbf{p}_{i}^{\mathcal{B}} - \mathbf{p}_{i}^{\text{COLMAP}} \|^2,
\end{equation}

Finally, the transformation matrix for the cone, \( T_{\text{cone}} \), is determined by translating the cone's position relative to the aligned rectangular block:
\begin{equation}
    T_{\text{cone}} = T_{\text{block}} \cdot \Delta \mathbf{p}_{\text{cone}}.
\end{equation}

\vspace{1mm}
\noindent \textbf{Gaussian Attributes Adjustment.}
After applying the coordinate transformation, we must adjust the Gaussian attributes for accurate photometric rendering. Specifically, we first decompose $T_{\mathtt {homo }}$ into its rotation component $R_{\mathtt {homo }} \in \mathbb{R}^{3 \times 3} $, translation vector $t_{\mathtt {homo }} \in \mathbb{R}^3$, and scale factor $s_{\mathtt {homo }} \in \mathbb{R} $. In addition, the 3D covariance matrix $\Sigma$ of the Gaussian points is parameterized by a scaling matrix $S \in \mathbb{R}^3$ and a rotation matrix $R \in \mathbb{R}^{3 \times 3}$, such that $\Sigma=R S S^T R^T$. The mean $\mu$, scaling $S$ and rotation $R$ are adjusted as follows:
\begin{equation}
    \mu^{\prime} = R_\mathtt{homo} \mu + t_\mathtt{homo},
\end{equation}
\begin{equation}
    S^{\prime} = S + log(s_\mathtt{homo}),
\end{equation}
\begin{equation}
    R_\mathtt{norm} = \frac{R_\mathtt{homo}}{s_\mathtt{homo}} \quad \Sigma^{\prime} =  R_\mathtt{norm} \Sigma R_\mathtt{norm}^{T},
\end{equation}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figures/rl.pdf}
    \vspace{-4mm}
    \caption{\textbf{RL policy training in the reconstructed simulation environment.} The agent leverages the ego-view GS photorealistic rendering as visual observations and interacts with the GS-extracted mesh in the Issac Sim environment. The agent receives the \texttt{image feature} $I_a$ from ViT~\cite{vit} encoder, \texttt{proprioception} $P_a$ from simulator sensors, a task-specific \texttt{RGB command} $C_a$, and last action as input, using an asymmetric actor-critic LSTM structure to output the \texttt{velocity command} $V_\text{raw\_cmd}$ for low-level policy control.}
    \vspace{-5mm}
    \label{fig:rl}
\end{figure*}

Since the spherical harmonics (SHs) for the 3D Gaussians are stored in world space (i.e., the COLMAP coordinate system), view-dependent colors change when the Gaussians rotate. To accommodate different rotations, we first extract the Euler angles $\alpha, \beta, \gamma$ from the rotation matrix $R_\mathtt{homo}$ and construct the corresponding Wigner D-matrix $D$. We then apply $D$ to rotate the SH coefficients. Formally, each band $\ell$ of the SH coefficients (of length $2 \ell+1$ ) transforms as:
\begin{equation}
    \mathbf{C}^{(\ell)^{\prime}}=D_{\ell}(\alpha,-\beta, \gamma) \mathbf{C}^{(\ell)}.
\end{equation}
where $\mathbf{C}^{(\ell)}$ is the vector of the spherical-harmonic coefficients for degree $\ell$ and $D_{\ell}$ is the $(2 \ell+1) \times(2 \ell+1)$ Wigner $D$-matrix.

\vspace{1mm}
\noindent \textbf{Occlusion-Aware Composition and Randomization.} To generate 3D object assets for environment randomization, we first capture high-quality multi-view images of real-world objects and reconstruct them using the aforementioned pipeline. With the resulting mesh and 3D Gaussians, we employ an interactive mesh editor to obtain both the 3D bounding box and its corresponding transformation matrix $T_{\mathtt {bbox }} \in \mathbb{R}^{4 \times 4}$. Since the object mesh and the 3D Gaussians share the same COLMAP coordinate system, we use the mesh bounding box to crop the object Gaussians and merge them into the environment coordinate space according to the transformation below:
\begin{equation}
    T_\mathtt{obj}^\mathtt{COLMAP-env} = T_\mathtt{sim}^\mathtt{COLMAP-env} \cdot T_\mathtt{COLMAP-obj}^\mathtt{sim} \cdot T_\mathtt{bbox}.
\end{equation}
As shown in ~\autoref{fig:random}, the merged object Gaussians is synchronized with the object mesh in Issac and can be rendered jointly with the environment Gaussians via a volumetric alpha-blending~\cite{3dgs} process. By incorporating z-buffering~\cite{z-buffer} on the opacity attributes of the Gaussian points, our method achieves occlusion-aware scene composition with accurate visibility. This randomization strategy substantially increases the diversity of the environment, thereby enabling more robust and scalable agent training.

\begin{figure}[ht]
    \centering
    % \vspace{-1mm}
    \includegraphics[width=1\linewidth]{figures/random_example.pdf}
    \vspace{-3mm}
    \caption{\textbf{Agent-object Randomization and Scene Composition.} At the beginning of each episode, we randomly sample the positions for the robot and three cones in the Isaac Sim environment (upper row). We synchronously merge the agent and object Gaussians into the environment and compose for joint rendering (lower row). Both mesh and Gaussian rendering results shown here are in the Bird's-Eye View (BEV) with the same camera pose.}
    \vspace{-7mm}
    \label{fig:random}
\end{figure}

\subsection{RL in Reconstructed Simulation}
\label{subsec:sim2real}
% \noindent \textbf{Setup.}
Given the simulation environment generated in Section~\ref{sec:hybrid-simulation}, the next stage of \themodel focuses on training a robust locomotion policy via reinforcement learning, as illustrated in \autoref{fig:rl}. This policy is designed to address diverse goal-tracking tasks under varying environmental conditions. Due to the limited onboard computational resources of real robots, we adopt a two-level asynchronous control strategy: the high-level policy operates at 5~Hz, while the low-level policy executes at 50~Hz. The high-level policy communicates with the low-level policy via velocity commands, represented as $V_{\text{cmd}} = (V_x, V_y, V_{yaw})$. All experiments are conducted primarily on the Unitree Go2 quadruped robot.

\vspace{1mm}
\noindent \textbf{Low-Level Policy.}
The first step is to train the low-level policy, similar to previous locomotion works~\cite{wu2023learning, rrw}. The low-level policy takes velocity commands $V_{\text{cmd}} = (V_x, V_y, V_{\text{yaw}})$ along with robot proprioception as input, and outputs the desired joint positions. The actor network is composed of a lightweight LSTM layer combined with multiple MLP layers. Further details on the state and action spaces, reward design, network architecture, and terrain settings can be found in Appendix~\ref{sec:low-level}. 

The low-level policy adopted here enables the quadruped robot to climb slopes and stairs measuring up to 15~cm in height. However, unlike previous parkour works~\cite{parkour, cheng2024extreme, hoeller2024anymal}, this policy does not allow the robot to directly climb onto terrain taller than 30~cm. While existing frameworks could be extended to handle higher climbs, our approach instead focuses on teaching the robot to recognize and utilize slopes or stairs to reach high platforms. This strategy is particularly useful in real-world scenarios featuring taller obstacles, where direct climbing or jumping may be infeasible for any policy.


\vspace{1mm}
\noindent \textbf{High-Level Policy.}
We freeze the previously trained low-level policy and train the high-level policy independently. The high-level policy is trained using reinforcement learning (PPO) within our GS-mesh hybrid simulation environment.

\textbf{\textit{State Space}}:
The high-level policy actor network receives four types of inputs:
\texttt{1) RGB Image Feature ($I_a$)}: We employ a frozen, pre-trained Vision Transformer (ViT)~\cite{vit} to extract features from RGB images. ViT's attention mechanism is well-suited for detecting positions and specific colors. This approach compresses the input size significantly and accelerates training.
\texttt{2) RGB Command ($C_a$)}: A RGB vector indicating the desired color of the target cone. For example, $[1, 0, 0]$ represents the red cone. 
\texttt{3) Last Action}: The action output by the policy during the previous timestep.
\texttt{4) Proprioception ($P_a$)}: This includes the robot’s base angular velocity, projected gravity, joint positions, and joint velocities.
We use an asymmetric actor-critic structure. The critic's observation includes the actor's observation (with noise removed) as well as the robot's world coordinate position and orientation, and the target cone's world coordinate position. This design improves the estimation of value function and enhances the training process.

\textbf{\textit{Action Space}}:
The actor outputs the raw velocity command, $V_{\text{raw\_cmd}} = (V_x, V_y, V_{\text{yaw}})$. This raw command will pass through a $\tanh$ layer and scaled by the velocity range $V_{\text{max\_cmd}} = (V_{\text{max\_x}}, V_{\text{max\_y}}, V_{\text{max\_yaw}})$ to ensure safety. Note that the $\tanh$ layer and velocity range scaling are applied outside the actor network. The resulting velocity command $V_{\text{cmd}} = (V_x, V_y, V_{\text{yaw}})$ is sent to the low-level policy, enabling the robot to move. 

\textbf{\textit{Reward Design}}:
\label{subsec:reward_highlevel}
The total reward consists of two main categories: \textbf{Task Rewards} $r_T$ and \textbf{Regularization Rewards} $r_R$. $r_T$ include \texttt{Reach\_goal}, \texttt{Goal\_dis}, \texttt{Goal\_dis\_z}, and \texttt{Goal\_heading}. To be specific, the robot receives a reward if it comes close enough to the goal. 
\begin{equation}
    r_\text{reach\_goal} = 
    \begin{cases} 
        R_\text{max}, & \text{if } \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2 \leq \epsilon, \\ 
        0, & \text{otherwise}.
    \end{cases}
\end{equation}

\texttt{Goal\_dis} is based on the change in the Euclidean distance to the goal between consecutive time steps:
\begin{align}
    r_\text{goal\_dis} &= d_\text{prev} - d_\text{current}, \\
    d &= \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2.
\end{align}

\texttt{Goal\_dis\_z} is based on the change in the vertical (z-axis) distance to the goal:
\begin{align}
    r_\text{goal\_dis\_z} &= d_{z,\text{prev}} - d_{z,\text{current}}, \\
    d_z &= \vert z_\text{robot} - z_\text{goal} \vert.
\end{align}

\texttt{Goal\_heading}: This reward encourages the robot to face the goal by minimizing the yaw angle error, defined as a linear function of the yaw difference:
\begin{equation}
    r_\text{goal\_heading} = -\vert \psi_\text{robot} - \psi_\text{goal} \vert,
\end{equation}
where $\psi_\text{robot}$ is the robot's current yaw angle, $\psi_\text{goal}$ is the desired yaw angle toward the goal.

In addition, regularization rewards are incorporated to further refine the robot’s performance and ensure stable and efficient behavior. It contains \texttt{Stop\_at\_goal}, \texttt{Track\_lin\_vel}, \texttt{Track\_ang\_vel}, and \texttt{Action\_l2}. The detailed mathematical expressions and weights of all rewards can be found in Appendix~\ref{sec:high-level}.


\textit{Training Process}:
We randomly sample the positions and orientations of the robot and cones at the beginning of each episode. The robot's camera poses and cone poses are obtained from Isaac Sim and sent to the aligned and editable 3D Gaussians mentioned above to render a photorealistic image. The policy then outputs an action based on this image, which is applied in Isaac Sim to interact with the mesh.

\textit{Domain Randomization}:
\label{sec:domain_random}
We apply domain randomization to reduce the sim-to-real gap. This includes three key components:
\texttt{1) Camera Pose Noise}: Before requesting the 3D Gaussians to render an image, we add uniform noise to the camera pose extracted from Isaac Sim.
% Specifically, we apply uniform noise of scale $[1~\text{cm}, 1~\text{cm}, 1~\text{cm}]$ in $[x, y, z]$ and $[1^\circ, 1^\circ, 2^\circ]$ in $[\text{roll}, \text{pitch}, \text{yaw}]$.
\texttt{2) Image Augmentation}: We randomly apply brightness, contrast, saturation, and hue adjustments to the image. This is followed by Gaussian blur to simulate camera blur caused by robot movement. Additionally, we randomly add Gaussian noise to the image with a small probability ($p = 0.05$).
\texttt{3) Image Delay:} We randomly apply a 0- or 1-step delay to the rendered images.

\section{Experiments}

\label{sec:experiments}
To evaluate the effectiveness of \themodel, we construct two \textit{Real-to-Sim} environments by reconstructing 3D scenes from phone-captured image collections. These two environments capture an indoor scene with complex terrains. Our experiments are designed to evaluate two main aspects of \themodel: 1) its ability to reconstruct realistic and interactive simulation environments with GS-mesh hybrid representation, supporting the locomotion policy training through both visual observations and mesh interactions; 2) its capability to minimize the \textit{Sim-to-Real} gap when deployed to the real world. We conducted extensive experiments in both simulation and real-world settings to validate these capabilities.

\subsection{Experiment Setup}
\label{subsec:setup}

\noindent \textbf{Real-to-Sim Reconstruction.}
% We separately reconstruct two different indoor environment with specific terrains. We also randomize the scene using three cones with  color red, green and blue. We use Apple iPad and iPhone to take the images: (Detail of image taken) (Coordinate align)
We reconstruct two distinct indoor room environments, each characterized by specific terrains. We also randomize the environments by incorporating three cones colored red, green, and blue. We use iPads or iPhones to take photos, which are easily available. Detailed procedures for image acquisition and coordinate alignment are provided in the Appendix~\ref{sec:recon}.

\vspace{1mm}
\noindent \textbf{Simulated Locomotion Training.}
% We use Isaac Sim on a single 48GB NVIDIA RTX 4090D GPU for policy training. For the low-level policy, we deploy 4,096 quadruped robot agents for parallel training. The training process consists of 80,000 iterations from scratch. The low-level policy operates at a frequency of 50 Hz. For the high-level policy, we deploy 64 quadruped robot agents and train for 8,000 iterations from scratch. The high-level policy operates at a frequency of 5 Hz. The entire training process takes approximately 3 days to complete. We use the ``vit\_tiny\_patch16\_224'' model as our ViT vision encoder, removing the final classification head. The Isaac Sim simulator and the 3DGS renderer are connected via TCP network.
We conduct policy training in Isaac Sim using a single 48GB NVIDIA RTX 4090D GPU. For the low-level policy, we instantiate 4,096 quadruped robot agents in parallel and train for 80,000 iterations from scratch. The low-level policy operates at a frequency of 50 Hz. In contrast, the high-level policy is trained from scratch over 8,000 iterations with 64 quadruped agents. The high-level policy operates at a frequency of 5 Hz. The entire training process requires approximately three days to complete. For visual encoding, we employ the ``vit\_tiny\_patch16\_224'' Vision Transformer model, omitting its final classification head. The Isaac Sim simulator and the 3DGS rasterization-based renderer communicate via a TCP network.

\vspace{1mm}
\noindent \textbf{Sim-to-Real Deployment.}
We deploy our policy on the Unitree Go2 quadruped robot, which is equipped with an NVIDIA Jetson Orin Nano (40 TOPS) as the onboard computer. We use ROS2~\cite{ros2} for communication between the high-level policy, low-level policy, and the robot. Both policies run onboard, with the high-level policy operating at 5 Hz and the low-level policy running at 50 Hz. The robot receives desired joint positions from the low-level policy for PD control ($K_p = 40.0$, $K_d = 1.0$). We use an Insta360 Ace camera to capture RGB images. The camera captures images at a resolution of $320 \times 180$. After calibration, it has a horizontal field of view (FOVX) of 1.5701 radians and a vertical field of view (FOVY) of 1.0260 radians.

\subsection{Simulation Experiments}

We conduct comparison and ablation experiments with various baselines. For comparison experiments, 
\begin{itemize}
    \item \textbf{Imitation Learning:} We collect 60 different real-world trajectories via teleoperation and train the policy through supervised learning using regression optimization.
    \item \textbf{Depth Policy:} Following a previous parkour approach~\cite{parkour}, we replace the RGB input with a depth image and adopt a simple CNN as the vision encoder.
\end{itemize}
For ablation experiments,
\begin{itemize}
    \item \textbf{Texture Mesh:} We incorporate a SuGaR~\cite{sugar}-reconstructed textured mesh into Isaac Sim and render RGB observations directly from this mesh.
    \item \textbf{CNN Encoder:} We replace the ViT vision encoder with a CNN image encoder, specifically ``MobilenetV3'' \cite{howard2019searching}, and remove the last classification layer.
\end{itemize}

\begin{figure}[ht]
    \centering
    % \vspace{-2mm}
    \includegraphics[width=0.9\linewidth]{figures/texture_depth.pdf}
    \vspace{-2mm}
    \caption{\textbf{Drawbacks of the Texture Mesh and Depth Input Images.} \textit{(Left: agent ego-centric observation; Right: simulation environment in bird-eye-view.)} The mesh is cropped for use in the simulator due to the large original size. As shown in the area marked by the red box, the mesh exhibits bulges in texture-less regions, such as the ground and walls, which hinder the robot's movement. Additionally, controlling the lighting in the simulator poses a significant challenge. The depth map is rendered directly from the simulator but cannot distinguish the color of the cones.}
    \vspace{-5mm}
    \label{fig:texture_depth}
\end{figure}

% We evaluated all methods in a 'red cone reaching' task within Scene 1. The red cone can be sampled in three areas: left (on the ground), middle (on the terrain), and right (on the ground). We randomly generate 150 sets of different cone positions. Note that we determine these tasks before conducting any experiments and maintain the same setup for all methods to ensure consistency.
The visualize explanations of Texture Mesh and Depth are shown in \autoref{fig:texture_depth}. We evaluated all methods in a `red cone reaching' task within Scene 1. The red cone is randomly placed in one of three regions: left (ground), middle (terrain), or right (ground). We generate 150 distinct cone placements, specified prior to any experiments, and apply the same setup for all methods to ensure a fair comparison.

% For evaluation metrics, we report the success rate (SR) and average reaching time (ART). An episode is considered successful if the robot reaches within 0.25 m of the target cone at least once within the maximum time of 15 seconds. If the robot successfully reaches the target cone, we record the reaching time. If not, the reaching time for that episode is recorded as the maximum time of 15 seconds. Finally, we compute the average of the reaching time.
To measure performance, we report both Success Rate (SR) and Average Reaching Time (ART). An episode is deemed successful if the robot comes within 0.25 m of the red cone at any point within a 15-second window. For successful episodes, we record the time taken to reach the cone; otherwise, we assign the maximum time of 15 seconds. The ART is then computed as the mean reaching time across all trials.

\begin{table}[H]
    \vspace{-2mm}
    \centering
    \caption{Comparison results in the simulation setting.}
    \vspace{-1mm}
    \label{tab:simulation_comparison}
    \setlength\tabcolsep{8pt}
    \fontsize{8}{10}\selectfont
    \begin{tabular}{p{3.0cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}}
        \toprule
        \textbf{Method} & \textbf{SR} $\uparrow$&  \textbf{ART (s)} $\downarrow$\\
         \midrule
         \textbf{Ours} & \textbf{100.00\%} & \textbf{4.94} \\
         Imitation Learning (IL) & 8.67\% & 14.01 \\
         Depth Policy & 6.00\% & 14.13 \\
         \bottomrule
    \end{tabular}
    \vspace{-6mm}
\end{table}

\begin{table}[H]
    % \vspace{2mm}
    \centering
    \caption{Ablation results in the simulation setting.}
    \vspace{-1mm}
    \label{tab:simulation_ablation}
    \setlength\tabcolsep{8pt}
    \fontsize{8}{10}\selectfont
    \begin{tabular}{p{3.0cm}<{\centering}|p{1.5cm}<{\centering}|p{1.5cm}<{\centering}}
        \toprule
        \textbf{Method} & \textbf{SR} $\uparrow$&  \textbf{ART (s)} $\downarrow$\\
         \midrule
         \textbf{Ours} & \textbf{100.00\%} & \textbf{4.94} \\
         Texture Mesh & 22.00\% & 12.73 \\
         CNN Encoder & 54.67\% & 10.04 \\
         \bottomrule
    \end{tabular}
    \vspace{-3mm}
\end{table}

\begin{table*}[htbp] 
    \vspace{2mm}
    \centering
    \caption{Comparison and ablation experimental results in the real-world setting.}
    \vspace{-1mm}
    \label{tab:real-world}
    \setlength\tabcolsep{8pt}
    \fontsize{8}{10}\selectfont
    % \begin{tabular}{c|ccc|ccc}
    \begin{tabular}{p{3.5cm}<{\centering} | p{1.6cm}<{\centering} | p{1.3cm}<{\centering} p{1.3cm}<{\centering} p{1.3cm}<{\centering}|p{1.1cm}<{\centering} p{1.1cm}<{\centering} p{1.1cm}<{\centering}}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Exteroception}} &\multicolumn{3}{c|}{\textbf{Success Rate} $\uparrow$}&  \multicolumn{3}{c}{\textbf{Average Reaching Time (s)} $\downarrow$}\\
         & & Easy& Medium& Hard& Easy& Medium& Hard\\
         \midrule
        \textbf{Ours} & RGB & \textbf{100.00\%} & \textbf{93.33\%} & \textbf{100.00\%} & \textbf{4.96} & \textbf{6.28} & \textbf{9.09} \\
        Imitation Learning (IL) & RGB & 0.00\% & 0.00\% & 0.00\% & 15.00 & 15.00 & 15.00 \\
        Depth Policy & Depth & 6.67\% & 0.00\% & 0.00\% & 14.33 & 15.00 & 15.00 \\
        SARO \cite{saro} & RGB & 66.67\% & 26.67\% & 0.00\% & 46.49 & 57.24 & 60.00 \\
        \midrule
        Texture Mesh & RGB & 20.00\% & 6.67\% & 0.00\% & 12.90 & 14.90 & 15.00 \\
        CNN Encoder & RGB & 73.33\% & 66.67\% & 6.67\% & 9.10 & 11.41 & 14.90 \\
        w/o Domain Randomization & RGB & 53.33\% & 6.67\% & 0.00\% & 10.04 & 14.76 & 15.00 \\
         \bottomrule
    \end{tabular}
    \vspace{-5mm}
\end{table*}

The comparison and ablation results are shown in \autoref{tab:simulation_comparison} and \autoref{tab:simulation_ablation} separately. Our method consistently attains higher performance across all evaluation metrics by a large margin. In particular, the Imitation Learning (IL) baseline suffers from insufficient data samples and lacks policy exploration. The depth policy fails to distinguish between different cones and cannot be effectively trained, resulting in behavior akin to random actions. Similarly, the texture mesh is unable to generate realistic images, and the CNN encoder struggles to extract precise features from the images. These findings collectively confirm the effectiveness and robustness of our proposed approach in diverse experimental setups.

\subsection{Real-World Experiments}

% We further conduct two baseline methods for real-world experiments in addition to the method mentioned above (these can only be compared in the real world): 
In addition to the methods mentioned above, we conduct two additional baseline methods for real-world experiments. These methods are specifically designed in the real-world setting and cannot be directly compared in simulation:
\begin{itemize}
    \item \textbf{SARO \cite{saro}:} SARO enables the robot to navigate across 3D terrains leveraging the vision-language model (VLM).
    \item \textbf{w/o Domain Randomization:} We exclude domain randomization as described in Section~\ref{sec:domain_random}.
\end{itemize}

We follow the `red cone reaching' task setting in simulation and conduct experiments in real-world Scene $\Rmnum{1}$. We categorize the task into three levels of difficulty based on the robot's starting position: \textit{easy}, \textit{medium}, and \textit{hard}. For \textit{easy} tasks, the robot starts with the target cone directly visible. For \textit{medium} tasks, the robot needs to turn to a certain degree to see the target cone. For \textit{hard} tasks, the robot starts very far from the cone and faces a nearly opposite direction to the target cone. We randomly select 3 sets of cone positions, with each set containing one easy, one medium, and one hard task, as shown in \autoref{fig:task_difficulty}. For each task, we repeat the experiment 5 times. We then calculate the Success Rate (SR) and Average Reaching Time (ART). For SARO, the maximum time is extended to 60 seconds due to the long inference time required by the Vision-Language Model (VLM). In the real robot experiments, we consider the task successful if the robot makes contact with the target cone.

\begin{figure}[ht]
    \centering
    \vspace{-2mm}
    \includegraphics[width=1.0\linewidth]{figures/task_difficulty.pdf}
    \vspace{-6mm}
    \caption{Tasks of different difficulties (\textit{easy}, \textit{medium}, and \textit{hard}) and different positions (left, middle, and right).}
    \vspace{-3mm}
    \label{fig:task_difficulty}
\end{figure}

% The quantitative results are shown in \autoref{tab:real-world}. Our method achieves the highest success rates across all difficulty levels. Furthermore, it consistently records the shortest average reaching times, demonstrating its efficiency. In addition to the methods mentioned above, SARO achieves moderate success on easy tasks (66.67\%) but performs poorly on medium and hard tasks. This is primarily due to the lack of historical context and exploration capability in complex scenarios. If the robot cannot see the goal initially, it is likely to fail. Additionally, the absence of domain randomization significantly reduces the model's effectiveness, primarily due to the sim-to-real gap in RGB observations.
The quantitative results are presented in \autoref{tab:real-world}. Our method achieves the highest success rates across all difficulty levels and consistently records the shortest average reaching times, demonstrating its efficiency and robustness. Among the other methods, SARO achieves moderate success on easy tasks (66.67\%) but performs poorly on medium and hard tasks. This underperformance is primarily attributed to the lack of historical context and limited exploration capability in complex scenarios. If the robot cannot initially see the goal, it is unlikely to succeed. Furthermore, the absence of domain randomization significantly reduces SARO's effectiveness, primarily due to the large sim-to-real gap with RGB-only observations.

We further conduct qualitative experiments under varying conditions, including various target cone colors, changes in lighting conditions, different scenes, random interference, background variations, and training on the Unitree G1 humanoid robot, as shown in \autoref{fig:quali_exp}. A detailed demonstration is provided in the supplementary materials. These experiments highlight the robustness of our method, showcasing its adaptability to a wide range of environments and conditions.

\begin{figure}[ht]
    \centering
    \vspace{-3mm}
    \includegraphics[width=1.0\linewidth]{figures/quali_exp.pdf}
    \vspace{-7mm}
    \caption{Qualitative experiments under varying conditions.}
    \vspace{-5mm}
    \label{fig:quali_exp}
\end{figure}

\section{Limitations and Conclusion} 
\label{sec:conclusion}
\subsection{Limitations} 
\textbf{1) Simple Scene and Task:} while \themodel facilitates easier real-to-sim transfer through its GS-mesh hybrid representation and coordinate alignment workflow, our current simulation environment is restricted to indoor scenes with static terrains. Future work could explore the simulation of large-scale and complex scenarios, such as factory settings or outdoor street environments, incorporating both static and dynamic obstacles to support a wider range of downstream tasks. \textbf{2) Workflow Efficiency}: despite the accelerated visual locomotion policy training afforded by the real-time rendering capabilities of 3DGS, training a large number of agents in parallel remains relatively slow due to the serial rendering pipeline across agents and the significant communication costs between the simulator and the GS renderer. This also limits training on finding cones of different colors at the same time. Future efforts may focus on designing CUDA-based parallel randomization and rendering pipelines, as well as more efficient interaction methods between the GS and the simulator. \textbf{3) Model Pretraining}: as discussed in ~\autoref{subsec:setup}, \themodel currently requires approximately three days to train a reinforcement learning (RL) locomotion policy from scratch for each task. We anticipate that more effective pretraining methods could mitigate this time bottleneck.

\subsection{Conclusion} 
We introduce \themodel, a real-to-sim-to-real framework designed to train visual locomotion policies within a photorealistic and physically interactive simulation environment, thereby minimizing the sim-to-real gap. Extensive experimental results demonstrate that by integrating real-world environments into the simulator with randomization, agents successfully learn robust and effective policies for challenging high-level tasks and can be zero-shot deployed to real-world scenarios. In future work, we aim to extend our framework to larger-scale and more complex environments, incorporating diverse agents such as humanoid robots and mobile manipulators.
\balance

% \section*{Acknowledgments}

%% Use plainnat to work nicely with natbib. 

% \clearpage

\bibliographystyle{plainnat}
\bibliography{main}
\balance

\clearpage
\appendix

\begin{table*}[bp]
    \centering
    \caption{\label{tab:reward}Reward Function}
    \renewcommand{\arraystretch}{1.3}
    \fontsize{10}{12}\selectfont
    \begin{tabular}{cccc}
        \toprule
        \textbf{Type} & \textbf{Item} & \textbf{Formula} & \textbf{Weight}\\ \midrule
        \multirow{2}{*}{\textbf{Task}} 
        & Lin vel tracking & $\exp\left(-\|\mathbf{v}_{t,xy}^\mathrm{des} - \mathbf{v}_{t,xy}\|_2 / 0.25\right)$ & $1.5$ \\ 
        & Ang vel tracking & $\exp\left(-\|\omega_{t,z}^\mathrm{des} - \omega_{t,z}\|_2 / 0.25\right)$ & $0.75$ \\ \midrule
        
        \multirow{7}{*}{\textbf{Regularizations}} 
        & Lin vel (z) & $\|\mathbf{v}_{t,z}\|_2^2$ & $-2.0$ \\
        & Ang vel (x, y) & $\|\omega_{t,x}\|_2^2 + \|\omega_{t,y}\|_2^2$ & $-0.05$ \\
        & Joint vel & $\|\dot{\mathbf{q}}\|_2^2$ & $-3\times10^{-4}$ \\
        & Joint acc & $\|\ddot{\mathbf{q}}\|_2^2$ & $-2.5\times10^{-7}$ \\
        & Joint torque & $\|\tau\|_2^2$ & $-2\times10^{-4}$ \\
        & Joint pos limits & $\sum \max(0, |\mathbf{q}| - q_\mathrm{lim})$ & $-0.75$ \\
        & Joint vel limits & $\sum \max(0, |\dot{\mathbf{q}}| - \dot{q}_\mathrm{lim})$ & $-0.02$ \\ \midrule
        
        \multirow{4}{*}{\textbf{Style}} 
        & Feet in air & $\displaystyle 
 \sum_{i=0}^{3}\left(\mathbf{t}_{air,i}-0.3\right)+10\cdot\min\left(0.5-\mathbf{t}_{air,i},0\right)$ & $0.05$ \\ 
        & Stand still & $\displaystyle (\sum^{12}_{i=1}{\lvert q-q_{default}\rvert)\cdot(\Vert V\Vert_2<0.1)}$ -0.5 \\ 
        & Balance & $\| F_{feet,0}+F_{feet,2}-F_{feet,1}-F_{feet,3} \|_2$ & $-2\times10^{-5}$ \\ 
        \midrule
        
        \textbf{Alive} & Alive & $1$ & $3.0$ \\ 
        \bottomrule
    \end{tabular}
    \vspace{-5mm}
\end{table*}

In \textit{Appendix}, we further illustrate the technical details of training and reconstruction. \textit{Appendix} mainly has these sections:
\begin{itemize}
    \item \textbf{A. Details of Low-Level Policy: }the state space, action space, reward design, network architecture and terrain settings of low-level policy.
    \item \textbf{B. Reward Details of High-Level Policy: }the reward design of the high-level policy..
    \item \textbf{C. Real-to-Sim Reconstruction Details: }details of the image collection and scene reconstruction.
\end{itemize}

\subsection{Details of Low-Level Policy}
\label{sec:low-level}
\noindent \textbf{State Space.}
The observation of low-level policy using only proprioception $\boldsymbol{p}_t$ and velocity command $\boldsymbol{v}_t$. Proprioception $\boldsymbol{p}_t$ contains gravity vector and base angular velocity from IMU, joint positions, joint velocities, and last action. Velocity command is the desired $\{V_\text{x}, V_\text{y}, V_\text{yaw}\}$ of the robot.

\noindent \textbf{Action Space.}
The action space $\boldsymbol{a}_t\in\mathbb{R}^{12}$ is the desired joint positions of 12 joints.

\noindent \textbf{Reward Design.}
The reward function consists of four components: task reward $r_t^T$, regularization reward $r_t^R$, style reward $r_t^S$, and alive reward $r_t^A$. The total reward is the sum of these components, given by $r_t = r_t^T + r_t^R + r_t^S + r_t^A$. The detailed formulas are provided in \autoref{tab:reward}.


\noindent \textbf{Network Architecture.}
Both the actor and critic use a simple LSTM network followed by an MLP.
\begin{table}[H]
    \vspace{-1mm}
    \renewcommand{\arraystretch}{1.3}
    \caption{\label{tab:network}Network architecture details}
    \vspace{-1mm}
    \centering
    \begin{tabular}{c|cccc}
        \toprule
        \textbf{Network} & \textbf{Type} & \textbf{Input} & \textbf{Hidden layers} & \textbf{Output}\\ \midrule
        Actor RNN & LSTM+MLP & $\boldsymbol{p}_t$, $\boldsymbol{v}_t$ & [512, 256] & $\boldsymbol{a}_t$\\
        Critic RNN & LSTM+MLP & $\boldsymbol{p}_t$, $\hat{\boldsymbol{v}}_t$ & [512, 256] & $\boldsymbol{V}_t$\\
        \bottomrule
    \end{tabular}
    \vspace{-2mm}
\end{table}

\noindent \textbf{Terrain Settings.}
We adopt the ``terrain curriculum'' approach as proposed in \cite{rudin2022learning}. Specifically, we design a total of 200 unique terrains, arranged in a $10 \times 20$ grid layout. These terrains are categorized into $6$ distinct types, each containing 20 variations that progressively increase in difficulty. Each terrain spans an area of $8 \times 8$ meters.

\begin{table}[htbp]
    \vspace{-1mm}
    \renewcommand{\arraystretch}{1.3}
    \caption{\label{tab:terrain}Terrain Categories and Specifications}
    \vspace{-1mm}
    \centering
    \begin{tabular}{c|c|c}
        \toprule
        \textbf{Category} & \textbf{Key Parameters} & \textbf{Columns} \\ \midrule
        Ascending stairs & Step height: $0.05 \sim 0.2$ m & 4 \\
        Descending stairs & Step height: $0.05 \sim 0.2$ m & 4 \\
        Ascending ramps & Inclination angle: $0^\circ \sim 45^\circ$ & 2 \\
        Descending ramps & Inclination angle: $0^\circ \sim 45^\circ$ & 2 \\
        Random boxes & Box height: $0.025 \sim 0.1$ m & 4 \\
        Rough terrain & Noise amplitude: $0.01 \sim 0.06$ m & 4 \\
        \bottomrule
    \end{tabular}
    \vspace{-2mm}
\end{table}

\subsection{Reward Details of High-Level Policy}
\label{sec:high-level}
The total reward consists of two main categories: \textbf{Task Rewards} $r_T$ and \textbf{Regularization Rewards} $r_R$. In addition to rask rewards $r_T$ in Section~\ref{subsec:reward_highlevel}, Regularization rewards $r_R$ include:

\texttt{Stop\_at\_goal}: Penalize the velocity command (action) when the robot is near the goal:
\begin{equation}
    r_\text{stop\_at\_goal} = -\Vert \mathbf{a} \Vert^2\cdot( \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2 \leq \epsilon).
\end{equation}

\texttt{Track\_lin\_vel}: The exponential error between the robot base's linear velocity and the velocity command. This encourages the robot to avoid getting stuck:
\begin{equation}
    r_\text{track\_lin\_vel} = \exp\left(-\Vert \mathbf{v}_\text{robot} - \mathbf{v}_\text{command} \Vert^2/0.25\right),
\end{equation}
where $\mathbf{v}_\text{robot}$ is the robot's linear velocity and $\mathbf{v}_\text{command}$ is the commanded linear velocity.

\texttt{Track\_ang\_vel}: The exponential error between the robot base's angular velocity and the commanded angular velocity. This helps the robot to avoid getting stuck:
\begin{equation}
    r_\text{track\_ang\_vel} = \exp\left(-\Vert \boldsymbol{\omega}_\text{robot} - \boldsymbol{\omega}_\text{command} \Vert^2/0.25\right),
\end{equation}
where $\boldsymbol{\omega}_\text{robot}$ is the robot's angular velocity and $\boldsymbol{\omega}_\text{command}$ is the commanded angular velocity.

\texttt{Action\_l2}: Penalizes the L2 norm of the action to prevent excessively large actions and improve training stability:
\begin{equation}
    r_\text{action\_l2} = -\Vert \mathbf{a} \Vert^2,
\end{equation}
where $\mathbf{a}$ represents the action vector, and $\lambda_a$ is a scaling coefficient.

All of the rewards and their weights are in \autoref{tab:reward_high}.

\begin{table*}[htbp]
    \centering
    \caption{\label{tab:reward_high}Reward Function}
    \renewcommand{\arraystretch}{1.3}
    \fontsize{10}{12}\selectfont
    \begin{tabular}{cccc}
        \toprule
        \textbf{Type} & \textbf{Item} & \textbf{Formula} & \textbf{Weight}\\ \midrule
        \multirow{4}{*}{\textbf{Task}} 
        & Reach goal & $r_\text{reach\_goal} = \begin{cases} 
            1, & \text{if } \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2 \leq \epsilon, \\ 
            0, & \text{otherwise}.
        \end{cases}$ & $0.5$ \\ 
        & Goal distance & $r_\text{goal\_dis} = d_\text{prev} - d_\text{current}, \, d = \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2$ & $5.0$ \\ 
        & Goal distance (z-axis) & $r_\text{goal\_dis\_z} = d_{z,\text{prev}} - d_{z,\text{current}}, \, d_z = \vert z_\text{robot} - z_\text{goal} \vert$ & $30.0$ \\ 
        & Goal heading & $r_\text{goal\_heading} = -\vert \psi_\text{robot} - \psi_\text{goal} \vert$ & $0.3$ \\ \midrule
        
        \multirow{4}{*}{\textbf{Regularizations}} 
        & Stop at goal & $r_\text{stop\_at\_goal} = -\Vert \mathbf{a} \Vert^2\cdot( \Vert \mathbf{p}_\text{robot} - \mathbf{p}_\text{goal} \Vert_2 \leq \epsilon)$ & $1.0$ \\ 
        & Linear velocity tracking & $r_\text{track\_lin\_vel} =  \exp\left(-\Vert \mathbf{v}_\text{robot} - \mathbf{v}_\text{command} \Vert^2/0.25\right)$ & $0.2$ \\ 
        & Angular velocity tracking & $r_\text{track\_ang\_vel} = \exp\left(-\Vert \boldsymbol{\omega}_\text{robot} - \boldsymbol{\omega}_\text{command} \Vert^2/0.25\right)$ & $0.2$ \\ 
        & Action L2 norm & $r_\text{action\_l2} = -\Vert \mathbf{a} \Vert^2$ & $0.002$ \\ 
        
        \bottomrule
    \end{tabular}
    \vspace{-5mm}
\end{table*}

\subsection{Real-to-Sim Reconstruction Details}
\label{sec:recon}
\begin{figure*}[bp]
    \centering
    \vspace{-5mm}
    \includegraphics[width=0.9\linewidth]{figures/image_take_method.pdf}
    \vspace{-3mm}
    \caption{Illumination of data collection pipeline for indoor scene reconstruction.}
    \vspace{-3mm}
    \label{fig:image_taken}
\end{figure*}

We have designed a set of efficient shooting methods tailored for indoor scenes with simple terrain, as shown in \autoref{fig:image_taken}. These methods allow a moderately trained operator to complete the entire process of shooting, reconstruction, and calibration within $3$ hours. The shooting process consists of four predefined routes: one inner circle and three outer circles. The camera orientations for each route are illustrated in the figure. Each route includes 20 evenly spaced nodes, and at each node, photos are captured from 5 distinct camera poses. Consequently, the entire process involves capturing approximately 400 photos, ensuring sufficient coverage for accurate reconstruction.
\balance

\end{document}


