\begin{abstract}
%Game theory provides a mathematical framework, called games, which is used to predict the strategies of payoff-maximizing players whose payoffs depend on one another's strategies.
%The outcome of a game is often modeled as a Nash equilibrium, i.e., a collection of players' strategies s.t.\@ no player can improve their payoff by deviating to another strategy.
In this paper, we study inverse game theory (resp.\@ inverse multiagent learning) in which the goal is to find
%First, given an observed Nash equilibrium, we seek an \emph{inverse (Nash) equilibrium}, i.e., 
parameters of a game's payoff functions for which the expected (resp.\@ sampled) behavior is an equilibrium. 
We formulate these problems as generative-adversarial (i.e., min-max) optimization problems, which we develop polynomial-time algorithms to solve, the former of which relies on an exact first-order oracle, and the latter, a stochastic one.
We extend our approach to solve inverse multiagent simulacral learning in polynomial time and number of samples.
In these problems, we seek a simulacrum, meaning parameters and an associated equilibrium that replicate the given observations in expectation.
We find that our approach outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.

\if 0
We formulate these problems as a generative-adversarial (i.e., min-max) optimization problem, and show that \samy{}{with an exact first-order oracle,} under broad assumptions satisfied by important classes of games, such as normal-form games, inverse game theory is convex-concave, implying that inverse equilibria form a convex set and can be computed efficiently, i.e., in polynomial-time.
We then generalize our characterization to inverse multiagent reinforcement learning settings, where we can access only a stochastic first-order simulator,
%we can only query samples from the potentially stochastic equilibrium \sdeni{strategies}{}\deni{We don't use strategy anywhere else in the paper.}
and show that we can learn an inverse equilibrium in polynomial time in a large class of games including finite state and action Markov games.
%\deni{polytime computational complexity + sample complexity}
%in real-world applications, 
%\deni{No! We assume the function is known. So in electricity markets for instance, we know that we observe aggregate demand, so the function is sum the demands}
%\amy{good, b/c the problem seemed totally underspecified otherwise.}  
%You might have a stoch. func. which might give you an unbiased estimate of the equilibrium strategies, for instance, add some gaussian noise to the actions, then you can learn the eqm. actions from samples. But if you take the actions and output the max, then you lose info on the strategy structure (you only get info on the max action not all actions!).
\sdeni{More generally, 
due to limitations in the sampling process, we may only observe samples from a known but lossy stochastic function of the unobserved equilibrium strategies.
In such settings, it may not be possible to learn
%\amy{not at all? or not efficiently} \deni{not at all. if you output the sum of the consumer's demand, there is no way to recover individual demands.} 
%\deni{The issue is that you cannot map your observation to an equilibrium one-to-one, so this means that there might be multiple policies that give you the same observation, so you have to simulate the Nash eqm. Note: in all our settings, the true parameters are not learnable. This is because there is no one-to-one mapping between parameters and eqa.}
%\amy{why not infer?} \deni{because you observe some function of the strategies, and then the goal is to derive uniform convergence bounds to show that the equilibrium strategies are PAC learnable.} 
equilibrium strategies from samples; as such, we study the problem of computing}
{Finally, we extend our characterization to multiagent apprenticeship learning settings, where we observe noisy realized samples of the equilibrium and seek} a \emph{(Nash) simulacrum}, i.e., parameters and an associated equilibrium, which replicate observations in expectation.
Although the computation %\deni{computation is def ppad, but it is pac learnable. idea is just use a chernoff bound since the domain of objective is bounded.} 
of a Nash simulacrum is in general PPAD-complete, we show that its computation can be formulated as a non-convex-concave min-max optimization problem, for which a local solution can be found in polynomial time \sdeni{}{with polynomially-many samples}.
Finally, 
%%% SPACE
% using both synthetic and real-world time-series data, 
we run a series of experiments in market games whose aim is to recover parameters that explain the observed pricing and demand decisions of sellers and buyers in order to make predictions about future prices and demands. 
We find that our approach to inferring Nash simulacrum strategies outperforms traditional statistical methods in predicting real-world electricity prices and demands.
% \samy{}{and demands} for the moment I only predicted prices, but I should predict demands, agreed!
\fi
\end{abstract}
