\section{Strategic Counterfactual Prediction \deni{Name subject to change, but I like it rn}}
% \deni{Make everything in terms of the distribution!!!}

% \deni{Realizable case first, and then unrealizable case? I think so.}
\deni{Not in love with notation.}
Consider a supervised learning problem $(\contexts, \contextdistrib, \outcomes, \{\context[\numsample]\}_{\numsample \in [\numsamples]}, \{ \optsol(\context[\numsample])\}_{\numsample \in [\numsamples]}, \hypotheses) $ which consists respectively of some \mydef{contexts} and associated \mydef{outcomes} $ \{\left( \context[\numsample], \outcome[][\numsample] \right)\}_{\numsample \in [\numsamples]} \sim \contextdistrib^\numsamples$ drawn from some distribution $\contextdistrib \sim \simplex(\contexts \times \outcomes)$ over a \mydef{context space} $\contexts$ and \mydef{outcome space} $\outcomes$.  \deni{Might call this an outcome statistic, or something along those lines.} 
% $\{ \outcome[][\numsample] \}_{\numsample \in [\numsamples]} \subset \outcomes$ coming from an 
Our goal is to learn a \mydef{hypothesis} $\outcomemap[][*]: \contexts \to \outcomes$, a mapping from the context space to the outcome space, in the \mydef{hypothesis space} $\outcomemap \subset \outcomes^\contexts$ that replicates the marginal distribution over outcomes most closely, e.g., taking the euclidean distance as a distance metric find $\outcomemap[][*] \in \argmin_{\outcomemap \in \outcomemaps} \Ex_{(\contextrv, \outcomerv) \sim \contextdistrib} \left[ \left\| \outcomerv - \outcomemap(\contextrv)\right\|^2 \right]$. 
The standard approach to learn such a mapping is to find a hypothesis in the hypothesis class that minimizes the \mydef{empirical error}, i.e.,  $\outcomemap[][*] \in \argmin_{\outcomemap \in \outcomemaps} \left[ \nicefrac{1}{\numsamples} \sum_{\numsample \in [\numsamples]} \left\| \outcome[][\numsample] - \outcomemap (\context[\numsample]) \right\|^2 \right]$. Although one can choose $\outcomemaps$ to be any hypothesis class, such an approach might prove to be inefficient for certain problems. For instance, if the optimal hypothesis is not a Lipschitz-continuous function, then the sample complexity of learning the optimal hypothesis can be exponential in the worst case, i.e., require an unacquirable amount of data which would be intractable to process. An example of such problems is the class of strategic counterfactual prediction problems. \deni{probably a majority of this discussion will have to be moved to the intro.} 

In strategic counterfactual prediction problems, we seek to predict the outcome of the interaction of rational agents if a different state of the world were to occur. For instance, consider first price auctions for a set of varying goods and a set of fixed bidders. Given access to a history of contexts, e.g., goods auctioned, and outcomes, e.g., the highest winning bids, we are often interested in predicting the \sdeni{winner}{winning bid} \amy{actually, we are usually interested in predicting the price, not the winner; i.e., something continuous, not discrete} \deni{I think the edit fixes the concern?} of an auction for a different set of goods, i.e., the state of the world, for the same bidders. As an auction can be modelled as a Bayesian game, for predictions in such problems to be good, it is often necessary to ensure that the predictor that is learned also captures the underlying Bayesian game model of the auction. That is, by fitting a Bayesian game to the contexts, i.e., learn a mapping from contexts to types such that the EPNE of the associated game replicates the history of highest winning bids most accurately. Unfortunately, learning such a model over a very broad hypothesis class $\hypotheses$ is often not feasible since Nash equilibria are not in general continuous in their parameters,  and as such, learning such a mapping using traditional function approximation methods, e.g., training a feedforward neural network, will often overfit to the data and not lead to learning a game theoretic model due to the possibly exponential sample complexity of the problem for general hypothesis classes. 


\deni{Need a description of the game here first.} However, if we choose to fit a game-theoretic model to the data a priori and specify a game structure $(\numplayers, \numactions,  \typespace, \actionspace, \util)$, consisting of a number of players $\numplayers \in \N_{++}$, an action space dimension $\numactions \in \N_{++}$, a joint type space $\typespace \doteq \bigtimes_{\player \in \players} \typespace[\player] \subset \R^{\numplayers \times \numtypes}$, a joint action space $\actionspace \doteq \bigtimes_{\player \in \players} \actionspace[\player] \subset \R^{\numplayers \times \numactions}$, and a payoff structure $\util: \actionspace \times \typespace \to \R^\numplayers$, we can restrict our hypothesis class to functions from contexts to outcomes which are compositions $\outcomemap \circ \eqmmap \circ \typemap$ of (a) a \mydef{type mapping}, i.e., mappings $\typemap \in \typemaps \doteq \{ \typemap: \contexts \to \typespace \}$ from contexts to types, (b) an \mydef{equilibrium mapping}, i.e., mappings $\eqmmap \in \eqmmaps \doteq \{ \eqmmap: \typespace \to \actionspace\}$ from types to (possible approximate equilibria and (c) an \mydef{outcome mapping}, i.e.,  mappings $\outcomemap \in \outcomemaps \doteq \{\outcomemap: \actionspace \to \outcomes\}$ from equilibria to outcomes. This restriction can in turn significantly reduce the domain of our search and can as such overcome issues of sample complexity and overfitting. Unfortunately, techniques to learn such mappings from data few and far between, and often work for limited classes of games or simplistic solution concepts or make very strong assumptions, e.g., uniqueness of equilibria. This dearth of literature, points to a need to develop methods that can fit games to data regardless of the type of the game. \deni{Need more discussion on PPAD-completeness and yaddeyyaddah.}

With \Cref{obs:bne} now in hand, we can however provide a mathematical characterization of mappings $(\outcomemap, \eqmmap, \typemap) \in \outcomemaps \times \eqmmaps \times \typemaps$ such that the hypothesis $\outcomemap \circ \eqmmap \circ \typemap$ minimizes the supervised learning loss, while the mapping $\eqmmap$ is a $\varepsilon$-EPNE.\footnote{Once again, in this work we focus on EPNE but we note that our results also apply to the more general solution concept of the BNE.} 
\begin{align}\label{eq:GGS}
    & \min_{(\typemap, \eqmmap, \outcomemap) \in \typemaps \times \eqmmaps \times \outcomemaps} & \nicefrac{1}{\numsamples} \sum_{\numsample \in [\numsamples]} \left[\left\|\outcomemap(\eqmmap(\typemap(\context[\numsample]))) - \outcome \right\|^2 \right]\\
    &\text{Subject to } &  \nicefrac{1}{\numsamples} \sum_{\numsample \in [\numsamples]} \exploit[][](\eqmmap(\typemap(\context[\numsample])); \typemap(\context[\numsample])) \leq \varepsilon
\end{align}
The above problem can be interpreted as a two-player general-sum game between a generator who chooses mappings $(\typemap, \outcomemap) \in \typemaps \times \outcomemaps$ from contexts to types and action profiles to outcomes, and a simulator who chooses a mapping $\eqmmap \in \eqmmaps$ from types to action profiles. The goal of the generator is to pick mappings which result in a game who equilibria replicate the outcome data most accurately, while the goal of the simulator is to simulate the EPNE associated with the game. 
The optimal mappings $(\outcomemap[][*], \eqmmap[][*], \typemap[][*]) \in \outcomemaps \times \eqmmaps \times \typemaps$ that solve \Cref{eq:GGS} are then called the \mydef{generative game simulator}. 


% To address this need, we introduce the \mydef{generative game simulator} which consists of finding a predictor $\hypothesis[][*]$ which maps each context to a type, and computes for this type a (possibly approximate) Nash equilibrium and outputs a function of this Nash equilibrium. More precisely, given $(\numplayers, \numactions,  \typespace, \actionspace, \util)$ consisting of specification of a Bayesian game where the type distribution and type distribution parameter  is unknown, the counterfactual learning consists of solving the following problem:
% \deni{EPNE in a Bayesian game}

% \begin{figure}
%     \centering
%     \includegraphics[scale=0.25]{figures/summary figure.jpeg}
%     \caption{Summary of the Counterfactual Prediction Framework}
%     \label{fig:summary}
% \end{figure}



% Consider a prediction problem $(\contexts, \contextdistrib, \outcomes, \{\context[\numsample]\}_{\numsample \in [\numsamples]}, \{\outcome[][\numsample] \}_{\numsample \in [\numsamples]}) $ which consists respectively of some \mydef{contexts} $\{\context[\numsample]\}_{\numsample \in [\numsamples]} \sim \contextdistrib^\numsamples$ drawn from some distribution $\contextdistrib \sim \simplex(\contexts)$ over a context space $\contexts$ and some associated \mydef{outcomes} $\{ \outcome[][\numsample] \}_{\numsample \in [\numsamples]}$  outcome space $\outcomes$. Our goal is to learn a \mydef{predictor} $\hypothesis: \contexts \to \outcomes$ from the context space to the outcome space that most closely replicates the joint distribution over the context and outcome space when given the access the marginal distribution over the context space