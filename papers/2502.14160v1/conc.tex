\section{Conclusion}


\sdeni{}{We introduced convex-concave generative-adversarial characterization of inverse Nash equilibria for a large class of games, including normal-form, finite state and action Markov games, and a number of continuous state and action Markov games. This novel formulation then allowed us to obtain polynomial-time computation guarantees for inverse equilibria in these games, a rather surprising result since the computation of a Nash equilibrium is in general PPAD-complete. Our result can be thus seen as a positive computation result for game theory. We then extended our characterization to a multiagent apprenticeship learning setting, where we souught to not only rationalize the observed behavior as an inverse Nash equilibrium but also make predictions based off the inverse Nash equilibrium, and have shown in experiments on prices in Spanish electricity markets that our approach to solving multiagent apprenticship learning can be effective at predicting behavior in multiagent systems. The approach to inverse game theory that we provided in this paper is a highly flexible one and thus can be used to solve inverse equilibrium beyond inverse Nash equilibria and future work could explore ways to extend our approach to other game-theoretic settings and equilibrium concepts.}


\amy{discuss extenstion to other eqm concepts? CE, CCE, etc.}

\amy{and other idea from yesterday. check text thread?}


