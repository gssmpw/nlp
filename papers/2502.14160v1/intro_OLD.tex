\section{Introduction}

%\amy{i am not sure if we want to say stochastic or noisy oracle? we also have lossy and lossless.}

\amy{
\begin{itemize}
\item
\end{itemize}
}

Game theory is a mathematical framework used to predict the outcome of the interactions, called games, of preference-maximizing agents, called players, whose preferences among outcomes are represented via payoff functions.
%%% SPACE
%\footnote{While it is commonly assumed \amy{what do you mean assumed? vN\&M PROVED a theorem saying that this is so!}\deni{This is me making a point against an argument which I see people utter all the time and I hate. People say that game theory is doomed because everything assumes rationality, i.e., payoff maximizing behavior. This is wrong. The paper I cite at the end of this footnote proves the existence of a \samy{Nash}{Walrasian} equilibrium for preferences which CANNOT be represented as payoff maximization problems. In fact, they make so few assumptions on the preference relation that they show that even ``irrational'' preferences \samy{outcome can be modelled as NE}{can lead to equilibrium behavior}.} that players' preferences can be represented via payoff functions under suitable assumptions on the preferences (i.e., completeness and transitivity), game theory and its predictions \samy{}{also} concern the interaction of agents whose preferences might not be representable via payoff functions \citep{mas1974equilibrium, shafer1975equilibrium}. \amy{this paper is actually about Walrasian eqm, not Nash. so are you assuming a reduction from one to the other?} \deni{Ah it seems I was missing on references, Shafer's paper generalizes the result to pseudo-games.}}
The canonical outcome, or solution concept, prescribed by game theory is the equilibrium: an action profile, i.e., a collection of actions, one per player, such that each player's action is rational, i.e., payoff maximizing, contingent on the payoff-maximizing behavior of the others.
When using game theory to make a prediction, it is typical to assume that \samy{the model of players' behavior, including their preferences, is}{the players' preferences are} known. 
%\amy{the behavioral model includes more than the preferences. it also includes whether they are rational, boundedly rational, etc. so maybe you mean e.g., instead of i.e., right after this comment:}\deni{Imo, a preference relation can represent any behavior, including boundedly rational behavior as long as we make no assumptions on the preference relation? but I will make the change} 
However, in many applications of interest, such as contract design \citep{holmstrom1979moral, grossman1992analysis} and
counterfactual prediction \citep{peysakhovich2019robust}, 
%and apprenticeship learning \citep{donge2022multi}, 
\samy{a behavioral model}{complete preference information} is not available; only the choices the players make are observable.
In such cases, we may seek to identify payoff functions\samy{}{---from a space of parameterized payoff functions---}that replicate the observed actions.
Assuming these actions are equilibrium actions, this identification task serves to \mydef{rationalize} them.
Identification problems of this nature characterize \mydef{inverse game theory} \citep{waugh2013computational, bestick2013inverse}.

The object of study of inverse game theory is the \mydef{inverse game}, which comprises a set of players, each characterized by an action space and a parametric payoff function \samy{}{with relevant parameters omitted},
%mapping the players' actions to a real number,
replaced by an observed 
%\amy{possibly mixed? or realized samples?} 
action profile. 
%\amy{we are also given a behavioral model in an inverse GT problem}
Given an inverse game, the goal is to compute an \mydef{inverse equilibrium}, i.e., parameter values such that the observed actions correspond to an equilibrium
%\samy{}{, under the given behavioral model,} 
of the payoff functions evaluated at those values.
\samy{}{Our first contribution is to formally pose and solve this problem, assuming access to an \emph{exact\/} oracle that returns the payoffs of queried action profiles.}
%\deni{Ok I think now this matches with the rest of the text and makes a lot of sense, thanks!!} \amy{well, i still don't like it!}
%As deterministic equilibria are not guaranteed to exist, players' actions and thus their payoffs can be stochastic, in which case the latter cannot be accessed directly and instead have to be estimated via simulation.\amy{REWRITE!}
%
Relaxing the assumption of an exact oracle, and instead assuming access to a game simulator that can be used to estimate the players' payoffs (i.e., a \emph{stochastic\/} oracle), brings us from the realm of inverse game theory into the realm of \mydef{inverse multiagent learning}.
%\sdeni{}{More generally, when the game is also non-stationary \amy{i really do not get this claim about non-stationarity. Markov games are stationary!!! maybe NE policies are not in general, but the themselves games are.}, the relevant model is that of stochastic (or Markov) games \citep{shapley1953stochastic,takahashi1964equilibrium,fink1964equilibrium}---time-based games where players observe a new state at each time-step and simultaneously take actions which stochastically determines their next state and yields an instantaneous payoff---}
When the underlying game is also stochastic, the relevant problem is one of \mydef{inverse multiagent reinforcement learning} (inverse MARL) \citep{russell1998learning, ng2000algorithms,natarajan2010multi}.
\samy{}{Our second contribution is to solve inverse MARL.
We solve both inverse game theory and inverse MARL in the \emph{online\/} case, where we assume simulator access and can thus sample any trajectory of play associated with an observed equilibrium.}

%\amy{does Ng really attempt inverse MARL in that first 2000 paper? i didn't remember that?} \deni{No the first two papers are pure IRL but felt appropriate to cite, we can remove tho.}
%One can similarly define various equilibrium concepts such as Nash equilibrium for stochastic games, but now in terms of policy profiles (i.e., mappings from states to action profiles), and then seek to compute game parameters (i.e., an inverse equilibrium) that rationalize observed behavior.
%\amy{how is it an instance of inverse MARL if we no longer have a simulator/generative model? isn't then apprenticeship learning the more general problem?} is 

More generally, in
\mydef{multiagent apprenticeship learning} \citep{abbeel2004apprenticeship, yang2020inferring}, the goal is to find a policy profile that rationalizes the players' observed behavior, given sample trajectories of play prepared \emph{offline}.
We study a yet more general version of this problem in which the observations are mere summaries of sampled agent behaviors, rather than the samples themselves, e.g., an aggregate demand curve for an economy, rather than all the individuals' demands.
This generalization of multiagent apprenticeship learning involves finding a \mydef{simulacrum}, meaning parameter values together with associated equilibrium policies that replicate the observed trajectories of play as samples from the simulacrum policies, in expectation.
\samy{}{By solving this 
%``simulacrum learning'' 
problem, we not only provide an explanation of the agents' observed (offline) behavior, which, upon computing an ensuing equilibrium, can be used to predict their future behavior, should the state of the world remain constant; we further provide counterfactual predictions of unobserved behavior, should the state of the world change.}

\amy{i guess it makes some sense to call the more general problems we are defining: simulacrum planning and learning.}

%Given noisy payoff samples obtained by simulating the players' policies,\amy{this sounds just like inverse MARL; distinction is not clear}\deni{I think if you are referring to the idea of learning only from samples, I think that's a step ahead, and would be offline multiagent apprenticeship learning, which is a contribution we make but I think it should be part of the contributions.}

\if 0
in the \mydef{online} case, where we assume simulator access and can thus sample any history associated with an observed equilibrium.
in the \mydef{offline} case, where we have only a noisy sample of realized equilibrium histories. 
%associated with an unobserved equilibrium.
\amy{histories or trajectories? be consistent!}
\fi

% \sdeni{}{in expectation replicate} an observed sample trajectory drawn from this policy profile as an equilibrium. 
% \amy{DEFINE SIMULACRUM here! talk about what replication means: i.e., only in expectation!}

% \amy{probably delete:} \samy{However, due to the non-stationarity of the game as well as possible stochasticity of the policies, one often cannot observe the entire policy, but rather only noisy samples of (state, action) tuples, and has to learn parameters from these samples.}{} 
%For instance, if we seek to model and explain the movement of two teams of soccer players on the field, we might at any second observe the positions of the players on the field, but we will never observe their movement in every single possible positioning of the player on the field.\amy{i'm not convinced. with cameras overhead, we can observe everyone's movements. i think you mean to say that we do not observe their policies. but you have not said this.}


\deni{Mention that we will focus on Nash equilibrium.}
\amy{this paragraph doesn't fit here. move to conclusion, perhaps. as future (mostly empirical) work!}
\amy{and if this paper at present has no conclusion, write one, or just delete this paragraph for now. don't worry---i know how it pains you do this, since you you LOVE my 2003 paper so much! :) but copy to conc.tex file, so we don't forget about it!} \deni{it really is a brilliant paper, idk why it's not your most cited one!!!} \amy{easy answer. b/c the correlated-Q paper, which is actually a source of great embarrassment to me, was picked up by DeepMind people! and therefore, it is highly cited. sigh!}
Common solution concepts for inverse game theory problems are \mydef{Nash}, \mydef{correlated}, and \mydef{coarse correlated equilibrium}, in which case we refer to the associated inverse equilibrium concepts as inverse Nash, inverse correlated, and inverse coarse correlated equilibrium, respectively.
At these and all equilibria, the players' feel no \mydef{regret} for not having played some other action, given the actions of the others.
The available \mydef{counterfactual deviations} (i.e., alternative behaviors), which generate these regrets, define the various equilibrium concepts.
In this paper, we focus on the inverse Nash equilibrium problem, but our characterization and algorithms can easily be extended to other inverse equilibrium concepts, and to inverse equilibrium problems beyond games \citep{arrow-debreu, facchinei2010generalized}).
%\amy{to pseudo-games}


A growing literature has been dedicated to the inverse game theory and inverse multiagent reinforcement learning problems under various guises, including parameter identification in econometrics \citep{wright1928tariff, syrgkanis2017inference}, model estimation in microeconomics/macroeconomics \citep{taylor1979estimation}, and multiagent immitation learning \citep{song2018multi}.
While some progress has been made in providing a mathematical characterization of inverse correlated equilibrium in normal-form games \citep{waugh2013computational, kuleshov2015inverse, syrgkanis2017inference} and inverse Nash equilibrium in restricted classes of finite state and action  games \citep{lin2017multiagent, lin2019multi}, very little is known about the computational complexity of inverse Nash equilibrium in normal-form or general-sum  games.
Furthermore, most theoretical guarantees for algorithms that compute inverse equilibria concern inverse correlated equilibria in inverse normal-form or Bayesian games with discrete actions; very little is understood about the computational complexity of inverse Nash equilibria in general-sum normal-form or  games, the latter with continuous action or state spaces.
%\sdeni{, or the sample complexity of the inverse equilibria in stochastic games, where only sample trajectories from equilibrium policies may be observed}{}.\amy{i am not sure if this second half of the sentence is supposed be crossed out or not.} \deni{Supposed to be crossed-out.}
This paper aims to develop a flexible mathematical and algorithmic framework to solve inverse equilibrium problems efficiently in a wide variety of games for various choices of equilibrium concepts, with the ultimate goal of making counterfactual predictions based on \samy{inverse}{observed} equilibria.

\amy{but we assume an observation distribution --- which is a form of a simulator. weaker than simulating policies and observing rewards. but we do simulate policies and observe ``observations'', which are functions of/lossy rewards.}

\alec{there are a lot of terms being thrown around...would it be possible to visualize a schematic of the inverse MARL/equilibrium computation problem? i.e. given time-series of, e.g. prices, rationalizing the demand is associated with computing some equilibrium? Perhaps a little cartoon of what it looks like for energy market or congestion game? specifically a diagram could depict what the generator and discriminator does in the inverse multi-agent game setting}

\alec{another comment is that I don't think we defined normal-form or general-sum anywhere. What does it mean with respect to agents' utility functions/how they interact?}
%\amy{Alec, please create and use a macro. o/w, we cannot automatically turn off your comments to submit!}

We formulate the problem of computing an inverse equilibrium \sdeni{}{(resp.\@ simulacrum)} in inverse games as a generative adversarial optimization problem between a generator and a discriminator.
The generator takes as input the observed action/policy profile \sdeni{}{(resp.\@ observed trajectories)}, based on which it generates candidate parameter values \sdeni{}{(resp.\@ parameter values and associated equilibrium policies)} for the players' parametric payoff functions, which the discriminator then judges by computing possible unilateral utility-improving counterfactual deviations.\amy{not the typical use of discriminator!}
%at the generator's parameters. 
In computing inverse equilibria (resp.\@ simulacra), the generator's goal is to choose parameters (resp.\@ parameters and a candidate equilibrium) that minimize the cumulative regret across all players
%(i.e., the sum of the per-player changes in utility for unilaterally deviating from one profile to another)
between the observed (resp.\@ candidate) equilibrium and the profile chosen by the discriminator, while the goal of the discriminator is to choose a profile that maximizes this regret.\amy{not the typical use of discriminator!}
Our formulation is fully general in the sense that it can be extended to any game and equilibrium concept for which a notion of regret is well-defined (see, for instance \citep{morrill2022hindsight}).

Our methods provide an efficient way to fit game-theoretic models onto real-world data, and in the case of multiagent apprenticeship learning, make behavioral predictions. 
To demonstrate this fact, we model the Spanish electricity market as a game between electricity re-sellers who set prices so as to maximize their profit and consumers who demand electricity so as to maximize their utilities.
We then compute a Nash simulacrum that replicates real-world price and demand in this market through 2018, which we test on electricity prices from 2018 to 2020 by simulating the simulacrum policies.

%\sdeni{}{Finally, we show that our formulation can be extended to multiagent apprenticeship learning, where the goal is not only to explain observed behavior but to also replicate it.}

% then takes as input based on which it outputs an equilibrium for the game associated with these payoff functions.
% Our method is ``generative,'' as it generates parameters of a game, that are intended to reproduce equilibria; and it is ``simulative'' as it can only simulate the equilibrium associated with the generated parameter.
% GGS can be used to compute an inverse equilibrium whenever the exploitability, i.e., distance of an action profile from an equilibrium in payoff space, can be computed efficiently.  


% \amy{all this stuff about the Spanish electricity market belongs somewhere in the introduction.}
% For instance, in our experimental set-up in \deni{Add spanish electricity  market prices experiement forward ref.}, we model the Spanish electricity market as a game between electricity re-sellers who set prices so as to maximize their profit and consumers who demand electricity so as to maximize their utilities.
% In this setting, electricity providers only collect data on prices and the aggregate demand of the consumers, not the particular demands of the individual consumers, which comprise their equilibrium actions.
% %, the sum of the demands across all consumers. 
% As such, we do not have access to individual electricity consumptions, meaning that in our game model the observation distribution takes as input price and demand policies of the sellers and consumers respectively and outputs a trajectory of prices for the sellers, and \emph{the sum} of the demand trajectories across all consumers.  
