\section{Introduction}

\input{todos}

\if 0
\deni{Remove this paragraph.}
\mydef{Game theory} is a \sdeni{}{collection of } mathematical framework\sdeni{}{s called games}, used to predict the outcome of the interactions\sdeni{, called games,}{} of preference-maximizing agents, called players, whose \sdeni{preferences among}{whose preference relation over} \mydef{outcomes} \sdeni{}{, or \mydef{action profiles} (i.e., a collection of actions, one per player)} are represented via payoff functions.
%%% SPACE
%\footnote{While it is commonly assumed \amy{what do you mean assumed? vN\&M PROVED a theorem saying that this is so!}\deni{This is me making a point against an argument which I see people utter all the time and I hate. People say that game theory is doomed because everything assumes rationality, i.e., payoff maximizing behavior. This is wrong. The paper I cite at the end of this footnote proves the existence of a \samy{Nash}{Walrasian} equilibrium for preferences which CANNOT be represented as payoff maximization problems. In fact, they make so few assumptions on the preference relation that they show that even ``irrational'' preferences \samy{outcome can be modelled as NE}{can lead to equilibrium behavior}.} that players' preferences can be represented via payoff functions under suitable assumptions on the preferences (i.e., completeness and transitivity), game theory and its predictions \samy{}{also} concern the interaction of agents whose preferences might not be representable via payoff functions \citep{mas1974equilibrium, shafer1975equilibrium}. \amy{this paper is actually about Walrasian eqm, not Nash. so are you assuming a reduction from one to the other?} \deni{Ah it seems I was missing on references, Shafer's paper generalizes the result to pseudo-games.}}
The canonical outcome, or solution concept, prescribed by game theory is the equilibrium: an action profile\sdeni{, i.e., a collection of actions, one per player,}{} such that each player's action is optimal, i.e., payoff maximizing, contingent on the payoff-maximizing behavior of the others.
When using game theory to make a prediction, it is assumed that the model of players' behavior,
%\amy{the behavioral model includes more than the preferences. it also includes whether they are rational, boundedly rational, etc. so maybe you mean e.g., instead of i.e., right after this comment:}\deni{Imo, a preference relation can represent any behavior, including boundedly rational behavior as long as we make no assumptions on the preference relation? but I will make the change} 
\sdeni{as well as their preferences}{that is, their preference relation over  outcomes (i.e., action profiles) of the game} \amy{are the players' preferences part of the game, as per the first sentence? or part of the behavioral model, as in this sentence?} \deni{Part of the behavior model. But I am now having issues with this sentence. I think this sentence before was making reference to the fact that the behavioral model (which is the type/parameter dependent utility function) is known but the actual behavior (i.e., the type/parameter is not known). But I think for this early into the intro that distinction is too subtle. It is in my opinion better to not make the distinction now and only introduce it later, by explaining that without a suitable parameterization, the problem is meaningless.} \sdeni{,}{is} are known. 
\fi

%\amy{b/c the profiles and the outcomes are not necessarily the same thing. e.g., in an auction, the outcome is the allocation and payments} \deni{I am not sure I agree with this. The auction rules (i.e., payment and allocation) are the parameters of a game, and every auction defines a game whose outcome is a bid profile (for each bidder). These bids do indeed go through the allocation and payment rule before affecting the payoffs of the players, but this does not change the fact that the payoff of the players depend on the bid profile, given the allocation and payment rule.} \amy{so you are saying that the game parameters define the game: e.g., an allocation and payment rule define an auction game. what i am saying is that the game outcome is NOT the bid profile, but rather, an outcome is the allocation and payments that ensue, after the allocation and payment rules are evaluated at the bid profile.} \deni{So to be more specific, the allocation and payment rule is the outcome of the *inverse* auction game (i.e., the inverse equilibrium, while the bids are the outcome of the auction game. So the outcome of a game is still an action profile, which in this case is a bid profile.}

Game theory provides a mathematical framework, called 
%\sdeni{}{(normal-form)} 
\mydef{games}, which is used to predict the outcome of the interaction of preference-maximizing agents called \mydef{players}. 
%Formally,
Each player in a game chooses a \mydef{strategy} from its \mydef{strategy space} according to its preference relation, often represented by a \mydef{payoff function} over possible \mydef{outcomes}, implied by a \mydef{strategy profile} (i.e., a collection of strategies, one-per-player). 
The canonical outcome, or \mydef{solution concept}, prescribed by game theory is the \mydef{Nash equilibrium (NE)} \citep{nash1950existence}: a strategy profile such that each player's strategy, fixing the equilibrium strategies of its opponents, is payoff-maximizing (or more generally, preference-maximizing).

% .\amy{sentence makes no sense to me! and this whole discussion was too early in the old version, and you only moved it to even earlier. this generality is not relevant at this early stage of the discussion.}
\if 0
In general, the set of counterfactual strategies of each player depends on its equilibrium strategy, meaning that it is better characterized \mydef{counterfactual strategy correspondence} (i.e., a point-to-set mapping) which outputs for each player's current equilibrium strategy a subset of its strategies to which it can deviate.%
\footnote{Traditionally, equilibrium concepts have been characterized in terms of sets of deviation functions (i.e., point-to-point mappings from strategies to strategies) which then induce a counterfactual strategy correspondence given by the union of all deviation functions \citep{greenwald2003general, gordon2008no}.} 
As such, by varying the structure of the counterfactual strategy correspondence associated with equilibrium, one can obtain a variety of equilibrium concepts \citep{greenwald2003general}, such as the \mydef{Nash} (\citeyear{nash1950existence}), \mydef{correlated}, and \mydef{coarse correlated equilibrium} \citep{aumann1974subjectivity}.
\fi

%\deni{Throughout, let's stick to ``estimation'' and not ``identification''. Identification seems to often be associated with the more specific question of whether parameters can represent the data faithfully. While estimation seems more generally to refer to the computational problem of finding the payoff functions.} \deni{I want to discuss this. I think it makes no sense to say that the goal is to recover the payoff function that induces the observed actions? What does it mean to induce? It is ill-defined. Maybe you could say induce the observed action as the outcome of a behavioral model or something like that but it complicates things, I think following edit is the simplest and clearest.} \amy{induce means motivate in this context. i.e., i want to recover a payoff function that induces (i.e., motivates players to play according to) the observed actions. and yes, i am assuming rationality. that is my (our?) behavioral model. and i am using the term in the AI sense of the word: rationality means optimizing an objective (e.g., maximizing expected utility). i am not using it in the sense of rational (i.e., consistent) preferences. that is an entirely different use of the term, which to me, does not pertain to behavior.} \deni{I understand but I think there is an inconsistency in this way of thinking. That is, if you are assuming the players are preference maximizing, that is effectively equivalent to assuming equilibrium behavior. Can we talk about this?} \amy{yes. i agree. for me, pref-max is the same as rat'l, which is the same as assuming eqm behavior.}

In many applications of interest, such as contract design \citep{holmstrom1979moral, grossman1992analysis} and counterfactual prediction \citep{peysakhovich2019robust}, the payoff functions (or more generally, preference relations) of the players are not available, but the players' strategies are.
In such cases, we are concerned with estimating payoff functions 
%%% SPACE
%(or more generally, preference relations) 
for which these observed strategies are an equilibrium.
%\amy{``are an equilibrium'' means you are assuming a behavioral model!!! namely rational behavior on the part of all agents. so, yes, even you are making this assumption -- at least implicitly.} \deni{I want to discuss this.}
This estimation task serves to \mydef{rationalize} the players' strategies (i.e., we can interpret the observed strategies as solutions to preference-maximization problems).
Estimation problems of this nature characterize \mydef{inverse game theory} \citep{waugh2013computational, bestick2013inverse}.

%\deni{An action profile can be rationalized, even if the observed strategies are *not* equilibrium strategies. Take for instance, any strategy profile profile in a normal-form game, then the zero game rationalizes it. I do not think that the assumption that the observed strategies are an equilibrium is a meaningful one.}

The primary object of study of inverse game theory is the \mydef{inverse game}, which comprises a 
%\sdeni{}{(normal-form)} 
game with the 
%\samy{}{pertinent details about} \deni{Discuss. I do not like this edit because it clashes with the next two sentences. Namely, in the sequel we mention that we have to restrict the class of payoff functions we consider to make the rationalization task meaningful and this description has nothing to deal with the parameterization of the payoffs and the parameters being omitted.} 
payoff functions omitted, and an \mydef{observed strategy profile}.
The canonical solution concept prescribed for an inverse game is the \mydef{inverse Nash equilibrium}, i.e., payoff functions 
%(or more generally, preference relations) 
for which the observed strategy profile corresponds to a Nash equilibrium.
%
% Given a counterfactual strategy correspondence, the canonical solution concept prescribed for an inverse game is the \mydef{inverse Nash equilibrium}, i.e., payoff functions (or more generally, preference relations) for which the observed strategies correspond to an equilibrium w.r.t. the counterfactual strategy correspondence. 
%
If the set of payoff functions in an inverse game is unrestricted, the set of inverse Nash equilibria can contain a wide variety of spurious solutions, e.g., in all inverse games, the payoff function that assigns zero payoffs to all outcomes is an inverse Nash equilibrium, because any strategy profile is a Nash equilibrium of a \mydef{constant game}: i.e., one whose payoffs are constant across strategies. 
To meaningfully restrict the class of payoff functions over which to search for an inverse Nash equilibrium, one common approach \citep{kuleshov2015inverse, syrgkanis2017inference} is to assume that the inverse game includes in addition to all the aforementioned objects, a \mydef{parameter-dependent payoff function} 
%%% SPACE
%(or more generally, a \mydef{parameter-dependent preference relation}) 
for each player, in which case an \mydef{inverse Nash equilibrium} is simply defined as parameter values such that the observed strategy profile is a Nash equilibrium of the parameter-dependent payoff functions evaluated at those values. 

\if 0
By varying the structure of the counterfactual strategy correspondence associated with an inverse equilibrium, one can obtain a variety of inverse equilibrium concepts analogous to their equilibrium counterparts such as the \mydef{inverse Nash} \citep{kuleshov2015inverse}, \mydef{inverse correlated}, and \mydef{inverse coarse correlated equilibrium} \citep{bestick2013inverse, waugh2013computational}.
For simplicity, in this paper, we focus on the inverse Nash equilibrium problem (i.e., the strategy deviation correspondence is constant and equal to the strategy space of the players), but our characterization and algorithms can easily be extended to other inverse equilibrium concepts, and to inverse equilibrium problems beyond games \citep{arrow-debreu, facchinei2010generalized}).
\fi

%\deni{The distinction between learning and planning has nothing to deal with first-order or zeroth-order oracle (or any other pth order oracle model), it only has to deal with exact vs.\ stochastic distinction. The first, second or pth order oracles only have implications on the computational complexity of the problem but not the nature of the problem.}

If one assumes \emph{exact\/} oracle access to the payoffs of the game (i.e., if there exists a function which, for any strategy profile, returns the players' payoffs%
\footnote{Throughout this work, we assume that the oracle evaluations are constant time and measure computational complexity in terms of the number of oracle calls.}),
% \sdeni{}{a constant number of time-steps} \amy{what is a time step in this context? can we delete?} \deni{I guess we should say somehow that the reason why we talk of an oracle, is because the oracle becomes the primitive of complexity evaluation. So in that sense, the oracle should be a constant-time operation so that if you use it as your complexity measure, the problem is indeed polytime.} \amy{add as footnote}
the problem of computing an inverse Nash equilibrium is one of \mydef{inverse multiagent planning}. 
In many games, however, a more appropriate assumption is \emph{stochastic\/} oracle access, because of inherent stochasticity in the game \citep{shapley1953stochastic} or because players employ randomized 
%%% SPACE
%(i.e., mixed) 
strategies \cite{nash1950existence}.
The problem of computing an inverse Nash equilibrium assuming stochastic oracle access is one of \mydef{inverse multiagent learning}.

%In these circumstances, it is necessary to relax the assumption of exact oracle access to \emph{stochastic} oracle access, in which case we refer to the problem of computing an inverse Nash equilibrium as the \mydef{inverse multiagent learning problem}.


\if 0
If one assumes \mydef{exact \sdeni{first-order}{} oracle access} to the payoffs of the game (i.e., if there exists a pair of functions which for any strategy profile return the value and the gradients \amy{wrt what?}\deni{this is not meanigful here, the gradient is an operator applied to a function regardless of a w.r.t.. That is, it just means w.r.t. all the variables by default.} of the players' payoff functions), the problem of computing an inverse Nash equilibrium is known as the \mydef{inverse multiagent planning problem}. \amy{i deleted optimization. it just wastes space imo. do you disagree?}
Often, however, games involve uncertainty, with stochastic payoffs \cite{shapley1953stochastic} \amy{shapley is the wrong reference here. i added it, btw, not you, but it's not what i want. what i want/need is a reference to a Markov game with BUT ONE state!!! e.g., a repeated one-shot game, with stochastic payoffs. read the last sentence of this paragraph and of the next one to see why.} and/or mixed strategies \cite{nash1950existence}.
In these circumstances, it is necessary to relax the assumption of exact oracle access to \mydef{stochastic oracle access}, \samy{}{and to further assume the players interact repeatedly, so that they can \emph{learn}, i.e., build payoff estimates, from noisy data}, in which case we refer to the problem of computing an inverse Nash equilibrium as the \mydef{inverse multiagent learning problem}.
\fi

%\deni{Maybe a better name for it is ``inverse (Markov) simulation''} \amy{so in addition to being given an inverse game, we are given a ``training set:'' i.e., a bunch of samples to learn from. and you want to call these a simulation. i think these are more like the real deal. and what are doing is building a simulacrum to replicate them. but they are not a simulation. they are our ``true'' inputs.} \deni{Now thinking more about this, I think I prefer simulation, the definition of simulation by Beaudrillard is: ``Simulation is the imitation of the operation of a real-world process or system over time.'' so I indeed what we have is a simulation, since we have both a model of the system, and then the imitation of the operation of the system over time}

%\amy{and leftover from before, what is a differentiable simulator?} \deni{the obvious answer is: A simulator whose outputs are differentiable w.r.t. to its inputs. The better answer for MDPs is a differentiable reward function oracle, and a differentiable probability transition function.}

One important class of inverse games is that of \mydef{inverse Markov games}, in which the underlying game is a \mydef{Markov game} \citep{shapley1953stochastic, fink1964equilibrium, takahashi1964equilibrium}, i.e., the game 
%is characterized by a set of states and 
unfolds over an infinite time horizon: at each time period, players observe a state, take an action (simultaneously), receive a reward, and transition onto a new state. 
In such games, each player's strategy,%
\footnote{Throughout this paper, a strategy refers to the complete description of a players' behavior at any state or time of the game, while an action refers to a specific realization of a strategy at a given state and time.} 
also called a \mydef{policy}, is a mapping from states to actions describing the action the player takes at each state, with any strategy profile inducing a \mydef{history distribution} over \mydef{histories of play} i.e., sequences of (state, action profile) tuples. 
The payoff for any strategy profile is then given by its \mydef{expected cumulative reward} over histories of play drawn from the history distribution associated with the strategy profile. 
Excluding rare instances,%
\footnote{For simple enough games, one can express the expected cumulative reward in closed form, and then solve the inverse (stochastic) game  assuming exact oracle access.}
the payoff function in Markov games is only accessible via a stochastic oracle, typically implemented via a game simulator that returns \emph{estimates} of the value of the game's rewards and transition probabilities.
As such, the computation of an inverse Nash equilibrium in an inverse Markov game is an inverse multiagent learning problem, which is often called \mydef{inverse multiagent reinforcement learning (inverse MARL)} \citep{natarajan2010multi}.

% \amy{what is the point of this next footnote?} \deni{Before, we said that the payoffs were parametric, but in Markov games, we are saying that the payoffs are determined by the rewards, so it must be that rewards are parametric, and we should mention that?} \amy{we should mention, yes. in the math. but maybe not in the intro?}


% \amy{why was this commented out? maybe something like this goes in the conclusion?}


%\sdeni{}{More generally, when the game is also non-stationary \amy{i really do not get this claim about non-stationarity. Markov games are stationary!!! maybe NE policies are not in general, but the themselves games are.}, the relevant model is that of stochastic (or Markov) games \citep{shapley1953stochastic,takahashi1964equilibrium,fink1964equilibrium}---time-based games where players observe a new state at each time-step and simultaneously take actions which stochastically determines their next state and yields an instantaneous payoff---}

In many real-world applications of inverse Markov games, such as robotics control \citep{coates2009apprenticeship}, one does not directly observe Nash equilibrium strategies but rather histories of play, which we assume are sampled from the history distribution associated with some Nash equilibrium. 
In these applications, we are given \mydef{an inverse simulation}---an inverse Markov game together with sample histories of play---based on which we seek parameter values which induce payoff functions that rationalize the observed histories. 
As a Nash equilibrium itself is not directly observed in this setting, we aim to compute parameter values that induce a Nash equilibrium that replicates the observed histories \emph{in expectation}.
We call the solution of such an inverse simulation (i.e., parameter values together with an associated Nash equilibrium) a \mydef{simulacrum}.
Not only does a simulacrum serve to explain (i.e., rationalize) observations, additionally, it can provide predictions of unobserved behavior.

We study two \mydef{simulacral learning} problems, a first-order version in which samples histories of play are faithful, and a second-order version in which they are not---a (possibly stochastic) function of each history of play is observed rather than the history itself.
Here, the use of the term ``first-order'' refers to the fact that the simulacrum does not necessarily imitate the actual equilibrium that generated the histories of play, 
% \samy{but is just an \emph{artificial placemarker},}{} \deni{I liked the phrase ``artificial placemaker'' because \citep{baudrillard1994simulacra} uses it. We can delete it but up to you.} 
since multiple equilibria can generate the same histories of play \citep{baudrillard1994simulacra}.
%\amy{is this latter part true: ``multiple equilibria can generate the same histories of play''? this explanation makes more sense to me for the second-order version, when info is lost. aggregate info, like the sum is 3, can come from agent A with value 2 and agent B with value 1, or conversely.} \deni{Yes, correct, this great, thank you!}
More generally, if the simulacrum is ``second-order,'' it is nonfaithful, meaning some information about the sample histories of play is lost.
We refer to the problems of computing first-order (resp.\@ second-order) simulacra as \mydef{first-order (resp.\@ second-order) simulacral learning}: i.e., build a first-order (resp.\@ second-order) simulacrum from faithful (resp.\@ non-faithful; e.g., aggregate agent behavior) sample histories of play.
%
We summarize the problems characterizing inverse game theory in \Cref{table:inverse_gt_summary}.

\if 0
\amy{i cannot parse. what is important about this next sentence?} \deni{I think you captured it in the previous paragraph.}
\sdeni{}{As one cannot observe the histories of play directly, the problem can only be interpreted as the problem of learning from a non-faithfully simulated copy of the players. In such cases, as information about the Nash equilibrium strategies might be lost, it is only possible to recover a non-faithful representation of the equilibrium strategies that generated the observations, in which case we are concerned with computing a \mydef{second-order Nash simulacrum} \citep{baudrillard1994simulacra}.} 
\fi

\if 0
Our first contribution is to formally pose the inverse  Nash equilibrium computation problem for \mydef{inverse Markov games} (i.e., a Markov game with the payoff functions omitted and observed strategy profile) and solve it, assuming \emph{exact} first-order oracle access to the parameter-dependent payoffs of the inverse game (i.e., assuming access to a game simulator that can be used to compute the value and gradient of the players' parameter-dependent reward functions, as well as the transition probability function of the game for any action profile and parameter); a problem known as \mydef{inverse multiagent planning}.
%\deni{Ok I think now this matches with the rest of the text and makes a lot of sense, thanks!!} \amy{well, i still don't like it!}
%As deterministic equilibria are not guaranteed to exist, players' strategies and thus their payoffs can be stochastic, in which case the latter cannot be accessed directly and instead have to be estimated via simulation.\amy{REWRITE!}
%
We then relax the assumption of exact first-order oracle access to \emph{noisy} first-order oracle access to the parameter-dependent payoff functions of the inverse game (i.e., assuming access to a game simulator that can be used to obtain noisy \emph{estimates\/} of the value and gradient of the players' parameter-dependent reward functions, as well as the transition probability function of the game for any action profile and parameter), extending our results from the realm of inverse game theory into the realm of \mydef{inverse multiagent reinforcement learning (inverse MARL)} \citep{russell1998learning, ng2000algorithms,natarajan2010multi}. Our results directly extend to normal-form, Bayesian and extensive-form games as Markov games can model these games \emph{mutatis mutandis}, in which case the inverse multiagent planning problem and the inverse MARL problem are respectively known under the names of \mydef{inverse multiagent optimization} and \mydef{inverse multiagent learning}.
%\sdeni{}{More generally, when the game is also non-stationary \amy{i really do not get this claim about non-stationarity. Markov games are stationary!!! maybe NE policies are not in general, but the themselves games are.}, the relevant model is that of stochastic (or Markov) games \citep{shapley1953stochastic,takahashi1964equilibrium,fink1964equilibrium}---time-based games where players observe a new state at each time-step and simultaneously take strategies which stochastically determines their next state and yields an instantaneous payoff---}
% \sdeni{}{When the underlying game is also stochastic in the sense of \citet{shapley1953stochastic}, \citet{fink1964equilibrium}, and 
% \citet{takahashi1964equilibrium},the relevant problem is one of \mydef{inverse multiagent reinforcement learning} (inverse MARL) \citep{russell1998learning, ng2000algorithms,natarajan2010multi}.
\fi


%\amy{does Ng really attempt inverse MARL in that first 2000 paper? i didn't remember that?} \deni{No the first two papers are pure IRL but felt appropriate to cite, we can remove tho.}
%One can similarly define various equilibrium concepts such as Nash equilibrium for Markov games, but now in terms of policy profiles (i.e., mappings from states to action profiles), and then seek to compute game parameters (i.e., an inverse equilibrium) that rationalize observed behavior.
%\amy{how is it an instance of inverse MARL if we no longer have a simulator/generative model? isn't then apprenticeship learning the more general problem?} is 
% \deni{I want to replace the subsequent paragraph with the following one. I think the progression of the intro then becomes clear: Game Theory => Inverse Game Theory=> Simulation Theory. In game theory the problem of finding an equilibrium assuming an exact first-order oracle access to payoff  find an equilibrium ass the equilibrium problem we use (reinforcement) learning, in inverse game theory to solve inverse equilibrium problem we use inverse (reinforcement) learning, and in Simulation Theory to solve the simulacrum problem, we use simulative (or multiagent apprenticeship) learning.}

\if 0
\begin{table}[]
\renewcommand{\arraystretch}{2} 
\begin{tabular}{|l||c|c|}
\hline
Setting & Exact Oracle & Noisy Oracle \\
\hline \hline
Fine-Grained Eqm. Samples & Multiagent Apprenticeship Planning & Multiagent Apprenticeship Learning \\
\hline
Coarse-Grained Eqm. Samples & Simulation Planning & Simulation Learning \\
\hline
\end{tabular}
\caption{Taxonomy of learning from observation (or demonstration) problems based on the oracle model and demonstration samples.}
\end{table}
\fi

% \begin{table}[]
% \renewcommand{\arraystretch}{2} 
% \begin{tabular}{|l|c|c|}
% \hline
% Setting & Exact oracle & Noisy Oracle  \\
% \hline
% Game Theory & \makecell[c]{Multiagent Optimization\\ {\color{blue}(Multiagent Planning)} }  & \makecell[c]{Multiagent Learning\\ {\color{blue}(Multiagent Reinforcement Learning)} }\\
% \hline
% Inverse Game Theory & \makecell[c]{Inverse Multiagent Optimization\\ {\color{blue}(Inverse Multiagent Planning)} }  & \makecell[c]{Inverse Multiagent Learning\\ {\color{blue}(Inverse Multiagent Reinforcement Learning)} } \\
% \hline
% Simulation Theory & Simulative Planning & \\
% \hline
% %  & Multiagent Optimization (Planning) & Inverse Multiagent Optimization (Planning) &  \\
% % \hline
% % \makecell[l]{Noisy oracle obtained\\ from a game simulator} &  \makecell[c]{Multiagent Learning ({\color{blue} MARL})} & Inverse multiagent (Reinforcement) Learning & Simulative Learning (Multiagent Apprenticeship Learning) \\
% % \hline
% \end{tabular}
% \caption{Taxonomy of game-theoretic problems based on the oracle model assumed for payoff functions and the game-theoretic settings.}
% \end{table}

\if 0
Nevertheless, in practice, in addition to not knowing payoff functions of the game, we also have access to the observed strategies only via noisy samples of realized equilibrium histories prepared \emph{offline} (i.e., sequences of state and action profile pairs). As such, to compute an inverse equilibrium, in addition one has to search over strategies which correspond to an equilibrium for the inverse equilibrium computed but also replicates the realized equilibrium histories, a problem known as \mydef{multiagent apprenticeship learning} \citep{abbeel2004apprenticeship, yang2020inferring}. That is, by generalizing inverse MARL, multiagent apprenticeship learning not only seeks to provide an explanation of observed behavior, but in addition, upon computing the associated equilibrium, allows replicate past observed behavior into the future. \deni{When the samples of equilibrium are lossy, then we have simulative learning, and all of these problems consitute Simulation theory...} Unforunately, when the samples of realized  
% \samy{, not only to provide an explanation, but also to predict future behavior by finding an equilibrium that replicates the observed behavior}{} \deni{This edit is incorrect, the goal of apprenticeship learning is not to learn the parameters, it is to learn the policy, and learning the parameters just makes the task easier.}.
\fi

\if 0
\sdeni{More generally, in
\mydef{multiagent apprenticeship learning} \citep{abbeel2004apprenticeship, yang2020inferring}, the theorist is given a fixed set of sample trajectories of play (prepared \emph{offline}), based on which she seeks parameter values which induce payoff functions that rationalize the players' observed behavior\samy{, not only to provide an explanation, but also to predict future behavior by finding an equilibrium that replicates the observed behavior}{} \deni{This edit is incorrect, the goal of apprenticeship learning is not to learn the parameters, it is to learn the policy, and learning the parameters just makes the task easier.}.
\samy{}{So doing not only provides an explanation of observed behavior, upon computing an ensuing equilibrium, it further provides counterfactual predictions of unobserved behavior.} \deni{This edit is also incorrect.}
%Given noisy payoff samples obtained by simulating the players' policies,\amy{this sounds just like inverse MARL; distinction is not clear}\deni{I think if you are referring to the idea of learning only from samples, I think that's a step ahead, and would be offline multiagent apprenticeship learning, which is a contribution we make but I think it should be part of the contributions.}
Multiagent apprenticeship learning thus involves finding a \mydef{simulacrum}, meaning parameter values together with associated equilibrium policies, which replicate the observed trajectories of play as samples from the simulacrum policies, in expectation.
In this paper, we tackle all of these problems---the first two in the \mydef{online} case, where we have simulator access and can sample any history associated with an observed equilibrium; and the last in the \mydef{offline} case, where we have only a noisy sample of realized equilibrium histories. \amy{histories or trajectories? be consistent!}}{}
%associated with an unobserved equilibrium.
\fi
% \sdeni{}{in expectation replicate} an observed sample trajectory drawn from this policy profile as an equilibrium. 
% \amy{DEFINE SIMULACRUM here! talk about what replication means: i.e., only in expectation!}

% \amy{probably delete:} \samy{However, due to the non-stationarity of the game as well as possible stochasticity of the policies, one often cannot observe the entire policy, but rather only noisy samples of (state, action) tuples, and has to learn parameters from these samples.}{} 
%For instance, if we seek to model and explain the movement of two teams of soccer players on the field, we might at any second observe the positions of the players on the field, but we will never observe their movement in every single possible positioning of the player on the field.\amy{i'm not convinced. with cameras overhead, we can observe everyone's movements. i think you mean to say that we do not observe their policies. but you have not said this.}

%\deni{Need to add text on the issue of parametrization modelling which we ignore.}



\if 0
%%% SPACE
In this paper, we tackle inverse multiagent planning, inverse multiagent learning, and simulacral learning. 
For concreteness, we focus on inverse Nash equilibrium, but our results, which are formulated in terms of the players' regret, can easily be extended to other inverse equilibrium (e.g., inverse (coarse) correlated \citep{kuleshov2015inverse}), as all equilibrium concepts can be understood in terms of generalized notions of regret \citep{greenwald2003general}. 
% and to inverse equilibrium problems beyond games \citep{arrow-debreu, facchinei2010generalized}).
A growing literature has been dedicated to inverse game theory problems under various guises, including parameter identification in econometrics \citep{wright1928tariff, syrgkanis2017inference}, model estimation in microeconomics/macroeconomics \citep{taylor1979estimation}, and multiagent imitation learning \citep{song2018multi}.
While there has been some progress on devising mathematical characterizations of inverse correlated equilibrium in 
%\amy{should the word Bayesian appear somewhere in this sentence?}
% \deni{i am avoiding the language ``normal-form'' since we do not really define it anywhere} \amy{well, we should really add one-shot everywhere then, to be precise, so maybe we should use the NFG terminology and just add a footnote saying ``A normal-form game is a Markov game with one state at which players take actions after which the game immediately terminates.''} \amy{and, if you keep reading this page, you will find that you use the term normal-form in multiple places. better to just define it!}
inverse \sdeni{}{finite action} normal-form games \citep{waugh2013computational, kuleshov2015inverse} and Bayesian games \citep{syrgkanis2017inference}, and inverse Nash equilibrium in restricted classes of finite state and action Markov games \citep{lin2017multiagent, lin2019multi}; very little is known about the computational complexity of inverse Nash equilibrium in \sdeni{}{finite action} normal-form games, concave games, general-sum (finite or continuous state and action) Markov games.
%Furthermore, most theoretical guarantees for algorithms that compute inverse equilibria concern inverse correlated equilibria in inverse normal-form or Bayesian games with discrete actions; very little is understood about the computational complexity of inverse equilibria in general-sum normal-form or Markov games, the latter with continuous action or state spaces.
%\sdeni{, or the sample complexity of the inverse equilibria in Markov games, where only sample trajectories from equilibrium policies may be observed}{}.\amy{i am not sure if this second half of the sentence is supposed be crossed out or not.} \deni{Supposed to be crossed-out.}
This paper develops a flexible mathematical and algorithmic framework based on min-max optimization to solve problems in inverse game theory efficiently for a wide variety of games, with the ultimate goal of making predictions based on inverse equilibria.
\fi



% \alec{there are a lot of terms being thrown around...would it be possible to visualize a schematic of the inverse MARL/equilibrium computation problem? i.e. given time-series of, e.g. prices, rationalizing the demand is associated with computing some equilibrium? Perhaps a little cartoon of what it looks like for energy market or congestion game? specifically a diagram could depict what the generator and discriminator does in the inverse multi-agent game setting}

\if 0
To this end, we first provide a zero-sum game (i.e., min-max optimization problem) that characterizes the solutions of any inverse game for which the set of inverse Nash equilibria is non-empty (\Cref{thm:inverse_NE}).
%, in spite of possibly stochastic equilibrium policies.
We then show that for any inverse concave game, when the regret of each player is convex in the parameters of the inverse game, an assumption satisfied by a large class of inverse games such as inverse discrete-action normal-form games, this min-max optimization problem is convex-concave, and can thus be solved in polynomial time (\Cref{thm:concave_game_inverse_NE}) with an exact or stochastic first-order oracle.
This result shows that the set of inverse Nash equilibria can be convex, even when the set of Nash equilibria is not; consequently, inverse multiagent planning and learning are polynomial-time problems, for all inverse concave games.
%
Second, we apply our our min-max characterization to inverse MARL%
\footnote{more specifically, multiagent apprenticeship learning} to show that under standard assumptions, which are satisfied by a large class of inverse Markov games (e.g., all finite state and action Markov games \samy{}{and a large class of continuous state and action Markov games}), the ensuing min-max optimization problem is convex-gradient dominated, and thus amenable to standard gradient descent ascent methods  (\Cref{thm:inverse_stoch_NE}).\amy{below, we say via policy gradient}
We thus characterize Markov games for which inverse multiagent learning 
%(i.e., the computation of an inverse Nash equilibrium assuming access to a game simulator) 
is polynomial time (\Cref{thm:online_sgda}). 
%
Third, we provide an extension of our min-max characterization to simulacral learning (\Cref{thm:inverse_simulacrum}).
%, and we seek to compute a Nash simulacrum, i.e., parameters and associated Nash equilibrium policies, which replicate the observed state-action trajectories in expectation \sdeni{. 
As above, we characterize the set of simulacra associated with any multiagent simulacral learning problem as the set of solutions to a min-max optimization problem.
Furthermore, we develop a policy gradient algorithm that computes locally optimal \amy{stationary?} solutions to this problem in polynomial time (\Cref{thm:apprenticeship_thm}).
Finally, we run experiments and find that our algorithm outperforms other widely-used methods \amy{state-of-the-art? add NN experiments} in predicting prices in Spanish electricity markets based on time-series data.
\fi

\if 0
We formulate inverse multiagent planning (resp. inverse, i.e., the computation of an inverse equilibrium (resp.\@ simulacrum) in inverse games as a zero-sum game (i.e., a min-max optimization problem) between a generator and a discriminator.
Given an observed strategy (resp. possibly non-faithful observed history of play samples), the generator selects a candidate inverse equilibrium (resp.\@ a candidate simulacrum), which the discriminator then ``judges'' by computing a best-response for each player (\Cref{thm:inverse_NE}).
%at the generator's parameters. 
In computing inverse equilibria (resp.\@ simulacra), the generator's goal is to choose parameters (resp.\@ parameters and a candidate equilibrium) that minimize the cumulative regret across all players
%(i.e., the sum of the per-player changes in utility for unilaterally deviating from one profile to another)
between the observed (resp.\@ candidate) equilibrium and the profile chosen by the discriminator, while the goal of the discriminator is to choose a strategy profile that maximizes this regret.
We then show that for any inverse concave game, when the regret of each player is convex in the parameters of the inverse game, an assumption satisfied by a large class of inverse games such as inverse discrete-action normal-form games, this min-max optimization problem is convex-concave, and can thus be solved in polynomial time (\Cref{thm:concave_game_inverse_NE}).
This characterization also shows that the set of inverse Nash equilibria can be convex, even when the set of Nash equilibria is not.
Second, we generalize our min-max characterization to online inverse MARL, where we are given an inverse Markov game, and 
%, once again, possibly Markov equilibrium policies, but 
correspondingly, only a \emph{stochastic\/} first-order oracle (\Cref{thm:inverse_stoch_NE}).
%that can only be accessed via \samy{}{a simulator that responds to queries with} sample \samy{queries}{trajectories} from the state-action history distribution.
% This setting covers inverse Nash equilibrium problems where the players' action might be noisy, as well as inverse multiagent reinforcement learning, when we are given a sample trajectory of play.  
We show that under standard assumptions, which are satisfied by a large class of inverse Markov games such as finite state and action Markov games, the ensuing min-max optimization problem is convex-gradient dominated, and thus an inverse Nash equilibrium can be computed via a policy-gradient algorithm in polynomial time (\Cref{thm:online_sgda}).
%
Third, we provide an extension of our
%%% SPACE
%generative-adversarial
min-max characterization to multiagent apprenticeship learning (\Cref{thm:inverse_simulacrum}).
%, and we seek to compute a Nash simulacrum, i.e., parameters and associated Nash equilibrium policies, which replicate the observed state-action trajectories in expectation \sdeni{. 
We once again characterize the problem as a solution to a min-max optimization problem.
Furthermore, we develop a policy gradient algorithm that computes local\samy{}{ly optimal} solutions to this problem, which, in the offline setting, generalizes to unseen observations in a polynomial number of noisy state-action trajectories (\Cref{thm:apprenticeship_thm}).
\amy{short version. DELETE if there is space for the full paragraph below!!!}
Finally, we run experiments and find that this algorithm outperforms other widely-used 
%%% SPACE
%statistical 
methods \amy{reviewer complained: we only tried one other} in predicting prices in Spanish electricity markets based on time-series data.


% Our formulation is fully general in the sense that it can be extended to any game and equilibrium concept for which a notion of regret is well-defined (see, for instance \citep{morrill2022hindsight}).
Our methods provide an efficient way to fit game-theoretic models onto real-world data, and in the case of multiagent apprenticeship learning, make behavioral predictions. 
To demonstrate this fact, we model the Spanish electricity market as a game between electricity re-sellers who set prices so as to maximize their profit and consumers who demand electricity so as to maximize their utilities.
We then compute a Nash simulacrum that replicates real-world price and demand in this market through 2018, which we test on electricity prices from 2018 to 2020 by simulating the simulacrum policies.
\fi


%\sdeni{}{Finally, we show that our formulation can be extended to multiagent apprenticeship learning, where the goal is not only to explain observed behavior but to also replicate it.}

% then takes as input based on which it outputs an equilibrium for the game associated with these payoff functions.
% Our method is ``generative,'' as it generates parameters of a game, that are intended to reproduce equilibria; and it is ``simulative'' as it can only simulate the equilibrium associated with the generated parameter.
% GGS can be used to compute an inverse equilibrium whenever the exploitability, i.e., distance of an action profile from an equilibrium in payoff space, can be computed efficiently.  


% \amy{all this stuff about the Spanish electricity market belongs somewhere in the introduction.}
% For instance, in our experimental set-up in \deni{Add spanish electricity  market prices experiement forward ref.}, we model the Spanish electricity market as a game between electricity re-sellers who set prices so as to maximize their profit and consumers who demand electricity so as to maximize their utilities.
% In this setting, electricity providers only collect data on prices and the aggregate demand of the consumers, not the particular demands of the individual consumers, which comprise their equilibrium actions.
% %, the sum of the demands across all consumers. 
% As such, we do not have access to individual electricity consumptions, meaning that in our game model the observation distribution takes as input price and demand policies of the sellers and consumers respectively and outputs a trajectory of prices for the sellers, and \emph{the sum} of the demand trajectories across all consumers.  
