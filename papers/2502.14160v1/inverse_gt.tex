\section{Inverse Multiagent Planning}
\label{sec:inverse_planning}

The goal of inverse multiagent planning is to invert an equilibrium: i.e., estimate a game's parameters, given observed behavior. 
In this section, we present our main idea, namely a zero-sum game (i.e., min-max optimization) characterization of inverse multiagent planning, where one player called the stabilizer picks parameters, while the other called the destabilizer picks per-player deviations.
This game is zero-sum) because the stabilizer seeks parameters that rationalize (i.e., minimize the exploitability of) the observed equilibrium, while the destabilizer aims to rebut the rationality of the observed equilibrium (i.e., seeks deviations that maximize cumulative regret).
%
We use this characterization to develop a gradient descent ascent algorithm that finds inverse NE in polynomial time, assuming access to \mydef{an exact first-order oracle}: specifically, a pair of functions that return the value and gradient of the payoff profile function.
%To our knowledge, ours is the first polynomial-time algorithm to compute inverse NE in inverse games.

An \mydef{inverse game} $\game[][-1] \doteq 
%(\numplayers, \numactions, \numparams, \stratspace, \params, \util, \truestrat)
(\game[][\paramtrue] \setminus \paramtrue, \truestrat)$ comprises a \mydef{game form} (i.e., a parametric game \emph{sans\/} its parameter) $\game[][\paramtrue] \setminus \paramtrue$ together with an observed strategy profile $\truestrat$, which we assume is a Nash equilibrium.
Crucially, we do not observe the parameters $\paramtrue$ of the payoff functions.
% 
% with payoff function $\util[\player]: \stratspace \times \params[\player] \to \R$ strategy and type spaces $\stratspace[\player] \subset \R^\numactions$ and $\params[\player] \subset \R^\numparams$ for every player $\player \in \players$. We denote the players' \mydef{joint strategy and type space} respectively by $\stratspace \doteq \bigtimes_{\player \in \players} \stratspace[\player] \subset \R^{\numplayers \numactions}$ and $\params \doteq \bigtimes_{\player \in \players} \params[\player]$ 
% 
% The players have played the $\varepsilon$-NE $\truestrat \in \stratspace$ in a simultaneous-move game whose payoff function we have not observed but which we know comes from a set of payoff functions $\utilspace \subset  \{\util: \stratspace \to \R^\numplayers \}$. 
Given an inverse game $\game[][-1]$, our goal is to compute an \mydef{$\varepsilon$-inverse Nash equilibrium}, meaning parameter values $\param[][][*] \in \params$ s.t.\@ $\truestrat \in \stratspace$ is an $\varepsilon$-NE of $\game[][{\param[][][*]}]$.
As usual, a 0-inverse NE is simply called an inverse NE.
Note that this definition does not require that we identify
%\amy{recover? maybe identify is okay. it is a technical term. so as long as we are using it in this technical way.} \deni{identify is great} 
the true parameters $\paramtrue$, as identifying $\paramtrue$ is impossible unless there exists a bijection between the set of parameters and the set of Nash equilibria, a highly restrictive assumption that is not even satisfied in games with a unique Nash equilibrium.
% \deni{Should we add an example? Yes, if space.} \amy{haha!}
% $\param[\player] \in \params[\player]$ denotes the type of player $\player$. $\utilspace[\player] \subset \R^{\stratspace}$ s.t. $\inner[][][*] \in \innerset$ is a $\varepsilon$-Nash equilibrium of the Nash game $(\numplayers, \numactions, \innerset, \util[][][*])$. \deni{Note that by definition of the problem I assume that the strategy profile is a $\varepsilon$-NE, so I do not have to make the assumption in my observation or the rest of the paper: Maybe worth stating it explicitly? not sure...}
% \deni{A utility function $\util$ is said to $\varepsilon$-rationalize $\inner[][][*]$, if it is an inverse $\varepsilon$-Nash equilibrium.}
% A solution to the inverse $\varepsilon$-NE problem $(\numplayers, \numactions, \innerset, \inner[][][*])$ is a utility function $\util$ which 
To compute an inverse NE is to find parameter values that minimize the exploitability of the observed equilibrium.
This problem is a min-max optimization problem, as the parameter values that minimize exploitability are those that maximize the players' cumulative regrets.
More precisely:

\if 0
The following theorem casts the computation of an inverse NE as a min-max optimization problem, one of finding parameter values that minimize the sum of the players' maximum regrets, assuming the others play according to the observed equilibrium.
%, whose set of  $\varepsilon$-optimal solutions to the outer minimization correspond to inverse $\varepsilon$-Nash equilibria.
\sdeni{Intuitively, the key idea is simple: find parameter values that minimize the exploitability of the observed equilibrium.}{}
\fi

%\deni{I was thinking of not making references to exploitability, the story follows without it and I think adds additional layers of math.} \amy{very little math. lots of intuition. but dropping saves space.} \deni{Removed the $\exploit$, I am using it in the last section and I want to avoid confusion.} \amy{i don't think defining exploitability adds confusion. i think it makes things more intuitive. also, i see that you use $\exploit$ later (in Thm 5.2), but i don't actually see it defined anywhere.}

\begin{restatable}{theorem}{thminverseNE}
\label{thm:inverse_NE}
    The set of inverse NE of $\game[][-1]$ is the set of parameter profiles $\param \in \params$ that solve the optimization problem $\min_{\substack{\param \in \params}} \exploit (\truestrat; \param)$, or equivalently, this min-max optimization problem:
    \begin{align} 
        \min_{\substack{\param \in \params}} \max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat) \doteq 
        \cumulregret[] (\truestrat, \otherstrat; \param) = \sum_{\player \in \players} \left[\util[\player] (\otherstrat[\player], \truestrat[-\player]; \param) - \util[\player] (\truestrat; \param) \right]
        \label{eq:min_max_gen_sim}
    \end{align}
\if 0
    \begin{align}
        \min_{\substack{\param \in \params}} \max_{\strat \in \stratspace} \obj (\param, \strat) \doteq  \underbrace{\sum_{\player \in \players} \left[ \util[\player] (\strat[\player], \truestrat[-\player]; \param) - \util[\player] (\truestrat; \param) \right]}_{ = \cumulregret[] (\truestrat, \strat; \param)}
        \label{eq:min_max_gen_sim}
    \end{align}
\fi   
\vspace{-1.5em}
%     \begin{align}
%     & \min_{\substack{\param \in \params \\ \strat \in \stratspace}} &\left\|\truestrat - \strat \right\|^2 \\
%     &\text{subject to } & \exploit(\strat; \param) \doteq \max_{\otherstrat \in \stratspace} \cumulregret[] (\strat, \otherstrat; \param) \leq \varepsilon
% \end{align}
\end{restatable}
% \deni{Regularization Business is important here!!!}

\amy{Generalization of inverse optimization to game-theoretic settings? do you cite \href{https://homes.cs.washington.edu/~todorov/papers/DvijothamICML10.pdf}{this paper}, or others by Todorov?}

This min-max optimization problem can be seen as a generalization of the dual of \citeauthor{waugh2013computational}'s (\citeyear{waugh2013computational}) maximum entropy likelihood maximization method for games with possibly continuous strategy spaces, taking Nash equilibrium rather than maximum entropy correlated equilibrium as the inverse equilibrium. 
In contrast to \citeauthor{waugh2013computational}'s dual, our min-max optimization problem characterizes the set of \emph{all\/} inverse NE, and not only a subset of the inverse correlated equilibria, in particular those that maximize entropy.
This formulation also generalizes \citeauthor{swamy2021moments}'s (\citeyear{swamy2021moments}) moment matching game from a single-agent to a multiagent setting.
 

\input{algos/sgda} 


Without further assumptions, the objective function $\obj$ in \Cref{eq:min_max_gen_sim} is non-convex non-concave; however, 
under suitable assumptions (\Cref{assum:concave_game}) satisfied by
%important classes of games, such as 
finite action normal-form games, for example, it becomes convex-concave.

\amy{how natural is the convex regret assumption? how often does it follow assuming concave utilities?} \deni{It works in finite-action games games.} \amy{let's make sure this is explained somewhere.}

\begin{assumption}
%[Concave Game \& Convex Parametrization]
\label{assum:concave_game}
    Given an inverse game $\game[][-1]$, assume 1.~(Concave game) for all parameters $\param \in \params$, $\game[][\param]$ is concave; and 
    2.~(Convex parametrization) $\params$ is non-empty, compact, and convex; and for all $\forall \player \in \players$, $\otherstrat[\player] \in \stratspace[\player]$, and $\truestrat \in \stratspace$, each player $\player$'s regret $\param \mapsto \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param) - \util[\player] (\truestrat; \param)$ is convex.
    %3.~(Lipschitz-smooth game) $\util$ is $\lipschitz[{\grad \util}]$-Lipschitz-smooth.
\end{assumption}

% \begin{assumption}[Convex Parameterization]
% \label{assum:convex_parametrization}
%     Given an inverse game $\game[][-1]$, 
% \end{assumption}

%If we make the above assumptions, that the game is concave and it is parametrized in a convex fashion, i.e., set of parameters is convex and the regret of each player is convex in its parameters, we can guarantee that $\obj$ is convex-concave. 

\begin{remark}
\label{rem:convex}
    Perhaps surprisingly, the set of inverse NE can be convex even when the set of NE is not, since the set of solutions to a convex-concave (or even convex-non-concave)\deni{removed footnote} 
    % \footnote{Recall that the pointwise maximum of convex functions is convex \citep{danskin1966thm}. Hence, the set of solutions to the minimization is convex if the constraint set is (see Theorem 2.6 of \citet{rockafellar2009variational}).}
    min-max optimization problem is convex. 
    \sdeni{More concretely, while in normal-form games the set of Nash equilibria is non-convex beyond highly structured classes such as zero-sum, potential, and monotone games, the set of inverse Nash equilibria is convex for all normal-form games.}{}
    This observation should alleviate any worries about the computational intractability of inverse game theory that might have been suggested by the computational intractability of game theory itself \citep{daskalakis2009complexity, chen2006settling}. 
\end{remark}

% \deni{We need assumptions here!!}
% \begin{assumption}
%     Consider the inverse game $(\numplayers, \numactions, \stratspace, \truestrat)$, and suppose that the following hold:
%     1.~
% \end{assumption}

% In general, since games are defined by parametric payoff functions, it is more suitable to assume a parametric form on the payoff functions as a function of the types of the players, i.e., redefine $\util[\player]: \stratspace \times \params[\player] \to \R$ such that $\util[\player] (\strat; \param[\player])$ is the payoff of player $\player \in \players$ for any type $\param[\player] \in \params[\player]$ coming from type space $\params[\player] \subset \R^{\numparams}$, and instead seek to learn the types of the players rather than their payoff functions. As such we will henceforth denote an inverse game by $(\numplayers, \numactions, \stratspace, \params, \util, \truestrat)$ and  and redefine the cumulative regret $\exploit[][\param] \doteq \exploit[][\util(\cdot; \param)]$ and the exploitability $\exploit[][\param] \doteq \exploit[][\util(\cdot; \param)]$ as a function of the type of the players. 

% The inverse $\varepsilon$-NE problem can then be recast as the following optimization problem instead:
% \begin{align}
%     & \min_{\substack{\param \in \params \\ \strat \in \stratspace}} &\left\|\truestrat - \strat \right\|^2\\
%     &\text{Subject to } & \max_{\otherstrat \in \stratspace} \cumulregret[][{\param}] (\strat, \otherstrat) \leq \varepsilon
% \end{align}

% \begin{remark}
%     The set of solutions of the above optimization is equal to the set of strong Stackelberg-Nash equilibria of a general-sum Stackelberg-Nash game with one leader and $\numplayers \in \N_{++}$ followers, in which the leader sets the types of the followers, who then respond by playing a Nash equilibrium based on these types. The goal of the leader is to pick types that minimize the Euclidean norm between the observed Nash equilibrium and the Nash equilibrium induced by the types chosen by the leader.
% \end{remark}

% Define the \mydef{leader-regularized cumulative regret}, the \mydef{follower-regularized cumulative regret}, and the \mydef{leader-regularized exploitability},
% %%% SPACE
% %\footnote{Note that these definitions of the regularized cumulative regret and exploitability are different than those provided by \citeauthor{goktas2022exploit} \citep{goktas2022exploit}.} 
% as $\regulcumulregret[\outer] (\inner, \otherinner; \outer) \doteq \cumulregret(\inner, \otherinner; \outer) + \frac{\lipschitz[\cumulregret]}{2} \left\| \outer \right\|^2$, $\regulcumulregret[\inner] (\inner, \otherinner; \outer) \doteq \cumulregret(\inner, \otherinner; \outer) - \frac{\lipschitz[{\grad \cumulregret}]}{2} \left\| \inner \right\|^2$, and $\regulexploit[\outer] (\inner; \outer) \doteq \max_{\otherinner \in \innerset} \regulcumulregret[\outer] (\inner, \otherinner; \outer)$, respectively

If additionally, we assume the players' payoffs are Lipschitz-smooth (\Cref{assum:smoothness}), \Cref{eq:min_max_gen_sim} can then be solved to $\varepsilon$ precision in $O\left( \nicefrac{1}{\varepsilon^2} \right)$ via gradient descent ascent (\Cref{alg:gda}). 
That is, as \Cref{thm:concave_game_inverse_NE} shows, an inverse $\varepsilon$-NE can be computed in $O\left( \nicefrac{1}{\varepsilon^2} \right)$ iterations.%
\footnote{We include detailed theorem statements and proofs in \Cref{sec:app_proofs}.}
We note that this convergence complexity can be further reduced to $O\left( \nicefrac{1}{\varepsilon} \right)$ (even without decreasing step-sizes) if one instead applies an extragradient descent ascent method \citep{golowich2020eglast} or optimistic GDA \citep{gorbunov2022last}.
%, which we omit for simplicity.

\begin{assumption}[Lipschitz-Smooth Game]
\label{assum:smoothness}
    %%% SPACE 
    %Given an inverse game $\game[][-1]$, 
    For all players $\player \in \players$, $\util[\player]$ is $\lipschitz[{\grad \util[\player]}]$-Lipschitz-smooth.
    %%% SPACE
    %for all players $\player \in \players$.
\end{assumption}

% \textcolor{cyan}{Alec: what is new/interesting about this result? help me contextualize it w.r.t. prior art.}\amy{please go ahead and suggest clarifications, Alec! thanks!}

\begin{restatable}[Inverse NE Complexity]{theorem}{thmconcavegameinverseNE}
\label{thm:concave_game_inverse_NE}
    Under Assumptions \ref{assum:concave_game}--\ref{assum:smoothness}, for $\varepsilon \geq 0$, if \Cref{alg:gda} is run with inputs that satisfy $\numiters \in \Omega(\nicefrac{1}{\varepsilon})$ and for all $\iter \in [\numiters]$, $\learnrate[\otherstrat][\iter] = \learnrate[\param][\iter] \asymp \nicefrac{1}{\lipschitz[\grad \obj]}$, 
    then the time-average of all parameters $\mean[{\param[][\numiters]}] \doteq \frac{1}{\numiters + 1}\sum_{\iter = 0}^\numiters \param[][\iter]$ is an $\varepsilon$-inverse NE.
    % , i.e., $\cumulregret[] (\truestrat, \otherstrat; \mean[{\param[][\numiters]}]) - \min_{\substack{\param \in \params}} \max_{\otherstrat \in \stratspace} \cumulregret[] (\truestrat, \otherstrat; \param) \leq \varepsilon$.
    %\deni{correct just seemed superfluous.}
\end{restatable}


\if 0
\amy{some good ideas, but delete. check that anything important is mentioned elsewhere!}
Unfortunately, running gradient descent ascent is not feasible in stochastic games.
Because the payoffs of the game cannot be accessed directly---they can only be observed via noisy queries placed for individual policy profiles---the gradient of the cumulative regret likewise cannot be accessed directly. 
Additionally, even with \samy{}{exact} oracle access to the payoffs of the game, a large class of stochastic games, including but not limited to finite state and finite strategy stochastic games and linear-quadratic stochastic games\deni{Maybe, remove.}, are unfortunately not concave, requiring additional care. 
Nonetheless, our computational results can be extended to these games, in polynomially-many payoff queries. \amy{new measure of complexity: sample complexity!}
\fi
