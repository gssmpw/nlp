\section{Inverse Multiagent Reinforcement Learning}
\label{sec:inverse_marl}

% \deni{INSTEAD THIS SHOULD BE THAT PAYOFFS ARE NOT ACCESSIBLE, THAT IS A WAY CLEANER STORY.} 

%\deni{We need to remove the language of discriminator/generator.} \amy{we need other terms if we remove these. for the inner and the outer player. the optimist and the pessimist? the actor and the critic?}

\if 0
In Markov games, as the environment \sdeni{is non-stationary}{} \amy{it's not the environment that is non-stationary. in Markov games, our object of study, we assume stationary. (it is when agents are learning, but agents have finished learning by now and are just playing some fixed policy.) nor are eqm strategies, necessarily. (maybe they can be?) so WHAT is actually non-stationary?} and/or strategies can be stochastic \amy{strategies were already stochastic in NFGs; maybe not quite in our concave game model, but this sounds weird}, the games' payoff functions and their gradients cannot be directly accessed, but instead can be simulated, and thus estimated from noisy samples. 
% \samy{As repeated games can be modelled efficiently as Markov games with an infinitely repeated singleton state, we will focus on Markov games.}{} \amy{if important, add to intro somewhere that our model of Markov games encompasses repeated games. but distracts from the story here.}
\fi

%Because the payoffs of the game cannot be accessed directly---they can only be observed via noisy queries placed for individual policy profiles---the gradient of the cumulative regret likewise cannot be accessed directly. 

\if 0
In Markov games, the payoffs of interest are \emph{expected cumulative rewards} accrued over the course of game trajectories.
As a result, without knowledge of the history distribution associated with an observed equilibrium $\truepolicy$, exact first-order oracle access to a game's reward model, as we assumed in our inverse game theory analysis, is insufficient.
\fi

In this section, we build on our zero-sum game (i.e., min-max optimization) characterization of inverse game theory to tackle inverse MARL in an analogous fashion.
As it is unreasonable to assume exact oracle access to the players' (cumulative) payoffs in inverse MARL, we relax this assumption
%the earlier assumption of an exact first-order oracle (\Cref{sec:inverse_planning}) 
in favor of a stochastic oracle model.
More specifically, we assume access to a \mydef{differentiable game simulator} \citep{suh2022differentiable}, which simulates histories of play $\hist \sim \histdistrib[][\policy]$ according to $\histdistrib[][\policy]$, given any policy profile $\policy$, and returns the rewards $\reward$ and transition probabilities $\trans$,%
\footnote{We note that in inverse reinforcement learning, as opposed to reinforcement learning, it is typical to assume that the transition model is known (see, for instance \citep{abbeel2004apprenticeship}, Footnote 8).} 
encountered along the way, together with their gradients.
%%% MAKES NO SENSE rn; and no time for footnotes
%The intuitive reason for this assumption is that in practice, the game has to first be designed by a domain expert to learn its parameters. \deni{Expand.} \amy{definitely expand. not making sense yet.}
%based on which we can estimate deviation payoffs, and ultimately cumulative regrets.
\sdeni{We can then average across multiple simulations to estimate estimate deviation payoffs, cumulative regrets, and the requisite gradients, to recover the parameters $\param[][][*]$ for which $\truepolicy$ is a Nash equilibrium 
using a policy gradient algorithm that converges 
in polynomial time.}{}

% \amy{Alec: i am working on the intro to this section, but all the math is in place, so you can skip the intro for now, and keep commenting/reading from here on.}

% \alec{But please: hit me with the touch points to related literature, as well as previous sections, at the beginning of the section!}\amy{once again, suggestions welcome, Alec!}


\if 0
In this section, we assume \emph{stochastic\/} first-order oracle access to the parametric differentiable reward model $\reward: \states \times \actionspace \times \params \to \R^\numplayers$,\amy{i think this might be wrong. i think we are assuming STOCHASTIC access to the UTILITY model, NOT the reward model} as well as \amy{exact or stochastic?} access to the probability transition model, i.e., $\grad \trans$ in addition to $\grad \reward$.
\fi

% While in one-shot settings it is assumed that the environment is stationary and that equilibria are directly observed, in repeated-play settings, the game's outcome might be stochastic when the game is non-stationary and players' strategies are stochastic and as such only outcome samples from the Nash equilibrium policies might observed. 

% An important property of Markov games is that they have stochastic payoff functions of the form $\util[\player](\strat, \param) \doteq \Ex_{\outcomerv \sim \outcomedistrib[][][\strat]} \left[\util[\player](\outcomerv, \param) \right]$ where $\outcomedistrib[][][\strat]$ is an outcome distribution over a set of outcomes. Such games' equilibria can instead by observed repeated play settings, for which it is not possible to observe the Nash equilibrium $\truestrat$ directly, but rather through a sample of outcomes of the played games $\left\{\outcomerv[\numsample] \right\}_{\numsample \in \numsamples} \sim \outcomedistrib[][][\truestrat]$. For instance, in repeated normal-form games (i.e., Markov games with a unique state), the sample of outcomes are the sequence of pure actions that the players take, while in the more general Markov game setting the outcome is a trajectory of play, i.e., $\outcomedistrib[][][\strat] \doteq \histdistrib[\strat]$. 
% We then seek to learn parameters $\param[][][*] \in \params$ for the Markov game that with high probability replicate the observed trajectories $\left\{\hist[][][\numsample]\right\}$ as a Nash equilibrium of the Markov game. \deni{We will suppose that we can simulate trajectories in the game and that we have first order-oracle access to the reward function of the game, i.e., we can obtain queries from $\reward$.}

Formally, an \mydef{inverse Markov game} $\mgame[][-1] \doteq
%\doteq \sdeni{(\numplayers, \numactions, \numparams, \states, \actionspace, \params, \reward, \trans, \discount, \initstates, \truepolicy)
(\mgame[][\paramtrue] \setminus \paramtrue, \truepolicy)$ is an inverse game that comprises a \mydef{Markov game form} (i.e., a parametric Markov game \emph{sans\/} its parameter) $\mgame[][\paramtrue] \setminus \paramtrue$ together with an observed policy profile $\truepolicy$, which we assume is a Nash equilibrium.
Crucially, we do not observe the parameters $\paramtrue$ of the payoff functions.
% \samy{An \mydef{$\varepsilon$-inverse NE} is a setting of the parameter values $\param[][][*] \in \params$ s.t.\@ $\truepolicy \in \policies$ is an $\varepsilon$-NE of $\mgame[][{\param[][][*]}]$.
% As usual, a $0$-inverse NE is simply called a  inverse NE.}{}
Since a Markov game is a normal-form game with payoffs given by $\util(\policy; \param) = \Ex_{\histrv \sim \histdistrib[][\policy]} \left[\sum_{\iter = 0}^\infty \reward(\staterv[\iter], \actionrv[][][\iter]; \param) \right]$, the usual definitions of inverse NE and cumulative regret apply, and the following result, which characterizes the set of inverse NE as the minimizers of a \emph{stochastic\/} min-max optimization problem, is a corollary of \Cref{thm:inverse_NE}.

\begin{corollary}
    \label{thm:inverse_stoch_NE}
    The set of inverse NE of $\mgame[][-1]$ is characterized by solutions to the following
    %%% SPACE
    %stochastic min-max optimization 
    problem: 
    % \deni{I think I don't want to include the exploitability notation at all, because we really do not use it in the math, we only use the cumulative regret} 
    \begin{align}     
    %\samy{}{\min_{\substack{\param \in \params}} \exploit (\truepolicy; \param) =} 
        \min_{\substack{\param \in \params}} \max_{\policy \in \policies} \obj (\param, \policy) \doteq
        \sum_{\player \in \players}  \Ex_{\substack{\histrv \sim \histdistrib[][{(\policy[\player], \truepolicy[-\player])}]\\ \histrv[][\dagger] \sim \histdistrib[][\truepolicy]}} \left[\sum_{\iter = 0}^\infty \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) - 
        \sum_{\iter = 0}^\infty \reward[\player](\staterv[\iter][][\dagger], \actionrv[][][\iter][\dagger]; \param) \right] 
    \label{eq:inverse_stoch_NE}
    \end{align}
\end{corollary}

\amy{i think that perhaps all the $x$'s in this section should be changed to $y$'s, to match the notation in the previous section. but this is a dangerous change to attempt? so maybe what we should do is just change all the $y$'s to $x$'s in the SGDA algo?}\deni{I agree but I think let's do this for the Arxiv version.}

As is usual in \sdeni{deep}{}reinforcement learning, we use policy gradient to solve the destabilizer's problem in \Cref{eq:inverse_stoch_NE}.
To do so, we restrict the destabilizer's action space to a policy class $\policies[][\stratspace]$ parameterized by $\stratspace \subset \R^\numstrats$.
%\sdeni{in which case the problem we aim to solve becomes $\min_{\substack{\param \in \params}} \max_{\policy[\strat] \in \policies[][\stratspace]} \obj (\param, \policy)$.}{}
Redefining $\obj (\param, \strat) \doteq \obj (\param, \policy[][][\strat])$, for $\policy[][\strat] \in \policies[][\stratspace]$, we aim to solve the stochastic min-max optimization problem $\min_{\param \in \params} \max_{\strat \in \stratspace} \obj (\param, \strat)$. 
Solutions to this problem are a superset \amy{subset? actually, i'm not sure why they intersect at all?} \deni{superset, because if the policy class becomes less representative, then the destabilizer will become ``weaker'', and the stabilizer will ``win'' more often. They intersect because restricting the policy class only makes the action space of the destabilizer smaller, but anything that was in the larger action space is still part of the solutions. This is kinda like the intuition of how CE is CCE because the set of swap deviations is larger than the set external deviations.} of the solutions to \Cref{eq:inverse_stoch_NE},
unless it so happens that all best responses can be represented by policies in $\policies[][\stratspace]$, \deni{Deleted footnote}
% \footnote{As it is out of the scope of this paper and an active area of current research, we ignore representation issues as they pertain to policy classes in this paper; but we note that for a large class of Markov games, such as finite state and action and linear quadratic Markov games, optimal policy characterizations are well-understood \citep{agarwal2020optimality, bhandari2019global}. \amy{which reference is LQR games? might cross out finite state and action for space.} \deni{Agarwal for finite, Bhandari for both.}}
because restricting the expressivity of the policy class decreases the power of the destabilizer.
As in \Cref{sec:inverse_planning}, without any additional assumptions, $\obj$ is in general non-convex, non-concave, and non-smooth.
While we can ensure convexity and smoothness of $\param \mapsto \obj (\param, \strat)$ under suitable assumptions on the game parameterization, namely by assuming the regret at each state is convex in $\param$, concavity in $\strat$ is not satisfied even by finite state and action Markov games. 
Under the following conditions, however, we can guarantee that $\obj$ is Lipschitz-smooth,
%\amy{in both?} \deni{Yes, but i think the phrase already says that.}
convex in $\param$, and gradient dominated in $\strat$.
%\samy{, meaning that each player's cumulative payoff function is gradient dominated in the policy parameters $\strat$ fixing the policies of other players}{}.

%\sdeni{This latter condition implies that for all parameters $\param \in \params$, any stationary point of $\obj$ is also a global maximum $\strat \mapsto \obj (\param, \strat)$,
%%\samy{but not necessarily the other way round,}{} 
%as gradient-dominance allows us to upper-bound the distance between the function at any point and the global optimum in terms of the gradient of the function at that point.}{}
%%, hence allowing us to obtain convergence guarantees for online SGDA.

\begin{assumption}[Lipschitz-Smooth Gradient-Dominated Game]
\label{assum:smooth_convex_invex}
    Given an inverse Markov game $\mgame[][-1]$, assume
    1.~$\states$ and $\actionspace$ are non-empty, and compact;
    2.~(Convex parameter spaces) $\stratspace, \params$ are non-empty, compact, and convex; 
    3.~(Smooth Game) $\grad \reward$, $\grad \trans$, and $\grad[\strat] \policy[][][\strat]$, \amy{do we already assume earlier (or know) that $\reward, \trans, \policy[][][\strat]$ are differentiable?} \deni{No, I don't think so?} \amy{i guess i just don't understand why the gradients have to be differentiable if the functions themselves don't have to be? why can't we use sub-gradients of the gradients as well?} 
    for all policies $\policy[][][\strat] \in \policies[][\stratspace]$, are continuously differentiable;
    4.~(Gradient-Dominated Game) for all players $\player \in \players$, states $\state \in \states$, action profiles $\action \in \actionspace$, and policies $\policy[][][\strat] \in \policies[][\stratspace]$, $\strat \mapsto \qfunc[\player][{\policy[][][\strat]}](\state, \policy[][][\strat](\state); \param)$ is $\scparam$-gradient-dominated for some $\scparam>0$; and
    %\amy{just checking that you don't mean gradient dominated here? if not, i guess you are saying that a game is gradient dominated if $q$ is concave in actions?} \deni{I realize it actually all generalizes to instead assuming gradient dominance so I will do that.}
    5.~(Closure under Policy Improvement)
    for all states $\state \in \states$, players $\player \in \players$, and policy profiles $\policy \in \policies$, there exists $\policy[][][\outer] \in \policies[][\outerset]$ s.t.\@ $\qfunc[\player][\policy] (\state, \policy[\player][][\outer] (\state), \policy[-\player] (\state)) = \max_{\policy[\player][][\prime] \in \policies[\player]} \qfunc[\player][\policy] (\state, \policy[\player][][\prime] (\state), \policy[-\player] (\state))$.
%\amy{what does this say in English? that the policy class always contains a best response?}
\end{assumption}

%\deni{Putting next assumption separately because I will use the above assumptions in the next section.}

%\Cref{assum:conve_param_stoch} implies the convexity of $\param \mapsto \obj (\param, \strat)$, for all $\strat \in \stratspace$.
%
Part 3 of \Cref{assum:smooth_convex_invex} implies that the game's cumulative payoff function is Lipschitz-smooth in the policy parameters $\strat$.
We note that a large class of Markov games satisfy Part 4, including 
%but not limited to 
linear quadratic games \citep{bhandari2019global}, 
finite state and action games, and continuous state and action games whose rewards (resp.\@ transition probabilities) are concave (resp.\@ stochastically concave) in each player's action \citep{atakan2003valfunc}. 
Finally, Part 5 is a standard assumption (see, for instance, Section 5 of \citet{bhandari2019global}), which guarantees that the policy parameterization is expressive enough to represent best responses.

\begin{assumption}[Convex Parameterization]
\label{assum:convex_param_stoch}
    Given an inverse Markov game $\mgame[][-1]$, assume that for all players $\player \in \players$, states $\state \in \states$, and action profiles $\action, \otheraction \in \actionspace$, the per-state regret $\param \mapsto \reward[\player](\state, \otheraction[\player], \action[-\player]; \param) - \reward[\player](\state, \action; \param)$ is convex.
    % \vspace{-1em}
\end{assumption}

With these assumptions in hand, we face a convex gradient-dominated optimization problem, i.e., $\param \mapsto \obj (\param, \strat)$ is convex, for all $\strat \in \stratspace$, and $\strat \mapsto \obj (\param, \strat)$ gradient-dominated, for all $\param \in \params$. 
As for normal-form games (see \Cref{rem:convex}), the set of inverse NE in Markov games is convex under Assumptions~\ref{assum:smooth_convex_invex} and \ref{assum:convex_param_stoch}.
%As the maximum of a family of convex functions is once again convex \citep{danskin1966thm}, the set of inverse NE is convex under \Cref{assum:smooth_convex_invex} (see, for instance, Theorem 2.6 of \citet{rockafellar2009variational}) \amy{maybe cite R+W?}. \amy{why are we inserting this statement here. it seems we can jump from the first sentence of this paragraph to the third, directly.} \deni{It is giving intuition. If the set set of inverse eqa. are convex then intuitively we should be able to compute an inverse eqm. in polytime.} \amy{but don't we already get that same intuition from the fact that the problem is convex-gradient-dominated? (i.e., the first sentence)}
Consequently, we can obtain polynomial-time convergence of stochastic gradient descent ascent (\Cref{alg:online-sgda}) by slightly modifying known results \citep{daskalakis2020independent}.
%provided we can estimate $\grad \obj$.


\input{algos/online-sgda}


% We thus turn our attention to deriving 
% %%% SPACE
% %first-order 
% polynomial-time computation guarantees for online inverse MARL \sdeni{}{which requires us to estimate the gradient of $\obj$}.
% \sdeni{But there is another obstacle to overcome first: estimating the gradient of $\obj$, and proving that our estimator's variance is bounded.}{}

\Cref{alg:online-sgda} requires an estimate of $\grad \obj$ w.r.t.\@ both $\param$ and $\strat$.
Under Part 3 of \Cref{assum:smooth_convex_invex}, the gradient of $\obj$ w.r.t.\@ $\strat$ can be obtained by the deterministic policy gradient theorem \citep{silver2014deterministic}, while the gradient of $\obj$ w.r.t.\@ $\param$ can be obtained by the linearity of the gradient and expectation operators.
However, both of these gradients involve an expectation---over $\histrv \sim \histdistrib[][{(\policy[\player][][\strat], \truepolicy[-\player])}]$ and $\histrv[][\dagger] \sim \histdistrib[][\truepolicy]$.
As such, we estimate them using simulated trajectories from the deviation 
%\deni{Call best-response history instead?}\amy{No!} 
history distribution $\histmatrix \doteq \left(\hist[][1], \hdots, \hist[][\numplayers]\right)^T \sim \bigtimes_{\player \in \players} \histdistrib[][{(\policy[\player][][\strat], \truepolicy[-\player])}]$ \amy{can we delete the $T$ superscript? isn't it implied?} and 
%the equilibrium trajectories from
the equilibrium history distribution $\hist[][\dagger][] \sim \histdistrib[][\truepolicy]$, respectively.
For a given such pair $(\histmatrix, \hist[][\dagger])$, the cumulative regret gradient estimators $\obj[\param]$ and $\obj[\strat]$ correspond to the gradients 
%w.r.t.\@ to $\param$ and $\strat$, respectively,
of the cumulative regrets between each deviation history $\hist[][\player]$ in $\histmatrix$ and $\hist[][\dagger]$, and can be computed directly using the chain rule for derivatives, as we assume access to a differentiable game simulator.%
\footnote{For completeness, we show how to compute $\obj[\strat]$ and $\obj[\param]$ in \Cref{sec_app:gradient_estimate}.
In our experiments, however, as has become common practice in the literature \citep{mora2021pods}, we compute these gradients by simply autodifferentiating the cumulative regret of any history w.r.t.\@ the policy parameters using a library like Jax \citep{jax2018github}. 
We also show that under \Cref{assum:smooth_convex_invex}, $(\obj[\param], \obj[\strat])$ is an unbiased estimate of $(\grad[\param]  \obj, \grad[\strat] \obj)$ whose variance is bounded. \amy{is this notation incorrect? feels like you should have something like $\hat{\nabla}$ before each $f$.} \deni{We recently switched to that type of notation but I think let's keep it this way for the camera-ready and we can change for the Arxiv submission?} \amy{well then what about $(\hat{\obj[\param]}, \hat{\obj[\strat]})$?} \deni{We don't have macros for them. Seems dangerous.}}

% respectively, as: \deni{Add that in practice this is not even needed since use autodiff.} 
% \amy{move these estimators to APPENDIX! can say straightforward estimators, or estimate in the obvious way, or whatever.}

Finally, we define the \mydef{equilibrium distribution mismatch coefficient} $\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty$ as the Radon-Nikodym derivative of the state occupancy distribution of the NE $\policy[][][\dagger]$ w.r.t.\@ the initial state distribution $\initstates$.
This coefficient, which measures the inherent difficulty of reaching states under $\policy[][][\dagger]$, 
% without knowing $\policy[][][\dagger]$, \amy{without knowing $\policy[][][\dagger]$? it is given, n'est-ce pas?} 
is closely related to other distribution mismatch coefficients introduced in the analysis of policy gradient methods \citep{agarwal2020optimality}. 
With this definition in hand, we can finally show polynomial-time convergence of stochastic GDA (\Cref{alg:online-sgda}) under Assumptions \ref{assum:smooth_convex_invex}--\ref{assum:convex_param_stoch}.
% This requires us to handle separately the finite state-action setting and continuous state-action settings, giving us the following theorem.
% $\Theta\left(\frac{\epsilon^4 \mu_y^2}{\ell^3\left(L^2+\sigma_y^2\right)(L / \ell+1)^2}\right)$
% $\Theta\left(\frac{\epsilon^8 \mu_y^4}{\ell^5 L(L / \ell+1)^4\left(L^2+\sigma_y^2\right) \sqrt{L^2+\sigma_x^2}} \wedge \frac{\epsilon^2}{\ell\left(L^2+\sigma_x^2\right)}\right)$

\begin{theorem}
\label{thm:online_sgda}
    % \deni{Add def'n of $\sigma$ and $\left\|\frac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|$ $AA$}
    Under Assumptions \ref{assum:smooth_convex_invex}--\ref{assum:convex_param_stoch}, for all $\varepsilon \in (0,1)$, if \Cref{alg:online-sgda} is run with inputs that satisfy $\numiters \in \Omega\left( \varepsilon^{-10}\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty\right)$ and for all $\iter \in [\numiters]$, $\learnrate[\otherstrat][\iter] \asymp  \varepsilon^4$ and
    $\learnrate[\param][\iter] \asymp \varepsilon^8$,   
    % $O\left(\nicefrac{\diam(\stratspace \times\params)}{\varepsilon^{10}} \right)$ 
    % \left( \frac{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}}{\varepsilon^{10} \left(\frac{AA}{1-\discount}\right)^4}\right) \right) \geq \frac{\max_{\param, \param[][][\prime] \in \params}\left\| \param[][0] - \param[][][*] \right\| + \max_{\otherstrat, \otherstrat[][][][\prime] \in \stratspace} \left\| \otherstrat[][] - \otherstrat[][][][\prime] \right\|}{\varepsilon^10}
    then the time-average of all parameters $\mean[{\param[][\numiters]}] \doteq \frac{1}{\numiters + 1}\sum_{\iter = 0}^\numiters \param[][\iter]$ is an $\varepsilon$-inverse NE.
     % , i.e., $\cumulregret[](\truestrat, \otherstrat; \mean[{\param[][\numiters]}]) - \min_{\substack{\param \in \params}} \max_{\otherstrat \in \stratspace} \cumulregret[](\truestrat, \otherstrat; \param) \leq \varepsilon$
% \amy{it would be so much nicer if everything in this section were instead about Markov perfect eqa!!!}
\end{theorem}

% \sdeni{Ignoring the standard Lipschitz constants $\lipschitz[\obj]$ and $\lipschitz[{\grad \obj}]$, and the diameter constants $\diam(\params \times \stratspace)$ that appear in the analysis of any first-order method,}{} 
 
% \deni{Discuss how this characterizes all Markov games for which inverse RL is polytime.}

%\amy{sorry for a dumb question, but i cannot quite see how/where the initial state distribution is input to \Cref{alg:online-sgda}. there is no $\mu$? so i guess you mean $\strat[][][0]$?}

% \begin{align}
%     \obj[\strat](\param, \otherstrat; \hist, \hist[][\prime]) &\doteq \sum_{\player \in \players} \grad[\action] \reward[\player](\state, \action; \param) \notag \\&+ \discount \int_{\state[1]} \grad[\action] \trans(\state[1] \mid \state, \action)\left[ \reward[\player]\left(\state[1], \action[][][1]; \param \right) + \sum_{\iter = 2}^\infty \int_{\state[\iter] }  \reward[\player]\left(\state[\iter], \action[][][\iter] \right) \prod_{k = 2}^{\iter} \discount^{k+1} \trans(\state[k] \mid \state[k-1], \action[][][k-1]) \right]
% \end{align}





% while the gradient w.r.t. $\param$ can be easily obtain by linearity of the gradient and expectation operators. Unfortunately, both of these gradients involve an expectation over $\histrv \sim \histdistrib[][{(\policy[\player], \truepolicy[-\player])}], \histrv[][\prime] \sim \histdistrib[][\truepolicy]$, as such we have to estimate them using the realized trajectories from the counterfactual deviation history distribution $\hist \doteq \left(\hist[1], \hdots, \hist[\numplayers]\right) \sim \bigtimes_{\player \in \players} \histdistrib[][{(\otherpolicy[\player], \truepolicy[-\player])}]$ and the equilibrium trajectories from the equilibrium history distributions $\hist[][\prime][] \sim \histdistrib[][\truepolicy]$ respectively. For a given pair of counterfactual deviation trajectory, and equilibrium trajectory $(\hist, \hist[][\prime])$, we define the cumulative gradient regret estimators $\obj[\strat]$ and $\obj[\param]$ w.r.t. to $\strat$ and $\param$, respectively, as: 
% % 
% \begin{align}
%     \obj[\strat](\param, \otherstrat; \hist, \hist[][\prime]) \doteq \sum_{\player \in \players}
%     \left[ \sum_{\iter} \discount^\iter \reward[\player](\state[\iter][\player], \action[\player][][\iter]; \param)  \grad[\outer] \log \left(\policy[\player][][\strat](\state[\iter][\player]) \right) \right] \\
%     \obj[\param](\param, \otherstrat; \hist, \hist[][\prime]) \doteq \sum_{\player \in \players} \left[\sum_{\iter} \discount^\iter \grad[\param]\reward[\player](\state[\iter][\player], \action[\player][][\iter]; \param)  -   \sum_{\iter} \discount^\iter \grad[\param] \reward[\player](\state[\iter][][\prime], \action[][][\iter][\prime]; \param) \right] 
% \end{align}

% \noindent where the sum over $\iter$ is dependent on the length of the trajectories $\hist, \hist[][\prime]$.
% % \deni{Add policy gradient thm assumption here.}

% % \begin{assumption}
% %     Given an inverse Markov game $(\numplayers, \numactions, \numparams, \states, \actionspace, \params, \reward, \trans, \discount, \initstates, \truepolicy)$ suppose that $\trans$ 
% % \end{assumption}

% \begin{align}
%     % \qfunc[\player](\state, \action; \param) &= \reward[\player](\state, \action; \param) + \discount\int_{\state[1]} \left[\trans(\state[1] \mid \state, \action) \reward[\player](\state[1], \action[][][1]; \param) + \discount \int_{\state[2] } \left[\trans(\state[1] \mid \state, \action) \trans(\state[2] \mid \state[1], \action[1]) \reward[\player](\state[2], \action[][][2]; \param) \hdots \right. \right.\\
%     % \qfunc[\player](\state, \action) &= \reward[\player](\state, \action; \param) + \discount\int_{\state[1], \state[2]...} \left[\trans(\state[1] \mid \state, \action) \reward[\player](\state[1], \action[][][1]; \param) + \discount \trans(\state[1] \mid \state, \action)   \trans(\state[2] \mid \state[1], \action[1]) \reward[\player](\state[2], \action[][][2]; \param) \right.\hdots\\
%     \qfunc[\player](\state, \action) &\doteq \reward[\player](\state, \action; \param) + \discount \int_{\state[1]} \trans(\state[1] \mid \state, \action)\left[ \reward[\player]\left(\state[1], \action[][][1]; \param \right) + \sum_{\iter = 2}^\infty \int_{\state[\iter] }  \reward[\player]\left(\state[\iter], \action[][][\iter] \right) \prod_{k = 2}^{\iter} \discount^{k+1} \trans(\state[k] \mid \state[k-1], \action[][][k-1]) \right]
% \end{align}

% the gradient of $\obj$ w.r.t. the best-response policy parameters $\param$ is continuous and given as follows:
% \begin{align}
%     &\grad[\strat] \obj (\param, \strat) \doteq \sum_{\player \in \players}\mathop{\Ex}_{\staterv \sim \statedist[\initstates][{(\policy[\player][][\strat], \truepolicy[-\player])}]
%     % \\ \actionrv \sim \policy[\outer](\cdot \mid \staterv)}
%     }  \left[\grad[{\action[\player]}] \left.\qfunc[\player][{(\policy[\player][][\strat], \truepolicy[-\player])}](\staterv, \action[\player], \truepolicy[-\player](\staterv)) \right|_{\action[\player] = \policy[\player][][\strat](\staterv)}  \grad[\outer] \policy[\player][][\strat](\staterv) \right] 
%     % &\grad[\inner] \cumulutil(\outerpoint, \innerpoint) = \mathop{\Ex}_{\staterv \sim \statedist[\initstates][{(\outerpoint, \innerpoint)}]
%     % \\ \actionrv \sim \policy[\outer](\cdot \mid \staterv)}
%     % }  \left[\grad[\inneraction] \actionvalue[][{(\outerpoint, \innerpoint)}](\staterv, \policy[\outerpoint](\staterv), \policy[\innerpoint](\staterv))  \grad[\inner] \policy[\outerpoint](\staterv) \right]
% \end{align}
% On the other hand, the gradient of $\obj$ w.r.t the parameters $\param$ is simply given by:
% \begin{align}
%     \grad[\param] \obj (\param, \strat) \doteq \sum_{\player \in \players} \Ex_{\substack{\histrv \sim \histdistrib[][{(\policy[\player], \truepolicy[-\player])}]\\ \histrv[][\prime] \sim \histdistrib[][\truepolicy]}} \left[\sum_{\iter = 0}^\infty \discount^\iter \grad[\param]\reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param)  - 
%         % \Ex_{\histrv[][\prime] \sim \histdistrib[][\truepolicy]}\left[
%         \sum_{\iter = 0}^\infty \discount^\iter \grad[\param] \reward[\player](\staterv[\iter][][\prime], \actionrv[][][\iter][\prime]; \param) \right] 
% \end{align}

% By notational overload define:

% \begin{align}
%     \qfunc[\player](\state, \action; \param) &= \reward[\player](\state, \action; \param) + \discount\int_{\state[1]} \left[\trans(\state[1] \mid \state, \action) \reward[\player](\state[1], \action[][][1]; \param) + \discount \int_{\state[2] } \left[\trans(\state[1] \mid \state, \action) \trans(\state[2] \mid \state[1], \action[1]) \reward[\player](\state[2], \action[][][2]; \param) \hdots \right. \right.\\
%     \qfunc[\player](\state, \action) &= \reward[\player](\state, \action; \param) + \discount\int_{\state[1], \state[2]...} \left[\trans(\state[1] \mid \state, \action) \reward[\player](\state[1], \action[][][1]; \param) + \discount \trans(\state[1] \mid \state, \action)   \trans(\state[2] \mid \state[1], \action[1]) \reward[\player](\state[2], \action[][][2]; \param) \right.\hdots\\
%     \qfunc[\player](\state, \action) &= \reward[\player](\state, \action; \param) + \discount \int_{\state[1]} \trans(\state[1] \mid \state, \action)\left[ \reward[\player]\left(\state[1], \action[][][1]; \param \right) + \sum_{\iter = 2}^\infty \int_{\state[\iter] }  \reward[\player]\left(\state[\iter], \action[][][\iter] \right) \prod_{k = 2}^{\iter} \discount^{k+1} \trans(\state[k] \mid \state[k-1], \action[][][k-1]) \right]
% \end{align}

% \begin{align}
%     \qfunc[\player](\state[0], \action[][][0]) = \reward[\player]\left(\state[0], \action[][][0] \right)  + \sum_{\iter = 1}^\infty \int_{\state[\iter] }\reward[\player]\left(\state[\iter], \action[][][\iter] \right) \prod_{k = 1}^\iter \discount^k \trans(\state[k] \mid \state[k-1], \action[][][k-1])\\
%     = \reward[\player]\left(\state[0], \action[][][0] \right) + \discount\int_{\state[1]} \trans(\state[1] \mid \state, \action) \reward[\player](\state[1], \action[][][1]) + \sum_{\iter = 2}^\infty \int_{\state[\iter] } \trans(\state[1] \mid \state[0], \action[][][0]) 
%     \reward[\player]\left(\state[\iter], \action[][][\iter] \right) \prod_{k = 1}^{\iter} \discount^{k+1} \trans(\state[k+1] \mid \state[k], \action[][][k]) 
% \end{align}

% In reinforcement learning settings, 
% As as we do not have direct access to the state visitation distribution $\statedist[\initstates][\policy]$ for any policy profile $\policy \in \policies$, we have to estimate the gradient of the above expectation using the realized trajectories from the counterfactual deviation history distribution $\hist \doteq \left(\hist[1], \hdots, \hist[\numplayers]\right) \sim \bigtimes_{\player \in \players} \histdistrib[][{(\otherpolicy[\player], \truepolicy[-\player])}]$ and the equilibrium trajectories from the equilibrium history distributions $\hist[][\prime][] \sim \histdistrib[][\truepolicy]$  respectively. For a given pair of counterfactual deviation trajectory, and equilibrium trajectory $(\{\hist[\player]\}_\player, \hist[][\prime])$, we define the cumulative gradient regret estimator: 
% % 
% \begin{align}
%     \obj[\strat](\param, \otherstrat; \hist, \hist[][\prime]) \doteq \sum_{\player \in \players}
%     % \\ \actionrv \sim \policy[\outer](\cdot \mid \staterv)}
%     \left[ \sum_{\iter} \discount^\iter \grad[\param]\reward[\player](\state[\iter][\player], \action[\player][][\iter]; \param)  \grad[\outer] \policy[\player][][\strat](\staterv) \right] \\
%     \obj[\param](\param, \otherstrat; \hist, \hist[][\prime]) \doteq \sum_{\player \in \players} \left[\sum_{\iter} \discount^\iter \grad[\param]\reward[\player](\state[\iter][\player], \action[\player][][\iter]; \param)  -   \sum_{\iter} \discount^\iter \grad[\param] \reward[\player](\state[\iter][][\prime], \action[][][\iter][\prime]; \param) \right] 
% \end{align}

% where the sum over $\iter$ is dependent on the length of the trajectories $(\{\hist[\player]\}_\player, \hist[][\prime])$

% \deni{Do something with this.} parametric class of policies $\policies[][\stratspace]$ which are affine in their parameters, i.e., $\policy[][\stratspace] \doteq \left\{\phi(\state) \cdot \strat \mid \strat \in \stratspace \right\}$ for a set of parameters $\stratspace \subset \R^\numstrats$, i.e.,
% % \begin{align}
% %     \obj[{\hist, \hist[][\prime]}](\param, \otherstrat) \doteq \Ex_{\histrv \sim \histdistrib[][{(\otherpolicy[\player], \truepolicy[-\player])}]} \left[\sum_{\iter = 0}^\infty \discount^\iter \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) \right] - \sum_{\iter = 0}^\infty \discount^\iter \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param)
% % \end{align}.

% This leads us to the following optimization problem:

% \begin{align}
%     \min_{\substack{\param \in \params}} \max_{\otherpolicy: \states \to \actionspace} \obj (\param, \otherstrat) \doteq \Ex_{\histrv \sim \histdistrib[][{(\otherpolicy[\player], \truepolicy[-\player])}]} \left[\sum_{\iter = 0}^\infty \discount^\iter \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) \right] - \Ex_{\histrv \sim \histdistrib[][\truepolicy]} \left[\sum_{\iter = 0}^\infty \discount^\iter \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) \right]
% \end{align}


% \deni{In cases when equilibria are stochastic the same theorem above applies for SGD version. provide example with Markov games.}
% % $(\numplayers, \numactions, \innerset, \left(\state[\numsample], \inner[][][*][\numsample]\right)_{\numsample \in [\numsamples]})$ comprises a number of players $\numplayers \in \N_{++}$, an action space dimension $\numactions \in \N_{++}$ a joint action space $\innerset \doteq \bigtimes_{\player \in \players} \innerset[\player]$, and for all $\numsample \in [\numsamples]$ samples of $(\state[\numsample], \inner[][][*][\numsample]) \in \states \times \innerset$ (state/context, Nash equilibrium) tuples, and; consists of finding an \mydef{inverse equilibrium mapping} $\hypothesis[][*]: \states \to \R^\stratspace$, which takes takes as input a state $\state \in \states$ and outputs utility functions $\hypothesis[][*]\left[\state\right]: \stratspace \to \R^\numplayers$ s.t. for all samples $\numsample \in \numsamples$, $\hypothesis[][*](\state[\numsample])$ $\varepsilon$-rationalize $\inner[][][*][\numsample]$. 

