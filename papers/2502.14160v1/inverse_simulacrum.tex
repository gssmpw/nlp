\section{Simulacral Learning}

% \deni{Here, we do not have an outcome, but rather an observation because we do not observe the outcome of the game, but rather a function of the outcome of the game.}

% \deni{We have an issue with the learning from trajectories part of things. Namely that if we use observed trajectories, then we have to use an off-policy gradient method to solve our min-max problem, but the off-policy gradient method is almost always biased, so GDA results do NOT apply.} 

% \deni{Here, we get learning result if we can get unbiased and bounded variance estimates of the off-policy gradient.}
% \deni{Even if you recover params via rl, since params can result in multiple eqa., you have to actually select an eqm.}
% \deni{Key thing here is that we are not able to reconstruct the rewards of the agents from the observation.}


%%% OFFLINE
%it might not be possible to simulate counterfactual deviation trajectories $\histrv \sim \histdistrib[][{(\policy[\player], \truepolicy[-\player])}]$, as we do not have access to $\truepolicy[][][\dagger]$.

% \deni{Add non-convex}

\deni{The content in the next two paragraphs feels there but organized slightly weirdly, maybe?} \amy{what do we need to say beyond: don't observe eqm, just trajectories. after inferring parameter values, we can push them forward to generate eqm policies. in this sense, an ensuing eqm is part of the solution -- my main problem with this, btw, is: what if the ensuing eqm is not unique? i guess you don't care. you'll take any eqm that generates the observed behavior.}

\amy{it seems to me that we are brushing under the rug a key assumption, namely that of the ``observation distribution''. although we cannot simulate policies per se, this ``observation distribution'' effectively acts like a simulator, given observational info about policies. so i don't think the result in this section is as magical as we seem to be making it out to be.}

In this section, we consider the more realistic setting in which we do not observe an equilibrium, but observe only sample histories $\left\{ \hist[][][\numsample] \right\}_{\numsample} = \left\{(\state[\iter, \numsample], \action[][][\iter, \numsample])_\iter \right\}_{\numsample} \sim \histdistrib[][\truepolicy]$ associated with an \emph{unobserved\/} equilibrium $\truepolicy$. 
The problem of interest then becomes one of not only inferring parameter values from observed behavior, but of additionally finding equilibrium policies that generate the observed behavior, a solution which we refer to as a first-order simulacrum.
A first-order simulacrum can be seen as a generalization of an inverse equilibrium, as it not only comprises parameters that rationalize the observed histories, but also policies that mimic them in expectation.
First-order simulacral learning is also known as \mydef{multiagent apprenticeship learning} \citep{abbeel2004apprenticeship, yang2020inferring}.

\amy{a simulacrum is generative. so i don't think it is a disaster to call the whole setup, and even the paper, generative-adversarial, even though in the earlier sections we do not use the generative capabilities of this, our most general, problem and solution.} \deni{I don't think a simulacrum is generative in the traditional sense of the world, i.e., it is not a stochastic function?}

%\samy{Note that both \emph{offline\/} inverse MARL (without a simulator) \amy{this sounds the same to me as multiagent apprenticeship learning. i can't tell the difference.} \sadie{just want to make sure does this mean that we observe the equilibrium policy but don't have a simulator?} and \emph{online\/} multiagent apprenticeship learning (with a simulator) \amy{and this is just inverse MARL, AFAICT}\sadie{I don't think so since I believe apprenticeship learning implies that we don't observe the equilibrium policy while we do in inverse MARL.} are special cases of this problem.}{}

\if 0
The added difficulty in simulacral learning arises from the fact that we cannot simulate counterfactual deviation histories $\histrv \sim \histdistrib[][{(\policy[\player], \truepolicy[-\player])}]$, because we do not observe $\truepolicy[][][\dagger]$.
Thus, we might observe the history of play of two soccer teams in a match, but we cannot then have each team re-play the game, fixing its opponent's policy, so that we can observe a best response.
\amy{let's discuss deleting the rest of this paragraph...} \deni{Yes, fine!}
This situation is typical of offline reinforcement learning \citep{levine2020offline, jarboui2021offline}, where we observe one policy but are interested in statistics about another (target) policy.
Unlike in offline RL, however, importance sampling methods cannot be used to correct the bias in simulated \amy{observed?} trajectories of unilateral deviations from equilibrium policies, because we do observe the equilibrium policies that are necessary to implement this bias correction. \amy{in inverse MARL, we know the eqm policy, so it sounds to me like we CAN actually do the requisite bias correction, instead of running expensive rollouts to calculate the estimator!}
\fi


\input{algos/offline-sgda}


% In many applications of interest, we can only obtain samples of realized equilibrium trajectories $\{ \hist[][\dagger][\numsample]\} \sim \histdistrib[][\truepolicy]$, and have to \emph{learn} an inverse NE from these samples, a problem we refer to as \mydef{offline inverse MARL}. 
% Offline inverse MARL is much harder to solve than offline inverse IRL, as it is very hard, if not impossible, to estimate the best-response trajectories from an equilibrium policy profile, since the policies of the other players, which we cannot simulate, determine the stochasticity of the environment.

\if 0
The main advantage of multiagent apprenticeship learning is that by simulating candidate Nash equilibrium policies, we can get around the problem of simulating trajectories from (a subset) of the unobserved Nash equilibrium policies.
Perhaps more importantly, by computing candidate equilibrium policies, the inverse multiagent apprenticeship learning approach can be used not only explain observed, but also to predict future, behavior in Markov games.
\fi

Even more generally, we might not have access to samples $\{ \hist[][\dagger][\numsample]\}_{\numsample \in [\numsamples]} \sim \histdistrib[][\truepolicy]$ from an equilibrium history distribution, but rather a lossy function of those histories according to some function $\rho: \hists \to \obspace$ that produces \mydef{observations} $\{ \trueobs[][\numsample] \}_{\numsample \in [\numsamples]} \doteq \{ \bm{\rho} \left(\hist[][\dagger][\numsample]\right)\}_{\numsample \in [\numsamples]} \sim \obsdistrib[][][\truepolicy]$, distributed according to some \mydef{(pushforward) observation distribution} 
%\amy{pushforward is undefined. this obs'n dist'n seems to me to be a lossy simulator.} \deni{The reader should have probably taken a class in measure theory to know what it is anyways, so if we define that we have to also define what a measure is.} \amy{huh? pushforward is a standard term in measure theory?} 
$\obsdistrib[][][{\policy}] \in \simplex(\obspace)$, parameterized by policy profile $\policy \in \policies$, where $\obspace$ is the observation space.
This more general framework is very useful in applications where there are limitations on the data collection process: e.g., if there are game states at which some of the players' actions are unobservable, or when only an unfaithful function of them is available.
Here, we seek to learn the more general notion of a \mydef{second-order simulacrum}.
%%% SPACE
%\citep{baudrillard1994simulacra}.}

%\sdeni{}{In which case, we are interested in inferring parameter values, and associated equilibrium policies that rationalize the \emph{observations} rather than the histories, in which case we are concerned with the computation of the more general notion of a second-order simulacrum \citep{baudrillard1994simulacra}.}

% We will assume going forward that we do have access to the observation mapping. An example of such a setting is multiagent inverse reinforcement learning for Markov games, in which we have observed some trajectories of state-action tuples $\left(\state[\iter], \action[][][\iter]\right)_{\iter} \sim \histdistrib[][{\policy[][][\dagger]}]$ associated with some Nash equilibrium policies $\policy[][][\dagger]: \states \to \actionspace$ and we would like to recover parameters $\param[][][*] \in \params$, that replicate the observed trajectory in expectation over all trajectories for some Nash equilibrium policy $\policy[][][*]: \states \to \actionspace$ associated with parameters $\param[][][*]$. Here, we have $\trueobs \doteq \{ \action[][][\iter]\}_{\iter}$ $\obsdistrib \doteq \histdistrib$, $\obspace \doteq \actionspace^\infty$, and $\stratspace \doteq \{ \policy: \states \to \actionspace\}$. In other settings of interest, the observation distribution might instead be an observation mapping, i.e., $\obsdistrib$ is a dirac delta distribution, which outputs a function of all the players' equilibrium actions. 

\deni{Explain difference between online and offline, bc for a given parameters, player's best response is not optimal.}

Formally, an \mydef{inverse simulation} $\inversesim \doteq (\mgame[][\paramtrue] \setminus \paramtrue, \obspace, \obsdistrib, \obsdistrib[][][\truepolicy])$ is a tuple consisting of a Markov game form $\mgame[][\paramtrue] \setminus \paramtrue$ with unknown parameters $\paramtrue$, an observation 
%\amy{simulator!!!} \deni{hmm, I see what you are saying but not really.} \amy{what i am saying is that the aforementioned $\rho$ which you just introduced yesterday is part of the problem description, unless somehow $\rho$ is implicit in $\obsdistrib$?} \deni{Yes, it is implicit in $\obsdistrib$.}
distribution $\obsdistrib: \policies \to \simplex (\obspace)$ mapping policies to distributions over the \mydef{observation space} $\obspace$, and an observation distribution $\obsdistrib[][][\truepolicy]$ for the \emph{unobserved\/} behavioral policy $\truepolicy$, which we assume is a Nash equilibrium.
% 
% a sample of $\numsamples \in \N_{++}$ observed equilibrium observation $\{ \trueobs[][\numsample] \}_{\numsample = 1}^\numsamples$.
% 
% With all this in mind, formally, \mydef{a stochastic inverse game} $(\numplayers, \numactions, \numparams, \stratspace, \params, \util, \obsdistrib, \trueobs)$ comprises $\numplayers \in \N_{++}$ players who have played a game $(\numplayers, \numactions, \numparams, \stratspace, \params, \paramtrue, \util)$ where we have observed an observation $\trueobs \sim \obsdistrib[][][\truestrat]$ associated with an unobserved Nash equilibrium equilibrium action $\truestrat$ determined by the parameter $\paramtrue \in \params$ of the players' payoffs. 
% 
Our goal is to find an \mydef{$(\varepsilon, \delta)$-Nash simulacrum}, meaning a tuple of parameters and policies $(\param[][][*], \policy[][][*]) \in \params \times \policies$ that $(\varepsilon, \delta)$-\mydef{simulates} the observations as a Nash equilibrium: i.e., 
% \begin{align}
$\util[\player] (\policy[][][*]; \param[][][*]) \geq \max_{\policy[\player] \in \policies[\player]} \util[\player] (\policy[\player], \policy[-\player][][*]; \param[][][*]) - \varepsilon $ and $\Ex_{(\obs,\trueobs) \sim \obsdistrib[][][{\policy[][][*]}] \times \obsdistrib[][][\truepolicy]} \left[\left\|\obs - \trueobs[]\right\|^2 \right] \leq \delta$.
% \end{align}
%
%\sdeni{For simplicity, we measure the distance between the observations and the Nash simulacrum in terms of Euclidean squared distance.}{}
%%% SPACE
%Notice that this definition does not require us to find the hidden parameters $\paramtrue$ (resp.\@ the unknown equilibrium policies $\truepolicy$), because finding the original parameters (resp.\@ Nash equilibrium policies) is impossible unless there is a bijection between the set of parameters and Nash equilibria (resp.\@ equilibrium observations), which does not hold even in zero-sum bimatrix games.
%
\Cref{thm:inverse_simulacrum}, which is analogous to \Cref{thm:inverse_stoch_NE}, characterizes the set of Nash simulacra of an inverse simulation. 

\begin{restatable}{theorem}{thminversesimulacrum}
\label{thm:inverse_simulacrum}
Given an inverse simulation $\inversesim$,
    for any $\mixparamone, \mixparamtwo > 0$, the set of Nash simulacra of $\mgame[][-1]$ is equal to the set of minimizers of the following stochastic min-max optimization problem:
    % \deni{Function $\otherobj$ is overloaded with exploitability, so remove epxloitability definition, it is not necessary.}
    \begin{align}
        \min_{\substack{\param \in \params \\ \policy \in \policies}} \exploit(\param, \policy) = \min_{\substack{\param \in \params \\ \policy \in \policies}} \max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy) \doteq  \mixparamone \Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][\policy] \times \obsdistrib[][][\truepolicy]}} \left[\left\|\obs - \trueobs \right\|^2 \right] + \mixparamtwo \cumulregret(\policy, \otherpolicy; \param)\label{eq:min_max_simulacrum}
    \end{align}
%     \begin{align}
%     & \min_{\substack{\param \in \params \\ \strat \in \stratspace}} &\left\|\strat[][][][*] - \strat \right\|^2 \\
%     &\text{subject to } & \exploit(\strat; \param) \doteq \max_{\otherstrat \in \stratspace} \cumulregret[](\strat, \otherstrat; \param) \leq \varepsilon
% \end{align}
\end{restatable}
% \deni{Regularization Business is important here!!!}

% \deni{have to re-write so things work in terms of the sample of observation rather than distribution.}
To tackle simulacral learning, we approximate $\otherobj$ via realized observation samples $\{ \trueobs[][\numsample] \} \sim \obsdistrib[][][\truepolicy]$, based on which we compute the empirical learning loss $\avg[\otherobj](\param, \policy, \otherpolicy) \doteq \mixparamone \Ex_{\substack{\obs \sim \obsdistrib[][][\policy]}} \left[\nicefrac{1}{\numsamples} \sum_{\numsample = 1}^\numsamples \left\|\obs - \trueobs[][\numsample] \right\|^2 \right]$ $+$ $\mixparamtwo \cumulregret(\policy, \otherpolicy; \param)$.
Additionally, as in the previous section, we once again restrict policies to lie within a parametric class of policies $\policies[][\stratspace]$, redefine $\otherobj(\param, \strat, \otherstrat) \doteq \otherobj(\param, \policy[][][\strat], \policy[][][\otherstrat])$ and $\avg[\otherobj](\param, \strat, \otherstrat) \doteq \avg[\otherobj](\param, \policy[][][\strat], \policy[][][\otherstrat])$, and solve the ensuing optimization problem over the empirical learning loss $\min_{(\param, \strat) \in \params \times \stratspace} \max_{\otherstrat \in \stratspace} \avg[\otherobj](\param, \strat, \otherstrat)$.

%\deni{The next paragraph contains all the explanation for stationary points.} \amy{arguable!}

In general, this stochastic min-max optimization is non-convex non-concave. 
By \Cref{assum:smooth_convex_invex}, however, the function $\otherstrat \mapsto \otherobj (\param, \strat, \otherstrat)$ is gradient dominated, for all $\param \in \params$ and $\strat \in \stratspace$.
Nevertheless, it is not possible to guarantee that $(\param, \strat) \mapsto \otherobj (\param, \strat, \otherstrat)$ is convex or gradient dominated, for all $\otherstrat \in \otherstratspace$, without overly restrictive assumptions.
%\amy{is there a reference?} \deni{No :(}
This claim is intuitive, since the computation of an inverse simulacrum involves computing a Nash equilibrium policy, which in general is a PPAD-complete problem \citep{daskalakis2009complexity,foster2023hardness}.  
Finally, defining gradient estimators as we did in \Cref{sec:inverse_marl}, to obtain gradient estimators $(\avg[{\otherobj[\param]}], \avg[{\otherobj[\strat]}], \avg[{\otherobj[\otherstrat]}])(\param, \strat, \otherstrat; \histmatrix, \hist[][\strat])$ from samples histories $\histmatrix \sim \bigtimes_{\player \in \players} \histdistrib[][{(\policy[\player][][\strat], \policy[-\player][][\otherstrat])}]$ and $\hist[][\strat][] \sim \histdistrib[][{\policy[][][\strat]}]$,\amy{drop $\strat$ superscript on $\hist$? (twice)} we can use \Cref{alg:offline-sgda} to compute a local solution of \Cref{eq:min_max_simulacrum} from polynomially-many observations.

%AMY: totally confused at 1am
%\amy{are these draws from $\obsdistrib[][][\policy]$, for candidate policies while learning, or draws from $\obsdistrib[][][\truepolicy]$. i think the former?} \deni{the latter!} \amy{i don't get it. we need to compute the empirical learning loss. we are given $k$ samples from $\obsdistrib[][][\truepolicy]$ as input to the problem. i thought you were trying to figure out how many samples we need from $\obsdistrib[][][\policy]$ to compute the empirical learning loss?} \deni{No I am trying to understand how many samples I would observations from the equilibrium I would need, to correctly estimate the equilibrium observation distribution. But maybe we should remove the sample complexity stuff entirely?}

%(see \citet{lin2020gradient} for additional details).}

% Under \Cref{assum:concave_game}, for all $\param \in \params$, $\strat \in \stratspace$, the cumulative regret $\cumulregret(\strat, \otherstrat; \param)$ is concave in $\otherstrat$ which in turn guarantees that $\otherobj(\param, \strat, \otherstrat)$ is concave in $\otherstrat$. This implies that \Cref{eq:min_max_gen_sim} is a non-convex-concave min-max optimization problem. Define the \mydef{marginal function} $\marginalfunc(\param, \strat) \doteq \max_{\otherstrat \in \stratspace} \otherobj(\param, \strat, \otherstrat)$. If one assumes in addition to \Cref{assum:concave_game} that payoff functions of the players are Lipschitz-smooth (\Cref{assum:smoothness}), then two-time scale stochastic gradient descent ascent is guaranteed to converge to a $\varepsilon$-stationary point of $\marginalfunc$ in $O\left(\nicefrac{1}{\varepsilon^{10}}\right)$ iterations \citep{lin2020gradient}.\footnote{Although \citeauthor{lin2020gradient}'s \citep{lin2020gradient} result was initially proven for non-convex-concave min-max optimization problems for which the variables minimized are unbounded, \citeauthor{daskalakis2020independent} \citep{daskalakis2020independent} has subsequently shown that this bound can easily extended to constrained settings (see Lemma 6 of \citep{daskalakis2020independent}).}
% 

\begin{restatable}{theorem}{thmapprenticeshipthm}\label{thm:apprenticeship_thm}
    % \deni{Add def'n of $\sigma$ and $\left\|\frac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|$ $AA$}
    Suppose that \Cref{assum:smooth_convex_invex} holds, and that for all $\policy[][][\strat] \in \policies[][\stratspace]$ , $\obsdistrib[][][{\policy[][][\strat]}]$ is twice continuously differentiable in $\strat$.
    For any $\varepsilon \in (0,1)$, if \Cref{alg:offline-sgda} is run with inputs that satisfy $\numiters \in \Omega\left(\nicefrac{\variance^2}{ \varepsilon^{10}}\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty\right)$ and for all $\iter \in [\numiters]$, $\learnrate[\otherstrat][\iter] \asymp  \varepsilon^4$\samy{}{, $\learnrate[\strat][\iter] \asymp  \varepsilon^8$,} and $\learnrate[\param][\iter] \asymp \varepsilon^8$,
    % and $\numiters \geq \left(\nicefrac{(1-\discount)}{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty} + \frac{\lipschitz[\obj]}{2 \lipschitz[{\grad \obj}]} \right)^{-1} \frac{\lipschitz[\obj] \diam(\stratspace \times\params)}{\varepsilon^{2}\learnrate[\otherstrat][\iter]}$, 
    % and     
    % $O\left(\nicefrac{\diam(\stratspace \times\params)}{\varepsilon^{10}} \right)$ 
    % \left(\frac{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}}{\varepsilon^{10} \left(\frac{AA}{1-\discount}\right)^4}\right) \right) \geq \frac{\max_{\param, \param[][][\prime] \in \params}\left\| \param[][0] - \param[][][*] \right\| + \max_{\otherstrat, \otherstrat[][][][\prime] \in \stratspace} \left\| \otherstrat[][] - \otherstrat[][][][\prime] \right\|}{\varepsilon^10}
    then the best iterate  $\smash{(\bestiter[{\param}], \bestiter[{\strat}][])}$ 
     % \in \argmin_{\iter \in [\numiters]} \left\|\grad \regulexploit(\param[][\iter], \strat[][][\iter]) \right\|
    converges to an $\varepsilon$-stationary point 
    %\amy{what is a stationary point?} \deni{I do not think its appropriate to define here because it requires the definition of the Moreau Envelope.} 
    of $\exploit$ (defined in \Cref{sec:app_proofs}).
      % \amy{uh oh!!! exploitability!}, i.e., $\left\|\grad \regulexploit(\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) \right\| \leq \varepsilon$. 
     Additionally, for any $\zeta, \xi \geq 0$, it holds with probability $1-\zeta$ that
     $
        % \begin{align}
            \avg[{\exploit}](\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) - \exploit(\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) \leq \xi
    $ if the number of sample observations $\numsamples \in \Omega(\nicefrac{1}{\xi^2} \log(\nicefrac{1}{\zeta}))$.
        % \end{align}
    \vspace{-1em}
\end{restatable}

% \begin{assumption}[Computability Assumptions]\label{assum:computability}
%     Given an inverse game $(\numplayers, \numactions, \numparams, \stratspace, \params, \util, \strat[][][][*])$, suppose that for all players $\player \in \players$ 1.~ $\util$ is $\lipschitz[{\grad \util[\player]}]$-Lipschitz-smooth, and
%     2. for all $\param \in \params$, $\otherstrat \in \stratspace$, the regret of each player $ \util[\player](\otherstrat[\player], \strat[-\player]; \param) - \util[\player](\strat; \param)$ is convex in the parameters $\param$.
% \end{assumption}

% \begin{assumption}[Concave Game]\label{assum:concave_game}
%     Given an inverse game $(\numplayers, \numactions, \numparams, \stratspace, \params, \util, \strat[][][][*])$, suppose that 1.~ $\stratspace$ and $\params$ are non-empty, compact, and convex; and
%     2.~ for all players $\player \in \players$ and parameters $\param \in \params$, $\util[\player](\strat; \param)$ is concave in the $\player$th player's action $\strat[\player]$.
% \end{assumption}
% Unfortunately, the parameters that two-time scale gradient descent ascent converges to might in general not be inverse-NE. However, for appropriate choices of $\mixparamone$ and $\mixparamtwo$, we can show that for all $\param \in \params$, $\otherstrat \in \stratspace$, $\otherobj(\param, \strat, \otherstrat)$ is convex in $\strat$, which combined with the following suitable assumptions satisfied by a large class of games such as normal-form games, guarantees that the min-max optimization becomes convex-concave.

% \begin{lemma}
% \label{lemma:min_max_convex_min}
%     Suppose that \Cref{assum:computability} holds and that $\nicefrac{\mixparamone}{\mixparamtwo} \geq 2 \sum_{\player \in \players} \lipschitz[{\grad \util[\player]}]$, then for all $\param \in \params$ and $\otherstrat \in \stratspace$,  $\otherobj(\param, \strat, \otherstrat)$ is convex in $\strat$.
% \end{lemma}

% \begin{proof}[Proof of \Cref{lemma:min_max_convex_min}]
% First, note that by \Cref{lemma:sum_lipschitz_smooth}, $\cumulregret$ is $\lipschitz[{\grad \cumulregret}]$-Lipschitz-smooth with $\lipschitz[{\grad \cumulregret}] \doteq 2 \sum_{\player \in \players} \lipschitz[{\grad \util[\buyer]}]$. By the assumption of the theorem we have $\mixparamone = c \mixparamtwo \lipschitz[{\grad \cumulregret}]$ for some $c \geq 1$, giving us:
% \begin{align}
%     \otherobj(\param, \strat, \otherstrat) &= c \mixparamtwo \lipschitz[{\grad \cumulregret}] \left\|\strat[][][][*] - \strat \right\|^2_2 + \mixparamtwo \cumulregret(\param, \strat, \otherstrat)\\
%     &= c \mixparamtwo \lipschitz[{\grad \cumulregret}] \left\|\strat[][][][*] \right\| - 2 c \mixparamtwo \lipschitz[{\grad \cumulregret}] \left<\strat[][][][*], \strat \right> + c \mixparamtwo \lipschitz[{\grad \cumulregret}] \left\| \strat \right\|^2_2 + \mixparamtwo \cumulregret(\param, \strat, \otherstrat)
% \end{align}

% Now, notice that by \Cref{lemma:sum_lipschitz_smooth} $\mixparamtwo \cumulregret(\param, \strat, \otherstrat)$ is $\mixparamtwo\lipschitz[{\grad \cumulregret}]$-Lipschitz-smooth, which by Lemma A.1. of \citep{lin2020gradient} (or alternatively by Lemma 5 of \citep{daskalakis2020independent}) implies that $\mixparamtwo \cumulregret(\param, \strat, \otherstrat)$ is $\mixparamtwo\lipschitz[{\grad \cumulregret}]$-weakly convex. Hence, we must have that $c \mixparamtwo \lipschitz[{\grad \cumulregret}] \left\| \strat \right\|^2_2 + \mixparamtwo \cumulregret(\param, \strat, \otherstrat)$ is convex.
% \end{proof}

% \begin{lemma}
% Suppose that \Cref{assum:computability} holds, then $\strat, \param \mapsto \cumulregret(\strat, \otherstrat, \param) - \frac{\lipschitz[\grad \cumulregret] }{2} \left\| \param \right\|_2^2$ is $\lipschitz[\grad \cumulregret]$-weakly-convex.
% \end{lemma}

% \begin{lemma}
%     Notice that $\cumulregret$
% \end{lemma}
% As previously mentioned, in practice Complementing this computational result, we also that Nash simulacra
% \begin{theorem}
%     Sample complexity result for the number of observations required.
% \amy{appendix or not at all! experiments MUCH more important!}
% \end{theorem}
