
% NOTES/TODOS:
% \begin{enumerate}
%     \item Add high probability bounds when  only sample of equilibria are observed. Discuss markov in last section, note that if we compare to reward 
%     % \item discuss how to generalize when data is missing.
%     \item Make sure to cite \cite{song2018multi} and compare our work to it, and cite all other related work!!! These guys are so annoying because they call their paper generative adversarial immitation learning but they compute a QRE and act like they solved the problem!!! The worst!!!! 
%     \item Make sure the distinction between action and strategy is clear
%     \item fix difference between ``numstrats'' and ``numactions" command. Maybe we can remove them altogether.
%     % \item make sure to discuss about the parametrization of the probability transition functions.
%     % \item There are two ways to see normal-form games, either as repeated infinite horizon stochastic games or simultaneous, and same with stochastic games. When we see things as simultaneous move games we have inverse game theory, and when we see things in "sequence space" we have inverse reinforcement learning.
%     % \item apprenticeship in multiagent settings will often deal wit more limited data hence the observation mapping business
% \end{enumerate}
