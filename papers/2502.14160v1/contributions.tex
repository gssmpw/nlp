%
%\sdeni{Earlier methods to compute inverse equilibria primarily concern normal-form games with discrete action spaces, although a small literature on inverse MARL aims to computing inverse equilibria in discrete state and action space Markov games.}{}
%
\vspace{-2.5mm}
\paragraph{Contributions}    

% In this paper, we introduce a new method for solving the inverse game theory problem using generative learning. Our method is based on a generative game simulator. The generative game simulator is a game \amy{i don't think the GGS "IS" a game. i think it solves a game. we may need a name for the game it solves. in the abstract, i referred to it as a meta-game.} in which a learner chooses the parameters of the payoff functions, and a simulator computes the equilibrium strategy associated with the ensuing payoffs. The learner then uses the \samy{}{simulated} equilibrium to improve its estimate of the types.

The algorithms introduced in this paper extend the class of games for which an inverse Nash equilibrium can be computed efficiently (i.e., in polynomial-time) to the class of normal-form concave games (which includes normal-form finite action games), finite state and action Markov games, and a large class of continuous state and action Markov games. 
While our focus is on Markov games in this paper, the results apply to normal-form \citep{nash1950existence}, Bayesian \citep{harsanyi1967games, harsanyi1968bayesian}, and extensive-form games \citep{zermelo1913anwendung}.
The results also extend to other equilibrium concepts, beyond Nash, such as (coarse) correlated \cite{aumann1974subjectivity, moulin1978strategically}, and more generally, $\Phi$-equilibrium \citep{greenwald2003general} \emph{mutatis mutandis}.

% \sdeni{}{Our contributions are as follows.}

% \alec{I think contributions should be an itemized list. Much easier to read.}
%\amy{please have a go at this, Alec!!! thanks!} 
% \alec{Try to underscore much more succinctly and forcefully (i) the theoretical results are new; and (ii) what is innovative or weird/unusual about the experimental results and how they differ from capabilities of prior algorithms for inverse games}

% \deni{A word on equilibrium selection?}

%\deni{Add theorem numbers in following para when done.}\amy{don't need 'em}

First, regarding inverse multiagent planning, we provide a min-max characterization of the set of inverse Nash equilibria of any inverse game for which the set of inverse Nash equilibria is non-empty, assuming an exact oracle (\Cref{thm:inverse_NE}).
%, in spite of possibly stochastic equilibrium policies.
We then show that for any inverse concave game, when the regret of each player is convex in the parameters of the inverse game, an assumption satisfied by a large class of inverse games such as inverse normal-form games, this min-max optimization problem is convex-concave, and can thus be solved in polynomial time (\Cref{thm:concave_game_inverse_NE}) via standard first-order methods.
This characterization also shows that the set of inverse Nash equilibria can be convex, even when the set of Nash equilibria is not.

Second, we generalize our min-max characterization to inverse multiagent learning, in particular inverse MARL, where we are given an inverse Markov game, and correspondingly, a \emph{stochastic\/} oracle, and we seek a first-order simulacrum (\Cref{thm:inverse_stoch_NE}).
%that can only be accessed via \samy{}{a simulator that responds to queries with} sample \samy{queries}{trajectories} from the state-action history distribution.
% This setting covers inverse Nash equilibrium problems where the players' action might be noisy, as well as inverse multiagent reinforcement learning, when we are given a sample trajectory of play.  
We show that under standard assumptions, which are satisfied by a large class of inverse Markov games (e.g., all finite state and action Markov games and a class of continuous state and action Markov games), the ensuing min-max optimization problem is convex-gradient dominated, and thus an inverse Nash equilibrium can be computed once again via standard first-order methods in polynomial time (\Cref{thm:online_sgda}).

Third, we provide an extension of our min-max characterization to (second-order) simulacral learning (\Cref{thm:inverse_simulacrum}).
%, and we seek to compute a Nash simulacrum, i.e., parameters and associated Nash equilibrium policies, which replicate the observed state-action trajectories in expectation \sdeni{. 
We once again characterize the problem as a solution to a min-max optimization problem, for which standard first-order methods compute a first-order stationary \citep{lin2020gradient} solution in polynomial-time, using a number of observations (i.e., unfaithful samples of histories of play) that is polynomial in the size of the inverse simulation (\Cref{thm:apprenticeship_thm}).

\if 0
\amy{short version. DELETE if there is space for the full paragraph below!!!}
Finally, we run experiments and find that this algorithm outperforms the widely-used ARIMA method in predicting prices in Spanish electricity markets based on time-series data.
\fi

% \amy{will there be space for synthetic settings?} 

Finally, we include two sets of experiments.
In the first, we show that our method is effective in synthetic economic settings where the goal is to recover buyers' valuations from observed competitive equilibria (which, in this market, coincide with Nash equilibria).
Second, using real-world time-series data, we apply our method to predict prices in Spanish electricity markets, 
%%% SPACE
%by modeling the market as a Markov game, 
and find that it outperforms the widely-used ARIMA method in predicting prices on this real-world data set.

\input{tables_intro}
