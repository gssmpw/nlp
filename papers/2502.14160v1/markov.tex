%\section{Inverse Multiagent Reinforcement Learning}


\if 0
An \mydef{(infinite-horizon, discounted, parametric) Markov game} \citep{shapley1953stochastic, fink1964equilibrium, takahashi1964equilibrium} $\mgame[][\param] \doteq (\numplayers, \numactions, \states, \sigmaalges, \actionspace, \params, \param, \reward, \transkernel, \discount, \initstates)$ is a dynamic game played over an infinite time horizon.
% \ssadie{}{The state space $\states$ is assumed to be a Polish space, and $\sigmaalges$, a $\sigma$-algebra on $\states$}.%
% \footnote{usually, the Borel algebra}

\ssadie{}{
The game initiates at time $\iter = 0$ in some state $\staterv[0] \sim \initstates$ 
drawn from an initial state measure $\initstates$ defined on $(\states, \sigmaalges)$.
At each time period $\iter = 0, 1, \hdots$, each player $\player \in \players$ plays an \mydef{action} $\action[\player][][\iter] \in \actionspace[\player]$ from an action space $\actionspace[\player] \subset \R^\numactions$.%
We define the space of joint actions $\actionspace = \bigtimes_{\player \in \players} \actionspace[\player]$.
Depending on the action profile action profile $\action[][][\iter] \in \actionspace$, each player $\player$ receives a reward 
$\reward[\player] (\state[\iter][], \action[][][\iter]; \param)$ given by a \emph{parameterized\/} reward function $\rewards: \states \times \actionspace \times \params \to \R^\numplayers$.

The game then either ends with probability $1-\discount$, where $\discount \in (0,1)$ is called the discount factor, or transitions to a new state $\state[\iter+1] \in \states$, according to a  probability transition kernel $\transkernel: \sigmaalges \times \states \times \actionspace \to [0,1]$, where $\transkernel(\sigmaalge \mid \state[\iter], \action[][][\iter]) \in [0,1]$ denotes the probability of transitioning to any state $\state[\iter+1]$ in the measurable set $\sigmaalge\in \sigmaalges$ from state $\state[\iter] \in \states$.
In line with the literature \citep{fu2021evaluating, yu2019multi}, for notational convenience, we assume the probability transition kernel is independent of the parameters, but we note that our characterizations apply more broadly.\amy{need to be more specific here. how much more broadly? are there any caveats?}

A \mydef{(Markov or stationary) policy} \citep{maskin2001markov} for player $\player \in \players$ is a mapping $\policy[\player]: \states \to \actionspace$ from states to actions so that $\policy[\player](\state) \in \actionspace[\player]$ denotes the action that player $\player$ takes at state $\state$.
For each player $\player \in \players$, we define the space of all possible policies $\policies[\player] \doteq \left\{\policy[\player]: \states \to \actionspace[\player]\right\}$. 
As usual, $\policy \doteq (\policy[1], \hdots, \policy[\numplayers]) \in \policies \doteq \smash{\bigtimes_{\player \in \players} \policies[\player]}$ denotes a \mydef{policy profile}.

A \mydef{history (of play)} $\hist[][][] \in \hists[\numiters]=(\states \times \actionspace)^\numiters$ of length $\numiters \in \N$ is a sequence of state-action tuples $\hist[][][] = (\state[\iter], \action[][][\iter])_{\iter = 0}^{\numiters - 1}$.
We use notation $\Ex^{\policy}_{\initstates}[\cdot]$ indicates the expectation taken over the history of play where $\state[0]$ is randomly drawn from the initial distribution $\initstates$ over $\states$ and the transition kernel $\Pr^{\policy}[\staterv[t+1]\in \sigmaalge \mid \state[0], \action[][][0], \cdots, \state[t], \action[][][t]]=\transkernel(\sigmaalge\mid \state[t], \policy(\state[t]))$, and we let $\Pr^{\policy}_\initstates[\cdot]$ be the corresponding probability measure. 
Throughout, we denote by $\histrv[][] \doteq \left( \staterv[\iter], \actionrv[][][\iter] \right) \sim \Pr^{\policy}_\initstates$ any randomly sampled history from $\Pr^{\policy}_\initstates$.

In analyzing dynamic games, we rely on the following terminology: Given a policy profile $\policy\in \policies$, for any player $\player\in \players$, the \mydef{state-value function} 
and the \mydef{action-value function}
are respectively defined as
% \begin{align}
\mbox{$
\vfunc[\player][\policy] (\state; \param) \doteq \Ex_{\state}^{\policy} \left[\sum_{\iter = 0}^\infty  \discount^\iter\reward[\player] (\staterv[\iter], \policy(\staterv[\iter]); \param)  \right]
$}
and
$
     \qfunc[\player][\policy] (\state, \action; \param) \doteq \reward[\player](\state, \action)+ \discount \int \vfunc[\player][\policy](\state[][][\prime])\transkernel(d\state[][][\prime]\mid \state, \action)
$.
Furthermore, the \mydef{discounted state-occupancy measure} under an initial state distribution $\initstates$ and a policy profile $\policy\in \policies$ is given by $\statedist[\initstates][\policy] (\sigmaalge) \doteq 
\Ex_{\initstates}^{\policy}[\sum_{t=0}^\infty \mathbbm{1}_{\staterv[\iter]\in \sigmaalge}]=
\sum_{\iter = 0}^\infty \discount^\iter \Pr_{\initstates}^{\policy}(\staterv[\iter] \in \sigmaalge)$. 
%
The \mydef{(cumulative) payoff} 
% $\util[\player]: \policies \times \params \to \R$ 
of a policy profile $\policy \in \policies$ for a player $\player \in \players$, 
is defined as $\util[\player](\policy; \param) \doteq \Ex^{\policy}_{\initstates} \left[\sum_{\iter = 0}^\infty \discount^\iter \reward[\player](\staterv[\iter], \policy(\staterv[][][\iter]); \param) \right]=\int \vfunc[\player][\policy](\state) \initstates(d\state).$ 
% =  \Ex_{\state \sim \initstates} [\vfunc[\player][\policy](\state; \param)]$.
%
As usual, an $\varepsilon$-\mydef{Nash equilibrium} ($\varepsilon$-\nash) of a game $\mgame[][\param]$ is a policy profile $\policy[][][*] \in \policies$ such that for all $\player \in \players$, $\util[\player] (\policy[][][*]; \param) \geq \max_{\policy[\player] \in \policies[\player]} \util[\player] (\policy[\player], \policy[-\player][][*]; \param) - \varepsilon$; and a Nash equiibrium ensues when $\varepsilon = 0$. 
}
\fi

\textbf{Dynamic Games. } An \mydef{(infinite-horizon, discounted, parametric) Markov game} \citep{shapley1953stochastic, fink1964equilibrium, takahashi1964equilibrium} $\mgame[][\param] \doteq (\numplayers, \numactions, \states, \actionspace, \params, \param, \reward, \trans, \discount, \initstates)$ is a dynamic game played over an infinite time horizon.
% 
The game initiates at time $\iter = 0$ in some state $\staterv[0] \sim \initstates$ 
%$\sim \initstates \in \simplex (\states)$ 
drawn from an \mydef{initial state distribution} $\initstates \in \simplex (\states)$.
At each time period $\iter = 0, 1, \hdots$, each player $\player \in \players$ plays an \mydef{action} $\action[\player][][\iter] \in \actionspace[\player]$ from an action space $\actionspace[\player] \subset \R^\numactions$.
%\footnote{Our results also apply when $\actionspace: \states \rightrightarrows \R^\numactions$ is a correspondence whose value $\actionspace(\state)$ depends on the state $\state$.} 
We define the space of action profiles $\actionspace = \bigtimes_{\player \in \players} \actionspace[\player]$.
After the players choose their \mydef{action profile} $\action[][][\iter] \doteq (\action[1][][\iter], \hdots, \action[\numplayers][][\iter]) \in \actionspace$, each player $\player$ receives a \mydef{reward}
%\amy{i think we can use reward as the one-shot thing (at a single state) and payoffs for the cumulative thing, sum of rewards over trajectories} 
$\reward[\player] (\state[\iter][], \action[][][\iter]; \param)$ according to a \emph{parameterized} \deni{Removed footnote}
% \footnote{As the players' payoff functions depend on state-contingent reward functions in Markov games, extending our modeling paradigm requires that the players' state-contingent reward functions be parameter dependent.}
\mydef{reward profile function} $\rewards: \states \times \actionspace \times \params \to \R^\numplayers$.
%parameterized by a vector $\param \in \params$ coming from a \mydef{parameter space} $\params \subset \R^\numparams$, 
The game then either ends with probability $1-\discount$, where $\discount \in (0,1)$ is called the \mydef{discount factor}, or transitions to a new state $\staterv[\iter+1] \sim \trans(\cdot \mid \state[\iter], \action[][][\iter])$ according to a \mydef{(Markov) probability transition kernel} $\trans$ whereby for all $(\state, \action) \in \states \times \actionspace$, $\trans (\cdot \mid \state, \action) \in \simplex(\states)$, and $\trans (\state[\iter+1] \mid \state[\iter], \action[][][\iter]) = \Pr(\staterv[\iter + 1] = \state[\iter+1] \mid \staterv[\iter] = \state[\iter], \actionrv[][][\iter] = \action[][][\iter])$ is the probability of transitioning to state $\state[\iter+1]$ from state $\state[\iter]$ when the players' take action profile $\action[][][\iter]$.%
\footnote{For notational convenience, 
%In line with the literature \citep{fu2021evaluating, yu2019multi}, 
we assume the probability transition function is independent of the parameters, but we note that our min-max characterizations apply more broadly without any additional assumptions, while our polynomial-time computation results apply when, in addition to \Cref{assum:convex_param_stoch}, one assumes the probability transition function is stochastically convex (see, for instance, \citet{atakan2003stochastic}) in the parameters of the game.}

\if 0
\amy{i recommend deleting the next paragraph for space. not important enough.}

\amy{this next sentence is confusing. so far, there is no reason that $\actionspace$ is not finite.} \deni{Idk, did this help?} \amy{no, we have to say somewhere earlier that we are modelling continuous action games.}
We can incorporate finite-action \sdeni{stochastic}{Markov game}s with $\numactions \in \N_+$ 
% \amy{is $d$ overloaded? still don't know what it was before.}
actions in this framework in the usual way, simply by taking the action space for each player to be the probability simplex in $\R^{\numactions}$, i.e., for all $\player \in \players$, $\actionspace[\player] = \simplex[{d}]$, and the players' reward functions to be multilinear in the players' actions, i.e., for all states $\state \in \states$, holding parameters $\param \in \params$ and all other players' actions $\action[-\player] \in \actionspace[-\player]$ constant, $\action[\player] \mapsto \reward[\player] (\state, \action[\player], \action[-\player]; \param)$ is linear.
%i.e., for all $\player \in \players and $\state \in \states$, $\reward[\player] (\state, \cdot)$ is multilinear.
Similarly, a \mydef{Markov decision process (MDP)} is a \sdeni{stochastic}{Markov game} with $\numplayers = 1$ player \citep{bellman1966dynamic}.
% \samy{, in which case we omit all subscripts $\player$ refering to the players}{}. \amy{do we ever do this? this is a paper about games.} 
\fi

A \mydef{(stationary Markov) policy} \citep{maskin2001markov} for player $\player \in \players$ is a mapping $\policy[\player]: \states \to \actionspace$ from states to actions so that $\policy[\player](\state) \in \actionspace[\player]$ denotes the action that player $\player$ takes at state $\state$.
For each player $\player \in \players$, we define the space of all (measurable) policies $\policies[\player] \doteq \left\{\policy[\player]: \states \to \actionspace[\player]\right\}$. 
%$\policies[\player] \subset \actionspace[\player]^{\states}$,
%%% SPACE
%and all policies parameterized by $\stratspace$: $\policies[\player][\stratspace] \doteq \left\{\policy[\player][][\strat]: \states \to \actionspace[\player] \mid \strat \in \stratspace \right\} \subset \policies[\player]$.
As usual, $\policy \doteq (\policy[1], \hdots, \policy[\numplayers]) \in \policies \doteq \smash{\bigtimes_{\player \in \players} \policies[\player]}$ denotes a \mydef{policy profile}.
%%such that $\policy(\state) \in \actionspace$ denotes the action profile $\action$ played by the players at state $\state \in \states$, 
%%% SPACE
%and $\policy[][][\strat] = (\policy[1][][\strat], \hdots, \policy[\numplayers][][\strat]) \in \policies[][\stratspace] \doteq \bigtimes_{\player \in \players} \policies[\player][\stratspace]$, a \mydef{parametric policy profile}. 
%%We define the space of joint policy profiles $\policies \doteq \bigtimes_{\player \in \players} \policies[\player]$ and joint parametric policy profiles $\policies[][\stratspace] \doteq \bigtimes_{\player \in \players} \policies[\player][\stratspace]$. 
A \mydef{history (of play)} $\hist[][][] \in (\states \times \actionspace)^\numiters$ of length $\numiters \in \N$ is a sequence of state-action tuples $\hist[][][] = (\state[\iter], \action[][][\iter])_{\iter = 0}^{\numiters - 1}$.  
% A \mydef{history of play} is a sequence of state-action tuples $\{(\state[\iter], \action[][][\iter])\}_{\iter} \subseteq (\states \times \actionspace)^{\N_+}$, which summarizes the states that have been observed throughout a Markov game, and the actions taken at those states.
%\sdeni{Note that if the sequence is not an infinite one, then the game has not yet ended.}{}\amy{HUH? the game can end with some probability?}\deni{I do not remember writing this, and would like to think I have not!!!}
% Then, a \mydef{non-Markovian (or non-stationary) policy}  $\policy: \states \times (\states \times \actionspace)^{\N_+} \to \simplex(\actionspace)$ is a mapping from states and histories of play to actions such that $\policy[\player]( \action \mid \state, \{(\state[\iter], \action[][][\iter])\}_{\iter} ) \in \actionspace[\player]$ denotes the probability that $\player \in \players$ plays action $\action$ at state $\state$, having observed the history of play $\{(\state[\iter], \action[][][\iter])\}_{\iter}$.
% \sdeni{}{A policy $\policy \in \actionspace^\states$ is said to be \mydef{feasible} if, for all states $\state \in \states$, $\policy(\state) \in \actions(\state)$.}
% We denote the set of all possible Markov games by $\games$.
% \deni{TO REMOVE ENTIRELY}For simplicity,\amy{with or without LOG?} we focus on Markov games with \mydef{joint constraints},%
% \footnote{We note that even in the static game setting the majority of the literature to date has focused on games with joint constraints; little is known beyond such games.}
% where for all players $\player, \otherplayer \in \players$ and constraints $\numconstr \in [\numconstrs]$, $\actionconstr[\player][\numconstr](\state, \inner[\player], \naction[\player]) = \actionconstr[\otherplayer][\numconstr](\state, \inner[\player], \naction[\player]) = \constr[\numconstr] (\state, \action)$, \samy{}{for some function $\constr$ s.t.\@ } \amy{or ``for some alternative set of constraints $\constr[\numconstr]$'', please finish!}. 
% When the constraints are joint, \samy{it}{the joint action correspondence} depends only on the states, i.e., $\actions(\state) = \left\{\action \in \actionspace \mid \constr(\state, \action) \geq 0 \right\}$\sdeni{,and is guaranteed to be non-empty, compact, and convex at all states}{}.\amy{why?}
% % introduce a function $\constrs : \actionspace^\states \to \R^{\numstates \times \numplayers \times \numconstrs}$ such that $\constrs(\policy) = (\constr(\state, \policy(\state))_{\state \in \states}$, and define feasible policy profiles as those $\policy \in  \actionspace^\states$ s.t.\ $\constrs(\policy) \geq \zeros$.
% \deni{Make this maybe only about the jointly constrained case?}\amy{seems like you did this?}
For any policy profile $\policy \in \policies$,
%\samy{If the sequence $\hist$ is not yet realized then we refer to it as a \mydef{trajectory of play}.}{} \amy{i have no idea what this sentence means! i think histories and trajectories are the same, and both are random variables with realizations.}
% Given 
%a game $\initgame$, and  
% and an initial state $\state[0]$
define the \mydef{(discounted) history distribution} $\histdistrib[][\policy] (\hist[][][]) \doteq \initstates (\state[0]) \prod_{\iter = 0}^\numiters \discount^\iter \trans (\state[\iter +1] \mid \state[\iter], \policy (\state[\iter]))$ as the probability of observing a history $\hist$ of length $\numiters$. 
Throughout, we denote by $\histrv[][] \doteq \left( \staterv[\iter], \actionrv[][][\iter] \right)_\iter \sim \histdistrib[][\policy]$ any randomly sampled history from $\histdistrib[][\policy]$.\footnote{Let $(\states, \calF_{\states})$, $(\actionspace, \calF_{\actionspace})$, and $(\states \times \actionspace, \calF_{\states \times \actionspace})$ be the measurable spaces associated with the state, action profile, and state-action profile ($\states \times \actionspace$) spaces, respectively. 
Further, let $([0,1], \calB_{[0,1]})$, $(\R^n, \calB_{\R^n})$ be measurable spaces on $[0,1]$ and $\R^n$ defined by the Borel $\sigma$-algebra. 
For simplicity, we do not explicitly represent the reward profile function, transition probability kernel, initial state distribution, or policies as measures or measurable functions.
We note, however, that for the expectations we define to be well-posed, 
%the reward profile function, transition probability kernel, initial state distribution, and policies
they all must be assumed to be measurable functions.
We simply write $\reward: \states \times \actionspace \to \R^\numplayers$, $\trans: \states \times (\states \times \actionspace) \to [0,1]$, $\initstates: \states \to [0,1]$, and $\policy: \states \to \actionspace$ to mean, respectively, $\reward: (\states \times \actionspace, \calF_{\states \times \actionspace}) \to (\R^\numplayers, \calB_{\R^\numplayers})$, $\trans: (\states, \calF_{\states}) \times (\states \times \actionspace, \calF_{\states \times \actionspace}) \to ([0,1], \calB_{[0,1]})$, $\initstates: (\states, \calF_{\states}) \to ([0,1], \calB_{[0,1]})$, and $\policy: (\states, \calF_{\states}) \to (\actionspace, \calF_{\actionspace})$.}

Fix a policy profile $\policy \in \policies$ and a player $\player$.
In our analysis of Markov games, we rely on the following terminology. 
The \mydef{expected cumulative payoff} 
%$\util[\player]: \policies \times \params \to \R$
is given by $\util[\player](\policy; \param) \doteq \Ex_{\histrv \sim \histdistrib[][\policy]} \left[\sum_{\iter = 0}^\infty \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) \right].$
The \mydef{state-} 
% \mbox{$\vfunc: \states \times \params \to \R^\numplayers$}
and \mydef{action-value functions}  \
% \mbox{$\qfunc: \states \times \actionspace \times \params \to \R^\numplayers$}
are defined, respectively, as
% \begin{align}
$\vfunc[\player][\policy] (\state; \param) \doteq \Ex_{\histrv \sim \histdistrib[][\policy]} \left[\sum_{\iter = 0}^\infty  \reward[\player] (\staterv[\iter], \actionrv[][][\iter]; \param) \mid \staterv[0] = \state \right]$
    %\label{eq:state_value}
and
$\qfunc[\player][\policy] (\state, \action; \param) \doteq \Ex_{\histrv \sim \histdistrib[][\policy]} \left[\sum_{\iter = 0}^\infty  \reward[\player](\staterv[\iter], \actionrv[][][\iter]; \param) \mid \staterv[0] = \state, \actionrv[][][0] = \action \right]$. 
    %\label{eq:action_value}
The \mydef{state occupancy distribution}
% \amy{isn't this typically called the occupancy measure? either way, its meaning must be explained in words.} \deni{yes, I won't call it measure, and call it distribution however to keep its interpretation as being a measure or pdf up to the interpretation of the reader.} 
$\statedist[\initstates][\policy] \in \simplex(\states)$ denotes the probability that a state is reached under a policy $\policy$, given initial state distribution $\initstates$, i.e., $\statedist[\initstates][\policy] (\state) \doteq  \Ex_{\histrv \sim \histdistrib[][\policy]} \left[ \sum_{\iter = 0}^\infty \setindic[{\staterv[\iter] = \state}] \right]$. 
% =  \Ex_{\state \sim \initstates} [\vfunc[\player][\policy](\state; \param)]$.
%
Finally, as usual, an $\varepsilon$-\mydef{Nash equilibrium} ($\varepsilon$-\nash) of a game $\mgame[][\param]$ is a policy profile $\policy[][][*] \in \policies$ such that for all $\player \in \players$, $\util[\player] (\policy[][][*]; \param) \geq \max_{\policy[\player] \in \policies[\player]} \util[\player] (\policy[\player], \policy[-\player][][*]; \param) - \varepsilon$; and a Nash equilibrium ensues when $\varepsilon = 0$.\deni{Removed fn here.}%
% \footnote{While for simplicity we state our results for inverse NE, we note that our results extend to inverse Markov perfect equilibrium, i.e., a parameter $\param[][][*] \in \params$ s.t.\@ for all players $\player \in \players$ and states $\state \in \states$, $\vfunc[\player][\truepolicy] (\state; \param[][][*]) \geq \max_{\policy[\player] \in \policies[\player]} \vfunc[\player][{\policy[\player] \truepolicy[-\player]}] (\state; \param)$, as each player's value function can be shown to be gradient-dominated w.r.t.\@ the policy parameters as a function of the coefficient mismatch distribution (see, for instance, \citet{agarwal2020optimality}).}
%\amy{why are considering Nash eqa and not Markov perfect eqa?}
% \deni{Put all the following in vector notation and remove player subscripts. Also remove MPNE definitions.} 
% \samy{For notational clarity, we denote the state-value function and the action-value function for any player $\player \in \players$, state $\state \in \states$, action profile $\action \in \actions$, and policy profile $\policy \in \actionspace^\states$, as $\vfunc[\player][\policy](\state; \param)$ and $\qfunc[\player][\policy](\state, \action; \param)$ respectively.}{}\amy{just did this in the first place in the def'ns above}
%
%\begin{align}
    % \policy[\outeraction](\outeraction[][][][\iter], \inneraction[][][][\iter]) \policy[\inneraction](\outeraction[][][][\iter], \inneraction[][][][\iter]) 
%\end{align}
% 
% \alec{does the discounted state distribution require a normalization factor of $\frac{1}{1-\gamma}$?} \deni{I don't think so, results do not make use of the normalization.}
% 
% For convenience, we also define $\histdistrib[][\policy] (\hist[][][]) \doteq \Ex_{\state[0] \sim \initstates} \left[ \histdistrib[][\policy][{\state[0]}](\hist[][][]) \right]$, and w
% For each player $\player \in \players$ and history of play $\hist$ \samy{}{of length $\numiters$}, we also define the marginal discounted history distribution: \amy{what is a marginal discounted history dist'n? b/c the RHS of the eq'n below looks the same as the RHS of the eq'n above.}
% %
% \begin{align}
%     \histdistrib[\player][{\policy[\player]}](\hist[][][]) \doteq \prod_{\iter = 0}^\numiters \discount^\iter \trans(\state[\iter +1] \mid \state[\iter], \policy(\state[\iter])) \enspace .
% \end{align}
%
% Furthermore, the \mydef{discounted state \sdeni{}{occupancy} distribution} \amy{isn't this typically called the occupancy measure? either way, its meaning must be explained in words.} $\statedist[\initstates][\policy] \in \simplex(\states)$ \amy{i changed this from: $\statedist[{\state[0]}][\policy] \in \simplex(\states)$. ok?} \samy{starting at a random state $\state[0] \sim \initstates \in \simplex(\states)$}{} is defined as $\statedist[\initstates][\policy] (\state) \doteq  \Ex_{\histrv \sim \histdistrib[][\policy]} \left[ \sum_{\iter = 0}^\infty \mathbbm{1}_{\staterv[\iter] = \state} \right] = \sum_{\iter = 0}^\infty \discount^\iter \Prx_{\histrv \sim \histdistrib[\initstates][\policy]}(\staterv[\iter] = \state)$. \amy{i added a ``more fundamental'' def'n in terms of an indicator function. ok?} \deni{Yes, except I removed the $\discount$, because the history distribution already includes the discount!}
%     \statedist[\initstates][\policy](\state) \doteq \sum_{\iter = 0}^\infty \discount^\iter \Prx\limits_{\substack{\staterv[\iter] \sim \trans(\cdot \mid \staterv[\iter - 1], \policy(\staterv[\iter - 1]))\\ \staterv[0] \sim \initstates}}(\staterv[\iter] = \state) 
%     \statedist[\initstates][\policy](\state[][][\prime]) \doteq \sum_{\iter = 0}^\infty \discount^\iter  \initstates(\state) \int_{\state, \state[][][\prime\prime]} \prod_{\hist \in (\states \times \actionspace)^{\iter-1}} \trans(\state[][][\prime] \mid \state, \policy(\state)) 
% By abuse of notation, for any distribution $\initstates \sim \simplex(\states)$ over states, we also define $\statedist[{\initstates}][\policy](\state) \doteq \Ex_{\state[0] \sim \initstates} \left[\statedist[{\state[0]}][\policy](\state) \right]$.
% and $\statedist[{\state[0]}][\policy][](\state) = \statedist[{\state[0]}][\policy](\state)$

\if 0
\sdeni{}{We note that although beyond zero-sum Markov games,\amy{i don't understand. they don't exist in det. strategies in zero-sum either???}\deni{They exists} deterministic Nash equilibria are not guaranteed to exist \sdeni{}{in continuous action \sdeni{stochastic}{Markov game}s}\citep{shapley1953stochastic}, and Nash equilibria exist in general only in stochastic policies \citep{takahashi1964equilibrium, fink1964equilibrium} our model description captures both types of policies.}\deni{Can you rephrase this?}

\deni{I re-wrote the following maybe it makes sense now?}\amy{it doesn't. sorry. tried to guess what you meant in my edits} \deni{In continuous action \sdeni{stochastic}{Markov game}s where the rewards are convex-concave at each state a deterministic policy is guaranteed to exist! That is what I am trying to say.}
\sdeni{}{We note that Nash equilibria are only guaranteed to exist in stochastic policies, i.e., $\actionspace \doteq \simplex(\calB)$, where $\calB$ is a set of profiles, in finite-action games. 
In continuous-action zero-sum \samy{stochastic}{} games, they are guaranteed to exist in deterministic policies when \sdeni{payoffs}{rewards} are convex-concave at each state. 
(This result is a straightforward generalization of  \citep{shapley1953stochastic}, and a consequence of the Banach fixed point theorem \citep{banach1922operations}).}
\fi

% \deni{Write everything in terms of $\policies$.}

\if 0
An $\varepsilon$-\mydef{Markov Perfect Nash equilibrium} ($\varepsilon$-MPNE) is a policy profile $\policy[][][*]$ such that for all players $\player \in \players$ and for all states $\state \in \states$, $\vfunc[\player][{\policy[][][*]}](\state; \param) \geq \max_{\policy[\player] \in \actionspace[\player]^\states} \vfunc[\player][\policy](\state; \param) - \varepsilon$.  
Given a Markov game $\game$, a \mydef{Markov perfect (Nash) equilibrium (MPE)} $\policy[][][*] \in \simplex(\actionspace)$ is a policy profile such that for all players $\player \in \players$, and states $\state \in \states$, $\policy[\player][][*] \in \argmax_{\policy[\player] \in \actionspace^\states} \vfunc[\player][{(\policy[\player], \policy[-\player][][*])}](\state)$. We note that if $\game$ is an MDP, then a Nash equilibrium is simply called an \mydef{non-Markovian optimal policy} and a Markov perfect equilibrium is called a \mydef{Markov optimal policy}.

We call a $0$-NE and $0$-MPNE simply NE and MPNE, respectively, and note that any NE is a MPNE with probability 1. \amy{why with prob 1? also, isn't MPNE stronger than NE? is the implication not reversed: any MPNE is a NE?}
\fi

% \deni{ADD DISCUSSION ON EXISTENCE OF NASH, REALLY IMPORTANT!}

%The \mydef{regret} $\regret[][\util]: \actionspace \times \actionspace \to \R^\numplayers$ for playing an action profile $\action$ as compared to another action profile $\otheraction$, as follows: for all followers $\player \in \players$,
% $\regret[\player][](\action, \otheraction; \outer) = \util[\player](\outer, (\otheraction[\player], \action[-\player])) - \util[\player](\outer, \action)$.

%\amy{i think we can cut this paragraph:}
% The \mydef{cumulative regret} $\cumulregret[][]: \actionspace^\states \times \actionspace^\states \times \params \to \R$ across all players, between two policy profiles $\policy, \otherpolicy \in \actionspace^\states$, for any parameter $\param \in \params$, is 
% $\cumulregret[][](\policy, \otherpolicy; \param) = \sum_{\player \in \players}  \util[\player](\otherpolicy[\player], \policy[-\player]; \param) - \util[\player](\policy; \param)$.
% Furthermore, the \mydef{exploitability} or (Nikaido-Isoda potential function \citep{nikaido1955note}) of a policy profile $\policy \in \actionspace^\states$ for any parameter $\param \in \params$ can be seen as $\exploit[][] (\policy; \param) = \max_{\otherpolicy \in \actionspace^\states} \cumulregret[][](\policy, \otherpolicy; \param)$ \citep{goktas2022exploit}. 
% Finally, for all parameters $\param \in \params$ and policy profiles $\policy \in \actionspace^\states$, $\exploit (\policy; \param) \geq 0$; moreover, $\policy[][][][*]$ is a Nash equilibrium iff $\exploit[] (\policy[][][][*]; \param) = 0$.

% \amy{we could say, but i don't even think we have to if (when!) we are short on space:}
% \samy{The definitions of cumulative regret and exploitability generalize in a straightforward manner from one-shot to dynamic games.}{}
