\subsection{Ommited Proofs}\label{sec:app_proofs}

\thminverseNE*
\samy{}
% {\color{red}Amy's proof}
\begin{proof}[Proof of \Cref{thm:inverse_NE}]
\if 0
By assumption, the set of inverse-NE is non-empty,
meaning there exists a parameter profile $\widehat{\param} \in \params$ such that $\truestrat$ is a NE of $\game$, i.e., $\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \widehat{\param}) - \util[\player] (\truestrat; \widehat{\param}) = 0$, for all players $\player \in \players$.
%
Equivalently,
\begin{align}
    \max_{\player \in \players} \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \widehat{\param}) - \util[\player] (\truestrat; \widehat{\param}) = 0
\end{align}
\fi

\if 0
% WRONG DIRECTION
But since $\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param) - \util[\player] (\truestrat; \param) \ge 0$, for all parameter profiles $\param \in \params$, it follows that
\begin{align}
    \sum_{\player \in \players} \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \widehat{\param}) - \util[\player] (\truestrat; \widehat{\param}) = 0
\end{align}
\fi

By the definition of $\obj$, for all parameter profiles $\param \in \params$,
    \begin{align}
        \max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat) 
        &= \max_{\otherstrat \in \stratspace} \sum_{\player \in \players} \left[\util[\player] (\otherstrat[\player], \truestrat; \param) - \util[\player] (\truestrat; \param) \right] \\
        &= \sum_{\player \in \players} \left[ \underbrace{\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param) - \util[\player] (\truestrat; \param)}_{\geq 0} \right] \\
        &\ge 0
    \end{align}

But since the set of inverse NE is non-empty by assumption, $\min_{\param \in \params} \max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat) = 0$. 
% {\color{red}Sadie: I don't think we can directly state this since this is what we want to prove, i.e., $\max_{\otherstrat\in \stratspace}\obj(\param^*, \otherstrat) \iff \param^*$ is a NE. I prefer stay with Deni's original proof.}
% \sadie{I don't think we can directly state this since this is what we want to prove, i.e., $\max_{\otherstrat\in \stratspace}\obj(\param^*, \otherstrat) \iff \param^*$ is a NE. I prefer stay with Deni's original proof.}
% {\color{red}Amy: Huh? He has the exact same assumption and the exact same implication in his proof: Additionally, note that under our assumption the set of inverse-NE is non-empty.}
% \amy{Huh? He has the exact same sentence in his proof: Additionally, note that under our assumption the set of inverse-NE is non-empty.}

But then if $\param^* \in \params$ minimizes $\max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat)$, then $\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param^*) - \util[\player] (\truestrat; \param^*) = 0$, for all players $\player \in \players$, which implies $\truestrat$ is a NE and $\param^*$ is an inverse NE.
\end{proof}


\amy{Deni's original proof:}
\begin{proof}[Proof of \Cref{thm:inverse_NE}]
    Notice that for all action profiles $\strat \in \stratspace$ and parameter profile $\param \in \params$, we have:
    \begin{align}
        \max_{\otherstrat \in \stratspace} \obj(\param, \otherstrat) &= \max_{\otherstrat \in \stratspace} \sum_{\player \in \players} \left[\util[\player](\otherstrat[\player], \truestrat; \param) - \util[\player](\truestrat; \param) \right] \\
        &= \sum_{\player \in \players} \left[\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player](\otherstrat[\player], \truestrat[-\player]; \param) - \util[\player](\truestrat; \param) \right]\\
        &\geq 0
    \end{align}
 
    Additionally, note that under our assumption the set of inverse-NE is non-empty. Hence, there exists a parameter profile $\widehat{\param} \in \params$ such that $\widehat{\param}$ is an inverse-NE,  of $\game[][-1]$, i.e., for all players $\player \in \player$:
    \begin{align}
        \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player](\otherstrat[\player], \truestrat[-\player]; \widehat{\param}) - \util[\player](\truestrat; \widehat{\param}) = 0
    \end{align}
% 
Summing the above equality across all players, we then have:
\begin{align}
    \sum_{\player \in \players} \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player](\otherstrat[\player], \truestrat[-\player]; \widehat{\param}) - \util[\player](\truestrat; \widehat{\param}) = 0 \label{eq:min_eq_zero_obj_cumul}
\end{align}
%     \begin{align}
%         &\max_{\otherstrat \in \stratspace} \obj(\param[][][*], \strat[][][][*], \otherstrat) \\
%         &= \max_{\otherstrat \in \stratspace} \left\{\mixparamone\left\|\truestrat - \strat[][][][*] \right\|^2_2 + \mixparamtwo \cumulregret[](\truestrat, \otherstrat; \param[][][*]) \right\}\\
%         &=  \mixparamone\left\|\truestrat - \truestrat \right\|^2_2 + \mixparamtwo \max_{\otherstrat \in \stratspace} \cumulregret[](\truestrat, \otherstrat; \param[][][*])\\ 
%         &\geq \mixparamone (0) + \mixparamtwo (0) = 0 \label{eq:min_eq_zero_obj}
%     \end{align}

% \noindent where the final line follows from the definition of the inverse Nash equilibrium, i.e., $\cumulregret[](\truestrat, \otherstrat; \param[][][*]) = 0$.

    This means that the minimum of $\max_{\otherstrat \in \stratspace} \obj(\param, \otherstrat)$ is achieved at 0, since for all $\param \in \params$, $\max_{\otherstrat \in \stratspace} \obj(\param, \otherstrat) \geq 0$. 
    

Let $(\param[][][*], \strat[][][][*])$ be any optimal solution to 
%the min-max optimization problem
$\min_{\param \in \params} \max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat)$.
We will show that $\param[][][*]$ is an inverse NE of $\game[][-1]$.

Since the minimum of $\max_{\otherstrat \in \stratspace} \obj (\param, \otherstrat)$ is achieved at $0$, it follows that
    \begin{align}
        \max_{\otherstrat \in \stratspace} \obj (\param[][][*], \otherstrat) = \sum_{\player \in \players} \underbrace{\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param[][][*]) - \util[\player] (\truestrat; \param[][][*])}_{\geq 0} = 0
    \end{align}
%
But then, since for all players $\player \in \players$, $\max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param[][][*]) - \util[\player] (\truestrat; \param[][][*]) \geq 0$, it must hold that:
    \begin{align}
        \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param[][][*]) - \util[\player] (\truestrat; \param[][][*]) = 0 \\
        \max_{\otherstrat[\player] \in \stratspace[\player]} \util[\player] (\otherstrat[\player], \truestrat[-\player]; \param[][][*]) = \util[\player] (\truestrat; \param[][][*])
    \end{align}
\noindent hence proving that $\truestrat$ is a Nash equilibrium under parameters $\param[][][*]$, i.e., $\param[][][*]$ is an inverse Nash equilibrium.
\end{proof}
% 
\vspace{2em}


% 
\begin{reptheorem}[\ref{thm:concave_game_inverse_NE}]
% \label{thm_app:concave_game_inverse_NE}
    Suppose that Assumptions \ref{assum:concave_game}--\ref{assum:smoothness} hold. 
    If \Cref{alg:gda} is run with inputs that satisfy for all $\varepsilon \geq 0$, $\iter \in [\numiters]$ $\learnrate[\otherstrat][\iter] = \learnrate[\param][\iter] = \frac{2\sum_{\player \in \players} \lipschitz[{\grad \util[\player]}]}{\iter}$, and $\numiters \geq \frac{\diam(\params \times \stratspace)}{\varepsilon^2}$ for $\varepsilon \geq 0$, then the time-average of all parameters $\mean[{\param[][\numiters]}] \doteq \frac{1}{\numiters + 1}\sum_{\iter = 0}^\numiters \param[][\iter]$ is an $\varepsilon$-inverse NE, i.e., $\cumulregret[] (\truestrat, \otherstrat; \mean[{\param[][\numiters]}]) - \min_{\substack{\param \in \params}} \max_{\otherstrat \in \stratspace} \cumulregret[] (\truestrat, \otherstrat; \param) \leq \varepsilon$.
    %\deni{correct just seemed superfluous.}
\end{reptheorem}



\begin{proof}[Proof of \Cref{thm:concave_game_inverse_NE}]
    The theorem is a direct consequence of Result 3.1 of \citet{nemirovski2009robust}.
\end{proof}
% 
\vspace{1em}
% 
\begin{reptheorem}[\ref{thm:online_sgda}]
% \label{thm_app:online_sgda}
    % \deni{Add def'n of $\sigma$ and $\left\|\frac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|$ $AA$}
    Under \Cref{assum:smooth_convex_invex}, if \Cref{alg:online-sgda} is run with inputs that satisfy for all $\iter \in \numiters$, $\varepsilon \in (0,1)$,  $\learnrate[\otherstrat][\iter] \asymp  \frac{\epsilon^4 \left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{(1-\discount)}\ssadie{}{\cdot \scparam}\right)^2}{\lipschitz[\grad \obj]^3\left(\lipschitz[\grad \obj]^2 + \sigma^2\right) \left( \nicefrac{\lipschitz[\obj]}{\lipschitz[\grad \obj]} + 1\right)} $,  
    $\learnrate[\param][\iter] \asymp \frac{\varepsilon^8 \left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{(1-\discount)}
    \ssadie{}{\cdot \scparam}\right)^4}{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}} \land \frac{\varepsilon^2}{\lipschitz[\grad \obj] \left(\lipschitz[\obj]^2 + \sigma^2\right)}$, and $\numiters \geq \ssadie{\left(\nicefrac{(1-\discount)}{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty} + \frac{\lipschitz[\obj]}{2 \lipschitz[{\grad \obj}]} \right)^{-1} }{\left( 1 + \frac{\lipschitz[\obj]}{2 \lipschitz[{\grad \obj}]} \right)^{-1}}
    \ssadie{\frac{\lipschitz[\obj] \diam(\stratspace \times\params)}{\varepsilon^{2}\learnrate[\otherstrat][\iter]}}{\frac{\lipschitz[\obj] \diam(\stratspace \times\params)}{\varepsilon^{2}\learnrate[\param][\iter]}}$, 
    \sadie{Here the first part should be the 1/gradient dominance param w.r.t $\param$ instead of $\otherstrat$ }
    % and     
    % $O\left(\nicefrac{\diam(\stratspace \times\params)}{\varepsilon^{10}} \right)$ 
    % \left( \frac{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}}{\varepsilon^{10} \left(\frac{AA}{1-\discount}\right)^4}\right) \right) \geq \frac{\max_{\param, \param[][][\prime] \in \params}\left\| \param[][0] - \param[][][*] \right\| + \max_{\otherstrat, \otherstrat[][][][\prime] \in \stratspace} \left\| \otherstrat[][] - \otherstrat[][][][\prime] \right\|}{\varepsilon^10}
     then the time-average of all parameters $\mean[{\param[][\numiters]}] \doteq \frac{1}{\numiters + 1}\sum_{\iter = 0}^\numiters \param[][\iter]$ is a $\varepsilon$-inverse NE, i.e., $\max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \mean[{\param[][\numiters]}]) - \min_{\substack{\param \in \params}} \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \param) \leq \varepsilon$. 
\end{reptheorem}
\begin{proof}[Proof of \Cref{thm:online_sgda}]
    Firstly, note that under \Cref{assum:convex_param_stoch}, $\obj (\param, \strat)$ is $1$-gradient-dominated in $\param$ since it is \ssadie{concave}{convex} in $\param$ for all $\otherstrat \in \stratspace$ (see Definition 2 of \citet{bhandari2019global}). 

    Now, define  the \mydef{equilibrium distribution mismatch coefficient} $\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty$ as the Radon-Nikodym derivative of the state-visitation distribution of the Nash equilibrium $\policy[][][\dagger]$ w.r.t.\@ the initial state distribution $\initstates$.
    Under \Cref{assum:smooth_convex_invex}, by \ssadie{Corollary 1}{Theorem 2} and Theorem 4 of \citet{bhandari2019global}, we also have that $\obj (\param, \strat)$ is \ssadie{$\left(\nicefrac{\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty}{(1- \discount)}\right)$}{$\left(\nicefrac{\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty}{(1- \discount)} \cdot \scparam\right)$}-gradient-dominated in $\strat$ for all $\param \in \params$.

    % Define \mydef{Moreau envelope of the empirical exploitability} $$\tilde{\obj}(\param, \strat) \doteq \max_{\strat[][][][\prime] \in  \stratspace} \left\{ \obj (\param, \strat) + \lipschitz[{\grad \avg[\otherobj]}]\left\| \strat - \strat[][][][\prime]\right\|^2\right\}$$, i.e., a point $(\param, \strat) \in \params \times \stratspace$ s.t. $\left\|\grad \regulexploit(\param, \strat) \right\| = 0$ 
    % Then, 
    \ssadie{}{Moreover, according to analysis in \Cref{sec_app:gradient_estimate}, the variance of the gradient estimator is bounded.}
    Hence, under our Theorem's assumptions, the assumptions of Theorem 2 of \citet{daskalakis2020independent} are satisfied, and we have:
    \begin{align}
        \frac{1}{\numiters + 1} \sum_{\iter = 0}^\numiters \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \param[][\iter]) - \min_{\substack{\param \in \params}} \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \param) \leq \varepsilon
    \end{align}
% 
% 
    Note that since $\cumulregret[] (\truestrat, \strat, \param)$ is convex in $\param$ for all $\strat \in \stratspace$, then $\strat \mapsto \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \param[][\iter])$ is convex by Dankin's theorem \cite{danskin1966thm}. Hence, using convexity, we obtain the theorem's result:
% 
    \begin{align}
         \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \frac{1}{\numiters + 1} \sum_{\iter = 0}^\numiters \param[][\iter]) - \min_{\substack{\param \in \params}} \max_{\strat \in \stratspace} \cumulregret[] (\truestrat, \strat; \param) \leq \varepsilon
    \end{align}
    
%     This latter condition implies that for all parameters $\param \in \params$, any stationary point of $\obj$ is also a global maximum $\strat \mapsto \obj (\param, \strat)$,
% %\samy{but not necessarily the other way round,}{} 
% as gradient-dominance allows us to upper-bound the distance between the function at any point and the global optimum in terms of the gradient of the function at that point.
\end{proof}

\Cref{thm:online_sgda} tells us that in inverse \sdeni{stochastic}{Markov game}s satisfying \Cref{assum:smooth_convex_invex}, an $\varepsilon$-inverse NE can be computed in $\numiters \asymp \nicefrac{\variance^2}{ \varepsilon^{10}} \|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \|_\infty$.
One way to interpret this result is that the closer the initial state distribution is to the equilibrium state visitation distribution, i.e., the smaller $\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|$ is, and the smaller the variance $\variance^2$ of the gradient estimators is, the faster the convergence.
On the other hand, if any state that is visited with strictly positive probability by the Nash equilibrium policy is not part of the support of the initial state distribution, then $\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\| \to \infty$, and the convergence bound degrades arbitrarily.


\thminversesimulacrum*

\begin{proof}[Proof of \Cref{thm:inverse_simulacrum}]
    Fix $\mixparamone, \mixparamtwo>0$. 
    Let $(\param[][][*], \policy[][][*])$ be the optimal solutions to the above optimization problem. Notice that for all policy profiles $\policy \in \policies$ and parameter profiles $\param \in \params$, we have $\mixparamone\left\|\policy -  \truepolicy \right\|^2_2 \geq 0$ by the definition of the euclidean norm. Additionally, we have for all $\policy \in \policies$, $\param \in \params$:
    \begin{align}
        \max_{\otherpolicy \in \policies} \mixparamtwo \cumulregret[] (\policy, \otherpolicy; \param) &= \max_{\otherpolicy \in \policies} \mixparamtwo \sum_{\player \in \players} \left[\util[\player] (\otherpolicy[\player], \policy; \param) - \util[\player] (\policy; \param) \right] \\
        &= \mixparamtwo \sum_{\player \in \players} \left[\max_{\otherpolicy[\player] \in \policies[\player]} \util[\player] (\otherpolicy[\player], \policy[-\player]; \param) - \util[\player] (\policy; \param) \right]\\
        &\geq 0
    \end{align}
    Hence, we have: 
    \begin{align}
        \max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy) 
        &= 
        \max_{\otherpolicy \in \policies} \left\{ \mixparamone \Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][\policy] \times \obsdistrib[][][\truepolicy]}} \left[\left\|\obs - \trueobs \right\|^2 \right] + \mixparamtwo \cumulregret[] (\policy, \otherpolicy; \param) \right\}\\
        &=  \mixparamone \Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][\policy] \times \obsdistrib[][][\truepolicy]}} \left[\left\|\obs - \trueobs \right\|^2 \right] + \max_{\otherpolicy \in \policies} \mixparamtwo \cumulregret[] (\policy, \otherpolicy; \param)\\ 
        &\geq \mixparamone (0) + \mixparamtwo (0) = 0
    \end{align}
    Additionally, note that under our assumption the set of inverse-NE is non-empty. Hence, there exists a tuple of parameter and action profiles $(\param[][][*], \policy[][][*])$ such that $\policy[][][*] = \truepolicy$, and $\param[][][*]$ is an inverse-NE,  of $(\numplayers, \numactions, \policies, \params, \util, \truepolicy)$:
% 
% \Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][\policy] \times \obsdistrib[][][\truepolicy]}}
% 
    \begin{align}
        &\max_{\otherpolicy \in \policies} \otherobj(\param[][][*], \policy[][][*], \otherpolicy) \\
        &= \max_{\otherpolicy \in \policies} \left\{\mixparamone\Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][{\policy[][][*]}] \times \obsdistrib[][][\truepolicy]}}  \left[\left\|\obs - \trueobs \right\|^2 \right] + \mixparamtwo \cumulregret[] (\truepolicy, \otherpolicy; \param[][][*]) \right\}\\
        &=  \mixparamone\Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][{\policy[][][*]}] \times \obsdistrib[][][\truepolicy]}}  \left[\left\|\obs - \trueobs \right\|^2 \right] + \mixparamtwo \max_{\otherpolicy \in \policies} \cumulregret[] (\truepolicy, \otherpolicy; \param[][][*])\\ 
        &= \mixparamone (0) + \mixparamtwo (0) = 0  \label{eq:min_eq_zero_obj}
    \end{align}

\noindent where the final line follows from the definition of the inverse Nash equilibrium, i.e., $\cumulregret[] (\truepolicy, \otherpolicy; \param[][][*]) = 0$.

    This in turn means that the minimum of $\max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy)$ is achieved at 0. 
    
    Finally, we show that any tuple $(\param[][][*], \policy[][][*])$ of parameter and action profiles which are a minimum of $\max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy)$, i.e. $(\param[][][*], \policy[][][*]) \in \argmin_{\substack{\param \in \params \\ \policy \in \policies}} \max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy)$ respectively correspond to a tuple $(\param[][][*], \policy[][][*])$ such that $\Ex_{\substack{(\obs,\trueobs) \sim \obsdistrib[][][{\policy[][][*]}] \times \obsdistrib[][][\truepolicy]}}$, and $\param[][][*]$ is an inverse-NE of $(\numplayers, \numactions, \policies, \params, \util, \policy[][][*])$.

    Recall that, by \Cref{eq:min_eq_zero_obj}, we have: 
    \begin{align}
        \min_{\substack{\param \in \params \\ \policy \in \policies}} \max_{\otherpolicy \in \policies} \otherobj(\param, \policy, \otherpolicy) &= \min_{\substack{\param \in \params \\ \policy \in \policies}} 
        \max_{\otherpolicy \in \policies} \left\{ \mixparamone\left\|\truepolicy - \policy \right\|^2_2 + \mixparamtwo \cumulregret[] (\policy, \otherpolicy; \param) \right\}\\ &= \min_{\substack{\param \in \params \\ \policy \in \policies}} 
        \left\{ \mixparamone \underbrace{\left\|\truepolicy - \policy \right\|^2_2}_{\geq 0} + \underbrace{\max_{\otherpolicy \in \policies} \mixparamtwo \cumulregret[] (\policy, \otherpolicy; \param)}_{\geq 0} \right\} \\
        &= 0 
    \end{align}

    Hence, it must be that $\left\|\truepolicy - \policy[][][*] \right\|^2_2 = 0$ and $\max_{\otherpolicy \in \policies} \cumulregret[] (\policy[][][*], \otherpolicy; \param[][][*]) = 0$, proving the desired result.
\end{proof}


% \thmapprenticeshipthm*
% \begin{proof}[Proof of \Cref{
% thm:apprenticeship_thm}]
    
% \end{proof}








As the computation of a simulacrum is in general a non-convex-non-concave problem, we cannot compute a solution to the min-max optimization in polynomial-time, however, we can obtain best iterate convergence to a stationary point of the \mydef{Moreau envelope of the empirical exploitability} $\regulexploit(\param, \strat) \doteq \min_{(\param[][][\prime], \strat[][][][\prime]) \in \params \times \stratspace} \left\{ \exploit(\param, \strat) + \lipschitz[{\grad \avg[\otherobj]}]\left\| (\param, \strat) - (\param[][][\prime], \strat[][][][\prime])\right\|^2\right\}$, i.e., a point $(\param, \strat) \in \params \times \stratspace$ \sadie{This exploitability is not empirical here} s.t. $\left\|\grad \regulexploit(\param, \strat) \right\| = 0$ under suitable assumptions satisfied by a large class of Markov games including discrete state and action space Markov games.%
\footnote{We note that stationary points of the Moreau envelope correspond to stationary points of the subgradient of $\avg[\exploit]$, but as exploitability \amy{exploitability!!! uh oh!!!} is not neccessarily differentiable, the Moreau envelope is used to measure distance to a stationary point~\cite{lin2020gradient}.}




\begin{reptheorem}[\ref{thm:apprenticeship_thm}]\label{thm_app:apprenticeship_thm}
    % \deni{Add def'n of $\sigma$ and $\left\|\frac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|$ $AA$}
    Suppose that \Cref{assum:smooth_convex_invex} holds, and assume in addition that for all $\policy[][][\strat] \in \policies[][\stratspace]$ , $\obsdistrib[][][{\policy[][][\strat]}]$ is twice continuously differentiable in $\strat$.  
    If \Cref{alg:offline-sgda} is run with inputs that satisfy for all $\iter \in [\numiters]$ $\varepsilon \in (0,1)$,  $\learnrate[\otherstrat][\iter] \asymp  \frac{\epsilon^4 \left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{1-\discount}\ssadie{}{\cdot \scparam}\right)^2}{\lipschitz[\grad \obj]^3\left(\lipschitz[\grad \obj]^2 + \sigma^2\right) \left( \nicefrac{\lipschitz[\obj]}{\lipschitz[\grad \obj]} + 1\right)} $,  
    $\learnrate[\param][\iter] \asymp \frac{\varepsilon^8 \left(\nicefrac{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty}{1-\discount} \ssadie{}{\cdot \scparam}\right)^4}{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}} \land \frac{\varepsilon^2}{\lipschitz[\grad \obj] \left(\lipschitz[\obj]^2 + \sigma^2\right)}$, \sadie{Do we have bound on $\numiters$ here?}
    % and $\numiters \geq \left(\nicefrac{(1-\discount)}{\left\|\nicefrac{\partial\statedist[\initstates][{\policy[][][*]}]}{\partial \initstates} \right\|_\infty} + \frac{\lipschitz[\obj]}{2 \lipschitz[{\grad \obj}]} \right)^{-1} \frac{\lipschitz[\obj] \diam(\stratspace \times\params)}{\varepsilon^{2}\learnrate[\otherstrat][\iter]}$, 
    % and     
    % $O\left(\nicefrac{\diam(\stratspace \times\params)}{\varepsilon^{10}} \right)$ 
    % \left( \frac{\lipschitz[\grad \obj]^5 \lipschitz[\obj] \left(\frac{\lipschitz[\obj]}{\lipschitz[\grad \obj]}^2 + 1\right)^4 \left(\lipschitz[\obj]^2 + \sigma^2\right)^{\nicefrac{3}{2}}}{\varepsilon^{10} \left(\frac{AA}{1-\discount}\right)^4}\right) \right) \geq \frac{\max_{\param, \param[][][\prime] \in \params}\left\| \param[][0] - \param[][][*] \right\| + \max_{\otherstrat, \otherstrat[][][][\prime] \in \stratspace} \left\| \otherstrat[][] - \otherstrat[][][][\prime] \right\|}{\varepsilon^10}
      then the best-iterate parameters and policies $(\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) \in \argmin_{\iter \in [\numiters]} \left\|\grad \regulexploit(\param[][\iter], \strat[][][\iter]) \right\|$ converge to a stationary point of the exploitability, i.e., $\left\|\grad \regulexploit(\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) \right\| \leq \varepsilon$. 

     Additionally, for any $\zeta, \xi \geq 0$ and for a sample size of equilibrium observations $\numsamples \asymp \nicefrac{1}{\xi^2} \log(\nicefrac{1}{\zeta})$, with probability $1-\zeta$, we have:
        \begin{align}
            \avg[{\exploit}] (\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) - \exploit(\bestiter[{\param}][\numiters], \bestiter[{\strat}][\numiters]) \leq \xi
        \end{align}
\end{reptheorem}

\begin{proof}[Proof of \Cref{thm_app:apprenticeship_thm}]
    Although \cite{daskalakis2020independent}'s Theorem 2 is stated for functions which are gradient-dominated-gradient-dominated, their proof falls through for any function which is non-convex-gradient-dominated. Define  the \mydef{equilibrium distribution mismatch coefficient} $\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty$ as the Radon-Nikodym derivative of the state-visitation distribution of the Nash equilibrium $\policy[][][\dagger]$ w.r.t.\@ the initial state distribution $\initstates$.
    Under \Cref{assum:smooth_convex_invex}, by Corollary 1 and Theorem 4 of \citet{bhandari2019global}, we have that $\otherobj(\param, \strat, \otherstrat)$ is\ssadie{ $\left(\nicefrac{\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty}{(1- \discount)}\right)$}{ $\left(\nicefrac{\|\nicefrac{\partial\statedist[\initstates][{\policy[][][\dagger]}]}{\partial \initstates} \|_\infty}{(1- \discount)}\cdot \scparam\right)$}-gradient-dominated in $\otherstrat$ for all $\param \in \params$ and $\strat \in \stratspace$.

    % Define \mydef{Moreau envelope of the empirical exploitability} $$\tilde{\obj}(\param, \strat) \doteq \max_{\strat[][][][\prime] \in  \stratspace} \left\{ \obj (\param, \strat) + \lipschitz[{\grad \avg[\otherobj]}]\left\| \strat - \strat[][][][\prime]\right\|^2\right\}$$, i.e., a point $(\param, \strat) \in \params \times \stratspace$ s.t. $\left\|\grad \regulexploit(\param, \strat) \right\| = 0$ 
    % Then, 
    
       \ssadie{}{Moreover, according to analysis in \Cref{sec_app:gradient_estimate}, the variance of the gradient estimator is bounded.} Hence, under our Theorem's assumptions, the assumptions of Theorem 2 of \citet{daskalakis2020independent} are satisfied, and we have:
    \begin{align}
        \frac{1}{\numiters + 1} \sum_{\iter = 0}^\numiters \left\|\grad \regulexploit(\param[][\iter], \strat[][][\iter]) \right\| \leq \varepsilon
    \end{align}
% 
% 
Taking a minimum across all $\iter = 0, 1, \hdots, \numiters$, we then have:
% 
    \begin{align}
          \min_{\iter = 0, 1, \hdots, \numiters} \left\|\grad \regulexploit(\param[][\iter], \strat[][][\iter]) \right\| \leq \varepsilon
    \end{align}
    
    
    The second part is then a direct consequence of the hoeffding bound, whose assumptions are satisfied since the objective is bounded from above and from below by 0, as the objective is continuous and its domain is non-empty, and compact.
\end{proof}
