\subsection{Gradient Estimators}\label{sec_app:gradient_estimate}

Notice that the deterministic policy gradient theorem tells us that to compute a the policy gradient we need to compute the gradient of state-action value function with respect to the actions and then multiply it by the gradient of the policy w.r.t. the policies. Since we have access to a first order oracle of the reward and transition we can then compute gradient of the cumulative regret with the following quantities:

\begin{align}
    &\obj[\strat](\param, \otherstrat; \histmatrix, \hist[][\prime]) \notag \\ 
    &\propto \sum_{\player \in \players} \left[ \grad[\action] \reward[\player](\state[0][][\player,], \action[][][0][\player,]; \param) \right. \notag \\&+ \left. \discount \grad[\action] \trans(\state[1][][\player,] \mid \state[0][][\player,], \action[][][0][\player,])\left[ \reward[\player]\left(\state[1][][\player,], \action[][][1][\player,]; \param \right) + \sum_{\iter = 2}^\infty \reward[\player]\left(\state[\iter][][\player,], \action[][][\iter][\player,] \right) \prod_{k = 2}^{\iter} \discount^{k+1} \trans(\state[k][][\player,] \mid \state[k-1][][\player,], \action[][][k-1][\player,]) \right] \right]
\end{align}
\begin{align}
    \obj[\param](\param, \otherstrat; \hist, \hist[][\prime]) &\doteq \sum_{\player \in \players} \left[\sum_{\iter}\grad[\param]\reward[\player](\state[\iter][][\player,], \action[][][\iter][\player,]; \param)  -   \sum_{\iter} \grad[\param] \reward[\player](\state[\iter][][\prime], \action[][][\iter][\prime]; \param) \right]
\end{align}




Under \Cref{assum:smooth_convex_invex}, these estimators are unbiased estimates of the gradients $\grad[\param] \obj$ and $\grad[\strat] \obj$, respectively.
Assuming these estimators have bounded variance, we can now solve the min-max optimization problem $\min_{\param \in \params} \max_{\strat \in \stratspace} \obj(\param, \strat)$ for an $\varepsilon$-inverse NE in $O(\nicefrac{1}{\varepsilon})$ iterations via stochastic gradient descent (\Cref{alg:online-sgda}).%
\footnote{With additional care, the assumption made on the rewards and probability transition functions in Part 2 of \Cref{assum:smooth_convex_invex} can be weakened to continuous differentiability and local Lipschitz-continuity, respectively (see
%%% SPACE
%, for instance, 
Lemma 3.2 of \citet{suh2022differentiable}) to obtain unbiased estimates;
%%% SPACE
%regardless,
for clarity we make the stronger assumption.} 
It thus remains to show that the variance of the gradient estimators are bounded: i.e., there exists $\variance \in [0, \infty)$ s.t.\@ for all $(\param, \strat) \in \params \times \stratspace$, $\left\| \Ex_{\hist, \hist[][\prime]}[(\obj[\param], \obj[\strat]) (\param, \strat; \hist, \hist[][\prime])] - \grad \obj(\param, \strat) \right\| \leq \variance$. 
Since rewards, transitions, and policies, are twice continuously-differentiable, both the gradient estimates $(\obj[\param], \obj[\strat])$ and $\grad \obj$ also are, and we have:
$\left\|\Ex_{\hist, \hist[][\prime]}[(\obj[\param], \obj[\strat])(\param, \strat; \hist, \hist[][\prime])] - \grad \obj(\param, \strat) \right\|\leq \max_{\param, \strat, \hist, \hist[][']} \left\|  (\obj[\param], \obj[\strat])(\param, \strat; \hist, \hist[][\prime])\right\| = \left\|(\obj[\param], \obj[\strat])\right\|_\infty$, where the max is well defined, since the objective is continuous and the maximization domains $\states, \actionspace, \stratspace, \params$ are non-empty and compact. 
This means that, under \Cref{assum:smooth_convex_invex}, the variance of gradient estimator $(\obj[\param], \obj[\strat])$ is bounded by $\variance^2 \doteq \left\|(\obj[\param], \obj[\strat])\right\|_\infty^2$.
