\subsection{Related Work}

\paragraph{Microeconomics}

The literature on characterizing agent preferences that can be rationalized by payoff functions, known under the names of \mydef{reveled preference theory} \cite{samuelson1948consumption, afriat1967construction, varian1982nonparametric, varian2006revealed} and the \mydef{integrability problem} \cite{mas-colell}, far predates concerns of computing payoff functions that generate observed behavior.
While revealed preference theory is concerned with understanding when a set of observed purchasing decisions for a consumer and associated market conditions (e.g., prices) is consistent with payoff-maximizing behavior, the integrability problem aims to characterize those consumption functions that can arise as the solution to a payoff maximization problem.\amy{i can't grok the integrability problem from this description.}\deni{Integrability assumes you observe the whole demand function, while learning from revealed preferences assumes you observe only samples from the demand function.}
The difference between revealed preference theory and the integrability problem is analogous to the difference between inverse optimization and inverse learning.


\paragraph{Econometrics}

A large body of work in the econometrics literature is dedicated to inverse game theory\amy{it doesn't make sense to me that the econometrics people would be doing IGT, and not IRL. they are all statistics all the time, and typically infer whatever it is they infer from samples}, with a recent focus on inferring bidders' valuations in online auctions. \citeauthor{nekipelov2015econometrics} \citeyear{nekipelov2015econometrics} analyzed inferring bidders' utilities in online ad auctions, assuming bidders are no-regret learners, and hence learn (coarse) correlated equilibrium.
%another common behavioral assumption is that players to play a Nash equilibrium.
\citeauthor{syrgkanis2017inference} \citeyear{syrgkanis2017inference} propose a method that infers agent types, assuming they play a Bayes-Nash equilibrium.
More broadly, the identification literature \cite{bresnahan1991empirical, lise2001estimating, bajari2010identification} is closely related to our work, but usually does not address computational complexity concerns.
Furthermore, the settings considered in this literature are overwhelmingly \sdeni{one-shot}{normal-form} Bayesian, game, while our primary focus in this paper is (complete-information) stochastic games.


\paragraph{Inverse Optimization.} 

Inverse optimization \cite{heuberger2004inverse,chan2021inverse} seeks to recover the parameters of an optimization problem given access to the solution of the problem. One of the central results in inverse optimization demonstrates that one can recover the objective function of any linear inverse optimization problem from its solution by solving a linear program \cite{chan2022inverse}.
Our work considers the more general inverse problem for multiple agents with arbitrary objective, i.e., payoff functions, and solves it using a mathematical program as well.


\paragraph{Inverse Algorithmic Game Theory}

A literature that lies at the intersection of economics and computer science has aimed to provide computationally-efficient methods for rationalizing equilibria, but has mainly focused on specific types of games, such as matching \cite{kalyanaraman2008complexisaac} and network formation games \cite{kalyanaraman2009complexfocs}.
In the latter case, they showed that game attributes that are local to a player can be rationalized.
More recently, \citeauthor{kuleshov2015inverse}
(\citeyear{kuleshov2015inverse}) showed that correlated equilibria can be rationalized in polynomial-time in succint games.
We note that these computational results concern stylized game models, and restrict certain aspects of the game, such as the size of the game's parameter space.
In contrast, our results abstract away the issue of efficiently representatng the game's parameter space, and show that under appropriate parameterization,
% \amy{i don't know what this means. i know what it means to restrict the size of a space, but what is an ``appropriate'' parameterization? it doesn't sound better.} 
inverse equilbirium can be computed in polynomial time.


\paragraph{Inverse Reinforcement Learning} 

Algorithms that infer the reward function of an agent operating within a Markov decision process \cite{bellman1952theory} have been studied extensively in recent years, starting with the initial investigations by \citet{ng2000algorithms}.
These algorithms can be broadly categorized as maximum margin methods \cite{ratliff2006maximum, silver2008high, abbeel2004apprenticeship, syed2007game}, i.e., methods that seek to maximize the margin between the value of observed behavior and the behavior associated with learned policy and rewards;
maximum entropy methods \cite{ziebart2008maximum, wulfmeier2015maximum, ziebart2008maximum,theodorou2010generalized, boularias2012structured, boularias2011relative}, i.e., methods that maximize the entropy of the observed and the learned behaviors;
Bayesian learning methods \cite{ramachandran2007bayesian, choi2011map,lopes2009active,levine2011nonlinear,babes2011apprenticeship}, i.e., methods that learn a posterior distribution over parameters using Bayesian updating;
and classification/regression methods \cite{klein2012inverse,taskar2005learning,klein2013cascaded,brown2019extrapolating}, i.e., methods that learn parameters that minimize the distance between the observed behavior and behavior generated by learned behavior using the inferred parameters.
Our methods, when used with only one player, can be characterized as a class of novel \samy{regret-minimizing}{} inverse reinforcement learning methods, which seek to recover parameters that minimize the players' regrets.
\amy{i really want to say minimize exploitability, but i think regret is a better understood term, colloquially, and i am not sure we have defined exploitability in the intro?}


\paragraph{Inverse Game Theory}

In multiagent settings, convex programming formulations have been proposed for inferring game parameters  \amy{what is the diff b/n these two settings? inverse game theory and the inverse eqm problem? i can't make sense of what's written.} \deni{changed word ordering, should be good now!}in normal-form games under the names of the inverse game theory problem \cite{kuleshov2015inverse} and the inverse equilibrium problem \cite{waugh2013computational, bestick2013inverse}.
These methods focus on computing an inverse correlated equilibrium, and in the case of \citeauthor{waugh2013computational}, further seek to reproduce observed equilibrium behavior via a maximum entropy correlated equilibrium.
\citeauthor{hadfield2016cooperative} \citeyear{hadfield2016cooperative} consider cooperative inverse reinforcement learning, which can be seen as an inverse Nash equilibrium problem in a particular zero-sum imperfect-information game, but this method is not accompanied by computational guarantees.

\paragraph{Multiagent Inverse Reinforcement Learning}

Multiagent inverse reinforcement learning generalizes inverse game theory from normal-form games to Markov games \cite{natarajan2010multi, lin2019multi, yu2019multi, lin2017multiagent, fu2021evaluating}.
Even more importantly, instead of observing equilibrium policies, only sample trajectories from equilibrium policies are observed.
\citet{lin2019multi} study the inverse Nash equilibrium problem in zero-sum games \cite{lin2017multiagent}, and extend their methods to solve for inverse correlated equilbrium in general-sum stochastic games, and inverse Nash equilibrium in a restricted class of %\samy{adversarial}{zero-sum}\deni{I specifically avoided zero-sum cause it is not zero-sum but some weird mix things they look at like 5 different type of games/solution concepts. Felt like the best word to summarize.}
adversarial stochastic games.
\citeauthor{yu2019multi} (\citeyear{yu2019multi}) propose gradient-based algorithms for computing inverse quantal response equilibria with function approximation.
