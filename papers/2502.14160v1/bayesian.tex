\subsubsection{Bayesian Games}
\deni{Makes distributions bold.}
A \mydef{Bayesian game} \cite{harsanyi1968bayesian} $\bgames \doteq (\numplayers, \numactions, \numtypes, \typespace, \typedistrib, \param, \actionspace, \util)$ is a simultaneous-move game which consists of $\numplayers \in \N_+$ players each of whom is characterized by a type space $\typespace[\player] \subset \R^\numtypes$. \amy{characterized by a type in a type space, n'est-ce pas?}\deni{I think these comments are not relevant now that we are reverting to the classical Bayesian game setting no?}\amy{yes, maybe not.}
The players share a \mydef{common prior distribution} \amy{the players don't need a common prior if they see each other's types. they only need this in the independent case, where they see only their own type.}
\amy{the setup does not feel very Bayesian to me. it feels stochastic. like there is a distribution over complete-info games, as determined by the prior and the types, both(?) of which the players observe.} $\typedistrib[\param] \sim \simplex(\typespace)$ over the the \mydef{joint type space} $\typespace \doteq \bigtimes_{\player \in \players} \typespace[\player]$ defined by a \mydef{parameter} $\param \in \params$ coming from a parameter space $\params \subset \R^{\numparams}$.\deni{Make sure ``numparams'' dimensionality does not conflict with other var. def'n}
Each player $\player \in \players$ takes an action $\action[\player] \in \actionspace[\player]$ from its action space $\actionspace[\player] \subset \R^{\numactions}$, and receives a payoff $\util[\player](\action; \type)$ given by the payoff function $\util[\player]: \actionspace \times \typespace \to \R$. We denote the players' \mydef{joint action space} by $\actionspace \doteq \bigtimes_{\player \in \players} \actionspace[\player] \subset \R^{\numplayers \numactions}$ and the vector-valued function of all players' utilities $\util(\action; \type) \doteq \left( \util[\player](\action; \type) \right)_{\player \in \players}$.


% and $\typespace \doteq \bigtimes_{\player \in \players} \typespace[\player] \subset \R^{\numplayers \numtypes}$, and refer to any collection of actions $\action = (\action[1], \hdots, \action[\numplayers]) \in \actionspace$ and types $\type = (\type[1], \hdots, \type[\numplayers]) \in \typespace$ as an \mydef{action profile} and \mydef{type profile} respectively.
% At each time-step $\iter = 0, 1, \hdots$, each player $\player \in \players$ takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.} simultaneously observe a \mydef{context} $\typerv[][\iter] \sim \initcontexts$, i.e., a type profile drawn from a distribution $\initcontexts \in \simplex(\typespace)$ over the set of types. Each player $\player \in \players$ , it takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.}

% Once player $\player \in \players$ has observed the realized contexts $\context[\iter]$, it takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.} and receive reward $\reward[\player](\context, \inner; \type[\player])$ given by a reward function $\reward[\player]: \contexts \times \innerset \times \typespace \to \R$. For a given context, action profile, and type profile tuple, we denote the vector of all  players utilities by $(\context, \action, \type) \mapsto \reward(\context, \action; \type)$  

A Bayesian game is \mydef{continuous} if for all $\type \in \contexts$, $\util(\action; \type)$ is continuous in $\action$ and $\actionspace$ is non-empty and compact.
A Bayesian game is \mydef{concave} if in addition to being continuous, for all types $\type \in \typespace$ and for all players $\player \in \players$, $\util[\player](\action; \type)$ is concave in $\action[\player]$ and $\actionspace[\player]$ is convex.

% A \mydef{joint strategy profile} $\strat: \typespace \to \actionspace$ is a mapping from the joint type space to the joint action space s.t.\ $\strat[\player](\type) \in \actionspace[\player]$ denotes the action played by player $\player$ under type profile $\type \in \typespace$. 
An \mydef{strategy} 
% \deni{Decide if we want discuss the whole decentralized/centralized issue} \deni{I think yes?}
$\strat[\player]: \typespace[\player] \to \actionspace[\player]$ for a player $\player \in \players$, is a mapping from player $\player$'s type to an action s.t. $\strat[\player](\type[\player]) \in \actionspace[\player]$ denotes the action played by player $\player$ when it is of type $\type[\player]$. 
A \mydef{strategy profile} $\strat \doteq \left( \strat[1], \hdots, \strat[\numplayers]\right)$ is a collection of independent strategies, one-per-player, s.t. for any type profile $\type \in \typespace$, $\strat(\type) \in \actionspace$ denotes the action profile played by the players.
% \amy{i think you need a different letter for independent strategy profiles. maybe $\tau$. overloading with $\sigma$ will probably be too confusing (and you can always change the macro later if it isn't).}

An \mydef{ex-ante $\varepsilon$-Bayesian Nash equilibrium} 
% \amy{maybe parameterized instead of Bayesian}\deni{We said no!} 
($\varepsilon$\mydef{-BNE}) is a strategy profile $\strat[][][*] \in \actionspace^\typespace$ s.t.\ for all players $\player \in \players$ and strategy profiles $\strat \in \actionspace^\typespace$, $\Ex_{\typerv \sim \typedistrib[\param]} \left[ \util[\player] (\strat[][][*](\typerv); \typerv)\right] \geq \Ex_{\typerv \sim \typedistrib[\param]} \left[ \util[\player] (\strat[\player][][](\typerv), \strat[-\player][][*] (\typerv); \typerv)\right] - \varepsilon$. 
A $\varepsilon$-ex-post Nash equilibrium ($\varepsilon$-EPNE) is a strategy profile $\strat[][][*] \in \actionspace^\typespace$ s.t. for all players $\player \in \players$, types $\type \in \typespace$, and strategy profiles $\strat \in \actionspace^\typespace$, $\util[\player](\strat[][][*](\type); \type) \geq \util[\player](\strat[\player][][](\type), \strat[-\player][][*](\type); \type) - \varepsilon$. 
A $0$-BNE and $0$-EPNE are simply called BNE and EPNE, respectively.
For concave games, a BNE is guaranteed to exist, while EPNE are not guaranteed exist.%
% \footnote{Traditionally, BNE and EPNE are only defined for independent strategies, in which case EPNE may not exist. Our more general definition applies to strategies that depend on all players' types. \amy{again, it doesn't feel very ex-post.}} 

\deni{We have to define the set of Nash equilibria and EPNE?} \deni{Update: Maybe not}

% \deni{Ignore next para, Old def'n, to remove, keeping it for visual reminder}
% An \mydef{ex-ante $\varepsilon$-Bayesian Nash equilibrium} (joint $\varepsilon$\mydef{-BNE}) is a strategy profile $\strat[][][*] \in \actionspace^\typespace$ s.t. for all players $\player \in \players$ and strategy profiles $\strat \in \actionspace^\typespace$, $\Ex_{\typerv \sim \typedistrib} \left[ \util[\player](\strat[][][*](\typerv); \typerv)\right] \geq \Ex_{\typerv \sim \typedistrib} \left[ \util[\player](\strat[\player][][](\typerv[\player]), \strat[-\player][][*](\typerv[-\player]); \typerv)\right]$. A $\varepsilon$-ex-post Nash equilibrium ($\varepsilon$-EPNE) is a strategy profile $\strat[][][*] \in \actionspace^\typespace$ s.t. for all players $\player \in \players$, types $\type \in \typespace$ and strategy profiles $\strat \in \actionspace^\typespace$, $\util[\player](\strat[][][*](\type); \type) \geq \util[\player](\strat[\player][][](\type[\player]), \strat[-\player][][*](\type[-\player]); \type)$.

% \amy{BNE seems different to me, b/c i don't think $\strat[][][*](\type) = (\strat[\player][][*](\type[\player]), \strat[-\player][][*](\type[-\player]))$. i think it might equal $(\strat[\player][][*](\type), \strat[-\player][][*](\type))$, in which case you have $\util[\player](\strat[][][*](\type); \type) = \util[\player](\strat[\player][][*](\type), \strat[-\player][][*](\type); \type)$ not $\util[\player](\strat[][][*](\type); \type) = \util[\player](\strat[\player][][*](\type[\player]), \strat[-\player][][*](\type[-\player]); \type)$. in the former case, parameters are known to all---it is a complete-info game; in the latter, players know only their own parameters, which is very different from an info-theoretic point of view (so it feels like we are comparing apples to oranges).}

% A \mydef{(Markov or stationary) policy} \cite{maskin2001markov} for player $\player \in \players$, $\policy[\player]: \contexts \to \innerset[\player]$ is a mapping from contexts to actions such that $\policy[\player](\context) \in \innerset[\player]$ denotes the action taken by player $\player$ when it observes context $\context$. We define a \mydef{policy profile} as the collection of policies, i.e., $\policy = (\policy[1], \hdots, \policy[\numplayers]) : \contexts \to \innerset$ such that $\policy(\context) \in \innerset$ denotes the action profile played by the players under context $\context \in \contexts$. The goal of all players $\player \in \players$ is to play a policy $\policy[\player][][*] \in \innerset[\player]^\context$ which maximizes their \mydef{expected cumulative payoff} $\util[\player](\policy[\player], \policy[-\player]; \type[\player]) \doteq \Ex_{\contextrv \sim \initcontexts} \left[ \reward[\player](\contextrv, \policy[\player](\contextrv), \policy[-\player](\contextrv); \type[\player])\right]$. 
% % 


% A \mydef{$\varepsilon$-subgame perfect Nash equilibrium ($\varepsilon$-SPNE)} of a contextual game is a policy profile $\policy[][][*] \in \innerset^\contexts$ such that for all player $\player \in \players$ and for all contexts $\context \in \contexts$,  $\reward[\player](\context, \policy[][][*](\context); \type[\player]) \geq \reward[\player](\policy[\player](\context), \policy[-\player][][*](\context); \type[\player]) - \varepsilon$. A $0$-SPNE is called a \mydef{subgame perfect Nash equilibrium (SPNE)}. A SPNE is guaranteed to exist in concave contextual games. Additionally, note that when the context space is a singleton, then the SPNE of contextual game simply reduce to the NE of the game played at that context. Further, notice that the SPNE of contextual games with different context distributions are all the same.

% \deni{Ex-ante definition is commented out below. Might want to make the introduction of the type inside the paranthesis not as a superscript.}

\deni{Need to make this a function of the parameter distribution and not a function of the parameter, and us the same expectation overload as the Markov game paper.}
Fixing the payoff functions of the players $\util$, for any type $\type \in \typespace$, we define the 
% \mydef{regret} $\regret[][\util]: \actionspace \times \actionspace \to \R^\numplayers$ for playing an action profile $\action$ as compared to another action profile $\otheraction$, as follows: for all followers $\player \in \players$,
% $\regret[\player][](\action, \otheraction; \outer) = \util[\player](\outer, (\otheraction[\player], \action[-\player])) - \util[\player](\outer, \action)$. The 
\mydef{cumulative regret}, $\cumulregret[][]: \actionspace \times \actionspace \times \typespace \to \R$ between two action profiles $\action \in \actionspace$ and $\otheraction \in \actionspace$ across all players in a game as $\cumulregret[][](\action, \otheraction; \type) = \sum_{\player \in \players} \util[\player](\otheraction[\player], \action[-\player]; \type) - \util[\player](\action; \type)$.
Further, the \mydef{exploitability} or (Nikaido-Isoda potential function \cite{nikaido1955note}) of an action profile $\action \in \actionspace$ is defined as 
$\exploit[][](\action; \type) = \max_{\otheraction \in \actionspace} \cumulregret[][](\action, \otheraction; \type)$ \cite{goktas2022exploit}. 
Overloading notation,
for any common prior distribution parameter $\param \in \params$, we define the \mydef{ex-ante cumulative regret} at any given type profile $\type \in \typespace$, $\cumulregret[][\param]: \actionspace^\typespace \times \actionspace^\typespace \to \R$ between two strategy profiles $\strat \in \actionspace^\typespace$ and $\otherstrat \in \actionspace^\typespace$ across all players as $\cumulregret[][\param](\strat, \otherstrat) = \sum_{\player \in \players} \left( \Ex_{\typerv \sim \typedistrib[\param]} \left[ \util[\player](\otherstrat[\player][][](\typerv[\player]), \strat[-\player][][](\typerv[-\player]); \typerv)\right] -\Ex_{\typerv \sim \typedistrib[\param]} \left[ \util[\player](\strat(\typerv); \typerv)\right] \right)$.
Further, the \mydef{ex-ante exploitability} or (Nikaido-Isoda potential function \cite{nikaido1955note}) of a strategy profile $\strat \in \actionspace^\typespace$ is defined as 
$\exploit[][\param](\strat) = \max_{\otherstrat \in \actionspace^\typespace} \cumulregret[][\param](\strat, \otherstrat)$ \cite{goktas2022exploit}. We note that for all $\strat \in \actionspace^\typespace$, $\exploit[][\param](\strat) \geq 0$, and $\strat[][][*]$ is a BNE of $\game$ iff $\exploit[][\param](\strat[][][*]) = 0$. 
% For
% for any type profile $\type \in \typespace$,
% we define the \mydef{ex-post cumulative regret} and the \mydef{ex-post exploitability} respectively as 
% $\cumulregret[][\type](\strat, \otherstrat) \doteq \sum_{\player \in \players} \left[ \util[\player](\otherstrat[\player][][](\type[\player]), \strat[-\player][][](\type[-\player]); \type) - \util[\player](\strat(\type); \type) \right]$
% $\cumulregret[][] (\action, \otheraction; \type) = \sum_{\player \in \players} \util[\player] (\otheraction[\player], \action[-\player]; \type) - \util[\player] (\action; \type)$
% and $\exploit[][](\strat; \type) \doteq \max_{\otherstrat \in \actionspace} \cumulregret[][] (\strat, \otherstrat; \type)$.
\deni{I think these two comments are now answered! Seems to be a typo from before! Alec, I also added two macros into ``auxiliary/filecommands.sty'' for you to add comments make edits if you prefer that!}\amy{not sure we are maxing over the right thing here. still wondering about strategies. actually, maybe we are, but $\otherstrat$ should be $\otheraction$. and i feel like we need an expectation of types $T$.}
\alec{$\rho$ seems undefined here. Also unclear what is the difference between $\mathcal{A}$ and $\mathcal{A}^{\mathcal{T}}$ } \deni{$\actionspace$ is the joint action space, while $\actionspace^\typespace$ is the space of joint strategy profiles, i.e. mappings from typespace to action space. } \deni{Although might just need to define the strategy space because there is a problem with def'n, i.e., the current notation also includes centralized strategies.}

% \subsubsection{Contextual Games}
% A \mydef{(simultaneous-move) contextual game} \cite{sessa2020contextual} $(\numplayers, \numactions, \contexts, \initcontexts, \innerset, \typespace, \type, \reward)$ is a repeated game played over an infinite horizon which comprises of $\numplayers \in \N_+$ players each of whom is characterized by a type space $\typespace[\player] \in \subset \R^\numtypes$. At each time-step $\iter = 0, 1, \hdots$, each player $\player \in \players$ takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.} simultaneously observe a \mydef{context} $\typerv[][\iter] \sim \initcontexts$, i.e., a type profile drawn from a distribution $\initcontexts \in \simplex(\typespace)$ over the set of types. Each player $\player \in \players$ , it takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.}

% % Once player $\player \in \players$ has observed the realized contexts $\context[\iter]$, it takes an action $\inner[\player][][][\iter] \in \innerset[\player](\context[\iter])$ from an action space $\innerset[\player](\context[\iter]) \subset \actionspace[\player]$\footnote{Going forward, for simplicity, without loss of generality, we drop the dependence of the action space $\innerset[\player]$ on the context $\context$ and take for all player $\player \in \players$, $\innerset[\player] \doteq \actionspace[\player]$.} and receive reward $\reward[\player](\context, \inner; \type[\player])$ given by a reward function $\reward[\player]: \contexts \times \innerset \times \typespace \to \R$. For a given context, action profile, and type profile tuple, we denote the vector of all  players utilities by $(\context, \action, \type) \mapsto \reward(\context, \action; \type)$  

% A contextual game is \mydef{continuous} if for all $\context \in \contexts$, $\reward(\context, \inner; \type)$ is continuous in $\inner$ and $\innerset$ is non-empty and compact.
% A contextual game is \mydef{concave} if in addition to being continuous, for all contexts $\context \in \contexts$ and for all players $\player \in \players$, $\reward[\player](\context, \inner)$ is concave in $\inner[\player]$ and $\innerset[\player]$ is convex.

% A \mydef{(Markov or stationary) policy} \cite{maskin2001markov} for player $\player \in \players$, $\policy[\player]: \contexts \to \innerset[\player]$ is a mapping from contexts to actions such that $\policy[\player](\context) \in \innerset[\player]$ denotes the action taken by player $\player$ when it observes context $\context$. We define a \mydef{policy profile} as the collection of policies, i.e., $\policy = (\policy[1], \hdots, \policy[\numplayers]) : \contexts \to \innerset$ such that $\policy(\context) \in \innerset$ denotes the action profile played by the players under context $\context \in \contexts$. The goal of all players $\player \in \players$ is to play a policy $\policy[\player][][*] \in \innerset[\player]^\context$ which maximizes their \mydef{expected cumulative payoff} $\util[\player](\policy[\player], \policy[-\player]; \type[\player]) \doteq \Ex_{\contextrv \sim \initcontexts} \left[ \reward[\player](\contextrv, \policy[\player](\contextrv), \policy[-\player](\contextrv); \type[\player])\right]$. 
% % 


% A \mydef{$\varepsilon$-subgame perfect Nash equilibrium ($\varepsilon$-SPNE)} of a contextual game is a policy profile $\policy[][][*] \in \innerset^\contexts$ such that for all player $\player \in \players$ and for all contexts $\context \in \contexts$,  $\reward[\player](\context, \policy[][][*](\context); \type[\player]) \geq \reward[\player](\policy[\player](\context), \policy[-\player][][*](\context); \type[\player]) - \varepsilon$. A $0$-SPNE is called a \mydef{subgame perfect Nash equilibrium (SPNE)}. A SPNE is guaranteed to exist in concave contextual games. Additionally, note that when the context space is a singleton, then the SPNE of contextual game simply reduce to the NE of the game played at that context. Further, notice that the SPNE of contextual games with different context distributions are all the same.

% \sdeni{}{Overloading notation, we define the \mydef{contextual cumulative regret} at any given type profile $\type \in \typespace$, $\cumulregret[][\type]: \contexts \times \actionspace \times \actionspace \to \R$ between two action profiles $\action \in \actionspace$ and $\otheraction \in \actionspace$ across all players in context $\context$ of a contextual game as $\cumulregret[][\type](\context, \action, \otheraction) = \sum_{\player \in \players} \reward[\player](\context, (\otheraction[\player], \action[-\player]); \type[\player]) - \reward[\player](\context, \action; \type[\player])$.
% Further, the \mydef{contextual exploitability} or (Nikaido-Isoda potential function \cite{nikaido1955note}) of an action profile $\action \in \actionspace$ is defined as 
% $\exploit[][\type](\context, \action) = \max_{\otheraction \in \actionspace} \cumulregret[][\type](\context, \action, \otheraction)$ \cite{goktas2022exploit}. We note that for all $\context \in \contexts$, $\action \in \actionspace$, $\exploit[][\type](\context, \action) \geq 0$, and $\action[][][][*]$ is a SPNE of $(\numplayers, \numactions, \actionspace, \typespace, \type, \util)$ iff $\exploit[][\type](\action[][][][*]) = 0$.} 
% $\util[\player](\policy[][][*]) \geq \util[\player](\policy[\player], \policy[-\player][][*]) - \varepsilon$

% \footnote{Note that although the set of $\varepsilon$-SPNE of any contextual game $(\numplayers, \numactions, \contexts, \initcontexts, \innerset, \reward)$ is also a Nash game $(\numplayers, \numactions, \innerset^\contexts, \util)$, this reduction is mostly vacuous, as Nash's theorem \cite{nash1950existence} or Arrow-Debreu's lemma on abstract economies \cite{arrow-debreu} does not provide existence in this constructed Nash game.}
 % is a policy profile $\policy[][][*] \in \innerset^\contexts$ s.t.\ for all players $\player \in \players$ and actions $\action[\player] \in \actionspace[\player]$, $\util[\player](\action[][][][*]) \geq \util[\player](\action[\player], \action[-\player][][][*]) - \varepsilon$.
% , and for each player $\player \in \players$, an action correspondence $\innerset[\player]: \contexts \rightrightarrows \actionspace$ s.t. for any context $\context \in \contexts$ and any player $\player \in \players$, $\innerset[\player](\context) \subset \actionspace$ denotes the set of actions player $\player$ can choose under context $\context$,  who , encounter a Nash game $(\numplayers, \numactions, \innerset(\contextrv), \util())$ 

\if 0 
\deni{Decide how to incorporate this into the big story.}
\paragraph{Stackelberg-Nash Games}
An $(\numplayers + 1)$-player \mydef{Stackelberg-Nash game} $\stackgame \doteq (\numplayers, \numactions, \outerset, \innerset, \util)$ comprises one player called the \mydef{leader} and $\numplayers \in \N_{++}$ players called \mydef{followers}.
In a Stackelberg-Nash game, the leader first commits to an action $\outer \in \outerset$ from an action space $\outerset \subset \R^{\outerdim}$.
Then, having observed the leader's action, each follower $\player \in \players$, responds with an action $\inner[\player]$ in their \mydef{action space} $\innerset[\player] \subset \R^{\numactions}$.
% determined by the \mydef{feasible action correspondence} $\innerset[\player]: \outerset \rightrightarrows \actionspace[\player]$ which takes as input the leader's action $\outer$ and outputs a subset of \mydef{the action space} $\actionspace[\player] \subset \R^{\numactions}$. 
We define the \mydef{followers' joint action space} $\innerset \doteq \bigtimes_{\player \in \players} \innerset[\player]$. 
% and the \mydef{follower joint feasible action correspondence} by $\innerset(\outer) = \bigtimes_{\player \in \players} \innerset[\player](\outer) \subset \bigtimes_{\player \in \players} \actionspace[] \subset \R^{\numplayers \numactions}$
We refer to a collection of actions $\inner = (\inner[1], \hdots, \inner[\numplayers]) \in \innerset$ as a \mydef{follower action profile}, and to a collection $(\outer, \inner) \in \outerset \times \innerset$ comprising an action for the leader and a follower action profile as simply an \mydef{action profile}. 
% \deni{Maybe also define feasible action profile.} A Stackelberg-Nash game is said to have \mydef{independent action sets} if the feasible action correspondence of each player $\player \in \players$, $\innerset[\player]$ is a constant correspondence, i.e., $\innerset[\player](\outer) = \innerset[\player](\outer[][][\prime]) = \actionspace$ for all leader actions $\outer, \outer[][][\prime] \in \outerset$.

After all players choose an action, the leader receives payoff $\util[0](\outer, \inner) \in \R$, while each follower $\player \in \players$ receives payoff $\util[\player](\outer, \inner) \in \R$. 
Each player $\player \in \allplayers$ aims to maximize her payoff $\util[\player]: \outerset \times \innerset \to \R$. 
For all followers $\player \in \players$, we define the $\delta$-\mydef{best-response correspondence} $\br[\player][\delta] (\outer, \inner[-\player]) \doteq \left\{ \inner[\player] \in \innerset \mid \util[\player](\outer, \inner[][][]) \geq \max_{\inner[\player] \in \innerset[\player]} \util[\player] (\outer, (\inner[\player], \inner[-\player][][])) - \delta \right\}$ and the \mydef{joint $\delta$-best-response correspondence} $\br[][\delta](\outer, \inner) \doteq \bigtimes_{\player \in \players} \br[\player][\delta] (\outer, \inner[-\player])$. 

%\amy{we use the semi-colon notation when talking about exploitability. should we use it here as well? i tend to think yes. e.g., $\br[\player][\delta] (\inner[-\player]; \outer)$}\deni{I feel like we don't have to because this is the follower's best response in the Stackelberg game. While for exploitability that is the exploitability of the lower level game which the leader's action parametrizes so it makes sense.}

Accordingly, we define the \mydef{follower regret} $\regret[][]: \innerset \times \innerset \times \outerset \to \R^\numplayers$ for playing an action profile $\inner$ as compared to another action profile $\otherinner$ when the leader plays $\outer \in \outerset$, as follows: for all followers $\player \in \players$,
$\regret[\player][](\inner, \otherinner; \outer) = \util[\player](\outer, (\otherinner[\player], \inner[-\player])) - \util[\player](\outer, \inner)$. 
The \mydef{follower cumulative regret}, $\cumulregret: \innerset \times \innerset \times \outerset \to \R$ between two action profiles $\inner \in \innerset$ and $\otherinner \in \innerset$ across all players in a game is given by $\cumulregret(\inner, \otherinner; \outer) = \sum_{\player \in \players} \util[\player](\outer, (\otherinner[\player], \inner[-\player])) - \util[\player](\outer, \inner)$.
Further, the \mydef{follower exploitability} or (Nikaido-Isoda potential function \cite{nikaido1955note}) of a follower action profile $\inner \in \innerset$ is defined as 
$\exploit(\inner) = \max_{\otherinner \in \innerset} \cumulregret(\inner, \otherinner)$ \cite{goktas2022exploit}. 

% \if 0
% The canonical solution concept for Stackelberg-Nash games is the $(\varepsilon, \delta)$-\mydef{Stackelberg-Nash equilibrium (SNE)}, an action profile $(\outer[][][*], \inner[][][*]) \in \innerset \times \outerset$ such that:
% \begin{align}
%     \util[0](\outer[][][*], \inner[][][*]) &\geq \max_{\outer \in \outerset: \inner \in \br[][\delta](\outer, \inner)} \util[0](\outer, \inner) - \varepsilon \\
%     \util[\player](\outer[][][*], \inner[][][*]) &\geq \max_{\inner[\player] \in \innerset[\player]} \util[\player](\outer[][][*], (\inner[\player], \inner[-\player][][*])) - \delta&& \forall \player \in \players
% \end{align}
% A $(0,0)$-Stackelberg-Nash equilibrium is simply called a Stackelberg-Nash equilibrium.
% Intuitively, a $(\varepsilon, \delta)$-SNE is an action profile at which the followers play a Nash equilibrium, while the leader $\varepsilon$-approximately maximizes its payoff over its action space, assuming that the followers will play a $\varepsilon$-Nash equilibrium for any of its actions. 
% \fi


% \deni{Can remove the eqm defs, since they're in the intro.}
% \amy{not sure, b/c we don't define $\epsilon-\delta$-SE.}


As the joint best-response correspondence is not necessarily singleton-valued, the leader's objective is likewise a correspondence: i.e., multiple values could be associated with a fixed strategy $\outer \in \outerset$.
As a result, we cannot re-formulate this problem as a single objective optimization without fixing a selection criteria over the followers' joint best-responses.
The results we prove in this paper rely on the strong Stackelberg-Nash Equilibrium as a solution concept:

\begin{definition}[Strong Stackelberg-Nash Equilibrium]
A $(\varepsilon, \delta)$-\mydef{strong Stackelberg-Nash equilibrium (SSNE)} is an action profile $(\outer, \inner) \in \outerset \times \innerset$ s.t.
% 
%\begin{align}
     $\util[0](\outer[][][*], \inner[][][*]) \geq \max_{\outer \in \outerset} \max_{\inner \in \br[][\delta](\outer, \inner)} \util[0](\outer, \inner) - \varepsilon$ and
     $\util[\player](\outer[][][*], \inner[][][*]) \geq \max_{\inner[\player] \in \innerset[\player]} \util[\player](\outer[][][*], (\inner[\player], \inner[-\player][][*])) - \delta$, for all $\player \in \players$.
%\end{align}

% \begin{align}
%     \outer[][][*] &\in \argmax_{\outer \in \outerset} \max_{\inner \in \br(\outer, \inner)} \util[0](\outer, \inner)\\
%     \inner[\player][][*] &\in \argmax_{\inner[\player] \in \innerset[\player]} \util[\player](\outer[][][*], (\inner[\player], \inner[-\player][][*])) && \forall \player \in \players
% \end{align}
\end{definition}
\fi
% \begin{definition}[Weak Stackelberg Equilibrium]
% A $(\varepsilon, \delta)$-\mydef{weak Stackelberg-Nash equilibrium (WSNE)} is an action profile $(\outer, \inner) \in \outerset \times \innerset$ s.t.
% %
% %\begin{align}
%      $\util[0](\outer[][][*], \inner[][][*]) \geq \min_{\inner \in \br[][\delta](\outer, \inner)} \util[0](\outer, \inner) - \varepsilon$, for all $\outer \in \outerset$, and
%      $\util[\player](\outer[][][*], \inner[][][*]) \geq  \max_{\inner[\player] \in \innerset[\player]}\util[\player](\outer[][][*], (\inner[\player], \inner[-\player][][*])) - \delta$, for all $\player \in \players$.
% %\end{align}
% \end{definition}

%%% SPACE
% %
% \if 0
% In these definitions, the leader approximately optimizes its strategy assuming the followers approximately optimize theirs, in which case $\delta \geq 0$. \amy{i think what this means is that $\epsilon$ is a function of $\delta$.} 
% As a result, a $(0,0)$-SSNE/WSNE might not be a $(0, \delta)$-SSNE/WSNE in general.
% \deni{Can we please discuss this, I want to add a footnote but not sure how to phrase it. There is this interesting phenonmenon in the general sum-setting where computing an approximate equilibrium might be hard because the follower's approximate best-response can induce discontinuous change in the *equilibrium* strategy of the leader, and as a result a $(0,0)$-SSNE/WSNE might not be a $(0, \delta)$-SSNE/WSNE. }
% \sdeni{}{Note that an important difference above approximate Stackelberg-Nash definitions are much harder to to compute }
% \amy{i know we discussed yesterday, but need to discuss again.} \amy{i think you might want to move this discussion to right after Obs 1. it might be easier to explain there.}
% \fi

% \deni{The reason why we need the joint convexity assumption is because our goal is to compute a VE, which does not generally exist (and because projection onto non-convex sets is often hard!).}

% We also define local versions of these equilibrium concepts, which are more tractable in general. A pseudo-game is simply called a \mydef{game} if for all players $\player \in \players$, $\actions(\naction[\player])$ is a constant correspondence, i.e., for all players $\player \in \players$, and strategy profiles $\action, \otheraction \in \innerset, \actions(\naction[\player]) = \actions(\otheraction[-\player])$.


%A \mydef{local generalized Nash equilibrium (local GNE)} is an strategy profile $\inner[][][*]$ s.t.\ for all players $\player \in \players$, and $\inner[\player] \in \actions[\player](\naction[\player][][][*]) \cap \ball[\varepsilon][{\inner[\player]}]$, \amy{fix me!} $\util[\player](\inner[][][*]) \geq \util[\player](\inner[\player], \naction[\player][][][*])$.
% A \mydef{local variational equilibrium (local VE) or normalized GNE} is an strategy profile $\inner[][][*]$ s.t.\ for all strategy profiles and $\action \in \innerset \cap \ball[\varepsilon][\action]$ s.t.\ $\actionconstr(\action) \geq \zeros$, $\util[\player](\inner[][][*]) \geq \util[\player](\inner[\player], \naction[\player][][][*])$. 


% $\hypothesis \in \argmin_{\R^{\states \times \actionspace}} \nicefrac{1}{\numsamples} \sum_{\numsample \in [\numsamples]} \left\| \mathrm{Nash}\left(\hypothesis[][*](\state[\numsample]\right) - \inner[][][][\numsample]  \right\|^2$ and for all $\state \in \states$, $\hypothesis[][*](\state[\numsample])$ is a Nash equilibrium of $$.