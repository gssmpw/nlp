\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url} 
\usepackage{hyperref} 
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{booktabs}
\usepackage{siunitx} % For number alignment
\usepackage{microtype}
\usepackage{booktabs}  % for \toprule, \midrule, etc.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
\title{Leveraging LLMs for Dynamic IoT Systems Generation through Mixed-Initiative Interaction}

\author{\IEEEauthorblockN{Bassam Adnan\textsuperscript{\dag}}
\IEEEauthorblockA{\textit{IIIT Hyderabad, India}\\
bassam.adnan@research.iiit.ac.in}
\and
\IEEEauthorblockN{Sathvika Miryala\textsuperscript{\dag}}
\IEEEauthorblockA{\textit{IIIT Hyderabad, India}\\
miryala.sathvika@research.iiit.ac.in}
\and
\IEEEauthorblockN{Aneesh Sambu\textsuperscript{\dag}}
\IEEEauthorblockA{\textit{IIIT Hyderabad, India}\\
sambu.aneesh@research.iiit.ac.in}
 \and
\IEEEauthorblockN{Karthik Vaidhyanathan}
\IEEEauthorblockA{\textit{IIIT Hyderabad, India}\\
karthik.vaidhyanathan@iiit.ac.in}
\and
\IEEEauthorblockN{Martina De Sanctis}
\IEEEauthorblockA{\textit{GSSI, L’Aquila, Italy}\\
martina.desanctis@gssi.it}
\and
\IEEEauthorblockN{Romina Spalazzese}
\IEEEauthorblockA{\textit{Malmö University, Sweden}\\
romina.spalazzese@mau.se}
}
\maketitle

\renewcommand{\thefootnote}{\dag} %  dagger for this footnote
\footnotemark
\footnotetext{These authors contributed equally to this work.}
\renewcommand{\thefootnote}{\arabic{footnote}} % Restore footnote numbering
\setcounter{footnote}{0}

\begin{abstract}
IoT systems face significant challenges in adapting to user needs, which are often under-specified and evolve with changing environmental contexts. To address these complexities, users should be able to explore possibilities, while IoT systems must learn and support users in the process of providing proper services, e.g., to serve novel experiences. The IoT-Together paradigm aims to meet this demand through the Mixed-Initiative Interaction (MII) paradigm that facilitates a collaborative synergy between users and IoT systems, enabling the co-creation of intelligent and adaptive solutions that are precisely aligned with user-defined goals. This work advances IoT-Together by integrating Large Language Models (LLMs) into its architecture. Our approach enables intelligent goal interpretation through a multi-pass dialogue framework and dynamic service generation at runtime according to user needs. To demonstrate the efficacy of our methodology, we design and implement the system in the context of a smart city tourism case study. We evaluate the system's performance using agent-based simulation and user studies. Results indicate efficient and accurate service identification and high adaptation quality. The empirical evidence indicates that the integration of Large Language Models (LLMs) into IoT architectures can significantly enhance the architectural adaptability of the system while ensuring real-world usability.

\end{abstract}

\begin{IEEEkeywords}
LLM, Self-Adaptation, Software Architecture, Service Generation, Dynamic Application Generation, IoT-Together Paradigm
\end{IEEEkeywords}

\section{Introduction}
\input{sections/introduction}


\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.80\textwidth]{figures/three_pass_diagram.png} 
    \label{fig:initial-interaction-diagram}
    \caption{Three-Pass Dialogue Flow: Progressive Identification of User Goals and Service Parameters enabling Goal-Driven Architecture}

\end{figure*}

\section{Motivating  Case Study}
\input{sections/case_study}

\section{Approach}
\input{sections/approachinitial}

\subsection{\textbf{Goal Management}}
\input{sections/usergoalparser}
\subsection{\textbf{Backend Generation}}
\input{sections/servicegeneration}
\input{sections/iui}

\subsection{\textbf{Context Management}}
\input{sections/context}

\subsection{\textbf{Knowledge Management}}
\input{sections/knowledge}


\section{Experiments and Results}

The evaluation focuses on assessing the proposed approach through the following research questions.

\begin{itemize}
    \item \textbf{RQ1: Effectiveness in Identifying Functionalities:}  
    How effective is the approach in identifying the correct set of functionalities corresponding to existing components ?
    
    \item \textbf{RQ2: Accuracy in Dynamic Service Generation:}  
    How accurate is the approach in dynamically generating services?  

    \item \textbf{RQ3: Effectiveness in System Generation:}  
    What is the effectiveness of the approach in generating the system as a whole?

    \item \textbf{RQ4: Efficiency in Application Generation:}  
    What is the efficiency of the approach in generating applications?
\end{itemize}
\subsection{Evaluation Setup} We evaluated our system using Hyderabad as a case study of a smart city implementation. The system comprises $9$ web-services: Air Quality, Crowd Monitoring, Event Notifier, Historic Information, Restaurant Locator, Travel Options, Water Quality, Exhibition Tracker, and Event Ticket Vendor. Several services operate on static contextual data feeds (e.g., Historic Information), while others process real-time data from a network of 12 distributed sensors. We implemented the IoT environment simulation using CupCarbon \footnotemark for realizing the architecture and generating sensor data based on domain-appropriate statistical distributions, with the core system in Python. we employed a two-fold evaluation strategy: (i) a multi-agent simulation framework modeling Tourist-Guide interactions across 100 experimental runs, and (ii) a user study ($n=15$) focusing on real-world usability and service adaptation quality.
\footnotetext{CupCarbon: \url{https://cupcarbon.com/}}
 \subsubsection{Tourist-Guide Simulations}
The system evaluation employed OpenAI's GPT-4o-mini \cite{openai2024gpt4technicalreport}, DeepSeek-V2.5 \cite{deepseekai2024deepseekv2strongeconomicalefficient} and CodeQwen1.5-7B \cite{bai2023qwentechnicalreport} models through the LangChain framework. We chose these based on the EvalPlus \cite{liu2023is} leader board. Experiments were conducted on an Nvidia L40S GPU with 8 vCPU, 62 GB RAM, and 48GB VRAM for hosting the CodeQwen1.5-7B model on HuggingFace. For GPT-4o-mini and DeepSeek-V2.5 model interactions, we utilized LangChain's OpenAI API. All these models were run with a temperature parameter of 0.7 based on preliminary experimentation.  
To evaluate the Goal Management, we designed a multi-agent simulation framework using CrewAI\footnotemark, modeling interactions between a Tourist agent and a Travel Guide agent (implementing our Goal Management's instruction set). The Tourist agent samples from $25$ predefined goals, generated through prompt engineering with domain-specific system knowledge, with time constraints uniformly distributed between $1-5$ hours. Each goal has an associated ground truth set of required services for validation.
\footnotetext{CrewAI: \url{https://www.crewai.com/}}
The goals were classified into concrete and ambiguous categories ($18:7$). Concrete goals have predictable mappings, such as ``Planning to visit Ramoji Film City'' mapping to \textit{ticket\_purchase} and \textit{travel\_options}. Ambiguous goals like ``First time in Hyderabad! Want to start with the locals' favorites'' may trigger multiple services (e.g., \textit{restaurant\_finder}, \textit{crowd\_monitor}, \textit{travel\_options}) based on conversation flow.
The simulation involved three sequential passes of Tourist-Guide interactions for service identification, repeated 100 times. While additional services could enhance user experience, we limit suggestions to avoid overwhelming users with options beyond their original goal.

\subsubsection{User Evaluation}
For complementing our simulation-based evaluation, we conducted a user study with students from the International Institute of Information Technology, Hyderabad (IIIT-H), which focused on understanding system effectiveness and overall user satisfaction through both quantitative metrics and qualitative feedback. The study involved participants ($n=15$) from diverse academic backgrounds within IIIT-H, specifically comprising 3 Ph.D. students (2 Computer Science and 1 Electronics/Communications Engineering), 5 Electronics/Communications Engineering students (B.Tech by M.S.), and 7 Computer Science students (B.Tech by M.S.). The participants were given a brief overview of the system's capabilities and were encouraged to interact with it based on their interests and needs. Each participant interacted with the system for approximately 10-15 minutes, with feedback collected through an integrated form in the user interface. The feedback mechanism collected three types of Quantitative Metrics: application rating, service accuracy rating, and service relevance rating (on a 1-5 likert scale), and Qualitative Feedback comprising query summaries, missing service identification, unnecessary service identification, and improvement suggestions. To further evaluate the effectiveness of dynamically generated services, we implemented a service rotation mechanism where three services were deliberately kept offline and replaced with generated implementations during each participant interaction, without informing participants, to assess integration seamlessness.







    
    



For assessing application generation efficiency, we integrated a metrics collection system with the Intelligent User Interface generator. The evaluation examined three critical metrics: total generation time (comprising dialogue latency, service discovery, template rendering, and deployment), token usage (aggregating input tokens from user queries, processing tokens from system context, and completion tokens from LLM responses), and build times per session. Given that service generation is not activated in every test scenario, we conducted a separate performance analysis of this component to ensure unbiased assessment.














\subsection{Results \& Discussions}
\noindent \textbf{RQ1: Effectiveness in Identifying Functionalities}


To evaluate the Goal Management's effectiveness, we analyze service identification accuracy using four key metrics defined in Table-\ref{tab:metrics-definition} in our simulation. The evaluation compares services identified after the third conversation pass against ground truth mappings derived from our tourism domain requirements.

\begin{table}[ht]
\setlength{\tabcolsep}{2pt}
\caption{Evaluation Metrics for Tourist-Guide simulation}
\label{tab:metrics-definition}
\centering
\begin{tabular}{l|p{7.5cm}}
\toprule
\textbf{Metric} & \textbf{Definition} \\
\midrule
Precision (P) & Ratio of correctly identified services to all identified services \\
\hline
Recall (R) & Ratio of correctly identified services to actual required services \\
\hline
F1 Score & Harmonic mean of precision and recall (2PR/(P+R)) \\
\hline
Parameter & Accuracy of identified service parameters (e.g., exact \\
Accuracy & locations, cuisines) against ground truth \\
\bottomrule
\end{tabular}
\end{table}

Analysis of the simulation results presented in Table-\ref{tab:goal-parser-categories} demonstrates comparable performance metrics between GPT-4o-mini and DeepSeek-V2.5 in service identification tasks. We noticed that both GPT-4o-mini and DeepSeek-V2.5 consistently respected time constraints while providing travel plans to the Tourist unlike CodeQwen1.5-7B which suggested plans spanning multiple days, exceeding the specified time constraints. CodeQwen1.5-7B exhibits lower precision values, displaying a tendency toward over-identification of required services. This over-identification introduces unnecessary complexity into the system architecture and imposes increased computational overhead during the build process.
\begin{table}[!htbp]
\setlength{\tabcolsep}{4pt}
\caption{Goal Parser Performance by Category}
\label{tab:goal-parser-categories}
\centering
\begin{tabular}{llcccc}
\toprule
\textbf{Model} & \textbf{Category} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Parameter} \\
& & & & & \textbf{Accuracy} \\
\midrule
CodeQwen1.5-7B & Ambiguous & 0.450 & 0.806 & 0.553 & 0.116 \\
& Concrete & 0.206 & 0.609 & 0.288 & 0.051 \\
& \textbf{Overall} & 0.282 & 0.670 & 0.370 & 0.071 \\
\midrule
GPT-4o-mini & Ambiguous & 0.683 & 0.795 & 0.730 & 0.549 \\
& Concrete & 0.467 & 0.773 & 0.559 & 0.739 \\
& \textbf{Overall} & 0.523 & 0.778 & 0.603 & 0.690 \\
\midrule
DeepSeek-V2.5 & Ambiguous & 0.681 & 0.788 & 0.725 & 0.585 \\
& Concrete & 0.492 & 0.830 & 0.591 & 0.743 \\
& \textbf{Overall} & 0.554 & 0.816 & 0.635 & 0.691 \\
\bottomrule
\end{tabular}
\end{table}

For the user evaluation (see Table-\ref{tab:user-satisfaction}) study, tourism-focused ($40\%$) and dining-related ($53\%$) queries dominated user sessions, with $67\%$ involving multi-service combinations. Restaurant Finder ($53\%$), Travel Options ($47\%$), and Historical Information ($33\%$) were the most frequently requested services. User feedback identified crowd monitoring (7 instances), air quality (3), and water quality (2) as desired additional services.
\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{User Satisfaction Metrics}
\label{tab:user-satisfaction}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Average Rating (out of 5)} & & \\
\midrule
Application Rating & 4.0 & & \\
Accuracy Rating & 4.1 & & \\
Relevance Rating & 4.2 & & \\
\bottomrule
\end{tabular}
\end{table}
User studies highlighted the need for improved local data processing, particularly for proximity-based routing and recommendations. These insights suggest optimization areas aligning with our mixed-initiative vision, especially in collaborative monitoring and user-adaptive location services.

\smallskip
\noindent \textbf{RQ2: Accuracy in Dynamic Service Generation}

To evaluate the quality of dynamically generated services, we conducted multiple generation attempts (three per service) across our 9 services. We used CodeBERTScore \cite{zhou2023codebertscoreevaluatingcodegeneration} to assess the semantic similarity between generated and reference implementations, measuring four key aspects: precision (code correctness), recall (code completeness), F1-score (balanced measure), and F3-score (emphasizing on code completeness).

\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{Service Generation Code Similarity}
\label{tab:code-similarity}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{F3} \\
\midrule
CodeQwen1.5-7B & 0.86 ± 0.02 & 0.79 ± 0.03 & 0.83 ± 0.02 & 0.80 ± 0.03 \\
DeepSeek-V2.5 & 0.91 ± 0.01 & 0.85 ± 0.03 & 0.88 ± 0.02 & 0.86 ± 0.03 \\
GPT-4o-mini & 0.90 ± 0.01 & 0.85 ± 0.03 & 0.87 ± 0.01 & 0.85 ± 0.02 \\
\bottomrule
\end{tabular}
\end{table}
As shown in Table \ref{tab:code-similarity}, DeepSeek-V2.5 achieved the highest overall performance with an F1-score of 0.88, outperforming CodeQwen1.5-7B by 6\% and comparable to GPT-4o-mini. Notably, all models maintained high precision (\(\ge\) 0.86), indicating reliable code generation quality. The relatively lower recall scores, particularly for CodeQwen1.5-7B (0.79), suggest occasional omissions in implementing complete functionality. These contrasting results from Table-\ref{tab:goal-parser-categories} suggest that while DeepSeek-V2.5 and GPT-4o-mini exhibit consistent performance across both service identification and code generation tasks, CodeQwen1.5-7B shows task-specific performance variations that could impact its suitability for general-purpose service generation in IoT environments. 

\smallskip
\noindent \textbf{RQ3: Effectiveness in System Generation}\\
We keep track of the total tokens required to generate these services across the models along with end-to-end latency
(including API request/response time) in Table-\ref{tab:service-generation}. \\
\begin{table}[!htb]
\setlength{\tabcolsep}{4pt}
\caption{Service Generation }
\label{tab:service-generation}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Time (s)} & \textbf{Tokens} \\
\midrule
CodeQwen1.5-7B & 7.67 ± 0.26 & 3482.00 ± 19.80 \\
DeepSeek-V2.5 & 42.40 ± 4.52 & 4376.25 ± 228.17 \\
GPT-4o-mini & 25.66 ± 2.55 & 2063.17 ± 191.76 \\
\bottomrule
\end{tabular}
\end{table}
While GPT-4o-mini and DeepSeek-V2.5 achieved 100\% service generation success rate, CodeQwen1.5-7B only succeeded in 37\% of attempts. On inspection, we found that CodeQwen1.5-7B's performance limitations stemmed from (1) inconsistent instruction following and (2) JSON formatting errors.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/input_tokens.png}
    \caption{Token consumption scaling with increasing number of services for GPT-4o-mini and DeepSeek-V2.5. The x-axis represents the number of services and the y-axis shows the corresponding input token count.}
    \label{fig:token-scaling}
\end{figure}

To evaluate scalability in mixed-initiative contexts, we analyze how the input token consumption scales with an increasing number of services in the system. Figure~\ref{fig:token-scaling} illustrates this relationship across GPT and DeepSeek model, where the token usage pattern diverges significantly even for small number of services. This rise is primarily seen due to the addition of extra services leading to the Description Refiner requiring more tokens to process the system state received by the Service Manager.
While DeepSeek-V2.5 and GPT-4o-mini have comparable pricing (0.14 USD and 0.15 USD per 1M input tokens)\footnotemark, their actual costs differ due to variations in token consumption, directly impacting adaptive service generation costs. GPT-4o-mini, with its relatively compact architecture and lower token usage (Table-\ref{tab:service-generation}), demonstrates more efficient performance for dynamic interactions compared to DeepSeek-V2.5's 238 billion parameter architecture.
\footnotetext{As of 2024-12-10 on billing websites}
\smallskip

\noindent \textbf{Results for RQ4: Efficiency in Application Generation}
We evaluated the system using 15 scenarios from our human evaluation study.\subsection{Performance Analysis}
The system achieved an average total generation time of 23.10 ± 6.47 seconds. Table~\ref{tab:generation-breakdown} presents the detailed breakdown of the processing stages.
\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{Generation Process Time Breakdown}
\label{tab:generation-breakdown}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Processing Stage} & \textbf{Mean (s)} & \textbf{SD (s)} \\
\midrule
Multi-pass conversation processing & 18.25 & 5.12 \\
Service identification \& parameter extraction & 3.82 & 1.14 \\
Template rendering \& application assembly & 1.03 & 0.31 \\
Final deployment & 0.004 & 0.002 \\
\bottomrule
\end{tabular}
\end{table}

Analysis of token distribution is presented in Table~\ref{tab:token-distribution}.

\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{Token Usage Distribution}
\label{tab:token-distribution}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Token Type} & \textbf{Count (Mean ± SD)} & \textbf{\% of Total} \\
\midrule
Input tokens & 101.8 ± 70.12 & 1.25\% \\
Processing tokens & 7,308.1 ± 2,607.49 & 89.51\% \\
Completion tokens & 755.0 ± 281.28 & 9.24\% \\
\bottomrule
\end{tabular}
\end{table}

The system demonstrated decent build performance, with an average build time of 4.85 ± 1.98 milliseconds. This sub-10 ms build time was anticipated, as the builder only needs to render the application by sending it to the hosting component. Table~\ref{tab:app-generation} summarizes the overall performance metrics.

\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{Application Generation Performance Metrics}
\label{tab:app-generation}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Metric} & \textbf{Mean ± SD} & \textbf{Min} & \textbf{Max} \\
\midrule
Total Duration (s) & 23.10 ± 6.47 & 13.46 & 33.08 \\
Total Token Usage & 8164.90 ± 2718.89 & 5531 & 13991 \\
Build Time (ms) & 4.85 ± 1.98 & 3.50 & 10.49 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/token_distribution.png}
    \caption{Token distribution during application generation. The high proportion of processing tokens (89.51\%) indicates potential for optimization through context management improvements.}
    \label{fig:token-distribution}
\end{figure}

\subsection{Service Generation Analysis}

The Backend generation component was evaluated by generating each of our nine services ten times. The evaluation revealed consistent results across different service types, with an average generation time of 15.53 seconds. The metrics are summarized in  Table~\ref{tab:service-metrics}.

\begin{table}[ht]
\setlength{\tabcolsep}{4pt}
\caption{Service Generation Performance Metrics}
\label{tab:service-metrics}
\centering
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Average processing time (s) & $15.53 \pm 1.12$ \\
Total token usage & $4,992.89 \pm 180.29$ \\
Coefficient of Variation (\%) & $3.61$ \\
\bottomrule
\end{tabular}
\end{table}


The Coefficient of Variation (CV), calculated as (Standard Deviation / Mean) × 100, measures dispersion across different metrics. A CV of 3.61\% indicates high consistency in generating any service regardless of its type. When service generation is incorporated into the total application generation metrics, we observe a total duration of 38.63 seconds (23.10 ± 6.47 + 15.53 ± 1.12) and total token usage of 13,157.79 tokens (8,164.90 ± 2,718.89 + 4,992.89 ± 180.29).




\renewcommand{\thefootnote}{} % Suppress footnote numbering
\footnotetext{Code available on GitHub: \url{https://github.com/sa4s-serc/SAS_llm_query/tree/iot-prototype}}
\renewcommand{\thefootnote}{\arabic{footnote}} % Restore footnote numbering

\input{sections/discussions}

\section{Threats to Validity}
\input{sections/threatstovalidity}

\section{Related Work}
\input{sections/relatedworks}


\section{Conclusion \& Future Directions}
\input{sections/conclusion}


\bibliographystyle{ieeetr}
\bibliography{references}





\end{document}