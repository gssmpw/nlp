\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{subcaption}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{multirow}
\usepackage{xspace}
\usepackage{bm}
\usepackage{epstopdf}
\usepackage{paralist}
\usepackage[subtle,tracking=tight,
all=normal
,floats=tight
,paragraphs=tight
,wordspacing=tight
,mathspacing=tight
,mathdisplays=tight
,lists=tight
,indent=tight
,leading=tight
]{savetrees}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\renewcommand{\UrlFont}{\ttfamily\scriptsize}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}
\usepackage[workinprogress]{icml2025}  % avoids printing ICML-related footnote.

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{textgreek}
\usepackage{lgreek}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Symbol definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Misc. defines
\newcommand{\etal}{\textit{et al.}\xspace}
\newcommand{\ind}{\perp\!\!\!\!\perp} 
\newcommand\mustdo[1]{{\color{red}[TODO]: #1}}
% Datasets and Models
\newcommand{\arce}{ARC-E\xspace}
\newcommand{\arcc}{ARC-C\xspace}
\newcommand{\hell}{Hellaswag\xspace}
\newcommand{\humaneval}{Human-eval\xspace}
\newcommand{\thegreektheta}{\begin{greek}j\end{greek}}
\newcommand{\topth}{Top-\thegreektheta\xspace}
\newcommand{\topk}{Top-k\xspace}
% Algorithms
\newcommand{\calibrationSet}{\mathcal{C}}
\newcommand{\selids}{\mathcal{I}}
\newcommand{\notselids}{\Bar{\mathcal{I}}}
%% Dimensions
\newcommand{\seqlen}{n}
\newcommand{\headdim}{d}
\newcommand{\embdim}{D}
\newcommand{\numKept}{\Tilde{k}}
%% Matrices
\newcommand{\attnMatPre}{A}
\newcommand{\attnVecPre}{\bm{a}}
\newcommand{\attnVecSparsePre}{\Tilde{\attnVecPre}}
\newcommand{\attnMatPost}{S}
\newcommand{\attnVecPost}{\bm{s}}
\newcommand{\attnVecSparsePost}{\Tilde{\attnVecPost}}

\newcommand{\prodVec}{\bm{p}}
\newcommand{\prodVecCompensated}{\hat{\bm{p}}}
% \newcommand{}{}
% \newcommand{}{}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding}

\begin{document}

\twocolumn[
\icmltitle{Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Konstantin Berestizshevsky}{equal,huawei}
\icmlauthor{Renzo Andri}{equal,huawei}
\icmlauthor{Lukas Cavigelli}{huawei}
\end{icmlauthorlist}

\icmlaffiliation{huawei}{Computing Systems Laboratory, Zurich Research Center, Huawei Technologies, Switzerland}

\icmlcorrespondingauthor{Renzo Andri}{renzo.andri@huawei.com}
\icmlcorrespondingauthor{Konstantin Berestizshevsky}{konstantin.berestizshevsky@huawei.com}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
%
%%
%%%
\begin{abstract}

The attention mechanism is essential for the impressive capabilities of transformer-based Large Language Models (LLMs). However, calculating attention is computationally intensive due to its quadratic dependency on the sequence length. We introduce a novel approach called \textit{Top-Theta Attention}, or simply \textit{\topth}, which selectively prunes less essential attention elements by comparing them against carefully calibrated thresholds. This method greatly improves the efficiency of self-attention matrix multiplication while preserving model accuracy, reducing the number of required V cache rows by 3x during generative decoding and the number of attention elements by 10x during the prefill phase. Our method does not require model retraining; instead, it requires only a brief calibration phase to be resilient to distribution shifts, thus not requiring the thresholds for different datasets to be recalibrated. Unlike top-k attention, \topth eliminates full-vector dependency, making it suitable for tiling and scale-out and avoiding costly top-k search. A key innovation of our approach is the development of efficient numerical compensation techniques, which help preserve model accuracy even under aggressive pruning of attention scores.
\end{abstract}
%%%
%%
%
%
%%
%%%
\section{Introduction}\label{sec:introduction}
%%%
%%
%
The transformer architecture has revolutionized both natural language processing~\cite{vaswani2017attention} and computer vision~\cite{dosovitskiy2020image} by enabling models to capture complex dependencies through self-attention mechanisms effectively. However, despite its advantages, the attention mechanism suffers from quadratic time and linear memory complexity~\cite{keles2023computational}. Furthermore, the commonly used key-value (KV) cache optimization increases the memory requirement linearly with sequence length during the generative decoding phase. As a result, cache size requirements often exceed the physical limits of available memory~\cite{ge2024modeltellsdiscardadaptive}, and memory bandwidth becomes a major bottleneck.
%

%
Attention approximations~\cite{wang2024limitssurveytechniquesextend,fuad2023survey}, such as sparsification, promise a solution to these challenges by reducing the number of computations and the amount of data moved, focusing only on the most relevant tokens in a sequence. 
%Hence, sparsification can lead to substantial speedups during both training and inference phases, allowing for faster processing of longer sequences with negligible quality sacrifice.
Research on sparsification has been predominantly focused on either fixed-sparsity patterns, which assume that specific token locations in the sequence are always more important (e.g., the first and last tokens in the sequence), or the content-based sparsity patterns, which require evaluating the attention scores to decide which tokens are more important (e.g., \topk attention~\cite{gupta2021memory}).
%

%
Our work focuses on the more challenging, content-based sparsity patterns. To exploit the sparsity potential, we investigate the sparsification of the attention elements through pruning by \textit{comparing to a threshold value}. We calibrate the thresholds to select a desired average number of $k$ important tokens in every attention row. We find that calibrating model-specific thresholds is sufficient to replace the top-k search over the attention elements. Once the important tokens have been quickly determined by thresholding, the remaining tokens can be excluded from participating in the softmax computation and in the multiplication by the V-matrix, thus avoiding the need to load them. Moreover, to preserve the high accuracy in the downstream task, we propose numerical compensation methods such as softmax denominator compensation and mean V-row compensation. 
%

%
\paragraph{Contributions} The goal of this work is to unlock the potential of the content-based sparsity for more compute- and memory-efficient attention while preserving a high-quality output. Our fundamental finding is:
%

%
\textit{Static thresholds can be calibrated for a given attention head and used to sparsify its attention matrices to approximately $k$ elements per row.} 
%

%
Based on our fundamental finding, we derive the \topth attention method, unlocking the following advantages:
\begin{enumerate}
    \item \textit{Efficiency} -- 3\texttimes\ fewer V-rows needed and 10\texttimes\  less attention elements needed for LLaMA2 and LLaMA3 models to achieve same accuracy.
    \item \textit{Tiling compatible}. Thresholding is a simple elementwise operation with no row dependency, making it applicable to tiled attention matrices required for high-performance kernels and distributed inference. 
    \item \textit{No retraining} is needed to calibrate the thresholds, which takes a few hundred calibration samples.
    \item \textit{Distribution shift resilience} -- we observe that the same thresholds of a given model apply across different input domains. In other words, thresholds are a strong characteristic of the model; hence, the calibration is required only once per model.
\end{enumerate}
%

%
\paragraph{Paper organization} \cref{sec:background} recaps the self-attention mechanism, emphasizing efficient inference based on sparsity and the \topk method. ~\cref{sec:method} describes our \topth method and shows how it overcomes the limitations of \topk and our numerical compensations that further improve the accuracy of sparsification.~\cref{sec:evaluations} first evaluates our method, including all compensations, and further provides ablations of its different aspects.~\cref{sec:related_work,sec:conclusion} review relevant studies and summarize our findings.
%
%%
%%%
\section{Background}\label{sec:background}
%%%
%%
%
%
%%
%%%
\subsection{Transformer Models}\label{sec:background:transformer}
%%%
%%
%
% Transformer-based models are predominantly composed of a sequence of transformer layers; each is input by a sequence of tokens and outputs a processed sequence of tokens. 
%A single transformer layer can be of various types, such as \textit{decoder-only}, \textit{encoder-only}, or \textit{encoder-decoder}.
% Nowadays, the majority of the LLMs used for text generation are autoregressive decoder-only models as they have demonstrated superior zero-shot generalization capabilities~\cite{wang2022languagemodelarchitecturepretraining} and have been successfully adopted in various chatbots and productivity services. In the autoregressive models, the initial input sequence is first processed as a whole in one forward computation through the entire model (i.e.,  \textit{prefill phase}), and then the additional tokens are autoregressively generated one by one, each requiring its forward computation through the entire model (i.e.,  \textit{generative decoding} phase). This work focuses on the \textit{decoder-only} transformer and on improving its self-attention mechanism due to its wide adoption and success among many state-of-the-art LLMs.
Transformer-based models consist of layers that process token sequences. Most current large language models (LLMs) for text generation are autoregressive decoder-only transformer layers, known for their strong zero-shot generalization~\cite{wang2022languagemodelarchitecturepretraining} and use in chatbots and productivity tools. These models first process the entire input sequence in a single forward pass (\textit{prefill phase}) and then generate additional tokens one at a time, each requiring a separate forward computation (\textit{generative decoding phase}). This research aims to enhance the decoder-only transformers.
%

%
% Inside the decoder-only transformer layer are two sequential computational steps: self-attention and feed-forward neural networks (FFN). In this work, we focus only on the attention. 
%
%%
%%%
\subsection{Self-Attention and Sparsity}\label{sec:background:attention}
%%%
%%
%
Multi-head self-attention (MHA) is the first computational step of the transformer layer. The MHA receives as an input a sequence of tokens $X\in\mathbb{R}^{\seqlen\times\embdim}$ where $\seqlen$ is the sequence length and $\embdim$ is the hidden dimension. Each of the heads processes $X$ in parallel by first multiplying it by 3 different trained matrices $W_Q,W_K,W_V\in\mathbb{R}^{\embdim\times\headdim}$ obtaining 3 matrices $Q,K,V\in\mathbb{R}^{\seqlen\times\headdim}$ and adding a positional encoding (e.g., RoPE~\cite{su2024roformer}) to them.
\begin{equation}\label{eqn:qkv}
    Q=XW_Q, K=XW_K, V=XW_V
\end{equation}
 Then, a \textit{pre-softmax attention matrix} $\attnMatPre$ is computed from matrices $Q$ and $K$ (\ref{eqn:attn_pre_softmax}). Although $\attnMatPre$ matrix is often normalized by $\sqrt{\headdim}$ for numerical stability and masked by a causality mask, we omit these 2 steps for simplicity.
 %which forces a negative infinity value on the upper triangular part of it, effectively disabling the non-causal relationship between tokens.
\begin{equation}\label{eqn:attn_pre_softmax}
    \attnMatPre = QK^T
\end{equation}
%
 After that, each row of the $\attnMatPre$ matrix is normalized by the Softmax operation yielding the \textit{post-softmax attention matrix} $\attnMatPost$ (\ref{eqn:attn_post_softmax}) followed by a multiplication by $V$ (\ref{eqn:sv}).
%
\begin{align}\label{eqn:attn_post_softmax}
    \attnMatPost&=\mathrm{row\_softmax}(\attnMatPre)\\
\label{eqn:sv}
    P&=SV
\end{align}
%

%
\paragraph{Prefill phase} Both pre-, and post-softmax matrices are of the shape $\seqlen\times\seqlen$, in which the element $(i,j)$ element signifies the importance of token $j$ for token $i$. The pre-softmax attention $\attnMatPre$ has a range of all real numbers distributed normally, whereas the post-softmax $\attnMatPost$ has the range of $[0,1]$ and is distributed log-normally. 
%In the ubiquitous decoder-only transformer models, 
The attention elements in the initial layers exhibit a less skewed distribution (i.e., high entropy), whereas the following layers have a more concentrated distribution (i.e., low entropy) with a few rare high-attention values~\cite{vig2019analyzingstructureattentiontransformer,nahshan2024linearlognormalattentionunbiased}. 
%Our exploration confirmed these prior observations, as we found that $\attnMatPre$ is distributed generally around negative values. In contrast, $\attnMatPost$ has a log-normal distribution. 
In both $\attnMatPre$ and $\attnMatPost$, a lower attention value has a lower contribution to the further computation because $\attnMatPost$ is obtained through an order-preserving transformation of $\attnMatPre$, after which $\attnMatPost$ is multiplied by the matrix $V\in\mathbb{R}^{\seqlen\times\headdim}$ resulting in the output matrix $P\in\mathbb{R}^{\seqlen\times\headdim}$ (\ref{eqn:sv}). Therefore, small values in column $i$ in $\attnMatPost$ diminish the impact of the row $i$ of the $V$ matrix. \textit{This fundamental property of the attention elements allows ranking them according to their significance and pruning the least significant ones for sparsification}. 
%An in-depth analysis of the attention sparsity is given in~\cref{sec:introduction:sparsity}.
%

%
\paragraph{Generative decoding phase and KV-cache} MHA employs a well-established performance optimization called KV-cache~\cite{shi2024costdownreviewmethods} which allows processing a single embedded token $\bm{x}\in\mathbb{R}^\embdim$, computing only the current token's $\bm{q},\bm{k},\bm{v}\in\mathbb{R}^\headdim$ vectors, while the complete $K, V$ matrices are loaded from the cache (avoiding recomputation) and the new $\bm{k},\bm{v}$ vectors are appended to them. In this situation, the attention matrices simplify to vectors $\attnVecPre, \attnVecPost\in\mathbb{R}^\seqlen$ representing the attention of the currently generated token to all the previous tokens, and the $P$ matrix becomes a single token embedding $\bm{p}=\bm{s}V\in\mathbb{R}^\headdim$. Due to the large size of the KV-caches, especially as the sequence length grows longer, the computation of the self-attention during the generative decoding is heavily limited by memory bandwidth, dominated by loading all the $\seqlen$ rows of the K and V matrices, while only performing 1 multiply-add per value read from memory. 
% \textit{It is beneficial to reduce the required entries of the V matrix by up to 2\texttimes\ reducing the memory bandwidth and hence gaining a speedup.}
\textit{Due to the memory bottleneck, a reduction of memory reads directly leads to a corresponding speed up.}
%

%
An established variant of multi-head self-attention is called grouped query attention (GQA). The GQA introduces sharing a single pair of $K,V$ matrices for a group of $g$ heads (queries), which reduces the amount of $K,V$ data to load by a factor of $g$. Recent studies have shown that GQA has only a marginal impact on the accuracy of the downstream task, making it a favorable optimization~\cite{ainslie2023gqa}.
%%
%%
%%%
\subsection{\topk Attention}\label{sec:introduction:topk}
One widely adopted approach of sparsifying the attention row is finding its top $k$ out of $\seqlen$ elements and discarding the rest, where $k$ is a hyperparameter~\cite{gupta2021memory}. However, since the attention rows are often partitioned (tiling across the sequence dimension) and processed in parallel, computing the exact top-k values in a vector imposes an undesired full-row dependency, thereby constraining the partitioning strategies. Moreover, the computation of the top elements requires several computational steps~\cite{zhang2023paralleltopk}, leading to a logarithmic complexity at the very best case. \textit{We conjecture that the top-k search algorithms can be replaced by comparison to a carefully calibrated threshold.}
%%%
%%
%

%
% \subsection{Attention Sparsity}\label{sec:introduction:sparsity} 
%%%%%%%%%%%%%%% LONG VERSION BEGIN %%%%%%%%%%%%%%%%%%%%%%%%%%
% Sparsity has been exploited in previous machine learning models (e.g., in SqueezeNet \cite{iandola2016squeezenet}). Sparse matrices have many zero elements leading to ineffectual operations which can be skipped, and therefore can reduce the computational requirement and the data movement, and thus improve the computation efficiency. Sparsity can be found both in the weights of the model and the intermediate feature maps. 
% Many practical computational problems involve matrices with most of their elements being zero~\cite{duff2017direct}. Such matrices are commonly referred to as \textit{sparse}. Exploiting the sparsity by excluding the zero elements from computation and data movement can yield significant computational savings and speedup.
%

%
% \paragraph{Lossy sparsification}
% The sparsity score of post-softmax attention matrix is the ratio of zero elements to the total number of elements, while the sparsity before softmax is defined by the ratio of negative infinity scores to all elements. Ignoring negative infinities and zeroes allows for lossless attention computation. However, in skewedly distributed attention matrices, pruning can be considered, where small and smaller values are discarded and few large attention values are retained.
% where sparsity is determined by thresholding against a positive value after softmax or a non-infinite value before it. 
% Such an approach minimizes information loss while significantly reducing computation and data movement. Moreover, since the attention scores also carry irrelevant information (i.e., noise), such a lossy sparsification might bear a denoising effect, making the attention more focused and improving the quality of results~\cite{ye2024differential}. \textit{Our work explores this lossy sparsity in detail.}
%

% %
% \paragraph{Sparsity patterns}
% Different languages and text styles may exhibit not only varying sparsity scores, proportional to the presence of insignificant values, but also different sparsity patterns -- the locations of these insignificant values in the attention matrix. It is crucial to distinguish between two types of assumptions regarding sparsity patterns: \textit{fixed sparsity} and \textit{content-based sparsity}~\cite{roy2021efficient}. Fixed sparsity assumes that certain areas of the attention matrix, such as the first and last tokens in a sequence, are generally more important. Also, neighboring tokens typically have more mutual relevance than far-away tokens and can be covered by a sliding windows pattern~\cite{beltagy2020longformer}. In contrast, content-based sparsity offers a more nuanced approach, evaluating token significance depending on the context. Content-based sparsity necessitates computationally intensive techniques to identify the essential tokens accurately. 
%Ideally, it would be beneficial to reduce or compress the attention matrix using a straightforward rule, avoiding complex algorithms. \textit{To this end, our work proposes comparing attention values against a threshold through a fast element-wise operation, making it suitable for various implementations on AI accelerators.}
%
%%%%%%%%%%%%%%% LONG VERSION END %%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% SHORT VERSION BEGIN %%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%
%%%
\section{\topth Method}\label{sec:method}
%%%
%%
%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/top_th_inference_decoding.pdf}
    \caption{\topth attention inference at generative decoding.}
    \label{fig:top_th_inference}
\end{figure}
%

%
\topth attention involves comparing each attention vector against a calibrated threshold. Attention elements that fall below the threshold are pruned away from subsequent computations, enhancing the efficiency and focus of the model. Our underlying assumption is that a particular distribution of values characterizes each row of the attention matrix, therefore using a static threshold should keep approximately the desired number of selected attention elements. The benefit of using a threshold instead of computing the top-k attention values is that thresholding is a simple elementwise operation, requiring only a constant time and involving no row dependency. In contrast, ranking the elements of a vector requires at least logarithmic time and depends on the full length of the vector. \topth can be seen as an approximation of \topk attention, as it allows calibrating the thresholds for specific user-defined $k$.
% In a sense, this row dependency is embodied in the distribution and hence in the threshold value we calibrate.
%

%
The rest of this section provides a detailed discussion of (1) the calibration process for the thresholds and (2) their application for efficient inference.
%

%
%%
%%%
\subsection{Threshold Calibration}\label{sec:method:calibration}
%%%
%%
%

%
Given a transformer model, a set $\calibrationSet$ of calibration data samples, and the desirable number $k$ of elements to keep per attention row, we propose to calibrate a single scalar threshold $\theta_{l,h,r}(k)\in\mathbb{R}$ 
for every transformer layer $l$, head $h$, and attention row id $r$. The objective of the threshold calibration is to determine a threshold value $\theta_{l,h,r}(k)$ such that on average $k$ elements are greater than $\theta_{l,h,r}(k)$. The calibration procedure listed in~\cref{alg:calibrate} calibrates the thresholds of a single attention head in a single layer, thus it can be launched as a part of the forward computation of self-attention. The key step in the calibration is collecting the $(\frac{r-k}{k})$-quantile of the $r^\mathrm{th}$ attention row values for each calibration sample into a set $\Theta_r$ (i.e., l. 
7). Each quantile is essentially a per-calibration sample threshold. After collecting the per-sample thresholds, the final threshold for attention row $r$ is computed as an average over $\Theta_r$ (l. 19). In ~\cref{sec:evaluations:rows}, we show the importance of calibrating individual thresholds for every row id. To obtain the thresholds for the entire model,~\cref{alg:calibrate} should be applied independently to all other layers and heads in the model.
%, thus a single pass over the calibration samples suffices to calibrate thresholds for the entire model.
%

%
\begin{algorithm}[tb]
    \caption{Calibrate($\calibrationSet,k,\alpha$) - 1 head threshold calibration}
    \label{alg:calibrate}
    \begin{algorithmic}[1]
        \REQUIRE \( \calibrationSet \) (Calibration set of inputs)
        \REQUIRE \( k\in\mathbb{N} \) (elements to keep per attention row)
        \REQUIRE \( \alpha\in\mathbb{R} \) (calibration offset in standard deviations)
        \STATE $\Theta_{r}=\emptyset, \forall r $ \COMMENT{empty sets of observed thresholds}
        \FOR{\( X \in \calibrationSet \)}
            \IF{$\mathrm{is\_prefill}(X)$} 
                \STATE $\attnMatPre = Q(X)K^T(X)$
                \STATE $\seqlen = \mathrm{num\_rows}(\attnMatPre)$
                \FOR{\( r = k \) to \( \seqlen - 1 \)}
                    \STATE $\Theta_{r}$ = $\Theta_{r}\cup\{\mathrm{quantile}_{\frac{\seqlen-k}{n}}(A_r)\}$
                    \STATE $A_r=\mathrm{top}_k(A_r)$ 
                \ENDFOR
                \STATE $\attnMatPost=\mathrm{row\_softmax}(\attnMatPre)$                
            \ELSE[generative decoding, $X\in\mathbb{R}^\headdim$]
                \STATE $\attnVecPre = Q(X)K^T(X)$
                \STATE $\seqlen = \mathrm{length}(\attnVecPre)$            
                \STATE $\Theta_{\seqlen}$ = $\Theta_{\seqlen}\cup\{\mathrm{quantile}_{\frac{\seqlen-k}{\seqlen}}(\bm{a})\}$
                \STATE $\attnVecPre=\mathrm{top}_k(\attnVecPre)$ 
                \STATE $\attnVecPost=\mathrm{softmax}(\attnVecPre)$                
            \ENDIF
        \ENDFOR
        \STATE \textbf{return} $\theta_{r}=\mathrm{mean}(\Theta_{r})+\alpha\cdot \mathrm{std\_dev}(\Theta_{r}), \forall r$
    \end{algorithmic}
\end{algorithm}
%

%
\paragraph{Calibration hyperparameters}
The main parameter is $k$, controlling the target number of elements to keep per attention row. Generally, $k$ can be individually chosen for every layer, head, and token position. To reduce the hyperparameterization, we found that selecting a single $k$ for all heads and attention row indices within the given layer is sufficient to preserve the quality of results. However, it is necessary to allow the flexibility of choosing different $k$ values for the different layers due to their inherently different sparsity levels. The hyperparameter $\alpha$ is used to offset the threshold to a higher value when keeping the target $k$ elements should be followed more conservatively
% calculate the threshold it is multiplied with the standard deviation and added to their average to calculate the threshold value 
(l. 19 in Alg. 1). 
% The size of the calibration set can be relatively small, i.e., less than 10\% compared to the size of the training set. 
See~\cref{fig:th_calibration_histograms} for the visualization of the threshold values collected for a specific layer, head, row, and selected threshold.
%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/th_calibration_histograms/LLaMA2-7b-Arc_challenge-post-softmax-calibration-with-single-k---MEAN-plus-0.1STD-aggregation_l10_h20_r1000.eps}
    \caption{Distribution of the set $\Theta_{1000}$, collected during calibration of threshold $\theta_{10,20,800}, \alpha=0.1$ on \hell, LLaMA2-7b model.}
    \vspace{-2mm}
    \label{fig:th_calibration_histograms}
\end{figure}
%

%
% \paragraph{Pre-softmax vs. Post-softmax}
% \cref{alg:calibrate} in its current form is calibrating pre-softmax thresholds, whereas it can be adapted to post-softmax by taking the quantile on the post-softmax attention $\attnMatPost,\attnVecPost$ rather than on the $\attnMatPre,\attnVecPre$. The decision of applying pre- or post-softmax thresholding can highly depend on the implementation of the attention kernel and can impact the model's accuracy.
%

%
\paragraph{Calibration details}
\cref{alg:calibrate} in its current form is calibrating pre-softmax thresholds, whereas it can be adapted to post-softmax by taking the quantile on the post-softmax attention $\attnMatPost,\attnVecPost$ rather than on the $\attnMatPre,\attnVecPre$. Another detail is that no calibration is needed for rows $0$ -- $k$ since, due to the causality mask, these rows will contain at most $k$ non-zero elements already. In addition, our calibration includes a special ``\topk at calibration'' (TAC) step (lines 8, 5) that emulates the impact of \topth sparsification already during the calibration by keeping the top $k$ elements ($\text{top}_k(\bm{v})$ in the algorithm stands for a subroutine returning a $k$-long vector of the $k$ largest elements in the vector $\bm{v}$). This step modifies the activations passed to the subsequent layer being calibrated such that these resemble more closely the activations that will be in place once \topth is deployed. The size of the calibration set $\calibrationSet$ can be relatively small, as we observed that a few hundred to 2 thousand calibration samples are sufficient to calibrate stable thresholds. 
%

%
\cref{fig:th_llama} visualizes the threshold values of the 11\textsuperscript{th} layer of LLaMA2-7b, which were calibrated for $k=64$. It is worth noticing that the different heads (different colors) have distinct threshold values, hence the importance of calibrating per-head thresholds. Secondly, thresholds obtained from the $\attnMatPre$ matrix approach a constant value as the sequence length increases, whereas thresholds obtained from the $\attnMatPost$ matrix tend to decrease in the longer sequence lengths. The latter is an effect of the softmax normalization that reduces the share of essential tokens out of the total row sum of 1 as the sequence length increases; therefore, keeping them requires lowering the threshold. The noisiness of the thresholds in the very last calibrated sequence length is purely due to the scarcity of the calibration samples in the dataset that corresponded to these sequence lengths (lower purple bars in~\cref{fig:th_llama}). For more examples of threshold values, refer to~\cref{appendix:th_llama}.
%

%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/threshold_visualization/th_llama7_layer10_hellaswag_pre_post_k64.png}
    \caption{\textbf{Threshold values as a function of a sequence length} LLaMA2-7b,
    %Top: pre-softmax, Bottom: post-softmax. Scatter plots: final calibrated threshold values. 
    11\textsuperscript{th} transformer layer, calibration targeted k=64. 
    %Purple columns represent the number of calibration inputs that contain the respective sequence length.
    }
    \label{fig:th_llama}
\end{figure}
%

%
\paragraph{Threshold function fitting} Our main proposal is to deploy the calibrated thresholds as complementary model parameters (one threshold parameter per every layer, head, and sequence length). However, as can be observed in~\cref{fig:th_llama}, the thresholds have a rather regular behavior as a function of the sequence length (i.e., of the attention row id). We can fit a function to it and thereby parameterize hundreds of thresholds by a few parameters.
%

%
\paragraph{Multi-k calibration} It is possible to calibrate multiple threshold sets $\theta_l(k_1),\theta_l(k_2),...$ targeted at different number of top elements $k_1,k_2,...$ to be preserved. Such calibration will allow one to dynamically decide at inference time on the desired accuracy-performance tradeoff by applying a set of thresholds of a different $k$, an attractive capability for LLM serving~\cite{miao2023efficientgenerativelargelanguage}. Instead of applying ~\cref{alg:calibrate} several times with the different $k$ values,  we have developed a generalized calibration algorithm, which performs a single pass over the calibration samples. The generalized procedure collects up to $n$ thresholds for every attention vector of length $n$ in the calibration sample, merges these thresholds information, and eventually outputs for each (layer, head, row) a function $\theta(k)$ from $k$ to a threshold that satisfies it. Although such a calibration procedure poses large memory requirements, reducing its intensity by sampling fewer than $n$ thresholds per vector is possible. (see~\cref{appendix:multik_cumulitive_calibration}).
%
%%
%%%
\subsection{\topth Attention Inference}\label{sec:method:inference}
%%%
%%
%
The inference procedure is depicted in~\cref{fig:top_th_inference}, focusing on a single transformer layer $l$, single head $h$, and single attention row  $n$ of length $n$ (as in generative decoding). The attention vector is compared against the calibrated threshold value $\theta_{l,h,n}$. Attention elements that do not pass the threshold are discarded. 
%The discarding can be done either by replacing the attention element with $-\infty$ in the pre-softmax setting, by replacing it with $0$ in the post-softmax setting, or by removing them and creating a compact version of the selected attention elements and their indices as shown in ~\cref{fig:top_th_inference}. %The multiplication of the attention elements after the softmax needs to take place only with the V rows, at the indices identical to the indices of the selected attention elements. 
Instead of multiplying by the entire $V$ matrix, only the selected $\Tilde{k}$ row indices are loaded to a compact matrix $\Tilde{V}$, which is used to compute the final product $p$.
The calibrated thresholds can be stored alongside the model parameters. Since the calibration might not cover the entire range of sequence lengths $r\in\{k, k+1,\ldots\}$, a threshold of the nearest calibrated sequence length ($r$) can be used. 
%In particular, when the attention vector exceeds the maximum length for which the thresholds were calibrated, we use the threshold of the maximum sequence length observed during calibration. This is because we found that the threshold as a function of a sequence length approaches a constant number as the sequence length grows longer. 

Applying a threshold calibrated towards a specific $k$ allows selecting $k$ attention row elements \textit{on average} because the distribution of the attention values slightly varies across the processed inputs.
To address the cases when thresholding results in more attention elements than desired ($\Tilde{k}>k$), we experimented with capping the number of selected elements to at most k, prioritizing first and last tokens (we dubbed this technique CAPK). However, in generative tasks, CAPK caused a noticeable degradation in performance.
%

%
We propose two compensation methods that recover the accuracy degradation incurred by the  \topth by providing a better mathematical approximation of the overall self-attention operation. These methods introduce a minor additional compute but may be crucial for accuracy-sensitive tasks. Moreover, these compensations apply to other attention sparsification approaches such as the \topk attention.

%
%%
%%%
\subsubsection{Softmax Denominator 
 Compensation}
%%%
%%
%
% When sparsifying the rows of the $\attnMatPre$ matrix, the subsequent softmax operation (\ref{eqn:attn_post_softmax}) normalizes each row by the sum of exponents of remaining elements only. As a result, each row in the $\attnMatPost$ matrix sums up to 1. Our experiments showed that sparsifying the $\attnMatPost$ matrix (post-softmax), which consequently leaves the row not summing up to 1, retains a higher accuracy. This suggests that normalizing the attention matrix using the sum of exponents of the non-sparsified $\attnMatPre$ matrix is significant, which motivates us to attempt to emulate the post-softmax sparsification when performing pre-softmax sparsification. Therefore, we introduce the softmax denominator compensation by renormalizing the rows of the $\attnMatPost$ matrix (after it was computed from a sparsified $\attnMatPre$ matrix) by a correct denominator. Let $R$ be the sum of exponents of the selected elements $\selids$ of the attention matrix $\attnMatPre$ and let $E$ denote the sum of exponents of the non-selected row elements. Every softmax element of the non-sparsified vector $\attnVecPost$ can be expressed in terms of the sparsified
Since the post-softmax sparsification showed higher accuracies in our experiments compared to pre-softmax, we are interested in approximating post-softmax-sparsified attention ($\attnVecSparsePost$) using pre-softmax-sparsified attention ($\attnVecSparsePre$).
Let $\selids\subseteq\left\{0,\dots,n-1\right\}$ denote a set of indices that we intend to keep during the sparsification, and let us establish the relation between post-softmax sparsified vector $\attnVecSparsePost$ and the pre-softmax sparsified vector that underwent softmax ($\mathrm{softmax}(\attnVecSparsePre)$). Let $R$ and $E$ denote sums of exponents of the selected and discarded elements, respectively.
\begin{align}
\forall i\in\selids, \attnVecSparsePost_i
 &= \mathrm{softmax}(\bm{a})_i = \frac{e^{a_i - \max_l{a_l}}}{\sum_{j=0}^{n-1}e^{a_j - \max_l{a_l}}} \\
 &= \frac{e^{a_i - \max_l{a_l}}}{\sum_{j\in\selids}e^{a_j - \max_l{a_l}} + \sum_{j\notin\selids}e^{a_j - \max_l{a_l}}} \\
 &= \frac{e^{a_i - \max_l{a_l}}}{R+E} = \frac{e^{a_i - \max_l{a_l}}}{R}\cdot\frac{R}{R+E} \\
 &= \mathrm{softmax}(\attnVecSparsePre)_i\cdot\frac{R}{R+E}\label{eqn:sdc}
\end{align}
In other words, to achieve a post-softmax sparsification effect, one can perform pre-softmax sparsification, estimate $\Tilde{E}\approx E$, and compensate by multiplying by a factor of $R/(R+\Tilde{E})$.
The multiplication step can be applied after the softmax (\ref{eqn:attn_post_softmax}) or even after the SV product (\ref{eqn:sv}) similarly to the flash-attention~\cite{dao2022flashattention}.
We consider 3 estimations:
\begin{enumerate}
    \item \texttt{offline-calibrated}: calibrate the compensation value $\Tilde{E}$ for every (layer, head, row) similarly to the method of threshold calibration. 
    \item \texttt{exp-threshold}: $\Tilde{E}=\gamma(n-\Tilde{k})e^\theta$ where $\theta$ is the calibrated threshold for the current sequence length $n$, and $\Tilde{k}$ is the number of selected attention elements. The intuition behind this approximation is that all the not-selected attention elements are less than $\theta$. The $\gamma$ is a small constant hyperparameter that we set to $0.05$ to approximate the difference between the sum of exponentiated thresholds and actual exponentiated discarded attention elements.
    \item \texttt{exact}: compute the exact $\Tilde{E}=E$ using the sum of exponents of the non-selected row elements. 
\end{enumerate}

Assuming that $\mathrm{argmax}_l{a_l}$ is included in the selected elements $\selids$, we do not correct the maximum value subtracted from the exponents during softmax computation.
%
%%
%%%
\subsubsection{V-Mean-Compensation (VMC)}
%%%
%%
%
Applying \topth in the following two cases will result in attention vector rows not summing up to 1: (i) post-softmax, (ii) pre-softmax followed by SDC. As a result, any attention element dropped due to thresholding will cancel the effect of its product by the corresponding row of the $V$ matrix in the final $p=sV$ product (\ref{eqn:sv}). In these situations, we propose compensating for the missing value by adding a mean row $\bm{\mu}$ (\ref{eqn:vmc:mu}) of the $V$ matrix scaled by the sum of all the discarded attention elements $\beta$ (\ref{eqn:vmc:beta}). Let $\bm{\Tilde{\attnVecPost}}\in[0,1]^{\Tilde{k}}$ be a post-softmax attention vector that underwent \topth and is eligible for VMC, and let $\attnVecPost\in[0,1]^n$ be the non-sparsified version of it.
%and let $\notselids\subseteq [0,n-1]$ the set of indices of elements that did not surpass the threshold. 
Equations~\ref{eqn:vmc:mu}--\ref{eqn:vmc:final} show how we compute the corresponding V-mean-compensated product $\hat{p}\approx\attnVecPost V$. 
%
\begin{align}\label{eqn:vmc:mu}
    % \forall 0\le j<\headdim: \mu_j=\frac{1}{\seqlen}\sum_{i=0}^{\seqlen-1}V_{i,j}\\
    \bm{\mu} &= \frac{1}{\seqlen}\sum_{i=0}^{\seqlen-1}V_{i}\\
\label{eqn:vmc:beta}
    \beta &= 1-\sum_{i=0}^{\Tilde{k}-1} \attnVecSparsePost_i \\
    \label{eqn:vmc:final}
    \prodVecCompensated &= \attnVecSparsePost V + \beta\bm{\mu}
\end{align}
%

%
We justify the VMC approximation in~\cref{appendix:vmc}. The intuition behind it is to approximate each of the $\seqlen-\numKept$ discarded values by an average and to multiply it by an average row of the V matrix. Such compensation can improve as the sequence length $\seqlen$ increases because the number of the averaged elements in (\ref{eqn:vmc:mu}) and (\ref{eqn:vmc:beta}) will increase accordingly. 
During generative decoding, $\bm{\mu}$ can be maintained as a running-mean to avoid full recalculation~(\ref{eqn:vmc:mu}).
\vspace{-1mm}

%
%%
%%%
\section{Evaluations}\label{sec:evaluations}
%%%
%%
%
We use the LM Evaluation Harness~\cite{gao2024lm} to evaluate the normalized accuracy metric on multiple-choice Q\&A datasets, including their standard few-shot settings. In addition, we assess the \humaneval dataset using the official library~\cite{chen2021codex} to measure the pass@1 metric on all 164 code generation tasks. Our source code is available\footnote{\tiny{\url{https://github.com/kostyanoob/top-theta-attention}}}.

We evaluate 3 main attention variants:
\begin{inparaenum}[(i)]
    \item Baseline -- full attention, without any sparsification;
    \item \topk\ -- keep $k$ attention elements per row;
    \item \topth\ -- keep $\Tilde{k}\approx k$ attention elements by thresholding.
\end{inparaenum}
We apply our compensation methods also to the \topk baselines for a fair comparison.

%

%
%%
%%%
\subsection{\topth  Overall Performance}\label{sec:evaluations:overall}
%%%
%%
%
%
%%
%%%
\begin{figure*}
\centering
%% 3 LLama2-7b plots
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-2-7b-hf_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA2-7b \arcc}
    \label{fig:evaluations:acc_kept_attn:llama2_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-2-7b-hf_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA2-7b \arce}
    \label{fig:evaluations:acc_kept_attn:llama2_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-2-7b-hf_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA2-7B \hell }
    \label{fig:evaluations:acc_kept_attn:llama2_hellaswag}
\end{subfigure}

%% 3 LLama3-8B plots
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA3-8B \arcc}
    \label{fig:evaluations:acc_kept_attn:llama3_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA3-8B \arce}
    \label{fig:evaluations:acc_kept_attn:llama3_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA3-8B \hell }
    \label{fig:evaluations:acc_kept_attn:llama3_hellaswag}
\end{subfigure}

%% 3 LLama3-70B plots
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-70B_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA3-70B \arcc}
    \label{fig:evaluations:acc_kept_attn:llama3_70_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-70B_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA3-70B \arce}
    \label{fig:evaluations:acc_kept_attn:llama3_70_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.33\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-70B_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA3-70B \hell}
    \label{fig:evaluations:acc_kept_attn:llama3_70_hellaswag}
\end{subfigure}

\caption{\textbf{Prefill-based tasks} - Tradeoff between model accuracy averaged across test samples (y-axis) and the portion of kept attention elements per attention head (x-axis). All post-softmax \topk and \topth employ VMC, and all pre-softmax variants employ both VMC and exact SDC. These compensations achieve little, if any, accuracy degradation while achieving up to 10\texttimes\ reduction in the attention elements. }
\label{fig:evaluations:acc_kept_attn}
\end{figure*}
%%%
%%
%



This section evaluates the best configurations of \topth, comparing them to the non-sparsified baseline and to \topk. 
%

%
In~\cref{fig:evaluations:acc_kept_attn}, LLaMA2 and LLaMA3 models are evaluated on Q\&A tasks, where the $k$ parameter of \topk and of the \topth is swept from 32 to 512. The first two layers are kept at $k=512$. The calibration set used for \topth in each dataset is $10\%$ of the training or validation sets (different from the test set). The $x$-axis shows the fraction of the attention elements that are involved in computation, normalized to an average number of attention elements in the entire model in a given forward pass. The $y$-axis shows the normalized accuracy. The main observation is that in all models, both \topk and \topth have \textit{increased} the accuracy (by $0.2\%-1\%$) compared to the baseline while pruning away a significant portion of attention elements ($2\times$ -- $5\times$ fewer elements were active). Second, post-softmax sparsification performs consistently better in both \topk and \topth, compared to the pre-softmax sparsification.
%

%
% \clearpage
In~\cref{fig:evaluations_generative:acc_kept_attn}, we focus on the generative tasks of the \humaneval dataset. 
%Specifically, we investigate how the number of required V-rows is reduced by our sparsification approaches.
We investigate the tradeoff between pass@1 score and two related metrics: the number of required V-rows (\cref{fig:evaluations_generative:acc_kept_vrow:llama3_8b_humaneval,fig:evaluations_generative:acc_kept_vrow:llama3_70b_humaneval})
%, and the number of attention elements during the prefill phase (\cref{fig:evaluations_generative:acc_kept_attn:llama3_8b_humaneval,fig:evaluations_generative:acc_kept_attn:llama3_70b_humaneval}). 
Importantly, for this benchmark, we use the instruction-tuned version of the LLaMA-3 models, and we apply the same system prompt to all the evaluated attention variants. The generation is done greedily (i.e. no sampling, T=0). To demonstrate domain adaptation capabilities of the calibrated \topth thresholds, they were first calibrated on a different dataset (\arcc) and then loaded for \humaneval evaluation. Notably, the post-softmax \topth performs the best in both LLaMA-3-8B and LLaMA-3-70B models, preserving pass@1 within $0.5\%$ of the baseline while reducing the required V-rows by $3\times$. Impressively, in the 70B model, \topth offered a 5\texttimes\ reduction at the expense of $1\%$ of the pass@1.
%score. 
%

%
Overall, both \topth and \topk performed similarly well on both Q\&A and generative tasks.
For a version of~\cref{fig:evaluations:acc_kept_attn,fig:evaluations_generative:acc_kept_attn} with standard deviations of the collected metrics, the reader can refer to~\cref{appendix:evaluation_statistics}. The remaining subsections closely evaluate the various aspects of our method.


%
%%
%%%
\begin{figure}
\centering
\begin{subfigure}{0.44\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B-Instruct_Human-Eval_-_generative_decoding_phase.eps}
    \caption{LLaMA3-8B-Instruct - V-rows at decoding}
    \label{fig:evaluations_generative:acc_kept_vrow:llama3_8b_humaneval}
\end{subfigure}
% \hfill
% \hspace{1cm}
% \begin{subfigure}{0.33\textwidth}
%     \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B-Instruct_Human-Eval_-_prefill_phase.eps}
%     \caption{LLaMA3-8B-Instruct - attention at prefill}
%     \label{fig:evaluations_generative:acc_kept_attn:llama3_8b_humaneval}
% \end{subfigure}
\hfill
\begin{subfigure}{0.44\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-70B-Instruct_Human-Eval_-_generative_decoding_phase.eps}
    \caption{LLaMA3-70B-Instruct V-rows at decoding}
    \label{fig:evaluations_generative:acc_kept_vrow:llama3_70b_humaneval}
\end{subfigure}
% \hfill
% \hspace{1cm}
% \begin{subfigure}{0.33\textwidth}
%     \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-70B-Instruct_Human-Eval_-_prefill_phase.eps}
%     \caption{LLaMA3-70B-Instruct attention at prefill}
%     \label{fig:evaluations_generative:acc_kept_attn:llama3_70b_humaneval}
% \end{subfigure}
        
\caption{\textbf{Generative Task \humaneval} -- Tradeoff between model accuracy averaged across test samples (y-axis) and the portion of required V-rows per group of heads (x-axis). The \topth variants employ threshold calibrated on \arcc dataset. All post-softmax \topk and \topth employ VMC, and all pre-softmax variants employ both VMC and exact SDC. 3\texttimes\ reduction of V rows is achieved.}
\label{fig:evaluations_generative:acc_kept_attn}
\end{figure}
%%%
%%
%

%
\vspace{-2mm}
\subsection{Pre- vs Post-Softmax Thresholding}
We evaluate the impact of attention matrix sparsification in its two main variants: on matrix $\attnMatPre$ and on the post-softmax matrix ($\attnMatPost$), where each of these two variants requires individual calibration. \cref{fig:pre-post} depicts how the different thresholdings impact the accuracy of LLaMA2-7b on the \hell dataset. For comparison, the \topk approach is also evaluated alongside \topth. We conclude that \textit{post-softmax preserves more accuracy compared to pre-softmax thresholding}. This conclusion is confirmed on extended evaluations on other datasets (see~\cref{appendix:pre-vs-post-softmax}).
\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/pre-post-llama2-7-hellaswag.eps}
    \vspace{-1mm}
    \caption{Comparison of pre- and post-softmax sparsification}
    \label{fig:pre-post}
\end{figure}
\vspace{-1mm}
%
%%
%%%
\subsection{Thresholding Different Layers}\label{sec:evaluations:layers}
%%%
%%
%
We explored the impact of thresholding different layers with a different target $k$. For the LLaMA models, we have observed that targeting a slightly higher $k$ in the first layers is crucial. As a best practice, we found keeping 2 initial layers at $k=512$, whereas the rest of the layers could be sparsified more aggressively.~\cref{fig:highK} shows the LLaMA2-7b model accuracy on \hell, comparing \topk and \topth using higher $k$ in first 2 layers against using equal $k$ in all layers. All variants do not perform any compensations. \textit{The conclusion is that preserving denser attention matrices in the early layers is beneficial for the accuracy of a downstream task, which is aligned with some of the works on quantization}~\cite{tang2024quest,huang2024quantization}.
%

%
\begin{figure}[th]
    \centering 
    \includegraphics[width=0.46\textwidth]{figures/highK-llama2-7-hellaswag.eps} 
    \vspace{-1mm}
    \caption{The positive impact of high-k in first two layers}
    \label{fig:highK} 
\end{figure}
\vspace{-3mm}
%

%
\subsection{Thresholding Different Attention Rows}\label{sec:evaluations:rows}

In~\cref{sec:evaluations:layers}, we have shown that \topth attention should use individually calibrated thresholds for every transformer layer in the model due to inherently different attention element distributions across the layers, and in~\cref{sec:method:calibration}, we saw that different heads require individual thresholds. However, it is less obvious whether the different rows of the attention matrix require different thresholds within a single head. Evaluations of the LLaMA2-7b model on \hell (\cref{fig:calibration_per_row}) show that calibrating individual thresholds per-attention row is beneficial, especially for the pre-softmax setting. For more supporting results, refer to~\cref{appendix:evaluations:rows}.
\begin{figure}[t]
    \centering 
    \includegraphics[width=0.46\textwidth]{figures/calibration-per-row-llama2-7-hellaswag.eps} 
    \vspace{-1mm}
    \caption{Calibrating per-attention-row thresholds vs a unified threshold for all rows (sequence lengths)}  
    \label{fig:calibration_per_row} 
\end{figure}
\vspace{-3mm}
%
%%
%%%
\subsection{Numerical Compensations}\label{sec:evaluations:numerical_compensations}
%%%
%%
%
We evaluate the proposed numerical compensation methods SDC, and VMC.~\cref{fig:numerical_compensations} shows that the more explicit SDC versions (exp-threshold, exact) reclaim more of the degraded accuracy on the hard task of \hell. Moreover, when applying the VMC as well, the benefit is further increased. 
%

%
\cref{fig:numerical_compensations} focuses on the \hell task, LLaMA2-7b model with \topth attention pre-softmax, in which the non-compensated \topth attention incurred up to 1.5\% accuracy degradation, whereas the compensations have effectively closed the gap to the baseline accuracy. 
%

%
In similar experiments on \arcc and \arce datasets, \topth has outperformed the baseline attention without the compensations thanks to increasing the focus of the model, or in other words, reducing the attention noise. In such cases, the compensations do not contribute much and may only bring back the noise, thereby degrading the accuracy down to the baseline accuracy levels.

\vspace{-2mm}
\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.46\textwidth]{figures/numerical_compensations_tight.eps} 
    \vspace{-1mm}
    \caption{Positive impact of SDC and VMC compensations}  
    \label{fig:numerical_compensations} 
\end{figure}
\vspace{-3mm}
%
%%
%%%
\subsection{Impact of Grouped Query Attention (GQA)}\label{sec:evaluations:impactofgqa}
%%%
%%
%
In GQA, multiple attention heads share the same V matrix. In LLaMA-3-8B models, the GQA group size is $g=4$; therefore, when \topk attention is applied, in the worst case of total disagreement between the heads, there could be $4\cdot k$ rows of V necessary. However, as seen in~\cref{fig:evaluations:gqa}, both \topk and \topth set for $k=128$ remain at approximately 250-300 selected V-rows per group. Similar results were observed in numerous layers, indicating that different heads in the group do have a certain agreement. To observe the selected tokens of the heads in the group, refer to~\cref{appendix:evaluations:impactofgqa}.
%

%
\vspace{-2mm}
\begin{figure}[t]
    \centering 
    \includegraphics[width=0.435\textwidth]{figures/v_row_popularity/kept_vrows_3approaches_e25_l2_g0.eps} 
    \vspace{-2mm}
    \caption{\textbf{GQA impact} - Number of required V-rows for every generated token, LLaMA-3-8B-Instruct (layer 2, first  GQA group), \humaneval task \#25.}  
    \label{fig:evaluations:gqa} 
    \vspace{-3mm}
\end{figure}
%

%
In our experiments, each head in the group only used its selected attention elements and V-rows. However, we envision 2 potentially better methods to deal with GQA from a sparsification perspective: 1) discarding V-rows that were requested by the \topth masks in very few heads in the group. 2) Augment the per-head $\selids$ with the group's union  - to reclaim more accuracy using V rows that are loaded anyway for the group.
%
%%
%%%
\subsection{Distribution Shifts}\label{sec:evaluations:distribution_shift}
%%%
%%
%
To examine how resilient the calibrated threshold is, we evaluate on \humaneval the two following \topth variants: 1) calibrated on Q\&A dataset of \arcc, and  2) calibrated on the first 10\% of \humaneval tasks. As seen in~\cref{fig:evaluations:distribution_shift}, the \topth post-softmax is even more accurate when using thresholds calibrated on a different dataset. For pre-softmax, there is a benefit of calibrating on the same dataset. \textit{Overall, the thresholds show resilience towards distribution shift, which suggests that they are strongly associated with the model rather than with the data. This allows calibrating thresholds once per model.}
\vspace{-3mm}
%

%
\begin{figure}[ht]
    \centering 
    \includegraphics[width=0.46\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff-lineplots/Llama-3-8B-Instruct_Human-Eval_-_prefill_phase_-_cross-domain-calibration.eps} 
    \vspace{-3mm}
    \caption{\textbf{Distribution shift} LLaMA3-8b-Instruct evaluated on \humaneval. cal.arcc variants were calibrated on \arcc}  
    \label{fig:evaluations:distribution_shift} 
\end{figure}
\vspace{-3mm}
%
%%
%%%
\section{Related Work}\label{sec:related_work}
%%%
%%
%

%
In the line of work on the \textit{content-based sparsity}, the most fundamental approach is \topk attention~\cite{gupta2021memory}, which selects the  $k$ highest values in each row of the $\attnMatPre$ matrix. However, in practical kernel implementations, where the attention rows are tiled, it is highly difficult to implement the top-k search algorithm due to the full row dependency. Works such as Energon~\cite{zhou2022energon} went further and aimed at reducing the reads from the K-cached matrix by iteratively quantizing K, computing the $\attnMatPre$ matrix, and keeping the higher values. However, reducing access to K prohibits Energon from accurately computing the $\attnMatPost$ matrix, which is therefore as limited as not using our compensation methods in \topth. The SpAtten~\cite{wang2021spatten} and A\textsuperscript{3}~\cite{ham2020a3acceleratingattentionmechanisms} present \topk attention variants which require specialized hardware for accelerating the top-k search, which can be avoided by \topth. The SparQ~\cite{ribar2023sparq} work also relies on the top-k search, however, it performs a similar compensation to our VMC to compensate for the rows of V matrix, which did not participate in the SV product (~\ref{eqn:sv}) due to the sparsification of the attention. The Swin Transformer~\cite{liu2021swin} focuses on a vision transformer (i.e., no autoregressive decoding) applying a fixed sparsity pattern, where each token (patch) attends to its neighboring M\texttimes M patches.
%

%
Learned Token Pruning (LTP)~\cite{kim2022learned} is closely related to our work as it discards tokens in the attention matrix based on a model-trained threshold. While LTP benefits from the simplicity of the thresholding approach, it requires full model retraining. Our method shares the threshold concept but improves upon it by identifying an inductive threshold within an already-trained model. This makes our approach more appealing for large models, where training or fine-tuning can be prohibitively expensive. Similarly, the work of Lou~\etal~\cite{lou2024sparserfastermoreefficient} also targets the selection of top-k elements but requires retraining.
%

%
Numerous works focused on \textit{fixed attention} sparsity. For example, in Longformer~\cite{beltagy2020longformer}, only a few manually picked ``global attention'' tokens are allowed to attend to other tokens, whereas other tokens are restricted to the neighboring tokens in a dilated sliding window. Our work, on the other hand, capitalizes on identifying content-based sparsity in the attention matrix, as it offers higher accuracy gains. The work of Kim and Cho~\cite{kim2021lengthadaptivetransformertrainlength} requires retraining, although accommodates dynamic shortening of the sequence on demand at inference time.
%

% Weakly relevent paragraph, commented-out. Kept for a longer journal version
% An alternative line of attention reduction abandons the aforementioned structural sparsifications and turns to\textit{ attention transformations, factorizations, and low-rank approximations}. The work of Child \etal~\cite{child2019generating} exploits sparsity by factorizing the attention into products of smaller matrices, which fundamentally differs from our work and requires retraining the model. The Linformer~\cite{wang2020linformer} performs low-rank approximation of the Q and K matrices, which introduces compression errors at the source and also requires retraining. FNet~\cite{lee2021fnet} applied Fourier transform before performing the scaled dot product to avoid an overly sparse attention matrix, however, operating in the frequency domain requires significant retraining.
%

% Weakly relevent paragraph, commented-out. Kept for a longer journal version
% An orthogonal line of works aimed at KV-cache compression. For example, Scissorhands~\cite{liu2024scissorhands} reduces the memory requirements of a KV cache by applying thresholding to determine which tokens are to be stored at the KV cache. Differently from our work, the KV cache compression methods aim at reducing the KV cache (memory and bandwidth), at the risk of information loss. Our work assumes the full KV-cache availability in the memory. Nevertheless, a smart reordering of the cache rows can help mitigate the memory capacity deficiency.
%
%%
%%%
\vspace{-1mm}
\section{Conclusion}\label{sec:conclusion}
%%%
%%
%
We have presented \topth, a new sparse attention algorithm based on fixed and calibrated thresholds. At inference time, it boils down to a simple elementwise operation, which overcomes the limitation of full-row dependent algorithms, such as \topk, and thereby unconstraining tiling and distributed inference implementations. Moreover, only a small subset of calibration samples is required to calibrate the thresholds, which we showed to be resilient to distribution shifts. Therefore, a short calibration is needed once per every model. Furthermore, we show minor to no performance degradation using 10\texttimes\ less attention elements at the computationally-bound prefill phase and using 3\texttimes\ less V matrix rows at the memory bandwidth-bound generative decoding phase. These reductions unlock promising speedups for inference.
%reduction reduces while a speedup/performance improvement of X could be shown due to the computational reduction and skipped V rows, and thus memory bandwidth reduction.
\clearpage
\bibliography{references}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Impact Statement}
This paper presents work whose goal is to advance the field of efficient Machine Learning. All potential societal consequences are unrelated to the specific work but are related to machine learning applications in general.

%
%%
%%%
\section{Threshold Calibration}\label{appendix:th_llama}
%%%
%%
%
In this appendix section, we show more calibrated threshold values (as a function of the sequence length) in more layers and heads. ~\cref{fig:appendix:th_llama} visualizes the thresholds as a function of a sequence length that was calibrated for The LLaMA2-7b model for the \hell dataset. Different heads have evidently different threshold values, hence the importance of per-head individual calibrations
%
%%
%%%
\begin{figure}[h!]
\centering
\begin{subfigure}{0.46\textwidth}
    \includegraphics[width=\textwidth]{figures/threshold_visualization/th_llama-2-7b-hellaswag-topth-k64-pre-vmc-sdc-exact.png}
    \caption{Pre-softmax}
    \label{fig:appendix:th_llama:pre}
\end{subfigure}
\hfill
\begin{subfigure}{0.46\textwidth}
    \includegraphics[width=\textwidth]{figures/threshold_visualization/th_llama-2-7b-hellaswag-topth-k64-post-vmc.png}
    \caption{Post-softmax}
    \label{fig:appendix:th_llama:post}
\end{subfigure}
        
\caption{\textbf{Threshold values as a function of a sequence length in LLaMA2-7b}. Left Column: pre-softmax, Right Column: post-softmax. Scatter plots: final calibrated threshold values. Rows top to bottom: layer 0,1,10,31. Purple columns: number of calibration inputs that contained the respective sequence length. Layers 0 and 1 were calibrated for $k=512$, and layers 10 and 31 were calibrated for $k=64$.}
\label{fig:appendix:th_llama}
\end{figure}
%%%
%%
%


%
%%
%%%
\section{Multi-k Cumulative Calibration}\label{appendix:multik_cumulitive_calibration}
%%%
%%
%
\cref{alg:calibrate}, which we presented in the paper, calibrates thresholds of a single attention head for a single target $k$. We propose to generalize it to calibrate all thresholds for a broader range of target $k$ and to do it in one pass over the calibration samples. We call such a calibration procedure ``Multi-k Cumulative'' (MKC).
%

%
The goal of MKC calibration is to construct a multi-k threshold function $\theta(k):\mathbb{N}\rightarrow\mathbb{R}$ for every (layer $l$, head $h$, attention row id $r$). The user will be able to query such a function using their $k$ of choice and obtain the threshold needed to accommodate this $k$. Such an ability allows a very flexible control over the sparsification of the model, which is a highly desirable tool for LLM inference service that should be able to dynamically trade some accuracy for speedup.
%

%
\paragraph{MKC algorithm} ~\cref{alg:mkc} describes how a threshold function $\theta(k)$ can be calibrated. First, for every given calibration sample, let $v$ represent the $r^{th}$ attention row. The $v$ undergoes a sorting and then is treated as a sequence of half-open intervals. For each interval, we treat its smaller endpoint as its threshold, and we also associate with it an effective $k$ (effective $k$ of a threshold w.r.t the vector $v$ is the number of elements that are greater than the threshold). For example, on line 2 we represent by $\langle r-1,[v_1,v_2)\rangle$ the interval between $v_1$ (inclusive) and $v_2$ (exclusive) that corresponds to the threshold $v_1$ and an effective $k$ of $r-1$. 
%
%
%
Second, per-calibration-sample interval sequences collected in the set $\Theta_r$ are merged to represent the function $\bar{k}(\theta):\mathbb{R}\rightarrow\mathbb{R}$ which maps each interval to a threshold and to an average effective $k$ achievable by this threshold on the calibration set. This merging is done by the \textbf{MergeIntervals($\Theta_r$)} subroutine, which computes the following:
\begin{align}
    \bar{k}_r(\theta) = \frac{1}{|\Theta_{r,\theta}|}\sum t_0 ,\forall\theta
\end{align}
where we define $\Theta_{r,\theta}=\{t | \theta\in t_1 \wedge t\in seq \wedge seq\in\Theta_r\}$ as a subset of intervals that include the $\theta$ in theinterval.
%This pivotal step of merging implements aggregating the overlapping intervals from the different calibration samples into minimum common sub-intervals and averaging their associated effective $k$s. 
Note that the resulting average effective $k$ might be fractional due to the averaging. Finally, the desired $\theta(k)$ is given by the inverse of $\bar{k}(\theta)$, at points where the function $\bar{k}(\theta)$ maps to a natural value. The full implementation of the MergeIntervals routine is available in the supplied source code, and we show its visualization on~\cref{fig:appendix:merge_intervals}.
%

%
\begin{algorithm}[h!]
    \caption{MKC($\calibrationSet$) - 1 head calibration of thresholds for all possible $k$}
    \label{alg:mkc}
    \begin{algorithmic}[1]
        \REQUIRE \( \calibrationSet \) (Calibration set of inputs)
        \STATE $\Theta_{r}=\emptyset, \forall r $ \COMMENT{empty sets of observed interval sequences}
        \FOR{\( X \in \calibrationSet \)}
            \IF{is\_prefill($X$)} %\COMMENT{prefill phase, $X\in\mathbb{R}^{\seqlen\times\headdim}$}
                \STATE $\attnMatPre = Q(X)K^T(X)$
                \STATE $\seqlen = $NumRows$(\attnMatPre)$
                \FOR{\( r = 1 \) to \( \seqlen - 1 \)}
                    \STATE $\bm{v} = $Sort$(\attnMatPre_r)$
                    \STATE $\Theta_{\seqlen}$ = $\Theta_{\seqlen}\cup\{\langle r-1,[v_0,v_1)\rangle,\langle r-2,[v_1,v_2)\rangle,\ldots,\langle 1,[v_{r-2},v_{r-1})\rangle\}$           
                \ENDFOR
            \ELSE[generative decoding, $X\in\mathbb{R}^\headdim$]
                \STATE $\attnVecPre = Q(X)K^T(X)$
                \STATE $\seqlen = $Length$(\attnVecPre)$            
                \STATE $\bm{v} = $Sort$(\attnVecPre)$
                \STATE $\Theta_{\seqlen}$ = $\Theta_{\seqlen}\cup\{\langle n-1,[v_0,v_1)\rangle,\langle n-2,[v_1,v_2)\rangle,\ldots,\langle 1,[v_{n-2},v_{n-1})\rangle\}$           
            \ENDIF
        \ENDFOR
        \STATE $\bar{k}_r(\theta) = $MergeIntervals$(\Theta_r), \forall r$
        \STATE \textbf{Return} $\theta_r(k) = \bar{k}_r^{-1}(k), \forall r$
    \end{algorithmic}
\end{algorithm}
%

%
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.36\textwidth]{figures/merge_intervals.pdf}
    \caption{\textbf{MergeIntervals($\Theta_r$)} subroutine is merging the set $\Theta_r$ of interval sequences by averaging effective $k$ of all overlapping intervals across all interval sequences in the set $\Theta_r$}
    \label{fig:appendix:merge_intervals}
\end{figure}
%


%
~\cref{fig:appendix:mkc_thresholds_visualized} presents how do the merged intervals ($\bar{k}(\theta)$) look like for a few selected layers and heads on LLaMA2-7b model, calibrated using MKC on the \hell.
%
%%
%%%
\begin{figure}[h!]
\centering
\begin{subfigure}{0.98\textwidth}
    \includegraphics[width=\textwidth]{figures/multik_cumulative_calibration/mkc_pre_softmax.png}
    \caption{Pre-softmax}
    \label{fig:appendix:mkc_thresholds_visualized:pre}
\end{subfigure}
\begin{subfigure}{0.98\textwidth}
    \includegraphics[width=\textwidth]{figures/multik_cumulative_calibration/mkc_post_softmax.png}
    \caption{Post-softmax}
    \label{fig:appendix:mkc_thresholds_visualized:post}
\end{subfigure}
\caption{MKC calibration of LLaMA2-7b on \hell dataset. Right: the $\bar{k}_r(\theta)$ function as obtained from~\cref{alg:mkc} by merging the intervals for sequence length $r=800$. Middle: zoom into a range of interest of $\bar{k}_r^{-1}(k)$ around $k=64$, the selected threshold is marked on using $\theta_r(64)=\bar{k}_r^{-1}(64)$, Left: thresholds obtained from MKC for all sequence lengths (also other than $r=800$.}
\label{fig:appendix:mkc_thresholds_visualized}
\end{figure}
%%%
%%
%
%

%
It is worth mentioning that the main downside of the multi-k calibration is that it is very time and memory-consuming to perform all these interval mergings individually for every layer, head, and attention row id (i.e., sequence length). Speeding up the MergeIntervals routine, using reasonable approximations (such as subsampling) is an interesting research direction. The second slight downside is that it does not allow to apply top$_k$ at calibration, as we used to apply in~\cref{alg:calibrate}, since during MKC there is a multitude of potential $k$ parameters we are targeting. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{V-Mean Compensation}\label{appendix:vmc}
In this section, we provide a formal proof for our V-Mean Compensation (VMC)  serving as a good approximation of the sparsified $\attnVecSparsePost\Tilde{V}$ product to the full $\mathbf{s}V$ product.
\begin{lemma}
 Let $\attnVecPost\in[0,1]^\seqlen$ denote the post-softmax attention vector, and let $\attnVecSparsePost\in[0,1]^{\Tilde{k}}$ denote its thresholded variant, containing the $\Tilde{k}$ selected values of $\attnVecPost$, and let $\selids\in\left\{0,\dots, n-1\right\}^{\Tilde{k}}$ denote the indices of $\attnVecPost$ that surpassed the threshold such that $\forall i\in\{0,\dots, \Tilde{k}-1\}: \attnVecSparsePost_i=\attnVecPost_{\selids_i}$. Let $V\in\mathbb{R}^{n\times d}$ be the value matrix and let $\Tilde{V}\in\mathbb{R}^{\Tilde{k}\times d}$ be  consisting only of the selected V rows such that 
 $\forall i\in\{0,\dots, \Tilde{k}-1\}: \Tilde{V}_i=V_{\selids_i}$.
 We claim that in the expectation the full product $\bm{p}=\attnVecPost V\in\mathbb{R}^\headdim$ is equal to the thresholded attention plus the residual probability mass $\beta$ (\ref{eqn:vmc:beta}) multiplied by the mean-V row $\bm\mu$ (\ref{eqn:vmc:mu}). Namely,
\begin{align}
    \forall 0\le j<\headdim: E[\bm{p}_j] = (\bm{\Tilde{s}}\Tilde{V})_j + \beta\bm{\mu}_j
\end{align}
% Where $\beta\triangleq\sum_{i\in SEL}s_i$ and $\bm{\mu}_j = \frac{1}{\seqlen}\sum_{i=0}^{\seqlen-1}V_{ij}$.
%

%
\end{lemma}
\begin{proof}
\begin{equation}\label{eqn:vmc:proof}
    \begin{split}
    E[\bm{p}_j] &= E\Big[\sum_{i=0}^{\seqlen-1}s_iV_{ij}\Big] \\
    &= E\Big[\sum_{i\in\selids}s_i V_{ij}\Big] + E\Big[\sum_{i\in\notselids}s_i V_{ij}\Big] \\
    &= (\bm{\Tilde{s}}\Tilde{V})_j + \sum_{i\in\notselids}E[s_i V_{ij}] \\ 
    &\underset{s\ind V_j}{=} (\bm{\Tilde{s}}\Tilde{V})_j + \sum_{i\in\notselids}E[s_i]E[V_{ij}] \\ &\underset{s,V_j\text{uniform}}{=} (\bm{\Tilde{s}}\Tilde{V})_j + \sum_{i\in\notselids}\frac{\beta}{\seqlen-\numKept}\bm{\mu}_j \\
    &= (\bm{\Tilde{s}}\Tilde{V})_j + (\seqlen-\numKept)\frac{\beta}{\seqlen-\numKept}\bm{\mu}_j \\
    &= (\bm{\Tilde{s}}\Tilde{V})_j + \beta\bm{\mu}_j
    \end{split}
\end{equation}
%

%
\end{proof}
Under the following assumptions:

\begin{enumerate}
    \item $s\ind V_j$, that is the attention vector $\attnVecPost$ is statistically independent on the elements in the columns of matrix $V$. They are conditionally independent given the input $X$ from which they were originally computed (\ref{eqn:qkv}).
    \item  The distribution of $s_i, \forall i\in\notselids$ within the long tail of the non-selected indices is close to uniform and hence we can approximate its expectation by an average.
    \item The expectation of $V_{ij}$ can be approximated by its average.
\end{enumerate}


\section{Evaluation statistics}\label{appendix:evaluation_statistics}
In this section, we present again the experimental results from ~\cref{sec:evaluations:overall}; however, to demonstrate statistical significance, we show the error bars. This is important since every data point is aggregated using averaging across layers, heads, and test examples, as we will describe below. Therefore standard deviation of such an averaged metric is of interest. 
%

%
\cref{fig:appendix:evaluations:acc_kept_attn} focus on Q\& tasks, showing the tradeoff between the model's accuracy (y-axis) and the number of attention elements as selected by the \topk or \topth (x-axis). 
%
%
%
The standard deviation in the accuracy was provided by the LM-Eval evaluation harness, as a part of the standardized evaluation procedure. %
%
%
%
The average ratio presented on the x-axis and its standard deviation were computed over the following population: number of test samples $\times$ number of model layers $\times$ number of attention heads. To present the ratios, we first compute the absolute average and absolute standard deviation of the total count of attention elements and in the attention matrix, second - we normalize both the standard deviation and the average by the average number of attention elements when no sparsification took place.
%

%
\cref{fig:appendix:evaluations_generative:acc_kept_attn,fig:appendix:evaluations_generative:acc_kept_vrow} show the evaluation on \humaneval dataset. ~\cref{fig:appendix:evaluations_generative:acc_kept_attn} is similar to the Q\&A plots as it shows how the reduction in the number of attention elements during \textit{prefill phase} impacts the accuracy score (pass@1). \cref{fig:appendix:evaluations_generative:acc_kept_vrow} focuses on the generative \textit{decoding phase} and shows how the number of the needed V rows is affected by the elements that were selected in every row independently, which introduces the effect of GQA - since the indices of the selected elements are united across the heads in the group. The average and the standard deviation in this plot are taken across the following population of samples: number of test tasks $\times$ number of autoregressive forward passes $\times$ number of model layers $\times$ number of attention heads $\times$.
%
%%
%%%
\begin{figure}
\centering
%% 3 LLama2-7b plots
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-2-7b-hf_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA2-7b \arcc}
    \label{fig:appendix:evaluations:acc_kept_attn:llama2_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-2-7b-hf_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA2-7b \arce}
    \label{fig:appendix:evaluations:acc_kept_attn:llama2_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-2-7b-hf_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA2-7B \hell }
    \label{fig:appendix:evaluations:acc_kept_attn:llama2_hellaswag}
\end{subfigure}

%% 3 LLama3-8B plots
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-8B_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA3-8B \arcc}
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-8B_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA3-8B \arce}
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-8B_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA3-8B \hell }
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_hellaswag}
\end{subfigure}

%% 3 LLama3-70B plots
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-70B_Arc-Challenge_-_prefill_phase.eps}
    \caption{LLaMA3-70B \arcc}
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_70_arc_challenge}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-70B_Arc-Easy_-_prefill_phase.eps}
    \caption{LLaMA3-70B \arce}
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_70_arc_easy}
\end{subfigure}
\hfill
\begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-70B_Hellaswag_-_prefill_phase.eps}
    \caption{LLaMA3-70B \hell }
    \label{fig:appendix:evaluations:acc_kept_attn:llama3_70_hellaswag}
\end{subfigure}

\caption{\textbf{Prefill-based tasks} - Tradeoff between model accuracy averaged across test samples (y-axis), and the portion of kept attention elements per attention head (x-axis). All post-softmax \topk and \topth employ VMC, and all pre-softmax variants employ both VMC and exact SDC. Using the VMC and the SDC compensations achieves little if any accuracy degradation while achieving up to 10\texttimes\ reduction in the attention elements.}
\label{fig:appendix:evaluations:acc_kept_attn}
\end{figure}
%%%
%%
%

%
%%
%%%
\begin{figure}
\centering
\begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-8B-Instruct_Human-Eval_-_prefill_phase.eps}
    \caption{LLaMA3-8B-Instruct fraction of attention elements needed}
    \label{fig:appendix:evaluations_generative:acc_kept_attn:llama3_8b_humaneval}
\end{subfigure}
\hfill
\begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-70B-Instruct_Human-Eval_-_prefill_phase.eps}
    \caption{LLaMA3-70B-Instruct fraction of attention elements needed}
    \label{fig:appendix:evaluations_generative:acc_kept_attn:llama3_70b_humaneval}
\end{subfigure}
\caption{\textbf{Generative Task \humaneval - during prefill} - Tradeoff between model accuracy averaged across test samples (y-axis), and the portion of required attention elements per head (x-axis). The \topth variants employ threshold calibrated on \arcc dataset. All post-softmax \topk and \topth employ VMC, all pre-softmax variants employ both VMC and exact SDC}
\label{fig:appendix:evaluations_generative:acc_kept_attn}
\end{figure}
%%%
%%
%

%
%%
%%%
\begin{figure}
\centering
\begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-8B-Instruct_Human-Eval_-_generative_decoding_phase.eps}
    \caption{LLaMA3-8B-Instruct fraction of V-rows needed}
    \label{fig:appendix:evaluations_generative:acc_kept_vrow:llama3_8b_humaneval}
\end{subfigure}
\hfill
\begin{subfigure}{0.43\textwidth}
    \includegraphics[width=\textwidth]{figures/accuracy-kept_attn-kept_vrow-tradeoff/Llama-3-70B-Instruct_Human-Eval_-_generative_decoding_phase.eps}
    \caption{LLaMA3-70B-Instruct fraction of V-rows needed}
    \label{fig:appendix:evaluations_generative:acc_kept_vrow:llama3_70b_humaneval}
\end{subfigure}
\caption{\textbf{Generative Task \humaneval - during generative decoding} - Tradeoff between model accuracy averaged across test samples (y-axis), and the portion of required V-rows per group of heads (x-axis). The \topth variants employ threshold calibrated on \arcc dataset. All post-softmax \topk and \topth employ VMC, all pre-softmax variants employ both VMC and exact SDC}
\label{fig:appendix:evaluations_generative:acc_kept_vrow}
\end{figure}
%%%
%%
%

\section{Pre- vs. post-softmax sparsification}\label{appendix:pre-vs-post-softmax}
This section provides extended experiment plots comparing pure pre-softmax and post-softmax accuracy on Q\&A tasks. In all of them the post softmax consistently achieves higher scores.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.46\textwidth]{figures/pre-post.eps}
    \caption{Comparison of Pre- and post-softmax thresholding}
    \label{fig:appendix:pre-post}
\end{figure}

%
%%
%%%
\clearpage
\section{Thresholding different layers}\label{appendix:layers}
%%%
%%
%
In this appendix, we present more results that support the decision to use denser initial layers. That is to use higher $k$ for \topk or calibrating towards a higher $k$ in \topth. ~\cref{fig:appendix:highK} shows LLaMA2-7b model accuracy on Q\&A datasets, and LLaMA2-70b model accuracy for \hell dataset. The figure compares \topk and \topth using higher $k$ in the first 2 layers against using equal $k$ in all layers. All variants do not perform any compensations.

\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.98\textwidth]{figures/highK.eps} 
    \caption{The positive impact of keeping first two layers dense (higher k for calibration), compared to keeping equal k in all layers}  
    \label{fig:appendix:highK} 
\end{figure}
% \clearpage

%
%%
%%%
\section{Thresholding different attention rows}\label{appendix:evaluations:rows}
%%%
%%
%
\begin{figure}[h!]
    \centering 
    \includegraphics[width=0.46\textwidth]{figures/calibration-per-row.eps} 
    \caption{Calibrating per-attention-row thresholds vs a unified threshold for all rows (sequence lengths)}  
    \label{fig:appendix:calibration_per_row} 
\end{figure}

%
%%
%%%
\clearpage
\section{Impact of GQA}\label{appendix:evaluations:impactofgqa}
%%%
%%
%
In~\cref{fig:appendix:evaluations:gqa}, we show how the different attention sparsification approaches (\topk, \topth with and without CAPK) affect the number of required V rows. \topk (~\cref{fig:appendix:evaluations:gqa:topk}) guarantees exactly 128 selected elements per row of every head. However, the unified set of 4 heads in the group reaches only about 250, which indicates a certain agreement between the heads, which is mostly found in the recent tokens (as seen on the heatmap). We observed very similar characteristics in other heads and layers. \topth approach with capping the number of selected elements to at most 128 per head yielded degraded quality of the generated text since it mainly focused attention on the most recent tokens. Finally, the ordinary \topth in~\cref{fig:appendix:evaluations:gqa:topth}, which provided good quality results, does seem to exhibit a certain variability in the number of selected elements per group, sometimes selecting more than 128 per group.
\begin{figure}[h!]
\centering
\begin{subfigure}{0.63\textwidth}
    \includegraphics[width=\textwidth]{figures/v_row_popularity/heatmap_Top-k-pre-softmax---vmc---exact-sdc-k-128_e24_l2_g0.png}%.eps} eps times-out the latex compiler 
    \caption{\topk attention with $k=128$}
    \label{fig:appendix:evaluations:gqa:topk} 
\end{subfigure}

\begin{subfigure}{0.63\textwidth}
    \includegraphics[width=\textwidth]{figures/v_row_popularity/heatmap_Top-th-pre-softmax---capk---vmc---exact-sdc-k-128_e25_l2_g0.png}%.eps} eps times-out the latex compiler 
    \caption{\topth with CAPK, and $k=128$ }
    \label{fig:appendix:evaluations:gqa:topth_capk} 
\end{subfigure}
\begin{subfigure}{0.63\textwidth}
    \includegraphics[width=\textwidth]{figures/v_row_popularity/heatmap_Top-th-pre-softmax---vmc---exact-sdc-k-128_e25_l2_g0.png}%.eps} % eps times-out the latex compiler 
    \caption{\topth with $k=128$ without CAPK}
    \label{fig:appendix:evaluations:gqa:topth} 
\end{subfigure}
        
\caption{\textbf{Attention popularity mask} -- LLaMA-3-8B (GQA group size$=4$), \humaneval task number 25, generative decoding iterations as rows. Left -- heat map showing how many heads had the corresponding attention element in their Top-128; on the right -- the number of V-rows required to be used (enough 1 head in the group is enough to require a V row).}
\label{fig:appendix:evaluations:gqa}
\end{figure}

\end{document}
