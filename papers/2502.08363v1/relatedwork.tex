\section{Related Work}
\label{sec:related_work}
%%%
%%
%

%
In the line of work on the \textit{content-based sparsity}, the most fundamental approach is \topk attention~\cite{gupta2021memory}, which selects the  $k$ highest values in each row of the $\attnMatPre$ matrix. However, in practical kernel implementations, where the attention rows are tiled, it is highly difficult to implement the top-k search algorithm due to the full row dependency. Works such as Energon~\cite{zhou2022energon} went further and aimed at reducing the reads from the K-cached matrix by iteratively quantizing K, computing the $\attnMatPre$ matrix, and keeping the higher values. However, reducing access to K prohibits Energon from accurately computing the $\attnMatPost$ matrix, which is therefore as limited as not using our compensation methods in \topth. The SpAtten~\cite{wang2021spatten} and A\textsuperscript{3}~\cite{ham2020a3acceleratingattentionmechanisms} present \topk attention variants which require specialized hardware for accelerating the top-k search, which can be avoided by \topth. The SparQ~\cite{ribar2023sparq} work also relies on the top-k search, however, it performs a similar compensation to our VMC to compensate for the rows of V matrix, which did not participate in the SV product (~\ref{eqn:sv}) due to the sparsification of the attention. The Swin Transformer~\cite{liu2021swin} focuses on a vision transformer (i.e., no autoregressive decoding) applying a fixed sparsity pattern, where each token (patch) attends to its neighboring M\texttimes M patches.
%

%
Learned Token Pruning (LTP)~\cite{kim2022learned} is closely related to our work as it discards tokens in the attention matrix based on a model-trained threshold. While LTP benefits from the simplicity of the thresholding approach, it requires full model retraining. Our method shares the threshold concept but improves upon it by identifying an inductive threshold within an already-trained model. This makes our approach more appealing for large models, where training or fine-tuning can be prohibitively expensive. Similarly, the work of Lou~\etal~\cite{lou2024sparserfastermoreefficient} also targets the selection of top-k elements but requires retraining.
%

%
Numerous works focused on \textit{fixed attention} sparsity. For example, in Longformer~\cite{beltagy2020longformer}, only a few manually picked ``global attention'' tokens are allowed to attend to other tokens, whereas other tokens are restricted to the neighboring tokens in a dilated sliding window. Our work, on the other hand, capitalizes on identifying content-based sparsity in the attention matrix, as it offers higher accuracy gains. The work of Kim and Cho~\cite{kim2021lengthadaptivetransformertrainlength} requires retraining, although accommodates dynamic shortening of the sequence on demand at inference time.
%

% Weakly relevent paragraph, commented-out. Kept for a longer journal version
% An alternative line of attention reduction abandons the aforementioned structural sparsifications and turns to\textit{ attention transformations, factorizations, and low-rank approximations}. The work of Child \etal~\cite{child2019generating} exploits sparsity by factorizing the attention into products of smaller matrices, which fundamentally differs from our work and requires retraining the model. The Linformer~\cite{wang2020linformer} performs low-rank approximation of the Q and K matrices, which introduces compression errors at the source and also requires retraining. FNet~\cite{lee2021fnet} applied Fourier transform before performing the scaled dot product to avoid an overly sparse attention matrix, however, operating in the frequency domain requires significant retraining.
%

% Weakly relevent paragraph, commented-out. Kept for a longer journal version
% An orthogonal line of works aimed at KV-cache compression. For example, Scissorhands~\cite{liu2024scissorhands} reduces the memory requirements of a KV cache by applying thresholding to determine which tokens are to be stored at the KV cache. Differently from our work, the KV cache compression methods aim at reducing the KV cache (memory and bandwidth), at the risk of information loss. Our work assumes the full KV-cache availability in the memory. Nevertheless, a smart reordering of the cache rows can help mitigate the memory capacity deficiency.
%
%%
%%%
\vspace{-1mm}