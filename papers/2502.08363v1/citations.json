[
  {
    "index": 0,
    "papers": [
      {
        "key": "gupta2021memory",
        "author": "Ankit Gupta and\nGuy Dar and\nShaya Goodman and\nDavid Ciprut and\nJonathan Berant",
        "title": "Memory-efficient Transformers via Top-k Attention"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhou2022energon",
        "author": "Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and Sun, Guangyu",
        "title": "Energon: Toward efficient acceleration of transformers using dynamic sparse attention"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2021spatten",
        "author": "Wang, Hanrui and Zhang, Zhekai and Han, Song",
        "title": "{SpAtten}: Efficient sparse attention architecture with cascade token and head pruning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ham2020a3acceleratingattentionmechanisms",
        "author": "Tae Jun Ham and Sung Jun Jung and Seonghak Kim and Young H. Oh and Yeonhong Park and Yoonho Song and Jung-Hun Park and Sanghee Lee and Kyoung Park and Jae W. Lee and Deog-Kyoon Jeong",
        "title": "A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ribar2023sparq",
        "author": "Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas",
        "title": "{SparQ} attention: Bandwidth-efficient llm inference"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2021swin",
        "author": "Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining",
        "title": "Swin transformer: Hierarchical vision transformer using shifted windows"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kim2022learned",
        "author": "Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt",
        "title": "Learned token pruning for transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lou2024sparserfastermoreefficient",
        "author": "Chao Lou and Zixia Jia and Zilong Zheng and Kewei Tu",
        "title": "Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Iz Beltagy and Matthew E. Peters and Arman Cohan",
        "title": "Longformer: The Long-Document Transformer"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "kim2021lengthadaptivetransformertrainlength",
        "author": "Gyuwan Kim and Kyunghyun Cho",
        "title": "Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "child2019generating",
        "author": "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya",
        "title": "Generating long sequences with sparse transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "lee2021fnet",
        "author": "Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago",
        "title": "Fnet: Mixing tokens with fourier transforms"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "liu2024scissorhands",
        "author": "Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali",
        "title": "Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time"
      }
    ]
  }
]