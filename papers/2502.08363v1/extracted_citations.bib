@article{beltagy2020longformer,
  author       = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{gupta2021memory,
  author       = {Ankit Gupta and
                  Guy Dar and
                  Shaya Goodman and
                  David Ciprut and
                  Jonathan Berant},
  title        = {Memory-efficient Transformers via Top-k Attention},
  journal      = {CoRR},
  volume       = {abs/2106.06899},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.06899},
  eprinttype    = {arXiv},
  eprint       = {2106.06899},
  timestamp    = {Fri, 27 May 2022 16:07:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-06899.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ham2020a3acceleratingattentionmechanisms,
      title={A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation}, 
      author={Tae Jun Ham and Sung Jun Jung and Seonghak Kim and Young H. Oh and Yeonhong Park and Yoonho Song and Jung-Hun Park and Sanghee Lee and Kyoung Park and Jae W. Lee and Deog-Kyoon Jeong},
      year={2020},
      eprint={2002.10941},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2002.10941}, 
}

@misc{kim2021lengthadaptivetransformertrainlength,
      title={Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search}, 
      author={Gyuwan Kim and Kyunghyun Cho},
      year={2021},
      eprint={2010.07003},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.07003}, 
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@article{lee2021fnet,
  title={Fnet: Mixing tokens with fourier transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{lou2024sparserfastermoreefficient,
      title={Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers}, 
      author={Chao Lou and Zixia Jia and Zilong Zheng and Kewei Tu},
      year={2024},
      eprint={2406.16747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16747}, 
}

@article{ribar2023sparq,
  title={{SparQ} attention: Bandwidth-efficient llm inference},
  author={Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas},
  journal={arXiv preprint arXiv:2312.04985},
  year={2023}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@inproceedings{wang2021spatten,
  title={{SpAtten}: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@article{zhou2022energon,
  title={Energon: Toward efficient acceleration of transformers using dynamic sparse attention},
  author={Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and Sun, Guangyu},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={42},
  number={1},
  pages={136--149},
  year={2022},
  publisher={IEEE}
}

