@book{duff2017direct,
  title={Direct methods for sparse matrices},
  author={Duff, Iain S and Erisman, Albert Maurice and Reid, John Ker},
  year={2017},
  publisher={Oxford University Press}
}

@misc{gao2024lm,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 07,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v0.4.3},
  doi          = {10.5281/zenodo.12608602},
  url          = {https://zenodo.org/records/12608602}
}
@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}
@misc{chen2021codex,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{zhou2022energon,
  title={Energon: Toward efficient acceleration of transformers using dynamic sparse attention},
  author={Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and Sun, Guangyu},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={42},
  number={1},
  pages={136--149},
  year={2022},
  publisher={IEEE}
}

@misc{wang2022languagemodelarchitecturepretraining,
      title={What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?}, 
      author={Thomas Wang and Adam Roberts and Daniel Hesslow and Teven Le Scao and Hyung Won Chung and Iz Beltagy and Julien Launay and Colin Raffel},
      year={2022},
      eprint={2204.05832},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.05832}, 
}

@article{gupta2021memory,
  author       = {Ankit Gupta and
                  Guy Dar and
                  Shaya Goodman and
                  David Ciprut and
                  Jonathan Berant},
  title        = {Memory-efficient Transformers via Top-k Attention},
  journal      = {CoRR},
  volume       = {abs/2106.06899},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.06899},
  eprinttype    = {arXiv},
  eprint       = {2106.06899},
  timestamp    = {Fri, 27 May 2022 16:07:27 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-06899.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{beltagy2020longformer,
  author       = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  title        = {Longformer: The Long-Document Transformer},
  journal      = {CoRR},
  volume       = {abs/2004.05150},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.05150},
  eprinttype    = {arXiv},
  eprint       = {2004.05150},
  timestamp    = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2021spatten,
  title={{SpAtten}: Efficient sparse attention architecture with cascade token and head pruning},
  author={Wang, Hanrui and Zhang, Zhekai and Han, Song},
  booktitle={2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages={97--110},
  year={2021},
  organization={IEEE}
}

@article{ribar2023sparq,
  title={{SparQ} attention: Bandwidth-efficient llm inference},
  author={Ribar, Luka and Chelombiev, Ivan and Hudlass-Galley, Luke and Blake, Charlie and Luschi, Carlo and Orr, Douglas},
  journal={arXiv preprint arXiv:2312.04985},
  year={2023}
}

@article{liu2024scissorhands,
  title={Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time},
  author={Liu, Zichang and Desai, Aditya and Liao, Fangshuo and Wang, Weitao and Xie, Victor and Xu, Zhaozhuo and Kyrillidis, Anastasios and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{lee2021fnet,
  title={Fnet: Mixing tokens with fourier transforms},
  author={Lee-Thorp, James and Ainslie, Joshua and Eckstein, Ilya and Ontanon, Santiago},
  journal={arXiv preprint arXiv:2105.03824},
  year={2021}
}

@inproceedings{kim2022learned,
  title={Learned token pruning for transformers},
  author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={784--794},
  year={2022}
}

@misc{shi2024costdownreviewmethods,
  title={Keep the Cost Down: A Review on Methods to Optimize LLM's KV-Cache Consumption}, 
  author={Luohe Shi and Hongyi Zhang and Yao Yao and Zuchao Li and Hai Zhao},
  year={2024},
  eprint={2407.18003},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2407.18003}, 
}

@article{ainslie2023gqa,
  title={{GQA}: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{su2024roformer,
title = {RoFormer: Enhanced transformer with Rotary Position Embedding},
journal = {Neurocomputing},
volume = {568},
pages = {127063},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127063},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223011864},
author = {Jianlin Su and Murtadha Ahmed and Yu Lu and Shengfeng Pan and Wen Bo and Yunfeng Liu}
}

@misc{kim2021lengthadaptivetransformertrainlength,
      title={Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search}, 
      author={Gyuwan Kim and Kyunghyun Cho},
      year={2021},
      eprint={2010.07003},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.07003}, 
}

@misc{wang2024limitssurveytechniquesextend,
      title={Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models}, 
      author={Xindi Wang and Mahsa Salmani and Parsa Omidi and Xiangyu Ren and Mehdi Rezagholizadeh and Armaghan Eshaghi},
      year={2024},
      eprint={2402.02244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.02244}, 
}

@article{fuad2023survey,
  title={A survey on sparsity exploration in transformer-based accelerators},
  author={Fuad, Kazi Ahmed Asif and Chen, Lizhong},
  journal={Electronics},
  volume={12},
  number={10},
  pages={2299},
  year={2023},
  publisher={MDPI}
}

@misc{lou2024sparserfastermoreefficient,
      title={Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers}, 
      author={Chao Lou and Zixia Jia and Zilong Zheng and Kewei Tu},
      year={2024},
      eprint={2406.16747},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16747}, 
}

@misc{vig2019analyzingstructureattentiontransformer,
      title={Analyzing the Structure of Attention in a Transformer Language Model}, 
      author={Jesse Vig and Yonatan Belinkov},
      year={2019},
      eprint={1906.04284},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1906.04284}, 
}

@misc{nahshan2024linearlognormalattentionunbiased,
      title={Linear Log-Normal Attention with Unbiased Concentration}, 
      author={Yury Nahshan and Joseph Kampeas and Emir Haleva},
      year={2024},
      eprint={2311.13541},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.13541}, 
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@misc{ham2020a3acceleratingattentionmechanisms,
      title={A$^3$: Accelerating Attention Mechanisms in Neural Networks with Approximation}, 
      author={Tae Jun Ham and Sung Jun Jung and Seonghak Kim and Young H. Oh and Yeonhong Park and Yoonho Song and Jung-Hun Park and Sanghee Lee and Kyoung Park and Jae W. Lee and Deog-Kyoon Jeong},
      year={2020},
      eprint={2002.10941},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2002.10941}, 
}

@misc{ge2024modeltellsdiscardadaptive,
      title={Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs}, 
      author={Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
      year={2024},
      eprint={2310.01801},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.01801}, 
}

@inproceedings{keles2023computational,
  title={On the computational complexity of self-attention},
  author={Keles, Feyza Duman and Wijewardena, Pruthuvi Mahesakya and Hegde, Chinmay},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={597--619},
  year={2023},
  organization={PMLR}
}

@article{ye2024differential,
  title={Differential transformer},
  author={Ye, Tianzhu and Dong, Li and Xia, Yuqing and Sun, Yutao and Zhu, Yi and Huang, Gao and Wei, Furu},
  journal={arXiv preprint arXiv:2410.05258},
  year={2024}
}

@article{
huang2024quantization,
title={Quantization Variation: A New Perspective on Training Transformers with Low-Bit Precision},
author={Xijie Huang and Zhiqiang Shen and Pingcheng Dong and Kwang-Ting Cheng},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=MHfoA0Qf6g},
note={}
}

@article{tang2024quest,
  title={Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
  author={Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
  journal={arXiv preprint arXiv:2406.10774},
  year={2024}
}

@inproceedings{dao2022flashattention,
 author = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {16344--16359},
 publisher = {Curran Associates, Inc.},
 title = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},

 volume = {35},
 year = {2022}
}

@inproceedings{zhang2023paralleltopk,
author = {Zhang, Jingrong and Naruse, Akira and Li, Xipeng and Wang, Yong},
title = {Parallel Top-K Algorithms on GPU: A Comprehensive Study and New Methods},
year = {2023},
isbn = {9798400701092},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581784.3607062},
doi = {10.1145/3581784.3607062},
abstract = {The top-K problem is an essential part of many important applications in scientific computing, information retrieval, etc. As data volume grows rapidly, high-performance parallel top-K algorithms become critical. We propose two parallel top-K algorithms, AIR Top-K (Adaptive and Iteration-fused Radix Top-K) and GridSelect, for GPU. AIR Top-K employs an iteration-fused design to minimize CPU-GPU communication and device data access. Its adaptive strategy eliminates unnecessary device memory traffic automatically under various data distributions. GridSelect can process data on-the-fly. It adopts a shared queue and parallel two-step insertion to decrease the frequency of costly operations. We comprehensively compare 8 open-source GPU implementations and our methods for a wide range of problem sizes and data distributions. For batch sizes 1 and 100, respectively, AIR Top-K shows 1.98--21.48\texttimes{} and 8.01--574.78\texttimes{} speedup over previous radix top-K algorithm, and 1.44--7.34\texttimes{} and 1.38--31.91\texttimes{} speedup over state-of-the-art methods. GridSelect shows up to 882.29\texttimes{} speedup over its baseline.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {76},
numpages = {13},
keywords = {top-K, K-selection, radix select, parallel algorithms, GPU, CUDA},
location = {Denver, CO, USA},
series = {SC '23}
}

@misc{miao2023efficientgenerativelargelanguage,
      title={Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems}, 
      author={Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Hongyi Jin and Tianqi Chen and Zhihao Jia},
      year={2023},
      eprint={2312.15234},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.15234}, 
}  

