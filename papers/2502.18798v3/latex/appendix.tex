\clearpage
\appendix
\section{Benchmarks used in Experiments}
\label{sec:app-benchmark}
In this paper, we perform experiments using seven multiple-choice benchmarks. The experiments are conducted with the Language Model Evaluation Harness~\cite{eval-harness}, and we follow the prompt and choice structures outlined by this library. Below, we provide detailed descriptions, evaluation templates, and examples of the benchmarks. Each template and example uses a monospaced font to indicate parts that vary between examples.

\subsection{Hellaswag}
HellaSwag~\cite{hellaswag} is a benchmark for evaluating commonsense natural language inference (NLI). The task involves selecting the most appropriate continuation of a given sentence. We use the validation set, which consists of 10,042 examples, for our experiment.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}
\hline
\textbf{[Template]} \\
\textbf{Prompt}: \texttt{activity\_label}: \texttt{ctx\_a} \texttt{ctx\_b} \\
\textbf{Choices}: [\texttt{endings1}, \texttt{endings2}, \texttt{endings3}, \texttt{endings4}] \\ \hline
\textbf{[Example]} \\
\textbf{Prompt}: \texttt{Clean and jerk}: \texttt{A lady walks to a barbell. She bends down and grabs the pole. The lady} \\
\textbf{Choices}: [\\
\hspace*{2em}\texttt{swings and lands in her arms.},\\
\hspace*{2em}\texttt{pulls the barbell forward.},\\
\hspace*{2em}\texttt{pulls a rope attached to the barbell.},\\
\hspace*{2em}\texttt{stands and lifts the weight over her head.}\\
    ] \\ \hline
\end{tabular}
\end{small}
\end{table}

\subsection{PiQA}

Physical Interaction: Question Answering(PiQA)~\cite{bisk2020piqa} is a benchmark to evaluate whether a model can answer questions based on physical commonsense knowledge. PiQA focuses on everyday situations with a preference for atypical solutions, and each question has two options. The validation set used for our evaluation consists of 1,838 questions.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}
\hline
\textbf{[Template]} \\
\textbf{Prompt}: Question: \texttt{goal}\\Answer: \\
\textbf{Choices}: [\texttt{sol1}, \texttt{sol2}] \\ 
\hline \textbf{[Example]} \\
\textbf{Prompt}: Question: \texttt{To fight Ivan Drago in Rocky for sega master system.} \\Answer:\\
\textbf{Choices}: [ \\
\hspace*{2em}\texttt{Drago isn't in this game because it was released before Rocky IV.}, \\
\hspace*{2em}\texttt{You have to defeat Apollo Creed and Clubber Lang first.} \\
] \\ \hline
\end{tabular}
\end{small}
\end{table}

\subsection{ARC}

The AI2 Reasoning Challenge(ARC)~\cite{clark2018think} comprises science questions and answers targeted at students from grade 3 to grade 9. It is divided into two difficulty levels: \textit{easy} and \textit{challenge}. For model evaluation, we use the test sets for both difficulty levels. The ARC-Easy test set includes 2,376 questions, while the ARC-Challenge test set contains 1,172 questions.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}  % Single column for better alignment
\hline
\textbf{[Template]} \\
\textbf{Prompt}: Question: \texttt{question}\\Answer: \\
\textbf{Choices}: [\texttt{choices1}, \texttt{choices2}, \texttt{choices3}, \texttt{choices4}] \\ 
\hline \textbf{[Example]} \\
\textbf{Prompt}: Question: \texttt{Which piece of safety equipment is used to keep mold spores from entering the respiratory system?} \\Answer: \\
\textbf{Choices}: [ \texttt{safety goggles}, \texttt{breathing mask}, \texttt{rubber gloves}, \texttt{lead apron}] \\ \hline
\end{tabular}
\end{small}
\end{table}

\subsection{LogiQA}

Logical Reasoning Question Answering(LogiQA)~\cite{liu2021logiqa} is a benchmark designed to assess a model's logical reasoning abilities. It consists of expert-written questions that cover multiple types of deductive reasoning. In our experiments, we use a test set of 651 problems.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}\hline
\textbf{[Template]} \\
\textbf{Prompt}: Passage: \texttt{context}\\
Question: \texttt{question} \\
Choices: \\
A: \texttt{option1}\\
B: \texttt{option2}\\
C: \texttt{option3}\\
D: \texttt{option4}\\
Answer: \\
\textbf{Choices}: [\texttt{option1}, \texttt{option2}, \texttt{option3}, \texttt{option4}] \\
\hline \textbf{[Example]} \\
\textbf{Prompt}: Passage: \texttt{There are five teams participating in the game. The audience had the following comments on the results? (1) The champion is either the Shannan team or the Jiangbei team. (2) The champion is neither Shanbei nor Jiangnan. (3) The champion is Jiangnan Team. (4) The champion is not the Shannan team.} \\
Question: \texttt{The result of the match showed that only one argument was correct, so who won the championship?}
\\Choices:\\A. \texttt{Shannan}\\B. \texttt{Jiangnan}\\C.                \texttt{Shanbei}\\D. \texttt{Jiangbei}\\Answer:\\
\textbf{Choices}: [\texttt{Shannan}, \texttt{Jiangnan}, \texttt{Shanbei}, \texttt{Jiangbei}] \\ \hline
\end{tabular}
\end{small}
\end{table}

\newpage
\subsection{RACE}

ReAding Comprehension dataset from Examinations(RACE)~\cite{lai2017race} is an English reading comprehension dataset derived from China's middle and high school English exam questions. Each question comprises an article followed by several questions and answer choices. For our evaluation, the test set contains 1,045 questions. We include all but the final question from each set in the prompt, ensuring that most of the context is part of the model's input.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}
\hline
\textbf{[Template]} \\
\textbf{Prompt}: Article: \texttt{article} \\
Question: \texttt{problem1}\\
Answer: \texttt{answer1}\\
...\\
Question: \texttt{problem\_last}\\
\textbf{Choices}: [\texttt{option1}, \texttt{option2}, \texttt{option3}, \texttt{option4}] \\
\hline \textbf{[Example]} \\
\textbf{Prompt}: Article: \texttt{A girl with blue eyes is a blue-eyed girl. $\dots$ There are sound-proof rooms in all broadcasting stations.}\\Question: \texttt{The clothes which you buy from the supermarket are called \_ clothes.}\\Answer: \texttt{ready-made} \\$\dots$ \\ \texttt{What do you think is the best title for the article?} \\
\textbf{Choices}: [ \\
\hspace*{2em}\texttt{The Forms of Compound Words.}, \\
\hspace*{2em}\texttt{Compound Words in Everyday Life}, \\
\hspace*{2em}\texttt{How to Use Compound Words.}, \\
\hspace*{2em}\texttt{Water-proof Cloth in the Best.} \\
] \\ \hline
\end{tabular}
\end{small}
\end{table}

\subsection{SciQ}

Scientific Question Answering(SciQ)~\cite{welbl2017crowdsourcing} is a dataset of science exam questions crowd-sourced across domains such as Physics, Chemistry, and Biology. Each question includes a question, answer choices, and a paragraph of supporting information to assist reasoning. For our evaluation, we use a test set comprising 1,000 questions.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}
\hline \textbf{[Template]} \\
\textbf{Prompt}: \texttt{support}\\
Question: \texttt{question}
\\Answer: \\
\textbf{Choices}: [\texttt{distractor1}, \texttt{distractor2}, \texttt{distractor3}, \texttt{correct\_answer}] \\
\hline \textbf{[Example]} \\
\textbf{Prompt}: \texttt{Tree rings, ice cores, and varves indicate the environmental conditions at the time they were made.}\\Question: \texttt{Ice cores, varves and what else indicate the environmental conditions at the time of their creation?}\\Answer: \\
\textbf{Choices}: [\texttt{mountain ranges}, \texttt{fossils}, \texttt{magma}, \texttt{tree rings}] \\ \hline
\end{tabular}
\end{small}
\end{table}

\newpage
\subsection{MMLU}

Massive Multitask Language Understanding(MMLU)~\cite{hendrycks2020measuring} evaluates a model's breadth and depth of knowledge across various domains. The dataset covers 57 topics, including STEM, humanities, and social sciences. Our experiments use the comprehensive test set, which contains 14,042 questions. Each multiple-choice question assesses the model's ability to integrate diverse knowledge.

\begin{table}[H]
\centering
\begin{small}
\begin{tabular}{|p{7cm}|}
\hline
\textbf{[Template]} \\
\textbf{Prompt}: \texttt{question}\\
A. \texttt{choice1} \\
B. \texttt{choice2} \\
C. \texttt{choice3} \\
D. \texttt{choice4} \\
Answer: \\
\textbf{Choices}: [A, B, C, D] \\
\hline \textbf{[Example]} \\
\textbf{Prompt}: \texttt{The following are multiple choice questions (with answers) about astronomy.}\\ \texttt{What is the second most common element in the solar system?}\\A. \texttt{Iron}\\B. \texttt{Hydrogen}\\C. \texttt{Methane}\\D. \texttt{Helium}\\Answer: \\
\textbf{Choices}: [A, B, C, D] \\ \hline
\end{tabular}
\end{small}
\end{table}

% data statistics (B6)

\section{Effects of Altering Choices}
\label{sec:hi_experiment}

{To further investigate the impact of {\small $P(Choice)$} on the model performance, we modify the choices for each problem and examine how these changes affect the model performance. 
We append the sentence "Hi" as the answer choice in each question. The sentence "Hi" appears frequently in various text data, resulting in a high prior probability \mbox{$P(\text{"Hi"})$}. However, since choices like "Hi" are unrelated to the prompt, the model's performance should remain stable if it truly relies on prompt understanding rather than \mbox{{\small $P(Choice)$}} alone. If such a choice affects the model's decisions, this would indicate that \mbox{\small $P(Choice)$} plays a significant role in the model's decision-making. We expect that appending a choice with a high prior probability, such as "Hi," will lead to cases where the model incorrectly selects this option over the correct one. To verify it, we perform an experiment using three instruction-tuned language models: Mistral-7B(version 0.3), Gemma-7B, and LLaMA3.1-8B, with three downstream tasks: Hellaswag, Arc-easy, and SciQ. The results are summarized in Table\mbox{~\ref{table:accuracy_metrics}}.}

\begin{table}[ht]
\centering
\caption{{The rate of answer change in the model's final decision on choices when the additional choice "Hi" is introduced.}}
\vspace{-0.5\baselineskip} % ACL Only
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{l|c|c|c}
\hline
\multirow{2}{*}{Model} & \textbf{Hellaswag} & \textbf{Arc-e} & \textbf{SciQ} \\
\cline{2-4} & Rate & Rate & Rate \\
\hline
\hline
Mistral-7B   & 94.90\%    & 19.19\%   &0.80\%   \\
Gemma-7B     & 94.88\%    & 25.25\%   & 2.20\%   \\
LLaMA3.1-8B    & 95.10\%    & 24.20\%   & 0.90\%   \\
\hline
\end{tabular}
}
\label{table:accuracy_metrics}
\end{table}

{Table\mbox{~\ref{table:accuracy_metrics}} shows that the model's final choice decisions are affected across all benchmarks when the additional choice is introduced. For SciQ, the effect is relatively minor, with the rate of answer change in the model's final decision ranging from 0.8\% to 2.2\%. 
% {\bf === what is the answer change rate ===} 
The log prior probability difference has less impact on performance than the PMI difference. However, in the case of Hellaswag, 94.88\% to 95.10\% of the model's decisions are changed due to the addition of the choice, demonstrating how significantly the prior probability difference can influence the model's decisions. The results demonstrate that \mbox{\small $P(Choice)$} substantially affects model performance depending on the benchmark.}


\section{Examples where ANPMI differs from Other Metrics}
\label{sec:comp_example}

{The following examples illustrate cases where ANPMI produces different outcomes than other metrics. These examples are derived from evaluations of HellaSwag\hbox{~\cite{hellaswag}} using Instruct-tuned LLaMA 3.1-8B\hbox{~\cite{dubey2024llama}}.}

{The first case (see Figure\hbox{~\ref{fig:example_case1}}) is a scenario where the correct choice has a long sequence length and a low prior probability (\hbox{$P(Choice)$}) value. In this case, selecting an answer based on \hbox{$P(Choice)$} or \hbox{$\log P(Choice | Prompt)$} often favors the shortest available option, such as Choice 2, which could lead to incorrect predictions. However, metrics that incorporate normalization based on sequence length or \hbox{$P(Choice)$}, such as length-normalized \hbox{$\log P(Choice | Prompt)$}, PMI, and ANPMI, can identify the correct answer: Choice 4.}

{The second case (see Figure\hbox{~\ref{fig:example_case2}}) is a scenario where the correct choice has a short sequence length and a high \hbox{$P(Choice)$} value. In this scenario, length-normalized \hbox{$\log P(Choice | Prompt)$} and PMI fail to select the correct answer, Choice 2. When these methods are applied to candidates with much longer sequence lengths or higher \hbox{$P(Choice)$} values, they tend to overcompensate for these factors, leading to unintended biases that prevent the correct answer from being selected. In contrast, ANPMI maintains the maximum value for each choice at 1, thereby accurately selecting Choice 2.}

{It is important to note that the advantage of ANPMI lies in its ability to evaluate model performance with a stronger emphasis on understanding the $Prompt$ rather than merely yielding higher performance measurements. In the two cases presented, certain biases inherent to other metrics prevent the model from selecting the correct answer. However, these same biases could also, in some cases, help the model select the correct answer even when it has misinterpreted the $Prompt$. Thus, ANPMI's role is not necessarily to enhance measured performance but to provide a more reliable evaluation framework.}

\begin{figure*}[htbp]
   \centering
   \includegraphics[width=1.0\linewidth]{figure/4_example1.pdf}
   \vspace*{-1\baselineskip}
   \caption{A case where the correct answer has a long sequence length and low prior probability. The model may select incorrect answers without normalization due to its wrong bias for choice.}
\label{fig:example_case1}
\end{figure*}

\begin{figure*}[htbp]
   \centering
   \includegraphics[width=1.0\linewidth]{figure/4_example2.pdf}
   \vspace*{-1\baselineskip}
   \caption{A case where the correct answer has a short sequence length and high prior probability. The model may select incorrect answers due to the wrong normalization method.}
\label{fig:example_case2}
\end{figure*}