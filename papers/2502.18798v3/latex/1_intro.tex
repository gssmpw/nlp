\section{Introduction} 
Suppose that a man/woman answers a multiple-choice question, and the answer is correct. Could he truly solve the problem if he only looked at the options and guessed? It would not accurately reflect his ability or understanding that was intended to be assessed by the question. 

A similar issue arises when we evaluate a language model. Currently, the natural language understanding capability of the model is often assessed using multiple choice questions~\citep{achiam2023gpt, team2023gemini, jiang2024mixtral, dubey2024llama}. The performance of the model is measured by how frequently it selects the correct answer, based on the probability {\small $P(Choice|Prompt)$} - the likelihood that the model will generate a given choice in response to the prompt. However, this method overlooks whether the decision is based on a genuine understanding of the prompt and only considers which choice the model ultimately selects. As a result, the model may sometimes end up selecting an answer as if it were only looking at the choices without seeing the question.

% \begin{figure}[!t]
%     \centering
%     \includegraphics[width=0.7\linewidth]{figure/1_intro.pdf}
%     \vspace*{-0.5\baselineskip}
%     \caption{When a model selects an answer solely based on the choices without understanding the question, accurately assessing its comprehension of the problem becomes difficult.}
%     \label{fig:example}
% \end{figure}

The options in multiple-choice questions consist of diverse sentences, and the language model is not trained to generate these sentences with equal probabilities without a given prompt. It is a natural phenomenon for the language model, but it may lead to performance measurements that do not accurately reflect the model's understanding of the prompt. For example, the model might select the correct choice $c$ because {\small $P(c)$} is much higher than others, leading to overestimating the model's actual performance. Conversely, it might choose an incorrect option if the correct choice has a lower probability, leading to an underestimation of its performance. 

To assess the model's actual ability to understand the given multiple-choice question and answer it correctly, it is important to equalize the generation probabilities of each answer choice. However, modifying the language model to deal with this problem is not only complicated but also sabotages the process of assessing the model's performance. Adjusting the answer choices in benchmarks is not a practical solution either, as finding suitable alternatives is challenging and could limit the diversity of the tasks, restricting the evaluation of the model's ability.

Instead of relying on {\small $P(Choice|Prompt)$}, alternative methods are often used to determine the selection by the model. For example, in benchmarks like Hellaswag~\cite{hellaswag}, the model's performance is usually measured by normalizing {\small $P(Choice|Prompt)$} based on the length of the choice~\citep{eval-harness, zhang2024tinyllama}, addressing the probability imbalance caused by the varying lengths of the choices. Another approach involves calculating mutual information to measure the dependence between the choice and the prompt~\cite{eval-harness}. However, these methods do not completely solve the issue stated above regarding the probability imbalance between choices.

This paper analyzes the impact of the imbalance in {\small $P(Choice)$} on language model performance and confirms the importance of addressing the issue. We propose a method to measure model performance by normalizing the Pointwise Mutual Information (PMI) between the prompt and choice using {\small $-\log P(Choice)$} to assess the model's actual understanding of the prompt. Our approach is theoretically less affected by the imbalance in {\small $P(Choice)$}. Using various pre-trained models and benchmarks, we show that the proposed method more accurately evaluates the understanding of prompts by the model than existing approaches.