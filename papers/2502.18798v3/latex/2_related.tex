\section{Related Work}
% In Deep Learning, objective functions, such as Mean Squared Error (MSE) and Cross-Entropy, are commonly optimized to train models effectively. However, these functions may not truly represent the quality of outcomes, such as the perceptual quality of generated images or a model's true language understanding capabilities. To address this issue, researchers have focused on developing diverse benchmarks~\cite{rajpurkar2016squad, sarlin2020superglue} and evaluation metrics~\cite{zhang2018unreasonable, ding2020image, ren2023c} that better align with human judgment and application-specific needs. 

In natural language processing (NLP), {multiple-choice questions (MCQs) are widely used as a benchmarking method for model evaluation.} The most common approach to evaluate generative language models involves measuring the likelihood of generating correct answers based on specific prompts. However, this method is sensitive to the choice of prompts, which can lead to substantial outcome variations and heavily affect measured performance. As a result, many studies have investigated techniques to identify prompts that most accurately reflect a model's language understanding capabilities~\cite{webson2021prompt, wei2022chain, leidinger2023language}. 

{Recent studies analyze language models' true comprehension by testing whether they can answer correctly with partial prompt information. Some studies have shown that model performance can be assessed using partial information from the prompt}{~\citep{belinkov2019don, gururangan2018annotation, poliak2018hypothesis, feng2019misleading, srikanth2022partial}.{ This demonstrates that problems can often be solved without fully understanding the prompt. Furthermore, \hbox{\citet{balepur2024artifacts}} have revealed that models can solve problems even when the original prompt information is excluded, relying solely on answer choices. These findings highlight the challenges of accurately measuring a model's understanding of prompts.}

{In contrast to previous studies, which mainly focus on prompts, our study examines how answer choice design influences the evaluation of models. By removing all information about other answer choices and additional prompt details, we isolate each choice to assess its impact. This approach reveals that a modelâ€™s inherent preference for specific choices significantly affects its decisions, even when prompts are provided. This paper explores the role of answer choice probability biases in language model assessment and proposes effective methods to address these biases, improving the reliability of evaluations.}

% However, our observation indicates that prompt selection and answer choice design significantly influence evaluating the language model's capabilities. This paper examines how the aspects of answer choices impact the assessment of language models and proposes effective methods to address the challenge.