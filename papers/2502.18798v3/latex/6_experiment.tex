\begin{table*}[!t]
\centering
\caption{Model performance when no prompt is provided.}
\label{tab:performance_comparison_without_prompt}
\vspace{-0.2\baselineskip} % ACL Only
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
% \textbf{Metric} & \textbf{Model} & \textbf{Hellaswag} & \textbf{Arc-c} & \textbf{LogiQA} & \textbf{MMLU} \\ \hline \hline
\textbf{Metric} & \textbf{Model} & \textbf{Hellaswag} & \textbf{PiQA} & \textbf{ARC-e} & \textbf{ARC-c} & \textbf{LogiQA} & \textbf{RACE} & \textbf{SciQ} & \textbf{MMLU} \\ \hline \hline
% Random          & -              & 25/25/26/24\%       & 23/27/26/24\%  & 20/24/28/28\%     & 23/25/25/27\% & & & \\ \hline
\multirow{4}{*}{Random} & label 0 & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.08\% & 25.93\% & 0.00\% & 22.95\% \\
                        & label 1 & 24.75\% & 50.49\% & 24.62\% & 26.54\% & 24.42\% & 24.78\% & 0.00\% & 24.65\% \\
                        & label 2 & 25.73\% & - & 26.64\% & 26.45\% & 27.50\% & 25.93\% & 0.00\% & 25.51\% \\
                        & label 3 & 24.48\% & - & 23.61\% & 24.32\% & 27.80\% & 23.35\% & 100\% & 26.89\% \\ \hline
\multirow{3}{*}{Acc} 
                & Mistral-7B & 46.22\% & 71.65\% & 35.06\% & 22.70\% & 19.35\% & 23.92\% & 27.50\% & 22.95\% \\ \cline{2-10}
                & Gemma-7B & 40.79\% & 67.79\% & 33.00\% & 23.72\% & 19.66\% & 25.55\% & 24.60\% & 22.95\% \\ \cline{2-10}
                & LLaMA3.1-8B & 43.24\% & 71.60\% & 35.23\% & 24.06\% & 19.35\% & 24.21\% & 27.50\% & 22.95\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{Norm}$} 
                & Mistral-7B & 59.06\% & 72.09\% & 32.45\% & 30.12\% & 24.42\% & 29.79\% & 31.90\% & 22.95\% \\ \cline{2-10}
                & Gemma-7B & 29.37\% & 57.24\% & 27.15\% & 28.24\% & 30.26\% & 29.09\% & 26.10\% & 22.95\% \\ \cline{2-10}
                & LLaMA3.1-8B & 54.74\% & 71.82\% & 33.84\% & 28.75\% & 24.88\% & 29.00\% & 32.30\% & 22.95\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{PMI}$}
                & Mistral-7B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \cline{2-10}
                & Gemma-7B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \cline{2-10}
                & LLaMA3.1-8B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{ANPMI}$} 
                & Mistral-7B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \cline{2-10}
                & Gemma-7B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \cline{2-10}
                & LLaMA3.1-8B & 25.04\% & 49.51\% & 25.08\% & 22.70\% & 20.28\% & 25.93\% & 0.00\% & 22.95\% \\ \hline
\end{tabular}
}
\end{table*}

\begin{table*}[!t]
\centering
\caption{Zero-shot model performance measured with various metrics. Bold numbers represent the best performance for each model and each benchmark.}
\label{tab:performance_comparison}
\vspace{-0.2\baselineskip} % ACL Only
% \setlength{\tabcolsep}{14pt}
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c|c|c|c|c}
\hline
\textbf{Metric} & \textbf{Model} & \textbf{Hellaswag} & \textbf{PiQA} & \textbf{ARC-e} & \textbf{ARC-c} & \textbf{LogiQA} & \textbf{RACE} & \textbf{SciQ} & \textbf{MMLU} \\ \hline \hline
\multirow{3}{*}{Acc} 
         & Mistral-7B & 64.73\% & 81.56\%  & 84.30\% & 57.51\% & 32.72\% & 46.70\% & 96.30\% & 59.72\% \\ \cline{2-10}
         & Gemma-7B & 55.97\% & 76.61\% & \textbf{75.72\%} & 47.53\% & 24.88\% & 41.34\% & 95.40\% & 50.27\% \\ \cline{2-10}
         & LLaMA3.1-8B & 59.05\% & 80.09\% & \textbf{81.78\%} & 51.28\% & 31.64\% & 44.31\% & 96.60\% & 67.70\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{Norm}$} 
         & Mistral-7B & \textbf{82.91\%} & \textbf{82.64\%}  & 82.87\% & 58.79\% & 33.79\% & 47.27\% & 94.50\% & 59.72\% \\ \cline{2-10}
         & Gemma-7B & \textbf{73.10\%} & \textbf{77.91\%} & 72.69\% & \textbf{48.81\%} & 29.19\% & \textbf{43.92\%} & 91.80\% & 50.27\% \\ \cline{2-10}
         & LLaMA3.1-8B & \textbf{79.25\%} & \textbf{81.01\%} & 79.55\% & 54.95\% & 31.95\% & 46.70\% & 96.10\% & 67.70\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{PMI}$} 
         & Mistral-7B & 69.44\% & 73.56\%  & 80.51\% & 62.54\% & 32.10\% & 47.46\% & 96.00\% & \textbf{60.00\%} \\ \cline{2-10}
         & Gemma-7B & 54.15\% & 66.76\% & 61.20\% & 46.67\% & \textbf{30.41\%} & 40.29\% & 84.50\% & 50.40\% \\ \cline{2-10}
         & LLaMA3.1-8B & 62.33\% & 68.61\% & 68.14\% & 55.38\% & 33.64\% & 44.69\% & 92.20\% & 66.32\% \\ \hline
\multirow{3}{*}{$\text{Acc}_\text{ANPMI}$} 
         & Mistral-7B & 77.67\% & 77.58\%  & \textbf{85.90\%} & \textbf{63.99\%} & \textbf{34.10\%} & \textbf{51.20\%} & \textbf{96.90\%} & 59.91\% \\ \cline{2-10}
         & Gemma-7B & 57.77\% & 76.55\% & 75.34\% & 47.78\% & 25.81\% & 42.11\% & \textbf{95.50\%} & \textbf{50.41\%} \\ \cline{2-10}
         & LLaMA3.1-8B & 73.73\% & 77.69\% & 80.98\% & \textbf{57.85\%} & \textbf{34.25\%} & \textbf{48.13\%} & \textbf{97.40\%} & \textbf{67.79\%} \\ \hline
\end{tabular}
}
\end{table*}

% \begin{table*}[!t]
% \centering
% \label{tab:13b}
% \resizebox{0.86\linewidth}{!}{%
% \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
% \hline
% \textbf{Metric} & \textbf{Model} & \textbf{Hellaswag} & \textbf{PiQA} & \textbf{ARC-e} & \textbf{ARC-c} & \textbf{LogiQA} & \textbf{RACE} & \textbf{SciQ} & \textbf{MMLU} \\ \hline \hline
% {Acc} 
%          & LLaMA2-13B & 60.06\% & 79.11\%  & 79.42\% & 48.46\% & 26.27\% & 40.48\% & 94.60\% & 50.47\% \\  \hline
% {$\text{Acc}_\text{Norm}$} 
%          & LLaMA2-13B & 79.36\% & 80.52\%  & 77.40\% & 49.06\% & 30.88\% & 42.20\% & 93.50\% & 50.47\% \\  \hline
% {$\text{Acc}_\text{PMI}$} 
%          & LLaMA2-13B & 64.37\% & 67.73\%  & 70.33\% & 50.94\% & 32.41\% & 43.44\% & 93.30\% & 48.73\% \\  \hline
% {$\text{Acc}_\text{ANPMI}$}
%          & LLaMA2-13B & 74.53\% & 78.02\%  & 80.51\% & 53.33\% & 31.80\% & 44.69\% & 96.40\% & 49.99\% \\  \hline
% \end{tabular}
% }
% \end{table*}

\section{Experiments}
In this section, we evaluate the performance of the models using ANPMI, while comparing it with the existing metrics. Specifically, we conduct experiments using instruction-tuned language models, such as Mistral-7B(version 0.3)~\citep{jiang2024mixtral}, Gemma-7B~\citep{team2024gemma}, and LLaMA3.1-8B~\citep{dubey2024llama}, along with seven widely used multiple-choice benchmarks~\cite{hellaswag, bisk2020piqa, clark2018think, hendrycks2020measuring, welbl2017crowdsourcing, liu2021logiqa, lai2017race}. We aim to highlight the differences between ANPMI and other popular existing metrics, demonstrating both their benefits and limitations through empirical analysis. The model performance is denoted as \textit{Acc}, \textit{$\text{Acc}_\text{Norm}$}, \textit{$\text{Acc}_\text{PMI}$}, and \textit{$\text{Acc}_\text{ANPMI}$} when measured using {\small $P(Choice|Prompt)$}, length-normalized {\small $P(Choice|Prompt)$}, PMI, and ANPMI. 
\textbf{Random} represents the baseline performance, reflecting the probability of selecting the correct label between labels 0, 1, 2, and 3 by chance, based solely on the label distribution. We exclude NPMI because it is not standard in language models, and it is impossible to compute {\small $P(Choice, Prompt)$} if {\small $Prompt+Choice$}  is larger than the maximum sequence length.

\subsection{Performance When No Prompt Provided}
To verify that ANPMI can evaluate performance while reducing bias from differences in {\small $P(Choice)$}, we measure the language model performance on the various benchmarks without providing prompts. The results of these evaluations are summarized in Table~\ref{tab:performance_comparison_without_prompt}. 

For MMLU, we observe identical performance across all models, regardless of the metric used.  This is because the same four choices — A, B, C, and D — are given throughout examples.  However, for other datasets, such as Hellaswag and ARC, which have a set of different answer choices for each example, model performance varies when evaluated using Acc or $\text{Acc}_\text{Norm}$.  For each benchmark, we observe a difference of up to 30\% in performance between models when evaluated using these metrics.  Section~\ref{sec:impact_of_prior_prob} demonstrates that variations in {\small $P(Choice)$} significantly influence a model's final decisions.  Thus, these performance differences observed without prompts, which highlight the impact of prior probabilities, may complicate accurately ranking models.  Moreover, the measured performance for Hellaswag, PiQA, and ARC-easy is significantly higher than that of random guessing.  This indicates that when using Acc or $\text{Acc}_\text{Norm}$, models may achieve high scores on these benchmarks without understanding the prompts, complicating the evaluation of their language comprehension capability.

In contrast, PMI and ANPMI have identical performance across all models when prompts are absent. These metrics always assign a zero value when prompts are not provided, resulting in consistent performance measurements by always choosing the same choice. Consequently, PMI and ANPMI effectively mitigate the influence of {\small $P(Choice)$} on performance, making them reliable metrics for accurately assessing a model's understanding of prompts to answer questions.

\begin{table}[!t]
\centering
\caption{The proportion of choices selected in the MMLU task based on PMI and ANPMI metrics for the LLaMA3.1-8B model.}
\label{tab:pmi_and_anpmi}
% \vspace{-0.5\baselineskip}
\resizebox{0.95\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
\multirow{2}{*}{} & \multicolumn{4}{|c}{Choices} \\
\cline{2-5}
 & A & B & C & D \\
\hline \hline
{\small $\log(P(Choice))$} & -9.14 & -10.08 & -10.27 & -9.95 \\
\hline
PMI & 12.36\% & 31.65\% & 33.31\% & 22.69\% \\
\hline
ANPMI & 18.01\% & 30.10\% & 29.66\% & 23.23\% \\
\hline
\end{tabular}
}
\end{table}

\subsection{Comparison of the Metrics}
The results of evaluating the model performance using various metrics are summarized in Table~\ref{tab:performance_comparison}. {Some examples where ANPMI differs from other metrics can be found in Appendix\mbox{~\ref{sec:comp_example}}.} The experiments are conducted using the Language Model Evaluation Harness~\cite{eval-harness} under a zero-shot setting.  

Benchmarks where the final decision of the model depends heavily on {\small $P(Choice)$} show a larger performance gap when measured using metrics other than {\small $P(Choice|Prompt)$}. 
For instance, when evaluating HellaSwag using LLaMA3.1-8B, about 65\% of decisions are influenced by the differences in {\small $P(Choice)$} as seen in Table~\ref{tab:comp_results}, resulting in a 14.68\% performance gap between Acc and $\text{Acc}_\text{ANMPI}$. Conversely, in MMLU, where only 13\% to 16\% of decisions of each model depend on the {\small $P(Choice)$} difference according to Table~\ref{tab:comp_results}, the maximum performance discrepancy is merely up to 0.19\% comparing Acc and $\text{Acc}_\text{ANPMI}$.

The difference between Length-normalized {\small $\log P(Choice|Prompt)$} and ANPMI can be observed on MMLU. Since all choices in MMLU have the same length in characters (1 char), $\text{Acc}_\text{Norm}$ is identical to Acc, with no performance change occurring due to length normalization. In contrast, ANPMI theoretically addresses the impact of the imbalance in {\small $P(Choice)$} on model performance measurement. As a result, differences between Acc and $\text{Acc}_\text{ANPMI}$ are consistently observed across all models.

The difference in model performance measured by PMI and ANPMI is caused by the fact that PMI does not perform any normalization. Table~\ref{tab:pmi_and_anpmi} shows how the lack of normalization affects the model's final choices in MMLU. PMI tends to assign smaller maximum values to choices with higher {\small $\log P(Choice)$}, making the model less likely to select options with large {\small $P(Choice)$} values. As demonstrated in Table~\ref{tab:pmi_and_anpmi}, under PMI, choice A (A has the highest {\small $\log P(Choice)$}) is the least frequently chosen, whereas choice C (C has the lowest {\small $\log P(Choice)$}) is the most frequently chosen. In contrast, this tendency is less evident when using ANPMI.

The experimental results indicate that when model performance is evaluated using a metric that fails to account for the {\small $P(Choice)$} imbalance, the model's performance does not accurately reflect its natural language understanding capability. As a result, ANPMI, which theoretically addresses the {\small $P(Choice)$} imbalance, is identified as the most appropriate metric for assessing a language model's natural language understanding capability.