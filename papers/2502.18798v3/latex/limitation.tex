\section*{Limitations}
\label{sec:limit}

{For benchmarks where the model's choice, driven by \mbox{\small$P(Choice)$}, is as critical as understanding the prompt, traditional metrics like \mbox{\small $P(Choice|Prompt)$} might be more suitable than ANPMI. For example, in tasks such as ToxiGen\mbox{~\cite{hartvigsen2022toxigen}}, which evaluates whether a model avoids harmful responses regardless of the prompt, it is essential to ensure that the model does not generate toxic outputs independently of the prompt. Thus, selecting the right metric based on a clear understanding of the task is crucial for accurate performance evaluation of language models.}

While some benchmarks for evaluating language model performance, such as HumanEval~\cite{chen2021evaluating} and IFEval~\cite{zhou2023instruction}, are not in multiple-choice format, this study focuses exclusively on multiple-choice benchmarks. Additionally, although the structure of prompts used in evaluations significantly impacts model performance, our analysis is limited to the effects of choice construction. In the future, we plan to address cases not covered in this study to ensure accurate performance measurement and fair comparisons across models.

{ANPMI ensures an accurate measurement of a modelâ€™s understanding of a prompt by satisfying two key properties: (1) when no prompt is given, all choices have the same metric value, and (2) the choices have the same minimum and maximum metric values. Normalizing PMI with $-\log P(Choice)$ is based on the assumption that PMI should increase significantly if the model properly understands the prompt. However, when $P(Choice)$ is extremely high, even if the model fully understands the prompt, $P(Choice | Prompt) \approx P(Choice)$ may hold, leading to minimal PMI changes. ANPMI does not account for this effect, and addressing this limitation remains an important direction for future work.}