\subsection{Discussions}
\hl{For Benchmarks where the model's choice based on \mbox{\small$P(Choice)$} is as critical as understanding the prompt, traditional metrics like \mbox{\small $P(Choice|Prompt)$} might be more suitable for them.}
% Benchmarks, where the model's choice is based on $P(Choice)$, are as critical as understanding the prompt. Traditional metrics like {\small $P(Choice|Prompt)$} might be more suitable for them. {\bf === check this sentence ===}
For example, in tasks such as ToxiGen~\cite{hartvigsen2022toxigen}, which evaluates whether a model avoids harmful responses regardless of the prompt, it is essential to ensure that the model does not generate toxic outputs independent of the prompt comprehension. Thus, selecting the right metric based on a clear understanding of the task is crucial for accurate performance evaluation of language models.