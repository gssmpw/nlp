\section{Conclusion}
This paper introduces ANPMI, a novel metric for assessing natural language understanding in language models for multiple-choice tasks. It ensures that the model performance reflects the true comprehension capability of the model rather than unrelated choice preferences. ANPMI is defined by normalizing PMI with {\small $-\log P(Choice)$}. All choices yield an identical score without a prompt under ANPMI, requiring the model to understand the prompt to solve the task. Unlike PMI, ANPMI maintains the same maximum and minimum values across all choices, mitigating bias towards any specific choice and focusing solely on the relationship between the prompt and choices. Through evaluations using diverse language models and benchmarks, we demonstrate that ANPMI effectively addresses the issue of inaccurate performance measurement caused by imbalances in {\small $P(Choice)$}. 