
\section{Existing Metrics}
Due to the limitations of evaluating model performance based solely on {\small $P(Choice|Prompt)$}, some benchmarks employ additional metrics. This section explores several alternative metrics commonly used in such evaluations. We explain how these metrics address the limitations of {\small $P(Choice|Prompt)$} and discuss their constraints. 

\subsection{Length-Normalized Accuracy}
\label{sec:metric_norm}
Language models generally assign higher probabilities to shorter sentences than longer ones. It means that when there are significant differences in the lengths of the choice options, the model's decision can be biased, favoring shorter options. This results in an imbalance in {\small $P(Choice)$} based on the length of a choice option $Choice$. To address it, length-normalized accuracy is used, which normalizes 
{\small $\log{P(Choice|Prompt)}$} based on the text length of $Choice$. For example, the Language Model Evaluation Harness uses length normalization by dividing each choice option's log-likelihood by its length in characters~\citep{eval-harness}. It is particularly effective for datasets, such as Hellaswag, where there are significant differences in choice lengths.

While the length-normalized accuracy addresses the problem of length imbalance and its impact on the model performance, {\small $P(Choice)$} is not always inversely proportional to the length in characters. Figure~\ref{fig:len_prob_distribution} shows the distribution of log probabilities and the length-normalized log probabilities for an instruction-tuned LLaMa3.1-8B on the choices used in Hellaswag. In Figure~\ref{fig:len_prob_distribution}(a), we observe that the relationship between the choice length and its log probability is not strictly linear. Consequently, the normalized log-likelihood is not constant with the text length, as shown in Figure~\ref{fig:len_prob_distribution}(b). As a result, normalizing by length can sometimes introduce new biases, particularly when {\small $P(Choice)$} values are already similar across options of varying lengths.

\subsection{Pointwise Mutual Information (PMI)}
\label{sec:metric_mi}
Using mutual information~\citep{mutualinfo} in language modeling has a different motivation. Its goal is to measure how much the presence of a prompt increases the likelihood of a particular choice $Choice$ relative to its prior probability {\small $P(Choice)$}. Specifically, the model selects a choice option based on the Pointwise Mutual Information (PMI) value~\citep{pmi}, {\small $\log{\dfrac{P(Choice|Prompt)}{P(Choice)}}$}. This approach counteracts the tendency of high prior probability choices to dominate the selection. When {\small $P(Choice)$} is high, indicating that the model is likely to select $Choice$ regardless of $Prompt$, PMI normalizes {\small $P(Choice|Prompt)$} using the prior probability of $Choice$, allowing selection of responses with low prior probability but high contextual relevance more often. 
Thus, PMI focuses on enhancing contextual relevance over raw likelihood. While less common than metrics, such as accuracy and length-normalized accuracy, PMI has been used selectively in some studies~\citep{askell2021general, biderman2024lessons}.

The PMI value is always zero when no prompt is given, regardless of the choice. It implies that in the absence of a prompt, each choice option has an equal probability of being chosen by the model. However, when a prompt is provided, the maximum possible PMI value is {\small $-\log{P(Choice)}$}, as PMI reaches its peak when {\small $P(Choice|Prompt)=1$}. As a result, each choice has a different maximum possible value based on its prior probability. When {\small $P(Choice)$} is high, the maximum PMI value decreases, resulting in an unintended issue: choices with high {\small $P(Choice)$} values are penalized by PMI, even if they are not inherently incorrect nor intentionally boosted. It becomes problematic when a correct $Choice$ has both a meaningfully high {\small $P(Choice|Prompt)$}, indicating relevance to the prompt, and a naturally high {\small $P(Choice)$}. This case prevents the model from selecting the correct answer simply because the answer's prior probability happens to be high.

\subsection{Normalized PMI (NPMI)}
\label{sec:metric_npmi}
PMI yields different maximum values depending on the choice. Due to this property, PMI is unsuitable for comparing different choices. To address this limitation, Normalized PMI (NPMI)~\citep{bouma2009normalized} was introduced, with PMI divided by {\small $-\log P(Choice, Prompt)$}.
% \hl{Here, normalize refers to division, that is, PMI is divided by {\small $-\log P(Choice, Prompt)$}, not subtracted.} 
NPMI normalizes PMI so that it falls within $[-1, 1]$ under the assumption that {\small $P(Choice,Prompt)=P(Prompt,Choice)$} to allow a fair comparison.


If {\small $P(Choice,Prompt) = P(Prompt,Choice)$}, PMI satisfies the following relationship:

\begin{small}
\begin{equation} 
\begin{array}{l}
\mbox{PMI}(Choice, Prompt) \\[5pt]
\quad = \log \dfrac{P(Choice, Prompt)}{P(Choice)P(Prompt)} \\[10pt]
\quad = \log \dfrac{P(Choice|Prompt)}{P(Choice)} \\[10pt]
\quad = \log \dfrac{P(Prompt|Choice)}{P(Prompt)}.  
% \mbox{PMI}(Choice, Prompt) = \log \dfrac{P(Choice, Prompt)}{P(Choice)P(Prompt)} \\
% = \log \dfrac{P(Choice|Prompt)}{P(Choice)} = \log \dfrac{P(Prompt|Choice)}{P(Prompt)}.  
\end{array}
\end{equation}
\end{small}
% In this case, we find, 
Since probability values are between 0 and 1, we have
\begin{small}
\begin{equation}
\label{eq:ieq_1}
\log \frac{P(Choice|Prompt)}{P(Choice)} \le -\log P(Choice).
\end{equation}
\end{small}
and 
\begin{small}
\begin{equation}
\label{eq:ieq_2}
\log \frac{P(Prompt|Choice)}{P(Prompt)} \le -\log P(Prompt).
\end{equation}
\end{small}
This means,
\begin{small}
\[
\begin{array}{l}
\max(\mbox{PMI}(Choice, Prompt)) \\[5pt]
\quad = \min(-\log P(Choice), -\log P(Prompt)) \\[5pt]
\quad \le - \log P(Choice, Prompt).
\end{array}
\]
\end{small}

% Normalization by {\small $  -\log P(Choice, Prompt)$} ensures the maximum value 1 because,

% \begin{small}
% \[ -\log P(Choice) \le -\log P(Choice, Prompt) \]
% \end{small}
% and 
% \begin{small}
% \[ -\log P(Prompt) \le -\log P(Choice, Prompt) \]
% \end{small}

% \begin{figure*}[!t]
%    \centering
%    \includegraphics[width=1.0\linewidth]{figure/3_metrics.pdf}
%    \vspace*{-2\baselineskip}
%    \caption{Comparison of the existing metrics and ANPMI for two different choice options, A and B. Dotted lines indicate values calculated without a prompt, while the heads of block arrows represent values after a prompt is provided. The red lines denote the theoretical minimum and maximum values, and the blue arrows highlight the difference caused by the two different choices.
%    \hl{The length of the black arrows is a critical factor in determining the model's choices, as it reflects their understanding of the prompt. To ensure a fair and accurate evaluation, the positions of the dashed lines and red lines must be consistent across all choices.}
%    The value of {\small $P(Choice| Prompt)$} differs depending on choices when no prompt is given. Furthermore, PMI has a different theoretical maximum value depending on the choice. Normalizing by the length and NPMI mitigates this difference but does not eliminate it due to their incorrect assumptions. ANMPI, on the other hand, always has the same value for all cases independent of the prompt.}
% % {\bf === explain the blue arrow in detail, also explain each figure in detail. Where is NPMI? ==}}
% \label{fig:metric_comparison}
% \end{figure*}


However, in the case of language models, {\small $P(Prompt, Choice)$} represents the probability of generating a full sentence where the prompt is followed by the choice, {\small $Prompt+Choice$}. Since changing the order of words results in a fundamentally different event in language models, this results in {\small $P(Prompt, Choice) \neq P(Choice, Prompt)$}. Thus, {\small $\mbox{PMI}_{\mbox{LM}}$} in language models satisfies the following relationship where $x$ is {\small $Choice$}, and $y$ is {\small $Prompt$}:

\begin{small}
\begin{equation}
\label{eq:pmi_lm}
\mbox{PMI}_{\mbox{LM}}(x, y) = \log \frac{P(x|y)}{P(x)} \neq \log \frac{P(y|x)}{P(y)}.
\end{equation}
\end{small}

Thus, NPMI is not an appropriate normalization method for PMI in language models. As a result, NPMI is treated as nonstandard in language model evaluation and is not commonly used.

% {\bf === describe NPMI here ===}