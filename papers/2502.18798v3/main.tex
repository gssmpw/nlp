% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{soul}
\usepackage{nccmath}

% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{balance}
\usepackage{float}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{ANPMI: Assessing the True Comprehension Capabilities of LLMs \\ for Multiple Choice Questions}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Gyeongje Cho$^*$$^1$ \\
  \texttt{gyeongje@aces.snu.ac.kr} \\\And
  Yeonkyoung So$^*$$^1$ \\
  \texttt{kathy1028@snu.ac.kr} \\\And
  Jaejin Lee$^1$$^2$ \\
  \texttt{jaejin@snu.ac.kr}
  }

\renewcommand{\topfraction}{0.85}
\renewcommand{\bottomfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{0.8}

\begin{document}
\maketitle

\def\thefootnote{*}\footnotetext{These authors contributed equally to this work}
\def\thefootnote{1}\footnotetext{Graduate school of Data Science, Seoul National University}
\def\thefootnote{2}\footnotetext{Department of Computer Science, Seoul National University}

\input{latex/0_abstract}

\input{latex/1_intro}
\input{latex/2_related}
\input{latex/3_analysis}
\input{latex/4_existing_metric}
\input{latex/5_new_metric}
\input{latex/6_experiment}
%\input{latex/7_discussion}
\input{latex/8_conclusion}
\input{latex/limitation}
\input{latex/ethics_statement}

\section*{Acknowledgements}
\footnotesize
\normalsize
This work was supported in part by the National Research Foundation of Korea (NRF) grant (No. RS-2023-00222663, Center for Optimizing Hyperscale AI Models and Platforms), by the Institute for Information and Communications Technology Promotion (IITP) grant (No. 2018-0-00581, CUDA Programming Environment for FPGA Clusters), by the BK21 Plus programs for BK21 FOUR Intelligence Computing (Dept. of Computer Science and Engineering, SNU, No. 4199990214639), all funded by the Ministry of Science and ICT (MSIT) of Korea. This work was also supported in part by the Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT(MSIT, Korea)\&Gwangju Metropolitan City. ICT at Seoul National University provided research facilities for this study.

% \bibliography{custom}

\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}

\bibitem[{Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones, Joseph, Mann, DasSarma et~al.}]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al. 2021.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}.

\bibitem[{Balepur et~al.(2024)Balepur, Ravichander, and Rudinger}]{balepur2024artifacts}
Nishant Balepur, Abhilasha Ravichander, and Rachel Rudinger. 2024.
\newblock Artifacts or abduction: How do llms answer multiple-choice questions without the question?
\newblock \emph{arXiv preprint arXiv:2402.12483}.

\bibitem[{Belinkov et~al.(2019)Belinkov, Poliak, Shieber, Van~Durme, and Rush}]{belinkov2019don}
Yonatan Belinkov, Adam Poliak, Stuart~M Shieber, Benjamin Van~Durme, and Alexander~M Rush. 2019.
\newblock Don't take the premise for granted: Mitigating artifacts in natural language inference.
\newblock \emph{arXiv preprint arXiv:1907.04380}.

\bibitem[{Biderman et~al.(2024)Biderman, Schoelkopf, Sutawika, Gao, Tow, Abbasi, Aji, Ammanamanchi, Black, Clive et~al.}]{biderman2024lessons}
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham~Fikri Aji, Pawan~Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et~al. 2024.
\newblock Lessons from the trenches on reproducible evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2405.14782}.

\bibitem[{Bisk et~al.(2020)Bisk, Zellers, Gao, Choi et~al.}]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al. 2020.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439.

\bibitem[{Bouma(2009)}]{bouma2009normalized}
Gerlof Bouma. 2009.
\newblock Normalized (pointwise) mutual information in collocation extraction.
\newblock \emph{Proceedings of GSCL}, 30:31--40.

\bibitem[{Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman et~al.}]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al. 2021.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}.

\bibitem[{Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman, Mathur, Schelten, Yang, Fan et~al.}]{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al. 2024.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}.

\bibitem[{Fano and Hawkins(1961)}]{pmi}
Robert~M Fano and David Hawkins. 1961.
\newblock Transmission of information: A statistical theory of communications.
\newblock \emph{American Journal of Physics}, 29(11):793--794.

\bibitem[{Feng et~al.(2019)Feng, Wallace, and Boyd-Graber}]{feng2019misleading}
Shi Feng, Eric Wallace, and Jordan Boyd-Graber. 2019.
\newblock Misleading failures of partial-input baselines.
\newblock \emph{arXiv preprint arXiv:1905.05778}.

\bibitem[{Gao et~al.(2024)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster, Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds, Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and Zou}]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024.
\newblock \href {https://doi.org/10.5281/zenodo.12608602} {A framework for few-shot language model evaluation}.

\bibitem[{Gururangan et~al.(2018)Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}]{gururangan2018annotation}
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel~R Bowman, and Noah~A Smith. 2018.
\newblock Annotation artifacts in natural language inference data.
\newblock \emph{arXiv preprint arXiv:1803.02324}.

\bibitem[{Hartvigsen et~al.(2022)Hartvigsen, Gabriel, Palangi, Sap, Ray, and Kamar}]{hartvigsen2022toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
\newblock Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection.
\newblock \emph{arXiv preprint arXiv:2203.09509}.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}.

\bibitem[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}.

\bibitem[{Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy}]{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, pages 785--794.

\bibitem[{Leidinger et~al.(2023)Leidinger, Van~Rooij, and Shutova}]{leidinger2023language}
Alina Leidinger, Robert Van~Rooij, and Ekaterina Shutova. 2023.
\newblock The language of prompting: What linguistic properties make a prompt successful?
\newblock \emph{arXiv preprint arXiv:2311.01967}.

\bibitem[{Liu et~al.(2021)Liu, Cui, Liu, Huang, Wang, and Zhang}]{liu2021logiqa}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021.
\newblock Logiqa: a challenge dataset for machine reading comprehension with logical reasoning.
\newblock In \emph{Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence}, pages 3622--3628.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:27730--27744.

\bibitem[{Poliak et~al.(2018)Poliak, Naradowsky, Haldar, Rudinger, and Van~Durme}]{poliak2018hypothesis}
Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van~Durme. 2018.
\newblock Hypothesis only baselines in natural language inference.
\newblock \emph{arXiv preprint arXiv:1805.01042}.

\bibitem[{Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn}]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn. 2024.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Shannon(1948)}]{mutualinfo}
Claude~Elwood Shannon. 1948.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27(3):379--423.

\bibitem[{Srikanth and Rudinger(2022)}]{srikanth2022partial}
Neha Srikanth and Rachel Rudinger. 2022.
\newblock Partial-input baselines show that nli models can ignore context, but they don't.
\newblock \emph{arXiv preprint arXiv:2205.12181}.

\bibitem[{Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth et~al.}]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al. 2023.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}.

\bibitem[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}.

\bibitem[{Webson and Pavlick(2021)}]{webson2021prompt}
Albert Webson and Ellie Pavlick. 2021.
\newblock Do prompt-based models really understand the meaning of their prompts?
\newblock \emph{arXiv preprint arXiv:2109.01247}.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:24824--24837.

\bibitem[{Welbl et~al.(2017)Welbl, Liu, and Gardner}]{welbl2017crowdsourcing}
Johannes Welbl, Nelson~F Liu, and Matt Gardner. 2017.
\newblock Crowdsourcing multiple choice science questions.
\newblock In \emph{Proceedings of the 3rd Workshop on Noisy User-generated Text}, pages 94--106.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}.

\bibitem[{Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu}]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{arXiv preprint arXiv:2401.02385}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\bibitem[{Zhou et~al.(2023)Zhou, Lu, Mishra, Brahma, Basu, Luan, Zhou, and Hou}]{zhou2023instruction}
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi~Luan, Denny Zhou, and Le~Hou. 2023.
\newblock Instruction-following evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2311.07911}.

\end{thebibliography}


\newpage
\input{latex/appendix}

\end{document}
