\section{Related Works}
\label{sec:related-work}

We review related works in domain adaptation and Stein discrepancy, particularly the applications of the latter to machine learning and computational statistics. We refer the reader to ____ and ____ for more comprehensive reviews of these topics.

\subsection{Domain Adaptation}

Foundational work for domain adaptation from ____ proved an upper bound for the generalization error in the target domain that depends on the error in the source domain and the distance between the source and target distributions.
% This aligns with an intuitive understanding of domain adaptation: if a model performs poorly on a dataset for which it has labels, it will likely perform poorly on a similar, unlabeled dataset.
% In addition, given a model trained on one dataset, we expect to get better results applying it to a similar dataset than to a totally unrelated one.
This bound motivated a large class of domain adaptation methods focused on feature alignment, learning feature representations that are invariant between domains but informative for classification.
The original bound used $\gH$-divergence to measure the distance between domains, but
$\gH$-divergence is difficult to estimate in practice, so later algorithms used other distances between distributions, including Wasserstein distance ____, Jensen-Shannon divergence ____, $\alpha$-R\'enyi distance ____, and KL divergence ____.
\Ac{mmd} was used for several \ac{uda} methods ____, and is of particular interest because of connections between \ac{mmd} and \ac{ksd}.
Several methods also took an adversarial approach to domain adaptation ____,
% Adversarial methods are formulated as a competition between a generator, which generates domain-invariant features, and a domain discriminator, which attempts to classify a sample as coming from the source or target domain, 
and they were later extended to conditional adversarial methods, inspired by conditional GANs ____.
f-domain adversarial learning (FDAL) uses f-divergences to measure the distance between domains as part of an adversarial approach ____.
% Conditional Domain Adversarial Networks (CDAN), inspired by conditional GANs, conditions the discriminator on cross-covariance of feature representations with classifier predictions, to capture which information is discriminative for the classifier ____.
% In DANN, the same classifier is used for source and target samples, but other methods, such as Coupled Generative Adversarial Networks (CoGAN), propose decoupling some of the parameters between the source and target classifiers ____.
% Margin disparity discrepancy seeks to maximize the margin between each sample and the classification boundary ____.
While adversarial methods are popular, challenges such as unstable training remain.
% Even with the same hyperparameters, adversarial models often achieve different levels of accuracy when retrained multiple times; to ameliorate this instability, the reported accuracy is often averaged over three training runs ____.

There exist other types of domain adaptation methods, which can often be paired with feature alignment methods to boost accuracy.
Early but successful techniques included importance weighting, which emphasizes source samples that are similar to the target distribution ____. % other references in confidence calibration paper
Another effective technique is pseudo-labeling target samples before training a classifier on the target domain ____.
% another method with pseudo-labeling, Confidenc eregularized self-training, by Zou et al, 2019
% Li et. al propose minimizing class confusion to improve accuracy in both domains ____.
% Domain Adapted Neural Architecture Search (DA-NAS) proposed replacing the pre-trained neural networks that most methods use as feature extractors, such as ResNet, by an architecture tailored to extract transferable features ____.
Gradient harmonization, which seeks to resolve or reduce conflicts between the direction of the gradients from the two optimization goals, minimizing classification error and distance between domains, can boost performance by several percentage points on benchmark datasets ____.
Graph-based methods represent the source and target features as graphs and align the source and target domains by aligning characteristics of their graphs; graph spectral alignment (SPA) method attempts to align the graph spectra ____.

There are several other common domain adaptation settings besides \ac{uda}.
Semi-supervised domain adaptation has access to a small number of labels for the target distribution.
Multi-source domain adaptation attempts to leverage information from several source domains at once, while multi-target attempts to improve performance over several target domains ____.
Open set domain adaptation allows new classes in the target dataset that are not part of the source dataset ____.
Finally, domain generalization and few-shot learning are both similar to the scarce target setting for \ac{uda}.
Domain generalization extends multi-source \ac{uda} by assuming that there is no access to the target data set; the goal is to learn features that are invariant to unseen distributions ____.
Domain generalization can be viewed as an extreme version of the scarce target setting, with no target data available, but does not leverage target information when it is available.
Few-shot learning can refer to a broad class of methods focused on learning from few data points; however, unlike the scarce target setting, the data in few-shot learning is usually labeled ____.


\subsection{Stein Discrepancy}

Stein's method was introduced to bound distances between probability distributions ____. ____ built on this by formalizing Stein discrepancy as a measure of distributional difference, particularly for assessing approximate MCMC samples. Its key advantage is applicability to unnormalized distributions, making it valuable for Bayesian inference. Since then, Stein discrepancies have gained popularity in machine learning and statistics. 
Originally, computing Stein discrepancies involved a maximization step, 
but
____, ____, and ____ 
independently developed the \ac{ksd}, which provides a closed-form solution using \ac{rkhs}. ____ also established theoretical conditions under which convergence in \ac{ksd} guarantees weak convergence between distributions.
They demonstrated that in dimensions $d \geq 3$, commonly used kernels such as Gaussian and Mat√©rn fail to detect when a sample is not converging to the target, highlighting the importance of kernel choice in practical applications. 
Stein discrepancies have been widely adopted for constructing sample approximations, leading to influential methods such as Stein Variational Gradient Descent (SVGD) ____, which iteratively updates sample locations; Stein points ____, which constructs sample sets sequentially; and Stein thinning ____, which compresses existing samples, all aiming to minimize \ac{ksd}. 
More recently, \ac{ksd} has been extended to non-parametric settings, where the score function of an implicit model is estimated. This extension has enabled the development of two-sample tests for implicit generative models, a crucial tool in machine learning. Since such models can generate unlimited synthetic data while real datasets remain limited, these tests must handle unbalanced sample sizes ____. This imbalance is analogous to our scarce target setting, where target domain samples are also limited compared to the source domain.