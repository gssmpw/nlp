\documentclass{article}

\usepackage{arxiv}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{rotating}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[section]{placeins}
\usepackage{booktabs} % for professional tables
\usepackage{natbib}

\usepackage{acro}
\DeclareAcronym{ksd}{
short = KSD,
long = kernelized Stein discrepancy
}

\DeclareAcronym{rkhs}{
short = RKHS,
long = reproducing kernel Hilbert space
}

\DeclareAcronym{uda}{
short = UDA,
long = unsupervised domain adaptation
}

\DeclareAcronym{mmd}{
short = MMD,
long = maximum mean discrepancy
}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc,3d}


\usepackage{hyperref}




% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}


% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


\usepackage{authblk}
\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}
% box is needed for correct spacing with authblk

\author[1]{%
	{\hspace{1mm}Anneke von Seeger\thanks{\texttt{vonse006@umn.edu}}}%
}
\author[2]{%
	{Dongmian Zou \thanks{\texttt{dongmian.zou@duke.edu}}}%
}
\author[1]{
Gilad Lerman \thanks{\texttt{lerman@umn.edu}}
}
\affil[1]{School of Mathematics, University of Minnesota, Minneapolis, MN, USA}
\affil[2]{Data Science Research Center, Duke Kunshan University, Suzhou, Jiangsu, China}
\title{Stein Discrepancy for Unsupervised Domain Adaptation}

% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{Stein Discrepancy for UDA}
% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document


\input{math_commands}


\begin{document}

\maketitle


\begin{abstract}
\Ac{uda} leverages  information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset.
  A common approach to \ac{uda} is aligning representations from the source and target domains by minimizing the distance between their data distributions.  
  Previous methods have employed distances such as Wasserstein distance and maximum mean discrepancy.
  However, these approaches are less effective when the target data is significantly scarcer than the source data. 
  Stein discrepancy is an asymmetric distance between distributions that relies on one distribution only through its score function.
  In this paper, we propose a novel \ac{uda} method that  uses Stein discrepancy to measure the distance between source and target domains.
  We develop a learning framework using both non-kernelized and kernelized Stein discrepancy.
  Theoretically, we derive an upper bound for the generalization error. 
  Numerical experiments show that our method outperforms existing methods using other domain discrepancy measures when only small amounts of target data are available.
\end{abstract}

\keywords{Machine Learning \and Transfer Learning \and Stein Discrepancy \and Domain Adaptation}

\acresetall

\section{Introduction}

Deep learning methods have been shown to outperform humans on tasks like image classification \citep{he2015delving}, but they typically require large amounts of labeled training data and assume that the training and test data are independent and identically distributed.
In practice, both of these requirements can be difficult to satisfy.
\Ac{uda} addresses both of these challenges: it leverages information from a labeled source dataset to improve accuracy on a related but unlabeled target dataset~\citep{ben-david_analysis_2007, ben-david_theory_2010}.
Since unlabeled data is often easier and cheaper to obtain than labeled data, and relaxing the assumption that training and test data are identically distributed broadens the range of applicable datasets, \Ac{uda} has become a crucial research area for solving real-world problems.


A common approach to \ac{uda} is feature alignment~\citep{ganin_unsupervised_2015, ganin_domain-adversarial_2016, long_learning_2015}, whose goal is to learn feature representations that are informative for downstream tasks but invariant across domains.
This can be accomplished by introducing a regularization term in the loss function that seeks to minimize the distance between the source and target feature distributions.
Previous methods have used Wasserstein distance~\citep{shen_wasserstein_2018} and \ac{mmd}~\citep{long_deep_2017} to estimate the distance between distributions.


Existing \ac{uda} methods rely on a large, unlabeled target dataset to perform feature alignment.
However, in some scenarios of interest, only a small amount of target data is available; we refer to this as the scarce target setting.
For example, \ac{uda} has been applied to improve performance of machine learning models on electroencephalograms (EEGs), where patient-specific variations prevent directly transferring a model trained on one patient to new patients \citep{shi_single-source_2024}.
A \ac{uda} method that requires balanced sample sizes or abundant target data would be challenging to apply to a new patient, who might have only a small amount of data available.
The scarce target setting also applies to online user training, where a model, such as one trained to predict a search query from the first few words, might be trained on large amounts of data generated by many users and then fine-tuned to give personalized results using the much smaller amount of data associated with a single user.

Stein discrepancy~\citep{stein1972bound} is a distance metric that performs well in the scarce target setting.
Stein discrepancy is a distance metric well-suited for settings with limited samples.
It measures the difference between distributions by applying a Stein operator $\gA_q$ to functions from a chosen function class $\gF$, using the most discriminative function to compute the final discrepancy.
For instance, \ac{ksd} arises when $\gF$ is the unit ball of a \ac{rkhs}, and provides a closed-form solution.
% The Stein discrepancy between two smooth distributions $p,q$ is defined as
% \begin{equation*}
%     \ermS(p,q) = \sup_{f \in \gF} \E_p[\gA_q f(x)],
% \end{equation*}
% where $\gA_q$ is a Stein operator.
% We focus on the score-Stein operator $$\gA_q f(x) = f(x)^\T \nabla_x \log q(x) + \nabla_x \cdot f(x).$$ 
% Furthermore, $\gF$ is a space of real-valued functions on the probability space. When 
% $\gF$ is a \ac{rkhs} with kernel $k(\cdot,\cdot)$, the Stein discrepency has a closed form solution:
% \begin{equation*}
%         \ermS (p,q) = \sE_{x, x' \sim p} [ \gA_q \gA_q k(x, x')].
% \end{equation*}
The Stein discrepancy is closely related to integral probability metrics including the Wasserstein distance and \ac{mmd}, which typically require integrating over both source and target distributions.
However, a key advantage of Stein discrepancy is that it replaces the integration over the target distribution with a dependence on the score function through the Stein operator.
In our method, this asymmetry allows us to estimate the Stein discrepancy even when the target domain contains significantly fewer samples than the source domain. 
On the other hand, it also necessitates special treatment of the target distribution, which we detail in our methodology. 
Additionally, the usage of Stein discrepancy is highly adaptable, and we derive multiple forms according to different assumptions of the target distribution.


We summarize our contributions as follows:
\begin{itemize}
\setlength\itemsep{0em}
    \item We introduce a new \ac{uda} method based on Stein discrepancy, specifically designed for the scarce target setting, in which only a small amount of unlabeled target data is available.
    % To address this setting, we develop a method using Stein discrepancy to estimate the distance between the source and target distributions.
    % Since Stein discrepancy is asymmetric and depends on the target distribution only through the score function, it is possible to accurately estimate the Stein discrepancy from a small amount of target data. 
    \item Our method is flexible: it has two forms, a non-kernelized form and a kernelized form, and several possible target distribution estimation approaches including a single Gaussian, GMM, or VAE.
    % , which must be simple enough to allow an explicit score function while being rich enough to describe complicated data distributions.
    \item Our method can be integrated into various UDA frameworks, leveraging their respective advantages; here we integrate it with the JAN and SPA frameworks.
    \item We prove an upper bound on the target error that depends on the Stein discrepancy between the source and target distributions and the classification error on the source domain.
    \item Numerical experiments show that our method excels previous methods in the scarce target setting.
    Code is included in the supplemental information.
\end{itemize}




The rest of the paper is organized as follows: we begin with an overview of related works on domain adaptation and Stein discrepancy in Section \ref{sec:related-work}.
In Section \ref{sec:sd-for-da}, we review Stein discrepancy and \ac{ksd}, introduce our method for \ac{uda}, and provide a generalization bound on the target error.
Experimental results are introduced in Section \ref{sec:experiments}, and the paper concludes in Section~\ref{sec:conclusion}.


\section{Related Works}\label{sec:related-work}

We review related works in domain adaptation and Stein discrepancy, particularly the applications of the latter to machine learning and computational statistics. We refer the reader to \citep{liu2022deep} and \citep{anastasiou_steins_2023} for more comprehensive reviews of these topics.

\subsection{Domain Adaptation}

Foundational work for domain adaptation from \cite{ben-david_analysis_2007, ben-david_theory_2010} proved an upper bound for the generalization error in the target domain that depends on the error in the source domain and the distance between the source and target distributions.
% This aligns with an intuitive understanding of domain adaptation: if a model performs poorly on a dataset for which it has labels, it will likely perform poorly on a similar, unlabeled dataset.
% In addition, given a model trained on one dataset, we expect to get better results applying it to a similar dataset than to a totally unrelated one.
This bound motivated a large class of domain adaptation methods focused on feature alignment, learning feature representations that are invariant between domains but informative for classification.
The original bound used $\gH$-divergence to measure the distance between domains, but
$\gH$-divergence is difficult to estimate in practice, so later algorithms used other distances between distributions, including Wasserstein distance \citep{courty_joint_2017}, Jensen-Shannon divergence \citep{shui_novel_2022}, $\alpha$-R\'enyi distance \citep{mansour2009multiple}, and KL divergence \citep{nguyenkl}.
\Ac{mmd} was used for several \ac{uda} methods \citep{long_learning_2015, long_deep_2017, rozantsev2018beyond}, and is of particular interest because of connections between \ac{mmd} and \ac{ksd}.
Several methods also took an adversarial approach to domain adaptation \citep{ganin_unsupervised_2015,liu_coupled_2016, zhang_bridging_2019},
% Adversarial methods are formulated as a competition between a generator, which generates domain-invariant features, and a domain discriminator, which attempts to classify a sample as coming from the source or target domain, 
and they were later extended to conditional adversarial methods, inspired by conditional GANs \citep{long_conditional_2018}.
f-domain adversarial learning (FDAL) uses f-divergences to measure the distance between domains as part of an adversarial approach \cite{acuna2021f}.
% Conditional Domain Adversarial Networks (CDAN), inspired by conditional GANs, conditions the discriminator on cross-covariance of feature representations with classifier predictions, to capture which information is discriminative for the classifier \citep{long_conditional_2018}.
% In DANN, the same classifier is used for source and target samples, but other methods, such as Coupled Generative Adversarial Networks (CoGAN), propose decoupling some of the parameters between the source and target classifiers \citep{liu_coupled_2016}.
% Margin disparity discrepancy seeks to maximize the margin between each sample and the classification boundary \citep{zhang_bridging_2019}.
While adversarial methods are popular, challenges such as unstable training remain.
% Even with the same hyperparameters, adversarial models often achieve different levels of accuracy when retrained multiple times; to ameliorate this instability, the reported accuracy is often averaged over three training runs \citep{tllib, jiang2022transferability}.

There exist other types of domain adaptation methods, which can often be paired with feature alignment methods to boost accuracy.
Early but successful techniques included importance weighting, which emphasizes source samples that are similar to the target distribution \citep{pmlr-v28-gong13, Long_2014_CVPR}. % other references in confidence calibration paper
Another effective technique is pseudo-labeling target samples before training a classifier on the target domain \citep{sohn2020fixmatch}.
% another method with pseudo-labeling, Confidenc eregularized self-training, by Zou et al, 2019
% Li et. al propose minimizing class confusion to improve accuracy in both domains \cite{MCC}.
% Domain Adapted Neural Architecture Search (DA-NAS) proposed replacing the pre-trained neural networks that most methods use as feature extractors, such as ResNet, by an architecture tailored to extract transferable features \citep{li_da-nas_2024}.
Gradient harmonization, which seeks to resolve or reduce conflicts between the direction of the gradients from the two optimization goals, minimizing classification error and distance between domains, can boost performance by several percentage points on benchmark datasets \citep{huang_gradient_2024}.
Graph-based methods represent the source and target features as graphs and align the source and target domains by aligning characteristics of their graphs; graph spectral alignment (SPA) method attempts to align the graph spectra \citep{xiao2024spa}.

There are several other common domain adaptation settings besides \ac{uda}.
Semi-supervised domain adaptation has access to a small number of labels for the target distribution.
Multi-source domain adaptation attempts to leverage information from several source domains at once, while multi-target attempts to improve performance over several target domains \citep{zhao2020multisourcedomainadaptationdeep}.
Open set domain adaptation allows new classes in the target dataset that are not part of the source dataset \citep{panareda_busto_open_2017}.
Finally, domain generalization and few-shot learning are both similar to the scarce target setting for \ac{uda}.
Domain generalization extends multi-source \ac{uda} by assuming that there is no access to the target data set; the goal is to learn features that are invariant to unseen distributions \citep{wang_generalizing_2022}.
Domain generalization can be viewed as an extreme version of the scarce target setting, with no target data available, but does not leverage target information when it is available.
Few-shot learning can refer to a broad class of methods focused on learning from few data points; however, unlike the scarce target setting, the data in few-shot learning is usually labeled \citep{parnami2022learning}.


\subsection{Stein Discrepancy}

Stein's method was introduced to bound distances between probability distributions \citep{stein1972bound}. \citet{gorham_measuring_2015} built on this by formalizing Stein discrepancy as a measure of distributional difference, particularly for assessing approximate MCMC samples. Its key advantage is applicability to unnormalized distributions, making it valuable for Bayesian inference. Since then, Stein discrepancies have gained popularity in machine learning and statistics. 
Originally, computing Stein discrepancies involved a maximization step, 
but
\citet{liu_kernelized_2016}, \citet{chwialkowski_kernel_2016}, and \citet{gorham2017measuring} 
independently developed the \ac{ksd}, which provides a closed-form solution using \ac{rkhs}. \citet{gorham2017measuring} also established theoretical conditions under which convergence in \ac{ksd} guarantees weak convergence between distributions.
They demonstrated that in dimensions $d \geq 3$, commonly used kernels such as Gaussian and Mat√©rn fail to detect when a sample is not converging to the target, highlighting the importance of kernel choice in practical applications. 
Stein discrepancies have been widely adopted for constructing sample approximations, leading to influential methods such as Stein Variational Gradient Descent (SVGD) \citep{liu_stein_2019}, which iteratively updates sample locations; Stein points \citep{chen2018stein}, which constructs sample sets sequentially; and Stein thinning \citep{riabiz2022optimal}, which compresses existing samples, all aiming to minimize \ac{ksd}. 
More recently, \ac{ksd} has been extended to non-parametric settings, where the score function of an implicit model is estimated. This extension has enabled the development of two-sample tests for implicit generative models, a crucial tool in machine learning. Since such models can generate unlimited synthetic data while real datasets remain limited, these tests must handle unbalanced sample sizes \citep{xu_kernelised_2022}. This imbalance is analogous to our scarce target setting, where target domain samples are also limited compared to the source domain.
\section{Stein Discrepancy for Domain Adaptation}\label{sec:sd-for-da}
We begin with an overview of Stein discrepancies and \ac{ksd}, before describing how to apply it to domain adaptation and providing an error bound.

\subsection{Preliminaries}

The starting point for Stein discrepancy is Stein's identity for  a distribution $q$ and a Stein operator $\gA_q$. This operator $\gA_q$ acts on functions from an associated set $\gF$, known as the Stein class. Stein's identity holds for any function $f \in \gF$: $ \sE_{x \sim q} [ \gA_q f(x) ] = 0 .$
We will focus on the score-Stein operator: \[\gA_q f(x) = f(x)^\T \nabla_x \log q(x) + \nabla_x \cdot f(x),\] also called the Langevin Stein operator \citep{anastasiou_steins_2023}.
The corresponding Stein class $\gF$ contains functions that satisfy $\lim_{\| x \| \to \infty} f(x)^\T q(x) = 0$, a relatively mild condition that includes all distributions with compact support.
If the expectation over $q$ in Stein's identity is replaced by expectation over another smooth distribution $p$, then a simple calculation shows \[\sE_{x \sim p}  [ \gA_q f] =  \sE_{x \sim p} \left[ f(x) \left( \nabla_x \log p(x) - \nabla_x \log q(x) \right) \right].\]
Finding the most discriminant $f \in \gF$ gives a measure of the distance between $p$ and $q$.
\begin{definition}[Stein discrepancy]
For smooth distributions $p$ and $q$, the Stein discrepancy is defined as
\begin{equation}\label{eq-steinDisc}
        \ermS (p,q) = \sup_{f \in \gF} \sE_{x \sim p} [ \gA_q f(x) ]. 
\end{equation}
\end{definition}

The choice of the function class $\gF$ is crucial: $\gF$ should be large enough to detect differences between any two distributions of interest, while being simple enough that identifying the most discriminant $f \in \gF$ is tractable.
When $\gF$ is the unit ball of an \ac{rkhs}, the optimization has a closed form solution \citep{chwialkowski_kernel_2016, liu_kernelized_2016}.
An \ac{rkhs} is a Hilbert space $\gH$ associated with a reproducing kernel, $k(\cdot,\cdot)$, which is positive definite and satisfy the reproducing property: $f(x) = \langle f(\cdot), k(x, \cdot) \rangle_\gH$, for any $f \in \gF$.
A common choice of kernel is the radial basis function (RBF) kernel: $k(x, x') = \exp(-\| x - x' \|^2 / (2 \sigma^2))$, where $\sigma$ is the bandwidth.
Due to the reproducing property of the kernel, $\sE_{x \sim p} [ \gA_q f(x)]$ can be rewritten as an inner product: \[\sE_{x \sim p} [ \gA_q f(x)] = \left\langle f(\cdot ) , \sE_{x \sim p} [\gA_q k(x, \cdot) ] \right\rangle_\gH. \]
Maximizing over an inner product is straightforward. The closed form solution is called the \acl{ksd} (KSD): 
\begin{equation}\label{eq-kernelSteinDisc}
    \ermS (p,q) = \sE_{x, x' \sim p} [ \gA_q \gA_q k(x, x')].
\end{equation}
Given an independent, identically distributed sample $\{ x_i \}_{i=1}^n$ and a score function for $q$, denoted $s_q(x)$, \ac{ksd} can be estimated by a U-statistic:
\begin{equation*}
        \hat{\ermS}(p,q) = \frac{1}{n(n-1)} \sum_{1 \leq i \neq j \leq n} u_q(x_i, x_j),
\end{equation*}
where
\begin{align*}
        u_q (x, x' ) &= s_q (x)^\T k(x, x') s_q(x') 
        + s_q(x)^\T \nabla_{x'} k(x, x') 
        + \nabla_{x} k(x, x')^\T s_q(x')  
        + \text{trace} ( \nabla_{x, x'} k(x, x') ).
\end{align*}
This U-statistic provides a minimum-variance, unbiased estimate of $\ermS (p,q)$.
If $p \neq q$, then $\hat{\ermS}(p,q)$ converges to $\ermS(p,q)$ with rate $O(n^{-1})$, where $n$ is the number of samples from the source distribution.
If $p = q$, then $\hat{\ermS}(p,q)$ converges with rate $O(n^{-1/2})$ \citep{liu_kernelized_2016}.
The convergence rate with respect to the number of target samples depends on the choice of model for the target distribution; if the target distribution is modeled by a Gaussian, it converges with rate $O(m^{-1/2})$, where $m$ is the number of target samples.


Stein discrepancy is closely related to integral probability metrics (IPMs).
IPMs include many probability metrics of interest, several of which have been applied to previous \ac{uda} methods, and can be written as \[\rd_{\gF}(p,q) = \sup_{f \in \gF} \sE_p [ f(x) ] - \sE_q[f(x)] .\]
For instance, if $\gF$ is the set of 1-Lipschitz functions, $\rd_{\gF}(p,q)$ is the 1-Wasserstein distance between the distributions.
If $\gF$ is the unit ball of an \ac{rkhs}, then $\rd_{\gF}(p,q)$ is the \ac{mmd}.
An IPM can be rewritten as a Stein discrepancy for test functions $h$ that solve the Stein equation: $\gA_q f(x) = h(x) - \sE_q [ h(x) ]$, and a solution is guaranteed to exist for many settings of interest \citep{anastasiou_steins_2023}.
The advantage in rewriting an IPM as a Stein discrepancy is that instead of taking an expectation over both distributions, as required to calculate an IPM, calculating a Stein discrepancy only requires an expectation over one distribution; the second distribution influences the Stein discrepancy only through its score function.
Another way of viewing this advantage is that more randomness enters $\ermS (p,q)$ through the samples from $p$ than from $q$.
The consequence is that we can accurately estimate $\ermS(p,q)$ with small amounts of data from $q$.


\subsection{Methodology}\label{subsec-applicationToDA}

To apply Stein discrepancy to domain adaptation, let $x_S, x_T$ denote samples drawn from the source and target distributions $\gD_S, \gD_T$ respectively.
Since this method is focused on \ac{uda}, source labels $y_S$ are available but no target labels $y_T$ are available.

A common framework in domain adaptation, which we adopt here, is feature alignment, where the goal is to learn features that are informative for classification but invariant between domains.
To accomplish this, features $z = g(x)$ are extracted by a function $g$, which is identical in both source and target domains.
Training seeks to simultaneously minimize the transfer loss $\gL_{\text{D}}$, which measures the distance between the source and target domains, and the classification loss on the source domain $\gL_{\text{C}}$. 
Any standard classification loss can be used, such as cross-entropy loss.
Our method uses Stein discrepancy as the transfer loss and we derive two forms: an adversarial form, based on \eqref{eq-steinDisc}:
$$\gL_{\text{D}}(\gD_S, \gD_T) = \sup_{f \in \gF} \E_{\gD_S} [ \gA_{\gD_T} f(x) ]$$
and a kernelized form, based on \eqref{eq-kernelSteinDisc}:
$$\gL_{\text{D}} (\gD_S, \gD_T) = \E_{x,x' \sim \gD_S} [\gA_{\gD_T} \gA_{\gD_T} k(x,x')].$$
Training the adversarial form requires a min-max optimization because estimating $\gL_{\text{D}}(\gD_S, \gD_T)$ requires maximizing to find the most discriminant $f$.
Given an estimate of $\gL_{\text{D}} (\gD_S,\gD_T)$, $\gL_{\text{D}}(\gD_S,\gD_T)$ and $\gL_{\text{C}}$ are both minimized.
Training the kernelized form requires only minimization.
The architecture for both forms is shown in Figure \ref{fig:architecture-diagram}.


\begin{figure}[t]
    \centering
    \resizebox{0.5\textwidth}{!}{\input{architecture_ksd}}

    \vspace{2em}

    \resizebox{0.5\textwidth}{!}{\input{architecture_adv_sd}}

    \caption{Architecture for Stein discrepancy-based \ac{uda}. Source and target data, $x_S, x_T$ pass through a feature extractor $g$.
    Source features $z_S$ classified by $c$ and classification loss $\gL_{\text{C}}$ is calculated.
    Target features $z_T$ are used to estimate a target distribution; the score function is $\nabla \log \gD_T$ is used in the Stein operator $\gA_{\gD_T}$.
    Top (kernelized architecture): $\gL_{\text{D}}$ is defined according to \eqref{eq-kernelSteinDisc}: $\gL_{\text{D}}= \E_{z_S} [ \gA_{\gD_T} \gA_{\gD_T} k(z_S, z_S')]$.
    Training minimizes $\gL_{\text{C}} + \lambda \gL_{\text{D}}$ over $g, c$, where $\lambda$ is a trade-off parameter between the two losses.
    Bottom (adversarial architecture): $\gL_{\text{D}}$ is defined according to \eqref{eq-steinDisc}: $\gL_{\text{D}}= \max_{f \in \gF} \E_{z_S} [ \gA_{\gD_T} f(z_S)]$.
    Training maximizes over $f$ to estimate $\gL_{\text{D}}$ and minimizes $\gL_{\text{C}} + \lambda \gL_{\text{D}}$ over $g,c$.
    }
    \label{fig:architecture-diagram}
\end{figure}


The asymmetry in Stein discrepancy gives an advantage in the scarce target setting to Stein discrepancy-based methods over domain adaptation methods based on traditional IPMs, such as Wasserstein distance and \ac{mmd} \citep{courty_joint_2017, long_learning_2015, long_deep_2017}.
Since the expectation is taken only over $\gD_S$, and $\gD_T$ only appears via its score function, randomness enters $\gL_{\text{D}} (\gD_S, \gD_T)$ mainly through the samples $x,x' \sim \gD_S$, and $\gL_{\text{D}} (\gD_S, \gD_T)$ can be accurately estimated even when only a small amount of target data is available.


Estimating the Stein discrepancy requires a score function for $\gD_T$ to be expressed in a parametric form. This parametric form must be simple enough to admit an explicit and tractable computation of the score function, while being flexible enough to describe complex distributions from real data.
We propose three possible models for the target distribution: a Gaussian model, a Gaussian mixture model (GMM), and a variational autoencoder (VAE).

\textbf{Gaussian.} A Gaussian distribution $\gN(\mu,\Sigma)$ with mean $\mu$ and covariance $\Sigma$ has a simple score function:
\begin{equation}
   \nabla \log \gD_T(z) = - \Sigma\inv (z - \mu).
\end{equation}
In our method, we estimate the parameters using the sample mean and sample covariance from the data.

\textbf{GMM.} A GMM, a weighted sum of $k$ Gaussians with weights $\{ w_i\}_{i=1}^k$ where $\sum_{i=1}^k w_i =1$, has a score function that is closely related to the Gaussian score function:
\begin{equation}
    \nabla \log \gD_{T}(z)  = -\sum_{i=1}^k \gamma_i(z) \Sigma_i^{-1}(z - \mu_i),
\end{equation}
where
\begin{equation*}
    \gamma_i(z) = \frac{w_i \mathcal{N}(z|\mu_i, \Sigma_i)}{\sum_{j=1}^k w_j \mathcal{N}(z|\mu_j, \Sigma_j)}.
\end{equation*}
In our method, the weights and parameters of Gaussians can be estimated using the EM algorithm.

\textbf{VAE.} A VAE, which is made up of an encoder and a decoder, embeds the data in a latent space, which is usually of lower dimension and is assumed to have a simple prior distribution, often a normal distribution.
Passing a sample from the latent space through the decoder should generate a sample from the original, more complex distribution \citep{luo_understanding_2022}.
Letting $\mathbf{E}$ denote the encoder, $\mathbf{D}$ denote the decoder,  $\xi$ denote samples in the latent space, $p(z | \xi), q(\xi|z)$ denote the conditional distributions in the original and latent space respectively, and assuming $\xi \sim \gN( \mu, \Sigma)$, the score function is:
\begin{equation}\label{def-VAEScoreFunction}
         \nabla_z \log p(z)  = \E_{q (\xi | z)} \left[ \nabla_z p (z|\xi) p (z | \xi)\right],
\end{equation}
where
\begin{equation*}
\begin{aligned}
    &\quad \nabla_z p (z | \xi) p (z | \xi) \\
    &= \left [ \frac{1}{(2 \pi)^{d/2} }  \exp  \left ( - \frac{\| z - \mathbf{D}(\xi) \|^2}{2} \right )\right  ]^2  \left ( \mathbf{D}(\xi) - z  \right). 
\end{aligned}
\end{equation*}
The derivation of \eqref{def-VAEScoreFunction} is included in Appendix \ref{app-vae}. The parameters of the VAE are trained together with other parameters in our model.

There is a tradeoff when selecting between the three models. 
The Gaussian model is straightforward to calculate and has the smallest number of parameters, but cannot accurately model complex data distributions.
Using a VAE to model the target distribution is the most flexible choice for complex data distributions, but adds additional training time and instability, as it can be challenging to train the VAE to model the target distribution, while training is simultaneously updating the feature distribution to align source and target features.
In numerical results, the GMM gave the best balance between a more flexible distribution and a more stable training process.


\subsection{Bounds on Target Error}\label{subsec-boundsOnTarget}


We prove a generalization bound on the target error, making use of theoretical framework developed for domain adaptation \citep{ben-david_analysis_2007, ben-david_theory_2010, long_learning_2015} and Stein discrepancies \citep{anastasiou_steins_2023, liu_kernelized_2016}.
The proof can be found in Appendix \ref{app-proof}.


\begin{theorem}\label{thm:main}
    Let $\gD_S, \gD_T$ be probability distributions on the feature space $X$ and $\gF$ be the unit ball of an \ac{rkhs} with kernel $k(x,x')$, with $x, x' \in X$.
    Let $f^*_S$ and $f^*_T$ denote the true labeling functions associated with the source and target distributions, respectively. Let $\eps_T(f) = \E_{x \sim \gD_T} [  | f(x) - f^*_T(x) | ]$ be the error function in the target domain, and ${\eps}_S(f)$ defined similarly for the source domain.
    Then the following bound holds for any labeling function $f \in \gF$:
    \begin{equation}
        \eps_T(f) \leq \eps_S(f) + 2 \sqrt{\ermS(D_S, D_T)} + C,
    \end{equation}
    where $\ermS(\cdot,\cdot)$ is the Stein discrepancy, % defined in \eqref{eq-kernelSteinDisc}
     and $C$ depends on $\gF$ and sample size.
\end{theorem}


Theorem~\ref{thm:main} suggests that in addition to minimizing the error in the source domain, minimizing the \ac{ksd} leads to minimizing the target error.
The proof relies on a connection between \ac{ksd} and \ac{mmd} \citep{liu_kernelized_2016}.
While this property has been shown for other discrepancies, in particular \ac{mmd},  the main advantage of the Stein discrepancy has been noticed in practice.
Specifically, it has an advantage in the scarce target setting,
% ; \ac{ksd} can be viewed as a special-case of \ac{mmd}, with \ac{ksd} using an asymmetric kernel \citep{liu_kernelized_2016}.
% If \ac{ksd} is a special case of \ac{mmd}, choosing the asymmetric kernel $\gA_{\gD_T} k(x,x')$, why don't we simply use \ac{mmd}?
 with small amounts of target data and larger amounts of source data, which leads to unbalanced sample sizes.
\ac{mmd} has a high error rate in this case; numerical experiments from \citet{xu_kernelised_2022} show that the type I error rate in two-sample testing for \ac{mmd} can be as high as 100\% with samples of sizes 50 and 1000.
On the other hand, \citet{xu_kernelised_2022} show that \ac{ksd} is not negatively impacted by unbalanced sample sizes to the same extent and maintains an error rate under 10\% in the same setting.
We hypothesize that this is due to the replacement of the  expectation over $\gD_T$ in \ac{mmd} by the score function $\nabla_x \log \gD_T (x)$ in \ac{ksd}.
For small sample sizes, estimating either the score function or the expectation of an unknown distribution may have significant error, and asymptotic rates of convergence to the true score function or the true expectation are not applicable when only a few dozen samples are available.
However, estimating the score function under the assumption of a particular distribution, such as Gaussian, will provide information about the whole distribution, unlike the numerical integration in the expectation, which will only provide information about the range in which samples are available.
Assuming a large number of samples from the target distribution, information about a wider range of the distribution from the score function will provide a better alignment between the source and target distributions than an alignment that relies on the expectation.
The main concern then is whether the choice of distribution is a good model for the true data distribution; a bad choice of target distribution could introduce significant bias into the model.
In the absence of information about the true distribution, the simplest model (i.e. Gaussian) is usually best.
Our numerical results also assess the impact of choosing more complex, flexible distributions, such as GMM or VAE.



\section{Experiments}\label{sec:experiments}

We evaluate the proposed method against baseline \ac{uda} methods. 
Code for our implementation is available in the supplemental material.
% could say that it will be available on github after anonymous part is over

\subsection{Setup}\label{subsec-setup}

We use several standard datasets for \ac{uda} benchmarks, including Office31 \citep{saenko2010adapting}, Office-Home \citep{venkateswara2017deep}, and VisDA-2017 \citep{peng2017visda}.
Office31 contains 4,652 images from 31 classes of common office items, with three domains: Amazon (A), webcam (W), and DSLR (D).
Office-Home contains approximately 15,500 images across 65 categories, with four domains: Art (Ar), Clipart (Cl), Product (Pr), and Real-world (Rw).
VisDA-2017 contains over 280,000 images across 12 categories and contains two domains, synthetic (S) and real (R).
Office31 and Office-Home are evaluated on all domain-pairs; VisDA-2017 is only evaluated on transfer from synthetic images to real images. To simulate the scarce target data setting, we use only 32 samples from the target data in Office31. We use a minimum of 1\% of the target data or 32 samples from Office-Home. For VisDA-2017, which is the largest dataset with approximately 55,000 images in the target domain, we consider both scenarios maintaining 1\% and 0.1\% of the target data to ensure that the setting is truly scarce, since 1\% of the data may have too many samples.


We compare our method against Deep Adversarial Neural Network (DANN) \citep{ganin_unsupervised_2015}, Deep Adaptation Network (DAN) \citep{long_learning_2015}, Joint Adaptation Network (JAN) \citep{long_deep_2017}, Adversarial Discriminative Domain Adaptation (ADDA) \citep{Tzeng_2017_CVPR}, Conditional Domain Adversarial Network (CDAN) \citep{long_conditional_2018}, Batch Spectral Penalization (BSP) \citep{BSP}, Adaptive Feature Norm (AFN) \citep{AFN}, Maximum Classifier Discrepancy (MCD) \citep{MCD}, Margin Disparity Discrepancy (MDD) \citep{zhang_bridging_2019}, and Minimum Class Confusion (MCC) \citep{MCC}, f-Domain Adversarial Learning (FDAL) \citep{acuna2021f}, and Graph SPectral Alignment (SPA) \citep{xiao2024spa}. 
We also include Empirical Risk Minimization (ERM), a model trained only on source data and evaluated on target data, as a baseline.
The baseline methods, except for FDAL and SPA, were implemented using the Transfer Learning Library (TLL) \citep{tllib, jiang2022transferability}.
FDAL and SPA were implemented following the code made available in the original papers.


We follow the same framework for implementation as in TLL, including identical data preprocessing and augmentation,  to ensure comparable results.
We use ResNet-50 \citep{he2016deep} as the feature extractor on the Office31 and Office-Home datasets and ResNet-101 on VisDA-2017; the features are classified using a single-layer fully connected neural network.
For the adversarial methods, the discriminator $f$ is implemented as a fully-connected neural network with one layer for a Gaussian target distribution and two layers for a GMM or VAE target distribution.
An RBF kernel is used for the kernelized methods, and the code to calculate \ac{ksd} is adapted from \citep{korba_kernel_2021}.

Finally, we present both a standard implementation of our approach and an implementation that adapts SPA.
The architecture of the standard implementation closely follows that of JAN, which is an \ac{mmd}-based method.
SPA incorporates two components based on graphs of images into the loss function, in addition to classification loss and a measure of distance between domains; we adapted SPA by replacing the domain distance, which used DANN or CDAN in the original implementation, with Stein discrepancy.
The SPA-based implementation serves as a proof of concept to incorporate recent advancements UDA and to evaluate whether Stein discrepancy retains its advantage in light of these developments.
Results should be assumed to be from the standard implementation, unless they are labeled as coming from the SPA framework.



Hyperparameters were selected using the HyperOpt search algorithm \citep{bergstra2013making}, implemented in Raytune \citep{liaw2018tune}; best hyperparameters on VisDA2017 are reported in Appendix \ref{app-hyperparams}.
For other methods, we keep the hyperparameters at the default value from TLL or from the original implementations.
All the experiments are implemented on a server with A100 GPUs.

\subsection{Results}\label{subsec-results}

For all methods, we report the average accuracy of the three highest-performing model runs in the scarce target setting on each domain, and report the pooled standard deviation.
Results for the full target setting and for each domain are reported individually in Appendix \ref{app-results}.
Experimental results on Office31, Office-Home and VisDA-2017 are reported in Figures \ref{fig:office31-scarce}--\ref{fig:visda-scarce}, respectively.


\begin{wrapfigure}{r}{0.95\textwidth} % Adjust width to fit both figures
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{office31_scarce.jpg}
        \caption{Average accuracy across 6 domain pairs (with pooled standard deviation) on Office31 dataset in scarce target setting (32 samples from target data). Horizontal line denotes the highest accuracy. The rightmost seven results correspond to our Stein discrepancy-based method.}
        \label{fig:office31-scarce}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{officehome_1pct_tgt_data.jpg}
        \caption{Average accuracy across 12 domain pairs (with pooled standard deviation) on Office-Home dataset in scarce target setting (minimum of 1\% of target data or 32 samples). Horizontal line denotes the highest accuracy. The rightmost seven results correspond to our Stein discrepancy-based method.}
        \label{fig:officehome-scarce}
    \end{minipage}
\end{wrapfigure}

\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \includegraphics[width=0.45\textwidth]{visda_scarce.jpg}
    \caption{Accuracy and standard deviation on VisDA-2017 dataset, with 1\% (orange) and 0.1\% (blue) of target data available during training. Horizontal lines denote the highest accuracy. The rightmost seven results correspond to our Stein discrepancy-based method.}
    \label{fig:visda-scarce}
\end{wrapfigure}


We report accuracy for seven Stein discrepancy-based methods at the right of each figure, including five methods that follow the framework from JAN and two that use the SPA framework.
The AGAU, AGMM, and AVAE methods use the adversarial loss function, with Gaussian, GMM, and VAE models for the target distribution respectively.
The KGAU and KGMM methods use the kernelized loss function, with Gaussian and GMM target distributions.
The kernelized method failed to sufficiently learn the target distribution with a VAE and the accuracy was much worse than other methods.

Across all three benchmark datasets, Stein discrepancy-based methods outperform comparable methods in the scarce target setting; in particular the standard implementation of Stein discrepancy methods framework outperform all but the SPA methods, and Stein discrepancy-based SPA methods outperform traditional SPA.
This suggests that Stein discrepancy can be combined with other \ac{uda} methods to provide an advantage in the scarce target setting, while benefiting from the advantages of the method it is being combined with.
On each dataset, the Stein discrepancy-based methods with SPA framework are the highest performing methods, with SPA-SD-KGAU achieving 90\% accuracy on Office31 and both target distributions achieving 72\% accuracy on Office-Home.
On VisDA2017, SPA is the best-performing method in the 1\% setting, achieving 78\% accuracy.
This suggests that 1\% of the target data from VisDA2017 (550 samples) may be abundant enough that the Stein discrepancy gives limited advantage and other methods can perform well, although SPA-SD-KGAU is competitive with original SPA in this setting.
However, in the 0.1\% setting, the Stein discrepancy-based methods outperform SPA, with SPA-SD-KGAU achieving 74\% accuracy.
A significant drawback shared by all of the SPA methods is the high variance in the scarce target setting, notably much higher variance than the Stein discrepancy-based methods with JAN framework.
This may suggest the need to continue evaluating frameworks beyond JAN and SPA, to identify a framework that balances the high performance of SPA with the stability of JAN.

The results also demonstrate the importance of the choice of target distribution.
In the standard implementation, the GMM target distribution outperforms the Gaussian target distributions across kernelized and adversarial methods, indicating that a Gaussian is not flexible enough to capture the feature distribution.
The VAE target distribution is more flexible than the GMM, but more challenging to learn and less stable during training.
The numerical results suggest that for a simple dataset like Office31, this trade-off does not favor the VAE distribution, since it fails to outperform the Gaussian.
On larger, more complex datasets such as Office-Home, the VAE and the Gaussian are competitive.
On the largest dataset, VisDA-2017, the VAE target distribution outperforms the Gaussian when larger amounts of data are present, in the 1\% setting, but the Gaussian and GMM perform better on the smallest amount of data in the 0.1\% setting.
On the other hand, in the SPA framework, the Gaussian distributions outperforms the GMM distribution, perhaps indicating that the training is less sensitive to the choice of target distribution, or less reliant on the Stein discrepancy overall, since it includes several other loss components specific to SPA.
% This indicates a possible advantage for the adversarial form of Stein discrepancy for \ac{uda} over the kernelized form, in being able to learn more complex distributions, since the kernelized method failed to learn with a VAE target distribution.

% Finally, the Gaussian and GMM methods show a small decline in accuracy of approximately 1\% when reducing the target data from 100\% to 1\%.
% However, the accuracy declines by 3\% for the VAE method, indicating that there is not enough data to accurately learn the more complex target distribution model.


To further explore the affect of the amount of available target data on \ac{uda} methods, we evaluate the methods on the Office31 dataset at the following levels of target data: $100\%$, $75\%$, $50\%$, $25\%$, $10\%$, $5\%$, $1\%$.
We include the two highest-performming methods from each implementation: SD-AGMM, SD-KGMM, SPA-SD-KGAU, and SPA-SD-KGMM, as well as benchmark methods.
We display the methods on the average across domains in Figure \ref{fig:target-pct-comparison}; results for each domain individually are included in Figure \ref{fig:target-pct-comparison-all-domains} of Appendix \ref{app-results}.

\begin{wrapfigure}{r}{0.6\textwidth}
    \centering
    \includegraphics[width=0.6\columnwidth]{all_methods_mean.png}
    \caption{Comparison of methods at different levels of target data on the Office31 dataset, averaged across domains.
    Stein discrepancy-based methods (solid lines) have a smaller decline in accuracy as the amount of target data is reduced.
    Methods with circular markers (SD-AGMM, SPA-SD-KGMM, JAN, and SPA) were chosen to help distinguish between Stein discrepancy-based methods that perform similarly, and to highlight the original methods our implementations follow most closely.}
    \label{fig:target-pct-comparison}
\end{wrapfigure}

Most methods see a decline in accuracy when the available data is below $10\%$.
The Stein discrepancy-based methods are the most stable with the change of percentages, and have a minimal decline in accuracy as the amount of target data decreases.
This suggests that Stein discrepancy-based methods have an advantage when target data is very scarce, which aligns with the results on the VisDA-2017 dataset, which showed little advantage for Stein discrepancy-based methods at the 1\% level but significant advantage at the 0.1\% level.


\section{Conclusion}\label{sec:conclusion}
We have proposed a novel method for \ac{uda} based on Stein discrepancy, and derived a theoretical generalization bound that motivates minimizing the Stein discrepancy.
% We prove a generalization bound, upper bounding the classification error on the target domain by the error on the source domain and the Stein discrepancy between the source and target distributions.
The proposed method is adaptable and has both a kernelized form and a non-kernelized, adversarial form, with several possible parametric models for the target distribution: Gaussian, GMM, or VAE.
In numerical experiments, our method outperformed baseline methods in the scarce target setting, where only a small amount of target data is available.
% The scarce target setting is relevant to applications including EEGs and online user training.

Directions for future work include adapting non-parametric \ac{ksd} from evaluation of implicit generative models to domain adaptation \citep{xu_kernelised_2022}.
The main advantage is avoiding an explicit score function, eliminating the need to model the target distribution.
While the current method benefits from the simplicity of a Gaussian or GMM target distribution, we will assess whether the added flexibility of a non-parametric approach is a worthwhile trade-off. % evaluate whether 
Following \citet{gorham2017measuring}, who showed that commonly-used kernels fail to detect non-convergence of distributions in higher dimensions, our future work will investigate the impact of using a kernel such as the inverse multi-quadratic kernel.
We will also adapt Stein discrepancy to other frameworks, in addition to JAN and SPA.
Another avenue for theoretical future work is deriving bounds on the error from replacing the exact Stein discrepancy by an empirical estimate, focusing on the affect of unbalanced sample sizes from the source and target distributions.

% Finally, future work will involve evaluating our method on more challenging domain adaptation datasets such as OfficeHome \citep{venkateswara2017deep} and DomainNet \citep{peng2019moment}, and evaluating methods for regularization with the goal of improving stability to different choices of hyperparameters.




\section*{Acknowledgements}

We thank Lester Mackey for his insightful comments on the historical development of Stein discrepancies. 
A.V. and G.L. thank Larry Goldstein for his inspiring talk and discussion on Stein discrepancy and kernels.  
A.V. and G.L. were partially supported by NSF award DMS 2427955.
D.Z. was partially supported by National Natural Science Foundation of China (NSFC) award 12301117.

\bibliography{DomainAdaptationStein}
\bibliographystyle{unsrtnat}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{appendix}


\end{document}

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun}
