\section{Proof of Theorem \ref{thm:main}}\label{app-proof}

\begin{proof}
We first show that \ac{ksd} can be viewed as a special case of squared \ac{mmd}. 
Since \ac{ksd} provides the exact Stein discrepancy when maximizing over a \ac{rkhs}, showing the relationship between \ac{ksd} and \ac{mmd} establishes the bound for the general Stein discrepancy.

Since $\gF$ is the unit ball of an RKHS, the MMD between $\gD_S$ and $\gD_T$ can be written as
\begin{equation*}
% \label{eq:MMD_def}
d_{\text{MMD}}(\gD_S, \gD_T) = \sup_{f \in \gF} \left |\E_{x \sim \gD_S} [f(x)]  - \E_{x \sim \gD_T} [f(x)] \right |.
\end{equation*}
Moreover, \citet{gretton_kernel_2012} showed that the squared MMD can be written in a kernelized form 
\begin{equation*}
    d_{\text{MMD}}^2(\gD_s, \gD_T) = \E_{x, x' \sim \gD_S} [k(x, x')] - 
    2 \E_{x \sim \gD_S, y \sim \gD_T} [k(x,y)] + \E_{y, y' \sim \gD_T} [ k(y,y')]. 
\end{equation*}


% Choose $\gA_{\gD_T} \gA_{\gD_T} k(x,x')$ to be the kernel of an \ac{rkhs}.
\citet{liu_kernelized_2016} verified that $\gA_{\gD_T} \gA_{\gD_T} k(x,x')$ is a valid positive definite kernel and is contained in the Stein class of $\gD_S$, as long as the same is true for $k(x,x')$.
% verify in stein class as part of proof of thm 3.8, see appendix
% verify positive definite in thm 3.6
Using Stein's identity, which states that $\E_q [ \gA_q f(x) ] = 0$, it follows that the squared \ac{mmd} and Stein discrepancy are equivalent:
\begin{align*}
    d_{\text{MMD}}^2(\gD_s, \gD_T) &  = \E_{x, x' \sim \gD_S} [\gA_{\gD_T} \gA_{\gD_T} k(x,x')] \\
    & \quad - 2 \E_{x \sim \gD_S} \E_{y \sim \gD_T} [\gA_{\gD_T} \gA_{\gD_T} k(x,y)] \\
    & \quad + \E_{y \sim \gD_T} \E_{y' \sim \gD_T} [ \gA_{\gD_T} \gA_{\gD_T} k(y,y')] \\
    & = \E_{x, x' \sim \gD_S} [\gA_{\gD_T} \gA_{\gD_T} k(x,x')] \\
    & \quad - 2 \E_{x \sim \gD_S} [0] + \E_{y \sim \gD_T} [0] \\
    & = \E_{x, x' \sim \gD_S} [\gA_{\gD_T} \gA_{\gD_T} k(x,x')] \\
    & = \ermS(\gD_S, \gD_T).
\end{align*}

The proof follows from combining the above equation and the result of \citet{long_learning_2015}, which states that $$\eps_T(f) \leq \eps_S(f) + 2 d_{\text{MMD}}(\gD_S, \gD_T) + C,$$ where $C$ depends on the VC dimension of $\gF$, the sample size from each distribution, and the smallest possible test errors in both domains.
% the error, $\lambda$, in the ideal joint hypothesis, which is defined as follows: $$\lambda = \min_{f \in \gF} \eps_S( f) + \eps_T(f).$$
\end{proof}

\section{Additional Experimental Results}\label{app-results}

More detailed experimental results are provided here.
We provide figures illustrating results of the full target setting, where we use all the available target data in each dataset, to complement the figures of the scarce target setting provided in the main text.
Figures \ref{app-fig:office31-full}, \ref{app-fig:officehome-full}, and \ref{app-fig:visda-full} show the accuracy in the full target setting for the Office31, Office-Home, and VisDA-2017 datasets respectively. Together, these figures provide a summary of the results across all possible domain pairs in each dataset (6 pairs in Office31, 12 pairs in Office-Home, 1 pair in VisDA2017).

In addition, we also provide tables that show the accuracy and standard deviation on each domain pair.
On individual domains, the accuracy reported is the highest accuracy achieved by each method, and the standard deviation is taken across the three highest performing model runs.
The accuracy reported averaged across domain is an average of the three top model runs on each domain, and the pooled standard deviation across domains is reported.
The results for Office31 are given in Tables \ref{table-office31-mean}-\ref{table-office31-1}, for Office-Home in Tables \ref{table-officehome-mean}-\ref{table:oh-1pct}, and  for VisDA-2017 in Table \ref{table-visda}.
The highest accuracy for each domain is bolded and the second-highest is underlined.
The ``Full" column refers to training on all of the available target data, the ``Scarce" column refers to the scarce target setting, in which the model was trained on 1\% of the available target data, with a minimum of 32 samples of target data.
VisDA-2017 contains the most samples per domainin the target domain (approximately 55,000), so we report accuracy on 100\%, 1\% and 0.1\% of target data to ensure that the scarce target setting is represented.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{office31_full.jpg}
    \caption{Average accuracy across domains and pooled standard deviation on Office31 dataset in full target setting (trained on 100\% of available target data). Horizontal line denotes the highest accuracy.}
    \label{app-fig:office31-full}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{officehome_100pct_tgt_data.jpg}
    \caption{Average accuracy across domains and pooled standard deviation on Office-Home dataset in full target setting (trained on 100\% of available target data). Horizontal line denotes the highest accuracy.}
    \label{app-fig:officehome-full}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{visda_100pct_tgt_data.jpg}
    \caption{Average accuracy across domains and pooled standard deviation on VisDA-2017 dataset in full target setting (trained on 100\% of available target data). Horizontal line denotes the highest accuracy.}
    \label{app-fig:visda-full}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[ht!]
    \centering
    \scriptsize
    \caption{Accuracy on Office31 dataset, averaged across six domain pairs. Full refers to accuracy trained on full target dataset; scarce refers to accuracy trained in scarce target setting (32 target samples).}
    \label{table-office31-mean}
    \input{office31_mean}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[ht!]
    \centering
    \scriptsize
    \caption{Accuracy and standard deviation on Office31 in the full target setting (100\% of target data available). The best accuracy is bolded and the second best is underlined.}
    \label{table-office31-100}
    \input{office31_100}
    \vspace{2.5em}
    \centering
    \caption{Accuracy and standard deviation on Office31 in the scarce target setting (32 training samples available; all domains are small enough that 1\% of target data would be less than 32 samples). The best accuracy is bolded and the second best is underlined.}
    \label{table-office31-1}
    \input{office31_1}
    \vspace{2.5em}
\end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
    \centering
    \scriptsize
    \caption{Accuracy on Office-Home, averaged across 12 domain pairs.  Full refers to accuracy trained on full target dataset (100\% of available data); scarce refers to accuracy trained in scarce target setting (minimum of 1\% of target data or 32 samples).}
    \label{table-officehome-mean}
    \input{office_home_mean}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table*}[h]
    \centering
    \scriptsize
    \caption{Results on OfficeHome dataset in full target setting (trained on 100\% of target data). The first six domain pairs are shown in the top half of the table; the remaining six appear in the bottom half.}
    \label{table:oh-100pct}
    \input{office_home_100}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table*}[h]
    \centering
    \scriptsize
    \caption{Results on OfficeHome dataset in scarce target setting (trained on minimum of 1\% of target data or 32 samples). The first six domain pairs are shown in the top half of the table; the remaining six appear in the bottom half.}
    \label{table:oh-1pct}
    \input{office_home_001}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[ht!]
    \centering
    \scriptsize
    \caption{Accuracy and standard deviation on VisDA-2017 from sythetic domain to real domain, with 100\%, 1\% and 0.1\% of target data available.}
    \label{table-visda}
    \input{visda_s2r}
\end{table}

Finally, since Figure \ref{fig:target-pct-comparison} in the main text only includes the average performance across all domains in the Office31 dataset, we provide a more detailed breakdown in
Figure \ref{fig:target-pct-comparison-all-domains}, which displays the performance on each domain individually.
We report the highest performing model run on each target percentage for each method.



\begin{figure}[ht]
    \centering
    \begin{tabular}{ccc}
        % First row
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_A2D_no_legend.png}
            \caption{A2D}
        \end{subfigure} & 
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_A2W_no_legend.png}
            \caption{A2W}
        \end{subfigure} &
        \raisebox{0.5\height}{  % This raises the legend to align with the top
            \begin{minipage}[t]{0.1\textwidth}
                \centering
                \includegraphics[width=\textwidth]{all_methods_mean_legend.png}
            \end{minipage}
        }
        \\
        % Second row
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_D2A_no_legend.png}
            \caption{D2A}
        \end{subfigure} &
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_D2W_no_legend.png}
            \caption{D2W}
        \end{subfigure} &
        \\
        % Third row
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_W2A_no_legend.png}
            \caption{W2A}
        \end{subfigure} & 
        \begin{subfigure}[t]{0.4\textwidth}
            \centering
            \includegraphics[width=\textwidth]{all_methods_W2D_no_legend.png}
            \caption{W2D}
        \end{subfigure} &
        \\
    \end{tabular}
    \caption{Comparison of UDA methods trained on varying percentages of target data, averaged across 6 domain pairs in Office31 dataset. Stein discrepancy-based methods (solid lines) have a smaller decline in accuracy at low levels of target data.}
    \label{fig:target-pct-comparison-all-domains}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Derivation of VAE Score Function}\label{app-vae}

We present the detailed derivation of the score function of a VAE, which is one of the models used for the target distribution.
A VAE is composed of an encoder \textbf{E}, which maps inputs $z$ to latent features $\xi$, and a decoder \textbf{D} which maps from the latent space back to the input space.
Let $p(z)$ be the data distribution. 
Let $p(z | \xi) = \textbf{D} (\xi)$ represent the likelihood and $q(\xi | z) =\textbf{E}(z)$ represent the approximate posterior.
% Assume that the latent distribution is multivariate Gaussian: $q(\xi | z) \sim \gN (\mu, \Sigma)$.
We estimate the score function $\nabla_z \log p(z)$ as follows:
\begin{align}
    \nabla_z \log p(z) & = \frac{1}{p(z)} \int \nabla_z p(z, \xi) d\xi \nonumber \\
    & = \frac{1}{p(z)} \int \frac{\nabla_z p(z, \xi) q( \xi | z)}{q(\xi | z)} d \xi \nonumber \\
    & = \E_{q ( \xi | z)} \left [ \frac{\nabla_z p(z | \xi) p (\xi)}{p(z) q (\xi | z) }\right] \nonumber \\
    & \approx \E_{q(\xi | z) } \left [ \frac{\nabla_z p(z | \xi) p(\xi) }{p(z) p (\xi | z) }\right ] \nonumber \\
    & = \E_{q(\xi | z)} \left [ \nabla_z p(z|\xi) p(z | \xi) \right ]. \label{eq:eqdpp}
\end{align}
Assuming $p(z | \xi)$ is a Gaussian distribution:
\begin{align}
    p(z| \xi) & = \frac{1}{(2 \pi)^{d/2}} \exp \left \{ - \frac{\| z - \mathbf{D}(\xi) \| ^2}{ 2} \right \}, \label{eq:pzxi}
\end{align}
where $d$ is the dimension of the feature space.
% Given $p(z | \xi)$, obtaining $\nabla_z p(z | \xi)$ is a simple calculation, and combining the pieces gives
Combining \eqref{eq:eqdpp} and \eqref{eq:pzxi} yields
\begin{equation*}
    \nabla_z \log p(z) = \left[ \frac{1}{(2 \pi)^{d/2}} \exp \left \{ -\frac{\| z - \mathbf{D}(\xi) \|^2  }{2} \right \} \right]^2 (\mathbf{D}(\xi) - z),
\end{equation*}
where $\xi \sim \mathbf{E}(z)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Algorithms}\label{app-algorithms}

In this section, we present the algorithms used for Stein discrepancy based UDA.
The main algorithm is presented in Section \ref{subsec:main-alg}.
Since calculating Stein discrepancy requires an explicit score function for the target distribution, we also provide algorithms to compute the score function for the three target distributions we investigated, namely Gaussian, GMM, and VAE, in Section \ref{subsec:score-function}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Main Algorithm}\label{subsec:main-alg}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present the algorithm for Stein-discrepancy based domain adaptation.
The kernelized version is presented in Algorithm \ref{alg:kernelized}, and the non-kernelized version is presented in Algorithm \ref{alg:adversarial}.
Letting $z = g(x)$ be the features extracted from the input data $x$, the domain loss $\gL_D$ for the kernelized version is computed as in Equation 2 of the main paper:
\begin{equation*}
\gL_{\text{D}}= \E_{z_S} [ \gA_{\gD_T} \gA_{\gD_T} k(z_S, z_S')].
\end{equation*}
The domain loss $\gL_D$ for the non-kernelized version is computed as in Equation 1 of the main paper:
\begin{equation*}
    \gL_{\text{D}}= \max_{f \in \gF} \E_{z_S} [ \gA_{\gD_T} f(z_S)].
\end{equation*}
Any standard classification loss, such as cross-entropy, can be used for $\gL_C$.


% todo: fix alignment in for loop
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[h]

% \SetKwInOut{Input}{input}
% \SetKwInOut{Output}{output}
        
% \caption{Algorithm for kernelized Stein-discrepancy based domain adaptation.}\label{alg:kernelized}
% \Input{Source, target training data $\gD_S, \gD_T$ \\
% Feature extractor $g$\, with pre-trained parameters $\theta_g$\\
% Classifier $h$, with randomly initialized parameters $\theta_h$\\
% Reproducing kernel $k$\\
% Learning rate $\eta$, trade-off parameter $\lambda$, epochs $E$, max iterations $N$}
% \Output{Trained parameters $\theta = (\theta_g, \theta_h)$}
% \For{$n = 1$ \KwTo $N$}{
%     \, Sample batches $(x_S, y_S)$ from $\gD_S$ and $(x_t)$ from $\gD_T$\;\\
%     Compute classification loss $\gL_C(\theta; h(g(x_S)), y_S)$\;\\
%     Compute score function of $\gD_T$ from features $z_T = g(x_T)$\;\\
%     \, Compute transfer loss $\gL_D(\theta; x_S) = \E_{g(x_S)} [ \gA_{\gD_T} \gA_{\gD_T} k(g(x_S), g(x_S'))]$\;\\
%     Compute combined loss $\gL = \gL_C + \lambda \gL_D$\;\\
%     Compute gradients $\nabla_\theta \gL$\;\\
%     Update parameters $\theta \gets \theta - \eta \nabla_\theta \gL$
%     % \If{convergence criterion met}{
%     %     \textbf{break}\;
%     % }
% }
% \KwRet{$\theta$}
% \end{algorithm}

\begin{algorithm}[tb]
   \caption{Kernelized Stein-Discrepancy Based Domain Adaptation}
   \label{alg:kernelized}
\begin{algorithmic}
   \STATE {\bfseries Input:} Source and target training data $\gD_S, \gD_T$, feature extractor $g$ with pre-trained parameters $\theta_g$, classifier $h$ with randomly initialized parameters $\theta_h$, reproducing kernel $k$, learning rate $\eta$, trade-off parameter $\lambda$, epochs $E$, max iterations $N$
   \STATE {\bfseries Output:} Trained parameters $\theta = (\theta_g, \theta_h)$
   \FOR{$n = 1$ {\bfseries to} $N$}
       \STATE Sample batches $(x_S, y_S)$ from $\gD_S$ and $(x_T)$ from $\gD_T$
       \STATE Compute classification loss $\gL_C(\theta; h(g(x_S)), y_S)$
       \STATE Compute score function of $\gD_T$ from features $z_T = g(x_T)$
       \STATE Compute transfer loss $\gL_D(\theta; x_S) = \E_{g(x_S)} [ \gA_{\gD_T} \gA_{\gD_T} k(g(x_S), g(x_S'))]$
       \STATE Compute combined loss $\gL = \gL_C + \lambda \gL_D$
       \STATE Compute gradients $\nabla_\theta \gL$
       \STATE Update parameters $\theta \gets \theta - \eta \nabla_\theta \gL$
   \ENDFOR
   \STATE {\bfseries Return:} $\theta$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{algorithm}[tb]
   \caption{Non-Kernelized Stein-Discrepancy Based Domain Adaptation}
   \label{alg:adversarial}
\begin{algorithmic}
   \STATE {\bfseries Input:} Source and target training data $\gD_S, \gD_T$, feature extractor $g$ with pre-trained parameters $\theta_g$, classifier $h$ with randomly initialized parameters $\theta_h$, discriminator $f$ with randomly initialized parameter $\theta_f$, learning rate $\eta$, trade-off parameter $\lambda$, max iterations $N$
   \STATE {\bfseries Output:} Trained parameters $\theta = (\theta_g, \theta_h)$
   \FOR{$n = 1$ {\bfseries to} $N$}
       \STATE Sample batches $(x_S, y_S)$ from $\gD_S$ and $(x_T)$ from $\gD_T$
       \STATE Compute classification loss $\gL_C(\theta; h(g(x_S)), y_S)$
       \STATE Compute score function of $\gD_T$ from features $z_T = g(x_T)$
       \STATE Compute transfer loss $\gL_D(\theta; x_S) = \E_{g(x_S)} [ \gA_{\gD_T} f(g(x_S))]$
       \STATE Compute combined loss $\gL = \gL_C + \lambda \gL_D$
       \STATE Compute gradients $\nabla_{\theta_f, \theta_g, \theta_h} \gL$
       \STATE Update $f$ parameters (maximization) $\theta_f \gets \theta_f + \eta \nabla_{\theta_f} \gL$
       \STATE Update $g, h$ parameters (minimization) $(\theta_g, \theta_h) \gets (\theta_g, \theta_h) - \eta \nabla_{(\theta_g, \theta_h)} \gL$
   \ENDFOR
   \STATE {\bfseries Return:} $\theta = (\theta_g, \theta_h)$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Score Function}\label{subsec:score-function}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, we present algorithms to calculate the score function for the three models used for the target distribution: Gaussian, GMM, and VAE.
For simplicity of notation, the score functions are computed using the features $z = g(x)$.
In the VAE algorithm, mean-squared error is a standard choice for the reconstruction loss $\gL_{\text{VAE}}$, but other loss functions such as mean absolute error can also be used.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[tb]
   \caption{Score Function for Gaussian Target Distribution}
   \label{alg:score-gaussian}
\begin{algorithmic}
   \STATE {\bfseries Input:} Target features $z_T$
   \STATE {\bfseries Output:} Score function $\nabla \log \gD_T$
   \STATE Compute sample mean $\mu$ and covariance $\Sigma$ of $z_T$
   \STATE {\bfseries Return:} $\nabla \log \gD_T(z) = - \Sigma^{-1} (z - \mu)$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{algorithm}[tb]
   \caption{Score Function for Gaussian Mixture Target Distribution}
   \label{alg:score-GMM}
\begin{algorithmic}
   \STATE {\bfseries Input:} Target features $z_T$, number of components $K$, maximum iterations $N$
   \STATE {\bfseries Output:} Score function $\nabla \log \gD_T$
   \STATE Randomly initialize $\rvmu = \{\mu_k\}_{k=1}^K$, $\rvSigma = \{\Sigma_k\}_{k=1}^K$, $\rvw = \{w_k\}_{k=1}^K$; ensure $\Sigma_k$ is positive definite for all $k$
   \FOR{$n = 1$ {\bfseries to} $N$}
       \STATE Compute log probability $\gL(\rvmu, \rvSigma, \rvw; z_T)$
       \STATE Compute gradients $\nabla_{\rvmu, \rvSigma, \rvw} \gL$
       \STATE Update means $\rvmu \gets \rvmu - \nabla_{\rvmu} \gL$
       \STATE Update covariances $\rvSigma \gets \rvSigma - \nabla_{\rvSigma} \gL$
       \STATE Update weights $\rvw \gets \rvw - \nabla_{\rvw} \gL$
   \ENDFOR
   \STATE {\bfseries Return:} $\nabla \log \gD_T(z) = -\sum_{i=1}^K \gamma_i(z) \Sigma_i^{-1}(z - \mu_i)$, where $\gamma_i(z) = \frac{w_i \mathcal{N}(z|\mu_i, \Sigma_i)}{\sum_{j=1}^K w_j \mathcal{N}(z|\mu_j, \Sigma_j)}$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{algorithm}[tb]
   \caption{Score Function for VAE Target Distribution}
   \label{alg:score-VAE}
\begin{algorithmic}
   \STATE {\bfseries Input:} Target features $z_T$, VAE model composed of encoder $\mathbf{E}$ and decoder $\mathbf{D}$, maximum iterations $N$
   \STATE {\bfseries Output:} Score function $\nabla \log \gD_T$
   \STATE Randomly initialize parameters $\theta_E, \theta_D$ for $\mathbf{E}$ and $\mathbf{D}$
   \FOR{$n = 1$ {\bfseries to} $N$}
       \STATE Compute latent features $\xi_T = \mathbf{E}(z_T)$
       \STATE Compute reconstruction loss $\gL_{\text{VAE}}(\theta_E, \theta_D; z_T, \mathbf{D}(\xi_T))$
       \STATE Compute gradients $\nabla_{\theta_E, \theta_D} \gL_{\text{VAE}}$
       \STATE Update encoder parameters $\theta_E \gets \theta_E - \eta \nabla_{\theta_E} \gL_{\text{VAE}}$
       \STATE Update decoder parameters $\theta_D \gets \theta_D - \eta \nabla_{\theta_D} \gL_{\text{VAE}}$
   \ENDFOR
   \STATE {\bfseries Return:} $\nabla \log \gD_T(z) = \E_{q(\xi|z)}[\nabla_z p(z|\xi) p(z|\xi)]$
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Hyperparameters}\label{app-hyperparams}

We report the best hyperparameters for each of the Stein methods in the JAN framework on the VisDA-2017 dataset as a representative example of the hyperparameters in Tables \ref{table-hyperparams-ker-gau}-\ref{table-hyperparams-adv-vae}.
Stein discrepancy-based methods in the SPA framework were observed to be less sensitive to hyperparameters than in the JAN framework; for Gaussian target distribution, the kernel bandwidth was set to 10.
For GMM target distribution, the bandwidth was set to 1.
All other hyperparameters were left at the default from the original SPA implementation.
For other methods, hyperparameters were selected using a randomized search algorithm, HyperOpt, and were optimized using RayTune, a machine learning and  hyperparameter tuning library in Python.
The ranges from which the hyperparameters were selected can be found in the code, available in the supplementary materials.

The trade-off parameter refers to the trade-off between the classification loss and the transfer loss.
LR refers to learning rate and WD refers to weight decay.
Cov. Eps. refers to covariance epsilon, which is used to regularize covariance matrices that become non-positive definite during training.
Bottleneck refers to the dimension of the features; for methods where it is not reported it was left at the default value of 256.
Bandwidth refers to the bandwidth of the kernel in kernelized methods; all experiments were conducted with a radial basis function (RBF) kernel.
Adv LR, VAE LR, and GMM LR refer to the learning rates for the discriminator network $f$ (in adversarial methods only), the VAE, and the GMM, for applicable methods.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[ht!]
    \centering
            \scriptsize

    \caption{Best hyperparameters for kernelized method with Gaussian target distribution, on VisDA-2017 dataset.}
    \label{table-hyperparams-ker-gau}
    \input{ker_gau}
    \caption{Best hyperparameters for adversarial method with Gaussian target distribution, on VisDA-2017 dataset.}
    \label{table-hyperparams-adv-gau}
    \input{adv_gau}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{sidewaystable*}[ht!]
    \centering
    \scriptsize
    \caption{Best hyperparameters for kernelized method with GMM target distribution, on VisDA-2017 dataset.}
    \label{table-hyperparams-ker-gmm}
    \input{ker_gmm}
        \caption{Best hyperparameters for adversarial method with GMM target distribution, on VisDA-2017 dataset.}
    \label{table-hyperparams-adv-gmm}
    \input{adv_gmm}
        \caption{Best hyperparameters for kernelized method with VAE target distribution, on VisDA-2017 dataset.}
    \label{table-hyperparams-adv-vae}
    \input{adv_vae}
\end{sidewaystable*}

