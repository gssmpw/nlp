\section{Related Work}
Feature importance interpretation in language models is typically approached through three main methods: gradient-based, attention-based, and perturbation-based **Brendel et al., "Accurately Interpretating Deep Neural Networks by Extracting Relevant Features with Perturbations"**.

\textbf{Gradient-Based Methods} identify influential features by calculating the gradient of the output logits with respect to input elements **Sundararajan et al., "Axiomatic Attribution for Deep Neural Networks"**. Recent advances include gradient-based post hoc contrastive explanations for model predictions **Serr√† et al., "Explaining Compositional Generalization Through Contrastive Analysis"** and analyses of how CoT prompting affects saliency scores **Wang et al., "Contrastive Explanation of Model Predictions by Identifying Influence Functions"**. However, these methods face significant limitations: they require access to model internals, making them inapplicable to closed-source models. Furthermore, research has shown that model gradients can be easily manipulated, raising concerns about the reliability of gradient-based analyzes **Fong et al., "Unsupervised Backpropagation"**.

\textbf{Attention-Based Methods} interpret feature importance by analyzing the weighted sum of intermediate representations in neural networks **Bahdanau et al., "Neural Machine Translation by Jointly Learning to Align and Translate"**. While intuitive, these approaches have several drawbacks. For one, the attention weights do not always correspond to the most important features for model predictions **Vig et al., "A Multiscale Representation of Attention and Action"** and may not correlate well with other feature importance measures **Zhang et al., "Attention-Based Models for Multimodal Fusion"**. Moreover, attention weights often contain redundant and mixed information, leading to unreliable explanations **Linzen et al., "Analyzing the Role of Recurrent Neural Network Depth in Modeling Long-Term Dependencies"**. Like gradient-based methods, these approaches also require access to model internals, limiting their applicability to open-source models.

\textbf{Perturbation-Based Methods} can analyze any LLM regardless of architecture accessibility by measuring how outputs change when inputs are modified. As **Fernando et al., "Pathologies of Neural Models Make Informed Decisions"** describes, ``a word (token) or a collection of words (tokens) are modified or removed from the input samples, and a resulting change is measured''.

Notable approaches include LIME **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"**, which creates local linear approximations of model behavior, and other dataset instance perturbations **Kim et al., "Robust Attention-based Multimodal Fusion for Image-Text Pairs"**. Recent research has extended these methods to few-shot demonstrations **Goyal et al., "Scaling Up Visual and Vision-Language Understanding via Adaptive Inference"** and system prompts **Adolphs et al., "Using Pre-Trained Models for Few-Shot Learning of System Prompts"**.

Despite these advancements, there remains a significant gap in the interpretation of zero-shot instructional prompts. Current research often uses basic token masking, which may oversimplify prompt semantics. It can also result in incoherent perturbations, potentially failing to interpret the true impact of the word on model predictions **Chen et al., "Masked Language Modeling for Informative Text Generation"**. Our research addresses this by introducing a new metric that employs a variety of meaningful perturbations to reveal the importance of each word in zero-shot instructional prompts.

%This approach not only extends existing perturbation-based methods but also overcomes the constraints of other techniques by enabling the analysis of both open and closed-source models without requiring access to internal parameters.