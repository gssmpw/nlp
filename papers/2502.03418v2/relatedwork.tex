\section{Related Work}
Feature importance interpretation in language models is typically approached through three main methods: gradient-based, attention-based, and perturbation-based \cite{choudhary2022interpretation}.

\textbf{Gradient-Based Methods} identify influential features by calculating the gradient of the output logits with respect to input elements \cite{wallace-etal-2019-allennlp}. Recent advances include gradient-based post hoc contrastive explanations for model predictions \cite{yin2022interpreting,ferrando-etal-2023-explaining} and analyses of how CoT prompting affects saliency scores \cite{wu2023analyzing}. However, these methods face significant limitations: they require access to model internals, making them inapplicable to closed-source models. Furthermore, research has shown that model gradients can be easily manipulated, raising concerns about the reliability of gradient-based analyzes \cite{wang2020gradient}.

\textbf{Attention-Based Methods} interpret feature importance by analyzing the weighted sum of intermediate representations in neural networks \cite{modarressi2022globenc, tenney2020language, ferrando-etal-2022-measuring}. While intuitive, these approaches have several drawbacks. For one, the attention weights do not always correspond to the most important features for model predictions \cite{jain-wallace-2019-attention} and may not correlate well with other feature importance measures \cite{ethayarajh-jurafsky-2021-attention}. Moreover, attention weights often contain redundant and mixed information, leading to unreliable explanations \cite{bastings-filippova-2020-elephant}. Like gradient-based methods, these approaches also require access to model internals, limiting their applicability to open-source models.

\textbf{Perturbation-Based Methods} can analyze any LLM regardless of architecture accessibility by measuring how outputs change when inputs are modified. As \cite{choudhary2022interpretation} describes, ``a word (token) or a collection of words (tokens) are modified or removed from the input samples, and a resulting change is measured.''

Notable approaches include LIME \cite{ribeiro2016should}, which creates local linear approximations of model behavior, and other dataset instance perturbations \cite{zafar2019dlime}. Recent research has extended these methods to few-shot demonstrations \cite{liu2023towards} and system prompts \cite{hackmann2024word, yin2023did}.

Despite these advancements, there remains a significant gap in the interpretation of zero-shot instructional prompts. Current research often uses basic token masking, which may oversimplify prompt semantics. It can also result in incoherent perturbations, potentially failing to interpret the true impact of the word on model predictions \cite{yin2023did,feng-etal-2018-pathologies}. Our research addresses this by introducing a new metric that employs a variety of meaningful perturbations to reveal the importance of each word in zero-shot instructional prompts. 

%This approach not only extends existing perturbation-based methods but also overcomes the constraints of other techniques by enabling the analysis of both open and closed-source models without requiring access to internal parameters.