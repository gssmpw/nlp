@inproceedings{bastings-filippova-2020-elephant,
    title = "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?",
    author = "Bastings, Jasmijn  and
      Filippova, Katja",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.14",
    doi = "10.18653/v1/2020.blackboxnlp-1.14",
    pages = "149--155",
    abstract = "There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.",
}

@article{choudhary2022interpretation,
  title={Interpretation of black box nlp models: A survey},
  author={Choudhary, Shivani and Chatterjee, Niladri and Saha, Subir Kumar},
  journal={arXiv preprint arXiv:2203.17081},
  year={2022}
}

@inproceedings{ethayarajh-jurafsky-2021-attention,
    title = "Attention Flows are Shapley Value Explanations",
    author = "Ethayarajh, Kawin  and
      Jurafsky, Dan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.8",
    doi = "10.18653/v1/2021.acl-short.8",
    pages = "49--54",
    abstract = "Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that {---} save for the degenerate case {---} attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values {---} which has driven their adoption among the ML community {---} we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.",
}

@inproceedings{feng-etal-2018-pathologies,
    title = "Pathologies of Neural Models Make Interpretations Difficult",
    author = "Feng, Shi  and
      Wallace, Eric  and
      Grissom II, Alvin  and
      Iyyer, Mohit  and
      Rodriguez, Pedro  and
      Boyd-Graber, Jordan",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1407",
    doi = "10.18653/v1/D18-1407",
    pages = "3719--3728",
    abstract = "One way to interpret neural model predictions is to highlight the most important input features{---}for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word{'}s importance is determined by either input perturbation{---}measuring the decrease in model confidence when that word is removed{---}or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.",
}

@inproceedings{ferrando-etal-2022-measuring,
    title = "Measuring the Mixing of Contextual Information in the Transformer",
    author = "Ferrando, Javier  and
      G{\'a}llego, Gerard I.  and
      Costa-juss{\`a}, Marta R.",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.595",
    doi = "10.18653/v1/2022.emnlp-main.595",
    pages = "8698--8714",
    abstract = "The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block {--}multi-head attention, residual connection, and layer normalization{--} and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.",
}

@inproceedings{ferrando-etal-2023-explaining,
    title = "Explaining How Transformers Use Context to Build Predictions",
    author = "Ferrando, Javier  and
      G{\'a}llego, Gerard I.  and
      Tsiamas, Ioannis  and
      Costa-juss{\`a}, Marta R.",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.301",
    doi = "10.18653/v1/2023.acl-long.301",
    pages = "5486--5513",
    abstract = "Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model{'}s prediction, it is still unclear how prior words affect the model{'}s decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.",
}

@article{hackmann2024word,
  title={Word Importance Explains How Prompts Affect Language Model Outputs},
  author={Hackmann, Stefan and Mahmoudian, Haniyeh and Steadman, Mark and Schmidt, Michael},
  journal={arXiv preprint arXiv:2403.03028},
  year={2024}
}

@inproceedings{jain-wallace-2019-attention,
    title = "{A}ttention is not {E}xplanation",
    author = "Jain, Sarthak  and
      Wallace, Byron C.",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1357",
    doi = "10.18653/v1/N19-1357",
    pages = "3543--3556",
    abstract = "Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful {``}explanations{''} for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.",
}

@article{liu2023towards,
  title={Towards understanding in-context learning with contrastive demonstrations and saliency maps},
  author={Liu, Fuxiao and Xu, Paiheng and Li, Zongxia and Feng, Yue and Song, Hyemi},
  journal={arXiv preprint arXiv:2307.05052},
  year={2023}
}

@article{modarressi2022globenc,
  title={GlobEnc: Quantifying global token attribution by incorporating the whole encoder layer in transformers},
  author={Modarressi, Ali and Fayyaz, Mohsen and Yaghoobzadeh, Yadollah and Pilehvar, Mohammad Taher},
  journal={arXiv preprint arXiv:2205.03286},
  year={2022}
}

@inproceedings{ribeiro2016should,
  title={" Why should i trust you?" Explaining the predictions of any classifier},
  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages={1135--1144},
  year={2016}
}

@article{tenney2020language,
  title={The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models},
  author={Tenney, Ian and Wexler, James and Bastings, Jasmijn and Bolukbasi, Tolga and Coenen, Andy and Gehrmann, Sebastian and Jiang, Ellen and Pushkarna, Mahima and Radebaugh, Carey and Reif, Emily and others},
  journal={arXiv preprint arXiv:2008.05122},
  year={2020}
}

@inproceedings{wallace-etal-2019-allennlp,
    title = "{A}llen{NLP} Interpret: A Framework for Explaining Predictions of {NLP} Models",
    author = "Wallace, Eric  and
      Tuyls, Jens  and
      Wang, Junlin  and
      Subramanian, Sanjay  and
      Gardner, Matt  and
      Singh, Sameer",
    editor = "Pad{\'o}, Sebastian  and
      Huang, Ruihong",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-3002",
    doi = "10.18653/v1/D19-3002",
    pages = "7--12",
    abstract = "Neural NLP models are increasingly accurate but are imperfect and opaque{---}they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions. Unfortunately, existing interpretation codebases make it difficult to apply these methods to new models and tasks, which hinders adoption for practitioners and burdens interpretability researchers. We introduce AllenNLP Interpret, a flexible framework for interpreting NLP models. The toolkit provides interpretation primitives (e.g., input gradients) for any AllenNLP model and task, a suite of built-in interpretation methods, and a library of front-end visualization components. We demonstrate the toolkit{'}s flexibility and utility by implementing live demos for five interpretation methods (e.g., saliency maps and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at \url{https://allennlp.org/interpret}.",
}

@inproceedings{wang2020gradient,
  title={Gradient-based analysis of NLP models is manipulable},
  author={Wang, Junlin and Tuyls, Jens and Wallace, Eric and Singh, Sameer},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={247--258},
  year={2020}
}

@article{wu2023analyzing,
  title={Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions},
  author={Wu, Skyler and Shen, Eric Meng and Badrinath, Charumathi and Ma, Jiaqi and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2307.13339},
  year={2023}
}

@article{yin2022interpreting,
  title={Interpreting language models with contrastive explanations},
  author={Yin, Kayo and Neubig, Graham},
  journal={arXiv preprint arXiv:2202.10419},
  year={2022}
}

@article{yin2023did,
  title={Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning},
  author={Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng Jason},
  journal={arXiv preprint arXiv:2306.01150},
  year={2023}
}

@article{zafar2019dlime,
  title={DLIME: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems},
  author={Zafar, Muhammad Rehman and Khan, Naimul Mefraz},
  journal={arXiv preprint arXiv:1906.10263},
  year={2019}
}

