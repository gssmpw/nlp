[
  {
    "index": 0,
    "papers": [
      {
        "key": "choudhary2022interpretation",
        "author": "Choudhary, Shivani and Chatterjee, Niladri and Saha, Subir Kumar",
        "title": "Interpretation of black box nlp models: A survey"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "wallace-etal-2019-allennlp",
        "author": "Wallace, Eric  and\nTuyls, Jens  and\nWang, Junlin  and\nSubramanian, Sanjay  and\nGardner, Matt  and\nSingh, Sameer",
        "title": "{A}llen{NLP} Interpret: A Framework for Explaining Predictions of {NLP} Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yin2022interpreting",
        "author": "Yin, Kayo and Neubig, Graham",
        "title": "Interpreting language models with contrastive explanations"
      },
      {
        "key": "ferrando-etal-2023-explaining",
        "author": "Ferrando, Javier  and\nG{\\'a}llego, Gerard I.  and\nTsiamas, Ioannis  and\nCosta-juss{\\`a}, Marta R.",
        "title": "Explaining How Transformers Use Context to Build Predictions"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wu2023analyzing",
        "author": "Wu, Skyler and Shen, Eric Meng and Badrinath, Charumathi and Ma, Jiaqi and Lakkaraju, Himabindu",
        "title": "Analyzing chain-of-thought prompting in large language models via gradient-based feature attributions"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2020gradient",
        "author": "Wang, Junlin and Tuyls, Jens and Wallace, Eric and Singh, Sameer",
        "title": "Gradient-based analysis of NLP models is manipulable"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "modarressi2022globenc",
        "author": "Modarressi, Ali and Fayyaz, Mohsen and Yaghoobzadeh, Yadollah and Pilehvar, Mohammad Taher",
        "title": "GlobEnc: Quantifying global token attribution by incorporating the whole encoder layer in transformers"
      },
      {
        "key": "tenney2020language",
        "author": "Tenney, Ian and Wexler, James and Bastings, Jasmijn and Bolukbasi, Tolga and Coenen, Andy and Gehrmann, Sebastian and Jiang, Ellen and Pushkarna, Mahima and Radebaugh, Carey and Reif, Emily and others",
        "title": "The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models"
      },
      {
        "key": "ferrando-etal-2022-measuring",
        "author": "Ferrando, Javier  and\nG{\\'a}llego, Gerard I.  and\nCosta-juss{\\`a}, Marta R.",
        "title": "Measuring the Mixing of Contextual Information in the Transformer"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jain-wallace-2019-attention",
        "author": "Jain, Sarthak  and\nWallace, Byron C.",
        "title": "{A}ttention is not {E}xplanation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ethayarajh-jurafsky-2021-attention",
        "author": "Ethayarajh, Kawin  and\nJurafsky, Dan",
        "title": "Attention Flows are Shapley Value Explanations"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "bastings-filippova-2020-elephant",
        "author": "Bastings, Jasmijn  and\nFilippova, Katja",
        "title": "The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "choudhary2022interpretation",
        "author": "Choudhary, Shivani and Chatterjee, Niladri and Saha, Subir Kumar",
        "title": "Interpretation of black box nlp models: A survey"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ribeiro2016should",
        "author": "Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos",
        "title": "\" Why should i trust you?\" Explaining the predictions of any classifier"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zafar2019dlime",
        "author": "Zafar, Muhammad Rehman and Khan, Naimul Mefraz",
        "title": "DLIME: A deterministic local interpretable model-agnostic explanations approach for computer-aided diagnosis systems"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "liu2023towards",
        "author": "Liu, Fuxiao and Xu, Paiheng and Li, Zongxia and Feng, Yue and Song, Hyemi",
        "title": "Towards understanding in-context learning with contrastive demonstrations and saliency maps"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "hackmann2024word",
        "author": "Hackmann, Stefan and Mahmoudian, Haniyeh and Steadman, Mark and Schmidt, Michael",
        "title": "Word Importance Explains How Prompts Affect Language Model Outputs"
      },
      {
        "key": "yin2023did",
        "author": "Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng Jason",
        "title": "Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "yin2023did",
        "author": "Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng Jason",
        "title": "Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning"
      },
      {
        "key": "feng-etal-2018-pathologies",
        "author": "Feng, Shi  and\nWallace, Eric  and\nGrissom II, Alvin  and\nIyyer, Mohit  and\nRodriguez, Pedro  and\nBoyd-Graber, Jordan",
        "title": "Pathologies of Neural Models Make Interpretations Difficult"
      }
    ]
  }
]