\section{Related Work}
\textbf{Bias Evaluation in Diffusion Models.} Evaluation of bias in text-to-image diffusion models has gained much interest recently. Numerous works have studied demographic biases (e.g., gender, race, etc.) in different domains such as occupation, physical characteristics, and so on \citep{bakr2023hrs,lee2024holistic,cui2023holistic,wan2024male,wan2024survey,luccioni2023stable,naik2023social}. These studies focus on constructing attributed prompts (e.g., photo of a \texttt{<objective>}) to probe the text-to-image models for any bias towards a specific attribute value (e.g., towards ``male'' when generating images of engineers). However, current studies overlook many domains such as healthcare, finance, and everyday activities and they rely on simplistic prompts for probing the models, hence, fail to capture the complexity and nuance of real-world user inputs. We address the two limitations and propose the \data benchmark which covers a broader range of sensitive attributes and domains, sampled from realistic statistical distribution of user prompts and rigorously filtered.   










\textbf{Bias Mitigation in Diffusion Models.}
Different approaches have been proposed to mitigate bias in diffusion models, such as by refining model weights \citep{orgad2023editing,shen2023finetuning,zhang2023iti}, intervening input prompts \citep{bansal2022well,fraser2023diversity,bianchi2023easily} or by employing guidance generation to control attributes \cite{friedrich2023fair}. These methods often compromise generation quality and lack flexibility to adapt them to any target distribution that is considered fair. Therefore, we introduce an adaptive latent guidance method that allows for more effective and flexible bias mitigation.