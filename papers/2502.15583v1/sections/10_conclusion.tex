


\section{Conclusion}

This paper systematically presents a comprehensive analysis of the traditional construction process for Q\&A systems, detailing key stages such as indexing strategies, retrieval techniques, and model selection. We introduce Chat-Grid, a cost-effective solution aimed at raising efficiency and answer quality in smart grid. In the pre-retrieval and retrieval stages, Chats-grid utilize a combination of dense and sparse retrievers to maximize the efficiency and coverage of the document retrieval process. In the post-retrieval stage, we employ a LLM to assess the relevance of retrieved documents, filtering out irrelevant content and re-ranking the results to enhance retrieval accuracy. Furthermore, we propose an innovative model self-checking mechanism coupled with question reformulation, enabling iterative retrieval that ensures the consistency and correctness of the answers by identifying and addressing inconsistencies in the facts retrieved.

Through three sets of experiments: retriever comparison, ablation study, and system comparison. We demonstrated the significant improvements brought by our proposed Chats-Grid scheme. The experimental results indicate that our approach significantly outperforms existing methods such as Self-RAG and ITRG in terms of fidelity, contextual recall rate, and answer accuracy. Specifically, Chats-Grid shows improvements of 2.37\%, 2.19\%, and 3.58\% in fidelity, context recall rate, and answer accuracy over Self-RAG, respectively, and 0.94\%, 4.39\%, and 2.45\% over ITRG. These findings confirm the effectiveness and potential of the proposed optimization strategy.

In future research, we will focus on the optimization of computational efficiency and expanding the implementation of the proposed system in real-world smart grid and other domain-specific applications to evaluate its practical effectiveness. Further refinement of the iterative retrieval process to reduce the computational overhead while maintaining high retrieval accuracy. We are looking forward to develop more advanced self-checking algorithms that improve system accuracy without significantly increasing complexity. 

% with a single GPU and achieves 3Ã— speedup compared with the
% state-of-the-art DLRM framework.
%
% Overall, \Mname~is a promising approach to lower the training cost of industry-scale DLRM model.


