%\section{Related Work}

% \begin{figure} [t] \small
%     \centering
%     \includegraphics[width=0.92\linewidth]{imgs/DLRM-BWD1.pdf}
%     \vspace{-5pt}
%     \caption{Speedup ($\times$) breakdown of the TT table \textit{backward} optimization.}
%     \label{fig: TT backward}
%     % \vspace{5pt}
% \end{figure}

% \textbf{System Designs for DLRM:} The DLRM is one of the most critical deep learning applications in the industry\cite{covington2016deep, okura2017embedding, gomez2015netflix}. The training and inference of the recommendation model come with various challenges due to the compute-intensive MLP layer and the memory-intensive embedding tables. 
% Work in \cite{nvidiainference} accelerates the inference of recommendation model by leveraging fused embedding lookup. DeepRecSys\cite{gupta2020deeprecsys} proposed an inference
% scheduler that maximizes latency-bounded throughput.
% As for DLRM training, NEO\cite{mudigere2021softwarehardware} and HugeCTR\cite{hugectr}  distribute large embedding tables to different GPUs and train the embedding table in a model-parallel style which incurs non-trivial data communication between GPUs. XDL\cite{jiang2019xdl} and FAE\cite{ebrahimzadeh2021accelerating} leverage CPU memory to handle the large embedding tables while keeping the MLP layer in GPU, the non-optimized hybrid CPU-GPU system design makes CPU side computation becomes a bottleneck. Overall, all of these techniques have not properly solved the issues of high data communication overhead. 

% \textbf{General Large-scale DLRM Training:}
% \yuke{delete this paragraph or merge some points with the previous one}
% % \yuke{merge this point with the point-1}
% Over the recent years, the number of parameters of deep models have increased dramatically~\cite{devlin2018bert, cao2020pretrained, brown2020language} and  large scale model training gains more and more attention. Generally, there are two ways to handle the out of memory training:1) Scale out training uses aggregate memory of multiple GPUs to satisfy the memory requirement for large model training~\cite{dean2012large, shoeybi2019megatron, harlap2018pipedream, huang2019gpipe}. 2) Scale up model size into a single GPU by several approaches. \yuke{reprase the second points} For instance, recomputing gradient from checkpoints
% ~\cite{chen2016training}, using an external memory such as the CPU memory~\cite{ren2021zero, huang2020swapadvisor}. \yuke{points out weakness of previous solutions}. Based on these works, we proposed a more targeted optimization for recommendatin models. \yuke{points out the exact solution and technical keywords}

% \textbf{Embedding Table Compression:}
% The embedding tables of the DLRMs have large memory footprint~\cite{zhao2020distributed, lan2019albert, weinberger2009feature, yang2020mixed, yin2021tt}. To cut down memory consumption, there have been many efforts on embedding table compression. ALBERT~\cite{lan2019albert} leverages factorized embedding parameterization and cross-layer parameter sharing to reduce the footprint of parameters. Kilian et al. propose feature hashing~\cite{weinberger2009feature} which maps multiple items to the same embedding vector. Jie et al.\cite{yang2020mixed} and Hui et al.\cite{guan2019post} use fewer bits to represent the embedding vectors. The above techniques decrease the footprint of embedding tables but often incur an accuracy tradeoff.
%
% Compressed data direct computing~\cite{zhang2021poclib,zhang2021tadoc,zhang2018efficient,pan2021exploring,zhang2022compressdb} is a novel processing method that can also be applied in embedding table compression.
%
% TT-Rec~\cite{yin2021tt} applied Tensor-train (TT) factorization on embedding tables which achieves a considerable reduction of embedding table size while keeping a high model accuracy. However, the TT factorization brings additional computation and increases training time. This motivates us to optimize the TT-based solution with system-level designs to capitalize on the benefits of TT-based embedding compression.

