\section{Preliminaries}
The construction of current mainstream Q\&A systems typically follows a process divided into three stages: index construction, pre-search and retrieval, and post-search and generation. In the index construction phase, text segmentation strategies are employed to address challenges related to natural text redundancy and model input limitations. During the pre-search stage, the optimization of user queries is performed to improve retrieval accuracy, achieved through the use of LLM and prompt engineering. In the retrieval phase, a search engine is utilized to identify similar texts. The post-retrieval stage involves text filtering and reordering, streamlining the context to enhance LLM attention. Finally, a large language model is selected during the generation phase to produce accurate and user-friendly answers.




\subsection{Index Construction}
The core task of the indexing phase is to balance information integrity with retrieval performance. Since natural language texts are often lengthy and may exceed the maximum input sequence length of Transformer models, it is essential to segment long texts to fit model constraints and reduce system overhead. An effective segmentation strategy should preserve sufficient contextual information, minimize noise, and enhance retrieval efficiency. Common text segmentation techniques include sliding windows, merging, and block summarization. For example, sentence-window retrieval improves query precision by dividing the text into shorter sentences and expanding the context window around retrieval results to enhance contextual understanding. Additionally, segmentation strategies may involve optimizing block organization structures, such as building hierarchical indexes or knowledge graphs, to accelerate data retrieval and processing. One such method, automatic merging retrieval, uses a parent-child node structure, replacing multiple related child nodes with their parent node during retrieval, thereby improving both accuracy and efficiency.



\subsection{Pre-search stage and retrieval stage}\label{tt_intro}
The primary goal of the pre-retrieval phase is to optimize the user's initial query to improve the accuracy and relevance of retrieval results. These optimizations are typically achieved through the use of LLM and prompt engineering. Since user queries often suffer from unclear descriptions, illogical structures, or inappropriate framing, this phase focuses on addressing issues such as poorly worded queries, linguistic complexity, and ambiguity.


The retrieval phase aims to identify similar texts within the knowledge base based on the user's query, a task performed by retrieval systems. These systems are typically categorized into sparse retrievers and dense retrievers. Sparse retrievers, such as Term Frequency-Inverse Document Frequency(TF-IDF) and Best Matching 25(BM25) algorithms, are considered earlier-generation methods but remain widely used in various fields due to their efficient encoding and stability. Dense retrievers, on the other hand, utilize deep learning techniques, employing neural networks to learn dense vector representations of documents and queries. Common models include the BGE series, text-embedding-ada-002 and Moka Massive Mixed Embedding(M3E). Current mainstream approaches involve transforming text into embedding vectors using these models and then calculating cosine similarity between vectors~\cite{karpukhin2020dense} to measure textual similarity. The embedding process can be expressed as equation~\ref{1}:
\begin{equation} \label{1}
\vec{q}=\operatorname{Encoder}_q(q) ; \overrightarrow{d_l}=\operatorname{Encoder}_d\left(d_i\right)
\end{equation}

where, ${Encoder}_q$ and ${Encoder}_d$ are embedded vector models, which usually share weights or architectures for mapping textual data into a vector space~\cite{cuconasu2024power}. This mapping allows the text data to be represented as dense vectors of fixed length, capturing the semantic relationships between words, which facilitates subsequent computation and processing. After obtaining the vector representation of the text, similar texts can be identified by calculating the cosine similarity, as demonstrated in equation~\ref{2}.
\begin{equation} \label{2}
\text { Similarity }=\cos \theta=\frac{\vec{q} \cdot \overrightarrow{d_l}}{\|\vec{q}\|\|\overrightarrow{d_{\imath}}\|}
\end{equation}

where $\vec{q} \cdot \overrightarrow{d_l}$ is the dot product of vectors $\vec{q}$ and $\overrightarrow{d_l}$, ${\|\vec{q}\|}$ and $\|\overrightarrow{d_{\imath}}\|$ are the lengths of ${\vec{q}}$ and $\overrightarrow{d_{\imath}}$ . The value of cosine similarity ranges from -1 to 1, with values closer to 1 indicating higher similarity and values closer to -1 indicating lower similarity. When selecting an embedding vector model, three key considerations must be addressed: the efficiency of the retrieval, the quality of the embedding vectors, and the alignment between the task, the data, and the model.


In addition to selecting an appropriate embedding vector model, model fine-tuning is an effective approach to enhance retrieval performance, particularly in highly specialized domains~\cite{houlsby2019parameter}. Common fine-tuning methods include Supervised Fine-Tuning(SFT), LLM-Supervised Retrieval(LSR), and Adapter~\cite{jiang2023llmlingua}. SFT, akin to traditional fine-tuning techniques, involves constructing a fine-tuning dataset based on domain-specific data for training. The LSR method leverages the output of a LLM to supervise and fine-tune the retrieval model. When the retriever is hosted on a cloud service platform or when cost-efficiency is prioritized, fine-tuning can be achieved through the Adapter module.



\subsection{Post-retrieval and Generation Stages}

Retrieved texts often contain low-relevance or erroneous information, and excessive context can introduce noise, hindering LLM from capturing key information and potentially leading to omissions. To mitigate these issues, the post-retrieval phase employs techniques such as text filtering and re-ranking. Text filtering streamlines the context, reduces LLM resource consumption, and minimizes response latency. For example, the LLMLingua method~\cite{liu2024lost} uses small language models to detect and remove irrelevant markers.


Re-ranking improves the LLM's focus on relevant documents and can be classified into rule-based and model-based approaches. Rule-based methods include strategies such as diversity-based or relevance-based re-ranking, while model-based approaches employ advanced tools, such as the bge-reranker-large model developed by the Beijing Academy of AI, to optimize result quality.

In the generation phase, selecting an appropriate LLM for answer generation is crucial. Options include proprietary models such as GPT-3.5, which offer high concurrency, strong performance, and no server maintenance burden, but also present data privacy risks and lack fine-tuning capabilities. Alternatively, locally deployed open-source models like Llama or Chat Generative Language Model(ChatGLM) offer greater flexibility and security, though they require significant computational resources. To enhance answer quality, larger parameter models or fine-tuning techniques can be used to incorporate domain-specific knowledge or adapt to particular data formats and styles.


 
% \begin{figure} [t] \small
%     \centering
%     \includegraphics[width=0.9\linewidth]{imgs/fig4.pdf}
%     \vspace{-5pt}
%     \caption{The Process of Fidelity Calculation.}
%     \label{fig: global info}
%     \vspace{-5pt}
% \end{figure}
