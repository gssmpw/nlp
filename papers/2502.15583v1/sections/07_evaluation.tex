
\section{Case Study}
In this chapter, we first introduce the dataset. Then, we introduce the experimental environment, and the project code has been made public. The access address is: {https://gitee.com/zhangjiqun/qa\_system}. Finally, we focus on presenting the experimental results. This experimental part is divided into three parts: the first part is the retrieval method comparison experiment, which aims to verify the superiority of the combination of sparse retrieval and dense retrieval strategies proposed in this paper; the second part is to conduct a system ablation experiment based on different answer evaluation indicators to evaluate the specific contribution of each indicator to the system performance; the third part is the overall system comparison experiment, which aims to comprehensively compare and display the overall optimization effect achieved by this scheme.
\subsection{Dataset Construction}

Currently, mainstream question answering datasets, such as Triviaqa [24] and Natural Questions, mostly collect data from Wikipedia. The characteristics of this data source are likely to cause test bias in pre-trained large language models, and general large models have disadvantages in adapting to the smart grid question answering system and are difficult to meet the needs of this specific field.
In view of this, this study selects the dataset involved in reference [22] and makes some data corrections on its existing basis, and then constructs the SGQA (Smart grid question and answer) dataset. This dataset has been made public, and the detailed website is: https://gitee.com/zhangjiqun/sgqa.
The SGQA dataset is a dataset specially designed for knowledge question answering tasks in the power field. It contains 33,500 power specification clauses and 20,000 question and answer pairs. The power specification clauses focus on training the LLM to learn the basic theoretical knowledge of power specifications and help the model master the basic principles of the power field. The question and answer pairs focus on the in-depth learning of power-related knowledge points, and the question and answer pairs can also provide a practical reference for model evaluation to ensure that the performance of the model in the power knowledge question answering scenario can be accurately measured.
The SGQA dataset covers a wide range of fields, comprehensively covering various key knowledge points in the power field such as thermal power technology, hydropower station equipment maintenance management, power capacitor and inductor testing, electrical materials, power construction, wind farm power assessment, power transformer selection, nuclear power plant equipment maintenance, power plant boiler unit dust collector maintenance, phase modifier maintenance, and overhead transmission line high-altitude rescue. From the perspective of question type distribution, the dataset includes three mainstream question types: single choice, fill in the blanks, and judgment. Table~\ref{tab:tab2} lists in detail the number and answer content of each question type in the dataset, and Table~\ref{tab:tab3}  shows sample examples of each question type. The "reference" field in Table 3 clearly marks the source of the question, providing convenience for users to understand the background of the question and trace the knowledge context.


\begin{table}[htbp]
\centering
\caption{Distribution of questions by category}
\begin{tabular}{lll}
\toprule
Question Category & Number & Answer content \\
\midrule
Single choice & 6000 & A|B|C|D \\
judgment & 7000 & - \\
fill in the blanks & 7000 & True/False \\
\bottomrule
\end{tabular}
\label{tab:tab2}
\end{table}



\begin{table}[htbp]
\centering
\caption{Sample examples of each type of question}
\begin{tabular}{p{0.04\textwidth}p{0.18\textwidth}p{0.05\textwidth}p{0.16\textwidth}}
\toprule
Category& Question & Answer & Reference \\
\midrule
Single choice & Which of the following terms and definitions is not applicable to this document? A: GB/T 2900.5 B: GB/T 25096 C: GB/T 8287.1 D: ISO 9001:2015 & D & The terms and definitions defined in GB/T 2900.5, GB/T 2900.8, GB/T 8287.1 and GB/T 25096 are applicable to this document. \\
 
judgment & The area of a single surface defect of a composite insulator does not exceed 25.0mm², the depth does not exceed 1.0mm, the protrusion height does not exceed 0.8mm, and the mold joint is flat & True & b) Composite insulator The area of a single surface defect does not exceed 25.0mm², the depth does not exceed 1.0mm, the protrusion height does not exceed 0.8mm, and the mold joint is flat. \\
 
fill in the blanks & According to the requirements of 7.2, on what facilities should the unpacked insulators be placed to protect the umbrella cover? Please fill in the keywords. & Protection measures & The unpacked insulators should have protection measures to avoid deformation or damage of the umbrella cover. \\
\bottomrule
\end{tabular}
\label{tab:tab3}
\end{table}



\textbf{Benchmark and Datasets}  
We utilize four commonly employed real-world datasets. 



%We use three widely adopted real-world datasets. \textbf{Avazu}\cite{Avazu} is a dataset for click through rate prediction which consists of 11 days worth of Avazu data. Each sample in Avazu has 1 numerical feature and 20 categorical features. \textbf{Criteo Terabyte}\cite{Terabyte} is the largest publicly available dataset for DLRM. 
% It contains 1.3TB of uncompressed click logs containing 
%It contains over four billion samples spanning 24 days. Each record contains 39 features: 13 numerical features, and 26 categorical features. \textbf{Criteo Kaggle}~\cite{kaggle} is a subset of Criteo Terabyte which is used for Criteo Kaggle Display Advertising Challenge. It contains a portion of Criteo's traffic over a period of 7 days and the data format is the same as Criteo Terabyte. 
%Table~\ref{table: Evaluation Dataset} shows the details of these datasets. It is worth noting that the footprint of Criteo Terabytes's embedding tables is about 59.2 GB, which has exceeded the memory capacity of most GPUs. 
%In industry, the training data is larger than these public datasets~\cite {zhao2020distributed} which highlights the importance of \textit{\Mname}.




%Our major evaluation platform is a single AWS p3.8xlarge instance with one Intel Xeon CPU@2.30GHz, 239 GB CPU memory, and 4$\times$ Nvidia Tesla V100 GPU. We also evaluate end-to-end performance on AWS g4dn.12xlarge instance which has Intel Xeon CPU@2.50GHz, 192 GB CPU memory and 4$\times$ Nvidia Tesla T4 GPU. On the software side, we employ Nvidia NVTabular~\cite{NvTabular} for data preprocessing. Nvidia NVTabular also provides a high-performance dataloader for loading DLRM training data from the disk. For a fair comparison, we replace the default PyTorch dataloader with the Nvidia NVTabular dataloader in the baseline frameworks. Our Eff-TT table implementation uses cuBLAS~\cite{cublas} library for batched-GEMM computation. 


\subsection{Experimental Environment} %
The experimental environment configuration of this paper is shown in Table\ref{tab:tab4}. The experiment is carried out based on the Ubuntu 22.04 system. The computer hardware configuration used is: equipped with 5 NVIDIA A100-PCIE-40GB GPUs, each GPU has 6912 CUDA cores, with a total video memory capacity of 200GB and a frequency of 3.2GHz; the CPU adopts Intel® Xeon® Gold 6248R, with a main frequency of 3.00GHz and 96 cores; the memory is 1007.3GB, and the disk capacity is 7.8TB.

\begin{table}
\caption{Experimental Environment}
\centering
\begin{tabular}{p{0.15\textwidth}p{0.23\textwidth}}
\toprule[0.8pt]
Hardware/Environment & Specification \\
\hline
Operating system & Ubuntu 22.04 \\
GPU & 5 * NVIDIA A100-PCIE-40GB \\
CUDA cores & 6912 \\
Video memory & 200GB \\
memory frequency & 3.2GHz \\
CPU & Intel® Xeon® Gold 6248R CPU @ 3.00GHz * 96 \\
Memory & 1007.3 GB \\
Disk capacity & 7.8TB \\
Python & 3.10.13 \\
PyTorch & 2.2.1 \\
CUDA & 11.8 \\
Ragas & 0.1.2 \\
Llama-index & 0.10.12 \\
\bottomrule[0.8pt]
\end{tabular}
\label{tab:tab4}
\end{table}

At the software level, the Python version is 3.10.13. PyTorch 2.2.1 is a deep learning framework used to build and train neural network models. CUDA 11.8, as a parallel computing platform and programming model launched by NVIDIA, enables GPUs to perform general computing and cooperates with PyTorch to improve the training speed of deep learning.

In addition, two important toolkits are used. Ragas 0.1.2 is mainly used to evaluate applications based on language models and help optimize model performance. Llama-index 0.10.12 realizes the management of large language models and improves development efficiency.

\subsection{Experimental Results}
\textbf{Retrieval Model Comparison Experiment}

To verify the effectiveness of the retrieval scheme proposed in this paper, we compares it with several common retrieval models, conducting multiple experiments to measure their performance differences.

\textbf{BM25 Algorithm~\cite{robertson2009probabilistic}:} It is a classic algorithm for information retrieval and one of the representative algorithms for sparse retrieval, commonly used in search engines. It assesses the relevance score between documents and queries based on the frequency of query terms in the documents and ranks them accordingly.

\textbf{BGE Model~\cite{luo2024bge}:} The bge-base-zh-v1.5 model from the series, developed by the Beijing Academy of Artificial Intelligence, is used in this experiment. It is an advanced dense retrieval method that converts text into dense embedding vectors and calculates the cosine similarity between different text vectors to find similar texts.

\textbf{Method Used in This Paper:} This paper enhances retrieval recall quality and robustness by using a parallel approach of sparse retrieva and dense retrieval. Since this method retrieves twice the number of texts as a single retriever, LLM is employed to score and filter the retrieved documents to obtain the same number of candidate documents as other schemes.


% \begin{figure} [h] 
%     %\vspace{-50pt}
%     %\setlength{\abovecaptionskip}{-0.3cm}
%     %\setlength{\belowcaptionskip}{-0.5cm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{imgs/fig9.pdf}
%     %\vspace{-25pt}
%     \caption{Comparison of Retrieval Methods in Experiments}
%     \label{fig: pre}
%     % \vspace{-5pt}
% \end{figure}

\begin{figure} [h] 
    %\vspace{-50pt}
    %\setlength{\abovecaptionskip}{-0.3cm}
    %\setlength{\belowcaptionskip}{-0.5cm}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig9.pdf}
    %\vspace{-25pt}
    \caption{Comparison of Retrieval Methods in Experiments}
    \label{fig:fig9}
    % \vspace{-5pt}
\end{figure}


Since too many query expansions will cause the system to spend too much, and too few expansions may not significantly improve the performance, in order to find the optimal number of query expansions (NIE), this paper conducts an experiment on the NIE when using the hybrid retrieval and fixing the recall text number to 5. It can be seen from Table~\ref{tab:tab5} that when the number of queries is expanded to 2, the growth rate of the recall rate is less than 1\%, and the trend becomes flat. In the actual question generation process, too many query expansion operations may lead to serious homogenization of the generated queries, which will greatly increase the system consumption while having a small improvement effect on the system answering effect. Therefore, finally, the number of query expansions in the system is set to 2.

\begin{table}[htbp]
\centering
\caption{Effect of the number of query expansion on contextual recall}
\begin{tabular}{cccccc}
\toprule
NIE & 0 & 1 & 2 & 3 & 4 \\
\midrule
Context\_Recall & 0.74 & 0.785 & 0.82 & 0.825 & 0.828 \\
Growth\_Rate & 6.08\% & 4.46\% & 0.61\% & 0.36\% & - \\
\bottomrule
\end{tabular}
\label{tab:tab5}
\end{table}

\textbf{System Ablation Experiment}

In this ablation experiment, this paper selects a naive RAG system as the baseline and consistently uses the Gpt3.5-Turbo model as the generator and the bge-base-zh-v1.5 model as the embedding vector model, with the maximum number of iterative retrievals set to three. The overall system optimization is divided into three parts, corresponding to different stages of the RAG system: pre-retrieval and retrieval optimization (query expansion + hybrid retrieval), retrieval and post-retrieval optimization (document filtering + reordering), and iterative retrieval (enabling the model to perform a new round of retrieval through answer self-checking and rewriting). The Ragas framework is used to score four indicators: fidelity, context recall rate, answer relevance, and answer accuracy.



The experimental results in Table \ref{tab:tab6} show that the baseline system performs poorly in context recall rate, which in turn negatively impacts answer accuracy due to its lack of optimization and reliance on a single retrieval method. By incorporating query expansion and hybrid retrieval strategies, the system’s retrieval scope is expanded, significantly improving the baseline model’s context recall rate and indirectly enhancing fidelity, answer relevance, and answer accuracy. The further introduction of document filtering and a redesigned reordering mechanism effectively improves context recall quality by eliminating irrelevant candidate documents, while substantially enhancing fidelity and answer accuracy. Finally, the introduction of an answer self-checking mechanism enables iterative improvements in the answers, further enhancing fidelity and answer accuracy. After these three optimizations, the RAG system described in this paper outperforms the baseline system by 15.69\% in fidelity, 22.64\% in context recall rate, 8.32\% in answer relevance, and 22.25\% in answer accuracy.

\begin{table}[h]\small
 \caption{Ablation Experiment Results}
    \centering
     \vspace{-5pt}
    \scalebox{0.55}{
   \begin{tabular}{ccccccc}
\toprule[1.5pt]
\begin{tabular}{l} 
Query \\
Expansion + \\
Hybrid Retrieval
\end{tabular} & \begin{tabular}{l} 
Document \\
Filtering + \\
Reordering
\end{tabular} & \begin{tabular}{l} 
Iterative \\
Retrieval
\end{tabular} & fidelity (\%) & \begin{tabular}{l} 
Context \\
Recall \\
Rate(\%)
\end{tabular} & \begin{tabular}{l} 
Answer \\
Relevance(\%)
\end{tabular} & \begin{tabular}{l} 
Answer \\
Accuracy(\%)
\end{tabular} \\
\hline$\times$ & $\times$ & $\times$ & 78.85 & 59.73 & 86.49 & 56.46 \\
$\sqrt{ }$ & $\times$ & $\times$ & 84.32 & 69.87 & 89.82 & 67.49 \\
$\sqrt{ }$ & $\sqrt{ }$ & $\times$ & 91.38 & 82.25 & 94.27 & 74.76 \\
$\sqrt{ }$ & $\sqrt{ }$ & $\sqrt{}$ & 94.54 & 82.37 & 94.81 & 78.71 \\
\bottomrule[1.5pt]
\end{tabular}}
\label{tab:tab6}
\end{table}

\textbf{System Comparison Experiment}

To verify the effectiveness of the optimization scheme, this study conducts comparative experiments with several other RAG Q\&A systems. The control schemes include: Naive RAG, RAG optimized using the HyDE method, ITRG, and Self-RAG, which are described as follows:

\textbf{Naive RAG~\cite{gao2023retrieval}:} A system built from the most basic RAG process.

\textbf{HyDE~\cite{gao2022precise}:} This method generates pseudo-answers using an LLM and uses these pseudo-answers as queries to retrieve real texts, thereby improving the system's context recall rate. Compared to directly using the original question for retrieval, these pseudo-answers are likely to be closer to the real text in the vector space, thereby enhancing the relevance and accuracy of retrieval.



\textbf{ITRG~\cite{feng2024retrieval}:} It can be viewed as an iterative retrieval version of the HyDE scheme. The core idea is that each retrieved text contains only partial information to answer the question. Therefore, the answer can be progressively refined through repeated generation of incomplete answers and retrieval of real texts based on these incomplete answers. In each iteration, the system generates an answer that may contain both false and partially correct information, which is then used as a query to retrieve similar texts, thereby collecting additional information to refine the answer.

\textbf{Self-RAG~\cite{asai2023self}:} This method introduces retrieval and evaluation tokens to the model. The retrieval token is used to allow the LLM to determine whether the retrieval process should be executed for a given question, while the evaluation token is employed for document filtering and ranking. Adaptive and iterative retrieval are facilitated through these tokens.

\begin{figure} [h] 
    %\vspace{-50pt}
    %\setlength{\abovecaptionskip}{-0.3cm}
    %\setlength{\belowcaptionskip}{-0.5cm}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig10.pdf}
    %\vspace{-25pt}
    \caption{Comparison of Retrieval Methods in Experiments}
    \label{fig:fig10}
    % \vspace{-5pt}
\end{figure}


\begin{table}[htbp]
\centering
\caption{Comparison of different RAG systems}
\begin{tabular}{ccccccc}
\toprule
RAG System & Naive RAG & HyDE & ITRG & Self-RAG & Ours \\
\midrule
Cost (tokens) & 397 & 464 & 703 & 769 & 826 \\
\bottomrule
\end{tabular}
\label{tab:tab8}
\end{table}


As can be seen from the data in Figure~\ref{fig:fig10}, the plain RAG underperforms in all aspects due to the fact that no additional optimisations have been implemented. However, it is this unoptimised nature that allows the method to consume relatively few tokens, and HyDE effectively improves answering by optimising the quality of retrieval. Although this optimisation strategy leads to improved response results, it comes at the cost of a slight increase in system overhead. both ITRG and Self-RAG use an iterative approach to achieve significant performance improvements in all aspects of the Q\&A system. However, the iterative process inevitably leads to a further increase in token consumption, as shown in Table~\ref{tab:tab8}.

The experimental results show that our proposed improvement scheme achieves better results in the four key metrics of fidelity, contextual recall, answer relevance, and answer accuracy. Specifically, in the Q\&A test with 100 pieces of data, the average system overhead of the proposed scheme increases by only 57 tokens per Q\&A session compared to the Self-RAG scheme, which confirms that our improved scheme achieves a good balance between performance improvement and system overhead control, and provides a more advantageous solution for the Phase-Intelligent Grid Q\&A system.

%In this section, we demonstrate the training throughput of \Mname~and compare with different baseline models.
%
%We design two main evaluation settings to show the advantages of \textit{\Mname}~in different aspects: 
%1) DLRM training with limited GPU resources, especially under the single GPU setting;
%2) DLRM training with multiple GPUs.


