\section{Optimization of Q\&A Framework for Iterative  Question Answering}

% \begin{figure} [h] \small
%     %\vspace{-30pt}
%     %\setlength{\abovecaptionskip}{-0.5cm}
%     %\setlength{\belowcaptionskip}{-0.5cm}
%     \centering
%     \includegraphics[width=0.9\linewidth]{imgs/fig1.pdf}
%     %\vspace{-50pt}
%     \caption{Optimized RAG System Process.}
%     \label{fig: overview}
%     %\vspace{10pt}
% \end{figure}

Figure 1 presents the overall process of the iterative retrieval question-answering optimization scheme, which mainly consists of three parts: pre-retrieval and retrieval stage optimization, post-retrieval optimization, and post-generation optimization. Specifically, the input information is first processed by a LLM to perform question expansion. Subsequently, the BM25 and BGE algorithms are used to retrieve candidate documents. On this basis, we use prompt engineering(PE) to fine-tune the LLM to form a scoring-capable $PE-LLM_1$. This model is used to filter, score, and rank the candidate documents. Through the above steps, the expanded questions and candidate documents can be obtained and input into the LLM to get corresponding answers. Then, according to the answer quality evaluation criteria we formulated, we use prompt engineering to fine-tune the LLM again to form a $PE-LLM_2$ with the ability to evaluate answer quality. If the self-test of $PE-LLM_2$ passes, the answer is output; if it fails, the question is expanded and the process restarts. This process comprehensively covers all links of the question-answering system from input to output, aiming to effectively improve the quality and effect of information retrieval and answer generation.

% \newpage
\subsection{Optimization of Pre-retrieval and Retrieval Stages}
\label{sec: eff_tt}
% In this section, we will detail our Eff-TT table design which includes forward phase intermediate result reuse and backward phase in-advance gradient aggregation.
User queries often suffer from issues such as vague descriptions, unclear logic, or incorrect framing, which negatively affect retrieval quality. To address these challenges, this study employs LLM to rephrase and expand the original queries text before retrieval. This approach enhances the scope and depth of document retrieval by improving query clarity and structure. As depicted in Figure ~\ref{fig:fig2}, we show the process of the original queries text expansion.

\begin{figure} [h] 
    %\vspace{-50pt}
    %\setlength{\abovecaptionskip}{-0.3cm}
    %\setlength{\belowcaptionskip}{-0.5cm}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig2.pdf}
    %\vspace{-25pt}
    \caption{Query Expansion from Original Query.}
    \label{fig:fig2}
    % \vspace{-5pt}
\end{figure}
Figure 2 presents the process architecture of the pre-retrieval and retrieval stage optimization. First, the user inputs the question "Q", and then the LLM is used to perform question expansion. Experimental verification shows that the optimal effect is achieved when the original query "Q" is expanded into "Q1" and "Q2". These expanded questions, together with the original question, are input into the hybrid retrieval module constructed by BM25 and BGE, and finally a candidate document set is obtained. This process lays a solid foundation for the subsequent information processing and retrieval links, and plays a crucial role in improving retrieval efficiency and accuracy.


Since the way users ask questions may affect the quality of retrieval, in this paper, before retrieval, the LLM is used to rewrite and expand the user's questions, so as to improve the coverage and depth of retrieved documents in the retrieval stage. The query expansion process is shown in Figure~\ref{fig:fig3}.


\begin{figure} [h] 
    %\vspace{-50pt}
    %\setlength{\abovecaptionskip}{-0.3cm}
    %\setlength{\belowcaptionskip}{-0.5cm}
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig3.pdf}
    %\vspace{-25pt}
    \caption{Query Expansion of the Original Query}
    \label{fig:fig3}
    % \vspace{-5pt}
\end{figure}

After completing the query expansion task, in order to efficiently mine relevant information, it is necessary to carry out parallel retrieval on multiple expanded queries simultaneously. Among many retrieval strategies, the dense retrieval method based on calculating the cosine similarity of the embedding vector model is widely used. The dense retrieval model selected in this paper is the BGE, a general vector model developed and open-sourced by the Beijing Academy of Artificial Intelligence. This model can effectively capture text semantics and provide strong support for retrieval in most scenarios. However, this dense retrieval method relying on the embedding vector model has inherent limitations. When facing emerging words or professional terms, due to the lack of training data, the BGE model is difficult to fully learn their semantic features, thus greatly reducing the accuracy of relevant document retrieval.
In view of this, this paper innovatively combines the dense retrieval of BGE with the sparse retrieval method of BM25. The BM25 algorithm can efficiently calculate the relevance between documents and queries based on term frequency and inverse document frequency, and its score can be obtained through Equation ~\ref{3} and Equation ~\ref{4}. 


\begin{equation} \label{3}
\small
\operatorname{Score}(D\!,\! Q) \! = \! \sum_{i=1}^n IDF\left(q_i\right) \! \cdot 
\frac{f\left(q_i, D\right) \! \cdot \! \left(k_1 \!+ \!1\right)}{f\left(q_i \!, \! D\right) \! + \! k_1  \! \cdot \! \left(1 \! - \! b \! + \! b \! \cdot \! \frac{|D|}{a v g d l}\right)}
\end{equation}

\begin{equation} \label{4}
\operatorname{IDF}\left(q_i\right)=\log ((\mathrm{M}-\mathrm{m}+0.5) /(\mathrm{m}+0.5))
\end{equation}

Where $M$ is the total number of documents, $m$ is the number of documents containing the word, $D$ is a document, $\mathrm{Q}$ is a query statement, $q_i$ is the the word in the query statement, $f\left(q_i, D\right)$ is the word frequency of word  in document $D$, $|D|$ is the length of document $D$, and $avgdl$ is the average of the lengths of all the documents in the set of documents. $k_1$ and $b$ are the tuning parameters.

The embedding vector model can usually capture semantic information and understand the similarity between words, while the BM25 algorithm is better at precise matching based on keywords. Using the embedding vector model and the BM25 algorithm for retrieval at the same time, two sets of search results are obtained. Combining the two can make use of both semantic similarity and keyword matching degree to improve the accuracy of retrieval. Through the combination of dense retrieval and sparse retrieval, a more comprehensive information coverage is achieved, the processing ability of long-tail queries is increased, and the robustness of the query is improved.



%\subsubsection{Two-sides of temporary data reuse}\label{sec:lookup}

% The footprint of the TT table is smaller, but it incurs extra computation when lookup embeddings in the forward phase and computing gradient in the backward phase. The additional computation overhead is non-trivial, resulting in a significant increase in training time.
% % The extra computation is non-negligible which leads to a significant increase in the training and inference time of the deep learning recommendation model. 
% In this work, we analyze the computation pattern of the TT table in-depth and explore the data reuse opportunity in the TT table computation. To make full use of the reusable intermediate result in the TT table computation, the main challenge is how to determine which intermediate result can be reused. To this end, we explore the intermediate result reuse opportunity at two different levels:
% % For expression convenient, we will takes 3 tensor cores TT table as an example. \yuke{no need to address ``3'' here} Later we will show that our optimizations can be generalized to TT table with more tensor cores easily.



\subsection{Flowchart of Post-retrieval Optimization}


The post-retrieval optimization process is shown in Figure~\ref{fig:fig4}. We use prompt engineering to fine-tune the LLM to obtain $PE-LLM_1$. Then, the candidate documents are input into $PE-LLM_1$, and finally, $PE-LLM_1$ performs operations such as filtering, rating, and ranking on the candidate documents to provide more accurate reference documents for the subsequent answer generation.

\begin{figure}[h!]
    \centering
    %\vspace{-75pt}
    %\setlength{\abovecaptionskip}{-3cm}
    %\setlength{\belowcaptionskip}{-3cm}
    \includegraphics[width=0.9\linewidth]{imgs/fig4.pdf}
    \vspace{-5pt}
    \caption{ Flowchart of Post-retrieval Optimization}
    \label{fig:fig4}
\end{figure}

After obtaining relevant documents through parallel retrieval, it is necessary to carry out reordering operations on these documents. Assuming that each retrieval engine returns the top N similar texts, then after M times of parallel retrieval, N*M results will be obtained. In this case, it is necessary to use a sorting method to sort these merged documents. Among them, the Reciprocal Rank Fusion (RRF) method is more commonly used, and the documents are usually arranged in reverse order of relevance. However, for LLM, this is not the best sorting method. The middle amnesia characteristic of LLM shows that when the useful text is in different positions of the prompt words, the answering effect of the model will be better [22]. And some research has confirmed that in the vector space, those documents that are close to the query but contain useful information may have a negative effect on the model's answer [18]. In view of this, this paper uses LLM (this LLM is based on the Gpt3.5-Turbo model and has been fine-tuned using 100 artificially constructed scoring data sets) to score each retrieval result according to its helpfulness in answering the question, rather than simply marking it as Relevant or Supported, and then eliminating those documents that are of no help in answering the question. The specific scoring criteria are shown in Table ~\ref{tab:tab1}.

\setcounter{table}{0}
\begin{table}[h!]\small
 \caption{Scoring Criteria for Constructing the Rating Dataset}
    \centering
     \vspace{-5pt}
    \scalebox{0.6}{
   \begin{tabular}{ll}
\toprule[1.5pt]
Score & Scope \\
\hline 0 &  
Documents that do not contain any useful information \\
 1-3 &  
Contains only some relevant background information and does not directly provide 
an answer. \\
$4-9$ & 
Contains direct evidence of the answer to the question and is scored higher based on completeness.
\\
10 & A golden document that contains all the information needed to answer the question.\\
\bottomrule[1.5pt]
\end{tabular}}
\label{tab:tab1}
\end{table}



The fine-tuned large-scale model is combined with prompt words to score the candidate documents, as shown in Figure~\ref{fig:fig5}. During the model inference process, in order to maintain the consistency of the scoring, the temperature parameter of the model needs to be set to 0. After obtaining the score of each document, the system will first eliminate those documents with a score of 0, and then use these scores to sort the documents.
\begin{figure} [h!] \small
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig5.pdf}
    \caption{Scoring Document Relevance Using an LLM.}
    \label{fig:fig5}
    %\vspace{0pt}
\end{figure}


\subsection{Optimization in the Generation Stage}\label{bijection}




The process of post-generation optimization is shown in Figure~\ref{fig:fig6}. First, the candidate documents generated in the previous two stages and "Q, Q1, Q2" (i.e., the original question and its expanded questions) are input into the LLM to generate an answer. Subsequently, prompt engineering is used to construct $PE-LLM_2$ (a model with the ability to evaluate answer quality) to assess the quality of the generated answer. $PE-LLM_2$ will perform an AI self-check. If the self-check passes, the corresponding answer will be output; if the self-check fails, the question rewriting step will be executed, and the iterative retrieval process will be entered again to ensure the quality and accuracy of the final output answer, thereby improving the performance and reliability of the entire system in information processing and Q\&A. Next, the answer quality evaluation criteria and the AI self-check mechanism will be introduced in detail.
\begin{figure} [h] \small
    \centering
    %\vspace{-75pt}
    %\setlength{\abovecaptionskip}{-3cm}
    %\setlength{\belowcaptionskip}{-3cm}
    \includegraphics[width=0.9\linewidth]{imgs/fig6.pdf}
    \vspace{-5pt}
    \caption{ Flowchart of Post-Generation Optimization}
    \label{fig:fig6}
    % \vspace{-5pt}
\end{figure}


\textbf{Answer Quality Evaluation Criteria}

Traditional natural language processing evaluation metrics, such as Exact Match (EM), Bilingual Evaluation Understudy(BLEU), etc., mainly focus on the degree of direct text repetition. Therefore, for texts with similar semantics but different forms of expression, misjudgments are likely to occur. Different from these, Trulens introduced a triple-element evaluation criterion, comprehensively considering three dimensions: context relevance, faithfulness, and answer relevance. Ragas, on this basis, further added indicators such as answer similarity, context recall rate, and precision.
We carried out ablation experiments on the above-mentioned indicators, extracted the important indicators that play a key role in the evaluation, and based on this, proposed a five-element evaluation criterion. This criterion covers the following five dimensions:
\begin{enumerate}
    \item Faithfulness: It is used to evaluate the degree of association between the generated answer and the context, and measures whether the answer accurately reflects the context information.
    \item Context Recall Rate: Also focusing on the correlation between the generated answer and the context, it judges whether the context contains the key information sufficient to support the generated answer.
    \item Answer Relevance: Aims to evaluate the degree of close association between the generated answer and the original question, ensuring that the answer closely adheres to the question topic.
    \item Answer Accuracy: Mainly evaluates the correctness of the finally generated answer, judging whether the answer conforms to the facts and logic of the general large-scale model.
    \item System Token Consumption: It is an evaluation of the system's performance consumption.
\end{enumerate}
The value range of each of these five indicators is set within the interval [0, 1]. The final evaluation score is obtained by calculating the average value after accumulating the scores of these five indicators.

\textbf{Fidelity}: fidelity mainly measures the degree of consistency between the generated answer and the given source text (context), that is, whether the answer is accurately based on the context without deviating, exaggerating, or wrongly reflecting the information in the context. A high degree of faithfulness means that the answer given by the system can accurately rely on the provided knowledge documents (context). The calculation of faithfulness follows equation~\ref{5}, that is, it is determined by calculating the ratio of the number of words in the answer that completely match the context (NAC) to the total number of words in the answer (NA).
\begin{equation}
\text { Fidelity }=\frac{NAC}{NC}
\label{5}
\end{equation}

\textbf{Context Recall Rate}: It is used to determine whether the true answer fully appears in the recalled context. It is similar to the recall rate in classification tasks. Regardless of whether the recalled text is redundant or not, a high score can be obtained as long as the true answer can be found in it. Its score is calculated according to equation~\ref{6}, which is obtained by dividing the number of true viewpoints in the context (NTIC) by the number of viewpoints in the true answer (NTIA): 

\begin{equation}
\text { Context recall }=\frac{NTIV}{NTIA}
\label{6}
\end{equation}

When calculating in practice, the real answer will be decomposed into several different statements, and the viewpoints supporting each statement will be searched for in the context one by one, as shown in Figure~\ref{fig:fig7}.
\begin{figure} [h] \small
    \centering
    %\vspace{-75pt}
    %\setlength{\abovecaptionskip}{-3cm}
    %\setlength{\belowcaptionskip}{-3cm}
    \includegraphics[width=0.9\linewidth]{imgs/fig7.pdf}
    \vspace{-5pt}
    \caption{Contextual Recall Calculation Process}
    \label{fig:fig7}
    % \vspace{-5pt}
\end{figure}


\textbf{Answer relevance (RA)}: RA is used to evaluate the degree of relevance between the generated answer and the original question. If the answer is incomplete or contains redundant information, it will get a lower score. If the answer directly and appropriately answers the question, it will get a higher score. Let the answer A and the question Q be represented as vectorsand,respectively. Then their cosine similarity is calculated as shown in equation~\ref{7}. When there are N expanded questions, similar calculations need to be carried out respectively and the average value is taken to obtain the cosine similarity of the question Q: 

\begin{equation}
\text Cosine\_Similarity(A,Q)=\frac{\sum_{i = 1}^{n}a_{i}b_{i}}{\sqrt{\sum_{i = 1}^{n}a_{i}^{2}}\times\sqrt{\sum_{i = 1}^{n}b_{i}^{2}}}
\label{7}
\end{equation}


\textbf{Answer accuracy} measures the accuracy of the system's answer by comparing the generated answer with the real answer. It is a weighted average score of semantic similarity and factual similarity, as shown in the formula, where $w$ represents the weight of factual relevance. The factual similarity is calculated by the F1 score. The formulas are shown in equation~\ref{8} and ~\ref{9}.

\begin{equation}
\text { Accuracy }=\frac{w * F 1+S s}{w+1}
\label{8}
\end{equation}

\begin{equation}
F 1=\frac{|T P|}{(|T P|+0.5 \times(|F P|+|F N|))}
\label{9}
\end{equation}

Where $TP$ represents viewpoints present in both the true and generated answers, $FP$ represents viewpoints present in the generated answer but not in the true answer, and $FN$ represents viewpoints present in the true answer but not in the generated answer. After obtaining the F1 score, semantic similarity is calculated using a cross-encoder-based measurement method called SAS[23].

\textbf{Token}: In the RAG system, resource consumption is usually measured by tokens. Before the text is input into the large language model, it will be converted into different numbers of tokens. Similarly, when the model outputs, it will also generate different numbers of tokens. Therefore, when using the same generation model, the total number of tokens in the input and output stages can be calculated to evaluate and compare the resource consumption of the system.



\textbf{AI Self-Check Mechanism}

In practice, large language models often produce incorrect answers because documents contain relevant context but lack sufficient context, especially when dealing with events involving specific dates, due to limitations in processing temporal information. Such errors can be circumvented by an AI self-testing mechanism [15].

In the AI self-checking process, we use the LLM to quantitatively score the responses based on the response quality assessment criteria provided in section \ref{bijection}. If the average score of the first four key metrics exceeds 85\%, the answer is judged as Pass; conversely, it is considered as Fail. for the Fail case, the system automatically rewrites the question and restarts the pre-retrieval phase with an iterative count increment. If the answer passes the evaluation (i.e. Pass), it will be directly output as the final answer. The system presets the maximum number of iterations to be five, beyond which if the answer still fails to meet the standard, it is considered that the system is unable to answer effectively, and it is necessary to rewrite the question according to Figure~\ref{fig:fig8} and re-enter the Q\&A system for processing.

\begin{figure} [h] \small
    \centering
    %\vspace{-75pt}
    %\setlength{\abovecaptionskip}{-0.5cm}
    %\setlength{\belowcaptionskip}{-0.5cm}
    \includegraphics[width=0.9\linewidth]{imgs/fig8.pdf}
    \vspace{-5pt}
    \caption{Example of Question Rewriting}
    \label{fig:fig8}
    % \vspace{-5pt}
\end{figure}

\textbf{Algorithms of the Iterative Retrieval Q\&A Optimization Scheme}

We focuses on elaborating three core algorithms, which are introduced as follows.


The Pre-search and search stage optimization (algorithm\ref{alg:alg1}: PreRetrievalOpt) algorithm obtains the user question $Q$ through the $GetUserQ$ function and initializes it as the extended question list $ExpQList$. Then, it uses the $CallLlm$ function to call the language model to generate two extended questions for the original question and adds them to $ExpQList$. Subsequently, it uses the BM25 Retrieve algorithm of BM25 and the BGERetrieve algorithm of BGE to retrieve based on $ExpQList$ and stores the results in $CandDocs$. This dual-retrieval method combines the statistical BM25 algorithm and the semantic understanding ability of the BGE model, increasing the possibility of retrieving information relevant to the user question.
\begin{algorithm}[h!]
\label{alg:alg1}
\SetAlgoLined
\KwIn{Issues raised by users: $Q$}
\KwOut{Expanding the list of questions: $ExpOList$, Candidate documents: $CandDocs$}
\BlankLine
$Q \leftarrow$ GetUserQ()\;
$ExpO$, $CandDocs$ $\leftarrow$ PreRetrievalOpt($Q$)\;
$ExpOList$ $\leftarrow$ [$Q$]\;
\BlankLine
// Problem expansion with LLM generates two expansion problems
\For{$i \leftarrow 1$ \KwTo $2$}{
    $NewQ$ $\leftarrow$ CallLLm("Gen ext q for " + $Q$)\;
    ExpOList.Add($NewQ$)\;
}
\BlankLine
// {Combined BM25 and BGE search}

\ForEach{$q$ in ExpOList}{
    $BM25Rs$ $\leftarrow$ BM25Retrieve($q$)\;
    $BGERs$ $\leftarrow$ BGERetrieve($q$)\;
    CandDocs.Add($BM25Rs$, $BGERs$)\;
}
\BlankLine
\Return{ExpOList, CandDocs}\;
\caption{PreRetrievalOpt}
\end{algorithm}

The Post-retrieval optimization algorithm (algorithm\ref{alg:alg2}: PostRetrievalOpt) mainly optimizes the candidate documents obtained in the previous stage. First, it fine-tunes the language model through $FineTuneLlmForRating$ to obtain $PE-LLM_1$ and uses $PE1RateDoc$ to score each document in $CandDocs$. Only the documents with a score greater than 0 and their scores are stored in $FiltDocs$ in the form of tuples. Then, it uses the Sort function to sort the documents in descending order according to the ByScoreDesc standard. The final output $FiltAndSortDocs$ contains the screened and scored documents for subsequent processing.
\begin{algorithm}[h]
\label{alg:alg2}
\SetAlgoLined
\KwIn{Candidate documents: $CandDocs$}
\KwOut{Filtered, scored documents: $FiltAndSortDocs$}
\BlankLine
FiltAndSortDocs $\leftarrow$ PostRetrievalOpt($CandDocs$)\;
\BlankLine
// {Fine-tuning LLM gets $PE-LLM_1$}

 $PE-LLM_1$ $\leftarrow$ FineTuneLlmForRating()\;
\BlankLine
// {Filtering, scoring and sorting candidate documents}

FiltDocs $\leftarrow$ []\;
\For{$doc$ In $CandDocs$}{
    \If{score $\leftarrow$ PERateDoc(doc) > 0}{
        FiltDocs.Add($doc$, $score$)\;
    }
}
\BlankLine
SortDocs $\leftarrow$ Sort($FiltDocs$, $ByScoreDesc$)\;
\Return{FiltAndSortDocs}\;
\caption{ PostRetrievalOpt}
\end{algorithm}

The Generation phase optimization algorithm (algorithm\ref{alg:alg3}: GenOpt) is responsible for generating the final answer to the user question. It takes the extended question list $ExpQList$ and the screened and scored documents $FiltAndSortDocs$ as input and uses the $CallLlmToGenAns$ function to generate the initial answer. Then, it fine-tunes the language model through $FineTuneLlmForAnsEval$ to obtain $PE-LLM_2$ and uses $PE2EvalAns$ to evaluate the generated answer $GenAns$. If the evaluation result $EvalRes$ does not meet the requirements and the iteration count $IterCount$ is less than 5, it rewrites the question using $RewriteQ$ based on the current answer, performs pre-retrieval and post-retrieval optimization again, and generates a new answer. The iteration continues until the evaluation passes or the maximum iteration count is reached. If the evaluation passes, the generated answer is the final answer $Ans$. The iteration process in this stage uses the feedback of the evaluation language model to continuously optimize the answer and improve the answer quality.
\begin{algorithm}[h]
\label{alg:alg3}
\SetAlgoLined
\KwIn{Expanding the list of questions: $ExpOList$; Filtered, scored documents: $FiltAndSortDocs$}
\KwOut{Results of the question and answer session: $Ans$}
\BlankLine
$Ans$ $\leftarrow$ GenOpt($ExpOList$, $FiltAndSortDocs$)\;
\BlankLine
// {Calling LLM to Generate Answers}

$GenAns$ $\leftarrow$ CallLlmToGenAns($ExpO$, $FiltAndSortDocs$)\;
\BlankLine
// {Fine-tuning LLM to get $PE-LLM_2$ assessment answers and self-tests}

$PE-LLM_2$ $\leftarrow$ FineTuneLlmForEval()\;
\BlankLine
$EvalRes$ $\leftarrow$ PE2EvalAns($GenAns$)\;
\BlankLine
IterCount $\leftarrow$ 0\;
\If{If the AI self-test fails, rewrite the question and re-enter the process for up to 5 iterations.}{
    \While{Not EvalRes and IterCount < 5}{
        $RewrittenQ$ $\leftarrow$ RewriteQ($GenAns$)\;
        $NewExpO$, $NewCandDocs$ $\leftarrow$ PreRetrievalOpt($RewrittenQ$)\;
        $NewFiltAndSortDocs$ $\leftarrow$ PostRetrievalOpt($NewCandDocs$)\;
        $GenAns$ $\leftarrow$ CallLlmToGenAns($NewExpO$, $NewFiltAndSortDocs$)\;
        $EvalRes$ $\leftarrow$ PE2EvalAns($GenAns$)\;
        $IterCount$ $\leftarrow$ $IterCount$ + 1\;
    }
}
\Return{$Ans$}\;
\caption{GenOpt}
\end{algorithm}

% \setlength{\textfloatsep}{1pt}% Remove \textfloatsep



