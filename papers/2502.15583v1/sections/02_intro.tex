\section{Introduction}
The advent of smart grid technologies has revolutionized the energy sector by integrating advanced metering, real-time monitoring, and two-way communication systems. These advancements have enabled more efficient energy management, greater resilience to disruptions, and an enhanced consumer experience. However, the growing complexity of smart grid systems necessitates innovative solutions for real-time decision-making, operational optimization, and effective consumer interaction. One such promising avenue is the integration of intelligent question-and-answer (Q\&A) systems, leveraging large language models (LLM) and advanced retrieval techniques to address the dynamic demands of smart grids. These systems can support grid operators by providing accurate, context-aware responses to operational queries and enhancing user engagement through natural language interactions.

Existing research has explored various facets of artificial intelligence (AI) applications in smart grids, including demand forecasting, fault detection, and grid stability assessment~\cite{omitaomu2021ai}. Notably, hybrid frameworks that combine machine learning and knowledge-based approaches have shown significant promise in decision-making and emergency response scenarios~\cite{glukhikh2022cbr}. The development of retrieval-enhanced generation models has further advanced Q\&A systems by improving the accuracy and contextual relevance of responses~\cite{razzak2024deep}. Despite these advancements, challenges such as scalability, real-time performance, and adaptability to evolving grid conditions remain inadequately addressed.

With the development of AI technologies,
question-answering (Q\&A) systems have played a crucial role
in fields such as information retrieval, knowledge management,
and human-computer interaction. 
Traditional Q\&A systems typically rely on a series of structured steps to process utilizer queries, including rule-based methods and retrieval-based methods. However, recent advancements in computing power have accelerated development of LLM technologies, making generative LLM-based Q\&A systems increasingly prevalent. Since OpenAI released the GPT-3.5 model in 2022, its exceptional performance in general domains has demonstrated the vast potential of LLM. Subsequently, major companies such as Meta, Google, and Baidu, have also launched their own LLM (e.g., LLama~\cite{touvron2023llama}, Gemini~\cite{team2023gemini}, Ernie Bot~\cite{ren2023evaluation}), further advancing the field.





%Accurate PV forecasting is crucial not only for improving the utilization efficiency of PV~\cite{blaga2019current}, but also for informing a range of decision-making processes of power grids, including dynamic economic dispatch~\cite{cheng2023mitigating}, coordinated optimization control~\cite{chang2021coordinated}, power system security analysis~\cite{stringer2020observed}, demand response~\cite{li2019capacity}, and electricity market trading~\cite{antonanzas2017value}. PV forecasting involves accurately predicting the output power for a certain period in the future using historical or current operational data of the PV power generation system~\cite{hyndman2018forecasting}. Based on the forecast duration, PV forecasting encompasses ultra-short-term, short-term, medium-term, and long-term forecasts~\cite{ahmed2020review}.

%Currently, there are primarily two categories of forecasting methods for PV power generation: physical modeling methods and data analysis methods. Physical modeling methods involve utilizing meteorological and geographical data for the location of the PV power plant during the forecast period, obtaining radiation forecasts through radiation transfer equations, and then estimating future PV power generation by integrating parameter information of the PV components, combined with operational equations and simulation models for the PV components. Physical modeling methods can be classified into two main categories: numerical weather prediction (NWP) techniques and image-based approaches. The NWP techniques utilize models to transform NWP-generated meteorological parameters into PV power output estimates~\cite{yang2018history}. To forecast a PV conversion model that incorporates NWP data, the Kalman filter was employed in~\cite{yang2021kalman}. On the other hand, the image-based approaches rely on sky~\cite{hu2018new} or satellite images~\cite{jang2016solar} obtained from camera sensors and remote sensing. These images are used to predict patterns in meteorological variations and the resulting PV power output. However, the numerous physical parameters, intricate computational procedures, and susceptibility to environmental factors of parameters lead to the poor robustness and limited forecast accuracy of physical models, thus restricting their practical applicability.
% In the field of PV power generation forecasting, traditional methods and models typically focus on statistical and physics-based approaches, such as time series analysis, linear regression, and support vector machines (SVM)~\cite{Wang2023}. While these methods can provide predictions to some extent, they often struggle with complex nonlinear relationships and high-dimensional data issues~\cite{Zhou2023}. In recent years, deep learning has emerged as a hot research topic, demonstrating significant advantages in handling large-scale data and complex nonlinear problems~\cite{Sabri2023}.
Although generative Q\&A systems can flexibly generate responses, they still face challenges in terms of accuracy and reliability. For instance, models could produce fabricated information on certain specific questions, a phenomenon known as ``hallucination"~\cite{huang2023survey}. To improve response accuracy, retrieval-augmented generation (RAG)~\cite{lewis2020retrieval} Q\&A systems synthesize generative LLM with external retrieval resources, retrieving relevant background knowledge from these resources and providing it as supplementary input to the generative LLM. This approach helps the generative LLM respond based on authentic external information, thereby effectively reducing the ``hallucination" phenomenon of the model. In this context, RAG technology has gradually become a key technology for applications
such as search engines, chat engines, and agents, improving
the accuracy and practicality of model responses by combining
retrieval with generation. 

Traditional RAG Q\&A systems are typically involve three steps: building a text index, retrieving text content, and generating responses. However, this method only relies on a single retrieval to find relevant texts and does not further optimize the retrieval and generation processes, thus limiting the system's retrieval and response quality. As RAG technology advances, enhanced and modular retrieval augmentations have emerged, but they still face the following issues:

\textbf{1) Insufficient retrieval quality:} This includes poor relevance between the user queries and the retrieved content, incomplete retrieval results, and excessive verbosity due to the large and heterogeneous datasets for smart grid environments.

\textbf{2) Limited response quality:} Due to the impact of retrieval quality, where LLM could be misled by irrelevant contexts. Additionally, the confusion between parametric and non-parametric memories can also lead to suboptimal performance of LLM, which further compromises the querying quality for smart grid environments.
%DLRMs~\cite{zhang2019deep, acun2021understanding, kal2021space, gupta2020deeprecsys, huang2021novel} attract a lot of attention from the research and industry. 

Current RAG optimization research
has focused on three areas respectively:
%

\textbf{1) Optimization based on model training and fine-tuning:} These methods focus on introducing optimization techniques during the pre-training and fine-tuning stages of retrieval and generative models to improve their overall performance.  Yu~\cite{yu2023augmentation} fine-tuned the retriever using feedback signals to align it with the preferences of the LLM, enhancing their coordination. Cheng~\cite{cheng2024lift} fine-tuned the generator to adapt the LLM to the input structure of text pairs. The Retro~\cite{borgeaud2022improving} method pre-trains the generative model from scratch, encodes retrieved documents using a Transformer encoder, and integrates them into the token hidden states of the attention layer using a cross-attention mechanism, achieving deep integration between retrieved information and the generative model. This approach not only reduces the model's parameter size but also achieves superior performance in perplexity metrics, demonstrating its potential in enhancing generation quality.

\textbf{2) Optimization based on data sources:} These methods extend the retrieval of single unstructured texts in knowledge bases to using structured data or data generated by the model itself to further improve the effectiveness of RAG. For instance, Ret-LLM~\cite{modarressi2023ret} constructs a knowledge graph as memory using past Q\&A data for subsequent dialogue reference. Similar to RET-LLM, the Selfmem method creates an unlimited memory pool, adding each round of LLM output to the pool and selecting the most appropriate memory through a memory selector to improve subsequent generation. The Surge~\cite{kang2023knowledge} method retrieves relevant subgraphs from knowledge graphs to improve model response quality, ensuring the relevance of retrieved subgraphs to questions through perturbed word embeddings and contrastive learning. Luyu Gao et al. proposed the Hypothetical Document Embedding (HyDE) method~\cite{gao2022precise}. When a utilizer provides a query, the HyDE method, unlike traditional retrieval approaches that rely on query-based search, first hypothesizes an answer and then retrieves relevant information from the knowledge base based on this hypothetical answer. It focutilizes on the similarity between the hypothesized answer and the actual answer rather than seeking embedding similarity for the query, thereby improving retrieval recall.

\textbf{3) Optimization based on the retrieval process:} These methods improve the traditional single-retrieval mode of RAG by introducing multiple retrieval strategies such as iterative retrieval, recursive retrieval, and adaptive retrieval~\cite{jiang2023active} to enhance retrieval performance and effectiveness. Iterative retrieval allows the model to perform multiple rounds of retrieval, enhancing the depth and relevance of the information obtained. Recursive retrieval refers to using the results of the previous retrieval as input for subsequent retrievals, which helps deeply mine relevant information, especially when dealing with complex or multi-step queries. Ori Ram~\cite{ram2023context} proposed a method called In-Context RALM, which generates a small number of incomplete answers through the LLM each time, then retrieves texts similar to these answers and concatenates them into the prompt for subsequent answer generation, completing the answer through multiple retrieval processes. Akari Asai et al. introduced a method called Self-RAG~\cite{asai2023self}, which marks utilizer queries and retrieved content using two key markers: retrieval markers and criticism markers. Retrieval markers determine the need to invoke the retrieval model, while criticism markers assess the relevance of retrieved content to the query. Finally, the results are re-ranked based on relevance scores to optimize the model's output. Zhangyin Feng~\cite{feng2024retrieval} proposed an Iterative Retrieval Generation Collaboration (ITRG) framework, which enhances response quality through multiple iterations. It initially retrieves information using the original query, and each subsequent iteration involves two steps: 1) generating a response with potential hallucinations using the LLM and retrieved content; 2) retrieving relevant content using the model's response from the previous step.

%DLRMs have accounted for a significant proportion of deep learning instances in the industry, including product recommendations from Amazon~\cite{ma2020temporal}, personalized advertisements from Google~\cite{cheng2016wide}, and e-commerce recommendations from Alibaba~\cite{wang2018billion}. 

% \yuke{add the high-level description of the DLRM workflow at here}
% Recommendation models are commonly used to servive the web-based recommendation and have become one of the most popular machine learning applications in the industry. 
%  brings new solutions to this field and has been changing architectures of recommendation systems dramatically
% Lots of companies employs recommendation models for their online service, including product recommendation from Amazon~\cite{ma2020temporal} and personalized advertisements from Google~\cite{cheng2016wide}. 
% To processing the highly sparse data, reommendation systems initially employed content-based filtering~\cite{pazzani2007content} and collaborative filtering~\cite{schafer2007collaborative} to extract features. 
% Recently, deep learning brings new solutions to this field and has been changing architectures of recommendation systems dramatically~\cite{zhang2019deep}.
% \yuke{The training data of recommendation model is highly sparse, such as the user's favorite movie type which is usually a one hot vector.} 
% To efficiently train a deep learning recommendation model (DLRM), there have been many efforts on software and hardware design~\todo{[cite]}. Unfortunately, existing DLRM system are still imperfect and inevitably fallshort the following aspects:

% \textbf{First, large memory footprint of embedding tables.}
% Generally, 


% ====================================================================================

% DLRM consists of two main components, the \textit{Embedding Tables} to process the sparse input and the \textit{multi-layer perceptron} (MLP) to make a final prediction of the user's click through rate (CTR)~\cite{naumov2019deep, zhou2019deep}. The extra-large footprint of embedding tables 

%with this expensive and energy-consuming hybrid-parallel training system design. 
%This drives the need for a more flexible and economical DLRM training system design, which could democratize the training of large-scale DLRMs with limited GPU resources and lower the training cost of industry-scale DLRM model.


Research published by Lexin Zhou~\cite{zhou2024larger} in the authoritative journal Nature pointed out that subtle adjustments in prompt engineering can have profound impacts on the output of LLM, emphasizing the need for a rigorous and meticulous approach in designing prompts to ensure output stability and reliability. Although RAG technologies have made significant progress, there is a lack of comprehensive Q\&A system architecture optimized for each critical stage: pre-retrieval, retrieval, post-retrieval processing, and response generation. Finally, this paper introduces ``Chats-Grid", an iterative retrieval Q\&A optimization scheme tailored to smart grid environments, featuring stage-specific optimizations and refined strategies to construct a comprehensive solution. By leveraging large models and retrieval enhancement generation, Chats-Grid aims to provide a robust framework for optimizing grid operations and consumer interactions. The approach addresses key limitations in existing systems, including response latency, contextual accuracy, and adaptability, thus paving the way for a more resilient and efficient smart grid infrastructure. In this study, we evaluate the proposed methodology's effectiveness through comprehensive simulations and case studies, highlighting its potential to transform smart grid management and user engagement. The overall optimization framework is detailed in Figure~\ref{fig:fig1}. Overall, we make the following contributions in this paper:
\begin{itemize}
    \item \textbf{Before retrieval}, we expand the query to increase the depth and scope of retrieval; during retrieval, we enhance retrieval robustness by employing both dense and sparse retrieval methods concurrently.

\item \textbf{After retrieval}, we fine-tune the LLM using prompt engineering to score the relevance of retrieved candidate documents, filtering out irrelevant documents for the smart grid environments to the following contributions and reordering them.

\item \textbf{During the generation stage}, we further avoid hallucinations through an answer self-checking mechanism and evaluate response quality using a quintuple assessment standard. If the self-check fails, the query is rephrased and re-entered into the Chats-Grid system for processing.

\item \textbf{Comprehensive Q\&A system architecture optimization}, we are pioneers in applying comprehensive Q\&A system architecture to improve efficiency and  answer quality in smart grid. Specifically, Chats-Grid shows improvements of 2.37\%, 2.19\%, and 3.58\% in fidelity, context recall rate, and answer accuracy over Self-RAG, respectively, and 0.94\%, 4.39\%, and 2.45\% over ITRG.
\end{itemize}


\begin{figure} [h] \small
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/fig1.pdf}
    %\vspace{-50pt}
    \caption{Optimized RAG System Process.}
    \label{fig:fig1}
    %\vspace{10pt}
\end{figure}


The remainder of this paper is organized as follows. Section II presents the preliminaries of the proposed method. Section III offers a comprehensive overview of the design aspects of the Eff-TT table and sorting indexes methods aimed at enhancing the performance of Eff-TT tables. Section IV offers an overview of the TT-based pipeline and the training system design, along with its solution for addressing Read-after-write conflicts within the DLRM training pipeline. Case studies are conducted in Section V and Section VI concludes this paper.



% \begin{figure}[t]
% \caption{\todo{Listing 1.} Efficient TT table API in Python}
% % \vspace{-10pt}
% \begin{lstlisting}[style=tt1,label={code: wmma API interface.}]
% import Efficient_TT_Embedding as Eff_TT
% import torch

% # Define an 1024*128 Efficient TT table .
% emt = Eff_TT(
%     num_emb=1024, emb_dim=128, tt_ranks=[64, 64])
        
% # Generate a random index, batch size=4096.
% index = torch.randint(0, 1024, (4096,1))

% # Lookup embeddings from Efficient TT table.
% embedddings = emt(index)

% \end{lstlisting} 
% % \vspace{-15pt}
% \end{figure}\label{list: tt example}




