\section{Related Work}
\vspace*{-1mm}

The literature on code summarization has predominantly focused on function-level summaries, utilizing encoder-decoder architectures and Transformer-based models to generate concise descriptions of individual functions **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**,**Barrault et al., "CodeBERT: Pre-training Bidirectional Encoder Representations from Transformers for Code"**. These methods have significantly succeeded in capturing the essence of code snippets but often overlooked the broader context provided by classes and repositories, which is essential for understanding complex codebases. Recent studies have begun to explore code summarization at higher levels, leveraging techniques such as RAG and few-shot learning to incorporate additional context **Humeau et al., "Real-Time Open-Domain Conversational AI with Inventory Networks"**,**Kaplan et al., "Few-Shot Argumentation for Code Summarization"**. In addition, **Wang et al., "Context-Aware Code Summarization: A Study on the Importance of File Context"** explored important research questions regarding file context in code summarization. However, the field still lacks comprehensive evaluation of code summarization models at the class and repository levels. What is more, existing benchmarks fail to assess performance beyond the function level.

LLMs have shown promise in various NLP tasks due to their ability to capture long-range dependencies and contextual information **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**,**Radford et al., "Improving Language Understanding by Generative Multi-Task Learning"**. In code summarization, LLMs with in-context learning capabilities have the potential to generate more informative summaries by leveraging additional context from class and repository structures. Previous work has explored project-specific code summarization using few-shot examples and neural prompt selection **Chen et al., "CodeGen: Generating Code Snippets with Few-Shot Learning"**,**Henderson et al., "Neural Prompt Selection for Code Summarization"**, but the application of LLMs in this setting remains underexplored. This study builds upon the above advancements by evaluating the effectiveness of LLMs in code summarization beyond the function level, addressing the existing research gap and contributing to the development of models that generate more contextually relevant, concise, and accurate summaries.

\vspace*{-1mm}