\section{Related Work}
\vspace*{-1mm}

The literature on code summarization has predominantly focused on function-level summaries, utilizing encoder-decoder architectures and Transformer-based models to generate concise descriptions of individual functions \cite{vaswani2017attention}, \cite{CodeT5}. These methods have significantly successed in capturing the essence of code snippets but often overlooked the broader context provided by classes and repositories, which is essential for understanding complex codebases. Recent studies have begun to explore code summarization at higher levels, leveraging techniques such as RAG and few-shot learning to incorporate additional context \cite{StarCoder}, \cite{liu2023repobench}. In addition, \cite{su2024revisiting} explored important research questions regarding file context in code summarization. However, the field still lacks comprehensive evaluation of code summarization models at the class and repository levels. What is more, existing benchmarks fail to assess performance beyond the function level.

LLMs have shown promise in various NLP tasks due to their ability to capture long-range dependencies and contextual information \cite{GPT-4}, \cite{DeepSeekCoder}. In code summarization, LLMs with in-context learning capabilities have the potential to generate more informative summaries by leveraging additional context from class and repository structures. Previous work has explored project-specific code summarization using few-shot examples and neural prompt selection \cite{PSCodeSum}, but the application of LLMs in this setting remains underexplored. This study builds upon the above advancements by evaluating the effectiveness of LLMs in code summarization beyond the function level, addressing the existing research gap and contributing to the development of models that generate more contextually relevant, concise, and accurate summaries.

\vspace*{-1mm}