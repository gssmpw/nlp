\section{Conclusion and Future Work}\label{sec:conclusion}

%Datasets suffer severe selection bias when labels are expensive. 
Datasets often suffer from severe selection bias when labels are expensive, 
making it difficult to reliably evaluate AI models or estimate causal effects. 
We propose a novel framework for adaptive labeling 
to improve the reliability of measurement.
%to evaluate AI models under out-of-support distribution shifts. 
Following a Bayesian framework, we formulate an MDP over posterior beliefs on the estimand of interest (model performance or ATE)
and demonstrate that 
planning problems can be efficiently solved using pathwise policy gradients, 
computed through a carefully designed auto-differentiable pipeline. 
We show that even one-step lookaheads can yield substantial improvements over heuristic algorithms inspired by active learning. 
Additionally, our results suggest that $\mathsf{Smoothed\text{-}Autodiff}$ gradient estimation can outperform \textsf{REINFORCE}-based methods, 
with potential implications for policy gradients in model-based reinforcement learning more broadly.  
% Furthermore, our framework generalizes seamlessly to other applications, such as efficient estimation of the average treatment effect.

Our experiments highlight several important future research directions. UQ methodologies should possess certain key properties.
Foremost,
posterior updates  should be consistent. 
Additionally, these updates should be readily available and easily differentiable.  Recent advances in  Bayesian 
Transformers~\citep{NguyenGr22, MullerHoArGrHu22} present an intriguing dimension to explore in this regard. 
Another interesting direction involves enabling multi-step lookaheads by addressing the autodiff bottleneck.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
