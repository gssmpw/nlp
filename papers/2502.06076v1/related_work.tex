
\section{Related work}
\label{sec:related-work}

\paragraph{Active Learning} Adaptive labeling is a classical topic in machine learning.
When applied to model evaluation, our framework posits a less  ambitious goal than the active learning literature~\citep{AggarwalKoGuHaPh14,Settles09} where labels are adaptively collected  to \emph{improve} model performance in classification problems. Typical active learning heuristics are greedy and select inputs based on uncertainty sampling techniques such as margin-sampling or entropy, or by leveraging disagreements among different models (e.g., query-by-committee and Bayesian Active Learning by Disagreement (BALD)~\citep{HoulsbyHuGhLe11}). 
Batching is a longstanding challenge for these approaches as it requires collecting labels over diverse inputs~\citep{Settles09}. Heuristic criteria based on diversity and density are sometimes incorporated to account for the feature space distribution~\citep{Settles09},  and recent extensions continue to rely on greedy algorithms (e.g., BatchBALD~\citep{KirschVaGa19}, BatchMIG~\citep{WangSuGr21}).

We leverage our limited scope on the efficient estimation of model performance (or ATE), rather than model improvement, 
to formalize a unified and novel computational framework that allows planning for the future.
Our formulation can flexibly handle different objectives and uncertainty quantification methodologies, and seamlessly handle regression and classification problems alike, in contrast to the traditional focus on classification in active learning.
We provide lookahead policies~\citep{BertsekasTs96,EfroniDaScMa18, EfroniGhMa20} that plan for the future, and show they dominate active learning heuristics in Section~\ref{sec:experiment}.

\paragraph{Simulation Optimization}
Bayesian optimization considers black-box functions that are computationally challenging to 
evaluate by modeling the function as a draw from a Gaussian process~\citep{Frazier18}. 
The knowledge gradient algorithm~\citep{FrazierPoDa08}
maximizes the single period expected increase in the black-box function value and can be viewed as a one-step lookahead policy; several authors propose extensions to non-myopic and cost-aware problems~\citep{AstudilloJiBaBaFr21}.
We extend these ideas to adaptive labeling  by incorporating batching and combinatorial action spaces, and accommodate more advanced uncertainty quantification methods from deep learning.

Our work is closely related to the  simulation optimization literature~\citep{AmaranSaShBu16},
 particularly to ranking and selection~\citep{ChenLiYuCh00,GlynnJu04, KimNe07, ChenChLePu15, HongNeXu15}.
Our adaptive labeling problem can be viewed as a ranking \& selection or pure-exploration bandit problem. However, in contrast to conventional formulations, our central focus on batching
introduces combinatorial action spaces.
Additionally, our formulation involves continuous covariates and a large number of alternatives,   making it  intractable for standard methods, whose performance deteriorates as the number of alternatives grow larger. 

Instead, we parameterize our discrete optimization problem through a continuous parametrization policy and use gradient-based methods to optimize it. We are able to exploit the similarity between the alternatives through this approximation, which is in contrast to the  traditional simulation optimization literature that mostly considers independent alternatives. 

\paragraph{Gradient Estimation} 

There is extensive literature on gradient estimation~\citep{Glynn87, Glasserman90, Glasserman92, FuHu12}, with applications in probabilistic modeling~\citep{KingmaWe14,JangGuPo17} and reinforcement learning~\citep{Williams92,SuttonMcSiMa99}. 
The \textsf{REINFORCE} approach uses the score-function estimator~\citep{Williams92,SuttonMcSiMa99}
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)] = \E_{X \sim \pi_\theta}[G(X) \nabla_\theta\log \pi_\theta (X)]$. While unbiased, these estimators suffer high variance~\citep{GreensmithBaBa04} and require many tricks to be effective for downstream applications~\citep{HuangDoRaKaWa22}.
Similarly, finite difference approximations~\citep{FuHi97}  and finite perturbation analysis~\citep{Cao87,HoCaCh89} also only require zero-th order function evalautions, but perform poorly in high-dimensional problems~\citep{Glynn89, Glasserman04}.


Alternatively, the reparameterization trick~\citep{MaddisonMnTe17,JangGuPo17, PaulusMaKr21} 
takes a random variable $Z$ whose distribution does not depend on the parameter of interest $\theta$ and 
exploits the identity
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)]
= \nabla_\theta \E_{Z}[G(h(Z, \theta))]$ for some function $h$.
 Gradient estimators based on the reparameterization trick 
 typically leads to much smaller
variance~\citep{MohamedRoFiMn20}, 
but can only be applied in special cases where $G(h(Z, \theta))$ is differentiable with respect to $\theta$.  
Infinitesimal Perturbation Analysis (IPA)~\citep{HoEyCh83, JohnsonJa89,Glasserman90} 
is a standard framework for constructing such pathwise gradient estimators, but conditions ensuring IPA to be valid~\citep{Cao85,HeidelbergerCaZaMiSu88,Glasserman90, Glasserman92} 
are often restrictive  and does not apply in our setting. 
In fact, in our adaptive labeling problem, the function $G(h(Z, \theta))$ of interest
is non-differentiable and only becomes differentiable in expectation.
To address this challenge, we construct a smooth approximation of the function $G(h(\cdot))$, enabling the application of pathwise gradient estimators by leveraging the known dynamics (posterior updates) of our MDP.

We are inspired  by a line of work on differentiable 
simulators~\citep{deAvilaFiSmAlTeKl18,HuangHuDuZhSuTeGa21,MoraAnHaVeCo21, XuMaNaRaMaGaMa21,SuhSiZhTe22} with applications in reinforcement learning~\citep{MoraAnHaVeCo21, XuMaNaRaMaGaMa21,MadekaToEiLuFoKa22, AlvoRuKa23}.
Theoretical analysis in stochastic optimization~\citep{GhadimiLa13, MohamedRoFiMn20} emphasizes the benefit of first-order estimators over
zeroth-order counterparts, and~\citet{SuhSiZhTe22} notes first-order estimators
perform well when the objective is sufficiently smooth and continuous. 
In our setting, although the simulation path is not differentiable, we introduce a smooth approximation of the path, enabling the application of first-order gradient estimates.
Our approach provides low-variance gradient estimates at the cost of bias, and  offers a novel way to balance bias and variance for non-differentiable functions~\citep{BengioLeCo13, JangGuPo17, TuckerMnMaLaSo17}.
Biased gradient estimates have not been studied much in the simulation optimization literature, 
aside from two notable exceptions:~\citet{EckmanHe20} show the utility of biased estimators when in the neighborhood of a local minimizer, and~\citet{CheDoNa24} study queueing network control and propose a smoothing scheme for computing biased policy gradient estimators. Focusing on adaptive data collection, we propose a tailored smoothing approach for  handling combinatorial action spaces and (approximate) posterior state transitions. 
