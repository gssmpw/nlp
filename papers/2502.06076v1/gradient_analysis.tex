
\newcommand{\cosim}{\mathsf{cosim}}


%Our codebase is available at \url{https://github.com/namkoong-lab/adaptive-labeling}.


\section{Analysis of Policy Gradient Estimators}
\label{sec:gradient_estimation_anlaysis}

In this section, we present a theoretical and empirical analysis of the statistical properties (bias-variance trade-off) of the \textsf{REINFORCE} and  $\mathsf{Smoothed\text{-}Autodiff}$ (smoothed-pathwise gradient) estimators and their efficacy
in downstream policy optimization task for adaptive labeling. First, in section \ref{sec:theory_gradient_estimation}, we theoretically compare the error in the $\mathsf{Smoothed\text{-}Autodiff}$ estimator and the \textsf{REINFORCE} estimator.
Next, in section \ref{sec:empirical_gradient_estimation_quality}, we empirically study the estimation quality of these two estimators within our framework, using  cosine similarity as the comparison metric. Finally, in section \ref{sec:gradient_estimation_algorithmic_performance}, we investigate the impact of gradient estimation quality on the performance of one-step lookaheads in the adaptive labeling setting.


%\textcolor{red}{Give a summary of the non-triviality of these results}.

%In this Section we present a theoretical and empirical analysis of when do smoothed pathwise gradients might out-perform \textsf{REINFORCE} gradients (and vice-versa) within our framework. Specifically, we present a bias-variance trade-off analysis for pathwise gradient and \textsf{REINFORCE} gradient -  

\subsection{Analytic insights}
\label{sec:theory_gradient_estimation}

 Recall that in our optimization problem (\ref{eqn:general-obj}), we encounter a combinatorial action space.
To build intuition, in Section~\ref{sec:theory_gradient_estimation_binary},   we start with a simple two-action setting to compare the 
$\mathsf{Smoothed\text{-}Autodiff}$ estimator and the \textsf{REINFORCE} estimator. 
The more  general setting is discussed in Section~\ref{sec:theory_gradient_estimation_multi}. 


 \subsubsection{Binary actions}
\label{sec:theory_gradient_estimation_binary}


To compare the two estimators, we consider a simplified setting with two possible actions 
$A \in \mathcal{A} := \{-1,1\}$, where the policy is parametrized by
$\theta \in [0,1]$, with
$\pi_\theta(-1) = \theta$ and $\pi_\theta(1) =1-\theta$. We are interested in the gradient $\nabla_\theta H(\theta)$, where $H(\theta) =\E_{A\sim \pi_\theta}G(A)$. For simplicity, we set $G(A)=A$.

\noindent Given $N$ i.i.d. samples of $A_i \sim \pi_\theta$, the \textsf{REINFORCE} estimator for $\nabla_\theta H(\theta)$ is defined as
\begin{align}
    \hat{\nabla}^{\mathsf{RF}}_N = \frac{1}{N}\sum_{i=1}^N\Big(G(A_i) \nabla_\theta \log(\pi_\theta(A_i)) \Big).
    \label{eqn:bin-reinf-def}
\end{align}
Using  the reparametrization trick, we can rewrite  $H(\theta)$ as $\E_U G(h(U,\theta))$, where 
\begin{align*}
    h(U,\theta):= 2\mc{I}(U > \theta)-1\stackrel{d}{=}A,
\end{align*}
with $U\sim \mathsf{Uni}[0,1]$. However, $h(U,\theta)$ is not differentiable with respect to $\theta$. Therefore, to enable the pathwise gradient estimation, we consider a smoothed approximation $ h_\tau(U,\theta)$ of $ h(U,\theta)$:
\begin{align*}
    h_{\tau}(U,\theta) = \frac{\exp\left(\frac{U-\theta}{\tau}\right)-1}{\exp\left(\frac{U-\theta}{\tau}\right)+1},
\end{align*}
where $\tau$ is a temperature parameter that controls the smoothness of $h_\tau(U,\theta)$.

\noindent The $\mathsf{Smoothed\text{-}Autodiff}$ estimator $\nabla_{\tau,N}^{\mathsf{grad}}$ is the $N$-sample approximation of 
$\E_{U}[\nabla_\theta G(h_{\tau}(U,\theta))]$, 
 given by
\begin{align}
    \hat{\nabla}^{\mathsf{grad}}_{\tau,N} = \frac{1}{N}\sum_{i=1}^N\Big(\nabla_\theta{G}(h_\tau(U_i,\theta))\Big).
    \label{eqn:bin-grad-def}
\end{align}
 Our result, with proof in Section~\ref{sec:proof-thm-binary},
 highlights the conditions under which 
 $\hat{\nabla}^{\mathsf{grad}}_{\tau,N}$~\eqref{eqn:bin-grad-def} achieves better gradient estimation (lower mean squared error)  compared to $\hat{\nabla}^{\mathsf{RF}}_{N}$~\eqref{eqn:bin-reinf-def}.
\begin{theorem}
For $\theta \in [0,1]$ and $N \le  \frac{1}{4\theta(1-\theta)} - 1$,
there exists $\Tilde{\tau}$ depending on $(N, \theta)$ such that 
\begin{align*}
\mse(\hat{\nabla}^{\mathsf{grad}}_{\Tilde{\tau},N})
< 
\mse(\hat{\nabla}^{\mathsf{RF}}_{N}).
\end{align*}
Additionally, for any $N$, $\theta = \frac{1}{k}$, and $k \to \infty$, we have that
$\mse(\hat{\nabla}^{\mathsf{RF}}_{N})  = \Omega(k) 
$, $\mse(\hat{\nabla}^{\mathsf{grad}}_{\tilde{\tau},N}) < 4$. 
The same statement holds for $\theta =  1-\frac{1}{k}$ as well. 
This implies that the MSE of $\hat{\nabla}^{\mathsf{RF}}_{N}$ is unbounded, while the MSE of gradient estimator is bounded.
\label{thm:binary}
\end{theorem}
\noindent In particular, this example illustrates how $\mathsf{Smoothed\text{-}Autodiff}$ gradients shine when 
the sample size $N$ is small and the policy takes on extreme values.

\subsubsection{General case}
\label{sec:theory_gradient_estimation_multi}

We now extend our analysis from Section~\ref{sec:theory_gradient_estimation_binary} to the setting with more than two actions.
Consider an  action space  $\mathcal{A}:= \{a_1,a_2,...,a_m\} \subset \mathbb{R}$ with $m \ge 2$ actions. 
Actions $A\in \mathcal{A}$ are drawn according to a policy $\pi_\theta$, such that $A=a_i$ with probability $\pi_\theta(a_i) = \frac{\theta_i}{\sum_{j=1}^m \theta_j}$. 
As in Section~\ref{sec:theory_gradient_estimation_binary}, we are interested in the gradient $\nabla_\theta H(\theta)$, where $H(\theta) =\E_{A\sim \pi_\theta}G(A)$. We assume that 
$G(\cdot)$ is differentiable and  sufficiently smooth such that
$G'(a) \le \bar{G}$ for all $a \in \mathsf{conv}(a_1,...,a_m)$. 
 %Assume that an action is sampled based on the policy $\pi_\theta$ as follows:
% \begin{align*}
%     \pi_\theta(a_i) = \frac{\theta_i}{\sum_{j=1}^m \theta_j}.
% \end{align*}

% \noindent To avoid complexities, below we consider a simple setting to convey intuition of why 
% our autodiff estimator performs better than the \textsf{REINFORCE} gradient estimator.

% \noindent \textbf{Specific Setting:} Let $\mathcal{A} = \{1,2,...,m\}$, with $\pi_\theta(i) = \frac{\theta_i}{\sum_{j=1}^m \theta_j}$. \textbf{Let $f(A) =A$.} 

% \vspace{5mm}


% \[\E_{A\sim \pi_\theta}(G(A)) = \sum_{i=1}^m \frac{\theta_i}{\sum_{j=1}^m \theta_j} G(a_i)\]


% \[\nabla_{\theta_i} \E_{A\sim \pi_\theta}(f(A)) =  
% \frac{
% f(a_i) (\sum_{j=1}^n \theta_j) - (\sum_{j=1}^n   \theta_j f(a_j))
% }{(\sum_{j=1}^n \theta_j)^2} \]

% \[\nabla_{\theta} \E_{A\sim \pi_\theta}(f(A)) =  
% \frac{
% 1}{ \sum_{j=1}^n \theta_j} \bm{f(a)} - \frac{\sum_{j=1}^n   \theta_j f(a_j)
% }{(\sum_{j=1}^n \theta_j)^2} \bm{1} \]


% Another helpful expression
% \[\nabla_{\theta_i} \E_{A\sim \pi_\theta}(f(A)) =  \frac{ \sum_{j=1}^n \theta_j (f(a_i)-f(a_j))}{(\sum_{j=1}^n \theta_j)^2} \]

% \paragraph{\textsf{REINFORCE} estimator} 
% $\nabla^{\mathsf{RF}}_{\theta_i} H(\theta) = \nabla_{\theta_i}  \E_{A\sim \pi_{\theta}} G(A) = {\E_{A\sim \pi_\theta} \Big(G(A) \nabla_{\theta_i} \log(\pi_\theta(A))} \Big)$



% To estimate it we take $N$ monte-carlo samples -

\noindent As in~\eqref{eqn:bin-reinf-def}, given $N$ i.i.d. samples of $A_j \sim \pi_\theta$,
the \textsf{REINFORCE} estimator 
is defined as
\begin{align}
\hat{\nabla}^{\mathsf{RF}}_{N,\theta_i}
\defeq 
\frac{1}{N}\sum_{j=1}^N\Big(G(A_j) \nabla_{\theta_i} \log(\pi_\theta(A_j)) \Big).
\label{eqn:eq-def-reinforce}
\end{align}

% Therefore, the expected squared error in the Reinforce estimator is  
% \[ \E[\\mse_i(\hat{\nabla}^{\mathsf{RF}}_{N,\theta_i})] = \frac{1}{N} \V_{A \sim \pi_\theta} \bigg(G(A) \nabla_{\theta_i} \log(\pi_\theta(A)) \bigg)\]


%\paragraph{Autodiff estimator (using gumbel softmax)} 

\noindent We now generalize the $\mathsf{Smoothed\text{-}Autodiff}$ estimator in ~\eqref{eqn:bin-grad-def}. Recall that $H(\theta) = \E_{A\sim \pi_\theta} {G}(A)$,  which can be rewritten as $$H(\theta) =  \E_{A\sim \pi_\theta}\left[ G \left(\sum_{i=1}^ma_i (\mc{I}(A)=a_i) \right) \right].$$
 Using the Gumbel-Max reparametrization trick  
 from~\citep{JangGuPo17}, we have
\begin{align*}
\{\mc{I}(A=a_i)\}_{i=1}^m 
\deq 
\tilde{h}(Z,\theta) \defeq
\mathsf{one\_hot}\left[ \argmax_i\set{\log(\theta_i) + Z_i} \right]
\end{align*}
 with  $Z_i$ being i.i.d samples drawn from Gumbel (0, 1), ensuring that 
  $A \sim \pi_\theta$. 
  
  
 \noindent Therefore, we can now express   $H(\theta) = \E_Z[G\left(h(Z,\theta) \right)]$, where $h(Z,\theta) = \tilde{h}(Z,\theta) \odot (a_i)_{i=1}^m $, the element-wise product  of $\tilde{h}(Z,\theta)$ and  the vector $(a_i)_{i=1}^m$. 
Since $h(Z,\theta)$ is not differentiable with respect to $\theta$, to enable pathwise gradient estimation, we consider a smoothed approximation $h_\tau(Z,\theta)$ of $ h(Z,\theta)$, as described in~\citep{JangGuPo17}:
\begin{align*}
h_\tau(Z,\theta)  
=
\left( \frac{\exp((\log (\theta_i)+Z_i)/\tau)}{\sum_{j=1}^{m}\exp((\log (\theta_j)+Z_j)/\tau)}\right)_{i=1}^m \odot (a_i)_{i=1}^m,
\end{align*} 
where $\tau$ is a temperature parameter controlling the smoothness of $h_\tau(Z,\theta)$. Thus, $$H(\theta)  \approx 
\frac{1}{N}\sum_{j=1}^N G(h_\tau(Z^{(j)},\theta)).$$
We now define the $\mathsf{Smoothed\text{-}Autodiff}$ estimator as
\begin{align}
  \hat{\nabla}^{\mathsf{grad}}_{\tau, N,\theta_i}
  \defeq \frac{1}{N}\sum_{j=1}^N\nabla_{\theta_i}G(h_\tau(Z^{(j)},\theta)).
\label{eqn:eq-def-autodiff}
\end{align}

We present our main result, Theorem~\ref{thm:multi}, for the $\mathsf{Smoothed\text{-}Autodiff}$ estimator defined in~\eqref{eqn:eq-def-autodiff} and \textsf{REINFORCE} estimator defined in~\eqref{eqn:eq-def-reinforce}.  Similar to Theorem~\ref{thm:binary},
the result below highlights 
that the \textsf{REINFORCE}  estimator will be worse
as compared to the $\mathsf{Smoothed\text{-}Autodiff}$ estimator
if the number of samples $N$ is not   large enough.
The proof is provided in Section~\ref{sec:proof-thm-multi}:
% but our results extend to the below estimator that we use in our pipeline:
%  \begin{align}
%   \Tilde{\nabla}^{\mathsf{grad}}_{N,\theta_i}
% \defeq  \nabla_{\theta_i}
%   G\left(  \sum_{i=1}^m a_i (h_\tau(Z,\theta))_i \right) \label{eqn:eq-def-autodiff-inside}
% \end{align}


% \begin{align}
%       \hat{\nabla}^{\mathsf{grad}}_{N,\theta_i}' =
%       f'\left(  \sum_{i=1}^m a_i (h_\tau(Z,\theta))_i \right)
%       \cdot 
%       \left( \sum_{i=1}^m a_i  \nabla(h_\tau(Z,\theta))_i \right)
% \end{align}

%  aâˆˆ[amina \in [a_{\min},a_{\max}],
% where a_{\min} = \min_{i=1}^m a_ia_{\min} = \min_{i=1}^m a_i


\begin{theorem}
Assuming
$G(\cdot)$ is differentiable and  sufficiently smooth such that
$|G'(a)| \le \bar{G}$ for all $a \in \mathsf{conv}(a_1,...,a_m)$. 
Fix $i \in [m]$.
For $\theta \in [0,1]^m$, any $\epsilon>0$ and 
\begin{align*}
N \le 
\frac{1}{\paran{\nabla_{\theta_i} \E_{A\sim \pi_\theta}(G(A))+\epsilon}^2}
\paran{
\frac{\sum_{j=1}^m\theta_j(G(a_j))^2}{(\sum_{j=1}^m\theta_j)^3} + \frac{(G(a_i))^2}{\theta_i\sum_{j=1}^m \theta_j} - 2 \frac{(G(a_i))^2}{(\sum_{j=1}^m \theta_j)^2}  - \left(\nabla_{\theta_i} \E_{A\sim \pi_\theta}(G(A))\right)^2},
\end{align*}
there exists $\Tilde{\tau}$ depending on $ (N, \theta)$ such that 
\begin{align*}
\mse(\hat{\nabla}^{\mathsf{grad}}_{\Tilde{\tau},N,\theta_i})
\le 
\left(\nabla_{\theta_i}\E_{A\sim \pi_\theta}(G(A))\right)^2 + {\epsilon}
\le 
\mse(\hat{\nabla}^{\mathsf{RF}}_{N,\theta_i}).
\end{align*}



 

Additionally, for any $N$, $\theta_i = \frac{1}{k}$, and $k \to \infty$, we have the following
\begin{align*}
\mse(\hat{\nabla}^{\mathsf{RF}}_{N,\theta_i}) & = \Omega(k),
 \mse(\hat{\nabla}^{\mathsf{grad}}_{\tilde{\tau},N,\theta_i})
 \le 
\left(\nabla_{\theta_i} \E_{A\sim \pi_\theta}(G(A))\right)^2.
\end{align*}

The same statement holds for $\theta_i =  1-\frac{1}{k}$ as well. This implies that the mse of $\hat{\nabla}^{\mathsf{RF}}_{N}$ is unbounded, while the mse of gradient estimator is bounded.
\label{thm:multi}
\end{theorem}





% If we assume 
% GG is sufficiently smooth: i.e., suppose
% G'(x) \le \bar{G}G'(x) \le \bar{G} for all xx, we can obtain the same results  while
% changing~\eqref{eqn:eq-def-autodiff}\eqref{eqn:eq-def-autodiff} to~\eqref{eqn:eq-def-autodiff-inside}\eqref{eqn:eq-def-autodiff-inside} carrying out similar analysis.




%In this Section, we demonstrate the bias-variance trade-off between the \textsf{REINFORCE} and pathwise gradient through its impact on the algorithmic performance and by also evaluating the mean-squared error and cosine similarity between the gradient estimators from each approach to the true gradient.


\subsection{Empirical comparison of gradient estimators}

\label{sec:empirical_gradient_estimation_quality}

In  the preceding theoretical analysis, we demonstrate  that $\mathsf{Smoothed\text{-}Autodiff}$ gradients can outperform \textsf{REINFORCE} gradients and there exists a temperature for which the mean squared error (MSE) of the $\mathsf{Smoothed\text{-}Autodiff}$ estimator is lower than that of the \textsf{REINFORCE} estimator.  While it is theoretically challenging to precisely characterize the difference between the $\mathsf{Smoothed\text{-}Autodiff}$ and \textsf{REINFORCE} gradients for varying $N$ and temperature $\tau$, we provide an empirical analysis of the gradient estimation within our adaptive labeling framework to further assess this comparison. 


As discussed, we denote the true gradient by $\nabla_{\theta}H(\theta)$.  For any gradient estimator $\hat{\nabla}_N $ (either $\hat{\nabla}_{\tau,N}^{\mathsf{grad}}$ or $\hat{\nabla}_N^{\mathsf{RF}}$), we measure the following performance metric, the cosine similarity, with the true gradient, denoted by $\cosim(\hat{\nabla}_N)$:
\begin{align*}
\cosim(\hat{\nabla}_N) = \E[\cos(\hat{\nabla}_N, \nabla_{\theta}H(\theta))],
\end{align*}
% \begin{align}
%     mse(\hat{\nabla}_N) = \frac{1}{d}\E[\mathbf{1}^T(\hat{\nabla}_N-\nabla_{\theta}H(\theta))^2]
% \end{align}
where expectation is taken over the randomness of the $\hat{\nabla}_N$.
Cosine similarity measures the alignment of direction between $\hat{\nabla}_N$ and $\nabla_{\theta}H(\theta)$, capturing both the bias and variance of the gradient estimators. 
The \textsf{REINFORCE} estimator is unbiased but suffers from high variance. As a result 
$\E [\cos(\hat{\nabla}_N^{\mathsf{RF}}, \nabla_{\theta}H(\theta))]$ might be small, even though $\E[\hat{\nabla}_N^{\mathsf{RF}}]= \nabla_{\theta} H(\theta)$.  
For the  $\mathsf{Smoothed\text{-}Autodiff}$  gradient, the $\cosim$ metric $\E [\cos(\hat{\nabla}_{N,\tau}^{\mathsf{grad}}, \nabla_{\theta}H(\theta))]$ is influenced by two factors. First, the  bias in the $\mathsf{Smoothed\text{-}Autodiff}$ gradient,  $\E (\hat{\nabla}_{N,\tau}^{\mathsf{grad}}) - \E (\nabla_{\theta}H(\theta))$, increases as $\tau$ increases. 
Second, the variance of the $\mathsf{Smoothed\text{-}Autodiff}$ gradient, $\V(\hat{\nabla}_{N,\tau}^{\mathsf{grad}})$,  decreases as $\tau$ increases. Our objective is to determine whether a favorable trade-off between bias and variance can be achieved, resulting in higher $cosim$ values and improved gradient alignment for the $\mathsf{Smoothed\text{-}Autodiff}$ gradient.













\begin{figure}
\centering
\begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[height=4cm, width=5cm]
{figures/compare_grad_cos_tau_0.1.pdf}
\centering{\small{{$\tau=0.1$ }}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[height=4cm, width=5cm]{figures/compare_grad_cos_tau_0.5.pdf}
\centering{\small{ $\tau=0.5$}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4cm, width=5cm]{figures/compare_grad_cos_tau_10.pdf}
\centering{\small{$\tau=10$}}
\end{minipage}
\caption{Comparison of $\mathsf{Smoothed\text{-}Autodiff}$ and \textsf{REINFORCE} estimators under different temperature ($\tau$) hyperparameter for the $\mathsf{Smoothed\text{-}Autodiff}$ gradient - metric used is cosine similarity}
\label{fig:compare-grad-cos}
\end{figure}




 


In the below mentioned results, we compare the $\mathsf{Smoothed\text{-}Autodiff}$ gradients and  \textsf{REINFORCE} gradients in the setting discussed in 
Figures~\ref{fig:var-l2-sim} and~\ref{fig:mean-l2-sim}.  
To approximate the ground truth gradient $\nabla_{\theta}H(\theta)$, we use the unbiased \textsf{REINFORCE} gradient estimated over many samples (in our case, $10^6$ samples). We evaluate both  $\mathsf{Smoothed\text{-}Autodiff}$ and \textsf{REINFORCE} gradients for different values of $N$. 
To estimate the expected values of cosine similarity, each gradient estimator was computed $200$ times. Figure~\ref{fig:compare-grad-cos} compares the $\mathsf{Smoothed\text{-}Autodiff}$ estimator and the \textsf{REINFORCE} estimator for different $N$.
We observe that for different $N$, pathwise estimator outperforms the \textsf{REINFORCE} estimator and has higher cosine similarity.
Moreover, \textsf{REINFORCE} estimator takes almost 100-1000 times more samples to reach the same level of cosine similarity.

%https://github.com/namkoong-lab/adaptive_sampling/blob/main/src/notebooks/new_plots/Plot_1006.ipynb
%We present here the results for one $\theta$, and relegate results for other $\theta$'s and other settings to the Appendix.




 
%Similar results were observed for different settings and $\theta$'s (we refer the reader to the Appendix \ref{}). 
%One may wonder that why cosine smilarity is so low for the both estimators - this is because coisne similarity scales poorly with dimesnion of the vector, $\theta$ in our setting is of dimension $285$. When we take multiple gradient steps the grad's correct for itself - as far as cosine similarity is positive with high probability, however lower the cosine similarity, more the number of steps it takes to converge. In Fig \ref{}, we demonstrate that cosine similarity is comparatively higher in a setting where $\theta$ has low dimension $d=10$. 

%In Figure \ref{}, we demonstrate the bias-variance trade-off between the pathwise gradient estimator as we vary the temperature. we also observe a range of temperature where the  pathwise gradient estimator performs better than reinforce estimator.

%\textcolor{red}{@EVERYONE : Should we have the results for gradient estimators for different $\theta$ and settings in the Appendix and reference it here?}



\subsection{Impact of gradient estimation on algorithmic performance}
\label{sec:gradient_estimation_algorithmic_performance}

We now compare the performance of $\mathsf{Smoothed\text{-}Autodiff}$  and \textsf{REINFORCE} gradients in the downstream optimization task of adaptive labeling, examining the effect of varying the number of samples $N$ used in the gradient estimator. Our results show that gradient quality directly impacts downstream task performance. In Figures~\ref{fig:var-l2-sim-in-N} and~\ref{fig:mean-l2-sim-N}, we   replicate the setting from 
Figure~\ref{fig:var-l2-sim} and~\ref{fig:mean-l2-sim}  in Section~\ref{sec:experiment-plan-GP}. As illustrated, $\mathsf{Smoothed\text{-}Autodiff}$ gradients perform exceptionally well even with a single sample in the estimator, with only marginal gains as $N$ increases due to limited room of improvement. In contrast, \textsf{REINFORCE} performs poorly with one sample but gradually improves as the sample size increases.

 
%In Figure \ref{}, we show the bias-variance tradeoff in pathwise gradient as we vary the temperature $\tau$ manifesting in the performance on adaptive labeling task.




\begin{figure}[t]
\centering
\begin{minipage}[b]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth, height=6cm]{figures/original_scale/N_samples_var_square_loss.pdf}
\caption{Comparing the impact of $\mathsf{Smoothed\text{-}Autodiff}$ and \textsf{REINFORCE} gradient estimators (with different values of $N$) on the performance of  \ouralgo. (Synthetic data) Variance of mean squared loss evaluated through the posterior belief $\mu_t$ at each horizon $t$.}
\label{fig:var-l2-sim-in-N}
\end{minipage}
\hfill
\begin{minipage}[b]{0.49\textwidth}
\centering \includegraphics[width=\textwidth, height=6cm]{figures/original_scale/N_samples_l2_loss.pdf}
\caption{Comparing the impact of $\mathsf{Smoothed\text{-}Autodiff}$ and \textsf{REINFORCE} gradient estimators (with different values of $N$) on the performance of  \ouralgo. (Synthetic data) Error between MSE calculated based on collected data $\mc{D}^{0:T}$ vs. population oracle MSE over $\mc{D}_{\rm eval} \sim P_X$.}
\label{fig:mean-l2-sim-N}
\end{minipage}
\end{figure}



