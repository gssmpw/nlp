\section{Adaptive labeling as a Markov decision process} 
\label{sec:formulation}

We illustrate our formulation for model evaluation, and extend it to the ATE estimation setting at the end of the section. 
Our goal is to evaluate the performance of a prediction model $\model: \statdomain \to \mathcal \labeldomain$ over the input distribution $P_X$ that we expect to see during deployment.  Given inputs $X  \in \mc{X}$,   labels/outcomes are generated
 from some unknown function $f\opt$: $
      Y = f\opt(X) + \varepsilon$, where $\varepsilon$ is the noise.
      %~~~\mbox{where}~~\varepsilon \sim N(0, \sigma^2)
  % \begin{equation*}
  %     Y = f\opt(X) + \varepsilon~~~\mbox{where}~~\varepsilon \sim N(0, \sigma^2).
  % \end{equation*}
When ground truth outcomes are costly to obtain, previously collected labeled data $\mc{D}^0 := \{(X_i,Y_i)\}_{i \in \mc{I}}$ 
typically suffers selection bias and covers only a subset of the support of input distribution $P_X$ over which we aim to evaluate the model performance. 

Assuming we have a   pool of data $\xpool$, we design
 adaptive sampling algorithms that iteratively select
inputs in $\xpool$ to be labeled.
Since labeling inputs takes time in practice, we model
real-world instances by considering \emph{batched} settings. Our goal is to sequentially label batches of data to accurately estimate model performance over $P_X$ and therefore we assume we have access to a set of inputs $\xeval \sim P_X$. %We assume the modeler pays a fixed and equal cost for each label/outcome. 
%Our framework is general as we do not assume \xpoolâˆ¼PX\xpool \sim P_X.
We use the squared loss to illustrate our framework,
where our goal is to evaluate $\E_{X \sim P_X}[ (Y - \model(X))^2]$. Under the ``likelihood" function $p(y | f, x) = p_{\varepsilon}(y - f(x))$,  let $g(f)$ be the performance of the AI model $\model(\cdot)$ under the  data generating function $f$, which we refer to as our estimand of interest.
When we consider the mean squared loss,  $g(f)$ is given by 
\begin{align}
    g(f) \defeq \E_{X \sim P_X}\left[ \E_{Y \sim p(\cdot|f,X) } \Big[ (Y - \model(X))^2 \Big] \mid f \right]. \label{eqn:l2-g-f}
\end{align}
Our framework is general and can be extended to other settings. For example, a clinically useful  metric is \texttt{Recall}, defined as the fraction of individuals that the model $\model(\cdot)$ correctly labels as positive  among all the individuals who actually have the positive label 
\begin{align*}
    g(f) \defeq  \E_{X \sim P_X}\left[ \E_{Y \sim p(\cdot|f,X) } \Big[\indic{\model(X)>0}|Y=1\Big] \mid f\right].
\end{align*}
 
  Since the true function $f\opt$ is unknown, we  model it from a Bayesian perspective by formulating a posterior given the available supervised data. We refer to uncertainty over the data generating function $f$ as \emph{epistemic} uncertainty---since we can resolve it with more data---and that over
 the measurement noise $\varepsilon$ as \emph{aleatoric} uncertainty. 
Assuming independence given features $X$, we model the  likelihood of the data via the product 
$p({Y}_{1:m}|f, {X}_{1:m}) = \prod_{i=1}^m p(Y_i|f,X_i)$.
 Our prior belief  $\mu$ over functions $f$   reflects our uncertainty about how
labels are generated given features. 
To adaptively label inputs from $\mc{X}_{\rm pool}$, we assume access to an uncertainty quantification (UQ) method that provides posterior beliefs $\mu(f \mid \mc{D})$ given
any supervised data $\mc{D}:= \{(X_i,Y_i)\}_{i \in \mc{I}}$. As we detail  in Section~\ref{sec:uq}, our framework can leverage both classical 
Bayesian models like Gaussian processes and recent advancements in deep learning-based UQ  methods.

As new batches are labeled, we update our posterior beliefs about $f$ over time, which we view as ``state transitions'' of a dynamical system.
Recalling the Markov decision process depicted in Figure~\ref{fig:overview}, we sequentially label a batch of inputs from $\mc{X}_{\rm pool}$ (actions), which lead to state transitions (posterior updates).
Specifically, our initial state is given by $\mu_0(\cdot) = \mu(\cdot \mid \mc{D}^0)$, where $\mc{D}^0$ represents the initial labeled dataset.
At each period $t$, we label a batch of $K_t$ inputs $\mc{X}^{t+1} \subset \mc{X}_{\rm pool}$ resulting in labeled data $ \mc{D}^{t+1} = (\mc{X}^{t+1}, \datay^{t+1})$. After acquiring the labels at each step $t$, we update the posterior state to $\mu_{t+1}(\cdot) = \mu_t(\cdot \mid \mc{D}^{t+1})$. Modeling practical instances, we consider a small horizon problem with limited adaptivity $T$. Formulating an MDP over posterior states has long conceptual roots, dating back to the Gittin's index for multi-armed bandits~\citep{Gittins79}.

 We denote by $\pi_t$ the adaptive labeling policy at period $t$. We account for randomized policies $\datax^{t+1} \sim \pi_t(\mu_t)$ with a flexible batch size $|\datax^{t+1}| = \batchsize_t$.   
We assume $\pi_t$ is $\mc{F}_t-$measurable for all $t < T$, where $\mc{F}_t$ is the filtration generated by the observations up to the end of step $t$.
 Observe that $\mu_{t+1}$ contains randomness in the policy $\pi_t$ as well as randomness in $\datay^{t+1} \mid (\datax^{t+1},\mu_t)$. Letting $\pi = \set{\pi_0,....,\pi_{T-1}}$,  we minimize the uncertainty over $g(f)$
 at the end of data collection
\begin{align}
H(\pi) \defeq \E_{\mc{D}^{1:T} \sim \pi} \left[G(\mu_{T}) \right]  \defeq  \E_{\mc{D}^{1:T} \sim \pi} \left[G(\mu(\cdot \mid \mc{D}^{0:T})) \right]
%\defeq \E_{\mc{D}^{1:T} \sim \pi} \left[ \V_{f \sim \mu_{T}}  g(f)  \right]
     = \E_{\mc{D}^{1:T} \sim \pi} \left[ \V_{f \sim \mu(\cdot \mid \mc{D}^{0:T})}  g(f)  \right],
     \label{eqn:general-obj}
\end{align}   
where $G(\mu_T) = \V_{f \sim \mu_T}  g(f)$.
In the above objective~\eqref{eqn:general-obj}, we assume
that the modeler pays a fixed and equal cost for each outcome. 
Our framework can also seamlessly accommodate variable labeling cost. Specifically, we can define a cost function $c(\cdot)$ 
 applied on the selected subsets 
 and update the objective~\eqref{eqn:general-obj} accordingly to include the term  $\lambda c(\mc{D}^{1:T})$,
 where $\lambda$
 is the penalty factor that controls the trade-off between minimizing variance and cost of acquiring samples.


Our framework can be easily extended to causal estimation problems.  Consider a feature vector ${X}$ and suppose we have two treatment arms $Z \in \{0,1\}$. Our objective is to evaluate the average treatment effect over the population distribution $P_X$.  Given feature vector $X$, and treatment $Z$,  outcomes are generated from an unknown function $f\opt$: 
$Y = f\opt(X,Z) + \varepsilon.$
%~~~\mbox{where}~~\varepsilon \sim N(0, \sigma^2)
  % \begin{equation*}
  %     Y = f\opt(X) + \varepsilon~~~\mbox{where}~~\varepsilon \sim N(0, \sigma^2).
  % \end{equation*}
The available data is denoted by $\mc{D}^0 := \{X_i,Y_i,Z_i\}_{i \in \mc{I}}$ and given a pool of candidates 
$\xpool$, we want an
 adaptive sampling algorithms that iteratively select
candidates in $\xpool$ to be assigned a random treatment so that we can estimate average treatment effect efficiently. Under the ``likelihood" function $p(y | f, x, z) = p_{\varepsilon}(y - f(x,z))$,  let $g(f)$ represent  the average treatment effect, which is our estimand of interest. Formally, this is expressed as:
\begin{align}
g(f) \defeq \E_{X \sim P_X} \left[\E_{Y_1 \sim p(\cdot|f,X,Z=1) , Y_0 \sim p(\cdot|f,X,Z=0)} \left[Y_1 -  Y_0 \right]\mid f \right]. \label{eqn:ate-g-f}
\end{align}




 %Again the true function $f\opt$ is unknown and we model it from a Bayesian perspective by formulating a posterior  given available data.
 Our prior belief  $\mu$ over functions $f$, now 
 reflects our uncertainty about how
outcomes are generated given features and treatments. 
  We sequentially observe outcome of a batch of inputs from $\mc{X}_{\rm pool}$ (actions), and treatments assigned to this batch. We assume that selected batch of inputs $\mc{X}^t$ is randomly assigned treatments $\mc{Z}^t$ with each $Z\sim p_Z$. We summarize our formulation in Figure~\ref{fig:MDP_framework_flowchart}.
%Specifically, our initial state is given by $\mu_0(\cdot) = \mu(\cdot \mid \mc{D}^0)$ and at each period $t$, we get outcomes for a batch of $K$ candidates $\mc{X}^{t+1} \subset \mc{X}_{\rm pool}$, with randomly assigned treatments $\mc{Z}^{t+1}$ (with each $Z\sim p_Z$) and get the data $ \mc{D}^{t+1} = (\mc{X}^{t+1} \times \datay^{t+1} \times \mc{Z}^{t+1} )$. After acquiring the data at each step $t$ we update posterior state to $\mu_{t+1}(\cdot) = \mu_t(\cdot \mid \mc{D}^{t+1})$. Modeling practical instances, we consider a small horizon problem with limited adaptivity $T$.  We denote by $\pi_t$ the adaptive labeling policy at period $t$. We account for randomized policies $\datax^{t+1} \sim \pi_t(\mu_t)$ with a flexible batch size $|\datax^{t+1}| = \batchsize_t$.   
Again, we assume $\mu_t$ is $\mc{F}_t-$measurable for all $t < T$, where $\mc{F}_t$ is the filtration generated by the observations up to the end of step $t$.
 Observe that $\mu_{t+1}$ contains randomness in the policy $\pi_t$, randomness in treatment assignment $\mc{Z}^{t+1}$ and randomness in $\datay^{t+1} \mid (\datax^{t+1}, \mc{Z}^{t+1},\mu_t)$. Letting $\pi = \set{\pi_0,....,\pi_{T-1}}$,  we minimize the uncertainty over $g(f)$
 at the end of data collection:
\begin{align}
\E_{\mc{D}^{1:T} \sim \pi} \left[G(\mu_{T}) \right] \defeq
\E_{\mc{D}^{1:T} \sim \pi} \left[ \V_{f \sim \mu_{T}}  g(f)  \right]
= \E_{\mc{D}^{1:T} \sim \pi} \left[ \V_{f \sim \mu(\cdot \mid \mc{D}^{0:T})}  g(f)  \right].
\label{eqn:general-ate-obj}
\end{align}  


 

\begin{figure}[ht]
\centering
\begin{tikzpicture}
[
roundnode/.style={circle, draw=black!60, very thick, minimum size=10mm},
squarednode/.style={rectangle, draw=black!60, very thick, minimum size=10mm, align =center,text width = 26mm},
]
%Nodes
\node[roundnode]      (maintopic)                              {$\mu$};
\node[roundnode]        (circle1)       [right=20mm of maintopic] {$\mu_0$};
\node[roundnode]      (circle2)       [right=5mm of circle1] {$\mu_t$};
\node[roundnode]        (circle3)       [right=20mm of circle2] {$\mu_{t+1}$};
\node[roundnode]        (circle4)       [right=5mm of circle3] {$\mu_T$};
\node[squarednode]        (circle5)       [right=5mm of circle4] {Reward/Cost $\E \left[ \V_{\mu_T} (g(f))\right]$};


%Lines
\draw[thick, ->, >=stealth] (maintopic.east) -- node[anchor=south] {$(\mathcal{X}^0,\mathcal{Y}^0,\mathcal{Z}^0)$} (circle1.west);
\draw[thick, ->, dashed] (circle1.east) --  (circle2.west);
\draw[thick, ->] (circle2.east)  -- node[above, align =center, text width = 18mm] { Query $(\mathcal{X}^t,\mathcal{Y}^t,\mathcal{Z}^t)$} (circle3.west);
\draw[thick, ->,dashed] (circle3.east) --  (circle4.west);
\draw[thick, ->] (circle4.east) --  (circle5.west);


\end{tikzpicture}
\caption{MDP framework for adaptive labeling to efficiently estimate the average treatment effect (ATE).}
\label{fig:MDP_framework_flowchart}
\end{figure}
 









\begin{comment}
\subsection{Broader applicability of the framework to other problem settings} \label{sec:broad-framework-accuracy}
 

Although  we describe our setting in a healthcare setting with the objective  to estimate the recall of a trained AI model $\model(\cdot)$, the framework caters to many other problem settings. The extension to the evaluation of model based on accuracy (in regression setting) is straightforward, we simply replace the definition of recall $g(f)$ in~\eqref{eqn:l2-g-f} with
\begin{align*}
    g(f) = \E_{\substack{ y \sim p(y|f,x) \\  \forall x \in \mathcal X}} \big( \E_{{\textbf x} \sim p_x} [y-\model(x)]^2 \big).
\end{align*}


\textcolor{red}{To discuss if we need to have it here}
We can also extend this setting to the efficient estimation of the ATE as well. We describe these in detail below:

\begin{itemize}
    
    \item Estimating accuracy:  \[g(f) = \E_{\substack{ y \sim p(y|f,x) \\  \forall x \in \mathcal X}} \big( \E_{{\textbf x} \sim p_x} [y-\model(x)]^2 \big)\]
%    \item Estimating ATE with known control arm: 
%\[g(f) = \E_{\substack{ y \sim p(y|f,x) \\  \forall x \in \mathcal X}} \big( \E_{{\textbf x} \sim p_x} [y-\model(x)] \big)\]
\item Estimating ATE  (with minor modifications - broad structure remains similar) : 


Consider feature vector ${\mathbf x} \in \mathcal X $  distributed as ${\mathbf x}  \sim p_{\mathbf x}$, treatment $z \in {\mathcal Z} = \{0,1\}$, and a class of random functions $f: {\mathcal X} \times {\mathcal Z} \to {\mathcal Y}$, which determines the likelihood $p(y_i|f,{\mathbf x_i},z_i)$. Note that $f$ is random and reflects our uncertainty about how
labels are generated given features and the treatment. Additionally, the joint likelihood is determined as follows,  

\[p(Y|f,X,Z) = \prod_{i} p(y_i|f,{\mathbf x_i}, z_i) \]

Assuming the prior over functions $f$ to be $\mu$, therefore we have 
\[p(Y|X,Z) = \int \prod_{i} p(y_i|f,{\mathbf x_i},z_i) d\mu(f) \]


Also, assuming that under the  true data generating function $f$ (if known precisely - which we don't), the estimand of interest is

\[ \E_{{\textbf x} \sim p_x}  \left( \E_{\substack{ y \sim p(y|x,f,z=1) }} y - \E_{\substack{ y \sim p(y|x,f,z=0) }} y \right) \]


Throughout the paper we assume the above data generating process.  Now, suppose we have some labeled  data $(\datax^0,\datay^0,Z^0) =({\mathbf x}_{1:m}^0,y_{1:m}^0, z_{1:m}^0)$. 
    We run a experiment, in which we want to query the labels (in batches), so as to minimize the uncertainty of the estimand of interest. Suppose, the horizon of the experiment is $T$. Now, given prior $\mu$ and labeled data $\datax^0,\datay^0,Z^0$, in the beginning of our experiment the posterior state is $\mu_0$.

 At each step $j$ ($j \geq 1$), we query labels for a batch (with size $k$) of unlabeled data $(\datax^j,Z^j) \subset \mathcal X \times \mathcal Z$  and get labels $\datay^j$. After acquiring the labels at each step $j$ we update posterior state to $\mu_{j+1}$, informed by $\mu_j$ and $(\datax^j,\datay^j,Z^j)$. 
 
 Let the policy at step $j$ be $\pi_j$ (potentially random), which gives $\datax^{j+1},Z^{j+1} \sim \pi_j(\mu_j)$.  Observe that $\mu_{j+1}$ is random because of the randomness of the policy $\pi_j$ and $\datay^{j+1}|\{\datax^{j+1},Z^{j+1},\mu_j\}$ (\textcolor{red}{can this be written in a better way?}). Let, $\pi = \{\pi_0,....,\pi_{T-1}\}$. Therefore, our objective is to

 
\[ \min_{\pi} \E \left[ {\mathbf {Var}}_{f \sim \mu_T} \left( \E_{{\textbf x} \sim p_x}  \left( \E_{\substack{ y \sim p(y|x,f,z=1) }} y - \E_{\substack{ y \sim p(y|x,f,z=0) }} y \right) \right) \right]\]

where, $\mu_T$ depends on $\{(\datax^i,\datay^i,Z^i)\}_{i=0}^T$ and outer expectation is over both $\pi$ and  $\datay^{j+1}|\{\datax^{j+1}, Z^{j+1},\mu_j\}$ for all $j \in [0,T-1]$.


%Constraining the action space is straightforward - by first choosing set of x's using k-subset and then assigning treatment with learnable probability parameters $w_1,...,w_n$.

\end{itemize}

 \[ g(f) = \E_{\substack{ y \sim p(y|f,x) \\  \forall x \in \mathcal X}}\E_{{\textbf x} \sim p_x} g(y,{\textbf x}) \approx \E_{\substack{ y \sim p(y|f,x) \\  \forall x \in  \datax^u}} \left( \frac{1}{n}\sum_{i=1}^n \tilde{g}(y,{\textbf x}_i^u) \right)\]




Notation borrowed from a combination of the following papers 
~\citep{LeeYuNaFoLe23, KatoOgKoIn24, FongHoWa24}

%  
\end{comment}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
