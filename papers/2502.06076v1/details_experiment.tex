\section{Experimental details (Section \ref{sec:experiment})} \label{sec:details-experiments}

In this section, we provide detailed information about the experiments discussed in Section~\ref{sec:experiment}. 

\subsection{Planning with Gaussian Processes - synthetic data experiments}

As mentioned earlier, we generate our data using a  Gaussian Process ({GP}). Specifically, we use a {GP} with an RBF kernel: $f_i \sim \mathcal{GP}(m,\mc{K}) $, where $m(X)=0$ and $\mc{K}(X,X') = \sigma_f^2 \exp\left(-\frac{||X-X'||_2^2}{2\loss^2}\right)$, further Gaussian   noise $N(0,\sigma^2)$ is added to the outputs. We set $\loss = 1$, $\sigma_f^2 = 0.69$, and $\sigma^2 = 0.01$.
Recall that the marginal distribution $P_X$ consists of $51$ \textit{non-overlapping} clusters. To achieve this, we create a polyadic sampler, which first samples $51$ anchor points spaced at linearly increasing distances from the center to avoid overlap. These anchor points serve as our cluster centers. We sample the points around these anchor points by adding a small Gaussian noise $N(0,0.25)$. The training points are drawn from a single cluster, while the pool and evaluation points are drawn from all the $51$ clusters. The setup includes $100$ initial labeled data points, $500$ pool points and $285$ evaluation points used to estimate the objective. 

\noindent\textbf{\ouralgo~ training and hyperparameters:} As mentioned earlier, the underlying uncertainty quantification (UQ) module for these experiments is the {GP}. Although our algorithm does not know the true data generating function, but it has access to the {GP} hyperparameters. Therefore we use a $\mathcal {GP}$ with an RBF kernel: $f_i \sim \mathcal{GP}(m,\mc{K}) $, where $m(X)=0$ and $\mc{K}(X,X') = \sigma_f^2 \exp\left(-\frac{||X-X'||_2^2}{2\loss^2} \right)$, with 
Gaussian   noise $N(0,\sigma^2)$ added to the outputs. We set $\loss = 1$, $\sigma_f^2 = 0.69$ and $\sigma^2 = 0.01$. For soft $K$-subset sampling  (see Algorithm~\ref{alg:k-subset}),  we set $\tau$ to $0.1$. Further for evaluating the objective function $\V (g(f))$ - we take $100$ samples of $f(\xeval)$ from the posterior state $\mu_+^{a(\theta)}$. 
For policy optimization in each horizon, we use the Adam optimizer with a learning rate of $0.1$ to perform policy gradient steps over $100$ epochs. The results presented are averaged over $10$ different training seeds.


\noindent\textbf{Policy gradient (\rein) training and hyperparameters:} As mentioned earlier, all the algorithms have access to the true to {GP} hyperparameters. For evaluating the objective $\V (g(f))$ under a given action,  we take $100$ samples of $f(\xeval)$.  To optimize the policy in each horizon, we use the Adam optimizer with a learning rate of $0.1$ to perform policy gradient updates over $100$ epochs. The results presented are averaged over $10$ different training seeds.


\subsection{Planning with Gaussian Processes - real data (eICU) experiments}


The eICU  dataset is a healthcare dataset that contains data from various critical care units across the United States. To create a supervised learning setup from this dataset, we first extracted the $10$ most important features using a \texttt{Random Forest} classifier (number of trees $=$ $100$, criterion $=$ ``gini'').  The outcome variable was in-hospital mortality. Some examples of the extracted features include: Hospital length of stay, Number of days on the ventilator and the Last recorded temperature on Day 1 of ICU admission. We transformed the classification outcome variable (in-hospital mortality) into a regression task using the probability of in-hospital mortality predicted by the classifier.
To adapt the extracted data to our setting, we introduced selection bias in the data. We generated 51 clusters through the standard $k-$means algorithm. Our initial labeled data comes from only $1$ cluster, while the pool and evaluation data comes from all the $51$ clusters. Our dataset consists $100$ initial labeled data points, $500$ pool points, and $285$ evaluation points used to estimate the objective. 


We then fitted a  {GP}  with an RBF kernel to above data using  {GP}-regression. The resulting {GP} hyperparameters were: $m(x)=0.255$, $\loss = 0.50, \sigma_f^2 = 1.0, \sigma^2 =0.0001$. This $\mathcal{GP}$ model serves as our uncertainty quantification (UQ) module.


\noindent\textbf{\ouralgo  ~ training and hyperparameters:} For soft $K$-subset sampling  (see Algorithm~\ref{alg:k-subset}),  we set $\tau$ to $0.1$. Further to evaluate the objective function $\V (g(f))$, we take 100 samples of $f(\xeval)$ from the posterior state $\mu_+^{a(\theta)}$.
For policy optimization in each horizon, we use the Adam optimizer with a learning rate of $0.1$ to perform policy gradient steps over $1000$ epochs. The results presented are averaged over $10$ different training seeds.


\noindent\textbf{Policy gradient (\rein) training and hyperparameters:} To evaluate the objective $\V (g(f))$ under an action, we take $100$ samples of $f(\xeval)$.  For optimizing the policy in each horizon, we use an Adam optimizer with learning rate of $0.1$,  performing policy gradient updates over $1000$ epochs. The results presented are averaged over $10$ different training seeds.



\subsection{Planning with \ensembleplus experiments}

  Once again, we use {GP}s as our data generating process. Specifically, we use a GP with an RBF kernel: $f_i \sim \mathcal{GP}(m,\mc{K}) $, where $m(X)=0$ and $\mc{K}(X,X') = \sigma_f^2 \exp\left(-\frac{||X-X'||_2^2}{2\loss^2} \right)$. Additionally, Gaussian noise   $N(0,\sigma^2)$ is added to the outputs. We set $\loss = 1$, $\sigma_f^2 = 0.69$, and $\sigma^2 = 0.01$.
The marginal distribution $P_X$ consists of $4$ \textit{non-overlapping} clusters. We follow the same process as in the Gaussian process experiments to form the cluster centers and sample points around them. While the training points are drawn from a single cluster, both the pool and evaluation points are drawn from all the $4$ clusters. Our setup includes $20$ initial labeled data points, $10$ pool points, and $252$ evaluation points used to evaluate the objective. 

%\footnotesize{Experimental details: We use Gaussian Processes as our data generating process, GP parameters are the same as in Section D.3.  The task is to select a batch of 2 points along one horizon.The marginal distribution $p_X$ has 4 \textit{non-overlapping} clusters. Initial data comes from one cluster, while pool and evaluation points comes from all the clusters. We have $20$ initial labeled data points, $10$ pool points, and $252$ evaluation points.  Training procedures are similar to the one in Section D.3

\noindent\textbf{\ensembleplus Architecture:} We use an ensemble of 10 models. Each model being a 2-hidden layer MLP with 50 units per hidden layer. In addition, each models includes an additive prior. The additive prior function for each model is fixed to be a 2-hidden layer MLP with 50 units in each hidden layer.  
 Specifically, the $m$-th model for $1\leq m \le 10$ of the \ensembleplus takes the form
\begin{align*}
    f_{\eta_m}(X) = g_{\eta_m}(X) + \alpha p_m(X),
\end{align*}
where $g_{\eta_m}$  is the trainable part of the network while $\alpha p_m(\cdot)$ is the additive prior function. Here $\alpha$ controls our prior belief about the uncertainty ---higher the $\alpha$, the greater the uncertainty. The prior functions $p_m(\cdot)$ differ across models $m$ due to different initializations. In our setup, we set $\alpha=100$ to reflect a high level of uncertainty in the prior beliefs.




\noindent\textbf{\ensembleplus training:} For a given dataset ${\mathcal D} =(X_i,Y_i)_{i=1}^n$, we train the $m$-th model so as to minimize the following loss function 
\begin{align*}
    \loss(\eta_m, \mathcal D) =  \frac{1}{n}\sum_{i=1}^n (g_{\eta_m}(X_i) + \alpha p_m(X_i)-Y_i)^2 + \lambda \norm{\eta_m}_2^2.
\end{align*}
We tune $\lambda$ to $0.1$ and use the Adam optimizer with a tuned learning rate of $0.1$. Each model is trained for 50 iterations.
%Although in standard \ensembleplus, each model is trained on a different bootstrapped subset from the dataset ${\mathcal D}$, in our case we train all the models  within \ensembleplus on the entire dataset available.



 
\noindent\textbf{\ouralgo  ~ training and hyperparameters:} Recall that in soft $K$-subset sampling there is a hyperparameter $\tau$  (see Algorithm~\ref{alg:k-subset}), which we set to $0.1$. To differentiate through the argmin operation, we employ the differentiable optimizer from the $\mathsf{torchopt}$ package, specifically the MetAdam Optimizer with a learning rate of $0.1$. For optimizing the sampling policy, we use the Adam optimizer with a learning rate of $0.05$, performing policy gradient updates over $500$ epochs. 




% \begin{table}[H]
% \vspace{-10pt}
% \caption{Performance under \ensembleplus as UQ module}
%     \centering
%     \begin{tabular}{|m{3cm}|m{2.5cm}|m{2cm}|} 
%     \hline
%       Algorithm   & Variance of $\loss_2$ loss estimate & Error of $\loss_2$ loss estimate  \\ \hline Random Sampling 
%          & $1710.9 \pm 1352.1$ & $8.67\pm6.62$ 
%       \\ \hline \ouralgo & $1.30 \pm 0.68$ & $0.91\pm0.25$ \\ \hline
%     \end{tabular}
%     \label{tab:UQ_ensemble_old}
%     %\vspace{-10pt}
% \end{table}

%\colorbox{pink}{${\mathbb E} [\V_{\mu_0} (g(f))]$}


%\colorbox{pink}{${\mathbb E} [\V_{\mu_2} (g(f))]$}



%\colorbox{pink}{${\mathbb E} [\V_{\mu_t} (g(f))]$}


%\definecolor{customgray}{gray}{0.85} % Lighter gray

%\colorbox{customgray}{Pick $\{{\mathcal X^1}:\subseteq \xpoolj{1} \}$}

%\colorbox{customgray}{Pick $\{{\mathcal X^2}:\subseteq \xpoolj{2} \}$}

%\colorbox{customgray}{Pick $\{{\mathcal X^t}:\subseteq \xpoolj{t} \}$}