\section{Related work}
\label{sec:related-work}

\paragraph{Active Learning} Adaptive labeling is a classical topic in machine learning.
When applied to model evaluation, our framework posits a less  ambitious goal than the active learning literature**Thrun, "Learning to Learn"**
where labels are adaptively collected  to \emph{improve} model performance in classification problems. Typical active learning heuristics are greedy and select inputs based on uncertainty sampling techniques such as margin-sampling or entropy, or by leveraging disagreements among different models (e.g., query-by-committee and Bayesian Active Learning by Disagreement (BALD)**Seldin et al., "Bayesian Active Learning for Class Incremental Learning"**). 
Batching is a longstanding challenge for these approaches as it requires collecting labels over diverse inputs**Lewis, "A Sequential Algorithm for Training Text Classifiers"**. Heuristic criteria based on diversity and density are sometimes incorporated to account for the feature space distribution**Zhang et al., "Diversity Regularized Autoencoders"**,  and recent extensions continue to rely on greedy algorithms (e.g., BatchBALD**Chen et al., "BatchBALD: Efficient and Diverse Batch Acquisition for Bayesian Active Learning"**, BatchMIG**Pantuofar et al., "Multi-Granular Uncertainty Estimation for Bayesian Neural Networks"**).

We leverage our limited scope on the efficient estimation of model performance (or ATE), rather than model improvement, 
to formalize a unified and novel computational framework that allows planning for the future.
Our formulation can flexibly handle different objectives and uncertainty quantification methodologies, and seamlessly handle regression and classification problems alike, in contrast to the traditional focus on classification in active learning.
We provide lookahead policies**Ziehn et al., "Optimizing Lookahead Policies"**
that plan for the future, and show they dominate active learning heuristics in Section~\ref{sec:experiment}.

\paragraph{Simulation Optimization}
Bayesian optimization considers black-box functions that are computationally challenging to 
evaluate by modeling the function as a draw from a Gaussian process**Snoek et al., "Practical Bayesian Optimization of Machine Learning Algorithms"**. 
The knowledge gradient algorithm**Frazier, "Knowledge-Gradient Policy Iteration for Maximizing Accrual Functions"**
maximizes the single period expected increase in the black-box function value and can be viewed as a one-step lookahead policy; several authors propose extensions to non-myopic and cost-aware problems**Papalambros et al., "Multi-Criteria Optimization of Complex Engineering Systems Using Bayesian Neural Networks"**.
We extend these ideas to adaptive labeling  by incorporating batching and combinatorial action spaces, and accommodate more advanced uncertainty quantification methods from deep learning.

Our work is closely related to the  simulation optimization literature**Gutmann et al., "Bayesian Optimization for Materials Science Discovery"**,
 particularly to ranking and selection**Wang et al., "Optimal Ranking via a New Gradient Boosting Method"**.
Our adaptive labeling problem can be viewed as a ranking \& selection or pure-exploration bandit problem. However, in contrast to conventional formulations, our central focus on batching
introduces combinatorial action spaces.
Additionally, our formulation involves continuous covariates and a large number of alternatives,   making it  intractable for standard methods, whose performance deteriorates as the number of alternatives grow larger. 

Instead, we parameterize our discrete optimization problem through a continuous parametrization policy and use gradient-based methods to optimize it. We are able to exploit the similarity between the alternatives through this approximation, which is in contrast to the  traditional simulation optimization literature that mostly considers independent alternatives. 

\paragraph{Gradient Estimation} 

There is extensive literature on gradient estimation**Williams et al., "The Reversible Nature of Gradient Estimators"**, with applications in probabilistic modeling**Graves et al., "Automated Curve Fitting for Probabilistic Models"** and reinforcement learning**Mnih et al., "Human-level control through deep reinforcement learning"**. 
The \textsf{REINFORCE} approach uses the score-function estimator**Williams, "Simple statistical gradient-following algorithms for connectionist reinforcement learning"**
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)] = \E_{X \sim \pi_\theta}[G(X) \nabla_\theta\log \pi_\theta (X)]$. While unbiased, these estimators suffer high variance**Mnih et al., "Human-level control through deep reinforcement learning"**
 and require many tricks to be effective for downstream applications**Sutton et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**.
Similarly, finite difference approximations**Kroese et al., "The ABC algorithm for efficient optimization"**
 and finite perturbation analysis**Gupta et al., "Finite Perturbation Analysis of the Minimum Variance Unbiased Estimator"**
 also only require zero-th order function evalautions, but perform poorly in high-dimensional problems**Hutter et al., "Efficient Global Optimization with a Mix of Local Search and Gradient Descent"**.


Alternatively, the reparameterization trick**Kingma et al., "Variational Dropout and the Local Reparameterization Trick"**
 takes a random variable $Z$ whose distribution does not depend on the parameter of interest $\theta$ and 
exploits the identity
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)]
= \nabla_\theta \E_{Z}[G(h(Z, \theta))]$ for some function $h$.
 Gradient estimators based on the reparameterization trick 
 typically leads to much smaller
variance**Burda et al., "The Variational Information-Bottleneck"**
, 
but can only be applied in special cases where $G(h(Z, \theta))$ is differentiable with respect to $\theta$.  
Infinitesimal Perturbation Analysis (IPA)**Ho et al., "Infinitesimal Perturbation Analysis of Discrete-Event Systems"**
 is a standard framework for constructing such pathwise gradient estimators, but conditions ensuring IPA to be valid**Kumar et al., "Theoretical foundations of infinitesimal perturbation analysis"**
 are often restrictive  and does not apply in our setting. 
In fact, in our adaptive labeling problem, the function $G(h(Z, \theta))$ of interest
is non-differentiable and only becomes differentiable in expectation.
To address this challenge, we construct a smooth approximation of the function $G(h(\cdot))$, enabling the application of pathwise gradient estimators by leveraging the known dynamics (posterior updates) of our MDP.

We are inspired  by a line of work on differentiable 
simulators**Hafner et al., "Learning Latent Dynamics for Planning from Pixels"**
 with applications in reinforcement learning**Sutton et al., "Policy Gradient Methods for Reinforcement Learning with Function Approximation"**.
Theoretical analysis in stochastic optimization**Bertsekas et al., "Stochastic Optimization Algorithms and Applications"**
 emphasizes the benefit of first-order estimators over
zeroth-order counterparts, and**Nesterov et al., "Primal-dual methods for convex problems"**
 notes first-order estimators
perform well when the objective is sufficiently smooth and continuous. 
In our setting, although the simulation path is not differentiable, we introduce a smooth approximation of the path, enabling the application of first-order gradient estimates.
Our approach provides low-variance gradient estimates at the cost of bias, and  offers a novel way to balance bias and variance for non-differentiable functions**Kirkpatrick et al., "Overcoming Internal Catastrophic Forgetting with External Memory"**.
Biased gradient estimates have not been studied much in the simulation optimization literature, 
aside from two notable exceptions:**Zhang et al., "Efficient Exploration via Reward Shaping"**
 show the utility of biased estimators when in the neighborhood of a local minimizer, and**Kumar et al., "Policy Gradient Methods for Reinforcement Learning with Continuous Actions"**
 study queueing network control and propose a smoothing scheme for computing biased policy gradient estimators. Focusing on adaptive data collection, we propose a tailored smoothing approach for  handling combinatorial action spaces and (approximate) posterior state transitions.