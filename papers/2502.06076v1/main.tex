% -*- Mode: Latex; -*-
% ------------------------------------------------------------------------
% Default style
% ------------------------------------------------------------------------

% If arxiv-style, comment below line

%\def\useorstyle{1}
\ifdefined\useorstyle
\documentclass{article}
\pdfoutput=1

%\documentclass[opre,blindrev]{informs3} % current default for manuscript submission
\usepackage[numbers]{natbib} 
%\usepackage{macros/neurips_2023}
\usepackage{macros/nips_2024}

\usepackage{macros/statistics-macros}
\usepackage{macros/packages}
\usepackage{macros/editing-macros}
\usepackage{macros/formatting}
\usepackage{macros/statistics-macros}
\usepackage{caption}
%\DoubleSpacedXI % Made default 4/4/2014 at request
%%\OneAndAHalfSpacedXI % current default line spacing
%%\OneAndAHalfSpacedXII 
%%\DoubleSpacedXII

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentclass. For example
%\documentclass[dvips,opre]{informs3}      % if dvips is used 
%\documentclass[dvipsone,opre]{informs3}   % if dvipsone is used, etc. 

%%% OPRE uses endnotes
\usepackage{endnotes}
\let\footnote=\endnote
\let\enotesize=\normalsize
\def\notesname{Endnotes}%
\def\makeenmark{\hbox to1.275em{\theenmark.\enskip\hss}}
\def\enoteformat{\rightskip0pt\leftskip0pt\parindent=1.275em
  \leavevmode\llap{\makeenmark}}
\usepackage{xspace}
% Private macros here (check that there is no clash with the style)

% Natbib setup for author-year style
% \usepackage{natbib}
%  \bibpunct[, ]{(}{)}{,}{a}{}{,}%
%  \def\bibfont{\small}%
%  \def\bibsep{\smallskipamount}%
%  \def\bibhang{24pt}%
%  \def\newblock{\ }%
%  \def\BIBand{and}%

%% Setup of theorem styles. Outcomment only one. 
%% Preferred default is the first option.
%\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)
%\ECRepeatTheorems

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
%\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...

% In the reviewing and copyediting stage enter the manuscript number.
%\MANUSCRIPTNO{} % When the article is logged in and DOI assigned to it,
                 %   this manuscript number is no longer necessary
 

% \usepackage{fullpage}
\usepackage{hyperref}

% \usepackage{setspace}
\usepackage{pgfplotstable}
\usepackage{graphicx}
% \usepackage{subcaption}
\usepackage{float} 
\usepackage{tabularx}
\usepackage{overpic}
\usepackage{tikz}
\usepackage{rotating}
\usepackage{psfrag}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{shapes, arrows, positioning}  


%%%%%%%%%%%%%%%%
%\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{Bundling Information Goods of Decreasing Value}
% Enter the (shortened) title:
% \RUNAUTHOR{Che and Namkoong}
% \RUNTITLE{Adaptive Experimentation at Scale}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\title{A Planning Framework for Adaptive Labeling}
 
% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows,
%   should be entered in ONE field, separated by a comma.
%   \EMAIL field can be repeated if more than one author
 
%  \AUTHOR{Daksh Mittal}
% \AFF{Decision, Risk, and Operations Division, Columbia Business School, New York, NY 10027, \EMAIL{namkoong@gsb.columbia.edu}} %, \URL{}}

% \AUTHOR{Yuanzhe Ma}

% \AUTHOR{Shalmali Joshi}


% \AUTHOR{Hongseok Namkoong}
% \AFF{Decision, Risk, and Operations Division, Columbia Business School, New York, NY 10027, \EMAIL{namkoong@gsb.columbia.edu}} %, \URL{}}

 
\date{\today}
%\author{}
% Sample
%\KEYWORDS{deterministic inventory theory; infinite linear programming duality;
%  existence of optimal policies; semi-Markov decision process; cyclic schedule}

% Fill in data. If unknown, outcomment the field
% \KEYWORDS{adaptive experimentation, A/B testing, experimental design}
% \HISTORY{This paper was first submitted on July,
%   2020.}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Samples of sectioning (and labeling) in MNSC
% NOTE: (1) \section and \subsection do NOT end with a period
%       (2) \subsubsection and lower need end punctuation
%       (3) capitalization is as shown (title style).
%
%\section{Introduction.}\label{intro} %%1.
%\subsection{Duality and the Classical EOQ Problem.}\label{class-EOQ} %% 1.1.
%\subsection{Outline.}\label{outline1} %% 1.2.
%\subsubsection{Cyclic Schedules for the General Deterministic SMDP.}
%  \label{cyclic-schedules} %% 1.2.1
%\section{Problem Description.}\label{problemdescription} %% 2.

% Text of your paper here

\else

\documentclass[11pt]{article}
\usepackage[numbers]{natbib}
%\usepackage[nonatbib]{neurips_2024}
\usepackage{macros/packages}
\usepackage{macros/editing-macros}
\usepackage{macros/formatting}
\usepackage{macros/statistics-macros}
 

% \onehalfspacing
% \renewcommand{\baselinestretch}{1.35}

% \title{Adaptive Labeling for Efficient Out-of-distribution Model Evaluation}


\begin{document}
%\maketitle

% Control whitespace around equations
\abovedisplayskip=8pt plus0pt minus3pt
\belowdisplayskip=8pt plus0pt minus3pt


% ------------------------------------------------------------------------
% Main Paper Body
% ------------------------------------------------------------------------


% ------------------------------------------------------------------------
% Default title and authorship
% ------------------------------------------------------------------------
\begin{center}
  {\huge A Planning Framework for Adaptive Labeling\footnote{
  A conference version of this work appeared at 
2024 Conference on Neural Information Processing Systems,
titled 
``Adaptive Labeling for Efficient Out-of-distribution Model Evaluation''.}} \\
  \vspace{.5cm}
  {\Large Daksh Mittal$^{*1}$~~  Yuanzhe Ma$^{*2}$ ~~
  Shalmali Joshi$^{3}$ ~~ Hongseok Namkoong$^{1}$
  } \\
{ Decision, Risk, and Operations Division$^{1}$,
  Department of Industrial Engineering and Operations Research$^{2}$,
  Department of Biomedical Informatics$^{3}$ \\
  Columbia University
  } \\ 
\end{center}

% ------------------------------------------------------------------------
% Abstract
% ------------------------------------------------------------------------



\fi
%\linenumbers

\def\thefootnote{*}\footnotetext{Equal contribution}
%\def\thefootnote{\arabic{footnote}}

\begin{abstract}
\input{abstract}
\end{abstract}

\input{introduction}
\input{related_work}
\input{formulation}     
\input{methodology}    
\input{experiments}
\input{gradient_analysis}
\input{practical_considerations}
\input{conclusion}

% \input{introduction}

 
% Acknowledgments---Will not appear in anonymized version


%% ========================== Bibliography =========================  = %%

%\bibliographystyle{abbrvnat}

%\ifdefined\useorstyle
%\setlength{\bibsep}{.0em}
%\else
%\setlength{\bibsep}{.7em}
%\fi

%\bibliography{bib}
% \bibliographystyle{format/icml2022} 

\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aggarwal et~al.(2014)Aggarwal, Kong, Gu, Han, and Philip]{AggarwalKoGuHaPh14}
C.~C. Aggarwal, X.~Kong, Q.~Gu, J.~Han, and S.~Y. Philip.
\newblock Active learning: A survey.
\newblock In \emph{Data classification}, pages 599--634. Chapman and Hall/CRC, 2014.

\bibitem[Alesiani(2023)]{Alesiani23}
F.~Alesiani.
\newblock Implicit bilevel optimization: differentiating through bilevel optimization programming.
\newblock In \emph{Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence}, 2023.

\bibitem[Alvo et~al.(2024)Alvo, Russo, and Kanoria]{AlvoRuKa23}
M.~Alvo, D.~Russo, and Y.~Kanoria.
\newblock Neural inventory control in networks via hindsight differentiable policy optimization.
\newblock \emph{arXiv:2306.11246 [cs.LG]}, 2024.

\bibitem[Amaran et~al.(2016)Amaran, Sahinidis, Sharda, and Bury]{AmaranSaShBu16}
S.~Amaran, N.~V. Sahinidis, B.~Sharda, and S.~J. Bury.
\newblock Simulation optimization: a review of algorithms and applications.
\newblock \emph{Annals of Operations Research}, 2016.

\bibitem[Amos(2022)]{Amos22}
B.~Amos.
\newblock Tutorial on amortized optimization.
\newblock \emph{Foundations and TrendsÂ® in Machine Learning}, 2022.

\bibitem[Astudillo et~al.(2021)Astudillo, Jiang, Balandat, Bakshy, and Frazier]{AstudilloJiBaBaFr21}
R.~Astudillo, D.~Jiang, M.~Balandat, E.~Bakshy, and P.~Frazier.
\newblock Multi-step budgeted bayesian optimization with unknown evaluation costs.
\newblock \emph{Advances in Neural Information Processing Systems 21}, 2021.

\bibitem[Avadhanula et~al.(2022)Avadhanula, Baki, Bastani, Bastani, Gocmen, Haimovich, Hwang, Karamshuk, Leeper, Ma, et~al.]{AvadhanulaEtAl22}
V.~Avadhanula, O.~A. Baki, H.~Bastani, O.~Bastani, C.~Gocmen, D.~Haimovich, D.~Hwang, D.~Karamshuk, T.~Leeper, J.~Ma, et~al.
\newblock Bandits for online calibration: An application to content moderation on social media platforms.
\newblock \emph{arXiv preprint arXiv:2211.06516}, 2022.

\bibitem[Balachandar et~al.(2024)Balachandar, Garg, and Pierson]{BalachandarGaPi24}
S.~Balachandar, N.~Garg, and E.~Pierson.
\newblock Domain constraints improve risk prediction when outcome data is missing.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Barron et~al.(1998)Barron, Rissanen, and Yu]{BarronRiYu98}
A.~Barron, J.~Rissanen, and B.~Yu.
\newblock The minimum description length principle in coding and modeling.
\newblock \emph{IEEE transactions on information theory}, 44\penalty0 (6):\penalty0 2743--2760, 1998.

\bibitem[Baydin et~al.(2017)Baydin, Pearlmutter, Radul, and Siskind]{BaydinPeRaSi17}
A.~G. Baydin, B.~A. Pearlmutter, A.~A. Radul, and J.~M. Siskind.
\newblock Automatic differentiation in machine learning: a survey.
\newblock \emph{Journal of Machine Learning Research}, 2017.

\bibitem[Bengio et~al.(2013)Bengio, L\'eonard, and Courville]{BengioLeCo13}
Y.~Bengio, N.~L\'eonard, and A.~Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock \emph{arXiv:1308.3432 [cs.LG]}, 2013.

\bibitem[Bertsekas and Tsitsiklis(1996)]{BertsekasTs96}
D.~Bertsekas and J.~N. Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Blondel et~al.(2022)Blondel, Berthet, Cuturi, Frostig, Hoyer, Llinares-Lopez, Pedregosa, and Vert]{BlondelBeCuFrHoLlPeVe22}
M.~Blondel, Q.~Berthet, M.~Cuturi, R.~Frostig, S.~Hoyer, F.~Llinares-Lopez, F.~Pedregosa, and J.-P. Vert.
\newblock Efficient and modular implicit differentiation.
\newblock In \emph{Advances in Neural Information Processing Systems 22}, 2022.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and Wierstra]{Blundell15}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine Learning}, pages 1613--1622. PMLR, 2015.

\bibitem[Cao(1985)]{Cao85}
X.-R. Cao.
\newblock Convergence of parameter sensitivity estimates in a stochastic experiment.
\newblock \emph{IEEE Transactions on Automatic Control}, 1985.

\bibitem[Che et~al.(2024)Che, Dong, and Namkoong]{CheDoNa24}
E.~Che, J.~Dong, and H.~Namkoong.
\newblock Differentiable discrete event simulation for queuing network control.
\newblock \emph{arXiv:2409.03740 [cs.LG]}, 2024.

\bibitem[Chen et~al.(2000)Chen, Lin, Yucesan, and Chick]{ChenLiYuCh00}
C.-H. Chen, J.~Lin, E.~Yucesan, and S.~E. Chick.
\newblock Simulation budget allocation for further enhancing the efficiency of ordinal optimization.
\newblock \emph{Discrete Event Dynamic Systems}, 10:\penalty0 251--270, 2000.

\bibitem[Chen et~al.(2015)Chen, Chick, Lee, and Pujowidianto]{ChenChLePu15}
C.-H. Chen, S.~E. Chick, L.~H. Lee, and N.~A. Pujowidianto.
\newblock Ranking and selection: efficient simulation budget allocation.
\newblock \emph{Handbook of Simulation Optimization}, pages 45--80, 2015.

\bibitem[Dawid(1984)]{Dawid84}
A.~P. Dawid.
\newblock Statistical theory: The prequential approach.
\newblock \emph{Journal of the Royal Statistical Society, Series A}, 147:\penalty0 278--292, 1984.

\bibitem[de~Avila Belbute-Peres et~al.(2018)de~Avila Belbute-Peres, Smith, Allen, Tenenbaum, and Kolter]{deAvilaFiSmAlTeKl18}
F.~de~Avila Belbute-Peres, K.~Smith, K.~Allen, J.~Tenenbaum, and J.~Z. Kolter.
\newblock End-to-end differentiable physics for learning and control.
\newblock In \emph{Advances in Neural Information Processing Systems}. Curran Associates, Inc., 2018.

\bibitem[Ding et~al.(2021)Ding, Hardt, Miller, and Schmidt]{DingHaMoSc21}
F.~Ding, M.~Hardt, J.~Miller, and L.~Schmidt.
\newblock Retiring adult: New datasets for fair machine learning.
\newblock \emph{Advances in Neural Information Processing Systems 34}, 34, 2021.

\bibitem[Du et~al.(2020)Du, Li, Xu, Spielberg, Wu, Rus, and Matusik]{DuLiXuSpWuRuMa20}
T.~Du, Y.~Li, J.~Xu, A.~Spielberg, K.~Wu, D.~Rus, and W.~Matusik.
\newblock D3{\{}pg{\}}: Deep differentiable deterministic policy gradients, 2020.

\bibitem[Dwaracherla et~al.(2020)Dwaracherla, Lu, Ibrahimi, Osband, Wen, and Roy]{DwaracherlaLuIbOsWeVa20}
V.~Dwaracherla, X.~Lu, M.~Ibrahimi, I.~Osband, Z.~Wen, and B.~V. Roy.
\newblock Hypermodels for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Eckman and Henderson(2020)]{EckmanHe20}
D.~J. Eckman and S.~G. Henderson.
\newblock Biased gradient estimators in simulation optimization.
\newblock In \emph{2020 Winter Simulation Conference (WSC)}, pages 2935--2946, 2020.

\bibitem[Efraimidis and Spirakis(2006)]{EfraimidisSp06}
P.~S. Efraimidis and P.~G. Spirakis.
\newblock Weighted random sampling with a reservoir.
\newblock \emph{Information processing letters}, 2006.

\bibitem[Efroni et~al.(2018)Efroni, Dalal, Scherrer, and Mannor]{EfroniDaScMa18}
Y.~Efroni, G.~Dalal, B.~Scherrer, and S.~Mannor.
\newblock Multiple-step greedy policies in approximate and online reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Efroni et~al.(2020)Efroni, Ghavamzadeh, and Mannor]{EfroniGhMa20}
Y.~Efroni, M.~Ghavamzadeh, and S.~Mannor.
\newblock Online planning with lookahead policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Fortini and Petrone(2023)]{FortiniPe23}
S.~Fortini and S.~Petrone.
\newblock Prediction-based uncertainty quantification for exchangeable sequences.
\newblock \emph{Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences}, 381\penalty0 (2247):\penalty0 20220142, 2023.

\bibitem[Frazier(2018)]{Frazier18}
P.~I. Frazier.
\newblock \emph{Bayesian Optimization}, pages 255--278.
\newblock 2018.

\bibitem[Frazier et~al.(2008)Frazier, Powell, and Dayanik]{FrazierPoDa08}
P.~I. Frazier, W.~B. Powell, and S.~Dayanik.
\newblock A knowledge-gradient policy for sequential information collection.
\newblock \emph{SIAM Journal on Control and Optimization}, 47\penalty0 (5):\penalty0 2410--2439, 2008.
\newblock \doi{10.1137/070693424}.

\bibitem[Fu and Hill(1997)]{FuHi97}
M.~C. Fu and S.~D. Hill.
\newblock Optimization of discrete event systems via simultaneous perturbation stochastic approximation.
\newblock \emph{IIE transactions}, pages 233--243, 1997.

\bibitem[Fu and Hu(2012)]{FuHu12}
M.~C. Fu and J.-Q. Hu.
\newblock \emph{Conditional Monte Carlo: Gradient estimation and optimization applications}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Gal and Ghahramani(2016)]{GalGh16}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty in deep learning.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine Learning}, pages 1050--1059, 2016.

\bibitem[Ghadimi and Lan(2013)]{GhadimiLa13}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Gittins(1979)]{Gittins79}
J.~C. Gittins.
\newblock Bandit processes and dynamic allocation indices.
\newblock \emph{Journal of the Royal Statistical Society, Series B}, 41\penalty0 (2):\penalty0 148--177, 1979.

\bibitem[Glasserman(1990)]{Glasserman90}
P.~Glasserman.
\newblock \emph{Gradient estimation via perturbation analysis}.
\newblock Springer Science \& Business Media, 1990.

\bibitem[Glasserman(1992)]{Glasserman92}
P.~Glasserman.
\newblock Derivative estimates from simulation of continuous-time {M}arkov chains.
\newblock \emph{Operations Research}, 40\penalty0 (2):\penalty0 292--308, 1992.

\bibitem[Glasserman(2004)]{Glasserman04}
P.~Glasserman.
\newblock \emph{Monte Carlo methods in financial engineering}, volume~53.
\newblock Springer, 2004.

\bibitem[Glynn(1989)]{Glynn89}
P.~Glynn.
\newblock Optimization of stochastic systems via simulation.
\newblock In \emph{1989 Winter Simulation Conference Proceedings}, 1989.

\bibitem[Glynn(1987)]{Glynn87}
P.~W. Glynn.
\newblock Likelilood ratio gradient estimation: an overview.
\newblock In \emph{Proceedings of the 19th Conference on Winter Simulation}, 1987.

\bibitem[Glynn and Juneja(2004)]{GlynnJu04}
P.~W. Glynn and S.~Juneja.
\newblock A large deviations perspective on ordinal optimization.
\newblock In \emph{Proceedings of the 2004 Winter Simulation Conference}, pages 577--586. IEEE, 2004.

\bibitem[Greensmith et~al.(2004)Greensmith, Bartlett, and Baxter]{GreensmithBaBa04}
E.~Greensmith, P.~L. Bartlett, and J.~Baxter.
\newblock Variance reduction techniques for gradient estimates in reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 2004.

\bibitem[Grefenstette et~al.(2019)Grefenstette, Amos, Yarats, Htut, Molchanov, Meier, Kiela, Cho, and Chintala]{GrefenstetteAmYaHtMoMeKiChCh19}
E.~Grefenstette, B.~Amos, D.~Yarats, P.~M. Htut, A.~Molchanov, F.~Meier, D.~Kiela, K.~Cho, and S.~Chintala.
\newblock Generalized inner loop meta-learning.
\newblock \emph{arXiv preprint arXiv:1910.01727}, 2019.

\bibitem[Grover et~al.(2022)Grover, Xu, Tittelfitz, Cheng, Li, Zablocki, Liu, and Zhou]{GroverXuTiCheLiZaLiZh22}
P.~Grover, J.~Xu, J.~Tittelfitz, A.~Cheng, Z.~Li, J.~Zablocki, J.~Liu, and H.~Zhou.
\newblock Fraud dataset benchmark and applications.
\newblock \emph{arXiv:2208.14417 [cs.LG]}, 2022.

\bibitem[Heidelberger et~al.(1988)Heidelberger, Cao, Zazanis, and Suri]{HeidelbergerCaZaMiSu88}
P.~Heidelberger, X.-R. Cao, M.~A. Zazanis, and R.~Suri.
\newblock Convergence properties of infinitesimal perturbation analysis estimates.
\newblock \emph{Management Science}, 1988.

\bibitem[Ho et~al.(1983{\natexlab{a}})Ho, Cao, and Cassandras]{HoCaCh89}
Y.~Ho, X.~Cao, and C.~Cassandras.
\newblock Infinitesimal and finite perturbation analysis for queueing networks.
\newblock \emph{Automatica}, 19\penalty0 (4):\penalty0 439--445, 1983{\natexlab{a}}.

\bibitem[Ho et~al.(1983{\natexlab{b}})Ho, Eyler, and Chien]{HoEyCh83}
Y.~C. Ho, M.~A. Eyler, and T.~T. Chien.
\newblock A new approach to determine parameter sensitivities of transfer lines.
\newblock \emph{Management Science}, 29\penalty0 (6), 1983{\natexlab{b}}.

\bibitem[Hong et~al.(2015)Hong, Nelson, and Xu]{HongNeXu15}
L.~J. Hong, B.~L. Nelson, and J.~Xu.
\newblock Discrete optimization via simulation.
\newblock In \emph{Handbook of Simulation Optimization}, pages 9--44. Springer, 2015.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and Lengyel]{HoulsbyHuGhLe11}
N.~Houlsby, F.~Husz{\'a}r, Z.~Ghahramani, and M.~Lengyel.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv:1112.5745 [cs.CV]}, 2011.

\bibitem[Huang et~al.(2022)Huang, Dossa, Raffin, Kanervisto, and Wang]{HuangDoRaKaWa22}
S.~Huang, R.~F.~J. Dossa, A.~Raffin, A.~Kanervisto, and W.~Wang.
\newblock The 37 implementation details of proximal policy optimization, 2022.
\newblock URL \url{https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/}.
\newblock ICLR 2022 Blog Track.

\bibitem[Huang et~al.(2021)Huang, Hu, Du, Zhou, Su, Tenenbaum, and Gan]{HuangHuDuZhSuTeGa21}
Z.~Huang, Y.~Hu, T.~Du, S.~Zhou, H.~Su, J.~B. Tenenbaum, and C.~Gan.
\newblock Plasticinelab: A soft-body manipulation benchmark with differentiable physics.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{JangGuPo17}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with {G}umbel-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Johnson and Jackman(1989)]{JohnsonJa89}
M.~E. Johnson and J.~Jackman.
\newblock Infinitesimal perturbation analysis: a tool for simulation.
\newblock \emph{Journal of the Operational Research Society}, 40\penalty0 (3):\penalty0 243--254, 1989.

\bibitem[Kim and Nelson(2007)]{KimNe07}
S.-H. Kim and B.~L. Nelson.
\newblock Recent advances in ranking and selection.
\newblock In \emph{Proceedings of the 2007 Winter Simulation Conference}, 2007.

\bibitem[Kingma and Welling(2014)]{KingmaWe14}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes, 2014.

\bibitem[Kirsch et~al.(2019)Kirsch, van Amersfoort, and Gal]{KirschVaGa19}
A.~Kirsch, J.~van Amersfoort, and Y.~Gal.
\newblock {BatchBALD}: Efficient and diverse batch acquisition for deep bayesian active learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~32, 2019.

\bibitem[Kotary et~al.(2023)Kotary, Dinh, and Fioretto]{KotaryDiFi23}
J.~Kotary, M.-H.~N. Dinh, and F.~Fioretto.
\newblock Backpropagation of unrolled solvers with folded optimization.
\newblock In \emph{International Joint Conference on Artificial Intelligence}, 2023.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and Blundell]{LakshminarayananPrBl17}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep ensembles.
\newblock In \emph{Proceedings of the 31st International Conference on Neural Information Processing Systems}, page 6405â6416, 2017.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and Duvenaud]{LorraineViDu20}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics}, 2020.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{MaddisonMnTe17}
C.~J. Maddison, A.~Mnih, and Y.~W. Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random variables.
\newblock In \emph{Proceedings of the Fifth International Conference on Learning Representations}, 2017.

\bibitem[Madeka et~al.(2022)Madeka, Torkkola, Eisenach, Luo, Foster, and Kakade]{MadekaToEiLuFoKa22}
D.~Madeka, K.~Torkkola, C.~Eisenach, A.~Luo, D.~P. Foster, and S.~M. Kakade.
\newblock Deep inventory management.
\newblock \emph{arXiv:2210.03137 [cs.LG]}, 2022.

\bibitem[Margossian(2019)]{Margossian19}
C.~C. Margossian.
\newblock A review of automatic differentiation and its efficient implementation.
\newblock \emph{WIREs Data Mining and Knowledge Discovery}, 2019.

\bibitem[Mnih and Gregor(2014)]{MnihGr14}
A.~Mnih and K.~Gregor.
\newblock Neural variational inference and learning in belief networks.
\newblock In \emph{Proceedings of the 31st International Conference on Machine Learning}, volume~32 of \emph{Proceedings of Machine Learning Research}, pages 1791--1799, 2014.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and Mnih]{MohamedRoFiMn20}
S.~Mohamed, M.~Rosca, M.~Figurnov, and A.~Mnih.
\newblock {Monte Carlo} gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (1), 2020.

\bibitem[Mora et~al.(2021)Mora, Peychev, Ha, Vechev, and Coros]{MoraAnHaVeCo21}
M.~A.~Z. Mora, M.~Peychev, S.~Ha, M.~Vechev, and S.~Coros.
\newblock Pods: Policy optimization via differentiable simulation.
\newblock In \emph{Proceedings of the 38th International Conference on Machine Learning}, 2021.

\bibitem[Mullainathan and Obermeyer(2022)]{MullainathanOb22}
S.~Mullainathan and Z.~Obermeyer.
\newblock Diagnosing physician error: A machine learning approach to low-value health care.
\newblock \emph{The Quarterly Journal of Economics}, 2022.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Hollmann, Arango, Grabocka, and Hutter]{MullerHoArGrHu22}
S.~M{\"u}ller, N.~Hollmann, S.~P. Arango, J.~Grabocka, and F.~Hutter.
\newblock Transformers can do bayesian inference.
\newblock In \emph{Proceedings of the Tenth International Conference on Learning Representations}, 2022.

\bibitem[Nado et~al.(2021)Nado, Band, Collier, Djolonga, Dusenberry, Farquhar, Filos, Havasi, Jenatton, Jerfel, Liu, Mariet, Nixon, Padhy, Ren, Rudner, Wen, Wenzel, Murphy, Sculley, Lakshminarayanan, Snoek, Gal, and Tran]{Nadoal21}
Z.~Nado, N.~Band, M.~Collier, J.~Djolonga, M.~Dusenberry, S.~Farquhar, A.~Filos, M.~Havasi, R.~Jenatton, G.~Jerfel, J.~Liu, Z.~Mariet, J.~Nixon, S.~Padhy, J.~Ren, T.~Rudner, Y.~Wen, F.~Wenzel, K.~Murphy, D.~Sculley, B.~Lakshminarayanan, J.~Snoek, Y.~Gal, and D.~Tran.
\newblock {Uncertainty Baselines}: Benchmarks for uncertainty \& robustness in deep learning.
\newblock \emph{arXiv::2106.04015 [cs.LG]}, 2021.

\bibitem[Namkoong et~al.(2020)Namkoong, Daulton, and Bakshy]{NamkoongDaBa20}
H.~Namkoong, S.~Daulton, and E.~Bakshy.
\newblock Distilled thompson sampling: Practical and efficient thompson sampling via imitation learning.
\newblock \emph{arXiv:2011.14266 [cs.LG]}, 2020.

\bibitem[Nguyen and Grover(2022)]{NguyenGr22}
T.~Nguyen and A.~Grover.
\newblock Transformer neural processes: Uncertainty-aware meta learning via sequence modeling.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, 2022.

\bibitem[of~Medicine (US) Committee~on Understanding et~al.(2003)of~Medicine (US) Committee~on Understanding, Racial, and in~Health~Care]{Institute03}
I.~of~Medicine (US) Committee~on Understanding, E.~Racial, and E.~D. in~Health~Care.
\newblock \emph{Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care}.
\newblock 2003.

\bibitem[Osband and Van~Roy(2015)]{OsbandVa15}
I.~Osband and B.~Van~Roy.
\newblock Bootstrapped {T}hompson sampling and deep exploration.
\newblock \emph{arXiv:1507.00300 [stat.ML]}, 2015.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and Cassirer]{OsbandAsCa18}
I.~Osband, J.~Aslanides, and A.~Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, volume~31, 2018.

\bibitem[Osband et~al.(2022{\natexlab{a}})Osband, Wen, Asghari, Dwaracherla, Lu, Ibrahimi, Lawson, Hao, O'Donoghue, and Roy]{OsbandWeAsDwLuIbLaHaDoRo22}
I.~Osband, Z.~Wen, S.~M. Asghari, V.~Dwaracherla, X.~Lu, M.~Ibrahimi, D.~Lawson, B.~Hao, B.~O'Donoghue, and B.~V. Roy.
\newblock {The Neural Testbed: Evaluating Joint Predictions}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Osband et~al.(2022{\natexlab{b}})Osband, Wen, Asghari, Dwaracherla, Lu, and Van~Roy]{OsbandWeAsSeDwLuVa22}
I.~Osband, Z.~Wen, S.~M. Asghari, V.~Dwaracherla, X.~Lu, and B.~Van~Roy.
\newblock Evaluating high-order predictive distributions in deep learning.
\newblock In \emph{Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence}, pages 1552--1560, 2022{\natexlab{b}}.

\bibitem[Osband et~al.(2023)Osband, Wen, Asghari, Dwaracherla, Ibrahimi, Lu, and Roy]{OsbandWenAsDwIbLuRo23}
I.~Osband, Z.~Wen, S.~M. Asghari, V.~Dwaracherla, M.~Ibrahimi, X.~Lu, and B.~V. Roy.
\newblock Epistemic neural networks.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin, Dillon, Lakshminarayanan, and Snoek]{OvadiaFeReNa19}
Y.~Ovadia, E.~Fertig, J.~Ren, Z.~Nado, D.~Sculley, S.~Nowozin, J.~V. Dillon, B.~Lakshminarayanan, and J.~Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural Information Processing Systems}, 2019.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and Restelli]{PapiniBiCaPiRe18}
M.~Papini, D.~Binaghi, G.~Canonaco, M.~Pirotta, and M.~Restelli.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, pages 4026--4035, 2018.

\bibitem[Paulus et~al.(2021)Paulus, Maddison, and Krause]{PaulusMaKr21}
M.~B. Paulus, C.~J. Maddison, and A.~Krause.
\newblock Rao-blackwellizing the straight-through gumbel-softmax gradient estimator.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Pollard et~al.(2018)Pollard, Johnson, Raffa, Celi, Mark, and Badawi]{PollardJoRaCeMaBa18}
T.~J. Pollard, A.~E. Johnson, J.~D. Raffa, L.~A. Celi, R.~G. Mark, and O.~Badawi.
\newblock {The eICU Collaborative Research Database, a freely available multi-center database for critical care research}.
\newblock \emph{Scientific data}, 5\penalty0 (1):\penalty0 1--13, 2018.

\bibitem[Rasmussen(2003)]{Rasmussen03}
C.~E. Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In \emph{Summer School on Machine Learning}, pages 63--71. Springer, 2003.

\bibitem[Rasmussen(2006)]{Rasmussen06}
C.~E. Rasmussen.
\newblock Gaussian processes for machine learning.
\newblock pages 63--71, 2006.

\bibitem[Ren et~al.(2023)Ren, Feng, Liu, Pan*, Fu, Mai, and Yang]{RenFeLiPaFuMaYa23}
J.~Ren, X.~Feng, B.~Liu, X.~Pan*, Y.~Fu, L.~Mai, and Y.~Yang.
\newblock {TorchOpt: An Efficient Library for Differentiable Optimization}.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (367):\penalty0 1--14, 2023.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and Wierstra]{RezendeMoWi14}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep generative models.
\newblock In \emph{Proceedings of the 31st International Conference on Machine Learning}, pages 1278--1286, 2014.

\bibitem[Roberts(1965)]{Roberts65}
H.~V. Roberts.
\newblock Probabilistic prediction.
\newblock \emph{Journal of the American Statistical Association}, 60\penalty0 (309):\penalty0 50--62, 1965.

\bibitem[Scieur et~al.(2022)Scieur, Gidel, Bertrand, and Pedregosa]{ScieurGiBePe22}
D.~Scieur, G.~Gidel, Q.~Bertrand, and F.~Pedregosa.
\newblock The curse of unrolling: Rate of differentiating through optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Settles(2009)]{Settles09}
B.~Settles.
\newblock Active learning literature survey.
\newblock 2009.

\bibitem[Seyyed-Kalantari et~al.(2021)Seyyed-Kalantari, Zhang, McDermott, Chen, and Ghassemi]{Seyyed-KalantariZhMcChGh21}
L.~Seyyed-Kalantari, H.~Zhang, M.~B. McDermott, I.~Y. Chen, and M.~Ghassemi.
\newblock Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations.
\newblock \emph{Nature medicine}, 2021.

\bibitem[Shaban et~al.(2018)Shaban, Cheng, Hatch, and Boots]{ShabanChHaBo18}
A.~Shaban, C.-A. Cheng, N.~Hatch, and B.~Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{Proceedings of the 21st International Conference on Artificial Intelligence and Statistics}, 2018.

\bibitem[Straw and Wu(2022)]{StrawWu22}
I.~Straw and H.~Wu.
\newblock Investigating for bias in healthcare algorithms: a sex-stratified analysis of supervised machine learning models in liver disease prediction.
\newblock \emph{BMJ health \& care informatics}, 2022.

\bibitem[Suh et~al.(2022)Suh, Simchowitz, Zhang, and Tedrake]{SuhSiZhTe22}
H.~J. Suh, M.~Simchowitz, K.~Zhang, and R.~Tedrake.
\newblock Do differentiable simulators give better policy gradients?
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162, 2022.

\bibitem[Sutton and Barto(2018)]{SuttonBa18}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{SuttonMcSiMa99}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~12, 1999.

\bibitem[Tucker et~al.(2017)Tucker, Mnih, Maddison, Lawson, and Sohl-Dickstein]{TuckerMnMaLaSo17}
G.~Tucker, A.~Mnih, C.~J. Maddison, J.~Lawson, and J.~Sohl-Dickstein.
\newblock Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2021)Wang, Sun, and Grosse]{WangSuGr21}
C.~Wang, S.~Sun, and R.~Grosse.
\newblock Beyond marginal uncertainty: How accurately can {B}ayesian regression models estimate posterior predictive correlations?
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2476--2484, 2021.

\bibitem[Williams(1992)]{Williams92}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Machine Learning}, 8:\penalty0 229--256, 1992.

\bibitem[Wu et~al.(2019)Wu, Brooks, Chen, Chen, Choudhury, Dukhan, Hazelwood, Isaac, Jia, Jia, et~al.]{WuEtAl19}
C.-J. Wu, D.~Brooks, K.~Chen, D.~Chen, S.~Choudhury, M.~Dukhan, K.~Hazelwood, E.~Isaac, Y.~Jia, B.~Jia, et~al.
\newblock Machine learning at facebook: Understanding inference at the edge.
\newblock In \emph{2019 IEEE international symposium on high performance computer architecture (HPCA)}, pages 331--344. IEEE, 2019.

\bibitem[{Xi-Ren Cao}(1987)]{Cao87}
{Xi-Ren Cao}.
\newblock First-order perturbation analysis of a simple multi-class finite source queue.
\newblock \emph{Performance Evaluation}, 7\penalty0 (1):\penalty0 31--41, 1987.
\newblock ISSN 0166-5316.

\bibitem[Xie and Ermon(2019)]{XieEr19}
S.~M. Xie and S.~Ermon.
\newblock Reparameterizable subset sampling via continuous relaxations.
\newblock In \emph{Proceedings of the 28th International Joint Conference on Artificial Intelligence}, IJCAI'19, page 3919â3925, 2019.

\bibitem[Xu et~al.(2021)Xu, Makoviychuk, Narang, Ramos, Matusik, Garg, and Macklin]{XuMaNaRaMaGaMa21}
J.~Xu, V.~Makoviychuk, Y.~Narang, F.~Ramos, W.~Matusik, A.~Garg, and M.~Macklin.
\newblock Accelerated policy learning with parallel differentiable simulation.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\end{thebibliography}



\newpage
\begin{appendix}
\input{details_GP}
\input{details_k_subset}
\input{details_smoothing_gp}
\input{details_smoothing_recall}
\input{details_experiment}
\input{proof}



\input{UQ/introduction}
\input{UQ/related_work}
\input{UQ/formulation}     
\input{UQ/uq_formulation}
\input{UQ/methodology}     
\input{UQ/benchmark}
\input{UQ/experiments}    
\input{UQ/conclusion}


%\input{UQ/details_dataset}
\input{UQ/uq_module}
\input{UQ/our_code_base}
\input{UQ/details_experiment}
\input{UQ/hyperparameter_and_sweep}
\input{UQ/experiment_other_data}



\end{appendix}
%\input{checklist}

\ifdefined\useorstyle

%\ECSwitch

%\ECDisclaimer
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Main head for the e-companion
%\ECHead{Appendix}



\else
\newpage
\appendix

\fi



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
