
% \section{Details of weighted GP} 
% \label{sec:details-weighted-GP}

% We present a weighted GP algorithm (Algorithm~\ref{alg:weighted-GP}) adapted from Algorithm 2.1 in~\citep{Rasmussen06}. 
% Recall that GPs $f(\cdot) \sim \mc{GP}(m(\cdot), \mc{K}(\cdot,\cdot))$ are defined by a mean function $m(\cdot)$ and kernel $\mc{K}(\cdot,\cdot)$.
% % for any inputs $X$,  $f(X)$ is Gaussian with mean $m(X)$ and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. 
% % In addition, the observation consists of $(X,\mc{Y})$ with $\mc{Y} = f\opt(X) + \varepsilon$ and ${\varepsilon} \sim \mc{N}(0, \sigma_d^2 I))$ for some noise level $\sigma_d > 0$.
% % For any inputs $\bm{X}$, we assume $f(\bm{X})$ is Gaussian with mean $m(\bm{X})$ and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. In addition, the observation consists of $(\bm{X},\bm{Y})$ with $\bm{Y} = f\opt(\bm{X}) + \epsilon$ and ${\epsilon} \sim \mc{N}(0, \sigma_d^2 I))$ for some noies level $\sigma_d > 0$ and size $d$.
% % Given training data $(\bm{X}, \bm{Y})$ and   test points $\bm{X}^*$, closed form posterior estimates over $f\opt$ can be estimated

% % \begin{align*}
% % \bm{f}^* \mid \bm{X, Y, X^*} & \sim \mc{N}(\bar{\bm{f}}|^*, \cov(\bm{f}^*)), \\ 
% % \text{where }  \bar{\bm{f}}|^* &\defeq \mc{K}(\bm{X}^*, \bm{X}) [\mc{K}(\bm{X},\bm{X}) + \sigma_d^2 I]^{-1} Y \\ 
% % \cov(\bm{f}^*)) & \defeq \mc{K}(\bm{X}^*, \bm{X}^*) - \mc{K}(\bm{X}^*, \bm{X}) [\mc{K}(\bm{X},\bm{X}) + \sigma_d^2 I]^{-1} \mc{K}(\bm{X},\bm{X}^*).
% % \end{align*} Using the above expression, we can update our belief  on $\bm{f}^*$ after observing new data.


% Now assuming, for the input data $(\bm{X},\bm{Y})$, we are also given some weights $w$ for these inputs and we want to perform posterior estimates over  $\bm{f}\opt$  for test points $\bm{X}^*$ under the weights $w$. Let us define the following terms
% $K \defeq \mc{K}(\bm{X},\bm{X}), K_* \defeq \mc{K}(\bm{X^*},\bm{X})$ and $K_{*,*} \defeq \mc{K}(\bm{X}^*, \bm{X}^*)$.
% Then GP Algorithm~\ref{alg:weighted-GP} gives
% posterior estimates of $\bar{\bm{f}\opt}$  for samples with weights $w$.
 

% %The justification of the algorithm is as follows:


% % Recall that posterior update in Gaussian Processes is given as follows (see Section \ref{sec:GP})

% % \begin{align*}
% % \bm{f}^* \mid \bm{X^1, Y, X^*} & \sim \mc{N}(\bar{\bm{f}}|^*, \cov(\bm{f}^*)), \\ 
% % \text{where }  \bar{\bm{f}}|^* &\defeq \mc{K}(\bm{X}^*, \bm{X}^1) [\mc{K}(\bm{X}^1,\bm{X}^1) + \sigma_d^2 I]^{-1} Y \\ 
% % \cov(\bm{f}^*)) & \defeq \mc{K}(\bm{X}^*, \bm{X}^*) - \mc{K}(\bm{X}^*, \bm{X}^1) [\mc{K}(\bm{X}^1,\bm{X}^1) + \sigma_d^2 I]^{-1} \mc{K}(\bm{X}^1,\bm{X}^*).
% % \end{align*} 


% % Now observe that, $\mc{K}(\bm{X}^*, \bm{X}^1) [\mc{K}(\bm{X}^1,\bm{X}^1) + \sigma_d^2 I]^{-1} Y = \mc{K}(\bm{X}^*, \bm{X}^1) [\mc{K}(\bm{X}^1,\bm{X}^1) + \sigma_d^2 I]^{-1} Y$ 






% \begin{algorithm}
% \caption{Weighted Gaussian process regression}\label{alg:weighted-GP}
% \begin{algorithmic}[1]
% \State Input: $K, K_*, K_{*,*}, \bm{Y}, w$
       
%        Return: $(\bar{f}, V)$
% \State $K_w = K ( (\bm{1}-I)   ww^\top + I)$
% \State $K_{w,*} = w \odot K_*$
% \State $L = \mathsf{Cholesky}(K_w + \sigma_d^2 I)$
% \State $\alpha = L^\top \setminus (L \setminus \bm{Y})$
% \State $\bar{f} = K_{w,*}^\top \alpha$
% \State $v =  L \setminus K_{w,*}$
% \State $V = K_{*,*} - v^\top v$
% \State Return: mean and covariance: $(\bar{f}, V)$ 
% \end{algorithmic}
% \end{algorithm}

% \href{https://github.com/namkoong-lab/adaptive_sampling/blob/main/src/autodiff/gp_pipeline_regression/gaussian_process_cholesky_advanced.py#L48}{weighted GP}

 
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
