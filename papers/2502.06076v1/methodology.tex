\section{Continuous Reformulation}
\label{sec:methodology}

The dynamic programs~\eqref{eqn:general-obj} and~\eqref{eqn:general-ate-obj} involve
continuous states and action spaces
that are combinatorially large in the number of data points in $\xpool$. In order to bring to bear the power of planning through policy gradients, 
we introduce continuous reformulations of the original planning problems.
First, we illustrate how common UQ approaches allow us to formulate a posterior belief $\mu(f \mid \mc{D})$ given any supervised data $\mc{D}$,
and update it when additional labels are gathered. 
In Section~\ref{sec:uq}, we describe  both classical 
Bayesian models such as 
Gaussian processes and recent advancements in deep learning-based UQ  methods that our framework can leverage.
Second, to deal with the combinatorial action space,
we propose a continuous policy parameterization $\pi_{t,\theta}$  for a single-batch policy and use $\batchsize$-subset sampling~\citep{EfraimidisSp06} to choose
$\batchsize$ samples, described in  Section\ref{sec:k-subset}.
Finally, to reliably optimize the policy $\pi_{t,\theta}$,  we simulate trajectories (``roll-out'') through which we approximate policy gradients 
(Section~\ref{sec:roll-out}). 

Though our conceptual framework is general and can leverage multi-step 
lookaheads~\citep{BertsekasTs96,EfroniDaScMa18, EfroniGhMa20}, we focus on one-step lookaheads. In subsequent sections, we  empirically demonstrate that even one-step lookaheads can  achieve significant improvement in sampling efficiency over other heuristic baselines. 

\subsection{Uncertainty Quantification (UQ) Module}
\label{sec:uq}

The UQ module maintains an estimate $\mu_t$ over posterior beliefs of $\dgm$ over time, enabling our planning framework. Our method is agnostic to the UQ module, and
we illustrate this using two instantiations: i) Gaussian Processes (GP) that are extensively used in the Bayesian Optimization literature and are known to work well in low dimensional data and regression setting, and ii) recently developed neural network-based UQ methods such as  
\ensembles/\ensembleplus~\citep{OsbandWeAsDwLuIbLaHaDoRo22}. %.  
%Our benchmarks suggest that Epistemic Neural Networks (ENNs), introduced by~\citep{OsbandWenAsDwIbLuRo23} consistently perform better than other UQ methodologies. 
%We choose  ENNs as our UQ methodology for further experiments. We now briefly describe the ENNs.
% Notably, as Gaussian Processes (GPs) - popularly used in  Bayesian optimization. As they  - we use this to isolate the imperfections in the UQ module - as GPs provide ......

\vspace{-10pt}
\paragraph{Gaussian Processes} $\mc{GP}(m(\cdot), \mc{K}(\cdot,\cdot))$ are defined by a mean function $m(\cdot)$ and kernel (or covariance function) $\mc{K}(\cdot,\cdot)$. For any set of  inputs $\mathcal{X} \equiv \{X_i\}_{i\in \mathcal{I}}$, the process $\bm{f}$ (where $f_i = {f}(X_i)$) follows a  multivariate normal distribution $\mc{N} (m(\mc{X}), \mc{K}(\mc{X},\mc{X}))$. Specifically, the mean and covariance are given by  $\E\left({f(X_i)}\right)=m(X_i)$  and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. 
Additionally, the observed output $\mathcal{Y}$ includes noise ${\varepsilon} \sim \mc{N}(0, \sigma^2 I)$, such that  $\mathcal{Y} = \bm{f} + \varepsilon$, where $\sigma > 0$  represents the noise level. % and size dd.




%For any inputs $\bm{X}$, we assume $f(\bm{X})$ is Gaussian with mean $m(\bm{X})$ and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. In addition, the observation consists of $(\bm{X},\bm{Y})$ with $\bm{Y} = f\opt(\bm{X}) + \epsilon$ and ${\epsilon} \sim \mc{N}(0, \sigma^2 I)$ for some noise level $\sigma > 0$.
Given training data $(\mathcal{X}, \mathcal{Y})$, and test points ${\mathcal{X}}'$, closed form posterior estimates for $\bm{{f}'}$ can be computed as follows:
\begin{align*}
\bm{{f}'} \mid {\mathcal{X}, \mathcal{Y}, \mathcal{X}'} & \sim \mc{N}(\bar{\bm{f'}}, \mathbb{V}), \\ 
\text{where }  \bar{\bm{f'}} &\defeq \mc{K}(\mc{X}', \mathcal{X}) [\mc{K}(\mc{X},\mc{X}) + \sigma^2 I]^{-1} \mc{Y} \\ 
\mathbb{V} & \defeq \mc{K}(\mc{X}', \mc{X}') - \mc{K}(\mc{X}', \mc{X}) [\mc{K}(\mc{X},\mc{X}) + \sigma^2 I]^{-1} \mc{K}(\mc{X},\mc{X}').
\end{align*} 

  %As we can see, posterior updates have a closed form and are differentiable. 



%Among the deep learning models \ensembles\citep{LakshminarayananPrBl17}, \ensembleplus\citep{OsbandAsCa18} have shown a lot of promise.  
\vspace{-10pt}
\paragraph{Ensembles}\ensembles~\citep{LakshminarayananPrBl17}  learn an ensemble of neural networks from given data, with each network having independently initialized weights and bootstrap sampling. 
%Posterior updates to \ensembles are autodifferentiable through the parametrizations of each  network. 
\ensembleplus\citep{OsbandAsCa18} extends this approach by combining ensembles of neural networks with randomized prior functions~\citep{OsbandVa15}. The prior function is added to each network in the ensemble, trained using L2 regularization. Recently,  
Epistemic neural networks (ENNs)~\citep{OsbandWenAsDwIbLuRo23} have also been shown to be an effective way of quantifying uncertainty. All these deep learning models are parametrized by some parameter $\eta$. For a given dataset $\set{(X_i,Y_i)}_{i \in \mc{I}}$, the model weights $\eta$ are update through gradient descent under a loss function $\loss(.)$, with the update rule expressed as: 
\begin{align}
    \eta_{\rm new} = \eta - 
    \sum_{i \in \mc{I}} \nabla_{\eta} \loss( X_i,Y_i,\eta). \label{eqn:ENN-update-naive}
\end{align}
    



% \ensembles\citep{LakshminarayananPrBl17}  learn an ensemble of neural networks from given data, trained using L2 regularization, with each network having independently initialized weights. Posterior updates to \ensembles are autodifferentiable through the parametrizations of each  network. 
% \ensembleplus\citep{OsbandAsCa18} consists of ensembles of neural networks with randomized prior functions combined with bootstrap sampling~\citep{osband2015bootstrapped}. The prior function is added to each network in the ensemble, trained using L2 regularization.  
% Epistemic neural networks (ENNs)~\citep{OsbandWenAsDwIbLuRo23} have been shown to be an effective way of quantifying uncertainty. %We corroborate these in our benchmarking experiments (see Section~\ref{sec:details-benchmark-UQ}).
%  ENNs train a network fη(X,Z)f_\eta(X,Z) parameterized by η\eta, where ZZ is a standard Gaussian noise with dimension dzd_z sampled repeatedly during training, where dzd_z is a hyperparamter.
% For a sample \set(xj,yj)j∈\mcI\set{(x_j,y_j)}_{j \in \mc{I}}, a gradient update of ENN, given a Gaussian sample zz, and a likelihood loss ℓENN\ell_{\texttt{ENN}}, can be written as 
% \begin{align}
%     \etanew = \eta - 
%     \sum_{j \in \mc{I}} \nabla_{\eta} \log \loss_{\texttt{ENN}}\left(f_\eta(x_j, z) \mid x_j; z\right). \label{eqn:ENN-update-naive}
% \end{align}






%Additionally,  we also use \textbf{Gaussian processes} as another UQ methodology. GPs are extensively used in Bayesian Optimization literature and are known to work well in low dimensional data and regression setting.



 
%To make our training pipeline fully autodifferentiable, we further enable differentiable updates of the sampling policy π\pi, parametrized by θ\theta, using soft KK-subset sampling, described in the following. 



\begin{algorithm}[t]
 \caption{ One-step lookahead planning}\label{alg:complete}
% \sj{Put the name of the method once you've decided here (instead of Adaptive Labeling Algorithm).}
 \begin{algorithmic}[1] 
 \State \textbf{Inputs:} Initial labeled data $\mc{D}^0$, horizon $T$, UQ module,  pool data  $\xpool$,  batch size $\batchsize$
 
\noindent \textbf{Returns:} %Queried batches $X^t$ and corresponding labels $Y^t$ for $1\leq t \leq T$
  Selected batches $(\datax^t, \datay^t)$ for $1 \le t \le T$
 and updated estimate of the objective $G(\mu_{T})$
 \State \textbf{Initialization}: Compute initial posterior state $\mu_0$ of the UQ module based 
 on initial labeled data $\mc{D}^0$.
\For{$0 \le t \le T-1$:}
\State Optimize $\pi_{t,\theta}$ using policy gradient updates  (based on  current posterior state $\mu_t$, pool  $\xpool$)
\State $\datax^{t+1} = \{X_j : X_j \in \texttt{SortDescending}(\xpool; \pi_{t,\theta}) \text{ for } 1\le j \le K \}$ 
\State Obtain labels $\datay^{t+1}$ to create $\mc{D}^{t+1}$
\State Update posterior state: $\mu_{t+1} \defeq \mu_t(\cdot\mid \mc{D}^{t+1})$
\State Estimate the objective $G(\mu_{t+1})$
\EndFor
 \end{algorithmic}
\end{algorithm}
\subsection{Sampling Policy}
\label{sec:k-subset}


Since we are using one-step lookahead policies, at each time step $t$,
we initiate a parametrized policy $\pi_{t,\theta}\defeq\pi_\theta$ that selects a  batch of $\batchsize$ samples to be queried. The samples are selected from a pool $\xpool$ of size $n$, based on weights ${\bm{w}}(\theta)$. %, learned by the parametrized policy πθ\pi_{\theta}.
%We then use \batchsize\batchsize-subset sampling to obtain  a batch of size \batchsize\batchsize from the pool (of size nn) using this policy π\pi. 
 Specifically, given weights $\bm{w} \ge \bm{0}$ and a batch size $\batchsize$, the  $\batchsize$-subset sampling mechanism generates a random vector ${S} \in \{0,1\}^{n}$ such that $\sum_{i=1}^n S_i = \batchsize$, whose distribution depends on $\bm{w}$.
Let 
$\bm{e}^j$ denote the $j$-th unit vector, and consider a sequence of unit vectors $[\bm{e}^{i_1} \cdots \bm{e}^{i_K}] \defeq s$. Using a parametrization known as  weighted reservoir sampling ($\wrs$)~\citep{EfraimidisSp06}, the probability of  selecting a sequence $s$ is given by:
\begin{align}
    p_{\wrs}([\bm{e}^{i_1}, \cdots, \bm{e}^{i_K}]|\bm{w}) \defeq \frac{w_{i_1}}{\sum_{j=1}^n w_j}\frac{w_{i_2}}{\sum_{j=1}^n w_j-w_{i_1}}...\frac{w_{i_K}}{\sum_{j=1}^n w_j-\sum_{j=1}^{K-1}w_{i_j}}.
    \label{eqn:p-S-w-def}
 \end {align}

The probability of selecting a batch $S$ can then be defined as: $ p(S|\bm{w}) \defeq \sum_{s \in \Pi(S)} p_{\wrs}(s|\bm{w}),$
where $\Pi(S) = \set{s:  \sum_{j=1}^K s[j]= S}$ denote all sequences of unit vectors with sum equal to $S$.

\subsection{Policy optimization using roll-outs}
\label{sec:roll-out}



% We showcase the power of \emph{planning using pathwise policy gradients} by considering the simplest possible planning algorithm: one-step lookaheads.  
At each time step $t$, we optimize the policy to minimize the uncertainty over the one-step target estimand $G(\mu_{t+1})$ based on current posterior belief $\mu_t$.
Once we optimize the policy, our algorithm  selects a batch of inputs $\datax^{t+1}$ to be queried based on the optimized policy ($\pi_{t,\theta}$).  After selecting the batch $\datax^{t+1}$, we observe the corresponding labels $\datay^{t+1}$ and update posterior belief over the data-generating function $f$ to $\mu_{t+1}$. Algorithm~\ref{alg:complete} summarizes each of these steps of the overall procedure.   The policy optimization is done using policy gradients estimated through simulated ``roll-outs" which we explain below. 


We optimize the policy $\pi_{t,\theta}$ using policy gradients and we estimate these policy gradients by  simulating the ``roll-outs" of posterior beliefs over $\dgm$ from the current belief $\mu_t$. Specifically, we first generate pseudo labels $\ybpool$ for the $\xpool$ using current belief $\mu_t$. We then use $\pi_{t,\theta}$ ($K$-subset sampling) to select batch $S\defeq\hat{\mathcal{X}}^{t+1}$ and do a pseudo-posterior update ($\mu_{+}^S$) using selected batch $\hat{\mathcal{X}}^{t+1}$ and the corresponding labels from $\ybpool$. Using the pseudo posterior  we estimate the objective $ G(\mu_{+}^S) \equiv \V_{f\sim\mu_{+}^S} (g(f))$ using Monte-Carlo sampling. 
 Figure \ref{fig:one_step_look_ahead_general} demonstrates a typical roll-out at time step $t$. Once we do the roll-out, we then estimate the policy gradient based on this roll-out as we describe in the next section. 

\begin{figure}[t]
\centering
\begin{tikzpicture}
[
roundnode/.style={circle, draw=black, very thick, minimum size=10mm, align=center, text width = 6mm},
roundnode2/.style={circle, draw=black, very thick, minimum size=10mm, align=center, text width = 8mm},
squarednodea/.style={rectangle, draw=black, very thick, minimum size=10mm, align =center,text width = 22mm},
squarednode1/.style={rectangle, draw=black, very thick, minimum size=10mm, align =center,text width = 20.5mm},
squarednode2/.style={rectangle, draw=red, very thick, minimum size=10mm, align =center,text width = 16mm},
squarednode3/.style={rectangle, draw=red, very thick, minimum size=10mm, align =center,text width = 37mm},
squarednode4/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 24mm},
squarednode4a/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 26mm},
squarednode5/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 22mm},
]
%Nodes
\node[squarednodea]      (maintopic)                              {State $\mu_t$ \\ Policy $\pi_{t,\theta} $ \\ $  \xpoolj{t+1}$\\ Sample $\ybpool^{t+1}$};
\node[squarednode2]        (circle1)       [right=4mm of maintopic] {$\batchsize$-subset sampling};
\node[squarednode1]      (circle2)       [right=4.5mm of circle1] {Select batch $S\defeq\hat{\mathcal{X}}^{t+1}$};
\node[squarednode2]        (circle3)       [right=4.5mm of circle2] {Pseudo Posterior update};
\node[roundnode]        (circle4)       [right=5.5mm of circle3] {${\mu}_{+}^S$};
\node[squarednode3]        (circle5)       [right=6mm of circle4] {Estimate\\ $ G(\mu_{+}^S) \equiv  \V_{{\mu}^S_{+}} (g(f))$};

% \node[squarednode4]        (circle1b)       [below=11mm of circle1] {Soft $\batchsize$-subset sampling};
% \node[roundnode]      (circle2b)       [right=5.4mm of circle1b] {\centering $a(\theta)$};
% \node[squarednode4a]        (circle3b)       [right=7mm of circle2b] {Soft posterior update (differentiable)};
% \node[roundnode]        (circle4b)       [right=4mm of circle3b] {${\mu}_{+}^{a(\theta)}$};
% \node[squarednode5]        (circle5b)       [right=5mm of circle4b] {Differentiable estimate};


%Lines
\draw[thick, ->, >=stealth] (maintopic.east) --  (circle1.west);
\draw[thick, ->] (circle1.east) --  (circle2.west);
\draw[thick, ->] (circle2.east)  --  (circle3.west);
\draw[thick, ->] (circle3.east) --  (circle4.west);
\draw[thick, ->] (circle4.east) --  (circle5.west);
% \draw[thick, -, dashed] (circle1.south) --  (circle1b.north);
% \draw[thick, -, dashed] (circle3.south) --  (circle3b.north);
% \draw[thick, -, dashed] (circle5.south) --  (circle5b.north);

% \draw[thick, ->] (maintopic.south) |-  (circle1b.west);
% \draw[thick, ->] (circle1b.east) --  (circle2b.west);
% \draw[thick, ->] (circle2b.east)  --  (circle3b.west);
% \draw[thick, ->] (circle3b.east) --  (circle4b.west);
% \draw[thick, ->] (circle4b.east) --  (circle5b.west);



\end{tikzpicture}
\caption{One-step lookahead roll-out for policy gradient estimation}
\label{fig:one_step_look_ahead_general}
\end{figure}






\section{Policy Gradient Estimation}
 \label{sec:diff-piepline}

Under the continuous policy parameterization policy $\pi(t,\theta) \defeq \pi_\theta$ we develop in the previous section, our objective is to minimize the \textit{one-step lookahead objective}
\begin{align*}
H(\theta) \defeq \E_{\mc{D}^{t+1} \sim \pi_\theta} \left[G(\mu_{t+1}) \right]  \defeq  \E_{\mc{D}^{t+1} \sim \pi_\theta} \left[G(\mu_t(\cdot \mid \mc{D}^{t+1})) \right]\defeq \E_{\mc{D}^{t+1} \sim \pi_\theta} \left[ \V_{f \sim \mu_{t+1}}  g(f)  \right].
\end{align*}
Let the gradient be defined as $ \nabla_\theta H(\theta) \defeq \nabla_\theta \E_{A\sim\pi_\theta}[G(A)]$. Here, we slightly abuse notations by letting  $A\defeq\mc{D}^{1:T}$ and $G(A)\defeq G(\mu(\cdot \mid \mc{D}^{0}, \mc{D}^{1:T}))$,  where the distribution of $A$ depends on $\pi_\theta$.

To estimate policy gradients, a popular approach is to use the score-trick (\textsf{REINFORCE}~\citep{Williams92}):
 $\nabla_\theta \E_{A \sim \pi_{\theta}}[G(A)] 
= \E_{A \sim \pi_{\theta}}[G(A) 
\nabla_\theta \log \pi_{\theta}(A)]
$.  Using the simulated roll-out described in the previous section, the \textsf{REINFORCE} estimator  $\frac{1}{N} \sum_{i=1}^N G(A^{(i)}) 
\nabla_\theta \log \pi_{\theta}(A^{(i)})$ can be used to optimize the  policy $\pi_{t,\theta}\defeq \pi_\theta$. 
While unbiased and only requiring zeroth order access to $G$, the \textsf{REINFORCE} estimator often suffers high variance~\citep{RezendeMoWi14}, especially when $\pi_{\theta}(A)$ is small. 
Although a stream of work strives to provide variance reduction techniques for \textsf{REINFORCE}~\citep{MnihGr14,PapiniBiCaPiRe18},
they require value function estimates that are also challenging to compute in our planning problem~\eqref{eqn:general-obj}. 

To remedy this,  we now introduce a new framework through which we can approximate policy gradients using direct backpropagation.
A fundamental challenge we grapple with is that pathwise gradients do not exist in our formulation: the objective is non-differentiable since ultimately inputs are either labeled or not, causing a fundamental discretness in the dynamics. 
We thus  devise a smooth approximation of the simulated roll-out to make it pathwise differentiable, which we now detail. The policies derived using our efficient differentiable simulator exploits the system structure and can achieve significantly improved performance compared to    policies 
that do not rely on gradient information,
as we illustrate in Section~\ref{sec:experiment}.  



\subsection{Smoothed-pathwise  policy gradients}

Since the dynamics of our planning problem---posterior updates---is known, instead of relying on high-variance score-based gradients
our goal is to leverage backpropagation and auto-differentiation to directly estimate approximate pathwise policy gradients~\eqref{eqn:grad-estimator}. 
A standard approach to pathwise gradient estimation is to find a random variable $Z \sim p_Z$ distributed independent of policy $\pi_{t,\theta}\defeq\pi_{\theta}$,  such that $A = {h}(Z,\theta)$ and
\begin{align}
    \nabla_\theta \E_{A \sim \pi_{\theta}}[G(A)] = \nabla_{\theta}\E_{Z\sim p_Z}[ G(h(Z, \theta))] \stackrel{(a)}{=}  \E_{Z\sim p_Z}[\nabla_{\theta} G(h(Z, \theta))]
    \label{eqn:grad-estimator-ideal}
\end{align} 
However,  in our formulations $h(Z, \theta)$ is non-differentiable w.r.t. $\theta$ because the actions are discrete; an input $X$ is either labeled or not.
As a result, the equality $(a)$ in~\eqref{eqn:grad-estimator-ideal}
no longer makes sense as $\nabla_{\theta}G(h(Z, \theta))$ does not exist. 

To address this, we  use a smoothed approximation, ${h}_{\tau}(Z,\theta)$, such that  $G({h}_{\tau}(Z,\theta))$ becomes differentiable
\begin{align}
\nabla_\theta \E_{A \sim \pi_{\theta}}[G(A)] \approx  \nabla_{\theta} \E[ G(h_{\tau}(Z, \theta))]
\approx \frac{1}{N}\sum_{i=1}^N \nabla_\theta G(h_\tau(Z_i,\theta)),
\label{eqn:grad-estimator}
\end{align}
where $\tau$ is a temperature parameter that controls the degree of smoothing.
It is important to note that sometimes even $G(\cdot)$ is non-differentiable, and in that case,
we must also consider a smooth approximation of $G(\cdot)$ 
(e.g., refer to Section~\ref{sec:details-recall-smoothing} for details on smoothing the \texttt{Recall} objective discussed earlier in Section~\ref{sec:formulation}). 
%\ym{Shall we move the sentence here: In what follows, we empirically demonstrate that through a careful smoothing of the planning objective, it is possible to trade-off bias and variance through auto-differentiation.}

To ease notation, rewrite the one-step objective parametrized by $\theta$ 
\begin{align}
   H(\theta) \defeq \E_{{\ybpool \sim \mu; S \sim \pi_\theta }}  [G\paran{\mu_+^S}],
  \label{eqn:obj-original-unsmoothed}
\end{align} 
where $G$ was defined in~\eqref{eqn:general-obj}. Here, $\pi_\theta \equiv p(\cdot|\bm{w}(\theta))$ is the distribution over subsets governed by $\bm{w}$ as defined by~\eqref{eqn:p-S-w-def}. Further, $\mu$ is the current posterior state and $\mu_+^S$ is the updated posterior state after incorporating the the additional batch  $S$ selected from $\xpool$ and the corresponding pseudo-outcomes from $\ybpool$ (drawn based on current posterior state $\mu$).
%We use parametrizations $\theta$ for the policy function $\pi$ which yields sampling probabilities $\bm w(\theta)$. 


We now describe a fully differentiable smoothed-pipeline by ensuring that each component of the ``roll-out'' pipeline: i) $\batchsize$-subset sampling, ii) posterior updates (UQ module), and iii) the objective is differentiable. 



\subsection{Smoothing sampling policy: Soft $\batchsize$-subset sampling} 



Recall the sampling policy introduced in Section \ref{sec:k-subset}. While the sampling policy features a continuous parametrization, it samples a discrete subset $S$ from the given pool $\mathcal{X}^{pool}$, making the operation non differentiable.  To address this, we adopt the \textit{``soft $\batchsize$-subset sampling procedure''} (Algorithm~\ref{alg:k-subset})  proposed by~\citet{XieEr19}, which enables the differentiability of sampling process. Broadly, this procedure  generates a random vector 
$\bm{a}(\theta) \in [0,1]^n$, a smoothed approximation of $S \in \{0,1\}^n$,  ensuring that $\sum_{i=1}^{n}a_i = \batchsize$. The vector  $\bm{a}({\theta})$ is approximately sampled from the sampling policy $ \pi_{\theta} \equiv p(\cdot|\bm{w}(\theta))$ defined in \eqref{eqn:p-S-w-def}, and it is differentiable with respect to $\theta$. The soft $\batchsize$-subset sampling procedure can be viewed as a generalization of the Gumbel-softmax trick, which smooths the sampling of a single item. For additional details, we refer readers to~\citep{XieEr19}.
%We defer the details to Section~\ref{sec:k-subset-detail}.



%Algorithm~\ref{alg:k-subset} for soft $K$-subset sampling was introduced in~\citep{XieEr19}. We present this algorithm here and 
\begin{algorithm}
\caption{Soft $\batchsize$-subset sampling   algorithm}\label{alg:k-subset}
\begin{algorithmic}[1]
\State \textbf{Inputs:} Weight vector $\bm{w}(\theta) \in \R_+^n$
\State  Sample $n$ independent standard Gumbel random variables $g_1,g_2,\ldots,g_n$
 \State Compute keys $\hat{r}_i = g_i + \log(w_i)$ for all $i$
\State
Initialize $\kappa^1_i = \hat{r}_i$ for all $i=1,\ldots,n$ 
\State  
For $j=1,..,\batchsize$, set
\begin{align*}
    a_i^j = \frac{\exp(\kappa_i^j/\tau)}{\sum_{k=1}^n \exp(\kappa_k^j/\tau)} \text{~for~all~} i=1,\ldots, n,
\end{align*}
$\kappa_i^{j+1} = \kappa_i^{j} + \log(1-a_i^j)$ for all $i=1,...,n$.
\State \textbf{Return:} The soft vector, $\bm{a} = \bm{a}^1+\bm{a}^2+...+\bm{a}^\batchsize$.  
\end{algorithmic}
\end{algorithm}



\subsection{Smoothing the Uncertainty Quantification (UQ) Module} 

Recall that our posterior updates rely on the subset $S$ of $\xpool$ selected using the policy $\pi(\theta)$. 
However, we have now introduced  soft $\batchsize$-subset samples $\bm{a}(\theta) \in [0,1]^n$, a smoothed approximation of $S\in\{0,1\}^n$, which does not represent a discrete subset of $\xpool$. This raises a question: how can posterior updates be performed using $\bm{a}(\theta)$?   
To address this, we reinterpret $\bm{a}(\theta)$ as weights for the  samples in $\xpool$ and adapt the  posterior updates of the respective uncertainty quantification methodologies accordingly. Notably, using weighted samples for posterior updates (to enable policy gradients) may hold independent value for other sequential decision-making tasks that rely on dynamic optimization.

We modify the closed-form Gaussian Process (GP) posterior updates to account for weighted inputs. Given training data $(\mathcal{X}, \mathcal{Y})$ and corresponding weights $\bm{a}$, the closed form posterior estimates for  $\bm{{f}'}$ at test points ${\mathcal{X}}'$ can be computed as follows:

\begin{align*}
\bm{{f}'} \mid {\mathcal{X}, \mathcal{Y}, \mathcal{X}'} & \sim \mc{N}(\bar{\bm{f'}}, \mathbb{V}), \\ 
\text{where }  \bar{\bm{f'}} &\defeq \mc{K}(\mc{X}', \mathcal{X}) \left [\mc{K}(\mc{X},\mc{X}) \left((\bm{1}-I)\bm{a}\bm{a}^T+I\right)+ \sigma^2 I\right ]^{-1} \mc{Y} \\ 
\mathbb{V} & \defeq \mc{K}(\mc{X}', \mc{X}') - \left(\bm{a} \odot\mc{K}(\mc{X}, \mc{X}')\right)^T \left[\mc{K}(\mc{X},\mc{X}) \left((\bm{1}-I)\bm{a}\bm{a}^T+I\right) + \sigma^2 I\right]^{-1} \left(\bm{a} \odot \mc{K}(\mc{X},\mc{X}') \right).
\end{align*} 


It can be easily verified that if $\bm{a}$ is a $K-$hot vector, then the standard GP posterior update is recovered. 
Additionally, we present a weighted GP algorithm  (Algorithm~\ref{alg:weighted-GP}) adapted from Algorithm 2.1 in~\citep{Rasmussen06}, to efficiently compute the proposed posterior updates. 
% Recall that GPs $f(\cdot) \sim \mc{GP}(m(\cdot), \mc{K}(\cdot,\cdot))$ are defined by a mean function $m(\cdot)$ and kernel $\mc{K}(\cdot,\cdot)$.
% Now assuming, for the input data $(\bm{X},\bm{Y})$, we are also given some weights $w$ for these inputs and we want to perform posterior estimates over  $\bm{f}\opt$  for test points $\bm{X}^*$ under the weights $w$. 
%Let us.
%Then GP Algorithm~\ref{alg:weighted-GP} gives posterior estimates of $\bar{\bm{f}'}$  for samples with weights $a$.
 






\begin{algorithm}
\caption{Weighted Gaussian process regression}\label{alg:weighted-GP}
\begin{algorithmic}[1]
\State Input: $\mc{X}, \mc{Y}, \mc{X}', \sigma^2, \bm{a}$
       
       Return: $(\bar{\bm{f}}',\mathbb{V})$

       Define: 
$K \defeq \mc{K}(\mc{X},\mc{X}), K_{*} \defeq \mc{K}(\mc{X},\mc{X}')$ and $K_{*,*} \defeq \mc{K}(\mc{X}', \mc{X}')$
\State $K_{\bm{a}} = K ( (\bm{1}-I)   \bm{a}\bm{a}^\top + I)$
\State $K_{\bm{a},*} = \bm{a} \odot K_*$
\State $L = \mathsf{Cholesky}(K_{\bm{a}} + \sigma^2 I)$
\State $\alpha = L^\top \setminus (L \setminus \bm{Y})$
\State $\bar{\bm{f}}' = K_{\bm{a},*}^\top \alpha$
\State $v =  L \setminus K_{\bm{a},*}$
\State $\mathbb{V} = K_{*,*} - v^\top v$
\State Return: mean and covariance: $(\bar{\bm{f}}',\mathbb{V})$ 
\end{algorithmic}
\end{algorithm}







Similarly, to perform weighted updates for deep learning-based UQ modules (\ensembles/ \ensembleplus), we modify previous gradient updates by incorporating soft sample weights $\bm{a}(\theta)$:
\begin{align*} 
\eta_1(\bm{a}(\theta)) \defeq  \eta_0 - 
\sum_{i \in \mc{I}}  a_i(\theta)   \nabla_{\eta} 
\loss (X_i,\bar{Y}_i,\eta) \mid_{\eta = \eta_0},
\end{align*}  
where $\loss$ is the loss function to train the deep learning based-UQ methodology, 
and we evaluate its gradient    at $\eta_0$. 
We interpret $\eta_1$ as an approximation of the optimal UQ module parameter with one gradient step.
Similarly, we define
a higher-order approximation $\eta_{h+1}(\bm{a}(\theta))$  as
\begin{align}
    \eta_{h+1} (\bm{a}(\theta)) \defeq  \eta_h (\bm{a}(\theta)) - 
    \sum_{i \in \mc{I}}  a_i(\theta)   \nabla_{\eta} \ell(X_i,\bar{Y}_i,\eta)\mid_{\eta = \eta_h(\bm{a}(\theta))}. \label{eqn:higher-eta-update}
\end{align}

 




 
% The UQ module maintains an estimate $\mu_t$ over posterior beliefs of $\dgm$ over time, enabling our planning framework. Our method is agnostic to the UQ module, and
% we illustrate this using two instantiations: i) Gaussian Processes (GP) that are extensively used in the Bayesian Optimization literature and are known to work well in low dimensional data and regression setting, and ii) recently developed neural network-based UQ methods such as  
% \ensembles/ \ensembleplus\citep{OsbandWeAsDwLuIbLaHaDoRo22}. %.  
% %Our benchmarks suggest that Epistemic Neural Networks (ENNs), introduced by~\citep{OsbandWenAsDwIbLuRo23} consistently perform better than other UQ methodologies. 
% %We choose  ENNs as our UQ methodology for further experiments. We now briefly describe the ENNs.
% % Notably, as Gaussian Processes (GPs) - popularly used in  Bayesian optimization. As they  - we use this to isolate the imperfections in the UQ module - as GPs provide ......

% \vspace{-10pt}
% \paragraph{Gaussian Processes} GPs $f(\cdot) \sim \mc{GP}(m(\cdot), \mc{K}(\cdot,\cdot))$ are defined by a mean function $m(\cdot)$ and kernel $\mc{K}(\cdot,\cdot)$, where
% for any inputs $X$,  $f(X)$ is Gaussian with mean $m(X)$ and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. 
% Additionally, the observation consists of $(X,{Y})$ with ${Y} = f\opt(X) + \varepsilon$ and ${\varepsilon} \sim \mc{N}(0, \sigma^2 I)$ for some noise level $\sigma > 0$. % and size dd.
% Posterior updates have a closed form and are differentiable, as we review in Section~\ref{sec:GP}.  


% %Among the deep learning models \ensembles\citep{LakshminarayananPrBl17}, \ensembleplus\citep{OsbandAsCa18} have shown a lot of promise.  
% \vspace{-10pt}
% \paragraph{Ensembles}\ensembles\citep{LakshminarayananPrBl17}  learn an ensemble of neural networks from given data, with each network having independently initialized weights. 
% %Posterior updates to \ensembles are autodifferentiable through the parametrizations of each  network. 
% \ensembleplus\citep{OsbandAsCa18} extends this approach by combining ensembles of neural networks with randomized prior functions and bootstrap sampling~\citep{OsbandVa15}. The prior function is added to each network in the ensemble, trained using L2 regularization. Recently,  
% Epistemic neural networks (ENNs)~\citep{OsbandWenAsDwIbLuRo23} have also been shown to be an effective way of quantifying uncertainty. All these deep learning models are parametrized by some parameter $\eta$. For a given sample $\set{(X_j,Y_j)}_{j \in \mc{I}}$, the model weights $\eta$ are update through gradient descent under a loss function $\loss(.)$, with the update rule expressed as: 
%     $\eta_{\rm new} = \eta - 
%     \sum_{j \in \mc{I}} \nabla_{\eta} \loss( X_j,Y_j,\eta)$. % \label{eqn:ENN-update-naive}



% % \ensembles\citep{LakshminarayananPrBl17}  learn an ensemble of neural networks from given data, trained using L2 regularization, with each network having independently initialized weights. Posterior updates to \ensembles are autodifferentiable through the parametrizations of each  network. 
% % \ensembleplus\citep{OsbandAsCa18} consists of ensembles of neural networks with randomized prior functions combined with bootstrap sampling~\citep{osband2015bootstrapped}. The prior function is added to each network in the ensemble, trained using L2 regularization.  
% % Epistemic neural networks (ENNs)~\citep{OsbandWenAsDwIbLuRo23} have been shown to be an effective way of quantifying uncertainty. %We corroborate these in our benchmarking experiments (see Section~\ref{sec:details-benchmark-UQ}).
% %  ENNs train a network fη(X,Z)f_\eta(X,Z) parameterized by η\eta, where ZZ is a standard Gaussian noise with dimension dzd_z sampled repeatedly during training, where dzd_z is a hyperparamter.
% % For a sample \set(xj,yj)j∈\mcI\set{(x_j,y_j)}_{j \in \mc{I}}, a gradient update of ENN, given a Gaussian sample zz, and a likelihood loss ℓENN\ell_{\texttt{ENN}}, can be written as 
% % \begin{align}
% %     \etanew = \eta - 
% %     \sum_{j \in \mc{I}} \nabla_{\eta} \log \loss_{\texttt{ENN}}\left(f_\eta(x_j, z) \mid x_j; z\right). \label{eqn:ENN-update-naive}
% % \end{align}





% %The intuition of ENNs is that they model \emph{epistemic uncertainy} by learning to disambiguate the noise in the input zz. 
% %\dm{May be we can combine ENNs and Ensembles into one}

% %Additionally,  we also use \textbf{Gaussian processes} as another UQ methodology. GPs are extensively used in Bayesian Optimization literature and are known to work well in low dimensional data and regression setting.



 
% %To make our training pipeline fully autodifferentiable, we further enable differentiable updates of the sampling policy π\pi, parametrized by θ\theta, using soft KK-subset sampling, described in the following. 


%\subsection{Objective}







% At each time step, a  batch of $\batchsize$ samples is queried to improve the objective. The samples are selected from a pool $\xpool$ of size $n$, based on weights ${\bm{w}}(\theta)$. %, learned by the parametrized policy πθ\pi_{\theta}.
% %We then use \batchsize\batchsize-subset sampling to obtain  a batch of size \batchsize\batchsize from the pool (of size nn) using this policy π\pi. 
%  Specifically, given weights $\bm{w} \ge \bm{0}$ and a batch size $\batchsize$, the  $\batchsize$-subset sampling mechanism generates a random vector ${S} \in \{0,1\}^{n}$, whose distribution depends on $\bm{w}$ such that $\sum_{i=1}^n S_i = \batchsize$.
% Let 
% $\bm{e}^j$ denote the $j$-th unit vector, and consider a sequence of unit vectors $[\bm{e}^{i_1} \cdots \bm{e}^{i_k}] \defeq s$. Using a parametrization known as  weighted reservoir sampling ($\wrs$)~\citep{XieEr19}, the probability of  selecting a sequence $s$ is given by:
% \begin{align}
%     p_{\wrs}([\bm{e}^{i_1}, \cdots, \bm{e}^{i_k}]|\bm{w}) \defeq \frac{w_{i_1}}{\sum_{j=1}^n w_j}\frac{w_{i_2}}{\sum_{j=1}^n w_j-w_{i_1}}...\frac{w_{i_k}}{\sum_{j=1}^n w_j-\sum_{j=1}^{k-1}w_{i_j}}.
%     \label{eqn:p-S-w-def}
%  \end {align}

% The probability of selecting a batch $S$ can then be defined as: $ p(S|\bm{w}) \defeq \sum_{s \in \Pi(S)} p_{\wrs}(s|\bm{w}),$
% where $\Pi(S) = \set{s: S = \sum_{j=1}^k s[j]}$ denote all sequences of unit vectors with sum equal to $S$. %This parametrization of $K$-subset sampling .


% % % The action in the above MDP - is to choose a k-sized subset of the pool. However this is 

% % %\subsection{Objectives}

% % %Our framework is agnostic to the choice of model performance metric. Although we consider mean squared error (\texttt{MSE}) for our experiments.  We consider accuracy,  and \texttt{Recall} as our main candidates for $g$. 

% \vspace{-10pt}
% \paragraph{Soft $\batchsize$-subset sampling} 
% %Recall that in Section \ref{sec:k-subset} we parametrized $\batchsize$-subset sampling using weights $\bm w$ over the pool and then used  $\batchsize-$subset sampling to select a batch of samples with size $\batchsize$ from the pool. 
% %However, to differentiate through the pipeline - we need a differentiable batch sampling process.
% %To accomplish this - 


% %Letting
% %$\bm{e}^j$ denote the $j$-th unit vector and
% %$Z=\sum_{i=1}^n w_i$, for a sequence of unit vectors $[\bm{e}^{i_1}, \cdots, \bm{e}^{i_k}]$,
% %we define
% %\begin{align}
% %    p_{\wrs}([\bm{e}^{i_1}, \cdots, \bm{e}^{i_k}]|\bm{w}) &\defeq \frac{w_{i_1}}{Z}\frac{w_{i_2}}{Z-w_{i_1}}...\frac{w_{i_k}}{Z-\sum_{j=1}^{k-1}w_{i_j}}, \\ 
% %    p(S|\bm{w}) &\defeq \sum_{s \in \Pi(S)} p_{\wrs}(s|\bm{w}), \label{eqn:p-S-w-def}
% %\end{align} 
% %where $\Pi(S) = \set{s: S = \sum_{j=1}^k s[j]}$ denote all sequence of unit vectors with sum equal to $S$.


% % The action in the above MDP - is to choose a k-sized subset of the pool. However this is 
% \vspace{-10pt}
% \paragraph{Smoothing the UQ module}

% % To obtain sample pathwise gradient estimator of our objective,



\begin{figure}[t]
\centering
\begin{tikzpicture}
[
roundnode/.style={circle, draw=black, very thick, minimum size=10mm, align=center, text width = 6mm},
roundnode2/.style={circle, draw=black, very thick, minimum size=10mm, align=center, text width = 8mm},
squarednodea/.style={rectangle, draw=black, very thick, minimum size=10mm, align =center,text width = 22mm},
squarednode1/.style={rectangle, draw=black, very thick, minimum size=10mm, align =center,text width = 20.5mm},
squarednode2/.style={rectangle, draw=red, very thick, minimum size=10mm, align =center,text width = 16mm},
squarednode3/.style={rectangle, draw=red, very thick, minimum size=10mm, align =center,text width = 28mm},
squarednode4/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 24mm},
squarednode4a/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 26mm},
squarednode5/.style={rectangle, draw=red, very thick, minimum size=10.5mm, align =center,text width = 22mm},
]
%Nodes
\node[squarednodea]      (maintopic)                              {State $\mu_t$ \\ Policy $\pi_{t,\theta} $\\  $  \xpoolj{t+1}$ \\ Sample $\ybpool^{t+1}$};
\node[squarednode2]        (circle1)       [right=4mm of maintopic] {$\batchsize$-subset sampling};
\node[squarednode1]      (circle2)       [right=4.5mm of circle1] {Select batch $S\defeq\datax^{t+1}$};
\node[squarednode2]        (circle3)       [right=4.5mm of circle2] {Pseudo Posterior update};
\node[roundnode]        (circle4)       [right=5.5mm of circle3] {${\mu}_{+}^S$};
\node[squarednode3]        (circle5)       [right=6mm of circle4] {Estimate $\E \left[ \V_{{\mu}_{+}} (g(f))\right]$};

\node[squarednode4]        (circle1b)       [below=11mm of circle1] {Soft $\batchsize$-subset sampling};
\node[roundnode]      (circle2b)       [right=5.4mm of circle1b] {\centering $a(\theta)$};
\node[squarednode4a]        (circle3b)       [right=7mm of circle2b] {Soft posterior update (differentiable)};
\node[roundnode]        (circle4b)       [right=4mm of circle3b] {${\mu}_{+}^{a(\theta)}$};
\node[squarednode5]        (circle5b)       [right=5mm of circle4b] {Differentiable estimate};


%Lines
\draw[thick, ->, >=stealth] (maintopic.east) --  (circle1.west);
\draw[thick, ->] (circle1.east) --  (circle2.west);
\draw[thick, ->] (circle2.east)  --  (circle3.west);
\draw[thick, ->] (circle3.east) --  (circle4.west);
\draw[thick, ->] (circle4.east) --  (circle5.west);
\draw[thick, -, dashed] (circle1.south) --  (circle1b.north);
\draw[thick, -, dashed] (circle3.south) --  (circle3b.north);
\draw[thick, -, dashed] (circle5.south) --  (circle5b.north);

\draw[thick, ->] (maintopic.south) |-  (circle1b.west);
\draw[thick, ->] (circle1b.east) --  (circle2b.west);
\draw[thick, ->] (circle2b.east)  --  (circle3b.west);
\draw[thick, ->] (circle3b.east) --  (circle4b.west);
\draw[thick, ->] (circle4b.east) --  (circle5b.west);



\end{tikzpicture}
\caption{Differentiable one-step lookahead pipeline for efficient adaptive sampling}
\label{fig:diff_one_step_look_ahead}
\end{figure}





\subsection{Putting everything together}


Note that when using GPs as our UQ methodology, the gradient takes the form $\nabla_{\theta} G\left(\mu_+\left(\bm{a}(\theta)\right)\right)$. Assuming $G(\cdot)$ is differentiable, pathwise gradients can be computed since the posterior update $\mu_+$ is differentiable for GPs, and the sample $\bm{a}(\theta)$ is differentiable with respect to $\theta$.

In the case of deep learning-based UQ methodologies, the gradient has the following form:

\[\nabla_{\theta} G\left( \argmin_{\eta} \loss \left(\eta\left(\bm{a}(\theta)\right)\right) \right) \equiv \nabla_{\theta} G\left( \eta_+\left(\bm{a}(\theta)\right) \right), \] 
%\ym{for some function $\eta_+$?}
%defined in~\eqref{eqn:higher-eta-update} can be challenging,  so we use  the package
where posterior beliefs are parametrized by $\eta$ (e.g., \ensembleplus weights) and $\eta_{+}$ represents the updated posterior parameters (optimized value of $\eta$). 
To compute this, we use $\mathsf{higher}$~\citep{GrefenstetteAmYaHtMoMeKiChCh19} and $\mathsf{torchopt}$~\citep{RenFeLiPaFuMaYa23}, which allows us to approximately differentiate through the argmin. In practice, there is a trade-off between the computational time and the accuracy of the gradient approximation depending on the number of training steps we do to estimate the argmin. 
The differentiable analogue of the  pipeline  is summarized in Figure~\ref{fig:diff_one_step_look_ahead}. We also explain our overall differentiable-algorithm (we call it \ouralgo) graphically in Figure~\ref{fig:Graphical_presentation}. % and  discussed in  detail in Section~\ref{sec:diff-piepline}. 
%In what follows, we empirically demonstrate that through a careful smoothing of the planning objective, it is possible to trade-off bias and variance through auto-differentiation.


\begin{figure}[h]
\centering
\includegraphics[width=1\textwidth, height=9cm]{figures/Diagram_final.pdf}
\caption{Description of \ouralgo ~ algorithm}
\label{fig:Graphical_presentation}
\end{figure}

 
