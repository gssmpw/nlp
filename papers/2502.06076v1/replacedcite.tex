\section{Related work}
\label{sec:related-work}

\paragraph{Active Learning} Adaptive labeling is a classical topic in machine learning.
When applied to model evaluation, our framework posits a less  ambitious goal than the active learning literature____ where labels are adaptively collected  to \emph{improve} model performance in classification problems. Typical active learning heuristics are greedy and select inputs based on uncertainty sampling techniques such as margin-sampling or entropy, or by leveraging disagreements among different models (e.g., query-by-committee and Bayesian Active Learning by Disagreement (BALD)____). 
Batching is a longstanding challenge for these approaches as it requires collecting labels over diverse inputs____. Heuristic criteria based on diversity and density are sometimes incorporated to account for the feature space distribution____,  and recent extensions continue to rely on greedy algorithms (e.g., BatchBALD____, BatchMIG____).

We leverage our limited scope on the efficient estimation of model performance (or ATE), rather than model improvement, 
to formalize a unified and novel computational framework that allows planning for the future.
Our formulation can flexibly handle different objectives and uncertainty quantification methodologies, and seamlessly handle regression and classification problems alike, in contrast to the traditional focus on classification in active learning.
We provide lookahead policies____ that plan for the future, and show they dominate active learning heuristics in Section~\ref{sec:experiment}.

\paragraph{Simulation Optimization}
Bayesian optimization considers black-box functions that are computationally challenging to 
evaluate by modeling the function as a draw from a Gaussian process____. 
The knowledge gradient algorithm____
maximizes the single period expected increase in the black-box function value and can be viewed as a one-step lookahead policy; several authors propose extensions to non-myopic and cost-aware problems____.
We extend these ideas to adaptive labeling  by incorporating batching and combinatorial action spaces, and accommodate more advanced uncertainty quantification methods from deep learning.

Our work is closely related to the  simulation optimization literature____,
 particularly to ranking and selection____.
Our adaptive labeling problem can be viewed as a ranking \& selection or pure-exploration bandit problem. However, in contrast to conventional formulations, our central focus on batching
introduces combinatorial action spaces.
Additionally, our formulation involves continuous covariates and a large number of alternatives,   making it  intractable for standard methods, whose performance deteriorates as the number of alternatives grow larger. 

Instead, we parameterize our discrete optimization problem through a continuous parametrization policy and use gradient-based methods to optimize it. We are able to exploit the similarity between the alternatives through this approximation, which is in contrast to the  traditional simulation optimization literature that mostly considers independent alternatives. 

\paragraph{Gradient Estimation} 

There is extensive literature on gradient estimation____, with applications in probabilistic modeling____ and reinforcement learning____. 
The \textsf{REINFORCE} approach uses the score-function estimator____
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)] = \E_{X \sim \pi_\theta}[G(X) \nabla_\theta\log \pi_\theta (X)]$. While unbiased, these estimators suffer high variance____ and require many tricks to be effective for downstream applications____.
Similarly, finite difference approximations____  and finite perturbation analysis____ also only require zero-th order function evalautions, but perform poorly in high-dimensional problems____.


Alternatively, the reparameterization trick____ 
takes a random variable $Z$ whose distribution does not depend on the parameter of interest $\theta$ and 
exploits the identity
$\nabla_\theta \E_{X \sim \pi_\theta}[G(X)]
= \nabla_\theta \E_{Z}[G(h(Z, \theta))]$ for some function $h$.
 Gradient estimators based on the reparameterization trick 
 typically leads to much smaller
variance____, 
but can only be applied in special cases where $G(h(Z, \theta))$ is differentiable with respect to $\theta$.  
Infinitesimal Perturbation Analysis (IPA)____ 
is a standard framework for constructing such pathwise gradient estimators, but conditions ensuring IPA to be valid____ 
are often restrictive  and does not apply in our setting. 
In fact, in our adaptive labeling problem, the function $G(h(Z, \theta))$ of interest
is non-differentiable and only becomes differentiable in expectation.
To address this challenge, we construct a smooth approximation of the function $G(h(\cdot))$, enabling the application of pathwise gradient estimators by leveraging the known dynamics (posterior updates) of our MDP.

We are inspired  by a line of work on differentiable 
simulators____ with applications in reinforcement learning____.
Theoretical analysis in stochastic optimization____ emphasizes the benefit of first-order estimators over
zeroth-order counterparts, and____ notes first-order estimators
perform well when the objective is sufficiently smooth and continuous. 
In our setting, although the simulation path is not differentiable, we introduce a smooth approximation of the path, enabling the application of first-order gradient estimates.
Our approach provides low-variance gradient estimates at the cost of bias, and  offers a novel way to balance bias and variance for non-differentiable functions____.
Biased gradient estimates have not been studied much in the simulation optimization literature, 
aside from two notable exceptions:____ show the utility of biased estimators when in the neighborhood of a local minimizer, and____ study queueing network control and propose a smoothing scheme for computing biased policy gradient estimators. Focusing on adaptive data collection, we propose a tailored smoothing approach for  handling combinatorial action spaces and (approximate) posterior state transitions.