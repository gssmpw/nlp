


% \section{Graphical Representation of AUTODIFF Algorithm}
% \label{sec:graphical_presentation}
% \begin{figure}[h]
% \centering
% \includegraphics[width=1\textwidth, height=9cm]{figures/Diagram_final.pdf}
% \caption{Description of \ouralgo algorithm}
% \label{fig:Graphical_presentation}
% \end{figure}

 





% \section{Gaussian Process posterior updates} \label{sec:GP}

% For any inputs $\bm{X}$, we assume $f(\bm{X})$ is Gaussian with mean $m(\bm{X})$ and $\cov(f(X_i), f(X_j)) = \mc{K}(X_i,X_j)$. In addition, the observation consists of $(\bm{X},\bm{Y})$ with $\bm{Y} = f\opt(\bm{X}) + \epsilon$ and ${\epsilon} \sim \mc{N}(0, \sigma^2 I)$ for some noise level $\sigma > 0$.
% Given the training data $(\bm{X}, \bm{Y})$ and   test points $X^*$, closed form posterior estimates over $f\opt$ can be computed
% \begin{align*}
% \bm{f}^* \mid \bm{X, Y, X^*} & \sim \mc{N}(\bar{\bm{f}}|^*, \cov(\bm{f}^*)), \\ 
% \text{where }  \bar{\bm{f}}|^* &\defeq \mc{K}(\bm{X}^*, \bm{X}) [\mc{K}(\bm{X},\bm{X}) + \sigma^2 I]^{-1} Y \\ 
% \cov(\bm{f}^*) & \defeq \mc{K}(\bm{X}^*, \bm{X}^*) - \mc{K}(\bm{X}^*, \bm{X}) [\mc{K}(\bm{X},\bm{X}) + \sigma^2 I]^{-1} \mc{K}(\bm{X},\bm{X}^*).
% \end{align*} 
% Using the above expression, we can update our belief  about $\bm{f}^*$ after observing new data.  

