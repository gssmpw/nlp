
%\linenumbers
 
\section{Introduction}
\label{sec:introduction}


Rigorous measurement is the foundation of scientific progress;
ground truth labels/outcomes $Y$ are essential in many applications, such as evaluating the performance of an AI model or estimating the average treatment effect (ATE) of an intervention. 
Since acquiring labels/outcomes is costly, randomly selecting  units $X$ to label can make data collection prohibitively expensive. 
Adaptive labeling can significantly improve the efficiency of data collection,
allowing measurements on more diverse units
and help assess robustness issues (tail risks) in prediction models and impact of interventions on 
underserved communities.

We illustrate this using two representative applications:

 
\begin{example}[Evaluating predictive accuracy under severe selection bias]
When ground truth labels are expensive, available datasets often suffer selection bias. For example, AI-based medical diagnosis systems are typically evaluated using past clinical labels, which are only available for patients who were initially screened by doctors~\citep{Institute03, Seyyed-KalantariZhMcChGh21,StrawWu22,MullainathanOb22,BalachandarGaPi24}.
Naive offline evaluation using existing data leads to unreliable assessment due to the mismatch between the overall population vs. those for whom labels are available, i.e., we cannot assess predictive accuracy on  patients who were never screened in the first place.  When there is overlap between 
the labeled and overall population, 
reweighting (importance sampling) can offer some reprieve. However, such overlap often fails to hold in many applications. Furthermore, 
due to the absence of positive pathological patterns in the available data, previously unseen patient types may never get diagnosed by the AI diagnosis system. Evaluating a model's performance on previously unscreened patients is thus a critical first step toward mitigating this vicious cycle, and can spur engineering innovations that improve the out-of-distribution robustness of AI models. 
\end{example}
    
\begin{figure}[t]
\centering
\includegraphics[scale=0.4]{figures/toy_illustration_regression.png}
\caption{\textbf{Adaptive labeling to reduce epistemic uncertainty over
model performance.}  Among the two clusters of unlabeled examples (left
vs. right), we must learn to prioritize labeling inputs from the left
cluster to better evaluate mean squared error.}
\label{fig:toy_illustration}
\end{figure}



\begin{example}[Estimating average treatment effects (ATE)]
A common goal in natural and social sciences is to estimate the average treatment effect of a treatment $Z=1$ 
compared to a control $Z=0$ across a population characterized by features $X$. When there is minimal overlap between the subset of the population for which observations are available and the population on which the treatment effect is to be estimated, an \emph{active} measurement effort becomes necessary to assess the ATE. The objective in such cases is to sequentially select batches from the population for experimentation.
\end{example}


In this work, we study adaptive labeling as a way to address severe distribution shifts in $X$---between screened vs. unscreend patients or treated vs. control groups. 
Our key contribution lies in the  formulation of a \emph{planning problem} 
for sequentially selecting batches of inputs/population from a large pool
(Section~\ref{sec:formulation}). 
We view rigorous empirical evaluation 
as a first step in 
designing grounded adaptive labeling policies and restrict attention to estimation problems in this paper.  The more ambitious goal of improving AI robustness or personalized treatment policies remains an open problem. 
Recognizing practical constraints such as engineering/organizational overhead or delays in receiving feedback, we consider a few-horizon setting where at
each period, a \emph{batch} of inputs is selected for labeling. 
While a fully online scenario has its conceptual appeal, we believe our few-horizon setting is of particular practical interest since fully online policies are often infeasible to implement even in large online platforms with mature engineering infrastructure~\citep{WuEtAl19, NamkoongDaBa20, AvadhanulaEtAl22}.

Effective labeling requires prioritizing measurements in regions with high
uncertainty that can be reduced with additional samples. In line with the literature, we refer to such uncertainty as  \emph{epistemic} (actionable). In contrast, some uncertainty is \emph{aleatoric} (irreducible), e.g., idiosyncratic measurement noise. 
We develop adaptive policies that
incorporate how beliefs on the estimand of interest (model performance or ATE) get sharper as more labels/outcomes are collected.   We model our current belief on the data generation process $Y|X$ (or $Y|X,Z$ in causal estimation settings) as the current state. After observing a new batch of data and it's corresponding outcomes/labels, we update
our belief on the epistemic uncertainty via a posterior update (``state
transition''). 
Our goal is to minimize the uncertainty over 
the estimand of interest
at the end of label collection, 
as measured by the variance under the
posterior after the final batch is observed. 

Modeling each batch as a time
period, we arrive at a Markov Decision Process (MDP) where states are posterior
beliefs, and actions correspond to 
selecting subsets (batches) from the pool of inputs/population (Figure~\ref{fig:overview}).
 Solving our planning problem 
is computationally challenging due to the
combinatorial action space and continuous state space. To address this, we approximately solve the problem by performing policy gradient updates over ``roll-outs" to minimize the uncertainty over the estimand of interest. Specifically, since the dynamics
of the MDP (posterior updates) are known, we execute ``roll-outs'' as follows: using the
current posterior beliefs we simulate potential outcomes that we may obtain if we label a particular batch,  and evaluate the updated uncertainty on 
the estimand
based on the imagined pseudo labels of this new batch.
We use continuously parameterized policies to select the batch of inputs~\citep{EfraimidisSp06} and then optimize it using policy gradients. 


\begin{figure}[t]
\vspace{-1.5cm}
%\centering \includegraphics[width=\textwidth, height=5.5cm]{figures/diagram_3.png}
\centering \includegraphics[width=\textwidth, height=5.5cm]{figures/Simpler-evolving-MDP.PNG}
%{figures/main_diagram_4.png}
\caption{\textbf{Overview of our adaptive sampling framework.}  At each
period, we select batch of inputs $\mathcal{X}^t$ to be labeled, and obtain a new
labeled data $\mc{D}_t$.  We view posterior beliefs $\mu_t(\cdot)$ on
$f\opt(Y|X)$ (or $f\opt(Y|X,Z)$) as the ``state'', and update it as additional labeled data is
collected. Our goal is to minimize uncertainty on the estimand of interest (performance of predictive model or ATE) at the end of $T$ periods.
}
\label{fig:overview}
\vspace{-.5cm}
\end{figure}

There are various uncertainty quantification (UQ) methodologies in the literature for estimating posteriors.   
Our approach is agnostic to the choice of  UQ methodology, and allows incorporating latest advances in deep 
learning-based UQ methods
(e.g.,~\citep{DuLiXuSpWuRuMa20, OsbandWenAsDwIbLuRo23}). While
Gaussian processes are effective in low-dimensional settings, quantifying epistemic uncertainty for high-dimensional inputs remains an active area of research, including
popular techniques such as Dropout~\citep{GalGh16}, Bayes by Backprop~\citep{Blundell15}, Ensemble+~\citep{LakshminarayananPrBl17,OsbandAsCa18}, and Epistemic Neural Networks~\citep{OsbandWenAsDwIbLuRo23}.
We stress that we do not require the posterior
updates to have a closed form (conjugacy),
and allow approximate posterior
inference using gradient-based optimizers; making it compatible with deep learning-based UQ models (e.g., ensembles) 
that rely on gradient-based posterior updates.

 Our framework supports the integration of various policy gradient methods. The well-known \textsf{REINFORCE} estimator  computes policy gradients using the ``score trick''~\citep{SuttonBa18, DuLiXuSpWuRuMa20}, relying solely on function evaluations.  However, it often suffers from high variance
and can be computationally expensive in practice. To address these limitations, we propose a computationally efficient alternative, $\mathsf{Smoothed\text{-}Autodiff}$. Direct computation of pathwise policy gradients is infeasible because the simulated
``roll-out'' of the adaptive labeling planning problem is not differentiable, owing to the combinatorial nature of the action space.  Differentiability is achieved only after smoothing the sample trajectories using the approximation we introduce.
Leveraging smoothly parameterized policies~\citep{XieEr19},
 we design a smooth approximation of the overall MDP, enabling direct backpropagationâ€”--a method we call $\mathsf{Smoothed\text{-}Autodiff}$.
 We demonstrate our idea with an end-to-end differentiable simulator where we implement all the operations of our planning problem in an \emph{auto-differentiable} framework (Section~\ref{sec:diff-piepline}), including state transitions (posterior updates). Our method allows us to backpropagate the
observed reward to the policy parameters and calculate the \emph{smoothed-pathwise} policy gradient. 

We demonstrate our planning
framework over beliefs formed by Gaussian processes, and Deep Ensembles~\citep{LakshminarayananPrBl17,OsbandAsCa18}. Deep Ensembles leverage the inductive biases of neural
networks in high dimensions
and gradient descent methods to update probabilistic beliefs over
$Y|X$. 
Empirically, we observe that even a single planning step can yield
significant practical benefits (Section~\ref{sec:experiment}). 
On both real and
simulated datasets with severe selection bias, we demonstrate that one-step
lookahead policies based on our planning framework achieve
state-of-the-art performance outperforming
active learning-based heuristics. 

We further observe that $\mathsf{Smoothed\text{-}Autodiff}$
can often outperform \textsf{REINFORCE} policy gradients; 
although our original problem is nonsmooth and discrete,
auto-differentiating over the smoothed MDP can provide reliable policy optimization methods. 
Empirically, $\mathsf{Smoothed\text{-}Autodiff}$ requires 100-1000 times fewer samples to achieve the same level of estimation accuracy as the \textsf{REINFORCE} estimator. We observe the statistical efficiency gain translates to better downstream task performance: even using $\mathsf{Smoothed\text{-}Autodiff}$ over 
a \emph{single} sample trajectory leads to policies that outperform those optimized using the standard \textsf{REINFORCE} estimator.
To better understand this empirical phenomenon, we offer theoretical insights into when the $\mathsf{Smoothed\text{-}Autodiff}$ gradients might outperform \textsf{REINFORCE}-based gradients (Section \ref{sec:gradient_estimation_anlaysis}). In particular, our analysis sheds light on the steep bias-variance trade-off and illustrates when it may be favorable to take a small bias to significantly reduce variance.


Finally, our thorough empirical evaluation highlights several open
research directions   to help scale our framework (Section~\ref{sec:practical_consideration}). 
First, although recent
advances in Bayesian modeling have achieved substantial progress in one-shot
uncertainty quantification, we observe it is often difficult
to maintain accurate beliefs as more data are collected (Section~\ref{sec:eval_posterior_consis}).
Second, we detail
engineering challenges in implementing our auto-differentiation framework over
multi-period roll-outs, highlighting the need for efficient implementations of
high-order auto-differentiation (Section~\ref{sec:auto-diff-bottleneck}). 


 

 





%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
