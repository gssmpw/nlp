
Ground truth labels/outcomes are critical for advancing scientific and engineering applications, e.g., evaluating 
the treatment effect of an intervention or performance of a predictive model. Since randomly sampling inputs for labeling can be prohibitively expensive, we introduce an adaptive labeling framework where measurement effort can be reallocated in batches. 
We formulate this problem as a Markov decision process where
posterior beliefs evolve over time as batches of labels are collected (state transition), 
and batches (actions) are chosen to minimize uncertainty 
at the end of data collection.
We design a computational framework that is agnostic to different uncertainty quantification
approaches including those based on deep learning, and
allows a diverse array of policy gradient approaches by 
relying on continuous policy 
parameterizations.
On real and synthetic datasets, we demonstrate even a one-step lookahead policy can substantially outperform common adaptive labeling heuristics, highlighting the
virtue of planning. 
On the methodological side,
we note that standard $\mathsf{REINFORCE}$-style policy gradient estimators 
can suffer high variance since they rely only on 
zeroth order information. 
We propose a direct backpropagation-based approach, $\mathsf{Smoothed\text{-}Autodiff}$,
based on a carefully smoothed version of the original non-differentiable MDP. Our method enjoys low variance at the price of introducing bias,
and we theoretically and empirically show that this trade-off can be favorable.


% Datasets often suffer severe selection bias; clinical labels are only available on patients for whom doctors ordered medical exams. To assess model performance outside the support of
% available data, we present a computational framework for adaptive labeling,
% providing cost-efficient model evaluations under severe distribution
% shifts. We formulate the problem as a Markov Decision Process over states
% defined by posterior beliefs on model performance. Each batch of new labels
% incurs a “state transition” to sharper beliefs, and we choose batches to
% minimize uncertainty on model performance at the end of the label collection process. Instead of relying on
% high-variance REINFORCE policy gradient estimators that do not scale, our
% adaptive labeling policy is optimized using path-wise policy gradients
% computed by auto-differentiating through simulated roll-outs. Our framework is
% agnostic to different uncertainty quantification approaches and highlights the
% virtue of planning in adaptive labeling. On synthetic and real datasets, we
% empirically demonstrate even a one-step lookahead policy substantially
% outperforms active learning-inspired heuristics. {In addition, we offer theoretical and empirical insights into the differences between smoothed pathwise gradients and REINFORCE gradients. Our framework generalizes to other settings, such as the efficient estimation of the average treatment effect (ATE) in causal inference. Based on empirical evaluations and the insights needed to scale our framework, we also propose compelling directions for future research.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
