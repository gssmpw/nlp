
 
%The chosen hyperparameters for different experiments are available in our code base. 





%
% For Figure~\ref{fig:task-1-joint-ood-all-data},  we train different methodologies for 5000 iterations for eICU-Linear Bias dataset. For eICU-clustering bias dataset and synthetic dataset we train methodologies for 200 iterations as training dataset is now of smaller size. The results presented are average over 10 different inference and test batch seeds. 


% For syenthetic datasets,
% we perform hyperparameter sweeps over the following sets for each agent:
% \begin{itemize}
%     \item For \textbf{mlp}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, and L2 weight decay in $\{10\}$.
%     \item For \textbf{dropout}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, length scale in $\{0.1\}$, and dropout rate in $\{0.2\}$.
%     \item For \textbf{ensemble}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, and L2 weight decay in $\{0.01\}$.
%     \item For \textbf{ensemble+}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, L2 weight decay in $\{0.01\}$, and prior scale in $\{30\}$.
%     \item For \textbf{hypermodel}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, L2 weight decay in $\{0.01\}$, and prior scale in $\{100\}$.
%     \item For \textbf{epinet}, we sweep over learning rate in $\{0.001\}$, training iterations in $\{20, 50, 100, 200\}$, L2 weight decay in $\{0.1\}$, and prior scale in $\{0.1\}$.
% \end{itemize}

%\ym{we need more parameters sweep}


% \subsection{The hyperparamters we choose}

% %\dm{How many seeds in the Figure 23 and 21}
% \dm{How many seeds for the other graphs}
%\dm{Mention that this standard deviation will go away if we can compute in the closed form}

%\subsection{Details  for synthetic data}



 


 