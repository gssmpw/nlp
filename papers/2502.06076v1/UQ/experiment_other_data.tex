
\subsubsection{Comparison with a Gaussian oracle}
In synthetic data generation setting we compare different UQ methodologies to the  Gaussian process oracle baseline with access to the parameters of the the data generation process (which is also an GP).  Figure~\ref{fig:task-1-joint-ood-all-data-with-oracle} compares  different UQ methodolgies and the GP oracle (the setting is same to that in Figure~\ref{fig:task-1-joint-ood-all-data}).
From Figure~\ref{fig:task-1-joint-ood-all-data-with-oracle}, we observe that UQ methodology ``GP'', which is the oracle, performs better for a large $k$, and for small $k$ values, many other UQ methodlogies are quite similar in performance. 

\begin{figure}
\centering 
\centering \includegraphics[height=4.5cm]{figures/GP_dynamic_0/Task_1_kl_estimate_dyadic_tau_10_ood_vs._k_val_for_num_batches=250_dynamic_0_with_oracle.pdf}

\centering{\small{ Synthetic Data (Clustering bias)}}
\caption{Joint log-loss on OOD data with increasing selection bias with a GP oracle.}
\label{fig:task-1-joint-ood-all-data-with-oracle}
\end{figure}


\subsubsection{Hyperparameter tuning breaks under distribution shifts}

\label{sec:hyperparameter_tuning_breaks_under_dis_shifts_experiment_simple}

Figure~\ref{fig:difficult_to_choose_weight_decay} demonstrates that increasing the weight decay deteriorates ID performance while first improving OOD performance and subsequently deteriorating.  
Similarly, Figure~\ref{fig:difficult_to_choose_stopping_time} shows that as number of iterations increases, ID performance improves on the test set.  On the other hand, test OOD performance first improves with increasing training iterations but later deteriorates for \ensembleplus and Hypermodels.




\begin{figure}[h]
\centering
\begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_weight_decay_ensemble+_with_500_training_iterations.pdf}

\centering {\small{\ensembleplus}}
%\label{fig:ensemble+_difficult_to_choose_weight_decay}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_weight_decay_epinet_with_500_training_iterations.pdf}
\centering {\small{Epinet}}
%\label{fig:epinet_difficult_to_choose_weight_decay}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_weight_decay_hypermodel_with_500_training_iterations.pdf}
\centering {\small{Hypermodel}}
%\label{fig:hypermodel_difficult_to_choose_weight_decay}
\end{minipage}
\caption{Trade-off between  in-distribution (ID) and out-of-distribution (OOD) performance with weight decay as the hyperparameter (eICU, Clustering bias). }
\label{fig:difficult_to_choose_weight_decay}
\end{figure}




    






\begin{figure}[h]
\centering
\begin{minipage}[b]{0.31\textwidth}
\centering
\includegraphics[height=4cm, width=\textwidth]{figures/eicu_clustering/Task_1_ood_id_ensemble+_Metric_vs._Number_of_Batches_for_k_val=30_dynamic_0.pdf}
\centering {\small{\ensembleplus}}
\label{fig:ensemble+_difficult_to_choose_stopping_time}
\end{minipage}
\hfill
\begin{minipage}[b]{0.31\textwidth}
\centering \includegraphics[height=4cm, width=\textwidth]{figures/eicu_clustering/Task_1_ood_id_epinet_Metric_vs._Number_of_Batches_for_k_val=30_dynamic_0.pdf}
\centering {\small{Epinet}}
\label{fig:epinet_difficult_to_choose_stopping_time}
\end{minipage}
\hfill
\begin{minipage}[b]{0.31\textwidth}
\centering \includegraphics[height=4cm, width=\textwidth]{figures/eicu_clustering/Task_1_ood_id_hypermodel_Metric_vs._Number_of_Batches_for_k_val=30_dynamic_0.pdf}
\centering {\small{Hypermodel}}
\label{fig:hypermodel_difficult_to_choose_stopping_time}
\end{minipage}
\caption{Trade-off between  in-distribution (ID) performance and out-of-distribution (OOD) performance with stopping time as the hyperparameter (eICU, clustering).}
\label{fig:difficult_to_choose_stopping_time}
\end{figure}
    


\subsubsection{Dynamic settings for   other datasets}


To further examine our findings in Figure~\ref{fig:dynamic_setting_k_30}, we conduct similar experiments using clustering bias as described in Section~\ref{sec:selection-bias} for all datasets in Section~\ref{sec:UQ_BENCH_datasets}.
We summarize our results in Figure~\ref{fig:dynamic_setting_IEEECIS}
for \ieeecis, Figure~\ref{fig:dynamic_setting_ccfraud}
for \ccfraud, Figure~\ref{fig:dynamic_setting_fraudecom}
for \fraudecom,  Figure~\ref{fig:dynamic_setting_vehicleloan}
for \vehicleloan,  Figure~\ref{fig:dynamic_setting_ACS_employment}
for \acsemployment, and 
Figure~\ref{fig:dynamic_setting_ACSincome_NY_}
for \acsincome. 
In summary, our results are consistent with the results from Figure~\ref{fig:dynamic_setting_k_30}. 
Interestingly, we observe some UQ agents can actually have a larger joint log-loss after adapting to new data.
 





\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/IEEECIS_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/IEEECIS_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/IEEECIS_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/IEEECIS_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }} \end{minipage}
\caption{Performance of different UQ modules in a dynamic setting for \ieeecis.
Although Hypermodels perform the best in the ID setting (a), they perform the worst in the OOD setting at $T=0$. However, we see that Hypermodels are good at adapting to new data for this dataset and quickly improve the performance as shown in plot (d).
}
\label{fig:dynamic_setting_IEEECIS}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ccfraud_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }}  
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ccfraud_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ccfraud_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T= 1$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ccfraud_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }} 
\end{minipage}
\caption{Performance of different UQ modules in a dynamic setting for \ccfraud. 
In this case hypermodels perform the best for both the ID setting (a) and the OOD setting (b) at $T=0$. However, we can see that the OOD performance improvement (d) is the best for the epinets, which again showcases the trade-off between having sharper posteriors and the performance on the  OOD data.}
\label{fig:dynamic_setting_ccfraud}
\end{figure}


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/fraudecom_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/fraudecom_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/fraudecom_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T=1$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/fraudecom_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }} 
\end{minipage}
\caption{Performance of different agents in a dynamic setting for \fraudecom. Surprisingly, we observe that all three models have a larger OOD joint log-loss after seeing new data. The reason for such a behaviour might be that the models start from a prior belief with lower level of uncertainty and as they see additional data they become more uncertain. }
\label{fig:dynamic_setting_fraudecom}
\end{figure}

\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth]{figures/new_data/vehicleloan_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/vehicleloan_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }} 
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth]{figures/new_data/vehicleloan_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/vehicleloan_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of different agents in a dynamic setting for \vehicleloan.
Although hypermodels perform the best in the ID setting (a), they perform the worst in the OOD setting at $T=0$. However, we see that hypermodels are good at adapting to new data for this dataset and quickly improve the performance as shown in plot (d). In addition, hypermodels are the only model that have a lower joint log-loss after seeing new data.
}
\label{fig:dynamic_setting_vehicleloan}
\end{figure}





\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSEmployment_NY_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSEmployment_NY_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSEmployment_NY_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T= 1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSEmployment_NY_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of different agents in a dynamic setting for \acsemployment. In this case hypermodels perform the best in both the ID setting (a) but in the OOD setting [(b),(c), and (d)], all the models perform similarly at $T=0$ and $T=1$.}
\label{fig:dynamic_setting_ACS_employment}
\end{figure}



\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSincome_NY_in_distribution_mean_T0.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSincome_NY_out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSincome_NY_out_of_distribution_mean_T50.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=2.5cm]{figures/new_data/ACSincome_NY_out_of_distribution_mean_difference.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of different agents in a dynamic setting for \acsincome.
Although hypermodels perform the best in the ID setting (a), they perform the worst in the OOD setting at $T=0$. However, we see that hypermodels are good at adapting to new data for this dataset and quickly improve the performance as shown in plot (d). In addition, hypermodels are the only model that have a lower joint log-loss after seeing new data.
}
\label{fig:dynamic_setting_ACSincome_NY_}
\end{figure}


\subsubsection{Hyperaprameter tuning suffers in dynamic settings}
As referred to in Section~\ref{sec:eval_posterior_consis}, hyperparameter tuning suffers in dynamic settings as well. 
In  Figures~\ref{fig:Ensemble+_weight_decay} 
to~\ref{fig:Hypermodel_prior_scale}, we demonstrate this for \ensembleplus, Epinets, and Hypermodels with hyperparameters weight decay and prior scale. We consider the eICU dataset with clustering bias in a dynamic setting for these experiments.
 Figure \ref{fig:Ensemble+_weight_decay}  shows that at $T=0$, weight decay that works the best for the \ensembleplus for the in-distribution data becomes worse related to other values for the 
out-of-distribution data, which  signifies the trade-off between in-distribution performance and out-of-distribution performance. 
Figure~\ref{fig:Ensemble+_weight_decay} (d) also shows  the performance improvement after acquiring new data.
As we can see, the weight decay for which the OOD performance was the best has the least performance improvement.
This shows the trade-off between having sharp posteriors and the OOD performance when we optimize for the hyperparameter weight decay. 
Similarly, we can see for the other models (Hypermodels and Epinets) this trade-off 
for the weight decay and prior scale hyperparameter.
We can also see that in some settings, performance even gets deteriorated (see Figures~\ref{fig:Epinet_weight_decay} 
and~\ref{fig:Epinet_prior_scale}) as we acquire more data points. 
The performance deterioration might be due to the following reason: UQ methodologies starts from a prior belief with lower level of the uncertainty and as the agents acquire more data at ($T=1$),
they become more uncertain about the outcomes as compared to the previous status ($T=0$). 
 
 %Experiments shown are for the eICU dataset with clustering bias.
% this measures how sharp posterior updates are for each of the models as it sees more data points. As we can see, while epinets were performing better on the OOD data -- their performance improvement (posterior update) on OOD data is not as sharp as the hypermodels and ensemble$+$. 


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_id_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(a)} ID performance ($T=0 $) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_ood_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_ood_dynamic_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_kl_estimate_dyadic_tau_10__diff_log_weight_decay.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of ensemble $+$ in a dynamic setting with varying weight decays [eICU data with clustering bias].
We can see the weight decay with values from $1e-4$ to $1e-2$ is the  best for the in-distribution data performance [plot (a)], while weight decay of $10$ is the best for the OOD data performance [plot (b)] at $T=0$. 
This again underlines the trade-off between the in-distribution performance and out-of-distribution performance. In plot (d), we can see that the performance improvement is maximum for the weight decay from $1e-4$ to $1e-2$ while it is the least for the weight decay of $10$, showcasing the trade-off between sharper posteriors and OOD performance. 
}
\label{fig:Ensemble+_weight_decay}
\end{figure}






\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_id_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_ood_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_ood_dynamic_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_kl_estimate_dyadic_tau_10__diff_log_weight_decay.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of epinets in a dynamic setting with varying weight decays [eICU data with clustering bias].
We can see the weight decay of $1e-4$ is the best for the in-distribution data performance [plot (a)], while  weight decay of $0.1$ is the best for the OOD data performance [plot (b)] at $T=0$.  
In plot (d), we can see that the performance improves only for the weight decay of $1.0$, while for other values it deteriorates even after acquiring the new data. 
}
\label{fig:Epinet_weight_decay}
\end{figure}




\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_id_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_ood_static_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_ood_dynamic_kl_estimate_dyadic_tau_10__log_weight_decay.pdf}
{\small{{(c)} OOD performance ($T= 1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=3.25cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_kl_estimate_dyadic_tau_10__diff_log_weight_decay.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of hypermodels in a dynamic setting with varying weight decay [eICU data with clustering bias].
We can see the weight decay with values from $1e-4$ to $1e-2$ is the best for the in-distribution data performance [plot (a)], while weight decay of $10$ to $100$ is best for the OOD data performance [plot (b)] at $T=0$, underlying the trade-off between the in-distribution performance and out-of-distribution performance. In plot (d), we can see that the performance improvement is maximum for the weight decays taking values from $1e-4$ to $1e-2$, while it is the least for the weight decay with values from $10$ to $100$,  showcasing the trade-off between sharper posteriors and OOD performance. 
}
\label{fig:Hypermodel_weight_decay}
\end{figure}




\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height=4.0cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_id_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=4.0cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_ood_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=4.0cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_ood_dynamic_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(c)} OOD performance ($T= 1$) }}\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height=4.0cm]{figures/dynamic_hyperparameter/Experiment_4_ensemble+_500_kl_estimate_dyadic_tau_10__diff_log_prior.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of ensemble $+$ in a dynamic setting with varying prior scale [eICU data with clustering bias]. We can see that the prior scale $0.1$ to $1$ is  the best for the in-distribution data performance [plot (a)]. 
However, prior scale of $10$ is the best for the OOD data performance [plot (b)] at $T=0$, underlying the trade-off between the in-distribution performance and out-of-distribution performance. In plot (d), we can see that the performance improvement is maximum for the prior scales  of values from $0.1$ to $1$, while it the least for the prior scale of $10$, showcasing the trade-off between sharper posteriors and OOD performance. 
}
\label{fig:Ensemble+_prior_scale}
\end{figure}




\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_id_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_ood_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(b)} OOD performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_ood_dynamic_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(c)} OOD performance ($T=1$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_epinet_500_kl_estimate_dyadic_tau_10__diff_log_prior.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of epinets in a dynamic setting with varying prior scale [eICU data with clustering bias]. We can see the prior scale of $10$ to $30$ works best for both the in-distribution data performance [plot (a)]  the OOD data performance [plot (b)] at $T=0$. However, in plot (d), we can see that the OOD performance improvement is maximum for the prior scale of $1e-3$. 
On the other hand, for other prior scales improvement is marginal or sometimes even deteriorates.
}
\label{fig:Epinet_prior_scale}
\end{figure}



\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_id_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_ood_static_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(b)} OOD performance ($T=0$) } }
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_ood_dynamic_kl_estimate_dyadic_tau_10__prior_scale.pdf}
{\small{{(c)} OOD performance ($T=1$)}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth, height = 4.0cm]{figures/dynamic_hyperparameter/Experiment_4_hypermodel_500_kl_estimate_dyadic_tau_10__diff_log_prior.pdf}
{\small{{(d)} OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of Hypermodels in a dynamic setting with varying prior scale [eICU data with clustering bias]. We can see the prior scale $100$ is best for the in-distribution data performance [plot (a)] while, prior scale of $10$ is the best for the OOD data performance [plot (b)] at $T=0$. In plot (d), we can see that the performance improvement is maximum for the prior scale of $1$, while it is least for prior scale of $10$, showcasing the trade-off between sharper posteriors and OOD performance. 
}
\label{fig:Hypermodel_prior_scale}
\end{figure}






\subsubsection{Sensitivity to inference seeds and early stopping time}

In Figures~\ref{fig:eicu-clustering-task-4-k-0} to~\ref{fig:eicu-clustering-task-2-tau10-ood}, we demonstrate the effect of early stopping and random seeds on the posterior inference of different 
 UQ methodologies. 
 We conduct these experiments for the eICU dataset with clustering bias. 
 In Figures~\ref{fig:eicu-clustering-task-4-k-0} to~\ref{fig:eicu-clustering-task-2-tau10-ood}, 
 we   early stop the training of different UQ methodologies and see its impact on the performance on in-distribution data and out-of-distribution data. Figure~\ref{fig:eicu-clustering-task-4-k-0} demonstrates the effect of early stopping on the in-distribution performance. 
 We can see that the performance deteriorates. Similar deterioration can be seen on the out-of-distribution performance with early stopping (Figure~\ref{fig:eicu-clustering-task-4-k-0-ood}).
To measure the sensitivity of different methodologies to the random inference seeds, we take the variance of inference (joint log-loss) across 10 different seeds - within each seed we sample 20 instances of the posterior models and evaluate it on 100 test samples.
 In Figure~\ref{fig:eicu-clustering-task-2-tau10-id} we showcase the sensitivity of the inference on in-distribution data due to different random seeds for different methodologies. 
 We can see that as we stop earlier, the sensitivity (standard deviation) of the inference (joint log-loss) increases for the in-distribution. 
 Moreover, the sensitivity is larger for \ensembleplus, Epinets and Hypermodels as compared to other methodologies. 
 An important thing to note here is that for Ensemble and \ensembleplus models, we had $100$ particles and in principle we can derive the posterior exactly without any Monte Carlo approximation and then the standard deviation will reduce to 0 (same as MLP) for these models. However, in current experiments, as specified earlier, we sample $20$ instances of the posterior from these $100$ particles. For agents such as Hypermodels and Epinets, we have to do   Monte Carlo approximation as the reference distributions for index $z$ follows a Gaussian distribution.  
 Figure~\ref{fig:eicu-clustering-task-2-tau10-ood} reports the sensitivity of the inference on out-of-distribution data due to different random seeds for different methodologies. 
 In this case, as we train more, sensitivity increases for some models such as hypermodels and ensemble $+$.
 This seems to be an effect of the performance deterioration on the OOD data as we saw earlier in Figure~\ref{fig:difficult_to_choose_stopping_time} in Section~\ref{sec:hyperparameter_tuning_breaks_under_dis_shifts_experiment_simple},
 which manifests itself in larger variance across different seeds as well.

%Sensitivity to stopping time out-distribution



  
\begin{figure}[h]
\centering
\begin{minipage}[b]{0.40\textwidth}
\centering
\includegraphics[width=\textwidth, height=5cm]{figures/eicu_clustering_task_4/Task_4,joint_log-loss_id_vs_training_iteration_at_k_=30.pdf}
\caption{Performance of UQ methodologies deteriorates with early stopping - Joint log-loss for in-distribution (ID) data - (eICU data with clustering bias)}
\label{fig:eicu-clustering-task-4-k-0}
\end{minipage}
\hfill
\begin{minipage}[b]{0.40\textwidth}
\centering
\includegraphics[width=\textwidth, height=5cm]{figures/eicu_clustering_task_2/Task_2_Variance_of_kl_estimate_dyadic_tau_10_id_training_iteration.pdf}
\caption{Sensitivity of UQ methodologies to random inference seeds - Standard deviation of joint log-loss for in-distribution (ID) data - (eICU data with clustering bias)}
\label{fig:eicu-clustering-task-2-tau10-id}
\end{minipage}
\end{figure} 


\begin{figure}[h]
\centering
\begin{minipage}[b]{0.40\textwidth}
\centering
\includegraphics[width=\textwidth, height=5cm]{figures/eicu_clustering_task_4/Task_4,joint_log-loss_ood_vs_training_iteration_at_k_=30.pdf}
\caption{Performance of UQ methodologies deteriorates with early stopping - Joint log-loss for out-of-distribution (OOD) data  - (eICU data with clustering bias)}
\label{fig:eicu-clustering-task-4-k-0-ood}
\end{minipage}
\hfill
\begin{minipage}[b]{0.40\textwidth}
\centering \includegraphics[width=\textwidth, height=5cm]{figures/eicu_clustering_task_2/Task_2_Variance_of_kl_estimate_dyadic_tau_10_ood_training_iteration.pdf}
\caption{Sensitivity of UQ methodologies to random inference seeds - standard deviation of Joint log-loss for out-of-distribution (OOD) data - (eICU data with clustering bias)}
\label{fig:eicu-clustering-task-2-tau10-ood}
\end{minipage}
\end{figure} 

% \begin{figure}
% \centering
% \begin{minipage}[b]{0.49\textwidth}
% \centering
% \includegraphics[width=\textwidth, height=6cm]{figures/eicu_clustering_task_4/Task_4,joint log-loss_ood_vs_training_iteration_at_k_= 30.pdf}
% \caption{Performance of UQ methodologies deteriorates with early stopping (Joint log-loss for out-of-distribution data) for eicu clustering}
% \label{fig:eicu-clustering-task-4-k-0}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.49\textwidth}
% \centering \includegraphics[width=\textwidth, height=6cm]{figures/eicu_clustering_task_4/Task_4,marginal log-loss_ood_vs_training_iteration_at_k_= 30.pdf}
% \caption{Performance of UQ methodologies deteriorates with early stopping (Marginal log-loss for out-of-distribution data) for  eicu clustering}
% \label{fig:eicu-clustering-task-4-k-1}
% \end{minipage}
% \end{figure}
 



% \begin{figure}
% \centering
% \begin{minipage}[b]{0.49\textwidth}
% \centering
% \includegraphics[width=\textwidth, height=6cm]{figures/eicu_simple/Task_4,kl_estimate_dyadic_tau_10_ood_vs_training_iteration_at_k_= 4.pdf}
% \caption{Performance of UQ methodologies deteriorates with early stopping (Joint log-loss for out-of-distribution data)}
% \label{fig:eicu-simple-task-4-k-0}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.49\textwidth}
% \centering \includegraphics[width=\textwidth, height=6cm]{figures/eicu_simple/Task_4,kl_estimate_uniform_tau_1_ood_vs_training_iteration_at_k_= 4.pdf}
% \caption{Performance of UQ methodologies deteriorates with early stopping (Marginal log-loss for out-of-distribution data)}
% \label{fig:eicu-simple-task-4-k-1}
% \end{minipage}
% \end{figure}


%     \begin{figure}
% \centering
% \begin{minipage}[b]{0.32\textwidth}
% \centering
% \includegraphics[height=4.5cm]{figures/eicu_simple/Task_1_kl_estimate_uniform_tau_1_ood_vs_k_val_for_all_agents.pdf}
% \caption{Marginal log-loss on out-of-distribution data with increasing selection bias in the training data (eICU - linear selection bias).  }
% \label{fig:eicu-simple-task-1-marginal-ood}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.32\textwidth}
% \centering
% \includegraphics[height=4.5cm]{figures/eicu_clustering/Task_1_kl_estimate_uniform_tau_1_ood_vs._k_val_for_num_batches=200_dynamic_0.pdf}
% \caption{Marginal log-loss on out-of-distribution data with increasing selection bias in the training data (eICU - clustering).  }
% \label{fig:eicu-clustering-task-1-marginal-ood}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.32\textwidth}
% \centering \includegraphics[height=4.5cm]{figures/GP_dynamic_0/Task_1_kl_estimate_uniform_tau_1_ood_vs._k_val_for_num_batches=250_dynamic_0.pdf}
% \caption{Marginal log-loss on out-of-distribution data with increasing selection bias in the training data (synthetic data setting). }
% \label{fig:gp-synthetic-task-1-joint-ood}
% \end{minipage}
% \caption{General}
% \end{figure}


% \begin{figure}
% \centering
% \begin{minipage}[b]{0.49\textwidth}
% \centering
% \includegraphics[width=\textwidth, height=6cm]{figures/eicu_simple/Task_2,Variance_of_kl_estimate_dyadic_tau_10_cd_vs_valfor_training_iteration_1000.pdf}
% \caption{Sensitivity of UQ methodologies to random inference seeds}
% \label{fig:eicu-simple-task-2-tau10-cd}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.49\textwidth}
% \centering \includegraphics[width=\textwidth, height=6cm]{figures/eicu_simple/Task_2,Variance_of_kl_estimate_dyadic_tau_10_cd_vs_valfor_training_iteration_500.pdf}
% \caption{Sensitivity of UQ methodologies to random inference seeds}
% \label{fig:eicu-simple-task-1-tau10-cd}
% \end{minipage}
% \end{figure} 


% \newpage
% \section{Questions pending}
% \textcolor{red}{@EVERYONE : Should we have the results for gradient estimators for different $\theta$ and settings in the Appendix and reference it here?}


% \textcolor{red}{We will show here problem setting 1 and relegate other settings to the appendix}
% \begin{itemize}
%     \item Different problem settings - 1 problem setting. (increasing complexity of the problem)
%     \item Different $\theta$'s
%     \item Average over multiple cosine-similarity estimates
% \end{itemize}