
\section{Experimental details (Section \ref{sec:practical_consideration})}
\label{sec:practical_consideration_experiment_details}


%This is important as if the UQ module is highly sensitive to these than posteriors from the UQ methodologies can not be relied upon for the downstream tasks.






% While Gaussian processes works well in low dimensions,  quantifying epistemic uncertainty on f\opt(Y|X)f\opt(Y|X) over high dimensional inputs is an active area of research, including
% popular techniques such as dropout~\citep{GalGh16}, Bayes by Backprop~\citep{BlundellCoKaWi15}, Ensembles/Ensemble++~\citep{LakshminarayananPrBl17,OsbandAsCa18}, and Epistemic Neural Networks \citep{OsbandEtAl23}


% In many high-stakes applications  accurate uncertainty quantification is necessary for making reliable decisions. 
\begin{comment}
 Recently, ~\citet{OsbandEtAl23} showed that for a UQ methodology to be useful for decision making, it should capture the epistemic uncertainty well. Epistemic uncertainty is the uncertainty that  can  be resolved  with more data as opposed to the aleatoric uncertainty which is due to idiosyncratic noise in the outcome measurement process~\citep{,}. 
 
 In this regard, 
the most common metric used for assessing the uncertainty quantification is the marginal likelihood. Traditionally, marginal likelihood of for predicting one test example, \E[logË†P(Y|X)]\E[\log \hat{P}(Y|X)], was used as an metric to assess the model performance~\citep{,}. 

However, ~\citet{OsbandEtAl23} showed that considering this metric  fails to asses the UQ methodology's ability to capture the epistemic uncertainty. Further \citet{OsbandEtAl23} showed that marginal likelihood for predicting a  batch of test examples, $\E[\log P(Y_{1:\tau}|X_{1:\tau})]\E[\log \hat{P}(Y_{1:\tau}|X_{1:\tau})]$, is a better metric to assess UQ methodology's ability to capture the epistemic uncertainty. 

~\citet{} referred to former as \textit{marginal log-loss} and latter as \textit{joint log-loss} respectively. 

For high dimensional data, $\tau$ needed to capture this phenomenon might be very large and computationally expensive.~\citet{} introduced joint log-loss with dyadic sampling to overcome this computational challenge.~\citet{} introduced posterior predictive correlations (PPCs) as a metric to assess the UQ methodologies in the regression setting,  which in spirit tries to capture a notion similar to joint log loss.  We use both the marginal likelihood for predicting one test example (\textit{marginal log-loss}) and a batch of test example (\textit{joint log-loss}) in our benchmark. 
\end{comment}

% \begin{center}
%     \begin{tabular}{|c|c|}
%     \hline
%     \textbf{Ability to induce right priors} &      1. Performance on OOD distribution \\
%     & 2. Difficulty of hyperparameter tuning  \\ \hline
%   \textbf{Posterior Consistency} & 1. Sensitivity to early stopping \\ & 2. Sensitivity to seeds (inference) \\ \hline
% \end{tabular}
% \end{center}



%Existing UQ benchmarks use marginal likelihood  to evaluate the quality of different UQ methodologies. Recently, joint log-loss was introduced by~\citet{OsbandEtAl23}, and was shown to be an effective metric distinguishing UQ agents. Motivated by~\citep{OsbandEtAl23},we propose a new metric that we call posterior consistency, which we detail in Section~??????\ref{sec:eval-pos-cons}, that is built on the notion of joint log-loss and consider its variation when one perform Monte Carlo sampling to estimate it.We also construct practical selection bias or distribution shifts settings and compare different UQ agents under those settings -- where each sample will be included in the initial data with a certain probability and there are several clusters. As emphasized by prior literature~\citep{LakshminarayananPrBl17,OvadiaFeReNa19,LuOsAsGoDwWeRo22}, it is important to ensure the UQ module perform well under distribution shifts.Further, we consider a dynamic setting giving the modeler access of new labels to examine how new data can help UQ agent sharpen belief. In addition, we have two settings covering real data and synthetic data and for the synthetic data we consider an oracle UQ module to fairly evaluate UQ modules.

%   Differences compared to past literature/ new ideas of our paper
% Out-of-support/Distance
% Dynamic setting
% Posterior Consistency as we discussed before
% Comparison with oracle baselines [They do not compare it with oracle (best agent that we can get) performance, Out-of-support, consistent posterior] oracle 
% Importance: maybe all UQ modules are bad, which needs to be confirmed using an oracle




% Literature on marginal likelihood in bayesian inference
% Joint log loss 
% Evaluating High-Order Predictive Distributions in Deep Learning
% From Predictions to Decisions: The Importance of Joint Predictive Distributions
% UQ methodologies 
% ENN
% HYPERMODELS FOR EXPLORATION
% Ensembles for Uncertainty Estimation: Benefits of Prior Functions and Bootstrapping
% Ensemble, Ensemble+, BBB, 
% Neural testbed -  benchmarked it on in-distribution  Neural Testbed
% Epinets are robust to out-distributions https://arxiv.org/pdf/2207.00137






% There is a rich literature around uncertainty estimation in deep learning. Much of this work
% has focused on agent development, with a wide variety of approaches including variational
% inference (Blundell et al., 2015), dropout (Gal and Ghahramani, 2016), ensembles (Osband
% and Van Roy, 2015; Lakshminarayanan et al., 2017), and MCMC (Welling and Teh, 2011;
% Hoffman et al., 2014). However, even when approaches become popular within particular
% research communities, there are still significant disagreements over the quality of the resultant
% uncertainty estimates (Osband, 2016; Hron et al., 2017).
% Bayesian deep learning has largely relied on benchmark problems to guide agent development
% and measure agent progress. These typically include classic deep learning datasets but
% supplement the usual goal of classification accuracy to include an evaluation of the probablistic
% predictions via negative log likelihood (NLL) and expected calibration error (ECE) (Nado
% et al., 2021). More recently, several efforts have been made to supplement these datasets
% with challenges tailored towards Bayesian deep learning, and explicit Bayesian inference
% (Wilson et al., 2021). This literature has largely focused on evaluating marginal predictions,
% paired with evaluation on downstream tasks (Riquelme et al., 2018). Our work is motivated
% by the importance of joint predictions in driving good performance in sequential decisions
% (Wen et al., 2022). We share motivation with the work of Wang et al. (2021), but show that
% directly measuring joint likelihoods can provide new information beyond marginals. Follow
% up work has built upon the research in our paper, to extend the analysis of joint distributions
% to higher-order joint distributions, and empirical datasets (Osband et al., 2022).






% It is important to evaluate the UQ quality over distributions $\xeval$
% that is very different compared to $\xtrain$.



% Why is it needed?
% If performance under different seeds is different seeds is different - that means UQ is not reliable

% Early stopping - limited computational resources - again for reliable UQ performance

% How do we measure it? variance of  g(UQ-state) across N different batch of seeds and early stoppings (see if early stopping causes the metric to change a lot) [first compute this for one point, then average this over a set of points/  $\xeval$ ]



% In Section~\ref{sec:experiment}, we emprically demonstrate that ceratin UQ modules perform well on distributions that are similar to $\xtrain$ but it deteriorates by a large amount as $\dist{\xeval}{\xtrain}$ increases. Moreover, this is also the case when the analysis is using the UQ module to perform active exploration.


