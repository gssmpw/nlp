Since ML models are always in operation, they often encounter inputs unseen during training. Uncertainty quantification (UQ) methods aim to capture when models are unreliably extrapolating, a foundational goal in trustworthy AI, active learning, and exploration in reinforcement learning. These downstream goals necessitate  i) reliable UQ performance on out-of-distribution (OOD) inputs, and ii) the ability to reliably sharpen beliefs as more data is gathered (which we term “posterior consistency”). We present \textsf{UQBench}, a comprehensive benchmarking framework that can evaluate UQ algorithms over distribution shifts and posterior consistency, and instantiate it on several real-world datasets. Our benchmark highlights several methodological challenges that are unobservable on existing UQ benchmarks that solely focus on static in-distribution (ID) settings. Due to tremendous variation in UQ quality across hyperparameters, we observe a direct tension between ID and OOD performance. These challenges magnify in the dynamic setting where posteriors are updated based on new observations. Our benchmark underscores the need for reliable posterior update methods, an often overlooked dimension in UQ algorithmic development focusing on static performance. 

% Uncertainty quantification (UQ) is important for many downstream tasks beyond supervised learning, such as active learning, and exploration in reinforcement learning. To achieve desired performance, UQ methodologies need to enable reliable priors, characterize uncertainty in  out-of-support data and update consistently  as new data is observed. We show existing UQ benchmarks fail to fairly evaluate the UQ methodologies on these aspects, which we call “posterior consistency”. To reliably estimate and benchmark different UQ methodologies on these dimensions, we propose an open-source benchmark that introduces datasets, commonly occurring practical out-of-distribution settings, and evaluates state-of-the-art UQ modules using standard metrics such as joint log-loss and marginal log-loss. In addition, we operationalize the evaluation of  “posterior consistency” for modern deep learning-based UQ methods, to measure  the noise in a UQ update, by tying it to canonical training practices of these methods such as different training seeds and early stopping. We demonstrate that the current methodologies such as Ensembles, ENNs, are unable to reliably encode prior information, require extensive hyperparameter tuning and ad-hoc mechanisms to achieve reliable performance. The proposed benchmark serves as a comprehensive evaluation of UQ modules to enable complex downstream tasks. 