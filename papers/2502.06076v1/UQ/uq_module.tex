

%While Gaussian processes works well in low dimensions,  quantifying epistemic uncertainty on $f\opt(Y|X)$  over high dimensional inputs is an active area of research, including popular techniques such as dropout~\citep{GalGh16}, Bayes by Backprop~\citep{BlundellCoKaWi15}, Ensembles/Ensemble++~\citep{LakshminarayananPrBl17,OsbandAsCa18}, hypermodels~\citep{DwaracherlaLuIbOsWeVa20} and Epistemic Neural Networks \citep{OsbandEtAl23}. Our framework is compatible with deep learning-based uncertainty quantification models such as ensembles where we can only do approximate posterior updates. 


%To overcome the computational issue of Ensembles/Ensemble+~\citep{LakshminarayananPrBl17,OsbandAsCa18},  ENNs were proposed and shown to outperform Ensembles at a much lower computational cost. For synthetic data, we use GP as an oracle and compare other UQ modules with it.






 





% \subsubsection{Bayes by Backprop (BBB)~\citep{BlundellCoKaWi15}:} is an efficient way of obtaining approximate posterior by minimizing the KL divergence between the approximate and
% true posterior and using a 
% reparameterization trick.


% \subsection{Metric:}
% \label{sec:joint-log-loss}

% \textbf{Joint log loss}: Following~\citep{WenOsQiLuIbDwAsVa22,OsbandWeAsSeDwLuVa22,OsbandEtAl23}, we consider joint log loss.

