

% We introduce our benchmark to evaluate UQ methods under distribution shifts, including out-of-support shifts, and evaluating posterior consistency. We first give a brief overview of both synthetic and publicly available real datasets, and methods to introduce severe distribution shifts, including out-of-support shifts in both static and dynamic settings  (see Section~\ref{sec:details-experiments} for complete details). We then introduce UQ methods we consider for the benchmark. Note that we build upon the neural-testbed~\citep{OsbandWeAsDwLuIbLaHaOdVa22} code-base for this benchmark. 



\paragraph{Dynamic setting} As specified in Section \ref{sec:eval_posterior_consis}, we  also evaluate posterior updates of different UQ methodologies in dynamic setting. We restrict attention to single-step updates as there is little algorithmic difference in single-step vs. multi-step updates.
Consider a setting where  $t=0$ we observe data $\mc{D}^0$ and our UQ methodology has posterior $\mu(f|\mc{D}^0)$.
At time $t = 1$, we see additional data $\mc{D}^1$ and
the posterior is updated to  $\mu(f|\mc{D}^0\cup \mc{D}^1)$. Performance under the updated posterior is:
\begin{align*}
    - \log {\left[\int \left(\prod_{i=m}^{m+\tau} p_\epsilon (y_i -f(X_i))\right) d\mu(f|\mc{D}^0 \cup \mc{D}^1) \right]}.
\end{align*}
 

\paragraph{Sensitivity to implementation details} Canonical engineering practices involve several hyperparameters (e.g., weight-decay, prior scale) and exogeneous randomness. In applications that require frequent posterior updates (e.g., active exploration in RL), posterior inference $\mu(\cdot | \mc{D}^0, \mc{D}^1)$ incurs a huge computational cost and heuristics like early stopping are used as an approximation. Additionally, the integral in (\ref{joint_log_loss}) is often challenging to compute and approximations are made; typically, the final estimator depends on the UQ methodology, as well as implementation details such as initialization and training randomness.  We also  evaluate the sensitivity of UQ methodologies against these implementation choices and observe that
they have an outsized influence in OOD performance, particularly in dynamic settings.




% For many UQ methods, the posterior predictive distribution~\eqref{eqn:ppd}
% often involves a high-dimensional integral and 
% cannot be computed exactly.
% In this case, we have to rely on Monte Carlo sampling to approximate~\eqref{eqn:ppd}
% \begin{align}
% \what{p}_{\rm mc}^0(\ypredsetl |\xpredsetl)
% := \frac{1}{K} \sum_{k=1}^K \prod_{i=1}^\tau 
%  \what{p}_{\varepsilon}\left(y_i - \what{f}(X_i, U_k)\right)~~~\mbox{where}~~~U_k \simiid \what{\pi}.
%  \label{eq:approximate_posterior-monte-carlo}
% \end{align}
% In dynamic settings, sampling from the posterior
% $U_k \sim \what{\pi}(\cdot; \mc{D}^0, \mc{D}^1)$
% is challenging to do exactly, and typically involves several approximations. 








\subsection{Datasets}  \label{sec:UQ_BENCH_datasets}
As we specify in Section \ref{sec:eval_posterior_consis}, we study both synthetic and real-world datasets
to allow comparison against oracle methods that know the true model class (synthetic), and represent realistic application areas (real). 

\noindent{\bf{Synthetic data.}} We simulate a regression task using Gaussian Processes (GPs).  
 As before, to introduce out-of-support bias, we sample features $X$ from multiple clusters distributed over $\R^n$, where $n$ is the feature dimension.  Specifically,  we create two anchor points, one at the origin and other at a linearly increasing distance from the origin parameterized by $k$. 
We then sample the $X$ data around these
anchor points by adding a small zero mean Gaussian noise to the anchor points. 
 Outcomes $Y$ are generated using the GP %GP\mathcal{GP} 
with an RBF kernel: $f_i \sim \mathcal{GP}(m,k) $, with $m(X)$ being the mean and $k(X,X') = \sigma_f^2 \exp\left(-\frac{(X-X')^2}{2\loss^2}\right)$ being the covariance kernel, further we add Gaussian noise $N(0,\sigma^2)$ to $f(X)$ to get $Y$.  We set $\loss = 2$, $\sigma_f^2 = 0.69$ and $\sigma^2 = 0.01$.
We select 100 points near the first anchor point as the training data and 
200 points randomly sampled from the two clusters form the test data, which we divide into in-distribution and out-of-distribution data depending on which cluster the data sample comes from.
 

%Besides allowing for an oracle comparison, GPs allow us to simulate well specified priors, allowing us to access whether different UQ methodologies can adapt  to different priors.
%Generating data from the GP's allows us to compare the performance of UQ methodologies to an Oracle UQ methodology, 

\noindent{\bf{eICU dataset.}} The eICU Collaborative Database  is a large multi-center critical care electronic health record (EHR) database consisting of patient data collected as part of critical care~\citep{PollardJoRaCeMaBa18}. We assess UQ performance for in-hospital mortality prediction using physiologic and laboratory measurements. Some examples of the features
include the  length of stay at the hospital, number of days on a ventilator, and  recorded temperature on Day 1 of ICU admission.

\noindent{\bf{Fraud dataset.}} 
The Fraud Dataset Benchmark (FDB)~\citep{GroverXuTiCheLiZaLiZh22} collects publicly available data  covering a broad range of fraud detection tasks, including content moderation. We demonstrate UQ performance on four fraud datasets: \texttt{IEEE-CIS Fraud Detection, Credit Card fraud, Fraud Ecommerce, Vehicle Loan Default Prediction}. 
% In the Fraud Dataset Benchmark (FDB)~\citep{GroverXuTiCheLiZaLiZh22}, the objective is to  detect fraud   in different settings.As discussed before, we mainly consider the following datasets:\texttt{IEEE-CIS Fraud Detection, Credit Card fraud, Fraud Ecommerce, Vehicle Loan Default Prediction}.  

\noindent{\bf{ACS dataset.}} For economic data, we use \texttt{ACS Employment, Income}  in New York datasets from the US-wide \texttt{ACS PUMS} data~\citep{DingHaMoSc21}, where
the outcome is employment, and whether an individual’s income exceeds $50k$.   
% We also use \texttt{ACS Employment, Income}~\citep{DingHaMoSc21} in New York, which is selected from the national ACS PUMS data to include only individuals in specific categories. The outcome variables are employment status and whether an individual’s income exceeds $\$50,000.$
 
 
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset name} & \textbf{\# Train} & \textbf{\# Test} & \textbf{\# Features} \\ \hline
eICU & 38,534 & 16,515 & 48 \\ \hline
$\ieeecis$ & 561,013 & 28,527 & 67 \\ \hline
$\ccfraud$ & 227,845 & 56,962 & 28 \\ \hline
$\fraudecom$& 120,889 & 30,223 & 6 \\ \hline
$\vehicleloan$& 186,523 & 46,631 & 38 \\ \hline
$\acsemployment (\mathtt{NY})$ & 137,876 & 59,091 & 98 \\ \hline
$\acsincome (\mathtt{NY})$& 72,114 & 30,907 & 811 \\ \hline
\end{tabular}
\caption{All datasets used in this section. 
For all datasets, we use standard one-hot encoding for categorical features.}
\label{table:data-details}
\end{table}







\subsection{Simulating out-of-distribution data} \label{sec:selection-bias} 

We introduce mild distribution shifts using a logit-based likelihood ratio---(log) ``linear selection bias''---and out-of-support shifts using a \emph{clustering-based} selection bias procedure, leveraging a ``polyadic sampler''. 

\noindent{\bf{Linear section bias}} Given a data $\mc{D}$, we standardize the data $x_i \in \R^d$, and then sample each point $i$ with probability 
%\begin{align*}
$\mathsf{sigmoid}\paran{k \cdot \beta^\top x_i/d}$,
%\end{align*}
 where $\beta$ is a random vector in $[-1,1]^d$ and $k \ge 0$ is a parameter governing the intensity of selection bias to create a random subset of data $\dtrain \subseteq \mc{D}$.

\noindent{\bf{Clustering-based selection bias}} %Introducing distributions shifts through \textbf{clustering}. 
 We cluster the data using  $k$-means clustering and select the training data just from one cluster while the test data comes from all the clusters to create out-of-support shifts. Further, we vary the distance between the cluster centers to model the severity of the distribution shift. Clusters spaced far apart serve as out-of-support shifts from the training data.  
%In addition to the clustering process described above, we also consider another way to generate selection bias through a linear function on the covariates. For creating an \textbf{out-of-distribution} setting for the synthetic datasets, we sample the initial training data \mcD0\mc{D}^0 just from one cluster. While the out-of-distribution data comes from the rest of the clusters. 
%For creating a \textbf{dynamic} setting
To enable out-of-support shifts in {\bf{dynamic}} settings, we restrict initial training data $\mc{D}^0$ to come  from one of the clusters, following which, the agent observes data sequentially, in batches, from other clusters. % while updating its posterior belief as it sees more data. %  we use polyadic sampler and GPs to generate data. Broadly, general population P′XP_X' is distributed over multiple non-overlapping clusters. Initial t

Specifically for the real datasets,  
we construct 
50 clusters using the $k$-means algorithm for each of the above dataset.
To make the clustering efficient,
we randomly sample 50,000 rows from all datasets except eICU, and we only apply clustering to this sampled data.
We define the largest cluster as the training cluster which we label as Cluster 1.  
We then relabel each cluster based on their distance  to the training cluster, the closest  one being labeled as Cluster 2 and so on. 
Next, we  select 250 points from Cluster 1 as the training data, 250 points from Cluster $k \in \set{5,10,15,20,25,30}$ as the out-of-distribution test data
and 250 points from Cluster 1 as in-distribution test data.
We also select 50 points from Cluster $k$ as the ``pool data'' that the modeler can acquire in the dynamic setting.
For linear selection bias,   we 
first draw $\beta \sim \mathsf{Uni}[-1,1]^d$ where $d$ is the dimension of the feature, we fix $\beta$ and  for each $k \in \set{0,0.5,1,2,4,8,16}$, 
we sample each point $x_i$ with probability 
$\mathsf{sigmoid}\paran{k \cdot \beta^\top x_i/d}$.



%For \emph{simulated data},  We begin with a low-dimensional scenario, generating a one-dimensional regression setting using a Gaussian Process. Though data-generating process is not known to the algorithms,  we assume that the Gaussian process hyperparameters are known to all algorithms make fair comparisons. This can be viewed as a setting where our prior is well-specified, that we can tease apart the effectsof different policy optimization approaches without any concerns about the misspecified priors or in-accurate posteriors. 

%For evaluation on out-of-distributions we consider both simple selection bias and clustering. Here clustering serves as more severe form of distribution shift (potentially out-of-support) as compared to linear selection bias. For dynamic setting we just rely on datasets generated through clustering, similar to the synthetic datasets.

\subsection{UQ methodologies}
\label{sec:UQ_bench_benchmark-uq-metric}

We compare Dropout, Ensemble, \ensembleplus, Epinet, Hypermodels with varying  hyperparameters for each model (see Section \ref{sec:UQ_BENCH_hyperparameter-sweep}). 
  For synthetic datasets we also compare to GPs as one of the UQ methods, which serves as an oracle baseline.  As specified in  Section~\ref{sec:eval_posterior_consis}, we explain different UQ methodologies here. We also specify the configurations of different methodologies that we use, mostly inspired from~\citep{OsbandWeAsDwLuIbLaHaDoRo22, OsbandWenAsDwIbLuRo23}. In the end of this section, we also discuss some aspects of varying these configurations of different methodologies.


\subsubsection{MLP}

We use Multi-layer perceptron (MLP) as a baseline to compare the benefits of the other UQ methodologies over a simple MLP.  We train the MLP on mean-squared error loss for regression, while for the classification we train it on cross-entropy loss, both with $L_2$ weight decay. In our experiments, we use  MLP with two hidden layers, with each layer having 50 hidden units with ReLU activation.

  \subsubsection{Ensemble}
Deep Ensemble is a basic approach introduced in~\citep{LakshminarayananPrBl17} for posterior approximation. 
Broadly, we learn an ensemble of networks (e.g., MLPs) on the given training data,  with each networks differing from each other through  different random initialization of weights. Specifically, the ensemble consists of $M$ models.  The $m$-th model of the Ensemble takes the form  $f_{\eta_m}(X)$ and is parameterized by $\eta_m$,
where $f_{\eta_m}(\cdot)$  is   trained on the given data and differs across $m$ through random weight initializations. In regression setting, the model is trained using the mean-squared error loss, while in classification setting ,it is trained using the cross-entropy loss. In both settings, we also use $L_2$ weight regularization. In our experiments, we use an ensemble of 100 MLPs where each MLP has two hidden layers with 50 hidden units each and we use the ReLU activation.

 \subsubsection{\ensembleplus}
 
As mentioned earlier, \ensembleplus was introduced in~\citep{OsbandAsCa18} building on deep ensembles. 
 \ensembleplus consists of ensemble of MLPs along with randomized prior function for each MLP and bootstrap sampling of the training data. Randomized prior functions and bootstrap sampling help in maintaining diversity among the models on the unseen data and is crucial for performance improvement over Ensembles. 
 Specifically, assume that Ensemble $+$ consists of $m$ models. 
 Then the $m$-th model ($1\leq m \le M$) of the Ensemble $+$ takes the form
\begin{align*}
    f_{\eta_m}(X) = g_{\eta_m}(X) + \alpha p_m(X),
\end{align*}
where $g_{\eta_m}$  is the trainable part of the network (parameterized by $\eta_m$) and $\alpha p_m(\cdot)$ is the additive prior function, where $\alpha$ (prior scale) controls our prior belief about the uncertainty -- a higher   $\alpha$ means a higher  uncertainty. 
Here $g_{\eta_m}(\cdot)$ and $p_m(\cdot)$ differs across $m$ through random weight initializations.  
 As in ensembles, we train the model on mean-squared error loss (in regression setting) or on the cross-entropy loss (in classification setting) along with $L_2$ weight regularization. In our experiments, we use an ensemble of 100 MLPs where each MLP has two hidden layers, each with 50 hidden units and ReLU activation. We use an identical MLP (with different weight initialization) as the prior generating function for each particle of the ensemble.


\subsubsection{Hypermodels}

A Hypermodel agent~\citep{DwaracherlaLuIbOsWeVa20} consists of a base model parameterized by $\theta \in \Theta$. Given 
$\theta$ and an input $X$, base model gives the output $ f_\theta(X)$. A hypermodel $g_\nu: {\mathcal Z} \to \Theta$ is parameterized by $\nu$ and $z \in \zdist$ is an index determining a specific instance of the base model. 
Along with this, a reference distribution $p_z$ is also specified for the index $z$ to determine the distribution over the base models. 
Therefore, overall the output corresponding to the index $z$ and input $X$ is $f_{g_\nu (z)}(X)$. 
In addition to this, we can have additive prior for the hypermodel similar to   Ensemble $+$. In our experiments, we use an MLP with two hidden layers, each with 50 hidden units and ReLU activation as our base model. We also use an additive prior with one hidden layer and 10 hidden units. 

% A base model generates an output Yt given parameters θ and input $Xt$, while a hypermodel
% generates base model parameters $g_\nu(z)$ given hypermodel parameters $\nu$ and an index $z$


% A hypermodel  is parameterized by parameters $\nu$ that determines a function $g_\nu: \zdist$, w
% In particular, given hypermodel parameters $\nu$, one generate base model parameters $\theta = g_\nu(z)$ and fit $\nu$ so that 
% $f_{g\nu(z)}(X_i) $ is close to $Y_i$.


 \subsubsection{Epistemic Neural Networks (ENNs - Epinet)}

Epistemic Neural Networks were introduced in~\citep{OsbandWenAsDwIbLuRo23}. 
We focus on Epinets   $f_\theta(X,z)$) that can be parameterized by $\theta$. 
It consists of a base model $\mu_\zeta(X)$ parameterized by 
$\zeta$ and an epinet $\sigma_\eta(\mathrm{sg}[\phi_\zeta(X),z])$ parameterized by $\eta$. 
Here $\phi_\zeta(X)$ is a subset of inputs and outputs of each layer of the base model and ``sg'' is the stop gradient 
operation that was shown to stabilize the training~\citep{OsbandWenAsDwIbLuRo23}.
In addition, $z$ is an epistemic index drawn from a Gaussian distribution. 
Therefore, ENN has parameters $\theta=(\zeta,\eta)$. Moreover, the epinet has a learnable part 
$\sigma_\eta^L(\mathrm{sg}[\phi_\zeta(X),z])$ and a prior net $\sigma^P_\eta(\mathrm{sg}[\phi_\zeta(X),z])$.
Overall,
\begin{align*}
    f_\theta(X,z) = \mu_\zeta(X) + \sigma_\eta^L(\mathrm{sg}[\phi_\zeta(X),z]) + \sigma^P_\eta(\mathrm{sg}[\phi_\zeta(X),z]).
\end{align*}
We use Algorithm 1 in~\citep{OsbandWenAsDwIbLuRo23} for training epinets, and for loss function we  again use  mean squared error (for regression) or cross-entropy loss (for classification) with $L_2$ weight decay. For our experiments, we set a two-layer MLP as our basenet. This MLP has two hidden layers with 50 hidden units in each layer and ReLU activation. For learning part of the epinet,  we take $\sigma_\eta^L([\phi_\zeta(X),z]) = g_\eta([\phi_\zeta(X),z])^Tz$  where $g_\eta(.)$ is an MLP with two hidden layers with 15 hidden units and ReLU activation. The prior function of the epinet 
takes the form $\sigma^P_\eta([x,z]) = \alpha \sum_{i=1}^{D_z}p^i(x)z_i$, where $D_z$ is the dimension of the index $z$, $\alpha$ is the prior scale controlling the prior belief of the uncertainty and each $p_i$ is an MLP with two hidden layers, each with 5 hidden units and ReLU activation. 


\subsubsection{Dropout}

Monte Carlo dropout was introduced in ~\citep{GalGh16} for posterior approximation. Broadly, the agent applies  dropout on each layer of a fully connected MLP with ReLU activation at both and training and test time, making the predictions random to approximate the posterior. For regression, we train the model on mean-squared error loss while for the classification, we train it on cross-entropy loss, both with $L_2$ weight decay. In our experiments, we apply dropout to the  MLP with two hidden layers, with each layer having 50 hidden units each with ReLU activation.





  \subsubsection{Gaussian Processes (GP)}
GPs~\citep{Rasmussen03} are extensively used in the Bayesian optimization literature and are known to work well for low dimensional data. 
As specified earlier, we use GPs as one our baselines for the synthetic regression experiments. Specifically, we use $\mathcal{GP}$ with an RBF kernel: $f_i \sim \mathcal{GP}(m,\mc{K}) $, where $m(x)$ is the mean and $\mc{K}(x,x') = \sigma_f^2 \exp\left(-\frac{(x-x')^2}{2\loss^2}\right)$,  we further add Gaussian   noise $N(0,\sigma^2)$ to the outputs. We use the same values for $m(x), \loss, \sigma_f, \sigma$ through which synthetic data is generated to have a well specified prior setting and an oracle baseline. 

%We set $\loss = 2$, $\sigma_f^2 = 0.69$ and $\sigma^2 = 0.01$. 


\paragraph{Discussion - configurations of different methodologies} Note that the configurations we use for different UQ methodologies such as Ensemble, \ensembleplus, Epinets and Hypermodels have different numbers of parameters. 
For instance, ensemble $+$ with 100 particles has many more parameters than Epinets. And as we saw in Section \ref{sec:eval_posterior_consis}, Epinets perform comparable to   \ensembleplus on in-distribution and out-of-distribution with these configuration. In a way, we can say Epinets perform as well as \ensembleplus with much fewer number of parameters. As mentioned in~\citep{OsbandWeAsDwLuIbLaHaDoRo22, OsbandWenAsDwIbLuRo23}, if we reduce the number of particles in \ensembleplus, its performance deteriorates. 
In our experiments, we do not concern ourselves with comparing different UQ methodologies with the same number of parameters. 
Rather, the main aim for our experiments is to showcase that the models (methodologies + configurations) performing similarly on in-distribution (ID) data  differ drastically in their performance on OOD performance and under dynamic settings. 

\paragraph{Evaluation} In all of our experiments, we set $\tau = 10$ and employ the dyadic sampling heuristic from ~\citet{OsbandWenAsDwIbLuRo23} to evaluate the joint log-loss~\eqref{joint_log_loss} (see Section~\ref{sec:UQ_eval_metrics} for details).

\subsection{Hyperparameter  sweep}
\label{sec:UQ_BENCH_hyperparameter-sweep}

In Table \ref{tab:hyperparameter_sweeps} we summarize the hyperparmeters we tune for each of the methodologies.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Agent} & \textbf{Hyperparameters} \\ \hline
\textbf{mlp} & learning rate, L2 weight decay \\ \hline
\textbf{dropout} & learning rate, length scale, dropout rate \\ \hline
\textbf{ensemble} & learning rate,  L2 weight decay \\ \hline
\textbf{ensemble+} & learning rate,  L2 weight decay, prior scale \\ \hline
\textbf{hypermodel} & learning rate, L2 weight decay, prior scale \\ \hline
\textbf{epinet} & learning rate,  L2 weight decay, prior scale \\ \hline
\end{tabular}
\caption{Hyperparameter sweeps for each agent}
\label{tab:hyperparameter_sweeps}
\end{table}

For learning rates, we swept over $\set{1e-4, 0.001, 0.01,0.1 }$ for each agent and found that learning rate of $0.001$ works well for all agents. We fix this learning rate for the rest of the experiments. For other hyperparameters, we sweep over the following sets for each agent:
\begin{itemize}
    \item For \textbf{mlp}, we sweep over L2 weight decay in $\set{0.01, 0.1, 1, 10, 100}$.
    \item For \textbf{dropout}, we sweep over  length scale in $\set{0.0001, 0.001, 0.01, 0.1, 1, 10, 100}$, and dropout rate in $\set{0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6}$.
    \item For \textbf{ensemble}, we sweep over L2 weight decay in $\set{0.01, 0.1, 1, 10, 100, 1000}$.
    \item For \textbf{ensemble+}, we sweep over  L2 weight decay in $\set{0.01, 0.1, 1, 10, 100}$, and prior scale in $\set{1, 3, 10, 30, 100}$.
    \item For \textbf{hypermodel}, we sweep over  L2 weight decay in $\set{0.01, 0.1, 1, 10, 100}$, and prior scale in $\set{1, 3, 10, 30, 100}$.
    \item For \textbf{epinet}, we sweep over L2 weight decay in $\set{0.001, 0.01, 0.1, 1, 10, 100}$, and prior scale in $\set{0.01, 0.1, 1, 10, 100}$.
\end{itemize}



\subsection{Other experimental details}
For Figures~\ref{fig:task-1-joint-ood-all-data} 
and~\ref{fig:task-1-joint-ood-all-data-with-oracle},  we average the results over 10 different inference seeds  with each seed evaluating 1000 batches for the joint log-loss (with dyadic sampling) and randomizing over Monte Carlo approximation of the posterior.  
For the rest of the experiments in Section \ref{sec:UQ_BENCH_additional_experiment}, we fix the seed unless we specify it otherwise. 
%As mentioned earlier, within each seed we evaluate 1000 batches of test data for the joint log-loss with dyadic sampling.

 
% We now define a few terms:
% \begin{align*}
% P\opt_{T+1: T+\tau} & \defeq \P(\ypredset \in \cdot | \env,  \xpredset) \\
% \bar{P}_{T+1: T+\tau} & \defeq \P(\ypredset \in \cdot | \mc{D}_T,  \xpredset) \\
% \hat{P}_{T+1: T+\tau} & \defeq \P(\ypredset \in \cdot | \theta_T,  \xpredset).
% \end{align*}

% We then define
% \begin{align*}
%     \Delta_\tau & \defeq \dkl{P\opt_{T+1: T+\tau}}{\hat{P}_{T+1: T+\tau}}, \\ 
% \dkltau &   = \E[\Delta_\tau],
% \end{align*} and to estimate \dkltau\dkltau, we use the following estimator:
% \begin{align}
%     \widehat{\dkltau} &= \frac{1}{JN} \sum_{j=1}^J \sum_{n=1}^n \log \paran{\frac{p_{j,n}}{\hat{p}_{j,n}}}
% \end{align} where we sample j=1,⋯,Jj=1,\cdots,J environment and the environment/ agent likelihood  is given by pj,np_{j,n} and ˆpj,n\hat{p}_{j,n} respectively.




 

