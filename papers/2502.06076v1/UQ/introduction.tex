%\section{Introduction}
%\label{sec:uq_introduction}

%Uncertainty quantification (UQ) is a foundational goal in building intelligent agents. Since automated models are always in operation, they  encounter atypical inputs unseen during training. Quantifying uncertainty in model outputs can assess when models extrapolate unreliably, a central focus if the burgeoning field of trustworthy AI. UQ methods can further inform data collection efforts in active learning and reinforcement learning by exploring regions/actions with high uncertainty~\citep{kochenderfer2015decision}. 

%Recent methodological developments in UQ have achieved substantive progress. Going beyond classical Gaussian Processes~\citep{Rasmussen03}, recent UQ methodologies range from Markov chain Monte Carlo based methods such as NUTS~\citep{HomanGe14} and SGLD~\citep{WellingTe11}, to variational inference-based UQ methods like MC-Dropout~\citep{GalGh16} and Bayes by Backprop~\citep{BlundellCoKaWi15}.  To enable UQ with modern ML models, more recent proposals such as Ensembles/ Ensemble $+$~\citep{LakshminarayananPrBl17,OsbandAsCa18}, Hypermodels~\citep{DwaracherlaLuIbOsWeVa20}, and Epistemic Neural Networks (ENNs)~\citep{OsbandEtAl23} provide simple adjustments to neural network architectures, by relying on model (dis)agreement or inducing input noise to encourage the model to disentangle signal and noise. These simple approachs have been observed to effectively characterize actionable (epistemic) uncertainty in a single-shot in-distribution environment that matches the training data~\citep{JospinLaBoBuBe22, OsbandWenAsDwIbLuRo23}.
% \sjtodo{Please cite appropriate uq papers above, I cited representative ones} 


%In many real-world applications of UQ methods,  distribution shift is a central concern.  In clinical settings, disease labels are only available for patients who have been screened for the condition of interest. The severe selection bias leading leads to high uncertainty over populations that are traditionally unscreened, likely to be inherently out-of-support from those that are typically screened. Analogous challenges occur in reinforcement learning contexts where observed environments differ markedly from real-world conditions due to distribution shift induced by the behavior policy that generates the data, and the policy that is deployed. Accordingly, it is critical to  reliably estimate the uncertainty over targets under distribution shift, including out-of-support shifts.

%When the initial data offers limited insights---as is typical for supervised learning under severe selection bias or in reinforcement learning---the agent must gather additional data sequentially. Correspondingly, UQ methods must be able to update beliefs as new data is observed. As we detail shortly, we focus on the reliability of posterior updates, which we call ``posterior consistency''. 

%Empirical benchmarking can accelerate the pace of methodological innovation in expanding the scope of UQ methods, yet there has been little focus on empirically validating performance under distribution shifts, particularly in settings involving dynamic updates to posterior beliefs. To systematically evaluate UQ methods, we move away from relying on ad-hoc arguments of unobserved latent factors---as is typical for Bayesian methods--by treating them as a probabilistic sequence prediction model (this practice is rooted in prequential statistics~\citep{Roberts65,Dawid84} and can be traced back to even De Finetti's seminal work~\citep{DeFinetti33}). Concretely, we evaluate the KL divergence between the oracle sequence prediction(a.k.a. posterior predictive) and the UQ model output.In Figures~\ref{fig:enter-label} and~\ref{fig:dynamic_setting_k_30} we observe that even state-of-the-art UQ methods that perform well in static environments perform poorly in dynamic settings, particularly on OOD inputs. 

%To systematically assess the performance of UQ methods under  distribution shift and dynamic posterior updates, we present \textsf{UQBench}, a benchmark comprising of a suite of datasets  and baselines across a range of environments constructed from  real-world datasets. Our comprehensive benchmark evaluates the ability of a UQ model to reliably encode prior information, characterize out-of-support uncertainty, and provide consistent updates as new batches of data are acquired. 

%Our empirical evaluation shows that performance disparity between in ID and OOD performance is intricately tied to canonical model training practices beyond conceptual UQ paradigms---the usual focus of methodological development. In particular, we observe implementation details such as weight-decay, prior scales (where applicable), and stopping times have an outsized influence. Moreover, we highlight the unreliability of typical approximate posterior update heuristics, that rely on gradient updates to the underlying network(s). We observe large variation in the quality of the posterior across similar implementation details and even random seeds, underscoring the importance of further methodological development along these dimensions. 

%Similarly, a significant disparity in uncertainty or performance across different seeds indicates a lack of UQ reliability. %Therefore, assessing bias in state-of-the-art UQ modules requires assessing the i) ability to induce right prior in deep learning architectures, ii) evaluating the out-of-support uncertainty quantification on severely selection-based data, and iii) ``posterior consistency'' due to  neural network training and optimization parameters (e.g., early stopping, seeds). 

%To further investigate, we manipulate supervised datasets to introduce selection bias through methods like clustering, creating new datasets for evaluation based on marginal likelihood in dynamic settings.



%particularly in dynamic settings where knowledge about out-of-support distributions is limited  due to   challenges in hyperparameter tuning (see Section~????????????????????????????????????????????????????????????\ref{sec:experiment}).  The hyperparameters α\alpha and λ\lambda are especially critical, relying heavily on prior knowledge, and determining their values based on current training data assumes a consistency in uncertainty, presenting a seemingly insurmountable problem. This could potentially be addressed through innovations like Bayesian transformers.


%Based on our experiments we showcase that current UQ methodologies are not able to perform well under the There are many tradeoffs, see Section ????????????????????????????????????????????????????????????\ref{}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{UQ/UQ_description.PNG}
%     \caption{Demonstration of posterior updates using different UQ methodologies - agent observes data $D_i$ at time $T_i$ and updates its posterior. Shaded-blue  region presents the uncertainty it has over rest of the space. An ideal uncertainty quantification (UQ) methodology updates the posterior predictions to provide confident predictions for in-distribution data while maintaining significant uncertainty for out-of-distribution data. An overconfident UQ approach results in reliable predictions for in-distribution data but suffers from performance degradation on out-of-distribution data due to false confidence. Conversely, an underconfident UQ method maintains high uncertainty for OOD data but compromises performance on in-distribution data, leading to poor outcomes even as more data is acquired. We develop benchmark to assess this trade-off in current UQ   methodologies.}
%     \label{fig:enter-label}
% \end{figure}





% \begin{figure}
% \centering
% \begin{minipage}[b]{0.48\textwidth}
% \centering
% \includegraphics[width = \textwidth]{maindagram_1.PNG}
% \caption*{\centering~~~~~~~~Ensemble$+$}
% %\label{fig:ensemble+_difficult_to_choose_weight_decay}
% \end{minipage}
% \hfill
% \begin{minipage}[b]{0.48\textwidth}
% \centering \includegraphics[width=\textwidth]{main_daigram_2.png}
% \caption*{\centering~~~~~~~~~~Epinet}
% %\label{fig:epinet_difficult_to_choose_weight_decay}
% \end{minipage}

% \caption{Trade-off between  in-distribution and out-of-distribution performance with weight decay as the hyperparameter (eICU, clustering). }
% \label{fig:enter-label}
% \end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{UQ_description.PNG}
%     \caption{Demonstration of posterior updates using different UQ methodologies - agent observes data $D_i$ at time $T_i$ and updates its posterior. Shaded-blue  region presents the uncertainty it has over rest of the space. An ideal uncertainty quantification (UQ) methodology updates the posterior predictions to provide confident predictions for in-distribution data while maintaining significant uncertainty for out-of-distribution data. An overconfident UQ approach results in reliable predictions for in-distribution data but suffers from performance degradation on out-of-distribution data due to false confidence. Conversely, an underconfident UQ method maintains high uncertainty for out-of-distribution data but compromises performance on in-distribution data, leading to poor outcomes even as more data is acquired. We develop benchmark to assess this trade-off in current UQ   methodologies.}
%     \label{fig:enter-label}
% \end{figure}