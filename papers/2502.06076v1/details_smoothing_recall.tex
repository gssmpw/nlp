\section{Smoothing the recall objective} \label{sec:details-recall-smoothing}
 
Performance metric, such as the \texttt{Recall} of a model $\model(.)$,  is not differentiable. Recall that $  H(\theta) \defeq \E_{{\ybpool \sim \mu; S \sim \pi_\theta } }  [G\paran{\mu_+^S}]$,  where $G(\mu_+^S) = \V_{f \sim \mu_+^S}  g(f)$, and $g(f) \defeq  \E_{X \sim P_X}\left[ \E_{Y \sim p(\cdot|f,X) } \Big[\indic{\model(X)>0}|Y=1\Big]\mid f \right]$,  with $\mu_T$ depending on $\pi_\theta$. To estimate $\nabla_\theta H(\theta)$  in a differentiable manner, in addition to approximately sampling 
$\bm{a}(\theta)$ using Algorithm \ref{alg:k-subset},  we also need to apply a smooth approximation to $g(f)$. This can be  achieved using the softmax trick~\citep{JangGuPo17}, where we draw $2n$ i.i.d. Gumbel$(0,1)$ samples $R_{i}^{l}$ with $l \in \set{1,2}$. 
Define:
\begin{align*}
g_{\tau}(f; \bm{R}) & \defeq \frac{\sum_{i=1}^n  Y^{\tau}_i(f; \bm{R})  \model(X_i)}{\sum_{i=1}^n Y^{\tau}_i(f; \bm{R})}, \\
&\text{ where }
Y^{\tau}_i(f;\bm{R}) = \frac{\Exp{(\log f_i) + R_i^1) / \tau}}{
\Exp{(\log f_i) + R_i^1) / \tau}
+ \Exp{(\log (1-f_i)) + R_i^2) / \tau}
},
\end{align*} here  $\tau > 0$ is the temperature hyperparameter that controls the smoothing of $g_{\tau}(f; \bm{R})$. 
It is easy to show that $\E_{\bm{R}}[Y^{\tau}_i(f;\bm{R})] \approx f_i$,  making $g_{\tau}(f; \bm{R})$  a natural approximation of the previously defined \texttt{Recall} $g(f)$.

% Performance metric accuracy (square loss) as introduced in Section~\ref{sec:broad-framework-accuracy}  is already continuous and smooth - so we don't need to further smooth this objective





