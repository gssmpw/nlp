\section{Practical considerations in implementing our framework}
\label{sec:practical_consideration}

As highlighted in our previous discussion, the success of our framework relies on  two critical components.  
First, the uncertainty quantification methodology must provide reliable and efficiently computable posterior updates. 
Second, the computational efficiency of the autodifferentiation software framework is crucial, 
as it enables end-to-end gradient estimation through the entire pipeline. 
This efficiency will be particularly important in multi-step lookahead scenarios. 
In this section, we highlight practical considerations associated with these two components, 
and discuss  remaining challenges and corresponding promising future research directions. 
We begin by evaluating the reliability of current UQ methodologies in Section~\ref{sec:eval_posterior_consis}, followed by a discussion on the efficiency of the autodiff framework in Section~\ref{sec:auto-diff-bottleneck}.


\subsection{Evaluating Posterior Consistency of Uncertainty Quantification Methods}
\label{sec:eval_posterior_consis}
The reliability of a UQ methodology depends on two key properties i) reliable UQ performance on out-of-distribution (OOD) inputs, and ii) the ability to sharpen beliefs reliably as more data is gathered. We refer to these two properties collectively as  ``posterior consistency''. 


% In this section, we evaluate and compare different UQ methodologies for their reliability (posterior consistency).  Given that Uncertainty Quantification is the foundational for fields such as trustworthy AI, active learning, and exploration in reinforcement learning - this analysis may be of independent interest to researchers in  these domains.  Most existing literature on UQ evaluation  focuses on static single-shot settings~\citep{OsbandWenAsDwIbLuRo23}, with some literature on evaluating UQ performance on OOD data~\citep{OvadiaFeReNa19,Nadoal21}. To our knowledge, none of these literature evaluates UQ in dynamic settings, which is critical for applications in active Learning and reinforcement learning settings. Continual learning~\citep{WangZhSuZh24} is an active area of research with a primary focus on efficiently retraining the models as new data is observed. In contrast, our focus on assessing whether a UQ model, even when retrained perfectly, performs posterior updates consistently—specifically, whether the posteriors are sharp on observed data and provide reliable UQ on unobserved data.

Most existing literature on UQ evaluation  focuses on static single-shot settings~\citep{OsbandWenAsDwIbLuRo23}, with some literature on evaluating UQ performance on OOD data~\citep{OvadiaFeReNa19,Nadoal21}. To our knowledge, none of these literature evaluates UQ in dynamic settings, which is critical for applications in active learning and reinforcement learning settings. 
In this section,  we focus on assessing whether a UQ model, 
even  
when retrained perfectly, performs posterior updates consistently or not—--specifically, whether the posteriors are sharp on observed data and provide reliable UQ on unobserved data. 
Since UQ is foundational to trustworthy AI and active exploration in reinforcement learning, our empirical insights may be of independent interest to researchers in these domains.  

Our experiments reveal several challenges that current UQ methodologies suffer from. Due to the variation in UQ quality across hyperparameters, we observe a clear trade-off between in-distribution (ID) and out-of-distribution (OOD) performance. 
The performance disparity between ID and OOD performance is intricately tied to canonical model training practices that extend beyond the conceptual UQ paradigms. In particular, we observe that implementation details such as weight-decay, prior scales (where applicable), and stopping times have an outsized influence on the performance of UQ methodologies. Moreover, we highlight
the unreliability of typical approximate posterior update heuristics,
which rely on gradient updates to the underlying network(s). 
We observe large variation in the quality of the posterior across similar implementation details and even random seeds,
underscoring the importance of further methodological development along these dimensions. 
These challenges magnify in the dynamic setting where posteriors are updated with new observations, highlighting the need for reliable posterior update methods---an often  overlooked dimension in UQ algorithmic development predominantly focused on static evaluations.  

We demonstrate the above discussion using the eICU dataset. Additional results from other datasets, along with further details on the evaluated UQ methodologies, the selection biases introduced, and the metrics used to assess UQ performance, are provided in
Section~\ref{sec:practical_consideration_experiment_details}.
 
\begin{figure}
\centering
\begin{minipage}[b]{0.32\textwidth}
\centering
\captionsetup{labelformat=empty,labelsep=none}
\includegraphics[height=4.5cm]
{figures/eicu_simple/Task_1_kl_estimate_dyadic_tau_10_ood_vs_k_val_for_all_agents.pdf}
{\small{\centering{eICU (Linear bias)}}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[height=4.6cm]{figures/eicu_clustering/Task_1_joint_log-loss_ood_vs._k_val_for_num_batches=200_dynamic_0.pdf}
{\small{ eICU (Clustering bias)}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4.5cm]{figures/GP_dynamic_0/Task_1_kl_estimate_dyadic_tau_10_ood_vs._k_val_for_num_batches=250_dynamic_0.pdf}
{\small{ Synthetic (Clustering bias)}}
\end{minipage}
\caption{Joint log-loss on OOD data with increasing selection bias. 
Though some agents such as ensemble and dropout perform well under no selection bias, their performance deteriorates significantly with a large selection bias (larger $k$).}
\label{fig:task-1-joint-ood-all-data}
\end{figure}



\paragraph{Performance under distribution shifts} In Figure~\ref{fig:task-1-joint-ood-all-data}, we compare the performance of different UQ methodologies on  out-of-distribution data under increasing levels of  \emph{linear} and \emph{out-of-support} selection bias. The methodology to introduce selection bias is detailed later in Section~\ref{sec:selection-bias}.
 Figure~\ref{fig:task-1-joint-ood-all-data} shows that most UQ methodologies are sensitive to selection bias, with performance often degrading  significantly on OOD data, while in-distribution performance remains stable.  These results emphasize the importance of evaluating UQ models beyond the data it has seen so far.



 
\paragraph{Hyperparameter tuning breaks under distribution shifts} 
UQ methodologies such as \ensembles/ \ensembleplus and Epinet require extensive hyperparameter tuning to perform well. Canonically, these hyperparameters are tuned by splitting the available dataset, $\mc{D}_0$, into a training and validation sets, and choosing the hyperparameters that yield the best results on the validation set.
However, we demonstrate that this approach has a significant limitation when the data experiences distribution shifts: the best hyperparameters for in-distribution (ID) data are not necessarily the best for out-of-distribution (OOD) test data. 
Moreover, in many cases, there is a unexpected trade-off between ID and OOD performance. In Figure~\ref{fig:difficult_to_choose_prior_scale}, we illustrate this phenomenon for  \ensembleplus, Epinets, and Hypermodels 
when tuning the Prior Scale hyperparameter (we provide a detailed explanation of each methodology in Section \ref{sec:UQ_bench_benchmark-uq-metric}). We observe that reducing prior scale for \ensembleplus improves ID  performance while the OOD performance deteriorates. Similar trends were noted for other hyperparameters, such as weight decay and stopping time, as shown in Figures~\ref{fig:difficult_to_choose_weight_decay} and~\ref{fig:difficult_to_choose_stopping_time} in Section \ref{sec:practical_consideration_experiment_details}.




\begin{figure}[h]
\centering
\begin{minipage}[b]{0.32\textwidth}
\centering
\includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_prior_scale_ensemble+_with_500_training_iterations.pdf}
{\small{\centering Ensemble$+$}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_prior_scale_epinet_with_500_training_iterations.pdf}
{\small{\centering Epinet}}
\end{minipage}
\hfill
\begin{minipage}[b]{0.32\textwidth}
\centering \includegraphics[height=4.5cm]{figures/eicu_clustering/Experiment_2_prior_scale_hypermodel_with_500_training_iterations.pdf}
{\small{\centering Hypermodel}}
\end{minipage}
\caption{Trade-off between  in-distribution and out-of-distribution joint log-loss with prior scale as the hyperparameter (eICU, Clustering bias).} \label{fig:difficult_to_choose_prior_scale}
\end{figure}



\paragraph{Performance under dynamic settings} We observe that even static OOD performance does not comprehensively capture the reliability of a UQ method. 
It is crucial that a UQ method  sharpens its beliefs as new data is observed while  still maintaining uncertainty over the unobserved space.  We capture this by assessing UQ performance in  a  dynamic setting. 
Figure~\ref{fig:dynamic_setting_k_30} summarizes  the results of this experiment. 
Figure~\ref{fig:dynamic_setting_k_30} (a) and (b)  shows that at $T=0$, models with better performance on in-distribution data are worse on 
out-of-distribution data, highlighting the previously discussed trade-off. For instance, Epinets perform better on out-of-support data, their ID performance is worse compared to \ensembleplus and Hypermodels. Figure~\ref{fig:dynamic_setting_k_30} (c) and (d)  shows performance improvement after acquiring new data, allowing us to assess the sharpness of posterior updates for each model. 
Although Epinets initially excel on OOD data, their performance improvement (posterior update) on OOD data is not as sharp as that of Hypermodels and \ensembleplus. Additionally, hyperparameter tuning remains challenging in dynamic settings (see Section \ref{sec:practical_consideration_experiment_details} for further evaluations). While one might consider re-tuning hyperparameters with each new data acquisition, no consistent procedure currently exists to ensure reliable performance. Apart from being computationally intensive, even re-tuned hyperparameters cannot be reliably trusted in dynamic OOD scenarios, as previously discussed. These experiments reveal a significant limitation of current UQ methodologies: SOTA UQ methods suffer from a trade-off between achieving sharper posterior updates on the observed data and maintaining performance on OOD data. Furthermore, there is no reliable method to control this trade-off. This inability of UQ modules to effectively balance these competing demands underscores the need for new methodological advancements in UQ to support complex downstream tasks.



%Further, hyperparameter tuning suffers in the dynamic setting as well (see Appendix for additional evaluation). Although in practice, one might try to tune these hyperparameters each time it sees new data, there is no consistent procedure to enable  reliable performance.  
%Apart from being computationally expensive,  if we are in a dynamic OOD setting as we saw in earlier discussion even the updated hyperparameters cannot be trusted for their performance in OOD settings.

%Note the change in rankings of different models as we go from in-distribution performance based ranking in Figure~\ref{fig:dynamic_setting_k_30}, to out-of-distribution based ranking, to dynamic ranking.   
%This also points to the need of developing better UQ methodologies.
 %We hope \textsf{UQBench} can help comprehensively assess any advances in UQ methodologies for both out-of-distribution and dynamic settings.



\begin{figure}[h]
\centering
\begin{minipage}[b]{0.24\textwidth}
\centering
\includegraphics[width = \textwidth]{figures/eicu_clustering_improvement_plots/in_distribution_mean_T0.pdf}

{\small{{(a)} ID performance ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth]{figures/eicu_clustering_improvement_plots/out_of_distribution_mean_T0.pdf}
{\small{{(b)} OOD performance, ($T=0$) }}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth]{figures/eicu_clustering_improvement_plots/out_of_distribution_mean_T50.pdf}
{\small{{(c)}~ OOD performance, ($T=1$)}}
\label{fig:eicu_clustering_improvement-k=30}
\end{minipage}
\hfill
\begin{minipage}[b]{0.24\textwidth}
\centering \includegraphics[width = \textwidth]{figures/eicu_clustering_improvement_plots/out_of_distribution_mean_difference.pdf}
{\small{{(d)} ~ OOD improvement ($T=0 \to 1$) }}
\end{minipage}
\caption{Performance of different UQ modules in a dynamic setting. \textbf{Experimental Setup:} Population is distributed across 2 clusters.  At $T=0$, agent  observes data $\mc{D}_0$ sampled from Cluster 1. 
Figure (a) and Figure (b) show how different UQ models perform on test data from Cluster 1 (in-distribution) and Cluster 2 (out-of-distribution). At $T=1$, UQ agent observes some data $\mc{D}_1$  sampled from Cluster 2.  Figure (c) performance on test data from Cluster 2 at $T=1$. Figure (d)  showcase performance improvement on test data from Cluster 2 at $T=1$. \textbf{Observations:} Comparison between Figure (a) and Figure (b) signifies the trade-off between  the  in-distribution and out-of-distribution performance discussed previously. Figure (d) shows Epinets, in this case, are much worse at adapting to new data compared to the other two agents.}
\label{fig:dynamic_setting_k_30}
\end{figure}


\vspace{-10pt}
\paragraph{Sensitivity of posterior estimates to implementation details} Apart from assessing the fundamental trade-offs  on OOD data and under dynamic setting, we also examine performance degradations under practical computational constraints.
Our experiments show that the current UQ methodologies are sensitive to early stopping and random seeds, we defer the experimental details to Section \ref{sec:UQ_BENCH_additional_experiment}. This sensitivity may limit the practical effectiveness of these UQ methodologies.
Our findings highlight the outsized impact of these factors on UQ performance, underscoring the importance of evaluating UQ methods within realistic computational constraints rather than in idealized settings.



\subsection{Auto-differentiability bottleneck} 
\label{sec:auto-diff-bottleneck}
Our smoothed-pathwise gradient methodology relies on the automatic differentiation (Autodiff) functionality of Pytorch/Jax. As we saw in Section \ref{sec:diff-piepline}, for \ouralgo, we use Autodiff to differentiate through the whole pipeline including soft $\batchsize$-subset sampling, posterior update and the objective evaluation.



Our implementation focused on one-step lookaheads, and extending it to longer horizons  involves efficiently differentiating through hierarchical (multi-level) optimization and accurately estimating Hessians, tasks for which the current autodifferentiation frameworks lack sufficient efficiency. Hessian estimation via autodifferentiation remains an active area of research~\citep{BaydinPeRaSi17,ShabanChHaBo18,Margossian19,LorraineViDu20,Amos22,BlondelBeCuFrHoLlPeVe22, ScieurGiBePe22,Alesiani23,KotaryDiFi23}, and exploring and adapting these advancements or developing new engineering solutions combined with innovative parallelization techniques, represents a promising avenue for future reseach.

Similarly, while the posterior update in GPs is differentiable, 
  deep learning-based UQ modules rely on a finite number of SGD steps for posterior updates.
Consequently, the Autodiff gradient through the posterior update  also requires differentiation through hierarchical (multi-level) optimization. Moreover, there is a trade-off between computational time
and the accuracy of the gradient approximation, depending on the number of training steps used in the posterior update. 
If the number of steps becomes large, estimation errors may increase due to round-off and vanishing gradient issues. 
This limitation, along with previously mentioned challenges such as performance on OOD datasets and in dynamic settings, 
restricts the applicability of 
current UQ methodologies.
Recent advancements, such as Bayesian 
Transformer~\citep{NguyenGr22, MullerHoArGrHu22}, 
offer
promising solutions by providing differentiable posterior updates 
without requiring gradient steps. 
Exploring this approach could be an interesting direction for further improvement.
