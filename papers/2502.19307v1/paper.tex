\documentclass[conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\usepackage{graphicx} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{siunitx} % For SI units
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}


\usepackage{array} % for better columns
\usepackage{comment}

\title{Anomaly Detection in Complex Dynamical Systems: A Systematic Framework Using Embedding Theory and Physics-Inspired Consistency}

\author{

\IEEEauthorblockN{Michael Somma\IEEEauthorrefmark{1}\IEEEauthorrefmark{3}, Thomas Gallien\IEEEauthorrefmark{2}\IEEEauthorrefmark{4}, Branka Stojanović\IEEEauthorrefmark{1}}

\IEEEauthorblockA{
\IEEEauthorrefmark{1}JOANNEUM RESEARCH Forschungsgesellschaft mbH,\\ 
DIGITAL – Institute for Digital Technologies,\\
Steyrergasse 17, Graz, Austria, 8010\\
Email: \texttt{michael.somma@joanneum.at},\\\texttt{branka.stojanovic@joanneum.at} 
}

\IEEEauthorblockA{
\IEEEauthorrefmark{2}JOANNEUM RESEARCH Forschungsgesellschaft mbH,\\ 
ROBOTICS – Institute for Robotics and Flexible Production,\\
Lakeside B13b, Klagenfurt, Austria, 9020\\
Email: \texttt{thomas.gallien@joanneum.at} 
}

\IEEEauthorblockA{
\IEEEauthorrefmark{3}TU Graz, Institute for Technical Informatics, Inffeldgasse 16/I, Graz, Austria, 8010 
}

\IEEEauthorblockA{
\IEEEauthorrefmark{4}AI AUSTRIA, RL Community, Wollzeile 24/12, Vienna, Austria, 1010
}



}

\begin{document}

\maketitle



\begin{abstract}
Anomaly detection in \textit{complex dynamical systems} is essential for ensuring reliability, safety, and efficiency in industrial and cyber-physical infrastructures. \textit{Predictive maintenance} helps prevent costly failures, while \textit{cybersecurity monitoring} has become critical as digitized systems face growing threats. Many of these systems exhibit \textit{oscillatory behaviors and bounded motion}, requiring anomaly detection methods that capture \textit{structured temporal dependencies} while adhering to \textit{physical consistency principles}. In this work, we propose a \textit{system-theoretic approach} to anomaly detection, grounded in \textit{classical embedding theory} and \textit{physics-inspired consistency principles}. We build upon the \textit{Fractal Whitney Embedding Prevalence Theorem}, extending traditional embedding techniques to complex system dynamics. Additionally, we introduce \textit{state-derivative pairs} as an embedding strategy to capture system evolution. To enforce temporal coherence, we develop a \textbf{Temporal Differential Consistency Autoencoder (TDC-AE)}, incorporating a \textbf{TDC-Loss} that aligns the approximated derivatives of latent variables with their dynamic representations. We evaluate our method on the \textbf{C-MAPSS dataset}, a benchmark for \textit{turbofan aeroengine degradation}. TDC-AE \textbf{outperforms LSTMs and Transformers} while achieving a \textbf{200x reduction in MAC operations}, making it particularly suited for \textit{lightweight edge computing}. Our findings support the hypothesis that anomalies disrupt stable system dynamics, providing a robust, interpretable signal for anomaly detection. 
\end{abstract}

\vspace{0.2cm}
%%
%% Keywords. The author(s) should pick words that accurately describe
%keywords with commas.
\begin{IEEEkeywords}
Complex DynamicalSystems, Anomaly Detection, System Theory, Embedology, Physics-Informed Machine Learning, Predictive Maintenance,
Edge Computing
\end{IEEEkeywords}


\section{Introduction}
Anomaly detection in complex physical dynamical systems is a critical research area as industrial and engineered systems become more sophisticated. Identifying deviations from expected behavior is essential for ensuring reliability, safety, and efficiency. This is particularly relevant in predictive maintenance and cybersecurity monitoring. In industrial systems and rotating machinery, early fault detection helps prevent costly failures and downtime \cite{nunes_challenges_2023}. 
Meanwhile, as critical infrastructures—such as power grids and water distribution systems—become increasingly digitized, the risk of cyber threats has grown significantly \cite{tuptuk_systematic_2021,riggs_impact_2023}. Recent attacks \cite{veolia2024incident,bajak2023hackers} on cyber-physical systems highlight the need for robust monitoring techniques to detect both malicious intrusions and system failures. Ensuring the security and stability of these dynamical systems requires adaptive anomaly detection methods capable of addressing evolving threats and operational challenges.\par

Many complex dynamical systems exhibit oscillatory behaviors and bounded motion, fundamental characteristics of both natural and engineered processes. The study of such systems intersects with two key fields: \textbf{time-series modeling} and the \textbf{incorporation of physical laws}. Given that many physical systems display structured temporal dependencies, effective modeling requires methods that capture correlations across time. Simultaneously, classical physics, which governs tangible objects and engineered systems, is largely defined by causal and deterministic principles, often described through differential equations and conservation laws. This study aims to bridge time-series modeling with physics-inspired approaches to develop more effective and sustainable anomaly detection methods.

\section{Related Work}
\subsection{Time-Series Modeling}
Models like \textit{LSTMs} and \textit{Transformers} with attention mechanisms have been successfully applied to time-series anomaly detection in fields such as mechanical enginered system, aerospace, and industrial monitoring \cite{lachekhab_lstm-autoencoder_2024,liu_deep_2023,wei_lstm-autoencoder-based_2023}. While these methods effectively capture long-range dependencies and irregular temporal patterns, they are computationally expensive. For example, common benchmark datasets require time steps ranging from 30 to 500 with hidden dimensions between 20 and 50, illustrating the complexity of modeling system dynamics efficiently \cite{mahmoud_ae-lstm_2022}. Their high memory requirements often exceed the constraints of typical MCU-level devices, making real-time deployment infeasible For example, a typical LSTM with 500 time steps and 50 hidden units already surpasses the available memory of common low-power devices, limiting its practical applicability. Additionally, the sequential nature of RNNs restricts parallelization, further increasing computational cost \cite{rezk_recurrent_2020}. These challenges highlight the need for alternative approaches that balance computational efficiency with robust anomaly detection, aligning with broader goals of sustainability and practical deployability. \par
\subsection{Physics-Informed Methods}
In many applications, incorporating domain knowledge can help reduce computational demands. Physics-informed neural networks (PINNs) embed physical laws directly into neural networks, enabling them to leverage known system dynamics \cite{cai_physics-informed_2021,raissi_physics-informed_2019,raissi_hidden_2020}. However, applying PINNs to complex dynamical physical systems remains challenging. These systems involve numerous interdependent physical principles, and explicitly modeling them within a neural network would require extensive computational resources and domain expertise, making large-scale applications impractical \cite{karniadakis_physics-informed_2021,wang_when_2022}.

\subsection{Benchmark Use Case: Turbofan Aeroengine Degradation}
We use the C-MAPSS dataset as a benchmark for studying complex dynamical systems due to its realistic representation of turbofan aeroengine degradation. Aeroengines are complex dynamical systems governed by nonlinear, time-dependent interactions of physical processes, making them an ideal test case for evaluating anomaly detection methods in real-world settings.\par

In the literature, anomaly detection for the C-MAPSS dataset generally follows two main approaches. The first focuses on fleet-level anomaly detection, where engines with shorter lifetimes are classified as abnormal based on their total life cycles \cite{jakubowski_anomaly_2022,yildirim_enhancing_2024}. This method aims to distinguish early failures from normal operating conditions at a system-wide level.

The second approach considers individual engine degradation, defining anomalies based on a 60/40 time-based split \cite{bataineh_autoencoder_2020,zhu_anomaly_2024}. In this setup, the first 60\% of an engine’s life is labeled as normal, while the final 40\% is considered abnormal. The dataset is then divided into train/test subsets, and models are evaluated based on their ability to classify each time step accordingly.

Since we aim to develop methods that capture system dynamics, we consider the second approach a more suitable benchmark, as it focuses on time-dependent degradation rather than static fleet-level classification.

Previous work on anomaly detection in this setting has explored various deep learning models. One study employed an LSTM-based approach \cite{zhu_anomaly_2024}, leveraging recurrent structures to model time dependencies. Another approach used a standard autoencoder (AE) without explicit temporal modeling \cite{bataineh_autoencoder_2020}. More recent research has investigated Transformer-based models, which excel at capturing long-range dependencies but introduce high computational costs and require extensive training data due to their large parameter space \cite{inproceedings,liu_deep_2023}. Moreover, both Transformer studies formulated the problem as a multiclass classification task, with one using a slightly different dataset and the other applying the method directly to C-MAPSS. However, defining well-separated fault categories in a complex dynamical system is challenging in practice, as non-trivial interactions between multiple physical components create highly unpredictable behaviors. As system dynamics grow more intricate, these interactions become even less predictable, further complicating precise fault categorization.
\subsection{Proposed Approach}

Instead of relying on overly complex models that attempt to directly differentiate between normal and abnormal behavior, we propose a system-theoretic approach inspired by \textbf{classical embedding theory} to study the dimensionality of the system's latent representation. Our approach introduces \textbf{physics-inspired consistency principles} that approximate the underlying causal mechanisms governing the system dynamics, without explicitly enforcing physical laws. We hypothesize that complex systems in a stable regime exhibit predictable behavior, allowing for well-approximated lower-dimensional embeddings. In contrast, anomalies introduce additional complexity, disrupting these stable relationships. We aim to leverage this property to detect anomalous states of the system when the learned embedding no longer adequately captures the system dynamics.


\section{Theoretical Foundation \& Methods}

In this work, we investigate the mathematical foundations of embedding a bounded dynamical system, such as a rotary machine, into a latent space using autoencoders. Given a system governed by well-defined dynamical laws and a set of measurement values recorded as a time series, our objective is to establish conditions under which the system's dynamics can be reliably mapped to the latent space. By leveraging the learned manifold structure, we aim to detect deviations from the normal operating state within the latent space and subsequently infer corresponding deviations in the measurement domain and, ultimately, in the physical system. This formulation aims to provide a rigorous framework for anomaly detection and system health monitoring based on latent space representations.

\subsection{General Definition of an Oscillatory Dynamical System}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/CMPSS_pendulum_scheme.png}
    \caption{A driven damped pendulum with angular displacement $\theta$, velocity $\dot{\theta}$, damping $-\gamma \dot{\theta}$, and external driving force $F$.}

    \label{fig:CMPSS_pendulum_scheme}
\end{figure}
A general oscillatory dynamical system is governed by a set of differential equations describing periodic motion. In its most general form, the system evolves according to
\begin{equation}
    \dot{\mathbf{x}} = \mathbf{f}(\mathbf{x}, t),
\end{equation}
where $\mathbf{x} \in \mathbb{R}^n$ represents the state variables, and $\mathbf{f}(\mathbf{x}, t)$ defines the governing dynamics. A common class of oscillatory systems follows a second-order differential equation of the form
\begin{equation}
    \ddot{\theta} + g(\theta, \dot{\theta}) + h(\theta, t) = 0,
\end{equation}
where $g(\theta, \dot{\theta})$ represents dissipative forces, such as damping, and $h(\theta, t)$ accounts for external periodic forcing. A widely studied case is the damped driven oscillator, as shown in Fig.~\ref{fig:CMPSS_pendulum_scheme}, which satisfies the equation
\begin{equation}
    \ddot{\theta} + \gamma \dot{\theta} + \omega_0^2 \sin\theta = A \cos(\omega_{\text{drive}} t),
\end{equation}
where $\theta$ denotes the angular displacement and $\dot{\theta}$ its corresponding angular velocity. The parameters $\gamma$ and $\omega_0$ represent the damping coefficient and the natural frequency of the system, respectively, while $A$ and $\omega_{\text{drive}}$ define the amplitude and frequency of an external periodic driving force \cite{goldstein_classical_2000}. To analyze deviations from the nominal oscillatory state, we introduce a slow drift in both the angular displacement and velocity components by modifying the evolution equations as
\begin{equation}
    {\theta} \leftarrow {\theta} + \alpha t, \quad
    \dot{\theta} \leftarrow \dot{\theta} + \beta t.
\end{equation}

For illustration, we consider a system with parameters $\gamma = 0.2$, $\omega_0 = 1.0$, $A = 0.8$, $\omega_{\text{drive}} = 1.2$, and drift rates $\alpha = 0.005$ and $\beta = 0.002$. Under these conditions, the system initially remains within a bounded oscillatory regime but gradually exhibits deviations due to the drift terms, leading to an eventual departure from the confined state space. The effect of this perturbation is visualized in Figure~\ref{fig:CMPSS_pendulum_theta_phasespac}, where the phase space trajectory initially remains within a bounded region but progressively transitions into an unbounded state. 




\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CMPSS_pendulum_theta_phasespace.png}
        \caption{Phase plot of a damped forced motion, showing $\theta$ and $\dot{\theta}$. The \textbf{blue trajectory} represents the stable phase space evolution, while the \textbf{red trajectory} highlights an unstable deviation from the stable state.}

    \label{fig:CMPSS_pendulum_theta_phasespac}
\end{figure}

This formulation illustrates that the state variables of an oscillatory system, the angular displacement $\theta$ and its derivative $\dot{\theta}$, define a phase space in which the system’s behavior can be analyzed. In this phase space, it is possible to identify bounded regions corresponding to normal oscillations, where the system remains confined to a predictable, periodic trajectory. When the system leaves this bounded region, it indicates the onset of unbounded oscillations or deviations from normal behavior.

The key question is whether this concept extends to complex dynamical systems and what constraints must be addressed for a successful transition. Specifically, we seek the mathematical and structural conditions for mapping system dynamics to a reduced representation while preserving phase space properties.

High-dimensional, nonlinear systems introduce challenges such as maintaining topological consistency and preserving dynamical coherence. The goal is to identify the fundamental conditions and assumptions governing this embedding.

We systematically examine the mathematical foundation and computational requirements for such mappings, informing the design of effective latent representations in an autoencoder architecture for detecting deviations in high-dimensional systems

\subsection{Embedding Complex System Dynamics into a Latent Space}

We begin by formalizing the mathematical foundation for embedding high-dimensional dynamical systems into a reduced representation. 
Consider a \textbf{complex bounded dynamical system} with state variables $x \in \mathbb{R}^{\pi}$, where $\pi$ represents the dimension of the \textbf{physical system} and is typically much larger than the number of available measurements $( \pi \gg k$).

At the \textbf{measurement level}, we access observables through a measurement function:
\begin{equation}
\mu: \mathbb{R}^{\pi} \to \mathbb{R}^{k},
\end{equation} \label{eq:Mu}
where \(\mu \) extracts a lower-dimensional set of measurements \( y = \mu(x) \), with \( y \in \mathbb{R}^{k} \), forming the input to the \textbf{autoencoder}.

The \textbf{encoding level} is defined by:
\begin{equation}
\mathcal{E}: \mathbb{R}^{k} \to \mathbb{R}^{n},
\end{equation}\label{eq:Epsilon}

where \( \mathcal{E} \) maps the measurement space to an \textbf{embedding space} of dimension \( n \). 

In the normal state, system trajectories should be mapped onto a \textbf{compact manifold} in the latent space. The goal is to leverage the embedded representation so that a deviation in the latent space reliably indicates a departure from the stable state at the physical level.
We first analyze the latent space encoding of a dynamical system from a system theory perspective. While system dynamics in a stable regime often evolve on a structured manifold, measurements are not necessarily taken from a smooth manifold. These manifolds, in the mathematical sense, locally resemble Euclidean space \( \mathbb{R}^n \) and allow for the application of calculus \cite{lee_introduction_2012}. However, real-world measurements may be noisy or lie on fractal-like sets, requiring a more general mathematical framework.

Therefore, we turn to the \textit{Fractal Whitney Embedding Prevalence Theorem}, which extends the classical Whitney and Takens embedding results to settings where the underlying structure is not a smooth manifold. A key aspect of this generalization is the notion of dimensionality for compact subsets, which leads to the definition of \textit{box-counting dimension}.

\begin{definition}[Box-Counting Dimension {\cite{sauer_embedology_1991}}]
For a positive number \( \varepsilon \), let \( A_\varepsilon \) be the set of all points within \( \varepsilon \) of \( A \), i.e.,
\[
A_\varepsilon = \{ x \in \mathbb{R}^n : \| x - a \| \leq \varepsilon \text{ for some } a \in A \}.
\]
Let \( \operatorname{vol}(A_\varepsilon) \) denote the \( n \)-dimensional outer volume of \( A_\varepsilon \). The \textbf{box-counting dimension} of \( A \) is then defined as
\[
\operatorname{boxdim}(A) = n - \lim_{\varepsilon \to 0} \frac{\log \operatorname{vol}(A_\varepsilon)}{\log \varepsilon}.
\]
\end{definition}

Equipped with this notion of dimensionality, we now state the embedding theorem that generalizes classical results to fractal structures.

\begin{theorem}[Fractal Whitney Embedding Prevalence Theorem {\cite{sauer_embedology_1991}}]
\label{th:fractal_emeddbing}
Let \( A \) be a compact subset of \( \mathbb{R}^k \) with box-counting dimension \( d \), and let \( n \) be an integer greater than \( 2d \). For almost every smooth map \( F: \mathbb{R}^k \to \mathbb{R}^n \),
\begin{enumerate}
    \item \( F \) is one-to-one on \( A \).
    \item \( F \) is an immersion on each compact subset \( C \) of a smooth manifold contained in \( A \).
\end{enumerate}
\end{theorem}

If the measurement function is well-behaved, we can approximate the box-counting dimension using the dimension of bounded physical quantities that govern the real system dynamics in a stable regime. This allows us to estimate the required embedding dimension \( n \) to achieve a faithful description of the system’s dynamics.

After analyzing the necessary embedding dimension, we now turn to the type of embedding that best captures the dynamics of the system. We draw inspiration from simple dynamical systems and propose using \textit{state-derivative pairs} as an embedding strategy. This choice is motivated by two key principles. Physical laws typically encode \textit{causal relationships} between a system's state and its derivatives, making this a natural way to describe a dynamical process. When chosen properly, state-derivative pairs help \textit{reduce redundancy} by maintaining linear independence between embedding dimensions.

We hypothesize that leveraging this type of embedding allows for two key insights:
\begin{itemize}
    \item Deviations beyond a certain region associated with normal variability indicate anomalous behavior, as suggested by \textbf{phase space considerations}.
    \item  As the dynamics become more complex, this leads to an increase in the box-counting dimension. By Th.~\ref{th:fractal_emeddbing}, this increase suggests that the chosen embedding dimension is \textbf{insufficient} to fully describe the system. Consequently, we hypothesize that this insufficiency results in a \textbf{collapse of the dynamical description}, leading to the loss of the \textbf{causal approximation} between the state and its derivative.

\end{itemize}

To effectively utilize this type of embedding, we require more than just a one-to-one mapping—we also need \textit{consistent differential information}. That is, small changes in the latent space should correspond to meaningful changes in the measurement space. 
A fundamental tool for analyzing smooth mappings between differentiable manifolds is the \textbf{Jacobian matrix}, which captures the local behavior of the map in terms of its partial derivatives.
The \textbf{immersion theorem} ensures that local neighborhoods in the latent space retain the differential structure of the original system. This guarantees that the mapping not only remains locally invertible but also that variations in the system state are meaningfully preserved in the latent representation.
\begin{theorem}[Immersion {\cite{lee_introduction_2012}}] \label{th:immerions_theorem}
Suppose $ F: M \to N $ is a smooth map and $ p \in M $. If $ dF_p $ is \textbf{injective}, then there exists a neighborhood $ U $ of $ p $ such that the restriction $ F|_U $ is an \textbf{immersion}.
\end{theorem}

A sufficient condition for the immersion property is that the Jacobian of $ F $ has full column rank, ensuring that $ dF_p $ is injective \cite{lee_introduction_2012}. 
By establishing that our encoder is a one-to-one map and an immersion within the considered subspace of the system’s stable regime, we create a meaningful latent representation. From a theoretical perspective, this allows us to associate deviations in the latent space with deviations in the physical system.

\subsection{Application to Autoencoder-based Representations}
Having established the theoretical framework, we now seek to apply it to a practical use case. This requires two key components. First, we need a bounded dynamical system that operates in a stable regime with accessible measurement values. For this, we use the NASA C-MAPSS dataset, which provides time-series sensor data from aircraft engines—a complex yet stable dynamical system suitable for our approach. 

Second, we need to develop an algorithm that trains an autoencoder to learn embeddings aligned with our theoretical framework. This involves constructing a latent space representation based on state-derivative pairs while ensuring a one-to-one mapping and an immersion within the system’s normal operating regime. 

\subsubsection{C-MAPSS Dataset}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/CMPSS_Fan_scheme.png}
    \caption{Simplified C-MAPSS engine model with five rotating components: Fan, LPC, HPC, HPT, and LPT. N1 represents the low-pressure spool (Fan, LPC, LPT), while N2 represents the high-pressure spool (HPC, HPT).}
    \label{fig:CMPSS_Fan_scheme}
\end{figure}

The \textbf{NASA Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset} \cite{Saxena2008CMAPSS} is a widely used benchmark in prognostics and health management studies \cite{Saxena2008Damage, AlDulaimi2019, Benker2020} %gurkan more
. It consists of multivariate time series data representing the operational history of a fleet of engines, each experiencing different initial wear and manufacturing variations. The dataset includes three operational settings that significantly affect engine performance, as well as sensor readings capturing engine conditions, which are subject to noise contamination. Each engine operates normally at the beginning of the time series before developing a fault over time.  

While originally designed for Remaining Useful Life (RUL) estimation, the C-MAPSS dataset has become more popular lately in health management studies for \textbf{anomaly detection, fault diagnostics, and predictive maintenance}~\cite{jakubowski_anomaly_2022}. Its structured time-series format and realistic degradation patterns make it valuable for developing and testing machine learning models aimed at early fault detection and system health assessment.  

In this study, we specifically use the \textbf{FD001 subset} of the C-MAPSS dataset, which contains data from 100 engines operating under a single operating condition with varying fault progressions. To facilitate anomaly detection, we implement a 60/40 split, as suggested by similar studies in the literature~\cite{bataineh_autoencoder_2020, zhu_anomaly_2024}, where the first 60\% of cycles for each engine are labeled as normal, and the remaining 40\% are considered anomalous. Following this segmentation, we apply a random 80/20 train-test split across all engines to ensure a robust evaluation of our anomaly detection approach.  

The dataset includes engine operational settings, temperature measurements at various engine components, pressure readings, fan and core speed indicators, as well as different ratios and flow parameters related to efficiency and fuel usage. Additionally, variables associated with coolant bleed and bleed enthalpy provide insights into engine cooling and overall performance. 

\subsubsection{Applying the Theoretical Foundation}
In the NASA C-MAPSS dataset, the measurement space has a dimension of \( \mathbf{k = 24} \), representing sensor readings from the turbine system. To determine a suitable embedding dimension consistent with Th.~\ref{th:fractal_emeddbing}, we make the following assumptions about the system’s essential dynamics.

We hypothesize that the turbine’s core behavior is captured by three one-dimensional attractors, which could correspond to three state-derivative pairs, such as:
\begin{itemize}
    \item Rotational speed and its rate of change,
    \item Temperature and its rate of change,
    \item Pressure and its rate of change.
\end{itemize}

Given that the measurement function provides a structured representation of the underlying physical dynamics, we assume that it maps the real attractor dimension 3 to a box-counting dimension below 4, Based on Th.~\ref{th:fractal_emeddbing}, this allows us to select an embedding dimension of \( \mathbf{n = 8} \), ensuring a one-to-one mapping. \par

Based on our approach, we select \textbf{four state-derivative pairs} as the embedding. To ensure that this representation is at least an immersion in the subspace corresponding to the system’s normal or stable regime, we apply a numerical method to verify whether the Jacobian has full rank in the normal state of the test data. While this does not constitute a strict mathematical proof, it serves as a reliable guideline for assessing the validity of the embedding.

Singular Value Decomposition (SVD) is a fundamental matrix factorization technique that can be applied to any complex-valued matrix \( X \in \mathbb{C}^{n \times m} \). The decomposition expresses \( X \) in the following form:
\[
X = U \Sigma V^*,
\]
where:
\begin{itemize}
    \item \( U \in \mathbb{C}^{n \times n} \) is a unitary matrix with orthonormal columns.
    \item \( \Sigma \in \mathbb{R}^{n \times m} \) is a diagonal matrix with nonnegative real numbers on the diagonal and zeros elsewhere. These diagonal entries are known as the singular values of \( X \).
    \item \( V^* \in \mathbb{C}^{m \times m} \) is the conjugate transpose of a unitary matrix \( V \in \mathbb{C}^{m \times m} \).
\end{itemize}

This decomposition exists for any complex matrix and provides significant insights into the structure of the matrix. The singular values in \( \Sigma \) are ordered such that:
\[
\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_{\min(n, m)} \geq 0.
\]


SVD is particularly powerful for analyzing the rank of a matrix. The rank of \( X \) can be determined by counting the number of non-zero singular values in \( \Sigma \). If all singular values are non-zero, the matrix is full rank \cite{brunton_data-driven_2019}. 

\subsubsection{Temporal Differential Consistency informed Autoencoder}

As a foundational step, we require a method to approximate the first-time derivative. A widely used approach is the \textit{central difference method}, which estimates the derivative of a function by computing the slope between two points symmetrically positioned around the point of interest. The central difference formula for the first derivative of a function \( f(t) \) is:

\[
\dot{f}(t) \approx \frac{f(t+\Delta t) - f(t-\Delta t)}{2\Delta t}
\]

where \( \Delta t \) represents a small step size. This method is often preferred over forward or backward differences due to its higher accuracy (\( O(\Delta t^2) \)), meaning that the approximation error decreases quadratically as \( \Delta t \) decreases. Moreover, evaluating points symmetrically around the target reduces truncation errors and improves numerical stability, making the central difference method a reliable choice for derivative estimation \cite{chapra_numerical_2015}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/TDC_AE_Scheme.png}
    \caption{Schematic of temporal difference consistency in the latent space of an autoencoder. The derivative node $z_{\dot{}}$ is aligned with the central difference of the static node $z$ using time steps $t-1$, $t$, and $t+1$.}
    \label{fig:TDC_AE_scheme}
\end{figure}

To accurately capture temporal dynamics, we incorporate the central difference method into the training framework of the latent space. The latent representations at the previous (\( t-1 \)) and next (\( t+1 \)) time steps are used to approximate the first-time derivative of the static latent variables (\( z \)) using the central difference formula. This derivative is then used as a target for the dynamic latent variables (\( \dot{z} \)), ensuring consistency with the central difference approximation, scaled by the time interval \( \Delta t \). A schematic representation of this approach is shown in Fig.~\ref{fig:TDC_AE_scheme}. 

To incorporate temporal dynamics into the training process, we introduce a temporal differential consistency loss (TDC-Loss). This loss enforces consistency between the approximated time derivative of the static latent variables (\( z \)) and the output of the corresponding dynamic latent variables (\( \dot{z} \)). During training, the central difference method is used to estimate the time derivative of \( z \), which is then compared with \( \dot{z} \). The TDC-Loss is combined with the standard reconstruction loss, ensuring that the autoencoder learns a latent representation that captures both state and derivative information. A compact version of the pseudocode for TDC-informed training is presented in Algo.~\ref{alg:TDC_informed_training}.

\begin{algorithm}
\label{alg:TDC_informed_training}
\caption{Temporal Differential Consistency Loss Enhanced Autoencoder (TDC-AE)}
\begin{algorithmic}[1]
\STATE \textbf{Initialize} Autoencoder with input \( X_t \in \mathbb{R}^k \) and latent variables and time derivatives (\( \mathbf{z} ,\dot{\mathbf{z}}) \in \mathbb{R}^n \), Optimizer, MSE Loss
\FOR{each epoch in training\_epochs}
    \FOR{each batch in training\_data}
        \STATE Perform forward pass through the Autoencoder and Encoder: \\
        \( X_{\text{rec}} \gets \text{Autoencoder}(X_t) \)
        \STATE Compute latent representations for neighboring time steps: \\
        \( (\mathbf{z}_{t-1}, \dot{\mathbf{z}}_{t-1})   \gets \text{Encoder}(X_{t-1}) \) \\
        \( (\mathbf{z}_{t+1}, \dot{\mathbf{z}}_{t+1})   \gets \text{Encoder}(X_{t+1}) \) 
        \STATE Compute central difference derivative: \\
        \( \Delta_t \mathbf{z} \gets ( \mathbf{z}_{t+1} - \mathbf{z}_{t-1}) / 2\Delta_t \)
        \STATE Compute temporal differential consistency loss using MSE:\\ 
        \( \text{TDC-Loss} \gets \text{MSE}(\Delta_t \mathbf{z}, \dot{\mathbf{z}}) \)
        \STATE Compute reconstruction loss using MSE: \\
        \( \text{Rec-Loss} \gets \text{MSE}(X_{\text{rec}}, X_t) \)
        \STATE Backpropagate the total loss: \\
        Compute gradients w.r.t. model parameters for \( \text{Rec-Loss} + \alpha \cdot \text{TDC-Loss} \)
        \STATE Update the Autoencoder parameters using the optimizer
    \ENDFOR
\ENDFOR
\end{algorithmic}
\end{algorithm}

\subsubsection{Consistency KPIs}
We employ an embedding based on state-derivative pairs, where the derivative is approximated using the central difference method. While numerical differentiation is generally ill-posed, particularly in the presence of noise \cite{van_breugel_numerical_2020}, our approach does not seek a fully precise description of the system's dynamics. Instead, we aim for a causal approximation that reliably indicates when the system deviates from normal variability. By choosing a sufficiently large embedding dimension, we ensure that the system operates on a simple attractor geometry. This allows us to expect a nearly constant trendline for both the state and its derivative. To evaluate this, we introduce two key performance indicators (KPIs).\par

To leverage the causal approximation between state and derivative, the variation of both quantities must remain in a comparable range. We quantify this using the ratio of their z-score normalized variances:

\begin{equation}
    \eta = \frac{\sigma_{\dot{x}_z}^2}{\sigma_{x_z}^2},
\end{equation}

where $\sigma^2$ represents the variance of the z-score normalized values. The interpretation is as follows:

\begin{itemize}
    \item If $\eta \approx 1$, the variations are well-matched, and the central difference approximation is appropriate.
    \item If $\eta \ll 1$, the derivative is overly smoothed, possibly suppressing meaningful dynamical features.
    \item If $\eta \gg 1$, the derivative is too noisy, necessitating further smoothing or the use of more advanced differentiation techniques.
\end{itemize}



The second KPI evaluates how well the approximated derivative maintains consistency with the state transitions. Specifically, we compute the mean squared error (MSE) between the integrated derivative and the actual state difference over small time intervals:

\begin{equation}
    \rho = \frac{1}{N} \sum_{i=1}^{N} \left( x_i - \sum_{j=1}^{i} \dot{x}_j \Delta t \right)^2.
\end{equation}

This metric does not require knowledge of the true derivative but serves as a self-consistency check. By integrating the derivative, short-term fluctuations are smoothed, making long-term trends more apparent. A significant increase in this error in anomalous conditions suggests a breakdown in the causal approximation, indicating that the embedding dimension is insufficient to describe the system dynamics adequately.

\subsection{Anomaly Detection Logic}
We aim to incorporate the developed mathematical foundations into anomaly detection logic. By leveraging state-derivative pairs in the latent space, which approximate the causal underlying process, we assume that normal states can be enclosed within a confined region. Deviations from this region indicate abnormal behavior in the physical system.\par

Training is conducted using the first 50\% of the dataset, while validation is performed on data from 50\% to 60\%. A threshold for each latent node is determined based on validation data, set as a percentile of the validation split. This percentile value is optimized to maximize a classification performance metric, such as the F1-score. The threshold remains constant across all engines rather than being individually adapted. In testing, only the current time step is used, and the model infers the derivative representation from this single time step. This approach is motivated by the fact that the measurement at time $t$ inherently contains values that correspond to rate changes, effectively carrying this derivative information.

The detection logic follows these principles:

\begin{itemize}
    \item If all latent nodes exceed their respective thresholds during normal operation, it is classified as a false positive (FP). Otherwise, it is a true negative (TN).
    \item If at least one latent node exceeds its threshold in the anomaly range, it is classified as a true positive (TP); otherwise, it is a false negative (FN).
\end{itemize}

The detection logic can be adjusted based on specific use cases and risk management strategies, such as requiring multiple nodes to surpass thresholds before classifying an instance as an anomaly.



\section{Results}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CMPSS_mse_tdc_loss.png}
    \caption{The two loss terms used in TDC-informed training across five independent attempts on the C-MAPSS test dataset.}
    \label{fig:CMPSS_mse_tdc_loss}
\end{figure}

\subsection{Consistency of the Proposed Approach}

First, we analyze the consistency of the proposed approach. Fig.~\ref{fig:CMPSS_mse_tdc_loss} presents the evolution of the loss terms over 50 training epochs across five independent attempts. The plot shows both the standard Mean Squared Error (MSE) loss and the newly introduced TDC loss term. We observe consistent convergence in the TDC loss term, indicating stable training behavior.

Furthermore, we conducted a Jacobian rank analysis using Singular Value Decomposition (SVD). Applying a zero-threshold of $\epsilon = 10^{-9}$, all samples in the test dataset exhibit a full-rank Jacobian. This result suggests that the mapping $\mathcal{E}$, which projects the measurement space $\mathbb{R}^k$ to an embedding space $\mathbb{R}^n$, behaves as an immersion, preserving the differential structure.\par

Next, we evaluate two key performance indicators (KPIs) introduced in this study. The first KPI, denoted as $\sigma$, assesses whether the central difference method provides an appropriate approximation of the first time derivative. This metric was computed individually for each engine in the training split (first 60\% of the timesteps). The results indicate values close to 1, with deviations in the order of $10^{-7}$, confirming the validity of the approximation.\par


The second KPI evaluates the relationship between state derivatives, reflecting how well the causal approximation holds across latent nodes in both normal and abnormal conditions. Table~\ref{tab:rho_values} summarizes the mean $\pm$ standard deviation values of the correlation coefficient $\rho$ in both normal and anomalous conditions.

\begin{table}[h]
    \centering
    \caption{Comparison of mean $\pm$ standard deviation values of $\rho$ for normal and anomalous conditions across latent node pairs.}
    \label{tab:rho_values}
    \begin{tabular}{lcc}
        \hline
        Latent Node Pair & Normal Condition ($\rho$) & Anomalous Condition ($\rho$) \\
        \hline
        (0-4) & $0.00037 \pm 0.00012$ & $0.00044 \pm 0.00016$ \\
        (1-5) & $0.00019 \pm 0.00002$ & $0.00034 \pm 0.00015$ \\
        (2-6) & $0.00026 \pm 0.00007$ & $0.00079 \pm 0.00027$ \\
        (3-7) & $0.00048 \pm 0.00014$ & $0.00073 \pm 0.00011$ \\
        \hline
    \end{tabular}
\end{table}

The results indicate that in the normal regime, the correlation values remain low with minimal variation. However, in the anomalous regime, we observe a slight increase in mean values and a broader standard deviation range, suggesting a stronger deviation in state derivative relationships under anomaly conditions.

\subsection{Phase Space Representation and State-Derivative Relationships}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CMPSS_detailed_phasespace.png}
    \caption{(a) Phase-space plot of the latent node pair (2,6) average.
(b) State-derivative consistency KPI for this pair.
(c) A detailed view of (a) (highlighted grey box), showing phase-space trajectories and the attractor region at the beginning of the system dynamics. The timestep color map applies to all figures.}
    \label{fig:CMAPSS_latent_detailed}
\end{figure}
Fig.~\ref{fig:CMAPPS_latent_big}a-f presents two state-derivative pairs along with their respective phase space representations. A gradual increase or decrease in both state and derivative nodes is observed as the system transitions toward the anomalous range. In the phase space plots, we applied a sliding window of three and slightly smoothed the node values for visualization purposes. This processing was not used in the actual anomaly detection model. During the initial phase of the system dynamics, the states evolve more gradually, supporting the hypothesis that, in the normal regime, the dynamics follow a simple and stable attractor geometry. The phase space representation further indicates that normal states are embedded more densely than anomalies, with a continuous trend toward the anomalous region. \par

 

The Jacobian analysis suggests that the latent states are linearly independent, as indicated by the full-rank property observed in the Singular Value Decomposition (SVD) analysis. However, this does not inherently enforce an orthonormal projection, meaning that scale freedom exists and angular relationships are not necessarily preserved.

Nevertheless, from a phase-space perspective, the qualitative relationship between states remains intact. Anomalous states continue to map to regions outside the bounded domain of normal states, ensuring that the essential structural distinction is preserved, even if exact angles and scales are distorted.\par
Our objective is to develop methods that integrate physics-inspired consistency principles while imposing only the necessary constraints. This approach maintains the model’s flexibility and ability to learn effectively. Therefore, we do not enforce an explicit orthonormality condition in the AE design.
\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/CMPSS_big_latentspace.png}
    \caption{Latent node state–derivative pairs for nodes (2, 6) are shown in panels (a) and (b), and for nodes (3, 7) in panels (d) and (e). The corresponding phase-space representations for these pairs are displayed in panels (c) and (f), respectively. In all phase-space plots, the node values have been smoothed using a sliding average window of 3.}
    \label{fig:CMAPPS_latent_big}
\end{figure*}
Let's have a more detailed view on the phase space evolution. Fig.~\ref{fig:CMAPSS_latent_detailed}a illustrates a clear drift toward the anomalous region, highlighting a gradual deviation in system behavior. Fig.~\ref{fig:CMAPSS_latent_detailed}b presents the evolution of $\rho$, which quantifies the causal approximation of state-derivative pairs. In the normal range, $\rho$ remains stable on average, indicating a consistent dynamical description. However, as the system enters the anomalous regime, a significant increase in $\rho$ is observed, suggesting a loss in the dynamical consistency of the system. The theoretical derivation showed that when system dynamics deviate from a stable state, the increasing complexity requires a higher-dimensional embedding to adequately capture the system’s behavior. We hypothesized that this effect would lead to the collapse of state-derivative approximations, serving as an indicator of anomalous behavior. Our observations now confirm this, as we see a loss of dynamical consistency in the latent space, reinforcing our hypothesis that anomalies emerge when the system transitions beyond the representational capacity of the learned embedding.

In Fig.~\ref{fig:CMAPSS_latent_detailed}c, we observe the evolution of the phase space representation over time, transitioning from normal to abnormal behavior. Initially, the phase space exhibits an attractor-like structure around which the dynamics evolve in a stable manner. As the system progresses, we observe increased dispersion and a gradual drift away from the attractor toward the anomalous region. Even while still within the range characterized as normal, the state-derivative approximation remains stable, as seen in Fig.~\ref{fig:CMAPSS_latent_detailed}a. However, subtle deviations from the attractor’s dynamics can already be detected in the earliest phase of the transition. This aligns with our second hypothesized effect, which suggests that even minor disruptions in the system’s stability could serve as an early indicator of anomalies.\par

These findings reinforce that our model is not merely a data-driven classifier for anomaly detection but instead captures meaningful insights into the underlying system dynamics.

\subsection{Comparison with Literature Benchmarks}
\begin{table*}[h]
\caption{Performance comparison of different approaches benchmarks for anomaly detection in the C-MAPSS dataset.
}
\centering
\begin{tabularx}{0.6\textwidth}{lXXXXX}
\toprule
\textbf{Algorithm} & \textbf{Acc. (\%)} & \textbf{Prec. (\%)} & \textbf{Rec. (\%)} & \textbf{F1 (\%)} \\
\midrule
\textbf{TDC-AE} & $\mathbf{99.3} \pm 0.3$ & $\mathbf{99.99}\pm 0.03$ & $\mathbf{98.30}\pm 0.07$ & $\mathbf{99.14} \pm 0 .04$  \\ \hline
DeepLSTM-AE \cite{zhu_anomaly_2024} & 96.45 & 94.81 & 98.12 & 96.44 \\  \hline
Random Forest \cite{zhu_anomaly_2024} & 92.33 & 97.40 & 86.66 & 91.71 \\  \hline
CNN-AE \cite{zhu_anomaly_2024} & 92.22 & 89.08 & 95.86 & 92.35 \\  \hline
SMOTE Transformer \cite{liu_deep_2023} & 89.59 & 93.95 & 91.77 & - \\ \hline
XGBoost \cite{zhu_anomaly_2024} & 84.57 & 77.73 & 96.00 & 85.90  \\ \hline
Dense-AE \cite{bataineh_autoencoder_2020} & - & 89.6 & 72.4 & 80.1 \\ 
\bottomrule
\end{tabularx}

\label{tab:performance_comparison}
\end{table*}
We have discussed the consistency of our method and analyzed how the TDC-AE aligns with the mathematical foundation developed in this paper. Now, we evaluate its detection performance in comparison to literature benchmarks.

Tab.~\ref{tab:performance_comparison} presents a comparison across different architectures. Our approach outperforms all reported models across all detection metrics, demonstrating consistent results across repeated attempts. The latent node threshold in all attempts was set using the 75th percentile of the validation set.

Even complex models, such as LSTM-AEs, Transformer with attention mechanism designed to capture long-range temporal correlations, do not achieve superior performance. In \cite{zhu_anomaly_2024}, the exact LSTM architecture is not explicitly provided, so we base our assumptions on similar studies of complex physical systems \cite{mahmoud_ae-lstm_2022}, considering the following configuration: sequence length \( L = 48 \), hidden dimension \( d_{\text{hidden}} = 16 \), and input dimension \( d_{\text{input}} = 24 \). This results in a Multiply-Accumulate Operations (MAC) count of 245,760.

In contrast, our approach achieves superior detection performance with significantly lower computational complexity, requiring only 2,688 MACs. 

\section{Conclusion}
We introduced an unsupervised framework for anomaly detection in complex physical systems, grounded in \textbf{classical embedding theory} and physics-inspired consistency principles, particularly \textbf{state-derivative relations} in the embedding space. To translate this theoretical foundation into practice, we developed \textbf{TDC-AE}, an algorithm designed to capture system dynamics efficiently.

Our results demonstrate that time correlation can be effectively preserved in a 1D embedding, enabling robust anomaly detection. TDC-AE outperforms benchmarks on the C-MAPSS dataset, surpassing computationally intensive models like LSTMs and Transformers, while achieving a \textbf{200x reduction in MAC operations}, keeping them below 5,000 MACs. This efficiency makes our approach particularly suitable for lightweight edge computing applications.\par
Future work will focus on further leveraging the concept of state-derivative pairs by expanding on system-theoretic perspectives and physical modeling principles to make the method accessible for further use cases and other types of dynamical systems.

\input{ref.bbl}

\end{document}