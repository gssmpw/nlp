@article{Argyle_2023,
   title={Out of One, Many: Using Language Models to Simulate Human Samples},
   volume={31},
   ISSN={1476-4989},
   url={http://dx.doi.org/10.1017/pan.2023.2},
   DOI={10.1017/pan.2023.2},
   number={3},
   journal={Political Analysis},
   publisher={Cambridge University Press (CUP)},
   author={Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
   year={2023},
   month=feb, pages={337–351} }

@ARTICLE{Karjus2024-zg,
  title     = "Evolving linguistic divergence on polarizing social media",
  author    = "Karjus, Andres and Cuskley, Christine",
  abstract  = "AbstractLanguage change is influenced by many factors, but often
               starts from synchronic variation, where multiple linguistic
               patterns or forms coexist, or where different speech communities
               use language in increasingly different ways. Besides regional or
               economic reasons, communities may form and segregate based on
               political alignment. The latter, referred to as political
               polarization, is of growing societal concern across the world.
               Here we map and quantify linguistic divergence across the
               partisan left-right divide in the United States, using social
               media data. We develop a general methodology to delineate
               (social) media users by their political preference, based on
               which (potentially biased) news media accounts they do and do
               not follow on a given platform. Our data consists of 1.5M short
               posts by 10k users (about 20M words) from the social media
               platform Twitter (now ``X''). Delineating this sample involved
               mining the platform for the lists of followers (n = 422M) of 72
               large news media accounts. We quantify divergence in topics of
               conversation and word frequencies, messaging sentiment, and
               lexical semantics of words and emoji. We find signs of
               linguistic divergence across all these aspects, especially in
               topics and themes of conversation, in line with previous
               research. While US American English remains largely intelligible
               within its large speech community, our findings point at areas
               where miscommunication may eventually arise given ongoing
               polarization and therefore potential linguistic divergence. Our
               flexible methodology --- combining data mining,
               lexicostatistics, machine learning, large language models and a
               systematic human annotation approach --- is largely language and
               platform agnostic. In other words, while we focus here on US
               political divides and US English, the same approach is
               applicable to other countries, languages, and social media
               platforms.",
  journal   = "Humanit. Soc. Sci. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  11,
  number    =  1,
  month     =  mar,
  year      =  2024,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@inproceedings{borst_death_2023,
	location = {Paris},
	title = {Death of the Dictionary? – The Rise of Zero-Shot Sentiment Classification},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3558/paper3130.pdf},
	eventtitle = {{CHR} 2023: Computational Humanities Research Conference},
	author = {Borst, Janos and Klähn, Jannis and Burghardt, Manuel},
	date = {2023-12-06},
}

@inproceedings{cegin_chatgpt_2023,
	location = {Singapore},
	title = {{ChatGPT} to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness},
	url = {https://aclanthology.org/2023.emnlp-main.117},
	doi = {10.18653/v1/2023.emnlp-main.117},
	shorttitle = {{ChatGPT} to Replace Crowdsourcing of Paraphrases for Intent Classification},
	eventtitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	pages = {1889--1905},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Cegin, Jan and Simko, Jakub and Brusilovsky, Peter},
	urldate = {2025-01-19},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\NDAWVE3D\\Cegin et al. - 2023 - ChatGPT to Replace Crowdsourcing of Paraphrases fo.pdf:application/pdf},
}

@article{de_lange_benchmarking_2024,
	title = {Benchmarking Zero-Shot Text Classification for Dutch},
	volume = {Computational Linguistics in the Netherlands Journal},
	url = {https://clinjournal.org/clinj/article /view/172},
	pages = {63--90},
	number = {13},
	author = {De Lange, Loic and Vanroy, Bram and De Bruyne, Luna and Singh, Pranaydeep and Lefever, Els and De Clercq, Orphée},
	date = {2024},
}

@article{dillion_can_2023,
	title = {Can {AI} language models replace human participants?},
	volume = {27},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661323000980},
	doi = {10.1016/j.tics.2023.04.008},
	pages = {597--600},
	number = {7},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Dillion, Danica and Tandon, Niket and Gu, Yuling and Gray, Kurt},
	urldate = {2025-01-28},
	date = {2023-07},
	langid = {english},
}

@article{duncan_does_2024,
	title = {Does {ChatGPT} have sociolinguistic competence?},
	volume = {8},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2530-9455},
	url = {https://polipapers.upv.es/index.php/jclr/article/view/21958},
	doi = {10.4995/jclr.2024.21958},
	abstract = {Large language models are now able to generate content- and genre-appropriate prose with grammatical sentences. However, these targets do not fully encapsulate human-like language use. For example, set aside is the fact that human language use involves sociolinguistic variation that is regularly constrained by internal and external factors. This article tests whether one widely used {LLM} application, {ChatGPT}, is capable of generating such variation. I construct an English corpus of “sociolinguistic interviews” using the application and analyze the generation of seven morphosyntactic features. I show that the application largely fails to generate any variation at all when one variant is prescriptively incorrect, but that it is able to generate variable deletion of the complementizer that that is internally constrained, with variants occurring at human-like rates. {ChatGPT} fails, however, to properly generate externally constrained complementizer that deletion. I argue that these outcomes reflect bias both in the training data and Reinforcement Learning from Human Feedback. I suggest that testing whether an {LLM} can properly generate sociolinguistic variation is a useful metric for evaluating if it generates human-like language.},
	pages = {51--75},
	journaltitle = {Journal of Computer-Assisted Linguistic Research},
	shortjournal = {J. Comp. Assist. Linguist. Res.},
	author = {Duncan, Daniel},
	urldate = {2025-01-29},
	date = {2024-11-15},
}

@article{dynel_lessons_2023,
	title = {Lessons in linguistics with {ChatGPT}: Metapragmatics, metacommunication, metadiscourse and metalanguage in human-{AI} interactions},
	volume = {93},
	issn = {02715309},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0271530923000605},
	doi = {10.1016/j.langcom.2023.09.002},
	shorttitle = {Lessons in linguistics with {ChatGPT}},
	pages = {107--124},
	journaltitle = {Language \& Communication},
	shortjournal = {Language \& Communication},
	author = {Dynel, Marta},
	urldate = {2025-01-29},
	date = {2023-11},
	langid = {english},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} Outperforms Crowd-Workers for Text-Annotation Tasks},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2303.15056},
	doi = {10.48550/ARXIV.2303.15056},
	abstract = {Many {NLP} applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as {MTurk} as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that {ChatGPT} outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of {ChatGPT} exceeds that of crowd-workers for four out of five tasks, while {ChatGPT}'s intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of {ChatGPT} is less than \$0.003 -- about twenty times cheaper than {MTurk}. These results show the potential of large language models to drastically increase the efficiency of text classification.},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	urldate = {2025-01-30},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences},
	annotation = {Other
Gilardi, Fabrizio, Meysam Alizadeh, and Ma{\textbackslash}"el Kubli. 2023. "{ChatGPT} Outperforms Crowd Workers for Text-Annotation Tasks". Proceedings of the National Academy of Sciences 120(30): e2305016120},
}

@article{gui_challenge_2023,
	title = {The Challenge of Using {LLMs} to Simulate Human Behavior: A Causal Inference Perspective},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2312.15524},
	doi = {10.48550/ARXIV.2312.15524},
	shorttitle = {The Challenge of Using {LLMs} to Simulate Human Behavior},
	abstract = {Large Language Models ({LLMs}) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when {LLM}-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of {LLM} simulations: controlled covariates become artificially salient in the simulated decision process, which introduces focalism. This trade-off between unconfoundedness and ecological validity is usually absent in traditional experimental design and represents a unique challenge in {LLM} simulations. We formalize this challenge theoretically, showing it stems from ambiguous prompting strategies, and hence cannot be fully addressed by improving training data or by fine-tuning. Alternative approaches that unblind the experimental design to the {LLM} show promise. Our findings suggest that effectively leveraging {LLMs} for experimental simulations requires fundamentally rethinking established experimental design practices rather than simply adapting protocols developed for human subjects.},
	author = {Gui, George and Toubia, Olivier},
	urldate = {2025-01-28},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Applications (stat.{AP}), Econometrics (econ.{EM}), {FOS}: Economics and business, Information Retrieval (cs.{IR})},
}

@article{han_chatgpt_2024,
	title = {Chatgpt in and for second language acquisition: A call for systematic research},
	volume = {46},
	issn = {0272-2631, 1470-1545},
	url = {https://www.cambridge.org/core/product/identifier/S0272263124000111/type/journal_article},
	doi = {10.1017/S0272263124000111},
	shorttitle = {Chatgpt in and for second language acquisition},
	pages = {301--306},
	number = {2},
	journaltitle = {Studies in Second Language Acquisition},
	shortjournal = {Stud Second Lang Acquis},
	author = {Han, {ZhaoHong}},
	urldate = {2025-01-29},
	date = {2024-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\WALDTGEL\\Han - 2024 - Chatgpt in and for second language acquisition A .pdf:application/pdf},
}

@misc{he_annollm_2023,
	title = {{AnnoLLM}: Making Large Language Models to Be Better Crowdsourced Annotators},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.16854},
	doi = {10.48550/ARXIV.2303.16854},
	shorttitle = {{AnnoLLM}},
	abstract = {Many natural language processing ({NLP}) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, {GPT}-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various {NLP} tasks. In this paper, we first claim that large language models ({LLMs}), such as {GPT}-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose {AnnoLLM}, an annotation system powered by {LLMs}, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt {LLMs} to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with {LLMs}. Our experiment results on three tasks, including user input and keyword relevance assessment, {BoolQ}, and {WiC}, demonstrate that {AnnoLLM} surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing {AnnoLLM}. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.},
	publisher = {{arXiv}},
	author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	urldate = {2025-01-08},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
	annotation = {Other
Accepted to {NAACL} 2024},
}

@misc{horych_promises_2024,
	title = {The Promises and Pitfalls of {LLM} Annotations in Dataset Labeling: a Case Study on Media Bias Detection},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2411.11081},
	doi = {10.48550/ARXIV.2411.11081},
	shorttitle = {The Promises and Pitfalls of {LLM} Annotations in Dataset Labeling},
	abstract = {High annotation costs from hiring or crowdsourcing complicate the creation of large, high-quality datasets needed for training reliable text classifiers. Recent research suggests using Large Language Models ({LLMs}) to automate the annotation process, reducing these costs while maintaining data quality. {LLMs} have shown promising results in annotating downstream tasks like hate speech detection and political framing. Building on the success in these areas, this study investigates whether {LLMs} are viable for annotating the complex task of media bias detection and whether a downstream media bias classifier can be trained on such data. We create annolexical, the first large-scale dataset for media bias classification with over 48000 synthetically annotated examples. Our classifier, fine-tuned on this dataset, surpasses all of the annotator {LLMs} by 5-9 percent in Matthews Correlation Coefficient ({MCC}) and performs close to or outperforms the model trained on human-labeled data when evaluated on two media bias benchmark datasets ({BABE} and {BASIL}). This study demonstrates how our approach significantly reduces the cost of dataset creation in the media bias domain and, by extension, the development of classifiers, while our subsequent behavioral stress-testing reveals some of its current limitations and trade-offs.},
	publisher = {{arXiv}},
	author = {Horych, Tomas and Mandl, Christoph and Ruas, Terry and Greiner-Petter, Andre and Gipp, Bela and Aizawa, Akiko and Spinde, Timo},
	urldate = {2025-01-14},
	date = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
}

@misc{karjus2024machineassisted,
      title={Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence}, 
      author={Andres Karjus},
      year={2024},
      eprint={2309.14379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.14379}, 
}

@article{kocon_chatgpt_2023,
	title = {{ChatGPT}: Jack of all trades, master of none},
	volume = {99},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S156625352300177X},
	doi = {10.1016/j.inffus.2023.101861},
	shorttitle = {{ChatGPT}},
	pages = {101861},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Kocoń, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydło, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and Kocoń, Anna and Koptyra, Bartłomiej and Mieleszczenko-Kowszewicz, Wiktoria and Miłkowski, Piotr and Oleksy, Marcin and Piasecki, Maciej and Radliński, Łukasz and Wojtasik, Konrad and Woźniak, Stanisław and Kazienko, Przemysław},
	urldate = {2025-01-29},
	date = {2023-11},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\BCDH XMG\\Zotero\\storage\\PHIFWDYI\\Kocoń et al. - 2023 - ChatGPT Jack of all trades, master of none.pdf:application/pdf},
}

@article{kosar_comparative_2024,
	title = {Comparative Evaluation of Topic Detection: Humans vs. {LLMs}},
	volume = {Computational Linguistics in the Netherlands Journal},
	url = {https://www.clinjournal.org/clinj/article/view/173},
	pages = {91--120},
	number = {13},
	author = {Kosar, A and De Pauw, G and Daelemans, W},
	date = {2024},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 Technical Report},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.08774},
	doi = {10.48550/ARXIV.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	publisher = {{arXiv}},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and {Pokorny} and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, CJ and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2025-01-28},
	date = {2023},
	note = {Version Number: 6},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
	annotation = {Other
100 pages; updated authors list; fixed author names and added citation},
}

@inproceedings{ostyakova_chatgpt_2023,
	location = {Prague, Czechia},
	title = {{ChatGPT} vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions},
	url = {https://aclanthology.org/2023.sigdial-1.23},
	doi = {10.18653/v1/2023.sigdial-1.23},
	shorttitle = {{ChatGPT} vs. Crowdsourcing vs. Experts},
	eventtitle = {Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue},
	pages = {242--254},
	booktitle = {Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue},
	publisher = {Association for Computational Linguistics},
	author = {Ostyakova, Lidiia and Smilga, Veronika and Petukhova, Kseniia and Molchanova, Maria and Kornev, Daniel},
	urldate = {2025-01-15},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\ZQE2MSAC\\Ostyakova et al. - 2023 - ChatGPT vs. Crowdsourcing vs. Experts Annotating .pdf:application/pdf},
}

@inproceedings{rebora_comparing_2023,
	location = {Paris},
	title = {Comparing {ChatGPT} to Human Raters and Sentiment Analysis Tools for German Children’s Literature},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3558/paper3340.pdf},
	eventtitle = {{CHR} 2023: Computational Humanities Research Conference,},
	author = {Rebora, Simone and Lehmann, Marina and Heumann, Anne and Ding, Wei and Lauer, Gerhard},
	date = {2023-12-06},
}

@misc{tornberg_chatgpt-4_2023,
	title = {{ChatGPT}-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2304.06588},
	doi = {10.48550/ARXIV.2304.06588},
	abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model ({LLM}) {ChatGPT}-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The {LLM} is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that {ChatGPT}-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The {LLM} is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that {LLM} will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
	publisher = {{arXiv}},
	author = {Törnberg, Petter},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Social and Information Networks (cs.{SI})},
	annotation = {Other
5 pages, 3 figures},
}

@inproceedings{van_dalfsen_direct_2024,
	location = {Aarhus, Denmark},
	title = {Direct and Indirect Annotation with Generative {AI}: A Case Study into Finding Animals and Plants in Historical Text},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3834/paper74.pdf},
	abstract = {This study explores the use of generative {AI} ({GenAI}) for annotation in the humanities, comparing direct
and indirect annotation approaches with human annotations. Direct annotation involves using {GenAI}
to annotate the entire corpus, while indirect annotation uses {GenAI} to create training data for a special-
ized model. The research investigates zero-shot and few-shot methods for direct annotation, alongside
an indirect approach incorporating active learning, few-shotting, and k-{NN} example retrieval. The task
focuses on identifying words (also referred to as entities) related to plants and animals in Early Modern
Dutch texts. Results show that indirect annotation outperforms zero-shot direct annotation in mimick-
ing human annotations. However, with just a few examples, direct annotation catches up, achieving
similar performance to indirect annotation. Analysis of confusion matrices reveals that {GenAI} annota-
tors make similar types of mistakes, such as confusing parts and products or failing to identify entities,
which are broader than those made by humans. Manual error analysis indicates that each annotation
method (human, direct, and indirect) has some unique errors. Given the limited scale of this study, it is
worthwhile to further explore the relative affordances of direct and indirect {GenAI} annotation methods.},
	eventtitle = {{CHR} 2024: Computational Humanities Research Conference},
	author = {Van Dalfsen, Arjan and Karsdorp, Folgert and Bagheri, Ayoub and Mentink, Dieuwertje and van Engelen, Thirza and Stronks, Els},
	date = {2024-12-04},
}

@inproceedings{wu_llms_2025,
	location = {Yokohama, Japan},
	title = {{LLMs} as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with {LLMs}},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	isbn = {79-8-4007-1395-8/25/04},
	url = {https://arxiv.org/abs/2307.10168},
	doi = {10.48550/ARXIV.2307.10168},
	shorttitle = {{LLMs} as Workers in Human-Computational Algorithms?},
	abstract = {{LLMs} have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether {LLMs} can replicate more complex crowdsourcing pipelines. We find that modern {LLMs} can simulate some of crowdworkers' abilities in these ``human computation algorithms,'' but the level of success is variable and influenced by requesters' understanding of {LLM} capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and {LLMs}' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for {LLMs}, and discuss the potential of training humans and {LLMs} with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate 1) the relative {LLM} strengths on different tasks (by cross-comparing their performances on sub-tasks) and 2) {LLMs}' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.},
	eventtitle = {{CHI} {EA} ’25},
	author = {Wu, Tongshuang and Zhu, Haiyi and Albayrak, Maya and Axon, Alexis and Bertsch, Amanda and Deng, Wenxing and Ding, Ziqi and Guo, Bill and Gururaja, Sireesh and Kuo, Tzu-Sheng and Liang, Jenny T. and Liu, Ryan and Mandal, Ihita and Milbauer, Jeremiah and Ni, Xiaolin and Padmanabhan, Namrata and Ramkumar, Subhashini and Sudjianto, Alexis and Taylor, Jordan and Tseng, Ying-Jui and Vaidos, Patricia and Wu, Zhijin and Wu, Wei and Yang, Chenyang},
	urldate = {2025-01-30},
	date = {2025-05-26},
	note = {Publisher: {arXiv}
Version Number: 3},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Human-Computer Interaction (cs.{HC})},
}

@article{xu_role_2024,
	title = {On the Role of Large Language Models in Crowdsourcing Misinformation Assessment},
	volume = {18},
	issn = {2334-0770, 2162-3449},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31417},
	doi = {10.1609/icwsm.v18i1.31417},
	abstract = {The proliferation of online misinformation significantly undermines the credibility of web content. Recently, crowd workers have been successfully employed to assess misinformation to address the limited scalability of professional fact-checkers. An alternative approach to crowdsourcing is the use of large language models ({LLMs}). These models are however also not perfect. In this paper, we investigate the scenario of crowd workers working in collaboration with {LLMs} to assess misinformation. We perform a study where we ask crowd workers to judge the truthfulness of statements under different conditions: with and without {LLMs} labels and explanations.  Our results show that crowd workers tend to overestimate truthfulness when exposed to {LLM}-generated information. Crowd workers are misled by wrong {LLM} labels, but, on the other hand, their self-reported confidence is lower when they make mistakes due to relying on the {LLM}. We also observe diverse behaviors among crowd workers when the {LLM} is presented, indicating that leveraging {LLMs} can be considered a distinct working strategy.},
	pages = {1674--1686},
	journaltitle = {Proceedings of the International {AAAI} Conference on Web and Social Media},
	shortjournal = {{ICWSM}},
	author = {Xu, Jiechen and Han, Lei and Sadiq, Shazia and Demartini, Gianluca},
	urldate = {2024-07-23},
	date = {2024-05-28},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\6ZQDILUZ\\Xu et al. - 2024 - On the Role of Large Language Models in Crowdsourc.pdf:application/pdf},
}

@article{ziems_can_2024,
	title = {Can Large Language Models Transform Computational Social Science?},
	volume = {50},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/50/1/237/118498/Can-Large-Language-Models-Transform-Computational},
	doi = {10.1162/coli_a_00502},
	abstract = {Abstract
            Large language models ({LLMs}) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot {LLMs} can also reliably classify and explain social phenomena like persuasiveness and political ideology, then {LLMs} could augment the computational social science ({CSS}) pipeline in important ways. This work provides a road map for using {LLMs} as {CSS} tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English {CSS} benchmarks. On taxonomic labeling tasks (classification), {LLMs} fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), {LLMs} produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s {LLMs} can augment the {CSS} research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, {LLMs} are posed to meaningfully participate in social science analysis in partnership with humans.},
	pages = {237--291},
	number = {1},
	journaltitle = {Computational Linguistics},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	urldate = {2025-01-29},
	date = {2024-03-01},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\DRFZM6BA\\Ziems et al. - 2024 - Can Large Language Models Transform Computational .pdf:application/pdf},
}

