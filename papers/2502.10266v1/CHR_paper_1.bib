
@article{wu2025,
	title = {A Post-Processing Framework for Crowd Worker Responses Using Large Language Models},
	url = {https://doi.org/10.1145/3706599.3706690},
	abstract = {We propose a framework that uses {LLMs} to
flexibly aggregate natural language responses from
workers and, as a promising example, consider this
framework for crime detection from surveillance cameras
using crowdsourced cognitive abilities.},
	author = {Ryuya {ITANO}, Tatsuki {TAMANO}, Takahiro {KOITAHonoka} {TANITSU}},
	date = {2023},
}

@article{tongshuang_wu_llms_2023,
	title = {{LLMs} as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with {LLMs}},
	url = {https://arxiv.org/pdf/2307.10168},
	author = {Tongshuang Wu},
	date = {2023},
}

@article{noauthor_comparative_2024,
	title = {A {COMPARATIVE} {STUDY} {ON} {ANNOTATION} {QUALITY} {OF} {CROWDSOURCING} {AND} {LLM} {VIA} {LABEL} {AGGREGATION}},
	abstract = {we first investigate
which existing crowdsourcing datasets can be used for a com-
parative study and create a benchmark. We then compare the
quality between individual crowd labels and {LLM} labels and
make the evaluations on the aggregated labels. In addition, we
propose a Crowd-{LLM} hybrid label aggregation method and
verify the performance. We find that adding {LLM} labels from
good {LLMs} to existing crowdsourcing datasets can enhance
the quality of the aggregated labels of the datasets, which is
also higher than the quality of {LLM} labels themselves.},
	author = {, Jiyi Li},
	date = {2024},
}

@article{jiechen_xu_lei_han_shazia_sadiq_gianluca_demartini_role_2024,
	title = {On the Role of Large Language Models in Crowdsourcing {MisinformationAssessment}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31417/33577},
	author = {Jiechen Xu, Lei Han, Shazia Sadiq, Gianluca Demartini},
	date = {2024},
}

@misc{wu_llms_2023,
	title = {{LLMs} as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with {LLMs}},
	url = {http://arxiv.org/abs/2307.10168},
	shorttitle = {{LLMs} as Workers in Human-Computational Algorithms?},
	abstract = {{LLMs} have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether {LLMs} can replicate more complex crowdsourcing pipelines. We find that modern {LLMs} can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of {LLM} capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and {LLMs}' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for {LLMs}, and discuss the potential of training humans and {LLMs} with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of {LLMs} on different tasks (by cross-comparing their performances on sub-tasks) and (2) {LLMs}' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.},
	number = {{arXiv}:2307.10168},
	publisher = {{arXiv}},
	author = {Wu, Tongshuang and Zhu, Haiyi and Albayrak, Maya and Axon, Alexis and Bertsch, Amanda and Deng, Wenxing and Ding, Ziqi and Guo, Bill and Gururaja, Sireesh and Kuo, Tzu-Sheng and Liang, Jenny T. and Liu, Ryan and Mandal, Ihita and Milbauer, Jeremiah and Ni, Xiaolin and Padmanabhan, Namrata and Ramkumar, Subhashini and Sudjianto, Alexis and Taylor, Jordan and Tseng, Ying-Jui and Vaidos, Patricia and Wu, Zhijin and Wu, Wei and Yang, Chenyang},
	urldate = {2024-07-23},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.10168 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:C\:\\Users\\BCDH XMG\\Zotero\\storage\\87CL2F65\\Wu et al. - 2023 - LLMs as Workers in Human-Computational Algorithms.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\BCDH XMG\\Zotero\\storage\\VQ7I9RYF\\2307.html:text/html},
}

@misc{li_comparative_2024,
	title = {A Comparative Study on Annotation Quality of Crowdsourcing and {LLM} via Label Aggregation},
	url = {http://arxiv.org/abs/2401.09760},
	abstract = {Whether Large Language Models ({LLMs}) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and {LLM} workers on some specific {NLP} tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and {LLM} labels and make the evaluations on the aggregated labels. In addition, we propose a Crowd-{LLM} hybrid label aggregation method and verify the performance. We find that adding {LLM} labels from good {LLMs} to existing crowdsourcing datasets can enhance the quality of the aggregated labels of the datasets, which is also higher than the quality of {LLM} labels themselves.},
	number = {{arXiv}:2401.09760},
	publisher = {{arXiv}},
	author = {Li, Jiyi},
	urldate = {2024-07-23},
	date = {2024-01-18},
	eprinttype = {arxiv},
	eprint = {2401.09760 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	annotation = {Comment: Accepted in {ICASSP} 2024},
	file = {arXiv Fulltext PDF:C\:\\Users\\BCDH XMG\\Zotero\\storage\\I4H53U35\\Li - 2024 - A Comparative Study on Annotation Quality of Crowd.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\BCDH XMG\\Zotero\\storage\\P9C32IQN\\2401.html:text/html},
}

@article{xu_role_2024,
	title = {On the Role of Large Language Models in Crowdsourcing Misinformation Assessment},
	volume = {18},
	issn = {2334-0770, 2162-3449},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31417},
	doi = {10.1609/icwsm.v18i1.31417},
	abstract = {The proliferation of online misinformation significantly undermines the credibility of web content. Recently, crowd workers have been successfully employed to assess misinformation to address the limited scalability of professional fact-checkers. An alternative approach to crowdsourcing is the use of large language models ({LLMs}). These models are however also not perfect. In this paper, we investigate the scenario of crowd workers working in collaboration with {LLMs} to assess misinformation. We perform a study where we ask crowd workers to judge the truthfulness of statements under different conditions: with and without {LLMs} labels and explanations.  Our results show that crowd workers tend to overestimate truthfulness when exposed to {LLM}-generated information. Crowd workers are misled by wrong {LLM} labels, but, on the other hand, their self-reported confidence is lower when they make mistakes due to relying on the {LLM}. We also observe diverse behaviors among crowd workers when the {LLM} is presented, indicating that leveraging {LLMs} can be considered a distinct working strategy.},
	pages = {1674--1686},
	journaltitle = {Proceedings of the International {AAAI} Conference on Web and Social Media},
	shortjournal = {{ICWSM}},
	author = {Xu, Jiechen and Han, Lei and Sadiq, Shazia and Demartini, Gianluca},
	urldate = {2024-07-23},
	date = {2024-05-28},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\6ZQDILUZ\\Xu et al. - 2024 - On the Role of Large Language Models in Crowdsourc.pdf:application/pdf},
}

@article{itano_post-processing_2023,
	title = {A Post-Processing Framework for Crowd Worker Responses Using Large Language Models},
	volume = {21},
	issn = {16904524},
	url = {https://www.iiisci.org/DOIJSCI/CK528SF23},
	doi = {10.54808/JSCI.21.02.1},
	abstract = {To develop quality crowdsourcing systems, aggregating responses from workers is a critical issue. However, it has been difficult to construct an automatic mechanism that flexibly aggregates worker responses in natural language. Accordingly, responses need to be collected in a standardized format, such as binary-choice or multiple categorizations, to avoid large aggregation costs. Recently, with the advent of large language models ({LLMs}), natural language responses can be automatically and flexibly aggregated. We propose a framework that uses {LLMs} to flexibly aggregate natural language responses from workers and, as a promising example, consider this framework for crime detection from surveillance cameras using crowdsourced cognitive abilities. In an experiment using subjective evaluation, our proposed framework is shown to be effective for automatically aggregating natural language responses from crowd workers.},
	pages = {1--6},
	number = {2},
	journaltitle = {Journal of Systemics, Cybernetics and Informatics},
	shortjournal = {{JSCI}},
	author = {Itano, Ryuya and Tamano, Tatsuki and Koita, Takahiro and Tanitsu, Honoka},
	urldate = {2024-07-23},
	date = {2023-04},
	langid = {english},
}

@ARTICLE{Karjus2023-vw,
  title        = "Evolving linguistic divergence on polarizing social media",
  author       = "Karjus, Andres and Cuskley, Christine",
  abstract     = "Language change is influenced by many factors, but often
                  starts from synchronic variation, where multiple linguistic
                  patterns or forms coexist, or where different speech
                  communities use language in increasingly different ways.
                  Besides regional or economic reasons, communities may form
                  and segregate based on political alignment. The latter,
                  referred to as political polarization, is of growing societal
                  concern across the world. Here we map and quantify linguistic
                  divergence across the partisan left-right divide in the
                  United States, using social media data. We develop a general
                  methodology to delineate (social) media users by their
                  political preference, based on which (potentially biased)
                  news media accounts they do and do not follow on a given
                  platform. Our data consists of 1.5M short posts by 10k users
                  (about 20M words) from the social media platform Twitter (now
                  ``X''). Delineating this sample involved mining the platform
                  for the lists of followers (n=422M) of 72 large news media
                  accounts. We quantify divergence in topics of conversation
                  and word frequencies, messaging sentiment, and lexical
                  semantics of words and emoji. We find signs of linguistic
                  divergence across all these aspects, especially in topics and
                  themes of conversation, in line with previous research. While
                  US American English remains largely intelligible within its
                  large speech community, our findings point at areas where
                  miscommunication may eventually arise given ongoing
                  polarization and therefore potential linguistic divergence.
                  Our methodology - combining data mining, lexicostatistics,
                  machine learning, large language models and a systematic
                  human annotation approach - is largely language and platform
                  agnostic. In other words, while we focus here on US political
                  divides and US English, the same approach is applicable to
                  other countries, languages, and social media platforms.",
  year         =  2023,
  primaryClass = "cs.SI",
  eprint       = "2309.01659"
}

@article{Argyle_2023,
   title={Out of One, Many: Using Language Models to Simulate Human Samples},
   volume={31},
   ISSN={1476-4989},
   url={http://dx.doi.org/10.1017/pan.2023.2},
   DOI={10.1017/pan.2023.2},
   number={3},
   journal={Political Analysis},
   publisher={Cambridge University Press (CUP)},
   author={Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David},
   year={2023},
   month=feb, pages={337–351} }

@article{ahmadabadi_2024,
	title = {Design and evaluation of crowdsourcing platforms based on users’ confidence judgments},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65892-7},
	doi = {10.1038/s41598-024-65892-7},
	abstract = {Abstract
            Crowdsourcing deals with solving problems by assigning them to a large number of non-experts called crowd using their spare time. In these systems, the final answer to the question is determined by summing up the votes obtained from the community. The popularity of these systems has increased by facilitating access for community members through mobile phones and the Internet. One of the issues raised in crowdsourcing is how to choose people and how to collect answers. Usually, users are separated based on their performance in a pre-test. Designing the pre-test for performance calculation is challenging; The pre-test questions should be selected to assess characteristics in individuals that are relevant to the main questions. One of the ways to increase the accuracy of crowdsourcing systems is by considering individuals’ cognitive characteristics and decision-making models to form a crowd and improve the estimation of their answer accuracy to questions. People can estimate the correctness of their responses while making a decision. The accuracy of this estimate is determined by a quantity called metacognition ability. Metacoginition is referred to the case where the confidence level is considered along with the answer to increase the accuracy of the solution. In this paper, by both mathematical and experimental analysis, we would answer the following question: Is it possible to improve the performance of a crowdsourcing system by understanding individuals’ metacognition and recording and utilizing users’ confidence in their answers?},
	pages = {18379},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Ahmadabadi, Samin Nili and Haghifam, Maryam and Shah-Mansouri, Vahid and Ershadmanesh, Sara},
	urldate = {2024-12-09},
	date = {2024-08-08},
	langid = {english},
}

@misc{veselovsky_artificial_2023,
	title = {Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.07899},
	doi = {10.48550/ARXIV.2306.07899},
	shorttitle = {Artificial Artificial Artificial Intelligence},
	abstract = {Large language models ({LLMs}) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of {LLMs}, human gold--standard annotations are key to understanding the capabilities of {LLMs} and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by {LLMs}, as crowd workers have financial incentives to use {LLMs} to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of {LLM} usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46\% of crowd workers used {LLMs} when completing the task. Although generalization to other, less {LLM}-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/{GPTurk}},
	publisher = {{arXiv}},
	author = {Veselovsky, Veniamin and Ribeiro, Manoel Horta and West, Robert},
	urldate = {2024-12-09},
	date = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences},
	annotation = {Other
9 pages, 4 figures},
}

@misc{wu_llms_2023-1,
	title = {{LLMs} as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with {LLMs}},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2307.10168},
	doi = {10.48550/ARXIV.2307.10168},
	shorttitle = {{LLMs} as Workers in Human-Computational Algorithms?},
	abstract = {{LLMs} have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether {LLMs} can replicate more complex crowdsourcing pipelines. We find that modern {LLMs} can simulate some of crowdworkers' abilities in these "human computation algorithms," but the level of success is variable and influenced by requesters' understanding of {LLM} capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and {LLMs}' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for {LLMs}, and discuss the potential of training humans and {LLMs} with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate (1) the relative strengths of {LLMs} on different tasks (by cross-comparing their performances on sub-tasks) and (2) {LLMs}' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.},
	publisher = {{arXiv}},
	author = {Wu, Tongshuang and Zhu, Haiyi and Albayrak, Maya and Axon, Alexis and Bertsch, Amanda and Deng, Wenxing and Ding, Ziqi and Guo, Bill and Gururaja, Sireesh and Kuo, Tzu-Sheng and Liang, Jenny T. and Liu, Ryan and Mandal, Ihita and Milbauer, Jeremiah and Ni, Xiaolin and Padmanabhan, Namrata and Ramkumar, Subhashini and Sudjianto, Alexis and Taylor, Jordan and Tseng, Ying-Jui and Vaidos, Patricia and Wu, Zhijin and Wu, Wei and Yang, Chenyang},
	urldate = {2024-12-09},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Human-Computer Interaction (cs.{HC})},
}

@article{xu_role_2024-1,
	title = {On the Role of Large Language Models in Crowdsourcing Misinformation Assessment},
	volume = {18},
	issn = {2334-0770, 2162-3449},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31417},
	doi = {10.1609/icwsm.v18i1.31417},
	abstract = {The proliferation of online misinformation significantly undermines the credibility of web content. Recently, crowd workers have been successfully employed to assess misinformation to address the limited scalability of professional fact-checkers. An alternative approach to crowdsourcing is the use of large language models ({LLMs}). These models are however also not perfect. In this paper, we investigate the scenario of crowd workers working in collaboration with {LLMs} to assess misinformation. We perform a study where we ask crowd workers to judge the truthfulness of statements under different conditions: with and without {LLMs} labels and explanations.  Our results show that crowd workers tend to overestimate truthfulness when exposed to {LLM}-generated information. Crowd workers are misled by wrong {LLM} labels, but, on the other hand, their self-reported confidence is lower when they make mistakes due to relying on the {LLM}. We also observe diverse behaviors among crowd workers when the {LLM} is presented, indicating that leveraging {LLMs} can be considered a distinct working strategy.},
	pages = {1674--1686},
	journaltitle = {Proceedings of the International {AAAI} Conference on Web and Social Media},
	shortjournal = {{ICWSM}},
	author = {Xu, Jiechen and Han, Lei and Sadiq, Shazia and Demartini, Gianluca},
	urldate = {2024-12-09},
	date = {2024-05-28},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\GCQNVUKJ\\Xu et al. - 2024 - On the Role of Large Language Models in Crowdsourc.pdf:application/pdf},
}

@article{cruz2023,
	title = {Linguistic factors modulating gender assignment in Spanish–English bilingual speech},
	volume = {26},
	issn = {1366-7289, 1469-1841},
	url = {https://www.cambridge.org/core/product/identifier/S1366728922000839/type/journal_article},
	doi = {10.1017/S1366728922000839},
	abstract = {Abstract
            Drawing on naturally-occurring bilingual speech from a well-defined codeswitching community in Southern Arizona, this study examined the influence of semantic gender (a.k.a. biological gender), analogical gender, and other-language phonemic cues in modulating gender assignment in Spanish–English codeswitched speech. Thirty-four Spanish–English early bilinguals completed a forced-choice elicitation task involving two codeswitching environments: Spanish determiner–English noun switches (Task 1) and English–Spanish switched copula constructions (Task 2). The results revealed that for human-denoting nouns, bilinguals assigned grammatical gender based on the presupposed sex of a noun's referent in both syntactic environments tested. As for inanimate nouns, bilinguals were more likely to assign masculine over feminine gender to such nouns in determiner–noun switches, but not in switched copula constructions. Other-language phonemic cues did not influence the assignment mechanism. A methodological implication is that the study replicated the codeswitching patterns observed in naturally-occurring bilingual speech from the same bilingual community.},
	pages = {580--591},
	number = {3},
	journaltitle = {Bilingualism: Language and Cognition},
	shortjournal = {Bilingualism},
	author = {Cruz, Abel},
	urldate = {2024-12-09},
	date = {2023-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\7XMS6GXY\\Cruz - 2023 - Linguistic factors modulating gender assignment in.pdf:application/pdf},
}

@article{lombard2021,
	title = {Neological intuition in French: A study of formal novelty and lexical regularity as predictors},
	volume = {254},
	issn = {00243841},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384121000279},
	doi = {10.1016/j.lingua.2021.103055},
	shorttitle = {Neological intuition in French},
	pages = {103055},
	journaltitle = {Lingua},
	shortjournal = {Lingua},
	author = {Lombard, Alizée and Huyghe, Richard and Gygax, Pascal},
	urldate = {2024-12-09},
	date = {2021-04},
	langid = {english},
}

@article{noauthor_notitle_nodate-2,
	doi = {https://doi.org/10.48550/arXiv.2303.16854},
}

@misc{he_annollm_2023,
	title = {{AnnoLLM}: Making Large Language Models to Be Better Crowdsourced Annotators},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.16854},
	doi = {10.48550/ARXIV.2303.16854},
	shorttitle = {{AnnoLLM}},
	abstract = {Many natural language processing ({NLP}) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, {GPT}-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various {NLP} tasks. In this paper, we first claim that large language models ({LLMs}), such as {GPT}-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose {AnnoLLM}, an annotation system powered by {LLMs}, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt {LLMs} to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with {LLMs}. Our experiment results on three tasks, including user input and keyword relevance assessment, {BoolQ}, and {WiC}, demonstrate that {AnnoLLM} surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing {AnnoLLM}. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.},
	publisher = {{arXiv}},
	author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	urldate = {2025-01-08},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
	annotation = {Other
Accepted to {NAACL} 2024},
}

@article{cowley_robots_2008,
	title = {Robots – the new linguistic informants?},
	volume = {20},
	issn = {0954-0091, 1360-0494},
	url = {http://www.tandfonline.com/doi/full/10.1080/09540090802518695},
	doi = {10.1080/09540090802518695},
	pages = {359--369},
	number = {4},
	journaltitle = {Connection Science},
	shortjournal = {Connection Science},
	author = {Cowley, Stephen J.},
	urldate = {2025-01-13},
	date = {2008-12},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\HGT6YY98\\Cowley - 2008 - Robots – the new linguistic informants.pdf:application/pdf},
}

@article{kortmann_reflecting_2021,
	title = {Reflecting on the quantitative turn in linguistics},
	volume = {59},
	issn = {0024-3949, 1613-396X},
	url = {https://www.degruyter.com/document/doi/10.1515/ling-2019-0046/html},
	doi = {10.1515/ling-2019-0046},
	abstract = {Abstract
            Linguistics, English linguistics in particular, has witnessed a remarkable quantitative turn since the 1990s and the early 2000s. It was a turn both in scale and in quality, a turn concerning the degree (including the degree of sophistication) to which quantitative empirical studies, statistical techniques, and statistical modelling have come to be used and determine linguistic research. Which role have corpus linguistics and probabilistic linguistics, including usage-based approaches, played in this development? Has this turn been to the detriment of qualitative methods, or even of linguistic theorizing in general? Has linguistics reached the point of a “quantitative crisis”, or is it still a discipline characterized by a healthy equilibrium, if not mutual reinforcement, of quantitative and qualitative approaches? What are, or should be, major repercussions of the strong quantitative turn for the publication system of (English) linguistics? These are the major overarching questions underlying the reflections offered in this opinion paper.},
	pages = {1207--1226},
	number = {5},
	journaltitle = {Linguistics},
	author = {Kortmann, Bernd},
	urldate = {2025-01-13},
	date = {2021-09-27},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\GVG9NU5Z\\Kortmann - 2021 - Reflecting on the quantitative turn in linguistics.pdf:application/pdf},
}

@inproceedings{van_dalfsen_direct_2024,
	location = {Aarhus, Denmark},
	title = {Direct and Indirect Annotation with Generative {AI}: A Case Study into Finding Animals and Plants in Historical Text},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3834/paper74.pdf},
	abstract = {This study explores the use of generative {AI} ({GenAI}) for annotation in the humanities, comparing direct
and indirect annotation approaches with human annotations. Direct annotation involves using {GenAI}
to annotate the entire corpus, while indirect annotation uses {GenAI} to create training data for a special-
ized model. The research investigates zero-shot and few-shot methods for direct annotation, alongside
an indirect approach incorporating active learning, few-shotting, and k-{NN} example retrieval. The task
focuses on identifying words (also referred to as entities) related to plants and animals in Early Modern
Dutch texts. Results show that indirect annotation outperforms zero-shot direct annotation in mimick-
ing human annotations. However, with just a few examples, direct annotation catches up, achieving
similar performance to indirect annotation. Analysis of confusion matrices reveals that {GenAI} annota-
tors make similar types of mistakes, such as confusing parts and products or failing to identify entities,
which are broader than those made by humans. Manual error analysis indicates that each annotation
method (human, direct, and indirect) has some unique errors. Given the limited scale of this study, it is
worthwhile to further explore the relative affordances of direct and indirect {GenAI} annotation methods.},
	eventtitle = {{CHR} 2024: Computational Humanities Research Conference},
	author = {Van Dalfsen, Arjan and Karsdorp, Folgert and Bagheri, Ayoub and Mentink, Dieuwertje and van Engelen, Thirza and Stronks, Els},
	date = {2024-12-04},
}

@misc{horych_promises_2024,
	title = {The Promises and Pitfalls of {LLM} Annotations in Dataset Labeling: a Case Study on Media Bias Detection},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2411.11081},
	doi = {10.48550/ARXIV.2411.11081},
	shorttitle = {The Promises and Pitfalls of {LLM} Annotations in Dataset Labeling},
	abstract = {High annotation costs from hiring or crowdsourcing complicate the creation of large, high-quality datasets needed for training reliable text classifiers. Recent research suggests using Large Language Models ({LLMs}) to automate the annotation process, reducing these costs while maintaining data quality. {LLMs} have shown promising results in annotating downstream tasks like hate speech detection and political framing. Building on the success in these areas, this study investigates whether {LLMs} are viable for annotating the complex task of media bias detection and whether a downstream media bias classifier can be trained on such data. We create annolexical, the first large-scale dataset for media bias classification with over 48000 synthetically annotated examples. Our classifier, fine-tuned on this dataset, surpasses all of the annotator {LLMs} by 5-9 percent in Matthews Correlation Coefficient ({MCC}) and performs close to or outperforms the model trained on human-labeled data when evaluated on two media bias benchmark datasets ({BABE} and {BASIL}). This study demonstrates how our approach significantly reduces the cost of dataset creation in the media bias domain and, by extension, the development of classifiers, while our subsequent behavioral stress-testing reveals some of its current limitations and trade-offs.},
	publisher = {{arXiv}},
	author = {Horych, Tomas and Mandl, Christoph and Ruas, Terry and Greiner-Petter, Andre and Gipp, Bela and Aizawa, Akiko and Spinde, Timo},
	urldate = {2025-01-14},
	date = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
}

@misc{veselovsky_artificial_2023-1,
	title = {Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2306.07899},
	doi = {10.48550/ARXIV.2306.07899},
	shorttitle = {Artificial Artificial Artificial Intelligence},
	abstract = {Large language models ({LLMs}) are remarkable data annotators. They can be used to generate high-fidelity supervised training data, as well as survey and experimental data. With the widespread adoption of {LLMs}, human gold--standard annotations are key to understanding the capabilities of {LLMs} and the validity of their results. However, crowdsourcing, an important, inexpensive way to obtain human annotations, may itself be impacted by {LLMs}, as crowd workers have financial incentives to use {LLMs} to increase their productivity and income. To investigate this concern, we conducted a case study on the prevalence of {LLM} usage by crowd workers. We reran an abstract summarization task from the literature on Amazon Mechanical Turk and, through a combination of keystroke detection and synthetic text classification, estimate that 33-46\% of crowd workers used {LLMs} when completing the task. Although generalization to other, less {LLM}-friendly tasks is unclear, our results call for platforms, researchers, and crowd workers to find new ways to ensure that human data remain human, perhaps using the methodology proposed here as a stepping stone. Code/data: https://github.com/epfl-dlab/{GPTurk}},
	publisher = {{arXiv}},
	author = {Veselovsky, Veniamin and Ribeiro, Manoel Horta and West, Robert},
	urldate = {2025-01-14},
	date = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences},
	annotation = {Other
9 pages, 4 figures},
}

@article{ahmadabadi_design_2024-1,
	title = {Design and evaluation of crowdsourcing platforms based on users’ confidence judgments},
	volume = {14},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-65892-7},
	doi = {10.1038/s41598-024-65892-7},
	abstract = {Abstract
            Crowdsourcing deals with solving problems by assigning them to a large number of non-experts called crowd using their spare time. In these systems, the final answer to the question is determined by summing up the votes obtained from the community. The popularity of these systems has increased by facilitating access for community members through mobile phones and the Internet. One of the issues raised in crowdsourcing is how to choose people and how to collect answers. Usually, users are separated based on their performance in a pre-test. Designing the pre-test for performance calculation is challenging; The pre-test questions should be selected to assess characteristics in individuals that are relevant to the main questions. One of the ways to increase the accuracy of crowdsourcing systems is by considering individuals’ cognitive characteristics and decision-making models to form a crowd and improve the estimation of their answer accuracy to questions. People can estimate the correctness of their responses while making a decision. The accuracy of this estimate is determined by a quantity called metacognition ability. Metacoginition is referred to the case where the confidence level is considered along with the answer to increase the accuracy of the solution. In this paper, by both mathematical and experimental analysis, we would answer the following question: Is it possible to improve the performance of a crowdsourcing system by understanding individuals’ metacognition and recording and utilizing users’ confidence in their answers?},
	pages = {18379},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Ahmadabadi, Samin Nili and Haghifam, Maryam and Shah-Mansouri, Vahid and Ershadmanesh, Sara},
	urldate = {2025-01-14},
	date = {2024-08-08},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\6EELXD9A\\Ahmadabadi et al. - 2024 - Design and evaluation of crowdsourcing platforms b.pdf:application/pdf},
}

@article{cruz_linguistic_2023-1,
	title = {Linguistic factors modulating gender assignment in Spanish–English bilingual speech},
	volume = {26},
	issn = {1366-7289, 1469-1841},
	url = {https://www.cambridge.org/core/product/identifier/S1366728922000839/type/journal_article},
	doi = {10.1017/S1366728922000839},
	abstract = {Abstract
            Drawing on naturally-occurring bilingual speech from a well-defined codeswitching community in Southern Arizona, this study examined the influence of semantic gender (a.k.a. biological gender), analogical gender, and other-language phonemic cues in modulating gender assignment in Spanish–English codeswitched speech. Thirty-four Spanish–English early bilinguals completed a forced-choice elicitation task involving two codeswitching environments: Spanish determiner–English noun switches (Task 1) and English–Spanish switched copula constructions (Task 2). The results revealed that for human-denoting nouns, bilinguals assigned grammatical gender based on the presupposed sex of a noun's referent in both syntactic environments tested. As for inanimate nouns, bilinguals were more likely to assign masculine over feminine gender to such nouns in determiner–noun switches, but not in switched copula constructions. Other-language phonemic cues did not influence the assignment mechanism. A methodological implication is that the study replicated the codeswitching patterns observed in naturally-occurring bilingual speech from the same bilingual community.},
	pages = {580--591},
	number = {3},
	journaltitle = {Bilingualism: Language and Cognition},
	shortjournal = {Bilingualism},
	author = {Cruz, Abel},
	urldate = {2025-01-14},
	date = {2023-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\A6D2BP7E\\Cruz - 2023 - Linguistic factors modulating gender assignment in.pdf:application/pdf},
}

@article{nagle_developing_2019,
	title = {Developing and validating a methodology for crowdsourcing L2 speech ratings in Amazon Mechanical Turk},
	volume = {5},
	rights = {https://benjamins.com/content/customers/rights},
	issn = {2215-1931, 2215-194X},
	url = {http://www.jbe-platform.com/content/journals/10.1075/jslp.18016.nag},
	doi = {10.1075/jslp.18016.nag},
	abstract = {Abstract
            Researchers have increasingly turned to Amazon Mechanical Turk ({AMT}) to crowdsource speech data, predominantly in English. Although {AMT} and similar platforms are well positioned to enhance the state of the art in L2 research, it is unclear if crowdsourced L2 speech ratings are reliable, particularly in languages other than English. The present study describes the development and deployment of an {AMT} task to crowdsource comprehensibility, fluency, and accentedness ratings for L2 Spanish speech samples. Fifty-four {AMT} workers who were native Spanish speakers from 11 countries participated in the ratings. Intraclass correlation coefficients were used to estimate group-level interrater reliability, and Rasch analyses were undertaken to examine individual differences in rater severity and fit. Excellent reliability was observed for the comprehensibility and fluency ratings, but indices were slightly lower for accentedness, leading to recommendations to improve the task for future data collection.},
	pages = {294--323},
	number = {2},
	journaltitle = {Journal of Second Language Pronunciation},
	shortjournal = {{JSLP}},
	author = {Nagle, Charles},
	urldate = {2025-01-15},
	date = {2019-09-17},
	langid = {english},
}

@article{sheehan_crowdsourcing_2019,
	title = {Crowdsourcing and Minority Languages: The Case of Galician Inflected Infinitives1},
	volume = {10},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/article/10.3389/fpsyg.2019.01157/full},
	doi = {10.3389/fpsyg.2019.01157},
	shorttitle = {Crowdsourcing and Minority Languages},
	pages = {1157},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Sheehan, Michelle and Schäfer, Martin and Parafita Couto, Maria Carmen},
	urldate = {2025-01-15},
	date = {2019-06-25},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\TX7QEFMQ\\Sheehan et al. - 2019 - Crowdsourcing and Minority Languages The Case of .pdf:application/pdf},
}

@article{thwaites_crowdsourced_2024,
	title = {Crowdsourced Comparative Judgement for Evaluating Learner Texts: How Reliable are Judges Recruited from an Online Crowdsourcing Platform?},
	rights = {https://academic.oup.com/pages/standard-publication-reuse-rights},
	issn = {0142-6001, 1477-450X},
	url = {https://academic.oup.com/applij/advance-article/doi/10.1093/applin/amae048/7719043},
	doi = {10.1093/applin/amae048},
	shorttitle = {Crowdsourced Comparative Judgement for Evaluating Learner Texts},
	abstract = {Abstract
            Recent studies of proficiency measurement and reporting practices in applied linguists have revealed widespread use of unsatisfactory practices such as the use of proxy measures of proficiency in place of explicit tests. Learner corpus research is one specific area affected by this problem: few learner corpora contain reliable, valid evaluations of text proficiency. This has led to calls for the development of new L2 writing proficiency measures for use in research contexts. Answering this call, a recent study by Paquot et al. (2022) generated assessments of learner corpus texts using a community-driven approach in which judges, recruited from the linguistic community, conducted assessments using comparative judgement. Although the approach generated reliable assessments, its practical use is limited because linguists are not always available to contribute to data collections. This paper, therefore, explores an alternative approach, in which judges are recruited through a crowdsourcing platform. We find that assessments generated in this way can reach near identical levels of reliability and concurrent validity to those produced by members of the linguistic community.},
	pages = {amae048},
	journaltitle = {Applied Linguistics},
	author = {Thwaites, Peter and Vandeweerd, Nathan and Paquot, Magali},
	urldate = {2025-01-15},
	date = {2024-07-23},
	langid = {english},
}

@misc{khalilia_crowdsourcing_2024,
	title = {Crowdsourcing Lexical Diversity},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2410.23133},
	doi = {10.48550/ARXIV.2410.23133},
	abstract = {Lexical-semantic resources ({LSRs}), such as online lexicons or wordnets, are fundamental for natural language processing applications. In many languages, however, such resources suffer from quality issues: incorrect entries, incompleteness, but also, the rarely addressed issue of bias towards the English language and Anglo-Saxon culture. Such bias manifests itself in the absence of concepts specific to the language or culture at hand, the presence of foreign (Anglo-Saxon) concepts, as well as in the lack of an explicit indication of untranslatability, also known as cross-lingual {\textbackslash}emph\{lexical gaps\}, when a term has no equivalent in another language. This paper proposes a novel crowdsourcing methodology for reducing bias in {LSRs}. Crowd workers compare lexemes from two languages, focusing on domains rich in lexical diversity, such as kinship or food. Our {LingoGap} crowdsourcing tool facilitates comparisons through microtasks identifying equivalent terms, language-specific terms, and lexical gaps across languages. We validated our method by applying it to two case studies focused on food-related terminology: (1) English and Arabic, and (2) Standard Indonesian and Banjarese. These experiments identified 2,140 lexical gaps in the first case study and 951 in the second. The success of these experiments confirmed the usability of our method and tool for future large-scale lexicon enrichment tasks.},
	publisher = {{arXiv}},
	author = {Khalilia, Hadi and Otterbacher, Jahna and Bella, Gabor and Noortyani, Rusma and Darma, Shandy and Giunchiglia, Fausto},
	urldate = {2025-01-15},
	date = {2024},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
}

@inproceedings{qi_pragmaticqa_2023,
	location = {Toronto, Canada},
	title = {{PragmatiCQA}: A Dataset for Pragmatic Question Answering in Conversations},
	url = {https://aclanthology.org/2023.findings-acl.385},
	doi = {10.18653/v1/2023.findings-acl.385},
	shorttitle = {{PragmatiCQA}},
	eventtitle = {Findings of the Association for Computational Linguistics: {ACL} 2023},
	pages = {6175--6191},
	booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Qi, Peng and Du, Nina and Manning, Christopher and Huang, Jing},
	urldate = {2025-01-15},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\2UBSTKI9\\Qi et al. - 2023 - PragmatiCQA A Dataset for Pragmatic Question Answ.pdf:application/pdf},
}

@article{capuano_semantic_2021,
	title = {Semantic Similarity of Alternatives Fostered by Conversational Negation},
	volume = {45},
	issn = {0364-0213, 1551-6709},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.13015},
	doi = {10.1111/cogs.13015},
	abstract = {Abstract
            Conversational negation often behaves differently from negation as a logical operator: when rejecting a state of affairs, it does not present all members of the complement set as equally plausible alternatives, but it rather suggests some of them as more plausible than others (e.g., “This is not a dog, it is a wolf/*screwdriver”). Entities that are semantically similar to a negated entity tend to be judged as better alternatives (Kruszewski et al., 2016). In fact, Kruszewski et al. (2016) show that the cosine similarity scores between the distributional semantics representations of a negated noun and its potential alternatives are highly correlated with the negated noun‐alternatives human plausibility ratings. In a series of cloze tasks, we show that negation likewise restricts the production of plausible alternatives to similar entities. Furthermore, completions to negative sentences appear to be even more restricted than completions to an affirmative conjunctive context, hinting at a peculiarity of negation.},
	pages = {e13015},
	number = {7},
	journaltitle = {Cognitive Science},
	shortjournal = {Cognitive Science},
	author = {Capuano, Francesca and Dudschig, Carolin and Günther, Fritz and Kaup, Barbara},
	urldate = {2025-01-15},
	date = {2021-07},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\7RWY6SIY\\Capuano et al. - 2021 - Semantic Similarity of Alternatives Fostered by Co.pdf:application/pdf},
}

@inproceedings{ostyakova_chatgpt_2023,
	location = {Prague, Czechia},
	title = {{ChatGPT} vs. Crowdsourcing vs. Experts: Annotating Open-Domain Conversations with Speech Functions},
	url = {https://aclanthology.org/2023.sigdial-1.23},
	doi = {10.18653/v1/2023.sigdial-1.23},
	shorttitle = {{ChatGPT} vs. Crowdsourcing vs. Experts},
	eventtitle = {Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue},
	pages = {242--254},
	booktitle = {Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue},
	publisher = {Association for Computational Linguistics},
	author = {Ostyakova, Lidiia and Smilga, Veronika and Petukhova, Kseniia and Molchanova, Maria and Kornev, Daniel},
	urldate = {2025-01-15},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\ZQE2MSAC\\Ostyakova et al. - 2023 - ChatGPT vs. Crowdsourcing vs. Experts Annotating .pdf:application/pdf},
}

@article{xu_role_2024-2,
	title = {On the Role of Large Language Models in Crowdsourcing Misinformation Assessment},
	volume = {18},
	issn = {2334-0770, 2162-3449},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/31417},
	doi = {10.1609/icwsm.v18i1.31417},
	abstract = {The proliferation of online misinformation significantly undermines the credibility of web content. Recently, crowd workers have been successfully employed to assess misinformation to address the limited scalability of professional fact-checkers. An alternative approach to crowdsourcing is the use of large language models ({LLMs}). These models are however also not perfect. In this paper, we investigate the scenario of crowd workers working in collaboration with {LLMs} to assess misinformation. We perform a study where we ask crowd workers to judge the truthfulness of statements under different conditions: with and without {LLMs} labels and explanations.  Our results show that crowd workers tend to overestimate truthfulness when exposed to {LLM}-generated information. Crowd workers are misled by wrong {LLM} labels, but, on the other hand, their self-reported confidence is lower when they make mistakes due to relying on the {LLM}. We also observe diverse behaviors among crowd workers when the {LLM} is presented, indicating that leveraging {LLMs} can be considered a distinct working strategy.},
	pages = {1674--1686},
	journaltitle = {Proceedings of the International {AAAI} Conference on Web and Social Media},
	shortjournal = {{ICWSM}},
	author = {Xu, Jiechen and Han, Lei and Sadiq, Shazia and Demartini, Gianluca},
	urldate = {2025-01-15},
	date = {2024-05-28},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\GINYDRTA\\Xu et al. - 2024 - On the Role of Large Language Models in Crowdsourc.pdf:application/pdf},
}

@misc{han_empirical_2023,
	title = {An Empirical Study on Information Extraction using Large Language Models},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2305.14450},
	doi = {10.48550/ARXIV.2305.14450},
	abstract = {Human-like large language models ({LLMs}), especially the most powerful and popular ones in {OpenAI}'s {GPT} family, have proven to be very helpful for many natural language processing ({NLP}) related tasks. Therefore, various attempts have been made to apply {LLMs} to information extraction ({IE}), which is a fundamental {NLP} task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in {LLMs}' information extraction ability, we assess the information extraction ability of {GPT}-4 (the latest version of {GPT} at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between {GPT}-4 and state-of-the-art ({SOTA}) {IE} methods. To alleviate this problem, considering the {LLMs}' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other {LLMs} and {NLP} tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving {GPT}-4's information extraction ability.},
	publisher = {{arXiv}},
	author = {Han, Ridong and Yang, Chaohao and Peng, Tao and Tiwari, Prayag and Wan, Xiang and Liu, Lu and Wang, Benyou},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
	annotation = {Other
52 pages, Version 2.0; This article has an original arxiv version entitled "Is Information Extraction Solved by {ChatGPT}? An Analysis of Performance, Evaluation Criteria, Robustness and Errors'', whose url link is {arXiv}:2305.14450v1},
}

@misc{qin_is_2023,
	title = {Is {ChatGPT} a General-Purpose Natural Language Processing Task Solver?},
	rights = {Creative Commons Attribution Share Alike 4.0 International},
	url = {https://arxiv.org/abs/2302.06476},
	doi = {10.48550/ARXIV.2302.06476},
	abstract = {Spurred by advancements in scale, large language models ({LLMs}) have demonstrated the ability to perform a variety of natural language processing ({NLP}) tasks zero-shot -- i.e., without adaptation on downstream data. Recently, the debut of {ChatGPT} has drawn a great deal of attention from the natural language processing ({NLP}) community due to the fact that it can generate high-quality responses to human input and self-correct previous mistakes based on subsequent conversations. However, it is not yet known whether {ChatGPT} can serve as a generalist model that can perform many {NLP} tasks zero-shot. In this work, we empirically analyze the zero-shot learning ability of {ChatGPT} by evaluating it on 20 popular {NLP} datasets covering 7 representative task categories. With extensive empirical studies, we demonstrate both the effectiveness and limitations of the current version of {ChatGPT}. We find that {ChatGPT} performs well on many tasks favoring reasoning capabilities (e.g., arithmetic reasoning) while it still faces challenges when solving specific tasks such as sequence tagging. We additionally provide in-depth analysis through qualitative case studies.},
	publisher = {{arXiv}},
	author = {Qin, Chengwei and Zhang, Aston and Zhang, Zhuosheng and Chen, Jiaao and Yasunaga, Michihiro and Yang, Diyi},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 3},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
}

@misc{tornberg_chatgpt-4_2023,
	title = {{ChatGPT}-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2304.06588},
	doi = {10.48550/ARXIV.2304.06588},
	abstract = {This paper assesses the accuracy, reliability and bias of the Large Language Model ({LLM}) {ChatGPT}-4 on the text analysis task of classifying the political affiliation of a Twitter poster based on the content of a tweet. The {LLM} is compared to manual annotation by both expert classifiers and crowd workers, generally considered the gold standard for such tasks. We use Twitter messages from United States politicians during the 2020 election, providing a ground truth against which to measure accuracy. The paper finds that {ChatGPT}-4 has achieves higher accuracy, higher reliability, and equal or lower bias than the human classifiers. The {LLM} is able to correctly annotate messages that require reasoning on the basis of contextual knowledge, and inferences around the author's intentions - traditionally seen as uniquely human abilities. These findings suggest that {LLM} will have substantial impact on the use of textual data in the social sciences, by enabling interpretive research at a scale.},
	publisher = {{arXiv}},
	author = {Törnberg, Petter},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 1},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Social and Information Networks (cs.{SI})},
	annotation = {Other
5 pages, 3 figures},
}

@misc{he_annollm_2023-1,
	title = {{AnnoLLM}: Making Large Language Models to Be Better Crowdsourced Annotators},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.16854},
	doi = {10.48550/ARXIV.2303.16854},
	shorttitle = {{AnnoLLM}},
	abstract = {Many natural language processing ({NLP}) tasks rely on labeled data to train machine learning models with high performance. However, data annotation is time-consuming and expensive, especially when the task involves a large amount of data or requires specialized domains. Recently, {GPT}-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various {NLP} tasks. In this paper, we first claim that large language models ({LLMs}), such as {GPT}-3.5, can serve as an excellent crowdsourced annotator when provided with sufficient guidance and demonstrated examples. Accordingly, we propose {AnnoLLM}, an annotation system powered by {LLMs}, which adopts a two-step approach, explain-then-annotate. Concretely, we first prompt {LLMs} to provide explanations for why the specific ground truth answer/label was assigned for a given example. Then, we construct the few-shot chain-of-thought prompt with the self-generated explanation and employ it to annotate the unlabeled data with {LLMs}. Our experiment results on three tasks, including user input and keyword relevance assessment, {BoolQ}, and {WiC}, demonstrate that {AnnoLLM} surpasses or performs on par with crowdsourced annotators. Furthermore, we build the first conversation-based information retrieval dataset employing {AnnoLLM}. This dataset is designed to facilitate the development of retrieval models capable of retrieving pertinent documents for conversational text. Human evaluation has validated the dataset's high quality.},
	publisher = {{arXiv}},
	author = {He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, A-Long and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences},
	annotation = {Other
Accepted to {NAACL} 2024},
}

@inproceedings{wang_want_2021,
	location = {Punta Cana, Dominican Republic},
	title = {Want To Reduce Labeling Cost? {GPT}-3 Can Help},
	url = {https://aclanthology.org/2021.findings-emnlp.354},
	doi = {10.18653/v1/2021.findings-emnlp.354},
	shorttitle = {Want To Reduce Labeling Cost?},
	eventtitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
	pages = {4195--4205},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
	urldate = {2025-01-16},
	date = {2021},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\QGBKMDJ5\\Wang et al. - 2021 - Want To Reduce Labeling Cost GPT-3 Can Help.pdf:application/pdf},
}

@inproceedings{ding_is_2023,
	location = {Toronto, Canada},
	title = {Is {GPT}-3 a Good Data Annotator?},
	url = {https://aclanthology.org/2023.acl-long.626},
	doi = {10.18653/v1/2023.acl-long.626},
	eventtitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	pages = {11173--11195},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Ding, Bosheng and Qin, Chengwei and Liu, Linlin and Chia, Yew Ken and Li, Boyang and Joty, Shafiq and Bing, Lidong},
	urldate = {2025-01-16},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\CJ9YA5SG\\Ding et al. - 2023 - Is GPT-3 a Good Data Annotator.pdf:application/pdf},
}

@inproceedings{li_coannotating_2023,
	location = {Singapore},
	title = {{CoAnnotating}: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation},
	url = {https://aclanthology.org/2023.emnlp-main.92},
	doi = {10.18653/v1/2023.emnlp-main.92},
	shorttitle = {{CoAnnotating}},
	eventtitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	pages = {1487--1505},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Li, Minzhi and Shi, Taiwei and Ziems, Caleb and Kan, Min-Yen and Chen, Nancy and Liu, Zhengyuan and Yang, Diyi},
	urldate = {2025-01-16},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\MPXYUC2T\\Li et al. - 2023 - CoAnnotating Uncertainty-Guided Work Allocation b.pdf:application/pdf},
}

@inproceedings{rebora_comparing_2023,
	location = {Paris},
	title = {Comparing {ChatGPT} to Human Raters and Sentiment Analysis Tools for German Children’s Literature},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3558/paper3340.pdf},
	eventtitle = {{CHR} 2023: Computational Humanities Research Conference,},
	author = {Rebora, Simone and Lehmann, Marina and Heumann, Anne and Ding, Wei and Lauer, Gerhard},
	date = {2023-12-06},
}

@inproceedings{borst_death_2023,
	location = {Paris},
	title = {Death of the Dictionary? – The Rise of Zero-Shot Sentiment Classification},
	volume = {{CEUR} Workshop Proceedings},
	url = {https://ceur-ws.org/Vol-3558/paper3130.pdf},
	eventtitle = {{CHR} 2023: Computational Humanities Research Conference},
	author = {Borst, Janos and Klähn, Jannis and Burghardt, Manuel},
	date = {2023-12-06},
}

@article{kosar_comparative_2024,
	title = {Comparative Evaluation of Topic Detection: Humans vs. {LLMs}},
	volume = {Computational Linguistics in the Netherlands Journal},
	url = {https://www.clinjournal.org/clinj/article/view/173},
	pages = {91--120},
	number = {13},
	author = {Kosar, A and De Pauw, G and Daelemans, W},
	date = {2024},
}

@misc{karjus2024machineassisted,
      title={Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence}, 
      author={Andres Karjus},
      year={2024},
      eprint={2309.14379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.14379}, 
}

@ARTICLE{Karjus2024-zg,
  title     = "Evolving linguistic divergence on polarizing social media",
  author    = "Karjus, Andres and Cuskley, Christine",
  abstract  = "AbstractLanguage change is influenced by many factors, but often
               starts from synchronic variation, where multiple linguistic
               patterns or forms coexist, or where different speech communities
               use language in increasingly different ways. Besides regional or
               economic reasons, communities may form and segregate based on
               political alignment. The latter, referred to as political
               polarization, is of growing societal concern across the world.
               Here we map and quantify linguistic divergence across the
               partisan left-right divide in the United States, using social
               media data. We develop a general methodology to delineate
               (social) media users by their political preference, based on
               which (potentially biased) news media accounts they do and do
               not follow on a given platform. Our data consists of 1.5M short
               posts by 10k users (about 20M words) from the social media
               platform Twitter (now ``X''). Delineating this sample involved
               mining the platform for the lists of followers (n = 422M) of 72
               large news media accounts. We quantify divergence in topics of
               conversation and word frequencies, messaging sentiment, and
               lexical semantics of words and emoji. We find signs of
               linguistic divergence across all these aspects, especially in
               topics and themes of conversation, in line with previous
               research. While US American English remains largely intelligible
               within its large speech community, our findings point at areas
               where miscommunication may eventually arise given ongoing
               polarization and therefore potential linguistic divergence. Our
               flexible methodology --- combining data mining,
               lexicostatistics, machine learning, large language models and a
               systematic human annotation approach --- is largely language and
               platform agnostic. In other words, while we focus here on US
               political divides and US English, the same approach is
               applicable to other countries, languages, and social media
               platforms.",
  journal   = "Humanit. Soc. Sci. Commun.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  11,
  number    =  1,
  month     =  mar,
  year      =  2024,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en"
}

@ARTICLE{alizadeh2025-cm,
  title    = "Open-source {LLMs} for text annotation: a practical guide for
              model setting and fine-tuning",
  author   = "Alizadeh, Meysam and Kubli, Ma{\"e}l and Samei, Zeynab and
              Dehghani, Shirin and Zahedivafa, Mohammadmasiha and Bermeo, Juan
              D and Korobeynikova, Maria and Gilardi, Fabrizio",
  abstract = "This paper studies the performance of open-source Large Language
              Models (LLMs) in text classification tasks typical for political
              science research. By examining tasks like stance, topic, and
              relevance classification, we aim to guide scholars in making
              informed decisions about their use of LLMs for text analysis and
              to establish a baseline performance benchmark that demonstrates
              the models' effectiveness. Specifically, we conduct an assessment
              of both zero-shot and fine-tuned LLMs across a range of text
              annotation tasks using news articles and tweets datasets. Our
              analysis shows that fine-tuning improves the performance of
              open-source LLMs, allowing them to match or even surpass
              zero-shot GPT - 3.5 and GPT-4, though still lagging behind
              fine-tuned GPT - 3.5. We further establish that fine-tuning is
              preferable to few-shot training with a relatively modest quantity
              of annotated text. Our findings show that fine-tuned open-source
              LLMs can be effectively deployed in a broad spectrum of text
              annotation applications. We provide a Python notebook
              facilitating the application of LLMs in text annotation for other
              researchers. Supplementary Information: The online version
              contains supplementary material available at
              10.1007/s42001-024-00345-9.",
  journal  = "J. Comput. Soc. Sci.",
  volume   =  8,
  number   =  1,
  pages    = "17",
  year     =  2025,
  keywords = "ChatGPT; FLAN; LLMs; LLaMA; NLP; Open source; Text annotation",
  language = "en"
}

@article{de_lange_benchmarking_2024,
	title = {Benchmarking Zero-Shot Text Classification for Dutch},
	volume = {Computational Linguistics in the Netherlands Journal},
	url = {https://clinjournal.org/clinj/article /view/172},
	pages = {63--90},
	number = {13},
	author = {De Lange, Loic and Vanroy, Bram and De Bruyne, Luna and Singh, Pranaydeep and Lefever, Els and De Clercq, Orphée},
	date = {2024},
}

@misc{karjusalone2023,
	title = {Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2309.14379},
	doi = {10.48550/ARXIV.2309.14379},
	shorttitle = {Machine-assisted quantitizing designs},
	abstract = {The increasing capacities of large language models ({LLMs}) have been shown to present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, by automating complex qualitative tasks otherwise typically carried out by human researchers. While numerous benchmarking studies have assessed the analytic prowess of {LLMs}, there is less focus on operationalizing this capacity for inference and hypothesis testing. Addressing this challenge, a systematic framework is argued for here, building on mixed methods quantitizing and converting design principles, and feature analysis from linguistics, to transparently integrate human expertise and machine scalability. Replicability and statistical robustness are discussed, including how to incorporate machine annotator error rates in subsequent inference. The approach is discussed and demonstrated in over a dozen {LLM}-assisted case studies, covering 9 diverse languages, multiple disciplines and tasks, including analysis of themes, stances, ideas, and genre compositions; linguistic and semantic annotation, interviews, text mining and event cause inference in noisy historical data, literary social network construction, metadata imputation, and multimodal visual cultural analytics. Using hypothesis-driven topic classification instead of "distant reading" is discussed. The replications among the experiments also illustrate how tasks previously requiring protracted team effort or complex computational pipelines can now be accomplished by an {LLM}-assisted scholar in a fraction of the time. Importantly, the approach is not intended to replace, but to augment and scale researcher expertise and analytic practices. With these opportunities in sight, qualitative skills and the ability to pose insightful questions have arguably never been more critical.},
	publisher = {{arXiv}},
	author = {Karjus, Andres},
	urldate = {2025-01-16},
	date = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
}

@inproceedings{cegin_chatgpt_2023,
	location = {Singapore},
	title = {{ChatGPT} to Replace Crowdsourcing of Paraphrases for Intent Classification: Higher Diversity and Comparable Model Robustness},
	url = {https://aclanthology.org/2023.emnlp-main.117},
	doi = {10.18653/v1/2023.emnlp-main.117},
	shorttitle = {{ChatGPT} to Replace Crowdsourcing of Paraphrases for Intent Classification},
	eventtitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	pages = {1889--1905},
	booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Cegin, Jan and Simko, Jakub and Brusilovsky, Peter},
	urldate = {2025-01-19},
	date = {2023},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\NDAWVE3D\\Cegin et al. - 2023 - ChatGPT to Replace Crowdsourcing of Paraphrases fo.pdf:application/pdf},
}

@article{lombard_neological_2021-1,
	title = {Neological intuition in French: A study of formal novelty and lexical regularity as predictors.},
	volume = {254},
	issn = {00243841},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384121000279},
	doi = {10.1016/j.lingua.2021.103055},
	shorttitle = {Neological intuition in French},
	abstract = {This paper investigates how neological intuition ({NI}), i.e., the metalinguistic ability to evaluate lexical novelty, is influenced by the linguistic properties of novel words. We focus on two properties: (i) formal novelty, depending on whether neologisms result from a morphological operation or are already existing forms that take on new meanings, and (ii) lexical regularity, depending on whether neologisms are created through regular linguistic processes or not. We hypothesize that morphological neologisms are more salient than semantic ones, and that irregular neologisms are more salient than regular ones. We designed a behavioral experiment to test these hypotheses with French native speakers, measuring detection rates and response times for various types of neologisms. The results support our main hypotheses, additionally showing that lexical regularity is a stronger predictor than formal novelty, and that an interaction effect exists between the two factors.},
	pages = {103055},
	journaltitle = {Lingua},
	shortjournal = {Lingua},
	author = {Lombard, Alizée and Huyghe, Richard and Gygax, Pascal},
	urldate = {2025-01-19},
	date = {2021-04},
	langid = {english},
}

@misc{carvalho_corpus_2012,
	title = {Corpus del Español en el Sur de Arizona ({CESA}).},
	rights = {University of Arizona},
	author = {Carvalho, A},
	date = {2012},
}

@article{van_zoonen_algorithmic_2024,
	title = {Algorithmic management of crowdworkers: Implications for workers’ identity, belonging, and meaningfulness of work},
	volume = {152},
	issn = {07475632},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0747563223004405},
	doi = {10.1016/j.chb.2023.108089},
	shorttitle = {Algorithmic management of crowdworkers},
	pages = {108089},
	journaltitle = {Computers in Human Behavior},
	shortjournal = {Computers in Human Behavior},
	author = {Van Zoonen, Ward and Sivunen, Anu E. and Treem, Jeffrey W.},
	urldate = {2025-01-28},
	date = {2024-03},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\XHZFIW8B\\Van Zoonen et al. - 2024 - Algorithmic management of crowdworkers Implicatio.pdf:application/pdf},
}

@article{dillion_can_2023,
	title = {Can {AI} language models replace human participants?},
	volume = {27},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661323000980},
	doi = {10.1016/j.tics.2023.04.008},
	pages = {597--600},
	number = {7},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Dillion, Danica and Tandon, Niket and Gu, Yuling and Gray, Kurt},
	urldate = {2025-01-28},
	date = {2023-07},
	langid = {english},
}

@article{gui_challenge_2023,
	title = {The Challenge of Using {LLMs} to Simulate Human Behavior: A Causal Inference Perspective},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2312.15524},
	doi = {10.48550/ARXIV.2312.15524},
	shorttitle = {The Challenge of Using {LLMs} to Simulate Human Behavior},
	abstract = {Large Language Models ({LLMs}) have shown impressive potential to simulate human behavior. We identify a fundamental challenge in using them to simulate experiments: when {LLM}-simulated subjects are blind to the experimental design (as is standard practice with human subjects), variations in treatment systematically affect unspecified variables that should remain constant, violating the unconfoundedness assumption. Using demand estimation as a context and an actual experiment as a benchmark, we show this can lead to implausible results. While confounding may in principle be addressed by controlling for covariates, this can compromise ecological validity in the context of {LLM} simulations: controlled covariates become artificially salient in the simulated decision process, which introduces focalism. This trade-off between unconfoundedness and ecological validity is usually absent in traditional experimental design and represents a unique challenge in {LLM} simulations. We formalize this challenge theoretically, showing it stems from ambiguous prompting strategies, and hence cannot be fully addressed by improving training data or by fine-tuning. Alternative approaches that unblind the experimental design to the {LLM} show promise. Our findings suggest that effectively leveraging {LLMs} for experimental simulations requires fundamentally rethinking established experimental design practices rather than simply adapting protocols developed for human subjects.},
	author = {Gui, George and Toubia, Olivier},
	urldate = {2025-01-28},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {{FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI}), Applications (stat.{AP}), Econometrics (econ.{EM}), {FOS}: Economics and business, Information Retrieval (cs.{IR})},
}

@article{gui_challenge_2023-1,
	title = {The Challenge of Using {LLMs} to Simulate Human Behavior: A Causal Inference Perspective},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=4650172},
	doi = {10.2139/ssrn.4650172},
	shorttitle = {The Challenge of Using {LLMs} to Simulate Human Behavior},
	journaltitle = {{SSRN} Electronic Journal},
	shortjournal = {{SSRN} Journal},
	author = {Gui, George and Toubia, Olivier},
	urldate = {2025-01-28},
	date = {2023},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\BCDH XMG\\Zotero\\storage\\Z2BP7TKZ\\Gui und Toubia - 2023 - The Challenge of Using LLMs to Simulate Human Beha.pdf:application/pdf},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4 Technical Report},
	rights = {{arXiv}.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.08774},
	doi = {10.48550/ARXIV.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	publisher = {{arXiv}},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and {Pokorny} and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, CJ and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	urldate = {2025-01-28},
	date = {2023},
	note = {Version Number: 6},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
	annotation = {Other
100 pages; updated authors list; fixed author names and added citation},
}

@article{kocon_chatgpt_2023,
	title = {{ChatGPT}: Jack of all trades, master of none},
	volume = {99},
	issn = {15662535},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S156625352300177X},
	doi = {10.1016/j.inffus.2023.101861},
	shorttitle = {{ChatGPT}},
	pages = {101861},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Kocoń, Jan and Cichecki, Igor and Kaszyca, Oliwier and Kochanek, Mateusz and Szydło, Dominika and Baran, Joanna and Bielaniewicz, Julita and Gruza, Marcin and Janz, Arkadiusz and Kanclerz, Kamil and Kocoń, Anna and Koptyra, Bartłomiej and Mieleszczenko-Kowszewicz, Wiktoria and Miłkowski, Piotr and Oleksy, Marcin and Piasecki, Maciej and Radliński, Łukasz and Wojtasik, Konrad and Woźniak, Stanisław and Kazienko, Przemysław},
	urldate = {2025-01-29},
	date = {2023-11},
	langid = {english},
	file = {Eingereichte Version:C\:\\Users\\BCDH XMG\\Zotero\\storage\\PHIFWDYI\\Kocoń et al. - 2023 - ChatGPT Jack of all trades, master of none.pdf:application/pdf},
}

@article{dynel_lessons_2023,
	title = {Lessons in linguistics with {ChatGPT}: Metapragmatics, metacommunication, metadiscourse and metalanguage in human-{AI} interactions},
	volume = {93},
	issn = {02715309},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0271530923000605},
	doi = {10.1016/j.langcom.2023.09.002},
	shorttitle = {Lessons in linguistics with {ChatGPT}},
	pages = {107--124},
	journaltitle = {Language \& Communication},
	shortjournal = {Language \& Communication},
	author = {Dynel, Marta},
	urldate = {2025-01-29},
	date = {2023-11},
	langid = {english},
}

@article{han_chatgpt_2024,
	title = {Chatgpt in and for second language acquisition: A call for systematic research},
	volume = {46},
	issn = {0272-2631, 1470-1545},
	url = {https://www.cambridge.org/core/product/identifier/S0272263124000111/type/journal_article},
	doi = {10.1017/S0272263124000111},
	shorttitle = {Chatgpt in and for second language acquisition},
	pages = {301--306},
	number = {2},
	journaltitle = {Studies in Second Language Acquisition},
	shortjournal = {Stud Second Lang Acquis},
	author = {Han, {ZhaoHong}},
	urldate = {2025-01-29},
	date = {2024-05},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\WALDTGEL\\Han - 2024 - Chatgpt in and for second language acquisition A .pdf:application/pdf},
}

@article{duncan_does_2024,
	title = {Does {ChatGPT} have sociolinguistic competence?},
	volume = {8},
	rights = {https://creativecommons.org/licenses/by-nc-nd/4.0},
	issn = {2530-9455},
	url = {https://polipapers.upv.es/index.php/jclr/article/view/21958},
	doi = {10.4995/jclr.2024.21958},
	abstract = {Large language models are now able to generate content- and genre-appropriate prose with grammatical sentences. However, these targets do not fully encapsulate human-like language use. For example, set aside is the fact that human language use involves sociolinguistic variation that is regularly constrained by internal and external factors. This article tests whether one widely used {LLM} application, {ChatGPT}, is capable of generating such variation. I construct an English corpus of “sociolinguistic interviews” using the application and analyze the generation of seven morphosyntactic features. I show that the application largely fails to generate any variation at all when one variant is prescriptively incorrect, but that it is able to generate variable deletion of the complementizer that that is internally constrained, with variants occurring at human-like rates. {ChatGPT} fails, however, to properly generate externally constrained complementizer that deletion. I argue that these outcomes reflect bias both in the training data and Reinforcement Learning from Human Feedback. I suggest that testing whether an {LLM} can properly generate sociolinguistic variation is a useful metric for evaluating if it generates human-like language.},
	pages = {51--75},
	journaltitle = {Journal of Computer-Assisted Linguistic Research},
	shortjournal = {J. Comp. Assist. Linguist. Res.},
	author = {Duncan, Daniel},
	urldate = {2025-01-29},
	date = {2024-11-15},
}

@article{ziems_can_2024,
	title = {Can Large Language Models Transform Computational Social Science?},
	volume = {50},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/50/1/237/118498/Can-Large-Language-Models-Transform-Computational},
	doi = {10.1162/coli_a_00502},
	abstract = {Abstract
            Large language models ({LLMs}) are capable of successfully performing many language processing tasks zero-shot (without training data). If zero-shot {LLMs} can also reliably classify and explain social phenomena like persuasiveness and political ideology, then {LLMs} could augment the computational social science ({CSS}) pipeline in important ways. This work provides a road map for using {LLMs} as {CSS} tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 25 representative English {CSS} benchmarks. On taxonomic labeling tasks (classification), {LLMs} fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), {LLMs} produce explanations that often exceed the quality of crowdworkers’ gold references. We conclude that the performance of today’s {LLMs} can augment the {CSS} research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the underlying attributes of a text). In summary, {LLMs} are posed to meaningfully participate in social science analysis in partnership with humans.},
	pages = {237--291},
	number = {1},
	journaltitle = {Computational Linguistics},
	author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
	urldate = {2025-01-29},
	date = {2024-03-01},
	langid = {english},
	file = {Volltext:C\:\\Users\\BCDH XMG\\Zotero\\storage\\DRFZM6BA\\Ziems et al. - 2024 - Can Large Language Models Transform Computational .pdf:application/pdf},
}

@misc{wei_chain--thought_2022,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2201.11903},
	doi = {10.48550/ARXIV.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the {GSM}8K benchmark of math word problems, surpassing even finetuned {GPT}-3 with a verifier.},
	publisher = {{arXiv}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	urldate = {2025-01-29},
	date = {2022},
	note = {Version Number: 6},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Artificial Intelligence (cs.{AI})},
}

@article{gilardi_chatgpt_2023,
	title = {{ChatGPT} Outperforms Crowd-Workers for Text-Annotation Tasks},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2303.15056},
	doi = {10.48550/ARXIV.2303.15056},
	abstract = {Many {NLP} applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as {MTurk} as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that {ChatGPT} outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of {ChatGPT} exceeds that of crowd-workers for four out of five tasks, while {ChatGPT}'s intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of {ChatGPT} is less than \$0.003 -- about twenty times cheaper than {MTurk}. These results show the potential of large language models to drastically increase the efficiency of text classification.},
	author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
	urldate = {2025-01-30},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 2},
	keywords = {Computation and Language (cs.{CL}), Computers and Society (cs.{CY}), {FOS}: Computer and information sciences},
	annotation = {Other
Gilardi, Fabrizio, Meysam Alizadeh, and Ma{\textbackslash}"el Kubli. 2023. "{ChatGPT} Outperforms Crowd Workers for Text-Annotation Tasks". Proceedings of the National Academy of Sciences 120(30): e2305016120},
}

@inproceedings{wu_llms_2025,
	location = {Yokohama, Japan},
	title = {{LLMs} as Workers in Human-Computational Algorithms? Replicating Crowdsourcing Pipelines with {LLMs}},
	rights = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	isbn = {79-8-4007-1395-8/25/04},
	url = {https://arxiv.org/abs/2307.10168},
	doi = {10.48550/ARXIV.2307.10168},
	shorttitle = {{LLMs} as Workers in Human-Computational Algorithms?},
	abstract = {{LLMs} have shown promise in replicating human-like behavior in crowdsourcing tasks that were previously thought to be exclusive to human abilities. However, current efforts focus mainly on simple atomic tasks. We explore whether {LLMs} can replicate more complex crowdsourcing pipelines. We find that modern {LLMs} can simulate some of crowdworkers' abilities in these ``human computation algorithms,'' but the level of success is variable and influenced by requesters' understanding of {LLM} capabilities, the specific skills required for sub-tasks, and the optimal interaction modality for performing these sub-tasks. We reflect on human and {LLMs}' different sensitivities to instructions, stress the importance of enabling human-facing safeguards for {LLMs}, and discuss the potential of training humans and {LLMs} with complementary skill sets. Crucially, we show that replicating crowdsourcing pipelines offers a valuable platform to investigate 1) the relative {LLM} strengths on different tasks (by cross-comparing their performances on sub-tasks) and 2) {LLMs}' potential in complex tasks, where they can complete part of the tasks while leaving others to humans.},
	eventtitle = {{CHI} {EA} ’25},
	author = {Wu, Tongshuang and Zhu, Haiyi and Albayrak, Maya and Axon, Alexis and Bertsch, Amanda and Deng, Wenxing and Ding, Ziqi and Guo, Bill and Gururaja, Sireesh and Kuo, Tzu-Sheng and Liang, Jenny T. and Liu, Ryan and Mandal, Ihita and Milbauer, Jeremiah and Ni, Xiaolin and Padmanabhan, Namrata and Ramkumar, Subhashini and Sudjianto, Alexis and Taylor, Jordan and Tseng, Ying-Jui and Vaidos, Patricia and Wu, Zhijin and Wu, Wei and Yang, Chenyang},
	urldate = {2025-01-30},
	date = {2025-05-26},
	note = {Publisher: {arXiv}
Version Number: 3},
	keywords = {Computation and Language (cs.{CL}), {FOS}: Computer and information sciences, Human-Computer Interaction (cs.{HC})},
}
