\section{Related Work}
\label{sec:related}
\subsection{Large Language Models as crowd workers in NLP}

LLMs are remarkable data annotators. Their pre-training and supervised instruction fine-tuning build intelligent assistants that mimic human behaviour \citep{openai_gpt-4_2023}. Consequently, LLMs exhibit exceptional capabilities in adhering to instructions designed to elicit specific model responses, while offering a user-friendly interaction experience. Within the field of NLP, ongoing research investigates these abilities through the development of frameworks that either incorporate human informants, or that operate independently of them \citep{kocon_chatgpt_2023}. Even if LLMs do not consistently outperform humans across all tasks, they are well-positioned to make meaningful contributions to social science analysis in collaboration with human researchers \citep{ziems_can_2024}. These pipelines often task computational models with performing direct or indirect annotation \citep{van_dalfsen_direct_2024}, using either supervised or unsupervised methods \citep{horych_promises_2024}, or by establishing semi-automated annotation workflows \citep{ostyakova_chatgpt_2023, xu_role_2024}. Recent findings suggest that LLMs match or surpass human crowd workers in numerous NLP tasks \citep{tornberg_chatgpt-4_2023, ziems_can_2024, he_annollm_2023, kocon_chatgpt_2023, gilardi_chatgpt_2023}. However, these studies predominantly focus on crowdsourcing scenarios where participants are required to assign labels to data, rather than engage in language generation or exercise complex judgment. \citet{wu_llms_2025} use LLMs in the replication of previous crowdsourcing pipelines in a course assignment and describe high variance in the results according to task complexity. They suggest that analysing LLMs within established pipelines or workflows provides a clearer insight into their strengths and limitations; accordingly, this paper aligns with that objective.


\subsection{Large Language Models as crowd workers in empirical Linguistics}

Although disciplines outside NLP, such as cognitive research \citep{dillion_can_2023} and economics \citep{gui_challenge_2023}, have already explored whether LLMs may replace or at least productively simulate human participants in empirical studies, this matter remains understudied in Linguistics. \citep{Argyle_2023} provided initial evidence suggesting that language models can serve as proxies for specific human sub-populations, offering new avenues for research in the social sciences. This has been applied to linguistic tasks such as sentiment analysis \citep{borst_death_2023, rebora_comparing_2023}, topic detection \citep{kosar_comparative_2024}, semantic annotation \citep{gilardi_chatgpt_2023}, paraphrase generation \citep{cegin_chatgpt_2023}, and text classification \citep{de_lange_benchmarking_2024}, but has not been extended systematically outside NLP-related annotation tasks, apart from few exceptions (see \citep{karjus2024machineassisted} on a set of case studies such as annotation of semantic change detection, and \citep{Karjus2024-zg} on measurements of linguistic divergence in US American English across political spectra). Even with a linguistic core interest, these annotation tasks display a strong connection to NLP-based pipelines rather than to actual ongoing linguistic research. Linguistics researchers continue to focus mostly on experimental designs involving human participants, or they engage with LLMs primarily through the ChatGPT interface \citep{dynel_lessons_2023, han_chatgpt_2024, duncan_does_2024}, rather than exploring a broader range of models within actual coding environments. Empirical linguistic studies vary in design, employing diverse data collection methods, analytical goals, and participant criteria, offering a valuable opportunity to evaluate LLMs as crowd workers.