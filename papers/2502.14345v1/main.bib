@article{Hallucination-survey,
  title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author={Yue Zhang and Yafu Li and Leyang Cui and Deng Cai and Lemao Liu and Tingchen Fu and Xinting Huang and Enbo Zhao and Yu Zhang and Yulong Chen and Longyue Wang and Anh Tuan Luu and Wei Bi and Freda Shi and Shuming Shi},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.01219},
  url={https://api.semanticscholar.org/CorpusID:261530162}
}

@misc{coze,
  author = {{Coze}},
  title = {Coze Platform},
  howpublished = {\url{https://www.coze.com}},
  year={2024},
  note = {Accessed: November 3, 2024}
}

@misc{dify,
  author = {{Dify}},
  title = {Dify Repository},
  howpublished = {\url{https://github.com/langgenius/dify}},
  year={2024},
  note = {Accessed: November 3, 2024}
}

@misc{flowise,
  author = {{Flowise}},
  title = {Flowise Repository},
  howpublished = {\url{https://github.com/FlowiseAI/Flowise}},
  year={2024},
  note = {Accessed: November 3, 2024}
}


% 来自qiu组
@article{Agents-survey,
  title={The Rise and Potential of Large Language Model Based Agents: A Survey},
  author={Zhiheng Xi and Wenxiang Chen and Xin Guo and Wei He and Yiwen Ding and Boyang Hong and Ming Zhang and Junzhe Wang and Senjie Jin and Enyu Zhou and Rui Zheng and Xiaoran Fan and Xiao Wang and Limao Xiong and Qin Liu and Yuhao Zhou and Weiran Wang and Changhao Jiang and Yicheng Zou and Xiangyang Liu and Zhangyue Yin and Shihan Dou and Rongxiang Weng and Wensen Cheng and Qi Zhang and Wenjuan Qin and Yongyan Zheng and Xipeng Qiu and Xuanjing Huan and Tao Gui},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.07864},
  url={https://api.semanticscholar.org/CorpusID:261817592}
}

@article{GenAgent,
  title={GenAgent: Build Collaborative AI Systems with Automated Workflow Generation - Case Studies on ComfyUI},
  author={Xiangyuan Xue and Zeyu Lu and Di Huang and Wanli Ouyang and Lei Bai},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.01392},
  url={https://api.semanticscholar.org/CorpusID:272366611}
}

@misc{AFlow,
  title={AFlow: Automating Agentic Workflow Generation},
  author={Jiayi Zhang and Jinyu Xiang and Zhaoyang Yu and Fengwei Teng and Xionghui Chen and Jiaqi Chen and Mingchen Zhuge and Xin Cheng and Sirui Hong and Jinlin Wang and Bingnan Zheng and Bangbang Liu and Yuyu Luo and Chenglin Wu},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273345847}
}

% === application === 
application: medical
@article{MedAgents,
  title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning},
  author={Xiangru Tang and Anni Zou and Zhuosheng Zhang and Yilun Zhao and Xingyao Zhang and Arman Cohan and Mark B. Gerstein},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.10537},
  url={https://api.semanticscholar.org/CorpusID:265281260}
}
application: GenerativeAgents
@article{GenerativeAgents,
  title={Generative Agents: Interactive Simulacra of Human Behavior},
  author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
  journal={Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:258040990}
}
application: software
@inproceedings{ChatDev,
  title={ChatDev: Communicative Agents for Software Development},
  author={Cheng Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:270257715}
}


% Toolformer
@article{Toolformer,
  title={Toolformer: Language Models Can Teach Themselves to Use Tools},
  author={Timo Schick and Jane Dwivedi-Yu and Roberto Dess{\`i} and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.04761},
  url={https://api.semanticscholar.org/CorpusID:256697342}
}

KnowledGPT, 来自xiao组
@article{KnowledGPT,
  title={Knowledgpt: Enhancing large language models with retrieval and storage access on knowledge bases},
  author={Wang, Xintao and Yang, Qianwen and Qiu, Yongting and Liang, Jiaqing and He, Qianyu and Gu, Zhouhong and Xiao, Yanghua and Wang, Wei},
  journal={arXiv preprint arXiv:2308.11761},
  year={2023}
}


@misc{WorFBench,
  title={Benchmarking Agentic Workflow Generation},
  author={Shuofei Qiao and Runnan Fang and Zhisong Qiu and Xiaobin Wang and Ningyu Zhang and Yong Jiang and Pengjun Xie and Fei Huang and Huajun Chen},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:273234185}
}

@inproceedings{CoT-survey,
  title={Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future},
  author={Zheng Chu and Jingchang Chen and Qianglong Chen and Weijiang Yu and Tao He and Haotian Wang and Weihua Peng and Ming Liu and Bing Qin and Ting Liu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263153015}
}



=========================
DAG-test-gen: 
@article{DAG-test-gen,
  title={Automated test generation to evaluate tool-augmented LLMs as conversational AI agents},
  author={Samuel Arcadinho and David Apar{\'i}cio and Mariana Almeida},
  journal={ArXiv},
  year={2024},
  volume={abs/2409.15934},
  url={https://api.semanticscholar.org/CorpusID:272832010}
}

@article{SystemPrompt,
  title={The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions},
  author={Eric Wallace and Kai Xiao and Reimar H. Leike and Lilian Weng and Johannes Heidecke and Alex Beutel},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.13208},
  url={https://api.semanticscholar.org/CorpusID:269294048}
}


PPTOD
@inproceedings{PPTOD,
  title={Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System},
  author={Yixuan Su and Lei Shu and Elman Mansimov and Arshit Gupta and Deng Cai and Yi-An Lai and Yi Zhang},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:238226978}
}
ATD
@article{ATD,
  title={A Task-Oriented Dialog Model with Task-Progressive and Policy-Aware Pre-training},
  author={Lucen Zhong and Hengtong Lu and Caixia Yuan and Xiaojie Wang and Jiashen Sun and Ke Zeng and Guanglu Wan},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.00597},
  url={https://api.semanticscholar.org/CorpusID:263333890}
}
SPACE-1 | GALAXY
@inproceedings{SPACE-1,
  title={GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection},
  author={Wanwei He and Yinpei Dai and Yinhe Zheng and Yuchuan Wu and Zhen Cao and Dermot Liu and Peng Jiang and Min Yang and Feiling Huang and Luo Si and Jian Sun and Yongbin Li},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:244714676}
}
SPACE-2
@inproceedings{SPACE-2,
  title={SPACE-2: Tree-Structured Semi-Supervised Contrastive Pre-training for Task-Oriented Dialog Understanding},
  author={Wanwei He and Yinpei Dai and Binyuan Hui and Min Yang and Zhen Cao and Jianbo Dong and Fei Huang and Luo Si and Yongbin Li},
  booktitle={International Conference on Computational Linguistics},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252222550}
}

PLATO
@inproceedings{PLATO,
  title={PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable},
  author={Siqi Bao and H. He and Fan Wang and Hua Wu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:204744108}
}
DialoGPT
@inproceedings{DialoGPT,
  title={DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation},
  author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and William B. Dolan},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:207869708}
}

@article{CoT,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed Huai-hsin Chi and F. Xia and Quoc Le and Denny Zhou},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.11903},
}

@misc{Mermaid,
    author = {Sveidqvist, Knut},
    license = {MIT},
    month = dec,
    title = {{Mermaid: Generate diagrams from markdown-like text}},
    url = {https://github.com/mermaid-js/mermaid},
    year = {2014}
}

SGD dataset
@inproceedings{SGD,
  title={Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset},
  author={Abhinav Rastogi and Xiaoxue Zang and Srinivas Sunkara and Raghav Gupta and Pranav Khaitan},
  booktitle={AAAI Conference on Artificial Intelligence},
  year={2019},
}


TOD-Adapters
This paper proposes an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network, and enhances the performance of the DST and NLG modules through reinforcement learning.
@article{TOD-Adapters,
  title={Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System},
  author={Namo Bang and Jeehyun Lee and Myoung-Wan Koo},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.02468},
}

LLM-dialogue-survey
@article{LLM-dialogue-survey,
  title={A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems},
  author={Zihao Yi and Jiarui Ouyang and Yuwen Liu and Tianhao Liao and Zhe Xu and Ying Shen},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.18013},
}

GLAD 采用 Joint Goal Accuracy 评估
This paper proposes the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules and shows that this significantly improves tracking of rare states.
@inproceedings{GLAD,
  title={Global-Locally Self-Attentive Encoder for Dialogue State Tracking},
  author={Victor Zhong and Caiming Xiong and Richard Socher},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2018},
}

TRADE 采用 Slot accuracy (SA) 评估
A Transferable Dialogue State Generator (TRADE) that generates dialogue states from utterances using copy mechanism, facilitating transfer when predicting (domain, slot, value) triplets not encountered during training.
@article{TRADE,
  title={Transferable Multi-Domain State Generator for Task-Oriented Dialogue Systems},
  author={Chien-Sheng Wu and Andrea Madotto and Ehsan Hosseini-Asl and Caiming Xiong and Richard Socher and Pascale Fung},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.08743},
}

@article{ReAct,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.03629},
}


@article{Qwen2,
  title={Qwen2 Technical Report},
  author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Ke-Yang Chen and Kexin Yang and Mei Li and Min Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Yang Fan and Yang Yao and Yichang Zhang and Yunyang Wan and Yunfei Chu and Zeyu Cui and Zhenru Zhang and Zhi-Wei Fan},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.10671},
}

@article{GPT-4,
  title={GPT-4 Technical Report},
  author={OpenAI Josh Achiam and Steven Adler and etc.},
  year={2023},
  journal={ArXiv},
}


% --------------------------------------------------------------------------------------------------------------
FlowBench: Workflow benchmark
This work formalizes different formats of workflow knowledge and presents FlowBench, the first benchmark for workflow-guided planning, and evaluates the efficacy of workflow knowledge across multiple formats, indicating that current LLM agents need considerable improvements for satisfactory planning.
@article{FlowBench,
  title={FlowBench: Revisiting and Benchmarking Workflow-Guided Planning for LLM-based Agents},
  author={Rui Xiao and Wen-Cheng Ma and Ke Wang and Yuchuan Wu and Junbo Zhao and Haobo Wang and Fei Huang and Yongbin Li},
  journal={ArXiv},
  year={2024},
  volume={abs/2406.14884},
}


Code_survey: Wizard & Wand 加入code训练可以提升下游能力 (agent). by UIUC, 24ICLR
An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.
一些结论: 
3.2 Empower LLMs’ Complex Reasoning -- 1] Code pre-training improves chain-of-thought performance. 2] Program-of-thought outperforms chain-of-thought. 
3.3 Enable LLMs to Capture Structured Knowledge -- 1] Code generation unveils superior structural commonsense reasoning. 2] Markup code mastery evolves visually situated natural language understanding.
@article{Code_survey,
  title={If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents},
  author={Ke Yang and Jiateng Liu and John Wu and Chaoqi Yang and Yi Ren Fung and Sha Li and Zixuan Huang and Xu Cao and Xingyao Wang and Yiquan Wang and Heng Ji and Chengxiang Zhai},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.00812},
}

CodeAct: 相较于输出ReAct/JSON, 直接输出代码可以提升agent的能力 (更少的交互轮次) by UIUC, Apple, 24ICML
视角: agent输出的动作空间? 不同语法对于结果性能的影响
https://github.com/xingyaoww/code-act 0.45k
This work proposes to use executable Python code to consolidate LLM agents' actions into a unified action space (CodeAct), and collects an instruction-tuning dataset CodeActInstruct, which can be used with existing data to improve models in agent-oriented tasks without compromising their general capability.
@article{CodeAct,
  title={Executable Code Actions Elicit Better LLM Agents},
  author={Xingyao Wang and Yangyi Chen and Lifan Yuan and Yizhe Zhang and Yunzhu Li and Hao Peng and Heng Ji},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.01030},
}

GoG: 在不完整的KG上推理&补全
https://github.com/YaooXu/GoG
@article{GoG,
  title={Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering},
  author={Yao Xu and Shizhu He and Jiabei Chen and Zihao Wang and Yangqiu Song and Hanghang Tong and Kang Liu and Jun Zhao},
  journal={ArXiv},
  year={2024},
  volume={abs/2404.14741},
}

ProAgent: 提出了 Agentic Process Automation (APA) 的概念, 并将ProAgent在n8n场景下验证了有效性 by Tsinghua
语法定义 Agentic Workflow Description Language. 在数据表述和流程表述分别用了: JSON Structure for Data Flow , Python Code for Control Flow 
https://github.com/openbmb/proagent 0.75k
This paper introduces Agentic Process Automation (APA), a groundbreaking automation paradigm using LLM-based agents for advanced automation by offloading the human labor to agents associated with construction and execution, and instantiates ProAgent, an LLm-based agent designed to craft workflows from human instructions and make intricate decisions by coordinating specialized agents.
@article{ProAgent,
  title={ProAgent: From Robotic Process Automation to Agentic Process Automation},
  author={Yining Ye and Xin Cong and Shizuo Tian and Jian Cao and Hao Wang and Yujia Qin and Ya-Ting Lu and Heyang Yu and Huadong Wang and Yankai Lin and Zhiyuan Liu and Maosong Sun},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.10751},
}

KnowAgent: 利用外部的action KB来增强LLM生成动作序列/planning 的能力 by ZJU
https://www.zjukg.org/project/KnowAgent/
数据: HotpotQA and ALFWorld 因为要求action KB所以有局限性
This work introduces KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge, and employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents.
@article{KnowAgent,
  title={KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents},
  author={Yuqi Zhu and Shuofei Qiao and Yixin Ou and Shumin Deng and Ningyu Zhang and Shiwei Lyu and Yue Shen and Lei Liang and Jinjie Gu and Huajun Chen},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.03101},
}

KG-Agent: KG上的agent推理框架, 代码还没开源. by RUC
实现了在KG上复杂推理的框架, 包括 LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. 
@article{KG-Agent,
  title={KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph},
  author={Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yang Song and Chen Zhu and Hengshu Zhu and Ji-Rong Wen},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.11163},
}

NL-planner: 比较早做planning的工作, by Berkeley 750+
实验: VirtualHome
https://github.com/huangwl18/language-planner
https://wenlong.page/language-planner/
@article{NL-planner,
  title={Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents},
  author={Wenlong Huang and P. Abbeel and Deepak Pathak and Igor Mordatch},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.07207},
}

NL-knwoledge-injection: 在文字游戏中, 通过NL形式来进行知识注入/增强
This paper proposes a knowledge-injection framework for improved functional grounding of agents in text-based games, and considers two forms of domain knowledge that are injected into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment.
@article{NL-knwoledge-injection,
  title={Knowledge-enhanced Agents for Interactive Text Games},
  author={Prateek Chhikara and Jiarui Zhang and Filip Ilievski and Jonathan M Francis and Kaixin Ma},
  journal={Proceedings of the 12th Knowledge Capture Conference 2023},
  year={2023},
}


% --------------------------------------------------------------------------------------------------------------
AutoAgents: 给定一个任务自动生成多agnets来实现 by PKU, 24IJCAI
https://github.com/Link-AGI/AutoAgents 1.2k
比较: AutoGPT, MetaGPT, AutoGen 等自动化agent框架
AutoAgents is an experimental open-source application for an Automatic Agents Generation Experiment based on LLM. This program, driven by LLM, autonomously generates multi-agents to achieve whatever goal you set.
@article{AutoAgents,
  title={AutoAgents: A Framework for Automatic Agent Generation},
  author={Guangyao Chen and Siwei Dong and Yu Shu and Ge Zhang and Jaward Sesay and B{\"o}rje F. Karlsson and Jie Fu and Yemin Shi},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.17288},
}

PlanBench: 评估agent的planning能力 by Arizona, 23NIPS
https://github.com/karthikv792/LLMs-Planning
相关: BIG-Bench, GSM8K, CommonsenseQA, StrategyQA
This work proposes PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change.
@inproceedings{PlanBench,
  title={PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
  author={Karthik Valmeekam and Alberto Olmo and Sarath Sreedharan and Subbarao Kambhampati},
  booktitle={Neural Information Processing Systems},
  year={2022},
}
LLM+P: 利用外部求解器 PDDL 来提升planning能力 by Texas
https://github.com/Cranial-XIX/llm-pddl 0.35k 比较了使用外部求解器带来的提升. 
PDDL 介绍: https://planning.wiki/guide/whatis/pddl; https://en.wikipedia.org/wiki/Planning_Domain_Definition_Language
实验场景: rebot, 包括 [barman, blocksworld, floortile, grippers, storage, termes, tyreworld]
@article{LLM+P,
  title={LLM+P: Empowering Large Language Models with Optimal Planning Proficiency},
  author={B. Liu and Yuqian Jiang and Xiaohan Zhang and Qian Liu and Shiqi Zhang and Joydeep Biswas and Peter Stone},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11477},
}

AutoFlow: 自动化workflow生成 (CoRE)
提供了两种生成方法: fine-tuning-based and in-context-based methods. 
@article{AutoFlow,
  title={AutoFlow: Automated Workflow Generation for Large Language Model Agents},
  author={Zelong Li and Shuyuan Xu and Kai Mei and Wenyue Hua and Balaji Rama and Om Raheja and Hao Wang and He Zhu and Yongfeng Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2407.12821},
}

CoRE: workflow 语法
This work develops a novel system for Code Representation and Execution (CoRE), which employs LLM as interpreter to interpret and execute natural language instructions, and unifies natural language programming, pseudo-code programming, and flow programming under the same representation for constructing language agents.
@article{CoRE,
  title={AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow Programming of AI Agents},
  author={Shuyuan Xu and Zelong Li and Kai Mei and Yongfeng Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2405.06907},
}


NL2Code_survey: 代码生成 
https://nl2code.github.io
This paper presents a comprehensive survey of 27 existing large language models for NL2Code, and provides an intuitive comparison of all existing models on the HumanEval benchmark, and concludes that the key factors contributing to the success of large language model forNL2Code are “Large Size, Premium Data, Expert Tuning”.
@inproceedings{NL2Code_survey,
  title={Large Language Models Meet NL2Code: A Survey},
  author={Daoguang Zan and B. Chen and Fengji Zhang and Di Lu and Bingchao Wu and Bei Guan and Yongji Wang and Jian-Guang Lou},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2022},
}

STAR: 数据集
A scalable crowd-sourcing paradigm to collect arbitrarily large datasets of the same quality as STAR is proposed and novel schema-guided dialog models that use an explicit description of the task(s) to generalize from known to unknown tasks are introduced.
@article{STAR,
  title={STAR: A Schema-Guided Dialog Dataset for Transfer Learning},
  author={Johannes E. M. Mosig and Shikib Mehri and Thomas Kober},
  journal={ArXiv},
  year={2020},
  volume={abs/2010.11853},
}

CGoDial: 中文TOD数据集, by Ali
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/cgodial
CGoDial is proposed, a new challenging and comprehensive Chinese benchmark for multi-domain Goal-oriented Dialog evaluation that contains 96,763 dialog sessions, and 574,949 dialog turns totally, covering three datasets with different knowledge sources.
@inproceedings{CGoDial,
  title={CGoDial: A Large-Scale Benchmark for Chinese Goal-oriented Dialog Evaluation},
  author={Yinpei Dai and Wanwei He and Bowen Li and Yuchuan Wu and Zhen Cao and Zhongqi An and Jian Sun and Yongbin Li},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022},
}

SPACE-3: TOD 模型 & 预训练
SPACE is proposed, a novel unified pre-trained dialog model learning from large-scale dialog corpora with limited annotations, which can be effectively fine-tuned on a wide range of downstream dialog tasks.
@article{SPACE-3,
  title={Unified Dialog Model Pre-training for Task-Oriented Dialog Understanding and Generation},
  author={Wanwei He and Yinpei Dai and Min Yang and Jian Sun and Fei Huang and Luo Si and Yongbin Li},
  journal={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year={2022},
}

persona-hub: perosona数据集
Perosona-Hub proposes a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data.
@article{persona-hub,
  title={Scaling synthetic data creation with 1,000,000,000 personas},
  author={Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
  journal={arXiv preprint arXiv:2406.20094},
  year={2024}
}