\clearpage
\appendix
\section*{Appendices}

\begin{envnew}
\section{Dataset Examples} \label{appendix:data_examples}

\subsection{PDL Example} \label{appendix:pdl_example}
Below is a PDL example in a real-world scenario. For formats of natural language, code and flowchat, see \citet{FlowBench}.

\begin{lstlisting}[language={}, caption=Example of PDL]
Name: 114 Hospital Appointment
Desc: Provides appointment services, allowing users to query and recommend hospitals and departments in Beijing.
Detailed_desc: Queries the availability of appointment slots based on the user's specified hospital, department, and time, and attempts to register; if no slots are available at the specified hospital, it will try to register at other hospitals.

APIs:
  - name: check_hospital
    request: [hospital_name]
    response: [hospital_exists]
    precondition: []
  - name: check_department
    request: [department_name, hospital_name]
    response: [department_exists]
    precondition: [check_hospital]
  - name: query_appointment
    request: [hospital_name, department_name, appointment_time]
    response: [available_slots, available_list, specialist_count, general_count]
    precondition: [check_hospital, check_department]
  - name: recommend_other_hospitals
    desc: Searches for available slots at other hospitals for the specified department and time.
    request: [department_name, appointment_time]
    response: [available_slots, available_list]
    precondition: [check_department]
  - name: register_hospital
    request: [id_number, appointment_type, hospital_name, department_name, appointment_time]
    response: [appointment_status]
    precondition: [query_appointment]
  - name: register_other_hospital
    request: [id_number, hospital_name, doctor_name]
    response: [appointment_status]
    precondition: [recommend_other_hospitals]

ANSWERs:
  - name: hospital_not_found
    desc: Sorry, we currently cannot provide appointment services for this hospital. Please contact the hospital directly or consider other hospitals.
  - name: department_not_found
    desc: $hospital_name does not have the department you are looking for. I will transfer you to a customer service representative for further assistance. Please wait.
  - name: no_available_slots
    desc: We apologize, but there are no available slots for the department you want to register at any hospital on our platform. Please follow the WeChat public account "Beijing 114 Appointment appointment" to register as per your needs. Thank you for calling, and have a nice day.
  - name: appointment_successful
    desc: Your appointment at $hospital_name $department_name for $appointment_time has been successful. A confirmation message will be sent to your phone number shortly. Is there anything else I can help you with?
  - name: appointment_failed
    desc: We apologize, but there are no available $appointment_type slots at $hospital_name $department_name for $appointment_time. Please follow the WeChat public account "Beijing 114 Appointment appointment" to register as per your needs. Thank you for calling, and have a nice day.
  - name: other_hospital_appointment_successful
    desc: Your appointment at $recommend_other_hospitals-hospital_name with $recommend_other_hospitals-doctor_name for $appointment_time has been successful. A confirmation message will be sent to your phone number shortly. Is there anything else I can help you with?
  - name: other_hospital_appointment_failed
    desc: We apologize, but the ID information is incorrect, and we cannot proceed with the appointment. Please follow the WeChat public account "Beijing 114 Appointment appointment" to register as per your needs. Thank you for calling, and have a nice day.
  - name: answer_out_of_workflow_questions
  - name: request_information

Procedure: |
  [hospital_exists] = API.check_hospital([hospital_name])
  if hospital_exists == false:
    ANSWER.hospital_not_found()
  elif hospital_exists == true:
    [department_exists] = API.check_department([department_name, hospital_name])
    if department_exists == false:
      ANSWER.department_not_found()
    elif department_exists == true:
      [available_slots, available_list, specialist_count, general_count] = API.query_appointment([hospital_name, department_name, appointment_time])
      if available_slots > 0:
        [appointment_status] = API.register_hospital([id_number, appointment_type, hospital_name, department_name, appointment_time])
        if appointment_status == "1":
          ANSWER.appointment_successful()
        elif appointment_status == "0":
          ANSWER.appointment_failed()
      elif available_slots == 0:
        [available_slots, available_list] = API.recommend_other_hospitals([department_name, appointment_time])
        if available_slots > 0:
          if appointment_willingness == "true":
            [appointment_status] = API.register_other_hospital([id_number, hospital_name, doctor_name])
            if appointment_status == "1":
              ANSWER.other_hospital_appointment_successful()
            elif appointment_status == "0":
              ANSWER.pther_hospital_appointment_failed()
        elif available_slots == 0:
          ANSWER.no_available_slots()
\end{lstlisting}


\subsection{User Profile Example} \label{appendix:user_profile_example}
Below is an example of a used user profile. The \quotes{User Details} contains some randomly generated attributes; \quotes{Dialogue Style} specifies the user's conversational style; \quotes{User Needs} describes the user's requirements related to a specific workflow; \quotes{Interactive Pattern} further details the possible dialogue process for the user within that workflow.

\begin{lstlisting}[language={}, caption=Example of user profile]
**Persona**:  
A 25-year-old bartender with three years of experience in the hospitality industry. He is known for his honesty, often giving customers sincere advice on their drink choices.

**User Details**:  
- Name: Michael James Carter  
- Sex: Male  
- Age: 25  
- Phone Number: 13812345678  
- ID Number: 110105199801012345  

**User Needs**:  
- Michael needs to query available appointment slots for specific hospitals and departments in Beijing.  
- He may need to verify the existence of certain hospitals and departments.  
- He wants to make an appointment for a medical consultation at a preferred hospital and department.  
- If the preferred hospital or department is not available, he may need recommendations for alternative hospitals and departments.  
- Michael may also need to know the success or failure status of his appointment registration.  

**Dialogue Style**:  
- Michael's dialogue style is likely to be straightforward and sincere, reflecting his honesty in his profession as a bartender.  
- He may prefer clear and concise information without unnecessary jargon.  
- His tone is likely to be polite and respectful but also direct, as he is used to providing sincere advice to customers.  
- He may appreciate a friendly and helpful attitude from the assistant.  

**Interactive Pattern**:  
- Michael might start by specifying the hospital and department he is interested in.  
- He is likely to ask for available appointment slots for a specific time.  
- If the hospital or department does not exist, he will appreciate being notified promptly and clearly.  
- If there are no available slots at his preferred hospital, he may ask for recommendations for other hospitals.  
- He will likely ask for the success status of his appointment registration and may need guidance on the next steps if the registration fails.  
- Michael may interact in a sequential manner, addressing one query at a time, and expecting clear and immediate responses from the assistant.
\end{lstlisting}







\section{Conversation Examples} \label{appendix:conversation_examples}

\subsection{Conversation Example on STAR Dataset} \label{appendix:conversation_example_star}
Below is a conversation generated on the STAR dataset (for turn-level evaluation). 
Note that in line 16, the user simulated by the LLM adopted an \quotes{intent-switching/detail-switching} OOW and modified the appointment time.

\begin{lstlisting}[language={}, caption=Example conversation on STAR, numbers=left]
...
BOT: Great, Alex. What's the name of the apartment you're interested in?
USER: Maple Apartments.
BOT: When would you like to view the Maple Apartments?
USER: I'd like to do it on Friday.
BOT: What time on Friday would you prefer for the viewing?
USER: How about 3 PM?
BOT: Have you paid the application fee?
USER: Yes, I have.
BOT: Would you like to add any custom message for the viewing?
USER: No, that's fine.
BOT: <Call API> book_apartment_viewing({'RenterName': 'Alex', 'Name': 'Maple Apartments', 'Day': 'Friday', 'StartTimeHour': '15', 'ApplicationFeePaid': 'Yes', 'Message': '', 'RequestType': 'CheckAvailability'})
SYSTEM: {'Status': 'Available'}
BOT: The viewing is available. Would you like to proceed with booking?
USER: Actually, I might need to change the time. Can we do 4 PM instead?
    (OOW type) intent_switching/detail-switching
BOT: <Call API> book_apartment_viewing({'RenterName': 'Alex', 'Name': 'Maple Apartments', 'Day': 'Friday', 'StartTimeHour': '16', 'ApplicationFeePaid': 'Yes', 'Message': '', 'RequestType': 'CheckAvailability'})
SYSTEM: {'Status': 'Available'}
BOT: The new time at 4 PM is also available. Shall I book it for you?
USER: Yes, please book it.
\end{lstlisting}


\section{Experimental Details} \label{appendix:exp_details}
\subsection{Implementation Details} \label{appendix:implementation_details}

For the GPT series, we specifically used the models \codef{gpt-4o-2024-05-13}, \codef{gpt-4o-mini-2024-07-18}, and \codef{gpt-4-turbo-2024-04-09}. To ensure stable output results, we set the temperature to 0.2. For the Qwen2 series models, we utilized the vllm framework for inference, also setting the temperature to 0.2.


\subsection{Prompts} \label{appendix:prompts}

\paragraph{Prompts for User Simulation}
Below are the prompts we used with LLM to simulate user behavior. The \quotes{User Profile} refers to Sec.~\ref{appendix:user_profile_example}. 

Note that for OOW simulation, we add an \quotes{additional constraints} field in the user profile to describe the user's current OOW intention. For example, \quotes{In this round, you can ask a question unrelated to the current topic} will be injected in the prompt if an \quotes{irrelevant answering} OOW intent is randomly chosen in the 5th round of dialogue. We dynamically adjust OOW queries during the conversation by controlling the injected instructions.

\begin{lstlisting}[language={}, caption=Prompt for user simulation]
You are a real-life user that interact with an assistant of {{ assistant_description }} to achieve your specific objectives. 

## User Profile
```
{{ user_profile }}
```

## History conversation
```
{{ history_conversation }}
```

## Specific requirements
1. Role Awareness: Remember you are playing the user role and speak in the first person.
2. Goal-Oriented: Keep the conversation focused on achieving your needs.
3. Style: Keep your response concise and real-life.
4. Engagement: Maintain an engaging and curious tone to facilitate effective dialogue.
5. Your output format should be:
```
Response: xxx (the response content)
```
6. Stop: End the conversation when the task is completed or when it becomes repetitive and no longer meaningful to continue. Set your response as "[END]" to stop the conversation.
\end{lstlisting}


\paragraph{Inference Prompt for \model}
Below is the inference prompt for our \model.
\begin{lstlisting}[language={}, caption=Prompt for \model]
You are a bot designed to assist the user for a specific task described by the Procedure Description Language (PDL). Your goal is to engage in a friendly conversation with the user while helping them complete the task.

### Constraints
1. **Step Identification**: Throughout the conversation, you should determine the user's current step, (whether it is in the PDL or just general questions), and dynamically follow PDL:
    - If the user's query aligns with the PDL logic, proceed to the next step.
    - If the user ask irrelevant questions, generate a response that maintains a fluent and logical conversation.
2. **PDL Components**: The PDL includes several components:
    - meta information: `name, desc, desc_detail` are meta information about the PDL.
    - slots: `slots`s define the information you may need to collect from user, or the values returned by the API.
    - reference answer: `answers` define the responses you should response to the user.
    - procedure: the final `procedure` string is a Pythonic language that defines the core logic of the procedure. 
3. Notes:
    - You have to collect enough parameter values from the user before calling the apis. 

### PDL
```PDL
{{ PDL }}
```

### Available APIs
{{ api_infos }}

### History Conversation
{{ conversation }}

### Current state
{{ current_state | trim }}

### Output Format
Your output format should be chosen from one of the two templates below. 
1. If you need to interact with the user without calling an API (inquire slot values or reply/answer):
```
Thought: xxx (description of your thought process ) 
Response: xxx (the content you need to inquire or reply)
```
2. If you need to call an API: 
```
Thought: xxx (description of your thought process ) 
Action: xxx (the function name to be called, do not prefix "API_".)
Action Input: xxx (the parameters for the function, must be in strictly valid JSON format)
```
\end{lstlisting}


\paragraph{Inference Prompt for ReAct}
For the baseline ReAct, we directly borrowed the prompt used in FlowBench \citep{FlowBench}.
\begin{lstlisting}[language={}, caption=Prompt for ReAct]
You are a helpful assistant for the task of {{task_description}}.

### Specific requirements
1. You need to act as an assistant and engage in a conversation with the user, following the business process and API information.
2. You have been provided with the flowchart information for different scenarios under a specific role.
3. You can only answer questions within the scope of the given several workflow processes. If the user asks a question beyond these scopes, please apologize and explain to the user in the response part.
4. When asking for API input parameters, ensure that the provided parameter values comply with the specified format regarding both the correctness of the format and the completeness of the content. Do not assign values arbitrarily. In instances where the parameters do not meet the format requirements, notify users to make the adjustments until the requirements are satisfied.
5. When the user has multiple requests at the same time, please select one appropriate request for processing first and inform the user that other requests will be resolved subsequently. If there is unfinished business in the previous conversation, continue to provide the necessary help and guidance to assist them in completing the business process. When multiple APIs need to be called, do so in separate rounds, with a maximum of one API call output per round. When the user indicates that the business is finished or says goodbye, respond politely and end the conversation. 

### Workflow information
```
{{workflow}}
```

### Tool information
{{toolbox}}

### Current time
{{current_time}}

### History conversation
{{history_conversation}}

### Output format
Your output format should be chosen from one of the two templates below:
1. If you need to interact with the user:
```
Thought: xxx (description of your thought process ) 
Response: xxx (the content you need to inquire or reply)
```
2. If you need to call an API (only one API call per time): 
```
Thought: xxx (description of your thought process ) 
Action: xxx (the function name to be called, do not prefix "functions.")
Action Input: xxx (the parameters for the function, must be in strictly valid JSON format)
```
\end{lstlisting}


\paragraph{Evaluation Prompts}
During the evaluation process, to ensure fairness in the results, we basically reused the prompts from FlowBench. However, for the final statistics, we only used binary results to mitigate the bias issue of the judge model (see the discussion in Sec.~\ref{subsec:eval_method}). 
Below are the prompts we used for turn-level evaluation.
\begin{lstlisting}[language={}, caption=Prompt for turn-level evaluation]
Please serve as an impartial judge to evaluate the response quality of the assistant. Your evaluation should be based on the following criteria:
(1) Correctness: Does the reply remain consistent with the workflow knowledge without any contradictions?
(2) Helpfulness: Has the user's request been reasonably understood and addressed, fulfilling the user 's needs within the provided workflow scope?
(3) Humanness: Is the response coherent, clear, complete, and does it include human acknowledgment?
Please compare the provided response with the reference response and evaluate it based on the mentioned dimensions. Then, aggregate these assessments to assign an overall score. 
A perfect score is 10 points, with 9-10 points indicating high quality, nearly identical to the reference answer; 7-8 points indicating quality close to the reference answer; 6-7 points being of moderate quality; 4-5 points indicating a lower quality response; and 2-3 points for a response with significant errors.
Finally, output a binary result to determine if the predicted and reference responses are consistent (Yes or No).

Here is the knowledge related to the workflow: 
```
{{ workflow_info }}
```

Here is the previous conversation:
```
{{ session }}
```

Here is the true value response from the reference: 
{{ reference_input }}

Here is the generated response from the assistant: 
{{ predicted_input }}


Please reply with the scores and consistency judgment in the following format:
```
Correctness Score: xxx
Helpfulness Score: xxx
Humanness Score: xxx
Consistency: Yes/No
```
\end{lstlisting}

\section{Additional Method Details} \label{appendix:method_details}


\subsection{Data Construction} \label{subsec:data_construction}

Based on existing datasets, we performed data transformation and construction to evaluate agent performance across the \ofollow and \oflex dimensions. Our data construction process consists of three stages: \textit{workflow collection, workflow representation}, and \textit{dialogue construction}.

\paragraph{Workflow Collection}
Our dataset comprises two existing datasets: SGD \citep{SGD} and STAR \citep{STAR}, as well as our own constructed dataset, \dinhouse. The SGD dataset includes 26 task flows across 16 domains, while the STAR dataset covers 24 task flows across 13 domains. The \dinhouse dataset, constructed manually based on real-world scenarios in business,
contains 6 workflows and 16 tools across 6 domains.


\paragraph{Workflow Representation}
To compare the performance of our PDL syntax with other workflow formats, we converted each workflow 
under investigation
into four formats: natural language, code, flowchart, and PDL.
Referring to \citet{FlowBench}, we first converted the workflows from the original datasets into natural language.
Then, we used a LLM to respectively transform them into code, flowchart, and PDL formats.
The definitions of tools (a.k.a., APIs) follows the OpenAI function calling formats.\footnote{\url{https://platform.openai.com/docs/guides/function-calling}}
The entire workflow format conversion process was completed using GPT-4-Turbo.

\paragraph{Dialogue Construction}
For {turn-level evaluation}, we constructed diverse user intentions from tasks, using GPT-4o to directly construct reference sessions. We then parsed and annotated tool calls at the turn level. Regarding the construction of OOW scenarios, we strategically insert OOW queries into the reference session and record the OOW information.


For {session-level evaluation}, we selected user personas from \citet{persona-hub} that exhibits real-world diversity in response style and format.
We incorporated them into workflows to construct task-related user profiles.
We employed three LLMs to respectively simulate the roles of user, agent, and system with the given user profiles, workflow descriptions, and tool definitions.
We collected these simulated dialogues to form the session-level evaluation dataset.
As for the OOW scenarios, we have simulated users generating OOW queries with a certain probability, prompting the agent to respond to these queries and continue the conversation.
The example of generated conversation is shown in App.~\ref{appendix:conversation_example_star}

\section{Additional Experimental Results} \label{appendix:experimental_results}

\subsection{Turn-level Evaluation Results} \label{appendix:turn_level_results}
The table below presents the turn-level experimental results of Qwen2-72B. It's important to note that because Out-of-Workflow (OOW) turns typically involve fewer complex conditional judgments or API calls, the turn-level \metricf{Success Rate} for OOW turns can sometimes be higher than for In-Workflow (IW) turns. Additionally, since the turn-level evaluation for the OOW portion involves fewer API calls, directly calculating this metric may introduce significant variance. Therefore, we have left it blank in the table.
\tableExpTurn



\end{envnew}


