\section{Evaluation and Data}

\tableExpSession
\tableExpSessionOOW

\subsection{Compliance Evaluation}
\label{subsec:eval_method}

We follow previous studies~\citep{FlowBench,AutoAgents} to conduct both turn-level and session-level assessments.
In \textbf{turn-level evaluation}, there is a reference session (considered as ground truth)~\citep{CGoDial}.
For each turn in the reference session, the evaluation system provides the prefix of the session $\mathcal{H}_{t-1}$ to the bot for predicting the current $\hat{a}_{t}$.
The judge compares $\hat{a}_{t}$ with $a_{t}$ to determine if the bot's response for that turn is correct, and the average result across all turns yields the \metricf{Pass Rate}. 
To assess the agent's tool usage capability, for turns involving tool callings, we evaluate the tool selection and parameter infilling performance of the agent in \metricf{Precision, Recall, and F1-score}.


For \textbf{session-level evaluation}, we simulate user interactions with the bot using an LLM, which serves to mimic real user behavior while minimizing human assessment costs. 
To ensure these simulated sessions accurately reflect real-world complexity, we define detailed user profiles comprising: (1) demographic information; (2) conversational style, capturing behavioral patterns; and (3) workflow-related user needs, detailing primary and secondary session objectives. 
An illustrative user profile is provided in App.~\ref{appendix:user_profile_example}. For each generated session, we conduct a binary assessment to verify whether the user's primary workflow objectives are achieved, yielding the \metricf{Success Rate}. Additionally, by tracking the number of sub-tasks initiated and completed, we derive the \metricf{Task Progress} metric. Sessions are evaluated end-to-end using prompts consistent with those recommended by \citet{FlowBench}. 
Furthermore, we evaluate the LLM agentâ€™s performance in tool invocation with \metricf{Precision}, \metricf{Recall}, and \metricf{F1-score} metrics.


\subsection{Flexibility Evaluation}
Previous work \citep{GLAD,TRADE,AutoFlow} has primarily focused on evaluating whether bots can follow a specific procedure to complete a conversation, which partially emphasizes \ofollow while neglecting \oflex in handling user requests. Such incomprehensive evaluation may not reflect the capabilities of LLM agents under real-world scenarios, where an \quotes{imperfect} user might not adhere to the procedure and violates the sequential steps during multiple rounds of interactions. Consequently, to evaluate the performance of workflow agents in OOW scenarios, we have additionally developed a targeted evaluation method to assess \oflex.

Specifically, we categorize OOW scenarios into three types: 
(1) \textit{intent switching}, where the user suddenly changes the original intent requests or requirements, including modification of API slots/parameters and demand for cancellations; 
(2) \textit{procedure jumping}, where the user does not follow the established workflow sequence to provide information and express confirmation, including skipping steps or jumping back; 
and (3) \textit{irrelevant answering}, where the user deliberately avoids direct reply to questions raised by the agent, such as answers with topic shifts and rhetorical questions;

Based on these classifications, \oflex can be evaluated by examining the agent's performance in OOW scenarios using the metrics introduced in Sec.~\ref{subsec:eval_method}. At the turn-level, we insert OOW user interventions to assess the agent's immediate adaptive responses in these specific interactions.  At the session-level, we assess the agent's overall performance in sessions that include OOW queries to measure its long-term \oflex. 

\subsection{Data} \label{subsec:data}

We constructed three test datasets based on existing datasets and business-related data: SGD \citep{SGD}, STAR \citep{STAR}, and \dinhouse. The data construction process is detailed in App.~\ref{subsec:data_construction}. Statistics for these datasets are shown in Tab.~\ref{tab:dataset-stat}, and differences from datasets used in other studies are highlighted in Tab.~\ref{tab:dataset-compare}.

Specifically, our datasets include: (1) four types of workflows (see App.~\ref{appendix:data_examples}); (2) user profiles required for session-level evaluation (see App.~\ref{appendix:user_profile_example}); and (3) conversations needed for turn-level evaluation (see App.~\ref{appendix:conversation_example_star}).

