\figRadar 
\tableAblation


\section{Experiments} \label{sec:experiments}

We raise the following research questions: \\
\textbf{Q1}:
Compared with other models, does our proposed \model show improvements in \ofollow and \oflex? \\
\textbf{Q2}:
In which way the proposed controllers exert constraints on the model to facilitate workflows with both \ofollow and \oflex? \\


\subsection{Experimental Setup} \label{subsec:experimental_setup}

\paragraph{Baselines}
We selected ReAct \citep{ReAct} as a baseline method for comparison, which makes decisions in each round by utilizing a combination of \textit{thought} and \textit{action}, and treats the feedback from environment an \textit{observation}. It belongs to the category of prompt-based methods introduced in Sec.~\ref{subsec:workflow-agent}. For representing the workflow, we chose three formats: natural language (NL), code, and FlowChart, denoted as $\text{ReAct}_{\text{NL}}$, $\text{ReAct}_{\text{code}}$, and $\text{ReAct}_{\text{FC}}$, respectively. To ensure a fair comparison, we reused the prompts from FlowBench \citep{FlowBench} in our experiments.

\paragraph{Implementation}
In session-level evaluation, \codef{GPT-4o-mini} is used for user simulation. For the bot, we initially tested two representative model series, the GPT series \citep{GPT-4} and the Qwen series \citep{Qwen2}.
Preliminary studies revealed that small models are not competent for complex workflow tasks.
Therefore, in the present study, we choose \codef{GPT-4o} and \codef{Qwen2-72B} for demonstrations.
During the evaluation process, we used \codef{GPT-4-Turbo} for judgment.
More implementation details can be seen in App.~\ref{appendix:implementation_details}.


\subsection{Session-level Experimental Results} \label{subsec:session_level_results}

\paragraph{A1.1: \model outperforms the other three baselines in terms of task compliance.}
We first compare the session-level performance of different methods in Tab.~\ref{tab:session}.
The results indicate that \model outperforms the other three baselines in terms of task completion metrics \metricf{Success Rate}, \metricf{Task Progress}, and tool usage metrics like \metricf{Tool F1}.

\paragraph{A1.2: \model exhibits robustness towards OOW interventions with higher flexibility.}
Tab.~\ref{tab:session-oow} presents the performance of different methods under OOW scenarios.
A general performance decline is observed across all models on the three datasets.
However, \model exhibits only a slight decline, achieving the best results across all datasets.
Fig.~\ref{fig:radar}(a) visualizes the \metricf{Task Progress} metric under different settings, highlighting \model's advantage in OOW scenarios, demonstrating strong \oflex.


\subsection{Turn-level Experimental Results} \label{subsec:turn_level_results}


\paragraph{A1.3: \model maintains the superior \ofollow and \oflex across datasets in turn-level evaluation.}
We present the turn-level experimental results of Qwen2-72B in Tab.~\ref{tab:turn}. 
The results show that the \model framework achieves the best performance in both IW and OOW settings. 
What's more, Fig.~\ref{fig:radar}(b) compares the \metricf{Success Rate} across different models and settings.





\subsection{Ablation Studies}

\paragraph{A2: Controllers play an indispensable role in enforcing steady progress of workflows with OOW interventions.}
We conducted ablation experiments on \model in OOW settings, with the results shown in Tab.~\ref{tab:ab}. 
In the table, \quotes{-post} indicates the removal of the post-decision controllers $\mathcal{C}_{\text{post}}$ from the complete model, while \quotes{-post-pre} further removes the pre-decision controllers $\mathcal{C}_{\text{pre}}$. 
According to the experimental results, it is evident that removing either controller negatively impacts model performance, validating that controllers in \model enhance the model's \ofollow.
