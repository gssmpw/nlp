
\section{Domain Randomization}
Domain randomization (DR) is a technique that targets the state element \textit{S} of the Markov Decision Process to bridge the ‘Sim-to-Real’ gap, enhancing model generalization by randomizing simulation parameters [28]. The core idea of domain randomization is to train models on extensive sets of randomized simulation data so that the real world appears as just another variant of these randomizations [28]. Due to its ease of implementation and capability to produce robust policies, DR has achieved remarkable success in bridging the “Sim-to-Real” gap. However, successful implementations often require expert-driven manual tuning of parameter distributions, making it a time-intensive process [Closing the loop]. This challenge raises several key questions: What data is needed to learn the parameter distribution? And how can the parameter distribution be learned? This challenge prompts several key questions: \textit{What data is needed to learn the parameter distribution}, and \textit{how can the parameter distribution be learned?} We will start by examining the data requirements for different methods (a breakdown is provided in the accompanying table), followed by a discussion on approaches to learning the parameter distribution.

\paragraph{What data is needed to learn the parameter distribution?}
To bypass the need for manually tuning parameter distributions, data can serve as a substitute for domain expertise. This data can be gathered in four main ways: using only the simulation, using pre-collected real-world data, using real-world policy rollouts, or using human demonstrations.

\begin{enumerate}
    \item The first strategy involves generating data solely through simulation to learn the parameter distribution. This approach is advantageous as it requires no additional information beyond the default simulation environment settings. However, DR may achieve better results when supplemented with real-world or human demonstration data, as these sources are more likely to reflect the target domain.
    \item The second approach uses pre-collected real-world data, which closely aligns the parameter distribution with that of the target domain. However, this method depends on access to real-world data, which may be limited, or requires direct access to the physical system, which isn’t always feasible.
    \item If ongoing access to the real-world physical system is available, the third approach—using real-world policy rollouts to learn the parameter distribution—can be effective. The benefit here is the ability to update the parameter distribution dynamically during training. However, the drawback is the continuous need for physical system access, which may not always be possible.
    \item Lastly, human demonstrations can be utilized to learn the parameter distribution. Rather than fine-tuning simulation parameters, a single human demonstration can provide expert data to set the distribution for DR. The limitation is that some tasks do not allow for human demonstration, rendering this strategy inapplicable.
\end{enumerate}


\paragraph{How can the parameter distribution be learned?}
Various methods are available for learning the parameter distribution in DR, and the choice of method often depends on data availability. If extensive real-world expert data is accessible, it is generally preferred over methods relying solely on simulation data. The most widely adopted approach involves statistical methods, though other techniques—such as adversarial environment generation and gradually expanding the parameter distribution—are also utilized. We will first discuss the statistical methods followed by other approaches to learning the parameter distribution.

\begin{enumerate}
  \item Statistical methods are commonly used for learning the parameter distribution, including Bayesian optimization [BayRn], KL-divergence constraints [Closing Loop], Covariance Matrix Adaptation Evolution Strategy [DROID], likelihood-based approaches [DROPO, Soft Robots], Elastic Weight Consolidation [Continual DR], and a likelihood-free inference method [BayesSim]. For example, Bayesian optimization [BayRn] adapts the domain parameter distribution during training, allowing for efficient policy optimization with minimal prior domain knowledge. Another approach [Closing Loop] uses an iterative KL-divergence constraint to refine the simulation parameter distribution, improving policy transfer with minimal real-world rollouts. Similarly, [DROID] formulates domain randomization as a distribution optimization task, using Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and human demonstrations to improve policy transfer across tasks. Additionally, [DROPO] applies a likelihood-based approach to model parameter uncertainty using a limited offline dataset, enhancing sim-to-real transfer without extensive real-world interactions. [Soft Robots] builds on [DROPO] by retaining the likelihood-based approach while adapting it to partially observable environments. In a different approach, [Continual DR] combines domain randomization with continual learning, using Proximal Policy Optimization (PPO) [?] and Elastic Weight Consolidation (EWC) [?] to evaluate neural network parameters. Lastly, [BayesSim] offers a framework for handling simulation parameter uncertainty through likelihood-free inference, enhancing DR strategies by deriving posterior distributions from simulation outputs and real-world trajectories.
  \item Many other approaches exist for learning the parameter distribution which do not center on statistical methods. [MARL UAV] and [SSR Active DR] both use some form of generative learning to learn the parameter distribution. [MARL UAV] uses a nature player as the adversary to generate more difficult environments for the agents. In contrast [SSR Active DR] pits two agents Alice and Bob against each other to co-evolve a goal curriculum and generate a self-supervised reward to use for evolving the environment curriculum. This work builds upon [Active DR] which uses a reference environment (default simulation) and DR generated simulation instances to learn a discriminator that learns to generate the reward in place of the self-supervised piece above. In both [SSR Active DR] and [Active DR], the idea to grow the environment towards agent difficulty remains the same. Another work which builds an environment curriculum during learning is [Rubik’s Cube], which progressively increases randomization in the environment as the agent is able to meet desired performance thresholds. The general idea remains the same across these works, demonstrating impressive performance by focusing on shifting the DR distribution towards areas of difficulty for the agent. Instead of building a curriculum of environments, [Efficient S2R] adopts a hybrid approach. They accept the challenge of a potentially too narrow DR distribution, but overcome this challenge using domain adaptation with real-world rollouts after DR has been applied. This work demonstrates the effectiveness of combining domain randomization with another sim-to-real technique, showcasing the potential effectiveness of hybrid DR approaches. Lastly, [Cyclical PD] focuses on splitting the ranges of randomized parameters into sub-domains to learn local policies for each, finally distilling the local policies into a global policy for sim-to-real transfer. This work provides a way to utilize large randomization ranges for DR, ensuring that the parameter distribution encapsulates the target domain which improves sim-to-real transfer.
\end{enumerate}

\begin{table*}[t]
    \centering
    \begin{tabular}{|p{5cm}|c|c|c|c|}
        \hline
        \textbf{References} & \textbf{Data} & \textbf{Statistical Methods} & \textbf{Generative} & \textbf{Other} \\ \hline
        [Active DR] & Simulation only && \checkmark & \\ \hline
        [SSR Active DR **] & Simulation only && \checkmark & \\ \hline
        [Rubik’s Cube **] & Simulation only && \checkmark & \\ \hline
        [Efficient S2R **] & Simulation only & & & \checkmark \\ \hline
        [MARL UAV] & Simulation only & & \checkmark & \\ \hline
        [Cyclical PD] & Simulation only & \checkmark & & \\ \hline
        [Continual DR] & Simulation only & \checkmark & & \\ \hline
        [DROPO] & Real-world data (pre-collected) & \checkmark && \\ \hline
        [Soft Robots] & Real-world data (pre-collected) & \checkmark && \\ \hline
        [BayesSim] & Real-world data (pre-collected) & \checkmark & & \\ \hline
        [Data Efficient] & Real-world policy rollouts & \checkmark & & \\ \hline
        [Closing Loop] & Real-world policy rollouts & \checkmark && \\ \hline
        [DROID] & Human demonstration & \checkmark && \\ \hline
        [DROPO ** repeat can use human demo] & Human demonstration & & \checkmark & \\ \hline
    \end{tabular}
    \caption{Summary of data sources and methodological approaches for domain randomization (DR) references. Simulation-only methods rely exclusively on simulations. Real-world data methods use precollected real data. Real-world policy rollouts involve real-world policies, and human demonstration methods may utilize or require human demonstrations for DR. Statistical Methods, Generative, and Other columns indicate the use of statistical, generative, or other relevant techniques.}
\end{table*}
