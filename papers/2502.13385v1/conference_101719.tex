\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is % \usepackage{cite} % 如果不需要引用格式调整，可以注释掉
\let\algorithmic\relax
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic} % 注释掉这个包，避免与 algpseudocode 冲突
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{algorithm} % 这个用于设置算法环境
% \usepackage{algpseudocode} % 这个用于编写伪代码

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{SNN-Driven Multimodal Human Action Recognition via Event
Camera and Skeleton Data Fusion\\
\thanks{Identify applicable funding agency here. If none, delete this.}
}
\author{\IEEEauthorblockN{Naichuan Zheng\IEEEauthorrefmark{1}, Hailun Xia\IEEEauthorrefmark{1}\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Beijing University of Posts and Telecommunications, Beijing, China \\
Email: 2022110134zhengnaichuan@bupt.edu.cn}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Corresponding Author, Email: xiahailun@bupt.edu.cn}
}

\maketitle

\begin{abstract}
Multimodal human action recognition based on RGB and skeleton data fusion, while effective, is constrained by significant limitations such as high computational complexity, excessive memory consumption, and substantial energy demands, particularly when implemented with Artificial Neural Networks (ANNs). These limitations restrict its applicability in resource-constrained scenarios. To address these challenges, we propose a novel Spiking Neural Network (SNN)-driven framework for multimodal human action recognition, utilizing event camera and skeleton data. Our framework is centered on two key innovations: (1) a novel multimodal SNN architecture that employs distinct backbone networks for each modality—an SNN-based Mamba for event camera data and a Spiking Graph Convolutional Network  (SGN) for skeleton data—combined with a spiking semantic extraction module to capture deep semantic representations; and (2) a pioneering SNN-based discretized information bottleneck mechanism for modality fusion, which effectively balances the preservation of modality-specific semantics with efficient information compression. To validate our approach, we propose a novel method for constructing a multimodal dataset that integrates event camera and skeleton data, enabling comprehensive evaluation. Extensive experiments demonstrate that our method achieves superior performance in both recognition accuracy and energy efficiency, offering a promising solution for practical applications.


\end{abstract}

\begin{IEEEkeywords}
Multimodal Human Action Recognition, Spiking Neural Networks,Event Camera, Skeleton
\end{IEEEkeywords}

\section{Introduction}
Human action recognition is a key research direction in the fields of computer vision and artificial intelligence, with wide-ranging applications in intelligent surveillance, human-computer interaction, medical rehabilitation, and other scenarios. The research methods for human action recognition are diverse, primarily based on different types of modality data\cite{b1}. RGB data is one of the most commonly used modalities, and its methods rely on Convolutional Neural Networks (CNNs)\cite{b2}, 3D Convolutional Networks (C3D)\cite{b3}, and more advanced Transformer architectures\cite{b4}, which excel at extracting spatial and temporal features from video frames. However, RGB data is highly sensitive to environmental factors such as lighting and background, making it susceptible to external interference. In contrast, skeleton data effectively extracts geometric information about actions by capturing the positions and dynamic changes of key points in the human body, and it is more robust as it is not affected by environmental lighting and background\cite{b5}. In recent years, models based on Graph Convolutional Networks (GCNs)\cite{b6} and Transformers\cite{b7} have made significant progress in the analysis of skeletal data, as they can model dependencies between human key points and capture temporal features. However, the process of removing background information and human details from skeleton data can lead to information loss, particularly when describing complex actions and subtle limb movements, limiting its expressive power\cite{b8}.

Event cameras, with their advantages of high dynamic range, low latency, and low power consumption, have recently become a popular research direction for human action recognition. Unlike traditional frame-based cameras, event cameras capture only the changes in a scene, thereby reducing data redundancy and computational load, making them highly suitable for handling fast movements and high dynamic range scenarios\cite{b9,b10,b11,b12}. With the successful application of Transformer architectures in visual tasks, researchers have also started to use Transformers to process event data to better model spatiotemporal features\cite{b13}. However, event cameras can easily introduce noise in scenes with frequent changes in lighting conditions, affecting the accuracy of action recognition.

To overcome the limitations of single-modality methods, multimodal human action recognition has gradually become a research trend. Multimodal methods can combine the advantages of different modalities to provide more robust recognition results. Existing multimodal approaches mainly involve the fusion of RGB and skeleton data\cite{b14,b15}, or event data with RGB data and language\cite{b10}, allowing for complementary information between different modalities and improving the model's adaptability to environmental changes and complex actions. Currently, the mainstream methods for multimodal fusion mostly adopt two-stream network architectures, which separately extract and fuse the features of each modality to enhance overall recognition performance. Additionally, recent studies have introduced the CLIP (Contrastive Language–Image Pretraining) model\cite{b16,b17,b18}, leveraging textual modality as guidance through multimodal contrastive learning to further enhance recognition performance. However, these multimodal methods generally rely on deep neural network models, which have high computational complexity and consume a lot of power. Moreover, multimodal fusion further increases the computational and energy consumption burden. Furthermore, since these methods mostly utlize RGB modality as the primary information source, they result in high memory usage, especially when handling high-resolution videos that require large amounts of storage resources and computational power.

To address the energy consumption issue, Spiking Neural Networks (SNNs), as a biologically inspired neural network model, are considered a more energy-efficient alternative due to their asynchronous computation and sparse activation characteristics\cite{b20}. Unlike traditional deep neural networks (DNNs), SNNs transmit information through spikes and activate neurons only under specific conditions, thereby effectively reducing computation and energy consumption\cite{b21,b22,b23}. Therefore, SNNs have recently been applied to single-modality action recognition tasks, including RGB\cite{b20}, skeletal\cite{b24,b25}, and event data\cite{b13,b27}, and have achieved some progress. Especially in processing event data, the asynchronous and sparse characteristics of SNNs are well-suited for handling sparse event streams. However, despite their advantages in energy efficiency, SNNs often exhibit a decrease in processing accuracy across modalities. Compared to traditional deep models, their performance is somewhat limited in complex scenarios and when high precision is required.

With a balanced consideration of accuracy, energy efficiency, and memory usage, we propose a novel multimodal Spiking Neural Network (SNN) framework for multimodal human action recognition using event cameras and skeletal data. To this end, we designed two SNN-based and Mamba-architecture-based backbone networks for initial feature extraction from each modality to fully leverage the energy efficiency advantages of SNNs and the characteristics of different modalities. On this basis, we propose a dynamic perception spiking graph convolution and spiking global feature extraction module to construct an efficient semantic extraction module that can capture deep modality features. Moreover, to preserve critical modality-specific semantics in multimodal fusion, we introduce the information bottleneck theory to compress redundant information, achieving more effective feature fusion. Finally, to validate our approach, we use skeletal data to extract regions of interest (ROI) from RGB videos and convert these regions into event camera modality data using the V2e algorithm, constructing a new multimodal dataset tailored to the designed SNN network structure.
Our main contributions are as follows:
\begin{itemize}
\item  Mamba-based SNN Backbone Network and Semantic Extraction Module: We propose a Mamba-based SNN backbone network and a semantic extraction module to efficiently extract features from event camera and skeletal data.

\item Feature Fusion via Information Bottleneck Theory: We develop a feature fusion method based on information bottleneck theory to retain essential modality-specific semantics while compressing redundant information.

\item Dataset Construction Method: We create a new dataset by extracting regions of interest (ROI) from RGB videos using skeletal data and converting these to event camera data with the V2e algorithm.

\item Extensive Experiments: We perform comprehensive experiments to validate our framework's effectiveness in terms of accuracy, energy efficiency, and memory usage.
\end{itemize}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Event-Skeleton.jpg}  % Replace 'example-image' with your image filename
    \caption{Network Architecture.}
    \label{fig:wide}
\end{figure*}

\section{Rleated Work}

\subsection{Multimodal Human Action Recongnition}
Multimodal human action recognition has gained significant attention as it combines multiple data modalities to achieve more robust and accurate recognition results. Unlike single-modality methods, which rely solely on RGB\cite{b31}, skeletal\cite{b32,b33,b34}, or event data\cite{b27,b35,b36} and are limited by their respective weaknesses (e.g., sensitivity to lighting for RGB, loss of detail in skeletal data, or noise in event data), multimodal approaches can capture complementary information, enhancing the model's ability to understand complex actions in diverse environments. Existing multimodal methods typically involve the fusion of RGB and skeletal data or RGB and event data using two-stream networks to independently extract and then fuse features from different modalities. Additionally, recent approaches have leveraged contrastive learning models like CLIP to incorporate text as a guiding modality, providing richer contextual information. However, there is a noticeable gap in current research regarding the fusion of event and skeleton modalities, which could offer a unique balance of high dynamic range and geometric detail with reduced noise and environmental dependency.
\subsection{SNN}
Spiking Neural Networks (SNNs) are a type of biologically inspired neural network that mimics the spiking behavior of neurons in the brain. SNNs use different neuron models, such as the Leaky Integrate-and-Fire (LIF) and Hodgkin-Huxley models, to simulate the way biological neurons accumulate input until a threshold is reached, at which point a spike is generated. The LIF model, one of the most commonly used, can be described by the following differential equation:

\[
\tau \frac{dV(t)}{dt} = -V(t) + RI(t)
\]
where \(V(t)\) is the membrane potential at time \(t\), \(I(t)\) is the input current, \(R\) is the membrane resistance, and \(\tau\) is the membrane time constant. When \(V(t)\) reaches a certain threshold, the neuron "fires" a spike and the potential is reset. This mechanism allows for sparse and event-driven computation, which significantly reduces energy consumption compared to conventional deep neural networks (DNNs).

SNNs have been increasingly explored for human action recognition due to their energy efficiency, especially in applications involving resource-constrained environments like mobile devices and edge computing. Current SNN-based methods have been applied to single-modality action recognition tasks, such as RGB, skeletal, and event data, showing promise in reducing power consumption. However, their use in multimodal action recognition is still limited, and maintaining high accuracy remains a challenge due to the difficulty in effectively combining information from different modalities.

\subsection{Mamba}
Mamba is a selective structured state space model (SSM) designed for efficient long-sequence modeling, offering global receptive fields and dynamic weighting with linear complexity\cite{b39}. It combines SSMs with a selection mechanism where model parameters depend on input sequences, enabling adaptive information propagation:
\begin{equation}
h'(t) = Ah(t) + Bx(t), \quad y(t) = Ch(t),
\end{equation}
\begin{equation}
B, C, \Delta = \text{Linear}(x).
\end{equation}

Mamba has shown strong performance in vision tasks like classification and segmentation. However, its integration with Spiking Neural Networks (SNNs) and multimodal human action recognition remains underexplored, especially for event and skeleton data.

\section{Method}\label{SCM}
In this section, we provide a detailed description of our method. The network architecture begins with a simple spiking encoding mechanism as follows:
\begin{equation}
X_m = SN \{\mathrm{BN}\left[\operatorname{Conv}\left(M\right)\right]\},
\end{equation}

where \( M \) represents both modalities \( S \) and \( E \). Specifically, \( S \) is the original event modality data, and \(E \) is the skeleton modality data. In this equation, \(\operatorname{Conv}\) and \(\mathrm{BN}\) denote convolutional layers and batch normalization, which can be either 1D or 2D. The \( SN \) represents a spiking neuron used for encoding.
\subsubsection{Skeleton-Backbone}

\subsubsection{Event-Backbone}
To encoded the event data into spiking-form, we follow the spiking patch splitting module(SPS) proposed in the classic Spikformer. Given the input \( X_e \in \mathbb{R}^{T \times C \times H \times W} \), the process is as follows:
\[
x_e = \operatorname{MP} \left( SN \left( \mathrm{BN} \left( \operatorname{Conv2d}(X_e) \right) \right) \right),
\]
where \(\operatorname{Conv2d}\) represents a 2D convolutional layer with a stride of 1 and a kernel size of \(3 \times 3\), and \(\operatorname{MP}\) represents max-pooling. This process results in an event patches sequence \( x_e \in \mathbb{R}^{T \times D \times N} \), where the \( N \) dimension is aligned with the skeletal point dimension.

\begin{algorithm}
\caption{Integrate-and-Fire Neuron Model with Selective State Space Model (SSM)}
\label{alg:IF_SSM_model}
\begin{algorithmic}[1]
\REQUIRE Input current $X_e \in \mathbb{R}^{T \times D \times N}$
\ENSURE Spike output $Y_e \in \mathbb{R}^{T \times D \times N}$
\STATE Initialize membrane potential $V(t) \leftarrow V_{\text{rest}}$  
\STATE Set threshold potential $V_{\text{th}}$, reset potential $V_{\text{reset}}$
\STATE Set parameters:
\STATE \hspace{1em} Membrane time constant $\tau_m$, Membrane resistance $R$, and Time step $\Delta t$
\STATE Initialize SSM hidden state $H(t, l) \leftarrow 0$  
\STATE Set SSM parameters: $\tilde{A}_{t, l}$, $\tilde{B}_{t, l}$, $\tilde{C}_{t, l}$
\FOR{each time step $t$}
    \STATE Update membrane potential:
    \STATE $V(t + \Delta t) \leftarrow V(t) + \frac{\Delta t}{\tau_m} \left( - (V(t) - V_{\text{rest}}) + R \cdot X(t, l) \right)$
    \FOR{each sequence step $l$ from $1$ to $L$}
        \STATE Update SSM hidden state and output:
        \STATE $H(t, l) \leftarrow \tilde{A}_{t, l} H(t, l-1) + \tilde{B}_{t, l} X(t, l)$
        \STATE $Y(t, l) \leftarrow \tilde{C}_{t, l} H(t, l)$
    \ENDFOR
    \IF{$V(t + \Delta t) \geq V_{\text{th}}$}
        \STATE $Y_e(t, l) \leftarrow 1$
        \STATE $V(t + \Delta t) \leftarrow V_{\text{reset}}$  
    \ELSE
        \STATE $Y_e(t, l) \leftarrow 0$ 
    \ENDIF
    \STATE Update time step $t \leftarrow t + \Delta t$
\ENDFOR
\RETURN Spike output $Y_e(t, l)$ \\
final membrane potential $V(t)$
\end{algorithmic}
\end{algorithm}
After the SPS, the event patches are fed into the spiking-form Mamba model for further processing. Specifically, the input event data \(X_s \in \mathbb{R}^{T \times D \times N}\) undergoes a sequence of transformations to adapt it into a spiking representation suitable for neural computations. The transformation is formally defined as:
\[
B ,C ,\Delta = \text{SN}\{\text{BN}[\text{LN}(X_e)]\}
\]
where \(\text{LN}(\cdot)\) denotes layer normalization.
Following this spiking transformation in the Mamba model, the generated spiking signals are then input into the Selective State Space Model (SSM). Within the SSM framework, the hidden state \(H(t, l)\) is initialized, and the SSM parameters \(\tilde{A}_{t, l}\), \(\tilde{B}_{t, l}\), and \(\tilde{C}_{t, l}\) are set. At each time step \(t\), for each sequence step \(l\), the hidden state \(H(t, l)\) is updated, and the output \(Y(t, l)\) is computed as follows:

\[
H(t, l) \leftarrow \tilde{A}_{t, l} H(t, l-1) + \tilde{B}_{t, l} X_b(t, l)
\]
\[
Y_e(t, l) \leftarrow \tilde{C}_{t, l} H(t, l)
\]

Based on the state of the membrane potential \(V(t)\), the spiking output \(Y_e(t, l)\) is determined, and the membrane potential is reset accordingly to regulate spiking behavior.

Through this sequence of operations, the Mamba model and the SSM work in tandem to convert the input event patches sequence into effective spiking sequence outputs, enabling efficient processing of complex temporal data. This hierarchical approach leverages both spiking dynamics and state space modeling to capture and process event-based information in a structured manner.

\subsubsection{Semantic Learning}
We propose a unified spiking semantic learning module to perform deeper semantic extraction on spiking-form skeletal and event features. The core of this module is to construct a sparse dynamic perception graph that better captures the local relationships among features.

Specifically, given the input features \( Y_m \in \mathbb{R}^{T \times D \times N} \), we first perform pairwise distance calculation to construct a distance matrix \( D \in \mathbb{R}^{T\times N \times N} \) in the feature space, which is calculated as follows:
\[
D = \|Y_m^T\|^2 + \|Y_m\|^2 - 2 Y_m^T Y,
\]
where \( Y_m^T Y_m \) represents the inner product of the node features, \( \|Y_m\|^2 = \text{diag}(Y_m^T Y_m) \) represents the squared sum of the node features,\( \|Y_m^T\|^2 = {\|Y_m\|^2}^T \) is the transpose of the squared sum of the node features.

Then based on the distance matrix \( D \), we find the \( k \) nearest neighbors for each node. The neighbor index matrix \( \text{Idx} \in \mathbb{R}^{T \times N \times k} \) is computed as:
\[
\text{Idx} = \text{topk}(-D, k, \text{dim}=-1)[1],
\]
where the \(\text{topk}\) function is used to select the top \( k \) smallest distances from the distance matrix \( D \), resulting in an index matrix of the \( k \) nearest neighbors for each node.
Using these neighbor indices, we construct the Dynamic Graph. This graph is represented as an edge index tensor \( E \in \mathbb{R}^{T \times 2 \times E_{\max}} \), where \( E_{\max} = N \times k \) is the maximum number of edges for each graph. The edge index tensor \( E \) is constructed as follows:
\[
E[i, 0, j:j+k] = n, \quad E[i, 1, j:j+k] = \text{Idx}[i, n, :],
\]
where \( i \) denotes the graph index in the batch, \( n \) denotes the node, and \( j \) is used to store \( k \) edges for each node in tensor \( E \). We build the semantic adjacency matrix \( A_{se} \in \mathbb{R}^{N \times N} \) by setting \( A_{se}[i, u, v] = 1 \) if there is an edge from node \( u \) to node \( v \) in graph \( i \). Then, we compute the updated node features as the following:

\[
Y^{m}_{se} = {D_{se}}^{-1/2} A_{se} {D_{se}}^{-1/2} Y_m W,
\]
\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\textwidth]{Semantic-learning.jpg}  % Replace 'example-image' with your image filename
    \caption{Semantic Learning}
    \label{fig:wide}
\end{figure}
where \( D_{se} \) is the degree matrix with elements \( {D_{se}}_{ii} = \sum_j {A_{se}}_{ij} \), \( {D_s}^{-1/2} \) is its inverse square root, \(  Y_m  \in \mathbb{R}^{N \times D} \) is the input node feature matrix, \( W \in \mathbb{R}^{D \times D'} \) is the learnable weight matrix.  We then apply spiking neuron activation, to incorporate temporal dynamics:
\[
 {Y^m_{se}}' = SN[BN(Y^{m}_{se})]
\]
The model captures both local and global semantic information in spiking-form feature. After that, we apply globals spiking attention model to get spiking global semantic.
We begin by detailing the process of applying height and width attention through pooling operations, We reshape \( Y'_{se}\) to separate the height and width dimensions as \(
  {Y^m_{se}}' \in \mathbb{R}^{T \times D \times H \times W}.
\)
 We perform adaptive average pooling along the width and height dimension, reducing the width to size 1:
  \[
  y^m_h = \text{Pool}_{\text{width}}( {Y^m_{se}}') \in \mathbb{R}^{T \cdot  \times D \times H \times 1}.
  \]
  \[
  y^m_w = \text{Pool}_{\text{height}}( {Y^m_{se}}') \in \mathbb{R}^{T \times D \times 1 \times W}.
  \]
We concatenate \( y^m_h \) and \( y^m_w \) along the spatial dimension:
\[
y^m = \text{Concat}(y^m_h, \, y^m_w, \, \text{dim}=2) \in \mathbb{R}^{T \times C \times (H + W) \times 1}.
\]
To enhance feature representation, we leverage the feature maps' height and width components. 
Initially, a convolutional layer with a kernel size of 1 (\(\text{Conv}_1\)) is applied to the input feature map \(y\). This is followed by batch normalization (\(\text{BN}_1\)) to stabilize the feature distribution. The resulting output is denoted as \(y'\):

\[
{y^m}' = SN\{\text{BN}_1\left( \text{Conv}_1(y^m) \right)\}.
\]
\
To capture spatial dependencies, we split \({y^m}' \) back into its height and width components, represented by \(a^m_h\) and \(a^m_w\) respectively. These components are then permuted to align with their original orientations:
\[
\begin{aligned}
a^m_h & = {y^m}'[:, \, :, \, :H, \, :] \in \mathbb{R}^{T \times D \times H \times 1}, \\
a^m_w & = {y^m}'[:, \, :, \, H:, \, :] \in \mathbb{R}^{T \times D \times W \times 1}, 
\end{aligned}
\]
Learnable scale (\(\gamma\)) and bias (\(\beta\)) parameters are introduced to further refine these spatial components:

\begin{align}
{a^{m}_{h}}' &= \gamma_h \cdot a_h + \beta_h, \\
{a^{m}_{w'}}'&= \gamma_w \cdot a_w + \beta_w.
\end{align}

where \(\gamma_h, \beta_h, \gamma_w, \beta_w \in \mathbb{R}^{1 \times C \times 1 \times 1}\) are learnable parameters optimized during training.

To generate the attention maps, we apply a series of convolutions followed by batch normalization and spiking activation functions to ${a^{m}_{h}}'$ and ${a^{m}_{w'}}'$. These operations yield height-based (\(\text{Attention}^m_h\)) and width-based (\(\text{Attention}^m_w\)) attention maps:

\[
\begin{aligned}
\text{Attention}^m_h & =  SN\{\text{BN}_h\left( \text{Conv}_h({a^m_h}') \right)\} \in \mathbb{R}^{T \times C \times H \times 1}, \\
\text{Attention}^m_h & =  SN\{\text{BN}_h\left( \text{Conv}_h({a^m_h}') \right)\} \in \mathbb{R}^{T  \times C \times 1 \times W}.
\end{aligned}
\]
Finally, we enhance the original features with both attention maps, effectively capturing the salient information in both the height and width dimensions:
\[
y^m_{wh} ={y^m}'\times \text{Attention}^m_h \times \text{Attention}^m_w.
\]
To capture dependencies in channel and spatial dimensions, we process the spiking-form feature in groups. We reshape \( y_{wh} \) for group processing:
\[
y^m_{\text{grouped}} =y^m_{wh} .\text{view}(T \cdot G, \, \tfrac{C}{G}, \, H, \, W),
\]
where \( G \) is the number of groups.We split \( y^m_{\text{grouped}} \) into two equal parts along the channel dimension:
\[
\begin{aligned}
y^m_{\text{channel}} & = y^m_{\text{grouped}} [:, \, :\tfrac{C}{2G}, \, :, \, :], \\
y^m_{\text{spatial}} & = y^m_{\text{grouped}}[:, \, \tfrac{C}{2G}:, \, :, \, :].
\end{aligned}
\]
We focus on enhancing important channels.We apply adaptive average pooling to each channel:
  \[
  s^m_c = \text{Pool}(y^m_{\text{channel}}) \in \mathbb{R}^{T \cdot G \times \tfrac{C}{2G} \times 1 \times 1}.
  \]
 We apply learnable scale (\( \gamma_c \)) and bias (\( \beta_c \)) parameters:
  \[
  {s^m_c}' = \gamma_c \cdot s^m_c + \beta_c.
  \]
  \[
  {s^m_c}'' = SN\{\text{BN}_c\left( \text{Conv}_c\left( {s^m_c}' \right) \right)\}.
  \]
  Multiply with the channel features:
  \[
  {y^m_{\text{channel}}}' = y^m_{\text{channel}} \times {s^m_c}''.
  \]
We focus on enhancing important spatial locations and apply a convolutional layer (\( \text{Conv}_s \)) and group normalization (\( \text{GN} \)):
  \[
  s^m_s = \text{GN}\left( \text{Conv}_s\left( y^m_{\text{spatial}} \right) \right).
  \]
Apply learnable scale (\( \gamma_s \)) and bias (\( \beta_s \)) parameters:
  \[
  {s^m_s}' = SN(\gamma_s^m \cdot s_s + \beta_s).
  \]
 Multiply with the spatial features:
  \[
  {y^m_{\text{spatial}}}' = y^m_{\text{spatial}} \times s_s'.
  \]
We concatenate the refined channel and spatial features along the channel dimension:
\[
y^m_{\text{out}} = \text{Concat}\left( {y^m_{\text{channel}}}', \, {y^m_{\text{spatial}}}', \, \text{dim}=1 \right).
\]
We reshape it to match the original dimensions. Finally, we add a residual connection with the initial input to preserve the original information:
\[
Y_{sM} = y^m_{\text{out}} + y^m_{wh}.
\]
\subsubsection{Feature Fusion through Information Bottleneck theory}
To effectively fuse the semantics of the event spiking-form \(Y_{sE}\) and the skeleton spiking-form \(Y_{sS}\), we leverage the Information Bottleneck (IB) theory. The IB principle seeks to find a compressed representation that captures the most relevant information from the input while discarding irrelevant details. In our case, this involves designing a model that compresses the input spiking-form semantics \(Y_{sE}\) and \(Y_{sS}\) into a fused latent representation while retaining the essential semantics.

The goal is to find a latent representation \(B_0\) that captures relevant information from both \(Y_{sE}\) and \(Y_{sS}\), but primarily retains the semantics of \(Y_{sE}\). This is achieved by maximizing the mutual information between \(B_0\) and the event spiking-form \(Y_{sE}\), while minimizing the mutual information between \(B_0\) and the input spiking-forms \(Y_{sE}\) and \(Y_{sS}\). The mutual information between \(Y_{sE}\) and \(B_0\) is given by:

\[
I(Y_{sE}; B_0) = H(Y_{sE}) - H(Y_{sE} | B_0)
\]

where \(I(Y_{sE}; B_0)\) denotes the mutual information between the event spiking-form \(Y_{sE}\) and the latent representation \(B_0\), \(H(Y_{sE})\) is the entropy of \(Y_{sE}\), and \(H(Y_{sE} | B_0)\) is the conditional entropy given \(B_0\).

The objective of the Information Bottleneck method is to find a compressed representation \(B_0\) such that the mutual information between \(B_0\) and the input spiking-forms \(Y_{sE}\) and \(Y_{sS}\) is minimized, while the mutual information between \(B_0\) and the target spiking-form \(Y_{sE}\) is maximized. This optimization problem is formulated as:

\[
\min_{q(B_0|Y_{sE}, Y_{sS})} I(Y_{sE}, Y_{sS}; B_0) - \beta I(B_0; Y_{sE})
\]

where \(I(Y_{sE}, Y_{sS}; B_0)\) represents the mutual information between \(B_0\) and the inputs \(Y_{sE}, Y_{sS}\), and \(\beta\) is a balancing parameter that controls the trade-off between these terms.

Direct computation of the mutual information terms is complex. Therefore, we apply variational approximations to estimate these terms. Let \(p(B_0)\) denote the prior distribution of \(B_0\), \(q(B_0|Y_{sE}, Y_{sS})\) represent the conditional distribution of \(B_0\) given \(Y_{sE}\) and \(Y_{sS}\), and \(q(Y_{sE}|B_0)\) be the reconstruction distribution. The mutual information \(I(Y_{sE}, Y_{sS}; B_0)\) can be approximated using the Kullback-Leibler (KL) divergence:

\[
I(Y_{sS}, Y_{sE}; B_0) \approx \mathbb{E}_{p(Y_{sS}, Y_{sE})} [D_{\text{KL}}(q(B_0|Y_{sS}, Y_{sE}) \| p(B_0))]
\]

where \(D_{\text{KL}}\) denotes the KL divergence between the posterior distribution \(q(B_0|Y_{sE}, Y_{sS})\) and the prior distribution \(p(B_0)\).

Similarly, the mutual information \(I(B_0; Y_{sE})\) can be approximated as:

\begin{align*}
I(B_0; Y_{sE}) \approx &\; \mathbb{E}_{p(Y_{sE})} \left[ \mathbb{E}_{q(B_0|Y_{sE}, Y_{sS})} [\log q(Y_{sE}|B_0)] - \right. \\
& \left. H(Y_{sE}) \right]
\end{align*}


where \(H(Y_{sE})\) is the entropy of \(Y_{sE}\), which is a constant and can be ignored during optimization.

Based on these approximations, the loss function for the first layer of the IB is defined as:
\begin{align*}
L_{IB_0} \approx & D_{\text{KL}}(q(B_0|Y_{sE}, Y_{sS}) \| p(B_0)) + \\
& \beta \mathbb{E}_{B_0 \sim q(B_0|Y_{sE}, Y_{sS})} [\|Y_{sE} - f(B_0)\|^2]    
\end{align*}
where \(f(B_0)\) is an MLP decoder that reconstructs \(Y_{sE}\) from the latent representation \(B_0\), and the reconstruction loss is approximated using the Mean Squared Error (MSE).

To further refine the representation while retaining the specific semantics of the skeleton spiking-form \(Y_{sS}\), we introduce a second layer of the Information Bottleneck. This layer encodes the latent representation \(B_0\) into a higher-level latent representation \(B_1\). The loss function for the second layer is given by:

\begin{align*}
L_{IB_1} \approx & D_{\text{KL}}(q(B_1 | B_0) \| p(B_1)) + \\
&\gamma \cdot \mathbb{E}_{B_1 \sim q(B_1 | B_0)} [\|Y_{sS} - g(B_1)\|^2]
\end{align*}

where \(q(B_1 | B_0)\) is the conditional distribution produced by the second encoder, which is obtained by further encoding \(B_0\); \(p(B_1)\) is the prior distribution, typically chosen as a standard normal distribution \(N(0, 1)\); \(g(B_1)\) represents the MLP2 decoder that reconstructs the skeleton spiking-form semantic \(Y_{sS}\) from \(B_1\); and \(\gamma\) is a balancing parameter.

Combining the information bottleneck losses from both layers, the total loss function is approximated as:

\[
L_{\text{IB\_total}} \approx L_{IB_0} + \lambda \cdot L_{IB_1}
\]

where \(\lambda\) is a balancing parameter that determines the relative importance between the first and second layers of the IB losses.

This hierarchical Information Bottleneck approach enables the Mamba encoder to efficiently compress the input spiking-form semantics while preserving essential information specific to each modality, thereby facilitating more effective multimodal representation learning. The proposed model leverages this framework to integrate both the event spiking-form \(Y_{sE}\) and the skeleton spiking-form \(Y_{sS}\), achieving a robust and semantically meaningful fused representation.

\section{Dataset}
The NTU RGB+D dataset is one of the most widely used datasets for human action recognition. It was collected using Microsoft Kinect v2 and includes 56,880 video samples of 60 different human actions performed by 40 distinct subjects. These actions are captured from three different camera views, encompassing a wide range of daily, health-related, and mutual (interaction) actions. The dataset provides four different types of data modalities: RGB videos, depth map sequences, 3D skeleton data, and infrared (IR) frames, making it ideal for evaluating multimodal action recognition algorithms\cite{b40}.
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Event-Skeleton-Dataset.jpg}  % Replace 'example-image' with your image filename
    \caption{Construction of multimodal datasets}
    \label{fig:wide}
\end{figure*}
% The NTU RGB+D 120 dataset is an extended version of the original NTU RGB+D dataset, making it the largest dataset for 3D human activity understanding as of its release. It contains 120 action classes and 114,480 video samples, recorded with three different cameras placed at various angles. The dataset captures actions performed by 106 distinct subjects across different environments. Like its predecessor, NTU RGB+D 120 includes multiple data modalities (RGB, depth, skeleton, and IR) and covers a wide range of action categories, including daily activities, medical conditions, and more complex interactions, providing a comprehensive benchmark for advanced action recognition methods.

% The NW-UCLA Multiview Action 3D dataset is another important dataset for human action recognition, specifically designed to evaluate models under multiview settings. Collected using three Kinect v1 cameras from different viewpoints, the dataset includes 1,475 samples from 10 different action classes, performed by 10 subjects. The actions involve daily activities such as drinking, picking up, and walking. The unique multiview setup allows researchers to explore the challenges of view-invariant action recognition and develop robust algorithms capable of handling varied viewpoints and camera perspectives.

To construct a multimodal dataset combining event camera data with skeleton data, we use the NTU RGB+D dataset as a foundational example to illustrate the process. The NTU RGB+D dataset provides RGB frames at a resolution of \(1920 \times 1080\) pixels, from which we derive the Region of Interest (ROI) for generating corresponding event stream data. The process begins by extracting the 2D skeleton coordinates from the RGB frames, specifically the color coordinates \((\text{colorX}, \text{colorY})\) of each joint for every frame in the action sequence. These coordinates are used to determine the bounding region of the subject in motion. To ensure that the entire subject and their movements are captured within the ROI, we first compute the minimum and maximum values of these coordinates across all frames:
\[\text{minX} = \min(\text{colorX}_i), \text{maxX} = \max(\text{colorX}_i),\] 
\[ \text{minY} = \min(\text{colorY}_i),\text{maxY} = \max(\text{colorY}_i).\]
where \(\text{colorX}_i\) and \(\text{colorY}_i\) represent the x and y coordinates of the skeleton joints in frame \(i\). We then expand the bounding box dimensions by a factor of 30\% for height and 20\% for width to accommodate the entire range of motion:
\[\text{ROI}_w = 1.2 \times (\text{maxX} - \text{minX}), \text{ROI}_h = 1.3 \times (\text{maxY} - \text{minY}).\]
Using these expanded dimensions, we extract the ROI from the original RGB frames for further processing.
With the ROI established, we apply the Video-to-Event (V2E) conversion technique to generate event stream data. The V2E process simulates an event camera by converting RGB video data into an event-based representation that captures changes in brightness asynchronously across each pixel. The key parameters for V2E are set to optimize the quality and relevance of the event data. The positive and negative brightness change thresholds, \(\text{pos\_thres}\) and \(\text{neg\_thres}\), are both set to 0.2:
\[
\text{pos\_thres} = 0.2, \quad \text{neg\_thres} = 0.2.
\]
The noise control threshold, \(\text{sigma\_thres}\), is set to 0.03 to reduce noise in the generated event data:
\[
\text{sigma\_thres} = 0.03.
\]
The output resolution of the event frames is configured to be \(\text{output\_height} = 512\) and \(\text{output\_width} = 320\), providing a balance between spatial resolution and computational efficiency. The temporal duration for the V2E conversion, \(\text{stop\_time}\), is dynamically set to match the duration of the original video segment being processed. The low-pass filter cutoff frequency, \(\text{cutoff\_hz}\), is set to 10 Hz to filter out high-frequency noise:
\[
\text{cutoff\_hz} = 10.
\]
The generated event stream data, together with the extracted 2D and 3D skeleton data, forms a comprehensive multimodal dataset suitable for advanced human action recognition tasks. This combination enables the capture of both fine-grained motion dynamics from the event data and detailed pose information from the skeleton data, enhancing the robustness and accuracy of models, particularly in scenarios involving complex motions, challenging lighting conditions, or requiring high temporal resolution.
\section{Experiment}
The SKMamba model achieves the highest accuracy (81.7\%) among all models listed in the table, demonstrating superior performance in action recognition tasks. This success can be attributed to its multimodal fusion approach, combining Event and Skeleton modalities, which provides richer information than single-modality models. Compared to other SNN models, which individually use either Skeleton or Event modalities, SKMamba’s fused approach enables it to capture a broader range of features, leading to higher accuracy. Additionally, SKMamba outperforms the traditonal ANN model (ST-GCN, 81.5\%), showcasing the advantages of leveraging SNN architecture combined with multimodal information. This result underscores the potential of SNNs with multimodal fusion in dynamic action recognition, highlighting SKMamba's innovation in efficiently modeling complex behavior patterns.
\begin{table}[]
\begin{tabular}{ll|l|l}
\hline
\multicolumn{2}{l|}{Method}                               & Modal                   & Acc(\%)        \\ \hline
\multicolumn{1}{l|}{}          & Part-aware LSTM\cite{b41}          & Joint                   & 62.93         \\
\multicolumn{1}{l|}{}          & Spatio-Temporal LSTM\cite{b42}& Joint                   & 61.70         \\
\multicolumn{1}{l|}{ANN}       & ST-GCN\cite{b43}                   & Joint                   & 81.5          \\ \hline
\multicolumn{1}{l|}{}          & Spikformer\cite{b21}               & Joint               & 73.9          \\
\multicolumn{1}{l|}{SNN}       & Spike-driven Transformer\cite{b22} & Joint               & 73.4          \\
\multicolumn{1}{l|}{}          & MK-SGN\cite{b25}                   & Joint                & 78.5          \\
\multicolumn{1}{l|}{}          & Signal-SGN\cite{b24}               & Joint                & 80.5          \\
\multicolumn{1}{l|}{}          & Spikformer\cite{b21}               & Event                   & 76.4          \\
\multicolumn{1}{l|}{\textbf{}} & \textbf{SKMamba(Ours)}   & \textbf{Event+Joint} & \textbf{84.7} \\ \hline
\end{tabular}
\end{table}
\begin{thebibliography}{00}
\bibitem{b1}Sun, Z., Ke, Q., Rahmani, H., Bennamoun, M., Wang, G., \& Liu, J. (2022). Human action recognition from various data modalities: A review. IEEE transactions on pattern analysis and machine intelligence, 45(3), 3200-3225.
\bibitem{b2} Shaikh, M. B., \& Chai, D. (2021). RGB-D data-based action recognition: a review. Sensors, 21(12), 4246.
\bibitem{b3} Jiang, S., Qi, Y., Zhang, H., Bai, Z., Lu, X., \& Wang, P. (2020). D3d: Dual 3-d convolutional network for real-time action recognition. IEEE Transactions on Industrial Informatics, 17(7), 4584-4593.
\bibitem{b4} Ahn, D., Kim, S., Hong, H., \& Ko, B. C. (2023). Star-transformer: a spatio-temporal cross attention transformer for human action recognition. In Proceedings of the IEEE/CVF winter conference on applications of computer vision (pp. 3330-3339).
\bibitem{b5} Yao, G., Lei, T., \& Zhong, J. (2019). A review of convolutional-neural-network-based action recognition. Pattern Recognition Letters, 118, 14-22.
\bibitem{b6} Ren, B., Liu, M., Ding, R., \& Liu, H. (2024). A survey on 3d skeleton-based action recognition using learning method. Cyborg and Bionic Systems, 5, 0100.
\bibitem{b7} Ahmad, T., Jin, L., Zhang, X., Lai, S., Tang, G., \& Lin, L. (2021). Graph convolutional neural network for human action recognition: A comprehensive survey. IEEE Transactions on Artificial Intelligence, 2(2), 128-145.
\bibitem{b8}Xin, W., Liu, R., Liu, Y., Chen, Y., Yu, W., \& Miao, Q. (2023). Transformer for skeleton-based action recognition: A review of recent advances. Neurocomputing, 537, 164-186.
\bibitem{b9}Gao, Y., Lu, J., Li, S., Ma, N., Du, S., Li, Y., \& Dai, Q. (2023). Action recognition and benchmark using event cameras. IEEE Transactions on Pattern Analysis and Machine Intelligence.
\bibitem{b10}Zhou, J., Zheng, X., Lyu, Y., \& Wang, L. (2024). ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 18633-18643).
\bibitem{b11}Ren, H., Zhou, Y., Huang, Y., Fu, H., Lin, X., Song, J., \& Cheng, B. (2023). Spikepoint: An efficient point-based spiking neural network for event cameras action recognition. arXiv preprint arXiv:2310.07189.
\bibitem{b12}Ren, H., Zhou, Y., Fu, H., Huang, Y., Xu, R., \& Cheng, B. (2023, October). Ttpoint: A tensorized point cloud network for lightweight action recognition with event cameras. In Proceedings of the 31st ACM International Conference on Multimedia (pp. 8026-8034).
\bibitem{b13}de Blegiers, T., Dave, I. R., Yousaf, A., \& Shah, M. (2023, October). EventTransAct: A video transformer-based framework for Event-camera based action recognition. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 1-7). IEEE.
\bibitem{b14}Li, J., Xie, X., Pan, Q., Cao, Y., Zhao, Z., \& Shi, G. (2020). SGM-Net: Skeleton-guided multimodal network for action recognition. Pattern Recognition, 104, 107356.
\bibitem{b15}Bruce, X. B., Liu, Y., Zhang, X., Zhong, S. H., \& Chan, K. C. (2022). Mmnet: A model-based multimodal network for human action recognition in rgb-d videos. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 3522-3538.
\bibitem{b16}Qiu, H.,\& Hou, B. (2024). Multi-grained clip focus for skeleton-based action recognition. Pattern Recognition, 148, 110188.
\bibitem{b17}Wang, Q., Du, J., Yan, K., \& Ding, S. (2023, October). Seeing in flowing: Adapting clip for action recognition with motion prompts learning. In Proceedings of the 31st ACM International Conference on Multimedia (pp. 5339-5347).
\bibitem{b18}Ke, Q., Bennamoun, M., An, S., Sohel, F., \& Boussaid, F. (2018). Learning clip representations for skeleton-based 3D action recognition. IEEE Transactions on Image Processing, 27(6), 2842-2855.
\bibitem{b19}Dampfhoffer, M., Mesquida, T., Valentian, A., \& Anghel, L. (2023). Backpropagation-based learning techniques for deep spiking neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems.
\bibitem{b20}Miki, D., Kamitsuma, K., \& Matsunaga, T. (2023). Spike representation of depth image sequences and its application to hand gesture recognition with spiking neural network. Signal, Image and Video Processing, 17(7), 3505-3513.
\bibitem{b21}Zhou, Z., Zhu, Y., He, C., Wang, Y., Yan, S., Tian, Y., \& Yuan, L. (2022). Spikformer: When spiking neural network meets transformer. arXiv preprint arXiv:2209.15425.
\bibitem{b22}Yao, M., Hu, J., Zhou, Z., Yuan, L., Tian, Y., Xu, B., \& Li, G. (2024). Spike-driven transformer. Advances in neural information processing systems, 36.
\bibitem{b23}Yao, M., Hu, J., Hu, T., Xu, Y., Zhou, Z., Tian, Y., ... \& Li, G. (2024). Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips. arXiv preprint arXiv:2404.03663.
\bibitem{b24}Zheng, N., Xia, H., \& Liu, D. (2024). Signal-SGN: A Spiking Graph Convolutional Network for Skeletal Action Recognition via Learning Temporal-Frequency Dynamics. arXiv preprint arXiv:2408.01701.
\bibitem{b25}Zheng, N., Xia, H., \& Liang, Z. (2024). MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and Knowledge Distillation for Skeleton-based Action Recognition. arXiv preprint arXiv:2404.10210.
\bibitem{b26}Hu, Y., Liu, S. C., \& Delbruck, T. (2021). v2e: From video frames to realistic DVS events. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 1312-1321).
\bibitem{b27}Ren, H., Zhou, Y., Huang, Y., Fu, H., Lin, X., Song, J., \& Cheng, B. (2023). Spikepoint: An efficient point-based spiking neural network for event cameras action recognition. arXiv preprint arXiv:2310.07189.
\bibitem{b28} Shabaninia, E., Nezamabadi-pour, H., \& Shafizadegan, F. (2024). Multimodal action recognition: a comprehensive survey on temporal modeling. 
\bibitem{b29} Multimedia Tools and Applications, 83(20), 59439-59489.
Ren, Z., Zhang, Q., Gao, X., Hao, P., \& Cheng, J. (2021). Multi-modality learning for human action recognition. Multimedia Tools and Applications, 80(11), 16185-16203.
\bibitem{b30} Duhme, M., Memmesheimer, R.,\& Paulus, D. (2021, September). Fusion-gcn: Multimodal action recognition using graph convolutional networks. In DAGM German conference on pattern recognition (pp. 265-281). Cham: Springer International Publishing.
\bibitem{b31} Shaikh, M. B., \& Chai, D. (2021). RGB-D data-based action recognition: a review. Sensors, 21(12), 4246.
\bibitem{b32} Yan, S., Xiong, Y., \& Lin, D. (2018, April). Spatial temporal graph convolutional networks for skeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1).
\bibitem{b33} Cheng, K., Zhang, Y., He, X., Chen, W., Cheng, J., \& Lu, H. (2020). Skeleton-based action recognition with shift graph convolutional network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 183-192).
\bibitem{b34} Zhang, X., Xu, C., \& Tao, D. (2020). Context aware graph convolution for skeleton-based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 14333-14342).
\bibitem{b35} Vicente-Sola, A., Manna, D. L., Kirkland, P., Di Caterina, G., \& Bihl, T. J. (2024). Spiking Neural Networks for event-based action recognition: A new task to understand their advantage. Neurocomputing, 128657.
\bibitem{b36}Deng, Y., Chen, H., \& Li, Y. (2024, March). A Dynamic GCN with Cross-Representation Distillation for Event-Based Learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 38, No. 2, pp. 1492-1500).
\bibitem{b37}Duan, H., Zhao, Y., Chen, K., Lin, D., \& Dai, B. (2022). Revisiting skeleton-based action recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 2969-2978).
\bibitem{b38}Li, J., Xie, X., Pan, Q., Cao, Y., Zhao, Z., \& Shi, G. (2020). SGM-Net: Skeleton-guided multimodal network for action recognition. Pattern Recognition, 104, 107356.
\bibitem{b39} Gu, A., \& Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752.
\bibitem{b40} Shahroudy, A., Liu, J., Ng, T. T., \& Wang, G. (2016). Ntu rgb+ d: A large scale dataset for 3d human activity analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1010-1019).
\bibitem{b41} Du, Y., Wang, W., \& Wang, L. (2015). Hierarchical recurrent neural network for skeleton based action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 1110-1118).
\bibitem{b42} Liu, J., Shahroudy, A., Xu, D., \& Wang, G. (2016). Spatio-temporal lstm with trust gates for 3d human action recognition. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 (pp. 816-833). Springer International Publishing.
\bibitem{b43} Yan, S., Xiong, Y., \& Lin, D. (2018, April). Spatial temporal graph convolutional networks for skeleton-based action recognition. In Proceedings of the AAAI conference on artificial intelligence (Vol. 32, No. 1).
\end{thebibliography}
\end{document}
