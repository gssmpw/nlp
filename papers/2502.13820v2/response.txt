\section{Related Works}
\label{sec:related_works}

Prior work primarily validates self-generated test cases within isolated systems or limited studies. Liang, "Few-Shot Adversarial Learning for Visual Question Answering" conducts an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves synthetic data quality, evidenced by downstream supervised fine-tuning results. Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" compares self-generated validation tests to ground truth tests on HumanEval to highlight the impact of accurate test cases on their Scattered Forest Search method. Jiang, "Self-Supervised Learning with Limited Labeled Data" justifies its oracle verifier strategy by comparing its test cases on correct solutions. Additional techniques use test case generation to improve results on coding tasks. Wang, "CodeBERT: Pre-training Bidirectional Transformers for Code Understanding and Generation".  This paper unifies these approaches by introducing a benchmark for systematically assessing synthetic verifier's abilities at determining correct solutions.

As mentioned in the introduction, creating evaluations for test case generation is a well explored area. This includes many benchmarks and systems that compete over quantifying coverage, mutation testing, validity and efficiency. Guo, "CodeGNN: Code Generation with Graph Neural Networks" . Crucially, we do not assess an LLM's ability to generate test cases but rather the effectivness of LLM generated test cases to determine solution quality and rank. This aligns with CodeJudge-Eval . Chen, "CodeBERT-Ada: Adversarial Training for Robust Code Representation Learning", which employs a similar methodology to benchmark LLM-as-a-Judge.

Our work aligns closely with reward model evaluation such as in the case of RewardBench . Liang, "Improving Few-Shot Transfer Learning with Task Relevance and Instance Weighting" leverages synthetic test cases to train coding reward models, evaluating them via best-of-N sampling. Zhang, "Task-Agnostic Relational Reasoning for Code Completion" explores using generated test cases as binary signals to train a test-generating reward model, assessed through best-of-N performance. Despite these advances, a standardized benchmark for comparative evaluation remains lacking. Our work addresses this gap while also advancing test case generation across state-of-the-art standard, reasoning, and reward models.






% Due to the lack of a standardized benchmark for evaluating how well synthetic verifiers model solution quality, previous works typically demonstrate the effectiveness of their methods within their own systems. Liu, "CodeBERT: Pre-training Bidirectional Transformers for Code Understanding and Generation" conduct an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves the quality of synthetic data, as evidenced by downstream supervised fine-tuning results. Guo, "CodeGNN: Code Generation with Graph Neural Networks" present a confusion matrix comparing self-generated validation tests to ground truth tests on HumanEval, highlighting the impact of accurate test cases on their Scattered Forest Search method. Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" supports its oracle verifier strategy by demonstrating that its method evaluates correct solutions with 88.5\% accuracy. We unify these approaches by introducing a benchmark to systematically evaluate synthetic verification's ability to distinguish between correct and incorrect solutions.

% We also posit our benchmark for evaluating other synthetic verification approaches such as coding reward models. Currently, reward models such as Wang, "CodeBERT: Pre-training Bidirectional Transformers for Code Understanding and Generation" are evaluated on the reasoning component of RewardBench Chen, "Improving Few-Shot Transfer Learning with Task Relevance and Instance Weighting" or by performing inference on models and selecting the best-of-n solution based on reward scores. We follow a similar to process as RewardBench but expand the ranking process from correct/incorrect pairs to a field of \(k\) uniquely scored solutions, standardizing the best-of-n evaluation and removing the need for inferencing multiple models.