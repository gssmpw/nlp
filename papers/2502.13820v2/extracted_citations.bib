@misc{acecoder,
      title={ACECODER: Acing Coder RL via Automated Test-Case Synthesis}, 
      author={Huaye Zeng and Dongfu Jiang and Haozhe Wang and Ping Nie and Xiaotong Chen and Wenhu Chen},
      year={2025},
      eprint={2502.01718},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.01718}, 
}

@misc{algo,
      title={ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers}, 
      author={Kexun Zhang and Danqing Wang and Jingtao Xia and William Yang Wang and Lei Li},
      year={2023},
      eprint={2305.14591},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14591}, 
}

@misc{alphacodium,
      title={Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering}, 
      author={Tal Ridnik and Dedy Kredo and Itamar Friedman},
      year={2024},
      eprint={2401.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08500}, 
}

@inproceedings{code_optimization,
author = {Xu, Qingyao and Yang, Dingkang and Zhang, Lihua},
title = {Code Optimization Chain-of-Thought: Structured Understanding and Self-Checking},
year = {2024},
isbn = {9798400710247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690407.3690479},
doi = {10.1145/3690407.3690479},
abstract = {In recent years, significant advancements have been made in the field of LLMs (large language models), particularly within the domain of code optimization. This paper explores the realm of code optimization in LLMs and presents comprehensive approaches to enhance the model's abilities to generate and correct code through fine-tuning, training, and and applying Chain-of-Thought techniques during the inference phase. Novel strategies are introduced to augment the model's understanding of coded structures during the fine-tuning phase by integrating structured code information, providing a more robust grasp of core principles. This knowledge augmentation reflects a significant improvement in the model's structured comprehension of code and lays the foundations for a more effective generation and revision of code. Furthermore, a unique Chain-of-thought technique is applied during the inference phase to generate core coding principles and several sets of unit test data. The large language model is empowered to utilize these testing datasets for an active self-check and modification process. This novel methodology fosters the model's ability to autonomously adjust and fix the produced code, thereby enhancing the overall robustness and reliability of the generated code. The concepts and techniques elucidated in this paper aim to carve a path for future research and advancements in large language model code optimization.},
booktitle = {Proceedings of the 2024 4th International Conference on Artificial Intelligence, Big Data and Algorithms},
pages = {425–430},
numpages = {6},
keywords = {Artificial Intelligence, Chain of Thought, Code Optimization, Fine-tuning, Large Language Models},
location = {
},
series = {CAIBDA '24}
}

@misc{codeaware,
      title={Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM}, 
      author={Gabriel Ryan and Siddhartha Jain and Mingyue Shang and Shiqi Wang and Xiaofei Ma and Murali Krishna Ramanathan and Baishakhi Ray},
      year={2024},
      eprint={2402.00097},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.00097}, 
}

@misc{codejudgeeval,
      title={CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?}, 
      author={Yuwei Zhao and Ziyang Luo and Yuchen Tian and Hongzhan Lin and Weixiang Yan and Annan Li and Jing Ma},
      year={2024},
      eprint={2408.10718},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.10718}, 
}

@misc{coffee,
      title={COFFE: A Code Efficiency Benchmark for Code Generation}, 
      author={Yun Peng and Jun Wan and Yichen Li and Xiaoxue Ren},
      year={2025},
      eprint={2502.02827},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.02827}, 
}

@misc{dstc,
      title={DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs}, 
      author={Zhihan Liu and Shenao Zhang and Yongfei Liu and Boyi Liu and Yingxiang Yang and Zhaoran Wang},
      year={2024},
      eprint={2411.13611},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.13611}, 
}

@misc{dynamic_scaling,
      title={Dynamic Scaling of Unit Tests for Code Reward Modeling}, 
      author={Zeyao Ma and Xiaokang Zhang and Jing Zhang and Jifan Yu and Sijia Luo and Jie Tang},
      year={2025},
      eprint={2501.01054},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.01054}, 
}

@misc{proces_supervised_rl,
      title={Process-Supervised Reinforcement Learning for Code Generation}, 
      author={Yufan Ye and Ting Zhang and Wenbin Jiang and Hua Huang},
      year={2025},
      eprint={2502.01715},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.01715}, 
}

@InProceedings{r2e,
  title = 	 {{R}2{E}: Turning any Github Repository into a Programming Agent Environment},
  author =       {Jain, Naman and Shetty, Manish and Zhang, Tianjun and Han, King and Sen, Koushik and Stoica, Ion},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {21196--21224},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jain24c/jain24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/jain24c.html},
  abstract = 	 {While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/}
}

@misc{rewardbench,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2403.13787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.13787}, 
}

@misc{scatteredforest,
      title={Scattered Forest Search: Smarter Code Space Exploration with LLMs}, 
      author={Jonathan Light and Yue Wu and Yiyou Sun and Wenchao Yu and Yanchi liu and Xujiang Zhao and Ziniu Hu and Haifeng Chen and Wei Cheng},
      year={2024},
      eprint={2411.05010},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.05010}, 
}

@misc{selfcodealign,
      title={SelfCodeAlign: Self-Alignment for Code Generation}, 
      author={Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro von Werra and Arjun Guha and Lingming Zhang},
      year={2024},
      eprint={2410.24198},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.24198}, 
}

@misc{swtbench,
      title={SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents}, 
      author={Niels Mündler and Mark Niklas Müller and Jingxuan He and Martin Vechev},
      year={2025},
      eprint={2406.12952},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.12952}, 
}

@misc{tddbench,
      title={TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get Resolved?}, 
      author={Toufique Ahmed and Martin Hirzel and Rangeet Pan and Avraham Shinnar and Saurabh Sinha},
      year={2024},
      eprint={2412.02883},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2412.02883}, 
}

@misc{testbench,
      title={TestBench: Evaluating Class-Level Test Case Generation Capability of Large Language Models}, 
      author={Quanjun Zhang and Ye Shang and Chunrong Fang and Siqi Gu and Jianyi Zhou and Zhenyu Chen},
      year={2024},
      eprint={2409.17561},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2409.17561}, 
}

@misc{testcasegenerators,
      title={Large Language Models as Test Case Generators: Performance Evaluation and Enhancement}, 
      author={Kefan Li and Yuan Yuan},
      year={2024},
      eprint={2404.13340},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.13340}, 
}

@misc{testeval,
      title={TESTEVAL: Benchmarking Large Language Models for Test Case Generation}, 
      author={Wenhan Wang and Chenyuan Yang and Zhijie Wang and Yuheng Huang and Zhaoyang Chu and Da Song and Lingming Zhang and An Ran Chen and Lei Ma},
      year={2025},
      eprint={2406.04531},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.04531}, 
}

@misc{testgeneval,
      title={TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark}, 
      author={Kush Jain and Gabriel Synnaeve and Baptiste Rozière},
      year={2024},
      eprint={2410.00752},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.00752}, 
}

@misc{valtest,
      title={VALTEST: Automated Validation of Language Model Generated Test Cases}, 
      author={Hamed Taherkhani and Hadi Hemmati},
      year={2024},
      eprint={2411.08254},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.08254}, 
}

