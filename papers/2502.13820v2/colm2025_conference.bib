% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@misc{helpsteer_2,
      title={HelpSteer2-Preference: Complementing Ratings with Preferences}, 
      author={Zhilin Wang and Alexander Bukharin and Olivier Delalleau and Daniel Egert and Gerald Shen and Jiaqi Zeng and Oleksii Kuchaiev and Yi Dong},
      year={2024},
      eprint={2410.01257},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.01257}, 
}

@misc{qwen2.5coder,
      title={Qwen2.5-Coder Technical Report}, 
      author={Binyuan Hui and Jian Yang and Zeyu Cui and Jiaxi Yang and Dayiheng Liu and Lei Zhang and Tianyu Liu and Jiajun Zhang and Bowen Yu and Keming Lu and Kai Dang and Yang Fan and Yichang Zhang and An Yang and Rui Men and Fei Huang and Bo Zheng and Yibo Miao and Shanghaoran Quan and Yunlong Feng and Xingzhang Ren and Xuancheng Ren and Jingren Zhou and Junyang Lin},
      year={2024},
      eprint={2409.12186},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12186}, 
}

@misc{qwen2.5,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{llama3,
      title={The Llama 3 Herd of Models}, 
      author={Meta AI},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{gpt4o,
      title={GPT-4o System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}

@misc{testgen2,
      title={CodeT: Code Generation with Generated Tests}, 
      author={Bei Chen and Fengji Zhang and Anh Nguyen and Daoguang Zan and Zeqi Lin and Jian-Guang Lou and Weizhu Chen},
      year={2022},
      eprint={2207.10397},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.10397}, 
}

@ARTICLE{testgen1,
  author={Schäfer, Max and Nadi, Sarah and Eghbali, Aryaz and Tip, Frank},
  journal={IEEE Transactions on Software Engineering}, 
  title={An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation}, 
  year={2024},
  volume={50},
  number={1},
  pages={85-105},
  keywords={Training;Test pattern generators;Documentation;Codes;Source coding;Software;Electronic mail;Test generation;JavaScript;language models},
  doi={10.1109/TSE.2023.3334955}}


@misc{codejudgeeval,
      title={CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?}, 
      author={Yuwei Zhao and Ziyang Luo and Yuchen Tian and Hongzhan Lin and Weixiang Yan and Annan Li and Jing Ma},
      year={2024},
      eprint={2408.10718},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.10718}, 
}

@misc{dynamic_scaling,
      title={Dynamic Scaling of Unit Tests for Code Reward Modeling}, 
      author={Zeyao Ma and Xiaokang Zhang and Jing Zhang and Jifan Yu and Sijia Luo and Jie Tang},
      year={2025},
      eprint={2501.01054},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.01054}, 
}

@misc{codeaware,
      title={Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM}, 
      author={Gabriel Ryan and Siddhartha Jain and Mingyue Shang and Shiqi Wang and Xiaofei Ma and Murali Krishna Ramanathan and Baishakhi Ray},
      year={2024},
      eprint={2402.00097},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.00097}, 
}

@misc{testcasegenerators,
      title={Large Language Models as Test Case Generators: Performance Evaluation and Enhancement}, 
      author={Kefan Li and Yuan Yuan},
      year={2024},
      eprint={2404.13340},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2404.13340}, 
}

@misc{testbench,
      title={TestBench: Evaluating Class-Level Test Case Generation Capability of Large Language Models}, 
      author={Quanjun Zhang and Ye Shang and Chunrong Fang and Siqi Gu and Jianyi Zhou and Zhenyu Chen},
      year={2024},
      eprint={2409.17561},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2409.17561}, 
}

@misc{coffee,
      title={COFFE: A Code Efficiency Benchmark for Code Generation}, 
      author={Yun Peng and Jun Wan and Yichen Li and Xiaoxue Ren},
      year={2025},
      eprint={2502.02827},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.02827}, 
}

@misc{tddbench,
      title={TDD-Bench Verified: Can LLMs Generate Tests for Issues Before They Get Resolved?}, 
      author={Toufique Ahmed and Martin Hirzel and Rangeet Pan and Avraham Shinnar and Saurabh Sinha},
      year={2024},
      eprint={2412.02883},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2412.02883}, 
}

@misc{valtest,
      title={VALTEST: Automated Validation of Language Model Generated Test Cases}, 
      author={Hamed Taherkhani and Hadi Hemmati},
      year={2024},
      eprint={2411.08254},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.08254}, 
}

@InProceedings{r2e,
  title = 	 {{R}2{E}: Turning any Github Repository into a Programming Agent Environment},
  author =       {Jain, Naman and Shetty, Manish and Zhang, Tianjun and Han, King and Sen, Koushik and Stoica, Ion},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {21196--21224},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/jain24c/jain24c.pdf},
  url = 	 {https://proceedings.mlr.press/v235/jain24c.html},
  abstract = 	 {While Large Language Models’ (LLMs) coding capabilities have advanced rapidly, corresponding evaluation benchmarks on real-world programming setups are yet to catch up. Building a scalable and interactive testbed for evaluating general-purpose AI coding agents for real-world code has been challenging, particularly due to a lack of high-quality test suites available. In this paper, we present Repository to Environment (R2E), a framework that can turn any GitHub repository into a test environment to evaluate the performance of code-generating systems, both static and interactive. R2E is powered by a synergistic combination of program analysis and LLMs to construct equivalence test harnesses for any GitHub function. We instantiate our framework to build the first large-scale benchmark, R2E-Eval1, for building realistic environments for AI coding assistants. Our results demonstrate that even when SOTA models cannot generate correct solutions with advanced prompting techniques, they can effectively use environment feedback highlighting the need to move from static functional coding to interactive programming paradigm. We hope that our framework (and the instantiated benchmark) can motivate research directions by providing web-scale open-ended coding environments. R2E code is available at https://r2e.dev/}
}


@misc{proces_supervised_rl,
      title={Process-Supervised Reinforcement Learning for Code Generation}, 
      author={Yufan Ye and Ting Zhang and Wenbin Jiang and Hua Huang},
      year={2025},
      eprint={2502.01715},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.01715}, 
}

@misc{alphacodium,
      title={Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering}, 
      author={Tal Ridnik and Dedy Kredo and Itamar Friedman},
      year={2024},
      eprint={2401.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.08500}, 
}

@misc{dstc,
      title={DSTC: Direct Preference Learning with Only Self-Generated Tests and Code to Improve Code LMs}, 
      author={Zhihan Liu and Shenao Zhang and Yongfei Liu and Boyi Liu and Yingxiang Yang and Zhaoran Wang},
      year={2024},
      eprint={2411.13611},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.13611}, 
}

@inproceedings{code_optimization,
author = {Xu, Qingyao and Yang, Dingkang and Zhang, Lihua},
title = {Code Optimization Chain-of-Thought: Structured Understanding and Self-Checking},
year = {2024},
isbn = {9798400710247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690407.3690479},
doi = {10.1145/3690407.3690479},
abstract = {In recent years, significant advancements have been made in the field of LLMs (large language models), particularly within the domain of code optimization. This paper explores the realm of code optimization in LLMs and presents comprehensive approaches to enhance the model's abilities to generate and correct code through fine-tuning, training, and and applying Chain-of-Thought techniques during the inference phase. Novel strategies are introduced to augment the model's understanding of coded structures during the fine-tuning phase by integrating structured code information, providing a more robust grasp of core principles. This knowledge augmentation reflects a significant improvement in the model's structured comprehension of code and lays the foundations for a more effective generation and revision of code. Furthermore, a unique Chain-of-thought technique is applied during the inference phase to generate core coding principles and several sets of unit test data. The large language model is empowered to utilize these testing datasets for an active self-check and modification process. This novel methodology fosters the model's ability to autonomously adjust and fix the produced code, thereby enhancing the overall robustness and reliability of the generated code. The concepts and techniques elucidated in this paper aim to carve a path for future research and advancements in large language model code optimization.},
booktitle = {Proceedings of the 2024 4th International Conference on Artificial Intelligence, Big Data and Algorithms},
pages = {425–430},
numpages = {6},
keywords = {Artificial Intelligence, Chain of Thought, Code Optimization, Fine-tuning, Large Language Models},
location = {
},
series = {CAIBDA '24}
}

@misc{solution_inclusion_ref,
      title={Rethinking the Influence of Source Code on Test Case Generation}, 
      author={Dong Huang and Jie M. Zhang and Mingzhe Du and Mark Harman and Heming Cui},
      year={2024},
      eprint={2409.09464},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2409.09464}, 
}

@misc{nemotron_4_340b,
      title={Nemotron-4 340B Technical Report}, 
      author={Nvidia},
      year={2024},
      eprint={2406.11704},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11704}, 
}

@misc{acecoder,
      title={ACECODER: Acing Coder RL via Automated Test-Case Synthesis}, 
      author={Huaye Zeng and Dongfu Jiang and Haozhe Wang and Ping Nie and Xiaotong Chen and Wenhu Chen},
      year={2025},
      eprint={2502.01718},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2502.01718}, 
}

@misc{rewardbench,
      title={RewardBench: Evaluating Reward Models for Language Modeling}, 
      author={Nathan Lambert and Valentina Pyatkin and Jacob Morrison and LJ Miranda and Bill Yuchen Lin and Khyathi Chandu and Nouha Dziri and Sachin Kumar and Tom Zick and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2403.13787},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.13787}, 
}

@misc{scatteredforest,
      title={Scattered Forest Search: Smarter Code Space Exploration with LLMs}, 
      author={Jonathan Light and Yue Wu and Yiyou Sun and Wenchao Yu and Yanchi liu and Xujiang Zhao and Ziniu Hu and Haifeng Chen and Wei Cheng},
      year={2024},
      eprint={2411.05010},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2411.05010}, 
}

@misc{algo,
      title={ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers}, 
      author={Kexun Zhang and Danqing Wang and Jingtao Xia and William Yang Wang and Lei Li},
      year={2023},
      eprint={2305.14591},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14591}, 
}

@misc{selfcodealign,
      title={SelfCodeAlign: Self-Alignment for Code Generation}, 
      author={Yuxiang Wei and Federico Cassano and Jiawei Liu and Yifeng Ding and Naman Jain and Zachary Mueller and Harm de Vries and Leandro von Werra and Arjun Guha and Lingming Zhang},
      year={2024},
      eprint={2410.24198},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.24198}, 
}

@misc{swtbench,
      title={SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents}, 
      author={Niels Mündler and Mark Niklas Müller and Jingxuan He and Martin Vechev},
      year={2025},
      eprint={2406.12952},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.12952}, 
}

@misc{testgeneval,
      title={TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark}, 
      author={Kush Jain and Gabriel Synnaeve and Baptiste Rozière},
      year={2024},
      eprint={2410.00752},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2410.00752}, 
}

@misc{testeval,
      title={TESTEVAL: Benchmarking Large Language Models for Test Case Generation}, 
      author={Wenhan Wang and Chenyuan Yang and Zhijie Wang and Yuheng Huang and Zhaoyang Chu and Da Song and Lingming Zhang and An Ran Chen and Lei Ma},
      year={2025},
      eprint={2406.04531},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.04531}, 
}

@article{codecontests,
   title={Competition-level code generation with AlphaCode},
   volume={378},
   ISSN={1095-9203},
   url={http://dx.doi.org/10.1126/science.abq1158},
   DOI={10.1126/science.abq1158},
   number={6624},
   journal={Science},
   publisher={American Association for the Advancement of Science (AAAS)},
   author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Sutherland Robson, Esme and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
   year={2022},
   month=dec, pages={1092–1097} }


@misc{evalplus,
      title={Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation}, 
      author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},
      year={2023},
      eprint={2305.01210},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2305.01210}, 
}

@misc{mbpp,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2108.07732}, 
}

@misc{humaneval,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.03374}, 
}

@misc{deepseek_r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}


@misc{openai_o1,
      title={OpenAI o1 System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2412.16720},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2412.16720}, 
}