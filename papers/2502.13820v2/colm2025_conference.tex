
\documentclass{article} % For LaTeX2e
\usepackage[preprint]{colm2025_conference}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{graphicx}
\usepackage{tcolorbox}
\usepackage{enumitem}
\usepackage{amsmath}
% \usepackage{subfigure}
\usepackage{subcaption}

\usepackage{lineno}

\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}


\title{Scoring Verifiers: Evaluating Synthetic Verification for Code and Reasoning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \colmfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Aleksander Ficek, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg \\
NVIDIA\\
Santa Clara, CA 15213, USA \\
\texttt{\{aficek,smajumdar,vnoroozi,bginsburg\}@nvidia.com} \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second8:15pm
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\begin{document}

\ifcolmsubmission
\linenumbers
\fi

\maketitle

\begin{abstract}


Synthetic verification techniques such as generating test cases and reward modelling are common ways to enhance the coding capabilities of large language models (LLM) beyond predefined tests. Additionally, code verification has recently found great success as a critical component in improving reasoning capability of LLMs via reinforcement learning. In this paper, we propose a an approach which can transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. We also propose multiple metrics to measure different aspects of the synthetic verifiers with the proposed benchmarks. By employing the proposed approach, we release four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+), and analyzed synthetic verification methods with standard, reasoning-based, and reward-based LLMs. Our experiments show that reasoning can significantly improve test case generation and that scaling the number of test cases enhances the verification accuracy.\footnote{The benchmarks and code used in this paper are publicly available at \url{https://huggingface.co/datasets/nvidia/Scoring-Verifiers} and \url{https://github.com/aleksficek/Scoring-Verifiers}.}

% Code verification has recently found great success as a critical component in the training of reasoning models for coding problems. Synthetic techniques such as self-generated test cases and reward models provide a way to enhance code capabilities beyond predefined tests. Building on these advancements, we propose new benchmarks designed to systematically evaluate the impact of synthetic verification methods on assessing solution correctness. We introduce HE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks into scoring and ranking datasets to evaluate the effectiveness of synthetic verifiers. Using these benchmarks, we analyze synthetic verification methods in standard, reasoning-based, and reward-based LLMs. Our results show that recent reasoning models significantly improve test case generation and that scaling test cases enhances verification accuracy.

\end{abstract}

\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, particularly in code generation. Their advancements extend to solving algorithmic challenges in competitive programming, real-world software engineering tasks, and enhancing automated code testing. Recently, reasoning models such as DeepSeek-R1 \citep{deepseek_r1} have found substantial improvements in math problem-solving and code generation by leveraging large-scale reinforcement learning (RL) and rule-based reward systems. In the context of coding capabilities, they utilized code execution on predefined test cases to generate the signal for RL, enabling the reasoning capability in an LLM. This highlights the importance of code verification with respect to the latest advances in reasoning models. 

Although effective, execution-based scoring approach faces a clear bottleneck due to the scarcity of the coding problems with predefined test cases. To address this constraint, many prior works have explored synthetically generated test cases and unit tests to automatically verify code quality and coverage \citep{testgen1, testgen2}. Additionally, some other works employ coding reward models to improve results on coding benchmarks \citep{acecoder, dynamic_scaling}. In this paper, we collectively refer to these approaches as synthetic verifiers.

\begin{figure}[ht]
    \hspace{-6mm}
    \centering
    % \vspace{-5mm}
    \includegraphics[scale=0.62]{figures/ScoreRankingPaperExample.pdf}
    \caption{The figure illustrates how predefined test cases rank different solutions and how synthetic verifier rankings are compared during evaluation.}
    \label{fig:example_test}
\end{figure}



There are numerous benchmarks that assess various aspects of software engineering capabilities. Datasets such as HumanEval (HE) \citep{humaneval}, Mostly Basic Programming Problems (MBPP) \citep{mbpp}, and CodeContests \citep{codecontests} are commonly used to evaluate the algorithmic and competitive programming skills of LLMs. Other benchmarks, including TESTEVAL \citep{testeval}, TestGenEval \citep{testgeneval}, and SWT-Bench \citep{swtbench}, focus on assessing an LLM's ability to generate test cases for a given solution or feature. While these benchmarks are highly useful, they do not evaluate whether synthetic verification methods can effectively select better code solutions, a task we exemplify in \autoref{fig:example_test}. In particular, rating test case generation suites by how well they rank solutions remains largely unexplored. Additionally, reinforcement learning approaches need a fine-grained score or assessment of a coding solution, more than just pass and fail, to be able to learn effectively. Reward model benchmarks, such as RewardBench \citep{rewardbench}, provide rankings of solutions, but these rankings are limited to preference pairs and their focus is not on coding problems. More details about past works can be found in \autoref{sec:related_works}. 

In this paper, we propose an approach to transform any existing coding benchmark with predefined test cases into a benchmarks that asses synthetic verifiers like test case generation or reward modelling. We also propose multiple evaluation metrics to measure different aspects of the synthetic verifiers with the new benchmarks. By employing the proposed approach, we created four ranking and scoring benchmarks; HE-R, HE-R+, MBPP-R and MBPP-R+ based on the HE-R, HE-R+, MBPP-R and MBPP-R+ respectively. The new benchmark assess how well synthetic verification methods approximate solution correctness and their ability to identify the best solution for a given problem. Then we demonstrate the benefit of these benchmarks by making quantitative observations about the ability of LLM's to generate test cases, the advantage of reasoning models in this domain, and the comparison of different synthetic verification methods like test case generation and reward models. To our knowledge, we are the first to study test case generation with reasoning models in depth. We plan to release our new benchmarks along with the code publicly. 


In summary, we make the following contributions in our work: 
\begin{enumerate}
    \item We provide a recipe to transform any coding benchmark with predefined test cases into a code scoring and ranking benchmark.
    \item We certify our recipe by creating code scoring and ranking versions of HumanEval and MBPP datasets: HE-R, HE-R+, MBPP-R, MBPP-R+.
    \item We use our benchmark to evaluate the test case generation capability in standard and reasoning LLMs alongside coding reward models. We show that reasoning model are more effective in generating test cases compared to non-reasoning ones. 
    
    % Moreover, we evaluated multiple reward models on the new benchmarks.

\end{enumerate}

\section{Proposed Approach}
\label{sec:proposed_approach}
%This enables us to evaluate how well synthetic verifiers can score and rank solutions.

We outline our proposed process to transform a coding benchmark into a scoring and ranking benchmark in \autoref{fig:diagram}. We assume the base benchmark contains a collections of coding questions or instructions along with their predefined test cases. We start by generating a set of solutions by employing multiple prompting techniques and filtering, with the goal of producing a set of solutions which can cover a wide spectrum of code accuracy. This property enables us to have a better and more fine-grained evaluation of synthetic verifiers. After deduplicating the solutions, we calculate the pass rate of each solution using the predefined tests. Then we use these scores to rank the final set of solutions per question. Finally, we propose a collection of metrics to measure the quality of the synthetic verifiers on the new benchmarks. We provide more details about each stage in this section.


%We describe how datasets are processed to generate multiple potential solutions, followed by scoring using predefined test cases, and conclude with filtering and ranking stages.

\begin{figure*}[ht]
    \centering
    \hspace{-5mm}
    \includegraphics[scale=0.52]{figures/Diagram3.pdf}
    \caption{Diagram of the process for turning a coding dataset into a code scoring and ranking benchmark.}
    \label{fig:diagram}
\end{figure*}



%\subsection{Benchmark Preprocessing}

%We build upon EvalPlus \citep{evalplus}, using the extended and original versions of HumanEval and MBPP as the foundation for our scoring and ranking benchmarks. We reconstruct the evaluation framework so that we can determine which test cases fail for each problem as solutions are currently labelled only as pass/fail. We then generate dataset versions that include the expected standard output for each test case, allowing them to be easily formatted as assertion statements. To ensure the accuracy of our approach, we validate that all correct solutions pass all test cases and that a sample dataset with varying correctness achieves similar scores to those provided by EvalPlus.

\subsection{Generating Solutions}
\label{sec:producing-solutions}

Initially, we generate some potential solutions by iterating over each question in the dataset and use multiple prompts with an LLM to produce a response/solution for the given question.  This generation cycle of solutions is repeated across multiple prompts, sampling hyperparameters, and seeds to increase the diversity of solutions. We used mainly two prompts provided in \autoref{fig:generate_samples_prompt}, one for generating correct and one for generating fully incorrect or partially incorrect solutions. The solutions generated by the first prompt are not necessarily correct but inexplicitly prompting the model to generate incorrect solutions will make the set of the solutions more diverse in terms of correctness. After responses are generated, the code solutions are extracted from the responses and their scores are calculated with the predefined test cases. We then aggregate all the generated solutions for each problem, building a diverse population of solutions per problem. 

%until the solutions satisfy the desired requirements.

%We then score each generated solution by computing the fraction of ground truth test cases passed. This generation cycle is repeated across multiple prompts, sampling hyperparameters, and seeds to increase the diversity of solutions. We also explicitly prompt the model to generate partially and fully incorrect solutions. This creates negative samples which will be used to test a synthetic verifiers ranking and scoring abilities. Sample prompts for this can be found in \autoref{fig:generate_samples_prompt}. After multiple cycles, we aggregate all generated solutions for each problem, building a diverse population of outputs.

\subsection{Filtering and Ranking}
\label{sec:filtering}

For each problem, we deduplicate solutions that achieve the same score (fraction of test cases passed) and tie-break using the lower average execution time to select the more optimal solutions. We always select the ground truth solution as the solutions which passes all predefined test cases. We also filter out solutions that fail completely due to non-assertion errors. This ensures we exclude solutions that may be almost correct but achieve a score of zero due to syntax errors or other trivial issues. Finally, we apply a selection algorithm to select the \(k\) solutions that are most evenly distributed in terms of the fraction of test cases passed. Formally, let
\[
S = \{ s_1, s_2, \dots, s_n \},
\]
denote the set of deduplicated solutions, sorted in descending order such that \( s_1 \ge s_2 \ge \cdots \ge s_n \), where \( s_i \) represents the fraction of test cases passed by solution \( i \). We assume that \( s_1 = M = 1.0 \) and define the minimum score \( m \) as
\[
m = \begin{cases}
\min\{ s \in S : 0 < s < 0.1 \} & \text{if such } s \text{ exists}\\[1mm]
s_n & \text{otherwise}
\end{cases}
\]
To account for cases when \( k \) exceeds \( n \), we define the effective selection count as:
\[
k' = \min\{ n, k \}.
\]
For \( i = 1, 2, \dots, k'-1 \), we compute target scores:
\[
T_i = 1 - \frac{i}{k'} (1 - m).
\]
For each \( T_i \), we select an unchosen solution that minimizes the absolute difference to \( T_i \):
\[
s_i^* = \operatorname*{argmin}_{s \in S \{ s_1^*, \dots, s_{i-1}^* \}} \lvert s - T_i \rvert.
\]
Finally, we include the solution corresponding to \( m \) as \( s_{k'}^* \), yielding the selected set:
\[
S^* = \{ s_1^*, s_2^*, \dots, s_{k'}^* \}.
\]



As an example, when \(m = 0.0\), our selection algorithm chooses solutions that best approximate the quantiles \( (0.0,\, 0.25,\, 0.5,\, 0.75,\, 1.0) \). We continue generating solutions as described in \autoref{sec:producing-solutions} and apply filtering stages until we achieve the desired number of uniquely scored solutions per problem. Any problem that does not reach the target \(k\) after multiple rounds may either be discarded or supplemented with manually created solutions. 


\section{Proposed Benchmarks: HE-R, HE-R+, MBPP-R, MBPP-R+}

\subsection{Creations of the Benchmarks}

We created four new benchmarks (HE-R, HE-R+, MBPP-R, and MBPP-R+) using the proposed approach introduced in \autoref{sec:proposed_approach} based on the commonly used coding benchmarks of HE, HE+, MBPP, and MBPP+. We used use GPT-4o (2024-11-20) \cite{gpt4o} as the generator LLM to produce the candidate solutions. The extended versions of HumanEval and MBPP include significantly more test cases, making the passing scores a more reliable proxy for the overall solution correctness. \autoref{tab:initial-benchmarks} shows statistics of the created benchmarks including dataset size, average number of test cases per problem, total number of synthetic solutions, and the average score of selected solutions. HE-R and MBPP-R have significantly lower number of test cases which can affect the quality of the code verification. In cases where the benchmark has a limited number of predefined test cases but a ground-truth solution, we recommend to follow previous methods to generate additional ground truth test cases, similar to how HE+ and MBPP+ benchmarks are created \cite{evalplus}.

For our transformed benchmarks, we set \( k = 5 \) to ensure that HE-R+ and MBPP-R+ contain at least five uniquely scored solutions per problem. For some problems, the automated process could not find enough solutions with the desired requirements, therefore we manually annotated 10 solutions for HE-R+ and 15 solutions for MBPP-R+. We use a \(k\) of 2 to 5 for samples in the base versions of HumanEval (HE-R) and MBPP (MBPP-R) because of the limited number of predefined test cases.


%We also apply our process to transform the original HE and MBPP datasets to demonstrate the lower limit of number of test cases necessary to valuable scoring and ranking benchmark.

\begin{table}[ht]
\centering
\Large

\resizebox{0.60\columnwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\vspace{1mm}
& HE-R & HE-R+ & MBPP-R & MBPP-R+ \\
\midrule
\vspace{1mm}
Number of problems & 164	& 164 & 974 & 378 \\
\vspace{1mm}
Average number of test cases & 9.6 & 764.1 & 3.0 & 108.5 \\
\vspace{1mm}
Number of synthetic solutions & 742 & 820 & 3249 & 820 \\
\vspace{1mm}
Average score of solutions & 0.52 & 0.50 & 0.50 & 0.49 \\

\bottomrule
\end{tabular}
}
\caption{Original and transformed benchmark metrics.}
\label{tab:initial-benchmarks}
\vspace{-3mm}
\end{table}



\subsection{Analysis of the Test Scores}

We showed the distribution of the test score differences per problem for HE-R+ and MBPP-R+ in \autoref{fig:combined_test_score_range}. Test score differences are calculated as the difference between the highest and lowest scoring solutions for each problem. This difference can be an indicator of the correctness coverage of the generated solutions. As it can be seen, all solutions exhibit a minimum score difference of 0.5, with the majority having a difference between 0.9 and 1.0. A higher difference between the solutions shows that each solution varies in quality such that it is distinguishable by generated test cases and coding reward models.

\begin{figure*}[ht]
    \begin{subfigure}{.43\linewidth}
        \includegraphics[width=\linewidth]{figures/HumanEval+_test_score_range.pdf}
        \caption{HE-R+.}
        \label{subfig:a}
    \end{subfigure}\hfill
    \begin{subfigure}{.43\linewidth}
        \includegraphics[width=\linewidth]{figures/MBPP+_test_score_range.pdf}
        \caption{MBPP-R+}
        \label{subfig:b}
    \end{subfigure}
    \caption{Histograms of maximum difference between test case scores for each problem.}
    \label{fig:combined_test_score_range}
\end{figure*}

\autoref{fig:combined_test_score_dist} presents the distribution of the fraction of the tests passed for all solutions in HE-R+ and MBPP-R+. The histograms reveal a bimodal distribution, which aligns with expectations, as the ground truth solution is always included, and there is commonly a solution that fails most tests. The remaining scores conform to the typical target quantiles of \( (0.0,\, 0.25,\, 0.5,\, 0.75,\, 1.0) \).

\begin{figure*}[ht]
    \begin{subfigure}{.43\linewidth}
        \includegraphics[width=\linewidth]{figures/HumanEval+_test_score_distribution.pdf}
        \caption{HE-R+}
        \label{subfig:HE+_test_score_dist}
    \end{subfigure}\hfill
    \begin{subfigure}{.43\linewidth}
        \includegraphics[width=\linewidth]{figures/MBPP+_test_score_distribution.pdf}
        \caption{MBPP-R+}
        \label{subfig:MBPP+_test_score_dist}
    \end{subfigure}
    \caption{Histograms of distribution of test case scores in each dataset.}
    \label{fig:combined_test_score_dist}
\end{figure*}


Histograms of the number of solutions per problem, fraction of tests passed and test score differences for all four benchmarks can be found in \autoref{sec:benchmark_analysis}. As it can be seen, HE-R and MBPP-R show similar pattern to HE-R+ and MBPP-R+ except that there are less unique candidate solution scores due to the limited number of test cases provided to differentiate their quality as shown in \autoref{tab:initial-benchmarks}.
% MBPP-R in particular shows how the lack of test cases limits the number of unique test case scores.



\section{Experimental Settings}

After creating the benchmarks, we used them to explore and evaluate two categories of synthetic verification methods: 1) synthetic test case generation, and 2) code reward modelling. We evaluated several Llama \cite{llama3}, Qwen \cite{qwen2.5, qwen2.5coder}, OpenAI \cite{gpt4o, openai_o1} and DeepSeek \cite{deepseek_r1} models on our benchmarks.

\subsection{Test Case Generation}

For evaluation models on the task of test case generation, we select a well-suited prompt (\autoref{fig:prompt_without_solutions}) to generate an appropriate number of test cases for each problem in the benchmark. In our prompt, we provide two examples in HumanEval format and ensure the model wraps each test case appropriately. We used nucleus sampling with temperature of 1.0 for all the generations. Considering the large number of experiments and the limited context size of the LLMs, we generated 10 test cases per problem for our primary results as we feel this provides a reasonable coverage of edge cases. In order to compute the test scores, we execute each test case and the provided solution individually with a timeout of 3 seconds. 

 





\subsection{Code Reward Modelling}

For evaluating reward models, we used the reward model to estimate the correctness and quality of all the solutions in the benchmarks with the prompt shown in \autoref{fig:reward_model_prompt}. We found applying a brief preamble in the prompt improves the accuracy of the reward models. We computed the reward score for each solution and normalized it using the highest and lowest scores for each problem. Then, the ranking achieved by these scores are evaluated with the different metrics. For the model Nemotron4-340B-Reward\citep{nemotron_4_340b} we use only correctness attribute for the reward score as this fits best to the goals of our evaluations. 


 

\subsection{Metrics}

% We compute the following metrics that quantify a model's synthetic verifiers ability at selecting the solution correctness:
For all the experiments, we propose to use the following metrics to compare and evaluate different LLMs. These evaluation metrics quantify a synthetic verifier's ability to correctly score and rank solution correctness.

\begin{itemize}[leftmargin=*]
    \item \textit{\underline{Top-1 Accuracy}}: Determines if the reward or the unit tests generated by an LLM can correctly rank the best solution first.

    \item \textit{\underline{Spearman's $\rho$ Coefficient}}: Evaluates the strength and correlation between the ranking achieved by the LLM's assessment versus the expected ranking. Expected ranking is determined by the ranking achieved by executing the predefined test cases.
    \item \textit{Bottom-1 Accuracy}: Determines if the reward model or the test cases generated by the LLM can correctly rank the worst solution last.
    % \item \textit{Kendall's $\tau$ Coefficient}: Measures the correlation between expected and actual rankings
    \item \textit{Mean Absolute Error (MAE)}: Quantifies the absolute difference between the expected and estimated fraction of test cases passed.
\end{itemize}

We underscore Top-1 Accuracy and Spearman's $\rho$ coefficient as the primary metrics that encapsulate the ability for synthetic verifiers to select the best solution and rank all solutions appropriately. Bottom-1 and Mean Absolute Error are suitable secondary metrics that provide additional signals on scoring incorrect solutions alongside the delta from expected score. All ranking-based metrics are averaged across all questions within each benchmark. If test case scores result in ties, we compute the fraction of correctly ranked top solutions relative to the number of tied entries for Top-1 and Bottom-1 evaluations. For Spearman's $\rho$ Coefficient we assign the tied ranks their average position.


\section{Results}
In \autoref{tab:main}, we present our main results on HE-R+ and MBPP-R+ with 17 standard, reward, and reasoning-based LLMs while \autoref{tab:main2} presents the results on HE-R and MBPP-R. % We evaluated the standard (non-reasoning) and reasoning models on the task of test case generation while reward models are being evaluated in the setting of reward modelling task. %To evaluate in the task of test case generation, LLMs are employed to generate the test cases, then synthetic solutions are evaluated and ranked based on the their pass rate on the generated test cases. For reward modelling task, the model estimates the score of the solutions directly.

\subsection{Results on Test Case Generation}
We find that the performance of self-generated test cases on our benchmarks generally correlates with the generating model's performance on the original HumanEval and MBPP. As it can be seen, top performing regular models on HumanEval and MBPP such as Qwen2.5-Coder-32B-Instruct, Qwen2.5-72B-Instruct, and GPT-4o perform the best among regular models. Also, larger and stronger models in each family of models outperform their smaller variants on almost all benchmarks and metrics.

% Qwen2.5-Coder-32B-Instruct performs best on HE-R similar to its dominance on HE-R+ among standard models. Similarly, AceCodeRM-32B and Nemotron4-340B-Reward are the two best reward models on HE-R. Results on MBPP-R still resemble those on MBPP-R+ except with slightly more deviation which is to be expected as there are less predefined test cases and resultantly less candidate solutions.

% \begin{table*}[ht]
%     \centering
%     \small
%     \setlength{\tabcolsep}{3pt}  % Reduce column spacing
%     \renewcommand{\arraystretch}{1.2}  % Increase row spacing for better readability
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lccccc|ccccc}
%         \toprule
%         & \multicolumn{5}{c|}{\textbf{HE-R+}} & \multicolumn{5}{c}{\textbf{MBPP-R+}} \\
%         \cmidrule(lr){2-6} \cmidrule(lr){7-11}
%         & Top-1 & Bottom-1 & Spearman & Kendall & MAE & Top-1 & Bottom-1 & Spearman & Kendall & MAE \\
%         \midrule
%         \multicolumn{11}{c}{\textbf{Standard Models}} \\
%         \midrule
%         Meta-Llama-3.1-8B-Instruct & 55.9 & 60.4 & 0.58 & 0.51 & 0.28 & 48.5 & 51.1 & 0.45 & 0.39 & 0.31 \\
%         Meta-Llama-3.1-70B-Instruct & 66.8 & 71.2 & 0.67 & 0.61 & 0.24 & 61.0 & 67.3 & 0.63 & 0.55 & 0.25 \\
%         Meta-Llama-3.3-70B-Instruct & 73.8 & 79.7 & 0.77 & 0.70 & 0.22 & 67.7 & 68.7 & 0.67 & 0.61 & 0.24 \\
%         Qwen2.5-7B-Instruct & 71.9 & 73.2 & 0.76 & 0.69 & 0.23 & 58.8 & 68.2 & 0.64 & 0.58 & 0.25 \\
%         Qwen2.5-32B-Instruct & 74.9 & 77.5 & 0.79 & 0.72 & 0.22 & 68.8 & \textbf{75.0} & 0.72 & 0.65 & 0.23 \\
%         Qwen2.5-72B-Instruct & 78.3 & 76.6 & 0.80 & 0.73 & \textbf{0.21} & \textbf{71.4} & \textbf{75.0} & \textbf{0.73} & \textbf{0.67} & \textbf{0.22} \\
%         Qwen2.5-Coder-7B-Instruct & 71.2 & 73.8 & 0.75 & 0.68 & 0.23 & 60.1 & 68.3 & 0.63 & 0.56 & 0.26 \\
%         Qwen2.5-Coder-32B-Instruct & \textbf{79.1} & \textbf{80.7} & \textbf{0.83} & \textbf{0.77} & \textbf{0.21} & 68.5 & 73.9 & 0.72 & 0.65 & 0.23 \\
%         GPT-4o (2024-11-20) & 76.8 & 76.4 & 0.81 & 0.74 & 0.21 & 70.8 & 71.9 & 0.71 & 0.64 & \textbf{0.22} \\
%         \midrule
%         \multicolumn{11}{c}{\textbf{Reward Models}} \\
%         \midrule
%         AceCodeRM-7B & 68.3 & \textbf{62.8} & 0.65 & 0.55 & \textbf{0.23} & 70.9 & 40.5 & 0.52 & 0.43 & 0.27 \\
%         AceCodeRM-32B & \textbf{77.4} & 53.5 & \textbf{0.68} & \textbf{0.58} & \textbf{0.23} & 74.9 & 39.2 & 0.57 & 0.48 & 0.26 \\
%         Llama-3.1-Nemotron-70B-Reward & 60.4 & 53.7 & 0.61 & 0.52 & 0.24 & 69.6 & 39.4 & 0.53 & 0.45 & 0.27 \\
%         Nemotron4-340B-Reward & 76.2 & 59.2 & 0.67 & 0.57 & \textbf{0.23} & \textbf{75.1} & \textbf{46.0} & \textbf{0.59} & \textbf{0.50} & \textbf{0.25} \\
%         \midrule
%         \multicolumn{11}{c}{\textbf{Reasoning Models}} \\
%         \midrule
%         DeepSeek-R1-Distill-Qwen-32B & 78.2 & 74.1 & 0.78 & 0.71 & 0.22 & 70.1 & 68.5 & 0.65 & 0.58 & 0.24 \\
%         DeepSeek-R1 & 83.8 & 81.4 & \textbf{0.85} & \textbf{0.79} & 0.20 & 77.5 & 75.7 & 0.74 & 0.67 & 0.21 \\
%         o1-mini (2024-09-12) & 82.5 & 79.7 & 0.83 & 0.76 & 0.20 & 74.5 & 73.5 & 0.72 & 0.65 & 0.21 \\
%         o3-mini (2025-01-31) & \textbf{88.2} & \textbf{84.0} & \textbf{0.85} & 0.78 & \textbf{0.18} & \textbf{79.9} & \textbf{80.1} & \textbf{0.78} & \textbf{0.71} & \textbf{0.20} \\
%         \bottomrule
%     \end{tabular}%
%     }
%     \caption{All model results on HE-R+ and MBPP-R+.}
%     \vspace{-3mm}
%     \label{tab:main}
% \end{table*}


\begin{table*}[ht]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}  % Reduce column spacing
    \renewcommand{\arraystretch}{1.2}  % Increase row spacing for better readability
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{lcccc|cccc}
        \toprule
        & \multicolumn{4}{c|}{\textbf{HE-R+}} & \multicolumn{4}{c}{\textbf{MBPP-R+}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE \\
        \midrule
        \multicolumn{9}{c}{\textbf{Standard Models}} \\
        \midrule
        Meta-Llama-3.1-8B-Instruct & 55.9 & 0.58 & 60.4 & 0.28 & 48.5 & 0.45 & 51.1 & 0.31 \\
        Meta-Llama-3.1-70B-Instruct & 66.8 & 0.67 & 71.2 & 0.24 & 61.0 & 0.63 & 67.3 & 0.25 \\
        Meta-Llama-3.3-70B-Instruct & 73.8 & 0.77 & 79.7 & 0.22 & 67.7 & 0.67 & 68.7 & 0.24 \\
        Qwen2.5-7B-Instruct & 71.9 & 0.76 & 73.2 & 0.23 & 58.8 & 0.64 & 68.2 & 0.25 \\
        Qwen2.5-32B-Instruct & 74.9 & 0.79 & 77.5 & 0.22 & 68.8 & 0.72 & \textbf{75.0} & 0.23 \\
        Qwen2.5-72B-Instruct & 78.3 & 0.80 & 76.6 & \textbf{0.21} & \textbf{71.4} & \textbf{0.73} & \textbf{75.0} & \textbf{0.22} \\
        Qwen2.5-Coder-7B-Instruct & 71.2 & 0.75 & 73.8 & 0.23 & 60.1 & 0.63 & 68.3 & 0.26 \\
        Qwen2.5-Coder-32B-Instruct & \textbf{79.1} & \textbf{0.83} & \textbf{80.7} & \textbf{0.21} & 68.5 & 0.72 & 73.9 & 0.23 \\
        GPT-4o (2024-11-20) & 76.8 & 0.81 & 76.4 & 0.21 & 70.8 & 0.71 & 71.9 & \textbf{0.22} \\
        \midrule
        \multicolumn{9}{c}{\textbf{Reward Models}} \\
        \midrule
        AceCodeRM-7B & 68.3 & 0.65 & \textbf{62.8} & \textbf{0.23} & 70.9 & 0.52 & 40.5 & 0.27 \\
        AceCodeRM-32B & \textbf{77.4} & \textbf{0.68} & 53.5 & \textbf{0.23} & 74.9 & 0.57 & 39.2 & 0.26 \\
        Llama-3.1-Nemotron-70B-Reward & 60.4 & 0.61 & 53.7 & 0.24 & 69.6 & 0.53 & 39.4 & 0.27 \\
        Nemotron4-340B-Reward & 76.2 & 0.67 & 59.2 & \textbf{0.23} & \textbf{75.1} & \textbf{0.59} & \textbf{46.0} & \textbf{0.25} \\
        \midrule
        \multicolumn{9}{c}{\textbf{Reasoning Models}} \\
        \midrule
        DeepSeek-R1-Distill-Qwen-32B & 78.2 & 0.78 & 74.1 & 0.22 & 70.1 & 0.65 & 68.5 & 0.24 \\
        DeepSeek-R1 & 83.8 & \textbf{0.85} & 81.4 & 0.20 & 77.5 & 0.74 & 75.7 & 0.21 \\
        o1-mini (2024-09-12) & 82.5 & 0.83 & 79.7 & 0.20 & 74.5 & 0.72 & 73.5 & 0.21 \\
        o3-mini (2025-01-31) & \textbf{88.2} & \textbf{0.85} & \textbf{84.0} & \textbf{0.18} & \textbf{79.9} & \textbf{0.78} & \textbf{80.1} & \textbf{0.20} \\
        \bottomrule
    \end{tabular}%
    }
    \caption{All model results on HE-R+ and MBPP-R+.}
    \vspace{-3mm}
    \label{tab:main}
\end{table*}




% \begin{figure}[ht]
%     \vspace{-4mm}
%     \hspace{-4mm}
%     \includegraphics[scale=0.46]{figures/qwen_MBPP_plus_error_distribution.pdf}
%     \vspace{-10mm}
%     \caption{Distribution of test case execution produced by DeepSeek-R1-Distill-Qwen-32B on MBPP-R+.}
%     \label{fig:qwen_error_dist}
%     \vspace{-2mm}
% \end{figure}



Our findings highlight the effectiveness of test case generation once a model surpasses a certain capability threshold. Achieving 79.1\% and 71.4\% accuracy in differentiating the best solution with only 10 test cases is a significant challenge, requiring deep problem understanding and the ability to construct a minimal yet highly effective test suite that exposes subtle errors. Spearman's $\rho$ Coefficient and Bottom-1 values demonstrate the generated test cases also label imperfect solutions accurately. Models with at least 32B parameters demonstrate these capabilities, accurately selecting the best Top-1 and Bottom-1 solutions. However, this is not their upper limit, we show in the following \autoref{sec:number_cases} that increasing the number of test cases improves performance further. Notably, these models have not been explicitly trained for test case generation yet still perform well, demonstrating significant potential for refining LLMs for this task.


\autoref{subfig:qwen_error_dist} shows the breakdown of the total number of generated test cases that are passed or failed due to assertion errors and non-assertion errors on MBPP-R+, generated by DeepSeek-R1-Distill-Qwen-32B. \autoref{subfig:qwen_test_score_dist} similarly shows the distribution of test case scores produced by the model which exhibits a similar bimodal distribution as the ground truth test scores when producing the benchmark in \autoref{subfig:MBPP+_test_score_dist}.

\begin{figure*}[ht]
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width=\linewidth]{figures/qwen_MBPP_plus_error_distribution.pdf}
        \vspace{-7mm}

        \caption{Error distribution of generated test cases.}
        \label{subfig:qwen_error_dist}
    \end{subfigure}\hfill
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width=\linewidth]{figures/qwen_MBPP_plus_test_score_distribution.pdf}
        \caption{Distribution of generated test case scores.}
        \label{subfig:qwen_test_score_dist}
    \end{subfigure}
    \caption{Analysis of test cases generated by DeepSeek-R1-Distill-Qwen-32B on MBPP-R+}
    \label{fig:qwen_test_score_range}
\end{figure*}

\subsection{Reasoning Model Results}
%a variety of reward models, Acecoder 7B and 32B \cite{acecoder}, Llama-3.1-Nemotron-70B-Reward \cite{helpsteer_2}, and Nemotron4-340B-Reward \cite{nemotron_4_340b} on all the four benchmarks.


We observe that the enhanced coding capabilities in reasoning models translates to improved test case generation. In a head-to-head comparison, DeepSeek-R1-Distill-Qwen-32B outperforms Qwen2.5-32B-Instruct in Top-1 accuracy but falls short in Spearman's coefficient evaluations. However, incorporating DeepSeek-R1, o1-mini, and o3-mini leads to significant improvements across all metrics, positioning them as the most effective synthetic verifiers currently available, especially when scaling the number of test cases. \autoref{fig:reasoning_cot} presents a sample CoT from DeepSeek-R1-Distill-Qwen-32B which illustrates how the model convincingly explores many pathways to cover potential solutions with its test cases.

\subsection{Number of Test Cases Study}
\label{sec:number_cases}

\autoref{fig:combined_scaling_test_cases} shows that while 10 test cases demonstrate reasonable effectiveness but scaling the test cases allows the model to better cover all of the possible cases in a problem. We see that the reasoning capabilities of DeepSeek-R1 allow the model to scale the number of test cases effectively achieving a HE-R+ Top-1 of 91.6\% while plateuing on MBPP-R+. Qwen2.5-Coder-32B-Instruct and DeepSeek-R1-Distill-Qwen-32B alternatively start to plateau at the 20 test case mark. This exemplifies similar findings from \cite{dynamic_scaling} where they scale test cases to improve reward signals in Llama3.1-70B. Further exploration is encouraged to assess the limits of reasoning and scaling of test cases to improve accuracy. Additional metrics while scaling test cases can be seen in \autoref{tab:scaling_test_cases}.


% \begin{figure*}[ht!]

%     \hspace{-2mm}
%     \includegraphics[scale=0.35]{figures/scaling_HE-R+.pdf}
%     \hspace{-2mm}
%     \includegraphics[scale=0.35]{figures/scaling_MBPP-R+.pdf}

%     \vspace{-2mm}
    
%     \caption{Scaling test cases on HE-R+ and MBPP-R+}
%     \label{fig:scaling_test_cases}
%     \vspace{-2mm}
% \end{figure*}


\begin{figure*}[ht]
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width=\linewidth]{figures/scaling_HE-R+.pdf}
        \caption{HE-R+}
        \label{subfig:he_scaling_test_cases}
    \end{subfigure}\hfill
    \begin{subfigure}{.45\linewidth}
        \includegraphics[width=\linewidth]{figures/scaling_MBPP-R+.pdf}
        \caption{MBPP-R+}
        \label{subfig:mbpp_scaling_test_cases}
    \end{subfigure}
    \caption{Top-1 scores of Qwen2.5-Coder-32B-Instruct, DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1 when scaling number of test cases.}
    \label{fig:combined_scaling_test_cases}
    \vspace{-3mm}
\end{figure*}



\subsection{Code Reward Model Results}

Converting the original benchmarks into a scoring and ranking format enables a unique comparison of different synthetic verification methods like test case generation and reward models. From our results in \autoref{tab:main}, the best reward models are AceCoderRM-32B for HE-R+ and Nemotron4-340B-Reward for MBPP-R+. We find that the best performing reasoning and standard models outperform the reward models in most metrics. In similarly sized models like Qwen2.5-Coder-32B-Instruct, the Top-1 scores are competitive in HE-R+ and better in the case of MBPP-R+ while struggling in ranking the varying qualities of incorrect solutions. This could be due to subjectivity in ranking incorrect solutions, they may be functionally incorrect but qualitatively exhibiting meaningful quality. We encourage self-generated test cases as a suitable synthetic verifier for determining the correctness of a solution but see promising opportunities to further enhance reward models for coding.


\subsection{Solution Inclusion}

Finally, we examine the impact of prompting with and without a provided solution, as shown in \autoref{fig:prompt_with_solutions}. All models exhibit significant performance degradation when given a potentially incorrect solution and tasked with writing test cases to evaluate it. We find that the models have a bias towards adhering to any solutions provided even when specifically prompting against this. This is supported by previous works that find LLM's to be worse at providing test cases when provided incorrect compared to correct code \citep{solution_inclusion_ref}.

\begin{figure}[ht!]
    \centering
    % \hspace{-3mm}
    \vspace{-2mm}
    \includegraphics[scale=0.27]{figures/double_bar_chart.pdf}
    \vspace{-4mm}
    \caption{Generating test cases with and without solutions for various models.}
    \label{fig:prompt_with_solutions}
    \vspace{-4mm}
\end{figure}


\section{Related Works}
\label{sec:related_works}

Prior work primarily validates self-generated test cases within isolated systems or limited studies. \cite{selfcodealign} conducts an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves synthetic data quality, evidenced by downstream supervised fine-tuning results. \cite{scatteredforest} compares self-generated validation tests to ground truth tests on HumanEval to highlight the impact of accurate test cases on their Scattered Forest Search method. \cite{algo} justifies its oracle verifier strategy by comparing its test cases on correct solutions. Additional techniques use test case generation to improve results on coding tasks \citep{dstc, alphacodium, code_optimization, proces_supervised_rl}.  This paper unifies these approaches by introducing a benchmark for systematically assessing synthetic verifier's abilities at determining correct solutions.

As mentioned in the introduction, creating evaluations for test case generation is a well explored area. This includes many benchmarks and systems that compete over quantifying coverage, mutation testing, validity and efficiency \citep{testeval, testgeneval, swtbench, r2e, valtest, tddbench, coffee, codeaware, testcasegenerators, testbench}. Crucially, we do not assess an LLM's ability to generate test cases but rather the effectivness of LLM generated test cases to determine solution quality and rank. This aligns with CodeJudge-Eval \cite{codejudgeeval}, which employs a similar methodology to benchmark LLM-as-a-Judge.

Our work aligns closely with reward model evaluation such as in the case of RewardBench \citep{rewardbench}. Similarly, \cite{acecoder} leverages synthetic test cases to train coding reward models, evaluating them via best-of-N sampling. \cite{dynamic_scaling} explores using generated test cases as binary signals to train a test-generating reward model, assessed through best-of-N performance. Despite these advances, a standardized benchmark for comparative evaluation remains lacking. Our work addresses this gap while also advancing test case generation across state-of-the-art standard, reasoning, and reward models.






% Due to the lack of a standardized benchmark for evaluating how well synthetic verifiers model solution quality, previous works typically demonstrate the effectiveness of their methods within their own systems. \cite{selfcodealign} conduct an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves the quality of synthetic data, as evidenced by downstream supervised fine-tuning results. \cite{scatteredforest} present a confusion matrix comparing self-generated validation tests to ground truth tests on HumanEval, highlighting the impact of accurate test cases on their Scattered Forest Search method. \cite{algo} supports its oracle verifier strategy by demonstrating that its method evaluates correct solutions with 88.5\% accuracy. We unify these approaches by introducing a benchmark to systematically evaluate synthetic verification's ability to distinguish between correct and incorrect solutions.

% We also posit our benchmark for evaluating other synthetic verification approaches such as coding reward models. Currently, reward models such as \cite{acecoder} are evaluated on the reasoning component of RewardBench \citep{rewardbench} or by performing inference on models and selecting the best-of-n solution based on reward scores. We follow a similar to process as RewardBench but expand the ranking process from correct/incorrect pairs to a field of \(k\) uniquely scored solutions, standardizing the best-of-n evaluation and removing the need for inferencing multiple models.

\section{Conclusion}

We introduce a systematic approach to transform any coding benchmark with predefined test cases into a ranking and scoring benchmark for evaluating synthetic verification methods. Our method involves generating a diverse set of LLM-produced solutions, scoring them based on the fraction of test cases passed, and applying a structured filtering process to create reliably ranked datasets. We validate this approach by developing HE-R, HE-R+, MBPP-R, and MBPP-R+, which provide a standardized framework for assessing the effectiveness of synthetic verification strategies. We then use our transformed datasets to explore and uncover the effectiveness of standard, reward and reasoning based LLM's. Using our transformed datasets, we investigate the effectiveness of test case-based verification, the impact of reasoning models, and the relative strengths of reward models. Our findings reveal key insights into the performance of various LLM paradigms, highlighting the potential of reasoning-enhanced models and scaling test case generation for improved accuracy.

% \section*{Limitations}

% While HumanEval and MBPP are widely used coding benchmarks, they primarily consist of relatively simpler problems and are of small quantity. To thoroughly evaluate performance on coding tasks, we aim to apply our approach to more difficult coding benchmarks and ones of greater size. Additionally, computational constraints limit our ability to fully explore the upper bound of test cases for all models.


% \section*{Acknowledgments}

% We would like to thank Sean Narenthiran, Siddhartha Jain, Shubham Toshniwal and Wasi Uddin Ahmad for their help reviewing and advising the direction of this work.


\bibliography{colm2025_conference}
\bibliographystyle{colm2025_conference}

\newpage
\clearpage
% \onecolumn

\appendix

\section{Producing Solution Prompt}
\label{sec:generate_samples_propmt}
\input{prompts/generate_samples_prompt}

\newpage
\clearpage
\onecolumn
\section{Benchmark Analysis Visualizations}
\label{sec:benchmark_analysis}


\begin{figure*}[ht]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{-6pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.39\textwidth]{figures/HumanEval_solutions_per_sample.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/HumanEval_test_score_distribution.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/HumanEval_test_score_range.pdf}
    \end{tabular}
    \vspace{-0.6cm}
    \caption{HumanEval (HE-R) benchmark analysis.}
    \vspace{-0.6cm}
    \label{fig:he_analysis}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{-6pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.39\textwidth]{figures/HumanEval+_solutions_per_sample.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/HumanEval+_test_score_distribution.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/HumanEval+_test_score_range.pdf}
    \end{tabular}
    \vspace{-0.6cm}
    \caption{HumanEval Plus (HE-R+) benchmark analysis.}
    \vspace{-0.6cm}
    \label{fig:he+_analysis}
\end{figure*}

\begin{figure*}[ht]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{-6pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.39\textwidth]{figures/MBPP_solutions_per_sample.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/MBPP_test_score_distribution.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/MBPP_test_score_range.pdf}
    \end{tabular}
    \vspace{-0.6cm}
    \caption{MBPP (MBPP-R) benchmark analysis.}
    \vspace{-0.6cm}
    \label{fig:mbpp_analysis}
\end{figure*}

\begin{figure*}[ht!]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{-6pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.39\textwidth]{figures/MBPP+_solutions_per_sample.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/MBPP+_test_score_distribution.pdf} &
        \includegraphics[width=0.39\textwidth]{figures/MBPP+_test_score_range.pdf}
    \end{tabular}
    \vspace{-0.6cm}
    \caption{MBPP Plus (MBPP-R+) benchmark analysis.}
    \vspace{-0.6cm}
    \label{fig:mbpp+_analysis}
\end{figure*}

\newpage
\section{Test Case Generation Prompts}
\label{sec:test_case_prompts}
\input{prompts/test_case_prompt_only}
\newpage
\input{prompts/test_case_prompt_solution}

\newpage
\section{Reward Model Prompts}
\label{sec:reward_model_prompts}
\input{prompts/reward_model_prompt}

\newpage
\section{Supplementary results}

\label{sec:main_results_2}
\begin{table*}[ht!]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}  % Reduce column spacing
    \renewcommand{\arraystretch}{1.2}  % Increase row spacing for better readability
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{p{4.7cm}cccc|cccc}
        \toprule
        & \multicolumn{4}{c|}{\textbf{HE-R}} & \multicolumn{4}{c}{\textbf{MBPP-R}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE \\
        \midrule
        \multicolumn{9}{c}{\textbf{Standard Models}} \\
        \midrule
        Meta-Llama-3.1-8B-Instruct & 63.6 & 0.69 & 70.6 & 0.24 & 55.4 & 0.42 & 56.4 & 0.35 \\
        Meta-Llama-3.1-70B-Instruct &  74.2 & 0.80 & 78.8 & 0.18 & 72.0 & 0.68 & 72.7 & 0.25 \\
        Meta-Llama-3.3-70B-Instruct & 81.6 & 0.89 & 84.8 & 0.14 & 76.1 & 0.73 & 76.2 & 0.23 \\
        Qwen2.5-7B-Instruct & 81.2 & 0.86 & 78.4 & 0.18 & 72.5 & 0.66 & 72.5 & 0.26 \\
        Qwen2.5-32B-Instruct & 85.7 & 0.90 & \textbf{86.9} & \textbf{0.13} & \textbf{78.3} & \textbf{0.75} & \textbf{78.8} & \textbf{0.22} \\
        Qwen2.5-72B-Instruct & 85.3 & 0.90 & 84.2 & 0.14 & 76.9 & 0.74 & 78.3 & \textbf{0.22} \\
        Qwen2.5-Coder-7B-Instruct & 81.6 & 0.86 & 82.4 & 0.17 & 67.2 & 0.61 & 69.4 & 0.27 \\
        Qwen2.5-Coder-32B-Instruct & \textbf{89.0} & \textbf{0.91} & 86.1 & \textbf{0.13} & 76.5 & 0.73 & 77.6 & 0.23 \\
        GPT-4o (2024-11-20) & 81.8 & 0.88 & 84.6 & 0.14 & 74.9 & 0.71 & 76.3 & \textbf{0.22} \\
        \midrule
        \multicolumn{9}{c}{\textbf{Reward Models}} \\
        \midrule
        AceCodeRM-7B & 71.3 & 0.68 & 18.9 & 0.22 & 71.2 & 0.53 & 35.7 & 0.26 \\
        AceCodeRM-32B & 80.5 & \textbf{0.75} & \textbf{29.3} & \textbf{0.21} & \textbf{75.8} & \textbf{0.60} & \textbf{38.6} & \textbf{0.24} \\
        Llama-3.1-Nemotron-70B-Reward & 65.2 & 0.65 & 16.5 & 0.22 & 45.7 & 0.31 & 24.9 & 0.35 \\
        Nemotron4-340B-Reward & \textbf{82.9} & 0.72 & 20.7 & \textbf{0.21} & 66.0 & 0.55 & 37.5 & 0.26 \\
        \midrule
        \multicolumn{9}{c}{\textbf{Reasoning Models}} \\
        \midrule
        DeepSeek-R1-Distill-Qwen-32B & 81.8 & 0.86 & 81.0 & 0.13 & 74.1 & 0.68 & 73.0 & 0.23 \\
        DeepSeek-R1 & \textbf{90.9} & \textbf{0.92} & \textbf{84.0} & \textbf{0.11} & \textbf{81.2} & \textbf{0.77} & \textbf{79.2} & \textbf{0.20} \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Model results on HE-R and MBPP-R.}
    \label{tab:main2}
\end{table*}

% \vspace{-0.1in}

% \newpage

\begin{table*}[ht!]
    \centering
    \small
    \setlength{\tabcolsep}{3pt}  % Reduce column spacing
    \renewcommand{\arraystretch}{1.2}  % Increase row spacing for better readability
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{p{4.7cm}cccc|cccc}        \toprule
        & \multicolumn{4}{c|}{\textbf{HE-R}} & \multicolumn{4}{c}{\textbf{MBPP-R}} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-9}
        & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE & \underline{Top-1} & \underline{Spearman} & Bottom-1 & MAE \\
        \midrule
        \multicolumn{9}{c}{\textbf{Qwen2.5-Coder-32B-Instruct}} \\
        \midrule
        5 Test Cases & 77.5 & 0.81 & 75.7 & 0.22 & 61.9 & 0.67 & 67.1 & 0.24 \\
        10 Test Cases & 79.1 & 0.83 & 80.7 & \textbf{0.21} & 68.5 & 0.72 & 73.9 & 0.23 \\
        15 Test Cases & 81.4 & \textbf{0.82} & \textbf{82.9} & \textbf{0.21} & 70.1 & 0.72 & 76.7 & 0.22 \\
        20 Test Cases & 80.9 & 0.80 & 82.2 & \textbf{0.21} & 70.2 & 0.71 & 76.7 & 0.23 \\
        25 Test Cases & \textbf{81.8} & 0.81 & 80.4 & 0.22 & \textbf{72.5} & \textbf{0.73} & \textbf{80.1} & \textbf{0.22} \\
        \midrule
        \multicolumn{9}{c}{\textbf{DeepSeek-R1-Distill-Qwen-32B}} \\
        \midrule
        5 Test Cases & 72.4 & 0.76 & 72.2 & 0.23 & 65.4 & \textbf{0.65} & 64.1 & 0.24 \\
        10 Test Cases & 78.2 & 0.78 & 74.1 & 0.22 & 70.1 & \textbf{0.65} & 68.5 & 0.24 \\
        15 Test Cases & \textbf{83.7} & 0.77 & 74.4 & 0.21 & 68.3 & 0.62 & 63.1 & 0.24 \\
        20 Test Cases & 78.0 & \textbf{0.81} & \textbf{79.9} & \textbf{0.20} & 69.2 & 0.64 & \textbf{69.0} & \textbf{0.23} \\
        25 Test Cases & 77.6 & 0.76 & 74.1 & 0.21 & \textbf{71.2} & 0.63 & 65.5 & \textbf{0.23} \\
        \midrule
        \multicolumn{9}{c}{\textbf{DeepSeek-R1}} \\
        \midrule
        5 Test Cases & 78.4 & 0.79 & 74.4 & 0.21 & 69.1 & 0.71 & 69.4 & 0.23 \\
        10 Test Cases & 83.8 & 0.85 & 81.4 & 0.20 & 77.5 & 0.74 & 75.7 & 0.21 \\
        15 Test Cases & 86.2 & 0.84 & 84.7 & 0.19 & 79.9 & \textbf{0.76} & \textbf{77.8} & 0.20 \\
        20 Test Cases & 88.2 & \textbf{0.86} & 85.4 & \textbf{0.18} & \textbf{81.2} & \textbf{0.76} & 76.7 & \textbf{0.19} \\
        25 Test Cases & \textbf{91.6} & \textbf{0.86} & \textbf{85.4} & 0.19 & 80.3 & 0.75 & 77.1 & 0.20 \\
        \bottomrule
    \end{tabular}%
    }
    \caption{Test case scaling results.}
    \label{tab:scaling_test_cases}
\end{table*}

\vspace{0.3in}


\newpage
\clearpage
\onecolumn

\section{Model Test Case Scoring Results}
\label{sec:model_test_cases}

\begin{figure*}[ht]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{2pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.48\textwidth]{figures/qwen_HE_base_error_distribution.pdf} &
        \includegraphics[width=0.48\textwidth]{figures/qwen_HE_plus_error_distribution.pdf}  \\
            \hspace{1.1cm}
        \includegraphics[width=0.48\textwidth]{figures/qwen_MBPP_base_error_distribution.pdf} &
        \hspace{0.6cm}
        \includegraphics[width=0.48\textwidth]{figures/qwen_MBPP_plus_error_distribution.pdf}
    \end{tabular}
    \vspace{-0.6cm}
    \caption{DeepSeek-R1-Distill-Qwen-32B test case error distributions.}
    \vspace{-0.6cm}
    \label{fig:mbpp_analysis}
\end{figure*}

\vspace{0.3in}

\newpage

\begin{figure*}[ht]
    \centering
    \hspace*{-0.6cm}
    \setlength{\tabcolsep}{-4pt} % Reduce spacing between figures
    \begin{tabular}{ccc} % Use tabular to tightly arrange images
        \includegraphics[width=0.48\textwidth]{figures/qwen_HE_base_test_score_distribution.pdf} &
        \includegraphics[width=0.48\textwidth]{figures/qwen_HE_plus_test_score_distribution.pdf} \\
        \includegraphics[width=0.48\textwidth]{figures/qwen_MBPP_base_test_score_distribution.pdf} &
        \includegraphics[width=0.48\textwidth]{figures/qwen_MBPP_plus_test_score_distribution.pdf}
    \end{tabular}
    \vspace{-0.4cm}
    \caption{DeepSeek-R1-Distill-Qwen-32B test case score distributions.}
    \vspace{-0.6cm}
    \label{fig:mbpp_analysis}
\end{figure*}

\vspace{0.3in}


\newpage
\section{Reasoning Model Chain-of-Thoughts}
\label{sec:reasoning_cot}
\input{prompts/reasoning_cot}




\end{document}

