\section{Related Works}
\label{sec:related_works}

Prior work primarily validates self-generated test cases within isolated systems or limited studies. ____ conducts an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves synthetic data quality, evidenced by downstream supervised fine-tuning results. ____ compares self-generated validation tests to ground truth tests on HumanEval to highlight the impact of accurate test cases on their Scattered Forest Search method. ____ justifies its oracle verifier strategy by comparing its test cases on correct solutions. Additional techniques use test case generation to improve results on coding tasks ____.  This paper unifies these approaches by introducing a benchmark for systematically assessing synthetic verifier's abilities at determining correct solutions.

As mentioned in the introduction, creating evaluations for test case generation is a well explored area. This includes many benchmarks and systems that compete over quantifying coverage, mutation testing, validity and efficiency ____. Crucially, we do not assess an LLM's ability to generate test cases but rather the effectivness of LLM generated test cases to determine solution quality and rank. This aligns with CodeJudge-Eval ____, which employs a similar methodology to benchmark LLM-as-a-Judge.

Our work aligns closely with reward model evaluation such as in the case of RewardBench ____. Similarly, ____ leverages synthetic test cases to train coding reward models, evaluating them via best-of-N sampling. ____ explores using generated test cases as binary signals to train a test-generating reward model, assessed through best-of-N performance. Despite these advances, a standardized benchmark for comparative evaluation remains lacking. Our work addresses this gap while also advancing test case generation across state-of-the-art standard, reasoning, and reward models.






% Due to the lack of a standardized benchmark for evaluating how well synthetic verifiers model solution quality, previous works typically demonstrate the effectiveness of their methods within their own systems. ____ conduct an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves the quality of synthetic data, as evidenced by downstream supervised fine-tuning results. ____ present a confusion matrix comparing self-generated validation tests to ground truth tests on HumanEval, highlighting the impact of accurate test cases on their Scattered Forest Search method. ____ supports its oracle verifier strategy by demonstrating that its method evaluates correct solutions with 88.5\% accuracy. We unify these approaches by introducing a benchmark to systematically evaluate synthetic verification's ability to distinguish between correct and incorrect solutions.

% We also posit our benchmark for evaluating other synthetic verification approaches such as coding reward models. Currently, reward models such as ____ are evaluated on the reasoning component of RewardBench ____ or by performing inference on models and selecting the best-of-n solution based on reward scores. We follow a similar to process as RewardBench but expand the ranking process from correct/incorrect pairs to a field of \(k\) uniquely scored solutions, standardizing the best-of-n evaluation and removing the need for inferencing multiple models.