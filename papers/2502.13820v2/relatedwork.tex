\section{Related Works}
\label{sec:related_works}

Prior work primarily validates self-generated test cases within isolated systems or limited studies. \cite{selfcodealign} conducts an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves synthetic data quality, evidenced by downstream supervised fine-tuning results. \cite{scatteredforest} compares self-generated validation tests to ground truth tests on HumanEval to highlight the impact of accurate test cases on their Scattered Forest Search method. \cite{algo} justifies its oracle verifier strategy by comparing its test cases on correct solutions. Additional techniques use test case generation to improve results on coding tasks \citep{dstc, alphacodium, code_optimization, proces_supervised_rl}.  This paper unifies these approaches by introducing a benchmark for systematically assessing synthetic verifier's abilities at determining correct solutions.

As mentioned in the introduction, creating evaluations for test case generation is a well explored area. This includes many benchmarks and systems that compete over quantifying coverage, mutation testing, validity and efficiency \citep{testeval, testgeneval, swtbench, r2e, valtest, tddbench, coffee, codeaware, testcasegenerators, testbench}. Crucially, we do not assess an LLM's ability to generate test cases but rather the effectivness of LLM generated test cases to determine solution quality and rank. This aligns with CodeJudge-Eval \cite{codejudgeeval}, which employs a similar methodology to benchmark LLM-as-a-Judge.

Our work aligns closely with reward model evaluation such as in the case of RewardBench \citep{rewardbench}. Similarly, \cite{acecoder} leverages synthetic test cases to train coding reward models, evaluating them via best-of-N sampling. \cite{dynamic_scaling} explores using generated test cases as binary signals to train a test-generating reward model, assessed through best-of-N performance. Despite these advances, a standardized benchmark for comparative evaluation remains lacking. Our work addresses this gap while also advancing test case generation across state-of-the-art standard, reasoning, and reward models.






% Due to the lack of a standardized benchmark for evaluating how well synthetic verifiers model solution quality, previous works typically demonstrate the effectiveness of their methods within their own systems. \cite{selfcodealign} conduct an ablation study showing that filtering LLM-generated solutions with self-generated test cases improves the quality of synthetic data, as evidenced by downstream supervised fine-tuning results. \cite{scatteredforest} present a confusion matrix comparing self-generated validation tests to ground truth tests on HumanEval, highlighting the impact of accurate test cases on their Scattered Forest Search method. \cite{algo} supports its oracle verifier strategy by demonstrating that its method evaluates correct solutions with 88.5\% accuracy. We unify these approaches by introducing a benchmark to systematically evaluate synthetic verification's ability to distinguish between correct and incorrect solutions.

% We also posit our benchmark for evaluating other synthetic verification approaches such as coding reward models. Currently, reward models such as \cite{acecoder} are evaluated on the reasoning component of RewardBench \citep{rewardbench} or by performing inference on models and selecting the best-of-n solution based on reward scores. We follow a similar to process as RewardBench but expand the ranking process from correct/incorrect pairs to a field of \(k\) uniquely scored solutions, standardizing the best-of-n evaluation and removing the need for inferencing multiple models.