\section{Related Work}
\textbf{Behavior Sequence Modeling.} 
In recommendation systems, user behavior sequences contain a wealth of valuable information that reflects user interest preferences, which is crucial to characterizing user personalization. Several studies leverage users' historical interaction behavior sequences to understand their preferences and recommend items that they might be interested in.
DIN \cite{zhou2018din}, DIEN \cite{zhou2019dien}, DSIN \cite{feng2019dsin}, and MIMN \cite{pi2019mimn} introduced the attention mechanism in modeling behavior sequences. By calculating the correlation between user historical behaviors and the target item, it assigns a dynamic weight to each behavior to better capture user interests. 
While longer user behavior sequences can provide more useful information for modeling user interests, they also place a significant burden on the latency and storage requirements of online serving systems. An possible solution is to retrieve the most relevant and important behaviors from an extremely long sequence by matching algorithm, such as category id \cite{pi2020search}, locality-sensitive hashing (LSH) \cite{chen2021end}, SimHash\cite{cao2022sampling}, and Efficient-Target-Attention \cite{chang2023twin}. \\

\noindent \textbf{Sequential Recommendation.}
Sequential recommendation models treat recommendation as a sequence-to-sequence generation task according to Markov Chains assumption.
They generate the next item directly by considering the sequence of past user interactions. 
Methods such as GRU4Rec \cite{hidasi2018gru4rec} and LSTM-based approaches \cite{wu2017recurrent} were particularly effective in capturing both long-term and short-term patterns in item transitions. Following the success of self-attention mechanisms \cite{waswani2017transformer} in natural language processing (NLP), researchers developed many Transformer-based models for sequential recommendation. SASRec \cite{kang2018self} applies self-attention to sequential recommendation tasks, while BERT4Rec\cite{sun2019bert4rec} uses the BERT architecture to model bidirectional relationships between items in a sequence. However, given the sparsity of user behaviors and the computational complexity, these methods typically focus only on users' positive behaviors in industrual implementations.
HSTU \cite{zhai24hstu} incorporates both positive and negative user behaviors into the sequence, capturing more accurate interests and preferences. 
\\



\noindent \textbf{Large Language Models for Recommender Systems.} Following the advent of ChatGPT, there has been an uptick in efforts within both the industry and academia to integrate Large Language Models (LLMs) into recommendation systems. Some studies utilize LLMs to produce semantic embeddings for items, which are then converted into semantic IDs through algorithms such as RQ-VAE \cite{lee2022autoregressive, rajput2023recommender}. UEM \cite{doddapaneni2024user} processed user history as plain text, generating token embeddings for history items. This approach greatly simplified user history tracking and enabled the incorporation of longer user histories into the language model, and allowed their representations to be learned in context.
ILM \cite{yang2024item} incorporating collaborative filtering knowledge into a frozen LLM for conversational recommendation tasks.
LLMs can also be used to encode target feature entities \cite{chen2024hllm}, extracting their implicit embedding representations to feed into subsequent recommendation models. 
These methods primarily leverage the content understanding capabilities of LLMs to provide additional informational gains, rather than applying the underlying paradigm of LLM modeling directly to the recommendation system.