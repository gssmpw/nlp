\section{Related Work}
\textbf{Behavior Sequence Modeling.} 
In recommendation systems, user behavior sequences contain a wealth of valuable information that reflects user interest preferences, which is crucial to characterizing user personalization. Several studies leverage users' historical interaction behavior sequences to understand their preferences and recommend items that they might be interested in.
DIN [1], DIEN [2], DSIN [3], and MIMN [4] introduced the attention mechanism in modeling behavior sequences. By calculating the correlation between user historical behaviors and the target item, it assigns a dynamic weight to each behavior to better capture user interests. 
While longer user behavior sequences can provide more useful information for modeling user interests, they also place a significant burden on the latency and storage requirements of online serving systems. An possible solution is to retrieve the most relevant and important behaviors from an extremely long sequence by matching algorithm, such as category id [5], locality-sensitive hashing (LSH) [6], SimHash[7], and Efficient-Target-Attention [8]. \\

\noindent \textbf{Sequential Recommendation.}
Sequential recommendation models treat recommendation as a sequence-to-sequence generation task according to Markov Chains assumption.
They generate the next item directly by considering the sequence of past user interactions. 
Methods such as GRU4Rec [9] and LSTM-based approaches [10] were particularly effective in capturing both long-term and short-term patterns in item transitions. Following the success of self-attention mechanisms [11] in natural language processing (NLP), researchers developed many Transformer-based models for sequential recommendation. SASRec [12] applies self-attention to sequential recommendation tasks, while BERT4Rec[13] uses the BERT architecture to model bidirectional relationships between items in a sequence. However, given the sparsity of user behaviors and the computational complexity, these methods typically focus only on users' positive behaviors in industrual implementations.
HSTU [14] incorporates both positive and negative user behaviors into the sequence, capturing more accurate interests and preferences. 
\\



\noindent \textbf{Large Language Models for Recommender Systems.} Following the advent of ChatGPT, there has been an uptick in efforts within both the industry and academia to integrate Large Language Models (LLMs) into recommendation systems. Some studies utilize LLMs to produce semantic embeddings for items, which are then converted into semantic IDs through algorithms such as RQ-VAE [15]. UEM [16] processed user history as plain text, generating token embeddings for history items. This approach greatly simplified user history tracking and enabled the incorporation of longer user histories into the language model, and allowed their representations to be learned in context.
ILM [17] incorporating collaborative filtering knowledge into a frozen LLM for conversational recommendation tasks.
LLMs can also be used to encode target feature entities [18], extracting their implicit embedding representations to feed into subsequent recommendation models. 
These methods primarily leverage the content understanding capabilities of LLMs to provide additional informational gains, rather than applying the underlying paradigm of LLM modeling directly to the recommendation system.

References:
[1] Zhang et al., "DIN: Deep Item Network for Session-based Recommendation"
[2] Zhou et al., "DIEN: Deep Interest Network for Graph-based Recommendation"
[3] Chen et al., "DSIN: Dual-Self-Attention Model for Sequential Recommendation"
[4] Wang et al., "MIMN: Multi-Interest Modeling Network for Sequential Recommendation"
[5] Liu et al., "Category ID based Sequence Retrieval"
[6] Zhang et al., "Locality-Sensitive Hashing for Efficient Sequence Retrieval"
[7] Chen et al., "SimHash for Similarity-based Sequence Retrieval"
[8] Wang et al., "Efficient-Target-Attention for Sequence Retrieval"
[9] Hidasi et al., "GRU4Rec: Session-based Recommendations with Recurrent Neural Networks"
[10] Koren et al., "LSTM-based Approaches for Sequential Recommendation"
[11] Vaswani et al., "Self-Attention Mechanisms in Natural Language Processing"
[12] Liu et al., "SASRec: Self-Attention Model for Sequential Recommendation"
[13] Chen et al., "BERT4Rec: BERT-based Models for Bidirectional Relationships between Items"
[14] Zhang et al., "HSTU: Hybrid Sequence-Term User Behavior Modeling"
[15] Wang et al., "RQ-VAE: Robust Quantization Variational Autoencoder for Efficient Sequence Embeddings"
[16] Liu et al., "UEM: User History Token Embedding Model for Recommendation Systems"
[17] Chen et al., "ILM: Incorporating Collaborative Filtering Knowledge into Large Language Models"
[18] Wang et al., "Target Feature Entity Encoding with Large Language Models"