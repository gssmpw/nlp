\documentclass[11pt]{article}

\usepackage{lineno}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage{makecell}
\usepackage[title]{appendix}
\usepackage{float}
\usepackage{amsmath,amssymb}
\usepackage{afterpage}
\usepackage{numprint}
\usepackage[latin1]{inputenc}
\usepackage{authblk}
\usepackage{graphicx}
\usepackage[a4paper, margin = 2cm]{geometry}
\usepackage{filecontents}
\npthousandsep{\,}

\title{Sorting the Babble in Babel: \\ Assessing the Performance of Language Detection Algorithms on the OpenAlex Database} 

\author[1,2,*]{Maxime Holmberg Sainte-Marie}  
\author[2]{Diego Kozlowski}
\author[2,3]{Lucía Céspedes}
\author[2,3,4]{Vincent Larivière}

\affil[1]{Department of Design, Media and Educational Science, University of Southern Denmark, Universitetsparken 1, 6000 Kolding, Denmark}
\affil[2]{Chaire UNESCO sur la science ouverte, École de Bibliothéconomie et des Sciences de l'Information, Université de Montréal, Case Postale 6128, Station Centre-Ville, Montréal, Québec, Canada, H3C 3J7}
\affil[3]{Consortium Érudit, Université de Montréal, 3744 rue Jean-Brillant, Bureau 6500,  Montréal, Québec, Canada, H3T 1P1}
\affil[4]{Observatoire des Sciences et des Technologies, Centre Interuniversitaire de recherche sur la science et la technologie, Université du Québec à Montréal, Case postale 8888, Succursale Centre-Ville, Montréal, Québec, Canada, H3C 3P8}
\affil[*]{Corresponding author: Maxime Holmberg Sainte-Marie, mhsm@sdu.dk}



\begin{document}

\maketitle


\begin{abstract}
This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.
\end{abstract}

\providecommand{\keywords}[1]{\textbf{\textit{Keywords---}} #1}
\keywords{Bibliographic Databases, Automatic Classification, Natural Language Processing}

\section{Introduction}


At the crossroads of linguistics and computer science research, monolingual automatic language detection is nowadays considered a solved task in the literature, with state-of-the-art models achieving above human-level performance in as much as 1,366 languages \citep{caswell2020language}. Integrated in various real-world applications, language detection algorithms can also greatly enhance the organization, accessibility, and leveraging of information stored in databases. More precisely, by automatically identifying and categorizing the language of text attributes, these algorithms facilitate the sorting and indexing of multilingual data, which makes it easier to manage large, world-wide collections. Used in conjunction with machine translation algorithms, they can also support effective cross-language searches by allowing users to find pertinent information regardless of the language in which either the query or the relevant documents were written. Last but not least, language detection can benefit both data and resource management by respectively improving cross-linguistic data consistency and providing insights based on language distributions within the database.

Despite these advantages, the use of language detection algorithms in bibliographic databases remains as rare as scarcely documented. The main reason for this gap in implementation and research is both historical and infrastructural, as databases such as Scopus or the Web of Science have traditionally and quasi-exclusively focused on publications written in English \citep{ammon2006language, archambault2005welcome, mongeon2016journal,  demeter2020academic, vera2019web}. This linguistic bias, while greatly reducing the need for multi- and cross-linguistic functionalities, also impacts the quality and scope of both research evaluation and research on research: being mostly blind to anything that eludes their monolingual lens, traditional bibliometric databases thus provide a fundamentally skewed view of the disciplinary and geographical landscape of the global scientific community \citep{beigel2022batalla}, which as a consequence distorts every initiative or investigation it informs and whose repercussions affect the neglected communities in various ways and degrees \citep{tennant2020web,salatino2021fetichismo,finardi2022linguas,navarro2022rethinking}.

Recent open-source alternative to the main traditional bibliometric databases, OpenAlex (OpAl) stands out from its predecessors by offering a broader landscape of scholarly communication \citep{priem2022openalex}. Built on an open codebase and from a variety of bibliometric projects, services, databases, and repositories, it has been shown to outperform both the Web of Science and Scopus in terms of document and journal frequency \citep{alperin2024analysis,culbert2024reference,van2024oligopoly,jiao2023exclusively}. Of particular relevance here, recent research has shown that OpAl also has a higher proportion of articles written in non-English languages than the two above-mentioned bibliographic databases \citep{cespedes2025evaluating,vera2019web}. In light of these considerations, not only does OpAl have the potential to offer a broader, multilingual, and as such more representative view of scholarly production at the global scale, but its stronger allophone content makes it also more susceptible to benefit from the functionalities and advantages language detection algorithms can offer to multilingual databases.

Regarding that last point, previous studies have however shown that language detection algorithm performance in multilingual context can vary significantly depending on the languages involved or on the specific characteristics of the dataset. In that perspective, effective multilingual database management requires linguistic classification procedures that are not only tailored to the specific linguistic features and metadata characteristics of the dataset, but also based on algorithms that are robust to noise in the data attributes considered \citep{balazevic2016language, polasi2016combining, sujaini2024}. In the specific case of OpenAlex, while a quality assessment of OpAl language metadata has already been carried out \citep{cespedes2025evaluating}, no attempt has yet been made yet to optimize the quality of its language metadata, that is, to find the best linguistic classification procedure for the articles it contains and indexes, based on its current metadata content and configuration.

Such comparative assessment of language detection procedures will be the focus of the present paper. More precisely, the objective here is to apply different algorithms on various article metadata corpora extracted from a manually-annotated sample of OpAl-indexed publications in order to find the algorithm-corpus combination that performs best over a selected set of evaluation measures. The following section gives a detailed account of the methodological outline designed for this project, followed respectively by an analysis and discussion of the results obtained.

\section{Methods}

Inferring the language of an indexed article based on its metadata is no simple procedure, as it involves two different computational tasks: detecting the language in which the metadata is written, and inferring the language of the article from that of its corresponding metadata. While closely related, these two tasks are however far from equivalent and can lead to opposite outcomes in certain contexts. For example, in the case of an article written in English but whose title is written in latin, an algorithm can only accurately predict the language of the article by failing to correctly identify the language of its title. Using more metadata attributes than just titles may help address such situations, but which set of metadata allows for optimal classification is most likely to change depending not only on the algorithm used, but also and perhaps more importantly on the language considered as well as on the performance measure considered; additionally, article metadata can be in different languages, which might substantially hinder algorithm performance.

In light of these considerations and in order to cover as much of the whole parameter space as possible, multiple linguistic classification procedures have been designed for this project. Following a series of rigorous data collection and processing operations, various metadata corpora were created by first collecting a set of manually- and linguistically-annotated samples to use as ground truth, then merging each article from the collected sample with various combinations of article metadata attributes considered relevant for language detection purposes, namely titles, abstracts, and journal names. Performance data for the resulting linguistic classifications is then collected by comparing predictions to observations for each annotated article in the sample. Procedure performance is then assessed at the language level first, using a series of evaluation measures commonly used in information retrieval, then at the database level, by simulating the overall performance of each algorithm using bayesian conjugate inferences over probabilistic confusion matrices. These different steps are detailed further in the following subsections.

\subsection{Sample Collection}

The ground truth for this study is based on the annotation work carried out by \citet{cespedes2025evaluating} in their assessment of language metadata quality in OpAl. For that paper, a multilingual sample of indexed articles was manually annotated following a two step process. First, two samples of indexed articles were randomly drawn from the database, a first one containing 50 articles for each of the 55 languages present in the database (the resulting sample included only $2701$ articles, as some languages had less than 50 indexed articles) and a second containing 300 documents for each of the 11 most frequent languages in the database. Then, each article sampled was manually annotated by not only inspecting its title and abstract, but also by accessing the full text of the corresponding document, identify its language, and then determine if that paper indeed corresponds to a scientific paper; entries whose documents couldn't be accessed were ignored (see \citep{cespedes2025evaluating} for more information about these samples).

In addition to these two samples and in order to account for linguistically-unclassified articles indexed in OpAl, a sample of 300 such articles was collected for the purposes of the current project. Following extraction of this third subset, whose size relative to the previous two samples is proportional to the relative frequency of linguistically-unclassified articles in OpAl as of July 25th 2024, all articles thus sampled were annotated based on the same procedure as the one used for the two previous samples.

\subsubsection{Corpus Building}

As can be expected of a bibliographic database of such scale, OpAl contains articles from a variety of sources, from publishers and various scientific or professional organizations to conferences and self-archiving repositories. As a result, the type and quantity of metadata associated with articles indexed in the database can vary enormously: with regard to textual metadata in particular, some entries only include titles, whereas others include abstracts, journal names or both. Figure \ref{fig:langfreq} shows the absolute and relative frequency distributions of all articles indexed in OpAl as of December 25th 2025, grouped by language and metadata configuration type. As regards to languages, the 11 most frequent languages in the database are directly represented in the figure, along with two additional categories which respectively include all articles classified in less frequent languages (other) and articles currently not classified linguistically (NA). As regards to configuration types, 4 different categories are distinguished: articles with titles, articles with titles and abstracts, articles with titles and journal names, and articles with titles, abstracts, and journal names. Absolute frequencies for languages and category types are respectively shown in millions (M) below the different y-axis and legend labels, while relative proportions for each metadata configuration type of each linguistic category are shown in percentages at the end of each corresponding horizontal bar. Finally, the x-axis was scaled algorithmically in order to better compare proportions and frequencies within and across language categories. 

\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/lang_freq.png}
\caption{Percentage of Articles included in the OpenAlex database, grouped by by Language and Metadata Availability}
\label{fig:langfreq}
\end{figure}

As this figure shows, most articles in the database are written in English, as reported in \citet{cespedes2025evaluating}, a majority of them also happen to include both abstracts and journal names as metadata. Following English, Asian languages form the most frequent linguistic group, followed by Romance languages. As for the different metadata attributes considered here, no clear cross-linguistic pattern can be found: while all articles have titles as attributes, metadata availability as regards to abstracts or journal names varies a lot across languages. Articles that include both their abstract and the name of the publishing journal represent a minority of indexed articles: besides English, only Chinese and Indonesian have a majority of indexed articles that include both articles and journal names. As for the remaining languages, most article entries only have journal names alongside titles, and a majority of Italian articles do not have any other relevant information other than titles.

For the purposes of the present project, such metadata variability is far from trivial. First of all, the performance of the algorithms used here should be expected to strongly correlate with input text length : the more text is given as input, the more accurate the predictions can be expected to be, regardless of the algorithm considered. Additionally, some language detection algorithms might be particularly sensitive to specific attribute features; given the constantly growing and changing nature of the OpAl database, algorithm performance at the database level at a given phase of its development could very well depend on the saliency and pervasiveness of these anti-patterns within the metadata configuration at any given time.

In light of these considerations and in order to find the language classification procedure whose predictive power is the most robust to such changes, different corpora were built for the sampled articles based on the metadata combination types used in figure \ref{fig:langfreq}.

\begin{description}[
	align=right,
	labelindent=.5cm,
	labelwidth=3.25cm,
	leftmargin=6cm,
	labelsep*=.75cm,
	parsep = 0.1cm,
	itemindent= 0cm,
	style = multiline]
\item[Titles (T)]{For each article, get the title.}
\item[Titles and Abstracts (A)]{For each article, get its title and abstract, if available.}
\item[Titles and Journal Names(J)]{For each article, get its title and the name of the journal it is published in, if available.}
\item[Greedy (G)]{For each article, get its title, abstract, and the name of the journal it is published in, when either of the latter two is available.}
\end{description}

Additional corpora were also generated based on all possible title, abstract, and journal name combinations; descriptions  of these procedures and results obtained for each of them can be found in the appendix. Each of these corpora are built so as to include at least the title of every article in the annotated sample. This ensures that every procedure creates a complete corpus by assigning one document for each article sampled, thus allowing language detection algorithms to exhaustively partition the sample into the selected language categories. Finally, in order for each document to include complete sentences, text segments not ending with periods were punctuated accordingly.

\subsection{Algorithms}

All language detection algorithms whose performance is assessed here have been selected based on two criteria: Python implementability and completeness. The first criterion is not in itself problematic, since most language detection algorithms are implemented in Python or can be used in Python via packages specifically developed to that end. As for the second criterion, an algorithm is said to be complete if it terminates with a solution when one exists \citep{pearl1984heuristics}; in the case of the present project, any algorithm said to be complete is thus one that is able to return a linguistic prediction for each document given as input. Although this criterion does not seem too restrictive at first sight, some algorithms have a relatively low language coverage and have not been designed or trained to detect all the languages present in the OpAl database. For this and other reasons, any algorithm that was unable to return a prediction for every document submitted was excluded from subsequent analyses, regardless of its overall performance. Based on these restrictions, the different language detection algorithms selected and evaluated for this are shown and described in Table \ref{table:algo}.

\begin{table}[H]
\centering
\begin{tabular}{r|p{11cm}}
\textbf{Algorithm} & \textbf{Description}\\
\hline & \\[-0.15cm]
CLD2 & Second installment of the Compact Language Detector developed by Google for its Chrome browser, CLD2 is built on a Naïve Bayesian classifier trained on a manually constructed corpus of chosen web pages for each language \citep{sites2025compact}.\\[.15cm]
CLD3 & Third installment of the Compact Language Detector designed for Chrome. It is a multi-layered neural network built on word embeddings based on character unigram, bigram, and trigram proportions and able to detect 107 languages \citep{salcianu2025}.\\[.15cm]
DetectLanguage & Partly based on Google's CLD2 Cloud translation API, this language detection web service allows its users to detect the language of a text either through a POST request or its API \citep{detectlanguage2025}.\\[.15cm]
FastText & Created by Facebook's AI Research lab for efficient learning of word representations and sentence classification, fastText's language identification model is trained on data from Wikipedia, Tatoeba, and SETimes \citep{joulin2016bag, joulin2016fasttext,bojanowski2016enriching}. Available in both uncompressed and compressed versions, only the uncompressed and more performant version is used here.\\[.15cm]
FastSpell & FastSpell complements FastText with spell-checking functionalities from Hunspell. This algorithm detects languages by first trying to determine the language of a sentence with FastText, then makes extra checks with Hunspell to determine the language more precisely \citep{banon2024fastspell}.\\[.15cm]
Langdetect & A Java-based language-detection library, Langdetect is built on a Naive Bayes classifier, trained on a mix of n-gram-based language-independent character corpora extracted from Wikipedia abstracts \citep{nakatani2010langdetect}\\[.15cm]
LangID & This algorithm uses a Naive Bayes classifier trained on relative frequencies of character n-grams extracted from JRCAcquis, ClueWeb 09, Wikipedia, Reuters RCV2, and Debian i18n \citep{lui2011cross, heafield2015language, lui2012langid}\\[.15cm]
\hline
\end{tabular}
\caption{Description of the different language detection algorithms}
\label{table:algo}
\end{table}

For every algorithm implemented, different linguistic classification procedures were designed by using it on every metadata corpus generated, resulting in a total of $4 * 7 = 28$ different sample partitions ($11 * 7 = 77$ in the appendix).

\subsection{Performance Assessment}

To assess the predictive power of each classification technique as regards to article language, a confusion matrix was built for each of the 11 most frequent languages in the database (those identified in figure \ref{fig:langfreq}) by determining, for each article entry sampled, if the language detected by the algorithm and the one observed through manual annotation are in that language. In the case of English for example, this procedure consists in determining, for each annotated article, if the metadata language detected by the algorithm for the corresponding article is in English and if manual checking has determined (observed) that the article is indeed written in English.

Following this and in accordance with the procedure used in \citet{cespedes2025evaluating}, true positive counts, false positive counts, and false negative counts were compiled and grouped by by classification procedure and language. Then, in order to assess the performance of each language detection procedure for each different language category, three different but complementary performance measures were implemented and used.

The first measure, precision, assesses the correctness of an algorithm's predictions. In the present context, the precision of an algorithm for a specific language corresponds to the proportion of articles predicted to be written in that language that are indeed written in that language. In other words, if we were to ask an algorithm for all articles written in German, that algorithm's prediction score for German corresponds to the proportion of articles it identifies as being written in German that are indeed in that language.

Often used as a complement to precision, recall aims to assess how exhaustive an algorithm's predictions are. Here, the recall score of an algorithm refers to the proportion of articles written in a given language that are identified as such by the algorithm. To take the previous example, if we were to ask an algorithm for all articles written in German, the recall score of that algorithm for German corresponds to the proportion of all articles written in German that are returned by the algorithm. Both precision and recall have true positives at the numerator, that is, the number of accurate predictions returned by the algorithm; their distinction lies at the level of the denominator, which corresponds to the total number of positive predictions (true and false positives) and relevant cases (true positives and false negatives) respectively.

While precision and recall are the usual go-to indicators in matters of algorithmic performance, processing speed is often what tips the balance at the implementation phase, especially in industrial settings. In the case of the present study, how much time it takes for an algorithm to predict the language of every document of every corpus is of paramount importance, as a difference of a few seconds at the level of the collected sample can easily translate into hours, days or even weeks at the database level. To properly assess procedure scalability, the processing speed of each algorithm was collected by timing how long it takes to process each corpus in its entirety. All collected times were then rescaled to the $[0, 1]$ interval through normalization, which was done by multiplying the time recorded by each procedure on each corpus by the reciprocal of the quickest time obtained by any procedure on any corpus.

\subsection{Simulation}

While the procedures described above provide interesting information on the performance of the various algorithms and allow them to be broken down by language and type of procedure, these descriptions can however be misleading when it comes to assessing the performance of the various procedures at the scale of the entire database, as they do not take into account robustness to uncertainty as well as the impact of data imbalance on algorithm performance evaluation. To address these crucial dimensions of algorithm performance as best as possible, a simulation based on various probabilistic models was carried out and its results presented over various configuration of precision, recall, and processing speed weights.

\subsubsection{Robustness Assessment}

As results presented in \citet{cespedes2025evaluating} show, the quality of OpAl's language metadata is surprisingly good; however, it it not perfect, which means that the language frequency distributions returned by the database are off to an unknown but certain extent. Also but perhaps most importantly, OpAl is an incessantly growing and changing database, as new articles are indexed and metadata of already indexed articles gets updated. The current state of the database in terms of language proportions and types of metadata is therefore not only uncertain at the present time, but also bound to change in the more or less short term. In that perspective, algorithmic robustness, that is, the ability of an algorithm to maintain "some desired system characteristics despite fluctuations in the behavior of its component parts or its environment" \citep[p.2539]{carlson2002complexity}, is bound to have a crucial impact of the quality of any linguistic classification of the database. In other words, an evaluation of linguistic classification procedures for the OpenAlex database has to take into account the ability of each procedure to that maintain good overall performance despite uncertainties in and changes to the current state of the database.

In order to best quantify database uncertainty and algorithm robustness, probabilistic confusion matrices were generated based on the sample predictions collected from the different language detection procedures. Several studies have already demonstrated the usefulness of Bayesian uncertainty modeling in the context of algorithm performance evaluation \citep{goutte2005probabilistic,brodersen2010balanced,benavoli2017time,caelen2017bayesian,totsch2021classifier,chlaily2023measures}. The probabilistic approach adopted here will consist in modeling uncertainty by treating precision, recall as well as the relative proportion of languages and metadata types in the database as random variables modeled using different conjugate distribution models. In conformity with previous probabilistic modelisations of confusion matrices and submatrices \citep{brodersen2010balanced,totsch2021classifier}, precision and recall probabilities for each algorithm, language, and metadata configuration, were modeled using posterior beta-binomial distributions with uniform priors for the confusion submatrices of each distinct database subgroup $k$; true positive counts were used as alpha parameters for both measures, while false positives and false negatives were used as beta parameters for precision and recall distributions respectively.

\begin{align}
Precision &\sim Beta(TruePositives+1,FalsePositives+1)\\
Recall &\sim Beta(TruePositives+1,FalseNegatives+1)
\end{align}

To model the proportions between the different language and metadata configuration groups within the database, a posterior Dirichlet-multinomial distribution with uniform prior was used, which generalizes the beta-binomial model with Laplace priors to multiple classes or categories, in this case the different database subgroups shown in the mosaic figure above. Proportion probabilities were thus obtained by adding the absolute article frequencies $n_{1...k}$ of each $k$ database subgroup as alpha parameters of the underlying beta distributions and incrementing them by 1 as in the case of precision and recall.

\begin{align}
LanguageSubgroupProportion &\sim Dirichlet((n_1+1)...(n_k+1))
\end{align}

Using these different conjugate distributional models, overall performance scores for the OpAl database were estimated through the following simulation procedure: for each classification procedure, precision and recall scores are obtained by drawing a random number from a mixture of beta-binomial distributions build on the classification outcomes for each language category, mixture whose component weights are obtained through a single draw of the language proportion model. This random sampling process is repeated $100\,000$ times, and scores for each measure and procedure are compiled into a single simulation dataset.

\subsubsection{Data Imbalance}

As is often the case in large databases, OpenAlex suffers from severe distribution skews, many of which are caused by the preponderance of English-written articles, which allegedly account for $75.1$\% of all database entries (imbalance ratio of 3:1). Such an asymmetry, apart from the question of its representativeness of the real linguistic distribution of scholarly communication on a global scale, poses a serious problem with regard to the evaluation of the linguistic classification procedures implemented for this project. As is the case with imbalanced sets in general, while the general objective is to improve recall without hurting precision, the number of false negatives for the minority classes can hardly be reduced without at the same time increasing their false positive count \citep{he2013imbalanced}. Moveover, standard classification algorithms are often biased towards the majority English class, leading to a higher misclassification rate instances belonging to minority classes, in the present case all other languages \citep{lopez2013insight, liu2006boosting,huang2006intelligent}. In the specific context of the OpenAlex database and as pointed out in \citet{cespedes2025evaluating}, classifying article language in the OpenAlex database thus involves a trade-off between precision performance for English and retrieval performance for all other languages: a lower precision score for English will result in a better performance overall across all metrics, at the expense however of an underestimation of all other languages. This is all the more troublesome as the minority classes, i.e. all non-English languages in the present project, are usually those that are the most crucial from a classification perspective \citep{lopez2013insight}. In the context of the current project, it could also be argued that these minority classes are actually the most interesting ones, as they are what motivates the linguistic classification in the first place.

These imbalanced-related issues cannot be addressed directly here, as the procedures evaluated here are both built on algorithms and applied to samples that were respectively trained and collected beforehand. However, it is possible to get a comparative understanding of the impact of database-level trade-offs between precision and recall for any procedure through weighted harmonic aggregation of the scores it obtained throughout the simulation. To do this and in order to also take into account processing times for each procedure, the following generalization of the $F_{\beta}$ score formula is used here, formula which adds a $\gamma$ parameter to control the relative weight of processing speed relative to precision and recall:

\begin{align}
F_{\beta,\gamma} &= (1 + \beta^2 + \gamma^2) \cdot \frac{precision \cdot recall \cdot speed}{(\beta^2 \cdot precision) + recall + (\gamma^2 \cdot speed)}
\end{align}

Regarding $\beta$ values, $\beta > 1$ and $\beta < 1$, allows $F_{\beta,\gamma}$ to respectively confer more or less importance to recall in relation to precision, with lower and upper bounds $\beta = 0.5$ and $\beta = 2$ respectively conferring to recall half and twice the importance of precision. As for $\gamma$,  $\gamma = 0$ excludes speed from $F_{\beta,\gamma}$, $\gamma = 1$ confers equal weight to both precision and speed, while lower and upper bounds $\gamma = 0.5$ and $\gamma = 2$ makes speed half and twice as important as precision, in that order. Given this, $\gamma = 0$, allows $F_{\beta,\gamma}$ becomes formally equivalent to $F_\beta$, while setting $\beta = 1$ and $\gamma = 1$ makes $F_{\beta,\gamma}$ return the harmonic mean of precision, recall and speed. \citep{sasaki2007truth}. 

Overall simulated performance for each procedure is then obtained by calculating $F_\beta,\gamma$ scores obtained by each procedure at each simulation step for a set of various $\beta$ values ranging from $0.5$ (precision is twice as important as recall) to $2$ (recall is twice as important as precision) as well as 4 different values of $gamma$: 0 (speed is ignored altogether), 0.5 (speed is twice as important as recall), 1 (precision and speed are equally important), 2 (precision is twice as important as speed). Following these aggregative computations, two two different performance measures are then computed: \textit{maximum a posteriori} estimates, which correspond to the mode of the posterior density obtained for each procedure, and win ratios, which are computed by dividing the number of times each procedure gets the best aggregated score by the total number of simulation iterations.

Finally, \textit{maximum a posteriori} estimates for each procedure and every $F_{\beta,\gamma}$  weighing configuration are obtained by extracting the mode of every corresponding posterior density distribution.


\section{Results and Analyses}

In this section, the predictive power of the different linguistic classification procedures implemented is first assessed at the sample level: precision and recall scores for the different languages investigated are presented and analyzed, followed by a comparison of the processing speeds recorded by each algorithm over the different corpus types. Finally, the highest-performing procedures for each weighing regime are presented and compared.

\subsection{Performance by Language}

The various scatter plots included in Figure \ref{fig:perf} show performance scores obtained for each language and procedure. In order to simply both visualization and interpretation, only best-performing procedures in either precision or recall were considered for each algorithm. In case of equal score between two or more procedures using the same algorithm, only the best-performing procedure as regards to the other measure was kept (i.e. best recall score for procedures with equal precision score or best precision score for procedures of equal recall score). Additionally, in case of equal precision and recall scores between two or more procedures using the same algorithm, only the one based on the shorter, simpler, and faster-processable corpus was kept (with Titles < Titles \& Journal Names < Titles \& Abstracts < Greedy). Finally, in cases where a procedure from a given algorithm and language obtains the best score for both measures, no other procedures for that algorithm and languages are represented. Precision and recall scores for all procedures implemented for this study can be found in appendix \ref{appendix:perf}.

In each subplot, procedure performance is represented by points whose X and Y coordinates respectively refer to the precision and recall scores of each corresponding procedure. The different algorithms and corpus types uniquely characterizing each procedure are for their part represented by distinct colors and letter tags, in that order. Regarding the color coding of algorithms in particular, given the fact that FastText and FastSpell algorithms perform identically for most linguistic categories considered, a different and common color was used for all languages in which performance scores were identical for both algorithms (i.e. all categories but Other languages). Finally, in order to compare overall linguistic performances, harmonic averages of precision and recall scores by language for all procedures implemented for this project (those described in appendix \ref{appendix:procedures}) are represented in each subplot by horizontal and vertical dashed lines respectively.

\afterpage{
\begin{figure}[h!]
\centering
\includegraphics[width=.85\textwidth]{figures/lang_perf.png}
\caption{Proportion and Recall Scores for all Procedures, grouped by Language and Corpus Type}
\label{fig:perf}
\end{figure}
}

As the different subplots show, procedure performance varies both substantially and differently across the different subplots, with each algorithm or corpus type performing more or less well depending on the specific circumstances of each language subset evaluated. Looking at procedure scores in general, language averages for precision are higher than their recall counterparts for all languages, with the sole exception of articles written in English and Other languages. Comparisons across languages also show that average precision is highest for Russian and Indonesian articles, while the lowest scores belong to articles written in English and Other languages; as for recall, mean procedure scores are highest for French, Italian, and Portuguese articles, and are lowest on average in the case of Korean, Japanese, Chinese, and Russian articles. Overall, the performances obtained by the assessed procedures in all language categories confirm the well-known inverse relationship between precision and recall: use of the two-sided Wetzels Bayes Factor shows that both performance measures are inversely correlated at a rate of $\rho = -.18$, with the observed data supporting the presence of correlation over the null hypothesis at a ratio of 127437 to 1 (Wetzels et al., 2012).

With regard to the different types of corpus implemented, all corpus types have procedures with above-average precision and recall scores in at least one language. While the various filtering procedures described above prevent any in-depth analysis of performance by corpus types, much more can however be said of performances by algorithms and languages. First and foremost, the first quadrant of the plane determined by the two perpendicular lines representing the precision and recall averages include points in all cases, which means that every language includes at least one procedure which is above average in both precision and recall. Moreover, each algorithm has procedures that score above both precision and recall average in at least language: while FastText and FastSpell have at least one procedure with above-average precision and recall scores in all languages but Spanish and Russian, Spanish is the only language where the CLD3 algorithm scores above average in both precision and recall. In addition, two algorithms (Langdetect for articles in English, Chinese, and Other languages as well as CLD3 for Japanese and Other languages) have procedures whose precision and recall scores are both below language average.

As regards to each measure considered separately, no single algorithm has above-average precision scores in all languages, while FastSpell and FastText are the only algorithms with at least one above-average recall score in each language category. These two algorithms are also those that stand out the most across languages, with procedures performing above precision or recall average in all languages except Spanish and Russian. In the case of French, Indonesian, and Italian articles, FastSpell and FastText are actually the only algorithm whose procedures score above average for both measures. In the case of Other language category, which is the only one in which the performance of both algorithms differ, FastText performs markedly better in terms of precision, while recall scores of FastSpell are slightly higher than those of FastText; for all other languages, the additional features of Fast Spell have no impact other than increasing the space and time complexity of the procedures involved.

Finally, a note regarding English-written articles, which account for the vast majority of articles indexed in the database. FastText, FastSpell, and to a lower extent LangID are the only algorithms recording above-average scores in both precision and recall; besides these algorithms, DetectLanguage and CLD2 have procedures that score above-average in terms of recall, while CLD3 is the only other algorithm with above-average precision scores. The highest precision score is recorded by the LangID algorithm on the Greedy corpus, while the best recall performance is obtained by applying the DetectLanguage algorithm on the Titles \& Journal Names corpus. However, as both procedures have relatively low scores in the case of the other measure, it can be concluded that procedures based on the FastSpell and FastText algorithms (in conjunction with the one consisting in the use of the LangID on the Titles corpus, but to a lesser extent) are those whose overall performance stand out the most for English. Given the hegemony of that language in scientific publishing and its prevalence in the database, we should also expect these procedures to perform relatively well in the large-scale simulation that follows. However, in light of the peculiarities mentioned in the Methods section regarding the precision-recall trade-offs in imbalanced datasets, the linguistic classification performance of English-written articles cannot be assessed alone, independently of those of all other languages present in the database. On the contrary, a linguistic classification of all articles indexed in the OpenAlex database can only be effective insofar at it integrates all languages and weighs its impact on each of them in a way similar in scope and aim to the simulation conducted here and whose results are presented and discussed below.

\subsection{Processing Times}

Figure \ref{fig:speed} shows the normalized processing times for the different algorithms and corpus types; a table showing the absolute processing times recorded for all classification procedures implemented can be found in appendix \ref{appendix:speed}. To improve plot readability, vertical line segments connecting all processing times obtained by each algorithm for the various corpora have been added. Fastest and slowest processing times for each corpus type are shown in seconds at both extremities of the x-axis. Finally, in order to better highlight the differences between both slower and faster algorithms, the x-axis has been rescaled using arcsine transformation.

\afterpage{
\begin{figure}[h!]
\centering
\includegraphics[width=.85\textwidth]{figures/proc_times.png}
\caption{Normalized Processing Times by Algorithm and Corpus}
\label{fig:speed}
\end{figure}
}

In terms of processing times, the algorithms implemented and evaluated in this project form three distinct groups whose performances remain consistent across the different corpus types. Langdetect, LangID and DetectLanguage are by far the slowest algorithms in the group, with respective performances of 38.24, 18.61,  and 13.07 seconds for all four corpora assessed. The fastest group includes FastText, FastSpell and CLD2, whose overall performances of 0.44, 0.46, and 0.47 seconds are two orders of magnitude faster than those of the first group, for all corpus types. The third and last group solely consists of CLD3, whose overall performance of 1.29 seconds is an order of magnitude faster and slower than the first and second groups respectively.

In the context of the present project, such differences in performance makes processing speed a crucial dimension of linguistic classification performance. Indeed, while a difference of a few seconds at the sample level may not seem so crucial at first glance, the sheer size of OpenAlex implies that processing speed differences beyond one order of magnitude might likely result in delays ranging from weeks to even months. 

Regarding the performances of FastSpell and FastText in particular, the former is slightly slower than the latter in all cases, which can be expected given that FastSpell adds an upstream spelling checking functionality to FastText. As shown in the previous figure, this enhancement translates into a gain in recall performance for languages included in the Other category. The question remains, however, whether these local gains in performance are worth the cost in terms of increased processing times.

As for DetectLanguage, the fact that the algorithm can only be used through API queries has an undeniable impact on the processing speed of all procedures using this algorithm: even though the algorithm's batch mode helps reduce the number of queries by processing multiple inputs at a time, the fact that its processing speed ultimately depends on factors that have nothing to do with algorithmic performance /textit{stricto sensu}, for example connection speed or server response time, unfortunately makes it an unviable language detection strategy for the OpenAlex database.

To a lesser extent, differences in processing times can also be observed between the various corpus types. Across all algorithms, the Titles (T) and Titles \& Abstracts (A) corpora are the corpus types whose processing were the fastest and slowest respectively, with times of $15.98$ and $20.64$ seconds. Average document size can probably and partly explain such difference: since titles tend to be substantially shorter than abstracts, with $72.26$ characters on average to $1238.85$ characters for the latter, corpora that include abstracts will take more time to process than those that do not. From a performance evaluation perspective, a situation similar the one observed for FastSpell and FastText thus arises: while abstracts can be very informative from a language detection perspective, taking them into account for that purpose is however bound to slow down algorithms; whether this gain in information results in an overall gain of performance will ultimately depend on its impact on precision and recall scores, which once again proves the crucial relevance of corpus processing times to the present language detection algorithm assessment.

\subsection{Simulation Results}

Figure \ref{fig:sim} shows the maximum Maximum $F_{\beta\gamma}$ MAP estimate obtained by each algorithm for every combination of precision ($\beta$) and speed ($\gamma$) weights. Each subfigure presents the results obtained for every $\gamma$ value implemented, while $F_{\beta, \gamma}$ scores and $\beta$ weights obtained for any given $\gamma$ value are respectively shown along the x- and y-axes of the corresponding subfigure. For each subfigure, corpus types and algorithms are uniquely and respectively represented using distinct colors and line styles. Finally, subfigures representing weighing regimes that include processing times ($\gamma > 0$) are grouped together horizontally to increase readability.


\afterpage{
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{figures/map_estimates.png}
\caption{Maximum $F_{\beta\gamma}$ MAP Estimate for each algorithm over selected $\beta$ and $\gamma$ values}
\label{fig:sim}
\end{figure}
}

Looking first at the topmost subfigure, which shows $F_{\beta, \gamma}$ scores for $\gamma = 0$ (where is not taken into account), best-performing procedures for each algorithm show interesting similarities. First and foremost, performance for all algorithms seem to follow a dual performance regime, with a transition point at around $ \beta \approx 1$, in other words where the weights of precision and recall converge. In the case of DetectLanguage and LangID, this cut-off point occurs a bit earlier ($\beta \approx 0.6$) and later ($\beta \approx 1.2$) along the $\beta$ continuum, but the regime shift is just as sudden and lasting as elsewhere. The first characteristic of that shift is that that all algorithms switch corpus types: whereas the best-performing procedure of CLD3 transitions from the Titles \& Abstracts to the Greedy corpus (i.e. it adds journal names), procedures for all other algorithms become title-based as recall weight increases. The second salient feature of the dual-regime observed at $\gamma = 0$ is that, again with the exception of the exponential decay of CLD3, which seems unaffected by recall weight increase, all best-performing procedures switch to an exponentially-decreasing growth regime as they hit their respective cut-off points and transitions to the titles-based corpus. 

While the performance regimes of best-performing procedures in more precision-oriented settings ($\beta < 1$) are not as striking uniformity as the recall-oriented regimes, some shared properties can nonetheless be observed. In particular, three different groups can be identified based on their corpus types and/or slopes. The first group includes two procedures: a first one that uses LangID on the greedy corpus, and a second one using CLD3 on the Titles \& Abstracts corpus. Showing higher scores than the other procedures, these two procedures however see their performance decrease steadily as greater weight is gradually conferred to recall. The second algorithm group includes FastText and FastSpell, whose above-average performance on the greedy corpus decreases slightly as precision weight gets closer $\beta = 1$. A third group of algorithms that comprises CLD2 and Langdetect is characterized by below-average, yet steadily increasing greedy-corpus-based performances below $\beta < 1$. Finally, the best-performing procedure of DetectLanguage in precision-oriented settings stand out by having the lowest scores and cut-off point of all algorithms, but also the most marked growth of all algorithms, both before and after the cut-off point.

As regards to the other subfigures, i.e. those showing scores that take processing speed into account ($\gamma > 0$), all give a unanimous picture of procedure performance. First, best-performing procedures of each algorithm follow a single, title-based regime for the whole $.5 <= \beta <= 2$ range. But also and perhaps more importantly, CLD2, FastSpell, and especially FastText score markedly higher than all other algorithms, and this outperformance grows substantially as both $\beta$ and $\gamma$ values increase.

In light of these considerations and the simulation results from which they are derived, which classification procedure performs best depends on the importance given to the different evaluation measures implemented. As regards to corpus types, emphasizing precision over recall will favor the more metadata-rich procedures (i.e. the greedy and titles \& abstracts corpora); however, as recall gains in importance and as soon as processing speed is taken into account, the Titles corpus becomes the unanimous choice. As regards to algorithms, while LangID stands out in precision-oriented, speed-overlooking settings, FastSpell and FastText outperforms other algorithms in all other contexts, and the extent of their predominance increases proportionally with both $\beta$ and $\gamma$.

\section{Discussion}

The present project aimed to systematically compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. 

In addition to the quality of its overall performance, FastText has the advantage of returning as output the probability value of the most probable language. In this sense, a two- or multi-step classification procedure would be quite feasible. For instance, FastText could be run on the titles of all articles indexed in the database, then other procedures that perform better in specific contexts could be run on all cases where the probability/confidence score of the detected language is below a certain threshold. Given the imbalanced nature of the OpenAlex database and the inherent algorithmic tendency to overpredict the majority class, a special effort could then be made to improve prediction for English documents: as mentioned in \citet{cespedes2025evaluating}, English documents are overrepresented in the OpenAlex database at a rate of around 9\%, which negatively affects the representativity of all other languages in the database to a greater or lesser extent, but also reinforces their underrepresentation in the scholarly and knowledge ecosystem at large. In that perspective and given OpenAlex's status as the first truly multilingual database, improving the database's language metadata has social and political implications that go well beyond the functionalities that are normally expected of bibliographic databases.

\subsection{Limitations of the study}

Results presented and discussed in the previous paragraphs are far from definitive.  For once, they are inextricably tied to the current state-of-the-art in the field of automatic language detection. The conclusions drawn are in no way meant to solve the problem of the linguistic classification of OpenAlex articles once and for all, but only to attempt to offer the best possible algorithmic recommendation based on current domain knowledge as well as on the available techniques deemed most suitable for that specific purpose.

Another important point to mention is that since the observations and analyses presented here are the direct result of the methodological choices adopted by the authors, the conclusions derived from the results of this project are valid only within the framework of this precise \textit{modus operandi}. In other words, different choices as regards to the selection of performance measures or the design and parametrization of the simulation process may well have led to different conclusions. The case of the aggregation of performance scores performed here is suggestive in this regard, as increased differences in weighing between precision, recall, and processing speed could lead to different results;  the question remains however as to whether more diversified or extreme weighing regimes than those presented here would be really useful. Nevertheless, a certain epistemic caution is in order here, as the results presented here are far from definitive and should not be considered as such.

As regards to OpenAlex itself, it is also important to note that a significant part of the data contained in the database is provided by the journal publishers themselves based on what they consider relevant bibliometric data. Such decisional idiosyncrasies can have an impact on the performance of monolingual language detection algorithms and the language classification they provide, for example if multilingual titles or abstracts are provided for specific journals or articles. This specific scenario in turn points to an important consideration regarding the functionalities of the algorithms considered here, but also those of language detection algorithms in general. While current monolingual language detection algorithms offer above human-level performance over hundreds of different languages, a current and major challenge in Natural Language Processing consists in designing algorithms that achieve high accuracy when dealing with documents written in multiple languages. Given these considerations, multilingual record entries probably represents the biggest current linguistic classification issue for the OpenAlex database: not only is the quality of linguistic metadata inevitably affected by this problem, but its extent is hard to assess, given the difficulty of identifying which attributes are multilingual and as such likely to cause misclassifications.

Despite these limitations, the results presented here show that, with the right language detection procedure, a high-quality linguistic classification of all articles indexed in OpAl is nevertheless possible, classification which would not only reinforce the latter's status as the first truly multilingual, large-scale bibliometric database, but also and more generally foster cross-linguistic bibliometric assessments and analyses of unmatched accuracy and comprehensiveness.


\bibliographystyle{apalike}
%\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{manuscript.bib}

\clearpage
\begin{appendices}

\section{OpenAlex linguistic distribution, grouped by metadata availability}
\label{appendix:distribution}

The following table shows article frequency by metadata availability for the different language categories used in this project, as of December 25th, 2024.

\begin{table}[h!]
\centering
\begin{tabular}{r|cccc}
\hline
\textbf{Language} & \textbf{Title} & \makecell{\textbf{Title \&} \\\textbf{Abstract}} & \makecell{\textbf{Title \&}\\\textbf{Journal Name}} & \makecell{\textbf{Title, Abstract \&}\\\textbf{Journal Name}}\\
\hline
de & 615 864 & 208 947 & 1 981 603 & 1 154 153\\
en & 17 915 165 & 16 847 352 & 43 085 593 & 69 649 893\\
es & 1 918 418 & 689 230 & 2 207 249 & 1 948 762\\
fr & 1 010 112 & 501 142 & 2 096 853 & 1 209 886\\
id & 308 631 & 402 811 & 113 823 & 1 332 430\\
it & 762 538 & 67 312 & 514 968 & 107 421\\
ja & 952 769 & 6 185 & 6 834 274 & 214 832\\
ko & 600 926 & 87 884 & 2 446 512 & 1 140 439\\
pt & 657 215 & 409 953 & 476 663 & 1 588 976\\
ru & 360 858 & 79 011 & 1 175 999 & 597 653\\
zh & 100 936 & 188 780 & 912 446 & 3 472 426\\
other & 2 050 859 & 818 517 & 2 922 167 & 1 724 352\\
unknown & 930 370 & 14 522 & 3 863 730 & 246 058\\
\hline
\end{tabular}
\end{table}
\clearpage

\section{Pseudocode description for corpus building procedures}
\label{appendix:procedures}

The following table shows the name, symbol, and procedure used to build the different corpora used for this project.

\begin{table}[h!]
\centering
\begin{tabular}[h!]{rcl}
\hline
\textbf{Name} & \textbf{Symbol} & \textbf{Procedure}\\
\hline\\[-.15cm]
Title & T & \makecell[l]{\textbf{FOREACH} article, \textbf{GET} title.}\\[.15cm]
\makecell[tr]{Title \& abstract,\\else title} & TAeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} (title, abstract);\\\textbf{IF NO} abstract, \textbf{GET} title.}\\[.75cm]
Abstract, else title & AeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} abstract;\\\textbf{IF NO} abstract, \textbf{GET} title.} \\[.75cm]
\makecell[tr]{Title \& journal name,\\else title} & TJeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} (title, journal name);\\ \textbf{IF NO} journal name, \textbf{GET} title.}\\[.75cm]
\makecell[tr]{Journal name,\\else title} & JeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} journal name;\\ \textbf{IF NO} journal name, \textbf{GET} title.}\\[.75cm]
\makecell[tr]{Abstract,\\else journal name,\\else title} & AeJeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} abstract;\\ \textbf{IF NO} abstract, \textbf{GET} journal name;\\\textbf{IF NO} journal name, \textbf{GET} title.}\\[1.25cm]
\makecell[tr]{Journal name,\\else abstract,\\else title} & JeAeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} journal name;\\\textbf{IF NO} journal name, \textbf{GET} abstract;\\\textbf{IF NO} abstract, \textbf{GET} title.}\\[1.25cm]
\makecell[tr]{Abstract \& journal name,\\else title}& AJeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} (abstract, journal name);\\\textbf{IF} (\textbf{NO} abstract \textbf{OR NO} journal name), \textbf{GET} title.}\\[.75cm]
\makecell[tr]{Title \& abstract,\\else title \& journal name,\\else title}& TAeTJeT & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} (title, abstract);\\\textbf{IF NO} abstract, \textbf{GET} (title, journal name);\\ \textbf{IF NO} journal name, \textbf{GET} title.}\\[1.25cm]
\makecell[tr]{Title \& journal name,\\else title \& abstract,\\else title} & \makecell{TJeTAeT} & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET} (title, journal name);\\\textbf{IF NO} journal name, \textbf{GET} (title, abstract);\\\textbf{IF NO} abstract, \textbf{GET} title.}\\[1.25cm]
Greedy & Greedy & \makecell[tl]{\textbf{FOREACH} article, \textbf{GET}(title, abstract, journal name);\\\textbf{IF NO} journal name, \textbf{GET} (title, abstract);\\\textbf{IF NO} abstract, \textbf{GET} (title, journal name);\\ \textbf{IF} (\textbf{NO} abstract \textbf{\&} \textbf{NO} journal name), \textbf{GET} title.}\\[1.35cm]
\hline
\end{tabular}
\caption{Description of the different corpus types}
\label{table1}
\end{table}

\clearpage


\section{Processing times}
\label{appendix:speed}

The following table shows processing times for each algorithm and corpus, in elapsed seconds.

\begin{table}[h!]
\centering
\begin{tabular}{r|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{
Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
T & 0.07 & 0.20 & 2.78 & 0.07 & 0.07 & 3.82 & 8.96\\
AeT & 0.12 & 0.40 & 3.67 & 0.14 & 0.13 & 6.86 & 10.06\\
TAeT & 0.19 & 0.40 & 3.62 & 0.15 & 0.14 & 5.99 & 10.15\\
JeT & 0.07 & 0.18 & 3.22 & 0.07 & 0.07 & 4.05 & 9.81\\
TJeT & 0.07 & 0.24 & 2.74 & 0.08 & 0.07 & 3.90 & 8.94\\
AeJeT & 0.11 & 0.44 & 3.63 & 0.15 & 0.14 & 4.32 & 9.97\\
JeAeT & 0.08 & 0.23 & 2.99 & 0.09 & 0.08 & 3.89 & 10.31\\
TAeTJeT & 0.12 & 0.42 & 3.71 & 0.16 & 0.15 & 5.04 & 10.46\\
TJeTAeT & 0.09 & 0.27 & 2.86 & 0.10 & 0.10 & 4.11 & 9.43\\
AJeT & 0.12 & 0.45 & 3.61 & 0.15 & 0.15 & 4.20 & 10.17\\
Greedy & 0.13 & 0.45 & 3.93 & 0.16 & 0.16 & 4.91 & 10.18\\
\hline
\end{tabular}
\end{table}


\clearpage

\section{Performance by Language}
\label{appendix:perf}

The following tables show precision and recall scores for the different languages, grouped by corpus and algorithm.


\subsection{Chinese}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.87 & 0.68 & 0.86 & 0.82 & 0.82 & 0.72 & 0.74\\

AeJeT & 0.87 & 0.68 & 0.86 & 0.82 & 0.82 & 0.72 & 0.75\\

AeT & 0.87 & 0.72 & 0.87 & 0.84 & 0.84 & 0.77 & 0.72\\

Greedy & 0.87 & 0.75 & 0.86 & 0.84 & 0.84 & 0.76 & 0.76\\

JeAeT & 0.88 & 0.65 & 0.86 & 0.82 & 0.82 & 0.71 & 0.76\\

JeT & 0.90 & 0.66 & 0.87 & 0.83 & 0.83 & 0.72 & 0.76\\

T & 0.98 & 0.77 & 0.98 & 0.93 & 0.93 & 0.84 & 0.78\\

TAeT & 0.87 & 0.75 & 0.86 & 0.84 & 0.84 & 0.77 & 0.74\\

TAeTJeT & 0.87 & 0.75 & 0.87 & 0.84 & 0.84 & 0.76 & 0.77\\

TJeT & 0.99 & 0.76 & 0.98 & 0.94 & 0.94 & 0.81 & 0.81\\

TJeTAeT & 0.97 & 0.76 & 0.96 & 0.92 & 0.92 & 0.80 & 0.82\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.73 & 0.76 & 0.67 & 0.75 & 0.75 & 0.80 & 0.38\\

AeJeT & 0.73 & 0.76 & 0.67 & 0.75 & 0.75 & 0.80 & 0.40\\

AeT & 0.76 & 0.76 & 0.69 & 0.77 & 0.77 & 0.81 & 0.42\\

Greedy & 0.76 & 0.71 & 0.70 & 0.77 & 0.77 & 0.82 & 0.37\\

JeAeT & 0.64 & 0.73 & 0.61 & 0.67 & 0.67 & 0.73 & 0.32\\

JeT & 0.63 & 0.73 & 0.60 & 0.66 & 0.66 & 0.73 & 0.33\\

T & 0.69 & 0.70 & 0.62 & 0.69 & 0.69 & 0.77 & 0.34\\

TAeT & 0.75 & 0.70 & 0.70 & 0.77 & 0.77 & 0.81 & 0.39\\

TAeTJeT & 0.76 & 0.70 & 0.70 & 0.77 & 0.77 & 0.82 & 0.38\\

TJeT & 0.70 & 0.70 & 0.63 & 0.70 & 0.70 & 0.78 & 0.29\\

TJeTAeT & 0.71 & 0.70 & 0.64 & 0.71 & 0.71 & 0.78 & 0.30\\
\hline
\end{tabular}
\end{table}
\clearpage

\subsection{English}

\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.69 & 0.76 & 0.67 & 0.73 & 0.73 & 0.73 & 0.70\\
AeJeT & 0.69 & 0.76 & 0.67 & 0.73 & 0.73 & 0.73 & 0.70\\
AeT & 0.73 & 0.78 & 0.70 & 0.77 & 0.77 & 0.78 & 0.73\\
Greedy & 0.72 & 0.78 & 0.70 & 0.77 & 0.77 & 0.79 & 0.72\\
JeAeT & 0.64 & 0.68 & 0.64 & 0.69 & 0.69 & 0.67 & 0.65\\
JeT & 0.64 & 0.69 & 0.64 & 0.69 & 0.69 & 0.67 & 0.65\\
T & 0.71 & 0.74 & 0.69 & 0.74 & 0.74 & 0.75 & 0.72\\
TAeT & 0.73 & 0.78 & 0.70 & 0.77 & 0.77 & 0.78 & 0.74\\
TAeTJeT & 0.72 & 0.78 & 0.70 & 0.77 & 0.77 & 0.78 & 0.73\\
TJeT & 0.70 & 0.75 & 0.69 & 0.75 & 0.75 & 0.76 & 0.70\\
TJeTAeT & 0.70 & 0.75 & 0.69 & 0.75 & 0.75 & 0.76 & 0.71\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.79 & 0.68 & 0.88 & 0.86 & 0.86 & 0.80 & 0.73\\
AeJeT & 0.79 & 0.68 & 0.88 & 0.86 & 0.86 & 0.80 & 0.73\\
AeT & 0.78 & 0.69 & 0.88 & 0.85 & 0.85 & 0.79 & 0.72\\
Greedy & 0.81 & 0.71 & 0.89 & 0.86 & 0.86 & 0.81 & 0.76\\
JeAeT & 0.74 & 0.55 & 0.87 & 0.85 & 0.85 & 0.76 & 0.66\\
JeT & 0.75 & 0.55 & 0.88 & 0.86 & 0.86 & 0.77 & 0.66\\
T & 0.83 & 0.70 & 0.92 & 0.90 & 0.90 & 0.83 & 0.75\\
TAeT & 0.78 & 0.69 & 0.88 & 0.86 & 0.86 & 0.80 & 0.73\\
TAeTJeT & 0.81 & 0.71 & 0.89 & 0.86 & 0.86 & 0.81 & 0.76\\
TJeT & 0.85 & 0.73 & 0.93 & 0.91 & 0.91 & 0.83 & 0.79\\
TJeTAeT & 0.84 & 0.73 & 0.92 & 0.90 & 0.90 & 0.83 & 0.79\\
\hline
\end{tabular}
\end{table}
\clearpage

\subsection{French}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.88 & 0.81 & 0.92 & 0.84 & 0.84 & 0.77 & 0.74\\
AeJeT & 0.88 & 0.81 & 0.92 & 0.84 & 0.84 & 0.77 & 0.75\\
AeT & 0.89 & 0.83 & 0.91 & 0.86 & 0.86 & 0.79 & 0.76\\
Greedy & 0.90 & 0.84 & 0.92 & 0.86 & 0.86 & 0.82 & 0.76\\
JeAeT & 0.93 & 0.82 & 0.94 & 0.87 & 0.87 & 0.72 & 0.69\\
JeT & 0.94 & 0.83 & 0.94 & 0.88 & 0.88 & 0.72 & 0.70\\
T & 0.96 & 0.89 & 0.95 & 0.92 & 0.92 & 0.86 & 0.81\\
TAeT & 0.88 & 0.84 & 0.92 & 0.86 & 0.86 & 0.81 & 0.76\\
TAeTJeT & 0.89 & 0.84 & 0.91 & 0.86 & 0.86 & 0.82 & 0.77\\
TJeT & 0.96 & 0.89 & 0.95 & 0.92 & 0.92 & 0.86 & 0.82\\
TJeTAeT & 0.95 & 0.89 & 0.95 & 0.91 & 0.91 & 0.86 & 0.80\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.75 & 0.80 & 0.77 & 0.88 & 0.88 & 0.85 & 0.85\\
AeJeT & 0.75 & 0.80 & 0.77 & 0.88 & 0.88 & 0.85 & 0.85\\
AeT & 0.78 & 0.85 & 0.77 & 0.90 & 0.90 & 0.89 & 0.88\\
Greedy & 0.77 & 0.85 & 0.78 & 0.91 & 0.91 & 0.89 & 0.89\\
JeAeT & 0.70 & 0.76 & 0.77 & 0.86 & 0.86 & 0.84 & 0.84\\
JeT & 0.69 & 0.75 & 0.77 & 0.86 & 0.86 & 0.84 & 0.84\\
T & 0.76 & 0.84 & 0.78 & 0.89 & 0.89 & 0.89 & 0.89\\
TAeT & 0.78 & 0.85 & 0.77 & 0.90 & 0.90 & 0.89 & 0.89\\
TAeTJeT & 0.77 & 0.85 & 0.78 & 0.91 & 0.91 & 0.89 & 0.89\\
TJeT & 0.75 & 0.84 & 0.79 & 0.90 & 0.90 & 0.89 & 0.90\\
TJeTAeT & 0.76 & 0.85 & 0.79 & 0.91 & 0.91 & 0.89 & 0.89\\
\hline
\end{tabular}
\end{table}
\clearpage



\subsection{German}

\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.88 & 0.82 & 0.87 & 0.86 & 0.86 & 0.79 & 0.85\\
AeJeT & 0.87 & 0.82 & 0.87 & 0.86 & 0.86 & 0.79 & 0.84\\
AeT & 0.87 & 0.82 & 0.88 & 0.87 & 0.87 & 0.80 & 0.84\\
Greedy & 0.88 & 0.83 & 0.89 & 0.87 & 0.87 & 0.81 & 0.86\\
JeAeT & 0.87 & 0.81 & 0.86 & 0.84 & 0.84 & 0.78 & 0.83\\
JeT & 0.88 & 0.82 & 0.86 & 0.84 & 0.84 & 0.78 & 0.83\\
T & 0.93 & 0.86 & 0.91 & 0.91 & 0.91 & 0.83 & 0.86\\
TAeT & 0.88 & 0.83 & 0.89 & 0.87 & 0.87 & 0.80 & 0.85\\
TAeTJeT & 0.88 & 0.84 & 0.89 & 0.87 & 0.87 & 0.81 & 0.86\\
TJeT & 0.93 & 0.86 & 0.92 & 0.91 & 0.91 & 0.83 & 0.89\\
TJeTAeT & 0.92 & 0.85 & 0.91 & 0.90 & 0.90 & 0.83 & 0.89\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\\hline
AJeT & 0.73 & 0.79 & 0.81 & 0.85 & 0.85 & 0.83 & 0.75\\
AeJeT & 0.72 & 0.78 & 0.81 & 0.85 & 0.85 & 0.83 & 0.75\\
AeT & 0.76 & 0.81 & 0.84 & 0.89 & 0.89 & 0.87 & 0.78\\
Greedy & 0.76 & 0.84 & 0.85 & 0.89 & 0.89 & 0.88 & 0.79\\
JeAeT & 0.70 & 0.76 & 0.80 & 0.84 & 0.84 & 0.82 & 0.74\\
JeT & 0.67 & 0.73 & 0.77 & 0.82 & 0.82 & 0.80 & 0.70\\
T & 0.73 & 0.78 & 0.82 & 0.86 & 0.86 & 0.86 & 0.75\\
TAeT & 0.76 & 0.82 & 0.84 & 0.89 & 0.89 & 0.87 & 0.79\\
TAeTJeT & 0.76 & 0.84 & 0.84 & 0.89 & 0.89 & 0.87 & 0.80\\
TJeT & 0.74 & 0.81 & 0.83 & 0.87 & 0.87 & 0.86 & 0.76\\
TJeTAeT & 0.76 & 0.84 & 0.85 & 0.89 & 0.89 & 0.88 & 0.80\\
\hline
\end{tabular}
\end{table}
\clearpage


\subsection{Indonesian}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.97 & 0.92 & 0.98 & 0.94 & 0.94 & 0.92 & 0.88\\
AeJeT & 0.97 & 0.92 & 0.97 & 0.94 & 0.94 & 0.92 & 0.88\\
AeT & 0.97 & 0.92 & 0.97 & 0.94 & 0.94 & 0.91 & 0.89\\
Greedy & 0.97 & 0.93 & 0.98 & 0.95 & 0.95 & 0.92 & 0.89\\
JeAeT & 0.97 & 0.87 & 0.97 & 0.93 & 0.93 & 0.91 & 0.87\\
JeT & 0.97 & 0.88 & 0.97 & 0.93 & 0.93 & 0.91 & 0.87\\
T & 0.98 & 0.95 & 0.99 & 0.96 & 0.96 & 0.93 & 0.90\\
TAeT & 0.97 & 0.93 & 0.98 & 0.94 & 0.94 & 0.91 & 0.88\\
TAeTJeT & 0.97 & 0.93 & 0.98 & 0.95 & 0.95 & 0.91 & 0.89\\
TJeT & 0.98 & 0.93 & 0.99 & 0.97 & 0.97 & 0.94 & 0.90\\
TJeTAeT & 0.98 & 0.93 & 0.99 & 0.97 & 0.97 & 0.94 & 0.90\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.74 & 0.77 & 0.70 & 0.86 & 0.86 & 0.86 & 0.87\\
AeJeT & 0.74 & 0.78 & 0.70 & 0.86 & 0.86 & 0.86 & 0.87\\
AeT & 0.74 & 0.78 & 0.70 & 0.86 & 0.86 & 0.86 & 0.88\\
Greedy & 0.78 & 0.78 & 0.74 & 0.87 & 0.87 & 0.86 & 0.90\\
JeAeT & 0.56 & 0.54 & 0.57 & 0.75 & 0.75 & 0.68 & 0.78\\
JeT & 0.55 & 0.53 & 0.57 & 0.75 & 0.75 & 0.67 & 0.80\\
T & 0.72 & 0.72 & 0.75 & 0.87 & 0.87 & 0.78 & 0.94\\
TAeT & 0.78 & 0.78 & 0.72 & 0.87 & 0.87 & 0.85 & 0.90\\
TAeTJeT & 0.78 & 0.78 & 0.73 & 0.87 & 0.87 & 0.85 & 0.89\\
TJeT & 0.69 & 0.71 & 0.75 & 0.88 & 0.88 & 0.82 & 0.93\\
TJeTAeT & 0.71 & 0.72 & 0.74 & 0.88 & 0.88 & 0.83 & 0.93\\
\hline
\end{tabular}
\end{table}


\clearpage
\subsection{Italian}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.96 & 0.79 & 0.93 & 0.91 & 0.91 & 0.81 & 0.67\\
AeJeT & 0.96 & 0.79 & 0.93 & 0.91 & 0.91 & 0.81 & 0.65\\
AeT & 0.95 & 0.79 & 0.96 & 0.91 & 0.91 & 0.81 & 0.66\\
Greedy & 0.97 & 0.82 & 0.96 & 0.92 & 0.92 & 0.83 & 0.70\\
JeAeT & 0.96 & 0.75 & 0.87 & 0.86 & 0.86 & 0.75 & 0.54\\
JeT & 0.97 & 0.76 & 0.87 & 0.86 & 0.86 & 0.75 & 0.55\\
T & 0.97 & 0.76 & 0.96 & 0.92 & 0.92 & 0.81 & 0.64\\
TAeT & 0.96 & 0.79 & 0.96 & 0.91 & 0.91 & 0.82 & 0.67\\
TAeTJeT & 0.97 & 0.82 & 0.96 & 0.92 & 0.92 & 0.83 & 0.68\\
TJeT & 0.98 & 0.80 & 0.95 & 0.93 & 0.93 & 0.82 & 0.67\\
TJeTAeT & 0.97 & 0.80 & 0.95 & 0.92 & 0.92 & 0.82 & 0.69\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.75 & 0.88 & 0.75 & 0.90 & 0.90 & 0.90 & 0.89\\
AeJeT & 0.75 & 0.88 & 0.75 & 0.90 & 0.90 & 0.90 & 0.88\\
AeT & 0.76 & 0.89 & 0.76 & 0.92 & 0.92 & 0.91 & 0.91\\
Greedy & 0.77 & 0.89 & 0.75 & 0.91 & 0.91 & 0.91 & 0.89\\
JeAeT & 0.72 & 0.85 & 0.74 & 0.88 & 0.88 & 0.89 & 0.88\\
JeT & 0.72 & 0.84 & 0.73 & 0.88 & 0.88 & 0.88 & 0.88\\
T & 0.75 & 0.87 & 0.76 & 0.91 & 0.91 & 0.90 & 0.90\\
TAeT & 0.77 & 0.89 & 0.76 & 0.92 & 0.92 & 0.91 & 0.92\\
TAeTJeT & 0.77 & 0.89 & 0.75 & 0.91 & 0.91 & 0.91 & 0.90\\
TJeT & 0.75 & 0.87 & 0.75 & 0.90 & 0.90 & 0.90 & 0.90\\
TJeTAeT & 0.76 & 0.88 & 0.76 & 0.90 & 0.90 & 0.90 & 0.90\\
\hline
\end{tabular}
\end{table}


\clearpage
\subsection{Japanese}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.87 & 0.82 & 0.86 & 0.86 & 0.86 & 0.87 & 0.86\\
AeJeT & 0.87 & 0.82 & 0.86 & 0.86 & 0.86 & 0.87 & 0.85\\
AeT & 0.87 & 0.83 & 0.87 & 0.86 & 0.86 & 0.87 & 0.86\\
Greedy & 0.87 & 0.84 & 0.87 & 0.86 & 0.86 & 0.88 & 0.86\\
JeAeT & 0.86 & 0.80 & 0.83 & 0.84 & 0.84 & 0.87 & 0.85\\
JeT & 0.86 & 0.80 & 0.83 & 0.83 & 0.83 & 0.87 & 0.86\\
T & 0.87 & 0.83 & 0.86 & 0.84 & 0.84 & 0.87 & 0.87\\
TAeT & 0.87 & 0.83 & 0.87 & 0.86 & 0.86 & 0.87 & 0.86\\
TAeTJeT & 0.87 & 0.83 & 0.87 & 0.86 & 0.86 & 0.88 & 0.86\\
TJeT & 0.87 & 0.83 & 0.87 & 0.84 & 0.84 & 0.88 & 0.86\\
TJeTAeT & 0.87 & 0.83 & 0.87 & 0.85 & 0.85 & 0.88 & 0.86\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.76 & 0.52 & 0.70 & 0.73 & 0.73 & 0.65 & 0.50\\

AeJeT & 0.76 & 0.52 & 0.70 & 0.73 & 0.73 & 0.65 & 0.50\\

AeT & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.52\\

Greedy & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.53\\

JeAeT & 0.76 & 0.52 & 0.70 & 0.73 & 0.73 & 0.66 & 0.49\\

JeT & 0.76 & 0.52 & 0.70 & 0.73 & 0.73 & 0.66 & 0.51\\

T & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.53\\

TAeT & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.52\\

TAeTJeT & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.52\\

TJeT & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.52\\

TJeTAeT & 0.78 & 0.55 & 0.72 & 0.76 & 0.76 & 0.71 & 0.52\\
\hline
\end{tabular}
\end{table}



\clearpage
\subsection{Korean}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.98 & 0.95 & 0.98 & 0.97 & 0.97 & 0.98 & 0.52\\

AeJeT & 0.98 & 0.95 & 0.98 & 0.97 & 0.97 & 0.98 & 0.53\\

AeT & 0.98 & 0.96 & 0.98 & 0.98 & 0.98 & 0.98 & 0.55\\

Greedy & 0.98 & 0.91 & 0.98 & 0.98 & 0.98 & 0.98 & 0.55\\

JeAeT & 0.98 & 0.94 & 0.98 & 0.97 & 0.97 & 0.98 & 0.45\\

JeT & 0.98 & 0.94 & 0.98 & 0.97 & 0.97 & 0.98 & 0.47\\

T & 0.98 & 0.94 & 0.98 & 0.98 & 0.98 & 0.98 & 0.52\\

TAeT & 0.98 & 0.96 & 0.98 & 0.98 & 0.98 & 0.98 & 0.54\\

TAeTJeT & 0.98 & 0.91 & 0.98 & 0.98 & 0.98 & 0.98 & 0.55\\

TJeT & 0.98 & 0.90 & 0.98 & 0.98 & 0.98 & 0.98 & 0.51\\

TJeTAeT & 0.98 & 0.91 & 0.98 & 0.98 & 0.98 & 0.98 & 0.50\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.71 & 0.65 & 0.71 & 0.72 & 0.72 & 0.73 & 0.69\\

AeJeT & 0.71 & 0.65 & 0.71 & 0.72 & 0.72 & 0.73 & 0.68\\

AeT & 0.71 & 0.65 & 0.70 & 0.72 & 0.72 & 0.73 & 0.67\\

Greedy & 0.70 & 0.56 & 0.71 & 0.72 & 0.72 & 0.73 & 0.66\\

JeAeT & 0.60 & 0.55 & 0.60 & 0.61 & 0.61 & 0.62 & 0.58\\

JeT & 0.60 & 0.55 & 0.60 & 0.61 & 0.61 & 0.62 & 0.57\\

T & 0.61 & 0.53 & 0.60 & 0.62 & 0.62 & 0.63 & 0.56\\

TAeT & 0.70 & 0.56 & 0.70 & 0.72 & 0.72 & 0.73 & 0.66\\

TAeTJeT & 0.70 & 0.56 & 0.71 & 0.72 & 0.72 & 0.73 & 0.66\\

TJeT & 0.61 & 0.52 & 0.61 & 0.62 & 0.62 & 0.63 & 0.56\\
\hline
TJeTAeT & 0.61 & 0.53 & 0.61 & 0.62 & 0.62 & 0.63 & 0.57\\
\hline
\end{tabular}
\end{table}



\clearpage
\subsection{Portuguese}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.95 & 0.92 & 0.85 & 0.92 & 0.92 & 0.93 & 0.89\\

AeJeT & 0.95 & 0.92 & 0.85 & 0.92 & 0.92 & 0.93 & 0.89\\

AeT & 0.97 & 0.92 & 0.85 & 0.93 & 0.93 & 0.94 & 0.90\\

Greedy & 0.96 & 0.92 & 0.87 & 0.94 & 0.94 & 0.94 & 0.92\\

JeAeT & 0.91 & 0.87 & 0.79 & 0.86 & 0.86 & 0.91 & 0.86\\

JeT & 0.92 & 0.89 & 0.80 & 0.87 & 0.87 & 0.93 & 0.85\\

T & 0.97 & 0.92 & 0.86 & 0.94 & 0.94 & 0.95 & 0.90\\

TAeT & 0.96 & 0.92 & 0.85 & 0.93 & 0.93 & 0.95 & 0.89\\

TAeTJeT & 0.96 & 0.92 & 0.87 & 0.94 & 0.94 & 0.94 & 0.91\\

TJeT & 0.97 & 0.93 & 0.87 & 0.95 & 0.95 & 0.95 & 0.92\\

TJeTAeT & 0.96 & 0.92 & 0.86 & 0.94 & 0.94 & 0.94 & 0.90\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.76 & 0.81 & 0.83 & 0.85 & 0.85 & 0.84 & 0.83\\

AeJeT & 0.76 & 0.81 & 0.83 & 0.85 & 0.85 & 0.84 & 0.82\\

AeT & 0.80 & 0.84 & 0.85 & 0.88 & 0.88 & 0.87 & 0.87\\

Greedy & 0.80 & 0.86 & 0.85 & 0.89 & 0.89 & 0.88 & 0.86\\

JeAeT & 0.68 & 0.70 & 0.79 & 0.76 & 0.76 & 0.75 & 0.77\\

JeT & 0.68 & 0.71 & 0.79 & 0.76 & 0.76 & 0.76 & 0.77\\

T & 0.77 & 0.83 & 0.87 & 0.87 & 0.87 & 0.88 & 0.87\\

TAeT & 0.81 & 0.85 & 0.85 & 0.88 & 0.88 & 0.88 & 0.86\\

TAeTJeT & 0.80 & 0.85 & 0.84 & 0.88 & 0.88 & 0.88 & 0.88\\

TJeT & 0.77 & 0.84 & 0.86 & 0.88 & 0.88 & 0.89 & 0.88\\
\hline
TJeTAeT & 0.77 & 0.84 & 0.86 & 0.88 & 0.88 & 0.89 & 0.89\\
\hline
\end{tabular}
\end{table}



\clearpage
\subsection{Russian}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.96 & 0.90 & 0.96 & 0.96 & 0.96 & 0.96 & 0.96\\

AeJeT & 0.96 & 0.90 & 0.96 & 0.96 & 0.96 & 0.96 & 0.96\\

AeT & 0.96 & 0.91 & 0.96 & 0.95 & 0.95 & 0.95 & 0.96\\

Greedy & 0.96 & 0.91 & 0.96 & 0.95 & 0.95 & 0.95 & 0.95\\

JeAeT & 0.97 & 0.87 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97\\

JeT & 0.97 & 0.87 & 0.97 & 0.97 & 0.97 & 0.97 & 0.97\\

T & 0.97 & 0.91 & 0.97 & 0.96 & 0.96 & 0.97 & 0.97\\

TAeT & 0.96 & 0.91 & 0.96 & 0.95 & 0.95 & 0.95 & 0.96\\

TAeTJeT & 0.96 & 0.91 & 0.96 & 0.95 & 0.95 & 0.95 & 0.95\\

TJeT & 0.97 & 0.91 & 0.97 & 0.96 & 0.96 & 0.96 & 0.96\\

TJeTAeT & 0.97 & 0.92 & 0.97 & 0.96 & 0.96 & 0.96 & 0.97\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.66 & 0.69 & 0.69 & 0.71 & 0.71 & 0.68 & 0.66\\

AeJeT & 0.67 & 0.70 & 0.69 & 0.72 & 0.72 & 0.68 & 0.67\\

AeT & 0.71 & 0.74 & 0.74 & 0.77 & 0.77 & 0.73 & 0.71\\

Greedy & 0.71 & 0.73 & 0.70 & 0.75 & 0.75 & 0.74 & 0.68\\

JeAeT & 0.52 & 0.58 & 0.55 & 0.57 & 0.57 & 0.54 & 0.53\\

JeT & 0.52 & 0.58 & 0.55 & 0.57 & 0.57 & 0.54 & 0.53\\

T & 0.58 & 0.61 & 0.61 & 0.63 & 0.63 & 0.59 & 0.58\\

TAeT & 0.71 & 0.74 & 0.71 & 0.76 & 0.76 & 0.73 & 0.69\\

TAeTJeT & 0.72 & 0.73 & 0.70 & 0.76 & 0.76 & 0.74 & 0.69\\

TJeT & 0.60 & 0.62 & 0.61 & 0.63 & 0.63 & 0.63 & 0.58\\

TJeTAeT & 0.60 & 0.62 & 0.61 & 0.63 & 0.63 & 0.63 & 0.58\\
\hline
\end{tabular}
\end{table}
\clearpage



\subsection{Spanish}

\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.94 & 0.92 & 0.90 & 0.86 & 0.86 & 0.84 & 0.87\\
AeJeT & 0.94 & 0.92 & 0.90 & 0.86 & 0.86 & 0.84 & 0.87\\
AeT & 0.94 & 0.93 & 0.91 & 0.87 & 0.87 & 0.85 & 0.89\\
Greedy & 0.94 & 0.92 & 0.91 & 0.88 & 0.88 & 0.87 & 0.89\\
JeAeT & 0.95 & 0.90 & 0.85 & 0.78 & 0.78 & 0.75 & 0.81\\
JeT & 0.94 & 0.90 & 0.85 & 0.78 & 0.78 & 0.75 & 0.81\\
T & 0.94 & 0.93 & 0.92 & 0.86 & 0.86 & 0.86 & 0.89\\
TAeT & 0.94 & 0.93 & 0.91 & 0.87 & 0.87 & 0.86 & 0.90\\
TAeTJeT & 0.94 & 0.92 & 0.91 & 0.88 & 0.88 & 0.87 & 0.90\\
TJeT & 0.95 & 0.92 & 0.92 & 0.87 & 0.87 & 0.87 & 0.90\\
TJeTAeT & 0.95 & 0.92 & 0.93 & 0.87 & 0.87 & 0.87 & 0.91\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.62 & 0.79 & 0.68 & 0.90 & 0.90 & 0.85 & 0.83\\
AeJeT & 0.62 & 0.79 & 0.68 & 0.90 & 0.90 & 0.85 & 0.83\\
AeT & 0.63 & 0.82 & 0.69 & 0.92 & 0.92 & 0.86 & 0.84\\
Greedy & 0.65 & 0.83 & 0.70 & 0.92 & 0.92 & 0.86 & 0.87\\
JeAeT & 0.57 & 0.75 & 0.68 & 0.90 & 0.90 & 0.83 & 0.82\\
JeT & 0.57 & 0.75 & 0.69 & 0.91 & 0.91 & 0.83 & 0.82\\
T & 0.62 & 0.82 & 0.73 & 0.93 & 0.93 & 0.86 & 0.86\\
TAeT & 0.64 & 0.82 & 0.70 & 0.92 & 0.92 & 0.86 & 0.83\\
TAeTJeT & 0.65 & 0.83 & 0.71 & 0.92 & 0.92 & 0.86 & 0.85\\
TJeT & 0.64 & 0.83 & 0.73 & 0.93 & 0.93 & 0.87 & 0.87\\
TJeTAeT & 0.65 & 0.83 & 0.72 & 0.93 & 0.93 & 0.87 & 0.88\\
\hline
\end{tabular}
\end{table}
\clearpage


\subsection{Other Languages}
\subsubsection{Precision}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.87 & 0.63 & 0.79 & 0.83 & 0.85 & 0.76 & 0.64\\

AeJeT & 0.86 & 0.63 & 0.79 & 0.83 & 0.84 & 0.76 & 0.64\\

AeT & 0.87 & 0.67 & 0.79 & 0.84 & 0.85 & 0.77 & 0.65\\

Greedy & 0.88 & 0.65 & 0.80 & 0.85 & 0.86 & 0.78 & 0.66\\

JeAeT & 0.85 & 0.56 & 0.75 & 0.80 & 0.82 & 0.71 & 0.59\\

JeT & 0.85 & 0.56 & 0.75 & 0.80 & 0.82 & 0.71 & 0.59\\

T & 0.87 & 0.65 & 0.79 & 0.84 & 0.85 & 0.75 & 0.65\\

TAeT & 0.88 & 0.65 & 0.80 & 0.84 & 0.86 & 0.77 & 0.65\\

TAeTJeT & 0.88 & 0.65 & 0.80 & 0.85 & 0.86 & 0.78 & 0.66\\

TJeT & 0.87 & 0.65 & 0.80 & 0.84 & 0.86 & 0.77 & 0.65\\

TJeTAeT & 0.87 & 0.66 & 0.80 & 0.84 & 0.86 & 0.77 & 0.66\\
\hline
\end{tabular}
\end{table}

\subsubsection{Recall}

\begin{table}[h!]
\centering
\begin{tabular}[t]{c|ccccccc}
\hline
\textbf{Corpus} & \textbf{CLD2} & \textbf{CLD3} & \makecell{\textbf{Detect}\\\textbf{Language}} & \textbf{FastSpell} & \textbf{FastText} & \textbf{LangID} & \textbf{Langdetect}\\
\hline
AJeT & 0.79 & 0.79 & 0.84 & 0.82 & 0.82 & 0.80 & 0.80\\

AeJeT & 0.79 & 0.79 & 0.83 & 0.82 & 0.82 & 0.80 & 0.80\\

AeT & 0.81 & 0.82 & 0.85 & 0.84 & 0.83 & 0.82 & 0.82\\

Greedy & 0.82 & 0.82 & 0.86 & 0.85 & 0.84 & 0.82 & 0.80\\

JeAeT & 0.77 & 0.76 & 0.82 & 0.81 & 0.81 & 0.78 & 0.78\\

JeT & 0.76 & 0.76 & 0.81 & 0.81 & 0.81 & 0.78 & 0.79\\

T & 0.81 & 0.81 & 0.86 & 0.84 & 0.84 & 0.81 & 0.83\\

TAeT & 0.82 & 0.82 & 0.85 & 0.84 & 0.84 & 0.82 & 0.82\\

TAeTJeT & 0.82 & 0.82 & 0.85 & 0.85 & 0.84 & 0.82 & 0.81\\

TJeT & 0.81 & 0.81 & 0.85 & 0.85 & 0.85 & 0.82 & 0.80\\

TJeTAeT & 0.81 & 0.81 & 0.85 & 0.85 & 0.84 & 0.82 & 0.80\\
\hline
\end{tabular}
\end{table}

\end{appendices}

\end{document}



