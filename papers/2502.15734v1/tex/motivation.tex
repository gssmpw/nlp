

% \begin{figure}[t]
%     \centering
%     \begin{minipage}[t]{0.57\linewidth}
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/divisions_prompts (7).pdf}
%         \caption{Token distribution of different prompt components (RAG chunks, query, etc.) across RAG use cases.}
%         \label{fig:prompt_parts}
%     \end{minipage}
%     \hfill
%     \begin{minipage}[t]{0.4\linewidth} % Adjust width as needed
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/prefix_diff_attn.pdf}
%         \caption{Deviation in output for chunk $C_1$ with increasing number of prefix chunks}
%         \label{fig:prefix_diff_attn}
%     \end{minipage}
%     \label{fig:combined_figures}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.3259\linewidth}
%         \centering
%         \includegraphics[width=1.01\linewidth]{figures/Plots/cdf_hits.pdf}
%         \vspace{-2.2em}
        
%         \caption{Individual chunks}
        
%         \label{fig:chunks_cdf}
%     \end{subfigure}
%     \begin{subfigure}{0.3259\linewidth}
%         \centering
%         \includegraphics[width=1.01\linewidth]{figures/Plots/hit_at_k (13).pdf}
%         \vspace{-2em}
%         \caption{Reuse Density}
%         \label{fig:reuse_cdf}
%     \end{subfigure}
%     \begin{subfigure}{0.3259\linewidth}
%         \centering
%         \includegraphics[width=1.01\linewidth]{figures/Plots/prefix_cdf.pdf}
%         \vspace{-2em}
        
%         \caption{Observed 5-tuples}
%         \label{fig:prefix_cdf}
%     \end{subfigure}
%     \caption{Fig. \ref{fig:chunks_cdf} and \ref{fig:prefix_cdf} show the CDF of retrieval hit rates of the individual chunks and the observed $5-$tuple chunks respectively, across all user requests. Fig. \ref{fig:reuse_cdf} shows the decreasing cache reuse density with increasing prefix lengths.
%     }
%     \label{fig:prefix_cache}
% \end{figure}




\subsection{Prefill Dominates Decode in RAG}\label{sec:prefill_vs_decode}
In a typical RAG-system, the overall prompt sequence $S$ consists of a few initial instruction text chunks, several retrieved chunks from a knowledge base, \hl{and the user's question or request $U$, i.e., ${S= C_1:C_kU}$, where $C_1:C_k$ denotes the concatenation of $k$ chunks.}\footnote{The instructions in the prompt are the same across all prompts. These instructions are similar to an always repeated chunk and can be dealt with under the same framework.}
\hl{In most production RAG systems, between 5 to 15 chunks are retrieved to answer a query $U$.} 
% \textcolor{red}{citation needed?}
The overall length of prefill tokens $|S|$ and the lengths of their constituents may vary for different requests. 
We analyze this in Fig.~\ref{fig:prompt_parts} for a proprietary system, \X, from 25 sessions. The majority of the tokens (60\% to 98\%) are from the retrieved chunks from a knowledge base (in blue). A few tokens are from the mother prompt (instructions for the chatbot), few-shot examples (for in-context learning~\cite{dong2022survey}), and the user's questions.  

Due to the extra chunks apart from the user's question, the number of prefill tokens becomes significantly more than that of decode. To verify this, we analyze three systems: proprietary production \X and \Y, and another open-source LMSys~\cite{zheng2023lmsys} chat system. 
Fig.~\ref{fig:token_dist} shows a disparity in the number of tokens between prefill and decode: \textbf{30k} prefill tokens for \textbf{600} decode tokens on average.


We compare prefill times and operations on 4xA100-80GB GPUs using the \llama-70B. For \X, prefill accounts for 55.4\% of total time and 19.3x decode operations. \Y takes 76\% of the time and 46x the operations, while LMSys uses 22\% time and 4.4x operations. 

\textit{Contrary to the popular belief that decode is slow in LLMs, for RAG-systems prefill phase typically dominates both the amount of token computation and total latency, despite being highly parallelized.} 



\subsection{Evidences of Chunk-Reuse}\label{sec:chunk_reuse}
Since prefill is the primary bottleneck in RAG, we find improvement opportunities by observing repetitions in chunk retrieval. If $N$ is the total number of chunks representing the knowledge base accessible for RAG, a significant portion of $N$ is retrieved multiple times across different user sessions, where a session consists of multiple user requests and LLM responses. 



\hl{We substantiate this by analyzing the \textit{retrieval hit rates}, defined as the fraction of all retrievals (across multiple sessions) in which a particular chunk is present. Fig \ref{fig:chunks_cdf} shows the retrieval hit rates of 3 RAG systems:} \X, 2wikiMQA~\cite{ho-etal-2020-constructing} and MuSiQue~\cite{10.1162/tacl_a_00475}. The top 5\% of chunks are accessed by \textbf{60\%} of the requests in both \X and MuSiQue, and \textbf{40\%} requests in 2wikiMQA. In \X, most chunk reuse occurs across users (94\%), with reuse within a session at 55\% and across sessions at 67\%.
%
Exploiting the high reuse of knowledge chunks can optimize the prefill by reusing caches that are computed in the previous sessions, instead of recomputing for every retrieval. 
However, cache reuse across different user requests is non-trivial. 



\subsection{Why Cache-Reuse is non-trivial?}
\label{sec:challenges}




\hl{\textbf{Limitations of Prefix-Caching:}} The prefix-cache approach is to store the KV-caches of ordered $k$-tuple chunks when they are co-retrieved for a request $U$, and reuse it for a future request $U'$ if the same $k$ chunks are retrieved in the same order. However, \hl{the \textit{reuse density}, defined as the number of ordered $k-$tuple chunks observed in previous requests}, drops significantly w.r.t. $k$, reducing the reusability of their cache. 
We analyze reuse density for 3 datasets over the most recent $1000$ requests in Fig.~\ref{fig:prefix_cache}b, and find that it drops 
% significantly for $k=5$.
to as low as 5 for $k=5$. 
This worsens further as we analyze the $5-$tuple retrieval hit rates, defined as the fraction of all observed $5-$tuple retrievals in which a particular $5-$ tuple is present. Fig.~\ref{fig:prefix_cache}c shows this hit rate is significantly low compared to those of the individual chunks in Fig.~\ref{fig:prefix_cache}a.
% in the log shown 
\hl{However, a high hit rate is crucial for cache utilization. Moreover, unlike Fig.~\ref{fig:prefix_cache}a, the distribution in Fig.~\ref{fig:prefix_cache}c does not follow the power law, indicating that, for a high re-usability, several k-tuple chunks should be cached. Therefore,
the combinatorial growth of the number of possible k-tuple makes the memory footprint of prefix-caching prohibitive.}





\hl{On the other hand, a method that can reuse the KV-cache of individual chunks (pre-computed while serving a past request) at any position, without restricting to the prefix order, would have a significantly high retrieval hit rate as evidenced in Fig. \ref{fig:prefix_cache}a.}




\begin{figure}[t]
    \centering
    \captionsetup[subfigure]{justification=centering}
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/Plots/chunks_sources.pdf}
        \caption{}
        \label{fig:chunk_sources}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\linewidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/Plots/rouge_sources.pdf}
        \caption{}
        \label{fig:motiv_shuffle}
    \end{subfigure}
    \caption{(a) Shows the distribution of chunk sources for non-prefix chunk reuse, while (b) Shows the impact of chunk reuse from multiple sources on ROUGE F1 scores, even when using new positional embeddings.}
    \label{fig:combined_plots}
\end{figure}

Although a high hit rate is encouraging, it presents a few major decision challenges. We lay them out in the following.


\hl{\textbf{Contextualization:}}
A chunk $C$ may have more than one stored KV-caches that were computed while serving different user requests in the past, e.g., $S^1=C_1^1\!:\!C_i\!:\!C_k^1U^1$ and $S^2=C_1^2\!:\!C_j\!:\!C_k^2U^2$, where $C=C_i=C_j$, but the positions $i$ and $j$ are not necessarily the same. Which of these KV caches of $C$ should be used for a new request $U^3$? 
\hl{A key challenge with reusing pre-computed KV-cache is contextualization. 
The stored KV-cache of $C_{i}$ have been contextualized by $C_1^1\!:\!C_{i-1}^1$. We analyze how the contextualization of $C_{i}$ changes with varying numbers of prefix chunks. In particular, we use the last layer hidden states $H^L_{C_i}(C_1\!:\!C_i) \in \mathbb{R}^{|C_i|\times d}$ corresponding to the tokens in $C_i$, when $C_1\!:\!C_i$ is given as input. Fig.~\ref{fig:prefix_diff_attn} shows its difference from that of no contextualization $H^L(C_i)$. Evidently, the contextualization grows with more prefix chunks.}

\hl{\textbf{Sensitivity to chunk ordering:} }
% Apart from the number of prefix chunks, 
The relative ordering of prefix chunks affects the contextualization due to two reasons: \hl{a) the unidirectional attention by the causal attention mask $M$ in \eqref{eq:attn} and b) the positional embedding that alters $Q$ and $K$ matrices specific to the token positions.} More precisely, $H^L_{C_i}(C_1\!:\!C_i) \neq H^L_{C_i}\big(C_{(1)}\!:\!C_{(i-1)}C_i\big)$, where $C_{(1)}\!:C_{(i-1)}$ is a permutation of the prefix chunks. 




\hl{\textbf{Cache from multiple sources:}} Another decision challenge occurs when the KV-caches are stored at different requests. Let $C$ and $C'$ are retrieved for serving $U$, the KV-cache of $C$ was stored from a past prompt $S_1=C_1^1\!:\!C\!:\!C_k^1U^1$ and that of $C'$ was stored from a different prompt $S_2=C_1^2\!:\!C'\!:\!C_k^2U^2$.
In such cases, can any of the chunk's KV-cache be used reliably to serve the new request $U$? Can both be used?
Fig.~\ref{fig:chunk_sources} shows that for a majority of the requests in \X and {\color{black}2wikiMQA}, the $5$ retrieved chunks were found in the retrievals of $3$ past requests. Finding all the $5$ chunks in the retrievals of only $1$ past request is not common (around 10\% for \X and {\color{black}8\% for 2wikiMQA}).
A naive reuse of KV-caches that were precomputed across different requests significantly degrades output quality. Our findings in Fig. \ref{fig:combined_plots}\subref{fig:motiv_shuffle} show a \textbf{50\%} drop in F1 score when all $5$ chunks are reused from five distinct past requests (Fig. \ref{fig:motiv_shuffle}), highlighting the need for a more advanced reuse strategy. 

To understand when naive reuse of the KV-cache works and when it does not, 
we analyze two example prompts, 
and their outputs in Figs. \ref{fig:tasks_quality_eval_} and \ref{fig:tasks_quality_eval2}. We use $k=2$ relevant chunks $C_1,C_2$ to construct the prompt of a question $U$. The KV-caches of $C_1$ and $C_2$ are precomputed from $H^L(C_0C_1)$ and $H^L(C_0'C_2)$ respectively. We observe in Fig. \ref{fig:tasks_quality_eval2} that when the values of intra-chunk attention weights (from $Q_{C_2}K^T_{C_2}$) and inter-chunk attention weights (from $Q_{C_2}K^T_{C_1}$) are highly overlapping, naive reuse of stale KV-cache results in a wrong output. Whereas if they are less overlapping, the precomputed KV-cache can lead to the right answer in Fig. \ref{fig:tasks_quality_eval_}. 


\begin{figure}[t]
    \centering
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/inter_intra_1.pdf}
        \caption{Inter-attention (C1, C2) and intra-attention (C2, C2) distributions for chunks C2 with C1 in context. The overlap in the distribution is less, meaning inter$<$intra, and hence the output for <C1, C2, Q> without letting C2 attend to C1 is correct due to less overlap indicating little contextualization.}
        \label{fig:tasks_quality_eval_}
    \end{minipage}
\end{figure}


\begin{figure}[t]
    \centering
    \begin{minipage}[t]{1.0\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/inter_intra_2.pdf}
        \caption{Inter-attention (C1, C2) and intra-attention (C2, C2) distributions for chunks C2 with C1 in context. The overlap in the distribution is more, meaning inter$\nless$intra, and hence the output for <C1, C2, Q> without letting C2 attend to C1 is incorrect due to more overlap indicating contextualization.}
        \label{fig:tasks_quality_eval2}
    \end{minipage}
\end{figure}
