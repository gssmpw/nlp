\section{Implementation}
\label{sec:impl}

\noindent\hl{\textsc{Cache-Craft} is a wrapper around \vllm ~\cite{kwon2023efficient}, built on \texttt{Xformers} ~\cite{xFormers2022} backend optimized with Triton ~\cite{Triton2021}. It enables \textit{chunk-cache} reuse for prefix and non-prefix tokens by efficiently managing positional shifts and enabling partial recomputation of prefill.}


\noindent \hl{\textbf{Chunk Storage Management:} \textsc{Cache-Craft} manages \textit{chunk-caches} by implementing a hash table at the granularity of individual RAG chunks. Unlike \texttt{vLLM}, which hashes entire prefixes pointing to the start of the KV cache (spanning multiple chunks), our approach generates independent hashes for each chunk, allowing direct access without dependence on prior context. Each chunk maps to a list of 16-token memory blocks for efficient and independent access. For optimized retrieval, the hash table stores address pointers across memory tiers, prioritizing faster tiers while allowing fallback to slower ones when necessary. Variable chunk sizes are padded to align with 16-token blocks, ensuring a consistent memory layout. Such padding causes negligible output deviation.}

% This kernel is designed using the \textbf{XXXXXXXX} operation to reverse RPE.

\noindent \hl{\textbf{RPE Management:} To enable the reuse of \textit{chunk-caches} in arbitrary positions, \textsc{Cache-Craft} stores all cached chunks without RPE and dynamically applies corrected positional embeddings during runtime based on the current context. To efficiently manage large caches, \textsc{Cache-Craft} employs a custom CUDA kernel to remove RPE from the Keys of the KV cache after processing each request. This kernel reverses the RPE operation, \( x \cos(\theta) - y \sin(\theta) \), by applying its inverse, \( y \cos(\theta) + x \sin(\theta) \), where \( x \) and \( y \) represent the upper and lower 64-dimensional components of each token's 128-dimensional embedding and $\theta$ is the rotational angle. 
\textsc{Cache-Craft} applies relative positional encoding (RPE) to cached chunks before attention computation and removes it after decoding, ensuring reusability across varying positions. In batched inference, it optimizes RPE handling by considering shared chunk positions within the batch. For requests with differing chunk positions, RPE is integrated directly into the attention mechanism during the prefill and decoding stages. For identical positions, RPE is applied before attention and removed post-decoding. This minimizes computational overhead while ensuring correct positional embedding.}





\noindent \hl{\textbf{Selective Token Recomputation:} \textsc{Cache-Craft} modifies the Triton-based flash attention kernel to enable selective token recomputation during the prefill phase. It computes QKV values for both the scattered recomputed tokens and new question tokens. The attention kernel processes queries from these tokens, performing attention computations with the entire KV matrix and parallelizing operations over recompute and question tokens. During block-wise attention, query blocks are multiplied with prior KV blocks, adhering to causal constraints enforced by a dynamic attention mask, ensuring recompute tokens attend only to preceding tokens. After the prefill phase, we inject corrected KV values for recomputed tokens into \texttt{vLLM}'s cache for autoregressive decoding. To prevent cache corruption, the updated cache is asynchronously swapped with the original after decoding.} 
