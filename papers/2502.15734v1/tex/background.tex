

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/divisions_prompts_7.pdf}
        \caption{Token distribution of different prompt components (Mother prompt, RAG chunks, Examples, Query, etc.) across RAG use cases.}
        \label{fig:prompt_parts}
    \end{minipage}
    \hfill
    \begin{minipage}{0.5\textwidth}
        \centering
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=1.01\linewidth]{figures/Plots/cdf_hits.pdf}
            \vspace{-1.75em}
            \caption{Individual chunks}
            \label{fig:chunks_cdf}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=1.01\linewidth]{figures/Plots/hit_at_k_13.pdf}
            \vspace{-1.75em}
            \caption{Reuse Density}
            \label{fig:reuse_cdf}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.32\textwidth}
            \centering
            \includegraphics[width=1.01\linewidth]{figures/Plots/prefix_cdf.pdf}
            \vspace{-1.75em}
            \caption{Observed 5-tuples}
            \label{fig:prefix_cdf}
        \end{subfigure}
        \caption{Fig. \ref{fig:prefix_cache}(a) and \ref{fig:prefix_cache}(c) show the CDF of retrieval hit rates of the individual chunks and the observed $5-$tuple chunks respectively, across all user requests. Fig. \ref{fig:prefix_cache}(b) shows the decreasing cache reuse density with increasing prefix lengths.}
        \label{fig:prefix_cache}
    \end{minipage}
    \hfill
     \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/prefix_diff_attn.pdf}
        \caption{Deviation in output for chunk $C_1$ with increasing prefix chunks}
        \label{fig:prefix_diff_attn}
    \end{minipage}
\end{figure*}


\section{Background and Motivation}
\label{sec: background}

\subsection{\hl{Preliminaries of LLM}}
\label{sec: Preliminaries}
A transformer-based LLM progressively contextualizes a sequence of tokens $S=\{t_1, \cdots, t_n\}$ using $L$ \textit{transformer} layers. Each layer $l\in [L]$ receives $d-$dimensional embeddings of $n$ tokens, $H^l \in \mathbb{R}^{n\times d}$, as input, and outputs contextualized embeddings $H^{l+1} \in \mathbb{R}^{n\times d}$, which are known as hidden states. We denote the LLM operations, from input tokens $S$ all the way up to the last hidden states $H^L$, as $H^L(S)$.
The last layer hidden states are used in a task-specific manner. In the text generation task, the hidden embedding of the last token \hl{$H^L_{n}(S) \in \mathbb{R}^d$} is used to predict the $(n+1)^{\text{th}}$ token. 

In $l^{\text{th}}$ transformer layer, first, the $H^l$ is linearly transformed into the Query, Key, and Value matrices, $Q, K, V \in \mathbb{R}^{n\times d}$ respectively. 
The $Q$ and $K$ matrices are further transformed by positional embeddings (either absolute \cite{su2024roformer} or relative \cite{likhomanenko2021cape}) to capture the sequential order of the tokens.
Then the attention mechanism contextualizes the value embedding of $j^\text{th}$ token as $\tilde{V}_j = \mathrm{softmax}(Q_jK_{:j}^T) V_{:j}$, where $Q_j \in \mathbb{R}^{1\times d}$ is the $j^\text{th}$ query vector and $K_{:j}, V_{:j} \in \mathbb{R}^{j\times d}$ are all the key and value vectors up to the $j^\text{th}$ token. Finally, the contextualized hidden state $H^{l+1}_j$ is obtained by normalizing $H^l_j +  \mathrm{FNN}(\tilde{V}_j)$. 

\hl{LLM operates in two phases. In the \textbf{prefill} phase, it contextualizes all available prompt tokens. The hidden states $H^L(S)$ are computed for the prompt $S=\{t_1, \cdots, t_n\}$ using the matrix operation}
\begin{align}\label{eq:attn}
    \tilde{V} = \mathrm{softmax}(QK^T \odot M)V,
\end{align}
where $\odot$ denotes element-wise product, and $M \in \{0,1\}^{n\times n}$ is a lower triangular matrix, known as causal attention mask, to ensure each token attends only to its previous tokens. This attention computation is $O(n^2)$, \hl{as both $Q$ and $K$ matrices are of size $n\times d$.
 During this phase, the model generates the $KV$ pairs for all tokens in the sequence, which are used to predict the next token $t_{n+1}$.}
 
\hl{In the \textbf{decode} phase, the model generates tokens autoregressively. 
For each newly generated token $t_j$ starting from the position $j=n+1$, the attention mechanism is applied to contextualize its raw embedding $H^0_j$.
However, instead of recomputing the $KV$ for all previous tokens again, the model uses the cached $K$ and $V$ matrices (KV cache) from the prefill phase. By reusing these cached representations of the previous $n$ tokens at every layer, the computation is reduced from $O(n^2)$ to $O(n)$. As each new token is generated, the KV cache is updated by adding the new token's key and value.}

\hl{\textbf{KV cache vs. Prefix cache:} While KV cache optimizes the decode phase by reusing KV pairs of previously processed tokens, the prefill phase still requires $O(n^2)$ computation to establish the full context. Prefix-cache stores and reuses previously computed context that matches with the prefix of the input prompt, and only computes the rest~\cite{zheng2023efficiently} to reduce prefill computation. 
}


