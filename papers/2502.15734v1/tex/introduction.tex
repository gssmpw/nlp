\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
     \centering
        \begin{subfigure}{0.48\linewidth}
         \centering
          \includegraphics[width=0.8\linewidth]{figures/Plots/File_2.png}
         \end{subfigure}~
         \hfill
        \begin{subfigure}{0.48\linewidth}
          \centering
          \includegraphics[width=0.8\linewidth]{figures/Plots/File_1.png}
       \end{subfigure}~
     \caption{Distribution of number tokens in prefill (left)  and decode (right phases for two real production RAG systems \X and \Y.}
     \label{fig:token_dist}
\end{figure}

\hl{Retrieval-Augmented Generation (RAG)} allows LLMs to access relevant context from a custom knowledge base outside its training data to generate grounded responses. 
RAG systems do not require model retraining and are therefore a cost-effective way to customize an LLM's output such that it is relevant and accurate with respect to a target \hl{knowledge base}.
The key components of a RAG-based system are a vector database and an LLM. The vector database stores the embedding of text chunks from a specific domain as indexes. 
During the \textit{retrieval} phase, relevant chunks are extracted based on these embeddings, using vector-similarity search~\cite{echihabi2021new, echihabi2020high}. 

In the \textit{generation} phase, the LLM uses the retrieved context to generate a response to \hl{the user's question}. The LLM processes its input prompt (retrieved chunks + user's question) in the \texttt{prefill} phase, building an initial computed-state called \textit{Key-Value} cache (KV-cache), which is then used in the \texttt{decode} phase for autoregressive token generation. The prefill phase is compute-bound because it processes all tokens of the input prompt in parallel; while the decode phase, which generates one token at a time, is memory-bound.

We aim to understand the bottlenecks in these systems. In this regard, we use two real production RAG systems, referred to as \X and \Y, for workload characterization, motivation, and evaluations.
\X helps users in setting up complex workflows for an enterprise SaaS product by answering queries and providing steps from user manuals and \Y helps users search and understand concepts from large \hl{knowledge bases} through repeated Q\&A.


\textbf{Computational bottlenecks in RAG-systems:}
Typically for RAG systems, the answers generated as well as user questions are short. However, longer input context with more relevant information is often crucial for the system to generate a well-informed answer~\cite{cuconasu2024power}. 
We highlight this in Fig. ~\ref{fig:token_dist} we show distributions of prefill (left) and decode (right) tokens for \X and \Y. It can be seen, that the number of prefill tokens is much more. 


\hl{The prefill time increases} quadratically with the length of input context, due to the attention computation in the transformer architecture \cite{vaswani2017attention}. 
{Fig.~\ref{fig:prefill_times_into}} shows prefill time increases with input token length across different batch sizes for \llama-3-70B~\cite{touvron2023llama} using \vllm~\cite{kwon2023efficient} with 4 NVIDIA A100-80GB GPUs, reaching up to \textbf{$\textbf{76 seconds}$} for $32k$-token sequences at a batch size $8$.
In real production workloads, the prefill time can often cross more than 100 seconds when serving multiple concurrent users. 
This increases the time-to-first-token (TTFT) ~\cite{agrawal2024taming} and degrades the user experience, as no response is generated until the whole input context is processed.
The impact of the prefill phase on overall latency is significant. In \X, it accounts for up to 77\% of total inference time. 

This problem is further exacerbated by the emergence of new LLMs, that can consume up to 1 million tokens (e.g., Claude 3~\cite{anthropic2024claude} and Gemini~\cite{reid2024gemini}). 
As more chunks can be used to improve response quality, longer context would lead to even longer TTFT.


\textbf{Opportunities for optimizing prefill in RAG:}
%
RAG systems typically operate on a finite knowledge base~\cite{lewis2020retrieval}. Moreover, our analysis, shown in Fig.~\ref{fig:lmsys_pd} for \X and RAG datasets like 2WikiMQA and MuSiQue, reveals that a subset of \hl{chunks gets retrieved frequently} by the system. For \X, 75\% of the retrieved chunks for a query were reprocessed, amounting to over 12B tokens in a month. Processing these tokens would require 9600 hours of GPU compute on \llama-3-70B using 8 A100 GPUs, costing approximately \$50k.


\textbf{Challenges in KV-cache reuse in RAG:}
Indiscriminate reuse of KV-caches from previously processed parts of the knowledge base can disrupt the relative positions of tokens, violating causal attention and degrading output quality~\cite{vaswani2017attention}. 
Additionally, for RAG systems with \hl{large} knowledge bases, precomputed KV-caches may not remain in GPU memory, as space is required for storing a) the LLM parameters and b) the growing KV-cache during the decode phase. The latency of loading precomputed KV-caches into GPU memory must not negate the savings from bypassing recomputation, requiring efficient system design and implementation.

\textbf{Limitations of existing works:}
Recent efforts to reduce prefill time and cost, such as \textit{Paged Attention}~\cite{kwon2023efficient}, \textit{CacheGen}~\cite{liu2023cachegen}, \textit{Radix Attention}~\cite{zheng2023efficiently}, \textit{RAG cache}~\cite{jin2024ragcache}
and \textit{context caching} in \textit{Gemini}~\cite{reid2024gemini} rely on \textit{prefix caching}, 
where different prompts with an identical prefix share KV-cache.
\hl{While this preserves the output quality by maintaining \textit{causal attention}~\cite{vaswani2017attention}, its usefulness is very limited in RAG systems because the RAG-system-retrieved text chunks and their relative ordering are sensitive to 
the input question. Slight variation in the user question can result in different sets of chunks and ordering, rendering the prefix caching technique ineffective. }
We found in production workloads, exact prefix caching applies to only a small fraction (8\%) of requests and 18\% of total prefill tokens. 



\begin{figure}[t]
    \centering
    \begin{minipage}{0.5\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Plots/prefill_decide_times.pdf}
        \caption{Prefill time across prefill length and batch size in \vllm on A100 80GB with TP=4.}
        \label{fig:prefill_times_into}
    \end{minipage}
    \hspace{0.5em}
    \begin{minipage}{0.4\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Plots/pdf_all_3.pdf}
        \caption{\textit{Chunk-cache} hit rate pdf for both \X and RAG datasets.}
        \label{fig:lmsys_pd}
    \end{minipage}
\end{figure}

\textbf{\sys:}
We propose \sys, a system
for managing and reusing precomputed KV-caches in RAG. 
We overcome the challenges by (1) efficiently identifying which \textit{chunk-caches} can be reused even if their prefix alters, \hl{(2) identifying how to recompute the KV of a few selected tokens of the prefix-altered-caches to prevent quality degradation}, and (3) how to manage these caches such that most important chunks are prioritized, the overhead of load/store is masked to effectively reduce expensive GPU compute and TTFT latency for a workload. 
Fig.~\ref{fig:rag_overview} illustrates 
\sys:

\begin{itemize}
\item 
On the left, we show how KV-caches are formed across the \texttt{Transformer} layers when attention computation is done on the text chunks (shown with \textit{yellow} and \textit{gray}) corresponding to a question $Q$.
These pre-computed \textit{chunk-caches} are stored and managed by \sys along with some metadata. 

\item On the right we show their reuse. For a \textit{new question} two chunks of the knowledge-base become important. 
\sys identifies that it already has a cache for the \textit{yellow} chunk, therefore it retrieves and reuses the caches at the appropriate layers and only computation for the new \textit{green} chunk happens across layers. 

\item \hl{When a \textit{chunk-cache} is reused, KV is recomputed (not shown here) for a limited number of \textit{tokens} that were originally contextualized by tokens outside the chunk.
\sys further reduces this recomputation by using the relevance of a chunk w.r.t. the new question.
Tokens of less relevant chunks are not recomputed beyond a certain number of layers.}

\item \sys prioritizes storing KV-caches for the \hl{\textit{important} chunks} to maximize computation savings. The importance of a chunk is determined by its potential for direct reuse without significant recomputation, as well as its expected frequency of use based on the RAG's workload.

\end{itemize}

We implement \sys and integrate its KV-cache management capabilities into \vllm~\cite{kwon2023efficient}, a widely used package for LLM inference. The implementation is non-trivial
% \footnote{We plan to open source it as part of \vllm   \href{https://anonymous.4open.science/r/CacheCraft-7EE4/}{(Link)}}
as it \hl{incorporates optimizations} such as \textit{FlashAttention}~\cite{dao2022flashattention} and \textit{PagedAttention}~\cite{kwon2023efficient} to enhance the \textit{Arithmetic Intensity}~\cite{ofenbeck2014applying} of computations.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\columnwidth]{figures/Plots/overview_intro.pdf}
    
    \caption{Overview of \sys}
    \label{fig:rag_overview}
\end{figure}


\hl{We evaluate \sys with \llama in real deployment scenarios based on public traces. We show that it achieves a 51\% reduction in GPU computation costs for production workloads compared to prefix-caching (\S\ref{sec:eval_performance}). Under continuous batching through \textit{ORCA} for \X, \sys improves throughput by 1.6$\times$ and reduces end-to-end response latency by 2.1$\times$ for \llama-3-8B model and for \llama-3-70B, it provides a 1.6$\times$ speedup in throughput and 2$\times$ reduction in end-to-end response latency compared to prefix-caching (\S\ref{sec:deploy_performance_eval}). In both cases, 30\% tokens are recomputed which maintains 90\% of the base ROUGE F1 score on average.}

In summary, this paper makes the following contributions:
\begin{enumerate}

    \item We analyze real \hl{production workloads} to show that RAG systems are prefill-heavy, yet prefix caching remains ineffective.
    
    
    \item We present the key challenge of reuse, stemming from causal attention calculation through a formal problem formulation, and present detailed techniques to identify the reusability of \textit{chunk-caches} along with an efficient recomputation strategy to fix any potential \hl{degradation in generation quality}.  
    
    \item We present end-to-end design details and rationale for \sys, which is our optimized KV-cache management system for RAG, implemented in \vllm,
    a widely used LLM \hl{inference} package and plan to open-source it. 
    
    \item We present extensive evaluations on real-world, large production RAG systems, along with six other datasets, supported by a human evaluation user study and several sensitivity studies.
\end{enumerate}

