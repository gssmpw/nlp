\section{Related Works}
\label{sec:related_works}


\noindent\textbf{LLM Serving Efficiency:} Multiple works looked into achieving service level objectives (SLOs) at scale\cite{crankshaw2020inferline,crankshaw2017clipper,gujarati2020serving,shen2019nexus}. 
The  works in \cite{dao2022flashattention,kwon2023efficient,shi2023welder} looked into optimizations of memory, while \cite{han2022microsecond,zhang2023shepherd,yu2022orca} have explored parallelism and batching. 
%Improving LLM's efficiency has been widely researched. 
\cite{dettmers2022gpt3,kwon2022fast,frantar2023sparsegpt} aimed to improve the KV computations, primarily using the model's sparsity. 

\noindent\textbf{Context Compression and KV Cache Reduction}: Improving decode speed of LLMs is a widely studied field. Several system-level techniques to optimize the prefill have been adopted \cite{qu2022dota,guo2023olive,ham2021elsa}.  
These works primarily aim to reduce the size of the KV cache during generation. Works like \cite{dong2024get,jiang2023llmlingua, jiang2023longllmlingua} focused on compressing the context length to optimize the prefill. 
Some other similar works \cite{liu2024scissorhands,zhang2024h2o,adnan2024keyformer} drop unimportant tokens while a few modify attention or use gist tokens to achieve KV reduction \cite{yan2021attention,mu2024learning}. Another set of works that aim at KV reuse has enabled increased decoding speed by saving redundant computation. Most of these assume prefix-match \cite{jin2024ragcache,liu2024optimizing,liu2023cachegen,zheng2023efficiently,gao2024attentionstore}, which is impractical for RAG-systems. 
Although \textit{Prompt Cache}\cite{gim2024prompt} enables KV cache reuse at various positions, it struggles to maintain satisfactory generation quality due to inaccuracies in positional encoding and a lack of consideration for cross-attention.
\sys enables efficient KV-cache reuse for RAG without compromising quality. 

\noindent\textbf{KV Cache Quantization and Management:} Quantization of KV-cache reduces computation while maintaining generation quality \cite{hooper2024kvquant,dong2024qaq}. Some works address fitting large KV caches in limited memory \cite{lee2024infinigen,kwon2023efficient,wang2023catalyst,wang2020put,ren2017slimdb}.
\vllm \cite{kwon2023efficient} reduces KV cache due to fragmentation.
\textit{Prompt Cache}~\cite{gim2024prompt} reuses KV-caches at different positions but relies on a rigid prompt structure, leading to poor quality when the structure changes.
These orthogonal techniques can complement \sys for additional efficiency.


\noindent\textbf{\hl{Approximation in Systems:}} 
\hl{Controlled approximation in KV-cache computation is inspired by prior works that used approximation techniques in image generation \cite{agarwal2024approximate, li2024distrifusion, ma2024deepcache,lu2024recon}, data analytics \cite{garofalakis2001approximate, park2018verdictdb, agarwal2023fast, ahmad2024scaleviz}, and video analytics \cite{xu2018videochef, zhang2017live}.}




