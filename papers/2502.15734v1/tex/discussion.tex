\begin{figure*}[t]
    \centering
    \begin{minipage}[t]{0.21\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/ablation.pdf}
        \caption{Design elements on Rouge F1 vs. recompute tokens for 2wikiMQA with \llama-3-8B and 30\% recomp. The dotted line connects different $\alpha$. $\alpha$=1 w/o ($\beta$ and focus)}
        \label{fig:design_ablate}
    \end{minipage}\hspace{0.25em}\vline\hspace{0.25em}
    \begin{minipage}[t]{0.21\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/chunks_ablate_1.pdf}
        \caption{ROUGE-F1 quality for context length, measured by varying chunk sizes (brown) and number of chunks (blue), with \llama-3-8B at 30\% recomputation.}
        \label{fig:chunk_length_ablate}
\end{minipage}\hspace{0.25em}\vline\hspace{0.25em}
    \begin{minipage}[t]{0.21\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/batch_times_serve.pdf}
         \caption{\hl{Prefill and decode latencies across batch size for \llama-3 8B and 70B/ Prefill takes up an increasing proportion of total time for larger batch sizes.}}
        \label{fig:batch_times_serve}
    \end{minipage}\hspace{0.25em}\vline\hspace{0.25em}
    \begin{minipage}[t]{0.165\linewidth}
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/hier_cache.pdf}
        \caption{\hl{Cache loading overhead in \sys from different memory hierarchies using \llama-3-8B with and w/o preloading.}}
        \label{fig:hier_cache}
    \end{minipage}\hspace{0.25em}\vline\hspace{0.25em}
    \begin{minipage}[t]{0.125\linewidth}
    \vspace{-6.5em}
        \centering
         \renewcommand{\arraystretch}{0.85} % Adjust row height for readability
    % \setlength{\tabcolsep}{2.0pt}       % Adjust column spacing  
        % using Llama-3-8B}
        % (1) None (2) Only Causality (3) Only RPE (Full-Cache) (4) 30\% Causality and RPE (\sys)}
        {\fontsize{7.1}{7.75}\selectfont
         \begin{tabular}{@{}c|c|c@{}}
        \toprule % Double line at the top
        {RPE} & {Causal} & {Rouge} \\ 
        \midrule
        \textcolor{red!80!black}{$\times$} & \textcolor{red!80!black}{$\times$} & 0.15 \\ 
        \textcolor{red!80!black}{$\times$} & \textcolor{green!60!black}{$\checkmark$} & 0.22 \\ 
        \textcolor{green!60!black}{$\checkmark$} & \textcolor{red!80!black}{$\times$} & 0.66  \\ %5
        \textcolor{green!60!black}{$\checkmark$} & \textcolor{green!60!black}{$\checkmark$} & \textbf{0.89} \\ %6
        \bottomrule
    \end{tabular}
    \vspace{0.5em}
    \captionof{table}{Impact of fixing only RPE/Causality and both RPE $+$ Causality for \textit{chunk-cache} reuse for 2WikiMQA}
    \label{tab:rope_causal_ablate1}
    }
    \end{minipage} 
    \vspace{-0.2em} 
\end{figure*}


\section{\hl{Ablations} 
 and Discussions}
\label{sec:discussion}


\textbf{Design Components in \sys: } 
The ablation study in Fig. \ref{fig:design_ablate}  on 2wikiMQA using \llama-3-8B highlights the impact of various design elements in \sys. 
We obtain a baseline score of 0.665 from Full KV Cache reuse with fixed RPE. 

Removing components of our recomputation logic-specifically, $\beta$, Cache Context Index ($CCI$), and focus chunking—provides insights into performance dynamics. Removing $\beta$ increases recomputation to 54\% without improving quality, emphasizing its role in minimizing unnecessary recomputation for well-matched chunks. Disabling focus chunking similarly raises recomputation to 70\% with minimal quality gains, underscoring the importance of both $\beta$ and focus in optimizing recompute efficiency. Additionally, when fixed recomputation is applied without $CCI$ (via random selection), 
quality declines dramatically to a ROUGE score of 0.73.

We also explore varying $\alpha$ values from 0.5 to 3, revealing an increasing quality trend: 0.825 for $\alpha=0.5$, 0.896 for $\alpha=1$, 0.94 for $\alpha=2$, and 0.953 for $\alpha=3$. However, this trend indicates diminishing returns as recomputation increases, highlighting a saturation point.








% \begin{figure}[t]
%     \centering
%     \begin{minipage}[t]{0.475\linewidth}
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/ablation.pdf}
%         \caption{Ablation of design elements on Rouge F1 vs. recompute tokens for 2wikiMQA with \llama-3-8B and 30\% recomp. Dotted line connects different $\alpha$. $\alpha$ = 1 for w/o $\beta$ and w/o focus}
%         \label{fig:design_ablate}
%     \end{minipage}
%     \hspace{0.1em}
%        \begin{minipage}[t]{0.475\linewidth}
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/chunks_ablate (1).pdf}
%         \caption{ROUGE-F1 quality trends with context length, measured by varying chunk sizes (brown) and number of chunks (blue), using \llama-3-8B with 30\% recomp.}
%         \label{fig:chunk_length_ablate}
%     \end{minipage}
    
% \end{figure}


% \begin{figure}[t]
%     \centering
%     \begin{minipage}[t]{0.475\linewidth}
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/batch_times_serve.pdf}
%          \caption{\hl{Proportions of prefill and decode latencies across batch sizes for \llama-3 8B and 70B models.}}
%         \label{fig:batch_times_serve}
%     \end{minipage}
%     \hspace{0.1em}
%        \begin{minipage}[t]{0.475\linewidth}
%         \centering
%         \includegraphics[width=1.0\linewidth]{figures/Plots/hier_cache.pdf}
%         \caption{\hl{Cache loading overhead in \sys from different memory hierarchies using \llama-3-8B.}}
%         \label{fig:hier_cache}
%     \end{minipage}
    
% \end{figure}



\textbf{Context Size (Number of Chunks vs. Chunk Size):} We analyze quality (ROUGE F1) trends with context lengths by varying chunk sizes (brown line) and the number of chunks (blue line) using \llama-80B with 30\% recomputation, as shown in Fig.~\ref{fig:chunk_length_ablate}. The brown line demonstrates that quality consistently increases with larger chunk sizes, stabilizing around 0.92, which underscores the reliability of our recomputation logic for longer contexts. The blue line, representing quality with more chunks, exhibits a similar upward trend but slightly declines after saturation (approximately 0.91). This drop, highlighted in red on the plot, indicates that focus chunk selection becomes less effective with too many chunks. Notably, when the "\textit{focused} chunks" filter is disabled (indigo line), quality remains stable, suggesting that the decline is attributed to the error from the "\textit{focused} chunks" selection mechanism.

\hl{\textbf{Why caching chunks is acceptable in practice?}}
\hl{Our evaluations in Fig.~\ref{fig:rouge_all} and the qualitative user study in Table~\ref{tab:user_study} demonstrate that recomputing attention scores for only 30\% of carefully selected tokens while using precomputed caches for the rest achieves 93\% of the user score attained by full attention computation while significantly improving performance. This is driven by two observations:}

\hl{\noindent\textbf{(1)} Retrieved RAG chunks, such as document sections in \X, typically exhibit low inter-dependencies, as attention scores decline with token distance. For large chunks ($>$883 tokens) in \X, the intra-attention is \textbf{2.18x} inter-attention on average with only \textbf{23\%} of chunks being highly contextualized. To address such chunks, \sys selectively recomputes KV caches for a few contextualized tokens, producing outputs close to ideal (\S\ref{sec: fixing_recomputation}). Fig.~\ref{fig:rouge_all} shows \sys offers the best trade-off between recomputation budget and quality, outperforming all baselines with a ROUGE F1 score of \textbf{0.87} using 20\% recomputation for \X~ while a threshold of \textbf{0.7} is considered good for semantic similarity \cite{li-etal-2024-traq}. This is also supported by our user study results which show \textbf{78.9\%} user
acceptance for \sys versus \textbf{83.7\%} for vanilla \vllm 
with exact computation using LLaMa-3-8B (Table~\ref{tab:user_study}).
}
%

\hl{\noindent\textbf{(2)} 
% In production, metrics like 
TTFT metric is critical in production. Current methods like prefix-caching suffer under heavy load, with TTFT reaching \textbf{35s} for LLaMa-3-70B on 4 A100 GPUs (Fig.~\ref{fig:batch_times_serve}). The proportion of TTFT in overall response latency increases with the higher system-load. \sys reduces TTFT latency by independent caching of prior context. Unlike prefix caching, which has low hit rates and high memory overhead (Fig.~\ref{fig:chunks_cdf}), \sys stores chunks independently, allowing to store more chunks in HBM and achieving higher cache hit rates by reusing chunks in different combinations.}

% \hl{\sys is designed to optimize inference latency while maintaining output quality by reusing independent \textit{chunk-caches}. A key factor enabling \sys’s efficiency is the observation that RAG chunks typically have low inter-dependencies as in \X. On average the intra-attention for the chunks in \X is 2.18$\times$ that of inter-attention and the proportion of chunks that are highly contextualized (high inter-attention) contribute to only around 23\% of the total chunk hits. This motivated us to identify the few tokens in a chunk that are previously contextualized from other chunks and recompute their KV caches to produce an output that is close to ideal  (\S\ref{sec: fixing_recomputation}). In Fig.~\ref{fig:rouge_all}, we show the
% trade-off between recomputation budget and quality for Cache-Craft and compare with several baselines, where \sys provides the best trade-off as its points are towards the \textit{top-left}. Our method achieves F1 scores of 0.87, with 20\% recomputation for \X, while a threshold of 0.7 is considered good for semantic similarity [Li et al.(2024)]. This is also supported by our user study in Sec. 5.2.4, where 78.9\% users find Cache-Craft response acceptable vs. 83.7\% users for the vanilla vLLM with exact computation for LLaMa-3-8B (Table~\ref{tab:user_study}). Thus, by balancing selective recomputation and efficient caching, \sys ensures optimized memory usage, reduces computational overhead, and improves throughput, making it an ideal solution for real-world RAG workloads at scale.}

% \hl{Prefix caching introduces a significant memory overhead, as the number of unique prefixes grows almost linearly with requests. By storing chunks independently, \sys allows more chunks in HBM, enabling higher cache hit rates as chunks can be reused in different combinations. This makes it highly effective in real-world deployments where reducing TTFT is critical for user experience. In high-workload scenarios like \X XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX on the \llama-3-70B model, TTFT can dominate response latency, reaching 24s out of 35s under heavy loads, even with prefix caching. By enabling independent chunk caching, \sys reduces TTFT to just 10s (Fig.~\ref{fig:batch_times_serve}), significantly improving responsiveness without compromising accuracy.}

% \begin{table}[t]
%     \small
%     \centering
%     \begin{minipage}[t]{0.62\linewidth}
%         \centering
%         % \includegraphics[width=1.0\linewidth]{figures/Plots/lingua_mapr_8_40 (2).pdf}
%          \caption{ROUGE-F1 scores comparing \sys with token compression techniques for 30\% recompute tokens using \llama-3-8B}
%         \label{tab:lingua_mapr_8_30}
%       \renewcommand{\arraystretch}{1.05} % Adjust row height for readability
%     \setlength{\tabcolsep}{3.0pt}       % Adjust column spacing  
%     {\fontsize{8.1}{8.75}\selectfont
% \begin{tabular}{@{}l|c|c|c@{}}
%     \toprule
%     {Dataset} & {\LLMLingua} & {\MapReduce} & {\ \ Our\ \ } \\ 
%     \midrule
%     2wikiMQA & 32.1\% & 53.0\% & 89.3\% \\ 
%     SQuAD & 45.2\% & 63.6\% & 93.6\% \\ 
%     XSUM & 51.2\% & 51.6\% & 91.1\% \\ 
%     \X & 56.4\% & 61.0\% & 92.0\% \\ 
%     \bottomrule
% \end{tabular}
% }

    
%     \end{minipage}
%     \hspace{0.7em}
%     \begin{minipage}[t]{0.325\linewidth}
%         \centering
%         \caption{Fixing RPE/Causality with cache reuse for 2WikiMQA }
        
%          \renewcommand{\arraystretch}{1.05} % Adjust row height for readability
%     \setlength{\tabcolsep}{3.0pt}       % Adjust column spacing  
%         % using Llama-3-8B}
%         % (1) None (2) Only Causality (3) Only RPE (Full-Cache) (4) 30\% Causality and RPE (\sys)}
%         {\fontsize{8.1}{8.75}\selectfont
%          \begin{tabular}{@{}c|c|c@{}}
%         \toprule % Double line at the top
%         {RPE} & {Causal} & {Rouge} \\ 
%         \midrule
%         \textcolor{red!80!black}{$\times$} & \textcolor{red!80!black}{$\times$} & 0.15 \\ 
%         \textcolor{red!80!black}{$\times$} & \textcolor{green!60!black}{$\checkmark$} & 0.22 \\ 
%         \textcolor{green!60!black}{$\checkmark$} & \textcolor{red!80!black}{$\times$} & 0.66  \\ %5
%         \textcolor{green!60!black}{$\checkmark$} & \textcolor{green!60!black}{$\checkmark$} & \textbf{0.89} \\ %6
%         \bottomrule
%     \end{tabular}
%     }
%         \label{tab:rope_causal_ablate1}
%     \end{minipage} 
    
% \end{table}



\textbf{Approximation (Position vs. Causal):} 
In Table~\ref{tab:rope_causal_ablate1}, we observe that reusing caches from non-prefix chunks significantly degrades performance, resulting in a ROUGE F1 score of 0.15 when neither RPE nor causality is fixed. Fixing causality without adjusting RPE yields minimal improvement (0.22) while optimizing RPE alone achieves 0.665, which serves as the \fullcache baseline. Notably, \sys achieves a ROUGE score of 0.896 with just 30\% recomputation, demonstrating that correct positional encoding combined with selective recomputation can effectively approximate the benefits of full recomputation, which scores 1.0.

