\section{Evaluation}
\label{sec:eval}

\subsection{Experimental Set up}

\subsubsection{System Configuration:}
\label{sec:eval_model_and_system}
We evaluate \sys on the \llama-3 8B and 70B models~\cite{dubey2024llama} with tensor parallelism (\texttt{TP}) of 1 and 4 respectively.
All our experiments are performed on EC2 p4de.24xlarge ~\cite{AmazonEC67:online} instances with 8 A100 GPUs ~\cite{9361255} with each having 80 GB GPU (HBM) memory. The host CPU is an Intel Xeon Platinum 8275L processor with 48 cores (96 vCPUs). The instance has 1152 GB of main memory and an 8 TB NVMe SSD with a read throughput of 16 GB/s. 
The CPU and GPUs are interconnected via PCIe 4.0 ×16, providing 64 GB/s bandwidth.

\subsubsection{Datasets and Workload:}
\label{sec:eval_workload}
We evaluate our technique with 
a real production RAG workload (\X) as well as relevant datasets following previous works\cite{bai2023longbench, jin2024ragcache}.


\begin{enumerate}[]
    

        \item \textbf{Real-world workloads:} \X helps users set up complex workflows for an enterprise SaaS by answering questions and prescribing steps from user manuals. It retrieves top-k=5 chunks based on the query.
        As \X creates a chunk based on the subsections of the user manual, each of the chunks can have a highly variable number of tokens. This results in a total input size 
        % (including a mother prompt) 
        of 1k-20k tokens with a median of 3.3k tokens (Fig.~\ref{fig:chunks_cdf}). 
    
        
        \item \textbf{Single-Hop QnA:}
        A question can be answered from a single chunk for this class of datasets. SQuAD \cite{rajpurkar2016squad} focuses on extracting answers from passages, while DROP \cite{dua2019drop} requires discrete reasoning over chunks. For multi-chunk RAG with $k=5$, we selected 200 questions and split them into 512-token chunks. 
        %with 100-token overlaps, 
        % and used RAG with top-k=5.

        \item \textbf{Multi-Hop QnA:} This class of datasets requires using facts and data from multiple chunks to answer each question properly. 
        We utilize 2WikiMQA \cite{ho-etal-2020-constructing} and MuSiQue \cite{10.1162/tacl_a_00475}, which are benchmarks for evaluating complex answers across multiple documents. We sampled 200 questions. 

\item \textbf{Summarization:} We use CNN dataset \cite{nallapati2016abstractive} that generates summaries of news articles from CNN, and XSUM \cite{narayan2018don} that focuses on single-sentence summaries from BBC. 
For sampling, we split long chunks into smaller segments and randomly selected top-k=5 chunks. 
This method is applied to 40 large chunks, resulting in 200 summarization tasks.
\end{enumerate}

\noindent\textbf{Cache Warm-Up:} For every dataset, we use the first 20 queries to warm up the system and set up the caches so that we can evaluate the steady-state characteristics. 

\noindent\textbf{\hl{Cache Storage:}}
\hl{We store $ N = 100 $ chunks with $ M = 5 $ variants, requiring 0.05 TB for \texttt{LLaMA}-3-8B model. Specifically, caching 100 chunks, each containing around 1000 tokens across 5 versions, consumes $ 100 \times 1000 \times 5 \times 0.1\text{ MB (per token)} = 50 \text{ GB} $. For \texttt{LLaMA}-3-70B, this increases to 150  GB with 0.3 MB cache per token}.


\noindent \textbf{Tasks:} In the Single-Hop and Multi-Hop QnA datasets, we perform both long and short answering tasks by adjusting the mother prompt, i.e., instructing the LLM. Additionally, we generate 200 True/False questions from the original dataset chunks. For the Summarization task, we focus solely on long summaries.






\subsubsection{Evaluation Metrics: } \label{sec
}

We use two quality metrics:
{ROUGE-L F1} \cite{lin2004rouge}, which measures long-answer quality in Single/Multi-Hop and summarization tasks, and {Jaccard Similarity} \cite{ivchenko1998jaccard}, which is used for short answers and True/False questions.
We also conduct a \textit{user-study} with 250 participants to assess response correctness and quality on 2wikiMQA and SQuAD datasets based on \texttt{Yes}/\texttt{No} ratings.


Note, according to several prior studies~\cite{chen2019evaluating, lin2004looking}, a 
{ROUGE-L F1} score $\ge 0.6$ is considered \text{good}, and a score $\ge 0.8$ is considered almost indistinguishable from the original answer.
From our user study, we also analyzed this correlation and found that for answers with {ROUGE-L F1} scores $\ge 0.6$ and $\ge 0.8$, 81\% and 93\% of users have given a \texttt{YES}, respectively.
For efficiency, we measure {Recompute Savings}, {Time-to-First-Token (\texttt{TTFT}} i.e., prefill latency), {System Throughput}, and {Cost Savings}. 



\subsubsection{Baselines}
\label{sec:eval_baselines}



\begin{figure*}[t]
    \centering
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/prop_data_sigmod_FINAL.pdf}
        \caption{\hl{Rouge F1 of answer generated using Llama-3 8B and 70B on multi-hop QA, single-hop QA, text summarization, and on production \X.}}
        \label{fig:rouge_all}
\end{figure*}

\begin{figure}[t]
    \centering
        \centering
        \includegraphics[width=1.0\linewidth]{figures/Plots/jac_acc_sigmod_FINAL.pdf}
        \caption{\hl{Jaccard Similarity and Accuracy of short answers and True/False generated using Llama-3 8B on multi-hop and single-hop datasets.}}
        \label{fig:tasks_quality_eval}
\end{figure}

We evaluate against the following baselines.
%to show its strengths against both commonly used strategies and alternative prefill efficiency methods.
\begin{enumerate}[left=0.05em]
    \item \textbf{Prefix Matching}: We compare with two methods: (1) \textbf{\prefixcache} ~\cite{kwon2023efficient}, which reuses the KV cache based on exact prefix matches. While this approach offers perfect accuracy, it has low reuse potential. (2) \textbf{\setcache}, which modifies RPE to reorder \textit{chunk-caches} and finds the longest exact prefix match with the query. While this provides higher reuse, it has \textit{lower accuracy}.
    
    \item \textbf{Naive KV Reuse (\fullcache)}: This baseline reuses the KV cache for each chunk irrespective of the previous context, fixing only the \RPE at the new position. No recomputation is performed for the chunks.
    
    \item \textbf{Recomputation Strategies}: We also evaluate against recomputation methods: (1) \textbf{\randrecomp}, which randomly recomputes tokens within each chunk, and (2) \textbf{\ho}~\cite{zhang2024h2o}, which recomputes the most-attended tokens in the chunks. For both strategies, we maintain the same average fraction of recomputed tokens as in \sys.
    
    \item \textbf{Full Recomp (\fullrecomp)}: This \textit{\bf oracle} baseline fully recomputes all chunks for a request without utilizing any cache, providing a benchmark for optimal performance.
    
    \item \textbf{Compression Techniques}: We compare with prefill compression methods: (1) \textbf{\LLMLingua}~\cite{jiang2023llmlingua} that reduces prefill length by discarding less significant tokens using a trained model (e.g., GPT-2), and (2) \textbf{\MapReduce}~\cite{dean2008mapreduce} that summarizes context chunks for compression. The compression rates are aligned with \sys's recomputation, where 80\% compression corresponds to 20\% recompute.
\end{enumerate}


\subsection{Generation Quality with KV Chunk Reuse}
\label{sec:eval_quality}

\subsubsection{Evaluation of Recomputation Strategy}


We evaluate the recomputation strategy of \sys for Question Answering (Long and Short), True/False, and Summarization tasks using \llama-3-8B and \llama-3-70B models across multiple datasets.

 Fig.~\ref{fig:rouge_all} shows ROUGE-F1 scores, comparing \sys with baseline KV-cache reuse techniques and the original \llama generation (i.e., \fullrecomp with ROUGE score=1). 
Using \fullcache incurs no recomputation but yields low quality, ROUGE dropping to 0.65 for multi-hop QA datasets like 2wikiMQA and MuSiQue. 
In contrast, recomputing 20\% of tokens with \llama-3-8B improves the ROUGE by 30\%, and further by 42\% with 30\% recomputation. This trend is consistent across single-hop QA and summarization datasets, with $\approx$20-35\% improvements for both 8B and 70B models. \hl{Moreover, increasing recomputation to 45\% and 60\% for \textsc{LlaMa}-3-8B, and 30\% and 40\% for \textsc{LlaMa}-3-70B, further improves ROUGE scores, reaching within 1–5\% of \textsc{Full-Recomp} across all datasets.}





We also compare our contextualization-based recomputation against \randrecomp (random token selection) and \ho (high-attention token selection). Notably, random selection can lower performance even below \fullcache as it neglects the key contextual tokens and overpowers wrong tokens, which can even shadow/underpower crucial ones. \ho shows only a modest 2-10\% improvement over \fullcache but struggles with multi-hop tasks. 
\sys identifies and recomputes critical tokens distorted by prior contexts, enhancing performance and minimizing missing or incorrect facts.
Fig.~\ref{fig:tasks_quality_eval} further shows that \sys outperforms \fullcache by up to 50\% in short-QA and True/False tasks, achieving ROUGE of 0.87, compared to 0.59.

\prefixcache offers exact answers, but due to low prefix match rates, 80-95\% of tokens go through regular KV-computation, leading to very low compute savings. 
%
\setcache gives slightly more savings of 15-35\% by making prefixes permutation invariant with lower ROUGE due to incorrect contextualization. 


To summarize, \sys offers the \textbf{\textit{best trade-off}} as its points on Fig.~\ref{fig:rouge_all} are the furthest towards the \textbf{\textit{top-left (ideal)}} corner.


\subsubsection{Comparison with Prompt Compression Techniques}

We compare \sys with established context reduction methods such as \LLMLingua~\cite{jiang2023llmlingua} and \MapReduce~\cite{dean2008mapreduce}, using datasets for multi-hop (2wikiMQA), single-hop (SQuAD), and summarization (XSUM), along with real-workload from \X, on for LLaMA-3-8B.
In Table \ref{tab:lingua_mapr_8_30}, it can be observed that with 30\% recomputation, \sys gives ROUGE-F1 scores around 0.9, which is $\approx$100\% higher than the scores for \LLMLingua (0.4) and MapReduce (0.5), for 70\% compression (i.e. comparable to 30\% recomputation).
The performance gap is due to \LLMLingua and \MapReduce's approach of discarding tokens, often losing critical information. In contrast, \sys retains all tokens by leveraging \textit{chunk-cache} reuse, ensuring no context is lost. 
Additionally, \sys selectively recomputes the most contextually impacted tokens
balancing efficiency and quality.


\subsubsection{\sys on Real Production RAG Workload:}
We evaluate \sys on production RAG workloads from \X focused on retrieval-based QA tasks where questions span multiple subsections of user manuals. As shown in Fig.~\ref{fig:rouge_all}, \sys achieves a ROUGE score of 0.87 with only 20\% token recomputation, outperforming \fullcache reuse (0.59) and other recomputation strategies by about 20-30\%. We also see that \prefixcache also saves just 18\% prefill tokens, proving ineffective. Table~\ref{tab:lingua_mapr_8_30} further compares \sys with prompt compression techniques, where \LLMLingua (0.56) and \MapReduce (0.61) score significantly lower on the \X dataset. 


\begin{table}[t]
    \centering

    \begin{minipage}{0.25\textwidth}
        \centering
        \caption{ROUGE-F1 scores comparing \sys with token compression techniques for 30\% recompute tokens using \llama-3-8B}
        \label{tab:lingua_mapr_8_30}
        % \renewcommand{\arraystretch}{1.0} % Adjust row height for readability
        % \setlength{\tabcolsep}{3.0pt}       % Adjust column spacing  
        {\fontsize{7.75}{8.0}\selectfont
        \begin{tabular}{@{}l|c|c|c@{}}
            \toprule
            {Dataset} & {\LLMLingua} & {\MapReduce} & {\ \ Our\ \ } \\ 
            \midrule
            2wikiMQA & 32.1\% & 53.0\% & 89.3\% \\ 
            SQuAD & 45.2\% & 63.6\% & 93.6\% \\ 
            XSUM & 51.2\% & 51.6\% & 91.1\% \\ 
            \X & 56.4\% & 61.0\% & 92.0\% \\ 
            \bottomrule
        \end{tabular}
        }
    \end{minipage}
    \hspace{0.15em}
    \vline
    \hspace{0.15em}
    \begin{minipage}{0.2\textwidth}
        {\fontsize{7.75}{8.0}\selectfont
        \centering
        \caption{User study comparing \texttt{\sys} (30\% recomp) with Full Cache, Prefill H20 and Full Recompute on Llama-3-8B.}
        \label{tab:user_study}
        % \renewcommand{\arraystretch}{1.0} % Adjust row height for readability
        % \setlength{\tabcolsep}{3.25pt}       % Adjust column spacing

        \begin{tabular}{@{}l|c|c@{}}
            \toprule
            & 2wikiMQA & SQuAD \\ 
            \midrule
            {\fullcache} & 29.8\% & 53.1\% \\ 
            \ho &  52.4\% & 66.8\%\\ 
            {Our} & \underline{71.2\%} & \underline{78.9\%} \\ 
            {\fullrecomp} & 76.9\% & 83.7\% \\ 
            \bottomrule
        \end{tabular}
        }
    \end{minipage}
    
\end{table}

% \begin{table}[t]
%     {\fontsize{7.75}{8.0}\selectfont
%     \centering
%     \caption{User study comparing \texttt{\sys} using 30\% recomputation with Full Cache, Prefill H20 and Full Recompute on Llama-3-8B.}
%     \label{tab:user_study}
%     \renewcommand{\arraystretch}{1.2} % Adjust row height for readability
%     \setlength{\tabcolsep}{3.25pt}       % Adjust column spacing

%     \begin{tabular}{@{}l|c|c|c|c@{}}
%         \toprule
%         Dataset & {\fullcache} & \cellcolor{darkorange!30} \ho & {Our} & {\fullrecomp} \\ 
%         \midrule
%         2wikiMQA & 29.8\% & \cellcolor{darkorange!30} 52.4\% & \underline{71.2\%} & 76.9\% \\ 
%         SQuAD     & 53.1\% & \cellcolor{darkorange!30} 66.8\% & \underline{78.9\%} & 83.7\% \\ 
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}


% \begin{table}[t]
%     {\fontsize{7.75}{8.0}\selectfont
%     \centering
%     \caption{User study comparing \texttt{\sys} using 30\% recomputation with Full Cache, Prefill H20 and Full Recompute on Llama-3-8B.}
%     \label{tab:user_study}
%     \renewcommand{\arraystretch}{1.2} % Adjust row height for readability
%     \setlength{\tabcolsep}{3.25pt}       % Adjust column spacing

%     \begin{tabular}{@{}l|c|c@{}}
%         \toprule
%         & 2wikiMQA & SQuAD \\ 
%         \midrule
%         {\fullcache} & 29.8\% & 53.1\% \\ 
%         \cellcolor{darkorange!30} \ho & \cellcolor{darkorange!30} 52.4\% & \cellcolor{darkorange!30} 66.8\% \\ 
%         {Our} & \underline{71.2\%} & \underline{78.9\%} \\ 
%         {\fullrecomp} & 76.9\% & 83.7\% \\ 
%         \bottomrule
%     \end{tabular}
%     }
% \end{table}


\subsubsection{User Study:}
\label{sec: user_study_writting}

% \begin{table}
%         \centering
%          \caption{ROUGE-F1 scores comparing \sys with token compression techniques for 30\% recompute tokens using \llama-3-8B}
%         \label{tab:lingua_mapr_8_30}
%       \renewcommand{\arraystretch}{1.05} % Adjust row height for readability
%     \setlength{\tabcolsep}{3.0pt}       % Adjust column spacing  
%     {\fontsize{8.1}{8.75}\selectfont
% \begin{tabular}{@{}l|c|c|c@{}}
%     \toprule
%     {Dataset} & {\LLMLingua} & {\MapReduce} & {\ \ Our\ \ } \\ 
%     \midrule
%     2wikiMQA & 32.1\% & 53.0\% & 89.3\% \\ 
%     SQuAD & 45.2\% & 63.6\% & 93.6\% \\ 
%     XSUM & 51.2\% & 51.6\% & 91.1\% \\ 
%     \X & 56.4\% & 61.0\% & 92.0\% \\ 
%     \bottomrule
% \end{tabular}
% }
% \end{table}
    
We conducted a user study with 250 participants with two datasets: 2wikiMQA and SQuAD.

\noindent\textbf{Task:}
We sampled 500 questions from each dataset, extracted 5 relevant chunks as the context from the datasets using vector-similarity search, and then generated answers using \hl{four methods:} (1) pass the text tokens of the chunks and the question to \llama-3-8B for a \fullrecomp to get the answers, (2) use \fullcache that simply reuses the \textit{chunk-caches} with \llama-3-8B without any recompute \hl{(3) \textsc{Prefill-H20} allows \textit{chunk-caches} reuse by recomputing the heavily attended 30\% tokens} and (4) \sys that decides which \textit{chunk-caches} to reuse and which tokens to recompute with 30\% recomputation.
%
Finally, each participant was presented with 15 questions, the relevant context, and answers generated by one of the methods and asked to mark Yes/No, based on the correctness and quality of the answers, when compared to the context.  
 
As illustrated in Table~\ref{tab:user_study}, \fullcache achieved significantly lower scores (30\% on 2wikiMQA, 53\% on SQuAD), whereas \sys consistently outperformed it (71\% on 2wikiMQA, 79\% on SQuAD). 
\hl{We also tested an alternative recomputation strategy, \textsc{Prefill-H20}, which showed moderate improvement (52\% on 2wikiMQA, 67\% on SQuAD), outperforming \textsc{Full-Cache} but still lagging behind \textsc{Cache-Craft}.}
It is interesting to observe that even answers from \llama-3-8B without any \textit{chunk-cache} reuse (\fullrecomp) did not get 100\% Yes, it got 77\% on 2wikiMQA, 83\% on SQuAD which is only marginally better than \sys.  
The preference for \sys was also statistically significant (\textit{p-value} < 0.05).


\begin{figure}[t]
    \centering
        \centering
        \begin{subfigure}[t]{0.48\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/Plots/perf_8b.pdf}
            \caption{\hl{{\sys} performance on \llama-3-8B on 1 A100-80GB for \X }}
            \label{fig:throughput_gain}
        \end{subfigure}
        \hfill
        \begin{subfigure}[t]{0.48\linewidth}
            \centering
            \includegraphics[width=1.0\linewidth]{figures/Plots/perf_70b.pdf}
            \caption{\hl{{\sys} performance on \llama-3-70B on 4 A100-80GB for \X}}
            \label{fig:latency_red}
        \end{subfigure}
        \caption{\hl{Throughput and overall system response latency speedup under varying computational loads for {\sys} deployed with \vllm using ORCA.}}
        \label{fig:orca_perf}
\end{figure}

\subsection{\hl{Performance Evaluation in Deployment}}
\label{sec:deploy_performance_eval}
\hl{We evaluate throughput and overall response latency under \textit{continuous batching} through \textit{ORCA}~\cite{yu2022orca}. In continuous batching, instead of waiting for all requests in a batch to complete before starting a new batch, it continuously schedules a new request when a request in the processing batch completes and slots are available. We use \X workload for both \llama-3 8B and 70B models on A100-80GB GPUs with a TP of 1 and 4, respectively. The maximum number of batched tokens in \textit{ORCA} is set to 150k tokens. The workload arrival patterns are based on public traces from \cite{proteus24,romero2020infaasmodellessmanagedinference} (which is based on Twitter traces) and proprietary data traces from Sys-X.}


\subsubsection{\hl{Throughput and Response Latency with \textit{Continuous Batching}: }} 
\hl{As shown in Fig.~\ref{fig:orca_perf}, \sys achieves up to a 1.9$\times$ speedup in throughput and a 3.3$\times$ reduction in response latency under a heavy load of 240 QPM (Queries per minute) for the \llama-3-8B model and for \llama-3-70B, it provides a 2$\times$ speedup in throughput and a 3.3$\times$ reduction in response latency under a similar heavy load of 120 QPM with no recomputation.
With 30\% token recomputation, maintaining 90\% of the base ROUGE F1 score on average, we still observe a 1.6$\times$ speedup in throughput and a 2.1$\times$ reduction in response latency for \llama-3-8B and for \llama-3-70B, the improvement is a 1.6$\times$ speedup in throughput and a 2$\times$ reduction in response latency under high load. Notably, a 30\% recomputation level for \llama-3-70B is sufficient to ensure a minimum of 90\% of the base ROUGE F1 score. 
The overall response latency reduction for \llama-3-70B under high load with 30\% recomputation is 2.07$\times$, compared to 2.26$\times$ under medium load. This difference arises due to the significantly higher wait time overhead at high load ($\approx$7.15s on average) compared to medium load ($\approx$2.15s on average). However, when excluding request wait time, the latency reduction at high load is even more pronounced (3.22$\times$ compared to 2.64$\times$).}




\hl{
\subsubsection{Preloding in Hierarchical Caching:}
We evaluate how asynchronous (\S\ref{sec: storage,swap,loading}) and layer-wise (\S\ref{sec: preloading_kv}) preloading help in hierarchical caching. 
In Fig.~\ref{fig:hier_cache} we show the timings to load the cache from CPU and SSD to GPU-memory when we start loading after the request reaches the head of the queue (\texttt{Sync}), when asynchronous preloading starts when the request is in the queue (\texttt{Async}), and when layer-wise preloading is used with \texttt{Async} (\texttt{Layer}). Cache loading takes 0.03s from the CPU and 0.59s from the SSD, adding significant overhead. With an average queue wait time of 0.32s (for \X), asynchronous preloading eliminates CPU overhead and reduces SSD overhead to 0.27s, as 0.32s overlaps with queue time.
Through layer-wise preloading, loading only the first 24 (out of a total of 32) layers in advance further reduces SSD overhead to 0.12s. CPU loading overhead is 0s for \texttt{Async} and \texttt{Layer} because loading time is already less than queue time. We compare our effective prefill time with the time taken to recompute the entire context in the fastest scenario, i.e. when the system is idle. With layer-wise preloading, our effective prefill time is shorter for both CPU and SSD. Note that for caches stored in GPU memory, there is no additional overhead for loading. In all three cases, the cache is brought to the GPU, and the time required for any retrieval from GPU memory for processing is already included in the TTFT.}





\subsection{\hl{Controlled Evaluations for TTFT}}
\label{sec:eval_performance}

We evaluate the \hl{TTFT Latency} of the \vllm implementation of \sys on the \X production workload. We also evaluate across various generation settings to ensure generalization to different models and datasets.  We compare performance for the setting for which \sys and the baselines achieve the same quality of ROUGE F1 score of 0.85. 
For \sys, \LLMLingua, and \ho, this corresponds to 25\% recomputation, 25\% compression, and 60\% recomputation, respectively. 
For \prefixcache, we copious scope by setting that 60\% of the prefill tokens will have a prefix match. Note that this is significantly higher than what (18\%) we observed for production workloads \X. 






\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=0.925\linewidth]{figures/Plots/sysx_eval_final.pdf}
        \caption{TTFT for \sys on \X data across different text quality}
        \label{fig:perf_compare_diff_qual}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=0.85\linewidth]{figures/Plots/model_size_perf.pdf}
        \caption{TTFT for different model sizes (prefill tokens 8192, batch size 4, TP 4)}
        \label{fig:model_sizes_perf}
    \end{subfigure}
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=0.925\linewidth]{figures/Plots/seq_len_perf.pdf}
        \caption{Baseline comparison across prefill lengths (batch size 4, TP 1)}
        \label{fig:seq_len_perf}
    \end{subfigure}
      \hfill
    \begin{subfigure}[t]{0.49\linewidth}
        \centering
        \includegraphics[width=0.925\linewidth]{figures/Plots/batch_size_perf.pdf}
        \caption{Baseline comparison across batch sizes (prefill tokens 8192, TP 1)}
        \label{fig:batch_size_perf}
    \end{subfigure}
    \\
    \caption{Performance evaluation of \sys on \llama-3-8B (except sizes)}
    \label{fig:2x2_subfigs}
\end{figure}




\subsubsection{Performance of \sys on \X: } In Fig. \ref{fig:perf_compare_diff_qual}, we compare the performance of \sys against Prefix Cache for requests from \X across sequence lengths. The range of sequences received by the system varies from 600 to 20000 tokens averaging around 5000 tokens. We observe about 2.5$\times$ speedup for \llama-3 8B in TTFT latency over Prefix Cache by recomputing 39\% of tokens while maintaining 90\% of the original quality. This is because on average only 18\% exact prefix match occurs for the requests received in the system rendering \prefixcache ineffective.

In Fig. \ref{fig:eval_sysx}, we show TTFT latencies of each request from a trace of the requests received overtime by \X. 
We indicate the warm-up period on the left.
In the bottom plot, we observe that as \sys can keep the TTFT spikes significantly lower than vanilla \llama with \prefixcache. \sys provides 3$\times$ reduction in the 99th percentile TTFT latency. 
Note, the spikes in TTFT are due to the fact that text chunks in \X are subsections of the user manuals and are unequal in size.
However, note that when prefill lengths are high (leading to spikes), \sys comes can reduce TTFT significantly as by reusing \textit{chunk-caches}, it avoids quadratic computational complexity (\S\ref{sec: background}). 
In the top plot of Fig. \ref{fig:eval_sysx},
we also observe a consistent reduction in token computation indicating higher reusability of cached chunks compared to \prefixcache. This trend is further supported by the increasing chunk hit rate achieved by our system. This results in a 51\% average reduction in token computation compared to \prefixcache.
In the middle plot of Fig. \ref{fig:eval_sysx}, we show how many chunks for the top-k=5 retrieval in \X was a hit in the \textit{chunk-cache}. It can be observed for a large number of requests, all the necessary chunks were already in the cache --- leading to a \textit{hit-rate of 5 out of 5}.









\noindent\textbf{Cache-store Characteristics:} Fig.~\ref{fig:snapshot_data} illustrates the cache-store state at the trace's end for \X. The X-axis represents the number of unique \textit{chunk-caches} (186), and the Y-axis indicates how many variants were created for each chunk (up to 11 for some). As detailed in \S~\ref{sec:variants}, \sys dynamically configures cache storage based on chunk popularity and reuse.

\subsubsection{\hl{Impact of Model Size, Prefill Length \& Batch Size}: }
Here we measure how \sys reduces TTFT latency compared to \fullrecomp across different model sizes of \llama, across different prefill-lengths, and batch sizes.
As shown in Fig. \ref{fig:model_sizes_perf}, \sys becomes more effective in reducing TTFT latency as the model size increases. This improvement is due to reduction of the number of tokens computed by \sys in each attention layer, with the gains increasing as the number of layers grows for larger models.
\sys reduces latency by 1.6$\times$ and 2.3$\times$ compared to \fullrecomp for \llama-3-8B and \llama-3-70B, respectively and with batch-size$=4$ and sequence length of 8192 tokens. 

In Fig. \ref{fig:seq_len_perf} we compare the TTFT latency against baselines, \prefixcache, \LLMLingua, and \ho, across varying sequence lengths for \llama-3-8B with batch-size$=4$. We can see \sys outperforms all baselines across different sequence lengths and for 16k length it is 1.7$\times$ faster than \fullrecomp. In Fig. \ref{fig:batch_size_perf} we show for \llama-3-8B as we increase batch-size, \sys is more effective in controlling 
TTFT latency increases than all other methods because it requires much less recomputation to maintain quality.  

%\vspace{-1em}

%As depicted in Fig. \ref{fig:batch_size_perf}, \sys increases by 14$\times$ from a batch size of 1 to 16, while full recomputation increases by 19$\times$ for \llama-3-8B. This shows that \sys offers 1.4$\times$ lower increase in TTFT latency as batch size increases.
%In a production system that processes 1B prefill tokens, this throughput improvement can lead to significant cost savings, estimated at approximately \$1.5M.
% \hl{\sys also shows a 3.4$\times$ reduction in overall response latency with a batch size of 32 when static batching is used for 0\% recompute for \llama-3-70B.} 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/Plots/sysx_trace.pdf}
    \caption{Evaluation of \sys on \X trace on \llama-3-8B}
    \label{fig:eval_sysx}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.965\linewidth, height=0.1\linewidth]{figures/Plots/heatmap_plot.pdf}
    \caption{Snapshot of $MXN$ Cache-Store for \X trace at the end}
    \label{fig:snapshot_data}
\end{figure}