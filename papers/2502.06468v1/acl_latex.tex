% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage[dvipsnames]{xcolor}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{todonotes}

\usepackage{booktabs}
\usepackage{tabularx,ragged2e}
\usepackage{multirow}
\usepackage{array}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\graphicspath{ {./figures/} }

\usepackage{pgf} % for calculating the values for gradient
 % enables the use of cellcolor make sure you have [table] option in the document class 
\usepackage{colortbl}

\newcommand{\gradientcell}[7]{%
    % The values are calculated linearly between \midval and \maxval
    \ifdimcomp{#1pt}{>}{#3 pt}{\cellcolor{#5!100.0!#4!#6}\hspace*{-4.0pt}#7 \hspace*{-4.0pt}}{%
    \ifdimcomp{#1pt}{<}{#2 pt}{\cellcolor{#5!0.0!#4!#6}\hspace*{-4.0pt}#7 \hspace*{-4.0pt}}{%
         \pgfmathparse{int(round(100*(#1/(#3-#2))-(#2 *(100/(#3-#2)))))}%
        \xdef\tempa{\pgfmathresult}%
        \cellcolor{#5!\tempa!#4!#6}\hspace*{-4.0pt}#7\hspace*{-4.0pt}%
    }}
}
\newcommand{\corr}[2]{\gradientcell{#1}{0.0}{0.79}{Cyan}{Yellow}{60}{#2}}
\newcommand{\C}[1]{\pgfmathparse{abs(#1)}\corr{\pgfmathresult}{#1}}


% If the title and author information does not fit in the area allocated, uncomment the following
%
\setlength\titlebox{6cm}
%
% and set <dim> to something 5cm or larger.

\title{Beyond Literal Token Overlap: Token Alignability for Multilinguality}

\author{
  \textbf{Katharina Hämmerl\textsuperscript{1,2}},
  \textbf{Tomasz Limisiewicz\textsuperscript{3}},
  \\
  \textbf{Jindřich Libovický\textsuperscript{3}},
 \textbf{Alexander Fraser\textsuperscript{4,2}}
\\
  \textsuperscript{1}Centre for Information and Language Processing, LMU Munich
\\
  \textsuperscript{2}Munich Center for Machine Learning
\\
  \textsuperscript{3}Faculty of Mathematics and Physics, Charles University, Czech Republic
\\
  \textsuperscript{4}Technical University of Munich, Germany
\\
\small{
    \textbf{Correspondence:} \texttt{haemmerl [at] cis [dot] lmu [dot] de}
  }
}

\begin{document}
\maketitle
\begin{abstract}

Previous work has considered token overlap, or even similarity of token distributions, as predictors for multilinguality and cross-lingual knowledge transfer in language models.
However, these very literal metrics assign large distances to language pairs with different scripts, which can nevertheless show good cross-linguality.
This limits the explanatory strength of token overlap for knowledge transfer between language pairs that use distinct scripts or follow different orthographic conventions.
In this paper, we propose \textit{subword token alignability} as a new way to understand the impact and quality of multilingual tokenisation.
In particular, this metric predicts multilinguality much better when scripts are disparate and the overlap of literal tokens is low.
We analyse this metric in the context of both encoder and decoder models, look at data size as a potential distractor, and discuss how this insight may be applied to multilingual tokenisation in future work.
We recommend our subword token alignability metric for identifying optimal language pairs for cross-lingual transfer, as well as to guide the construction of better multilingual tokenisers in the future.
We publish our code and reproducibility details\footnote{\url{https://github.com/KathyHaem/token-alignability}}.
\vspace{5pt}
\end{abstract}


\section{Introduction}


\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{figures/unigram-20/0_unigram-20.page_one.pdf}
    \caption{Eflomal score (bottom), a measure of token alignability, predicts downstream transfer performance better than the previous metric of distributional token overlap (top).
    The difference is especially stark for language pairs with \textcolor{magenta}{different scripts ($\bullet$)}, compared to language pairs with the \textcolor{teal}{same script ($\times$)}. The orange line
    shows the linear fit across all included pairs.
    }
    \label{fig:jsd-vs-eflomal}
\end{figure}


Highly multilingual language models have received plenty of research attention in recent years.
\textit{Cross-lingual alignment} of representations, that is, the similar representation of similar meanings regardless of input language
\citep{libovicky-etal-2020-language, hammerl-etal-2024-understanding}, as well as good downstream cross-lingual transfer ability
(\citealp[cf.][]{huang-etal-2019-unicoder, schuster-etal-2019-cross-lingual, hu2020xtreme, pham-etal-2024-unibridge}, etc.), have been considered desirable properties for such models.
Representation alignment is typically seen as a key contributing factor to transfer ability, which in turn enables efficient handling of numerous task-language combinations.
%
A number of papers have asked when and why information is shared across language boundaries in multilingual models and enables cross-lingual transfer (\citealp{dufter-schutze-2020-identifying,deshpande-etal-2022-bert,limisiewicz-etal-2023-tokenization,hua-etal-2024-mothello, schäfer2024rolelanguageimbalancecrosslingual}, inter alia).

Token overlap, i.e., the occurrence of identical tokens in the corpora of multiple languages, has been shown to affect the cross-lingual capabilities of models \cite{wu-dredze-2019-beto}.
Another approach is to compare the distributions of token literals in parallel corpora (\citealp{limisiewicz-etal-2023-tokenization}).
Still, both metrics have a crucial limitation:
They cannot explain why related languages with different scripts are well-aligned by the models (see~\S~\ref{subsec:related-tokenisation}).


Here, we propose another angle: token alignability.
This concept captures the intuition that models may rely on statistical correspondences between subword tokens (`token alignment') that are more nuanced than literal string matching.
From token alignments produced by a statistical word aligner, we derive two kinds of \textit{token alignability scores} for any language pair in a multilingual tokeniser: one directional, one symmetrised (\S~\ref{subsec:alignability-calc}).

We compute correlations of these scores both to downstream transfer performance on classification and sequence labelling
tasks~(cf.~\S~\ref{subsec:def-transfer}),
and to measures of cross-lingual alignment in the model representations (cf.~\S~\ref{subsec:def-cla}).
Our primary object of study is a set of small encoder models trained with several different multilingual tokenisers (BPE, Unigram, and `TokMix').
Furthermore, we also consider recent larger, pre-trained decoder models.
In addition to showing that token alignability is a better predictor of downstream cross-lingual transfer than distributional overlap (\S~\ref{subsec:main-results}), we consider the impact of pre-training data size (\S~\ref{subsec:data-size}),
and show the correlation of token alignability with representation alignment inside the model
(also~\S~\ref{subsec:main-results}).
Finally, we discuss how this insight may be applied to future multilingual tokenisers (\S~\ref{sec:future}).


\section{Related Work}

Subword tokenisation is currently the standard input processing approach of language models, with BPE \citep{sennrich-etal-2016-neural} and UnigramLM \citep{kudo-2018-subword} being the most common algorithms for deriving these tokens.
However, there has been increased interest in recent years in addressing limitations of the subword token paradigm \citep[e.g.,][]{alkaoud-syed-2020-importance, hofmann-etal-2022-embarrassingly, schmidt2024tokenizationcompression}
or even moving beyond it \citep[e.g.,][]{xue-etal-2022-byt5, mofijul-islam-etal-2022-vocabulary}.

\subsection{Influence of tokenisers on cross-linguality}\label{subsec:related-tokenisation}

Most relevant for our purposes are measurements of tokeniser properties
\citep[e.g.,][]{zouhar-etal-2023-tokenization, batsuren2024evaluatingsubwordtokenizationalien}, particularly for multilingual language models.
\citet{limisiewicz-etal-2023-tokenization} measure the distance of a language pair's token vocabulary via
divergence of the two token distributions.
They find that this kind of `soft overlap' measure correlates well with downstream transfer performance, with an important caveat: the observed correlations are strong for language pairs with the same script, but weaker for pairs with different scripts.
%
This is because of how the metric is calculated:
The occurrences of subword tokens are counted on each side of a parallel corpus, giving a distribution per language.
Then, Jensen-Shannon-Divergence (\textbf{JSD}; \citealp{lin2006divergence}) is calculated, which gives a symmetrized distance between the two distributions of subword tokens.
The literal matching limits the predictive power of their metric for pairs with different scripts---for instance, Hindi and Urdu are known to be related languages written in different scripts.
Transfer between them works well, while the computed distance is large.


\subsection{Word Alignment in MT}

\textit{Alignment}, in the sense used in statistical Machine Translation (MT) \citep{brown93:tmo} is
a mapping between parallel sentences, showing which tokens are translations of one another and how often they correspond across whole corpora.
The original intuition behind attention is that it finds this kind of mapping in a contextualised manner \citep{bahdanau2015attention},
whereas statistical word aligners
(we use eflomal; \citealp{oestling-tiedemann-2016-efficientWA}) give a discrete mapping.

\section{Methodology}\label{sec:methods}

Our central analysis relies on rank correlations, showing which tokeniser metrics (\S~\ref{subsec:jsd}, \S~\ref{subsec:alignability-calc}) are more predictive of downstream cross-lingual transfer (\S~\ref{subsec:def-transfer}) and cross-lingual alignment of representations (\S~\ref{subsec:def-cla}).
We ensure that within each task, the metrics are always compared over the same set of language pairs.

\subsection{Distributional/Soft Overlap (JSD)}\label{subsec:jsd}

We measure soft overlap between the token distributions of two tokenised corpora.
We follow the setting used by \citet{limisiewicz-etal-2023-tokenization} and outlined in \S~\ref{subsec:related-tokenisation}, but we compute it on the FLORES-200 corpus \cite{guzman-etal-2019-flores,goyal-etal-2022-flores,nllb2022} for comparison with our proposed metrics.
This score is symmetric between both directions of a language pair.
A lower score corresponds to a smaller distance and is thus better.


\subsection{Token alignability of a language pair}\label{subsec:alignability-calc}

We define the \textit{token alignability score} for a language pair based on the symmetrised word alignment of one parallel corpus after training the tool on another.
To train the priors, we use OPUS-100 data \cite{tiedemann-2012-parallel,zhang-etal-2020-improving} 
for en-xx language pairs, and subsets of MultiCCAligned \cite{tiedemann-2012-parallel,el-kishky-etal-2020-ccaligned}
for non-English language pairs.
Seee Appendix~\ref{sec:app-langs} for a breakdown of language pairs.
For each training corpus, we take up to 300k sentence pairs.

As our test corpus, we use FLORES-200 \cite{guzman-etal-2019-flores,goyal-etal-2022-flores,nllb2022} because of its multi-parallel nature and less noise compared to MultiCCAligned.
Following \citet{vazquez-etal-2019-university},
we run a statistical (discrete) word aligner (specifically \textbf{eflomal}; \citealp{oestling-tiedemann-2016-efficientWA}) on the test corpus with a single iteration. 
Based on the final symmetrised alignment over the test corpus, we can determine:
\begin{itemize}
    \item[a)] The \textit{proportion of 1-1 token alignments} (higher is better), i.e., the rate of subword tokens in the source language text with a one-to-one correspondence to subword tokens of the target language text. We take this measure per direction, since it can be markedly lower if the source language is over-segmented.
    \item[b)] The \textit{eflomal score} (lower is better), which represents the tool's estimation of the ``maximum unnormalized log-probability of links
    in the last sampling iteration'' \citep{vazquez-etal-2019-university}, given the learned priors over the subword vocabulary and corpus. We average this score over both directions of a language pair.
\end{itemize}


\subsection{Downstream cross-lingual transfer}\label{subsec:def-transfer}

We were able to obtain model instances with several distinct tokenisers (BPE, Unigram, TokMix), and results for downstream cross-lingual transfer, from the authors of \citet{limisiewicz-etal-2023-tokenization}.
See Appendix~\ref{sec:encoder_architecture} for brief model descriptions.
This allowed us to run correlation analyses without retraining the models, instead testing our metrics against an existing set of experiments.
The downstream results were obtained by fine-tuning the models on a given source language
(any of the available languages for the task) and evaluating on a target language,
resulting in many data points.
The tasks tested are XNLI \citep{conneau-etal-2018-xnli},
part-of-speech tagging (POS) and
dependency tagging (UD) (both based on~\citealp{zeman-etal-2019-ud25}), 
and named entity recognition (NER; \citealp{pan-etal-2017-cross}).
We always use Spearman's rank correlation to estimate the metrics' predictive power, following the previous work.

\subsection{Cross-lingual embedding alignment}\label{subsec:def-cla}

We measure
cross-lingual alignment between a language pair as retrieval accuracy on the Tatoeba dataset \citep{artetxe-schwenk-2019-massively}
as well as the FLORES-200 development set.
% 
Following \citet{jones-etal-2021-massively}, we additionally compute average margin distances on the latter, that is, how much closer the correct match is to the source sentence than other target-side sentences are.
We do not compute word-level embedding alignment scores.

For encoder models, we create sentence embeddings by feeding the sentence to the model and averaging the encoder representations from layer 7 (with attention mask applied).
The reasoning is that the middle layers in XLM-R and similar encoder models, such as the ones we use, have been found to be more cross-lingually aligned
than the output layers \citep[e.g.][]{muller-etal-2021-first}.
For decoder models, we follow \citet{jiang2023scalingsentenceembeddingslarge} in using the prompt
``This sentence: \{sentence\} means in one word:'', then taking the last token representation of the last hidden layer as the sentence embedding.

\section{Results and Discussion}

\subsection{Main results}\label{subsec:main-results}

\begin{table}[t]

\footnotesize
\centering
\setlength{\tabcolsep}{4.5pt}
%\input{|python scripts/task_transfer.py unigram --include-all}
\begin{tabular}{l ccc@{\hskip 10pt}ccc@{\hskip 10pt}ccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{JSD} & \multicolumn{3}{c}{one-to-one} & \multicolumn{3}{c}{eflomal} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& all & $=$ & $\neq$ & all & $=$ & $\neq$ & all & $=$ & $\neq$ \\
\midrule
XNLI  & \corr{0.32705537480624125}{-.33} & \corr{0.5670424483986245}{-.57} & \corr{0.3962737225709869}{\bf -.40} & \corr{0.29486428004102955}{\hphantom{-}.29} & \corr{0.5013683634373289}{\hphantom{-}.50} & \corr{0.21166660493168038}{\hphantom{-}.21} & \corr{0.4541827018801494}{\bf -.45} & \corr{0.6010430594244608}{\bf -.60} & \corr{0.3809108295618151}{-.38} \\
POS  & \corr{0.4470465041220072}{-.45} & \corr{0.6427212277787118}{\bf -.64} & \corr{0.4518107260414783}{-.45} & \corr{0.32066740982870195}{\hphantom{-}.32} & \corr{0.35960591133004927}{\hphantom{-}.36} & \corr{0.2900335205219909}{\hphantom{-}.29} & \corr{0.6406349884355937}{\bf -.64} & \corr{0.5012348141550704}{-.50} & \corr{0.6431149551523524}{\bf -.64} \\
UD  & \corr{0.2272662636409092}{-.23} & \corr{0.24787542231738713}{-.25} & \corr{0.24526885371739598}{-.25} & \corr{0.16003998342107087}{\hphantom{-}.16} & \corr{0.33278598795840175}{\hphantom{-}.33} & \corr{0.12654130128388702}{\hphantom{-}.13} & \corr{0.40921728295269644}{\bf -.41} & \corr{0.36084519443548835}{\bf -.36} & \corr{0.41551787295698384}{\bf -.42} \\
NER  & \corr{0.6288810824082914}{\bf -.63} & \corr{0.24690842283616005}{-.25} & \corr{0.4901424878565244}{\bf -.49} & \corr{0.28648233259060296}{\hphantom{-}.29} & \corr{0.3486095661846496}{\bf \hphantom{-}.35} & \corr{0.25094399137412293}{\hphantom{-}.25} & \corr{0.5164127888382387}{-.52} & \corr{0.2103624107918187}{-.21} & \corr{0.48112651723826266}{-.48} \\

\bottomrule
\end{tabular}

(a) Unigram

\vspace{5pt}

%\input{|python scripts/task_transfer.py bpe --include-all}
\begin{tabular}{l ccc@{\hskip 10pt}ccc@{\hskip 10pt}ccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{JSD} & \multicolumn{3}{c}{one-to-one} & \multicolumn{3}{c}{eflomal} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& all & $=$ & $\neq$ & all & $=$ & $\neq$ & all & $=$ & $\neq$ \\
\midrule
XNLI  & \corr{0.5500301354267327}{\bf -.55} & \corr{0.4463951189521087}{-.45} & \corr{0.40408864401690736}{\bf -.40} & \corr{0.11453009246856921}{\hphantom{-}.11} & \corr{0.4625068418171866}{\bf \hphantom{-}.46} & \corr{0.05311941788014686}{\hphantom{-}.05} & \corr{0.44160198880160495}{-.44} & \corr{0.39374901155726544}{-.39} & \corr{0.29397695850178673}{-.29} \\
POS  & \corr{0.17227924150740712}{-.17} & \corr{0.6503987851071265}{\bf -.65} & \corr{0.07991243237853478}{-.08} & \corr{0.3528880268918219}{\hphantom{-}.35} & \corr{0.4395183360700602}{\hphantom{-}.44} & \corr{0.3329702643456619}{\hphantom{-}.33} & \corr{0.48753891439749886}{\bf -.49} & \corr{0.5231706922362551}{-.52} & \corr{0.46249046399642263}{\bf -.46} \\
UD  & \corr{0.15656120384151526}{-.16} & \corr{0.3005215297122304}{-.30} & \corr{0.14598684532143233}{-.15} & \corr{0.1847619225228333}{\hphantom{-}.18} & \corr{0.28735632183908044}{\hphantom{-}.29} & \corr{0.19114517333755224}{\hphantom{-}.19} & \corr{0.32838166067096686}{\bf -.33} & \corr{0.3575548127233106}{\bf -.36} & \corr{0.3201852377420793}{\bf -.32} \\
NER  & \corr{0.5131946615211677}{-.51} & \corr{0.37615651421248936}{-.38} & \corr{0.2984246755675126}{-.30} & \corr{0.29932113501342816}{\hphantom{-}.30} & \corr{0.5261401557285873}{\bf \hphantom{-}.53} & \corr{0.2828958974597811}{\hphantom{-}.28} & \corr{0.5738643304671307}{\bf -.57} & \corr{0.2549307181629667}{-.25} & \corr{0.5230900173043488}{\bf -.52} \\

\bottomrule
\end{tabular}

(b) BPE

\vspace{5pt}

%\input{|python scripts/task_transfer.py tokmix --include-all}
\begin{tabular}{l ccc@{\hskip 10pt}ccc@{\hskip 10pt}ccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{JSD} & \multicolumn{3}{c}{one-to-one} & \multicolumn{3}{c}{eflomal} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& all & $=$ & $\neq$ & all & $=$ & $\neq$ & all & $=$ & $\neq$ \\
\midrule
XNLI  & \corr{0.4507226442455602}{\bf -.45} & \corr{0.438717561623694}{\bf -.44} & \corr{0.4276799004135657}{\bf -.43} & \corr{0.06999578184951909}{-.07} & \corr{0.3448275862068966}{\hphantom{-}.34} & \corr{0.2262795053264527}{-.23} & \corr{0.35846844619079316}{-.36} & \corr{0.4343303860074571}{-.43} & \corr{0.22372797932156918}{-.22} \\
POS  & \corr{0.2057630630846998}{-.21} & \corr{0.6898833656532588}{\bf -.69} & \corr{0.11023558578565013}{-.11} & \corr{0.10519607066096628}{\hphantom{-}.11} & \corr{0.23371647509578544}{\hphantom{-}.23} & \corr{0.05830147605798354}{\hphantom{-}.06} & \corr{0.5363006387463846}{\bf -.54} & \corr{0.5100091653875443}{-.51} & \corr{0.5109600439457852}{\bf -.51} \\
UD  & \corr{0.17835161106603833}{-.18} & \corr{0.17439023074541837}{-.17} & \corr{0.16273364808610852}{-.16} & \corr{0.008905818013977407}{\hphantom{-}.01} & \corr{0.039956212370005476}{\hphantom{-}.04} & \corr{0.0026576041402034505}{-.00} & \corr{0.38386476704973804}{\bf -.38} & \corr{0.3301349651218297}{\bf -.33} & \corr{0.38619879747725644}{\bf -.39} \\
NER  & \corr{0.38373709728881145}{-.38} & \corr{0.32445727766195764}{\bf -.32} & \corr{0.08591988530953415}{-.09} & \corr{0.11305713045601282}{\hphantom{-}.11} & \corr{0.23203559510567295}{\hphantom{-}.23} & \corr{0.07801022071494197}{\hphantom{-}.08} & \corr{0.48215278853858723}{\bf -.48} & \corr{0.2745407734062718}{-.27} & \corr{0.4224916576006504}{\bf -.42} \\

\bottomrule
\end{tabular}

(c) TokMix

\caption{Spearman's rank correlation of downstream transfer with JSD, proportion of one-to-one alignment, and eflomal score, for language pairs with the same ($=$) and with a different script ($\neq$).
}
\label{tab:tokeniser-vs-downstream}
\end{table}

Table~\ref{tab:tokeniser-vs-downstream} shows that eflomal score is better than JSD at predicting downstream transfer performance in the multilingual encoder models from \citet{limisiewicz-etal-2023-tokenization}.
This holds across all three tokenisation types, particularly for the word-level tasks.
XNLI seems to behave differently, possibly because it is a sentence-level task in contrast with the other three, or because it has results available for fewer, mostly higher-resource, language pairs.
Note also that XNLI transfer results were quite low in absolute terms.

Intuitively, JSD clusters language pairs with different scripts very closely together, even when they have markedly different transfer performance (see visualisations in App. Fig.~\ref{fig:unigram-eflomal-over-jsd}--\ref{fig:tokmix-eflomal-over-jsd}).
Eflomal score is not confounded by the different scripts, yielding better rankings within that group, and usually a
better overall ranking.
Meanwhile, the proportion of one-to-one alignments shows weaker or no correlation.
This implies that the proportion of one-to-one alignments may be too simplistic here, while the eflomal score, as an estimate of log-probability, captures more nuance.

Table~\ref{tab:tokeniser-vs-embedding-alignment} lists correlations of JSD and eflomal score with three measures of embedding similarity (retrieval on Tatoeba and FLORES-200, and average margin on FLORES-200).
These results are for the BPE model.
The underlying distributions are shown in Fig.~\ref{fig:bpe-metrics-vs-cla}.
We see that JSD gives clear correlations for all three measures in \textit{same-script} language pairs, while eflomal score correlates more strongly on \textit{different-script} language pairs.

All the correlations are much stronger on the FLORES dataset, likely because this dataset was used to calculate the tokeniser metrics in the first place.
We can therefore see these as a kind of upper bound on how well the tokeniser metrics can predict cross-lingual alignment.
%
The fact that the eflomal score is less predictive in the same-script group may indicate that the model does rely on more literal token matching when that information is available.
To the extent that the behaviour differs from what is seen in Table~\ref{tab:tokeniser-vs-downstream}, this underscores that cross-lingual embedding alignment, as measured by similarity, is just one factor in the cross-lingual transfer ability of the model.


\begin{table}[t]

\footnotesize\centering
\setlength{\tabcolsep}{4.5pt}
%\input{|python scripts/embedding_alignment.py bpe --include-all}
\begin{tabular}{l ccc@{\hskip 10pt}ccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{JSD} & \multicolumn{3}{c}{eflomal} \\
\cmidrule(lr{10pt}){2-4} \cmidrule(lr){5-7}
& all & $=$ & $\neq$ & all & $=$ & $\neq$ \\
\midrule
F1 Flores   & \corr{0.7917754931925537}{-.79} & \corr{0.6969696969696969}{\bf -.70} & \corr{0.6678991596638655}{-.67} & \corr{0.825062517365935}{\bf -.83} & \corr{0.6242424242424242}{-.62} & \corr{0.8134933973589434}{\bf -.81} \\
Avg mgn Flores  & \corr{0.736093359266463}{-.74} & \corr{0.7212121212121211}{\bf -.72} & \corr{0.5878991596638654}{-.59} & \corr{0.8026674076132262}{\bf -.80} & \corr{0.4545454545454545}{-.45} & \corr{0.7933253301320528}{\bf -.79} \\
Tatoeba  & \corr{0.33378096461282464}{\bf -.33} & \corr{0.4588773602950718}{\bf -.46} & \corr{0.18936338104125688}{-.19} & \corr{0.32558510439095356}{-.33} & \corr{0.2660417380699054}{-.27} & \corr{0.24416498319842914}{\bf -.24} \\

\bottomrule
\end{tabular}

\caption{Spearman's rank correlation of embedding alignment with JSD and eflomal scores, on the BPE tokenizer/model.
We show overall correlations (all), same-script ($=$), and different script ($\neq$) pairs.}
\label{tab:tokeniser-vs-embedding-alignment}
\end{table}


\subsection{Is data size a confounder?}\label{subsec:data-size}

Table~\ref{tab:datasize-vs-downstream} shows data size in the trained encoders (and tokenizers), correlated with downstream transfer performance from English.
Here, we consider only the pairs where English is the source language because English is generally the most dominant language, and there is some research suggesting that models ``work'' in English \citep{wendler-etal-2024-llamas}.
This correlates very well for XNLI, but much less in the other tasks.
Again, XNLI stands out as a sentence-level task with fewer overall language pairs and relatively low transfer performance, so this result should be taken with a grain of salt.
Overall, the correlations suggest that there is indeed a connection between data size and transfer ability, but data size cannot account for the whole effect.
%
See also Table~\ref{tab:datasize-vs-metrics} in the Appendix.


\begin{table}[t]

\centering\footnotesize
\begin{tabularx}{170pt}{l *4{>{\Centering}X}}
\toprule
Model & XNLI & POS & UD & NER \\ \midrule
Unigram & \C{.87} & \C{.37} & \C{.33} & \C{.34} \\
BPE & \C{.80} & \C{.37} & \C{.49} & \C{.33} \\
TokMix & \C{.81} & \C{.34} & \C{.54} & \C{.26} \\
\bottomrule
\end{tabularx}

\caption{
Rank correlation of downstream transfer from English with training size of the target language.}
\label{tab:datasize-vs-downstream}
\end{table}



\subsection{What about decoders?}

We additionally experiment with Mistral-7B-v0.1, Aya23-8B, and Llama-3-8B-Instruct, varying the model type, as well as the amount of multilinguality in pre- and post-training.
For these, we calculate alignability scores, JSD, and representation alignment for a subset of language pairs.
Table~\ref{tab:aya-cla-vs-metrics} shows rank correlation results.
In Mistral, eflomal is still more predictive of overall representation alignment than JSD, while in Aya23 and Llama3, the opposite is true.
This may suggest that cross-linguality in these decoder models works differently than in encoder models, or that they \textit{do} rely more on literal token matches for their cross-linguality.
Nevertheless, in Llama3-8B-Instruct, the eflomal score shows an unusually high correlation for same-script language pairs.
%
Note also that absolute retrieval performance from the Mistral and Llama3 representations is quite low---Aya23 performs better.
The corresponding visualisations are shown in Appendix~\ref{subsec:app-decoders}.


\begin{table}[t]

\footnotesize\centering
\setlength{\tabcolsep}{4.5pt}

\input{decoder_table}

\caption{Spearman's rank correlation of embedding alignment with JSD and eflomal scores, on decoders.
We show overall correlations (all), same-script ($=$), and different script ($\neq$) pairs.
}
\label{tab:aya-cla-vs-metrics}
\end{table}


\section{Future Work}\label{sec:future}

We showed here that good tokeniser alignability correlates well with crosslinguality, an important factor for the performance of multilingual language models.
Hence, the eflomal score may be applied to improve vocabulary learning for fairer multilingual tokenisers (see also \citealp{ahia2024magnetimprovingmultilingualfairness,limisiewicz-etal-2024-myte}).
However, a naive implementation, where alignability score is checked at every decision point (merges for BPE, or pruning tokens for Unigram), is far too intensive.
Therefore, future work in this area will require finding suitable approximations, like calculating alignability score difference for some fraction (e.g., on the order of 10\%) of all candidate tokens at a time.


\section{Conclusion}

We have proposed a new metric for describing the quality of a multilingual tokenisation,
with implications for cross-lingual alignment in multilingual pre-trained models: token alignability.
This metric is particularly relevant for language pairs with different scripts and thus no literal token overlap.
We showed correlations with transfer performance on
downstream classification tasks, as well as with measures of cross-lingual alignment.
These findings show the potential of our token alignability metric to guide the development of robust multilingual tokenisers and to identify suitable language pairs for cross-lingual transfer.


\section*{Limitations}

Our study has focused on a relatively small set of models.
%
We do not have extensive cross-lingual transfer experiments for decoder models because fine-tuning each model on any number of languages would take too much compute.
Some of the downstream results from the previous work (particularly for XNLI) were quite poor in absolute terms, so they may not entirely reflect the situation in a higher-performance model.
While alignability score for one language pair is not very time-consuming to compute (and can be done on CPU), the time adds up quickly for a broader set of language pairs.
%
In its present formulation, alignability is also a corpus-wide score,
meaning it would require reformulating for word-level tasks.


\section*{Acknowledgments}

Thank you to Jindra Helcl for helpful discussions about this research.
KH is supported by the Munich Center for Machine Learning, and did much of the work on this project during a research visit to Prague.
The work at CUNI was supported by the Charles University project PRIMUS/23/SCI/023.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{anthology,custom}

\appendix

\section{Languages Included}
\label{sec:app-langs}

We start from a set of 20 languages, namely the ones used by \citet{limisiewicz-etal-2023-tokenization} for their tokenizers: Arabic~(ar), Turkish~(tr), Chinese~(zh), Greek~(el), Spanish~(es), English~(en), Swahili~(sw), Hindi~(hi), Marathi~(mr), Urdu~(ur), Tamil~(ta), Telugu~(te), Thai~(th), Russian~(ru), Bulgarian~(bg), Hebrew~(he), Georgian~(ka), Vietnamese~(vi), French~(fr), and German~(de).

This gives us up to 190 language pairs (before accounting for direction), but we typically do not calculate numbers for \textit{all} pairs, and each downstream task only has data available for some subset of the languages.
We do compute all language pairs with English as either the source or target language.
For non-English pairs, we compute token alignability for the product of these languages: ar, tr, zh, hi, ur, mr, ru, bg, vi, fr, es, ta, he.


\section{Encoder Details}
 \label{sec:encoder_architecture}

The encoders were trained by \citet{limisiewicz-etal-2023-tokenization}. The models' architecture is based on XLM-RoBERTa \cite{conneau-etal-2020-unsupervised}.
The size of the embeddings is 768, the number of attention layers is 8, and the number of attention heads is 6. 
The maximum sentence length is 128, and the vocabulary size in each tokenizer is 120000. 
The number of parameters is 150M, roughly half the size of XLM-R$_{base}$.
See \citet{limisiewicz-etal-2023-tokenization} for training details.
Their training corpus was a 10\% subset of CC-100, with a balancing factor of $\alpha=0.25$ (cf.~\citealp{conneau-lample-2019-cross}).
The model names BPE, Unigram, and TokMix are shorthand for their different vocabulary creation approaches.
For BPE and Unigram, they simply applied the respective algorithm to the training set of all 20 languages, until reaching the target vocabulary size of 120000.
For TokMix, they
trained Unigram LM tokenisers for each language separately, and merged them by averaging token probabilities across tokenisers, then sorting and trimming.
Our own experiments with these models were able to run on CPU.


\section{Additional Detail on Results}

\subsection{Graphs for Main Results}\label{subsec:extra-graphs-main}

Figures~\ref{fig:unigram-eflomal-over-jsd},~\ref{fig:bpe-eflomal-over-jsd}, and~\ref{fig:tokmix-eflomal-over-jsd} visualise the distributions underlying Table~\ref{tab:tokeniser-vs-downstream}.
The sets of same- and different-script language pairs are colour-coded, and the overall correlations along with p-values are placed in the bottom left corner of each graph.
%
Similarly, Figure~\ref{fig:bpe-metrics-vs-cla} shows the distributions behind Table~\ref{tab:tokeniser-vs-embedding-alignment}.


\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{figures/unigram-20/0_unigram-20.tokenizer_vs_downstream.pdf}
  \caption{Unigram model: The eflomal score generally correlates better with downstream transfer than JSD. NER is the exception.
  Proportion of 1-1 token alignments, while it also breaks up the cluster of different-script language pairs, shows weaker or no correlations.
  }
  \label{fig:unigram-eflomal-over-jsd}
\end{figure*}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{figures/bpe-20/0_bpe-20.tokenizer_vs_downstream.pdf}
  \caption{BPE model: The eflomal score correlates better with downstream transfer than JSD, with the exception of XNLI.
  Proportion of 1-1 token alignments, while it also breaks up the cluster of different-script language pairs, shows weaker or no correlations.
  }
  \label{fig:bpe-eflomal-over-jsd}
\end{figure*}


\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{figures/tokmix-20/0_tokmix-20.tokenizer_vs_downstream.pdf}
  \caption{TokMix model: The eflomal score correlates better with downstream transfer than JSD, again with the exception of XNLI.
  Proportion of 1-1 token alignments, while it also breaks up the cluster of different-script language pairs, shows no correlations.
  }
  \label{fig:tokmix-eflomal-over-jsd}
\end{figure*}

% -----------------



\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/bpe-20/0_bpe-20.eflomal_jsd_vs_embeds.pdf}
\caption{BPE Model: Eflomal scores correlates well with cross-lingual embedding alignment. Nevertheless, both metrics perform similarly over the Tatoeba dataset.
}
\label{fig:bpe-metrics-vs-cla}
\end{figure*}


% --------------------

\subsection{Analysis by Language Family}

Similarly to our analysis of scripts, we assign language \textit{pairs} to groups of same vs. different macro language families.
We do this because some language families have just one representative in our set, while Indo-European accounts for many of the languages.
We do not subdivide the macro language families for this analysis.

Table~\ref{tab:by-family} shows the correlations of eflomal score with downstream cross-lingual transfer, over different-script pairs.
We then split by same and different language families.
In several cases, we see very similar correlations as on different-script pairs in general.
XNLI stands out again, with pairs from the same language family tending to be more correlated across all tokenisers.


\begin{table}[t]

\footnotesize
\centering
\setlength{\tabcolsep}{4.5pt}
%\input{|python scripts/by_family.py}
\begin{tabular}{l ccc@{\hskip 12pt}ccc@{\hskip 12pt}ccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{Unigram} & \multicolumn{3}{c}{BPE} & \multicolumn{3}{c}{TokMix} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
& all & $=$ & $\neq$ & all & $=$ & $\neq$ & all & $=$ & $\neq$ \\
\midrule
XNLI  & \corr{0.3809108295618151}{-.38} & \corr{0.5957999109809452}{-.60} & \corr{0.22493424068017584}{-.22} & \corr{0.29397695850178673}{-.29} & \corr{0.34480115955558166}{-.34} & \corr{0.26324925352280415}{-.26} & \corr{0.22372797932156918}{-.22} & \corr{0.4231688640995684}{-.42} & \corr{0.22577756613005404}{-.23} \\
POS  & \corr{0.6431149551523524}{-.64} & \corr{0.41763370895828206}{-.42} & \corr{0.6938120272539409}{-.69} & \corr{0.46249046399642263}{-.46} & \corr{0.23428232453757286}{-.23} & \corr{0.47773538200050264}{-.48} & \corr{0.5109600439457852}{-.51} & \corr{0.383231300351042}{-.38} & \corr{0.44364409367186686}{-.44} \\
UD  & \corr{0.41551787295698384}{-.42} & \corr{0.2990511049880749}{-.30} & \corr{0.40701834958710553}{-.41} & \corr{0.3201852377420793}{-.32} & \corr{0.08148950418698185}{-.08} & \corr{0.36936357835324063}{-.37} & \corr{0.38619879747725644}{-.39} & \corr{0.3317237835536101}{-.33} & \corr{0.33483400068332}{-.33} \\
NER  & \corr{0.48112651723826266}{-.48} & \corr{0.3201922499422448}{-.32} & \corr{0.5213832969100821}{-.52} & \corr{0.5230900173043488}{-.52} & \corr{0.5089250167149244}{-.51} & \corr{0.5133869874572256}{-.51} & \corr{0.4224916576006504}{-.42} & \corr{0.3267267856553518}{-.33} & \corr{0.3767321092436638}{-.38} \\

\bottomrule
\end{tabular}

\caption{Spearman's rank correlation of downstream transfer with JSD, proportion of one-to-one alignment, and eflomal score.
This analysis shows only language pairs that use \textit{different scripts}, further differentiated by whether they are in the same ($=$) or a different ($\neq$) \textit{language family}.
}
\label{tab:by-family}
\end{table}


\subsection{Data Size Correlated with Metrics}

Table~\ref{tab:datasize-vs-metrics} shows the correlations of target language pre-training data sizes with our tokeniser metrics.


\begin{table}[h]

\footnotesize\centering
%\input{| python scripts/data_size_vs_align.py}
\begin{tabularx}{.9\columnwidth}{l *3{>{\Centering}X}}
\toprule
& JSD & one-to-one & eflomal \\ \midrule
Unigram & \corr{0.3025516248337339}{-.30} & \corr{0.4890078587428955}{\hphantom{0}.49} & \corr{0.4362372265044535}{-.44} \\
BPE & \corr{0.4028158260867736}{-.40} & \corr{0.23834735561029616}{\hphantom{0}.24} & \corr{0.5356219172201858}{-.54} \\
TokMix & \corr{0.48373079551905124}{-.48} & \corr{0.29991309322181176}{\hphantom{0}.30} & \corr{0.5180317064740385}{-.52} \\
\bottomrule\end{tabularx}

\caption{Spearman's rank correlation of the target language pre-training data size with our metrics. Only pairs with English as the source language are considered for this table.}
\label{tab:datasize-vs-metrics}
\end{table}

% ----------------------

\subsection{Graphs for Decoder Results}\label{subsec:app-decoders}

The underlying distributions of Table~\ref{tab:aya-cla-vs-metrics} are visualised in Figure~\ref{fig:metrics-vs-cla-aya} for Aya23-8B,
Figure~\ref{fig:metrics-vs-cla-llama} for Llama-3-8B-Instruct, and Figure~\ref{fig:metrics-vs-cla-mistral} for Mistral.
Both in Llama3-8B-Instruct and Aya23-8B, JSD correlates more strongly with cross-lingual alignment of representations, but all correlations here are weaker than is the case in the encoder models.
For Mistral, eflomal score correlates more with cross-lingual alignment, which is in contrast to the other two decoder models.

Also, note that Aya23 shows decent retrieval performance, while the representations from Llama3 and Mistral both perform poorly on retrieval F1.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/aya/0_aya.eflomal_jsd_vs_embeds.pdf}
\caption{Aya23: Spearman's rank correlation of cross-lingual embedding alignment with JSD and eflomal score.
}
\label{fig:metrics-vs-cla-aya}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/llama3/0_llama3.eflomal_jsd_vs_embeds.pdf}
\caption{Llama3: Spearman's rank correlation of cross-lingual embedding alignment with JSD and eflomal score.
}
\label{fig:metrics-vs-cla-llama}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/mistral/0_Mistral.eflomal_jsd_vs_embeds.pdf}
\caption{Mistral: Spearman's rank correlation of cross-lingual embedding alignment with JSD and eflomal score.
}
\label{fig:metrics-vs-cla-mistral}
\end{figure}

\end{document}
