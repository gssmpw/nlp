\section{Related Work}
Subword tokenisation is currently the standard input processing approach of language models, with BPE ____ and UnigramLM ____ being the most common algorithms for deriving these tokens.
However, there has been increased interest in recent years in addressing limitations of the subword token paradigm ____
or even moving beyond it ____.

\subsection{Influence of tokenisers on cross-linguality}\label{subsec:related-tokenisation}

Most relevant for our purposes are measurements of tokeniser properties
____, particularly for multilingual language models.
____ measure the distance of a language pair's token vocabulary via
divergence of the two token distributions.
They find that this kind of `soft overlap' measure correlates well with downstream transfer performance, with an important caveat: the observed correlations are strong for language pairs with the same script, but weaker for pairs with different scripts.
%
This is because of how the metric is calculated:
The occurrences of subword tokens are counted on each side of a parallel corpus, giving a distribution per language.
Then, Jensen-Shannon-Divergence (\textbf{JSD}; ____) is calculated, which gives a symmetrized distance between the two distributions of subword tokens.
The literal matching limits the predictive power of their metric for pairs with different scripts---for instance, Hindi and Urdu are known to be related languages written in different scripts.
Transfer between them works well, while the computed distance is large.


\subsection{Word Alignment in MT}

\textit{Alignment}, in the sense used in statistical Machine Translation (MT) ____ is
a mapping between parallel sentences, showing which tokens are translations of one another and how often they correspond across whole corpora.
The original intuition behind attention is that it finds this kind of mapping in a contextualised manner ____,
whereas statistical word aligners
(we use eflomal; ____) give a discrete mapping.