\section{Related Work}
Subword tokenisation is currently the standard input processing approach of language models, with BPE \citep{sennrich-etal-2016-neural} and UnigramLM \citep{kudo-2018-subword} being the most common algorithms for deriving these tokens.
However, there has been increased interest in recent years in addressing limitations of the subword token paradigm \citep[e.g.,][]{alkaoud-syed-2020-importance, hofmann-etal-2022-embarrassingly, schmidt2024tokenizationcompression}
or even moving beyond it \citep[e.g.,][]{xue-etal-2022-byt5, mofijul-islam-etal-2022-vocabulary}.

\subsection{Influence of tokenisers on cross-linguality}\label{subsec:related-tokenisation}

Most relevant for our purposes are measurements of tokeniser properties
\citep[e.g.,][]{zouhar-etal-2023-tokenization, batsuren2024evaluatingsubwordtokenizationalien}, particularly for multilingual language models.
\citet{limisiewicz-etal-2023-tokenization} measure the distance of a language pair's token vocabulary via
divergence of the two token distributions.
They find that this kind of `soft overlap' measure correlates well with downstream transfer performance, with an important caveat: the observed correlations are strong for language pairs with the same script, but weaker for pairs with different scripts.
%
This is because of how the metric is calculated:
The occurrences of subword tokens are counted on each side of a parallel corpus, giving a distribution per language.
Then, Jensen-Shannon-Divergence (\textbf{JSD}; \citealp{lin2006divergence}) is calculated, which gives a symmetrized distance between the two distributions of subword tokens.
The literal matching limits the predictive power of their metric for pairs with different scripts---for instance, Hindi and Urdu are known to be related languages written in different scripts.
Transfer between them works well, while the computed distance is large.


\subsection{Word Alignment in MT}

\textit{Alignment}, in the sense used in statistical Machine Translation (MT) \citep{brown93:tmo} is
a mapping between parallel sentences, showing which tokens are translations of one another and how often they correspond across whole corpora.
The original intuition behind attention is that it finds this kind of mapping in a contextualised manner \citep{bahdanau2015attention},
whereas statistical word aligners
(we use eflomal; \citealp{oestling-tiedemann-2016-efficientWA}) give a discrete mapping.