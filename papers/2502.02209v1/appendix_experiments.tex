% \section{Reproducibility Checklist}
% Please refer to the technical appendix titled 'Reproducibility Checklist', which is attached in the supplementary material of this submission.

\section{Experimental Setup \label{sec:hyperParams}} 
All training experiments were performed
on public datasets using a single A100 40GB GPU for a
maximum of two days. All experiments were conducted using PyTorch, and results are averaged over three seeds. All hyperparameters are detailed in Tab.~\ref{tab:NLPhyperpams} and Tab.~\ref{tab:Vsionhyperpams}.

\begin{table}[h]
\centering
\small
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Model-width & 192 \\
Number of layers & 24 \\
Number of patches & 196 \\
%Scan Mode {\color{red} ???} & one directional \\
Batch-size & 512 \\
Optimizer & AdamW \\
Momentum & \( \beta_1, \beta_2 = 0.9, 0.999 \) \\
Base learning rate & $5e-4$ \\
Weight decay & 0.1 \\
Dropout & 0 \\
Training epochs & 300 \\
Learning rate schedule & cosine decay \\
Warmup epochs & 5 \\
Warmup schedule & linear \\ 
Degree of Taylor approx. (Eq.~\ref{eq:simplifiedModel}) & 3 \\
\bottomrule
\end{tabular}
\caption{Hyperparameters for image-classification via Vision Mamba variants} 
\label{tab:Vsionhyperpams}
\end{table}

\begin{table}[h]
\centering
\small
\begin{tabular}{l c}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Model-width & 386 \\
Number of layers & 12 \\
Context-length (training) & 1024 \\
Batch-size & 32 \\
Optimizer & AdamW \\
Momentum & \( \beta_1, \beta_2 = 0.9, 0.999 \) \\
Base learning rate & $1.5e-3$ \\
Weight decay & 0.01 \\
Dropout & 0 \\
Training epochs & 20 \\
Learning rate schedule & cosine decay  \\
Warmup epochs & 1  \\
Warmup schedule & linear  \\ 
Degree of Taylor approx. (Eq.~\ref{eq:simplifiedModel}) &  3\\
\bottomrule
\end{tabular}
\caption{Hyperparameters for language modeling via Mamba-based LMs} 
\label{tab:NLPhyperpams}
\end{table}

% \section{Additional Background Material}

% {\noindent\textbf{Rademacher Complexities}}
% We explore the generalization capabilities of overparameterized NNs by analyzing their Rademacher complexity. This measure provides an upper bound on the worst-case generalization gap, which represents the difference between training and testing errors within a specific hypothesis class. It is defined as the expected performance of the class averaged over all possible data labelings, with labels independently and uniformly drawn from the set $\{\pm 1\}$. For further details, refer to \citep{mohri2018foundations, Shalev-Shwartz2014, bartlett2002rademacher}.


% \begin{definition}[Rademacher Complexity] Let $\mathcal{F}$ be a set of real-valued functions $f_w:\mathcal{X} \to \mathbb{R}^\mathcal{C}$ defined over a set $\mathcal{X}$. Given a fixed sample $X = \{ x_j\}_{j=1}^m \in \mathcal{X}^m$, the empirical Rademacher complexity of $\mathcal{F}$ is defined as follows: 
% \begin{equation*}
% \mathcal{R}_{X}(\mathcal{F}) ~:=~ \frac{1}{m} \mathbb{E}_{\xi: \xi_{ic} \sim U[\{\pm 1\}]} \left[ \sup_{f_w \in \mathcal{F}} \sum^{m}_{j=1}\sum^{\mathcal{C}}_{c=1} \xi_{ic} f_w(x_j)_c  \right].
% \end{equation*}
% \end{definition}

% {\color{red}
% It should be moved:
% We focus on training models for classification tasks. The problem is formally defined by a distribution $P$ over pairs $(x,y)\in \cX\times \cY$, where $\cX \subset \R^{\mathcal{D}}$ represents the input space, and $\cY \subset \R^\mathcal{C}$ is the label space containing one-hot encoded labels for the integers $1,\ldots,\mathcal{C}$. 

% We define a hypothesis class $\cF \subset \{f_w:\cX\to \R^\mathcal{C}\}$ (such as a neural network architecture), where each function \( f_w \in \cF \) is parameterized by a vector of trainable weights $w$. Given any input \( x \in \cX \), \( f_w \) provides a predicted label, and the performance is evaluated based on the \emph{expected error}, \(\err_P(f_w) := \mathbb{E}_{(x, y) \sim P}\left[\mathbb{I}\left[\max_{j \neq y}(f_w(x)_j) \geq f_w(x)_{y}\right]\right]\). Here, the indicator function \(\mathbb{I}\) returns 1 for True and 0 for False.

% Since the full distribution \( P \) is not directly accessible, our objective is to train a model \( f_w \) using a training dataset \( S = \{(x_i, y_i)\}_{i = 1}^m \) consisting of independent and identically distributed (i.i.d.) samples from \( P \). We aim to achieve accurate predictions while applying regularization to control the complexity of \( f_w \). 

% To denote the entire $n$th row and $n$th column of a matrix $M$, we use the notation:
% \begin{align*}
% M_{n*} & \text{ denotes the entire } n \text{th row of matrix } M. \\
% M_{*n} & \text{ denotes the entire } n \text{th column of matrix } M. \\
% M_{nk} & \text{ denotes the element in the } n \text{th row and } k \text{th column.}
% \end{align*}
% {\bf Selective State Space Models.\enspace} 
% Time-variant SSMs, namely, the matrices $A,B,C$ of each channel are modified over $L$ time steps. We are focusing on selective SSMs of the following form. 
% A neural network $f_w : \mathbb{R}^{D \times L} \rightarrow \mathbb{R}^{\mathcal{C}}$ takes a sequence $x=(x_{*1},...,x_{*L}) \in \mathbb{R}^{D \times L}$ as input where $L$ is the length of the sequence and $D$ is the dimension of the tokens $x_i$. We denote
% \begin{align*}
%     B_i &= B x_{*i}, B \in \mathbb{R}^{N \times D} \\
%     C_i &= C x_{*i}, C \in \mathbb{R}^{N \times D} \\
%     \Delta_{*i} &= S_{\Delta} x_{*i}, S_{\Delta} \in \mathbb{R}^{D \times D} \rightarrow \Delta \in \mathbb{R}^{D \times L} \\
%     \bar{A}_{j,i} &= (z^2 + \alpha z)(\Delta_{j,i} * \underbrace{A_{j*}}_{\textbf{ $j$th row of A } } ), A \in \mathbb{R}^{D \times N}, \text{$(z^2+\alpha z)$ is an activation function, } \alpha \geq 0 \\ 
%     W &\in \mathbb{R}^{\mathcal{C} \times D} 
% \end{align*}
% }