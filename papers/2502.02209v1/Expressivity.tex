We identify an expressivity gap between S6 and attention via multivariate polynomials. %We characterize this gap with Theorems~\ref{theorem:exprrsFull} and~\ref{theorem:AnyPolywithMambas}.
This gap is delineated through Thm.~\ref{theorem:exprrsFull} and Thm.~\ref{theorem:AnyPolywithMambas}. The former establishes that attention models require $O(\log L)$ layers to represent $L$-degree multivariate polynomials, whereas Mamba models can express such polynomials within a single layer. Furthermore, Thm.~\ref{theorem:AnyPolywithMambas} extends this finding by demonstrating that the expressiveness gap is not limited to anecdotal or edge-case polynomials. Rather, it encompasses a broad spectrum of polynomial functions.

% \begin{assumption}
%     The state size in S6 is equal to the hidden size of transformers.
% \end{assumption}

% {\noindent \textbf{Assumptions}:}
% \begin{enumerate}\label{par:assumptions}
%     \item The state size in S6 is equal to the hidden size of transformers.
%     % \item We assume that the S6 layers employ the simplified polynomial variant described in Eq.~\ref{eq:simplifiedModelFortheoren}.% (Sec.~\ref{sec:simplify}
%     \item We analyze self-attention with polynomial activations $p(x) = x^p$ instead of softmax.
%     % \item We consider the regime of 'many-to-one', which deals with models that operate on sequences of size $L$ and produce a signal output.
% \end{enumerate}
\begin{theorem}\label{theorem:exprrsFull}(informal)
%Under the assumptions specified in~\ref{par:assumptions},
%Let $n\in\mathbb{N}$ be the hidden state of 
Consider an S6 layer and single Transformer layer, both with hidden dimension $N$. 
% Assume S6 layer with hidden-state of dimension $N$, 
For input sequences of length \( L \geq 3 \), a single layer of Mamba is logarithmically more expressively efficient in depth compared to a single causal {\color{black}linear} self-attention layer with a single head and polynomial activations instead of softmax. 
\end{theorem}

We consider this theorem relatively surprising, as transformers are considered highly expressive models, in contrast to state-space layers, which are traditionally constrained. The proof follows from Lemma~\ref{lemma:dir2} and Lemma~\ref{lemma:dir1} which are presented next.

% Below is the proof:
% \begin{proof}[Proof of Theorem~\ref{theorem:exprrsFull}]
% We derive the theorem from the following two lemmas:


\begin{lemma}\label{lemma:dir2}
There exists a function \( f : \mathbb{R}^L \rightarrow \mathbb{R} \) that can be implemented by one channel of S6 such that a single {\color{black}linear} attention head would require at least  \( O(\log L) \) layers to express $f$.%this function.
\end{lemma}

\begin{lemma}\label{lemma:dir1}
Any function that can be expressed by a single {\color{black}causal linear} attention head can %also
be expressed by a single channel of S6.
\end{lemma}

For the complete details and proof of Lemma~\ref{lemma:dir1}, we refer the reader to the appendix (Lemma \ref{lemma:dir1Appendix}). As for Lemma~\ref{lemma:dir2}, we present here a proof sketch for a simplified version (see Lemma~\ref{lemma:dir2appendix}
in the appendix for the complete proof). In this simplified case, our proof focuses exclusively on linear-attention%, omitting the softmax over the attention scores
. Additionally, we consider models that process scalar sequences. 

\begin{proof}[Proof of Lemma~\ref{lemma:dir2} (without Softmax% and scalar sequences
)]
Our proof relies on the characterization of the hypothesis classes that are realizable through S6 and self-attention via {\underline{multivariate polynomials}}. We start by presenting this formulation for S6:

\smallskip
\noindent\textbf{Single S6 Layer as Multivariate Polynomials\quad}
One channel of the variant of the S6 layer we consider is described in detail in Eq.~\ref{eq:simplifiedModel}.
%s the following recurrent rule:
% \begin{equation}\label{eq:simplifiedModelFortheoren1}
%     C_t = S_C x_t, \quad \bar{B}_t = S_B x_t, \quad \bar{A}_t = p_A(S_{\Delta} x_t)
% \end{equation}
%
% \begin{equation}
%     y_{t} = C_t h_{t}, \quad h_{t}= \bar{A}_t h_{t-1} + \bar{B}_t x_{t}, \quad   C_t = S_C x_t \quad \quad \bar{B}_t = S_B x_{t} \quad \quad \bar{A}_t = S_A x_{t} 
% \end{equation}
%
%
Since we are interested in identifying the minimal polynomial degree required to characterize models that employ Eq.~\ref{eq:simplifiedModel}, we can assume that $P_A$ is linear. Thus, we can incorporate the coefficients of $P_A$ into $S_{\Delta}$, namely, $\bar{A}_t = S_{\Delta} x_t$. Consequently, Eq.\ref{eq:simplifiedModel} can be unrolled as follows:
{\small
\begin{equation}\label{eq:unrolledChanSl}
    y_{t}  %C_t \sum_{j=1}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} %\nonumber\\
    %&
    = S_C S_B  \sum_{j=1}^t {S_{\Delta}}^{t-j-1} \Big{(} {\Pi_{k=j+1}^t x_k  }\Big{)} x_t {x_j}^2 
\end{equation}
}
% \begin{equation}
%     \small
%     y_{t} = C_t \sum_{j=1}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} = 
% % \end{equation}
% % \begin{equation}    
%     S_C S_B  \sum_{j=1}^t {S_{\Delta}}^{t-j-1} \Big{(} {\Pi_{k=j+1}^t x_k  }\Big{)} x_t {x_j}^2 
% \end{equation}

Hence, we can characterize the last output $y_L$ as a multivariate polynomial with at least $L$ monomials, and a maximal degree of at least $L+3$. Denote $c_j = S_C S_B S_{\Delta}^{L-j-1}$,%, and an aggregated total degree higher than $\sum_{j=1}^L j+3 = 3L + \sum_{j=1}^L j = \frac{1}{2} (L^2 + 7L)$:
\vspace{-5pt}
\begin{equation}\label{eq:SimpleMamba1Chan}
    y_L = \sum_{j=1}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2 %, \quad c_j = S_C S_B S_{\Delta}^{L-j-1}
    % y_L = \sum_{j=1}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2, \quad c_j = S_C S_B S_{\Delta}^{L-j-1} 
\end{equation}
% \vspace{-2pt}
% where $c_j = S_C S_B S_{\Delta}^{L-j-1}$.
% where $c_j = S_C S_B S_{\Delta}^{L-j-1}$.
\noindent\textbf{Attention as Multivariate Polynomials\quad}
Given the parameters of the layers $W_Q, W_K, W_V \in \mathbb{R}$, the last element in the output sequence can be computed by:
\vspace{-5pt}
{\small
\begin{equation}
y=\frac{(x W_{Q}) (x  W_{K})^T}{\sqrt{d_k}} (x W_{V})%, \quad y_L = W_Q W_K W_V \sum_{j=1}^L  {x_j}^2 x_L = \sum_{j=1}^L c_j{x_j}^2 x_L 
\end{equation}
}
\vspace{-5pt}
{\small
\begin{equation}
\small
y_L = W_Q W_K W_V \sum_{j=1}^L  {x_j}^2 x_L = \sum_{j=1}^L c_j{x_j}^2 x_L 
\vspace{-3pt}
\end{equation}
}

where $c_j = W_Q W_K W_V$. Hence, we can characterize the last output element $y_L$ of a self-attention layer by a 3-degree multivariate polynomial with $L$ monomials.%, each of degree 3.%, resulting in a total aggregated degree of $3L$.

% \smallskip
\noindent\textbf{$N$-Stacked Attention Layers as Multivariate Polynomials\quad}
In light of the characterization of one head of a self-attention layer, we can now proceed to establish the characterization of $N$-stacked self-attention layers. Since the composition of multivariate polynomials results in another multivariate polynomial, where the maximal degree of the resulting polynomial amounts to the product of the maximum degrees of the composed polynomials, we can prove by induction that $N$-Stacked layers can be represented by a polynomial with a maximal degree of $3^N$.

% \smallskip
\noindent\textbf{Identify and Refine the Expressivity Gap\quad} The description of one head of a self-attention layer and one channel of an S6 via multivariate polynomials reveals the expressiveness gap in terms of the maximal degree, which is 3 for self-attention and $L+3$ for S6, as depicted in %the right panel of
Fig.~\ref{fig:mainfig}. Thus, our formulation presents a broad family of functions that can be implemented by selective SSMs but cannot be realized by a single self-attention head. Furthermore, when processing a sequence of length $L$, it is clear that in order to express a function realized by S6 using $N$-stacked self-attention layers, $O(\log L)$ stacked layers are required.
\end{proof}
% \vspace{-10pt}
% \end{proof}
% \vspace{-5pt}

% \smallskip
\noindent\textbf{Single SSM as Multivariate Polynomials\quad} For completeness, we formalized a single standard SSM (not S6) via multivariate polynomials. Since SSMs can be represented as a single non-circular convolution between the input \(x\) and a kernel \(\bar{K} = (C\bar{B}, C\bar{A}\bar{B}, \ldots, C\bar{A}^{L-1}\bar{B})\), it is evident that the last output element of the SSM can be expressed by a 1-degree multivariate polynomial consisting of \(L\) monomials.%, each of degree 1.%, with an aggregated total degree of \(L\).
%
% \begin{proof}[Proof of Lemma~\ref{lemma:dir1} (sketch)]. TBD
% \end{proof}
%



% \smallskip
\noindent\textbf{Sharpening the Expressivity Gap.} 
Thm~\ref{theorem:exprrsFull} establishes the existence of an expressivity gap. However, it remains unclear how many polynomials are encompassed within this gap, and whether they constitute a significant portion of the function class or are merely anecdotal instances. To refine our separation results %between the hypothesis classes derived from Mamba and attention-based architectures
, we now quantify the expressiveness gap by analyzing the number of layers required to represent the entire class of $L$-degree multivariate polynomials. As established previously, attention-based models necessitate $O(\log L)$ layers. In contrast, the following theorem demonstrate that a Mamba model with just 4-stacked layers and a sufficiently large number of channels can represent \textit{any multivariate polynomial of arbitrary degree}, thereby highlighting the significant expressiveness gap, which constitute a significant portion of the class of $L$-degree polynomials.% between these architectures.

\begin{theorem}\label{theorem:AnyPolywithMambas}
Given an input scalar sequence $x \in \mathbb{R}^L $, a model with four stacked Mamba layers, a sufficiently large number of channels, learnable PE, sufficient padding and a linear encoder layer at the first layer can express any  multivariate polynomial with bounded degree of $x$.
\end{theorem}

For simplicity, we assume that the state size $N = 1$ and the SiLU activations in Mamba are removed. We justify these decisions as they only restrict our model. The proof is presented in the A
appendix, and it follows a technical construction that demonstrates how to express a general polynomial using Mamba. The core argument is built on the following lemma, which is %also
visualized in Fig.~\ref{fig:exprresTheorem}:

\begin{figure*}[]
    \centering
    \vspace{-4pt}
    \includegraphics[width=0.92\linewidth]{figures/polynomialExpresivness.jpg}
    \vspace{-7pt}
    \caption{Visualization of 3-stacked Mamba layers expressing monomials of a univariate polynomial, as formulated in Lemma~\ref{lemma:3layerMambaExpresivity}. To simplify the visualization, the Conv1D layer has been omitted.}
    \label{fig:exprresTheorem}
     \vspace{-10pt}
\end{figure*} 
%

\begin{lemma}\label{lemma:3layerMambaExpresivity}
Given an input scalar sequence $x \in \mathbb{R}^L$, for any $j$, a model $M$ with 3-stacked Mamba layers, a sufficiently large number of channels, learnable PE, and a linear encoder in the first layer can express any monomial of a univariate polynomial in $x_j$. Specifically, for any constants $c \in \mathbb{R}$ and $P \in \mathbb{R}$, there exists a configuration of $M$ such that the output of the $k$-th channel $M(x)_k = c \cdot x_j^P$ for $k > P + j$.
\end{lemma}
%
%
For a detailed proof of Lemma~\ref{lemma:3layerMambaExpresivity}, we refer the reader to the appendix. Here, we provide an intuitive explanation of the proof, which hinges on the following two key capability of the Mamba architecture:


{\noindent\textbf{(i) Per-position selection:}} By utilizing Mamba's auxiliary components, including the gating branch, linear layers, and learnable PE, each Mamba layer can isolate a specific channel $k$ at a given position $j$. Notably, the output of the first Mamba block can effectively filter out all unnecessary positions, producing a sequence mask with zeros at every position except $i = j$, which contains $x_j$ at position $j$. This is done by setting the S6 to the identity function ($\bar{A}=0, \bar{B}=\bar{C}=1)$, ensuring $x_j$ is not modified. Additionally, mask the other positions achieved by set the parameters of a learnable PE at one of the channels to the indicator function $\mathbb{I}_{=j}$, which is 1 only when focusing on the $j$-th position, and ensuring this PE passed into the gate branch through the linear layers.


{\noindent\textbf{(ii) Express powers by aggregate multiplications of duplicated elements:}} Given an input $x = (0, \cdots, x_j, 0, \cdots)$, which can be obtained from the first layer, the second Mamba block can duplicate the value of $x_j$ exactly $P-2$ times. This duplication holds even if $P > L$, thanks to the ZeroPad component. The duplication process is achieved by setting the linear layers to identity mappings and utilizing a degenerate single SSM channel where the system matrices always equal $\bar{A},\bar{B},\bar{C}=1$. Therefore, if the $k$-th channel receives an input sequence $x = (0, \cdots, x_j, 0, \cdots)$, it will output $x = (0, \cdots, x_j, \cdots, x_j)$.
Through the gating mechanism, learnable PE (which can pass through skip-connections to the subsequent layers), and biases in the linear layer, the entire block can then produce a filtered output $z = (1, \cdots, 1, x_j, \cdots, x_j, 1, \cdots, 1)$, ensuring that there are exactly $P-2$ copies of $x_j$.
Next, the S6 layer in the third block can multiply the sequence elements in $z$ via the operation described in Eq.~\ref{eq:SimpleMamba1Chan}, which include the term $\Pi_{k=j+1}^t \bar{A}_k$. This term yields an output sequence with the values $(1, \cdots, 1, x_j^3, \cdots, x_j^P, \cdots)$. To specifically isolate ${x_j}^P$, we begin by generating all unwanted elements by applying the same SSM to the sequence $z$, introducing an additional zero at the initial occurrence of $x_j$ denoted by $z'$. We then subtract the outputs from these two SSM channels at the final linear layer of the block. This subtraction yields a telescoping series $SSM(z) - SSM(z')$:
\vspace{-2pt}
{\small
\begin{equation}
\sum_{j=1}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2 - \sum_{j=2}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2
     % C_t \sum_{j=1}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} - C_t \sum_{j=2}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} 
\vspace{-3pt}
\nonumber
\end{equation}
\vspace{-3pt}
}
%
that effectively eliminates any term except for ${x_j}^P$. %Finally, leveraging the gate branch, learnable PE, and skip-connections, we can ensure that all values, except those at position $i$, are zeroed out.

% Additionally, to filter out only ${x_j}^P$, we can produce all the elements we want to omit by applying the same SSM over the same sequences $z$ with an a additional zero at the first occurrence of $x_j$. After that, we can subtract the outputs of these two SSM channel in the last linear layer in the block,  which result in telescoping series that remove any term than $x_j$. Finally, Once again, through the gate branch, learnable PE and skip-connections, all values except at position $i$ can be masked with zeros. 


Finally, it is crucial to highlight that incorporating auxiliary components into the Transformer, such as learnable PE and gating, does not mitigate the logarithmic increase in depth required with L. This limitation arises because the expressible degree within each block remains unchanged, thereby leading to the observed asymptotic behavior.

