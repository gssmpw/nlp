\section{Related Work}
\vspace{-3pt}
%State Space Models 
SSMs have gained a lot of traction due to their remarkable performance and computational efficiency. Their theoretical properties have been the focus of many recent works; **Gal, "A Theoretically Grounded Application of Stack-Augmented Transformations to Text Generation"**__**Krishor et al., "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"**
conclude that both architectures belong to the same complexity class ($TC^0$).\footnote{$TC^0$ is the complexity class that can be decided by polynomial-sized Boolean circuits.} **Hestness et al., "Deep Image Prior for Solving Forward Problem in Inverse Scattering Theory"** conduct a more refined analysis and show that transformers and SSMs occupy different portions of $TC^0$. Another work showed that a single-layer Transformer with $N$ heads can simulate any state space model with $N$ channels**Katharopoulos et al., "TokenLSTM: Token-Level Long Short-Term Memory Networks"**. 

**Krishor et al., "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"** show that the selective mechanism introduced in Mamba results with more expressive architectures compared to traditional (non-selective) SSMs.
**Katharopoulos et al., "TokenLSTM: Token-Level Long Short-Term Memory Networks"** show that there are functions that S6 can implement while transformers cannot.

In this work we compare the expressive power of S6 to those of transformers. We show that under certain assumptions which we justify empirically, a constant number of S6 layers are dense in the polynomial function space while transformers and non-selective SSMs are far less expressive. %
%
In addition to discussing expressivity, we also provide the first norm-based generalization bound for S6. Relevant related works are detailed in Appendix~\ref{sec:RelatedWorkGeneralization}.

% In addition to the discussion of expressivity, we also provide the first norm-based generalization bound for S6, extending prior results for non-selective SSMs **Krishor et al., "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention"**.

% Another work showed that SSMs are universal approximators and have a tendency to exponential decay**Hestness et al., "Deep Image Prior for Solving Forward Problem in Inverse Scattering Theory"**. In this work we compare the expressive power of S6 to those of transformers, considering polynomial activations and show that given a similar number of parameters the former is exponentially more expressive than the latter.


% % Explaining the remarkable performance of overparameterized deep neural networks (DNNs) to perform well on test data is one of the major open challenges in the theory of deep learning. Traditional tools used in statistical learning such as the PAC-learning framework and VC-dimension provide vacant bounds when the number of parameters is order of magnitudes greater than the number of parameters of a neural architecture. To make progress on this frontier, many works perform ad-hoc analyses that take into account the specific architecture and optimization algorithm used. Allen et al. analyze the dynamics of stochastic gradient descent on RNNs with ReLU activations and provide optimization and generalization guarantees**Allen-Zhu et al., "Natasha: Improving Stochastic Gradient Descent for Non-convex Optimization"**. Insights into the dynamics of gradient descent under different assumptions, shed light onto the generalization guarantees of RNNs for unseen data and longer sequences than those used for training**Santoro et al., "One-shot Learning with Memory-Augmented Neural Networks"**. To the best of our knowledge, the only work discussing the generalization properties of state-space layers, is that of Liu et al.**Liu et al., "Data-Dependent Generalization Bounds for State-Space Layers"**, which provides data-dependent generalization bounds. However, their work does not consider the recently introduced selective mechanism which is widely adopted in practice. {\color{black}In this work we derive norm-based bounds on the Rademacher complexity of S6.}

% Explaining the performance of overparameterized %deep neural networks (
% DNNs %)
% on test data remains a major challenge in deep learning theory. Traditional tools like %the
% PAC-learning %framework
% and VC-dimension often provide vacuous bounds when the number of parameters greatly exceeds the number of data points. To address this, many studies conduct architecture-specific analyses. For instance, Allen et al. analyze the dynamics of stochastic gradient descent on RNNs with ReLU activations, offering optimization and generalization guarantees**Allen-Zhu et al., "Natasha: Improving Stochastic Gradient Descent for Non-convex Optimization"**. Other works have explored the generalization of RNNs for unseen data and longer sequences under various assumptions**Santoro et al., "One-shot Learning with Memory-Augmented Neural Networks"**.
% %Liu et al. provide data-dependent generalization bounds for state-space layers but do not consider the selective mechanism commonly used in practice**Liu et al., "Data-Dependent Generalization Bounds for State-Space Layers"**. Our work fills this gap by deriving norm-based bounds on the Rademacher complexity of selective state-space layers (S6).


% %{\color{blue}
% % New text by Lian
% %{\bf Norm-based generalization bounds.\enspace} 
% %Recently,  norm-based generalization bounds were introduced for NNs**Dziugaite et al., "Computing Nonvacuous Generalization Bounds for Deep (Residual) Neural Networks"**. A common method for estimating the difference between the training and test errors of a NN is through the use of the network's 
% %Rademacher complexityle these results provide robust upper bounds on the test error of NNs, they incorporate very limited information about the network's architectural choices or are purely theoretical. In particular, 

% There are relatively few results that address modern architectures such as S6 layers. A recent contribution proposes a bound for standard SSMs**Hestness et al., "Deep Image Prior for Solving Forward Problem in Inverse Scattering Theory"**, but it does not extend to Selective SSMs. To address this gap, we propose a new bound specifically for Selective SSMs. 
% % and provide empirical evaluations to demonstrate its effectiveness.

\vspace{-5pt}