%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\input{math_commands.tex}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{amsmath, amssymb, mathtools, amsthm}
\usepackage{times}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
%\usepackage{icml2025}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On the Expressivity of Selective State-Space Layers}

\begin{document}

\twocolumn[
\icmltitle{On the Expressivity of Selective State-Space Layers:\\ A Multivariate Polynomial Approach}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Edo Cohen-Karlik}{equal,yyy}
\icmlauthor{Itamar Zimerman}{equal,yyy}
\icmlauthor{Liane Galanti}{equal,yyy}
\icmlauthor{Ido Atad}{yyy}
\icmlauthor{Amir Globerson}{yyy}
\icmlauthor{Lior Wolf}{yyy}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy}
% \icmlauthor{Firstname8 Lastname8}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{The School of Computer Science, Tel Aviv University}
%\icmlaffiliation{comp}{Google Research}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}


\icmlcorrespondingauthor{Edo Cohen-Karlik}{edocohen@mail.tau.ac.il}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Mamba, S6, Expressivity, Generelization}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Recent advances in efficient sequence modeling have introduced selective state-space layers, a key component of the Mamba architecture, which have demonstrated remarkable success in a wide range of NLP and vision tasks. %These include language modeling, machine translation, long-context question answering, and document-level sentiment analysis, among others.%
While Mambaâ€™s empirical performance has matched or surpassed SoTA transformers on such diverse %NLP
benchmarks, the theoretical foundations underlying its powerful representational capabilities remain less explored. In this work, we investigate the expressivity of selective state-space layers using multivariate polynomials, and prove that they surpass {\color{black}linear} transformers in expressiveness. Consequently, our findings reveal that Mamba offers superior representational power over {\color{black}linear} attention-based models for long sequences{\color{black}, while not sacrificing their generalization.} Our theoretical insights are validated by a comprehensive set of empirical experiments on various datasets.%, and we have included our code as supplementary material.
\vspace{-6pt}
\end{abstract}

\vspace{-6pt}
\section{Introduction}
Sequence modeling has been the focus of many works in recent years, with remarkable results enabling applications such as ChatGPT. To date, these models are largely based on the Transformer architecture~\cite{NIPS2017_3f5ee243}. While transformers have proven extremely effective, they suffer from several drawbacks compared to traditional recurrent models, one of which is their computational complexity which scales quadratically with the input sequence length.

In attempt to mitigate the computational inefficiency of sequence modeling of transformers, a line of work has attempted to resurrect RNNs%recurrent neural networks
, \cite{gu2021efficiently,dss,gu2021combining} %, orvieto2023resurrecting}
have introduced a series of architectures called State Space Models (SSMs) which include a linear recurrence that admits efficient computations and special structure on the transition matrices.

While these architectures have demonstrated impressive performance in long sequence modeling, their performance on fundamental tasks such as language modeling fall short compared to transformers, mostly due to intrinsic superiority of transformers in modeling interactions between different elements of the input sequence. Recent work~\cite{gu2023mamba}, has introduced Mamba, an SSM variant with Selective SSMs (S6) as its core block. In an S6 layer, the parameters are a function of the input, providing the SSM with content awareness. The empirical success of Mamba is undeniable, with applications spanning large-scale language modeling~\cite{zuo2024falcon,lieber2024jamba,waleffe2024empirical}, image~\cite{mambaViT1}, %,mambaViT2}
and video~\cite{li2025videomamba} %,chen2024video}
processing, medical imaging~\cite{mambaMedical3}, %mambaMedical8,mambaMedical4,mambaMedical3}%,,mambaMedical5}
 tabular data analysis~\cite{ahamed2024mambatab}, Reinforcement Learning~\cite{lv2024decision}, point-cloud analysis~\cite{mambaPoint}, graph processing~\cite{mambaGraph1}%,mambaGraph2}
, and N-dimensional sequence modeling~\cite{mambaND}.

The success of Mamba models across various domains ignites interest in their theoretical properties. Establishing a comprehensive theoretical understanding is crucial, as it enhances our knowledge of these layers, promotes their adoption within the research community, and paves the way for future architectural advancements. Additionally, since S6 can be considered a variant of attention with linear complexity~\cite{ali2024hidden}, deeper theoretical insights could elucidate the relationships between gated RNNs, transformers, and SSMs, thereby advancing our knowledge of these interconnected architectures. 
Initial efforts to establish a theoretical framework for the expressiveness of selective (and non-selective) SSM layers have been undertaken by several researchers. Using Rough Path Theory, \citet{cirone2024theoretical} demonstrated that diagonal selective SSMs, such as Mamba, possess less expressive power than their non-diagonal counterparts. Additionally, \citet{merrill2024illusion} investigated the expressiveness relationships between SSM variants and transformers using the lens of circuit complexity, revealing that both models share the same expressive power (belonging to ${TC}^0$). Finally, \citet{jelassi2024repeat} examined the copying ability of various SSM variants compared to transformers. It concluded that from both theoretical and empirical perspectives SSMs struggle with this task. While these significant studies highlight the limited expressive capabilities of Mamba models compared to other architectures, our work introduces a different trend. We demonstrate the superior expressive power of S6 layers, using a theoretical framework based on multivariate polynomials. 

{\color{black}In addition to exploring the expressiveness gap between transformers and S6 layers, we develop norm-based length-agnostic generalization bounds for S6 layers. Suggesting that the added expressivity of the selective mechanism does not hinder the generalization properties compared to traditional SSMs as studied in \citep{liu2024generalization},  and that while polynomial expressivity increases with sequence length, it does not impact generalization.}
%
% % have introduced a time dependent linear recurrence where each 
%
% Recently, Selective State-Space Layers (S6)~\cite{gu2023mamba}, the core component of the Mamba block, have demonstrated exceptional performance across a wide range of applications. These applications span language modeling~\cite{gu2023mamba,mambamoe1,mambamoe2,wang2024mambabyte,lieber2024jamba}, image~\cite{mambaViT1,mambaViT2} and video~\cite{mambaVideo,li2024videomamba,chen2024video} processing, medical imaging~\cite{mambaMedical7,mambaMedical5,mambaMedical8,mambaMedical4,mambaMedical3,mambaMedical2,mambaMedical1}, tabular data analysis~\cite{ahamed2024mambatab}, Reinforcement Learning~\cite{Ota2024DecisionMR,mambaRL1,Dai2024IsMC}, point-cloud analysis~\cite{mambaPoint}, graph processing~\cite{mambaGraph1,mambaGraph2}, and N-dimensional sequence modeling~\cite{mambaND}. Mamba models are distinguished by their hardware-aware design and dual-view computation, which introduce linear complexity in sequence length during training and enable fast RNN-like computation during inference. This unique combination results in a five fold increase in throughput in comparison to  Transformers, making Mamba models a powerful and efficient choice for modern deep-learning tasks.
%
% Despite the widespread success of Mamba models across various application domains, their underlying theoretical foundations remain relatively unexplored. Establishing a comprehensive theoretical understanding is crucial, as it enhances our knowledge of these layers, promotes their adoption within the research community, and paves the way for future architectural advancements. Additionally, since S6 can be considered a variant of attention with linear complexity~\cite{ali2024hidden}, deeper theoretical insights could elucidate the relationships between gated RNNs, transformers, and State Space Models (SSMs), thereby advancing our knowledge of these interconnected architectures. 
%
% Our study investigates these important aspects from the perspective of expressiveness and generalization. To do so, we introduce a simplified and effective variant of the S6 layer, which is easier to analyze and empirically comparable with the original Mamba block on large-scale image classification and language modeling. Our variant includes the first discretization-free S6 layer, which involves only polynomial operations. Such simplifications have served as a foundation for other theoretical works (e.g. \citep{kileel2019expressive, chrysos2021deep} in the context of deep polynomial NNs, and \citep{ kim2023polynomial, song2023expressibility} for polynomial self-attention).
%
%
% To obtain meaningful theoretical results, we suggest a simplified variant of the original S6 layer which is easier to analyzed as well as present comparable performance. %
% To derive theoretical results we present a simplified layer of the complex S6 layer as customary with theoretical works in deep learning. We empirically validate our model and show it achieves competitive results to the original architecture. 
%
% 
% Initial efforts to establish a theoretical framework for the expressiveness of selective state-space layers have been undertaken by several researchers. Using Rough Path Theory, \cite{cirone2024theoretical} demonstrated that diagonal selective SSMs, such as Mamba, possess less expressive power than their non-diagonal counterparts. Additionally, \cite{merrill2024illusion} investigated the expressiveness relationships between SSM variants and transformers from the perspective of circuit complexity, revealing that both models share the same expressive power (belonging to ${TC}^0$). Finally, \cite{jelassi2024repeat} examined the copying ability of various SSM variants compared to transformers. It concluded that from both theoretical and empirical perspectives SSMs struggle with this task. While these significant studies highlight the limited expressive capabilities of Mamba models compared to other architectures, our work introduces a different trend. We demonstrate the superior expressive power of these layers, using a theoretical framework based on multivariate polynomials. 
%
% {\color{blue}
% As for the generalization of selective state-space layers, less effort has been investigated. Our research takes the first steps in this direction. Our bound utilizes Rademacher complexity, which provides an upper bound on the difference between training and testing errors within a specific hypothesis class. It is defined as the expected performance of the class averaged over all possible data labelings, with labels independently and uniformly drawn from the set $\{\pm 1\}$. More details in \citep{mohri2018foundations, Shalev-Shwartz2014}. We provide norm-based generalization bound for the S6 that do not
% depend on the input sequence length. We also provide experiments 
% that empirically validate our theoretical findings.
% %  {\color{black}AND DOES WHAT}. % ? However, several results have been proposed for traditional non-selective state-space layers~\cite{liu2024generalization}.
% % \amirg{there's not enough in the intro about what you do. The contribution part below should summarize contributions, but before, you should give a general idea for what your approach and reuslts are.}
%
%
%
% The generalization capabilities of neural networks have been widely studied~\citep{neyshabur2015norm,golowich2018size,Bartlett2001RademacherAG,Harvey2017NearlytightVB, bartlett2017spectrally,Neyshabur2018APA,cao2019generalization,daniely2019generalization,wei2019data,allen2019learning,https://doi.org/10.48550/arxiv.1806.05159}. Many efforts have have been made to make these bounds more applicable to practical settings. Some researchers have developed norm-based generalization bounds for more complex architectures like RNN~\citep{chen2019generalization, tu2019understanding}, and transformers ~\citep{trauger2024sequence}. Despite these advances, most of the research remains focused on fully-connected networks, which generally underperform compared to modern architectures such as S6 and transformers~\citep{NIPS2017_3f5ee243,dosovitskiy2020image}. As a result, these bounds have limited ability to explain the success of contemporary architectures.
% % \amirg{there's certainly some theory work on transformers (e.g., that they have bias towards sparsity, and some stuff about expressive power). OK I see you cover some of it below, so tone down the claim here.)}
% }

% The generalization capabilities of neural networks have been widely studied, with various efforts made to develop bounds applicable to practical settings~\citep{neyshabur2015norm,golowich2018size,Bartlett2001RademacherAG,Harvey2017NearlytightVB, bartlett2017spectrally,Neyshabur2018APA,cao2019generalization,wei2019data,allen2019learning,li2018tighter}. Notably, norm-based generalization bounds have been proposed for complex architectures, such as RNNs~\citep{chen2019generalization, tu2019understanding} and transformers~\citep{trauger2024sequence}. However, most research remains focused on fully-connected networks %, which generally underperform compared to modern architectures such as S6 and transformers~\citep{NIPS2017_3f5ee243,dosovitskiy2020image}. Consequently, these bounds have limited ability to explain the success of contemporary architectures.
% % and the generalization of S6 layers has received less attention. Our research takes the first steps in this direction. %O×•r bound utilizes Rademacher complexity, which provides an upper bound on the difference between training and testing errors within a specific hypothesis class. This complexity is defined as the expected performance of the class averaged over all possible data labelings, with labels independently and uniformly drawn from the set $\{\pm 1\}$. For further details, refer to~\citep{mohri2018foundations, Shalev-Shwartz2014}.
% % Notably, the norm-based generalization bound we compute for the S6 architecture does not depend on the input sequence length. 
%
% %We conduct experiments to empirically validate our theoretical findings. This work aims to extend the understanding of generalization in modern neural network architectures, particularly those involving selective state-space layers.
%
%
%
\noindent{\textbf{Our main contribution}} encompasses the following aspects: (i) We present simplified polynomial variants of Mamba that exhibit comparable performance on NLP and vision tasks, promoting a simpler model that can serve as a foundation for other theoretical contributions, as well as shed light on the inner dynamics of Mamba and its critical components. (ii) By establishing the connection between multivariate polynomials and S6 layers, we theoretically prove that S6 layers are expressive as a {\color{black}linear} self-attention with depth that scales logarithmically with the sequence length; that is, there are functions that can be modeled with a single S6 that would require a logarithmic number of {\color{black}linear} %self-
attention layers. More generally, we establish that a Mamba model with only 4 layers is sufficient to represent the class of multivariate polynomials with bounded degree L. In contrast, a {\color{black}linear} attention-based model requires a logarithmic number of layers in $L$ to achieve the same representational capability. (iii) %Finally, t% 
Through experiments on a synthetic dataset in a controlled environment designed to isolate expressivity issues, we empirically validate that our theory is reflected in practice. (iv) {\color{black}Finally, although S6 has better polynomial expressivity for long sequences, we show that this does not come at the cost of limited generalization, by proving the first length-agnostic generalization bound for S6 %layers
. This leads us to conclude that S6 layer possesses superior theoretical properties over linear attention for long-range tasks.}  %for long-range tasks, we show that S6 layers possess favorable theoretical expressivity compared to transformers, as shown in Fig.~\ref{fig:mainfig}.
%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.82\linewidth]{figures/theoryMambaMainNewSmall_only_express.jpg}
    % \vspace{-2pt}
    % \caption{\textbf{(Left)} For long-range regimes, Mamba has better theoretical properties than self-attention in both generalization and expressivity. %{\color{blue}\textbf{(Right)} A schematic overview of the Mamba variant we explored.}
    % \textbf{(Right)} Our characterization of SSMs, S6 layers, and causal self-attention via multivariate polynomials allows us to identify the expressiveness gap between these layers through maximal polynomial degree. 
    % }
    \vspace{-3pt}
    \caption{\textbf{Expressivity via Polynomial Degree:} Our characterization of SSMs, S6 layers, and causal self-attention via multivariate polynomials allows us to identify the expressiveness gap between these layers through maximal polynomial degree.}
    \label{fig:mainfig}
    \vspace{-12pt}
\end{figure} 
%
\vspace{-12pt}
\section{Related Work}
\vspace{-3pt}
%State Space Models 
SSMs have gained a lot of traction due to their remarkable performance and computational efficiency. Their theoretical properties have been the focus of many recent works; \citet{merrill2024illusion} compare the expressive capacity of (non-selective) SSMs and transformers, concluding that both architectures belong to the same complexity class ($TC^0$).\footnote{$TC^0$ is the complexity class that can be decided by polynomial-sized Boolean circuits.} \citet{sarrof2024expressive} conduct a more refined analysis and show that transformers and SSMs occupy different portions of $TC^0$. Another work showed that a single-layer Transformer with $N$ heads can simulate any state space model with $N$ channels~\citep{zimerman2023long}. 
 

\citet{cirone2024theoretical} show that the selective mechanism introduced in Mamba results with more expressive architectures compared to traditional (non-selective) SSMs.
\citet{ali2024hidden} show that there are functions that S6 can implement while transformers cannot.

In this work we compare the expressive power of S6 to those of transformers. We show that under certain assumptions which we justify empirically, a constant number of S6 layers are dense in the polynomial function space while transformers and non-selective SSMs are far less expressive. %
%
In addition to discussing expressivity, we also provide the first norm-based generalization bound for S6. Relevant related works are detailed in Appendix~\ref{sec:RelatedWorkGeneralization}.

% In addition to the discussion of expressivity, we also provide the first norm-based generalization bound for S6, extending prior results for non-selective SSMs \cite{liu2024generalization}.

% Another work showed that SSMs are universal approximators and have a tendency to exponential decay~\citep{wang2024state}. In this work we compare the expressive power of S6 to those of transformers, considering polynomial activations and show that given a similar number of parameters the former is exponentially more expressive than the latter.


% % Explaining the remarkable performance of overparameterized deep neural networks (DNNs) to perform well on test data is one of the major open challenges in the theory of deep learning. Traditional tools used in statistical learning such as the PAC-learning framework and VC-dimension provide vacant bounds when the number of parameters is order of magnitudes greater than the number of parameters of a neural architecture. To make progress on this frontier, many works perform ad-hoc analyses that take into account the specific architecture and optimization algorithm used. Allen et al. analyze the dynamics of stochastic gradient descent on RNNs with ReLU activations and provide optimization and generalization guarantees~\citep{allen2019can}. Insights into the dynamics of gradient descent under different assumptions, shed light onto the generalization guarantees of RNNs for unseen data and longer sequences than those used for training~\citep{cohen2022implicit, emami2021implicit, cohen2022learning, hardt2018gradient}. To the best of our knowledge, the only work discussing the generalization properties of state-space layers, is that of Liu et al.~\cite{liu2024generalization} which provides data-dependent generalization bounds. However, their work does not consider the recently introduced selective mechanism which is widely adopted in practice. {\color{black}In this work we derive norm-based bounds on the Rademacher complexity of S6.}

% Explaining the performance of overparameterized %deep neural networks (
% DNNs %)
% on test data remains a major challenge in deep learning theory. Traditional tools like %the
% PAC-learning %framework
% and VC-dimension often provide vacuous bounds when the number of parameters greatly exceeds the number of data points. To address this, many studies conduct architecture-specific analyses. For instance, Allen et al. analyze the dynamics of stochastic gradient descent on RNNs with ReLU activations, offering optimization and generalization guarantees~\citep{allen2019can}. Other works have explored the generalization of RNNs for unseen data and longer sequences under various assumptions~\citep{cohen2022implicit, emami2021implicit, cohen2022learning, hardt2018gradient}.
% %Liu et al. provide data-dependent generalization bounds for state-space layers but do not consider the selective mechanism commonly used in practice~\cite{liu2024generalization}. Our work fills this gap by deriving norm-based bounds on the Rademacher complexity of selective state-space layers (S6).


% %{\color{blue}
% % New text by Lian
% %{\bf Norm-based generalization bounds.\enspace} 
% %Recently,  norm-based generalization bounds were introduced for NNs~\citep{neyshabur2015norm,golowich2018size,Bartlett2001RademacherAG,Harvey2017NearlytightVB, bartlett2017spectrally,Neyshabur2018APA,cao2019generalization,daniely2019generalization,wei2019data,https://doi.org/10.48550/arxiv.1806.05159,chen2019generalization,tu2019understanding}. A common method for estimating the difference between the training and test errors of a NN is through the use of the network's 
% %Rademacher complexityle these results provide robust upper bounds on the test error of NNs, they incorporate very limited information about the network's architectural choices or are purely theoretical. In particular, 

% There are relatively few results that address modern architectures such as S6 layers. A recent contribution proposes a bound for standard SSMs~\citep{liu2024generalization}, but it does not extend to Selective SSMs. To address this gap, we propose a new bound specifically for Selective SSMs. 
% % and provide empirical evaluations to demonstrate its effectiveness.

\vspace{-5pt}
\section{Background}\label{sec:background}
\vspace{-3pt}
In this section we present the technical details required for the theoretical analysis and provide relevant %definitions and
notations.


\noindent{\textbf{{Notations\quad}}  Let $X \in \mathbb{R}^{D \times L}$ be an input sequence of length $L$ with dimension size $D$, denote the element in the $i$-th channel and position $j$ as $X_{ij}$. We denote the entire channel at a specific position $j$ and the entire sequence at a specific channel $i$ as $X_{*j}$ and $X_{i*}$, respectively. For simplicity, we denote a general single channel of $X$ without channel index by $x := (x_1, x_2, \cdots, x_L)$ such that $x_i \in \mathbb{R}$.

\noindent{\textbf{{Mamba\quad}} Given these notations, a Mamba block, which is built on top of the S6 layer% and gated-MLP
, is specified as follows:
{\small
\begin{equation} \nonumber
    X = \sigma(\text{Conv1D}(\text{Linear}(U)), \quad Z = \sigma(\text{Linear}(U)) 
    \vspace{-3pt}
\end{equation}
}
\vspace{-8pt}
{\small
\begin{equation} \label{eq:mamba1}
    Y = \text{S6}(X),\quad \hat{Y} = \text{Linear}(Y \otimes Z)
\end{equation}
}

Here, $U$ is the input to the Mamba block, and $X$ is the input to the S6 layers, $X,Z,Y,\hat{Y} \in \mathbb{R}^{L \times D}$, the linear layers operate independently for each sequence element, $\sigma$ represents SiLU activation function, and $\otimes$ denotes element-wise multiplication with the gate branch. 

\noindent{\textbf{S6 }}
An S6 layer is a recent variant of SSM. A standard diagonal SSM is parameterized by a diagonal transition matrix $A \in \mathbb{R}^{N \times N}$, input and output matrices $B,C \in \mathbb{R}^{N \times 1}$, and a timescale $\Delta \in \mathbb{R}$. An input scalar sequence $x$ is mapped to an output scalar sequence $y$ via the following recurrent rule:
%
\vspace{-5pt}
{\small
\begin{equation}\nonumber
\vspace{-3pt}
h_t =  \bar{A} h_{t-1} + \bar{B}x_t, \quad y_k = C h_t
\vspace{-5pt}
\end{equation}
}
%
{\small
\begin{equation}\label{eq:recRule}
\vspace{-3pt}
\bar{A}=f_A (A, \Delta), \quad  \bar{B}=f_B (B, \Delta)
\end{equation}
}
% \begin{align} \label{eq:recRule}
% \small
%     h_t =  \bar{A} h_{t-1} + \bar{B}x_t, \textbf{  } y_k = Cx_t, \textbf{  } %\bar{A}=f_A (A, \Delta), \quad \bar{B}=f_B (A,B, \Delta)
% % \end{equation}
% % \begin{equation} 
% % \label{eq:recRule1}
% % \small
%  \bar{A}=f_A (A, \Delta), \textbf{  }  \bar{B}=f_B (B, \Delta)
% \end{align}
%
where $f_A,f_B$ are discretization functions, and the discrete system matrices are $\bar{A} \in \mathbb{R}^{N \times N}$ and $\bar{B} \in \mathbb{R}^{N \times 1}$. The recurrent rule in Eq.~\ref{eq:recRule} can be computed efficiently in parallel on modern hardware accelerators using work-efficient parallel scans~\cite{smith2022simplified} or a simple scalar convolution via FFTs~\cite{gu2021combining}. Note that Eq.~\ref{eq:recRule} is a map from $\mathbb{R}^L$ to $\mathbb{R}^L$, and to process $D$ channels, multiple independent instances are used.
% \begin{equation} \label{eq:discretization}
%      A_i = \exp (\Delta_i A), \quad B_i = \Delta_i B_i, 
% \end{equation}

Contrary to traditional SSMs, which utilize time-invariant system matrices and process each channel independently, S6 layers incorporate a data-dependent mechanism that is parameterized by $S_B, S_C \in \mathbb{R}^{N \times D}$, $A \in \mathbb{R}^{D \times N}$ and $S_{\Delta} \in \mathbb{R}^{1 \times D}$ to define the time-variant matrices as follows:
%
%
{\small
\begin{equation}\nonumber 
    \vspace{-3pt}
    B_t = S_B X_{*t}, \ C_t = S_C X_{*t}, \ \Delta_t = \text{softplus}(S_{\Delta} X_{*t})%, \quad \bar{A}_t = \exp(\Delta_t A), \quad \bar{B}_t = \Delta_t B_t
    \vspace{-2pt}
\end{equation}
}
\vspace{-5pt}
{\small
\begin{equation}\label{eq:TimeVariantMatrices1} 
%\Delta_t = \text{softplus}(S_{\Delta} X_{*t}), \quad 
\bar{A}_t = \exp(\Delta_t A), \quad \bar{B}_t = \Delta_t B_t
\end{equation}
}
% \begin{equation} \label{eq:discretization}
%      \bar{A}_i = \exp (\Delta_i A), \quad f_B(\Delta_i, B_i) = \Delta_i B_i, 
% \end{equation}
% \begin{equation}\label{eq:TimeVariantMatrices2}
%     \bar{A}_i = f_A(\Delta_i, A), \quad \bar{B}_i = f_B(\Delta_i, B_i)
% \end{equation}
%
The resulting time-variant recurrent rule is:
{\small
\begin{equation}\label{eq:timeVaraintRecRule}
     \vspace{-2pt}
     h_t =  \bar{A}_t h_{t-1} + \bar{B}_t x_t, \quad y_k = C_t h_t
\end{equation}
}
Our analysis focuses on the regime of 'many-to-one', which deals with models that operate on sequences %of length $L$
and produce a single output after processing the entire input sequence.%{\color{blue}, as illustrated in the right panel of Fig.~\ref{fig:mainfig}.}
%
% Additional background material is presented in {\color{black}Appendix TBD}.

\vspace{-4pt}
\section{Theoretical Results}
\vspace{-3pt}
We begin by presenting our simplified model in Sec.~\ref{sec:simplify}, which forms the basis for our theory on the expressivity {\color{black}and generalization of S6 layers, discussed in} Sec.\ref{sec:expressivity} and Sec.~\ref{sec:generalization}, respectively. In the experiments section we demonstrate that the simplified model used in our exposition achieves comparable results to those of the original S6 layers, encouraging further exploration of the suggested simplification.
%
% Our analysis is carried out on a simplified version of S6 where the non-linear operators are approximated using polynomials. 

\vspace{-3pt}
\subsection{Model Simplifications\label{sec:simplify}} %
\vspace{-2pt}
The original S6 layer is parameterized by $S_{\Delta}$, $S_{B}$, $S_C$ and $A$, and it is defined in Eqs. \ref{eq:TimeVariantMatrices1} and \ref{eq:timeVaraintRecRule}.%
% which define the discrete time-variant system matrices as:
% \begin{equation} \label{eq:TimeVariantMatrices1}
%     B_i = S_B x_i, \quad C_i = S_C x_i, \quad \Delta_i = \text{softplus}(S_{\Delta} x_i), \quad
% % \end{equation}
% % \begin{equation} \label{eq:discretization}
%      % f_A(\Delta_i, A) = \exp (\Delta_i A), \quad f_B(\Delta_i, B_i) = \Delta_i B_i, 
% % \end{equation}
% % \begin{equation}\label{eq:TimeVariantMatrices2}
%     \bar{A}_i = \exp (\Delta_i A), \quad \bar{B}_i = \Delta_i B_i
% \end{equation}

Our approach utilizes a simplified model described below:%as follows:
% \vspace{-3pt}
{\small
\begin{equation*}
      \bar{B}_i = S_B x_i,\quad C_i = S_C x_i,\quad %\Delta_i = p_1 (S_{\Delta} x_i),  \;%, \quad  \bar{A}_i =  p_2 ( \Delta_i A), \quad \bar{B}_i = B_i 
% \end{equation}
% }
% \vspace{-5pt}
% {\small
% \begin{equation}\nonumber
\Delta_i = p_1 (S_{\Delta} x_i),\quad \bar{A}_i =  p_2 ( \Delta_i A) 
\end{equation*}
}
% {\color{black}maybe add dimensions for formality and the activation function exp}

where $p_1$ and $p_2$ represent polynomials that operate independently per element, for instance, a second-degree Taylor approximation for softplus and exponent accordingly. An equivalent model would be:
% \vspace{-5pt}
{\small
\begin{equation}\label{eq:simplifiedModel}
 \vspace{-5pt}
    C_i = S_C x_i, \quad \bar{B}_i = S_B x_i, \quad \bar{A}_i = p_2\left(p_1 \left(\frac{S_\Delta x_i}{\sqrt{D}}\right) A\right)
\end{equation}
}
% \vspace{-2pt}

in which $D$ is the width of the model, and $\frac{1}{\sqrt{D}}$ is a constant normalization factor. This model can be interpreted as state-space layer without discretization, with $\bar{A}$ being selective, and $p_1$ and $p_2$ are stabilizers designed to control the values of $A$, which must be positive. It is imperative to note that standard SSMs without discretization are both effective and simple~\cite{gupta2022simplifying}.

%For simplicity, we define $p_A(x)=p_2 (p_1(\frac{x}{\sqrt{D}}))$

For simplicity, we define an additional polynomial $p_A(x) = p_2\left(p_1 \left(\frac{x}{\sqrt{D}}\right) A\right)$ which is parameterized by $A$ and ties $p_1$ and $p_2$. Hence, we can denote $\bar{A}_i = p_A(S_{\Delta} x_i)$.

Alternatively we can utilize the following simplified non-polynomial model:
% 
{\small
\begin{equation}\nonumber
\vspace{-3pt}
\bar{B}_i = S_B x_i, \quad C_i = S_C x_i
\vspace{-2pt}
\end{equation}
}
\vspace{-5pt}
{\small
\begin{equation}\label{eq:model}
\Delta_i = \text{softplus}(S_{\Delta} x_i),\quad
% \end{equation}
% \begin{equation}\label{eq:model}
\bar{A}_i =  \exp ( \Delta_i A)%, \quad \bar{B}_i = B_i 
\end{equation}
}
% {\color{black}where $R>1$ is a constant normalization factor.}
% For simplicity, we define an additional polynomial $p_A$ which is parameterized by $A$ and ties $p_1$ and $p_2$. It defines the following polynomial and model:
% \begin{equation}\label{eq:simplifiedModelFortheoren}
%     \forall x : p_A(x) = p_2(p_1 (\frac{x}{\sqrt{D}}) A), \quad C_i = S_C x_i, \quad \bar{B}_i = S_B x_i, \quad \bar{A}_i = p_A(S_{\Delta} x_i)
% \end{equation}
% \vspace{-12pt}
%
%
% 
%
\vspace{-18pt}
\subsection{Expressivity\label{sec:expressivity}
}
\vspace{-3pt}
\input{Expressivity}

\vspace{-4pt}
\subsection{Generalization\label{sec:generalization}}
\vspace{-2pt}
\input{Generalizatoin}



\vspace{-7pt}
\section{Experiments}\label{sec:experiments}
\vspace{-4pt}
% \setcounter{section}{3}
In this section, we extensively validate our theorems and assumptions through empirical analysis. First, in Sec.\ref{sec:modelJustification},
we demonstrate that our simplified variant of the Mamba layer achieves performance comparable to the original Mamba layer when incorporated into standard settings and deep networks, thereby justifying the exploration of this variant. Next, in Sec.\ref{sec:LearningPoly},
we validate our theory on expressiveness by showing that self-attention struggles to learn high-degree multivariate polynomials, which S6 can model effectively. %Finally, in Sec.~\ref{sec:empiricalBounds}, we assess our generalization bounds empirically.

\vspace{-5pt}
\subsection{Model Justification\label{sec:modelJustification}}
\vspace{-3pt}
Our theoretical study employs the simplified S6 variant described in Eq.~\ref{eq:simplifiedModel}. We conduct experiments in both the NLP and vision domains, evaluating this variant when integrated into the Mamba backbone, with the goal of showing that it performs similarly to the original S6 layer.
%We start by justifying the exploration of this variant and demonstrating its comparability to the original selective SSMs when integrated into a standard Mamba backbone and training process. 



% _________________________________________________
% \medskip
{\noindent\textbf{NLP\quad}}
In NLP, we trained variants of our simplified S6 layer within Mamba backbones on the Wikitext-103 dataset using a self-supervised scheme for \textit{Next Token Prediction}. Our models feature 12 layers with a hidden dimension size of 386 and were trained with a context length of 1024 tokens. The final results are detailed in right panel of Tab.~\ref{tab:empricalModelJustifiction} and in the right panel of Fig.~\ref{fig:modelJustifications}, which illustrates the evolution of test-perplexity across epochs. Evidently, our simplified S6 variant performs well in the NLP domain, with only a slight reduction in perplexity with respect to the original model. Specifically, the polynomial variant achieved a perplexity score of 26.42, 0.69 points lower than its original baseline score of 25.73. In contrast to the polynomial variant, the other simplified variants that employ $\bar{B}_i = B_i$ achieve a slightly lower perplexity %score
compared to the baseline, highlighting the significance of this aspect in the architecture.

% \medskip
{\noindent\textbf{Vision\quad}} %
Image classification experiments are conducted on the ImageNet-100 benchmark. We built upon the Vision-Mamba (ViM) architecture~\cite{mambaViT1}, replacing the S6 layers with our simplified variant while maintaining the same training procedures and hyper-parameters. The left panel of Tab.~\ref{tab:empricalModelJustifiction} presents the results: the simplified variant from Eq.~\ref{eq:simplifiedModel} achieves a %top-1 
accuracy of 78.62\%, which is 2.4\% lower than the original model which achieve a score of $81.02$. For reference, we include the results of DeiT~\cite{touvron2021training}, which achieved a top-1 accuracy of 78.21\% for the same model size%on this benchmark
~\cite{baron2024a}. %
%
% To understand which aspects of our simplification have the most dominant impact of the decreasing in performance, we compare additional two variations. First, we employ a polynomial S6 variant without omitting the discretization, and second we run a model with standard discretization, however, not polynomial. Our empirical analysis reveals that the polynomial model introduce surprisingly strong performance, with a accuracy score of $80.28$, just 0.74 behind the original model and 0.92 above the the non-polynomial sinplified model.
%


\begin{table*}[]
    \centering
    \small
    \vspace{-11pt}
    \caption{Ablations of our simplified S6 variants, with vision tasks on the left and NLP on the right. 'S' for simplified variants. Results for Transformer models are provided as a reference point.}
    \smallskip
    \label{tab:empricalModelJustifiction}
    \begin{tabular*}{0.48\linewidth}{@{\extracolsep{\fill}}lcc}
        \toprule
        Model & Top-1 & \# Parameters \\
        \midrule
        ViM (baseline) & 81.02 & 6.2M  \\
        ViM (S., $\bar{B}_i = B_i$) & 79.36 & 6.2M\\
        ViM (S., $\bar{A}_i = p_A(S_{\Delta}(x_i))$) & 80.28 & 6.2M \\
        ViM (S., Eq.~\ref{eq:simplifiedModel}) & 78.62 & 6.2M  \\
        Transformer (DeiT) & 78.21 & 6.2M  \\
        \bottomrule
    \end{tabular*}
    \hfill
    \begin{tabular*}{0.49\linewidth}{@{\extracolsep{\fill}}lcc}
        \toprule
        Model & PPL & \# Parameters \\
        \midrule
        Mamba (baseline) & 25.73 & 30.1M  \\
        Mamba (S., $\bar{B}_i = B_i$) & 29.49 & 30.1M \\
        Mamba (S., $\bar{A}_i = p_A(S_{\Delta}(x_i))$) & 26.42 & 30.1M \\
        Mamba (S., Eq.~\ref{eq:simplifiedModel}) & 31.12 & 30.1M  \\
        Transformer & 28.31 & 31.4M  \\
        \bottomrule
    \end{tabular*}
    \vspace{-7pt}
\end{table*}



To identify which aspects of our simplification most significantly impact performance, we compare two additional variations. First, we use a polynomial S6 variant without omitting the discretization. Second, we run a vanilla non-polynomial model where be $\bar{B}_i = B_i$. Our empirical analysis reveals that the polynomial model performs remarkably well, achieving an accuracy score of 80.28, just 0.74 points below the original model and 0.92 points above the non-polynomial simplified model. To provide a comprehensive view, the training curves are presented in left panel of Fig.~\ref{fig:modelJustifications}, which also empirically analyzes several variants of the simplified model compared to the baseline. The full set of hyper-parameters can be found in the Appendix at Tab.~\ref{tab:Vsionhyperpams}.% at Appendix~\ref{sec:hyperParams}.


% \begin{table}[h!]
% \scriptsize
% \caption{Ablations of our simplified SSSL variants {\color{black}Waiting for final results in NLP}}
% \label{tab:validateTheorem2}
% \centering
% \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lcc}
% \toprule
% Model &  Top-1  &  \# Of Parameters \\
% \midrule
% Vision Mamba (baseline) & 80.56 & TBD  \\
% Vision Mamba (simplified variant, $\bar{B}_i = B_i$ ) & 78.24 \\
% Vision Mamba (simplified variant, $\bar{A}_i = p_3(S_{\Delta} (x_i) )$ ) & 78.24 \\
% Vision Mamba (simplified variant, Eq.~\ref{eq:simplifiedModel} ) & 78.24 & TBD  \\
% DeiT & 78.21 & TBD  \\
% \bottomrule
% \end{tabular*}
% \end{table}






% \begin{figure}[h]
% \centering
% \begin{minipage}{0.49\textwidth}
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/vim_acc.png}
%     %\caption{Model ablations for our simplified model over image classification.}
%     %\label{fig:modelJustificationsVison}
% \end{minipage}
% % \hspace{0.02\textwidth}
% \begin{minipage}{0.49\textwidth}
%     \centering
%     \includegraphics[width=0.85\textwidth]{figures/ppl_test.png}
%     %\caption{Model ablations for our simplified model over image classification.}
%     %\label{fig:modelJustificationsNLP}
% \end{minipage}
% \caption{\textbf{Model justifications \& ablations: }In the \textbf{left} panel, we present the top-1 accuracy %score
% for image classification via the ImageNet-100 % benchmark
% , while the \textbf{right} panel displays the perplexity %score
% for language modeling using the WikiText-103. The y-axis represents the model's score across different epochs. In both figures, the blue curve represents the baseline, the yellow curve corresponds to Eq.\ref{eq:simplifiedModel}, the green curve illustrates Eq.\ref{eq:model}, and the red curve depicts the polynomial variant using standard discretization.
% % In both figures, the blue curve shows the baseline, the yellow curve represents Eq.\ref{eq:simplifiedModel}, the green curve corresponds to Eq.\ref{eq:model}, and the red curve depicts the polynomial variant with standard discretization.
% }
% \label{fig:modelJustifications}
% \end{figure}

\begin{figure}[t]
\vspace{-6pt}
\centering
\begin{tabular}{cc}
    \includegraphics[width=0.225\textwidth]{figures/vim_acc.png} &
    \includegraphics[width=0.225\textwidth]{figures/ppl_test.png} \\
\end{tabular}
\vspace{-8pt}
\caption{\textbf{Model justifications \& ablations: }In the \textbf{left} panel, we present the top-1 accuracy score
for image classification via the ImageNet-100 benchmark, while the \textbf{right} panel displays the perplexity score
for language modeling using the WikiText-103. The y-axis represents the model's score across different epochs. In both figures, the blue curve represents the baseline, the yellow curve corresponds to Eq.\ref{eq:simplifiedModel}, the green curve illustrates Eq.\ref{eq:model}, and the red curve depicts the polynomial variant using standard discretization.
% In both figures, the blue curve shows the baseline, the yellow curve represents Eq.\ref{eq:simplifiedModel}, the green curve corresponds to Eq.\ref{eq:model}, and the red curve depicts the polynomial variant with standard discretization.
}
\vspace{-9pt}
\label{fig:modelJustifications}
\end{figure}
% \vspace{-2pt}

\vspace{-4pt}
\subsection{Learning Polynomials\label{sec:LearningPoly}}
\vspace{-2pt}

In this section, we empirically validate our theoretical claims concerning the expressiveness of S6 and self-attention layers from Thms.~\ref{theorem:exprrsFull} and ~\ref{theorem:AnyPolywithMambas}. Since our analysis of the expressiveness gap between those layers relies on their characterization via multivariate polynomials, we focus on learning such functions over synthetic data. To isolate factors other than expressiveness, we employ a control setup with small NNs, comprising up to four layers with narrow widths (2 or 4 channels) and an additional output linear projection head. For each architecture, we used the standard implementations: (i) self-attention with softmax and positional encoding, and (ii) the original S6 architecture~\cite{gu2023mamba} with and without PE. The experiments examine the clean implementation of these components, without additional elements such as Conv1D, activations, or FFNs. We conduct experiments on two tasks: classification and regression. %Details of the hyperparameters are provided in Appendix TBD. 

% \medskip
{\noindent\textbf{Classification}\quad} Our dataset consists of binary random sequences %, each
of length $L=20$. The labels are uniformly distributed between 0 and $L$, determined using the ``Count in Row'' function~\cite{ali2024hidden}, defined as follows:
    \begin{definition}
        The count in row problem: Given a binary sequence $x_1, x_2, \ldots x_L \in  \{0,1\}^L$ such that $x_i \in \{0,1\}$ %for all $i \leq L$%
        , the ``count in row'' function $f$ is defined to produce an output sequence $y_1, y_2, \ldots, y_L$, where each $y_i = f(x_1, .. , x_i)$ is determined based on the contiguous subsequence of 1s to which $x_i$ belongs. Formally: %\footnote{where $[x_k > 0]$ is the Iverson bracket, equaling 1 if $x_k> 0$ and 0 otherwise.}:%
        \vspace{-7pt}
        {\small
        \begin{equation}
        y_i = \max_{0\leq j \leq i} \Big{(} \{ i-j+1 \mid \prod_{k=j}^i [x_k > 0] = 1\} \cup \{ 0\} \Big{)}
        \end{equation}
        }
        %
        %, capturing the essence of "counting in a row" by identifying the length of the contiguous sequence of 1s ending at $x_i$ if $x_i = 1$.
        where $[x_k > 0]$ is the Iverson bracket, equaling 1 if $x_k> 0$ and 0 otherwise.
    \end{definition}
    \vspace{-5pt}
    
The top part in Tab.~\ref{tab:EmpricalExpressClassification} presents the results. Remarkably, even a single layer of selective SSMs, both with and without PE, outperforms attention models with double the number of layers and channels, all while utilizing significantly fewer parameters, as suggested by Thm~\ref{theorem:exprrsFull}.


% \begin{table}[h!]
% \scriptsize
% \caption{Learning Polynomials:'SA' for self-attention, 'SSSM' for selective SSM, 'k' for the number of layers. Results are averaged over 3 seeds.}
% \label{tab:validateTheorem2}
% \centering
% \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llccc}
% \toprule
% \multicolumn{5}{c}{Classification}\\
%  SSSM (K=1) &  SSSM (K=2)  &  SA (K=1) & SA (K=2) & SA (K=4) \\
% \midrule
% {83.1\%} & \textbf{92.3\%} & {32.9\%} & {41.1\%}& {44.8\%}\\
% \bottomrule
% \multicolumn{5}{c}{Regression}\\
% \midrule
% 0 & 0 &0 & 0& 0\\
% \bottomrule
% \end{tabular*}
% \end{table}

\begin{table}[t]
\vspace{-8pt}
\caption{\small\textbf{Learning multivariate polynomials over synthetic data.} Classification results are presented in the top, while regression results are displayed on the bottom. Best results for each model depth in bold. %'Acc' for Accuracy, 'Params' for parameter count, 'S.-Attn' for Self-Attention and
'$D$' for the number of channels.}
\smallskip
\small
% \begin{minipage}[]{0.495\linewidth}
% \caption{TBD}
\label{tab:EmpricalExpressClassification}
\centering
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccc}
\toprule
\multicolumn{4}{c}{Classification}\\
\toprule
 Model &   \# Layers &  Accuracy  &  \# Parameters \\
\midrule
S6 w/ PE $(D=2)$ & 1  & 83.1 & 35 \\
S6 w/o PE $(D=2)$ & 1 & \textbf{84.8} & 35 \\
S6 w/ PE $(D=2)$ & 2 & 93.4 & 63 \\
S6 w/o PE $(D=2)$ & 2 & \textbf{97.1} & 63 \\
\midrule
Self-Attention$(D=2)$ & 1 & 32.9 & 29 \\
Self-Attention$(D=2)$ & 2 & 41.1 & 51 \\
Self-Attention$(D=2)$ & 4 & 44.8 & 95 \\
Self-Attention$(D=4)$ & 1 & 36.2 & 176 \\
Self-Attention$(D=4)$ & 2 & 44.2 & 244 \\
Self-Attention$(D=4)$ & 4 & 55.8 & 380 \\
% {83.1\%} & \textbf{92.3\%} & {32.9\%} & {41.1\%}& {44.8\%}\\
\bottomrule
\end{tabular*}
% \end{table} 
% \end{minipage}
\hfill
% \begin{minipage}[]{0.48\linewidth}
%\begin{table}%{r}{0.5\linewidth}
% \caption{TBD}
\label{tab:CelebA}
%\small
\centering
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}llcc}
\toprule
\multicolumn{4}{c}{Regression}\\
\toprule
 Model &   D  &  MSE  & \# Parameters \\
\midrule
S6 w/ PE & 4 & 12.67  & 101 \\
S6 w/o PE & 4 & \textbf{11.81} & 101   \\
S6 w/ PE & 6 & 12.45  & 157 \\
S6 w/o PE & 6 & \textbf{11.04} & 157   \\
S6 w/ PE & 8 & 12.17  & 377 \\
S6 w/o PE & 8 & \textbf{9.057} & 377 \\
\midrule
Self-Attention & 4 & 19.22   & 81\\
Self-Attention & 6 & 19.10   & 157\\
Self-Attention & 8 & 19.048  & 257\\
% {83.1\%} & \textbf{92.3\%} & {32.9\%} & {41.1\%}& {44.8\%}\\
\toprule
\end{tabular*}
% \end{minipage}
\vspace{-19pt}
\end{table}


{\noindent\textbf{Regression}\quad} We synthetically construct the dataset \( S = \{(x_i, y_i)\}_{i = 1}^m \)
by first randomly selecting a polynomial denoted by P. For each example in the dataset, we then generate $x$ values uniformly at random and compute the corresponding labels using this P.
% \vspace{-4pt}
{\small
\begin{equation}
\vspace{-3pt}
   %\quad 
   c_i \sim \mathbb{U}([-2,2]), \text{  } p_{i,j}\sim \mathbb{U}([ L ]), \text{  } x_j \sim \mathbb{U}([0.1,2])
   \vspace{-4pt}
\end{equation}
}
\vspace{-6pt}
% \vspace{-9pt}
{\small
\begin{equation}\label{eq:randomPoly}
\vspace{-4pt}
%\small
   y = P(x) = \sum_{i = 1}^3 c_i \Pi_{j=1}^L {x_j}^{p_{i,j}}% \quad p_{i,j}\sim \mathbb{U}([ L ]), \quad 
    % \end{equation}
% \vspace{-3pt}
% \begin{equation}\label{eq:assumptionsforsyntData}
   %,
\end{equation}
}

Our models consist of a single layer with either 4 or 8 channels, processing sequences of length $L=5$. As demonstrated in the bottom part of Tab.~\ref{tab:EmpricalExpressClassification}, both S6 variants, with and without PE, significantly outperform traditional self-attention layers across both model sizes. For example, while a self-attention model with 8 channels obtains an MSE score of 19.05, all S6 variants achieve an MSE below 12.67. These experiments demonstrate that at least in controlled environments with small models, S6 layers outperform traditional self-attention layers in approximating polynomials where the total multivariate degree exceeds the sequence length.
%
%
%
%
% \subsection{{\color{black} Empirical Generalization Bounds}\label{sec:empiricalBounds}}
% In this section, we empirically evaluate the generalization bounds derived in section~\ref{thm:genbound}. We focus on simple one-layer S6 neural networks trained on MNIST, exploring how the bound behaves with different hyperparameters. Our models consist of a single layer with $D$ channels, processing sequences of length $L = 784$. For our calculations, we used $\delta = 0.001$ and $\gamma = 1$.
%
% \begin{tabular}{|c|c|c|c|c|c|c|c|}
% \hline
% D, N & $K$ & generalization bound & $\rho_A(w)$ & $\rho_B(w)$ & $\rho_C(w)$ & $\rho_{\Delta}(w)$ & $\rho_W(w)$\\
% \hline
% 4 & 0.93 & 324.74 & 0.28 & 6.32 & 9.4 & 0.89 & 22.82\\
% 16 & 0.94 & 441.15 & 0.17 & 7.95 & 18.71 & 0.92 & 19.47\\
% 32 & 0.97 & 1585.57 & 0.24 & 13.46 & 25.66 & 0.96 & 20.79 \\
% \hline
% \end{tabular}
%
% The table above shows our results, confirming that the value of $K$ is consistently less than $1$ in all tested cases. This aligns with our assumptions in the theorem, providing further evidence for its validity. 
%




\vspace{-6pt}
\section{Discussion}
\vspace{-2pt}
To understand the implications of our theory, we first explain why analyzing Softmax-free attention is realistic. Then, we discuss the consequences for standard attention models.

\noindent\textbf{Transformers Without Softmax\quad} The softmax function is primarily associated with optimization and stability, as it normalizes attention scores to the [0, 1] range and prevents numerical instabilities. However, transformer variants without softmax have proven effective in several domains, including reducing latency~\cite{hua2022transformer,lu2021soft,ramapuram2024theory} and in applications such as vision~\cite{wortsman2023replacing}, NLP~\cite{ma2022mega}, and other areas~\cite{zimerman2023converting}. {\color{black}Additionally, these models have recently become even more practical, as researchers have successfully scaled linear attention far beyond 7B parameters~\cite{li2025minimax}, enabling LLMs to extend their context window to 4 million tokens while matching the performance of GPT-4o and Claude-3.5 Sonnet. Several additional linear attention-based LLMs were presented in~\cite{shen2024scaling, qin2023transnormerllm, sun2023retentive}.}
Since these models achieve near-SoTA, focusing on a softmax-free attention model is well justified.

\noindent\textbf{Transformers With Softmax\quad} While the softmax function can theoretically be expressed as an infinite-degree polynomial, we provide careful considerations. The softmax function involves both exponentiation and proportional normalization. The former can be well approximated using low-degree polynomials~\cite{zhang2024secure}, while the latter primarily serves to normalize the scores. We refine our assumption by analyzing transformers that apply exponentiation to each attention score, assuming this can be approximated by a polynomial of degree P. The resulting model expresses higher-degree polynomials within each layer, but it remains based on pairwise interactions via Key, Query, and Values, leading to an maximal polynomial degree of $3P+1$, independent of the sequence length $L$. This supports the validity of our argument in more common regimes.

%
% \smallskip
%\noindent\textbf{Language Modeling Capabilities\quad}
\noindent\textbf{Interpretation and Intuition}\quad}
Our characterization of S6 layers through the lens of %multivariate
polynomials offers a novel perspective on the semantic capabilities of Mamba. Specifically, we extend the concept of polynomial degree to quantify the number of tokens involved in each interaction within a layer of an model. For instance, low-degree polynomials correspond to interactions involving only a few tokens, while high-degree polynomials represent dependencies spanning many tokens. This analysis highlights the unique strength of S6 layers in modeling continuous, multi-token interactions, such as counting and recurrent operations. In contrast, transformers, % which primarily rely on pairwise token interactions,
are naturally biased toward sparser and more fragmented representations such as induction heads. 

This perspective can also shed light on the remarkable performance of hybrid models that combine modern RNNs and attention by leveraging their complementary strengths~\cite{lieber2024jamba,de2024griffin}. While a formal characterization of their trade-offs is yet to be established, our analysis suggests that S6 and attention capture fundamentally distinct types of interactions, characterized by the number of tokens involved in each interaction.


\vspace{-8pt}
\section{Conclusions}%} \& Limitations}
\vspace{-3pt}
% {\color{blue} I think we're promising too much for future work, we can write that these are promising directions for future work and leave it a bit more vague}
This study explores the expressivity of Mamba models. By reducing the S6 layer to a polynomial form and composing an associated theory, we have established a novel connection between S6 layers and high-degree multivariate polynomials. This connection enables us to identify the expressivity gap between S6 layers and attention mechanisms comprehensively. {\color{black}We show that although the S6 layer has better theoretical expressivity than linear attention for long sequences, this does not negatively impact generalization. We provide a length-agnostic generalization bound to support this result, allowing us to conclude that the S6 layer has superior theoretical properties compared to linear attention for long-range tasks.} %Moving forward, we aim to broaden our theoretical framework to encompass other gated RNNs, including xLSTM~\cite{beck2024xlstm}, RWKV~\cite{peng2023rwkv,peng2024eagle}, and
%and Griffin~\cite{de2024griffin}. %, and HGRN2~\cite{qin2024hgrn2}. 
%Additionally, %we plan to incorporate a more extensive array of components from both Mamba and transformer architectures into our analyses, such as LayerNorm, FFN, Softmax, Conv1D, and gated branches. 
% future research will focus on identifying specific domains where dependencies modeled by high-degree multivariate polynomials are prevalent, thereby assessing whether Mamba models demonstrate superior performance in these contexts. %One initial hypothesis relates to the general product rule~\cite{feller1991introduction} $( P(x_1, \dots, x_L) = \prod_{j=1}^L P(x_j | x_{j-1}))$, which plays a crucial aspect in conditional probability. Such polynomials can be effective in domains like NLP, where it is fundamental to n-gram models, and RL, where it determines the likelihood of trajectories.
Finally, the limitations of our work are discussed in Appendix~\ref{sec:limitations}.
% Regarding generalization, our findings offer insights into providing generalization guarantees for S6 with minimal reliance on network size or sequence length. This research underscores the critical impact of network architecture on testing performance and paves the way for future enhancements and the development of theory-driven regularization techniques, e.g., %several similar examples are
% \cite{ledent2021norm, long2019generalization}. %{\color{black}Our next steps include extending these insights to deeper networks.}
%
%
%
\newpage

% \section*{Impact Statement}
% {\color{black}Our work establishes a stronger theoretical foundation for the expressivity of Mamba layers, demonstrating their superior representational power compared to linear attention. Developing such theoretical insights not only deepens our understanding and trust in these models but also paves the way for more principled, theory-driven advancements in sequence modeling. By bridging empirical success with rigorous analysis, our findings contribute to the design of more effective, expressive, and efficient architectures, driving significant progress in deep sequence modeling.}

\section{Acknowledgments}
This work was supported by a grant from the Tel Aviv University Center for AI
and Data Science (TAD) and the Ministry of
Innovation, Science \& Technology ,Israel (1001576154) and the Michael J. Fox
Foundation (MJFF-022407). This research was also supported by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080).

\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\input{appendix_experiments}

\input{appendix_proofs}

\input{appendix_generalization}

\section{Additional Related Work on Generalization\label{sec:RelatedWorkGeneralization}}
Explaining the performance of overparameterized deep neural networks (DNNs) on test data remains a major challenge in deep learning theory. Traditional tools like the PAC-learning framework and VC-dimension often provide vacuous bounds when the number of parameters greatly exceeds the number of data points. To address this, many studies conduct architecture-specific analyses. For instance, Allen et al. analyze the dynamics of stochastic gradient descent on RNNs with ReLU activations, offering optimization and generalization guarantees~\citep{allen2019can}. Other works have explored the generalization of RNNs for unseen data and longer sequences under various assumptions~\citep{cohen2022implicit, emami2021implicit, cohen2022learning, hardt2018gradient}.

There are relatively few results that address modern architectures such as S6 layers. A recent contribution proposes a bound for standard SSMs~\citep{liu2024generalization}, but it does not extend to Selective SSMs. To address this gap, we propose a new bound specifically for Selective SSMs.

\section{Limitations\label{sec:limitations}}
In this paper, we provide a new perspective on the expressivity gap between S6 layers and self-attention, the core layers of Mamba models and transformers. While our analysis leverages multivariate polynomial degrees as measures of expressiveness and offers insightful results, it does not formally connect these measures to widely-used expressivity metrics in the literature, such as Rademacher complexity or VC dimension. Establishing such connections remains an open challenge.

Additionally, our work focuses on a simplified architecture, omitting several components of the original models. While we empirically justify these simplifications and highlight the opportunity to use simpler models by identifying the key components responsible for the performance gap, analyzing the full complexity of softmax-based Transformer and Mamba architectures is an important direction for future research. 

Finally, although expressiveness is a critical property to explore, LLMs with billions of parameters involve additional factors that influence their capacity and performance. These include optimization challenges, gradient behavior, implicit biases, and training stability. Addressing these aspects is beyond the scope of this study, which is centered on a theoretical characterization of expressiveness. We believe these topics represent promising avenues for future work.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
