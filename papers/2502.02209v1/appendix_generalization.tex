\setcounter{section}{2}
\subsection{Generalization}

Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$. Our generalization bound is based on a uniform-convergence generalization bound provided in ~\citep{galanti2024norm}. The following lemma bounds the gap between the test error and the empirical margin error, represented as $\err^{\gamma}_S(f_w)= 
\fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X^{}_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})]$. 

\begin{lemma}\label{lem:loss_ramp}
Let $P$ be a distribution over $\mathbb{R}^{\mathcal{D}} \times [C]$ and $\mathcal{F} \subset \{f':\mathcal{X} \to \R^C \}$. Let $S = \{(X_j,y_j)\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$ and $X=\{X_j\}^{m}_{j=1}$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, we have
\begin{equation}\label{eq:Radbound}
\err_P(f_w) - \err^{\gamma}_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}_{X}(\mathcal{F}) + 3\sqrt{\frac{\log(2/\delta)}{2m}}.
\end{equation} 
\end{lemma}
\begin{lemma}\label{lem:peeling}
Let $\sigma_j$ be a $l_j$-Lipschitz, positive-homogeneous function and $l=\max_j l_j$. Let $\xi_i \sim U[\{\pm 1\}]$. Then for any class of vector-valued functions $\mathcal{F} \subset \{f \mid ~f:\mathbb{R}^d \to \mathbb{R}\}$ and any convex and monotonically
increasing function $g : \mathbb{R} \to [0,\infty)$, 
\begin{small}
\begin{equation*}
\begin{aligned}
\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)
~\leq~ 2\E_{\xi} \sup_{f\in \mathcal{F}} g\left(l\left|\sum^{m}_{i=j} \xi_j  \cdot f(x_j)\right| \right).
\end{aligned}
\end{equation*}
\end{small}
\end{lemma}
% \begin{restatable}[Contraction Lemma]{lemma}{peeling}\label{lem:peeling}
% Let $\sigma_j$ be a $l_j$-Lipschitz, positive-homogeneous function and $l=\max_j l_j$. Let $\xi_i \sim U[\{\pm 1\}]$. Then for any class of vector-valued functions $\mathcal{F} \subset \{f \mid ~f:\mathbb{R}^d \to \mathbb{R}\}$ and any convex and monotonically
% increasing function $g : \mathbb{R} \to [0,\infty)$, 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)
% ~\leq~ 2\E_{\xi} \sup_{f\in \mathcal{F}} g\left(l\left|\sum^{m}_{i=j} \xi_j  \cdot f(x_j)\right| \right).
% \end{aligned}
% \end{equation*}
% \end{small}
% \end{restatable}
\begin{proof}
We notice that since $g(|z|) \leq g(z)+g(-z)$,
\begin{small}
\begin{equation*}
\begin{aligned}
&\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)\\
&\leq \E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
&+\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( - \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right) \\
&= 2\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
\end{aligned}
\end{equation*}
\end{small}
where the last equality follows from the symmetry in the distribution of the $\xi_i$ random variables. By Equation 4.20 in~\cite{Ledoux1991ProbabilityIB}  we have the following:
\begin{small}
\begin{equation*}
\begin{aligned}
&\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
&=\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \sum^{m}_{j=1} \xi_j \cdot \frac{1}{l}\sigma_j(f(x_j)) \right)\\
&\leq\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \sum^{m}_{j=1} \xi_j \cdot f(x_j) \right)\\
&\leq\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \left| \sum^{m}_{j=1} \xi_j \cdot f(x_j) \right| \right)\\
\end{aligned}
\end{equation*}
\end{small}
where the last equality follows from $g$ being monotonically increasing. 
\end{proof}
% \rademacher*
The model is denoted by:
\begin{equation}\label{eq:modelAppendix}
\begin{aligned} 
      &B_i = S_B x_i, \quad C_i = S_C x_i, \quad \Delta_i = \sigma(S_{\Delta} x_i), \quad  \\&\bar{A}_i =  \exp(\Delta_i A), \quad \bar{B}_i = B_i 
\end{aligned}
\end{equation}
Where \( \sigma \) is a 1-Lipschitz activation function. 
% For our analysis, we will use \( \text{ReLU} \).
We consider a classifier $f : \mathbb{R}^{D \times L} \rightarrow \mathbb{R}^{\mathcal{C}}$ defined as follows. We have parameters $(A^{}, S_B^{}, S_C^{}, S_{\Delta}^{})$ associated with the layer. The norms are defined as follows:

\begin{equation}
\begin{aligned} 
    \rho_A^{}(w) &= \|A^{}\|_{\max} \\
    \rho_B^{}(w) &= \|S_B^{}\|_{2,\infty} \\
    \rho_C^{}(w) &= \|S_C^{}\|_{F} \\
    \rho_{\Delta}^{}(w) &= \|S_{\Delta}^{}\|_{2}
\end{aligned}
\end{equation}

We denote the product of these norms as:
\begin{equation}
\Gamma^{}(w) = \rho_A^{}(w) \cdot \rho_B^{}(w) \cdot \rho_C^{}(w) \cdot \rho_{\Delta}^{}(w)
\end{equation}

Given these definitions, the classifier $f$ for a specific class $c \in [\mathcal{C}]$ is computed as:

\begin{small}
\begin{equation*}
\begin{aligned}
&f^c(X_{*1},...,X_{*L}) =  
\sum_{d=1}^D W_{c,d} \left(S_C^{} X^{}_{*L}\right)^T \sum_{i=1}^L \left(\prod_{k=i+1}^L \bar{A}^{}_{dk} \right) S_B^{} X^{}_{*i} X^{}_{di}
\end{aligned}
\end{equation*}
\end{small}

Here, $W \in \mathbb{R}^{\mathcal{C} \times D}$ represents a linear projection from the output to the number of classes, and $\mathcal{C}$ is the number of classes. 

We denote the parameters of the classifier by 
\[
w = (A^{}, S_B^{}, S_C^{}, S_{\Delta}^{}, W) 
\] and the function induced by a specific instance of $w$ is denoted by $f_w$. The class of functions taking on different parameter instances $w$ is denoted by $\mathcal{F}$.
Denote 
\[
\rho = \{\rho_W, \rho_A, \rho_B, \rho_C, \rho_{\Delta}\}
% \quad \text{where} \quad \rho_A = \{\rho_A^{}\}_{h=1}^{H}, \ \rho_B = \{\rho_B^{}\}_{h=1}^{H}, \ \rho_C = \{\rho_C^{}\}_{h=1}^{H}, \ \rho_{\Delta} = \{\rho_{\Delta}^{}\}_{h=1}^{H}
\]
and let:
\begin{equation}
\begin{aligned}
   \mathcal{F}_{\rho} 
    &=\{f_w \in \mathcal{F} : \Gamma(w) \leq  \rho_A \rho_B \rho_C \rho_{\Delta} \rho_W=:\Gamma \}
\end{aligned}
\end{equation}

The following theorem provides a bound on the Rademacher complexity of the class $\mathcal{F}_{\rho}$.
\setcounter{section}{2}
\setcounter{theorem}{3}
\begin{theorem}\label{theorem:rad}
Let $\rho = \{\rho_W, \rho_A, \rho_B, \rho_C, \rho_{\Delta}\}$. Suppose we have \(m\) sample sequences \(X = \{ X_{(j)} \}_{j=1}^{m}\), where each \(X_{(j)} = (X_{(j)_{*1}}, \ldots, X_{(j)_{*L}}) \in \mathbb{R}^{D \times L}\). Assume that $\forall j \in [m]: ||X_{(j)}||_{\max} \leq 1$. Additionally, suppose $\forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1$. Then,
\begin{small}
\begin{equation*}
\begin{aligned}
\mathcal{R}_X(\mathcal{F}_{\rho})
&\leq\frac{1}{m} D^{2} \Gamma (1 + \sqrt{2\log (2L \mathcal{C} D^4 N)}) \cdot  \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}
\end{aligned}
\end{equation*}
\end{small}
where the maximum is taken over \(t \in [D]\),  \(k \in [L]\). 
\end{theorem}

\begin{proof}
% We approximate the exponential function using the first three terms of its Taylor series expansion, $\exp(z) \sim 1 + z + z^2/2$.
For aesthetic purposes, we define \( B \) as \( S_B \) and \( C \) as \( S_C \).
The Rademacher complexity of $\mathcal{F}_{\rho}$ is given by:
\begin{small}
\begin{equation*}
\begin{aligned}
&m\mathcal{R(\mathcal{F}_{\rho})} = \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} f^c_w(X_{(j)}) \right] \\
&= \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
\left(S_C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right. \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) 
\cdot A^{}_{d*}\right)\right) S_B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
&= \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
\left(C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right. \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) 
\cdot A^{}_{d*}\right)\right) B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] 
\end{aligned}
\end{equation*}
\end{small}

Here, {$A^{}_{d*}$ represents the $d$-th row of $A^{}$}, which has a size of $N$. We think of $A^{}_{d*}$ as a diagonal matrix of size $N\times N$, where its diagonal elements are the values in the $d$th row of $A$. Thus, $A^{}_{d*} = \text{diag}(a^{}_{d1},...,a^{}_{dN})$. Hence,
\begin{small}
\begin{equation*}
\begin{aligned}
& \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
\left(C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right. 
 \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
A^{}_{d*}\right) \right) B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
&=~ \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
\sum_{l=1}^N \left(C^{} X^{}_{{(j)}_{*L}}\right)_l^T \sum_{i=1}^L \right. \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
&=~ \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
\sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
&=~ \E_{\xi}\left[\sup_{w} \sum_{c=1}^{\mathcal{C}} \sum_{d=1}^D W_{c,d} \sum_{j=1}^m \xi_{jc} 
\sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right]
\end{aligned}
\end{equation*}
\end{small}

This follows from expressing the model in a more explicit form. Next,
\begin{small}
\begin{equation*}
\begin{aligned}
&~ \E_{\xi}\left[\sup_{w} \sum_{c=1}^{\mathcal{C}}  \sum_{d=1}^D W_{c,d} \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% &\leq~ \rho_W\E_{\xi}\left[\sup_{w,c}  \left( \sum_{d=1}^D \left( \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L  \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right)^2 \right) \right] \\
% &\leq~ \rho_W\E_{\xi}\left[\sup_{w,c, d}  \sqrt{ D \left( \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right)^2} \right] \\
&=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right.\left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} | \right]
\end{aligned}
\end{equation*}
\end{small}
where the inequality follows from moving the norm of $W$ to $W_{c}$ for maximizing the inner term and applying the Cauchy-Schwartz inequality.
Next,
\begin{small}
\begin{equation*}
\begin{aligned}
&~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} | \right] \\
&=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(\sum_{s=1}^D C_{ls}^{} X^{}_{{(j)}_{sL}}\right) \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} | \right] \\
&=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{l=1}^N \sum_{s=1}^D C_{ls}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} | \right] \\
% &\leq~ \sqrt{D} \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l}    \sqrt{ \sum_{s=1}^D \left( \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} \right)^2} \right] \\
&\leq~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} |  \right] \\
&=~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{s'=1}^D B_{ls'}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right.  \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right]
\end{aligned}
\end{equation*}
\end{small}
where the first inequality follows from moving the norm of $C$ to $C_{l}$ for maximizing the inner term and applying the Cauchy-Schwartz inequality. 
\begin{small}
\begin{equation*}
\begin{aligned}
&=~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{s'=1}^D B_{ls'}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right.  \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% &\leq~ D \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s}  \sqrt{ \sum_{s'=1}^D \left( \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} \right)^2 }  \right] \\
&\leq~ D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
&=~ D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L  \right. \left.\exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
&\leq~ D^{1.5} \rho^{}_W \rho^{}_C  \rho^{}_B \sum_{i=1}^L \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot  \right. \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right]
\end{aligned}
\end{equation*}
\end{small} where the second to last inequality follows from $\|x\|_2 \leq \sqrt{n} \|x\|_{\infty}$ for any $x \in \mathbb{R}^{n}$. 
Jensen's inequality gives the following inequality:
\begin{small}
\begin{equation}\label{eq:eq1}
\begin{aligned}
&~ \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% &\leq~ \E_{\xi}\left[\sup_{w,c, d, l, s, s'} \left| \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}}  \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} \right|  \right] \\
&\leq~  \frac{1}{\lambda_i} \log ( \E_{\xi}\left[\sup_{w,c, d, l, s, s'}  \exp (\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \left.  \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) \\
&\leq~  \frac{1}{\lambda_i} \log ( \sum_{c, d, l, s, s'} \E_{\xi}\left[\sup_{w} \exp(\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot  \right. \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) \\
&\leq~  \frac{1}{\lambda_i} \log ( \mathcal{C} D^3 N  \max_{c, d, l, s, s'} \E_{\xi}\left[\sup_{w} \exp(\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right.  \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) :=\Theta \\
\end{aligned}
\end{equation}
\end{small}
For fixed $\lambda_i > 0$. The second inequality follows from the fact that $\sup_x \sup_y f(x,y) = \sup_{x,y} f(x,y)$.
We observe that the inner expectation $\sup$ depends only on $w$.
Next we use Lemma~\ref{lem:peeling} with $\sigma_{ij}(z) = \exp(z)X^{}_{{(j)}_{sL}} X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}}$ on its domain and $g_i(X_{(j)})=\sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}$. The corresponding Lipschitz constants are $l_{ij} = \max_{z \in dom(\sigma_{ij})}(\exp(z)X_{(j)_{sL}}X_{(j)_{s'i}} X_{(j)_{di}})$ and $l_i = \max(l_{ij})$. Therefore:
\begin{small}
\begin{equation}
\begin{aligned}
&\E_{\xi}\left[\sup_{w} \exp(\lambda_i | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] \\
% &\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i  \left| \sum_{j=1}^m \xi_{jc}  X^{(j)}_{sL}  (\exp(\sum_{k=i+1}^L \Delta^{(j)}_{d,k}a_{dl})) X^{(j)}_{s'i} X^{(j)}_{di} \right| \right)\right] \\ 
&=\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i  \left| \sum_{j=1}^m \xi_{jc} (\sigma_{ij}(g_i(X^{(j)}))) \right| \right)\right] \\
&\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i l_i \left| \sum_{j=1}^m \xi_{jc}    \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl} \right| \right)\right] \\
&\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i \rho^{}_A  l_i  \left| \sum_{j=1}^m \xi_{jc}    \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \right| \right)\right] \\
&\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc}   \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \right| \right)\right] \\
&\leq 4\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc}   S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
\end{aligned}
\end{equation}
\end{small}
This follows from applying Lemma~\ref{lem:peeling} with $\sigma$ which has a Lipschitz constant of 1, as assumed. Hence:
\begin{small}
\begin{equation} \label{eq:lip}
\begin{aligned}
% &4\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A   l_i \left| \sum_{j=1}^m \xi_{jc} \sum_{\substack{k=i+1 \\ S_{\Delta} X_{(j)_{*k}} > 0}}^{L} S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
& 4\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc} S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
&\leq 4\E_{\xi}\left[ \sup_{k}  \exp \left( \lambda_i \rho^{}_A \rho^{}_{\Delta} (L-i) l_i || \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{*k}} || \right)\right] \\
&\leq 4\E_{\xi}\left[ \sup_{t,k}  \exp \left( \lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i \left| \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}} \right| \right)\right] \\
&\leq 4DL \max_{t, k} \E_{\xi}\left[ \exp \left(\lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i  \left|  \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}}  \right| \right)\right] \\
% &\leq 4\E_{\xi}\left[ \sup_{w, k}  \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} \sum_{i=1}^L (L-i)l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
% &\leq 2\E_{\xi}\left[ \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
\end{aligned}
\end{equation}
\end{small}

Denote:
\begin{small}
\begin{equation} \label{eq:lip2}
\begin{aligned}
M_i := \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i 
\end{aligned}
\end{equation}
\end{small}

% We notice that:

% \begin{small}
% \begin{equation} \label{eq:lip}
% \begin{aligned}
% &\leq 2\E_{\xi}\left[ \sup_{ k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A  \left| \left| \sum_{j=1}^m \xi_{jc}  X_{*k}^{(j)} \right|\right|_2 \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{ k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A  \sqrt{ \sum_{t=1}^D \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right|^2 } \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{t,k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A \sqrt{D}  \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right| \right)\right] \\
% &\leq 2DL \max_{t, k} \E_{\xi}\left[ \exp \left((L-i) \lambda_i l_i \rho_{\Delta} \rho_A \sqrt{D}  \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right| \right)\right] \\
% \end{aligned}
% \end{equation}
% \end{small}
%  where the second to last inequality follows from $\|x\|_2 \leq \sqrt{n} \|x\|_{\infty}$ for any $x \in \mathbb{R}^{n}$. 

As a next step, we would like to bound the above term using a function of the data that is not dependent on an expected value of noise labels $\xi$. For this purpose we apply a technique that was introduced in the proof of Theorem~1 in~\citep{golowich2018size}. We apply this process separately for each $i \in [L]$. Let $i \in [L]$:
We define a random variable $Z$:
\begin{small}
\begin{equation*}
\begin{aligned}
&Z = M_i  \left| \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}} \right| \\
&z_j = X^{}_{{(j)}_{tk}} \rightarrow Z = M_i |\sum_{j=1}^m \xi_{jc} z_j | \\
\end{aligned}
\end{equation*}
\end{small}
The random variable $Z$ depends on the random variables $\xi_{jc}$.
Then, we have: 
\begin{small}
\begin{equation*}
\begin{aligned}
&=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda _iZ) \right] \\
&=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z + \lambda_i\E(Z) - \lambda_i \E(Z)) \right] \\
&=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z - \lambda_i \E(Z)) \right] +  \E_{\xi} (Z) \\
\end{aligned}
\end{equation*}
\end{small}
By Jensen’s inequality, we obtain a bound for $\E(|\sum_{j=1}^m \xi_{jc} z_j |)$:
\begin{small}
\begin{equation*}
\begin{aligned}
&\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j |) = \E_{\xi} (\sqrt{|\sum_{j=1}^m \xi_{jc} z_j |^2}) \leq \sqrt{\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j |^2)} = \\& \sqrt{\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j} |^2) = \sqrt{\E_{\xi} (|\sum_{j,j'=1}^m \xi_{jc} \xi_{j'c} z_j z_{j'}} |) = \sqrt{\sum_{j=1}^m |z_j|^2} 
\end{aligned}
\end{equation*}
\end{small}
Namely $\E_{\xi}(Z) \leq M_i \sqrt{\sum_{j=1}^m |z_j|^2} $.
$Z$ is a deterministic function of the i.i.d. random variables  $\xi_{jc}$ and satisfies the following:
\begin{small}
\begin{equation*}
\begin{aligned}
Z(\xi_{1c},..., \xi_{jc},...,\xi_{mc}) - Z(\xi_{1c},..., -\xi_{jc},...,\xi_{mc}) \leq 2|z_j|
\end{aligned}
\end{equation*}
\end{small}
This follows from the triangle inequality.
This means that $Z$ satisfies a bounded-difference condition, which, by the proof of Theorem~6.2 in ~\citep{Boucheron2010}, implies that $Z$ is sub-Gaussian, with variance factor:
\begin{small}
\begin{equation*}
\begin{aligned}
v = \frac{1}{4} \sum_{j=1}^m (2M_i|z_j|)^2 = M_i^2 \sum_{j=1}^m |z_j|^2
\end{aligned}
\end{equation*}
\end{small}
It follows that:
\begin{small}
\begin{equation*}
\begin{aligned}
&\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z - \lambda_i \E_{\xi}(Z)) \right] \leq \\& \frac{1}{\lambda_i} \frac{\lambda_i^2 M_i^2 \sum_{j=1}^m |z_j|^2}{2} =  \frac{\lambda_i M_i^2 \sum_{j=1}^m |z_j|^2}{2}  
\end{aligned}
\end{equation*}
\end{small}
Therefore:
\begin{small}
\begin{equation}\label{eq:eq2}
\begin{aligned}
&\frac{1}{\lambda_i} \log \E_{\xi}\left[ \exp( \lambda_i Z - \lambda_i \E_{\xi}(Z)) \right] +  \E_{\xi} (Z) \\
&\leq \frac{\lambda_i M_i^2 \sum_{j=1}^m |z_j|^2}{2} + M_i\sqrt{\sum_{j=1}^m |z_j|^2} 
\end{aligned}
\end{equation}
\end{small}
\paragraph{Analyzing Lipschitz constants $l_{ij}$.}
Next, we analyze the Lipschitz constants \( l_{ij} \).
For $l_{ij} = \max_{z \in dom(\sigma_{ij})}(\exp(z)X_{(j)_{sL}}X_{(j)_{s'i}} X_{(j)_{di}})$ and $l_i = \max(l_{ij})$ is of the form $g_i(X_{(j)})=\sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}$ (see \eqref{eq:lip}). Since  
\begin{small}
\begin{equation*}
\begin{aligned}
&l_i = \max_{j} l_{ij} = \max_{j} \max_{z \in dom(\sigma_{ij})}(\exp(z)X^{(j)}_{sL}X^{(j)}_{s'i} X^{(j)}_{di}) \\& \leq \max_{j} \max_{z \in dom(\sigma_{ij})} \exp \left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl} \right) \cdot 1 < K^{L-i}
\end{aligned}
\end{equation*}
\end{small}
which is followed from our assumptions. 
% {\color{black}$\forall n \in [N]:(\exp(\Delta^{(j)}_{d,k} * A_{d*}))_n < K < 1$ and the fact that the data is normalized.}
% We get:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \sum_{i=1}^L l_i &\leq \sum_{i=1}^{L} K^{L-i} = \frac{K^L - 1}{K - 1}  \\
% % &= \frac{(L-1)M^{L+1} - LM^L + M}{(M-1)^2 M}
% \end{aligned}
% \end{equation*}
% \end{small}

% We conclude that:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \lim_{L \rightarrow \infty} \frac{K^L - 1}{K - 1}= \frac{1}{1-K}
% \end{aligned}
% \end{equation*}
% \end{small}

We get:
\begin{small}
\begin{equation*}
\begin{aligned}
\sum_{i=1}^L l_i (L-i) &\leq \sum_{i=1}^{L} (L-i) (K)^{L-i} = \frac{(L-1)K^{L+1} - L K^L +K}{(K-1)^2}  \\
% &= \frac{(L-1)M^{L+1} - LM^L + M}{(M-1)^2 M}
\end{aligned}
\end{equation*}
\end{small}

We conclude that:
\begin{small}
\begin{equation*}
\begin{aligned}
\lim_{L \rightarrow \infty} \frac{(L-1)K^{L+1} - L K^L +K}{(K-1)^2}= \frac{K}{(K-1)^2}
\end{aligned}
\end{equation*}
\end{small}

\paragraph{Concluding the proof.} By combining ~\eqref{eq:eq1},~\eqref{eq:lip} and~\eqref{eq:eq2}, we have:
\begin{small}
\begin{equation*}
\begin{aligned}
& { \Theta} \leq  \frac{1}{\lambda_i} \log \left(4 L \mathcal{C} D^4 N \right) + \max_{c, d, l, s, s', t, k} \frac{1}{\lambda_i} \cdot \\& \log \left(  \E_{\xi}\left[ \exp \left(\lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i  \left|  \sum_{j=1}^m \xi_{jc} 
X^{}_{{(j)}_{tk}}  \right| \right)\right] \right) \\
&\leq \frac{1}{\lambda_i}  \log \left(4L \mathcal{C} D^4 N \right) +  \max_{c, d, l, s, s', t, k} \frac{\lambda_i (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)^2 \sum_{j=1}^m (X_{(j)_{tk}})^2}{2} + (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)\sqrt{\sum_{j=1}^m (X_{(j)_{tk}})^2} \\
\end{aligned}
\end{equation*}
\end{small}
We choose $\lambda_i = \sqrt{\frac{2\log (4 L \mathcal{C} D^4 N)}{M_i^2  \max_{t,k}\sum_{j=1}^m (X_{(j)_{tk}})^2}}$ which minimizes the above term and obtain the following inequality:
\begin{small}
\begin{equation*}
\begin{aligned}
&\sum_{i=1}^L \frac{1}{\lambda_i} \log \left(4L \mathcal{C} D^4 N \right) + \max_{t, k} \frac{\lambda_i (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)^2 \sum_{j=1}^m (X_{(j)_{tk}})^2}{2} + (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)\sqrt{\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
&\leq \sum_{i=1}^L (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)})M_i\sqrt{ \max_{t, k}\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
&= \sum_{i=1}^L (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i \cdot \sqrt{ \max_{t, k}\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
&\leq (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{D} \rho^{}_A \rho^{}_{\Delta}\sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}\\
% &\frac{1}{\lambda} \log \left(2 \mathcal{C} D^3 N \right)  +  \frac{\lambda M^2  \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}{2}  +M  \sqrt{ \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2} \\
% &\frac{1}{\lambda} \log \left( \mathcal{C} 2^{3(H-1)} D^4 N L^{H} \right)  +  \max_{c, d, l, s, s'}  \max_{k_1,...,k_{H-1}, k, t}  \frac{\lambda M^2 \sum_{j=1}^m (X_{{(j)}_{tk}})^2}{2} + M\sqrt{\sum_{j=1}^m (X_{{(j)}_{tk}})^2} \\
% &\leq (1 + \sqrt{2\log (2 \mathcal{C} D^3 N)})M  \sqrt{ \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}  \\
\end{aligned}
\end{equation*}
\end{small}
We conclude that:
\begin{small}
\begin{equation*}
\begin{aligned}
&m\mathcal{R(\mathcal{F}_{\rho})} \\ &\leq 
 D^{2} \Gamma (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}\\
 % \\&\leq
 % D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \rho^{}_A \rho^{}_{\Delta} (1 + \sqrt{2\log (2 \mathcal{C} D^3 N)})  \frac{1}{1-K}   \sqrt{\max_i \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}\\
\end{aligned}
\end{equation*}
\end{small}
\end{proof}
\setcounter{theorem}{4}
\begin{theorem}\label{theorem:genproof}
Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$ and $\delta > 0$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$. Assume that $\forall j \in [m]: ||X_{(j)}||_{\max} \leq 1$. Additionally, suppose $\forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, 
\begin{small}
\begin{equation*}
\begin{aligned}
&\err_P(f_w) - \fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})] \\& = \err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma m} ({\Gamma(w) } +{\frac{1}{D^2N^2}}) D^{2} \cdot \\& (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} \\&+ 3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\Gamma(w) }+2)}{2m}},
\end{aligned}
\end{equation*}
\end{small}
where the maximum is taken over \(t \in [D]\),  \(k \in [L]\). 

\end{theorem}
\begin{proof}
For aesthetic purposes, we define \( B \) as \( S_B \) and \( C \) as \( S_C \). We want to prove the bound for all $f_w \in \mathcal{F}$ where:
{
\begin{small}
\begin{equation*}
\begin{aligned}
&\mathcal{F} := \\&\{f_w : w = (A, B, C, S_\Delta, W), \forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1 \}
\end{aligned}
\end{equation*}
\end{small}}
Let $t \in \mathbb{N}$. Denote:
\begin{small}
\begin{equation*}
\begin{aligned}
&\mathcal{S}(t) := \{f_w \in \mathcal{F} , \Gamma(w) < {\frac{t}{D^2N^2}} \}
\end{aligned}
\end{equation*}
\end{small}
Correspondingly subdivide $\delta$ as:
\begin{small}
\begin{equation*}
\begin{aligned}
&\delta(t) := \frac{\delta}{t(t+1)}
\end{aligned}
\end{equation*}
\end{small}

By Lemma ~\ref{lem:loss_ramp} and Theorem ~\ref{theorem:rad}, with probability at least $1-\delta(t)$:
for any function $f_w \in \mathcal{S}(t) $, we have the following inequality:
\begin{small}
\begin{equation*}
\begin{aligned}
\err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}(\mathcal{S}(t)) + 3\sqrt{\frac{\log(2/\delta(t))}{2m}}.
\end{aligned}
\end{equation*}
\end{small}
Using the union bound over all possible set $\mathcal{S}(t)$, we establish that the above probabilistic bound holds uniformly for all functions $f_w \in \mathcal{S}(t)$ with probability at least $1 - \delta$.
Hence, let  $f_w \in \mathcal{F}$ with weight vector {$w = (A, B, C, S_{\Delta}, W)$}. We choose the smallest $(t)$ such that, $f_w \in \mathcal{S}(t)$. We have:
\begin{small}
\begin{equation*}
\begin{aligned}
&\err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}(S(t)) + 3\sqrt{\frac{\log(2/\delta(t))}{2m}}
\\ &= \frac{2\sqrt{2}}{\gamma m} \frac{t}{D^2N^2} D^{2} (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)})\cdot \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} + 3\sqrt{\frac{\log(2/\delta)+2\log(t+1)}{2m}}
\\ &\leq \frac{2\sqrt{2}}{\gamma m} ({\Gamma(w) } +{\frac{1}{D^2N^2}}) D^{2} (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \cdot  \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}  3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\Gamma(w) }+2)}{2m}}
\end{aligned}
\end{equation*}
\end{small}
\end{proof}

