Our theoretical analysis in Sec.~\ref{sec:expressivity} demonstrates the superior expressive power of S6 compared to linear attention, particularly in terms of multivariate polynomial degree and long-range processing capabilities. Furthermore, Thm.~\ref{theorem:AnyPolywithMambas} and Lemma~\ref{lemma:dir2} establishes that the hypothesis class associated with Mamba models with just four layers is significantly larger than that of transformers, even with greater depth. While increased expressivity can often come at the cost of generalization, in this section, we show that S6 layers do not suffer from this trade-off. To do so, we % 
%Our second theoretical contribution 
provide a generalization bound for simplified S6 layers, as {\color{black}defined in Eq.~\ref{eq:modelAppendix}, which is a generalization of Eq.~\ref{eq:model}}. Our analysis is based on a classifier \( f : \mathbb{R}^{D \times L} \rightarrow \mathbb{R}^{\mathcal{C}} \), parameterized by  \( (A^{}, S_B^{}, S_C^{}, S_{\Delta}^{}) \).
% Let \( H \) be the number of layers in the network, and for each layer \( h \), where \( h \in [H] \), 
% We have parameters \( (A^{}, S_B^{}, S_C^{}, S_{\Delta}^{}) \) associated with the model. 
% Between each layer, there is an activation function \( \sigma^{(h)} \) with a Lipschitz constant of 1, which can be any activation function of choice.
The classifier \( f \) for a specific class \( c \in [\mathcal{C}] \) denoted by $f^c(X_{*1},...,X_{*L})$ is computed as:
% Our analysis is based on a classifier $f : \mathbb{R}^{D \times L} \rightarrow \mathbb{R}^{\mathcal{C}}$ defined as follows, $\forall c \in [\mathcal{C}]$,
% \begin{equation}
%    f^c(X^{}_{*1},...,X^{}_{*L}) =  \sum_{d=1}^D W_{c,d} (S_C X^{}_{*L})^T\sum_{i=1}^L \left(\prod_{k=i+1}^L (\exp(\Delta^{(j)}_{d,k} \cdot A_{d*}) \right)S_B X^{}_{*i} X^{}_{di}
% \end{equation}
% \begin{small}
% \begin{equation*}
% \begin{aligned}
%
\vspace{-3pt}
{\small
\begin{equation}
%\begin{small}
% &f^c(X_{*1},...,X_{*L}) =  
%\\&
\sum_{d=1}^D W_{c,d} \left(S_C^{} X^{}_{*L}\right)^T \sum_{i=1}^L \left(\prod_{k=i+1}^L \bar{A}^{}_{dk} \right) S_B^{} X^{}_{*i} X^{}_{di}
%\end{small}
\end{equation}
}
% \end{aligned}
% \end{equation*}
% \end{small}
%
where $\mathcal{C}$ is the number of classes, $ \bar{A}^{}_{dk} = \exp(\Delta_k A_d)$ ($A_d$ is the $d$th row of $A$) as defined in Eq.~\ref{eq:TimeVariantMatrices1} and $W \in \mathbb{R}^{\mathcal{C} \times D}$ represents a linear projection from the output to the number of classes.
% {\color{black} 
% In this notation, $X^{}_{ij}$ represents the input for the $i$-th channel of the $j$-th token in the sequence.
% }
% where $\mathcal{C}$ is the number of classes and $W \in \mathbb{R}^{\mathcal{C} \times D}$ represents a linear projection from the output to the number of classes. 
We denote the parameters of a classifier by 
$w  = \left(A, S_B, S_C, S_{\Delta}, W\right)$
and the corresponding function induced by a specific instance of $w$ by $f_w$.
The class of functions taking on different parameter instances $w$, is denoted by $\mathcal{F}$.
As customary in the derivation of Rademacher complexity based bounds (e.g. \citep{golowich2018size}), our analysis takes into account the (different) norms of the parameters, for which we 
denote:
\vspace{-3pt}
{\small
\begin{equation}\nonumber
\begin{aligned}
    \rho_W(w)&=||W||_{F},\; \rho_A^{}(w) = \|A^{}\|_{\max},\; \rho_B^{}(w) = \|S_B^{}\|_{2,\infty}, \\ \rho_C^{}(w) &= \|S_C^{(h)}\|_{F},\;
    \rho_{\Delta}^{}(w) = \|S_{\Delta}^{}\|_{2}, \\ \Gamma(w) &=  \rho_W(w) \cdot \rho_A^{}(w) \cdot \rho_B^{}(w) \cdot \rho_C^{}(w) \cdot \rho_{\Delta}^{}(w)
\end{aligned}
\end{equation}
}
Equipped with these notations, we are now ready to state our main generalization bound.
% {\color{black}
% \begin{theorem}\label{theorem:genbound}
% Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$ and $\delta > 0$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$. Assume that $|X^{}_{(j)_{kl}}| \leq 1$ for all $j \in [m], k \in [D], l \in [L]$. Additionally, suppose $\forall n, n' \in [N], k\in[L]:  (\bar{A}^{}_k)_{n,n'} < K < 1$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \small
% &\err_P(f_w) - \fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X^{}_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})] \\& = \err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma m} ({\color{black}\Gamma(w) } +{\color{black}\frac{1}{D^2N^2}}) D^{1.5} \cdot \\ & (1 + \sqrt{2\log (4 \mathcal{C} D^3 N)})  \frac{1}{1-K}   \sqrt{ \max_i \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2} \\&+ 3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\color{black}\Gamma(w) }+2)}{2m}}
% \end{aligned}
% \end{equation*}
% \end{small}
% % where the maximum is taken over \(k \in [L]\), \(t \in [D]\). 
% \end{theorem}
%  }
 \begin{theorem}\label{theorem:genbound}
Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$ and $\delta > 0$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$, where each \(X_{(j)} = (X_{(j)_{*1}}, \ldots, X_{(j)_{*L}}) \in \mathbb{R}^{D \times L}\). Assume that $\forall j \in [m]: ||X_{(j)}||_{\max} \leq 1$. Additionally, suppose $\forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, 
{\small
\begin{equation*}
\begin{aligned}
&\err_P(f_w) - \fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})] \\& = \err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma m} ({\Gamma(w) } +{\frac{1}{D^2N^2}}) D^{2} \cdot \\& (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} \\&+ 3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\Gamma(w) }+2)}{2m}},
\end{aligned}
\end{equation*}
}
where the maximum is taken over \(t \in [D]\),  \(k \in [L]\). 
\end{theorem} 
See Appendix~\ref{theorem:genproof} for the complete details and proof. A uniform convergence bound for a class $\mathcal{F}$ is an upper bound on the generalization gap that uniformly holds for all $f \in \mathcal{F}$. The more direct Rademacher complexity bound presented in  Lemma~\ref{lem:loss_ramp} 
is an example of a uniform convergence bound. However, these bounds become ineffective when a function $f \in \mathcal{F}$ exists that can perfectly fit any labeling of the dataset. In such situations, uniform convergence bounds fail to provide meaningful insights and are considered vacuous.

In contrast, the bound derived in Thm.~\ref{theorem:genbound} is also based on Rademacher complexity, but it is not a uniform convergence bound and hence it is not inherently vacuous. In the proof, we partition the class $\mathcal{F}$ into subsets $\mathcal{F}_{\rho}$, where the partitioning criterion $\rho$ is based on the norm of the S6 matrices, and apply our Rademacher bound (see Appendix \ref{theorem:rad}) %Lemma~\ref{lem:loss_ramp} 
within each subset. We then apply a union bound to combine these results, obtaining a bound %that is
individualized for each $f_w \in \mathcal{F}$. 

% {\color{black} CONCLUSION??}
%As discussed previously in Theorem~\ref{prop:rademacher}, t
% The Rademacher complexity bound on \(\mathcal{F}_{\rho}\) scales with \(\mathcal{O}(D^{2} \rho_W \rho_B \rho_C \rho_A \rho_{\Delta} \cdot \frac{1}{K} \sqrt{\frac{\beta}{m}})\) {\color{black}SINCE ... AND IF TOO COMPLEX GIVE INTUITION AND REFER TO THE APPENDIX. BUT BETTER BE SELF CONTAINED}. The second term in the bound %(see Theorem~\ref{thm:genbound}) 
% {\color{black} YOU CAN WRITE THE TERMS HERE OR DESCRIBE THEM MORE GLOBALLY WHAT EVER IS BEST FOR THE PAPER}
% is typically smaller than the first term, as it scales with \(\sqrt{\log(DN\rho_W \rho_B \rho_C \rho_A \rho_{\Delta})}\). Therefore, we conclude that our generalization bound scales with \(\mathcal{O}(D^{2} \rho_W \rho_B \rho_C \rho_A \rho_{\Delta} \cdot \frac{K}{(K-1)^2} \sqrt{\frac{\beta}{m}})\). This implies that the bound is largely unaffected by the sequence length $L$, making it applicable to various sequence lengths without being significantly impacted by the length. 

To understand our bound, we analyze its terms. %Note that 
First note that from the standard assumption that the data is normalized, we have,
%
%
\vspace{-8pt}
{\small
\begin{equation}
%\begin{aligned}
% \begin{equation}  
\sqrt{ \max_{t,k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \leq \sqrt{m}   
% \end{equation}
%\end{aligned}
\vspace{-2pt}
\end{equation}
}
%
% 
% Let's assume the tokens are drawn from a normal distribution centered around 0, which is reasonable due to Gaussian modeling in high-dimensional settings. Given this assumption, the sum of the tokens has a mean of 0, so the expected value of the above sum is also 0, indicating that, on average, the it tends to be close to 0.
% follows from our assumption about the data, which is standard, as we typically normalize the data. })
Regarding the term \(\sqrt{\log(\mathcal{C}DNL)}\), even when choosing exceptionally large values for \(D\) or \(L\), such as \(2^{100}\), the impact on the bound remains minimal. The second term in the bound (see Thm~\ref{theorem:genbound}) is typically smaller than the first term, as it scales with \(\sqrt{\log(DN\Gamma(w))/m}\). Therefore, we conclude that our generalization bound scales with \(\mathcal{O}\left(\frac{1}{\sqrt{m}}D^{2} \Gamma(w) \cdot  \frac{K}{(K-1)^2}\right)\). This implies that the bound is largely unaffected by the sequence length \(L\), making it applicable to various sequence lengths without being significantly impacted by them. Note that since \( K < 1 \), when \( K \) is small, the term \(\frac{1}{1-K}\) approaches 1, further reducing its impact on the bound. This implies that for very small \( K \), the generalization bound becomes even tighter.
%
% Regarding the term {\color{black} \(\sqrt{\log (\mathcal{C} 2^{3(H-1)} D^4 N L^{H})}\)}, even when choosing exceptionally large values for \(L\) or \(D\), such as \(2^{100}\), the impact on the bound remains minimal. {\color{black}The bound does scale with $\sqrt{H}$ so as $H$ increases, the impact grows, but at a slow rate.} The second term in the bound (see Theorem~\ref{thm:genbound}) is typically smaller than the first term, as it scales with \(\sqrt{\log(DN\Gamma(w)\rho_W(w))}\). Therefore, we conclude that our generalization bound scales with \(\mathcal{O}\left(\frac{\sqrt{H}R^{-H} D^{H}}{\sqrt{m}} \Gamma(w) \rho_W(w) \cdot \prod_{h=1}^H \frac{K_h}{(K_h-1)^2}\right)\).
% This implies that choosing $R = D$  as the normalizing constant leads to potential improved generalization.
% This implies that the bound is largely unaffected by the sequence length \(L\), making it applicable to various sequence lengths without being significantly impacted by them. Note that since \( K < 1 \), when \( K \) is very small, the term \(\frac{K}{(K-1)^2}\) approaches 0, further reducing its impact on the bound. This implies that for very small \( K \), the generalization bound becomes even tighter.
