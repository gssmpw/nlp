
\section{Proofs}\label{app:proofs}
This section details our proofs.
\subsection{Expressivity}
\setcounter{section}{2}
% \renewcommand{\thesection}{\arabic{section}}
% \renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
\setcounter{theorem}{0}
\setcounter{lemma}{0}
% \subsection{Generalization}
% Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$. Our generalization bound is based on a uniform-convergence generalization bound provided in ~\citep{galanti2024norm}. The following lemma bounds the gap between the test error and the empirical margin error, represented as $\err^{\gamma}_S(f_w)= 
% \fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X^{}_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})]$. 

% \begin{lemma}\label{lem:loss_ramp}
% Let $P$ be a distribution over $\mathbb{R}^{\mathcal{D}} \times [C]$ and $\mathcal{F} \subset \{f':\mathcal{X} \to \R^C \}$. Let $S = \{(X_j,y_j)\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$ and $X=\{X_j\}^{m}_{j=1}$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, we have
% \begin{equation}\label{eq:Radbound}
% \err_P(f_w) - \err^{\gamma}_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}_{X}(\mathcal{F}) + 3\sqrt{\frac{\log(2/\delta)}{2m}}.
% \end{equation} 
% \end{lemma}
% \begin{lemma}\label{lem:peeling}
% Let $\sigma_j$ be a $l_j$-Lipschitz, positive-homogeneous function and $l=\max_j l_j$. Let $\xi_i \sim U[\{\pm 1\}]$. Then for any class of vector-valued functions $\mathcal{F} \subset \{f \mid ~f:\mathbb{R}^d \to \mathbb{R}\}$ and any convex and monotonically
% increasing function $g : \mathbb{R} \to [0,\infty)$, 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)
% ~\leq~ 2\E_{\xi} \sup_{f\in \mathcal{F}} g\left(l\left|\sum^{m}_{i=j} \xi_j  \cdot f(x_j)\right| \right).
% \end{aligned}
% \end{equation*}
% \end{small}
% \end{lemma}
% % \begin{restatable}[Contraction Lemma]{lemma}{peeling}\label{lem:peeling}
% % Let $\sigma_j$ be a $l_j$-Lipschitz, positive-homogeneous function and $l=\max_j l_j$. Let $\xi_i \sim U[\{\pm 1\}]$. Then for any class of vector-valued functions $\mathcal{F} \subset \{f \mid ~f:\mathbb{R}^d \to \mathbb{R}\}$ and any convex and monotonically
% % increasing function $g : \mathbb{R} \to [0,\infty)$, 
% % \begin{small}
% % \begin{equation*}
% % \begin{aligned}
% % \E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)
% % ~\leq~ 2\E_{\xi} \sup_{f\in \mathcal{F}} g\left(l\left|\sum^{m}_{i=j} \xi_j  \cdot f(x_j)\right| \right).
% % \end{aligned}
% % \end{equation*}
% % \end{small}
% % \end{restatable}
% \begin{proof}
% We notice that since $g(|z|) \leq g(z)+g(-z)$,
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \left| \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j))\right| \right)\\
% &\leq \E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
% &+\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( - \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right) \\
% &= 2\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
% \end{aligned}
% \end{equation*}
% \end{small}
% where the last equality follows from the symmetry in the distribution of the $\xi_i$ random variables. By Equation 4.20 in~\cite{Ledoux1991ProbabilityIB}  we have the following:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left( \sum^{m}_{j=1} \xi_j \cdot \sigma_j(f(x_j)) \right)\\
% &=\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \sum^{m}_{j=1} \xi_j \cdot \frac{1}{l}\sigma_j(f(x_j)) \right)\\
% &\leq\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \sum^{m}_{j=1} \xi_j \cdot f(x_j) \right)\\
% &\leq\E_{\xi} \sup_{\substack{f\in \mathcal{F}}} g\left(l \left| \sum^{m}_{j=1} \xi_j \cdot f(x_j) \right| \right)\\
% \end{aligned}
% \end{equation*}
% \end{small}
% where the last equality follows from $g$ being monotonically increasing. 
% \end{proof}
% % \rademacher*
% The model is denoted by:
% \begin{equation}\label{eq:model}
% \begin{aligned} 
%       &B_i = S_B x_i, \quad C_i = S_C x_i, \quad \Delta_i = \sigma(S_{\Delta} x_i), \quad  \\&\bar{A}_i =  \exp(\Delta_i A), \quad \bar{B}_i = B_i 
% \end{aligned}
% \end{equation}
% Where \( \sigma \) is a 1-Lipschitz activation function. 
% % For our analysis, we will use \( \text{ReLU} \).
% We consider a classifier $f : \mathbb{R}^{D \times L} \rightarrow \mathbb{R}^{\mathcal{C}}$ defined as follows. We have parameters $(A^{}, S_B^{}, S_C^{}, S_{\Delta}^{})$ associated with the layer. The norms are defined as follows:

% \begin{equation}
% \begin{aligned} 
%     \rho_A^{}(w) &= \|A^{}\|_{\max} \\
%     \rho_B^{}(w) &= \|S_B^{}\|_{2,\infty} \\
%     \rho_C^{}(w) &= \|S_C^{}\|_{F} \\
%     \rho_{\Delta}^{}(w) &= \|S_{\Delta}^{}\|_{2}
% \end{aligned}
% \end{equation}

% We denote the product of these norms as:
% \begin{equation}
% \Gamma^{}(w) = \rho_A^{}(w) \cdot \rho_B^{}(w) \cdot \rho_C^{}(w) \cdot \rho_{\Delta}^{}(w)
% \end{equation}

% Given these definitions, the classifier $f$ for a specific class $c \in [\mathcal{C}]$ is computed as:

% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &f^c(X_{*1},...,X_{*L}) =  
% \\&\sum_{d=1}^D W_{c,d} \left(S_C^{} X^{}_{*L}\right)^T \sum_{i=1}^L \left(\prod_{k=i+1}^L \bar{A}^{}_{dk} \right) S_B^{} X^{}_{*i} X^{}_{di}
% \end{aligned}
% \end{equation*}
% \end{small}

% Here, $W \in \mathbb{R}^{\mathcal{C} \times D}$ represents a linear projection from the output to the number of classes, and $\mathcal{C}$ is the number of classes. 

% We denote the parameters of the classifier by 
% \[
% w = (A^{}, S_B^{}, S_C^{}, S_{\Delta}^{}, W) 
% \] and the function induced by a specific instance of $w$ is denoted by $f_w$. The class of functions taking on different parameter instances $w$ is denoted by $\mathcal{F}$.
% Denote 
% \[
% \rho = \{\rho_W, \rho_A, \rho_B, \rho_C, \rho_{\Delta}\}
% % \quad \text{where} \quad \rho_A = \{\rho_A^{}\}_{h=1}^{H}, \ \rho_B = \{\rho_B^{}\}_{h=1}^{H}, \ \rho_C = \{\rho_C^{}\}_{h=1}^{H}, \ \rho_{\Delta} = \{\rho_{\Delta}^{}\}_{h=1}^{H}
% \]
% and let:
% \begin{equation}
% \begin{aligned}
%    \mathcal{F}_{\rho} 
%     &=\{f_w \in \mathcal{F} : \Gamma(w) \leq  \rho_A \rho_B \rho_C \rho_{\Delta} \rho_W=:\Gamma \}
% \end{aligned}
% \end{equation}

% The following theorem provides a bound on the Rademacher complexity of the class $\mathcal{F}_{\rho}$.
% \setcounter{section}{2}
% \setcounter{theorem}{0}
% \begin{theorem}\label{theorem:rad}
% Let $\rho = \{\rho_W, \rho_A, \rho_B, \rho_C, \rho_{\Delta}\}$. Suppose we have \(m\) sample sequences \(X = \{ X_{(j)} \}_{j=1}^{m}\), where each \(X_{(j)} = (X_{(j)_{*1}}, \ldots, X_{(j)_{*L}}) \in \mathbb{R}^{D \times L}\). Assume that $\forall j \in [m]: ||X_{(j)}||_{\max} \leq 1$. Additionally, suppose $\forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1$. Then,
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \mathcal{R}_X(\mathcal{F}_{\rho})
% &\leq\frac{1}{m} D^{2} \Gamma (1 + \sqrt{2\log (2L \mathcal{C} D^4 N)}) \cdot \\&  \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}
% \end{aligned}
% \end{equation*}
% \end{small}
% where the maximum is taken over \(t \in [D]\),  \(k \in [L]\). 
% \end{theorem}

% \begin{proof}
% % We approximate the exponential function using the first three terms of its Taylor series expansion, $\exp(z) \sim 1 + z + z^2/2$.
% For aesthetic purposes, we define \( B \) as \( S_B \) and \( C \) as \( S_C \).
% The Rademacher complexity of $\mathcal{F}_{\rho}$ is given by:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &m\mathcal{R(\mathcal{F}_{\rho})} = \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} f^c_w(X_{(j)}) \right] \\
% &= \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
% \left(S_C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right.\\
% &\quad \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) 
% \cdot A^{}_{d*}\right)\right) S_B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% &= \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
% \left(C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right.\\
% &\quad \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) 
% \cdot A^{}_{d*}\right)\right) B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] 
% \end{aligned}
% \end{equation*}
% \end{small}

% Here, {$A^{}_{d*}$ represents the $d$-th row of $A^{}$}, which has a size of $N$. We think of $A^{}_{d*}$ as a diagonal matrix of size $N\times N$, where its diagonal elements are the values in the $d$th row of $A$. Thus, $A^{}_{d*} = \text{diag}(a^{}_{d1},...,a^{}_{dN})$. Hence,
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% & \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
% \left(C^{} X^{}_{{(j)}_{*L}}\right)^T \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
% A^{}_{d*}\right) \right) B^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% &=~ \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
% \sum_{l=1}^N \left(C^{} X^{}_{{(j)}_{*L}}\right)_l^T \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
% a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% &=~ \E_{\xi}\left[\sup_{w} \sum_{j=1}^m \sum_{c=1}^{\mathcal{C}} \xi_{jc} \sum_{d=1}^D W_{c,d} 
% \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
% a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% &=~ \E_{\xi}\left[\sup_{w} \sum_{c=1}^{\mathcal{C}} \sum_{d=1}^D W_{c,d} \sum_{j=1}^m \xi_{jc} 
% \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot 
% a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right]
% \end{aligned}
% \end{equation*}
% \end{small}

% This follows from expressing the model in a more explicit form. Next,
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &~ \E_{\xi}\left[\sup_{w} \sum_{c=1}^{\mathcal{C}}  \sum_{d=1}^D W_{c,d} \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right] \\
% % &\leq~ \rho_W\E_{\xi}\left[\sup_{w,c}  \left( \sum_{d=1}^D \left( \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L  \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right)^2 \right) \right] \\
% % &\leq~ \rho_W\E_{\xi}\left[\sup_{w,c, d}  \sqrt{ D \left( \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} \right)^2} \right] \\
% &=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} | \right]
% \end{aligned}
% \end{equation*}
% \end{small}
% where the inequality follows from moving the norm of $W$ to $W_{c}$ for maximizing the inner term and applying the Cauchy-Schwartz inequality.
% Next,
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(C_{l*}^{} X^{}_{{(j)}_{*L}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) B_{l*}^{} X^{}_{{(j)}_{*i}} X^{}_{{(j)}_{di}} | \right] \\
% &=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{j=1}^m \xi_{jc} \sum_{l=1}^N \left(\sum_{s=1}^D C_{ls}^{} X^{}_{{(j)}_{sL}}\right) \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} | \right] \\
% &=~ \sqrt{D} \rho_W \E_{\xi}\left[\sup_{w,c, d}   | \sum_{l=1}^N \sum_{s=1}^D C_{ls}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} | \right] \\
% % &\leq~ \sqrt{D} \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l}    \sqrt{ \sum_{s=1}^D \left( \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} \right)^2} \right] \\
% &\leq~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) \left(\sum_{s'=1}^DB_{ls'}^{} X^{}_{{(j)}_{s'i}} \right) X^{}_{{(j)}_{di}} |  \right] \\
% &=~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{s'=1}^D B_{ls'}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right]
% \end{aligned}
% \end{equation*}
% \end{small}
% where the first inequality follows from moving the norm of $C$ to $C_{l}$ for maximizing the inner term and applying the Cauchy-Schwartz inequality. 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &=~ D \rho^{}_W \rho^{}_C \E_{\xi}\left[\sup_{w,c, d, l, s} | \sum_{s'=1}^D B_{ls'}^{} \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \\ 
% &\quad \left.\left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% % &\leq~ D \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s}  \sqrt{ \sum_{s'=1}^D \left( \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} \right)^2 }  \right] \\
% &\leq~ D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L \right. \\
% & \quad \left. \left(\prod_{k=i+1}^L \exp\left(\sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) \right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% &=~ D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \sum_{i=1}^L  \right. \\
% & \quad \left.\exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% &\leq~ D^{1.5} \rho^{}_W \rho^{}_C  \rho^{}_B \sum_{i=1}^L \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot  \right. \\
% & \quad \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right]
% \end{aligned}
% \end{equation*}
% \end{small} where the second to last inequality follows from $\|x\|_2 \leq \sqrt{n} \|x\|_{\infty}$ for any $x \in \mathbb{R}^{n}$. 
% Jensen's inequality gives the following inequality:
% \begin{small}
% \begin{equation}\label{eq:eq1}
% \begin{aligned}
% &~ \E_{\xi}\left[\sup_{w,c, d, l, s, s'} | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \\
% & \quad \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |  \right] \\
% % &\leq~ \E_{\xi}\left[\sup_{w,c, d, l, s, s'} \left| \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}}  \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} \right|  \right] \\
% &\leq~  \frac{1}{\lambda_i} \log ( \E_{\xi}\left[\sup_{w,c, d, l, s, s'}  \exp (\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \\
% & \quad \left.  \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) \\
% &\leq~  \frac{1}{\lambda_i} \log ( \sum_{c, d, l, s, s'} \E_{\xi}\left[\sup_{w} \exp(\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot  \right. \\
% & \quad \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) \\
% &\leq~  \frac{1}{\lambda_i} \log ( \mathcal{C} D^3 N  \max_{c, d, l, s, s'} \E_{\xi}\left[\sup_{w} \exp(\lambda_i  | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \\
% & \quad \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] ) := {\color{red} \Theta}\\
% \end{aligned}
% \end{equation}
% \end{small}
% For fixed $\lambda_i > 0$. The second inequality follows from the fact that $\sup_x \sup_y f(x,y) = \sup_{x,y} f(x,y)$.
% We observe that the inner expectation $\sup$ depends only on $w$.
% Next we use Lemma~\ref{lem:peeling} with $\sigma_{ij}(z) = \exp(z)X^{}_{{(j)}_{sL}} X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}}$ on its domain and $g_i(X_{(j)})=\sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}$. The corresponding Lipschitz constants are $l_{ij} = \max_{z \in dom(\sigma_{ij})}(\exp(z)X_{(j)_{sL}}X_{(j)_{s'i}} X_{(j)_{di}})$ and $l_i = \max(l_{ij})$. Therefore:
% \begin{small}
% \begin{equation}
% \begin{aligned}
% &\E_{\xi}\left[\sup_{w} \exp(\lambda_i | \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{sL}} \cdot \right. \\
% & \quad \left. \exp\left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}\right) X^{}_{{(j)}_{s'i}} X^{}_{{(j)}_{di}} |)  \right] \\
% % &\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i  \left| \sum_{j=1}^m \xi_{jc}  X^{(j)}_{sL}  (\exp(\sum_{k=i+1}^L \Delta^{(j)}_{d,k}a_{dl})) X^{(j)}_{s'i} X^{(j)}_{di} \right| \right)\right] \\ 
% &=\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i  \left| \sum_{j=1}^m \xi_{jc} (\sigma_{ij}(g_i(X^{(j)}))) \right| \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i l_i \left| \sum_{j=1}^m \xi_{jc}    \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl} \right| \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i \rho^{}_A  l_i  \left| \sum_{j=1}^m \xi_{jc}    \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \right| \right)\right] \\
% &\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc}   \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \right| \right)\right] \\
% &\leq 4\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc}   S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
% \end{aligned}
% \end{equation}
% \end{small}
% This follows from applying Lemma~\ref{lem:peeling} with $\sigma$ which has a Lipschitz constant of 1, as assumed. Hence:
% \begin{small}
% \begin{equation} \label{eq:lip}
% \begin{aligned}
% % &4\E_{\xi}\left[ \sup_{w}  \exp \left(\lambda_i \rho^{}_A   l_i \left| \sum_{j=1}^m \xi_{jc} \sum_{\substack{k=i+1 \\ S_{\Delta} X_{(j)_{*k}} > 0}}^{L} S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
% & 4\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda_i \rho^{}_A  (L-i) l_i \left| \sum_{j=1}^m \xi_{jc} S^{}_{\Delta} X^{}_{{(j)}_{*k}} \right| \right)\right] \\
% &\leq 4\E_{\xi}\left[ \sup_{k}  \exp \left( \lambda_i \rho^{}_A \rho^{}_{\Delta} (L-i) l_i || \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{*k}} || \right)\right] \\
% &\leq 4\E_{\xi}\left[ \sup_{t,k}  \exp \left( \lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i \left| \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}} \right| \right)\right] \\
% &\leq 4DL \max_{t, k} \E_{\xi}\left[ \exp \left(\lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i  \left|  \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}}  \right| \right)\right] \\
% % &\leq 4\E_{\xi}\left[ \sup_{w, k}  \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} \sum_{i=1}^L (L-i)l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
% % &\leq 2\E_{\xi}\left[ \sup_{w}  \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
% % &\leq 2\E_{\xi}\left[ \exp \left( \lambda   \rho^{}_A \rho^{}_{\Delta} l_i  \left|\left| \sum_{j=1}^m \xi_{jc}  \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|_2 \right)\right] \\
% \end{aligned}
% \end{equation}
% \end{small}

% Denote:
% \begin{small}
% \begin{equation} \label{eq:lip2}
% \begin{aligned}
% M_i := \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i 
% \end{aligned}
% \end{equation}
% \end{small}

% % We notice that:

% % \begin{small}
% % \begin{equation} \label{eq:lip}
% % \begin{aligned}
% % &\leq 2\E_{\xi}\left[ \sup_{ k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A  \left| \left| \sum_{j=1}^m \xi_{jc}  X_{*k}^{(j)} \right|\right|_2 \right)\right] \\
% % &\leq 2\E_{\xi}\left[ \sup_{ k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A  \sqrt{ \sum_{t=1}^D \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right|^2 } \right)\right] \\
% % &\leq 2\E_{\xi}\left[ \sup_{t,k} \exp \left( (L-i)\lambda_i l_i \rho_{\Delta} \rho_A \sqrt{D}  \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right| \right)\right] \\
% % &\leq 2DL \max_{t, k} \E_{\xi}\left[ \exp \left((L-i) \lambda_i l_i \rho_{\Delta} \rho_A \sqrt{D}  \left|  \sum_{j=1}^m \xi_{jc} X_{(j)_{tk}}  \right| \right)\right] \\
% % \end{aligned}
% % \end{equation}
% % \end{small}
% %  where the second to last inequality follows from $\|x\|_2 \leq \sqrt{n} \|x\|_{\infty}$ for any $x \in \mathbb{R}^{n}$. 

% As a next step, we would like to bound the above term using a function of the data that is not dependent on an expected value of noise labels $\xi$. For this purpose we apply a technique that was introduced in the proof of Theorem~1 in~\citep{golowich2018size}. We apply this process separately for each $i \in [L]$. Let $i \in [L]$:
% We define a random variable $Z$:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &Z = M_i  \left| \sum_{j=1}^m \xi_{jc} X^{}_{{(j)}_{tk}} \right| \\
% &z_j = X^{}_{{(j)}_{tk}} \rightarrow Z = M_i |\sum_{j=1}^m \xi_{jc} z_j | \\
% \end{aligned}
% \end{equation*}
% \end{small}
% The random variable $Z$ depends on the random variables $\xi_{jc}$.
% Then, we have: 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda _iZ) \right] \\
% &=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z + \lambda_i\E(Z) - \lambda_i \E(Z)) \right] \\
% &=\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z - \lambda_i \E(Z)) \right] +  \E_{\xi} (Z) \\
% \end{aligned}
% \end{equation*}
% \end{small}
% By Jensen’s inequality, we obtain a bound for $\E(|\sum_{j=1}^m \xi_{jc} z_j |)$:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j |) = \E_{\xi} (\sqrt{|\sum_{j=1}^m \xi_{jc} z_j |^2}) \leq \sqrt{\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j |^2)} = \\& \sqrt{\E_{\xi} (|\sum_{j=1}^m \xi_{jc} z_j} |^2) = \sqrt{\E_{\xi} (|\sum_{j,j'=1}^m \xi_{jc} \xi_{j'c} z_j z_{j'}} |) = \sqrt{\sum_{j=1}^m |z_j|^2} 
% \end{aligned}
% \end{equation*}
% \end{small}
% Namely $\E_{\xi}(Z) \leq M_i \sqrt{\sum_{j=1}^m |z_j|^2} $.
% $Z$ is a deterministic function of the i.i.d. random variables  $\xi_{jc}$ and satisfies the following:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% Z(\xi_{1c},..., \xi_{jc},...,\xi_{mc}) - Z(\xi_{1c},..., -\xi_{jc},...,\xi_{mc}) \leq 2|z_j|
% \end{aligned}
% \end{equation*}
% \end{small}
% This follows from the triangle inequality.
% This means that $Z$ satisfies a bounded-difference condition, which, by the proof of Theorem~6.2 in ~\citep{Boucheron2010}, implies that $Z$ is sub-Gaussian, with variance factor:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% v = \frac{1}{4} \sum_{j=1}^m (2M_i|z_j|)^2 = M_i^2 \sum_{j=1}^m |z_j|^2
% \end{aligned}
% \end{equation*}
% \end{small}
% It follows that:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\frac{1}{\lambda_i} \log \E_{\xi}\left[\exp( \lambda_i Z - \lambda_i \E_{\xi}(Z)) \right] \leq \\& \frac{1}{\lambda_i} \frac{\lambda_i^2 M_i^2 \sum_{j=1}^m |z_j|^2}{2} =  \frac{\lambda_i M_i^2 \sum_{j=1}^m |z_j|^2}{2}  
% \end{aligned}
% \end{equation*}
% \end{small}
% Therefore:
% \begin{small}
% \begin{equation}\label{eq:eq2}
% \begin{aligned}
% &\frac{1}{\lambda_i} \log \E_{\xi}\left[ \exp( \lambda_i Z - \lambda_i \E_{\xi}(Z)) \right] +  \E_{\xi} (Z) \\
% &\leq \frac{\lambda_i M_i^2 \sum_{j=1}^m |z_j|^2}{2} + M_i\sqrt{\sum_{j=1}^m |z_j|^2} 
% \end{aligned}
% \end{equation}
% \end{small}
% \paragraph{Analyzing Lipschitz constants $l_{ij}$.}
% Next, we analyze the Lipschitz constants \( l_{ij} \).
% For $l_{ij} = \max_{z \in dom(\sigma_{ij})}(\exp(z)X_{(j)_{sL}}X_{(j)_{s'i}} X_{(j)_{di}})$ and $l_i = \max(l_{ij})$ is of the form $g_i(X_{(j)})=\sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl}$ (see \eqref{eq:lip}). Since  
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &l_i = \max_{j} l_{ij} = \max_{j} \max_{z \in dom(\sigma_{ij})}(\exp(z)X^{(j)}_{sL}X^{(j)}_{s'i} X^{(j)}_{di}) \\& \leq \max_{j} \max_{z \in dom(\sigma_{ij})} \exp \left( \sum_{k=i+1}^L \sigma(S^{}_{\Delta} X^{}_{{(j)}_{*k}}) \cdot a^{}_{dl} \right) \cdot 1 < K^{L-i}
% \end{aligned}
% \end{equation*}
% \end{small}
% which is followed from our assumptions. 
% % {\color{red}$\forall n \in [N]:(\exp(\Delta^{(j)}_{d,k} * A_{d*}))_n < K < 1$ and the fact that the data is normalized.}
% % We get:
% % \begin{small}
% % \begin{equation*}
% % \begin{aligned}
% % \sum_{i=1}^L l_i &\leq \sum_{i=1}^{L} K^{L-i} = \frac{K^L - 1}{K - 1}  \\
% % % &= \frac{(L-1)M^{L+1} - LM^L + M}{(M-1)^2 M}
% % \end{aligned}
% % \end{equation*}
% % \end{small}

% % We conclude that:
% % \begin{small}
% % \begin{equation*}
% % \begin{aligned}
% % \lim_{L \rightarrow \infty} \frac{K^L - 1}{K - 1}= \frac{1}{1-K}
% % \end{aligned}
% % \end{equation*}
% % \end{small}

% We get:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \sum_{i=1}^L l_i (L-i) &\leq \sum_{i=1}^{L} (L-i) (K)^{L-i} = \frac{(L-1)K^{L+1} - L K^L +K}{(K-1)^2}  \\
% % &= \frac{(L-1)M^{L+1} - LM^L + M}{(M-1)^2 M}
% \end{aligned}
% \end{equation*}
% \end{small}

% We conclude that:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \lim_{L \rightarrow \infty} \frac{(L-1)K^{L+1} - L K^L +K}{(K-1)^2}= \frac{K}{(K-1)^2}
% \end{aligned}
% \end{equation*}
% \end{small}

% \paragraph{Concluding the proof.} By combining ~\eqref{eq:eq1},~\eqref{eq:lip} and~\eqref{eq:eq2}, we have:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% & {\color{red} \Theta} \leq  \frac{1}{\lambda_i} \log \left(4 L \mathcal{C} D^4 N \right) + \max_{c, d, l, s, s', t, k} \frac{1}{\lambda_i} \cdot \\& \log \left(  \E_{\xi}\left[ \exp \left(\lambda_i \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i  \left|  \sum_{j=1}^m \xi_{jc} 
% X^{}_{{(j)}_{tk}}  \right| \right)\right] \right) \\
% &\leq \frac{1}{\lambda_i}  \log \left(4L \mathcal{C} D^4 N \right) + \\& \max_{c, d, l, s, s', t, k} \frac{\lambda_i (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)^2 \sum_{j=1}^m (X_{(j)_{tk}})^2}{2} \\&+ (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)\sqrt{\sum_{j=1}^m (X_{(j)_{tk}})^2} \\
% \end{aligned}
% \end{equation*}
% \end{small}
% We choose $\lambda_i = \sqrt{\frac{2\log (4 L \mathcal{C} D^4 N)}{M_i^2  \max_{t,k}\sum_{j=1}^m (X_{(j)_{tk}})^2}}$ which minimizes the above term and obtain the following inequality:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\sum_{i=1}^L \frac{1}{\lambda_i} \log \left(4L \mathcal{C} D^4 N \right) + \\& \max_{t, k} \frac{\lambda_i (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)^2 \sum_{j=1}^m (X_{(j)_{tk}})^2}{2} + \\& (\sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i)\sqrt{\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
% &\leq \sum_{i=1}^L (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)})M_i\sqrt{ \max_{t, k}\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
% &= \sum_{i=1}^L (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{D} \rho^{}_A \rho^{}_{\Delta} (L-i) l_i \cdot \\& \sqrt{ \max_{t, k}\sum_{j=1}^m (X_{(j)_{tk}})^2}\\
% &\leq (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{D} \rho^{}_A \rho^{}_{\Delta}\sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}\\
% % &\frac{1}{\lambda} \log \left(2 \mathcal{C} D^3 N \right)  +  \frac{\lambda M^2  \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}{2}  +M  \sqrt{ \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2} \\
% % &\frac{1}{\lambda} \log \left( \mathcal{C} 2^{3(H-1)} D^4 N L^{H} \right)  +  \max_{c, d, l, s, s'}  \max_{k_1,...,k_{H-1}, k, t}  \frac{\lambda M^2 \sum_{j=1}^m (X_{{(j)}_{tk}})^2}{2} + M\sqrt{\sum_{j=1}^m (X_{{(j)}_{tk}})^2} \\
% % &\leq (1 + \sqrt{2\log (2 \mathcal{C} D^3 N)})M  \sqrt{ \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}  \\
% \end{aligned}
% \end{equation*}
% \end{small}
% We conclude that:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &m\mathcal{R(\mathcal{F}_{\rho})} \\ &\leq 
%  D^{2} \Gamma (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2}\\
%  % \\&\leq
%  % D^{1.5} \rho^{}_W \rho^{}_C \rho^{}_B \rho^{}_A \rho^{}_{\Delta} (1 + \sqrt{2\log (2 \mathcal{C} D^3 N)})  \frac{1}{1-K}   \sqrt{\max_i \sum_{j=1}^m \left|\left| \sum_{k=i+1}^L  X^{}_{{(j)}_{*k}} \right|\right|^2_2}\\
% \end{aligned}
% \end{equation*}
% \end{small}
% \end{proof}
% \begin{theorem}\label{theorem:genproof}
% Let $P$ be a distribution over $\mathbb{R}^{D \times L} \times [C]$ and $\delta > 0$. Let $S = \{( X^{}_{(j)},y_{(j)})\}^{m}_{j=1}$ be a dataset of i.i.d. samples selected from $P$. Assume that $\forall j \in [m]: ||X_{(j)}||_{\max} \leq 1$. Additionally, suppose $\forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1$. Then, with probability at least $1-\delta$ over the selection of $S$, for any $f_w \in \mathcal{F}$, 
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\err_P(f_w) - \fr{1}{m}\sum^{m}_{j=1}\bI[\max_{c \neq c'}(f^c_w( X_{(j)})) + \gamma \geq f^{c'}_w( X^{}_{(j)})] \\& = \err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma m} ({\Gamma(w) } +{\frac{1}{D^2N^2}}) D^{2} \cdot \\& (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} \\&+ 3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\Gamma(w) }+2)}{2m}},
% \end{aligned}
% \end{equation*}
% \end{small}
% where the maximum is taken over \(t \in [D]\),  \(k \in [L]\). 

% \end{theorem}
% \begin{proof}
% For aesthetic purposes, we define \( B \) as \( S_B \) and \( C \) as \( S_C \). We want to prove the bound for all $f_w \in \mathcal{F}$ where:
% {
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\mathcal{F} := \\&\{f_w : w = (A, B, C, S_\Delta, W), \forall k \in [L], d \in [D]:||\bar{A}^{}_{dk}||_{\max} < K < 1 \}
% \end{aligned}
% \end{equation*}
% \end{small}}
% Let $t \in \mathbb{N}$. Denote:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\mathcal{S}(t) := \{f_w \in \mathcal{F} , \Gamma(w) < {\frac{t}{D^2N^2}} \}
% \end{aligned}
% \end{equation*}
% \end{small}
% Correspondingly subdivide $\delta$ as:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\delta(t) := \frac{\delta}{t(t+1)}
% \end{aligned}
% \end{equation*}
% \end{small}

% By Lemma ~\ref{lem:loss_ramp} and Theorem ~\ref{theorem:rad}, with probability at least $1-\delta(t)$:
% for any function $f_w \in \mathcal{S}(t) $, we have the following inequality:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% \err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}(\mathcal{S}(t)) + 3\sqrt{\frac{\log(2/\delta(t))}{2m}}.
% \end{aligned}
% \end{equation*}
% \end{small}
% Using the union bound over all possible set $\mathcal{S}(t)$, we establish that the above probabilistic bound holds uniformly for all functions $f_w \in \mathcal{S}(t)$ with probability at least $1 - \delta$.
% Hence, let  $f_w \in \mathcal{F}$ with weight vector {$w = (A, B, C, S_{\Delta}, W)$}. We choose the smallest $(t)$ such that, $f_w \in \mathcal{S}(t)$. We have:
% \begin{small}
% \begin{equation*}
% \begin{aligned}
% &\err_P(f_w) - \err^\gamma_S(f_w) \leq \frac{2\sqrt{2}}{\gamma} \cdot \mathcal{R}(S(t)) + 3\sqrt{\frac{\log(2/\delta(t))}{2m}}
% \\ &= \frac{2\sqrt{2}}{\gamma m} \frac{t}{D^2N^2} D^{2} (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)})\cdot \\ &\sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} + 3\sqrt{\frac{\log(2/\delta)+2\log(t+1)}{2m}}
% \\ &\leq \frac{2\sqrt{2}}{\gamma m} ({\Gamma(w) } +{\frac{1}{D^2N^2}}) D^{2} (1 + \sqrt{2\log (4L \mathcal{C} D^4 N)}) \cdot \\& \sqrt{\max_{t, k} \sum_{j=1}^m (X_{(j)_{tk}})^2} \frac{K}{(K-1)^2} \\&+ 3\sqrt{\frac{\log(2/\delta)+2\log({D^2N^2\Gamma(w) }+2)}{2m}}
% \end{aligned}
% \end{equation*}
% \end{small}
% \end{proof}
% \newpage

\setcounter{section}{1}
% \renewcommand{\thesection}{\arabic{section}}
% \renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
% \setcounter{theorem}{0}
% \setcounter{lemma}{0}
\begin{lemma}\label{lemma:dir2appendix}
There exists a function \( f : \mathbb{R}^L \rightarrow \mathbb{R} \) that can be realized by one channel of S6 such that a single attention head would require at least  \( O(\log(L)) \) layers to express this function.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:dir2appendix}]\quad
The proof begins by characterizing the functions that an S6 layer can implement as multivariate polynomials with a maximal degree that scales linearly with the sequence length $L$. It then demonstrates that this property does not hold for self-attention layers.

\noindent\textbf{Single S6 Layer as Multivariate Polynomials\quad} 
Let $f$ be a function implemented by a S6 layer with the parameters $A, S_{\Delta}, S_{B}$ and $S_{C}$.

\smallskip
Recall that we deal with the following polynomial variant of S6, which is defined as follows:
\begin{equation}\label{eq:simplifiedModelAPPEND23}
    C_i = ( {S_C}X_i )^T, \quad \bar{B}_i = S_B X_i, \quad \bar{A}_i = P_A ( S_\Delta X_i )
\end{equation}
\begin{equation}\label{eq:simplifiedModelAPPEND234}
     H_{i} =  \bar{A}_t H_{i-1} + \bar{B}_i X_i, \quad Y_k = C_i X_i
\end{equation}

Since we are interested in identifying the minimal polynomial degree required to characterize S6 models, we can assume that $P_A$ is linear, namely $\bar{A}_i = S_\Delta X_i + A $. By plugging the zero matrix into $A$ we get:

Now we can write Eq.~\ref{eq:simplifiedModelAPPEND23} by:

\begin{equation}\label{eq:simplifiedModelAPPEND25}
     H_{i} =  S_\Delta X_i H_{i-1} + \bar{B}_i X_i, \quad Y_i = C_i X_i
\end{equation}


Now, assuming $S_{\Delta}, S_{B}$ and $S_{C}$ are sparse matrices such that they have zeros at all elements except a single column, namely, the time-variant matrices are controlled only by the first channel of the sequence $X$. Hence, this channel can be defined by:
\begin{equation}\label{eq:simplifiedModelAPPEND6}
     h_{i} =  S_\Delta x_i h_{i-1} + \bar{B}_i x_i, \quad y_i = C_i x_i
\end{equation}

This exact recurrent rule is discussed in Eq.~\ref{eq:simplifiedModel} (Single S6 layer as multivariate polynomials), and it can be represented as a multivariate polynomial with a maximum degree of $L+2$. Hence, we can deduce that one element in the output of an S6 layer has a minimal maximum degree of $L + 2$ when processing sequences of length $L$. 


\smallskip
\noindent\textbf{Single Self-Attention Layer as Multivariate Polynomials\quad} 
Given an input matrix $X$, the self-attention mechanism without softmax operates as follows. The input is projected into query $Q$, key $K$, and value $V$ matrices using parameter matrices $W_Q$, $W_K$, and $W_V$, respectively:
\begin{align*}
Q = X W_Q, \quad K = X W_K, \quad V = X W_V.
\end{align*}
The attention scores $A$ are then computed as:
\begin{align*}
A = Q K^T = (X W_Q)(X W_K)^T.
\end{align*}
The output matrix $Y$ is calculated by:
\begin{align*}
Y = A V = (X W_Q W_K^T X^T)(X W_V).
\end{align*}
This formulation leads to the conclusion that each element in $Y$ can be expressed as a multivariate polynomial with a maximum degree of 3 in the elements of $X$, where the polynomial arises from trilinear interactions. Additionally, it is important to note that in the case of causal attention, the mechanism is more constrained, and the maximum degree does not exceed 3.

Now, as it is clear that each single attention layer can be represented by a multivariate polynomial with a maximum degree of 3, we can generalize our characterization for $N$-stacked self-attention layers. Recall that the composition of multivariate polynomials is also a multivariate polynomial. Moreover, the maximum degree of the resulting polynomial is the product of the maximum degrees of the composed polynomials. Hence, we can argue that each element in the output of $N$-stacked self-attention layers can be represented by multivariate polynomials with a maximum degree $p \leq 3^N$. Therefore, it is clear that to represent a polynomial with a maximum degree of $L+2$ by $N$-stacked self-attention layers, at least $N \in O(\log L)$ layers are required.
\end{proof}


\begin{lemma}\label{lemma:dir1Appendix}
Any function that can be expressed by a single attention head can also be expressed by a single channel of S6.
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:dir1Appendix}]\quad 
Let $f$ be a function implemented via a single attention head, which has the parameters $W_K$, $W_V$, $W_Q$.


Recall that we deal with the following polynomial variant of S6, which is defined as follows:
\begin{equation}\label{eq:simplifiedModelAPPEND}
\small
    C_i = ( {S_C}X_i )^T, \quad \bar{B}_i = S_B X_i, \quad \bar{A}_i = P_A ( S_\Delta X_i )%, \quad
\end{equation}
\begin{equation}
\small
% \end{equation}
% \begin{equation}\label{eq:timeVaraintRecRule}
     H_{i} =  \bar{A}_t H_{i-1} + \bar{B}_i X_i, \quad Y_k = C_i X_i
\end{equation}

 % where $X$ is the input sequence $X = (X_1, \cdots , X_L) \in \mathbb{R}^{L \times D}$. 
For simplicity, assume that we are dealing with a 1-degree polynomial $P_A$ such that $  \bar{A}_i = S_\Delta X_i + A $. 

By substituting the zero matrix for $S_\Delta$ and $A=1$, and plugging $S_C = W_Q$ and $S_B = W_K$, we get:
\begin{equation}
\small
    C_i = (W_Q X_i)^T, \quad \bar{B}_i = W_K X_i, \quad \bar{A}_i = \bar{A}_{i-1}
\end{equation}

By simply unrolling this equation:

\begin{equation} \label{eq:unrolling3a}
\small
 H_{i} = \sum_{j=1}^i \big{(} \Pi_{k=j+1}^i \bar{A}_k \big{)} W_K X_{j} X_j = \sum_{j=1}^i W_K X_{j} X_j, \quad %
 \end{equation}
\begin{equation} \label{eq:unrolling4a}
\small
 Y_{i} =  {X_i}^T {W_Q}^T \sum_{j=1}^i W_K X_{i} X_j =  \sum_{j=1}^i {X_i}^T {W_Q}^T W_K X_{j} X_j
\end{equation}



By converting Eq.~\ref{eq:unrolling4a} into a matrix form:

\begin{equation}\label{eq:MAMbaASmatmul1}
\small
Y = \tilde{\alpha} X%, \quad
% \begin{bmatrix}
% % Y_1 \\
% % Y_2\\ 
% % \vdots \\
% % yY_L \\
% \end{bmatrix} 
\end{equation} 
where $\tilde{\alpha}$ is defined by:
\begin{equation}\label{eq:MAMbaASmatmul}\nonumber
\tiny
% \tilde{\alpha} =
\hspace{-11pt}
\begin{bmatrix}
    {X_1}^T {W_Q}^T  W_K X_1 & 0 & \hspace{-2pt}\cdots & 0 \\
    {X_2}^T {W_Q}^T  W_K X_1 & {X_2}^T {W_Q}^T  W_K X_2 & \hspace{-2pt}\cdots & 0 \\
    % C_3 \bar{A}_2 \bar{A}_1 W_K X_1 & C_3 \bar{A}_2 W_K X_2s & \ddots & \vdots \\
    \vdots & \vdots &\hspace{-2pt} \ddots & 0 \\
    {X_L}^T {W_Q}^T   W_K X_{1} \quad & {X_L}^T {W_Q}^T  W_K X_{2} \quad & \hspace{-2pt}\cdots \quad & {X_L}^T {W_Q}^T  W_K X_L
\end{bmatrix}
% \begin{bmatrix}
% X_1 \\
% X_2\\ 
% \vdots \\
% X_L \\
% \end{bmatrix}
\end{equation}

Which is the exact formulation of causal self-attention (without softmax) using attention matrices denoted by $\tilde{\alpha}$. Furthermore, to incorporate the value matrix $W_V$ an additional linear layer should be applied after step S6. These layers are indeed present in the Mamba block.
%
\end{proof}
\setcounter{theorem}{1}
%\setcounter{lemma}{0}
\begin{theorem}\label{theorem:AnyPolywithMambasAppendix}
Given an input scalar sequence $x \in \mathbb{R}^L $, a model with four stacked Mamba layers, a sufficiently large number of channels, learnable PE, and a linear encoder at the first layer can express any multivariate polynomial of $x$.
\end{theorem}
% \begin{proof}[Proof of Theorem~\ref{theorem:AnyPolywithMambasAppendix}] 
% \end{proof}

\begin{proof}[Proof of Theorem~\ref{theorem:AnyPolywithMambasAppendix}]\quad
We start with the following definition of our model. We begin by describing the hidden Mamba layers, followed by the input and output layers:

\smallskip
\noindent\textbf{Model Definition\quad} We define the model with d channels, ignoring its activations. We denoted the \( i \)-th Mamba block by (see Eq.~\ref{eq:mamba1}):

\begin{equation}
    \small
    U^{i+1} = {\text{Linear}_1}^i {(\text{S6}}^i({\text{Conv1D}}^i({\text{Linear}_2}^i(U^i)) \otimes {\text{Linear}_3}^i(U^i) )
\end{equation}

where sequences and sub-layers associated with the \( i \)-th layer are denoted by the super-index \( i \). Thus, the entire computation of single layer can be described by

\begin{equation}
    \small
    U^{i+1} = \text{Mamba}^i (U_i)
\end{equation}

The output linear layer projects the last token (which include d channels) into a single output, parameterized by a matrix \( W_{\text{out}} \in \mathbb{R}^{d \times 1} \). The input layer includes the learnable positional encoding, represented by a matrix \( PE \in \mathbb{R}^{L \times d} \), and an encoding layer parameterized by \( \text{Encoding}(x) = W_{\text{in}}x + b + PE \), where \( W_{\text{in}}, b \in \mathbb{R}^{1 \times d} \).

\smallskip
\noindent\textbf{Proof by Construction\quad}
Let \( P(x) \) be a multivariate polynomial with coefficients \( c_1, \cdots, c_T \) and variables \( x = (x_1, x_2, \ldots, x_n) \). Specifically, \( P(x) \) can be expressed as:

\begin{equation}
P(x) = \sum_{i=1}^{T} c_i \cdot P_i(x), \quad \forall i: P_i (x) = \Pi_{j=1}^L \alpha_{i,j} {x_j}^{p_{i,j}}
\end{equation}

We assign the values of \( c_1, \cdots, c_T \) to \( W_{\text{out}} \) such that:
\begin{equation}
W_{\text{out}}[i,1] = 
\begin{cases} 
       c_i & \text{if } i \leq T, \\
       0 & \text{otherwise}.
\end{cases}
\end{equation}

It remains to show that for any \( i \), \( P_i(x) \) can be expressed by the last (4th) Mamba block. From Lemma~\ref{lemma:3layerMambaExpresivityAppendix}, it is evident that for any \( j \), the univariate polynomial \( s_j = \alpha_{i,j} x_j^{p_{i,j}} \) can be represented by the 3rd Mamba block. Thus, in the first linear layer of the 3rd block (\( \text{Linear}_1^3 \)), these univariate polynomials can be merged, as follows:


Define the following two sequences with Lemma 3 and $ \text{Linear}_1^3$:
\[
S' = (0, \cdots, 0, 1, s_1, s_2, \cdots, s_L, 1),
\]
\[
S'' = (0, \cdots, 0, 0, s_1, s_2, \cdots, s_L, 1),
\]
which are injected into the last SSM (\( \text{S6}^4 \)).

When applying the same SSM with system matrices equal to 1 (\( A_i = B_i = C_i = 1 \)), and subtracting them, placing the result in the i-th channel (which can be easily implemented by the last linear layer in the final Mamba block), yield:

\begin{align}
\forall i: U^{4}_i = \prod_{j=1}^L s_j = P_i(x), \quad \rightarrow W_{out}U^4 = P(x)
\end{align} 
as required.
\end{proof}


\begin{lemma}\label{lemma:3layerMambaExpresivityAppendix}
Given an input scalar sequence $x \in \mathbb{R}^L$, for any $j$, a model $M$ with 3-stacked Mamba layers, a sufficiently large number of channels, learnable PE, and a linear encoder in the first layer can express any monomial of a univariate polynomial in $x_j$. Specifically, for any constants $c \in \mathbb{R}$ and $P \in \mathbb{R}$, there exists a configuration of $M$ such that the output of the $k$-th channel $M(x)_k = c \cdot x_j^P$ for $k > P + j$.
\end{lemma}



\begin{proof}[Proof of Lemma~\ref{lemma:3layerMambaExpresivityAppendix}]\quad
\smallskip
Given an input sequence \( x = (x_1, x_2, \dots, x_L) \) with \( d \) channels, we need to show that a model \( M \) composed of three stacked Mamba layers can express any univariate polynomial \( x_j^P \) for a particular \( j \), where \( P \) are constant.

For simplicity, we assume that $P <<L$. However, this assumption can be addressed by extending the sequence length using the ZeroPad component.

\noindent \textbf{Step 1: Position Selection (First Mamba Block)} 

The first block's role is to isolate the desired position \( j \) in the input sequence $(x_j)$ at channel $s$. We achieve this as follows:

\begin{itemize}
    \item \textbf{Positional Encoding and Gating:} We configure the learnable positional encoding at channel $s$ (\( PE_{*,s} \)) such that the \( j \)-th position is highlighted, and the others positions are masked. Specifically, set the PE vector for the \( j \)-th position to act as an indicator function:
    \[
    PE[i,s] = \begin{cases} 
       1 & \text{if } i = j, \\
       0 & \text{otherwise}.
    \end{cases}
    \]\

    Please note that $s> \frac{d}{2}$, and the first half of the channels in the PE are set to zeros to ensure a clear separation between the input values $X$ and the positional encoding. Additionally, we set $b=0$, and configure the first half of the entries in $W_{in}$ to 1, while setting the remaining entries to zero. This configuration effectively duplicates the input sequence, allowing for several manipulations without affecting the original signal.
    
    \item \textbf{Linear Layers Configuration:} The linear layers \( \text{Linear}_2^1 \) and \( \text{Linear}_3^1 \) are configured to be identity mappings, i.e., they do not alter the input sequence.
    
    \item \textbf{S6 Module Configuration:} Set the S6 parameters \( \bar{A} = 0 \) and \( \bar{B} = \bar{C} = 1 \), effectively making it an identity operation. This setup ensures that the output of this block isolates \( x_j \) while setting all other positions to zero:
    \[
    U^2 = (0, \dots, 0, x_j, 0, \dots, 0)
    \]
\end{itemize}

\noindent \textbf{Step 2: Duplication of \( x_j \) (Second Mamba Block)}

The second block is responsible for duplicating the selected element \( x_j \) to match the desired power \( P \). Here's how:

\begin{itemize}
    \item \textbf{Identity Mapping:} We set \( \text{Linear}_2^2 \) and \( \text{Linear}_3^2 \) to identity mappings. Additionally, the gate branch on the channels operating on the isolated \( x_j \) is set to 1 at positions smaller than $h$ and 0 for the rest, also functioning as an identity mapping at positions $i \leq h$, and masking positions for indexes $i>j$.

    \item \textbf{S6 :} The S6 module is configured to allow duplication. Specifically, by setting \( \bar{A} = 1 \) and \( \bar{B}, \bar{C} = 1 \), the S6 can output a sequence with multiple copies of \( x_j \):
    \[
    U^3 = (0, \dots, 0, x_j, x_j, \dots, x_j, 0, \dots, 0)
    \]
    Here, \( x_j \) appears \( P-2 \) times, and the last $x_j$ positioned at index $i=h$.
\end{itemize}



\noindent \textbf{Step 3: Aggregate Multiplications to Powers (Third Mamba Block)}

The third Mamba block is designed to aggregate the duplicated elements \( x_j \) into the form \( x_j^P \), utilizing the multiplicative capabilities of the S6 module and the subtraction mechanism in the final linear layer.

\begin{itemize}
    \item \textbf{S6 Module Configuration:} In this block, the S6 module is configured to perform the necessary multiplications that aggregate the duplicated values of \( x_j \). This is achieved by setting the system input and output matrices to 1. Hence, the output of the SSM when applied on $U_2$ at position $h$ will result at:
    
    \begin{equation}
    \small
    \sum_{j=1}^{P-2} c_j {x_j}^{j+2}
    \nonumber
    \end{equation}


Thus, we construct an additional sequence, similar to $U_2$, denoted by $U_2'$, by introducing an additional zero at the initial occurrence of $x_j$. We then subtract the outputs from these two identical SSM channels at the final linear layer of the block. This subtraction yields a telescoping series:
    \begin{equation}
    \text{SSM}(U_2) - \text{SSM}(U_2') = 
    \end{equation}
     
    \begin{equation}
    \small
    \sum_{j=1}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2 - \sum_{j=2}^L c_j \Big{(} \Pi_{k=j+1}^{t-1} x_k \Big{)} {x_t}^2 {x_j}^2 
         % C_t \sum_{j=1}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} - C_t \sum_{j=2}^t \Big{(}\Pi_{k=j+1}^t \bar{A}_k \Big{)} \bar{B}_j x_{j} 
%     \nonumber
% \end{equation}
%
% \begin{equation}
    =\sum_{j=1}^{P-2} c_j {x_j}^{j+2} - \sum_{j=2}^{P-2} c_j {x_j}^{j+2} = {x_j}^{P}
\end{equation}
yields
   \[
    U^4 = \left(0, \dots, 0, x_j^P, 0, \dots, 0\right)
    \]
\end{itemize}

This construction shows that a model with three stacked Mamba layers and sufficient channels can indeed express any univariate polynomial \( x_j^P \), thereby proving Lemma~\ref{lemma:3layerMambaExpresivity}.

\end{proof}
