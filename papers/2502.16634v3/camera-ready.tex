\maketitle

% tl;dr
% This paper presents OptionZero, a method that integrates options into the MuZero algorithm, which autonomously discovers options through self-play games and utilizes options during planning.

\begin{abstract}
Planning with options -- a sequence of primitive actions -- has been shown effective in reinforcement learning within complex environments.
Previous studies have focused on planning with predefined options or learned options through expert demonstration data.
Inspired by MuZero, which learns superhuman heuristics without any human knowledge, we propose a novel approach, named \textit{OptionZero}.
OptionZero incorporates an \textit{option network} into MuZero, providing autonomous discovery of options through self-play games.
Furthermore, we modify the dynamics network to provide environment transitions when using options, allowing searching deeper under the same simulation constraints.
Empirical experiments conducted in 26 Atari games demonstrate that OptionZero outperforms MuZero, achieving a 131.58\% improvement in mean human-normalized score.
Our behavior analysis shows that OptionZero not only learns options but also acquires strategic skills tailored to different game characteristics.
% 5d: better ending?
Our findings show promising directions for discovering and using options in planning.
Our code is available at https://rlg.iis.sinica.edu.tw/papers/optionzero.



% We have made our option MuZero design and experimental data publicly available, with the hope that these experimental results can contribute to future research on options and zero-knowledge learning.

% OptionZero addresses the limitations of traditional Monte Carlo Tree Search (MCTS) in MuZero by integrating the concept of options, inspired by temporal abstraction, into the planning phase.


\end{abstract}

% TODO
% * Sokoban做training/evaluation時，可以調低simulation，看看是不是少simulation時，option會更有用(因為可以看的越深)
% * Atari那邊要把整個方法可以調整的參數都列出來，到時候實驗數據可能要有一些調整參數的表格
% 表格、background、method
% TODO: VQ-Jump
\section{Introduction}
% guei: check this section
% alex: check this section (OK)
Reinforcement learning is a decision-making process in which an agent interacts with environments by selecting actions at each step to maximize long-term rewards.
Actions, commonly referred to as \textit{primitive action}, advance the state by one step each.
While this granularity allows precise control at each time step, it can lead to inefficiencies in scenarios where predictable sequences of actions are beneficial.
For example, in a maze navigation task, it is more efficient to choose a sequence of actions -- such as following a straightforward path until reaching a new junction -- rather than deciding an action at each time step.
This approach reduces the frequency of decision-making and accelerates the learning process.
To address these challenges, the concept of \textit{options} \cite{sutton_mdps_1999} has emerged, providing a framework for executing temporally extended actions based on the current state.
Options bridge single-step decision-making and strategic long-term planning, not only speeding up the learning process to handle complex scenarios but also simplifying the decision-making by reducing the frequency of choices an agent must consider.

Previous works have proposed adopting the concept of options by either predefining options or learning from expert demonstration data \cite{sharma_learning_2016,durugkar_deep_2016,dewaard_monte_2016,gabor_subgoalbased_2019,czechowski_subgoal_2021,kujanpaa_hierarchical_2023,kujanpaa_hybrid_2024}.
However, the predefined options often rely on a deep understanding of specific environments, and expert data may not be available for every environment, making it difficult to generalize these methods to other environments.
Moreover, when planning with options, previous methods require recurrently executing each action within the option to obtain the next states \cite{dewaard_monte_2016} or verifying whether the subgoal can be reached through primitive actions \cite{czechowski_subgoal_2021,kujanpaa_hierarchical_2023,kujanpaa_hybrid_2024}.
This increases the computational cost when executing longer options during planning, especially in scenarios where environment transitions are expensive.

Inspired by the success of MuZero \cite{schrittwieser_mastering_2020}, which employs a learned dynamics network to simulate the environment transitions during planning and achieves superhuman performance from scratch without requiring any human knowledge, this paper proposes a novel approach, named \textit{OptionZero}.
We modify the MuZero algorithm by integrating an \textit{option network} that predicts the most likely option for each state.
During training, OptionZero autonomously discovers options through self-play games and utilizes them during planning, eliminating the requirement for designing options in advance.
Furthermore, OptionZero improves the dynamics network to efficiently simulate environment transitions across multiple states with options, significantly reducing the overhead for iterative examination of internal states.

We conduct experiments on Atari games, which are visually complex environments with relatively small frame differences between states, making them suitable for learning options.
% 5d: todo, update new exp (alex) (OK)
Our results show that using options with maximum lengths of 3 and 6, OptionZero achieved mean human-normalized scores of 1054.30\% and 1025.56\%, respectively.
% 5d: todo, update new exp (alex) (OK)
In contrast, MuZero achieves a score of only 922.72\%.
In addition, we provide a comprehensive behavior analysis to examine the options learned and used during planning.
% 5d: check the following two sentences after writing experiments
Interestingly, the adoption of options varies across different games, aligning with the unique characteristics of each game.
This demonstrates that OptionZero effectively discovers options tailored to the specific game states and challenges of each environment.
In conclusion, our findings suggest that OptionZero not only discovers options without human knowledge but also maintains efficiency during planning.
This makes OptionZero easily applicable to other applications, further extending the versatility of the MuZero algorithm.


\section{Related works}
% alex: check 
% alex/peng: more references?
% guei: check
% 5d: todo
Numerous studies have explored the concepts of \textit{options} in reinforcement learning.
For example, \citet{dewaard_monte_2016} incorporated options from a predefined option set into Monte Carlo tree search (MCTS) and extended it to focus exploration on higher valued options during planning.
\citet{sharma_learning_2016} proposed using two policies for planning: one determines which primitive action to use, and the other determines how many times to repeat that action.
\citet{durugkar_deep_2016} explored the effects of repetition and frequency by statistics in Atari games.
\citet{vezhnevets_strategic_2016} introduced a method which learns options through end-to-end reinforcement learning.
\citet{lakshminarayanan_dynamic_2017} proposed a method that allows agents to dynamically adjust rates of repeated actions.
\citet{bacon_optioncritic_2017} derived an option-critic framework, which learns a policy over options and a policy within options.
The option policy not only determines how to select and execute an action within options but also learns when to terminate the option.
\citet{kim_lesson_2023} proposed to adaptively integrate multiple exploration strategies for options based on the option-critic framework.
\citet{riemer_role_2020} introduced a parameter-sharing approach for deep option learning.
\citet{young_iterative_2023} discovered options by learning the option policies and integrated them with a Monte Carlo search.
\citet{jinnai_finding_2019} formalized the problem of selecting the optimal option set, and produced an algorithm for discovering the suboptimal option set for planning.
\citet{veeriah_discovery_2021} proposed a meta-gradient approach for discovering reusable, task-independent options.
In addition, several works have studied subgoals, which represent a target state to achieve after several time steps, either segmented by predefined time step intervals or predicted dynamically by a learned network.
For example, \citet{gabor_subgoalbased_2019} used predefined subgoals for planning in MCTS.
\citet{czechowski_subgoal_2021} introduced a Subgoal Search method to obtain fixed-length subgoals with a low-level policy that predicts primitive actions for reaching subgoals.
% \citet{czechowski_subgoal_2021} introduced the Subgoal Search method to obtain fixed-length subgoals with a low-level policy and a value function, where the low-level policy predicts primitive actions for reaching subgoals, and the value function guides the search for a planner.
\citet{kujanpaa_hierarchical_2023} proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data.
% \citet{kujanpaa_hierarchical_2023} proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data, and employed VQ-VAE \cite{vandenoord_neural_2017} to store subgoals in a codebook.
\citet{kujanpaa_hybrid_2024} extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that subgoals are reachable.
% \citet{kujanpaa_hybrid_2024} extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that all goal states are reachable during the search process.
In summary, these previous works either adopt predefined options, learn subgoals from expert data, or do not incorporate options in MCTS planning.
Compared to these works, our goal is to automatically discover options without relying on predefined options or expert data and to use options during planning.

% \section{Background}% MuZero/Option

% \subsection{MuZero}
\section{MuZero}
% peng: check this section
MuZero \cite{schrittwieser_mastering_2020} is based on the foundation of AlphaZero \cite{silver_general_2018}, distinguishing itself by learning environment transitions using neural networks.
This allows MuZero to plan in advance without extra interaction with the environment, which is particularly advantageous in environments where such interactions are computationally expensive.
Consequently, MuZero has achieved success in a wide range of domains \cite{schrittwieser_mastering_2020,danihelka_policy_2022,antonoglou_planning_2021,hubert_learning_2021,mandhane_muzero_2022,wang_optimizing_2023}.
% Consequently, MuZero has achieved success in a wide range of domains, such as visually complex games \cite{schrittwieser_mastering_2020,danihelka_policy_2022}, stochastic environments \cite{antonoglou_planning_2021}, physical control tasks \cite{hubert_learning_2021}, and optimization problems \cite{mandhane_muzero_2022,wang_optimizing_2023}.

For planning, MuZero adopts Monte Carlo tree search (MCTS) \cite{kocsis_bandit_2006, coulom_efficient_2007, browne_survey_2012}, integrating three distinct networks: \textit{representation}, \textit{dynamics}, and \textit{prediction}.
Specifically, for an observation $x_t$ at time step $t$, the search determines an action $a_{t+1}$ using multiple simulations, each consisting of three phases: selection, expansion, and backup.
% First, the representation network $h_\theta$ converts $o_t$ into a hidden state $s^0$ as the root node for the search tree, denoted as $s^0 = h_\theta(o_t)$.
% Then, MCTS repeats multiple simulations during planning, each consisting of three phases: selection, expansion, and backup.
The selection starts from the hidden state root node $s^0$, selecting child nodes recursively until an unexpanded leaf node $s^l$ is reached.
For each non-leaf node $s^k$, the child node $s^{k+1}$ (corresponding to action $a^{k+1}$) is selected according to the highest PUCT \cite{rosin_multiarmed_2011, silver_mastering_2017} score:
\begin{equation}\label{eq:puct}
Q(s^k, a^{k+1}) + P(s^k, a^{k+1}) \times \frac{\sqrt{\sum_b N(s^k, b)}}{1 + N(s^k, a^{k+1})} \times c_{puct},
\end{equation}
where $Q(s^k,a^{k+1})$ is the estimated Q-value, $P(s^k,a^{k+1})$ is the prior probability, $N(s^k,a^{k+1})$ is the visit counts, and $c_{puct}$ is a constant for exploration.
In the expansion phase, to expand the leaf node $s^l$, the dynamics network $g_\theta$ is applied to perform the environmental transition: $s^l, r^l = g_\theta(s^{l-1}, a^l)$, where $r^l$ is the immediate reward.
Note that when $l=0$, the representation network $h_\theta$ is used to initialize the root node: $s^0 = h_\theta(x_t)$.
Then, the prediction network $f_\theta$ is applied to evaluate its policy and value: $p^l, v^l = f_\theta(s^l)$, where $p^l$ is used for $P(s^l, a)$ and $v^l$ is the estimated value for $s^l$.
The backup phase uses the obtained value $v^l$ to update the statistics $Q(s^k,a^{k+1})$ and $N(s^k,a^{k+1})$:
\begin{equation}\label{eq:mcts_backup}
% \begin{aligned}
Q(s^k,a^{k+1}) := \frac{N(s^k,a^{k+1}) \times Q(s^k,a^{k+1}) + G^{k+1}}{N(s^k,a^{k+1}) + 1} \text{ and }
N(s^k,a^{k+1}) := N(s^k,a^{k+1}) + 1,
% \end{aligned}
\end{equation}
where $G^{k+1} = \sum_{\tau=0}^{l-k-1}{\gamma ^ \tau r^{k + 1 + \tau}} + \gamma^{l - k} v^l$ is the cumulative reward discounted by a factor $\gamma$.

During training, MuZero continuously performs self-play and optimization.
The self-play process collects game trajectories, including $x_t$, $\pi_t$, $a_{t+1}$, $u_{t+1}$, and $z_t$ for all time steps.
For each $x_t$, MCTS is conducted to produce the search policy $\pi_t$.
Then, an action $a_{t+1} \sim \pi_t$ is applied to the environment, obtaining an immediate reward $u_{t+1}$ and moving forward to the next observation $x_{t+1}$.
In addition, $z_t$ is the n-step return.
The optimization process updates the networks by sampling records from collected trajectories.
For each sampled record $x_t$, the process uses the networks to unroll it for $K$ steps to obtain $s_t^k$ with corresponding $p_t^k$, $v_t^k$, and $r_t^k$ for $0 \leq k \leq K$, where $s_t^0 = h_\theta(x_t)$ and $s_t^k, r_t^k = g_\theta(s_t^{k-1}, a_{t+k})$ for $k > 0$.
Then, all networks are jointly updated using
\begin{equation}\label{eq:mz_loss}
L_t=\sum_{k=0}^{K}l^{p}(\pi_{t+k},p_t^k)
+\sum_{k=0}^{K}l^{v}(z_{t+k},v_t^{k})
+\sum_{k=1}^{K}l^{r}(u_{t+k},r_t^{k})
% +\sum_{k=1}^{K}{l^{c}(s_{t+k}^0,s_t^k)}
+c||\theta||^{2},
\end{equation}
where $l^p$ is the policy loss, $l^v$ is value loss, $l^r$ is the reward loss, and $c||\theta||^{2}$ is the L2 normalization.
% 5d: add this in the experiment
% Note that the state consistency loss is introduced by \citet{ye_mastering_2021}, which adopts SimSiam \cite{chen_exploring_2021} to align the hidden states produced by $h_\theta$ and $g_\theta$.



\section{OptionZero}\label{sec:ozero}
% This section proposes a novel approach, named \textit{OptionZero}, which modifies the MuZero algorithm to automatically learn and utilize options during planning.
% First, the option network is introduced in subsection \ref{sec:ozero-option_network}, followed by a modified MCTS that utilizes options to generate self-play games.
% Lastly, subsection \ref{sec:ozero-training} describes the method for learning the option network from generated games.

\subsection{Option network}\label{sec:ozero-option_network}
\textit{Options} are the generalization of actions to include temporally extended actions, which is applied interchangeably with primitive actions \cite{sutton_mdps_1999,bacon_optioncritic_2017}.
In this context, options on Markov decision process (MDP) form a special case of decision problem known as a semi-Markov decision process (SMDP).
% , which provides the theory foundation of planning and learning with options.
Given a state $s_t$ at time step $t$ and an option length $L$, we enumerate all possible options, denoted as $o_{t+1}=\{a_{t+1}, a_{t+2}, ..., a_{t+L}\}$, by considering every sequence of primitive actions starting at $s_t$.
When executing the option $o_{t+1}$, we obtain a sequence of states and actions $s_t, a_{t+1}, s_{t+1}, a_{t+2}, ..., s_{t+L-1}, a_{t+L}, s_{t+L}$.
Ideally, the probability of selecting each option can be calculated by multiplying the probabilities of each primitive action within the option, as illustrated in Figure \ref{fig:option_decision_tree}.
% guei: s, a1, a2, a3 do not match the above definitation ==> change to s_0 (guei)
For example, when $L=4$, the probability of option $o_1=\{a_1, a_2, a_3, a_4\}$ for $s_0$ is $P(a_1)\times P(a_2)\times P(a_3)\times P(a_4)=0.8^4=0.4096$, where $P(a_i)$ is the probability of selecting action $a_i$.
A naive approach to obtaining the option probabilities involves using a policy network to evaluate all possible states from $s_t$ to $s_{t+L}$.
However, this approach is computationally expensive, and the number of options grows exponentially as the option length $L$ increases, making it infeasible to generate all options.

% alex, peng: fix figure
\begin{figure}[h]
    \centering
    \subfloat[]{
    \includegraphics[width=0.3\columnwidth]{figures/option_decision_tree.pdf}
    \label{fig:option_decision_tree}
    }
    \subfloat[]{
    \includegraphics[width=0.6\columnwidth]{figures/option_network.pdf}
    \label{fig:option_network}
    }
    \caption{An illustration of calculating option in a decision tree. Each node represents a with two possible actions, \textit{L} and \textit{R}, corresponding to the left and right transitions to the subsequent state. (a) The decision tree and probabilities for each option at state $s$. (b) The procedure of determining the dominant option from the option network.}
    \label{fig:option_prob}
\end{figure}

In practice, since most options occur infrequently due to their lower probabilities, our primary interest lies in the \textit{dominant option}.
The dominant option, $o_1=\{a_1, a_2, ..., a_l\}$, is defined such that $\Pi_{i=1}^lP(a_i)>0.5 \land \Pi_{i=1}^{l+1}P(a_i)\leq0.5$, where $\Pi_{i=1}^lP(a_i)$ is the cumulative product of probabilities and $1\leq l\leq L$.
For example, in Figure \ref{fig:option_decision_tree}, the dominant option for $s_0$ is $o_1=\{a_1, a_2, a_3\}$ because $P(a_1)\times P(a_2)\times P(a_3) =0.512$ and $P(a_1)\times P(a_2)\times P(a_3)\times P(a_4) = 0.4096$, and the dominant option for $s'_0$ is $o'_1=\{a'_1, a'_2\}$.
This indicates that the length of the dominant option can vary, depending on how long the cumulative probabilities remain above the threshold of 0.5.
In addition, this design ensures that there is only one dominant option for each state $s$, effectively preventing exponential growth in the number of possible options.

Next, we incorporate the \textit{option network} into the prediction network in MuZero, denoted as $\Omega, p, v=f_\theta(s)$, which predicts an additional option output, $\Omega$, for predicting the dominant option at state $s$.
Given the maximum option length $L$, the option network produces $L$ distributions, $\Omega=\{\omega_1, \omega_2, ..., \omega_L\}$, which are used to derive the dominant option, $o_1=\{a^*_1, a^*_2, ..., a^*_l\}$, where $a^*_i=\argmax_a \omega_i(a)$.
Each $\omega_i$ represents the conditional cumulative product probability of selecting a sequence of actions from $a^*_1$ to $a^*_i$, i.e, $\omega_i(a^*_i)=\Pi_{j=1}^iP(a^*_j)$.
Furthermore, a virtual action, called \textit{stop}, is introduced to provide a termination condition.
% guei: inconsistent notation used: a_i <---> a_{stop} (guei)
This \textit{stop} action is the sum of probabilities for all actions except $a^*$, defined as $\omega(stop)=1-\omega(a^*)$.
To derive the dominant option from $\Omega$, we progressively examine each $\omega_i$ from $\omega_1$ to $\omega_L$, selecting $a^*_i$ as $a^*_i=\argmax_a \omega_i(a)$ until $i=L$ or $a^*_i$ becomes a \textit{stop} action.
We provide an example for obtaining the dominant options for state $s_0$ and $s'_0$, as shown in Figure \ref{fig:option_network}.
This method allows for determining the dominant option at any state $s$ without recurrently evaluating future states, reducing the computational costs.


\subsection{Planning with Dominant Option in MCTS}\label{sec:ozero-mcts}
This subsection describes the modifications to MCTS implemented in OptionZero to incorporate planning with the dominant option.
For simplicity, we will use \textit{option} to represent the \textit{dominant option} in the rest of this paper.
The planning generally follows the MuZero but with two modifications, including the network architecture and MCTS.
For the network architecture, we add an additional option output to the prediction network, denoted as $\Omega^k, p^k, v^k=f_\theta(s^k)$, where $\Omega^k$, $p^k$, and $v^k$ are the option distribution, policy distribution, and value at state $s^k$, respectively.
Note that we use superscript $s^k$ instead of subscript $s_k$ in this subsection.
This is because $s^k$ represents the hidden state, obtained after unrolling $k$ steps by the dynamics network from the initial hidden state $s^0$.
In contrast, $s_k$ denotes the actual observed state in the environment at time step $k$.
As illustrated in the previous section, we can derive the option $o^k$ from $\Omega^k$.
The dynamics network, denoted as $s^{k+l}, r^{k+1,k+l}=g_\theta(s^k, \mathcal{A}^{k+1})$, is modified to predict the next hidden state $s^{k+l}$ and the accumulated discounted reward $r^{k+1,k+l}$ upon executing a composite action $\mathcal{A}^{k+1}$ at $s^k$.
% 5d: \mathcal{A}=>\alpha
The composite action, $\mathcal{A}^{k+1}$, can be either a primitive action $a^{k+1}$ or an option $o^{k+1}$ with the length $l$.
% 5d: check all r^{?,?} in this subsection (guei)
The accumulated discounted reward $r^{k+1,k+l}$ is computed as $\sum_{i=1}^l\gamma^{i-1} r^{k+i,k+i}$, where $r^{i,i}$ represents the single immediate reward obtained by applying $a^i$ at state $s^{i-1}$.
Note that the dynamics network supports unrolling the option directly, eliminating the need to recurrently evaluate each subsequent state from $s^k$ to $s^{k+l}$.

Next, we demonstrate the incorporation of options within MCTS.
The search tree retains the structure of the original MCTS but includes edges for options that can bypass multiple nodes directly, as shown in Figure \ref{fig:mcts_stages}.
This adaptation integrates options subtly while preserving the internal actions within options, allowing the tree to traverse states using either a primitive action or an option.
Each edge within the tree is associated with statistics $\{N(s,\mathcal{A}), Q(s,\mathcal{A}), P(s,\mathcal{A}), R(s,\mathcal{A})\}$, representing its visit counts, estimated Q-value, prior probability, and reward, respectively.
Moreover, for nodes that possess both a primitive edge and an option edge, the statistics of the primitive edge are designed to include those of the option edge.
For example, if the tree traverses the node via the option edge, the visit counts for both the primitive and option edges are incremented.
This ensures the statistics remain consistent with MuZero when only primitive edges are considered within the tree.
We illustrate the modifications made to each phase of MCTS in the following.

% alex, peng: fix figure
\begin{figure}
    \centering
    \includegraphics[width=0.69\linewidth]{figures/mcts_stages_v2.pdf}
    \caption{An illustration of each phase in MCTS in OptionZero.}
    \label{fig:mcts_stages}
\end{figure}

% 5d: why using two stage? let's wait for reviewers to ask this question
\textbf{Selection.} For any node $s^k$, the selection of the next child node includes two stages: \textit{primitive selection} and \textit{option selection}.
The primitive selection only considers primitive child nodes and remains consistent with MuZero by selecting the next action $a^{k+1}$ based on the PUCT score based on \eqref{eq:puct}.
If the selected action $a^{k+1}$ matches the first action in option $o^{k+1}$, we then proceed with the option selection to determine whether to select this primitive action $a^{k+1}$ or option $o^{k+1}$.
Option selection is similar to the primitive selection, using the PUCT score to compare both the primitive and option nodes.
Since the option node is a successor node of the primitive node, the statistics for the primitive node need to be adjusted to exclude contributions from the option node in the option selection.
We select either primitive or option nodes based on the higher PUCT score, which is calculated as follows:
\begin{equation}\label{eq:option_mcts_puct}
\begin{cases}
    Q(s^k,o^{k+1})+P(s^k,o^{k+1}) \times \frac{\sqrt{\sum_b N(s^k, b)}}{1+N(s^k, o^{k+1})} \times c_{puct} & \text{if }\textit{option node}\text{,}\\
    \Tilde{Q}(s^k,a^{k+1})+\Tilde{P}(s^k, a^{k+1}) \times \frac{\sqrt{\sum_b N(s^k, b)}}{1+(N(s^k, a^{k+1})-N(s^k, o^{k+1}))} \times c_{puct} & \text{if }\textit{primitive node}\text{.}
\end{cases}
\end{equation}
The $\Tilde{P}(s^k, a^{k+1})=\max(0, P(s^k, a^{k+1})-P(s^k, o^{k+1}))$ ensures that the prior remains non-negative.
The adjusted estimated Q-value, $\Tilde{Q}(s^k,a^{k+1})$, is calculated as $\frac{N(s^k, a^{k+1})Q(s^k,a^{k+1}) - N(s^k, o^{k+1})Q(s^k,o^{k+1})}{N(s^k, a^{k+1})-N(s^k, o^{k+1})}$.
Note that $\sum_b N(s^k, b)$ is the total visit counts for selecting $a^{k+1}$ and $o^{k+1}$, which is equivalent to $N(s^k, a^{k+1})$ because the statistics of the primitive node already include the statistics of option node.
The selection process begins at the root node $s^0$ until an unevaluated node $s^l$ is reached, as shown in Figure \ref{fig:mcts_stages}.

\textbf{Expansion.} Assume the last two node in the selection path is $s^m$ and $s^l$, where $s^m$ is the parent node of $s^l$.
To expand node $s^l$, we derive $r^{m+1,l}, \Omega^l, p^l, v^l$ using the dynamics and prediction network.
The reward $r^{m+1,l}$ is from $s^{m}$ to $s^l$ and used to initialize the edge $R(s^{m}, \mathcal{A}^{m+1})=r^{m+1,l}$.
The edge of all primitive child nodes are initialized as $\{N(s^l, a^{l+1})=R(s^l, a^{l+1})=Q(s^l, a^{l+1})=0\}$ and $P(s^l, a^{l+1})=p^l$.
Then, if the length of option $o^{l+1}$ derived from $\Omega^l$ is larger than 1, we expand the internal nodes following the action within the option.
The statistics of each edge are initialized as 0 since these internal nodes are unevaluated.
For the option node, the edge is initialized as $\{N(s^l, o^{l+1})=R(s^l, o^{l+1})=Q(s^l, o^{l+1})=0\}$ and $P(s^l, o^{l+1})=\omega^l$.

\textbf{Backup.} The backup phase updates the visit counts and estimated Q-value from $s^l$ back to $s^0$.
Considering that $s^l$ may be accessed through various selection paths from $s^0$, all edges on the possible paths from $s^0$ to $s^l$ must be updated.
This ensures that both the visited count and estimated Q-value of all nodes remain consistent within the search, regardless of the selection path chosen.
We first obtain the $l-k$-step estimate of the cumulative discounted reward as $G^{k}=r^{k+1,l}+\gamma^{l-k}v^l$, where $r^{k+1,l}$ is the discounted reward from $s^k$ to $s^l$ and $v^l$ is the value at state $s^l$.
% 5d: is it clear?
Since not all edges have been evaluated, we calculate $r^{k+1,l}$ by using $\frac{r^{1,l}-r^{1,k}}{\gamma^k}$, where $r^{1,k}$ and $r^{1,l}$ represent discounted rewards from the root node $s^0$ to $s^k$ and $s^l$, respectively.
Then, we update the estimated Q-value of each edge, $Q(s^k,\mathcal{A}^{k+1})$, using a similar approach as introduced in \eqref{eq:mcts_backup}:
% guei: s^k , A^{k} <--> A^{k+1} ?? (guei)
\begin{equation}\label{eq:option_mcts_backup}
\begin{aligned}
    Q(s^k,\mathcal{A}^{k+1}) & :=\frac{N(s^k,\mathcal{A}^{k+1})\times Q(s^k,\mathcal{A}^{k+1})+G^{k+1}}{N(s^k,\mathcal{A}^{k+1})+1},\\
    N(s^k,\mathcal{A}^{k+1}) & :=N(s^k,\mathcal{A}^{k+1})+1,
\end{aligned}
\end{equation}
% where $|\mathcal{A}^{k+1}|$ is the length of the composite action.

During planning, the MCTS performs a fixed number of simulations, each including the above three phases.
Upon the search completed, MCTS selects a child node from the root node $s^0$ based on probabilities proportional to their visit counts and performs the composite action in the environment.
\revision{
Overall, the additional complexity introduced by OptionZero, including the costs for the option network and maintaining statistics for option edges, is negligible compared to the original MuZero.
}

\subsection{Training OptionZero}\label{sec:ozero-training}
% 5d: explain when using $s_t$ and $s^t$
We describe the optimization process using the self-play trajectory in OptionZero, as shown in Figure \ref{fig:training}.
For better illustration, we utilize three additional symbols, including $\mathcal{O}$, $\mathcal{U}$, $\tau$, and $\hat{\tau}$.
% 5d: \mathcal{A} => \mathcal{O} (guei)
Given a state $s_t$ at time step $t$, $\mathcal{O}_i$ represents the $i$-th executed composite action starting from $s_t$, $\mathcal{U}_i$ is defined as the discounted reward obtained after executing $\mathcal{O}_i$, $\tau_i$ denotes the action sequence length of $\mathcal{O}_i$, and $\hat{\tau}_i=\sum_{j=1}^i\tau_j$ is the accumulated length from $\mathcal{O}_1$ to $\mathcal{O}_i$.
For example, in Figure \ref{fig:training}, from the perspective of $s_t$, we can obtain $\mathcal{O}_1=o_{t+1}=\{a_{t+1}, a_{t+2}\}, \mathcal{O}_2=\{a_{t+3}$\}, with corresponding discounted rewards $\mathcal{U}_1=u_{t+1}+\gamma u_{t+2}, \mathcal{U}_2=u_{t+3}$, action sequence lengths $\tau_1=2, \tau_2=1$, and accumulated lengths $\hat{\tau}_1=2, \hat{\tau}_2=3$.
Then, the observed discounted reward $\mathcal{U}_1$ at $s_t$ is calculated as $\sum_{i=0}^{\tau_1-1}\gamma^i u_{t+1+i}$, aggregating the observed rewards provided by the environment with a discount factor $\gamma$.
The $n$-step return $z_t$ is calculated as $\mathcal{U}_1 + \gamma^{\hat{\tau}_1} \mathcal{U}_2 + ... + \gamma^{\hat{\tau}_{T-1}} \mathcal{U}_T + \gamma^{\hat{\tau}_T} v_{t+\hat{\tau}_T}$, where $\hat{\tau}_T=n$.
Note that $v_{t+n}$ is not always available, as $s_{t+n}$ may be bypassed when options are executed.
Consequently, we identify the smallest $T$ such that $\hat{\tau}_T\geq n$, ensuring that the step count for the n-step return approximates $n$ as closely as possible.
In Figure \ref{fig:training}, if $n=5$, since $s_{t+5}$ is skipped by option, we then approximate the $n$-step return by using $v_{t+6}$ as $z_t=\mathcal{U}_1 + \gamma^2 \mathcal{U}_2 + \gamma^3 \mathcal{U}_3 + \gamma^4 \mathcal{U}_4 + \gamma^6 v_{t+6}$.

Next, we describe the training target for both the policy and option network.
The search policy distribution $\pi_t$ is calculated in the same manner as in MuZero.
For the option network, given an option length $L$ at state $s_t$, we examine its subsequent states to derive the training target, $\Phi_t=\{\phi_t, \phi_{t+1}, ..., \phi_{t+L-1}\}$.
Each $\phi_i$ is a one-hot vector corresponding to the training target for $\omega_i$.
Specifically, for any state $s$, if the option network predicts an option $o=\{a_1, a_2, ..., a_l\}$ that exactly matches the composite action $\mathcal{O}=\{a_1, a_2, ..., a_l\}$ executed in the environment, then the option learns the action sequence, i.e., $\phi_i=\text{onehot}(a_{i+1})$ for $0\leq i\leq l-1$.
Conversely, if $o\neq \mathcal{O}$, then the option learns to \textit{stop}, i.e., $\phi_i=\text{onehot}(stop)$.
We iterate this process to set each $\phi$ from $s_t$ to $s_{t+L-1}$.
If $\phi_{t+i}$ is set to learn \textit{stop}, subsequent $\phi_{t+j}$ should follow, i.e., $\phi_{t+j}=\text{onehot}(stop)$ for $i \leq j \leq L-1$.
Note that if the length of predicted option $o_i$ is zero, $o_i$ is defined as $\{a_{i+1}\}$, where $a_{i+1}=\argmax_a p_i(a)$ is determined according to the policy network.
This method ensures that the option network eventually learns the cumulative probability of the dominant option, as described in subsection \ref{sec:ozero-option_network}.
% 5d: we provide a proof in the appendix
% We provide proof in the appendix.
Figure \ref{fig:training} shows an example of setting the training target for the option network.
If $o_{t+2}\neq\mathcal{O}_2$, then the option network learns $\{a_{t+1}, a_{t+2}$, \textit{stop}$\}$, $\{$\textit{stop}, \textit{stop}, \textit{stop}$\}$, and $\{a_{t+4}, a_{t+5}, a_{t+6}\}$, for $s_t$, $s_{t+2}$, and $s_{t+3}$, respectively.

% alex, peng: fix figure
% guei: check
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/training.pdf}
    \caption{An illustration of optimization in OptionZero. The notion is from the perspective of $s_t$.}
    \label{fig:training}
\end{figure}

During the optimization phase, the sampled state $s_t$ is trained with $K$ unrolling steps, where each step can be either a primitive action or an option.
This enables the dynamics network to learn the environment transitions that incorporate options.
The loss is modified from \eqref{eq:mz_loss} as follows:
% 5d: r^k_t is incorrect
\begin{equation}\label{eq:oz_loss}
    L_t=
    \sum_{k=0}^{K} l^{p}(\pi_{t+\hat{\tau}_k},p^k_t)+
    \sum_{k=0}^{K} l^{v}(z_{t+\hat{\tau}_k},v^k_t)+
    \sum_{k=1}^{K} l^{r}(\mathcal{U}_k,r^k_t)+
    \sum_{k=0}^{K} l^{o}(\Phi_{t+\hat{\tau}_k},\Omega^k_t)+
    c||\theta||^{2},
\end{equation}
where $\hat{\tau}_0=0$.
Note that the option loss $l^{o}$ includes $L$ cross-entropy losses.

\section{Experiment}
% TODO: 訓練參數設定
% - sokoban (peng)
% how to select puzzle?
% Potential-based Reward Shaping in Sokoban
% 1. how to compare with this paper?
% 2. why use the same puzzle of this paper?
% analysis:
% 1. how option predict the following actions?
% 2. compare with subgoal search?
% 3. toy example to show that option network can be useful for prediction

\subsection{OptionZero in GridWorld}
\label{sec:optionzero_gridworld}
% peng: check this subsection
% guei: check this subsection
% alex: check this subsection
We first train OptionZero in \textit{GridWorld}, a toy environment where the objective is to navigate an agent through a grid map with walls from a start position (S) to a goal (G) via the shortest possible route.
The maximum option length is set to nine.
% peng: todo
Other training details are provided in Appendix \ref {appendix:implementation}.
Figure \ref{fig:maze} shows the options learned by the option network at four stages of training: 25\%, 50\%, 75\%, and 100\% completion.
It can be observed that the learning behavior of OptionZero evolves distinctly across different stages.
In the early stage (25\%), the model mainly relies on primitive actions, identifying options only when approaching the goal.
In the middle stages (50\% and 75\%), the model begins to establish longer options, progressively learning options with lengths from two up to nine.
In the final stage (100\%), the model has learned the optimal shortest path using options.
Notably, using only primitive actions, the optimal path requires an agent to take at least 30 actions.
In contrast, OptionZero achieves this with just four options, accelerating the training process by approximately 7.5 times in this example.
This substantial reduction highlights OptionZero's efficacy, especially in more complex environments.
This experiment also shows that the option network can progressively learn and refine options during training, without requiring predefined options.

\begin{figure}[h!t]
\centering
\subfloat[25\%]{
    \includegraphics[width=0.22\columnwidth]{figures/gridworld/gridworld_20000.png}}
    \label{fig:maze_25}
\subfloat[50\%]{
    \includegraphics[width=0.22\columnwidth]{figures/gridworld/gridworld_40000.png}}
    \label{fig:maze_50}
\subfloat[75\%]{
    \includegraphics[width=0.22\columnwidth]{figures/gridworld/gridworld_60000.png}}
    \label{fig:maze_75}
\subfloat[100\%]{
    \includegraphics[width=0.22\columnwidth]{figures/gridworld/gridworld_80000.png}}
    \label{fig:maze_100}
\caption{Visualization of options learned by OptionZero at different stages of training in GridWorld.}
\label{fig:maze}
\end{figure}


\subsection{OptionZero in Atari Games}
\label{sec:optionzero_atari}
Next, we evaluate OptionZero on \textit{Atari} games, which are commonly used for investigating options \cite{sharma_learning_2016,dewaard_monte_2016,durugkar_deep_2016,bacon_optioncritic_2017,vezhnevets_strategic_2016,kim_lesson_2023,lakshminarayanan_dynamic_2017,riemer_role_2020} due to their visually complex environments and subtle frame differences between states, making training with primitive actions inefficient.
We train three OptionZero models, denoted as $\ell_1$, $\ell_3$, and $\ell_6$, each configured with maximum option lengths $L = 1, 3$, and $6$, respectively.
Detailed experiment setups are provided in Appendix \ref {appendix:experiment-setup}.
The model $\ell_1$ serves as a baseline, identical to MuZero, where no options are used during training.
In addition, we adopt the frameskip technique \cite{mnih_humanlevel_2015}, commonly used in training on Atari games, set to 4.
Namely, this results in a frame difference between 24 states when executing an option of length 6.
This requires OptionZero to strategically utilize options when necessary, rather than indiscriminately.

% alex: update the score (OK)
Table \ref{tab:Atari26-score} shows the results of 26 Atari games.
Both $\ell_3$ and $\ell_6$ outperform the baseline $\ell_1$ in mean and median human-normalized scores, with $\ell_3$ achieving the best performance at 1054.30\% and 391.69\%, representing improvements of 131.58\% and 63.29\% over $\ell_1$.
Overall, 20 out of 26 games perform better than $\ell_1$ for $\ell_3$, and 17 for $\ell_6$.
There are 12 games where scores consistently increase as the option length increases.
For example, in \textit{up n down}, the scores rise by 63810.83 and 79503.9 from $\ell_1$ to $\ell_6$.
Conversely, there are only four games where scores decrease as the option length increases.
For example, in \textit{bank heist}, scores drop by 156.54 and 511.26, respectively.
We find that this decline is likely due to the difficulty of the dynamics network in learning environment transitions \cite{vries_visualizing_2021, he_what_2024, guei2024interpreting} in games with more complex action spaces.
As the option length increases, the number of possible option combinations grows.
Although we focus on learning the dominant option, the dynamics network still needs to learn across all dominant options.
In games like \textit{bank heist}, which offers a wide range of strategic possibilities for different option combinations, the learning complexity for the dynamics network increases.
Nevertheless, most of the games still improve when training with options, demonstrating that options enable more effective planning.
% rebuttal TODO : analysis
% 分析圖片、多寫遊戲、3個summary
\input{atari26_score}

% guei: put ablation study in appendix and ref at here?
% 5d: move to next subsection?
% For example, $\ell_6$ performs the best in seaquest, in the game, the agent needs to surface the submarine to avoid it blowing up.
% The frames during which the submarine is replenishing the oxygen tank are unimportant, as no action is required from the agent, and the agent is not attacked by enemies until it descends again.
% In this case, $\ell_6$ uses longer options to bypass these frames.
% On the other hand, dodging and blasting enemies is important for surviving, $\ell_6$ mostly uses short option or primitive actions during these frames.

% In contrast, $\ell_6$ performs slightly worse in gopher, in the game, the agent moves right or left to protect a crop of three carrots from a gopher.
% The movement range is limited, and a primitive action of moving right or left significantly affects the agent's position.
% In this case, it becomes more challenging for the dynamics network to accurately learn the transitions of a long option.
% This can introduce bias in predicting the precise positions of both the agent and the gopher, as the longer options may obscure finer details of the transitions between frames.



\subsection{Option Utilization and Behavior Analysis}
\label{sec:behavior-analysis}
This subsection analyzes how options are applied to better understand the planning process of OptionZero.
Table \ref{tab:option-length-with-repeat} presents the average percentages of primitive actions (\% $a$) and options (\% $o$), and the distribution of different option lengths (\% $l$) across 26 games.
In addition, columns ``$\bar{l}$'', ``\% Rpt.'', and ``\% NRpt.'' show the average action sequence length executed in games, the proportions of options that repeat a single primitive action or involve more than one action types, respectively.
Detailed statistics for each game are provided in Appendix \ref{appendix:behavior-analysis}.
From the table, we observe that primitive actions are generally the majority, accounting for over 60\% in both $\ell_3$ and $\ell_6$.
This is because Atari uses a frameskip of four, which means that each primitive action already spans across four states.
The use of frameskip four is well-established in previous research, and our experiments further corroborate these findings.
However, there are still nearly 30\% of states that can adopt options.
When comparing the use of options, it is notable that $\ell_6$ applies options less frequently, with a percentage of 30.57\% compared to 37.62\% in $\ell_3$.
However, the average action sequence length for $\ell_6$ (2.03) is longer than that of $\ell_3$ (1.69).
This is because action sequences that involve taking two consecutive three-step options in $\ell_3$ are merged into a single six-step option in $\ell_6$, resulting in a lower usage rate of options in statistics.
In summary, our findings reveal that OptionZero strategically learns to utilize options as well as employ primitive actions at critical states instead of indiscriminately utilizing longer options.

\begin{table}[h!]
    \caption{Proportions of options with different lengths and options with repeated primitive actions for $\ell_3$ and $\ell_6$ in Atari games.}
    \centering
    \small
    \begin{tabular}{l|rr|rrrrr|r|rr}
        \toprule
        & \% $a$ & \% $o$ & \% 2 & \% 3 & \% 4 & \% 5 & \% 6 & $\bar{l}$ & \% Rpt. & \% NRpt. \\
        \midrule
        $\ell_3$ & 62.38\% & 37.62\% & 6.23\% & 31.39\% & - & - & - & 1.69 & 75.94\% & 24.06\% \\
        $\ell_6$ & 69.43\% & 30.57\% & 8.55\% & 3.52\% & 1.86\% & 0.99\% & 15.64\% & 2.03 & 74.12\% & 25.88\% \\
        \bottomrule
    \end{tabular}
    \label{tab:option-length-with-repeat}
\end{table}

Next, among the different option lengths used, we observe that generally the longer option lengths are preferred.
This suggests that if a state already has applicable options, it is likely these options will be extended further, resulting in a trend towards longer options.
This behavior is consistent with the gradual increase in option lengths observed in gridworld as described in subsection \ref{sec:optionzero_gridworld}, illustrating the capability of OptionZero to effectively discover and extend longer options when beneficial.

Finally, we investigate the composition of primitive actions in options.
From Table \ref{tab:option-length-with-repeat}, approximately 75\% of options consist of repeated primitive actions, similar to the findings in \citet{sharma_learning_2016}.
For example, in \textit{freeway}, a game where players control chickens across a traffic-filled highway from bottom to top, the most commonly used options by OptionZero are sequences of repeated \textit{Up} actions (\textit{U-U-U} in $\ell_3$ and \textit{U-U-U-U-U-U} in $\ell_6$), guiding the chicken to advance upwards.
In addition, OptionZero prefers repeated \textit{Noop} actions, strategically pausing to let cars pass before proceeding.
On the other hand, some games still require options composed of diverse combinations of primitive action.
For example, in \textit{crazy climber}, a game where players control the left and right side of the body to climb up to the top, OptionZero utilizes options consisting of non-repeated actions.
These options often interleave \textit{Up} and \textit{Down} actions to coordinate the movements of the player's hands and feet, respectively.
More interestingly, OptionZero also learns to acquire options involving combination skills under specific circumstances.
In \textit{hero}, only 4.60\% of options involve non-repeated actions.
Although the chance is small, such options are crucial during the game.
For example, as depicted in Figure \ref{fig:hero}, the agent executes a series of strategically combined options, including landing from the top, planting a bomb at the corner to destroy a wall, swiftly moving away to avoid injury from the blast, and then skillfully times its movement to the right while firing after the wall is demolished.
It is worth noting that there are a total of 24 primitive actions executed from Figure \ref{fig:hero_1} to \ref{fig:hero_5}, but only four options are executed in practice, showing that using option provides effective planning.
In conclusion, our results demonstrate that OptionZero is capable of learning complex action sequences tailored to specific game dynamics, effectively discovering the required combinations whether the options are shorter, longer, or contain repeated actions.
We have provided the top frequency of options used in each game in the Appendix \ref{appendix:topk-analysis}.

\begin{figure}[h]
    \centering
    \subfloat[\tiny \textit{RF-RF-RF-RF-D-D}]{
    \includegraphics[width=0.18\columnwidth]{figures/atari_hero/frame_62.pdf}
    \label{fig:hero_1}
    }
    \subfloat[\tiny \textit{D-L-L-L-L-L}]{
    \includegraphics[width=0.18\columnwidth]{figures/atari_hero/frame_63.pdf}
    \label{fig:hero_2}
    }
    \subfloat[\tiny \textit{L-L-RF-RF-RF-RF}]{
    \includegraphics[width=0.18\columnwidth]{figures/atari_hero/frame_64.pdf}
    \label{fig:hero_3}
    }
    \subfloat[\tiny \textit{RF-RF-RF-RF-RF-RF}]{
    \includegraphics[width=0.18\columnwidth]{figures/atari_hero/frame_65.pdf}
    \label{fig:hero_4}
    }
    \subfloat[\tiny \textit{RF-RF-RF-RF-RF-RF}]{
    \includegraphics[width=0.18\columnwidth]{figures/atari_hero/frame_66.pdf}
    \label{fig:hero_5}
    }
\caption{Sequence of game play from (a) to (e) for OptionZero in \textit{hero}. The actions \textit{R}, \textit{L}, \textit{D}, and \textit{F} represent moving right, moving left, placing bombs, and firing, respectively.}
    \label{fig:hero}
\end{figure}


\subsection{Option Utilization in the search}
\label{sec:behavior-analysis-in-search}
We further investigate the options used during planning.
Table \ref{tab:option-in-tree} lists the results for $\ell_1$, $\ell_3$, and $\ell_6$, including the proportions of search trees that consist of at least one option edge is expanded in MCTS (``\% in Tree''), the proportions of simulations that at least one option has been selected during search (``\% in Sim.''), the average tree depth\revision{, the median tree depth,} and the maximum tree depth.
Detailed statistics for each game are provided in Appendix \ref{appendix:options-in-search}.
% 5d: most selection path relies on primitive actions, not indiscriminately to use option
The results show that approximately 90\% of search trees expand options, but only around 30\% of search trees choose options during selection.
Considering the nature of exploration in MCTS, it is reasonable that not all simulations will incorporate options.
Surprisingly, there are still certain game states for which the search process does not use options at all.
% 5d: example?
Especially in \textit{hero}, From $\ell_3$ to $\ell_6$, the proportion of search trees utilizing options decreases from 74.43\% to 54.39\%, showing that there are numerous game states where options are not required.
However, the performance remains consistent, suggesting that the planning could concentrate on applying options in certain states.
Note that the less frequent use of options does not cause undesirable results; eventually, the search behaves similarly to that of MuZero.
% We hypothesize that the models cannot discover options for uncommon scenes since the background image frequently changes as the player moves in this game.
% guei: hero example?


\begin{table}[h!]
    \caption{Proportions of options in search tree for $\ell_3$ and $\ell_6$ in Atari games.}
    \centering
    \small
    \begin{tabular}{l|rr|rrr}
        \toprule
        & \% in Tree & \% in Sim. & Avg. tree depth & \revision{Median tree depth} & Max tree depth \\
        \midrule
        $\ell_1$ & 0.00\% & 0.00\% & 14.52 & \revision{12.58} & 48.54 \\
        $\ell_3$ & 91.43\% & 28.94\% & 20.74 & \revision{18.23} & 121.46 \\
        $\ell_6$ & 87.48\% & 22.28\% & 24.92 & \revision{19.35} & 197.58 \\
        \bottomrule
    \end{tabular}
    \label{tab:option-in-tree}
\end{table}


% \begin{table}[h!]
%     \caption{Proportions of options in search tree for $\ell_3$ in selected Atari games}
%     \centering
%     \small
%     \begin{tabular}{l|rr|rr}
%         \toprule
%         Game & \% Env. & \% MCTS & \% in Tree & \% in Sim. \\
%         \midrule
%         crazy climber & 50.49\% & 71.04\% & 98.86\% & 40.19\% \\
%         freeway & 60.80\% & 86.08\% & 99.11\% & 43.83\% \\
%         hero & 18.28\% & 39.68\% & 74.43\% & 20.07\% \\
%         kung fu master & 52.53\% & 73.26\% & 99.78\% & 36.47\% \\
%         ms pacman & 34.99\% & 56.34\% & 96.90\% & 30.82\% \\
%         seaquest & 25.12\% & 44.43\% & 95.88\% & 16.88\% \\
%         \midrule
%         Average (all) & 36.64\% & 55.03\% & 91.57\% & 28.20\% \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:option-in-tree-op3}
% \end{table}

% \begin{table}[h!]
%     \caption{Proportions of options in search tree for $\ell_6$ in selected Atari games}
%     \centering
%     \small
%     \begin{tabular}{l|rr|rr}
%         \toprule
%         Game & \% Env. & \% MCTS & \% in Tree & \% in Sim. \\
%         \midrule
%         crazy climber & 39.31\% & 59.56\% & 98.53\% & 37.09\% \\
%         freeway & 54.62\% & 74.97\% & 98.75\% & 38.75\% \\
%         hero & 9.03\% & 21.01\% & 54.39\% & 8.86\% \\
%         kung fu master & 42.77\% & 67.89\% & 99.57\% & 26.49\% \\
%         ms pacman & 22.87\% & 39.34\% & 95.10\% & 21.84\% \\
%         seaquest & 13.57\% & 27.36\% & 84.94\% & 12.20\% \\
%         \midrule
%         Average (all) & 30.02\% & 48.54\% & 87.29\% & 21.68\% \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:option-in-tree-op6}
% \end{table}

Finally, we compare the tree depths of the MCTS process with and without options.
It is naturally considered that applying options provides a deeper tree, which helps in identifying longer future state sequences for better planning and avoiding pitfalls.
From the statistics, the average search tree depths generally increase as the maximum option length increases, rising by 6.22 from $\ell_1$ to $\ell_3$ and by 10.4 from $\ell_1$ to $\ell_6$.
Interestingly, there are counterexamples where the average depth decreases, such as \textit{hero}.
% for the $\ell_3$ models are generally deeper than those for the baseline, while it is still counterexamples, such as in \textit{hero}.
Although the average tree depth decreases in \textit{hero} (22.30, 17.06, and 12.15 for $\ell_1$, $\ell_3$, and $\ell_6$), the performance is improved, as shown in Table \ref{tab:Atari26-score}.
\revision{Furthermore, by comparing the median tree depth (19, 10, and 7) and maximum tree depth (50, 147, and 276) in \textit{hero}, it can be derived that the model learns to perform deep searches depending on whatever the state requires.}
Ultimately, whether to conduct a deeper or shallower search tree is learned by OptionZero automatically.
For the maximum tree depth, the baseline $\ell_1$ approaches the simulation budget of 50 nodes, meaning the search process may continuously exploit the same promising branch.
When integrating with options, although the maximum depths increase, they do not always approach the simulation budgets of 150 and 300.
The average numbers of maximum depths are 48.54, 121.46, and 192.27, equivalent to 97.08\%, 80.97\%, and 64.09\% of the budgets, reflecting that the maximum depth is converging.
% 5d: ?
% We hypothesize that convergence can be caused by the degraded environmental transitions caused by excessive unroll.
% As the training unrolls only five steps, hundreds of steps may have exceeded the capability of the dynamics network.
This observation suggests that using an option length of 3 or 6 is sufficient in Atari games.

% \begin{table}[h]
%     \caption{Tree depth for $\ell_1$, $\ell_3$, and $\ell_6$ in selected Atari games}
%     \centering
%     \small
%     \begin{tabular}{l|rr|rr|rr}
%         \toprule
%         \multirow{2}{*}{Game} & \multicolumn{2}{c|}{$\ell_1$} & \multicolumn{2}{c}{$\ell_3$} & \multicolumn{2}{c}{$\ell_6$} \\
%          & Avg & Max & Avg & Max & Avg & Max \\
%         \midrule
%         crazy climber & 17.90 & 50 & 34.81 & 147 & 61.50 & 288 \\
%         freeway & 15.03 & 48 & 27.56 & 135 & 44.50 & 210 \\
%         hero & 22.30 & 50 & 17.06 & 147 & 12.15 & 234 \\
%         kung fu master & 15.83 & 49 & 21.39 & 99 & 22.38 & 113 \\
%         ms pacman & 14.87 & 49 & 21.51 & 135 & 23.58 & 169 \\
%         seaquest & 10.94 & 49 & 12.34 & 114 & 10.45 & 115 \\
%         \midrule
%         Average (all) & 14.50 & 48.54 & 19.90 & 121.46 & 24.14 & 192.27 \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:tree-depth}
% \end{table}


% analysis: 
%   1. MCTS search with option: option count, search depth, ...
%   2. the ratio of option used in env
%   3. does the learned option converge (at which iter?) or change frequently

\section{Discussion}

This paper presents \textit{OptionZero}, a method that integrates options into the well-known MuZero algorithm.
OptionZero autonomously discovers options through self-play games and efficiently simulates environment transitions across multiple states with options during planning, which not only eliminates the requirement for obtaining options in advance but also reduces the overhead for examining consecutive states during planning.
The empirical results on Atari games demonstrate a significant improvement of 131.58\% in mean human-normalized scores, and the behavior analysis reveals that OptionZero effectively discovers options tailored to the specific challenges of each environment.
In conclusion, our findings suggest that OptionZero not only discovers options without human knowledge but also maintains efficiency during planning.
This makes OptionZero easily applicable to other applications, further extending the versatility of the MuZero algorithm.

As OptionZero builds upon MuZero, it can be easily applied to various environments.
For example, when applied to two-player games, OptionZero is expected to discover optimal strategies for both players at specific states.
In strategic games such as StarCraft, our approach can learn skillfully combined options, enhancing further explainability and facilitating human learning, as illustrated in subsection \ref{sec:behavior-analysis}.
% rebuttal TODO
% limitation: 環境不適合option, dynamics network學習
% future work: 不定義max length的情況下動態調整option長度
% 處理連續動作空間，結合sampled MuZero, reference of Sampled MuZero, S4, Dreamer
\revision{
% OptionZero also has the potential to integrate with other algorithms. In environments with continuous action spaces, such as robotic environments, the original MuZero algorithm is not applicable.
% However, Sampled MuZero, an extension of MuZero, can handle continuous action spaces by planning over sampled actions. The concept of utilizing an option network and incorporating options into MCTS could be applied to Sampled MuZero.
OptionZero can also be integrated with Sampled MuZero \cite{hubert_learning_2021} to support environments with complex action spaces, like robotic environments.
% Although OptionZero outperforms the MuZero baseline in most games, there are certain games where performance declines as the option length increases.
% This indicates that OptionZero still has some underlying limitations.
% We believe this is primarily due to the increased difficulty the dynamics network faces in learning.
% Future work could explore integrating OptionZero with other dynamics models, such as S4 or Dreamer, to address this challenge.
% Additionally, in environments with highly uncertain strategies, where identifying a dominant option with a significantly higher probability is difficult, OptionZero may not be helpful.
% Furthermore, since the option network is currently designed in a straightforward manner, it requires a predefined maximum option length.
% Enhancing the option network to dynamically allocate options of arbitrary lengths could be another promising direction for improvement.
Nevertheless, our experiments show that OptionZero does not improve performance in all games, especially in environments with numerous option types or visually complex observations, the dynamics network might struggle to learn well.
Future work could explore integrating OptionZero with other dynamics models, such as S4 \cite{gu_efficiently_2021} or Dreamer \cite{hafner_dream_2020}.
Finally, the current design of the option networks requires a predefined maximum option length.
Dynamically extending this maximum option length could be a promising direction for future work.
% Nevertheless, OptionZero does not improve performance in certain games, implying that it may not be well generalized with specific environmental characteristics.
% For example, in environments with highly uncertain strategies or less predictable future states, identifying a dominant option becomes difficult; in environments with numerous option types or visually complex observations, the dynamics network might struggle to learn well, potentially decreasing performance when longer options are utilized.
% Future work could explore integrating OptionZero with other dynamics models, such as S4 \cite{gu_efficiently_2021} or Dreamer \cite{hafner_dream_2020}.
% Furthermore, the design of the option networks requires a predefined maximum option length, which introduces additional overhead of hyperparameter tuning.
% Dynamically extending options of arbitrary lengths could be a promising direction.
}
We hope our approach and findings provide promising directions in planning with reinforcement learning for future researchers.
% % board game, more complex games, 



\section*{Ethics statement}
% Authors are encouraged to include a paragraph of Ethics Statement (at the end of the main text before references) to address potential concerns where appropriate, topics include, but are not limited to, studies that involve human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, pontential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues (e.g., IRB, documentation, research ethics). The optional ethic statement will not count toward the page limit, but should not be more than 1 page.

We do not foresee any ethical issues in this research work. All data are generated by our programs.



\section*{Reproducibility statement}
% It is important that the work published in ICLR is reproducible. Authors are strongly encouraged to include a paragraph-long Reproducibility Statement at the end of the main text (before references) to discuss the efforts that have been made to ensure reproducibility. This paragraph should not itself describe details needed for reproducing the results, but rather reference the parts of the main paper, appendix, and supplemental materials that will help with reproducibility. For example, for novel models or algorithms, a link to a anonymous downloadable source code can be submitted as supplementary materials; for theoretical results, clear explanations of any assumptions and a complete proof of the claims can be included in the appendix; for any datasets used in the experiments, a complete description of the data processing steps can be provided in the supplementary materials. Each of the above are examples of things that can be referenced in the reproducibility statement. This optional reproducibility statement will not count toward the page limit, but should not be more than 1 page.

For reproducing the work, we have provided the details of the proposed algorithm in Section \ref{sec:ozero} and Appendix \ref{appendix:implementation}, and the setup of training in Appendix \ref{appendix:experiment-setup}.
% guei: theoretical results?
% 加link
The source code, scripts for processing behavior analysis, and trained models are available at https://rlg.iis.sinica.edu.tw/papers/optionzero.
% Once accepted, we will make available the source code and the trained models used in this work, along with the training configuration files and the scripts for processing behavior analysis.

% This paper provides detailed information in Section \ref{sec:ozero}, Section \ref{sec:experiments}, and appendix to reproduce the main experimental results.
% We will release the source code and the trained models once this paper is accepted.
% 補其他計畫編號
\section*{Acknowledgement}
This research is partially supported by the National Science and Technology Council (NSTC) of the Republic of China (Taiwan) under Grant Number NSTC 113-2221-E-001-009-MY3, NSTC 113-2634-F-A49-004, and NSTC 113-2221-E-A49-127.
