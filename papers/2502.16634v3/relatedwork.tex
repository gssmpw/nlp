\section{Related works}
% alex: check 
% alex/peng: more references?
% guei: check
% 5d: todo
Numerous studies have explored the concepts of \textit{options} in reinforcement learning.
For example, \citet{dewaard_monte_2016} incorporated options from a predefined option set into Monte Carlo tree search (MCTS) and extended it to focus exploration on higher valued options during planning.
\citet{sharma_learning_2016} proposed using two policies for planning: one determines which primitive action to use, and the other determines how many times to repeat that action.
\citet{durugkar_deep_2016} explored the effects of repetition and frequency by statistics in Atari games.
\citet{vezhnevets_strategic_2016} introduced a method which learns options through end-to-end reinforcement learning.
\citet{lakshminarayanan_dynamic_2017} proposed a method that allows agents to dynamically adjust rates of repeated actions.
\citet{bacon_optioncritic_2017} derived an option-critic framework, which learns a policy over options and a policy within options.
The option policy not only determines how to select and execute an action within options but also learns when to terminate the option.
\citet{kim_lesson_2023} proposed to adaptively integrate multiple exploration strategies for options based on the option-critic framework.
\citet{riemer_role_2020} introduced a parameter-sharing approach for deep option learning.
\citet{young_iterative_2023} discovered options by learning the option policies and integrated them with a Monte Carlo search.
\citet{jinnai_finding_2019} formalized the problem of selecting the optimal option set, and produced an algorithm for discovering the suboptimal option set for planning.
\citet{veeriah_discovery_2021} proposed a meta-gradient approach for discovering reusable, task-independent options.
In addition, several works have studied subgoals, which represent a target state to achieve after several time steps, either segmented by predefined time step intervals or predicted dynamically by a learned network.
For example, \citet{gabor_subgoalbased_2019} used predefined subgoals for planning in MCTS.
\citet{czechowski_subgoal_2021} introduced a Subgoal Search method to obtain fixed-length subgoals with a low-level policy that predicts primitive actions for reaching subgoals.
% \citet{czechowski_subgoal_2021} introduced the Subgoal Search method to obtain fixed-length subgoals with a low-level policy and a value function, where the low-level policy predicts primitive actions for reaching subgoals, and the value function guides the search for a planner.
\citet{kujanpaa_hierarchical_2023} proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data.
% \citet{kujanpaa_hierarchical_2023} proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data, and employed VQ-VAE \cite{vandenoord_neural_2017} to store subgoals in a codebook.
\citet{kujanpaa_hybrid_2024} extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that subgoals are reachable.
% \citet{kujanpaa_hybrid_2024} extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that all goal states are reachable during the search process.
In summary, these previous works either adopt predefined options, learn subgoals from expert data, or do not incorporate options in MCTS planning.
Compared to these works, our goal is to automatically discover options without relying on predefined options or expert data and to use options during planning.

%