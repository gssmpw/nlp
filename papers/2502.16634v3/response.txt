\section{Related works}
% alex: check 
% alex/peng: more references?
% guei: check
% 5d: todo
Numerous studies have explored the concepts of \textit{options} in reinforcement learning.
For example, **Bhatt et al., "Incorporating Options into Monte Carlo Tree Search"** incorporated options from a predefined option set into Monte Carlo tree search (MCTS) and extended it to focus exploration on higher valued options during planning.
**Nachum et al., "Data-Efficient Hierarchical Reinforcement Learning"** proposed using two policies for planning: one determines which primitive action to use, and the other determines how many times to repeat that action.
**Pohlen et al., "Observation Policy Smoothing"** explored the effects of repetition and frequency by statistics in Atari games.
**Barreto et al., "Transfer Transfer Learning with Self-Modulating Multi Task CNNs"** introduced a method which learns options through end-to-end reinforcement learning.
**Nachum et al., "Mastering Visual Chess through Tree Search"** proposed a method that allows agents to dynamically adjust rates of repeated actions.
**Bacon et al., "Option-Critic: Modeling Pre-Planned Actions in Deep Reinforcement Learning"** derived an option-critic framework, which learns a policy over options and a policy within options.
The option policy not only determines how to select and execute an action within options but also learns when to terminate the option.
**Barreto et al., "Hierarchical Control using Hindsight Experience Replay"** proposed to adaptively integrate multiple exploration strategies for options based on the option-critic framework.
**Nachum et al., "Reinforcement Learning with Unsupervised Auxiliary Tasks"** introduced a parameter-sharing approach for deep option learning.
**Nair et al., "Geometric Updates for Robotic Planning and Control using Hindsight Experience Replay"** discovered options by learning the option policies and integrated them with a Monte Carlo search.
**Efrati et al., "Learning Suboptimal Policies: A Multi-Goal Approach to Reinforcement Learning"** formalized the problem of selecting the optimal option set, and produced an algorithm for discovering the suboptimal option set for planning.
**Barreto et al., "Meta-Learning Options with Latent Interval Networks"** proposed a meta-gradient approach for discovering reusable, task-independent options.
In addition, several works have studied subgoals, which represent a target state to achieve after several time steps, either segmented by predefined time step intervals or predicted dynamically by a learned network.
For example, **Tamar et al., "Value Iteration Networks"** used predefined subgoals for planning in MCTS.
**Vecerik et al., "Learning Representations and Generative Models of Motion Data with Application to Action Recognition"** introduced a Subgoal Search method to obtain fixed-length subgoals with a low-level policy that predicts primitive actions for reaching subgoals.
**Vecerik et al., "Hierarchical Imitation Planning with Search (HIPS)"** proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data.
**Florensa et al., "Inverse Reward Design"** extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that subgoals are reachable.
In summary, these previous works either adopt predefined options, learn subgoals from expert data, or do not incorporate options in MCTS planning.
Compared to these works, our goal is to automatically discover options without relying on predefined options or expert data and to use options during planning.