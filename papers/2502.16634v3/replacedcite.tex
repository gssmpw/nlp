\section{Related works}
% alex: check 
% alex/peng: more references?
% guei: check
% 5d: todo
Numerous studies have explored the concepts of \textit{options} in reinforcement learning.
For example, ____ incorporated options from a predefined option set into Monte Carlo tree search (MCTS) and extended it to focus exploration on higher valued options during planning.
____ proposed using two policies for planning: one determines which primitive action to use, and the other determines how many times to repeat that action.
____ explored the effects of repetition and frequency by statistics in Atari games.
____ introduced a method which learns options through end-to-end reinforcement learning.
____ proposed a method that allows agents to dynamically adjust rates of repeated actions.
____ derived an option-critic framework, which learns a policy over options and a policy within options.
The option policy not only determines how to select and execute an action within options but also learns when to terminate the option.
____ proposed to adaptively integrate multiple exploration strategies for options based on the option-critic framework.
____ introduced a parameter-sharing approach for deep option learning.
____ discovered options by learning the option policies and integrated them with a Monte Carlo search.
____ formalized the problem of selecting the optimal option set, and produced an algorithm for discovering the suboptimal option set for planning.
____ proposed a meta-gradient approach for discovering reusable, task-independent options.
In addition, several works have studied subgoals, which represent a target state to achieve after several time steps, either segmented by predefined time step intervals or predicted dynamically by a learned network.
For example, ____ used predefined subgoals for planning in MCTS.
____ introduced a Subgoal Search method to obtain fixed-length subgoals with a low-level policy that predicts primitive actions for reaching subgoals.
% ____ introduced the Subgoal Search method to obtain fixed-length subgoals with a low-level policy and a value function, where the low-level policy predicts primitive actions for reaching subgoals, and the value function guides the search for a planner.
____ proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data.
% ____ proposed Hierarchical Imitation Planning with Search (HIPS), which learns subgoals from expert demonstration data, and employed VQ-VAE ____ to store subgoals in a codebook.
____ extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that subgoals are reachable.
% ____ extended HIPS to HIPS-$\epsilon$, adding a low-level (primitive action) search to the high-level (subgoal) search, guaranteeing that all goal states are reachable during the search process.
In summary, these previous works either adopt predefined options, learn subgoals from expert data, or do not incorporate options in MCTS planning.
Compared to these works, our goal is to automatically discover options without relying on predefined options or expert data and to use options during planning.

%