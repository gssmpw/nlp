\section{Related Work}
\paragraph{\textbf{Style Transfer.}}
% % InST, StyleDiffusion, DiffStyle,  Zero-shot contrastive loss for text-guided diffusion image style transfer DEA-Diff, 
% Neural style transfer is an example-guided image generation task that transfers the style of one image onto another while retaining the content
% of the original. 近年来，越来越多的工作采用diffusion model来实现 Neural Style Transfer。这些方法主要分为两个类别，tuning-based和tuning-free。作为代表工作，InST introduced a textual inversion-based approach, aiming to
% map a given style into corresponding textual embeddings，然而该方法对Style的反转并不准确，在应用反转后的style进行迁移，会改变原始图像的内容。StyleDiffusion [45] aimed to disentangle style and content by introducing CLIP-based style disentanglement loss for fine-tuning DM for style transfer. FreeTuner提出了compositional personalization，能够composing different subject and style concepts，然而组合式的定制化often overlooking the
% preservation of the content image’s structural integrity.

% tuning-free的方法能够在single-forward 过程中实现风格迁移，而无需微调。
% DEADiff [12] stands out by extracting
% disentangled representations of content and style using a paired dataset, facilitated by the Q-Former technique. InstantStyle [20], a recent innovation, employs
% block-specific injection techniques to implicitly achieve a decoupling of content
% and style, offering a nuanced approach to style transfer
% StyleID [1] operates by manipulating self-attention layers, proposing innovative techniques such as query preservation and initial latent AdaIN to maintain the integrity of the content

% 在这篇论文中，我们的目的是提出一个基于单图微调的高效style迁移方法，能够消除严重的过拟合问题，实现style的精准反转和无缝的风格迁移而保持原始的内容完整。

% % DiffStyle，DEADiff，StyleID，InstantStyle，InstantStyle-Plus，
Style transfer \cite{zhang2019multimodal,wang2020diversified,wang2020collaborative,park2019arbitrary,lu2019closed,li2018closed,li2017universal,lai2017deep,gatys2016image} applies the artistic style of one image to another while preserving the latter's content and structure. Early methods \cite{zhang2013style,wang2004efficient} relied on handcrafted features, while CNN-based approaches \cite{gatys2016image, gatys2017controlling, kolkin2019style} leveraged pre-trained networks to capture style pattern. Arbitrary style transfer methods \cite{deng2020arbitrary,huang2017arbitrary,liao2017visual,zhang2022exact} further improved this by using unified feed-forward models for flexible inputs.




In recent years, diffusion models have increasingly been employed for style transfer. These methods can be broadly categorized into two types: tuning-based methods and tuning-free methods. A representative example of the former is InST \cite{zhang2023inversion}, which introduces a text inversion-based approach, aiming to map a given style to its corresponding text token. 
% However, this method lacks precision in style inversion, often altering the content of the original image during style transfer. 
StyleDiffusion \cite{wang2023stylediffusion} refines diffusion models by introducing a CLIP-based style decoupling loss, effectively separating style from content. 
% FreeTuner \cite{xu2024freetuner} proposes compositional personalization, allowing the combination of different subject and style concepts. However, this approach often alters the structure and content of the original image.
Tuning-free methods achieve style transfer in a single forward process without model fine-tuning. DEADiff \cite{qi2024deadiff} utilizes the Q-Former \cite{li2023blip} and paired datasets to extract decoupled representations of content and style, facilitating style transfer. InstantStyle \cite{wang2024instantstyle} uses specific block injection techniques to implicitly decouple content and style for effective style transfer. Building on this, InstantStyle-Plus \cite{wang2024instantstyleplus} introduces ControlNet \cite{zhang2023adding} to further maintain the integrity of image content. StyleID \cite{chung2024style} adjusts self-attention layers and introduces novel techniques such as query preservation and initial latent AdaIN to maintain content integrity.

Nevertheless, these approaches struggle with achieving signature style transfer. In this paper, we address this challenge by utilizing a personalized text-to-image model to perform signature style transfer, generating visually compelling and aesthetically enhanced outputs.  


\paragraph{\textbf{Personalized Text-to-Image Generation.}} 

Recent studies \cite{ruiz2023dreambooth, gal2022image, ruiz2023hyperdreambooth, kumari2023multi} have increasingly focused on using visual exemplars as a foundation for image generation to address the inherent ambiguity and unpredictability of text-based prompts. This approach involves collecting multiple reference images to fine-tune diffusion models. 
However, for style transfer tasks that rely on a single style image, the aforementioned methods often cause severe overfitting when fine-tuning on this single image, significantly diminishing the effectiveness of the style transfer.
In contrast, our proposed hypernetwork-powered style aware fine-tuning method can overcome the limitations associated with fine-tuning on a single style image. It enables precise inversion of style attributes without the drawbacks of overfitting
% \yilin{will single image based personalized method work?}

% In addition to customizing the general essence of the reference image, some works have begun to explore attribute-level T2I customization \cite{voynov2023p+, zhang2023prospect, goel2023pair, yang2023paint} . However, these methods either require multiple reference images \cite{voynov2023p+} or extensive fine-tuning \cite{goel2023pair,yang2023paint, ye2023ip} across large datasets. 




\paragraph{\textbf{Parameter Efficient Fine Tuning (PEFT).}} PEFT represents an innovative approach in the refinement of deep learning models, emphasizing the adjustment of a subset of parameters rather than the entire model. These parameters are identified as either specific subsets from the originally trained model or a minimal number of newly introduced parameters during the fine-tuning phase. PEFT has been applied in text-to-image diffusion models \cite{saharia2022photorealistic,rombach2022high} through techniques such as LoRA \cite{ryu2023low} and adapter tuning \cite{mou2023t2i,ye2023ip,wei2023elite,chen2024subject,ma2023unified}. In this paper, we leverage a hypernetwork to adjust and refine a unique subset of pre-trained parameters.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{imgs/pipeline.pdf}
    \caption{The SigStyle framework. First, given a style image, we perform hypernetwork-powered style-aware fine-tuning for style inversion and represent the reference style as a special token * (see Figure \ref{fig:method}.a). In Figure \ref{fig:method}.b, the upper branch represents the reconstruction process of the content image, while the lower branch represents the generation process of the target image. When generating the target image using a pre-trained model and target text, we first use DDIM Inversion to map the content image into noise latents, which are then copied as the initial noise for generating the target image. Then, we adopt time-aware attention swapping to inject structural and content information during the first $k$ steps of the denoising process (see Figure b). In the subsequent $T-k$ steps, we proceed with the usual denoising process without any swapping. Finally, by decoding with VAE, we obtain the style-transferred image. }
    \label{fig:method}
\end{figure*}