[
  {
    "index": 0,
    "papers": [
      {
        "key": "Oorloff_2023_ICCV",
        "author": "Oorloff, Trevine and Yacoob, Yaser",
        "title": "Expressive Talking Head Video Encoding in StyleGAN2 Latent Space"
      },
      {
        "key": "Bounareli_2023_ICCV",
        "author": "Bounareli, Stella and Tzelepis, Christos and Argyriou, Vasileios and Patras, Ioannis and Tzimiropoulos, Georgios",
        "title": "HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "tao2023learning",
        "author": "Jiale Tao and Shuhang Gu and Wen Li and Lixin Duan",
        "title": "Learning Motion Refinement for Unsupervised Face Animation"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Ni_2023_CVPR",
        "author": "Ni, Haomiao and Shi, Changhao and Li, Kai and Huang, Sharon X. and Min, Martin Renqiang",
        "title": "Conditional Image-to-Video Generation With Latent Flow Diffusion Models"
      },
      {
        "key": "Pang_2023_CVPR",
        "author": "Pang, Youxin and Zhang, Yong and Quan, Weize and Fan, Yanbo and Cun, Xiaodong and Shan, Ying and Yan, Dong-Ming",
        "title": "DPE: Disentanglement of Pose and Expression for General Video Portrait Editing"
      },
      {
        "key": "wang2022latent",
        "author": "Wang, Yaohui and Yang, Di and Bremond, Francois and Dantcheva, Antitza",
        "title": "Latent Image Animator: Learning to Animate Images via Latent Space Navigation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "ji2021audio-driven",
        "author": "Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Wayne and Loy, Chen Change and Cao, Xun and Xu, Feng",
        "title": " Audio-Driven Emotional Video Portraits"
      },
      {
        "key": "textbasedediting",
        "author": "Fried, Ohad and Tewari, Ayush and Zollh\\\"{o}fer, Michael and Finkelstein, Adam and Shechtman, Eli and Goldman, Dan B and Genova, Kyle and Jin, Zeyu and Theobalt, Christian and Agrawala, Maneesh",
        "title": "Text-based editing of talking-head video"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Zhou2021Pose",
        "author": "Zhou, Hang and Sun, Yasheng and Wu, Wayne and Loy, Chen Change and Wang, Xiaogang and Liu, Ziwei",
        "title": "Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation"
      },
      {
        "key": "Wang_2023_CVPR",
        "author": "Wang, Duomin and Deng, Yu and Yin, Zixin and Shum, Heung-Yeung and Wang, Baoyuan",
        "title": "Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "li2024ae",
        "author": "Li, Dongze and Zhao, Kang and Wang, Wei and Peng, Bo and Zhang, Yingya and Dong, Jing and Tan, Tieniu",
        "title": "Ae-nerf: Audio enhanced neural radiance field for few shot talking head synthesis"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "gururani2022SPACE",
        "author": "Siddharth Gururani and Arun Mallya and Ting-Chun Wang and Rafael Valle and Ming-Yu Liu",
        "title": "{SPACE: Speech-driven Portrait Animation with Controllable Expression}"
      },
      {
        "key": "wang2021audio2head",
        "author": "Suzhen, Wang and Lincheng, Li and Yu, Ding and Changjie, Fan and Xin, Yu",
        "title": "Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "ma2023styletalk",
        "author": "Ma, Yifeng and Wang, Suzhen and Hu, Zhipeng and Fan, Changjie and Lv, Tangjie and Ding, Yu and Deng, Zhidong and Yu, Xin",
        "title": "Styletalk: One-shot talking head generation with controllable speaking styles"
      },
      {
        "key": "zhang2023sadtalker",
        "author": "Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei",
        "title": "SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "10.1145/3414685.3417774",
        "author": "Zhou, Yang and Han, Xintong and Shechtman, Eli and Echevarria, Jose and Kalogerakis, Evangelos and Li, Dingzeyu",
        "title": "MakeltTalk: speaker-aware talking-head animation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wav2lips",
        "author": "Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.",
        "title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024eat",
        "author": "Wang, Haodi and Jia, Xiaojun and Cao, Xiaochun",
        "title": "EAT-Face: Emotion-Controllable Audio-Driven Talking Face Generation via Diffusion Model"
      },
      {
        "key": "yu2023talking",
        "author": "Yu, Zhentao and Yin, Zixin and Zhou, Deyu and Wang, Duomin and Wong, Finn and Wang, Baoyuan",
        "title": "Talking head generation with probabilistic audio-to-visual diffusion priors"
      },
      {
        "key": "shen2023difftalk",
        "author": "Shen, Shuai and Zhao, Wenliang and Meng, Zibin and Li, Wanhua and Zhu, Zheng and Zhou, Jie and Lu, Jiwen",
        "title": "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sun2024diffposetalk",
        "author": "Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-jin",
        "title": "Diffposetalk: Speech-driven stylistic 3d facial animation and head pose generation via diffusion models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "wav2lips",
        "author": "Prajwal, K R and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P. and Jawahar, C.V.",
        "title": "A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ma2023styletalk",
        "author": "Ma, Yifeng and Wang, Suzhen and Hu, Zhipeng and Fan, Changjie and Lv, Tangjie and Ding, Yu and Deng, Zhidong and Yu, Xin",
        "title": "Styletalk: One-shot talking head generation with controllable speaking styles"
      },
      {
        "key": "ma2023dreamtalk",
        "author": "Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong",
        "title": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "thambiraja2023imitator",
        "author": "Thambiraja, Balamurugan and Habibie, Ikhsanul and Aliakbarian, Sadegh and Cosker, Darren and Theobalt, Christian and Thies, Justus",
        "title": "Imitator: Personalized speech-driven 3d facial animation"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "gururani2023space",
        "author": "Gururani, Siddharth and Mallya, Arun and Wang, Ting-Chun and Valle, Rafael and Liu, Ming-Yu",
        "title": "SPACE: Speech-driven Portrait Animation with Controllable Expression"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "schneider2019wav2vec",
        "author": "Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli",
        "title": "wav2vec: Unsupervised Pre-training for Speech Recognition"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "ma2023styletalk",
        "author": "Ma, Yifeng and Wang, Suzhen and Hu, Zhipeng and Fan, Changjie and Lv, Tangjie and Ding, Yu and Deng, Zhidong and Yu, Xin",
        "title": "Styletalk: One-shot talking head generation with controllable speaking styles"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "pmlr-v37-sohl-dickstein15",
        "author": "Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya",
        "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"
      },
      {
        "key": "ho2020denoising",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "ho2020denoising",
        "author": "Ho, Jonathan and Jain, Ajay and Abbeel, Pieter",
        "title": "Denoising diffusion probabilistic models"
      },
      {
        "key": "zhou2023shifted",
        "author": "Zhou, Yufan and Liu, Bingchen and Zhu, Yizhe and Yang, Xiao and Chen, Changyou and Xu, Jinhui",
        "title": "Shifted diffusion for text-to-image generation"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "harvey2022flexible",
        "author": "Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank",
        "title": "Flexible diffusion modeling of long videos"
      },
      {
        "key": "wu2023tune",
        "author": "Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng",
        "title": "Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "huang2023collaborative",
        "author": "Huang, Ziqi and Chan, Kelvin CK and Jiang, Yuming and Liu, Ziwei",
        "title": "Collaborative diffusion for multi-modal face generation and editing"
      },
      {
        "key": "kim2023dcface",
        "author": "Kim, Minchul and Liu, Feng and Jain, Anil and Liu, Xiaoming",
        "title": "DCFace: Synthetic Face Generation with Dual Condition Diffusion Model"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "stypulkowski2024diffused",
        "author": "Stypu{\\l}kowski, Micha{\\l} and Vougioukas, Konstantinos and He, Sen and Zieba, Maciej and Petridis, Stavros and Pantic, Maja",
        "title": "Diffused heads: Diffusion models beat gans on talking-face generation"
      },
      {
        "key": "du2023dae",
        "author": "Du, Chenpeng and Chen, Qi and He, Tianyu and Tan, Xu and Chen, Xie and Yu, Kai and Zhao, Sheng and Bian, Jiang",
        "title": "Dae-talker: High fidelity speech-driven talking face generation with diffusion autoencoder"
      }
    ]
  }
]