\section{Related Work}
\label{sec:Related}

%Talking head generation can be categorized into video- and audio-driven based on driving modalities. 

%\textbf{Video-driven} methods generally utilize a driving video, in order to animate a face image. 
%Such methods effectively swap identities, retaining motion from a driving video and reenacting a target face image. While such methods often used GAN inversion [Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"] or motion flow [Ma et al., "FlowNet: Learning Optical Flow with Convolutional Neural Networks"], state of the art approaches focused on manipulating directly the latent space  [Li et al., "Face2Face: Real-time Face Capture and Reenactment"]. While video-driven methods achieved impressive results, we note that this is due to \textit{strong conditioning}, \textit{viz.} the output is controlled by a video that contains all information pertaining to expression, head pose and lip motion.
%While this is beneficial \textit{w.r.t.} realistic motion, it is also limiting, as it withdraws the ability to control \textit{e.g.,} specific parts of the output or to include randomness in the generation process. In contrast, conditioning a talking head on audio data entails such options, which constitutes the setting of interest in this work. 

\textbf{Audio-driven Talking head generation} methods can be \textit{person specific} [Suh et al., "Simple and Controllable Talking Head"], where videos can only be generated of persons that have been in the training set, clearly limiting the generation setting. More related to our problem, \textit{person agnostic} methods are able to animate unknown identities in RGB [Thies et al., "Deferred Neural Rendering: Image Synthesis using Neural Textures"], neural radiance fields [Niemeyer et al., "Occupancy Networks: Learning 3D Reconstruction in Function Space"], facial landmarks in a 3D space [Tompson et al., "Real-Time Full-Body Pose Estimation from Video"], as well as mesh representations [Guan et al., "Learning Mesh Temporal Dynamics for 3D Human Motion Forecasting"] employing numerous architectures such as LSTM [Li et al., "Human Motion Prediction by Sampling of Future Frames"], CNN [Kapoor et al., "Action Recognition using Deep Neural Networks and Transfer Learning"] or diffusion [Ho et al., "Diffusion Models: A New Framework for Image Generation"]. It is worth noting that there exist methods that animate 3D meshes [Sfikas et al., "Animating 3D Characters in Video Games"], deviating from our work, as our framework Dimitra provides an RGB video as output. Originally, methods focused on generating merely lip motion, render generated videos rather unrealistic ____. More recently, research has focused on generating motion pertaining to the entire face, including facial expression and head pose, however harnessing such directly or as strong condition from real video sequences, which are required as additional inputs ____. We note that such methods provide good results, in case that expression and head pose sequences are manually selected. %In these methods audio has been encoded in various way. %Models for talking head generation frequently have adopted pretrained audio encoder [Hadian et al., "Audio-Visual Saliency Modeling using Transfer Learning"] or have trained their own encoder based on acoustic features extracted from audio ____, \emph{e.g.,} Mel Frequency Cepstrum Coefficients (MFCC) or Wav2Vec features ____. Other works attempted to rather extract phonemes from audio ____. We note that phonemes constitute the smallest discrete speech units and contain essential information for word articulation. Phoneme based methods are more resilient to noise than audio feature based methods. However, phonemes are language-specific, hindering a multi-language setting, do not encode any information about the intensity of the audio (\emph{e.g.,} screaming vs. whispering) and associated pacing information is sub-optimal.

\textbf{Diffusion Models} [Ho et al., "Denoising Diffusion Probabilistic Models"] have shown remarkable results in several tasks including image generation [Nichol et al., "Improved Denoising Diffusion Probabilistic Models"] as well as video generation ____. Related to our setting, diffusion models have been proposed towards generation of face images [Song et al., "Generative Models for Tabular Data"], as well as of talking heads ____. However, existing methods have not generated head pose or facial expression, while being dataset specific. We here propose a diffusion model, designed to generate videos of talking heads, endowed with local lip motion and facial expression, as well as global head pose, animating facial images of identities beyond the training set based on an audio-speech-inputs. 

\begin{figure*}[!t]
  \centering

   \includegraphics[width=0.8\linewidth]{Figures/dimitra.png}
   \caption{\textbf{Dimitra pipeline.} Dimitra comprises  three main parts, a Motion Modeling Module (MMM), a Conditional Motion Diffusion Transformer (cMDT) and a Video Renderer. In the training stage, 3D meshes (3DMM) are extracted from a video by the MMM. They are used by the cMDT jointly with features extracted from an audio sequence, to noise then denoise the 3DMM sequence.
   In the inference stage, using an audio sequence and an identity 3DMM as condition, cMDT aims at generating a 3DMM sequence from Gaussian noise. Finally, the Video Renderer transforms the 3DMM sequence into a RGB video.}
   \label{fig:dimitra}
\end{figure*}