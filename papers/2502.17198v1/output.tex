%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8
%
% Slightly modified by Shaun Canavan for FG2025
%

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper
\usepackage{FG2025}
\usepackage{amsmath,amsfonts}
\usepackage{graphics}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{booktabs}
\usepackage{amsmath}                 
\usepackage{amssymb}
\usepackage{balance}
\usepackage{xcolor} 
\newcommand{\blue}[1]{\textcolor{blue}{\textbf{#1}}}

\FGfinalcopy % *** Uncomment this line for the final submission



\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\def\FGPaperID{42} % *** Enter the FG2025 Paper ID here

\title{\LARGE \bf
Dimitra: Audio-driven Diffusion model for 

Expressive Talking Head Generation}

%use this in case of a single affiliation
%\author{\parbox{16cm}{\centering
%    {\large Huibert Kwakernaak}\\
%    {\normalsize
%    Faculty of Electrical Engineering, Mathematics and Computer Science, University of Twente, Enschede, The Netherlands\\}}
%    \thanks{This work was not supported by any organization.}% <-this % stops a space
%}

%use this in case of several affiliations
\author{\parbox{16cm}{\centering
    {\large Baptiste Chopin$^1$ Tashvik Dhamija$^1$ Pranav Balaji$^1$ Yaohui Wang$^2$ Antitza Dantcheva$^1$}\\
    {\normalsize
    $^1$ Université Côte d’Azur, Inria, STARS Team, France\\
    $^2$ Shanghai Artificial Intelligence Laboratory, China}}
    %\thanks{This work was not supported by any organization}% <-this % stops a space
}

\begin{document}

\ifFGfinal
\thispagestyle{empty}
\pagestyle{empty}
\else
\author{Anonymous FG2025 submission\\ Paper ID \FGPaperID \\}
\pagestyle{plain}
\fi
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

 We propose Dimitra, a novel framework for audio-driven talking head generation, streamlined to learn lip motion, facial expression, as well as head pose motion. Specifically, we train a conditional Motion Diffusion Transformer (cMDT) by modeling facial motion sequences with 3D representation. We condition the cMDT with only two input signals, an audio-sequence, as well as a reference facial image. By extracting additional features directly from audio, Dimitra is able to increase quality and realism of generated videos. In particular, phoneme sequences contribute to the realism of lip motion, whereas text transcript to facial expression and head pose realism. 
Quantitative and qualitative experiments on two widely employed datasets, VoxCeleb2 and HDTF, showcase that Dimitra is able to outperform existing approaches for generating realistic talking heads imparting lip motion, facial expression, and head pose. 
\end{abstract}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:intro}

Talking head generation aims at animating face images, placing emphasis on generation of realistic appearance and motion. The latter has been enabled by the rapid progress of generative models. Related results have sparked attention in domains of application including digital humans, AR/VR, as well as film-making. While \textit{video-driven} talking head generation has become highly realistic~\cite{siarohin2019first,wang2022latent,zhao2022thin}, animation driven by \textit{audio-speech} allows for additional applications such as video games and chat-bots. 
Audio-driven talking head generation models \cite{hong2022depth,li2024ae,wav2lips} entail the animation of a face image by synchronizing the audio-speech to lip motion.  
Hence, related work~\cite{wav2lips,guan2023stylesync} predominantly focuses on generating \textit{lip motion}.  
However, it is only when \textit{head pose} and \textit{facial expressions} are animated that talking heads appear realistic, as such facial behavior is crucial in \textit{human communication}. 
Motivated by this, most recent methods~\cite{ma2023styletalk,ye2024real3d} attempt to incorporate such facial behavior. 
Such methods have primarily utilized \textit{additional existing video sequences}, in order to condition the generation of \textit{facial expression}. \textit{W.r.t.} head pose, related movement has been mainly directly copied and transferred from real video sequences, which might initially appear realistic. However this is limited, as head pose sequences might be of different length, and more importantly, will not be in accordance with the speech to the generated video, resulting in unnatural motion. 

\noindent Deviating from the above, in this work, we introduce Dimitra, a novel framework for audio-driven talking head generation, streamlined to animate a face image \textit{locally and globally} based on audio speech. Specifically, we place emphasis on generating natural and diverse face motion and appearance by \textit{learning} intrinsically \textit{behavior of talking faces} that includes motion of lips, head pose, as well as facial expressions - directly \textit{from an audio input}. Towards this, we propose a conditional Motion Diffusion Transformer (cMDT), which accepts a reference facial image, as well as an audio sequence as inputs. The latter 
contributes to (i) Wav2Vec \cite{schneider2019wav2vec} features, (ii) text transcript of the audio-speech, as well as (iii) phoneme sequences that we then employ as input of the following network. In particular, the latter utilizes an intermediate 3D mesh  representation that facilitates generation of facial motion, namely 3DMM \cite{deng2019accurate}. This is beneficial in reducing the number of parameters of Dimitra and improving head motion in the 3D space. In addition, 3DMMs allow for flexibility \textit{w.r.t.} image resolution, as we are able to simply alter the final video renderer.% Finally, 3DMM enable a conversion to other 3D mesh formats, \emph{e.g.,} such employed for virtual avatars. 


\noindent Our main contributions include the following.
\begin{itemize}
    \item We introduce a novel audio-driven talking head generation model, referred to as Dimitra, which generates motion pertained to lips, expression, as well as head pose in a \textit{reference facial image} based on a \textit{single audio sequence}. We extract multiple features from the audio sequence, conditioning the generation of talking head videos. Deviating from previous methods, we locally animate the mouth, as well as globally the entire face by generating facial expressions and head pose - without additional inputs. Such facial behavior is merely extracted and \textit{learned from audio sequences}. 
    \item We conducted extensive experiments that quantitatively and qualitatively demonstrate that videos generated by Dimitra are realistic, imparting expressive and natural motion.% in contrast to state-of-the-art.
    %\item We provide clear and detailed training and testing protocols for VoxCeleb2 and HDTF in the context of audio-driven talking head generation. This is instrumental in future research to conduct fair comparison with state of the art. We also analyze commonly used metrics for talking head generation and conclude that those are not adequate in evaluating the quality of generated video quality. 
\end{itemize}

\section{Related Work}
\label{sec:Related}

%Talking head generation can be categorized into video- and audio-driven based on driving modalities. 

%\textbf{Video-driven} methods generally utilize a driving video, in order to animate a face image. 
%Such methods effectively swap identities, retaining motion from a driving video and reenacting a target face image. While such methods often used GAN inversion \cite{Oorloff_2023_ICCV,Bounareli_2023_ICCV} or motion flow \cite{tao2023learning}, state of the art approaches focused on manipulating directly the latent space  \cite{Ni_2023_CVPR,Pang_2023_CVPR,wang2022latent}. While video-driven methods achieved impressive results, we note that this is due to \textit{strong conditioning}, \textit{viz.} the output is controlled by a video that contains all information pertaining to expression, head pose and lip motion.
%While this is beneficial \textit{w.r.t.} realistic motion, it is also limiting, as it withdraws the ability to control \textit{e.g.,} specific parts of the output or to include randomness in the generation process. In contrast, conditioning a talking head on audio data entails such options, which constitutes the setting of interest in this work. 

\textbf{Audio-driven Talking head generation} methods can be \textit{person specific} \cite{ji2021audio-driven,textbasedediting}, where videos can only be generated of persons that have been in the training set, clearly limiting the generation setting. More related to our problem, \textit{person agnostic} methods are able to animate unknown identities in RGB \cite{Zhou2021Pose,Wang_2023_CVPR}, neural radiance fields \cite{li2024ae}, facial landmarks in a 3D space \cite{gururani2022SPACE,wang2021audio2head}, as well as mesh representations \cite{ma2023styletalk,zhang2023sadtalker} employing numerous architectures such as LSTM \cite{10.1145/3414685.3417774}, CNN \cite{wav2lips} or diffusion \cite{wang2024eat,yu2023talking,shen2023difftalk}. It is worth noting that there exist methods that animate 3D meshes \cite{sun2024diffposetalk}, deviating from our work, as our framework Dimitra provides an RGB video as output. Originally, methods focused on generating merely lip motion, render generated videos rather unrealistic \cite{wav2lips}. More recently, research has focused on generating motion pertaining to the entire face, including facial expression and head pose, however harnessing such directly or as strong condition from real video sequences, which are required as additional inputs \cite{ma2023styletalk,ma2023dreamtalk}. We note that such methods provide good results, in case that expression and head pose sequences are manually selected. %In these methods audio has been encoded in various way. %Models for talking head generation frequently have adopted pretrained audio encoder \cite{thambiraja2023imitator} or have trained their own encoder based on acoustic features extracted from audio \cite{gururani2023space} \emph{e.g.,} Mel Frequency Cepstrum Coefficients (MFCC) or Wav2Vec features \cite{schneider2019wav2vec}. Other works attempted to rather extract phonemes from audio \cite{ma2023styletalk}. We note that phonemes constitute the smallest discrete speech units and contain essential information for word articulation. Phoneme based methods are more resilient to noise than audio feature based methods. However, phonemes are language-specific, hindering a multi-language setting, do not encode any information about the intensity of the audio (\emph{e.g.,} screaming vs. whispering) and associated pacing information is sub-optimal.

\textbf{Diffusion Models}~\cite{pmlr-v37-sohl-dickstein15,ho2020denoising} have shown remarkable results in several tasks including image generation~\cite{ho2020denoising,zhou2023shifted} as well as video generation \cite{harvey2022flexible,wu2023tune}. Related to our setting, diffusion models have been proposed towards generation of face images \cite{huang2023collaborative,kim2023dcface}, as well as of talking heads \cite{stypulkowski2024diffused,du2023dae}. However, existing methods have not generated head pose or facial expression, while being dataset specific. We here propose a diffusion model, designed to generate videos of talking heads, endowed with local lip motion and facial expression, as well as global head pose, animating facial images of identities beyond the training set based on an audio-speech-inputs. 

\begin{figure*}[!t]
  \centering

   \includegraphics[width=0.8\linewidth]{Figures/dimitra.png}
   \caption{\textbf{Dimitra pipeline.} Dimitra comprises  three main parts, a Motion Modeling Module (MMM), a Conditional Motion Diffusion Transformer (cMDT) and a Video Renderer. In the training stage, 3D meshes (3DMM) are extracted from a video by the MMM. They are used by the cMDT jointly with features extracted from an audio sequence, to noise then denoise the 3DMM sequence.
   In the inference stage, using an audio sequence and an identity 3DMM as condition, cMDT aims at generating a 3DMM sequence from Gaussian noise. Finally, the Video Renderer transforms the 3DMM sequence into a RGB video.}
   \label{fig:dimitra}
\end{figure*}


\section{Method}
\label{method}

In this work, our goal is to generate RGB videos, where a reference facial image has been animated based on a provided audio-speech input. 
Towards this, we introduce Dimitra, comprising of three parts (see Fig.~\ref{fig:dimitra}), namely a Motion Modeling Module (MMM), a Conditional Motion Diffusion Transformer (cMDT) and a Video Renderer. While the MMM extracts motion features from the RGB image using 3D mesh, the cMDT extracts relevant features from audio that jointly, with the 3D mesh, are targeted to generate a 3D mesh sequence of facial motion. Finally, the Video Renderer converts the meshes into the RGB space, in order to create an RGB video. We proceed to elaborate below.


\subsection{Motion Modeling Module}
\label{MMM}
While we seek to generate RGB videos, recent work \cite{zhang2023sadtalker} has showcased that generating mesh representations allows for increased %extracted from videos are superior to RGB videos, due to better 
control of generated motion in the 3D space that contributes to realistic generation results that are highly limiting parameters in the network. %  and sequence generated by taking the 3D information into account gain in realness. For these reasons 
Motivated by this, we here adopt a facial mesh representation, namely the 3DMM representation proposed by Deng \emph{et al.} \cite{deng2019accurate}.
Firstly, we encode the RGB data into 3D meshes by adopting an encoder \cite{deng2019accurate}. % endowed with 257 parameters, representing a mesh. We select this encoder for its resilience  \emph{w.r.t.} resolution, effectiveness and ability to be easily converted into other formats.
The parameters encode texture, identity, luminosity,  and importantly 64 parameters, encoding facial landmarks and 6 parameters representing global head pose (rotation and translation). In the following, we only refer to these 64+6 parameters as facial features. We note that out of the 64 facial parameters, 13 encode the mouth region.
Formally, from a video, we obtain a 3DMM sequence $x^{1:N}={x^1,...,x^N}$ with $x^i \in \mathbb{R}^{d}$ denoting a single 3DMM frame containing $d$ motion parameters and $N$ frames in the sequence.
\subsection{Conditional Motion Diffusion Transformer (cMDT)}

We formulate talking head generation as a reverse diffusion process and propose a Conditional Motion Diffusion Transformer (cMDT) to learn this process. The cMDT takes an audio sequence and the 3DMM representation of a face as an input (Sec. \ref{MMM}) towards generating a 3DMM sequence of facial motion. The cMDT can be divided into Audio Feature Extractor, Motion Transformer and Video Renderer, three parts.
\subsubsection{Diffusion background}

Denoising diffusion probabilistic models (DDPMs)~\cite{sohl2015deep} % are powerful generative models, that have achieved remarkable results in image synthesis~\cite{imagen}, video synthesis \cite{singer2023makeavideo} and 3D motion generation \cite{tevet2023human}. DDPMs 
entail two processes, namely forward diffusion, as well as reverse diffusion. During the forward diffusion process, Gaussian noise is gradually added to the data up to the point, the data becomes Gaussian noise. On the other hand, during the reverse process, a neural model learns to gradually denoise the data.

% \noindent The forward process is a Markov chain that gradually adds noise according to a variance schedule $\beta_t$ on a real sample from the real data distribution $x_0{\sim} q(x)$. The goal is to obtain $q(x_{1:T}|x_0)$ with $x_1$ to $x_T$ the latent data
% \begin{equation}
% \begin{aligned}
% &q(x_{1:T}|x_0):= \prod_{t=1}^{T} q(x_t|x_{t-1}), 
% \allowbreak
% &q(x_t|x_{t-1}):=\mathcal{N}\left(x_t;\sqrt{1-\beta_t}x_{t-1},\beta_t\mathbf{I}\right).
% \label{eq:Diffusion_forward}
% \end{aligned}
% \end{equation}


% \noindent The reverse diffusion process is also a Markov chain, however here it is used to remove the noise. Formally, the reverse process $p_\theta(x_{0:T})$ removes noise from $x_T$, recursively prior to obtaining the real data $x_0$. An arbitrary condition $c$ can also be utilized during the reverse process. With $p(x_T)=\mathcal{N}(x_T;\mathbf{0}, \mathbf{I})$
% \begin{equation}
% \begin{aligned}
% &p(x_{0:T}):= p(x_T)\prod_{t=1}^{T} p_\theta(x_{t-1}|x_t),\\
% &p_\theta(x_{t-1}|x_t):=\mathcal{N}(x_{t-1};\mu_\theta(x_t,t,c), \Sigma_\theta(x_t,t,c)).
% \label{eq:Diffusion_reverse}
% \end{aligned}
% \end{equation}


% Here, the goal is to estimate $\mu_\theta(x_t,t,c)$ and $\Sigma_\theta(x_t,t,c)$. Ho \emph{et al.} \cite{ddpm} found that $\Sigma_\theta(x_t,t,c){=}\sigma_t^2\mathbf{I}$ can be utilized and set $\sigma_t$ as a constant. Further, the authors observed that $\mu_\theta(x_t,t,c)$ can be replaced with

% \begin{equation}
% \mu_\theta(x_t,t,c)=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_{t}}}}\epsilon_\theta(x_t,t,c)\right).
% \label{eq:Diffusion_reverse_2}
% \end{equation}
% Then, we only need to estimate $\epsilon_\theta(x_t,t,c)$ to denoise the latent data, as we can easily find $x_{t-1}$ with \begin{equation}
% x_{t-1}=\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_{t}}}}\epsilon_\theta(x_t,t,c)\right)+\sigma_t\gamma,
% \label{eq:Diffusion_reverse_3}
% \end{equation}
% with $\gamma{\sim}\mathcal{N}(\mathbf{0}, \mathbf{I})$. In this work, we have $\sigma_t{=}\operatorname{log}\left(\beta_t\dfrac{1-\alpha_{t-1}}{1-\alpha_t}\right)$, following Ho \emph{et al.} \cite{ddpm}. However, unlike them, we do not optimize $\epsilon_\theta(x_t,t,c)$ directly, instead following Ramesh \emph{et al.} \cite{ramesh2022hierarchical} by employing the following loss to minimize the distance between the output of a multi-condition Transformer network $\hat{x}^{1:N}_0=f(x_t,t,c)$, as defined in the next subsection and the real data $x_0$


% \begin{equation}
% \begin{aligned}
% L_{diff}:=&E_{t\in[1,T],x_0\sim q(x_0)}[\|x_0-\hat{x}^{1:N}_0\|^2].\\
% \label{eq:Diffusion_loss}
% \end{aligned}
% \end{equation}


\subsubsection{Audio Feature Extractor}
Towards employing the audio sequence as condition, we extract relevant features from raw audio. 
To leverage the strengths of both, phonemes representation and audio features, we proceed to employ both, \textit{audio features} Wav2Vec \cite{schneider2019wav2vec} features, \textit{as well as phonemes} as input of our model. Wav2Vec is selected for associated efficiency at encoding speech. In addition, we utilize a \textit{text transcript}, in order to obtain semantic information. We employ a phoneme aligner \cite{yuan2008speaker}, providing phonemes with timestamps from an audio-input and the corresponding text transcript. Based on the timestamps we create a sequence of phonemes, corresponding to the frame rate of the video and tokenize it. We note that phonemes constitute the smallest discrete speech units and contain essential information for word articulation. Formally we obtain $a^{1:N}={a^1,...,a^N}$ a Wav2Vec feature sequence with $a_i\in \mathbb{N}$ being the features in one frame; a tokenized phoneme sequence $p^{1:N}={p^1,...,p^N}$ with $p_i\in \mathbb{N}$ being a single phoneme token and a text transcript $S$.

\subsubsection{Motion Transformer}

We propose a Transformer architecture that accepts as input a Wav2Vec feature sequence $a^{1:N}$, a tokenized phoneme sequence $p^{1:N}$, a 3DMM sequence $x^{1:N}$, and the text transcript corresponding to the audio $S$. Additionally, we provide the first frame of the 3DMM sequence $x^1_0$ to condition the network on the first pose of the sequence to ensure that the generated 3DMM sequence starts from the original position. We formulate the facial motion generation as a reverse diffusion problem, where we sample a random noise $x^{1:N}_T$ towards obtaining a real 3DMM, representing the facial motion corresponding to the inputs. We propose in this context a Transformer architecture to learn the denoising process.
In this architecture, $S$ is encoded using a pretrained CLIP \cite{clip} Transformer encoder, $a^{1:N}$ and $p^{1:N}$ are encoded using simple trainable Transformer encoders and $x^1_0$ is encoded using a simple MLP encoder. $x^{1:N}_t$ is then denoised using a Transformer decoder containing self and cross attention layers to learn the relationship between the motion and the conditions. %(detailed architecture in the supplementary materials). %firstly passes through an embedding layer, followed by a positional encoding layer that encodes the temporal information from each frame in the sequence. Then we add the encoding of $a^{1:N}$ to $x^{1:N}_t$, in order to obtain $h^{1:N}_t$. Next, we perform self attention followed by three separate cross attention operations in parallel. These cross attention operations are done to find correlation between $h^{1:N}_t$ and the encodings of $p^{1:N}$, $S$, and $x^1_0$. The outputs of the cross attention layers are concatenated jointly and with the output of the self attention. Next, the dimension of the concatenated latent encoding is reduced and it goes trough several feedforward layers.  Finally, after 8 such Transformer layers, the data trans-passes through a final linear layer. The output of this linear layer has the goal to optimize the losses. 
Towards reducing complexity, all attention layers employ efficient attention \cite{shen2021efficient}.

% \begin{figure*}[t]
%   \centering

%    \includegraphics[width=0.7\linewidth]{Figures/cmdt.png}
%    \caption{\textbf{Conditional Motion Diffusion Transformer (cMDT).} cMDT takes an audio sequence and a 3DMM frame as condition using four different encoders (i.e., ID Encoder, CLIP Encoder, Phoneme Encoder, and Audio Encoder). A transformer diffusion decoder is applied to denoise a sequence of noises to facial motions based on input conditions.}
%    \label{fig:overview}

% \end{figure*}
\subsubsection{Learning}
\label{losses}
Best results in our experiments are obtained for Dimitra, in case that we train separate models for lip motion, facial expression and head pose. This is partly due to the unbalanced number of parameters between each component (13 for lips, 51 for face, 6 for head pose). Consequently, we train three separate models with the same network architecture and the same losses except for the head pose model. In these, for $x^i \in \mathbb{R}^{d}$, $d=13$, $d=51$ and $d=6$ for the lip model, facial expression model and head pose model, respectively.
We only utilize the weighted diffusion loss $L_{G} = \lambda \times L_{diff}$ as training loss for the lips and expression models. Head pose, however is very sensitive to the previous pose in the sequence, and discontinuities in the generation are highly noticeable, in particular when generating recursively, in order to obtain long sequences. To limit these discontinuities, in addition to giving the first pose as input, we add a first pose diffusion loss that corresponds to diffusion only on the first frame $x^1_t$. Consequently, the loss for the head pose model is $L_{G} = \lambda \times L_{diff} + L_{first}$. $\lambda=6$ in our implementation. 
\subsection{Video Renderer}
To transfer the generated 3DMM data to video space, we utilize a pretrained image renderer, proposed by Ren \textit{et al.}  \cite{9711291} that is able to generate videos with $256\times256$ resolution from a 3DMM input. Nevertheless, 3DMM data is not limited to this resolution, rather depends on the renderer. % and a different renderer would allow for higher resolutions. 
Furthermore, 3DMM is transferable to other 3D mesh formats, enabling versatility in applications.






\section{Experiments}
\label{sec:Experiments}

\subsection{Experimental settings}

We train and test our network on a subset of VoxCeleb2 \cite{Chung18b}. For testing only, we also use the HDTF dataset \cite{zhang2021flow}.  We evaluate Dimitra by computing standard metrics for talking head generation. %, see supplementary material (SM). 
Specifically, F-LMD and M-LMD \cite{chen2018lip,ma2023styletalk} evaluate the facial and mouth landmarks distance, respectively, whereas we employ the SyncNet \cite{chung2017out} distance and confidence score towards evaluating lip synchronisation of generated video and audio. We compare our framework to state of the art methods StyleTalker \cite{ma2023styletalk}, SadTalker \cite{zhang2023sadtalker}, and DreamTalk \cite{ma2023dreamtalk}. \textbf{More details about datasets, metrics and baselines can be found in the supplementary material (SM).}

% \subsection{Implementation}
% As discussed in Section \ref{losses}, we train three networks separately, for lips, face and head pose, respectively. We train each network for $200$ epochs on one Nvidia RTX 6000 GPU using pytorch. Training take approximately 40h while inference take 3 seconds for 100 frames (using 25 ddim steps and on a batch of 25 samples). To this we can add about 5 second for the video generation by the renderer (for 100 frames) which correspond to 12.5 fps (for a single video). Preprocessing (3DMM extraction, phoneme extraction, Wav2Vec features extraction) can also take a few second but can be done in parallel. We use a batch size of $64$,  $500$ diffusion steps with the noise uniformly sampled with a linear scheduler and 8 attention head for the Transformers and we have a learning rate of $0.0001$. %with a scheduler to reduce it, in case the loss stagnates. 
% Finally, while our framework can generate motion of any length, this implies large attention matrices and excessive memory usage. Therefore, given the length of videos in our VoxCeleb2 subset, we limit the length of audio and 3DMM sequence to $100$ frames, which corresponds to $4$ seconds of video at $25fps$. During training, we randomly select $100$ frames sequences within videos. During testing, we recursively generate sequences of $100$ frames, in order to build the full video. Specifically, to generate sequence $k$, we use the last generated frame $\hat{x}^{100}_0$ from sequence $k-1$ as the condition on the first pose $x^1_0$. Doing so, we ensure that the video remains temporally coherent in spite of the multiple generations. Dreamtalk \cite{ma2023dreamtalk} does not natively support the generation on long sequences. For evaluation on HDTF, we generate several smaller videos that we then concatenate to create the full video.\\
% \subsection{Evaluation protocol}

% We train and test our network on a subset of the VoxCeleb2 dataset \cite{Chung18b}. For testing only, we also use the HDTF dataset \cite{zhang2021flow}. As there is no clear, comprehensive, and widely used protocol for evaluating talking head generation on any dataset in the state-of-the-art literature, we propose a new detailed protocol for VoxCeleb2 and HDTF as well as their preprocessing that can be used n the future for evaluating talking head generation. This way we hope to resolve the issue of every new work using a different protocol making it difficult to compare with those methods. 

% \subsubsection{Datasets and preprocessing}


% On VoxCeleb2 we first randomly select $50,000$ English language samples from the dataset, corresponding to $409$ identities. We preprocess this dataset, cropping the face for each frame and resizing the crop to $256\times 256$. Then, we extract facial landmarks \cite{bulat2017far} and proceed to generate the corresponding 3DMM. At the same time, we use Whisper \cite{radford2023robust} to transcribe the audio. We also use Whisper to detect the language of the video, selecting only videos, containing English speech. Next, based on both, script and audio, we elicit phonemes with timestamps, employing P2FA \cite{yuan2008speaker}. As we can not directly use the phonemes with timestamps provided by P2FA \cite{yuan2008speaker} with our model, we tokenize them based on the video framerate. For example, in case that P2FA gives the phoneme "HH" for 0.12s, we tokenize it to 3 tokens at 25 fps ("HH,HH,HH"). Additionally, to match exactly the number of frames from the video, we adjust the final result by adding or removing token, where the timestamp does not correlate to a number of frames that is an integer. We extract audio features using Wav2Vec \cite{schneider2019wav2vec}.  After this preprocessing, we have $48,138$ samples that correspond to $387$ identities. 
% Finally, we randomly select $38$ subjects, in order to obtain $3931$ samples as test set  and dedicate the rest as training set. HDTF is preprocessed in the same way yielding $393$ long videos in English.
% For both VoxCeleb2 and HDTF some sample are removed (1862 for VoxCeleb2). These removed samples correspond to videos, where we were not able to obtain either facial landmarks, for the purpose of generating the 3DMM or audio that failed to have at least $66\%$ of the words in the phoneme dictionary that we utilize (CMU Pronouncing Dictionary\footnote{\url{http://www.speech.cs.cmu.edu/cgi-bin/cmudict}}). Issues related to audio are often due to wrongly detecting a language (mostly in videos, where two languages occur) or with too many words that are not present in the word to phoneme dictionary. However, since we automatize the entire process, some non-English videos are still present in small quantity. In such cases, our model does function, however, since the phoneme extractor becomes less accurate, the generation is rendered less convincing.\\

% \subsubsection{Metrics}
% We evaluate Dimitra by computing standard metrics for talking head generation. 
% Specifically, F-LMD and M-LMD \cite{chen2018lip,ma2023styletalk} evaluate the facial and mouth landmarks, respectively (68 and 20 landmarks extracted with dlib \cite{dlib09}, respectively). Both metrics compare landmarks pertained to generated sample and ground truth, and compute the average Euclidean norm considering landmarks and frames. We normalize the landmarks \emph{w.r.t.} the head pose when evaluating F-LMD and M-LMD using the Kabsch-Umeyama algorithm \cite{88573} between the ground truth landmarks and the generated landmarks. This is done independently for each frame of the sequence and allow us to remove the influence of head pose translation and rotation when evaluating the expression and lips motion. It also scale the data which is especially important when evaluating Dreamtalk and Styletalk on VoxCeleb2 as the methods change the viewpoint. Due to the large variations in head pose in the VoxCeleb2 dataset, dlib \cite{dlib09} was sometimes unable to find landmarks in the ground truth videos. In those case the samples are ignored for the two metrics. 
% We employ the SyncNet \cite{chung2017out} distance and confidence score towards evaluating lip synchronisation of generated video and audio. In addition, we compute FID, SSIM and PSNR, as they are commonly reported in evaluation of talking head generation methods. However, we note that these latter three metrics, targeted to evaluate image quality of videos, are ill fitted for methods that generate 3DMM. Specifically, they evaluate the quality of the image renderer rather than our talking head model.
% Additionally, as our model generates head pose, we compare the average head displacement (AHD)  $AHD=\frac{1}{n}\sum_{i=1}^n|x_{nose}^1-x_{nose}^i|$, where $x_{nose}^i$ is the nose edge landmark (number 30 in dlib \cite{dlib09}) at frame i. The metric evaluates that level that head pose changes compared to the original frame, the closer to the ground truth value the better. 

% \subsubsection{Baselines.} 
% We compare our framework to state of the art methods StyleTalker \cite{ma2023styletalk}, SadTalker \cite{zhang2023sadtalker}, DreamTalk \cite{ma2023dreamtalk} and Wav2Lips \cite{wav2lips}. Wav2Lips is a relatively old method but as recent literature still use it as a baseline due to its results on lipsync metrics we follow their choice. We note that StyleTalker and DreamTalk generate a video based on an input facial image and an audio sequence by creating a 3DMM representation similarly to Dimitra. Deviating from our model, StyleTalker and DreamTalk additionally require a head pose sequence, as well as a "style sequence" that captures the identity of the speaker and speaking style. For the sake of fairness, in the scope of generating talking heads only from audio and one image, we provide StyleTalker and DreamTalk with each first frame of the 3DMM sequence as "style sequence" and "head pose sequence". 
% StyleTalker only employs a phoneme representation for the audio, whereas DreamTalk incorporates Wav2vec features \cite{schneider2019wav2vec}. As the StyleTalker-code associated to their phoneme extractor is not publicly available, we convert our phonemes to their format as input. SadTalker accepts audio and image as inputs towards generating talking head endowed with head pose motion. Wav2Lips does not generate the entire face, only the lip region, while requiring a real video, in order to adopt from it head pose and expression. To stay within the bound of generating from only one image we use Wav2Lips on the first frame of the video. The training code for StyleTalker, DreamTalk and SadTalker were not available, therefore we use pretrained weights, as provided by the authors. These were obtained by training on a number of datasets including HDTF and VoxCeleb2. Hence, samples of our training set might have been part of the training set of these methods as well. Wav2Lips have provided training code, however we followed the protocol from other works \cite{zhang2023sadtalker,ma2023dreamtalk}, where related network was not retrained.

\begin{table}[!t] 
	\centering
		\caption{Quantitative results pertained to the VoxCeleb2 dataset}
		\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{@{}c |c c lc c c cl} 
\toprule
Method                                      & F-LMD$\downarrow$       & M-LMD$\downarrow$  & $sync_{dist}\downarrow$ &$sync_{conf}\uparrow$\\
\midrule
\textbf{GT}                                        & -       & -       &8.08           &5.80            \\
\midrule

%Wav2Lip \cite{wav2lips}                  & 5.96    & 4.46             &\textbf{8.19}  &\textbf{6.76}\\
StyleTalker \cite{ma2023styletalk}        & \underline{5.73}& \underline{4.41} &11.16  &3.24 \\
SadTalker \cite{zhang2023sadtalker}       & 6.14& 4.70 &\underline{9.12}   &\underline{5.28} \\

DreamTalk \cite{ma2023dreamtalk}          & 5.78& 4.43 &\textbf{8.48}   &\textbf{5.64}\\

 Dimitra & \textbf{5.00}& \textbf{3.79}& 9.43  & 4.93\\ 
 \hline
\end{tabular}}
\label{tab:results_vox}
%\vspace{-0.4cm}
\end{table}

\subsection{Quantitative results}
\label{sec:quanti}
Tables \ref{tab:results_vox} and \ref{tab:results_HDTF} depict quantitative results pertained to the datasets VoxCeleb2 and HDTF, respectively. We observe that our method outperforms or is competitive \textit{w.r.t.} the state of the art on both datasets. 
Regarding \textit{landmark based metrics (F-LMD, M-LMD)} our method outperforms the other methods on both datasets. This indicates that given an audio sequence, Dimitra generates realistic videos, resembling the ground truth, as opposed to other methods \emph{w.r.t.} lip motion and facial expression. %, hence the generated videos are more realistic. 
These results is supported by the \textbf{user study in our SM}. 

%The syncnet based metrics confirm results from the literature that have stated that Wav2Lips obtains best results.
Dimitra is competitive \textit{w.r.t.} \textit{lip synchronisation metrics}. 
While qualitative results and the related user study (see SM) demonstrate the realism and synchronization between lip movement and audio-speech, the 
%However, we note that there is a discrepancy between qualitative (see Sec. \ref{sec:qual_res}) and 
lip synchronisation (\textit{i.e.,} syncnet) results do not reflect on that. This stems from the fact that syncnet-metrics are not adequate in evaluating these. %do not reflect on the actual quality and realism of generated videos.
For example, SadTalker obtains competitive results \emph{w.r.t.} syncnet distance and confidence, while being able to only generate two states for the mouth, namely open and closed. Intermediate mouth shapes corresponding to pronounced sounds are not well generated (Fig.\ref{fig:example_vox1}). 

We note that deviating from DreamTalk, our framework Dimitra was not trained on HDTF. 
%We conclude that the lip-sync score, as it is currently evaluated, does not determine whether generated videos are synchronized with the audio. The area of audio based talking head generation inevitably necessitates a metric evaluating lip synchronization.



In Table \ref{tab:results_HDTF} we show results of two versions of our method: with and without head pose (Dimitra (HP) and Dimitra (no HP) respectively). For a single 3DMM generated by the cMDT we use one directly (HP)  while we freeze the head pose 3DMM parameters at the renderer level for the other (no HP). 
Surprisingly, while the F-LMD and M-LMD improve, the syncnet score decreases. This should not be the case, as only head pose was changed, while lip motion and expression remain exactly the same. This indicates that the metrics are instable.




\begin{table}[!t] 
	\centering
		\caption{Quantitative results pertained to the HDTF dataset}
		\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{@{}c |c c lc c c cl} 
\toprule
Method                                      & F-LMD$\downarrow$       & M-LMD$\downarrow$  & $sync_{dist}\downarrow$ &$sync_{conf}\uparrow$&\\
\midrule
\textbf{GT}                                          & -     & -       &8.63   &6.82        \\
\midrule

%Wav2Lip \cite{wav2lips}                    & \textbf{2.40}&  \textbf{2.26}&9.60   &\textbf{6.15}     \\

StyleTalker\cite{ma2023styletalk}             & \textbf{2.51}&  \underline{2.42}&13.06  &1.88      \\
SadTalker \cite{zhang2023sadtalker}         & 3.74&  4.19 &10.36  &5.34     \\
\
DreamTalk\cite{ma2023dreamtalk}             & 2.52&  \underline{2.42} &\textbf{8.66}   &\textbf{6.14}      \\

Dimitra (HP)& 3.85&  3.23&\underline{9.42}   &\underline{5.69}    \\ 

 Dimitra (no HP)& \textbf{2.51}& \textbf{2.38} & 9.60& 5.49 \\
\hline
\end{tabular}}
\label{tab:results_HDTF}
\vspace{-0.5cm}
\end{table}



\subsection{Qualitative results}\label{sec:qual_res}

\begin{figure*}[t]
  \centering

   \includegraphics[width=1.0\linewidth]{Figures/Figure_FG_full.jpg}
   \caption{\textbf{Qualitative results.} Examples of generated samples pertained to the VoxCeleb2 dataset on the left and pertained to HDTF on the right.}

   \label{fig:example_vox1}

\end{figure*}




\noindent We present qualitative results pertained to the VoxCeleb2 and HDTF datasets in Figure \ref{fig:example_vox1}. \noindent Specifically, our method generates coherent videos that  synchronize well with the ground truth, temporally and spatially. Notably, results \emph{w.r.t.} HDTF are convincing, despite our model not being trained on this dataset. All other methods comprise visual spatio-temporal synchronization errors. 
We also showcase that as discussed in Sec.\ref{sec:quanti}, SadTalker does not generate continuous mouth shapes, it only generates an opened or closed mouth, sometimes failing to generate a coherent mouth (Figure \ref{fig:example_vox1}, right side). Despite these issues, SadTalker obtains high lip-sync scores in the quantitative analysis, indicating the limitation of the metric.
%While Wav2Lips produces high lip synchronization, the quality of the generated mouth area appears highly blurry (most noticeable in the frames illustrated in Figure \ref{fig:example_vox1}, left side or in the videos from the SM), despite the fact that we already work in a setting of low resolution ($256\times256$). 
%We also showcase that deviating from other methods, SadTalker does not generate continuous mouth shapes, it only generates an opened or closed mouth, sometimes failing to generate a coherent mouth (Figure \ref{fig:example_vox1}, right side). Despite these issues, SadTalker obtains high lip-sync scores in the quantitative analysis, indicating the limitation of the metric. 


We observe that our method generates expressive talking faces. While DreamTalk  includes facial expressions in generated videos, they require a strong conditioning based on an additional real "style sequence". Dimitra is the \textit{only method to generate realistic head pose motion}. %While SadTalker does provide minor head pose motion, it is limited and appears unnatural (potentially to avoid issues with the renderer). 












%We note that the "style sequence" used by StyleTalker and DreamTalk encodes identity features, appearing in the generated video 
%In case that the sequence differs significantly from the original identity image, identity features of the style sequence appear in the generated video \emph{e.g.,} using a female style sequence on a male image, leading to unrealistic motion. We present in Fig.\ref{fig:example_style} (and in the corresponding video in the supplementary material) an example of such behavior. Using Dreamtalk \cite{ma2023dreamtalk} and Styletalk \cite{ma2023styletalk}, we generate videos with the same identity image and audio sequence, however changing the "style sequence" from a male subject to a female subject. We observe that both methods keep the original identity, when using the male sequence, however not when using the female sequence. We also notice that both methods deform one of the eyes, when using the female sequence. This experiment shows that, while style sequences work well when using adequate and selected sequences, generating videos for unknown identities does not generalize well. This indicates that facial expression should be generated from another condition. We note that our method obtains the information about expression from the identity image and audio sequence only. 

% \begin{figure}[t]
%   \centering

%    \includegraphics[width=0.6\linewidth]{Figures/style_seqences.jpg}
%    \caption{Examples of generated samples by Styletalk and Dreamtalk using different "style sequences".}

%    \label{fig:example_style}

% \end{figure}


% \subsection {User study}
% \label{sec:userstudy}
% We conduct a user study to evaluate generated video quality. Towards achieving fair evaluation, we display paired videos or generated by the 5 approaches (Dimitra, Styletalker, Wav2Lips, Sadtalker, Dreamtalk) on the HDTF dataset and ask 20 human raters for each paired video the question `which clip is more realistic and natural?'.
% Each video-pair contains a generated video from our method, as well as a video generated from Wav2lips, Styletalk, Sadtalker, Dreamtalk or the ground truth. 


% \begin{table}[!thb]
% \caption{\textbf{User study.} We ask 20 human raters to conduct a subjective video quality evaluation on the HDTF dataset. Results show that videos generated by Dimitra are consistently rated as more realistic.}
% \label{tab:user_study}
% \begin{center}
% \setlength{\tabcolsep}{3.3pt}
% \setlength\arrayrulewidth{1pt}
% \begin{tabular}{cccc}
% \hline
% & user preference (\%)\\
% \hline
% Dimitra/Wav2lips    & \textbf{61.3}/38.7 \\
% Dimitra/Styletalk   & \textbf{87.5}/12.5 \\
% Dimitra/Sadtalker   & \textbf{75.0}/25.0 \\
% Dimitra/Dreamtalk   & \textbf{68.8}/31.2 \\
% \hline
% \end{tabular}
% \end{center}
% \end{table}


% Results suggest that our generated videos are the most realistic in comparison to other methods on the HDTF dataset, see Table~\ref{tab:user_study}. This further strengthens our claim that some of the current metrics for talking head generation are unfit. We notice however that the ground truth is largely seen as the most realistic, indicating that there is large room for improvement in talking head generation.


% \subsection{Ablation study}

% \noindent We perform an ablation study, highlighting the effect of each audio-feature on generated videos \emph{w.r.t.} landmark metrics. In Table \ref{tab:results_abla}, we indicate results for related audio-feature configurations.
% We show in figure \ref{fig:abla} the qualitative results of each configuration used (better seen in the video provided in the supplementary materials). 
% We present results using only the Wav2Vec features (W2V) as well as adding the text or phoneme as additional inputs and then both to get the current version of Dimitra. We also compare Dimitra to an architecture that would use a single model to model lips motion, facial expression and head pose together instead of three separate models (Dimitra (single model)). We see that using additional conditions on top of Wav2Vec features improve the quality of the generation, with text improving more than phoneme. We believe this to be due to the fact that phonemes improve lip motion, whereas text improves expression and head pose; that will lead to greater improvement in landmark-based metric. Text is more strongly linked to head pose and expression than phoneme as text contains information about emotion and is linked to some head motions (\emph{e.g.} head shake when saying "no"). Finally the single model version of Dimitra achieve worse results than our current version. This is because the network is challenged in focusing on smaller motion (\emph{i.e., lip motion}), as improving  their quality yields smaller improvement of the loss function. The qualitative evaluation confirms the quantitative evaluation. Wav2Vec brings most of the information about lips motion while phoneme improves it even further. Using text as a condition improves the expression and the head pose of the generate videos. Using a single model instead of three decreases the performance significantly because the network has issues focusing on specific tasks which might not be entirely correlated.

% \begin{table}[!t] 
% 	\centering
% 		\caption{Ablation results to the VoxCeleb2 dataset}
% 		\resizebox{0.9\linewidth}{!}{%
% \begin{tabular}{@{}c |c c c } 
% \toprule
% Method                                      & F-LMD$\downarrow$       & M-LMD$\downarrow$  &AHD\\
% \midrule
% GT                                        & -     & -      &16.99\\
%  W2V &5.44&4.06 &14.95 \\
%  W2V + Phoneme& 5.34& 3.99& 13.52 \\
%  W2V + Text & 5.12& 3.86& 14.26 \\
% %D
%  Dimitra (single model)& 5.40& 4.04& 15.13 \\
%  W2V + Text + Phoneme (Dimitra)& \textbf{5.00}& \textbf{3.79}&\textbf{16.50 }\\
% \hline
% \end{tabular}}
% \label{tab:results_abla}

% \end{table}

% \begin{figure*}[t]
%   \centering
%  %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=0.8\linewidth]{Figures/Figure_quali_abla.jpg}
%    \caption{Examples of qualitative results for ablation pertained to the VoxCeleb2 dataset.}
%    \label{fig:abla}

% \end{figure*}



% \subsection{Limitations and societal impact}
% \label{sec:limitation}
% While our method generates highly realistic facial behavior, animating facial images, Dimitra is limited in some cases. In case that head pose differs largely from the source image, the generated video appears noisy, due to the image renderer. An improved image renderer is able to improve quality and resolve this limitation. While facial expression and head pose are generated based on audio data and reference input image, Dimitra does not allow for further control. A possibly way forward has to do with allowing for an additional input, \emph{e.g.} emotion-condition based on script and audio. Finally, while we are able to generate videos of any length using recursively the small size of each sequence ($100$ frames), this might cause minor discontinuities. We acknowledge that our method could be used to generate \emph{i.e.} by coupling it with voice cloning methods. However we are able to use our research on talking head generation to improve DeepFake detection. Moreover when releasing the model, steps will be taken to prevent misuse \emph{e.g.} using watermarks. 



\section{Conclusions}

We introduced Dimitra, a network streamlined to generate talking head videos based on a reference facial image and an audio-speech sequence. Deviating from previous models, Dimitra endows the reference image with lip motion associated to the audio, as well as with learned facial expression and head pose. Our qualitative and quantitative results showcase superiority \emph{w.r.t.} state of the art on two widely used datasets, due to reliable lip synchronization and expressiveness in our generated videos. By extracting features (phoneme and text transcript) directly from audio-speech, Dimitra generates local (\textit{i.e.,} facial expression) and global (\textit{i.e.,} head pose) motion.
%Generating global and local head motion from only two inputs (identity image and audio sequence) renders our model more efficient and prompt than previous approaches.
Dimitra is able to generate realistic videos, animating subjects outside of the original training data distribution. Future work will focus on learning representations and generating the entire upper body including articulated hand gestures. The code, preprocessing code and training - testing split will be released. %available in the release of our code.



\clearpage

%\section{Ethical Impact Statement}
%We acknowledge that our method could be used to generate deepfakes \emph{i.e.,} by coupling it with voice cloning methods. However we are able to use our research on talking head generation to improve deepfake detection. Moreover when releasing the model, steps will be taken to prevent misuse \emph{e.g.,} using watermarks.


{\small
\balance
\bibliographystyle{ieee}
\bibliography{main}
}

\pagebreak
\begin{center}
\textbf{\large Dimitra: Audio-driven Diffusion model for Expressive Talking Head Generation Supplemental Material}
\end{center}
%%%%%%%%%% Merge with supplemental materials %%%%%%%%%%
%%%%%%%%%% Prefix a "S" to all equations, figures, tables and reset the counter %%%%%%%%%%
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\renewcommand{\thesection}{S-\Roman{section}}
\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}

In this supplementary material we provide additional details \textit{w.r.t.} user study, as well as datasets, baselines and metrics. 

\section {User study}
\label{sec:userstudy}
We conduct a user study to evaluate generated video quality. Towards achieving fair evaluation, we display paired videos or generated by the 4 approaches (Dimitra, Styletalker, Sadtalker, Dreamtalk) on the HDTF dataset and ask 20 human raters for each paired video the question `which clip is more realistic and natural?'.
Each video-pair contains a generated video from our method, as well as a video generated from Styletalk, Sadtalker, Dreamtalk or the ground truth. 


\begin{table}[!thb]
\caption{\textbf{User study.} We ask 20 human raters to conduct a subjective video quality evaluation on the HDTF dataset. Results show that videos generated by Dimitra are consistently rated as more realistic.}
\label{tab:user_study}
\begin{center}
\setlength{\tabcolsep}{3.3pt}
\setlength\arrayrulewidth{1pt}
\begin{tabular}{cccc}
\hline
& user preference (\%)\\
\hline
%Dimitra/Wav2lips    & \textbf{61.3}/38.7 \\
Dimitra/Styletalk   & \textbf{87.5}/12.5 \\
Dimitra/Sadtalker   & \textbf{75.0}/25.0 \\
Dimitra/Dreamtalk   & \textbf{68.8}/31.2 \\
\hline
\end{tabular}
\end{center}
\end{table}


Results suggest that our generated videos are the most realistic in comparison to other methods on the HDTF dataset, see Table~\ref{tab:user_study}. %This further strengthens our claim that some of the current metrics for talking head generation are unfit. %We notice however that the ground truth is largely seen as the most realistic, indicating that there is large room for improvement in talking head generation.

%\section {Qualitative results}
%We also provide a html-file  (to be opened in an internet browser) presenting videos to support our claims about the quality of videos generated by our method and the state of the art. We have created mosaics of videos portraying from left to right, top row: ground truth, Dimitra, Styletalk; bottom line: Wav2Lips, Sadtalker, Dreamtalk. %We provide a description of the videos below each of them. To make the 
%For a fair comparison videos were generated in $256\times256$ resolution, however we also present videos in $512\times512$ resolution either upscaled with GFPGAN \cite{wang2021gfpgan} (which can create artifacts in the low quality video of VoxCeleb2) or directly generated in high resolution by the renderer retrained on higher resolution data.
% \begin{figure*}[t]
%   \centering

%   %\fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
%    \includegraphics[width=1.0\linewidth]{Figures/Figure_FG_full.jpg}
%    \caption{Examples of generated samples pertained to the VoxCeleb2 dataset.}

%    \label{fig:example_vox1}

% \end{figure*}

\section{Datasets, Baselines and Evaluation metrics}
\subsection{Datasets}
\noindent\textbf{VoxCeleb2} is a dataset containing more than 1 millions low resolution clips (average length 10s) of people talking in TV shows or interviews. We only use a subset of 50000 clips of which 3931 are used for testing (corresponding to 39 identities). We use face crop with a resolution of 256*256 but the quality of face crops vary as faces sizes in original videos can be widely different. The data is diverse with many different settings, a lot of facial expressions and head motions.

\noindent\textbf{HDTF} is a datasets containing 400 videos (average length 3-5 min) of people talking, all videos are using for testing. The video resolutions vary between 512*512 and 1024*1024, we resize all videos to 256*256 for a fair comparison with the other methods. Unlike VoxCeleb2, HDTF doesn't contain many facial expression or head motions. 

\subsection{Baselines}
We compare our framework to state of the art methods StyleTalker \cite{ma2023styletalk}, SadTalker \cite{zhang2023sadtalker}, DreamTalk \cite{ma2023dreamtalk}. We note that StyleTalker and DreamTalk generate a video based on an input facial image and an audio sequence by creating a 3DMM representation similarly to Dimitra. Deviating from our model, StyleTalker and DreamTalk additionally require a head pose sequence, as well as a "style sequence" that captures the identity of the speaker and speaking style. For the sake of fairness, in the scope of generating talking heads only from audio and one image, we provide StyleTalker and DreamTalk with each first frame of the 3DMM sequence as "style sequence" and "head pose sequence". 
StyleTalker only employs a phoneme representation for the audio, whereas DreamTalk incorporates Wav2vec features \cite{schneider2019wav2vec}. As the StyleTalker-code associated to their phoneme extractor is not publicly available, we convert our phonemes to their format as input. SadTalker accepts audio and image as inputs towards generating talking head endowed with head pose motion. The training code for StyleTalker, DreamTalk and SadTalker were not available, therefore we use pretrained weights, as provided by the authors. These were obtained by training on a number of datasets including HDTF and VoxCeleb2. Hence, samples of our training set might have been part of the training set of these methods as well.


\subsection{Metrics}
We evaluate Dimitra by computing standard metrics for talking head generation. 
Specifically, F-LMD and M-LMD \cite{chen2018lip,ma2023styletalk} evaluate the facial and mouth landmarks, respectively (68 and 20 landmarks extracted with dlib \cite{dlib09}, respectively). Both metrics compare landmarks pertained to generated sample and ground truth, and compute the average Euclidean norm considering landmarks and frames. We normalize the landmarks \emph{w.r.t.} the head pose when evaluating F-LMD and M-LMD using the Kabsch-Umeyama algorithm \cite{88573} between the ground truth landmarks and the generated landmarks. This is done independently for each frame of the sequence and allow us to remove the influence of head pose translation and rotation when evaluating the expression and lips motion. It also scale the data which is especially important when evaluating Dreamtalk and Styletalk on VoxCeleb2 as the methods change the viewpoint. Due to the large variations in head pose in the VoxCeleb2 dataset, dlib \cite{dlib09} was sometimes unable to find landmarks in the ground truth videos. In those case the samples are ignored for the two metrics. 
We employ the SyncNet \cite{chung2017out} distance and confidence score towards evaluating lip synchronisation of generated video and audio. For those last two metrics, following the literature we simply feed the generated videos into the code provided by the author of \cite{chung2017out}.


\end{document}
