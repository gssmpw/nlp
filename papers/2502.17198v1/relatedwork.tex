\section{Related Work}
\label{sec:Related}

%Talking head generation can be categorized into video- and audio-driven based on driving modalities. 

%\textbf{Video-driven} methods generally utilize a driving video, in order to animate a face image. 
%Such methods effectively swap identities, retaining motion from a driving video and reenacting a target face image. While such methods often used GAN inversion \cite{Oorloff_2023_ICCV,Bounareli_2023_ICCV} or motion flow \cite{tao2023learning}, state of the art approaches focused on manipulating directly the latent space  \cite{Ni_2023_CVPR,Pang_2023_CVPR,wang2022latent}. While video-driven methods achieved impressive results, we note that this is due to \textit{strong conditioning}, \textit{viz.} the output is controlled by a video that contains all information pertaining to expression, head pose and lip motion.
%While this is beneficial \textit{w.r.t.} realistic motion, it is also limiting, as it withdraws the ability to control \textit{e.g.,} specific parts of the output or to include randomness in the generation process. In contrast, conditioning a talking head on audio data entails such options, which constitutes the setting of interest in this work. 

\textbf{Audio-driven Talking head generation} methods can be \textit{person specific} \cite{ji2021audio-driven,textbasedediting}, where videos can only be generated of persons that have been in the training set, clearly limiting the generation setting. More related to our problem, \textit{person agnostic} methods are able to animate unknown identities in RGB \cite{Zhou2021Pose,Wang_2023_CVPR}, neural radiance fields \cite{li2024ae}, facial landmarks in a 3D space \cite{gururani2022SPACE,wang2021audio2head}, as well as mesh representations \cite{ma2023styletalk,zhang2023sadtalker} employing numerous architectures such as LSTM \cite{10.1145/3414685.3417774}, CNN \cite{wav2lips} or diffusion \cite{wang2024eat,yu2023talking,shen2023difftalk}. It is worth noting that there exist methods that animate 3D meshes \cite{sun2024diffposetalk}, deviating from our work, as our framework Dimitra provides an RGB video as output. Originally, methods focused on generating merely lip motion, render generated videos rather unrealistic \cite{wav2lips}. More recently, research has focused on generating motion pertaining to the entire face, including facial expression and head pose, however harnessing such directly or as strong condition from real video sequences, which are required as additional inputs \cite{ma2023styletalk,ma2023dreamtalk}. We note that such methods provide good results, in case that expression and head pose sequences are manually selected. %In these methods audio has been encoded in various way. %Models for talking head generation frequently have adopted pretrained audio encoder \cite{thambiraja2023imitator} or have trained their own encoder based on acoustic features extracted from audio \cite{gururani2023space} \emph{e.g.,} Mel Frequency Cepstrum Coefficients (MFCC) or Wav2Vec features \cite{schneider2019wav2vec}. Other works attempted to rather extract phonemes from audio \cite{ma2023styletalk}. We note that phonemes constitute the smallest discrete speech units and contain essential information for word articulation. Phoneme based methods are more resilient to noise than audio feature based methods. However, phonemes are language-specific, hindering a multi-language setting, do not encode any information about the intensity of the audio (\emph{e.g.,} screaming vs. whispering) and associated pacing information is sub-optimal.

\textbf{Diffusion Models}~\cite{pmlr-v37-sohl-dickstein15,ho2020denoising} have shown remarkable results in several tasks including image generation~\cite{ho2020denoising,zhou2023shifted} as well as video generation \cite{harvey2022flexible,wu2023tune}. Related to our setting, diffusion models have been proposed towards generation of face images \cite{huang2023collaborative,kim2023dcface}, as well as of talking heads \cite{stypulkowski2024diffused,du2023dae}. However, existing methods have not generated head pose or facial expression, while being dataset specific. We here propose a diffusion model, designed to generate videos of talking heads, endowed with local lip motion and facial expression, as well as global head pose, animating facial images of identities beyond the training set based on an audio-speech-inputs. 

\begin{figure*}[!t]
  \centering

   \includegraphics[width=0.8\linewidth]{Figures/dimitra.png}
   \caption{\textbf{Dimitra pipeline.} Dimitra comprises  three main parts, a Motion Modeling Module (MMM), a Conditional Motion Diffusion Transformer (cMDT) and a Video Renderer. In the training stage, 3D meshes (3DMM) are extracted from a video by the MMM. They are used by the cMDT jointly with features extracted from an audio sequence, to noise then denoise the 3DMM sequence.
   In the inference stage, using an audio sequence and an identity 3DMM as condition, cMDT aims at generating a 3DMM sequence from Gaussian noise. Finally, the Video Renderer transforms the 3DMM sequence into a RGB video.}
   \label{fig:dimitra}
\end{figure*}