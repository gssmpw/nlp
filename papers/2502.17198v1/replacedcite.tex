\section{Related Work}
\label{sec:Related}

%Talking head generation can be categorized into video- and audio-driven based on driving modalities. 

%\textbf{Video-driven} methods generally utilize a driving video, in order to animate a face image. 
%Such methods effectively swap identities, retaining motion from a driving video and reenacting a target face image. While such methods often used GAN inversion ____ or motion flow ____, state of the art approaches focused on manipulating directly the latent space  ____. While video-driven methods achieved impressive results, we note that this is due to \textit{strong conditioning}, \textit{viz.} the output is controlled by a video that contains all information pertaining to expression, head pose and lip motion.
%While this is beneficial \textit{w.r.t.} realistic motion, it is also limiting, as it withdraws the ability to control \textit{e.g.,} specific parts of the output or to include randomness in the generation process. In contrast, conditioning a talking head on audio data entails such options, which constitutes the setting of interest in this work. 

\textbf{Audio-driven Talking head generation} methods can be \textit{person specific} ____, where videos can only be generated of persons that have been in the training set, clearly limiting the generation setting. More related to our problem, \textit{person agnostic} methods are able to animate unknown identities in RGB ____, neural radiance fields ____, facial landmarks in a 3D space ____, as well as mesh representations ____ employing numerous architectures such as LSTM ____, CNN ____ or diffusion ____. It is worth noting that there exist methods that animate 3D meshes ____, deviating from our work, as our framework Dimitra provides an RGB video as output. Originally, methods focused on generating merely lip motion, render generated videos rather unrealistic ____. More recently, research has focused on generating motion pertaining to the entire face, including facial expression and head pose, however harnessing such directly or as strong condition from real video sequences, which are required as additional inputs ____. We note that such methods provide good results, in case that expression and head pose sequences are manually selected. %In these methods audio has been encoded in various way. %Models for talking head generation frequently have adopted pretrained audio encoder ____ or have trained their own encoder based on acoustic features extracted from audio ____ \emph{e.g.,} Mel Frequency Cepstrum Coefficients (MFCC) or Wav2Vec features ____. Other works attempted to rather extract phonemes from audio ____. We note that phonemes constitute the smallest discrete speech units and contain essential information for word articulation. Phoneme based methods are more resilient to noise than audio feature based methods. However, phonemes are language-specific, hindering a multi-language setting, do not encode any information about the intensity of the audio (\emph{e.g.,} screaming vs. whispering) and associated pacing information is sub-optimal.

\textbf{Diffusion Models}____ have shown remarkable results in several tasks including image generation____ as well as video generation ____. Related to our setting, diffusion models have been proposed towards generation of face images ____, as well as of talking heads ____. However, existing methods have not generated head pose or facial expression, while being dataset specific. We here propose a diffusion model, designed to generate videos of talking heads, endowed with local lip motion and facial expression, as well as global head pose, animating facial images of identities beyond the training set based on an audio-speech-inputs. 

\begin{figure*}[!t]
  \centering

   \includegraphics[width=0.8\linewidth]{Figures/dimitra.png}
   \caption{\textbf{Dimitra pipeline.} Dimitra comprises  three main parts, a Motion Modeling Module (MMM), a Conditional Motion Diffusion Transformer (cMDT) and a Video Renderer. In the training stage, 3D meshes (3DMM) are extracted from a video by the MMM. They are used by the cMDT jointly with features extracted from an audio sequence, to noise then denoise the 3DMM sequence.
   In the inference stage, using an audio sequence and an identity 3DMM as condition, cMDT aims at generating a 3DMM sequence from Gaussian noise. Finally, the Video Renderer transforms the 3DMM sequence into a RGB video.}
   \label{fig:dimitra}
\end{figure*}