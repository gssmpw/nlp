\section{Related work}
\subsection{Automated glossing}

%- Without LLMs

Models for automated glossing have used both non-neural \cite{palmer-etal-2009-evaluating,moeller-hulden-2018-automatic} and neural \cite{moeller-hulden-2018-automatic,zhao-etal-2020-automatic} learning methods. The SIGMORPHON challenge baseline \cite{ginn2023baseline} uses the RoBERTa architecture, while the challenge winner \cite{girrbach-2023-tu-cl} uses an encoder-decoder model with hard attention 
which automatically induces morphological segmentations from the data. Such relatively small task-specific and language-specific models still represent the state of the art for the glossing task, because they can learn efficiently from the small datasets available and because they are cheap to apply in the computational sense.

Neither \citet{ginn2023baseline} nor \citet{girrbach-2023-tu-cl} use sentence translations. Although this simplicity is in some sense an advantage, it is also a limitation of the approach, since in some cases, the correct gloss is more evident from the translation, for instance in disambiguating homophones or syncretic morphemes \cite{zhao-etal-2020-automatic}. \citet{yang2024embedded} adds embeddings of the sentence translations on top of the \citet{girrbach-2023-tu-cl} model and obtain an improvement of 4\% in the word-level accuracy score.

%- With LLMs

The utility of translations motivates \citet{ginn2024teachlanguagemodelsgloss} to apply LLMs in the glossing task.\footnote{\citet{yang2024embedded} report preliminary and unsuccessful attempts to use Llama2 for translation embeddings.} Like ours, their attempt is retrieval-based. They use one prompt per sentence, retrieving up to 100 similar sentences per instance; the most effective similarity metric is chrF \cite{popovic-2015-chrf}. In the four languages they ran, their results lie between the challenge baseline and \citet{girrbach-2023-tu-cl} except in Gitksan, which has the lowest absolute scores due to its very high OOV rate. \citet{aycock2024llmsreallylearntranslate} also apply LLMs to glossing the Kalamang language using glossed examples from a grammar book. Their glossing system improves on the challenge baseline, and benefits slightly from the addition of generated text explaining some typological features of Kalamang (such as the absence of definite articles). Unlike this work, they do not attempt to disambiguate particular constructions.

\citet{yang-etal-2024-multiple} and \citet{shandilya-palmer-2025-boosting} use LLMs in a different way, to post-correct glosses proposed by an encoder-decoder model. \citet{shandilya-palmer-2025-boosting} find that retrieval-based post-correction can improve the glosses proposed by a competitive encoder-decoder model. Recent LLMs are capable of producing informative explanations of their decisions, and surprisingly in light of the results surveyed in the next subsection, are able to incorporate information from a linguistic grammar to do so. Like this paper, their results show some promise for incorporating linguistic instructions into the glossing process.

A third approach is taken in \citet{ginn-etal-2024-glosslm}. Rather than relying on in-context learning by applying a retrieval and prompting approach, they pretrain a T5-based LM on a large corpus of glossed text. Such a model is effective for glossing, but, unlike retrieval approaches using generic LLMs, is not well-suited to processing other kinds of instructional text.

\subsection{LLMs for low-resource NLP}

The motivations for using LLMs in glossing (increased sensitivity to translations, and the accessibility of using explicit instructions in place of examples) also apply to other linguistic tasks, notably low-resource translation. Retrieval-based translation systems use a similar framework to LLM-based glossing in which sentences from parallel corpora, or entries from dictionaries and other linguistic documentation, are retrieved in an item-specific way and added to prompts. Several studies in this area have found that, although dictionary sources can improve translation, explicit grammatical instructions do not \cite{court-elsner-2024-shortcomings,zhang2024teaching,elsner-needle-2023}. One claimed exception, \citet{reid2024gemini}, uses an entire grammar for the Kalamang language as a source, but subsequent work \cite{aycock2024llmsreallylearntranslate} shows that the improvement is due to the example sentences, not the grammatical text. Even when the retrieved grammar instructions are curated by hand to assure that the passages are relevant, they still do not lead to improvements \cite{court-elsner-2024-shortcomings}. \citet{kornilov2024classification} ask LLMs to answer explicit questions about the typological features of various languages using retrieval of grammar passages, with good but not perfect results. However, this task requires the LLM to understand and respond to linguistic terminology at an abstract level, rather than applying linguistic terminology to the analysis of individual language examples. We find below that this task remains challenging even when the abstract instructions are relevant and correct. Understanding how linguistic terminology is processed in the glossing task might also lead to better techniques for retrieval-based translation.

%- Translation
%- Explicit instructions not helping in translation
%- Other tasks (briefly)