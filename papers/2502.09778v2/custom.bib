% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@inproceedings{ginn-etal-2023-findings,
    title = "Findings of the {SIGMORPHON} 2023 Shared Task on Interlinear Glossing",
    author = "Ginn, Michael  and
      Moeller, Sarah  and
      Palmer, Alexis  and
      Stacey, Anna  and
      Nicolai, Garrett  and
      Hulden, Mans  and
      Silfverberg, Miikka",
    editor = {Nicolai, Garrett  and
      Chodroff, Eleanor  and
      Mailhot, Frederic  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigmorphon-1.20/",
    doi = "10.18653/v1/2023.sigmorphon-1.20",
    pages = "186--201",
    abstract = {This paper presents the findings of the SIGMORPHON 2023 Shared Task on Interlinear Glossing. This first iteration of the shared task explores glossing of a set of six typologically diverse languages: Arapaho, Gitksan, Lezgi, Nat{\"u}gu, Tsez and Uspanteko. The shared task encompasses two tracks: a resource-scarce closed track and an open track, where participants are allowed to utilize external data resources. Five teams participated in the shared task. The winning team T{\"u}-CL achieved a 23.99{\%}-point improvement over a baseline RoBERTa system in the closed track and a 17.42{\%}-point improvement in the open track.}
}

@inproceedings{ginn2024teachlanguagemodelsgloss,
    title = "Can we teach language models to gloss endangered languages?",
    author = "Ginn, Michael  and
      Hulden, Mans  and
      Palmer, Alexis",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.337/",
    doi = "10.18653/v1/2024.findings-emnlp.337",
    pages = "5861--5876",
    abstract = "Interlinear glossed text (IGT) is a popular format in language documentation projects, where each morpheme is labeled with a descriptive annotation. Automating the creation of interlinear glossed text would be desirable to reduce annotator effort and maintain consistency across annotated corpora. Prior research has explored a number of statistical and neural methods for automatically producing IGT. As large language models (LLMs) have showed promising results across multilingual tasks, even for rare, endangered languages, it is natural to wonder whether they can be utilized for the task of generating IGT. We explore whether LLMs can be effective at the task of interlinear glossing with in-context learning, without any traditional training. We propose new approaches for selecting examples to provide in-context, observing that targeted selection can significantly improve performance. We find that LLM-based methods beat standard transformer baselines, despite requiring no training at all. These approaches still underperform state-of-the-art supervised systems for the task, but are highly practical for researchers outside of the NLP community, requiring minimal effort to use."
}



@incollection{baerman2004typology,
  title={Typology and the formal modelling of syncretism},
  author={Baerman, Matthew},
  booktitle={Yearbook of morphology 2004},
  pages={41--72},
  year={2004},
  publisher={Springer}
}

@book{polinsky-tsez,
author = {Polinsky, Maria},
year = {2014},
month = {11},
pages = {},
title = {Tsez Syntax: A Description},
doi = {10.13140/2.1.2378.4003}
}

@inproceedings{palmer-etal-2009-evaluating,
    title = "Evaluating Automation Strategies in Language Documentation",
    author = "Palmer, Alexis  and
      Moon, Taesun  and
      Baldridge, Jason",
    editor = "Ringger, Eric  and
      Haertel, Robbie  and
      Tomanek, Katrin",
    booktitle = "Proceedings of the {NAACL} {HLT} 2009 Workshop on Active Learning for Natural Language Processing",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-1905/",
    pages = "36--44"
}

@inproceedings{moeller-hulden-2018-automatic,
    title = "Automatic Glossing in a Low-Resource Setting for Language Documentation",
    author = "Moeller, Sarah  and
      Hulden, Mans",
    editor = "Klavans, Judith L.",
    booktitle = "Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-4809/",
    pages = "84--93",
    abstract = "Morphological analysis of morphologically rich and low-resource languages is important to both descriptive linguistics and natural language processing. Field documentary efforts usually procure analyzed data in cooperation with native speakers who are capable of providing some level of linguistic information. Manually annotating such data is very expensive and the traditional process is arguably too slow in the face of language endangerment and loss. We report on a case study of learning to automatically gloss a Nakh-Daghestanian language, Lezgi, from a very small amount of seed data. We compare a conditional random field based sequence labeler and a neural encoder-decoder model and show that a nearly 0.9 F1-score on labeled accuracy of morphemes can be achieved with 3,000 words of transcribed oral text. Errors are mostly limited to morphemes with high allomorphy. These results are potentially useful for developing rapid annotation and fieldwork tools to support documentation of morphologically rich, endangered languages."
}

@inproceedings{girrbach-2023-tu-cl,
    title = {T{\"u}-{CL} at {SIGMORPHON} 2023: Straight-Through Gradient Estimation for Hard Attention},
    author = "Girrbach, Leander",
    editor = {Nicolai, Garrett  and
      Chodroff, Eleanor  and
      Mailhot, Frederic  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigmorphon-1.19/",
    doi = "10.18653/v1/2023.sigmorphon-1.19",
    pages = "171--185",
    abstract = "This paper describes our systems participating in the 2023 SIGMORPHON Shared Task on Morphological Inflection and in the 2023 SIGMORPHON Shared Task on Interlinear Glossing. We propose methods to enrich predictions from neural models with discrete, i.e. interpretable, information. For morphological inflection, our models learn deterministic mappings from subsets of source lemma characters and morphological tags to individual target characters, which introduces interpretability. For interlinear glossing, our models learn a shallow morpheme segmentation in an unsupervised way jointly with predicting glossing lines. Estimated segmentation may be useful when no ground-truth segmentation is available. As both methods introduce discreteness into neural models, our technical contribution is to show that straight-through gradient estimators are effective to train hard attention models."
}

@misc{ginn2023baseline,
      title={SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model}, 
      author={Michael Ginn},
      year={2023},
      eprint={2303.14234},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.14234}, 
}

@misc{yang2024embedded,
      title={Embedded Translations for Low-resource Automated Glossing}, 
      author={Changbing Yang and Garrett Nicolai and Miikka Silfverberg},
      year={2024},
      eprint={2403.08189},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08189}, 
}

@inproceedings{popovic-2015-chrf,
    title = "chr{F}: character n-gram {F}-score for automatic {MT} evaluation",
    author = "Popovi{\'c}, Maja",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3049/",
    doi = "10.18653/v1/W15-3049",
    pages = "392--395"
}

@inproceedings{zhao-etal-2020-automatic,
    title = "Automatic Interlinear Glossing for Under-Resourced Languages Leveraging Translations",
    author = "Zhao, Xingyuan  and
      Ozaki, Satoru  and
      Anastasopoulos, Antonios  and
      Neubig, Graham  and
      Levin, Lori",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.471/",
    doi = "10.18653/v1/2020.coling-main.471",
    pages = "5397--5408",
    abstract = "Interlinear Glossed Text (IGT) is a widely used format for encoding linguistic information in language documentation projects and scholarly papers. Manual production of IGT takes time and requires linguistic expertise. We attempt to address this issue by creating automatic glossing models, using modern multi-source neural models that additionally leverage easy-to-collect translations. We further explore cross-lingual transfer and a simple output length control mechanism, further refining our models. Evaluated on three challenging low-resource scenarios, our approach significantly outperforms a recent, state-of-the-art baseline, particularly improving on overall accuracy as well as lemma and tag recall."
}

@inproceedings{yang-etal-2024-multiple,
    title = "Multiple Sources are Better Than One: Incorporating External Knowledge in Low-Resource Glossing",
    author = "Yang, Changbing  and
      Nicolai, Garrett  and
      Silfverberg, Miikka",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.261/",
    doi = "10.18653/v1/2024.emnlp-main.261",
    pages = "4537--4552",
    abstract = "In this paper, we address the data scarcity problem in automatic data-driven glossing for low-resource languages by coordinating multiple sources of linguistic expertise. We enhance models by incorporating both token-level and sentence-level translations, utilizing the extensive linguistic capabilities of modern LLMs, and incorporating available dictionary resources. Our enhancements lead to an average absolute improvement of 5{\%}-points in word-level accuracy over the previous state of the art on a typologically diverse dataset spanning six low-resource languages. The improvements are particularly noticeable for the lowest-resourced language Gitksan, where we achieve a 10{\%}-point improvement. Furthermore, in a simulated ultra-low resource setting for the same six languages, training on fewer than 100 glossed sentences, we establish an average 10{\%}-point improvement in word-level accuracy over the previous state-of-the-art system."
}

@misc{tanzer2024mtob,
      title={A Benchmark for Learning to Translate a New Language from One Grammar Book}, 
      author={Garrett Tanzer and Mirac Suzgun and Eline Visser and Dan Jurafsky and Luke Melas-Kyriazi},
      year={2024},
      eprint={2309.16575},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{reid2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-{B}aptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{zhang2024teaching,
      title={Teaching Large Language Models an Unseen Language on the Fly}, 
      author={Chen Zhang and Xiao Liu and Jiuheng Lin and Yansong Feng},
      year={2024},
      eprint={2402.19167},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{aycock2024llmsreallylearntranslate,
      title={Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?}, 
      author={Seth Aycock and David Stap and Di Wu and Christof Monz and Khalil Sima'an},
      year={2024},
      eprint={2409.19151},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.19151}, 
}

@inproceedings{elsner-needle-2023,
    title = "Translating a low-resource language using {GPT}-3 and a human-readable dictionary",
    author = "Elsner, Micha  and
      Needle, Jordan",
    editor = {Nicolai, Garrett  and
      Chodroff, Eleanor  and
      Mailhot, Frederic  and
      {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i}},
    booktitle = "Proceedings of the 20th SIGMORPHON workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.sigmorphon-1.2",
    doi = "10.18653/v1/2023.sigmorphon-1.2",
    pages = "1--13",
    abstract = "We investigate how well words in the polysynthetic language Inuktitut can be translated by combining dictionary definitions, without use of a neural machine translation model trained on parallel text. Such a translation system would allow natural language technology to benefit from resources designed for community use in a language revitalization or education program, rather than requiring a separate parallel corpus. We show that the text-to-text generation capabilities of GPT-3 allow it to perform this task with BLEU scores of up to 18.5. We investigate prompting GPT-3 to provide multiple translations, which can help slightly, and providing it with grammar information, which is mostly ineffective. Finally, we test GPT-3{'}s ability to derive morpheme definitions from whole-word translations, but find this process is prone to errors including hallucinations.",
}
@inproceedings{court-elsner-2024-shortcomings,
    title = "Shortcomings of {LLM}s for Low-Resource Translation: Retrieval and Understanding Are Both the Problem",
    author = "Court, Sara  and
      Elsner, Micha",
    editor = "Haddow, Barry  and
      Kocmi, Tom  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Ninth Conference on Machine Translation",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.wmt-1.125/",
    doi = "10.18653/v1/2024.wmt-1.125",
    pages = "1332--1354",
    abstract = "This work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of information retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of prompt type, retrieval method, model type, and language community-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world`s 7,000+ languages and their speakers."
}
@misc{kornilov2024classification,
      title={From MTEB to MTOB: Retrieval-Augmented Classification for Descriptive Grammars}, 
      author={Albert Kornilov and Tatiana Shavrina},
      year={2024},
      eprint={2411.15577},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15577}, 
}
@inproceedings{carlini2019secret,
  title={The secret sharer: Evaluating and testing unintended memorization in neural networks},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={28th USENIX security symposium (USENIX security 19)},
  pages={267--284},
  year={2019}
}

@article{bromham2022global,
  title={Global predictors of language endangerment and the future of linguistic diversity},
  author={Bromham, Lindell and Dinnage, Russell and Skirg{\aa}rd, Hedvig and Ritchie, Andrew and Cardillo, Marcel and Meakins, Felicity and Greenhill, Simon and Hua, Xia},
  journal={Nature ecology \& evolution},
  volume={6},
  number={2},
  pages={163--173},
  year={2022},
  publisher={Nature Publishing Group UK London}
}
@article{meighan2021decolonizing,
  title={Decolonizing the digital landscape: The role of technology in Indigenous language revitalization},
  author={Meighan, Paul J},
  journal={AlterNative: An International Journal of Indigenous Peoples},
  volume={17},
  number={3},
  pages={397--405},
  year={2021},
  publisher={SAGE Publications Sage UK: London, England}
}
@misc{bickel-leipzig,
author={Bickel, Balthasar and Comrie, Bernard and Haspelmath, Martin},
title={The {Leipzig} Glossing Rules: Conventions for interlinear morpheme-by-morpheme glosses},
year={2015},
date={May 15},
url={https://www.eva.mpg.de/lingua/pdf/Glossing-Rules.pdf},}
@book{haspelmath-lezgi,
author={Haspelmath, Martin},
title={A Grammar of {Lezgian}},
year={1963},
publisher={Mouton de Gruyter},
place={Berlin}}
@article{Bird_2011, title={Bootstrapping the Language Archive: New prospects for Natural Language Processing in Preserving Linguistic Heritage}, volume={6}, url={https://journals.colorado.edu/index.php/lilt/article/view/1243}, DOI={10.33011/lilt.v6i.1243}, abstractNote={&amp;lt;p&amp;gt;There are grounds to believe that language technology in general, and natural language processing in particular, have important roles to play in creating and analyzing corpora for small languages. This goes beyond the development of data management tools to the application of natural language processing techniques to small and noisy datasets, and the design of new methods that operate within the constraints of linguistic field data. A set of seven such constraints (or &amp;quot;axioms for scalable work with small languages&amp;quot;) are presented, and suggestions for further NLP research are related back to these axioms.&amp;lt;/p&amp;gt;}, journal={Linguistic Issues in Language Technology}, author={Bird, Steven}, year={2011}, month={Oct.} }
@book{moseley2012unesco,
  title={The UNESCO atlas of the world's languages in danger: Context and process},
  author={Moseley, Christopher},
  year={2012},
  publisher={World Oral Literature Project}
}
@inproceedings{anastasopoulos-etal-2018-part,
    title = "Part-of-Speech Tagging on an Endangered Language: a Parallel {G}riko-{I}talian Resource",
    author = "Anastasopoulos, Antonios  and
      Lekakou, Marika  and
      Quer, Josep  and
      Zimianiti, Eleni  and
      DeBenedetto, Justin  and
      Chiang, David",
    editor = "Bender, Emily M.  and
      Derczynski, Leon  and
      Isabelle, Pierre",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1214/",
    pages = "2529--2539",
    abstract = "Most work on part-of-speech (POS) tagging is focused on high resource languages, or examines low-resource and active learning settings through simulated studies. We evaluate POS tagging techniques on an actual endangered language, Griko. We present a resource that contains 114 narratives in Griko, along with sentence-level translations in Italian, and provides gold annotations for the test set. Based on a previously collected small corpus, we investigate several traditional methods, as well as methods that take advantage of monolingual data or project cross-lingual POS tags. We show that the combination of a semi-supervised method with cross-lingual transfer is more appropriate for this extremely challenging setting, with the best tagger achieving an accuracy of 72.9{\%}. With an applied active learning scheme, which we use to collect sentence-level annotations over the test set, we achieve improvements of more than 21 percentage points."
}

@inproceedings{shandilya-palmer-2025-boosting,
    title = "Boosting the Capabilities of Compact Models in Low-Data Contexts with Large Language Models and Retrieval-Augmented Generation",
    author = "Shandilya, Bhargav  and
      Palmer, Alexis",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.499/",
    pages = "7470--7483",
    abstract = "The data and compute requirements of current language modeling technology pose challenges for the processing and analysis of low-resource languages. Declarative linguistic knowledge has the potential to partially bridge this data scarcity gap by providing models with useful inductive bias in the form of language-specific rules. In this paper, we propose a retrieval augmented generation (RAG) framework backed by a large language model (LLM) to correct the output of a smaller model for the linguistic task of morphological glossing. We leverage linguistic information to make up for the lack of data and trainable parameters, while allowing for inputs from written descriptive grammars interpreted and distilled through an LLM. The results demonstrate that significant leaps in performance and efficiency are possible with the right combination of: a) linguistic inputs in the form of grammars, b) the interpretive power of LLMs, and c) the trainability of smaller token classification networks. We show that a compact, RAG-supported model is highly effective in data-scarce settings, achieving a new state-of-the-art for this task and our target languages. Our work also offers documentary linguists a more reliable and more usable tool for morphological glossing by providing well-reasoned explanations and confidence scores for each output."
}

@inproceedings{ginn-etal-2024-glosslm,
    title = "{G}loss{LM}: A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text",
    author = "Ginn, Michael  and
      Tjuatja, Lindia  and
      He, Taiqi  and
      Rice, Enora  and
      Neubig, Graham  and
      Palmer, Alexis  and
      Levin, Lori",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.683/",
    doi = "10.18653/v1/2024.emnlp-main.683",
    pages = "12267--12286",
    abstract = "Language documentation projects often involve the creation of annotated text in a format such as interlinear glossed text (IGT), which captures fine-grained morphosyntactic analyses in a morpheme-by-morpheme format. However, there are few existing resources providing large amounts of standardized, easily accessible IGT data, limiting their applicability to linguistic research, and making it difficult to use such data in NLP modeling. We compile the largest existing corpus of IGT data from a variety of sources, covering over 450k examples across 1.8k languages, to enable research on crosslingual transfer and IGT generation. We normalize much of our data to follow a standard set of labels across languages.Furthermore, we explore the task of automatically generating IGT in order to aid documentation projects. As many languages lack sufficient monolingual data, we pretrain a large multilingual model on our corpus. We demonstrate the utility of this model by finetuning it on monolingual corpora, outperforming SOTA models by up to 6.6{\%}. Our pretrained model and dataset are available on Hugging Face: https://huggingface.co/collections/lecslab/glosslm-66da150854209e910113dd87"
}