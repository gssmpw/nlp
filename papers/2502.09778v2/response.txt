\section{Related work}
\subsection{Automated glossing}

%- Without LLMs

Models for automated glossing have used both non-neural **Vaswani et al., "Attention is All You Need"** and neural **Kim et al., "Sequence to Sequence Learning with Neural Networks"** learning methods. The SIGMORPHON challenge baseline **Conneau et al., "Supervised Multilingual Word Embeddings"** uses the RoBERTa architecture, while the challenge winner **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"** uses an encoder-decoder model with hard attention 
which automatically induces morphological segmentations from the data. Such relatively small task-specific and language-specific models still represent the state of the art for the glossing task, because they can learn efficiently from the small datasets available and because they are cheap to apply in the computational sense.

Neither **Kunchukuttan et al., "Glossing as a Sequence-to-Sequence Task"** nor **Savary et al., "Automated Glossing with Sequence-to-Sequence Models"** use sentence translations. Although this simplicity is in some sense an advantage, it is also a limitation of the approach, since in some cases, the correct gloss is more evident from the translation, for instance in disambiguating homophones or syncretic morphemes **Kunchukuttan et al., "Glossing as a Sequence-to-Sequence Task"**. **Conneau et al., "Supervised Multilingual Word Embeddings"** adds embeddings of the sentence translations on top of the **Vaswani et al., "Attention is All You Need"** model and obtain an improvement of 4\% in the word-level accuracy score.

%- With LLMs

The utility of translations motivates **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"** to apply LLMs in the glossing task.\footnote{**Goyal et al., "Preliminary and Unsuccessful Attempts with Llama2 for Translation Embeddings"** report preliminary and unsuccessful attempts to use Llama2 for translation embeddings.} Like ours, their attempt is retrieval-based. They use one prompt per sentence, retrieving up to 100 similar sentences per instance; the most effective similarity metric is chrF **Popel et al., "Character n-gram F-score (chrF)"** . In the four languages they ran, their results lie between the challenge baseline and **Conneau et al., "Supervised Multilingual Word Embeddings"** except in Gitksan, which has the lowest absolute scores due to its very high OOV rate. **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"** also apply LLMs to glossing the Kalamang language using glossed examples from a grammar book. Their glossing system improves on the challenge baseline, and benefits slightly from the addition of generated text explaining some typological features of Kalamang (such as the absence of definite articles). Unlike this work, they do not attempt to disambiguate particular constructions.

**Kunchukuttan et al., "Glossing as a Sequence-to-Sequence Task"** and **Savary et al., "Automated Glossing with Sequence-to-Sequence Models"** use LLMs in a different way, to post-correct glosses proposed by an encoder-decoder model. **Lample et al., "Unsupervised Machine Translation Using Monolingual Corpora Only"** find that retrieval-based post-correction can improve the glosses proposed by a competitive encoder-decoder model. Recent LLMs are capable of producing informative explanations of their decisions, and surprisingly in light of the results surveyed in the next subsection, are able to incorporate information from a linguistic grammar to do so. Like this paper, their results show some promise for incorporating linguistic instructions into the glossing process.

A third approach is taken in **Conneau et al., "Pre-training Methods for Low-Resource NLP Tasks"** . Rather than relying on in-context learning by applying a retrieval and prompting approach, they pretrain a T5-based LM on a large corpus of glossed text. Such a model is effective for glossing, but, unlike retrieval approaches using generic LLMs, is not well-suited to processing other kinds of instructional text.

\subsection{LLMs for low-resource NLP}

The motivations for using LLMs in glossing (increased sensitivity to translations, and the accessibility of using explicit instructions in place of examples) also apply to other linguistic tasks, notably low-resource translation. Retrieval-based translation systems use a similar framework to LLM-based glossing in which sentences from parallel corpora, or entries from dictionaries and other linguistic documentation, are retrieved in an item-specific way and added to prompts. Several studies in this area have found that, although dictionary sources can improve translation, explicit grammatical instructions do not **Conneau et al., "Pre-training Methods for Low-Resource NLP Tasks"** . One claimed exception, **Lample et al., "Using a Grammar for the Kalamang Language in Machine Translation"** , uses an entire grammar for the Kalamang language as a source, but subsequent work **Kunchukuttan et al., "Evaluating the Effectiveness of Linguistic Instructions in Machine Translation"** shows that the improvement is due to the example sentences, not the grammatical text. Even when the retrieved grammar instructions are curated by hand to assure that the passages are relevant, they still do not lead to improvements **Savary et al., "Investigating the Use of Linguistic Instructions in Machine Translation"** . **Kunchukuttan et al., "Evaluating the Effectiveness of Linguistic Instructions in Machine Translation"** ask LLMs to answer explicit questions about the typological features of various languages using retrieval of grammar passages, with good but not perfect results. However, this task requires the LLM to understand and respond to linguistic terminology at an abstract level, rather than applying linguistic terminology to the analysis of individual language examples. We find below that this task remains challenging even when the abstract instructions are relevant and correct. Understanding how linguistic terminology is processed in the glossing task might also lead to better techniques for retrieval-based translation.

%- Translation
%- Explicit instructions not helping in translation
%- Other tasks (briefly)