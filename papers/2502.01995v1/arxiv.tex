%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}
\usepackage{arxiv}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{bm}
\usepackage{bbm}
\usepackage{lipsum}
\usepackage{listings}
% \usepackage[finalizecache,cachedir=.]{minted}
\usepackage[frozencache,cachedir=.]{minted}
\usepackage{xcolor} % to access the named colour LightGray
\usepackage{makecell}
\usepackage[hyphens]{url}
\usepackage{natbib}
\definecolor{LightGray}{gray}{0.9}

\renewcommand{\cellalign}{l}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Theoretical and Practical Analysis of Fréchet Regression via Comparison Geometry}
\title{Theoretical and Practical Analysis of Fréchet Regression via Comparison Geometry}
\author{Masanari Kimura \\
School of Mathematics and Statistics, \\ The University of Melbourne
\\
\texttt{m.kimura@unimelb.edu.au} \\
\And Howard Bondell \\
School of Mathematics and Statistics, \\ The University of Melbourne \\
\texttt{howard.bondell@unimelb.edu.au}
}

\begin{document}
\maketitle


% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \end{icmlauthorlist}

% \icmlaffiliation{uom}{School of Mathematics and Statistics, \\ The University of Melbourne}

% \icmlcorrespondingauthor{Masanari Kimura}{m.kimura@unimelb.edu.au}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}


% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Fréchet regression extends classical regression methods to non-Euclidean metric spaces, enabling the analysis of data relationships on complex structures such as manifolds and graphs. This work establishes a rigorous theoretical analysis for Fréchet regression through the lens of comparison geometry which leads to important considerations for its use in practice. The analysis provides key results on the existence, uniqueness, and stability of the Fréchet mean, along with statistical guarantees for nonparametric regression, including exponential concentration bounds and convergence rates. Additionally, insights into angle stability reveal the interplay between curvature of the manifold and the behavior of the regression estimator in these non-Euclidean contexts. Empirical experiments validate the theoretical findings, demonstrating the effectiveness of proposed hyperbolic mappings, particularly for data with heteroscedasticity, and highlighting the practical usefulness of these results.
\end{abstract}

\section{Introduction}
Fréchet regression~\citep{petersen2019frechet} is a powerful statistical tool for analyzing relationships between variables when the response or predictor lies in a non-Euclidean space.
It generalizes classical regression to settings where the response variable $Y$ resides in a metric space $\mathcal{M}$.
Given predictors $X$, Fréchet regression seeks to estimate the conditional Fréchet mean.
\begin{equation}
    \mu(x) = \argmin_{m \in \mathcal{M}} \mathbb{E}\left[d^2(Y, m) \mid X = x \right], \label{eq:conditional_frecet_mean}
\end{equation}
where $d$ is the metric on $\mathcal{M}$.
This approach accommodates data in various non-Euclidean spaces, such as manifolds, trees, and graphs~\citep{lin2021total,ferguson2022computation,ghosal2023application,qiu2024random,chen2022uniform}.
In recent years, several variants of Fréchet regression have been proposed~\citep{tucker2023variable,bhattacharjee2023single,song2023errors,ghosal2023frechet,zhang2024dimension,yan2024frequentist}, each addressing different aspects such as variable selection, error modeling, and high-dimensional data handling.
However, most existing studies primarily focus on specific geometric settings or lack a comprehensive theoretical framework that accounts for varying curvature bounds.
This study fills this gap by leveraging comparison geometry to provide a unified theoretical analysis of Fréchet regression across $\mathrm{CAT}(K)$ spaces with diverse curvature properties.

Fréchet regression allows the assumption of a non-Euclidean space in the space of the data, so one can expect that its behavior can be described depending on the geometrical properties of the space.
To investigate this, this study utilizes comparison geometry, which is a fundamental branch of differential geometry that investigates the geometric properties of a given space by comparing it to model spaces of constant curvature~\citep{cheeger1975comparison,grove1997comparison,cheeger2007metric,wei2009comparison}.
Unlike information geometry~\citep{amari2016information,ay2017information,nielsen2020elementary,amari2000methods,kimura2021alpha,kimura2022information}, which focuses on general statistical manifolds, this framework leverages classical comparison theorems to derive insights about the structure and behavior of more complex or less regular spaces.
By establishing inequalities and structural similarities between a target space and well-understood model spaces (e.g., Euclidean, spherical, or hyperbolic geometries), comparison geometry enables the extension of geometric and topological results to broader contexts, including spaces that may lack smoothness or traditional manifold structures.
In this framework, $\mathrm{CAT}(K) $ spaces are pivotal objects of study, which are the generalization of constant curvature space~\citep{ballmann1995lectures,jost2012nonpositive,bridson2013metric}.
$\mathrm{CAT}(K)$ spaces are geodesic metric spaces, where geodesic triangles are thinner than their comparison triangles in the model space of constant curvature $K$.
Consider several known examples of $\mathrm{CAT}(K)$ spaces.
Euclidean spaces $\mathbb{R}^n$ are classic examples with $K=0$, exhibiting flat geometry.
Hyperbolic spaces, which have constant negative curvature ($K < 0$), serve as models for spaces exhibiting exponential growth and are useful in areas like network analysis and evolutionary biology.
On the other hand, trees can be viewed as $\mathrm{CAT}(0)$ spaces, providing a discrete analog with unique geodesics between points.
Additionally, certain types of manifold structures used in shape analysis and computer graphics also qualify as $\mathrm{CAT}(K)$ spaces under specific curvature conditions.
These examples demonstrate the broad applicability of $\mathrm{CAT}(K)$ spaces in modeling diverse geometric contexts encountered in statistical analysis.
By considering such spaces, this study aims to describe the behavior of the Fréchet regression in terms of curvature $K$ in particular.

% \subsection{Motivation}
% The choice of comparison geometry as the foundational framework for this study stems from its powerful ability to relate complex metric spaces to well-understood model spaces with constant curvature.
% By leveraging comparison theorems, researchers can transfer geometric and topological properties from these model spaces to more intricate or less regular spaces where Fréchet regression is applied.
% This framework not only facilitates a deeper theoretical understanding of regression behavior in non-Euclidean contexts but also aids in deriving generalizable results that hold across various geometric settings.
% Consequently, comparison geometry serves as an indispensable tool in bridging the gap between abstract geometric concepts and practical statistical applications.

\section{Notation}
In this section, the notations and definitions required for the following analysis are organized.
Let $\mathcal{M}$ be a metric space and $d$ be the metric on $\mathcal{M}$.
Here, the metric space $(\mathcal{M}, d)$ is geodesic space if every pair of points in $\mathcal{M}$ can be connected by a geodesic, a curve whose length equals the distance between the points.
\begin{definition}[$\mathrm{CAT}(K)$ space]
    Let $(\mathcal{M}, d)$ be a geodesic metric space and let $K \in \mathbb{R}$.
    The space $\mathcal{M}$ is said to be a $\mathrm{CAT}(K)$ space if it satisfies the following curvature condition:
    for any geodesic triangle $\triangle pqr$ in $\mathcal{M}$ with perimeter less than $2 D_K$ (where $D_K = \pi / \sqrt{K}$ if $K > 0$, and $D_K = \infty$ otherwise), and for any points $x, y$ on the edges $[pq]$ and $[qr]$ respectively, the distance between $x$ and $y$ in $\mathcal{M}$ does not exceed the distance between the corresponding points $\bar{x}$ and $\bar{y}$ on the comparison triangle $\triangle \bar{pqr}$ in the model space of constant curvature $K$:
    \begin{align*}
        d(x, y) \leq d_{\mathbb{M}^2_K}(\bar{x}, \bar{y}),
    \end{align*}
    where the comparison triangle $\triangle \bar{pqr}$ is a triangle in the simply connected, complete 2-dimensional Riemannian manifold $\mathbb{M}^2_K$ of constant curvature $K$ that preserves the side lengths as $d_{\mathbb{M}^2_K}(\bar{p}, \bar{q}) = d(p, q)$, $d_{\mathbb{M}^2_K}(\bar{q}, \bar{r}) = d(q, r)$, and $d_{\mathbb{M}^2_K}(\bar{r}, \bar{p}) = d(r, p)$.
\end{definition}


\begin{definition}[Geodesic convexity]
    \label{def:geodesic_convexity}
    A function $f\colon \mathcal{M} \to \mathbb{R}$ is geodesically convex if for every geodesic $\gamma \colon [0, 1] \to \mathcal{M}$, $f(\gamma(t)) \leq (1 -t) f(\gamma(0)) + t f(\gamma(1))$, for all $t \in [0, 1]$.
\end{definition}

\begin{definition}[$\lambda$-strong geodesic convexity]
    \label{def:strong_geodesic_convexity}
    A function $f\colon \mathcal{M} \to \mathbb{R}$ is $\lambda$-strongly geodesically convex around $p \in \mathcal{M}$ if there exists a constant $\lambda > 0$ depending only on $K$ and $\mathrm{diam}(\mathcal{M})$ such that
    \begin{equation}
        f(x) - f(p) \geq \lambda d^2(x, p),
    \end{equation}
    for every $x \in \mathcal{M}$.
\end{definition}

\begin{definition}[Lower semicontinuity]
    \label{def:lower_semicontinuity}
    A functional $F \colon \mathcal{M} \to \mathbb{R} \cup \{+\infty\}$ is lower semicontinuous at a point $x \in \mathcal{M}$ if for every sequence $\{x_n\}$ converging to $x$, it satisfies
    \begin{equation}
        F(x) \leq \liminf_{n \to +\infty} F(x_n).
    \end{equation}
\end{definition}

\begin{definition}[Weak convergence in metric space]
    \label{def:weak_convergence}
    A sequence of probability measures $\{\nu_n\}$ on $\mathcal{M}$ is said to converge weakly to a probability measure $\nu$ (denoted by $\nu_n \Rightarrow \nu$) if for every bounded continuous function $f\colon \mathcal{M} \to \mathbb{R}$,
    \begin{equation*}
        \lim_{n \to +\infty}\int_\mathcal{M} f(y) d\nu_n(y) = \int_\mathcal{M} f(y) d\nu(y).
    \end{equation*}
\end{definition}

\begin{definition}[Alexandrov angle]
    \label{def:alexandrov_angle}
    The Alexandrov angle $\angle_x(y, z)$ is defined as the limit of secular angles between short sub‐segments.
    Concretely, if $y'$ is a point on $[xy]$ with $d(x, y') \to 0$ and $z'$ is a point on $[xz]$ with $d(x, z') \to 0$.
    Then,
    \begin{equation*}
        \angle_x(y, z) \coloneqq \lim_{y' \to x, z' \to x} \angle^{(\mathrm{sec})}_x(y' z'),
    \end{equation*}
    where $\angle^{(\mathrm{sec})}_x(y' z')$ is the ordinary angle in the comparison triangle for $\triangle xy'z'$ in the model space.
\end{definition}

\begin{definition}[Riemannian exponential map]
    \label{def:riemannian_exponential_map}
    Let $T_z\mathcal{M}$ be the tangent space of $\mathcal{M}$ at a point $z \in \mathcal{M}$.
    For a fixed point $z$, the Riemannian exponential map at $z$, denoted by $\exp_z$ is a map from the tangent space at $z$ to the manifold $\mathcal{M}$: $\exp_z \colon T_z\mathcal{M} \to \mathcal{M}$.
    Here, the Riemannian exponential map is constructed as
    \begin{itemize}
        \item[i)] Choose a tangent vector $v \in T_z\mathcal{M}$.
        \item[ii)] Consider the unique geodesic $\gamma_v(t)$ emanating from $z$ with initial velocity $v$.
        Formally, $\gamma_v(t)$ satisfies $\gamma_v(0) = z$ and $\gamma'_v(0) = v$.
        \item[iii)] The exponential map sends the tangent vector $v$ to the point on the manifold reached by traveling along the geodesic $\gamma_v$ for unit time, $\exp_z(v) = \gamma_v(1)$.
    \end{itemize}
\end{definition}

\section{Theory}
\label{sec:theory}
See Appendix~\ref{apd:proofs} for complete proofs of all statements.

\subsection{Existence and Uniqueness of the Fréchet Mean}
\label{sec:theory:existence_and_uniqueness}
First, it can be shown that in $\mathrm{CAT}(K)$ spaces with $K \leq 0$, the convexity properties ensure the existence and uniqueness of the Fréchet mean under mild conditions.
For $\mathrm{CAT}(K)$ spaces with $K > 0$, additional constraints on the diameter of the space may be necessary to ensure uniqueness due to potential multiple minima arising from positive curvature.

\begin{lemma}
    \label{lem:convexity_of_squared_distance_function}
    Let $(\mathcal{M}, d)$ be a $\mathrm{CAT}(K)$ space for $K \leq 0$.
    For any fixed point $p \in \mathcal{M}$, the function $f\colon \mathcal{M} \to \mathbb{R}$ defined by $f(x) = d^2(p, x)$ is geodesically convex.
\end{lemma}

Lemma~\ref{lem:convexity_of_squared_distance_function} establishes that the squared distance function retains geodesic convexity in $\mathrm{CAT}(K)$ spaces with non-positive curvature.
This property is fundamental because it ensures that the Fréchet functional, which aggregates squared distances, inherits convexity.
Consequently, optimization procedures to find the Fréchet mean are well-behaved, avoiding local minima and guaranteeing global optimality under the given conditions.

\begin{lemma}
    \label{lem:existence_minimizer_in_complete_cat_k}
    Let $(\mathcal{M}, d)$ be a complete $\mathrm{CAT}(K)$ space.
    For any probability measure $\nu$ on $\mathcal{M}$ with compact support, there exists at least one minimizer $m \in \mathcal{M}$ of the Fréchet functional:
    \begin{equation*}
        m = \argmin_{x \in \mathcal{M}}\int_\mathcal{M} d^2(y, x)d\nu(y).
    \end{equation*}
\end{lemma}

\begin{lemma}
    \label{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k}
    Let $(\mathcal{M}, d)$ be a $\mathrm{CAT}(K)$ space with $K \leq 0$ that is strictly geodesically convex, meaning that the squared distance function $f(x) = d^2(p, x)$ is strictly geodesically convex for any fixed point $p \in \mathcal{M}$.
    Then, for any probability measure $\nu$ on $\mathcal{M}$ with compact support, the Fréchet mean $m$ is unique.
\end{lemma}

Based on Lemma~\ref{lem:convexity_of_squared_distance_function}, which ensures geodesic convexity of the squared distance function in non-positively curved $\mathrm{CAT}(K)$ spaces, and Lemma~\ref{lem:existence_minimizer_in_complete_cat_k}, which guarantees the existence of a Fréchet mean under compact support, one can establish the stability of the Fréchet mean under measure perturbations.
Furthermore, Lemma~\ref{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k} ensures uniqueness under strict geodesic convexity, thereby enabling Proposition~\ref{prp:stability_non_positive_curvature} to assert the convergence of Fréchet means in non-positively curved spaces.
\begin{proposition}
    \label{prp:stability_non_positive_curvature}
    Let $(\mathcal{M}, d)$ be a $\mathrm{CAT}(K)$ space with $K \leq 0$.
    Suppose $\{\nu_n\}$ is a sequence of probability measures on $\mathcal{M}$  that converges weakly to a probability measure $\nu$.
    Assume that for each $n$, the measure $\nu_n$ has a unique Fréchet mean $m_n$, and $\nu$ also has a unique Fréchet mean $m$.
    Then, the sequence of Fréchet means $\{m_n\}$ converges to $m \in \mathcal{M}$.
\end{proposition}
Proposition~\ref{prp:stability_non_positive_curvature} claims that the $\mathrm{CAT}(K)$ condition with $K \leq 0$ ensures that the space is non-positively curved, which imbues the space with strict convexity properties crucial for the uniqueness and stability of minimizers.
This geometric structure prevents the existence of multiple local minima, thereby facilitating the continuity of minimizers under perturbations of the measure.
Here, the stability of the Fréchet mean under measure perturbations is foundational for Fréchet regression.
It ensures that as predictors vary and induce changes in the conditional distributions of responses, the conditional Fréchet means (regression estimates) behave predictably and converge appropriately as sample size increases.

\begin{proposition}
    \label{prp:compactness_criterion_for_uniqueness_positive_curvature}
    Let $(\mathcal{M}, d)$ be a $\mathrm{CAT}(K)$ space with positive curvature bound $K > 0$.
    If the diameter of the support of the probability measure $\nu$, denoted by $\mathrm{diam}(\mathrm{supp}(\nu))$, satisfies $\mathrm{diam}(\mathrm{supp}(\nu)) < \frac{\pi}{2\sqrt{K}}$,
    then the Fréchet mean $m$ of $\nu$ is unique.
\end{proposition}
In Proposition~\ref{prp:compactness_criterion_for_uniqueness_positive_curvature}, the diameter constraint ensures that all points in the support of $\nu$ lie within a geodesic ball of radius $R = \pi / 2\sqrt{K}$.
In $\mathrm{CAT}(K)$ spaces with $K > 0$, such balls are geodesically convex, meaning any geodesic between two points within the ball lies entirely inside the ball.
This local convexity is crucial for preserving strict convexity properties of the Fréchet functional.
Here, the strict convexity implies that the Fréchet functional cannot have multiple minimizers within the convex neighborhood defined by the diameter constraint.
If two distinct minimizers existed, the functional would attain a strictly lower value at intermediate points along the geodesic connecting them, violating their minimality.
One can see that exceeding this bound could allow the support to span regions where the curvature induces multiple local minima of the Fréchet functional.

In addition, applying Lemmas~\ref{lem:existence_minimizer_in_complete_cat_k} and~\ref{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k}, the following theorem can be obtained.
\begin{theorem}
    \label{thm:existence_uniqueness_conditional_frecet}
    Let $(\mathcal{M}, d)$ be a complete $\mathrm{CAT}(K)$ space and consider a conditional distribution $\nu_x$ of $Y$ given $X = x$.
    If for each $x$, the support of $\nu_x$ satisfies
    \begin{equation*}
        \mathrm{diam}(\mathrm{supp}(\nu_x)) < D_K = \begin{cases}
            +\infty & \text{if $K \leq 0$}, \\
            \frac{\pi}{\sqrt{K}} & \text{if $K > 0$},
        \end{cases}
    \end{equation*}
    then then the conditional Fréchet mean in Eq.~\eqref{eq:conditional_frecet_mean} exists and is unique for each $x$.
\end{theorem}

% \noindent\textbf{Implications:}  
% The results presented in Section~\ref{sec:theory:existence_and_uniqueness} establish foundational guarantees for Fréchet regression in $\mathrm{CAT}(K)$ spaces.
% Specifically, Lemma~\ref{lem:convexity_of_squared_distance_function} and Lemma~\ref{lem:existence_minimizer_in_complete_cat_k} ensure that the Fréchet mean exists under broad conditions, while Lemma~\ref{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k} and Proposition~\ref{prp:compactness_criterion_for_uniqueness_positive_curvature} guarantee its uniqueness in spaces with non-positive curvature or under diameter constraints in positively curved spaces.
% These guarantees are critical for Fréchet regression as they ensure that the conditional Fréchet mean, which serves as the regression function, is well-defined and uniquely identifiable.


\subsection{Convergence Rates and Concentration}
\label{sec:theory:convergence_rates_and_concentration}
Let $\hat{\mu}^*_n$ denote a nonparametric Fréchet regression estimator (e.g., Nadaraya–Watson–type kernel smoothing~\citep{nadaraya1964estimating,watson1964smooth,bierens1988nadaraya} on the predictor space).
Then, the following statements for the concentration results, the pointwise consistency, and rates of convergence can be obtained.
The important point is that one has to rely on exponential concentration inequalities valid in $\mathrm{CAT}(K)$ spaces (e.g., specific versions of concentration of measure or deviation bounds for Fréchet means).

\begin{theorem}[Concentration for the sample Fréchet mean]
    \label{thm:concentration_for_sample_frechet_mean}
    Let $(\mathcal{M}, d)$ be a complete $\mathrm{CAT}(K)$ space of diameter at most $D$.
    Suppose that $Y_1, Y_2,\dots, Y_n$ are independent and identically distributed random points in $\mathcal{M}$, and let $\mu$ and $\hat{\mu}_n$ be the population and sample Fréchet mean. 
    \begin{align*}
        \mu &\coloneqq \argmin_{z \in \mathcal{M}}\mathbb{E}[d^2(Y, z)], \\
        \hat{\mu} &\coloneqq \argmin_{z \in \mathcal{M}}\frac{1}{n}\sum^n_{i=1}d^2(Y_i, z).
    \end{align*}
    Assume further that each $d^2(Y_i, z)$ is essentially bounded by $D^2$, or more generally that $d^2(Y_i, z)$ has sub-Gaussian tails uniformly in $z$.
    Then there exists $\delta > 0$ such that for every $\epsilon > 0$,
    \begin{equation}
        \mathbb{P}\left[d(\hat{\mu}, \mu) > \epsilon \right] \leq 2\left(\frac{\alpha(K, D) D}{\delta}\right)^m e^{-\frac{n(\alpha(K, D)\epsilon^2)^2}{8D^2}},
    \end{equation}
    where $m$ is the dimension of the manifold, and $\alpha(K, D)$ is the strong convexity constant.
\end{theorem}
\begin{proof}[Sketch of Proof]
    i) The key is that in a $\mathrm{CAT}(K)$ space, with small diameter (or global non-positive curvature), the map $z \mapsto d(Y, z)$ is geodesically convex (or strictly convex in the sense of comparison).
    ii) One then applies concentration-of-measure arguments akin to those used for vector-valued means, taking advantage of the fact that variance-like functionals have a unique minimizer and that small fluctuations in the empirical mean lead to exponential tail bounds.
\end{proof}

In addition to the concentration for the sample Fréchet mean in the standard sense, the following proposition gives the concentration in $L_p$ sense.
\begin{proposition}
    \label{prp:l_p_concentration}
    Under the hypotheses of Theorem~\ref{thm:concentration_for_sample_frechet_mean}, there exist explicit constants $C_p(K, D)$ such that for any integer $n \geq 1$ and $p \geq 1$,
    \begin{align}
        \mathbb{E}[d^p(\hat{\mu}_n, \mu)] \leq C_p(K, D)(n^{-p/2}).
    \end{align}
    That is, $d(\hat{\mu}_n, \mu)$ converges to 0 in $L^p$ at a rate on the order of $n^{-p/2}$.
\end{proposition}
\begin{proof}[Sketch of Proof]
    This follows from integrating the exponential tail bound in Theorem~\ref{thm:concentration_for_sample_frechet_mean}.
    The boundedness of $\mathcal{M}$ (or sub-Gaussian tails for $Y$) is used to control moments of the distance.
\end{proof}

Moreover, the following theorem gives the pointwise consistency of nonparametric Fréchet regression in a $\mathrm{CAT}(K)$ space.
The main idea parallels classical kernel‐based regression arguments in $\mathbb{R}^d$, but replaces ordinary arithmetic means by Fréchet means in the metric space $(\mathcal{M}, d)$.
\begin{assumption}[Kernel LLN condition]
    \label{asm:kernel_lln_condition}
    For any bounded (or square‐integrable) function $f\colon \mathcal{M} \to \mathbb{R}$, nonnegative weights $\{w_{n,i}(x)\}^n_{i=1}$ satisfies
    \begin{align}
        \sum^n_{i=1}w_{n,i}(x)f(Y_i) \overset{a.s.}{\underset{n\to\infty}{\to}} \mathbb{E}[f(x) \mid X = x].
    \end{align}
\end{assumption}
\begin{theorem}[Pointwise consistency of nonparametric Fréchet regression]
    \label{thm:pointwise_consistency_of_nonparametric_frechet_regression}
    Let $\{(X_i, Y_i)\}^n_{i=1}$ be i.i.d. sample with $X_i \in \mathbb{R}^d$ and $Y_i \in \mathcal{M}$, where $(\mathcal{M}, d)$ is a complete $\mathrm{CAT}(K)$ space with diameter $\mathrm{diam}(\mathcal{M}) \leq D$.
    Define the population Fréchet regression function:
    \begin{align*}
        \mu^*(x) \coloneqq \argmin_{z \in \mathcal{M}} \mathbb{E}[d^2(Y, z) \mid X = x].
    \end{align*}
    Assume that $\mu^*(x)$ is well‐defined and unique for each $x$, provided as Theorem~\ref{thm:existence_uniqueness_conditional_frecet}
    Also, let $\{w_{n,i}(x)\}^n_{i=1}$ be nonnegative weights that sum to $1$ for each fixed $x$.
    For instance, in kernel regression, one sets
    \begin{align*}
        w_{n, i}(x) = \frac{W(\|x - X_i\| / h_n)}{\sum^n_{j=1}W(\|x - X_j\| / h_n)},
    \end{align*}
    where $W(\cdot)$ is a usual kernel (with compact support or exponential decay), and $h_n \to 0$ is a bandwidth.
    Define the nonparametric Fréchet‐regression estimator at $x$ by
    \begin{align}
        \hat{\mu}^*_n(x) = \argmin_{z \in \mathcal{M}}\sum^n_{i=1} w_{n,i}(x) d^2(Y_i, z).
    \end{align}
    Then, under mild regularity conditions on the weights in Assumption~\ref{asm:kernel_lln_condition}, $\hat{\mu}^*_n(x) \overset{a.s.}{\underset{n\to\infty}{\to}} \mu^*(x)$,
    for each fixed $x \in \mathbb{R}^d$.
\end{theorem}
\begin{proof}[Sketch of Proof]
    i) By definition, $\hat{\mu}^*(x)$ minimizes the empirical Fréchet functional weighted by $w_{n,i}(x)$.
    ii) As $n \to \infty$, for each fixed $x$ the weighted empirical distribution converges (in the sense of weak convergence or weighted law of large numbers) to the conditional distribution of $Y \mid X = x$.
    iii) The unique minimizer of the limiting Fréchet functional is $\mu^*(x)$.
    iv) Continuity and (local) geodesic convexity arguments in $\mathrm{CAT}(K)$ spaces yield consistency.
\end{proof}

Here, additional assumptions allow us to obtain the convergence rates in $\mathrm{CAT}(K)$ spaces.
\begin{theorem}[Convergence rates in $\mathrm{CAT}(K)$ spaces]
    \label{thm:convergence_rates_in_cat_k}
    Under the assumptions of Theorem~\ref{thm:pointwise_consistency_of_nonparametric_frechet_regression}, suppose additionally:
    \begin{itemize}
        \item $\mu^* \colon \mathbb{R}^d \to \mathcal{M}$ is $\beta$-Hölder (or Lipschitz) continuous, with respect to the usual Euclidean norm on $\mathbb{R}^d$ and the distance $d$ on $\mathrm{CAT}(K)$.
        That is, there exists $L > 0$ and $\beta > 0$ such that
        \begin{align}
            d(\mu^*(x), \mu^*(x')) \leq L \cdot \|x - x'\|^\beta,
        \end{align}
        for all $x, x' \in \mathbb{R}^d$.
        \item The kernel weights $w_{n,i}(x)$ satisfy standard nonparametric conditions:
        \begin{align}
            \sum^n_{i=1}w_{n,i}(x) &= 1,\ w_{n,i}(x) \approx W\left(\frac{\| x - X_i \|}{h_n}\right), \nonumber \\
            h_n \to 0, &\quad n h_n^d \to +\infty.
        \end{align}
        \item Each conditional distribution $Y \mid X = x$ has finite second moments in the $\mathrm{CAT}(K)$ space and a unique Fréchet mean $\mu^*(x)$.
        \item The distribution of $Y \mid X = x$ varies smoothly in a local neighborhood of $x$. Formally, one assumes that for $x'$ near $x$, the conditional distributions $\mathbb{P}[Y \in \cdot \mid X = x']$ do not differ too much, ensuring small bias when $x' \approx x$.
    \end{itemize}
    Then for the nonparametric Fréchet regression estimator $\hat{\mu}^*_n$,
    \begin{align}
        \sup_{x \in \mathcal{X}_0}\mathbb{E}\left[d^2(\hat{\mu}^*_n(x), \mu^*(x))\right] = O\left(\frac{1}{n h^d_n} + h_n^{2\beta}\right),
    \end{align}
    where $\mathcal{X}_0 \subseteq \mathbb{R}^d$ is any compact subset over which the kernel is applied.
\end{theorem}
\begin{proof}[Sketch of Proof]
    i) The proof parallels the standard bias-variance decomposition in kernel regression.
    ii) One controls the variance term by using Theorem~\ref{thm:concentration_for_sample_frechet_mean}–type concentration results for Fréchet means in small neighborhoods (small variance region).
    iii) Also, one controls the bias term via the assumed Hölder (or Lipschitz) continuity of $\mu^*$ plus the continuity of the conditional distributions in $x$.
    iv) Combining these yields the classical balance of nonparametric regression, now in a $\mathrm{CAT}(K)$ framework.
\end{proof}
From the above theorem, one can see that the usual $\left(\frac{1}{n h^d_n} + h_n^{\beta}\right)$ trade‐off from Euclidean nonparametric statistics carries over to the $\mathrm{CAT}(K)$ setting, once one accounts for i) geodesic convexity for controlling variance and ii) the Hölder continuity of $\mu^*(x)$ for controlling bias.

\noindent\textbf{Implications:}  
Section~\ref{sec:theory:convergence_rates_and_concentration} provides the statistical properties of Fréchet regression estimators within $\mathrm{CAT}(K)$ spaces.
Theorem~\ref{thm:concentration_for_sample_frechet_mean} offers exponential concentration bounds for the sample Fréchet mean, indicating that the estimator converges to the true mean with high probability as the sample size increases. 
Proposition~\ref{prp:l_p_concentration} further quantifies this convergence in an $L^p$ sense, demonstrating that the expected distance between the sample and population Fréchet means decreases at a rate proportional to $n^{-1/2}$.
These results are pivotal for understanding the efficiency and reliability of Fréchet regression estimators.
They assure that given sufficient data, the regression estimates will not only be consistent but also achieve convergence rates comparable to those observed in classical Euclidean nonparametric regression.


\subsection{Angle Stability for Conditional Fréchet Means}
\label{sec:theory:angle_stability_for_conditiona_frechet_means}
Understanding not just the position but also the directional relationships around the Fréchet mean is crucial for capturing the local geometry of the data distribution.
Angle stability ensures that small perturbations in the underlying probability measures or data configurations do not lead to significant distortions in the angular relationships among points relative to the Fréchet mean.
This property is particularly valuable when analyzing directional data or when the regression function's local behavior depends on angular relationships, such as shape analysis or directional statistics.

First, the following lemma for the angle comparison in $\mathrm{CAT}(K)$ spaces is provided.
\begin{lemma}
    \label{lem:angle_comparison}
    Let $(\mathcal{M}, d)$ be a $\mathrm{CAT}(K)$ space, and let $\triangle xyz \subset \mathcal{M}$ be a geodesic triangle of perimeter $\leq \pi / \sqrt{K}$ when $K > 0$.
    Let $\triangle \bar{x}\bar{y}\bar{z}$ be its comparison triangle in the simply connected model space of constant curvature $K$.
    Then for each vertex $x$ and the corresponding comparison vertex $\bar{x}$, $\angle_x(y, z) \leq \angle_{\bar{x}}(\bar{y}, \bar{z})$,
    where $\angle_x(y, z)$ is the Alexandrov angle (or geodesic angle) at $x$ formed by the geodesic segments $[xy]$ and $[xz]$.
\end{lemma}
Note the assumption that the perimeter of $\triangle xyz$ is $\leq \pi / \sqrt{K}$ (when $K > 0$) is used to ensure
\begin{itemize}
    \item[i)] The geodesics $[xy]$, $[yz]$, $[zx]$ are short enough so that the entire triangle $\triangle xyz$ (and sub‐triangles $\triangle xy'z'$) can be compared in the standard simply connected model space (the sphere of radius $1 / \sqrt{K}$ if $K > 0$).
    \item[ii)] One avoids the potential degeneracy where side lengths might exceed $\pi / \sqrt{K}$, which could cause the model triangle in spherical geometry to become ambiguous or wrap around the sphere.
\end{itemize}
In the case $K \leq 0$, there is no maximum perimeter restriction because the simply connected model space (Euclidean or hyperbolic) is unbounded in diameter.

Next, the lemma for the angle continuity under small perturbation is provided.
\begin{lemma}
    \label{lem:angle_continuity}
    Let $\triangle pqr$ and $\triangle p'q'r'$ be two geodesic triangles in a $\mathrm{CAT}(K)$ space $(\mathcal{M}, d)$.
    Suppose each has a perimeter $\pi / \sqrt{K}$ when $K > 0$ (no restriction is needed if $K \leq 0$).
     Also assume $d(p, p') + d(q, q') + d(r, r')$ is small.
     Then, for the angles at $p$ in $\triangle pqr$ and at $p'$ in $\triangle p'q'r'$,
     \begin{align}
         |\angle_p(q, r) - \angle_{p'}(q', r')| \leq C\delta_{pp'qq'rr'},
     \end{align}
     where $C > 0$ is a constant depending only on $K$ and the maximum side length (or perimeter) constraints, and
     \begin{align}
         \delta_{pp'qq'rr'} \coloneqq d(p, p') + d(q, q') + d(r, r').
     \end{align}
\end{lemma}

Based on the above lemmas, the following statements are obtained.
\begin{proposition}[Angle perturbation via conditional measures]
    \label{prp:angle_perturbation}
    Let $\{\nu_x\}$ be a family of probability measures on a $\mathrm{CAT}(K)$ space $(\mathcal{M}, d)$, each supported in a geodesic ball of diameter $\leq D = \pi / 2\sqrt{K}$ when $K > 0$.
    Let $\mu^*(x)$ be the unique Fréchet mean of $\nu_x$.
    Suppose $\nu_x$ and $\nu_{x'}$ are close in the Wasserstein metric on measures: $d_W(\nu_x, \nu_{x'}) \leq \epsilon$.
    Then, for any fixed $u, v \in \mathcal{M}$, one has
    \begin{align*}
        |\angle_{\mu^*(x)}(u, v) - \angle_{\mu^*(x')}(u, v)| \leq C\epsilon,
    \end{align*}
    where the constant $C > 0$ depends on the strong‐convexity modulus $\alpha(K, D)$.
    In particular, smaller $\epsilon$ implies the angles at $\mu^*(x)$ and $\mu^*(x')$ to points $u, v$ differ by at most $O(\epsilon)$.
\end{proposition}
\begin{proof}[Sketch of Proof]
    i) By definition, $\mu^*(x)$ minimizes $\int d^2(y, z)d\nu_x(y)$. Similarly, $\mu^*(x')$ does so for $\nu_{x'}$.
    ii) By strong geodesic convexity (via $\mathrm{CAT}(K)$ geometry), if $\mu^*(x)$ and $\mu^*(x')$ were far apart, that would imply a large gap in the Fréchet functionals, contradicting the smallness of $D_w(\nu_x, \nu_{x'})$. So $\mu^*(x) \approx \mu^*(x')$.
   iii) One then form triangles $\triangle\mu^*(x)u\mu^*(x')$ and $\triangle\mu^*(x)v\mu^*(x')$.
   Applying Lemma~\ref{lem:angle_continuity}, one see the angles at $\mu^*(x)$ and $\mu^*(x')$ differ by $C d(\mu^*(x), \mu^*(x'))$.
   iv) Combine with the uniform bound, finishing the proof.
\end{proof}

\begin{theorem}[Angle stability for conditional Fréchet means]
    \label{thm:angle_stability_conditional_frechet_means}
    Let $\{(X_i, Y_i)\} \subset \mathbb{R}^d \times \mathcal{M}$ with $\mathcal{M}$ a $\mathrm{CAT}(K)$ space of diameter $\leq D = \pi / 2\sqrt{K}$ if $K > 0$.
    For each $x \in \mathbb{R}^d$, let $\nu_x(\cdot)$ be the conditional distribution of $Y$ given $X = x$.
    Assume each $\nu_x$ has the unique Fréchet mean $\mu^*(x)$.
    Moreover, suppose that for $x, x'$ sufficiently close, the measures $\mu^*(x)$ and $\mu^*(x')$ differ by at most $\epsilon(\|x - x'\|)$ in the Wasserstein distance.
    Then for any finite set of points $\{u_1,\dots,u_m\} \subset \mathcal{M}$,
    \begin{align*}
        \sup_{1 \leq i < j \leq m} |\angle_{\mu^*(x)}(u_i, u_j) - \angle_{\mu^*(x')}(u_i, u_j)| \leq C\epsilon_{xx'},
    \end{align*}
    where $C > 0$ is a constant depending on the strong‐convexity modulus $\alpha(K, D)$ and $\epsilon_{xx'} = \epsilon(\|x - x'\|)$.
    Thus, all angles at $\mu^*(x)$ relative to a finite set of directions $u_1,\dots,u_m$ vary continuously and Lipschitzly with $x$.
\end{theorem}
\begin{proof}[Sketch of Proof]
    i) Apply Proposition~\ref{prp:angle_perturbation} to each pair $(u_i, u_j)$.
    ii) Use a union bound or net argument if one wants a finite set of directions $\{u_1,\dots,u_m\}$.
    iii) The constant $C$ grows modestly in $m$ (the number of directions) due to the union bound or covering dimension arguments.
\end{proof}

\noindent\textbf{Implications:}  
The established angle stability results in Section~\ref{sec:theory:angle_stability_for_conditiona_frechet_means} imply that the geometric structure surrounding the conditional Fréchet mean remains consistent under minor changes in the data distribution.
This consistency is essential for applications where the relative orientation of data points carries meaningful information, ensuring that the regression estimates preserve intrinsic geometric relationships.
% Consequently, angle stability enhances the robustness of Fréchet regression models in capturing and maintaining the underlying geometric patterns inherent in non-Euclidean data.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/manifold_mapping.png}
    \caption{Mapping from spherical data into hyperbolic space.}
    \label{fig:manifold_mapping}
\end{figure*}

\subsection{Local Jet Expansion of Fréchet Functionals}
\label{sec:theory:local_jet_expansion_of_frechet_functionals}

\begin{lemma}
    \label{lem:projection_of_angles_in_tangent_cones}
    Let $z \in \mathcal{M}$ and let $\exp_{z} \colon T_z\mathcal{M} \to \mathcal{M}$ be the Riemannian exponential map (in a local sense if $\mathcal{M}$ is a manifold, or a suitable geodesic parameterization if $\mathcal{M}$ is just a geodesic metric space).
    Then for points $u, v$ sufficiently close to $z$, define $U \coloneqq \exp_z^{-1}(u)$ and $V \coloneqq \exp_z^{-1}(v)$.
    Then,
    \begin{align*}
        \angle_z(u, v) = \angle_0(U, V) + O(\|\exp_z^{-1}(u)\|^2 + \|\exp_z^{-1}(v)\|^2),
    \end{align*}
    where $\angle_0(U, V)$ is the standard Euclidean angle in $T_z\mathcal{M} \approx \mathbb{R}^m$, and the big‐Oh term depends on curvature bounds near $z$.
\end{lemma}

\begin{proposition}[Local Jet expansion of Fréchet functionals]
    \label{prp:local_jet_expansion_of_frechet_functionals}
    Let $\nu$ be a probability measure on a sufficiently regular $\mathrm{CAT}(K)$ space $(\mathcal{M}, d)$.
    Suppose that $\mu(x)$ is the Fréchet mean of $\nu_x$: $\mu(x) \coloneqq \argmin_{z \in \mathcal{M}}\int d^2(y, z)d\nu_x(y)$, and consider the Fréchet functional $F_x(z) = \int d^2(y, z)d\nu_x(y)$.
    Then, in a sufficiently small neighborhood of $\mu$, the functional $F$ can be expanded in the tangent space $T_\mu\mathcal{M}$ via the exponential map. Specifically, using local coordinates $\exp_\mu \colon T_\mu\mathcal{M} \supset B_r(0) \to \mathcal{M}$, for a vector $v$ with $\|v\|$ small, define $z = \exp_\mu(v)$. The expansion is given by
    \begin{align*}
        F(\exp_\mu(v)) = F_x(\mu) + \langle \nabla F_x(\mu), v \rangle + \frac{1}{2}\langle H_x v, v\rangle + R(v),
    \end{align*}
    where $\nabla F_x(\mu)$ is the gradient (which is zero if $\mu$ is the unique minimizer), $H_x$ is the Hessian (a linear operator on $T_\mu\mathcal{M}$), and the remainder term $R(v)$ satisfies $|R(v)| = O(\|v\|^3)$.
\end{proposition}
% \begin{proof}[Sketch of Proof]
%     i) By second‐variation of energy arguments in $\mathrm{CAT}(K)$ spaces (or Riemannian manifold expansions if $\mathcal{M}$ is smooth), the functiona $F_x$ is twice differentiable at $\mu(x)$.
%     ii) The angle‐based stability ensures that local directions (in the tangent space) do not twist too quickly when $x \mapsto x'$.
%     iii) Hence, the Hessians $H_x$ (a linear operator in the tangent space) vary continuously with $x$.
% \end{proof}

\noindent\textbf{Implications:}  
The analysis in Section~\ref{sec:theory:local_jet_expansion_of_frechet_functionals} offers a nuanced understanding of the Fréchet functional's local behavior around its minimizer, the Fréchet mean.
By expanding the Fréchet functional in the tangent space via the exponential map, one can gain insights into the functional's curvature and higher-order properties.
% This local jet expansion is analogous to a Taylor series expansion in Euclidean spaces and is instrumental in deriving second-order properties of Fréchet regression estimators. 

\subsection{Auxiliary Statements}
\label{sec:theory:auxiliary_statements}

Here, a couple of auxiliary propositions that facilitate a deeper understanding of the structural properties of the Fréchet functional within $\mathrm{CAT}(K)$ spaces are introduced in this section.
These propositions decompose the Fréchet functional into radial and angular components, enabling a more nuanced analysis of variance and stability around the Fréchet mean.
% By isolating the contributions of radial distances and angular relationships, these auxiliary statements provide the necessary groundwork for subsequent theoretical developments, such as variance decomposition and stability analysis.
% This decomposition is particularly valuable for dissecting the interplay between geometric curvature and statistical estimation, ensuring that both distance and directional information are appropriately accounted for in the regression framework.

\begin{proposition}[Angle Splitting in Distance Sums]
    \label{prp:angle_splitting_in_distance_sums}
    Consider the Fréchet functional $F(z) = \int d^2(y, z)d\nu(y)$.
    For $z$ near $\mu^*$, decompose:
    \begin{align*}
        d^2(y, z) = d^2(y, \mu^*) + \Pi_{d}(y, z, \mu^*) + \Pi_{\angle}(y, z, \mu^*),
    \end{align*}
    where $\Pi_d$ captures radial changes in distances $\Pi_{\angle}$ represents angular corrections around $\mu^*$.
    If $\angle_{\mu^*}(y, z)$ remains small near $\mu^*$, then $\Pi_\angle$ is of order $\langle \angle_{\mu^*}(y, z) \rangle d(\mu^*, z)$.
\end{proposition}
% \begin{proof}[Sketch of Proof]
%     i) In a small ball around $\mu^*$, form triangles $\triangle \mu^* y z$.
%     ii) By angle comparison (e.g., Toponogov or expansions of the law of cosines in the manifold), one isolates the portion of $d^2(y, z)$ that depends on $\angle_{\mu^*}(y, z)$.
%     iii) If that angle is small or controlled, $\Pi_\angle$ must be correspondingly small (often second‐order in the distances).
% \end{proof}

\begin{proposition}[Angle–Distance Decomposition of Conditional Variance]
    \label{prp:angle_distance_decomposition}
    Let $\nu_x$ be the conditional distribution of $Y$ given $X = x$ on a  sufficiently smooth $\mathrm{CAT}(K)$ space $(\mathcal{M}, d)$.
    Suppose $\mu^*(x)$ is the unique Fréchet mean of $\nu_x$.
    Around $\mu^*(x)$, let
    \begin{align}
        R_x(y) \coloneqq d(y, \mu^*(x)), \quad \phi_x(y) \coloneqq \angle_{\mu^*(x)}(u_0, y),
    \end{align}
    for a fixed reference point $u_0 \in \mathcal{M}$.
    Then the conditional variance can be partially decomposed into a radial variance term, an angle–radial covariance term, and higher‐order corrections:
    \begin{align}
        &\mathrm{Var}_{\nu_x}\left[d^2(Y, \mu^*(x))\right] \nonumber \\
        &\quad\quad = \mathrm{Var}[A_x(Y)] + \mathrm{Cov}\left(\phi_x(Y), R_x(Y)^2\right) + \beta,
    \end{align}
    where $A_x$ is the radial part and $\beta$ is the higher-order term.
\end{proposition}
\begin{proof}[Sketch of Proof]
     i) Proposition~\ref{prp:angle_splitting_in_distance_sums} expresses $d^2(Y, \mu^*(x))$ in terms of radial and angular offsets.
     ii) The variance decomposition is akin to writing $\mathrm{Var}[r + \delta]$ in Euclidean expansions but now with an additional angular term.
     iii) For small angles, the correlation between $\phi_x(Y)$ and $R_x(Y)$ might be partial or vanish to second order, allowing a meaningful separation.
\end{proof}

\noindent\textbf{Implications:}
The auxiliary propositions presented in Subsection~\ref{sec:theory:auxiliary_statements} play an important role in refining the theoretical underpinnings of Fréchet regression within $\mathrm{CAT}(K)$ spaces.
By decomposing the Fréchet functional into radial and angular components, these propositions enable a more granular analysis of variance and stability around the Fréchet mean.

\section{Experiments}
\label{sec:experiments}
From the discussion in Section~\ref{sec:theory}, it can be seen that the negative curvature space has better properties in terms of estimation than the positive curvature space with broader support.
To confirm these results, this section considers numerical experiments.
See Appendix~\ref{apd:intuitive_understanding_for_hyperbolic_mapping} for the intuitive understanding of the following hyperbolic mapping.

\subsection{Illustrative Example}
\label{sec:experiments:illustrative_example}
A point on the unit sphere is parameterized as
\begin{align*}
    x = \sin(\phi)\cos(\theta),\  y = \sin(\phi)\sin(\theta),\  z = \cos(\phi),
\end{align*}
where $\phi \in [0, \pi]$ is the polar angle and $\theta \in [0, 2\pi]$ is the azimuthal angle.
Let $R$ be the radius of the sphere.
Here, consider the stereographic projection:
The plane is tangent to the sphere at the south pole $(0, 0, -R)$ and is defined $z = -R$, and the north pole $N = (0, 0, R)$ serves as the projection point.
For a point $p = (x, y, z)$, the stereographic projection $\pi(p) = (u, v)$ on the plane is given by
\begin{align*}
    u = \frac{Rx}{R + z}, \quad v = \frac{Ry}{R + z}.
\end{align*}
This plane can be considered in the hyperbolic space, and one can visualize it as the pseudosphere (see Figure~\ref{fig:manifold_mapping}).
Also, a point $(x, y, z)$ can be mapped back to the sphere as
\begin{align*}
    x = \frac{2R^2u}{R^2 + u^2 + v^2}, y = \frac{2R^2v}{R^2 + u^2 + v^2}, z = R\frac{u^2 + v^2 - R^2}{R^2 + u^2 + v^2}.
\end{align*}
\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \toprule
         Data manifold &  Mean squared error (MSE) \\
         \hline
         Sphere ($K = 1$) & $0.4915 (\pm 0.0086)$ \\
         Hyperbolic ($K = -1$) & $0.4228 (\pm 0.0021)$ \\
         \bottomrule
    \end{tabular}
    \caption{Evaluation of Fréchet regression on different spaces.}
    \label{tab:evaluation_of_frechet_regression}
\end{table}
See Appendix~\ref{sec:details_of_experiments} (including Python code in Listing~\ref{lst:python_code_hyperbolic_mapping}) for the detailed data-generating process.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/astronexus.png}
    \caption{Visualization of the HYG Stellar database.}
    \label{fig:astronexus}
\end{figure}

Table~\ref{tab:evaluation_of_frechet_regression} shows the evaluation results of Fréchet regression on the spherical and hyperbolic coordinates.
It can be seen that the hyperbolic mapping yields better results.
Note that, the previous studies~\citep{downs2003spherical,eybpoosh2022applying} reported the effectiveness of such mapping for statistical problems of spherical data, and the objective of experiments in this section is just to confirm the theoretical results.

\subsection{Experiment on Real-world Dataset}
\label{sec:experiments:real_world_dataset}

\begin{table}[H]
    \centering
    \begin{tabular}{c|c}
        \toprule
         Dataset &  MSE \\
         \hline
         HYG Stellar & $0.3765 (\pm 0.0036)$ \\
         USGS Earthquake & $0.5832 (\pm 0.0831)$ \\
         NOAA Climate & $0.4384 (\pm 0.0678)$ \\
         \hline
         HYG Stellar (hyperbolic) & $0.2660 (\pm 0.0032)$ \\
         USGS Earthquake (hyperbolic) & $0.4743 (\pm 0.0541)$ \\
         NOAA Climate (hyperbolic) & $0.3259 (\pm 0.0683)$\\
         \bottomrule
    \end{tabular}
    \caption{Evaluation of Fréchet regression on different spaces.}
    \label{tab:evaluation_of_frechet_regression_real_data}
\end{table}

In addition to the illustrative example, consider the experiments on the real-world datasets.
This section uses the following: i) HYG Steller database~\footnote{\url{https://github.com/astronexus/HYG-Database?tab=readme-ov-file}}, which is a comprehensive dataset containing information on stars brighter than magnitude 6.5.
% There are several versions of this dataset, and \path{v3/hyg_v38.csv} is used in our experiments.
ii) USGS Earthquake catalogue~\footnote{\url{https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_week.csv}}, represented in spherical coordinates.
iii) NOAA Climate data~\footnote{\url{http://celestrak.org/NORAD/elements/table.php?GROUP=weather&FORMAT=tle}}, from weather satellites.
% To visualize sphere data, one can represent the positions of stars or celestial objects using their Right Ascension (RA) and Declination (Dec) coordinates, where RA is analogous to longitude, measured in hours, minutes, and seconds (0h to 24h), and Dec is analogous to latitude, measured in degrees (-90° to +90°).
See Appendix~\ref{sec:experiments:real_world_dataset} for the details of this experiment (including Python code in Listing~\ref{lst:python_code_visualization_hyg} for the visualization and data format check of the dataset).

Table~\ref{tab:evaluation_of_frechet_regression_real_data} shows the experimental results of Fréchet regression on different coordinates for the real datasets.
The mapping procedure is the same as Section~\ref{sec:experiments:illustrative_example}.
As with the illustrative example, we can confirm that Fréchet regression on hyperbolic surfaces yields better results on the real datasets.
As discussed in more detail in Appendix~\ref{apd:intuitive_understanding_for_hyperbolic_mapping}, such a mapping of responses to hyperbolic space may be particularly useful when heteroscedasticity is assumed in the data.
Indeed, heteroscedasticity can be observed in the HYG Stellar dataset (see Figure~\ref{fig:heteroscedastic_hyg}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/heteroscedastic.png}
    \caption{Heteroscedasticity in the HYG Stellar dataset.}
    \label{fig:heteroscedastic_hyg}
\end{figure}

\section{Conclusion}
This study provides a comprehensive theoretical analysis of Fréchet regression within the framework of comparison geometry, focusing on $\mathrm{CAT}(K)$ spaces.
It establishes foundational results on the existence, uniqueness, and stability of the Fréchet mean under varying curvature conditions.
Notably, the analysis demonstrates how curvature properties influence statistical estimation, with non-positive curvature spaces offering advantageous stability and convergence properties.
The paper also extends statistical guarantees to nonparametric Fréchet regression, including exponential concentration bounds and convergence rates, which align with classical Euclidean results. 
Angle stability and local jet expansion further highlight the behavior of Fréchet functionals, offering geometric insights of regression in non-Euclidean spaces.
Experimental results support the theoretical findings, showing that hyperbolic mappings often improve performance under heteroscedasticity assumption.

\noindent\textbf{Limitations:}  
While this study provides a robust theoretical foundation for Fréchet regression in $\mathrm{CAT}(K)$ spaces, several limitations exist.
Firstly, the analysis predominantly focuses on spaces with constant curvature bounds, which may not encompass all practical scenarios where data resides in more heterogeneous geometric contexts.
Additionally, the reliance on strong convexity conditions and diameter constraints in positively curved spaces may restrict the applicability of the results.
As has been done in the information geometry framework~\citep{akaho2004pca,peter2008information,carter2011information,kimura2021generalized,kimura2024density,murata2004information,amari1998natural}, future work could explore relaxing assumptions, extending the framework to broader classes of metric spaces, and developing efficient algorithms.

\clearpage

\section*{Broader Impact Statement}
This paper presents work whose goal is to advance the field of statistics.
There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.

\bibliography{main}
\bibliographystyle{icml2025}

% ==========================
% Appendix
% ==========================
\appendix
\onecolumn
\input{appendix/proofs}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
