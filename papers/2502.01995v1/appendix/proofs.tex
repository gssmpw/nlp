\section{Intuitive Understanding for Hyperbolic Mapping}
\label{apd:intuitive_understanding_for_hyperbolic_mapping}
In regression analysis, transforming the response variable can often lead to improved model performance by stabilizing variance, normalizing distributions, or linearizing relationships.
A classical example is the logarithmic transformation $Y \mapsto \log(Y)$ which can enhance the performance of a linear regression model under certain conditions.
Similarly, mapping spherical responses into hyperbolic space can offer analogous benefits, particularly in scenarios where the data exhibits inherent geometric or hierarchical structures.

\paragraph{Log Transformation in Linear Regression}
Consider the simple linear regression model:
\begin{align*}
    Y = \beta X + \epsilon,
\end{align*}
where $Y$ is the response variable, $X$ is the predictor, $\beta$ is the regression coefficient, and $\epsilon$ is the error term with $\mathbb{E}[\epsilon] = 0$ and $\mathrm{Var}(\epsilon) = \sigma^2$.
Applying a logarithmic transformation to $Y$ yields
\begin{align*}
    \log(Y) &= \beta X + \epsilon, \\
    Y &= \exp(\beta X + \epsilon) = \exp(\beta X)\cdot\exp(\epsilon).
\end{align*}
Assuming $\epsilon$ is small and approximately normally distributed, $\exp(\epsilon)$ introduces multiplicative noise to $Y$ effectively stabilizing variance across different levels of $X$.
This transformation often reduces heteroscedasticity in the residuals, leading to improved regression performance.
Here, the heteroscedasticity refers to the phenomenon where the variability of the errors (or residuals) in a regression model is not constant across the range of predictor variables.

\begin{definition}[Heteroscedasticity]
    \label{def:heteroscedasticity}
    Consider a regression model:
    \begin{align*}
        Y_i = \beta X_i + \epsilon_i,
    \end{align*}
    where $\epsilon_i \sim \mathcal{N}(0, \sigma^2(X_i))$.
    Here, the variance of the error term $\sigma^2(X)$ depends on $X$.
    In a heteroscedastic model, the variance of $\epsilon_i$ is a function of the predictors $X_i$:
    \begin{align*}
        \mathrm{Var}(\epsilon_i \mid X_i) = \sigma^2(X_i).
    \end{align*}
    In contrast, for homoscedasticity, the variance of $\epsilon_i$ is constant.
\end{definition}

\paragraph{Hyperbolic Mapping via Stereographic Projection}
Analogous to the log transformation, hyperbolic mapping transforms the response variable into a space where the geometric structure can lead to improved regression characteristics.
The procedure involves mapping points from a spherical representation to a hyperbolic plane using stereographic projection.
A point on the unit sphere of radius $R$ is parameterized using spherical coordinates:
\begin{align*}
    x &= R\sin(\phi)\cos(\theta), \\
    y &= R\sin(\phi)\sin(\theta), \\
    z &= R\cos(\phi),
\end{align*}
where $\phi \in [0, \pi]$ is the polar angle and $\theta \in [0, 2\pi)$ is the azimuthal angle.
The stereographic projection maps a point $p = (x, y, z)$ on the sphere to a point $p \mapsto \psi(p) = (u, v)$ on the plane tangent to the sphere at the south pole $(0, 0, -R)$ and defined by $z = -R$.
The north pole $N = (0, 0, R)$ serves as the projection point.
The projection formulas are
\begin{align*}
    u &= \frac{Rx}{R + z}, \\
    v &= \frac{Ry}{R + z}.
\end{align*}
This plane can be interpreted as a model of hyperbolic space, specifically visualized as a pseudosphere, which inherently possesses properties conducive to handling hierarchical or tree-like data structures.

Both the logarithmic transformation and hyperbolic mapping aim to stabilize variance and linearize relationships, through different geometric transformations. 
To understand the benefits of hyperbolic mapping, consider the effect of each transformation on the variance of the response variable.
Starting with $Y = \beta X + \epsilon$, applying the log transformation yields
\begin{align*}
    \log Y = \beta X + \epsilon.
\end{align*}
Assuming $\epsilon \sim \mathcal{N}(0, \sigma^2)$, The variance of $\log Y$ remains $\sigma^2$ which can be advantageous if the original $Y$ exhibits multiplicative noise:
\begin{align*}
    \mathrm{Var}(Y) = \mathrm{Var}(\exp(\beta X + \epsilon)) = \exp(2\beta X) \cdot \left( \exp(\sigma^2) - 1 \right).
\end{align*}
The transformation effectively decouples the variance from $X$ stabilizing it across different predictor values.

For hyperbolic mapping, consider a response variable represented as a point on the sphere.
The stereographic projection transforms this spherical representation into the hyperbolic plane.
Let $Y$ be the original response mapped to a point $p = (x, y, z)$ on the sphere, and $\psi(p) = (u, v)$ its hyperbolic projection.
Assuming small deviations around a mean direction, the hyperbolic mapping can linearize angular variations similarly to how the log transformation linearizes multiplicative variations.
Specifically, fluctuations in $Y$ around the mean direction correspond to additive noise in the hyperbolic plane, potentially reducing variance in a manner akin to the log transformation.
Formally, if $Y$ is modeled on the sphere with
\begin{align*}
    Y = R \cdot p + \epsilon,
\end{align*}
where $\epsilon$ represents angular noise, the hyperbolic projection yields
\begin{align*}
    \psi(Y) = \left(\frac{Rx}{R + z}, \frac{Ry}{R + z}\right) + \epsilon',
\end{align*}
whre $\epsilon'$ is the transformed noise.
Under specific conditions (e.g., small angular deviations), $\epsilon'$ exhibits reduced variance compared to $\epsilon$, analogous to the variance stabilization achieved by the log transformation.

\begin{example}[Stabilizing Variance in Hierarchical Data]
    Consider a dataset where the response variable $Y$ represents hierarchical relationships, such as the popularity of topics in a taxonomy.
    The inherent tree-like structure implies that differences between nodes (topics) grow exponentially with depth.
    Direct regression on $Y$ would face increasing variance as depth increases.
    By mapping $Y$ into hyperbolic space via stereographic projection, the exponential growth inherent in hierarchical data is linearized.
    This transformation stabilizes variance across different levels of the hierarchy, enabling more effective regression modeling.
    Specifically, the hyperbolic mapping aligns the geometric properties of the data with the regression framework, similar to how the log transformation aligns multiplicative relationships with additive modeling.

    Let $Y$ be mapped to hyperbolic space via stereographic projection:
    \begin{align*}
        u &= \frac{Rx}{R + z}, \\
        v &= \frac{Ry}{R + z}.
    \end{align*}
    Assuming $Y$ lies close to the north pole $N = (0, 0, R)$, small perturbations $\epsilon$ around $N$ imply
    \begin{align*}
        z &= R\cos(\phi) \approx R\left(1 - \frac{\phi^2}{2}\right), \\
        x &= R\sin(\phi)\cos(\theta) \approx R\phi\cos(\theta), \\
        y &= R\sin(\phi)\sin(\theta) \approx R\phi\sin(\theta).
    \end{align*}
    Substituting into the projection formulas,
    \begin{align*}
        u &\approx \frac{R\cdot R\phi\cos(\theta)}{R + R\left(1 - \frac{\phi^2}{2}\right)} = \frac{R^2 \phi \cos(\theta)}{2R - \frac{\phi^2}{2}} \approx \frac{R\phi\cos(\theta)}{2}, \\
        v &\approx \frac{R \cdot R\phi\sin(\theta)}{R + R\left(1 - \frac{\phi^2}{2}\right)} = \frac{R^2 \phi \sin(\theta)}{2R - \frac{\phi^2}{2}} \approx \frac{R\phi\sin(\theta)}{2}.
    \end{align*}
    Thus, small angular deviations $\phi$ result in approximately linear changes in $u$ and $v$, effectively reducing the variance from multiplicative to additive in the hyperbolic plane:
    \begin{align*}
        \mathrm{Var}(u, v) \approx \left(\frac{R}{2}\right)^2\mathrm{Var}(\phi).
    \end{align*}
    Compared to the original spherical variance $\mathrm{Var}(\phi)$, the hyperbolic mapping scales and linearizes the variance, analogous to the stabilizing effect of the log transformation.
    Figure~\ref{fig:hyperbolic_log_mapping} shows the illustrative example of transformed responses for $Y = \beta X + \epsilon$ with heteroscedastic errors $\epsilon = \mathcal{N}(0, g(\sigma X))$, $\sigma = 0.2$ and $\beta = 2$.
    This figure shows $g(\sigma X) = \sigma X$ and $g(\sigma X) = \exp(\sigma X)$ cases.
\end{example}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/hyperbolic_log_mapping.png}
    \caption{Illustrative example of transformed responses.
    Under the heteroscedastic errors assumption, the appropriate transformations of response variable yield stabilized variance.
    In this figure, $Y$ is the original response variables, $\log(Y)$ is the log-transformed variables and $W$ is the hyperbolic mapped variables.}
    \label{fig:hyperbolic_log_mapping}
\end{figure}

\clearpage

\section{Proofs}
\label{apd:proofs}

\subsection{Proofs for Section~\ref{sec:theory:existence_and_uniqueness}}

\begin{proof}[Proof for Lemma~\ref{lem:convexity_of_squared_distance_function}]
    To establish the geodesic convexity of the squared distance function $f(x) = d^2(p, x)$ in a $\mathrm{CAT}(K)$ space $(\mathcal{M}, d)$ with $K \leq 0$, one must show that for any two points $x, y \in \mathcal{M}$ and any geodesic $\gamma\colon [0, 1] \to \mathcal{M}$ connecting $x$ to $y$, the function $t \mapsto f(\gamma(t))$ is convex on the interval $[0, 1]$.

    In the model space $\mathbb{M}^2_K$ of constant curvature $K \leq 0$, construct a comparison triangle $\bar{\triangle}$ corresponding to $\triangle = \{p, x, y\}$ in $\mathcal{M}$.
    Let $\bar{p}, \bar{x}, \bar{y}$ be the vertices of $\bar{\triangle}$ in $\mathbb{M}^2_K$ with side lengths matching those of $\triangle$.
    Then, for any points $a, b$ on the sides $[x, y]$ and $[p, x]$ or $[p, y]$, the distance $d(a, b)$ in $\mathcal{M}$ is at most the distance $d_{\mathbb{M}^2_K}(\bar{a}, \bar{b})$ in the model space.

    Let $\gamma(t)$ corresponds to a point $\bar{\gamma}(t)$ on the side $[\bar{x}, \bar{y}]$ in $\bar{\triangle}$.
    By the $\mathrm{CAT}(K)$ property,
    \begin{align*}
        d(p, \gamma(t)) \leq d_{\mathbb{M}^2_K}(\bar{p}, \bar{\gamma}(t)).
    \end{align*}
    In $\mathbb{M}^2_K$, which is a uniquely geodesic space, the squared distance satisfies the law of cosines
    \begin{align*}
        d^2(\bar{p}, \bar{\gamma}(t)) \leq (1 - t) d^2(\bar{p}, \bar{x}) + t d^2(\bar{p}, \bar{y}) - t(1 - t)c_K,
    \end{align*}
    where $c_K$ is a non-negative constant dependent on $K$ and the geometry of the triangle.
    Here, since $K \leq 0$, the space $\mathbb{M}^2_K$ exhibits non-positive curvature, which implies that the term $-t(1 - t)c_K$ does not negatively affect the inequality.
    Therefore,
    \begin{align*}
        d^2(p, \gamma(t)) \leq d^2_{\mathbb{M}^2)K}(\bar{p}, \bar{\gamma}(t)) \leq (1 - t)d^2(p, x) + t d^2(p, y),
    \end{align*}
    and $f$ is geodesically convex.
\end{proof}

\begin{proof}[Proof for Lemma~\ref{lem:existence_minimizer_in_complete_cat_k}]
    Consider a sequence $\{x_n\}$ in $\mathcal{M}$ that converges to $x \in \mathcal{M}$.
    Given the continuity of the distance function in metric spaces, for each $y \in \mathcal{M}$, $d(y, x_n) \to d(y, x)$ as $n \to +\infty$.
    Since $d^2(y, x)$ is continuous in $x$, by Fatou's lemma,
    \begin{align*}
        \liminf_{n \to +\infty} d^2(y, x_n) \leq d^2(y, x).
    \end{align*}
    Integrating both sides with respect to $\nu$,
    \begin{align*}
        \liminf_{n \to +\infty} \int_\mathcal{M} d^2(y, x_n) d\nu(y) \leq \int_\mathcal{M} d^2(y, x) d\nu(y).
    \end{align*}
    Thus, $F$ is lower semicontinuous.
    Also, since
    \begin{align*}
        F(x) = \int_\mathcal{M} d^2(y, x) d\nu(y) \geq 0,
    \end{align*}
    for any $x \in \mathcal{M}$, $F$ is bounded below by zero.
    Therefore, there exists a sequence $\{m_m\}$ in $\mathcal{M}$ such that
    \begin{align*}
        F(m_n) \to \inf_{x \in \mathcal{M}} F(x),
    \end{align*}
    as $n \to +\infty$.
    Let $\{m_n\}$ be called a minimizing sequence.
    Given that the support of $\nu$, denoted by $\mathrm{supp}(\nu)$, is compact, denote it by $S \subseteq \mathcal{M}$.
    That is, $S$ is compact and $\nu(S) = 1$.

    To ensure that the existence of a convergent subsequence, one need to show that $\{m_n\}$ is contained within a compact subset of $\mathcal{M}$.
    Since $S$ is compact, it is bounded.
    Thus, there exists a radius $R > 0$ and a point $p \in \mathcal{M}$ such that $S \subseteq B(p, R)$, where $B(p, R) = \{x \in \mathcal{M} \mid d(p, x) \leq R\}$.
    Using the triangle inequality in metric spaces,
    \begin{align*}
        d(y, m_n) \geq d(p, m_n) - d(y, p) \geq d(p, m_n) - R.
    \end{align*}
    Then,
    \begin{align*}
        F(m_n) &= \int_S d^2(y, m_n)d\nu(y) \\
        &\geq \int_S \left\{d(p, m_n) - d(y, p)\right\}^2 d\nu(y) \\
        &= \int_S \left\{d(p, m_n)^2 - 2d(p, m_n) + d^2(y, p)\right\}d\nu(y) \\
        &= d(p, m_n)^2 - 2d(p, m_n)\int_S d(y, p)d\nu(y) + \int_S d^2(y, p)d\nu(y) \leq C
    \end{align*}
    Let $A = \int_S d(y, p)\nu(y)$ and $B = \int_S d^2(y, p)d\nu(y)$, both finite due to the compactness.
    Thus,
    \begin{align*}
        d(p, m_n)^2 - 2A d(p, m_n) + B &\leq C \\
        d(p, m_n) &\leq A \pm \sqrt{A^2 + C - B}.
    \end{align*}
    Hence, the sequence $\{m_n\}$ lies within the closed ball $\overline{B}(p, A + \sqrt{A^2 + C - B})$, which is compact if $\mathcal{M}$ is proper.
    Here, $\mathrm{CAT}(K)$ spaces are not necessarily proper in general, bu since $\mathrm{supp}(\nu)$ is compact and $\{m_n\}$ is bounded, one can extract a convergent subsequence under the assumption that $\mathcal{M}$ is complete.
    Given that $\{m_n\}$ is bounded and $\mathcal{M}$ is complete, one can utilize the Bolzano-Weierstrass theorem in $\mathrm{CAT}(K)$ spaces to extract a convergent subsequence.
    Specifically, since $\mathcal{M}$ is a geodesic space and $\{m_n\}$ is bounded, there exists a subsequence $\{m_{n_k}\}$ that converges to some $m \in \mathcal{M}$.

    Since $F$ is lower semicontinuous and $m_{n_k} \to m$,
    \begin{align*}
        F(m) \leq \liminf_{k \to +\infty} F(m_{n_k}) = \inf_{x \in \mathcal{M}} F(x).
    \end{align*}
    This implies that $m$ achieves the infimum of $F$,
    \begin{align*}
        F(m) = \inf_{x \in \mathcal{M}} F(x).
    \end{align*}
    Therefore, $m$ is a minimizer of the Fréchet functional.
\end{proof}

\begin{proof}[Proof for Lemma~\ref{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k}]
    For the sake of contradiction, suppose that there are two distinct points $m_1, m_2 \in \mathcal{M}$ such that both are minimizers of the Fréchet functional.
    \begin{align*}
        m_1 &= \argmin_{x \in \mathcal{M}} \int_\mathcal{M} d^2(y, x)d\nu(y), \\
        m_2 &= \argmin_{x \in \mathcal{M}} \int_\mathcal{M} d^2(y, x)d\nu(y),
    \end{align*}
    with $m_1 \neq m_2$.
    Since $\mathcal{M}$ is a $\mathrm{CAT}(K)$ space and thus a geodesic metric space, there exists a unique geodesic $\gamma \colon [0, 1] \to \mathcal{M}$ connecting $m_1$ to $m_2$.
    \begin{align*}
        \gamma(0) &= m_1, \\
        \gamma(1) &= m_2, \\
        d(\gamma(t), \gamma(t')) &= |t - t'|\cdot d(m_1, m_2), \quad \forall t, t' \in [0, 1].
    \end{align*}
    Define a function $F \colon [0, 1] \to \mathbb{R}$ by evaluating the Fréchet functional along the geodesic $\gamma(t)$:
    \begin{align*}
        F(t) = \int_\mathcal{M} d^2(y, \gamma(t))d\nu(y).
    \end{align*}
    Since both $m_1$ and $m_2$ are minimizers,
    \begin{align*}
        F(0) = F(1) = \inf_{x \in \mathcal{M}} F(x).
    \end{align*}
    Given that $\mathcal{M}$ is strictly geodesically convex, the squared distance function $f(x) = d^2(y, x)$ is strictly convex along any geodesic.
    Therefore, for each fixed $y \in \mathcal{M}$, the function $t \mapsto d^2(y, \gamma(t))$ satisfies
    \begin{align*}
        d^2(y, \gamma(t)) < (1 - t) d^2(y, m_1) + t d^2(y, m_1),
    \end{align*}
    for all $t \in (0, 1)$.

    Integrate the strict inequality with respect to the measure $\nu$ yields
    \begin{align*}
        F(t) &= \int_\mathcal{M} d^2(y, \gamma(t)) d\nu(y) \\
        &< \int_\mathcal{M} \left\{(1 - t) d^2(y, m_1) + t d^2(y, m_2)\right\}d\nu(y) \\
        &= (1 - t) \int_\mathcal{M} d^2(y, m_1)d\nu(y) + t \int_\mathcal{M} d^2(y, m_2) d\nu(y).
    \end{align*}
    But since $m_1$ and $m_2$ are both minimizers,
    \begin{align*}
        \int_\mathcal{M} d^2(y, m_1)d\nu(y) = \int_\mathcal{M} d^2(y, m_2)d\nu(y) = \int_{x \in \mathcal{M}}F(x).
    \end{align*}
    Thus,
    \begin{align*}
        F(t) < (1 - t) \inf_{x \in \mathcal{M}} F(x) + t \inf_{x \in \mathcal{M}} F(x) = \inf_{x \in \mathcal{M}} F(x).
    \end{align*}
    However, this is a contradiction because $F(x)$ cannot be less than the infimum $\inf_{x \in \mathcal{M}} F(x)$.
    The contradiction arises from the assumption that two distinct minimizers $m_1$ and $m_2$ exist.
    Therefore, there can be at most one minimizer.
    Given that the Fréchet functional attains its infimum by Lemma~\ref{lem:existence_minimizer_in_complete_cat_k}, this minimizer is unique.
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:stability_non_positive_curvature}]
    The Fréchet functional $x \mapsto F_\nu(x)$ for a measure $\nu$ is defined as
    \begin{align*}
        F_\nu(x) = \int_\mathcal{M} d^2(y, x) d\nu(y).
    \end{align*}
    Given that the squared distance function $d^2(y, x)$ is continuous in $y$ for each fixed $x$, weak convergence $\nu_n \Rightarrow \nu$ implies that for each fixed $x \in \mathcal{M}$,
    \begin{align*}
        \lim_{n \to +\infty} F_{\nu_n}(x) = F_\nu(x).
    \end{align*}
    In addition, given that $d^2(y, x)$ is continuous and bounded by zero, and assuming that the measures $\nu_n$ and $\nu$ have compact supports, as established in Lemma~\ref{lem:existence_minimizer_in_complete_cat_k}, the convergence $\nu_n \Rightarrow \nu$ implies that
    \begin{align*}
        \lim_{n \to +\infty} F_{\nu_n}(x) = F_\nu(x), \quad \text{uniformly for $x \in \mathcal{M}$}.
    \end{align*}
    This uniform convergence is a consequence of the boundedness of the squared distance function over compact supports, and the equicontinuity provided by the geometric properties of the $\mathrm{CAT}(K)$ spaces.

    Suppose that $m_n$ does not converge to $m$,
    Then, there exist an $\epsilon > 0$ and a subsequence $\{m_{n_k}\}$ such that
    \begin{align*}
        d(m_{n_k}, m) \geq \epsilon,
    \end{align*}
    for all $k$.
    Since $\mathcal{M}$ is a $\mathrm{CAT}(K)$ space with $K \leq 0$ and hence a geodesic and proper metric space under the assumption of compact support from Lemma~\ref{lem:existence_minimizer_in_complete_cat_k}, the sequence $\{m_{n_k}\}$ has a convergent subsequence.
    Without loss of generality, assume that $m_{n_k} \to m'$ as $k \to +\infty$.
    By the continuity of the Fréchet functional,
    \begin{align*}
        \lim_{k \to +\infty} F_{\nu_{n_k}}(m_{n_k}) &= \lim_{k \to +\infty}\inf_{x \in \mathcal{M}} F_{\nu_{n_k}}(x) \\
        &= F_\nu(m),
    \end{align*}
    since $m$ is the unique minimizer for $\nu$.

    Consider $\nu_n \Rightarrow \nu$ and $m_{n_k} \to m'$,
    \begin{align*}
        \lim_{k \to +\infty} F_{\nu_{n_k}}(m_{n_k}) = F_\nu(m').
    \end{align*}
    Then,
    \begin{align*}
        F_\nu(m') = F_\nu(m).
    \end{align*}
    Therefore, $m'$ is also a minimizer of $F_\nu(x)$.
    Since $\nu$ has a unique Fréchet mean $m$, it must be that $m' = m$.
    Recall that $d(m_{n_k}, m) \geq \epsilon$ for all $k$, but $m_{n_k} \to m' = m$, which implies that
    \begin{align*}
        \lim_{k \to +\infty} d(m_{n_k}, m) = d(m', m) = 0,
    \end{align*}
    contradicting $d(m_{n_k}, m) \geq \epsilon$.
    Therefore, it must be that
    \begin{align*}
        m_n \to m, \quad \text{as $n \to +\infty$}.
    \end{align*}
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:compactness_criterion_for_uniqueness_positive_curvature}]
    For $K > 0$, the comparison space is the standard sphere $\mathbb{S}^n$ with radius $1 / \sqrt{K}$.
    In $\mathbb{S}^n$, geodesics are great circles, and the distance between two points is given by the central angle multiplied by $1 / \sqrt{K}$.
    The diameter of $\mathbb{S}^n$ is $\pi / \sqrt{K}$, meaning that the maximal distance between any two points is $\pi / \sqrt{K}$.

    Given $R < \pi / 2\sqrt{K}$, the geodesic ball $B(p, R)$ lies entirely within a hemisphere of $\mathbb{S}^n$.
    In this setting, any two points $x, y \in B(p, R)$ are separated by a distance $d(x, y)$, satisfying
    \begin{align*}
        d(x, y) &\leq d(x, p) + d(p, y) \\
        &< \frac{\pi}{2\sqrt{K}} + \frac{\pi}{2\sqrt{K}} \\
        &= \frac{\pi}{\sqrt{K}}.
    \end{align*}
    Since $d(x, y) < \pi / \sqrt{K}$, there exists a unique minimal geodesic connecting $x$ and $y$ within $\mathbb{S}^n$.

    Assume, for contradiction, that the minimal geodesic $\gamma$ between $x$ and $y$ exits $B(p, R)$.
    Then, there exists a point $z \in \gamma$ such that $d(p, z) = R$.
    Consider the geodesic triagles $\triangle pzx$ and $\triangle pzy$.
    Since $d(p, x) < R$ and $d(p, y) < R$, and $\gamma$ is minimal, the angle at $p$ opposite the side $\gamma$ must satisfy certain angular constraints derived from the spherical law of cosines.
    However, because $R < \pi / 2\sqrt{K}$, the triangle $\triangle pzx$ lies within a convex hemisphere, ensuring that the path from $p$ to $z$ to $x$ remains within $B(p, R)$.
    This contradicts the assumption that $\gamma$ exits $B(p, R)$.
    Therefore, since any two points in $B(p, R)$ can be connected by a unique minimal geodesic that remains entirely within $B(p, R)$, the geodesic ball $B(p, R)$ is geodesically convex in $\mathbb{S}^n$ for all radius $R < \pi / 2\sqrt{K}$.
    This ensures that $\mathrm{CAT}(K)$ condition preserves the strict convexity.

    Given that $\mathrm{diam}(\mathrm{supp}(\nu)) < \pi / 2\sqrt{K}$, for any geodesic $t \mapsto \gamma(t)$ connecting two distinct points $m_1, m_2 \in \mathcal{M}$, the Fréchet functional satisfies
    \begin{align*}
        F(\gamma(t)) < (1 - t) F(m_1) + t F_2(m_2),
    \end{align*}
    for all $t \in (0, 1)$, provided $m_1 \neq m_2$.
    Here, strict convexity of $F(x)$ ensures that any local minimum is a global minimum, and further, that such a minimum is unique within the convex neighborhood.
\end{proof}

\clearpage

\subsection{Proofs for Section~\ref{sec:theory:convergence_rates_and_concentration}}

\begin{proof}[Proof for Theorem~\ref{thm:concentration_for_sample_frechet_mean}]
    Define the population Fréchet functional $F(z)$ and empirical Fréchet functional $F_n(z)$ as follows.
    \begin{align*}
        F(z) &\coloneqq \mathbb{E}[d^2(Y, m)], \\
        F_n(z) &\coloneqq \frac{1}{n}\sum^n_{i=1}d^2(Y_i, z).
    \end{align*}
    By definition,
    \begin{align*}
        \mu &= \argmin_{z \in \mathcal{M}} F(z), \\
        \hat{\mu}_n &= \argmin_{z \in \mathcal{M}} F_n(z).
    \end{align*}
    Assume that $\mu$ is unique, which holds if $\mathrm{diam}(\mathcal{M}) < \pi / 2\sqrt{K}$ when $K > 0$ or automatically if $K \leq 0$, from Lemmas~\ref{lem:existence_minimizer_in_complete_cat_k},~\ref{lem:uniqueness_frechet_mean_in_strictly_convex_cat_k} and Propositions~\ref{prp:stability_non_positive_curvature},~\ref{prp:compactness_criterion_for_uniqueness_positive_curvature}.

    A key geometric fact in $\mathrm{CAT}(K)$ spaces is that the map
    \begin{align*}
        z \mapsto \mathbb{E}[d^2(Y, z)] = F(z)
    \end{align*}
    is $\lambda$-strongly geodesically convex around $\mu$, provided $\mathrm{diam}(\mathcal{M})$ is small enough.
    Concretely, there exists a constant
    \begin{align*}
        \alpha = \alpha(K, D) > 0,
    \end{align*}
    such that for every $z \in \mathcal{M}$,
    \begin{align*}
        F(z) - F(\mu) \geq \alpha d^2(z, \mu).
    \end{align*}
    A fully explicit formula for $\alpha(K, D)$ can be extracted from standard $\mathrm{CAT}(K)$ lemmas.
    \begin{itemize}
        \item If $K \leq 0$, one can take $\alpha(K, D) = \frac{1}{2}$. Indeed, $\mathrm{CAT}(K)$ spaces are sometimes called Hadamard spaces, for which $d^2(y, \cdot)$ is $1$-convex along geodesics.
        \item If $K > 0$ but $\mathrm{diam}(\mathcal{M}) = D < \pi / 2\sqrt{K}$, one obtains an explicit lower bound
        \begin{align*}
            \alpha(K, D) \geq \frac{\sin(2\sqrt{K}R)}{2R},
        \end{align*}
        where $R = D / 2$.
        One often sees, for example,
        \begin{align*}
            \alpha(K, D) = \frac{2}{\pi}\sqrt{K}\sin\left(\frac{\pi}{2} - \sqrt{K}D\right).
        \end{align*}
    \end{itemize}

    Since $\hat{\mu}_n$ is the minimizer of $F_n$, one can obtain
    \begin{align*}
        F_n(\hat{\mu}_n) \leq F_n(\mu).
    \end{align*}
    Here, rewriting $F_n = F_n - F + F$,
    \begin{align*}
        F_n(\hat{\mu}_n) - F_n(\mu) &= \left\{F_n(\hat{\mu}_n) - F(\hat{\mu}_n)\right\} - \left\{F_n(\mu) - F(\mu)\right\} + \left\{F(\mu_n) - F(\mu)\right\} \\
        &\leq 0, \\
        F(\hat{\mu}_n) - F(\mu) &\leq \left\{F_n(\mu) - F(\mu)\right\} - \left\{F_n(\hat{\mu}_n) - F(\hat{\mu}_n)\right\} \\
        &\leq \left|F_n(\mu) - F(\mu)\right| + \left|F_n(\hat{\mu}_n) - F(\hat{\mu}_n)\right| \\
        &\leq 2\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z)\right|.
    \end{align*}
    On the other hand, by the strong convexity of $F(z)$,
    \begin{align*}
        F(\hat{\mu}_n) - F(\mu) \geq \alpha(K, D) d^2(\hat{\mu}_n, \mu).
    \end{align*}
    Therefore, by combining them, if $d(\hat{\mu}_n, \mu) \geq \epsilon$, then
    \begin{align*}
        \alpha(K, D)\epsilon^2 &\leq F(\hat{\mu}_n) - F(\mu) \\
        &\leq 2\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z)\right|.
    \end{align*}
    Hence,
    \begin{align*}
        \left\{d(\hat{\mu}_n, \mu) \geq \epsilon \right\} \subseteq \left\{\sup_{z \in \mathcal{M}} \left|F_n(z) - F(z) \right| \geq \frac{\alpha(K, D)}{2}\epsilon^2 \right\},
    \end{align*}
    and
    \begin{align*}
        \mathbb{P}\left[d(\hat{\mu}_n, \mu) \geq \epsilon \right] \leq \mathbb{P}\left[\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z) \right| \geq \frac{\alpha(K, D)}{2}\epsilon^2 \right].
    \end{align*}
    So, it suffices to control $\sup_{z \in \mathcal{M}} \left|F_n(z) - F(z) \right|$ by an exponential tail.

    Recall that
    \begin{align*}
        F_n(z) - F(z) = \frac{1}{n}\sum^n_{i=1}\left\{d^2(Y_i, z) - \mathbb{E}[d^2(Y, z)]\right\}.
    \end{align*}
    Define
    \begin{align*}
        X_i(z) = d^2(Y_i, z) - \mathbb{E}[d^2(Y, z)].
    \end{align*}
    Then, $\mathbb{E}[X_i(z)] = 0$ and
    \begin{align*}
        F_n(z) - F(z) = \frac{1}{n}\sum^n_{i=1} X_i(z).
    \end{align*}

    Because $\mathcal{M}$ has diameter $\mathrm{diam}(\mathcal{M}) \leq D$, $d^2(\cdot, \cdot) \leq D^2$.
    Hence, for any $z$,
    \begin{align*}
        X_i(z) \in [-D^2, D^2].
    \end{align*}
    By Hoeffding's inequality, for a fixed $z$,
    \begin{align*}
        \mathbb{P}\left[|F_n(z) - F(z)| \geq t \right] &= \mathbb{P}\left[\left|\sum^n_{i=1}X_i(z)\right| \geq nt \right] \\
        &\leq 2\exp\left(-\frac{nt^2}{2D^4}\right).
    \end{align*}
    Here, for every fixed $\epsilon$, one obtains a bound of the form
    \begin{align*}
        \mathbb{P}\left[\sup_{z \in \mathcal{M}}|F_n(z) - F(z)| \geq t \right] \leq c_1' \exp\left(-c_2' nt^2 \right),
    \end{align*}
    for constants $c_1', c_2' > 0$ depending on $K, D$ and on the metric complexity of $\mathcal{M}$,
    \begin{align*}
        c_1' &= 2\left(\frac{\alpha(K, D) D}{\delta}\right)^m, \\
        c_2' &= \frac{\alpha(K, D)}{8D^2},
    \end{align*}
    that are from standard references in manifold‐valued statistics.

    Putting it all together, 
    \begin{align*}
        \mathbb{P}\left[d(\hat{\mu}_n, \mu) \geq \epsilon \right] &\leq \mathbb{P}\left[\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z) \right| \geq \frac{\alpha(K, D)}{2}\epsilon^2 \right] \\
        &\leq  c_1' \exp\left\{-c_2 n\left(\frac{\alpha(K, D)}{2}\epsilon^2 \right)^2\right\}.
    \end{align*}
    This concludes the required proof.
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:l_p_concentration}]
    By Theorem~\ref{thm:concentration_for_sample_frechet_mean}, there exist positive constants $c_1 = c_1(K, D)$ and $c_2 = c_2(K, D)$, such that for every $\epsilon > 0$,
    \begin{align*}
        \mathbb{P}\left[d(\hat{\mu}_n, \mu) > \epsilon \right] \leq c_1 \exp\left(-c_2 n \epsilon^2 \right).
    \end{align*}
    For any nonnegative random variable $Z$ and any $p \geq 1$, one has the standard identity
    \begin{align*}
        \mathbb{E}[Z^p] = \int^\infty_0 p \epsilon^{p - 1}\mathbb{P}(Z > \epsilon)d\epsilon.
    \end{align*}
    This follows from writing $\mathbb{E}[Z^p] = \int^\infty_0 p \epsilon^{p - 1}\mathbbm{1}(Z > \epsilon)d\epsilon$ and exchanging expectation and integral.
    Applying this to $Z = d(\hat{\mu}_n, \mu)$,
    \begin{align*}
        \mathbb{E}[d^p(\hat{\mu}_n, \mu)] = \int^\infty_0 p \epsilon^{p - 1}\mathbb{P}[d(\hat{\mu}_n, \mu) > \epsilon]d\epsilon.
    \end{align*}
    Therefore,
    \begin{align*}
        \mathbb{E}[d^p(\hat{\mu}_n, \mu)] &\leq \int^\infty_0 p \epsilon^{p - 1}\left[c_1 \exp(-c_2 n \epsilon^2)\right]d\epsilon \\
        &= c_1 \int^\infty_0 p\epsilon^{p - 1} \exp(-c_2 n \epsilon^2) d\epsilon.
    \end{align*}
    Let $u = \sqrt{n}\epsilon$.
    Then, $\epsilon = u / \sqrt{n}$ and $d\epsilon = \frac{1}{\sqrt{n}}du$.
    Also,
    \begin{align*}
        \epsilon^{p-1} &= (\frac{u}{\sqrt{n}})^{p-1} = n^{-(p-1)/2}u^{p-1}, \\
        \exp(-c_2 n \epsilon^2) &= \exp(-c_2 u^2).
    \end{align*}
    So,
    \begin{align*}
        \int^\infty_0 \epsilon^{p-1}\exp(-c_2 n \epsilon^2)d\epsilon &= \int^\infty_0 n^{-(p-1)/2}u^{p-1} \exp(-c_2 u^2)\frac{1}{\sqrt{n}}du \\
        &= n^{-\frac{p-1}{2}}n^{-\frac{1}{2}}\int^\infty_0 u^{p-1}\exp(-c_2 u^2)du \\
        &= n^{-\frac{p}{2}}\int^\infty_0 u^{p-1}\exp(-c_2 u^2)du.
    \end{align*}
    Now, evaluate $\int^\infty_0 u^{p-1}\exp(-c_2 u^2)du$.
   This is a known integral that can be expressed via the Gamma function.
   Indeed,
   \begin{align*}
       \int^\infty_0 u^{p-1}\exp(-c_2 u^2)du = \frac{1}{2}c_2^{-\frac{p}{2}}\Gamma\left(\frac{p}{2}\right),
   \end{align*}
   and
   \begin{align*}
       \int^\infty_0 \epsilon^{p-1}\exp(-c_2 n \epsilon^2) d\epsilon = n^{-\frac{p}{2}}\left[\frac{1}{2}c_2^{-\frac{p}{2}}\Gamma\left(\frac{p}{2}\right)\right].
   \end{align*}
   Therefore,
   \begin{align*}
       \mathbb{E}\left[d^p(\hat{\mu}_n, \mu)\right] \leq c_1 p \left\{n^{-\frac{p}{2}}\left[\frac{1}{2}c_2^{-\frac{p}{2}}\Gamma\left(\frac{p}{2}\right)\right]\right\}.
   \end{align*}
   Collecting constants and it gives the proof.
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:pointwise_consistency_of_nonparametric_frechet_regression}]
    Fix a point $x \in \mathbb{R}^d$.
    Define the weighted empirical measure of $Y$ given $x$ as
    \begin{align*}
        \nu_{n,x} \coloneqq \sum^n_{i=1}w_{n,i}(x)\delta_{Y_i},
    \end{align*}
    where $\delta_{Y_i}$ denotes the Dirac measure at $Y_i$.
    Because $\sum^n_{i=1}w_{n,i}(x) = 1$, this is indeed a probability measure on $\mathcal{M}$.
    Similarly, let $\nu_x$ be the true conditional distribution of $Y$ given $X = x$ as
    \begin{align*}
        \nu_x \coloneqq \mathbb{P}\left[Y \in A \mid X = x\right],
    \end{align*}
    for Borel sets $A \subseteq \mathcal{M}$.
    Then, observe that the estimator $\hat{\mu}^*_n(x)$ can be written as
    \begin{align*}
        \hat{\mu}^*_n(x) &= \argmin_{z \in \mathcal{M}} \sum^n_{i=1} w_{n,i}(x) d^2(Y_i, z) \\
        &= \argmin_{z \in \mathcal{M}} \int^{+\infty}_{-\infty} d^2(y, z)d\nu_{n,x}(y).
    \end{align*}
    That is, $\hat{\mu}^*_n(x)$ is precisely the Fréchet mean of the measure $\nu_{n,x}$.
    Meanwhile, $\mu^*(x)$ is the Fréchet mean of $\nu_x$:
    \begin{align*}
        \mu^*(x) = \argmin_{z \in \mathcal{M}}\int^{+\infty}_{-\infty}d^2(y, z)d\nu_x(y).
    \end{align*}
    Hence, the problem reduces to showing that as $n \to +\infty$, $\nu_{n,x}$ converges to $\nu_x$ in a sense strong enough to force their Fréchet means to converge.

    From Assumption~\ref{asm:kernel_lln_condition}, one can expect that for any bounded function $f\colon \mathcal{M} \to \mathbb{R}$,
    \begin{align*}
        \int f d\nu_{n,x} = \sum^n_{i=1} w_{n,i}(x) f(Y_i) \overset{a.s.}{\underset{n\to\infty}{\to}} \mathbb{E}[f(Y) \mid X = x] = \int f d\nu_x.
    \end{align*}
    Thus, $\nu_{n,x}$ converges to $\nu_x$ in the weak topology on probability measures.

    For each measure $\nu$, define its Fréchet functional $F_\nu \colon \mathcal{M} \to \mathbb{R}$ by
    \begin{align*}
        F_\nu(z) \coloneqq \int d^2(y, z) d\nu(y).
    \end{align*}
    Here,
    \begin{align*}
        \hat{\mu}^*_n(x) &= \argmin_{z \in \mathcal{M}} F_{\nu_{n,x}}(z), \\
        \mu^*(x) &= \argmin_{z \in \mathcal{M}} F_{\nu_x}(z).
    \end{align*}
    One want $F_{\nu_{n,x}} \to F_{\nu_x}$ in a suitable sense that implies $\argmin$ convergence.
    In fact, for pointwise consistency, it suffices to show that for each $z \in \mathcal{M}$,
    \begin{align*}
        F_{\nu_{n,x}}(z) = \sum^n_{i=1}w_{n,i}(x) d^2(Y_i, z) \overset{a.s.}{\to} \int d^2(y, z)d\nu_{x}(y) = F_{\nu_x}(z).
    \end{align*}
    By Assumption~\ref{asm:kernel_lln_condition}, this convergence holds for each $z \in \mathcal{M}$.

    To pass from pointwise convergence of $F_{\nu_{n,x}}$ to convergence of the minimizers $\hat{\mu}^*_n(x) \to \mu^*_(x)$, one can rely on the strict geodesic convexity of $d^2(\cdot, \cdot)$ in a $\mathrm{CAT}(K)$ space with small diameter.
    Concretely, from earlier arguments, there is a constant $\alpha(K, D)$ such that
    \begin{align*}
        F_{\nu_x}(z) - F_{\nu_x}(\mu^*(x)) \geq \alpha(K, D) d^2(z, \mu^*(x)),
    \end{align*}
    for all $z \in \mathcal{M}$.
    This follows from the strong geodesic convexity of $z \mapsto \int d^2(y, z)d\nu_x(y)$.
    Equivalently, if $z$ is $\epsilon$-far from $\mu^*(x)$, then $F_{\nu_x}(z)$ exceeds the global minimum $F_{\nu_x}(\mu^*(x))$ at least $\alpha(K, D)\epsilon^2$.

    Now, let $\epsilon > 0$.
    Suppose, contrary to what one want, that
    \begin{align*}
        d(\hat{\mu}^*_n(x), \mu^*(x)) \geq \epsilon.
    \end{align*}
    By $\mathrm{CAT}(K)$-convexity,
    \begin{align*}
        F_{\nu_x}(\hat{\mu}^*_n(x)) - F_{\nu_x}(\mu^*(x)) \geq \alpha(K, D)\epsilon^2.
    \end{align*}
    On the other hand,
    \begin{align*}
        F_{\nu_x}(\hat{\mu}^*_n(x)) - F_{\nu_x}(\mu^*(x)) = \left\{F_{\nu_{n,x}}(\hat{\mu}^*_n(x)) - F_{\nu_{n,x}}(\mu^*(x))\right\} + (F_{\nu_x} - F_{\nu_{n,x}})(\hat{\mu}^*_n(x)) - (F_{\nu_x} - F_{\nu_{n,x}})(\mu^*(x)).
    \end{align*}
    Since $\hat{\mu}^*_n(x)$ minimizes $F_{\nu_{n,x}}$,
    \begin{align*}
        F_{\nu,x}(\hat{\mu}^*_n(x)) \leq F_{\nu_{n,x}}(\mu^*(x)).
    \end{align*}
    Thus,
    \begin{align*}
        F_{\nu_{n,x}}(\hat{\mu}^*_n(x)) - F_{\nu_x}(\mu^*(x)) \leq (F_{\nu_x} - F_{\nu_{n,x}})(\hat{\mu}^*_n(x)) - (F_{\nu_x} - F_{\nu_{n,x}})(\mu^*(x)).
    \end{align*}
    Hence,
    \begin{align*}
        \alpha(K, D)\epsilon^2 \leq \left|(F_{\nu_x} - F_{\nu_{n,x}})(\hat{\mu}^*_n(x))\right| + \left|(F_{\nu_x} - F_{\nu_{n,x}})(\mu^*(x))\right|.
    \end{align*}
    But as $n \to +\infty$,
    \begin{align*}
        F_{\nu_{n,x}}(z) \to F_{\nu_x}(z),
    \end{align*}
    pointwise for each $z$, so the difference $|F_{\nu_x}(z) - F_{\nu_{n,x}}(z)| \to 0$.
    By dominated convergence theorem,
    \begin{align*}
        \sup_{z \in \{\hat{\mu}^*_n(x), \mu^*(x)\}}\left|F_{\nu_{n,x}}(z) - F_{\nu_x}(z) \right| \overset{a.s.}{\underset{n \to 0}{\to}} 0.
    \end{align*}
    Hence, for large $n$, the right-hand side in the above inequality is smaller than $\frac{1}{2}\alpha(K, D)\epsilon^2$, which is incompatible.
    Thus, for large $n$,
    \begin{align*}
        d(\hat{\mu}^*_n(x), \mu^*(x)) < \epsilon,
    \end{align*}
    and
    \begin{align*}
        \hat{\mu}^*_n(x) \overset{a.s.}{\to} \mu^*(x).
    \end{align*}
    This completes the proof of pointwise consistency.
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:convergence_rates_in_cat_k}]
    For each $x$, define the empirical weighted measure as follows.
    \begin{align*}
        \nu_{n,x} \coloneqq \sum^n_{i=1}w_{n,i}(x)\delta_{Y_i},
    \end{align*}
    where $\delta_y$ is the Dirac measure at $y$.
    Then,
    \begin{align*}
        \hat{\mu}^*_n(x) = \argmin_{z \in \mathcal{M}}\int d^2(y,z) d\nu_{n,x}(y).
    \end{align*}
    Simultaneously, define the local population measure near $x$:
    \begin{align*}
        \pi_{n,x} \coloneqq \frac{\mathbb{E}\left[W\left(\frac{\| x - X\|}{h_n}\right)\mathbbm{1}(Y \in \cdot)\right]}{\mathbb{E}\left[W\left(\frac{\| x - X\|}{h_n}\right)\right]},
    \end{align*}
    which is the ideal measure that the kernel weighting is trying to approximate.
    Then define the local population Fréchet mean as
    \begin{align*}
        \tilde{\mu}^*_n(x) = \argmin_{z \in \mathcal{M}} \int d^2(y, z) d\pi_{n,x}(y).
    \end{align*}
    Here, $\tilde{\mu}^*_n(x)$ is the minimizer of the population version of the local kernel functional, and $\hat{\mu}^*_n(x)$ is the minimizer of the empirical version.
    Then one can write
    \begin{align*}
        d(\hat{\mu}^*_n(x), \mu^*(x)) \leq d(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x)) + d(\tilde{\mu}^*_n(x), \mu^*(x)).
    \end{align*}
    Squaring and taking expectation, and applying $2ab \leq a^2 + b^2$, one can get a bias–variance decomposition:
    \begin{align*}
        \mathbb{E}[d^2(\hat{\mu}^*_n(x), \mu^*(x))] \leq 2\mathbb{E}[d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x))] + 2d^2(\tilde{\mu}^*_n(x), \mu^*(x)).
    \end{align*}
    The first term in the right-hand side is the variance term, capturing how the empirical local measure $\nu_{n,x}$ fluctuates around $\pi_{n,x}$.
    The second term in the right-hand side is the bias term, capturing how the local population mean $\tilde{\mu}^*_n(x)$ differs from $\mu^*(x)$.

    Recall that in a $\mathrm{CAT}(K)$ space, of diameter $\mathrm{diam}(\mathcal{M}) \leq D$, there is a strong geodesic convexity constant $\alpha(K, D)$ such that
    \begin{align*}
        \int d^2(y, z)d\nu(y) - \int d^2(y, z^*)d\nu(z^*) \geq \alpha(K, D) d^2(z, z^*),
    \end{align*}
    for all probability measures $\nu$ on $\mathcal{M}$, provided the measure is fully supported in a ball of diameter $\mathrm{diam}(\mathcal{M}) \leq D$.
    Hence, for the local measure $\pi_{n,x}$,
    \begin{align*}
        \int d^2(y, \hat{\mu}^*_n(x)) d\pi_{n,x} - \int d^2(y, \tilde{\mu}^*_n(x))d\pi_{n,x}(y) \geq \alpha(K, D) d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x)).
    \end{align*}
    Because $\hat{\mu}^*_n(x)$ minimizes $\int d^2(y, z) d\nu_{n,x}(y)$,
    \begin{align*}
        \int d^2(y, \hat{\mu}^*_n(x)) d\nu_{n,x}(y) \leq \int d^2(y, \tilde{\mu}^*_n(x)) d\nu_{n,x}(y).
    \end{align*}
    By subtracting the corresponding population measure integrals,
    \begin{align*}
        \left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \hat{\mu}^*_n(x)) - \left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \tilde{\mu}^*_n(x)) &\leq \int d^2(y, \tilde{\mu}^*_n(x)) d\pi_{n,x}(y) - \int d^2(y, \hat{\mu}^*_n(x)) d\pi_{n,x}(y) \\
        \int d^2(y, \hat{\mu}^*_n(x)) d\pi_{n,x}(y) - \int d^2(y, \tilde{\mu}^*_n(x)) d\pi_{n,x}(y) &\leq \Delta_n(x),
    \end{align*}
    where
    \begin{align*}
        \Delta_n(x) \coloneqq \left|\left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \hat{\mu}^*_n(x))\right| + \left|\left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \tilde{\mu}^*_n(x))\right|.
    \end{align*}
    Combining with the strong convexity inequality,
    \begin{align*}
        \alpha(K, D) d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x)) &\leq \Delta_n(x) \\
        d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x)) &\leq \frac{\Delta_n(x)}{\alpha(K, D)}.
    \end{align*}
    Taking expectation with respect to the sample $\{(X_i, Y_i)\}^n_{i=1}$,
    \begin{align*}
        \mathbb{E}[d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x))] &\leq \frac{\mathbb{E}[\Delta_n(x)]}{\alpha(K, D)}.
    \end{align*}
    Recall that
    \begin{align*}
        \Delta_n(x) &= \left|\left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \hat{\mu}^*_n(x))\right| + \left|\left[\nu_{n,x} - \pi_{n,x} \right]d^2(\cdot, \tilde{\mu}^*_n(x))\right| \\
        &= \left|\sum^n_{i=1} w_{n,i}(x)\left\{d^2(Y_i, \hat{\mu}^*_n(x)) - \mathbb{E}[d^2(Y, \tilde{\mu}^*_n(x) \mid X \approx x]\right\} \right| \\
        &\quad\quad\quad\quad\quad + \left|\sum^n_{i=1} w_{n,i}(x)\left\{d^2(Y_i, \hat{\mu}^*_n(x)) - \mathbb{E}[d^2(Y, \tilde{\mu}^*_n(x) \mid X \approx x]\right\} \right|.
    \end{align*}
    Since $\hat{\mu}^*_n$ itself depends on the sample, a straightforward application of Hoeffding’s inequality is tricky. 
    However, one can use Efron–Stein or Bennett–type inequalities for U‐statistics, or the bounded differences approach, carefully analyzing how a single $Y_i$ affects $\hat{\mu}^*_n$.
    Such arguments appear in standard references on manifold‐valued kernel regression.
    Thus, one can obtain
    \begin{align*}
        \mathbb{E}[\Delta_{n}(x)] = O\left((n h^d_n)^{-1/2}\right).
    \end{align*}
    Hence,
    \begin{align*}
        \mathbb{E}[d^2(\hat{\mu}^*_n(x), \tilde{\mu}^*_n(x))] \leq \frac{C_{\mathrm{var}}}{\alpha(K, D)}(n h^d_n)^{-1/2},
    \end{align*}
    where $C_{\mathrm{var}}$  is a constant depending on the kernel shape, the distribution of $(X, Y)$ near $x$ and the geometry constants $(K, D)$.

    Next, recall that
    \begin{align*}
        \tilde{\mu}^*_n(x) &= \argmin_{z \in \mathcal{M}}\int d^2(y, z)d\pi_{n,x}(y), \\
        \mu^*(x) &= \argmin_{z \in \mathcal{M}} \int d^2(y, z) d\nu_x(y),
    \end{align*}
    where $\nu_x(\cdot) = \mathbb{P}[Y \in \cdot \mid X = x]$.
    As one move from $X = x$ to a local neighborhood $\{x' \mid \|x - x'\| \leq O(h_n) \}$, it can be expected that $\tilde{\mu}^*_n(x)$ to approximate $\mu^*(x')$ for some $x' \approx x$.
    Then $\mu^*(x')$ is close to $\mu^*(x)$ if $\mu^*$ is $\beta$-Hölder.

    Because $\pi_{n,x}$ is essentially the distribution of $Y \mid X \in \{x' \mid \|x' - x\| \leq c h_n\}$, let $x^\natural$ be some effective point near $x$.
    Then by using smoothness or local Lipschitz condition on the conditional distributions, 
    \begin{align*}
        d(\tilde{\mu}^*_n(x), \mu^*(x')) \leq C_{\mathrm{bias}}(h^\beta_n),
    \end{align*}
    for some constant $C_{\mathrm{bias}} > 0$.
    Then one adds
    \begin{align*}
        d(\mu^*(x'), \mu^*(x)) \leq L \cdot \|x' - x\| \approx L h_n^\beta.
    \end{align*}
    Hence,
    \begin{align*}
        d(\tilde{\mu}^*_n(x), \mu^*(x)) \leq d(\tilde{\mu}^*_n(x), \mu^*(x')) + d(\mu^*(x'), \mu^*(x)) = O(h_n^\beta),
    \end{align*}
    and
    \begin{align*}
        d^2(\tilde{\mu}^*_n(x), \mu^*(x)) = O(h_n^{2\beta}).
    \end{align*}
    Putting it all together in the bias–variance decomposition, it completes the required proof.
\end{proof}

\clearpage
\subsection{Proofs for Section~\ref{sec:theory:angle_stability_for_conditiona_frechet_means}}

\begin{proof}[Proof for Lemma~\ref{lem:angle_comparison}]
    Let $y'$ be a point on the geodesic segment $[xy$ such that $y'$ is very close to $x$.
    Similarly, pick $z'$ on $[xz]$.
    So,
    \begin{align*}
        d(x, y') = \delta, \\
        d(x, z') = \delta,
    \end{align*}
    for some $\delta > 0$.
    Thi triangle $\triangle xy'z'$ has perimeter $\leq d(x, y) + d(y, z) + d(z, x)$, which is assumed $\leq \pi / \sqrt{K}$ if $K > 0$.
    For $\delta$ small enough, the side lengths of $\triangle xy'z'$ are also $\leq \pi / \sqrt{K}$.
    By the $\mathrm{CAT}(K)$ definition,
    \begin{align*}
        d(y', z') \leq d_{\mathbb{M}_k}(\bar{y}', \bar{z}'),
    \end{align*}
    and
    \begin{align*}
        d(x, y') &= d(\bar{x}, \bar{y}') = \delta, \\
        d(x, z') &= d(\bar{x}, \bar{z}') = \delta.
    \end{align*}
    The triangle $\triangle \bar{x} \bar{y}' \bar{z}'$ is in the same model plane as $\triangle \bar{x}\bar{y}\bar{z}$, but its typically much smaller near $\bar{x}$.

    By definition of the Alexandrov angle,
    \begin{align*}
        \angle_x(y, z) = \lim_{\delta \to 0} \angle_x^{(\mathrm{sec})}(y', z'),
    \end{align*}
    where $\angle_x^{(\mathrm{sec})}(y', z')$ is the secular angle of $\triangle xy'z'$ at $x$.
    Equivalently, it is the Euclidean angle $\angle_{\bar{x}}(\bar{y}', \bar{z}')$ in the comparison triangle $\triangle\bar{x}\bar{y}'\bar{z}'$.
    Thus,
    \begin{align*}
        \angle_x(y, z) = \lim_{\delta \to 0}\angle_{\bar{x}}(\bar{y}', \bar{z}').
    \end{align*}
    One also have the angle $\angle_{\bar{x}}(\bar{y}, \bar{z})$ in the large triangle $\triangle \bar{x}\bar{y}\bar{z}$, and want to show
    \begin{align*}
        \angle_{\bar{x}}(\bar{y}', \bar{z}') \leq \angle_{\bar{x}}(\bar{y}, \bar{z}),
    \end{align*}
    for each small $\delta$, from which it will follow in the limit that $\angle_x(y, z) \leq \angle_{\bar{x}}(\bar{y}, \bar{z})$.

    The $\mathrm{CAT}(K)$ condition states that $\triangle x y' z'$ is no thicker than the model $\triangle \bar{x} \bar{y}' \bar{z}'$.
    More precisely, if one place $\triangle x y' z'$ and $\triangle \bar{x} \bar{y}' \bar{z}'$ side by side so that $x \leftrightarrow \bar{x}$, $y' \leftrightarrow \bar{y}'$, $z' \leftrightarrow \bar{z}'$ correspond, one have
    \begin{align*}
        d(y', z') \leq d_{\mathbb{M}_K}(\bar{y}', \bar{z}').
    \end{align*}
    Meanwhile, $\triangle\bar{x}\bar{y}'\bar{z}' \subset \triangle\bar{x}\bar{y}\bar{z}$ or can be inscribed in it, with the property that $as y' \to x$ and $z' \to x$, the points $\bar{y}' \to \bar{x}$ and $\bar{z}' \to \bar{x}$.

    Geometrically, on the model side, it is known (from classical geometry in constant curvature) that
    \begin{align}
        \angle_{\bar{x}}(\bar{y}', \bar{z}') \leq \angle_{\bar{x}}(\bar{y}, \bar{z}).
    \end{align}
    This is because in a convex geometry (like a sphere of radius $1 / \sqrt{K}$ or a Euclidean plane if $K = 0$), drawing smaller radii $\bar{x}\bar{y}'$ and $\bar{x}\bar{z}'$ inside the bigger radii $\bar{x}\bar{y}$ and $\bar{x}\bar{z}$ yields smaller or equal angles from the center $\bar{x}$.

    More precisely, if one revolve the segment $\bar{y}'\bar{z}'$ about $\bar{x}$ within the triangle $\triangle\bar{x}\bar{y}\bar{z}$, the angle $\angle_{\bar{x}}(\bar{y}', \bar{z}')$ cannot exceed $\angle_{\bar{x}}(\bar{y}, \bar{z})$.

    One thus have, for each small $\delta > 0$,
    \begin{align*}
        \angle_{\bar{x}}(\bar{y}', \bar{z}') \leq \angle_{\bar{x}}(\bar{y}, \bar{z}).
    \end{align*}
    By the definition,
    \begin{align*}
        \angle_x(y, z) = \lim_{\delta\to 0}\angle_{\bar{x}}(\bar{y}', \bar{z}') \leq \angle_{\bar{x}}(\bar{y}, \bar{z}).
    \end{align*}
    This completes the proof.
    Thus the angle at $x$ in the real triangle $\triangle xyz$ is bounded above by the corresponding angle at $\bar{x}$ in the comparison triangle $\triangle \bar{x}\bar{y}\bar{z}$.
\end{proof}

\begin{proof}[Proof for Lemma~\ref{lem:angle_continuity}]
    Let $\triangle pqr \subset \mathcal{M}$ have side lengths
    \begin{align*}
        a = d(p, q), \quad b = d(q, r), \quad c = d(r,p),
    \end{align*}
    and let $\angle_p(q, r)$ denote the Alexandrov angle at $p$.
    Similarly, let $\triangle p'q'r'$ have side lengths
    \begin{align*}
        a' = d(p', q'), \quad b' = d(q', r'), \quad c' = d(r',p'),
    \end{align*}
    with angle $\angle_{p'}(q', r')$.

    Assume that both triangles have perimeter $\leq \pi / \sqrt{K}$ if $K > 0$, ensuring they can be compared to triangles in the simply connected model space of curvature $K$ (sphere of radius $1 / \sqrt{K}$ if $K > 0$, Euclidean plane if $K = 0$, or hyperbolic plane if $K < 0$).
    Then, the goal is to show that
    \begin{align*}
        \left|\angle_p(q, r) - \angle_{p'}(q', r') \right| \leq C\left[d(p, p') + d(q, q') + d(r, r') \right],
    \end{align*}
    for some constant $C$ depending on $\alpha(K, D)$ or directly $\pi / \sqrt{K}$.

    From the triangle inequality, one get for instance
    \begin{align*}
        |a - a'| &= |d(p, q) - d(p', q')| \\
        &\leq d(p, p') + d(q, q'),
    \end{align*}
    and similarly,
    \begin{align*}
        |b - b'| &\leq d(q, q') + d(r, r'), \\
        |c - c'| &\leq d(r, r') + d(p, p').
    \end{align*}
    Hence, each difference in corresponding side lengths is at most
    \begin{align*}
        \max\{|a - a'|, |b - b'|, |c - c'|\} \leq d(p, p') + d(q, q') + d(r, r') \eqcolon \delta_{pp'qq'rr'}.
    \end{align*}
    Then,
    \begin{align*}
        |a - a'| \leq \delta_{pp'qq'rr'}, \quad |b - b'| \leq \delta_{pp'qq'rr'}, \quad |c - c'| \leq \delta_{pp'qq'rr'}.
    \end{align*}

    In classical geometry of constant curvature $K$ (sphere, Euclidean plane, and hyperbolic plane), the side lengths $(a, b, c)$ uniquely determine the shape of a triangle (up to rigid motion) provided $a, b, c$ satisfy the triangle inequality.
    The angle $\eta \coloneqq \angle_p(q,r)$ (or its model‐space counterpart $\bar{\eta}$) is a continuous function of $(a, b, c)$.
    \begin{itemize}
        \item If $K = 0$ (Euclidean), one have the law of cosines
        \begin{align*}
            c^2 = a^2 + b^2 - 2ab \cos(\eta),
        \end{align*}
        so
        \begin{align*}
            \cos(\eta) = \frac{a^2 + b^2 + c^2}{2ab}.
        \end{align*}
        This is a rational, continuous function of $(a, b, c)$.
        \item If $K > 0$ (spherical), the spherical law of cosines yield
        \begin{align*}
            \cos(\sqrt{K}c) = \cos(\sqrt{K}a)\cos(\sqrt{K}b) + \sin(\sqrt{K}a)\sin(\sqrt{K}a)\sin(\sqrt{K}b)\cos(\eta).
        \end{align*}
        \item If $ < 0$ (hyperbolic), one have similar hyperbolic law of cosines with $\cosh$ and $\sinh$.
        \begin{align*}
            \cosh(c / K) = \cosh(a / K)\cosh(b / K) - \sinh(a / K)\sinh(b / K)\cos(\eta).
        \end{align*}
    \end{itemize}
    In each case, as long as $a, b, c \leq \pi / \sqrt{|K|}$, one remain in a region where the side‐length–angle relation is well‐defined and continuously differentiable.
    Then, there exists a function
    \begin{align*}
        F \colon \{(a, b, c)\} \subset 
        \mathbb{R}^3_{>0} \to [0, \pi],
    \end{align*}
    so that if $\triangle xyz$ in the model space has sides $(a, b, c)$, then the angle at $x$ is $F(a, b, c)$.
    Moreover, $F$ is Lipschitz continuous on the domain $\{(a, b, c) \mid a + b + c \leq \pi / \sqrt{K} \}$.
    Hence, if $(a, b, c)$ and $(a', b', c')$ are close in $\mathbb{R}^3$, then
    \begin{align*}
        \left|F(a, b, c) - F(a', b', c')\right| \leq K_0\left(|a - a'| + |b - b'| + |c - c'|\right),
    \end{align*}
    for some constant $K_0$ depending only on $\max(a, b, c) \leq \pi / \sqrt{K}$.

    Now connect the actual angles $\angle_p(q,r)$, $\angle_{p'}(q',r')$ in $\mathrm{CAT}(K)$ to their comparison angles $\bar{\alpha}$, $\bar{\alpha}'$ in the model space.  For $\triangle pqr\subset M$, choose the comparison triangle $\triangle \bar{p}\bar{q}\bar{r}\subset \bar{M}$ in the model space of curvature $K$, with side lengths $\bar{p}\bar{q}=a$, $\bar{q}\bar{r}=b$, $\bar{r}\bar{p}=c$.
    Let $\bar{\eta} = \angle_{\bar{p}}(\bar{q},\bar{r})$.  
    For $\triangle p'q'r'\subset M$, choose $\triangle \bar{p}'\bar{q}'\bar{r}'\subset \bar{M}$ similarly with side lengths $a',b',c'$.
    Let $\bar{\eta}' = \angle_{\bar{p}'}(\bar{q}',\bar{r}')$.  
    
    By Lemma~\ref{lem:angle_comparison} in $\mathrm{CAT}(K)$:
    \begin{align*}
        \angle_p(q,r) &\leq \bar{\eta}, \\
        \angle_{p'}(q',r') &\leq \bar{\eta}'.
    \end{align*}
    Symmetrically reversing the roles, one also get
    \begin{align*}
        \bar{\eta} \leq \angle_p(q,r).
    \end{align*}
    Here, $\angle_p(q,r)\approx \bar{\eta}$ and $\angle_{p'}(q',r')\approx \bar{\eta}'$.  
    Hence
    \begin{align*}
        |\angle_p(q,r) - \angle_{p'}(q',r')| &\leq |\bar{\alpha} - \bar{\eta}'| + |\angle_p(q,r) - \bar{\eta}| + |\angle_{p'}(q',r') - \bar{\eta}'|.
    \end{align*}
    But each difference $|\angle_p(q,r)-\bar{\eta}|$ is known to be small by the usual $\mathrm{CAT}(K)$ thin triangle property. 
    Specifically, if the perimeter is $\le \pi/\sqrt{K}$, the difference $\angle_p(q,r)-\bar{\eta}$ can be bounded by a constant times the diameter of $\triangle pqr$; but that diameter is $\le \max(a,b,c)$, already controlled.  

    In fact, in standard statements, one typically get an inequality of the form
    \begin{align*}
        |\angle_p(q,r) - \bar{\eta}| \leq  \varepsilon_1(a,b,c)\quad\text{with }\varepsilon_1\to0\text{ as }a,b,c\to0,
    \end{align*}
    and similarly for $\angle_{p'}(q',r')$.
    Since one are only after a linear bound in the final statement, it suffices that each difference is bounded by a universal constant (depending on $\pi/\sqrt{K}$).
    Thus, effectively
    \begin{align*}
        |\angle_p(q,r) - \angle_{p'}(q',r')| \leq 2\,(\text{const}) + |\bar{\eta} - \bar{\eta}'|.
    \end{align*}
    
    Hence collecting all,
    \begin{align*}
        \bigl|\angle_p(q,r) \;-\; \angle_{p'}(q',r')\bigr| \leq C_1 + C_2\Delta
    \end{align*}
    for constants $C_1$ and $C_2$.
    In typical statements of the lemma, one either arranges that $\Delta$ is small so that the additive constant $C_1$ is overshadowed, or uses a slightly refined thinness difference argument to show $\angle_p(q,r)$ and $\bar{\eta}$ differ by $\le \tilde{C}\cdot\Delta$.
    In either case, one get a final bound of the form
    \begin{align*}
        \bigl|\angle_p(q,r) \;-\; \angle_{p'}(q',r')\bigr| \leq C\Delta = C(d(p,p') + d(q,q') + d(r,r')).
    \end{align*}
    This completes the proof.
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:angle_perturbation}]
    First, from the geodesic convexity, if $\nu_x$ and $\nu_{x'}$ are close in distribution, then
    \begin{align*}
        d\bigl(\mu^*(x),\mu^*(x')\bigr) = C''\epsilon,
    \end{align*}
    for some constant $C''$ depending on \(\alpha(K,D)\) and distributional assumptions (e.g. sub‐Gaussianity or bounded diameter ensuring all integrals are finite).

    Compare angles $\angle_{\mu^*(x)}(u,v)$ and $\angle_{\gamma^*(x')}(u,v)$.
    Let $[\mu^*(x),u]$ be the geodesic from $\mu^*(x)$ to $u$, $[\mu^*(x'),u]$ be the geodesic from $\mu^*(x')$ to $u$, and similarly for $[\mu^*(x), v]$ and $[\mu^(x'), v]$.
    Consider two triangles $\triangle \bigl(\mu^*(x),\,u,\,\mu^*(x')\bigr)$ and $\triangle \bigl(\mu^*(x),\,v,\,\mu^*(x')\bigr)$.  
    Observe that $\mathrm{diam}(\mathcal{M})\le D$, so if $\mu^*(x)$ and $\mu^*(x')$ are also $\le O(\epsilon)$ apart, then each of these triangles has perimeter $2D + O(\epsilon)$.
    If $K>0$, $2D + O(\epsilon)< \pi/\bigl(\sqrt{K}\bigr)$ by the initial assumption $D< \frac{\pi}{2\sqrt{K}}$ and $\epsilon$ small enough.
    Hence, each triangle is validly contained in a region where one can apply $\mathrm{CAT}(K)$ angle comparisons (and the model‐space comparison).

    Let
    \begin{align*}
        p = \mu^*(x),\;\; q = u,\;\; r = \mu^*(x'),        
    \end{align*}
    and
    \begin{align*}
        p' = \mu^*(x'),\;\; q' = u,\;\; r' = \mu^*(x).
    \end{align*}
    Then the pair \(\triangle pqr\) and \(\triangle p'q'r'\) have corresponding points:
    \begin{align*}
        p \leftrightarrow p', \quad q \leftrightarrow q',\quad r \leftrightarrow r'.
    \end{align*}
    Notice that \(q=q'\) is actually the same point \(u\).  
    The sum of vertex perturbations is
    \begin{align*}
        d\bigl(p,p'\bigr) + d\bigl(q,q'\bigr) + d\bigl(r,r'\bigr) &= d\bigl(\mu^*(x),\mu^*(x')\bigr) + 0 + d\bigl(\mu^*(x'),\mu^*(x)\bigr) \\
          &= 2 d\bigl(\mu^*(x),\mu^*(x')\bigr),
    \end{align*}
    and \(d(\mu^*(x),\mu^*(x'))\le C''\,\epsilon\).  
    By Lemma~\ref{lem:angle_continuity},
    \begin{align*}
        \bigl|\angle_p(q,r) - \angle_{p'}(q',r')\bigr| \leq C_1 \bigl[d(p,p') + d(q,q') + d(r,r')\bigr].
    \end{align*}
    Hence
    \begin{align*}
        \Bigl|\angle_{\mu^*(x)}\bigl(u,\mu^*(x')\bigr) - \angle_{\mu^*(x')}\bigl(u,\mu^*(x)\bigr) \Bigr| &\leq  C_1\,\bigl(2\,d(\mu^*(x),\mu^*(x'))\bigr) \\
        &\leq 
        2\,C_1\,C''\,\epsilon.
    \end{align*}
    Similarly, for \(\triangle \mu^*(x)\,v\,\mu^*(x')\), one get the same type of bound in terms of \(\epsilon\). 

    Recall that \(\angle_{\mu^*(x)}(u,v)\) is the Alexandrov angle between geodesics \([\mu^*(x)u]\) and \([\mu^*(x)v]\).
    In a \(\mathrm{CAT}(K)\) space, the angle \(\angle_{\mu^*(x)}(u,v)\) can be added or compared if we know angles involving a third point \(\mu^*(x')\). 
    Thus, 
    \begin{align*}
        \bigl|\angle_{\mu^*(x)}(u,v)
        \;-\;
        (\angle_{\mu^*(x)}(u,\mu^*(x')) \;+\; \angle_{\mu^*(x')}(u,v) - \pi)\bigr|
        \;\le\;
        C_2\cdot d(\mu^*(x),\mu^*(x')),
    \end{align*}
    for some constant \(C_2\).

    Putting all these small angle increments together, conclude that
    \begin{align*}
        \bigl|\angle_{\mu^*(x)}(u,v) 
        \;-\;
        \angle_{\mu^*(x')}(u,v)\bigr|
        \;\;\le\;\;
        C\,d(\mu^*(x),\mu^*(x'))
        \;=\;
        O(\epsilon).
    \end{align*}
    Hence the angles at \(\mu^*(x)\) versus \(\mu^*(x')\) differ by a linear factor in \(\epsilon\). 
\end{proof}

\begin{proof}[Proof for Theorem~\ref{thm:angle_stability_conditional_frechet_means}]
    From Proposition~\ref{prp:angle_perturbation}, if \(\nu_x\approx \nu_{x'}\) (i.e.\ their distance is \(\le\epsilon\)), then for any pair \((u,v)\),
    \begin{align*}
        \Bigl|\,
        \angle_{\mu^*(x)}(u,v)
        \;-\;
        \angle_{\mu^*(x')}(u,v)
        \,\Bigr|
        \;\;\le\;\;
        C_1\,\epsilon,
    \end{align*}
    for some constant \(C_1>0\).
    Hence for one pair of directions \((u,v)\), one get a linear‐in‐\(\epsilon\) bound on how much the angle can change.
    
    Now consider not just one pair, but all pairs \((u_i,u_j)\) with \(1\le i<j\le m\). 
    But since each \(\angle_{\mu^*(x)}(u_i,u_j)\) is covered by the same result,
    \begin{align*}
        \Bigl|\,
        \angle_{\mu^*(x)}(u_i,u_j)
        \;-\;
        \angle_{\mu^*(x')}(u_i,u_j)
        \,\Bigr|
        \;\;\le\;\;
        C_1\,\epsilon,
    \end{align*}
    for each pair \((u_i,u_j)\).
    Then the supremum over \(i<j\) is also \(\le C_1\,\epsilon\).
    In fact, it is not even needed a union bound in probability sense, and each pair is bounded by the same linear factor \(C_1\,\epsilon\).
    Hence
    \begin{align*}
        \sup_{1\le i<j\le m}
        \Bigl|\,
        \angle_{\mu^*(x)}(u_i,u_j)
        \;-\;
        \angle_{\mu^*(x')}(u_i,u_j)
        \,\Bigr|
        \;\;\le\;\;
        C_1\,\epsilon.
    \end{align*}
    Thus one immediately extend from one pair to all \(\binom{m}{2}\) pairs \((u_i,u_j)\). 

    In the hypothesis, it is typically stated that whenever \(\|x - x'\|\) is small, then \(\nu_x\) and \(\nu_{x'}\) differ by \(\epsilon(\|x-x'\|)\).
    For instance, in a classical kernel or smoothing scenario, if \(\|x - x'\|\le\delta\), then
    \begin{align*}
        d_W\bigl(\nu_x,\nu_{x'}\bigr)
        \;\le\;
        \epsilon(\delta).
    \end{align*}
    Hence setting \(\epsilon=\epsilon(\delta)\), for \(\|x - x'\|\le\delta\),
    \begin{align*}
        \sup_{1\le i<j\le m}
        \Bigl|\,
        \angle_{\mu^*(x)}(u_i,u_j)
        \;-\;
        \angle_{\mu^*(x')}(u_i,u_j)
        \,\Bigr|
        \;\;\le\;\;
        C_1\;\epsilon(\delta).
    \end{align*}
    Thus the angle difference is a function of \(\delta\).
    Hence define \(C := C_1\) (it might also absorb small distributional constants if needed), and putting it all together yields the proof.
\end{proof}

\clearpage
\subsection{Proofs for Section~\ref{sec:theory:local_jet_expansion_of_frechet_functionals}}

\begin{proof}[Proof for Lemma~\ref{lem:projection_of_angles_in_tangent_cones}]
    In a smooth Riemannian manifold, for sufficiently close \(u\) and \(v\), the unique geodesics \(\gamma_u \colon [0,\|U\|]\to \mathcal{M}\) and \(\gamma_v : [0,\|V\|]\to \mathcal{M}\) from \(z\) to \(u\), respectively from \(z\) to \(v\), have well‐defined initial velocity vectors at \(z\).  
    Let \(\dot{\gamma}_u(0)\in T_z\mathcal{M}\) be the tangent vector to \(\gamma_u\) at \(z\).
    By construction, this is precisely \(U\) if we identify \(U\in T_z\mathcal{M}\) with the velocity vector in normal coordinates.
    Similarly, \(\dot{\gamma}_v(0)=V\in T_z\mathcal{M}\).  

    In Riemannian geometry (without singularities around \(z\)), one then have:
    \begin{align*}
        \angle_z(u,v)
        \;=\;
        \angle\Bigl(\,\dot{\gamma}_u(0),\,\dot{\gamma}_v(0)\Bigr)
        \;=\;
        \cos^{-1}\Bigl(\frac{g_z\bigl(\dot{\gamma}_u(0),\,\dot{\gamma}_v(0)\bigr)}
                                 {\|\dot{\gamma}_u(0)\|\;\|\dot{\gamma}_v(0)\|}\Bigr).
    \end{align*}
    Here \(g_z(\cdot,\cdot)\) is the Riemannian metric at \(z\).
    In simpler notation, if one identify \(\dot{\gamma}_u(0)=U\) and \(\dot{\gamma}_v(0)=V\), then
    \begin{align*}
        \angle_z(u,v)
        \;=\;
        \cos^{-1}\Bigl(
         \frac{g_z(U,V)}
              {\sqrt{\,g_z(U,U)\,g_z(V,V)\,}}
        \Bigr).
    \end{align*}
    
    Use a geodesic coordinate system \(\Phi\colon T_z\mathcal{M} \supset B_{\delta}(0)\to \mathcal{M}\) around \(z\), with \(\Phi(0)=z\) and \(\mathrm{d}\Phi|_0 = \mathrm{Id}\).
    Concretely, \(\Phi(U) = \exp_z(U)\).  
    In these coordinates, the metric \(g_{ij}(X)\) at a point \(X\) in a small ball around \(0\in T_z\mathcal{M}\) has the well‐known expansions:
    \begin{align*}
        g_{ij}(X) 
          \;=\;
          \delta_{ij} 
          \;-\;
          \tfrac13\,R_{ikj\ell}(0)\,X^k\,X^\ell
          \;+\;
          O(\|X\|^3),
    \end{align*}
    where \(R_{ikj\ell}\) is the Riemann curvature tensor at \(z\).
    The \(-\,\tfrac13\) factor is a standard convention from normal coordinate expansions; the main point is that the first non‐trivial corrections appear at second order in \(\|X\|\).  

    Hence, for vectors \(U,V\in T_z\mathcal{M}\) with small norms, the inner product in the manifold at \(z\) is
    \begin{align*}
        g_z(U,V)
        \;=\;
        \delta_{ij}\,U^i\,V^j
        \;-\;
        \tfrac13\,\sum_{k,\ell}\Bigl(\frac12R_{i k j \ell}(0)\Bigr)\,\dots
        \;+\;
        O\bigl(\|U\|\|V\|\max(\|U\|,\|V\|)\bigr).
    \end{align*}
    In simpler notation:
    \begin{align*}
        g_z(U,V)
        \;=\;
        \langle U,V\rangle_{\mathrm{Eucl}}
        \;+\;
        O\bigl(\|U\|\;\|V\|\;\max(\|U\|,\|V\|)\bigr).
    \end{align*}
    
    From the above expansions,
    \begin{align*}
        \sqrt{\,g_z(U,U)\,}
           \;=\;
           \|U\|_{\mathrm{Eucl}}
           \bigl[\,1 + O(\|U\|^2)\bigr]^{1/2}
           \;=\;
           \|U\|
           +O(\|U\|^3).
    \end{align*}
    Similarly for \(\|V\|\).
    In addition,
    \begin{align*}
        g_z(U,V)
       \;=\;
       \langle U,V\rangle_{\mathrm{Eucl}}
       \;+\;
       O(\|U\|\;\|V\|\;\max(\|U\|,\|V\|)).
    \end{align*}
    Thus
    \begin{align*}
        \frac{g_z(U,V)}{\sqrt{\,g_z(U,U)\,g_z(V,V)\,}}
        \;=\;
        \frac{\langle U,V\rangle}{\|U\|\;\|V\|}
        \;+\;
        O(\|U\|^2 + \|V\|^2),
    \end{align*}
    since each correction is second‐order in \(\|U\|\) or \(\|V\|\). 
    Moreover,
    \begin{align*}
        \angle_z(u,v)
        \;=\;
        \cos^{-1}\Bigl(\frac{g_z(U,V)}{\sqrt{\,g_z(U,U)\,g_z(V,V)\,}}\Bigr)
        \;=\;
        \cos^{-1}\Bigl(
         \frac{\langle U,V\rangle}{\|U\|\;\|V\|}
         \;+\;
         O(\|U\|^2+\|V\|^2)
        \Bigr).
    \end{align*}

   When \(\theta_0 = \angle_0(U,V)\) denotes the Euclidean angle in the tangent space,
   \begin{align*}
       \cos(\theta_0)
       \;=\;
       \frac{\langle U,V\rangle}{\|U\|\;\|V\|}.
   \end{align*}
   Then
   \begin{align*}
       \cos(\angle_z(u,v))
       \;=\;
       \cos(\theta_0) + O(\|U\|^2 + \|V\|^2).
   \end{align*}

    Since \(\cos\) is locally invertible around angles not equal to \(0,\pi\) (and we assume \(\theta_0\) is not degenerate or extremely close to \(\pi\) for typical use), a standard expansion yields:
    \begin{align*}
        \angle_z(u,v)
        \;=\;
        \theta_0 
        \;+\;
        O(\|U\|^2 + \|V\|^2).
    \end{align*}
    Concretely, if \(\theta_1=\theta_0+\delta\) satisfies \(\cos(\theta_1)=\cos(\theta_0)+\eta\), then \(\delta=O(\eta)\) for small \(\eta\). Here, \(\eta = O(\|U\|^2 + \|V\|^2)\).  

    Hence,
    \begin{align*}
        \angle_z(u,v)
        \;=\;
        \theta_0
        \;+\;
        O(\|U\|^2 + \|V\|^2),
    \end{align*}
    where \(\theta_0 = \angle_0(U,V)\) is the Euclidean angle of \(U\) and \(V\) in \(T_zM\).
    This completes the proof.
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:local_jet_expansion_of_frechet_functionals}]
    Let \(\gamma(t)\) be a geodesic in \((\mathcal{M},g)\) with \(\gamma(0)=\mu^*\) and \(\dot{\gamma}(0)=v\).
    Consider \(F(\gamma(t))\).
    Then
    \begin{align*}
        \frac{d}{dt}\,F(\gamma(t))
        \Big|_{t=0}
        &=
        \frac{d}{dt}\,\int d^2\bigl(y,\gamma(t)\bigr)\,d\nu(y)
        \Big|_{t=0} \\
        &=
        \int \frac{d}{dt}\,d^2\bigl(y,\gamma(t)\bigr)
        \Big|_{t=0}
        \,d\mu(y).
    \end{align*}
    By standard Riemannian geometry formulas, if \(\sigma(s)\) is the geodesic \([\,y\,\gamma(t)]\), then
    \begin{align*}
        \frac{d}{dt}\,d^2\bigl(y,\gamma(t)\bigr)
        \;=\;
        2\,d(y,\gamma(t))
        \,\Bigl\langle 
           \dot{\gamma}(t),\,\dot{\sigma}(0)
        \Bigr\rangle_{g_{\gamma(t)}}.
    \end{align*}
   At \(t=0\), since \(\gamma(0)=\mu^*\), one interpret \(\dot{\sigma}(0)\) as the initial velocity from \(\mu^*\) toward \(y\).  
   If \(\mu^*\) is a minimizer, the directional derivative must vanish for all directions \(v\).
   Formally, this implies
   \begin{align*}
        \nabla F(\gamma^*)
        \;=\;
        0.
   \end{align*}
    Hence the first‐order term in the expansion of \(F(z)\) around \(z=\mu^*\) vanishes.  

    Next, examine the second derivative (or Hessian) of \(F\) at \(\gamma^*\).
    \begin{align*}
        \mathrm{Hess}_z(F)(v,v)
        \;=\;
        \frac{d^2}{dt^2}\,F(\exp_z(t\,v))
        \Big|_{t=0}.
    \end{align*}
    When \(z=\mu^*\), and \(\mu^*\) is the unique minimizer, these second derivatives measure how strongly \(F\) curves upward around \(\mu^*\).  

    In fact, the Gauss–Manasse–Busemann formula for second variation of distance shows that
    \begin{align*}
        \mathrm{H}_{\mu^*}(F)(v,v)
        \;=\;
        \int \mathrm{H}_{\mu^*}\bigl[d^2(y,\cdot)\bigr](v,v)
        \;d\mu(y).
    \end{align*}
    Each term \(\mathrm{H}_{\mu^*}\bigl[d^2(y,\cdot)\bigr](v,v)\) can be computed from the second variation of \(\rho(\mu^*,y)=d(\mu^*,y)\).
    In standard curvature conditions (especially nonpositive curvature or small diameter in positive curvature), this Hessian is positive semidefinite, ensuring local convexity around \(\mu^*\).
    If \(\mathrm{CAT}(0)\) or if \(\mathrm{diam}<\pi/(2\sqrt{K})\) in \(\mathrm{CAT}(K)\), then \(d^2(y,\cdot)\) is geodesically convex with a definite strong convexity modulus \(\alpha>0\).  
    Integrating preserves that positivity, giving \(\mathrm{H}_{\mu^*}(F)\succeq 0\).  
    Hence there is a well‐defined linear operator \(H_{\mu^*}\) on \(T_{\mu^*}\mathcal{M}\) representing \(\mathrm{H}_{\mu^*}(F)\).  

    Because \(F\) is at least \(C^2\), one can write the remainder \(R(v)\) in a standard Taylor expansion form:
    \begin{align*}
        R(v)
       \;=\;
       O\bigl(\|v\|^3\bigr)
       \quad\text{as }v\to 0.
    \end{align*}
    Concretely, one can show this by analyzing the third derivative of \(F\) in normal coordinates:  
    \begin{align*}
        \frac{d^3}{dt^3}F\bigl(\exp_{\mu^*}(t\,v)\bigr)
    \end{align*}
    remains bounded as \(t\to0\), so the third‐order term is well‐defined.
    
    Hence the local expansion is
    \begin{align*}
        F\bigl(\exp_{\mu^*}(v)\bigr)
        \;=\;
        F(\mu^*)
        \;+\;
        \underbrace{\bigl\langle\nabla F(\mu^*),\,v\bigr\rangle}_{=0}
        \;+\;
        \tfrac12\,\bigl\langle H_{\mu^*}\,v,\;v\bigr\rangle
        \;+\;
        R(v),
        \quad
        R(v)=O(\|v\|^3).
    \end{align*}
    That is precisely the jet expansion for the Fréchet functional around \(\mu^*\).
\end{proof}

\subsection{Proofs for Section~\ref{sec:theory:auxiliary_statements}}

\begin{proof}[Proof for Proposition~\ref{prp:angle_splitting_in_distance_sums}]
    From the local Riemannian (or \(\mathrm{CAT}(K)\)) law of cosines in \(\triangle \mu^*\,y\,z\):
    \begin{align*}
        d^2(y,z)
        &=\;
        d^2\bigl(y,\mu^*\bigr)
        \;+\;
        d^2\bigl(z,\mu^*\bigr)
        \;-\;
        2\,d\bigl(y,\mu^*\bigr)\,d\bigl(\mu^*,z\bigr)\,
        \cos\Bigl(\angle_{\mu^*}(y,z)\Bigr).
    \end{align*}
    Rewriting as
    \begin{align*}
        d^2(y,z) 
        \;-\;
        d^2\bigl(y,\mu^*\bigr)
        &=\;
        d^2\bigl(z,\mu^*\bigr)
        \;-\;
        2\,d\bigl(y,\mu^*\bigr)\,d\bigl(\mu^*,z\bigr)\,
        \cos\Bigl(\angle_{\mu^*}(y,z)\Bigr).
    \end{align*}
    Here, let
    \begin{align*}
        \Delta_{\mathrm{dist}}\bigl(y,z,\mu^*\bigr)
        \;\coloneqq\;
        d^2\bigl(\mu^*,z\bigr)
        \;-\;
        2\,d\bigl(y,\mu^*\bigr)\,d\bigl(\mu^*,z\bigr),
    \end{align*}
    \begin{align*}
        \Delta_{\mathrm{angle}}\bigl(y,z,\mu^*\bigr)
        \;:=\;
        2\,d\bigl(y,\mu^*\bigr)\,d\bigl(z,\mu^*\bigr)\,
        \Bigl[\,
        1
        \;-\;
        \cos\bigl(\angle_{\mu^*}(y,z)\bigr)
        \Bigr].
    \end{align*}
    Observe that
    \begin{align*}
        -2\,d(y,\mu^*)\,d(\mu^*,z)\,\cos(\angle_{\mu^*}(y,z))
        \;\;=\;\;
        \bigl[\Delta_{\mathrm{dist}} - d^2(\mu^*,z)\bigr]
        \;-\;
        \Delta_{\mathrm{angle}},
    \end{align*}
    and
    \begin{align*}
        d^2(y,z)
        \;=\;
        d^2\bigl(y,\mu^*\bigr)
        \;+\;
        \Delta_{\mathrm{dist}}\bigl(y,z,\mu^*\bigr)
        \;+\;
        \Delta_{\mathrm{angle}}\bigl(y,z,\mu^*\bigr).
    \end{align*}
    So the desired identity is obtained.
\end{proof}

\begin{proof}[Proof for Proposition~\ref{prp:angle_distance_decomposition}]
    Let
    \begin{itemize}
        \item \(r_0 \;=\; d\bigl(\mu^*(x),\,u_0\bigr)\).  (A constant for each \(x\) if \(u_0\) is fixed.)
        \item \(r(y) \;=\; d\bigl(\mu^*(x),\,y\bigr)=R_x(y)\).  (A variable depending on \(y\).)
        \item \(\alpha(y) \;=\; d(u_0,y)\).  Another side of the triangle.
    \end{itemize}
    Then from the local law of cosines,
    \begin{align*}
        r(y)^2 
        \;=\;
        r_0^2 + \alpha(y)^2 
        \;-\;
        2\,r_0\,\alpha(y)\,\cos\bigl(\angle_{\mu^*(x)}(u_0,y)\bigr).
    \end{align*}
    But \(\angle_{\mu^*(x)}(u_0,y)=\phi_x(y)\).
    So
    \begin{align*}
        r(y)^2
        \;=\;
        r_0^2 + \alpha(y)^2 \;-\; 2\,r_0\,\alpha(y)\,\cos\bigl(\phi_x(y)\bigr).
    \end{align*}
    We write it as
    \begin{align*}
        \Psi_x(y)
        \;=\;
        r(y)^2
        \;=\;
        r_0^2 + \alpha(y)^2 \;-\; 2\,r_0\,\alpha(y)\,\cos\bigl(\phi_x(y)\bigr).
    \end{align*}
    Now, to link \(\alpha(y)=d(u_0,y)\) with \(r(y)\) and \(\phi_x(y)\), we may do yet another small expansion or an additional law-of-cosines approach.
    If the manifold is small enough in diameter, we can treat \(\alpha(y)\) also as a function of \((r(y),\phi_x(y))\).

    Also, let
    \begin{align*}
        \alpha(y)^2
        \;=\;
        r_0^2 + r(y)^2 
        \;-\;
        2\,r_0\,r(y)\,\cos\bigl(\angle_{u_0}(\mu^*(x),\,y)\bigr).
    \end{align*}
    But \(\angle_{u_0}(\mu^*(x),y)\) is not necessarily the same as \(\phi_x(y)\).
    Then,
    \begin{align*}
        \alpha(y) 
        \;=\;
        \alpha\bigl(r(y),\,\phi_x(y)\bigr)
        \;=\;
        r_0 + O\bigl(r(y)\bigr)
    \end{align*}
    plus terms involving \(\phi_x(y)\). 
    In a small neighborhood, these expansions typically become second-order in \(\phi_x(y)\).
    Hence, \(\alpha(y)\) is not an independent variable; it’s determined once \(\phi_x(y)\) and \(r(y)=R_x(y)\) are known. 

    In addition, 
    \begin{align*}
        r(y)^2
        \;=\;
        r_0^2 + \alpha(y)^2 - 2\,r_0\,\alpha(y)\,\cos\bigl(\phi_x(y)\bigr).
    \end{align*}
    This yields a final expression of form
    \begin{align*}
        r(y)^2
          \;=\;
          r_0^2
          \;+\;
          \Bigl(\text{some linear or quadratic function in }r(y)\Bigr)
          \;+\;
          \Bigl(\text{terms in }\phi_x(y)\Bigr).
    \end{align*}

    In short, the function \(\Psi_x(y) = r(y)^2\) can be viewed as
    \begin{align*}
        \Psi_x(y)
        \;=\;
        \underbrace{f_{\mathrm{radial}}\bigl(r(y)\bigr)}_{\text{part ignoring angles}}
        \;+\;
        \underbrace{f_{\mathrm{angle}}\bigl(r(y), \,\phi_x(y)\bigr)}_{\text{angle corrections}},
    \end{align*}
    where \(f_{\mathrm{angle}}\) is typically second‐order or cross‐term in \(\phi_x(y)\).

    Consider
    \begin{align*}
        \mathbb{E}_{\nu_x}\bigl[\Psi_x(Y)\bigr]
        \;=\;
        \int r(y)^2\,d\nu_x(y).
    \end{align*}
    Let
    \begin{itemize}
        \item \( \mathbb{E}_{\nu_x}[\,r(Y)\,] \) as some average radius. 
        \item \( \mathbb{E}_{\nu_x}[\phi_x(Y)] \) as average angle.  
    \end{itemize}

    One obtains expansions, where
    \begin{align*}
        \Psi_x(Y) - r(y)^2_{\big| \phi_x(Y)=0}
    \end{align*}
    is some cross or higher‐order term in \(\phi_x(Y)\).

    Then,
    \begin{align*}
        \mathbb{E}\bigl[\Psi_x(Y)^2\bigr]
        \;=\;
        \int \bigl[r(y)^2\bigr]^2\,d\nu_x(y).
    \end{align*}
    Expanding \(\bigl[r(y)^2\bigr]^2\) yields
    \begin{align*}
        \bigl[r(y)^2\bigr]^2
        \;=\;
        r(y)^4
        \;=\;
        \Bigl( f_{\mathrm{radial}}(r(y)) + f_{\mathrm{angle}}(r(y),\,\phi_x(y)) \Bigr)^2.
    \end{align*}
    One obtains terms:
    \begin{itemize}
        \item \(\bigl[f_{\mathrm{radial}}(r)\bigr]^2\),  
        \item cross terms \(2\,f_{\mathrm{radial}}(r)\,f_{\mathrm{angle}}(r,\phi)\),  
        \item \(\bigl[f_{\mathrm{angle}}(r,\phi)\bigr]^2\).  
    \end{itemize}

    By taking expectation,
    \begin{align*}
        \mathbb{E}\bigl[r(y)^4\bigr]
        \;=\;
        \mathbb{E}\Bigl(\bigl[f_{\mathrm{radial}}(r)\bigr]^2\Bigr)
        \;+\;
        2\,\mathbb{E}\Bigl( f_{\mathrm{radial}}(r)\,f_{\mathrm{angle}}(r,\phi)\Bigr)
        \;+\;
        \mathbb{E}\Bigl(\bigl[f_{\mathrm{angle}}(r,\phi)\bigr]^2\Bigr).
    \end{align*}
    Then, \(\mathrm{Var}[\Psi_x(Y)] = \mathbb{E}[\Psi_x(Y)^2] - (\mathbb{E}[\Psi_x(Y)])^2\) can be rearranged, grouping the radial part of the variance from the angle cross terms:
    \begin{align*}
        \mathrm{Var}\bigl[\Psi_x(Y)\bigr]
        =
        \mathrm{Var}\Bigl(\underbrace{f_{\mathrm{radial}}(r(Y))}_{\text{like }r(Y)^2\text{ ignoring angles}}\Bigr)
        +
        \mathrm{Cov}\bigl[\phi_x(Y),\,r(Y)^2\bigr]
        +
        \bigl(\text{smaller or higher‐order expansions in }\phi_x(Y)\bigr).
    \end{align*}

    Explicitly, let 
    \begin{align*}
        A_x(Y) \;=\; f_{\mathrm{radial}}\bigl(r(Y)\bigr)\quad(\text{often }=r(Y)^2)
    \end{align*}
   ignoring angular corrections, and
   \begin{align*}
       B_x(Y) \;=\; f_{\mathrm{angle}}\bigl(r(Y), \phi_x(Y)\bigr)
    \quad(\text{some function capturing dependence on angle }\phi_x(Y)).
   \end{align*}
   Then
   \begin{align*}
       \Psi_x(Y)
       \;=\;
       A_x(Y) \;+\; B_x(Y).
   \end{align*}

    Using
    \begin{align*}
        \mathrm{Var}[A+B] 
        = 
        \mathrm{Var}[A] + \mathrm{Var}[B] + 2\,\mathrm{Cov}(A,B),
    \end{align*}
    one have
    \begin{align*}
        \mathrm{Var}[\Psi_x(Y)]
        \;=\;
        \mathrm{Var}[A_x(Y)]
        \;+\;
        \mathrm{Var}[B_x(Y)]
        \;+\;
        2\,\mathrm{Cov}\bigl(A_x(Y),\,B_x(Y)\bigr).
    \end{align*}
    If \(B_x(Y)\) is small or mostly depends on \(\phi_x(Y)\) with some bounding condition, one can interpret \(\mathrm{Var}[B_x(Y)]\) and \(\mathrm{Cov}(A_x(Y),\,B_x(Y))\) as cross/higher‐order expansions.  
    Here, \(\mathrm{Var}[A_x(Y)]\) is the purely radial piece \(\mathrm{Var}[R_x(Y)^2]\). The cross terms or expansions in \(\phi_x(Y)\) become \(\mathrm{Cov}\bigl(\phi_x(Y),\,R_x(Y)^2\bigr)\).
    Hence we get the claimed partial decomposition.
\end{proof}

\clearpage

\section{Additional Analysis on $\epsilon$-Approximate $\mathrm{CAT}(K)$ Space}
\label{sec:additional_analysis_approximate_cat_k}
In comparison geometry framework, the theoretical statements are provided on the model space with constant curvature.
In practice, however, real-world datasets may lie in spaces that only approximately satisfy the curvature conditions.
Below we introduce an $\epsilon$-approximate version of $\mathrm{CAT}(K)$ space, and derive perturbed versions of existence, uniqueness, and convexity-type results.
\begin{definition}[$\epsilon$-Approximate $\mathrm{CAT}(K)$ Space]
    Let $\epsilon > 0$.
    A geodesic metric space $(\mathcal{M}, d)$ is said to be $\epsilon$-approximate $\mathrm{CAT}(K)$ space if for every geodesic triangle $\triangle pqr$ of perimater less than $2D_K$ (where $D_K = \pi / \sqrt{K}$ if $K > 0$, otherwise $D_K = \infty$), and for any points $x$ and $y$ on the edges $[pq]$ and $[qr]$, respectively, one has
    \begin{align}
        d(x, y) \leq d_{\mathbb{M}^2_K}(\bar{x}, \bar{y}) + \epsilon,
    \end{align}
    where $\triangle \bar{p}\bar{q}\bar{r} \subset \mathbb{M}_K^2$ is the usual comparison triangle in the simply connected model space of constant curvature $K$.
\end{definition}
This definition allows a small additive slack $\epsilon$ in the usual comparison inequality.
When $\epsilon = 0$, we recover the standard definition of $\mathrm{CAT}(K)$.

\begin{theorem}[Approximate Geodesic Convexity of Squared Distance]
\label{thm:approximate_geodesic_convexity_of_squared_distance}
    Let $(\mathcal{M}, d)$ be an $\epsilon$-approximate $\mathrm{CAT}(K)$ space with $K < 0$.
    Fix any $p \in \mathcal{M}$, and define $f(x) = d^2(p, x)$.
    Then, for any geodesic $\gamma \colon [0, 1] \to \mathcal{M}$,
    \begin{align}
        f(\gamma(t)) \leq (1 - t) f(\gamma(0)) + t f(\gamma(1)) + O(\epsilon D),
    \end{align}
    where $D$ is the diameter of the relevant geodesic segment under consideration, or the whole space if bounded.
\end{theorem}
\begin{proof}
    Let $\gamma \colon [0, 1] \to \mathcal{M}$ be a geodesic from $\gamma(0) = x$ to $\gamma(1) = y$.
    Define $\gamma(t)$ as the point at parameter $t$.
    We form a (possibly degenerate) triangle $\triangle pxy$ in $\mathcal{M}$.
    Then, $\triangle \bar{p}\bar{x}\bar{y}$ is the comparison triangle in the model space $\mathbb{M}_K^2$ that has side lengths
    \begin{align*}
        d_{\mathbb{M}_K^2}(\bar{p}, \bar{x}) = d(p, x), \quad d_{\mathbb{M}^2_K}(\bar{x}, \bar{y}) = d(x, y), \quad _{\mathbb{M}_K^2}(\bar{y}, \bar{p}) = (y, p).
    \end{align*}
    Let $\bar{\gamma}(t)$ be the point on $[\bar{x}, \bar{y}] \subset \triangle \bar{p}\bar{x}\bar{y}$ at fraction $t$.
    Because $\gamma$ is a geodesic and $[\bar{x}, \bar{y}]$ is also a geodesic in $\mathbb{M}_K^2$, the pair $\gamma(t) \leftrightarrow \bar{\gamma}(t)$ correspond naturally for the sub-segment ratio $t$.
    Here, we have
    \begin{align*}
        d(p, \gamma(t)) \leq d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t)) + C_1\epsilon,
    \end{align*}
    for some constant $C_1$.
    By taking squares,
    \begin{align*}
        d^2(p, \gamma(t)) \leq \left(d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t))\right)^2 + 2C_1 \epsilon d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t)) + (C_1\epsilon)^2.
    \end{align*}
    Since $K < 0$, the model space $\mathbb{M}_K^2$ is either Euclidean or hyperbolic.
    In both cases, it is known that
    \begin{align*}
        \{\bar{\gamma}(t) \mid t \in [0, 1] \} \subset [\bar{x}, \bar{y}],
    \end{align*}
    which yields $\bar{\gamma}(t)$ satisfying the usual convexity of the squared distance in a non-positive curvature setting.
    \begin{align*}
        \left(d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t))\right)^2 \leq (1 - t)\left(d_{\mathbb{M}_K^2}(\bar{p}, \bar{x})\right)^2 + t\left(d_{\mathbb{M}_K^2}(\bar{p}, \bar{y})\right)^2.
    \end{align*}
    Therefore,
    \begin{align*}
        d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t))^2 \leq (1 - t)d^2(p, x) + t d^2(p, y),
    \end{align*}
    and
    \begin{align*}
        d^2(p, \gamma(t)) &\leq (1 - t)d^2(p, x) + t d^2(p, y) + 2 C_1 \epsilon \left(d_{\mathbb{M}_K^2}(\bar{p}, \bar{\gamma}(t))\right) + (C_1 \epsilon)^2 \\
        &\leq (1 - t)d^2(p, x) + t d^2(p, y) + 2 C_1 \epsilon D' + (C_1 \epsilon)^2 \\
        &\leq (1 - t)d^2(p, x) + t d^2(p, y) + C_2 \epsilon D,
    \end{align*}
    for some constant $C_2 > 0$, where $D'$ is the diameter of the model space, and can be bounded by local diameter $D$.
    This can be written as
    \begin{align*}
        f(\gamma(t)) = d^2(p, \gamma(t)) \leq (1 - t) f(\gamma(0)) + t f(\gamma(1)) + C_2 \epsilon D,
    \end{align*}
    and it exactly states the approximate geodesic convexity for $f(x) = d^2(p, x)$.
\end{proof}
\begin{corollary}[Approximate Uniqueness of Fréchet Mean]
    Under the same $\epsilon$-approximate $\mathrm{CAT}(K)$ assumptions, consider the Fréchet functional
    \begin{align}
        F(x) = \int_\mathcal{M} d^2(y, x) d\nu(y),
    \end{align}
    for a compactly supported probability measure $\nu$.
    Then, one has the following.
    \begin{itemize}
        \item  A minimizer of $F$ exists for any $\epsilon > 0$.
        \item If $\epsilon$ is small, any two minimizers $m_1$ and $m_2$ must lie within a small neighborhood of each other:
        \begin{align}
            d(m_1, m_2) \leq O(\sqrt{\epsilon}).
        \end{align}
        Hence, strict uniqueness is replaced by an $\epsilon$-dependent bound.
    \end{itemize}
\end{corollary}
\begin{proposition}[Local Existence and Uniqueness]
    \label{prp:local_existence_and_uniqueness}
    Let $\mathcal{M}$ be a geodesic metric space that is $\mathrm{CAT}(K)$ (or $\epsilon$-approximately $\mathrm{CAT}(K)$ space) locally in a geodesic ball $B(p_0, R)$.
    That is, for any geodesic triangle fully contained in $B(p_0, R)$, the usual $\mathrm{CAT}(K)$ (or approximate) triangle comparison property holds.
    Suppose $\nu$ is a probability measure on $\mathcal{M}$ whose support $\mathrm{supp}(\nu)$ is contained in $B(p_0, R)$.
    Define the Fréchet functional
    \begin{align*}
        F(x) = \int_\mathcal{M} d^2(y, x) d\nu(y).
    \end{align*}
    Then, one has the following.
    \begin{itemize}
        \item The function $F(x)$ attains its minimum at some $m \in B(p_0, R)$.
        \item If $K > 0$ but $\mathrm{diam}(\mathrm{supp}(\nu)) < \frac{\pi}{2\sqrt{K}}$, or if $K \leq 0$ (no diameter restriction), then $m$ is unique within $B(p_0, R)$.
    \end{itemize}
\end{proposition}
In other words, the Fréchet mean $m$ exists in the local ball $B(p_0, R)$ and is unique when the (local) curvature constraints enforce strict geodesic convexity.

\begin{proposition}[Heavy-Tailed Distributions and Slower Convergence]
    \label{prp:tail_bound_frechet_mean}
    Let $\mathcal{M}$ be either a strict $\mathrm{CAT}(K)$ space or an $\epsilon$-approximate $\mathrm{CAT}(K)$ space of diameter $\leq D$.
    Suppose $Y_1,Y_2,\dots,Y_n$ are i.i.d. random points in $\mathcal{M}$ with common distribution $\nu$.
    Denote by
    \begin{align*}
        \mu &= \argmin_{z \in \mathcal{M}}\mathbb{E}[d^2(Y, z)] \\
        \hat{\mu} &= \argmin_{z \in \mathcal{M}}\frac{1}{n}\sum^n_{i=1}d^2(Y_i, z).
    \end{align*}
    Assume that
    \begin{enumerate}
        \item $\nu$ has finite second moments $\mathbb{E}[d^2(Y, z_0)] < \infty$ for some reference point $z_0$, and
        \item the random variable $d^2(Y, z_0)$ satisfies a sub-exponential-type tail bound: there exist constants $\alpha \geq 0$, $\gamma \in (0, 1]$ such that
        \begin{align}
            \mathbb{P}\left(d^2(Y, z_0) > t\right) \leq \exp(-\alpha t^\gamma),
        \end{align}
        for all $t > 0$.
    \end{enumerate}
    Then, there exist constants $c, C$ such that for all $n \geq 1$ and all $\epsilon > 0$,
    \begin{align}
        \mathbb{P}\left(d(\hat{\mu}_n, \mu) \geq \epsilon \right) \leq C \exp\left(-cn \epsilon^{2\gamma} \right).
    \end{align}
    Hence $\hat{\mu}_n$ converges to $\mu$ in probability, and its deviation tails decay sub-exponentially with arte $\epsilon^{2\gamma}$.
\end{proposition}
\begin{proof}
    Define the population and empirical Fréchet functionals
    \begin{align*}
        F(z) = \mathbb{E}[d^2(Y, z)], \quad F_n(z) = \frac{1}{n}\sum^n_{i=1}d^2(Y_i, z).
    \end{align*}
    By definition,
    \begin{align*}
        \mu = \argmin_{z \in \mathcal{M}} F(z), \quad \hat{\mu}_n = \argmin_{z \in \mathcal{M}} F_n(z).
    \end{align*}
    Observe that
    \begin{align*}
        F(\hat{\mu}_n) - F(\mu) &= \left\{F(\hat{\mu}_n) - F_n(\hat{\mu}_n)\right\} + \left\{F_n(\hat{\mu}_n) - F_n(\mu)\right\} + \left\{F_n(\mu) - F(\mu)\right\} \\
        &\leq \left\{F(\hat{\mu}_n) - F_n(\hat{\mu}_n)\right\} - \left\{F(\mu) - F_n(\mu)\right\}, \\
        \left|F(\hat{\mu}_n) - F(\mu)\right| &\leq \left|F(\hat{\mu}_n) - F_n(\hat{\mu}_n)\right| + \left|F(\mu) - F_n(\mu)\right|.
    \end{align*}
    Therefore,
    \begin{align*}
        \left\{d(\hat{\mu}_n, \mu) \geq \epsilon \right\} \subseteq \left\{F(\hat{\mu}_n) - F(\mu) \geq \alpha(K, D)\epsilon^2 \right\} \subseteq \left\{\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z) \geq \frac{\alpha(K, D)}{2}\epsilon^2 \right|\right\}.
    \end{align*}
    Here,
    \begin{align*}
        \sup_{z \in \mathcal{M}}\left|F_n(z) - F(z)\right| \leq \max_{1\leq j \leq N_\delta}\left|F_n(z_j) - F(z_j)\right| + \eta(\delta),
    \end{align*}
    where $N_\delta \leq \exp(C_1(D / \delta)^m)$ is a $\delta$-net for some $m$ and $\eta(\delta) \to 0$ as $\delta \to 0$.
    Taking $\delta \to 0$,
    \begin{align*}
        \mathbb{P}\left(\sup_{z \in \mathcal{M}}\left|F_n(z) - F(z)\right| \geq t \right) &\leq N_\delta \cdot 2\exp\left(-c' n t^\gamma \right) + \mathbb{P}(\eta(\delta) \geq t / 2) \\
        &\approx \exp(\ln N_\delta - c' n t^\gamma).
    \end{align*}
    For fixed $D$, $\log N_\delta$ is polynomial in $(1 / \delta)$ so we can absorb that into a constant factor.
\end{proof}

\clearpage

\section{Details of Experiments}
\label{sec:details_of_experiments}
This section describes the details of experiments in Section~\ref{sec:experiments}.

\paragraph{Model Details}
Throughout the experiment, we use an implementation of Fréchet regression based on the Nadaraya-Watson estimator~\citep{davis2010population,hein2009robust,steinke2008non}.
\begin{align*}
    \mu^*(x) = \argmin_{z \in \mathcal{M}}\frac{1}{n}\sum^n_{i=1}K_h(X_i - x)d^2(Y_i, z),
\end{align*}
where $K_h$ is a smoothing kernel that corresponds to a probability density with $K_h(\cdot) = h^{-1}K(\cdot / h)$.
For the optimization, we use Limited-memory BFGS~\citep{liu1989limited}.

\begin{listing}[H]
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,fontsize=\footnotesize,linenos]{python}
        import numpy as np
        from scipy.optimize import minimize

        # Kernel function (Gaussian kernel)
        def gaussian_kernel(x, x_data, bandwidth):
            dists = np.linalg.norm(x_data - x, axis=1)
            weights = np.exp(-0.5 * (dists / bandwidth) ** 2)
            return weights / np.sum(weights)
        
        # Fréchet objective function
        def frechet_objective(y, responses, weights, distance_func):
            dists = np.array([distance_func(y, r) for r in responses])
            return np.sum(weights * dists**2)
        
        # Fréchet regression function
        def frechet_regression(X, Y, x_query, bandwidth, distance_func):
            weights = gaussian_kernel(x_query, X, bandwidth)
            y_init = np.mean(Y, axis=0)
            result = minimize(
                frechet_objective,
                y_init,
                args=(Y, weights, distance_func),
                method='L-BFGS-B'
            )
            return result.x
    \end{minted}
    \caption{Python code for the Fréchet regression.}
    \label{lst:python_code_frechet_regression}
\end{listing}

\paragraph{Stereographic Projection}
Listing~\ref{lst:python_code_hyperbolic_mapping} shows the Python code for the stereographic projection from sphere surface to hyperbolic plane.

\begin{listing}[H]
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,fontsize=\footnotesize,linenos]{python}    
    # Define the stereographic projection function
    def stereographic_projection(x, y, z, R):
        u = R * x / (R + z)
        v = R * y / (R + z)
    return u, v
    \end{minted}
    \caption{Python code for the stereographic projection.}
    \label{lst:python_code_hyperbolic_mapping}
\end{listing}

\subsection{Details for Illustrative Example~\ref{sec:experiments:illustrative_example}}

\paragraph{Data Generating Process}
To assess the performance of the Fréchet regression estimator, consider to generate simulated data.
The regression function is
\begin{align*}
    \mu(x)(\cdot) = ((1 - x^2){1/2}\cos(\pi x), (1 - x^2)^{1/2}\sin(\pi x), x), \quad x \in (0, 1),
\end{align*}
which maps a spiral on the sphere.
To generate a random sample $\{(X_i, Y_i)\}^n_{i=1}$, let $X_i \sim \mathcal{U}(0, 1)$ followed by a bivariate normal
random vector $U_i$, and
\begin{align*}
    Y_i = \cos(\|U_i\|)\mu(X_i) + \sin(\|U_i\|)\frac{U_i}{\|U_i\|}.
\end{align*}
The sample size of the simulation data is $n = 50$, and Gaussian noise with variance $0.4$ is added to each instance.

\subsection{Details for Experiments on Real-world Datasets~\ref{sec:experiments:real_world_dataset}}

\paragraph{Details of Datasets}
\begin{itemize}
    \item {\bf HYG Stellar}: The HYG Stellar Database is a comprehensive star catalog that amalgamates data from several prominent astronomical catalogs, including HIPPARCOS, the Yale Bright Star Catalog, and the Gliese Catalog of Nearby Stars. This integration provides detailed information on stars' positions, brightness, spectral types, and various identifiers such as traditional names and Bayer designations.
    It contains detailed information on 119,614 stars including position data, photometric data and luminosity and variability.
    \item {\bf USGS Earthquake}: The USGS Earthquake catalogue provides information on earthquakes worldwide with a magnitude of 2.5 and above that have occurred over the past week, and it contains 300 instances.
    \item {\bf NOAA Climate}: The NOAA Climate data provides Two-Line Element (TLE) sets for weather satellites, including those operated by NOAA, and contains 72 instances.
    A TLE consists of two 69-character lines of data, each containing specific parameters that describe the satellite's orbit.
\end{itemize}
Table~\ref{tab:dataset_details} shows the detailed breakdown of variables $X$ and $Y$ for each dataset.

\begin{table}[h]
    \centering
    \begin{tabular}{c|rll}
        \toprule
         Dataset &  Sample size & Predictor $X$ & Response $Y$ \\
         \midrule
         HYG Stellar & 119,614 & \makecell[l]{\textbullet~~Observation time $t$ \\ \textbullet~~Brightness of the star $m$ \\ \textbullet~~Absolute Magnitude $m'$ \\ \textbullet~~Spectral type $s$} & Position on the celestial sphere \\
         \hline
          USGS Earthquake & 300 & \makecell[l]{\textbullet~~Observation time $t$ \\ \textbullet~~Magnitude of the earthquake $m$ \\ \textbullet~~Depth of the earthquake $d$} & Earthquake location \\
          \hline
          NOAA Climate & 72 & \makecell[l]{\textbullet~~Timestamp of the TLE $t$ \\ \textbullet~~Orbital parameters $\theta$ \\ \textbullet~~Inclination $i$} & Satellite position \\
         \bottomrule
    \end{tabular}
    \caption{Detailed breakdown of variables for each dataset.}
    \label{tab:dataset_details}
\end{table}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/noaa_usgs.png}
    \caption{Visualizations for USGS Earthquake catalogue and NOAA Climate dataset.}
    \label{fig:noaa_usgs}
\end{figure*}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/noaa_usgs_additional.png}
    \caption{Heteroscedasticity in the NOAA and USGS datasets.}
    \label{fig:heteroscedasticity_noaa_usgs}
\end{figure*}

\paragraph{Visualizations of Real-world Spherical Datasets}
Figure~\ref{fig:noaa_usgs} shows the additional visualizations of real-world spherical datasets, and Figure~\ref{fig:heteroscedasticity_noaa_usgs} shows the heteroscedasticity in the NOAA and USGS datasets.
In addition, Python code in Listing~\ref{lst:python_code_visualization_hyg} shows the implementation for the visualization of HYG Steller dataset.


\begin{listing}[t]
    \begin{minted}[frame=lines,framesep=2mm,baselinestretch=1.2,fontsize=\footnotesize,linenos]{python}
    
    import numpy as np
    import matplotlib.pyplot as plt
    from astropy.io import ascii
    
    # Load the Bright Star Catalog
    url = '{Data URL}' # URL for HYG Steller database
    data = ascii.read(url)
    
    # Extract Right Ascension and Declination
    ra = np.array(data['ra'])  # in hours
    dec = np.array(data['dec'])  # in degrees
    
    # Convert RA from hours to degrees
    ra_deg = ra * 15
    
    # Convert RA and Dec to radians for plotting
    ra_rad = np.radians(ra_deg)
    dec_rad = np.radians(dec)
    
    
    # Create a 3D scatter plot
    fig = plt.figure(figsize=(12, 8))
    ax = fig.add_subplot(111, projection='3d')
    
    # Convert spherical coordinates to Cartesian for plotting
    x = np.cos(dec_rad) * np.cos(ra_rad)
    y = np.cos(dec_rad) * np.sin(ra_rad)
    z = np.sin(dec_rad)
    
    # Plot the stars
    ax.scatter(x, y, z, color='white', s=0.01, label="data points")
    
    ax.xaxis.set_ticklabels([])
    ax.yaxis.set_ticklabels([])
    ax.zaxis.set_ticklabels([])
    
    # Set plot parameters
    ax.set_facecolor('black')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    plt.legend(markerscale=80, fontsize=30)
    plt.show()
    \end{minted}
    \caption{Python code for the visualization of HYG Steller database.}
    \label{lst:python_code_visualization_hyg}
\end{listing}