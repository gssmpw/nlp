% !TEX root = ../main.tex

\section{Introduction}
\label{sec:intro}

\myfirstpara{Model Stealing Attacks}
%
Driven by a huge surge in the capabilities of Machine Learning (ML) techniques, many companies now deploy trained ML models on the cloud, and monetize by providing paid access to users via Application Programming Interfaces (APIs). The trained model and the training dataset are often the intellectual property of the company, and therefore the model internals, including the training dataset, model architecture, and weights are kept hidden, providing only black-box access to users. However, such models are vulnerable to model stealing attacks \cite{papernot2017practical,tramer2016stealing,orekondy2019knockoff}, wherein malicious users replicate the behavior of a model by querying the API on a select set of inputs, and training a substitute model on the acquired predictions (see \Cref{teaser}(a)). 
The model thus obtained can either be used as a substitute for the victim model, thereby extracting commercial value from it, or to launch further attacks like adversarial \cite{goodfellow2014explaining,mazeika2022steer} or model inversion \cite{zhang2020secret} on the victim model (called adversarial transfer). In either case, the integrity of the victim model is compromised.
 

\mypara{Foundation Models}
%
Recent development in foundation models such as Vision Transformers \cite{dosovitskiy2020image}, and powerful pre-trained encoders like CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} have greatly advanced the field of Computer Vision. 
The availability of foundation models pre-trained on massive datasets has greatly enhanced the capabilities of model owners, who can train task-specific models on downstream applications easily by fine-tuning these models. The resulting downstream models boast high accuracy, causing model owners to be more inclined towards choosing foundation models over conventional vision architectures \cite{he2016deep,inception}. 
While ViTs \cite{dosovitskiy2020image} are known to be more robust compared to convolutional architectures in terms of both adversarial attacks \cite{shao2022adversarial} and common corruptions \cite{paul2022vision}, the model stealing risks associated with downstreaming these models for commercial APIs have not been systematically investigated.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{images/teaser.pdf}
	\caption{(a) Standard model stealing setup: An adversary picks images from a proxy dataset and queries from the victim model to obtain labels. This labeled proxy dataset is used to train the thief model. (b) Victims derived from foundation models are more prone to stealing: We steal three victim models trained on the CIFAR-10 dataset, using a ViT-L/16 thief. Even though using stronger victims based on foundation models like ViT-L/16 improves victim accuracy, but the agreement between the victim's and thief's predictions also increases at the same time, underlining the increased severity of the threat.}
	\label{teaser}
\end{figure*}

\mypara{Our Focus}
%
In this paper, we study the vulnerability of image classification APIs based on foundation models to model stealing attacks. The victim models are foundation models (ViTs) fine-tuned on downstream datasets that are made accessible via black-box APIs. We consider both final layer fine-tuning, aka \emph{linear probing}, and \emph{full fine-tuning}. 
%
Firstly, we show that using more accurate victim models may not always be the best bet for model owners from the perspective of model stealing. This is because, not only the model owners, but attackers can also avail of these powerful pre-trained models. Assuming a well equipped thief, which has access to at least as strong a foundation model as the victim, we show that victim models obtained by fine-tuning from foundation models have a heightened susceptibility to theft compared to smaller models like ResNets. This is due to the extensive knowledge encapsulated within these foundation models, which is now available to both victim and adversary/thief models. 
Secondly, attackers armed with access to a foundation model, by virtue of stronger representation available, can execute model theft with relative ease, when compared to shallower thieves. 

\myparand{Higher Accuracy or Better Privacy?}
%
By carrying out this study, we aim to raise awareness about the risks associated with using foundation models in the Machine Learning as a Service (MLaaS) setup. While it is always beneficial for an adversary to use foundation models for stealing, the victim needs to be more careful in their choice, and must choose between higher accuracy and better privacy. Our study also underscores the necessity for robust security protocols and countermeasures in the deployment and utilization of these models.

\mypara{Contributions} We make the following contributions:
%
\begin{enumerate*}[label=\textbf{(\arabic*)}]
	\item We conduct a thorough systematic study on three datasets, seven victim models, and four thief models to evaluate the model stealing vulnerability of victim models obtained from foundation models, particularly ViTs, either via linear probing or by fine-tuning all layers.
	\item Our studies conclude that, under a strong attack wherein the thief has access to pre-trained foundation models, models fine-tuned from ViTs are more vulnerable to theft as compared to models fine-tuned from convolutional architectures. Using a ViT-L/16 thief, we report agreements of 94.28\%, 60.52\% and 62.94\% for victim models based on ViT-L/16 model trained on CIFAR-10, Indoor-67 and Caltech-256 datasets respectively, compared to agreements of 73.20\%, 40.22\% and 46.23\% respectively for ResNet-18 based victims.
	\item Even when the victim model is not fine-tuned from a foundation model, we show that foundation models greatly enhance the attacker's capabilities. When stealing a ResNet-18 victim trained on CIFAR-10 dataset, we report an agreement of 77.12\% for a ViT-B/16 CLIP-based thief, compared to 64.53\% agreement for a ResNet-34 thief (refer \Cref{fig:lp_datasets}).
\end{enumerate*}

