% !TEX root = ../main.tex

\section{Experimental Setup}

\subsection{Victim Datasets and Architectures}

\mypara{Datasets} 
%
We adopt three commonly-used image recognition datasets to train our victim models.
 \textbf{CIFAR-10} \cite{krizhevsky2009learning}, \textbf{Caltech-256} \cite{griffin2007caltech}, and  \textbf{Indoor-67} \cite{quattoni2009recognizing}. Of these, CIFAR-10 and Caltech-256 are general-purpose object recognition datasets with 10 and 256 object classes respectively, while Indoor-67 is a fine-grained classification dataset comprising 67 types of indoor scenes.

\mypara{Architectures}
We work with seven different victim architectures in our experiments. These include four conventional CNN architectures: \textbf{ResNet-18}, \textbf{ResNet-34}, \textbf{ResNet-50} and \textbf{ResNet-101} \cite{he2016deep} pre-trained on ImageNet-1K dataset \cite{russakovsky2015imagenet} containing 1.2M images, and three vision foundational models: \textbf{ViT-S/16}, \textbf{ViT-B/16} and \textbf{ViT-L/16} \cite{dosovitskiy2020image} pre-trained on the larger ImageNet-21K dataset \cite{deng2009imagenet} containing 14M images from 21,841 classes. 
%We also use and a vision-language foundation model \textbf{CLIP} \cite{radford2021learning} based on the ViT-B/16 architecture, and pre-trained on the much larger LAION-2B dataset \cite{schuhmann2022laion} containing 2.32 billion image-text pairs using contrastive learning, and fine-tuned later on the ImageNet-1K dataset. 
The power of these models comes not only from the deeper architectures, but also from stronger pre-training on massive datasets.  

\mypara{Training details}
Fine-tuning on ImageNet-pretrained weights is a widely adopted method of training CNN models. However, with the advent of powerful foundation models, fine-tuning only the final layer (a.k.a linear probing) is gaining popularity, as it offers good results at low training costs. We, therefore, consider both scenarios in our experiments. 
All models are initialized using publicly available ImageNet \cite{russakovsky2015imagenet} pre-trained weights: the ResNet models are pre-trained on ImageNet-1K, and the ViTs are pre-trained using the larger ImageNet-21K dataset. All pre-trained weights are obtained from open-source libraries like Pytorch hub \cite{torchhub} or timm \cite{rw2019timm}.
%
All victim models are trained using SGD optimizer for 100 epochs, with a momentum of 0.9, weight decay of $5 \times 10^{-4}$, and batch size of 128. An initial learning rate is $0.002$ is decayed by a factor of $10$ after every 30 epochs. All input images are resized to $224\times224$ and augmented using random cropping and random horizontal flipping. 



\subsection{Thief Datasets and Architectures} 

\mypara{Dataset}
%
We use publicly available unannotated images from the training set of ILSVRC-2012 challenge dataset \cite{russakovsky2015imagenet} as the thief's proxy dataset. Commonly known as ImageNet, the dataset contains 1.2 million images of varying sizes, of which we only use a subset of 128K images resized to $224\times224$. 

\mypara{Architectures}
We consider multiple model architectures for the thief model. These include a CNN model ResNet-34 \cite{he2016deep}, two ViT models: ViT-B/16 and ViT-L/16 \cite{dosovitskiy2020image}, as well as a multimodal foundation model ViT-B/16 CLIP \cite{radford2021learning}. 
%
In general, we assume that the thief has access to all publicly available pre-trained models that the victim has access to.  

\mypara{Training details}
Thief models are trained using SGD optimizer for 100 epochs, with a momentum of 0.9, weight decay of $5 \times 10 ^{-4}$, and a batch size of 128 (for ResNet-34), 32 (for ViT-B/16 and ViT-B/16 CLIP) or 64 (ViT-L/16). The initial learning rate is $0.001$, and it is decayed by a factor of $10$ after 50 epochs. 
For stealing CIFAR-10 vicitm, input images are resized to $224\times224$, and augmented using random cropping, random rotation and random horizontal flip. For stealing other victims, we use random crop, random horizontal flip and RandAugment \cite{cubuk2020randaugment}. 
%
All networks are initialized from ImageNet-pretrained weights. For ResNet thieves, all layers are fine-tuned, whereas for ViT's only the final layer is fine-tuned.
%
\mypara{Sample Selection} For majority of our experiments, we consider the random-adversary based sample selection method from Knockoff Nets \cite{orekondy2019knockoff}, and limit the thief's query budget to 5000 samples. Of these, $10\%$ samples are set aside as validation data for hyper-parameter selection, and the rest are used for training. 
%
In \Cref{sec:ablation}, we evaluate the impact of the sample selection method and query budget on our findings. 

\subsection{Evaluation Metrics} 
%
We use two evaluation metrics prevalent in the literature. \emph{Accuracy} measures the performance of the thief model on the victim model's held-out test set, while \emph{Agreement} measures how often the thief's prediction matches the victim's. Note that the held-out victim dataset is not accessible to the thief, and is only used for evaluation. 




