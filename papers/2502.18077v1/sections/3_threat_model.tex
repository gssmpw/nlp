% !TEX root = ../main.tex

\section{Methodology}

\subsection{Victim Model} 
The victim model is a neural network $f_\textrm{V}$ trained for image classification. The model owner trains $f_\textrm{V}$ on labeled images from training data distribution $P_V(X)$. To ease the training, the model owner uses an open-source pre-trained model, and fine-tunes it on their dataset, called victim dataset. The pre-trained model could either be conventional architectures popularly used in computer vision, e.g., VGG \cite{simonyan2014very} or ResNet \cite{he2016deep}, or modern large foundation models like ViT \cite{dosovitskiy2020image} or CLIP \cite{radford2021learning} which are pretrained on huge datasets and come with strong representation power. The output of the model, $y \in \{1, \dots K\}$, is a distribution over $K$ classes. 

The trained victim model is deployed on a cloud service platform as a black-box. In this setting, the victim's architecture and weights are hidden, but a user can query the model via an API and obtain its predictions on a given image. This setup allows the victim to monetize from their model by charging the user on query basis. 
Several previous works assume the availability of the full probability vector $f_V(x) \in \mathbb{R}^K$ (also known as \emph{soft labels}) as the victim's output. However, many real-world APIs only return the topmost prediction $\argmax_{i\in\{1, \dots, K\}} f_V(x)_i$ for a queried image, also known as \emph{hard-label}. We adapt the hard-label setup in our experiments, on account of it being closest to real-world scenario.

\subsection{Attacker's Goal} 
The attacker's objective is to replicate the behaviour of the victim model. We don't mean to replicate the exact weights of the victim neural network, but to train a substitute/thief model $f_{T}$ that is functionally equivalent to the victim model in terms of predictions on the victim's held-out test set. 

\subsection{Attack Method} \label{sec:attack_method}
A major constraint for the attacker is that it does not have access to the victim's training data distribution $P_V(X)$. It therefore, uses a \textit{proxy} distribution $P_A(X)$ to query the victim model. The attacker selects a subset of images $\{x^i\}_{i=1}^m$ from $P_A(X)$ and receives predictions for the queried images, thus constructing a labeled set $\mathcal{D}_{l}=\{(x^i, f_\textrm{V}(x^i)\}_{i=1}^{m}$ of size $m$ which is then used to train the thief model $f_{T}$. Fundamentally, a thief model uses information obtained from the victim in the form of queried labels to learn similar decision boundaries as the victim model. The performance of the thief model depends on the following factors:

\mypara{Proxy dataset}
For computer vision applications, the proxy dataset can be constructed from publicly accessible natural images, or by generating synthetic images. In line with previous works \cite{orekondy2019knockoff,pal2020activethief}, we use large scale publicly available datasets of natural images as the proxy distribution.

\mypara{Query selection method} For practical model stealing attacks, the thief has to work under limited query budgets. As such, there is a large body of works dedicated to selecting the best set of samples from the proxy dataset to query the victim model. This includes the reinforcement-learning methods by \cite{orekondy2019knockoff}, active learning based methods by \cite{pal2020activethief} and GRAD-CAM based methods by \cite{wang2021black}. In this work, we adopt the simple yet effective Random selection strategy \cite{orekondy2019knockoff} for most of our experiments. The impact of changing the sample selection technique is studied in \Cref{sec:ablation}.

\mypara{Thief architecture}
Typically, the attacker has no knowledge of the victim model's architecture or hyper-parameters. However, several works on model stealing assume that the attacker uses the same architecture as the victim model, for ease of experimentation, and also owing to the belief that the thief model's architecture does not significantly affect the model stealing performance. However, given the easy availability of large and powerful foundation models, it would be prudent for an attacker to use these larger models instead. We therefore assume that an informed thief is able to use open-source pretrained models that are available on the internet, and fine-tunes the model on the labeled dataset $\mathcal{D}_{l}$ constructed from the proxy data. For completeness, we also study the special case scenario when the thief model has the same architecture as the victim (in the Supplementary).

