% !TEX root = ../main.tex

\section{Conclusion}
We studied the susceptibility of image classification models fine-tuned from powerful foundation models to model stealing attacks. Our findings reveal that victim models derived from foundation models exhibit greater vulnerability to such attacks from strong, foundation-model based thieves, compared to those derived from shallower backbones. In stark contrast to the celebrated robustness of foundation models against adversarial and natural corruptions, our study sheds light on their heightened susceptibility in the model stealing context. This highlights a crucial trade-off between privacy and accuracy inherent in deploying models fine-tuned from foundation models in commercial APIs, and emphasizes the necessity for enhanced security measures in model deployment strategies.

