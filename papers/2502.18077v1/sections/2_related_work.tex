% !TEX root = ../main.tex

\section{Related Work}

\myfirstpara{Model Stealing Attacks}
%
Model stealing attacks aim to replicate black-box machine learning models, either by extracting exact weights or hyperparameters of the victim model \cite{milli2019model,jagielski2020high,carlini2020cryptanalytic,batina2018csi,hua2018reverse,duddu2018stealing}, or by approximately mimicking the victim model's predictions  \cite{batina2018csi,hua2018reverse,duddu2018stealing}. 
The latter method involves iteratively querying the victim on a designated set of inputs, often referred to as a "proxy" dataset, and subsequently training a substitute model based on the acquired predictions.
Some studies \cite{correia2018copycat,orekondy2019knockoff,pal2020activethief,wang2021black} use natural images from publicly available datasets as the proxy data, devising techniques to select the most informative samples to minimize the attacker's cost. Another school of methods \cite{zhou2020dast, kariyappa2021maze, truong2021data, wang2021delving,sanyal2022towards,Beetham2023} generates synthetic data for querying the victim, eliminating the reliance on natural images but incurring significant costs for the attacker due to the necessity of millions of queries. In this work, we adopt an approximate model stealing setup utilizing natural images, recognizing its practicality and resemblance to real-world attack scenarios.

\mypara{Foundation Models in Computer Vision}
%
Foundation models, initially conceived within the NLP domain, have found widespread adoption within the computer vision community. Trained on massive amounts of data in a supervised or self-supervised manner, these models learn rich, generalized representations that can serve as the backbone for various downstream tasks. Among these, Vision Transformers (ViTs) \cite{dosovitskiy2020image} stand out as prominent foundation models in vision research that excel in capturing global dependencies and contextual information from visual data using self-attention mechanisms. 
Since the original Vision Transformer (ViT) \cite{dosovitskiy2020image} model, several variants have been proposed including DeiT \cite{touvron2021training}, Swin Transformer \cite{liu2021swin}, DINO \cite{caron2021emerging}, etc. 
%
Additionally, models like CLIP \cite{radford2021learning} and ALIGN \cite{jia2021scaling} have further expanded the horizon of foundation models in vision, by aligning visual and textual representations into a shared embedding space. 

\mypara{Foundation Models and Model Stealing}
%
In the NLP domain, Krishna \etal \cite{krishna2019thieves} demonstrated that models fine-tuned from large pretrained language models like BERT can be stolen by issuing semantically irrelevant input queries. In computer vision, model stealing attacks have been studied only for CNN based victim models for the image classification task. Battis \etal evaluated the role of ViTs in model stealing, but only from the attacker's perspective.  An emerging thread of work \cite{liu2022stolenencoder,dziedzic2022difficulty,sha2023can} deals with stealing large pre-trained image encoders like SimCLR \cite{chen2020simple}, MoCo \cite{he2020momentum}, BYOL \cite{grill2020bootstrap} and CLIP \cite{radford2021learning}. These methods aim to steal general-purpose encoders that return rich feature embeddings rather than model posteriors or labels, rendering them highly vulnerable to theft. Differently, in our work, we steal classification models fine-tuned from large foundation models on downstream tasks.
