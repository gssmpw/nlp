@article{Gneiting.2007,
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	issn = {0162-1459},
	doi = {10.1198/016214506000001437},
	number = {477},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E.},
	year = {2007},
	pages = {359--378},
}

@article{JMLR:v25:23-0038,
	title = {Probabilistic forecasting with generative networks via scoring rule minimization},
	volume = {25},
	number = {45},
	journal = {Journal of Machine Learning Research},
	author = {Pacchiardi, Lorenzo and Adewoyin, Rilwan A. and Dueben, Peter and Dutta, Ritabrata},
	year = {2024},
	pages = {1--64},
}

@inproceedings{NEURIPS2021_a7c95857,
	title = {Laplace redux - effortless bayesian deep learning},
	volume = {34},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {20089--20103},
}

@inproceedings{NIPS2017_9ef2ed4b,
	title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	volume = {30},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {1566-2535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	doi = {10.1016/j.inffus.2021.05.008},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
	urldate = {2024-07-25},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	keywords = {Artificial intelligence, Bayesian statistics, Deep learning, Ensemble learning, Machine learning, Uncertainty quantification},
	pages = {243--297},
}

@article{bi_accurate_2023,
	title = {Accurate medium-range global weather forecasting with {3D} neural networks},
	volume = {619},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	doi = {10.1038/s41586-023-06185-3},
	abstract = {Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
	language = {en},
	number = {7970},
	urldate = {2024-07-25},
	journal = {Nature},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Atmospheric dynamics, Computer science},
	pages = {533--538},
}

@inproceedings{botev_practical_2017,
	title = {Practical {Gauss}-{Newton} {Optimisation} for {Deep} {Learning}},
	abstract = {We present an efficient block-diagonal approximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is competitive against state-of-the-art first-order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a laborious process, our approach can provide good performance even when used with default settings. A side result of our work is that for piecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {557--565},
}

@article{bulte_uncertainty_2024,
      author = "Christopher Bülte and Nina Horat and Julian Quinting and Sebastian Lerch",
      title = "Uncertainty quantification for data-driven weather models",
      journal = "Artificial Intelligence for the Earth Systems",
      year = "2025",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      doi = "10.1175/AIES-D-24-0049.1",
      url = "https://journals.ametsoc.org/view/journals/aies/aop/AIES-D-24-0049.1/AIES-D-24-0049.1.xml"
}

@article{chen_generative_2024,
	title = {Generative machine learning methods for multivariate ensemble postprocessing},
	volume = {18},
	issn = {1932-6157, 1941-7330},
	doi = {10.1214/23-AOAS1784},
	abstract = {Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require postprocessing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate postprocessing have been proposed where ensemble predictions are first postprocessed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular, the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate postprocessing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper scoring rule, which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. Our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. In two case studies on multivariate temperature and wind speed forecasting at weather stations over Germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.},
	number = {1},
	urldate = {2024-07-16},
	journal = {The Annals of Applied Statistics},
	author = {Chen, Jieyu and Janke, Tim and Steinke, Florian and Lerch, Sebastian},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Generative machine learning, ensemble postprocessing, multivariate postprocessing, probabilistic forecasting, weather forecasting},
	pages = {159--183},
}

@article{guo_ib-uq_2023,
	title = {{IB}-{UQ}: {Information} bottleneck based uncertainty quantification for neural function regression and neural operator learning},
	shorttitle = {{IB}-{UQ}},
	url = {http://arxiv.org/abs/2302.03271},
	abstract = {We propose a novel framework for uncertainty quantification via information bottleneck (IB-UQ) for scientific machine learning tasks, including deep neural network (DNN) regression and neural operator learning (DeepONet). Specifically, we incorporate the bottleneck by a confidence-aware encoder, which encodes inputs into latent representations according to the confidence of the input data belonging to the region where training data is located, and utilize a Gaussian decoder to predict means and variances of outputs conditional on representation variables. Furthermore, we propose a data augmentation based information bottleneck objective which can enhance the quantification quality of the extrapolation uncertainty, and the encoder and decoder can be both trained by minimizing a tractable variational bound of the objective. In comparison to uncertainty quantification (UQ) methods for scientific learning tasks that rely on Bayesian neural networks with Hamiltonian Monte Carlo posterior estimators, the model we propose is computationally efficient, particularly when dealing with large-scale data sets. The effectiveness of the IB-UQ model has been demonstrated through several representative examples, such as regression for discontinuous functions, real-world data set regression, learning nonlinear operators for partial differential equations, and a large-scale climate model. The experimental results indicate that the IB-UQ model can handle noisy data, generate robust predictions, and provide confident uncertainty evaluation for out-of-distribution data.},
	language = {en},
    doi = {10.48550/arXiv.2302.03271},
	urldate = {2024-06-05},
	journal = {arXiv preprint},
	author = {Guo, Ling and Wu, Hao and Zhou, Wenwen and Wang, Yan and Zhou, Tao},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{magnani_linearization_2024,
	title = {Linearization {Turns} {Neural} {Operators} into {Function}-{Valued} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2406.05072},
	abstract = {Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using functionvalued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure–a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations.},
	language = {en},
	urldate = {2024-07-25},
	journal = {arXiv preprint},
    doi = {10.48550/arXiv:2406.05072},
	author = {Magnani, Emilia and Pförtner, Marvin and Weber, Tobias and Hennig, Philipp},
	year = {2024},
	keywords = {Computer Science - Machine Learning, G.1.0, I.2.6, G.3, G.1.8, Statistics - Machine Learning},
}

@inproceedings{pmlr-v48-gal16,
	address = {New York, New York, USA},
	series = {Proceedings of machine learning research},
	title = {Dropout as a bayesian approximation: {Representing} model uncertainty in deep learning},
	volume = {48},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
	booktitle = {Proceedings of the 33rd international conference on machine learning},
	publisher = {PMLR},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
	month = jun,
	year = {2016},
	pages = {1050--1059},
}

@article{rasp_neural_2018,
	title = {Neural {Networks} for {Postprocessing} {Ensemble} {Weather} {Forecasts}},
	volume = {146},
	issn = {0027-0644, 1520-0493},
	doi = {10.1175/MWR-D-18-0187.1},
	abstract = {Abstract
            Ensemble weather predictions require statistical postprocessing of systematic errors to obtain reliable and accurate probabilistic forecasts. Traditionally, this is accomplished with distributional regression models in which the parameters of a predictive distribution are estimated from a training period. We propose a flexible alternative based on neural networks that can incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned in a data-driven way rather than requiring prespecified link functions. In a case study of 2-m temperature forecasts at surface stations in Germany, the neural network approach significantly outperforms benchmark postprocessing methods while being computationally more affordable. Key components to this improvement are the use of auxiliary predictor variables and station-specific information with the help of embeddings. Furthermore, the trained neural network can be used to gain insight into the importance of meteorological variables, thereby challenging the notion of neural networks as uninterpretable black boxes. Our approach can easily be extended to other statistical postprocessing and forecasting problems. We anticipate that recent advances in deep learning combined with the ever-increasing amounts of model and observation data will transform the postprocessing of numerical weather forecasts in the coming decade.},
	number = {11},
	urldate = {2024-07-25},
	journal = {Monthly Weather Review},
	author = {Rasp, Stephan and Lerch, Sebastian},
	month = nov,
	year = {2018},
	pages = {3885--3900},
}

@article{walz_easy_2024,
	title = {Easy {Uncertainty} {Quantification} ({EasyUQ}): {Generating} {Predictive} {Distributions} from {Single}-{Valued} {Model} {Output}},
	volume = {66},
	issn = {0036-1445},
	shorttitle = {Easy {Uncertainty} {Quantification} ({EasyUQ})},
	doi = {10.1137/22M1541915},
	abstract = {Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs.},
	number = {1},
	urldate = {2024-07-25},
	journal = {SIAM Review},
	author = {Walz, Eva-Maria and Henzi, Alexander and Ziegel, Johanna and Gneiting, Tilmann},
	month = feb,
	year = {2024},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {91--122},
}

@inproceedings{weber2024uncertainty,
	title = {Uncertainty quantification for fourier neural operators},
	url = {https://openreview.net/forum?id=knSgoNJcnV},
	booktitle = {{ICLR} 2024 workshop on {AI4DifferentialEquations} in science},
	author = {Weber, Tobias and Magnani, Emilia and Pförtner, Marvin and Hennig, Philipp},
	year = {2024},
}

