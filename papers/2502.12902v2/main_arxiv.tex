\documentclass{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[breaklinks, colorlinks=true, citecolor=black, urlcolor=black, linkcolor=black]{hyperref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{subcaption}
\usepackage[breaklinks, colorlinks=true, citecolor=black, urlcolor=black, linkcolor=black]{hyperref}
\usepackage{subcaption}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,bbm,amsfonts,bm}
\usepackage{color,graphicx,cases,caption,enumerate}
\usepackage{mathtools}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{placeins}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\q}{\ensuremath{\mathbb{Q}}}
\newcommand{\e}{\ensuremath{\mathbb{E}}}
\newcommand{\Ree}{\ensuremath{\mathbb{R}}}
\DeclarePairedDelimiter\parens{\lparen}{\rparen}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\bracks{\lbrack}{\rbrack}
\DeclarePairedDelimiter\braces{\lbrace}{\rbrace}
\DeclareMathOperator*{\argmin}{argmin}

\usepackage[textsize=tiny]{todonotes}



\title{Probabilistic neural operators for functional uncertainty quantification}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{{Christopher BÃ¼lte}\\
  Ludwig-Maximilians-Universit\"at M\"unchen \\
    Munich Center for Machine Learning (MCML) \\
	Munich, Germany\\
	\texttt{buelte@math.lmu.de} \\
	%% examples of more authors
	 \And
    {Philipp Scholl}\\
  Ludwig-Maximilians-Universit\"at M\"unchen \\
    Munich Center for Machine Learning (MCML) \\
	Munich, Germany\\
	\texttt{scholl@math.lmu.de} \\
    \And    
    {Gitta Kutyniok}\\
  Ludwig-Maximilians-Universit\"at M\"unchen \\
      University of Troms\o{} \\
  DLR-German Aerospace Center \\
  Munich Center for Machine Learning (MCML) \\
	Munich, Germany\\
	\texttt{kutyniok@math.lmu.de} \\
}


% Uncomment to remove the date
\date{}

\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
\renewcommand{\headerleft}{Probabilistic neural operators for functional uncertainty quantification}
\renewcommand{\shorttitle}{}


% Define theorems
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

% \def\month{MM}  % Insert correct month for camera-ready version
% \def\year{YYYY} % Insert correct year for camera-ready version
% \def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}
\maketitle


% keywords can be removed
%\keywords{Neural operators \and Uncertainty quantification \and Probabilistic predictions}


\begin{abstract}
\noindent
Neural operators aim to approximate the solution operator of a system of differential equations purely from data. They have shown immense success in modeling complex dynamical systems across various domains. However, the occurrence of uncertainties inherent in both model and data has so far rarely been taken into account\textemdash{}a critical limitation in complex, chaotic systems such as weather forecasting. In this paper, we introduce the probabilistic neural operator (PNO), a framework for learning probability distributions over the output function space of neural operators. PNO extends neural operators with generative modeling based on strictly proper scoring rules, integrating uncertainty information directly into the training process. We provide a theoretical justification for the approach and demonstrate improved performance in quantifying uncertainty across different domains and with respect to different baselines. Furthermore, PNO requires minimal adjustment to existing architectures, shows improved performance for most probabilistic prediction tasks, and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. Implementing our approach into large-scale models for physical applications can lead to improvements in corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models.
\end{abstract}

\section{Introduction}
Many complex phenomena in the sciences are described via parametric partial differential equations (PDEs), making their study a crucial research topic for systems in biology, physics, or engineering sciences. Since traditional solvers, such as finite element methods are based on a space discretization there exists a tradeoff between solution accuracy and the computational complexity imposed by the discretization resolution. A recent breakthrough has been achieved by neural operators \citep{JMLR:v24:21-1524, luDeepONetLearningNonlinear2021}, which are a class of neural networks that have the capacity to approximate any continuous operator, including any solution operator of a given family of parametric PDEs. Neural operators, and especially Fourier neural operators \citep{li_fourier_2021}, are now commonly used in practice and have been applied to problems such as weather forecasting \citep{pathak_fourcastnet_2022}, fluid dynamics \citep{renn_forecasting_2023}, seismic imaging \citep{li_solving_2023}, carbon capture \citep{wen_real-time_2023}, or plasma modeling in nuclear fusion \citep{gopakumar_plasma_2024}. However, in application scenarios, the entire operator learning process is usually corrupted by different sources of noise. In addition, the model itself can be misspecified or\textemdash{}as in the case of dynamical systems\textemdash{}long-term predictions might be too chaotic to be predictable in a reliable manner. For these cases, it becomes crucial to have a precise understanding of the uncertainty of the corresponding model predictions. 

A common approach to uncertainty quantification for standard neural networks is the Laplace approximation \citep{NEURIPS2021_a7c95857}, which has been extended to Fourier neural operators by \citet{weber2024uncertainty} and \citet{magnani_linearization_2024}. While this approach is efficient to implement and does not require additional training, it has notable limitations. Specifically, it does not incorporate uncertainty during network training, and its assumption of a Gaussian predictive distribution may be overly restrictive. \citet{guo_ib-uq_2023} introduce an information bottleneck structure that utilizes a decoder with a predictive Gaussian distribution. While innovative, this method shares the limitations of the Gaussian assumption.

% and also does not incorporate uncertainty during training.

Our work addresses these challenges by proposing a theoretically grounded approach to incorporate predictive uncertainty during training in the infinite-dimensional setting of neural operators. By leveraging strictly proper scoring rules, our method enables better-calibrated uncertainty estimates while supporting more flexible predictive distributions.


\subsection{Our contributions}
In this article, we introduce the \emph{probabilistic neural operator (PNO)}, which extends the neural operator framework to a probabilistic setting, where the output is in the form of a (functional) probability distribution. Our novel approach is based on recent advances in training generative neural networks via proper scoring rules \citep{JMLR:v25:23-0038}. PNO is designed to generate samples from an (empirical) probability distribution and is trained on an objective function that fits the predictive distribution to the corresponding observations, thereby integrating uncertainty directly into the training process. We build upon prior research by introducing a theoretical framework for proper scoring rules in separable Hilbert spaces. Specifically, we prove that the energy score is a strictly proper scoring rule in these spaces and, therefore, leads to desirable properties as a loss function.
% We extend previous work by providing a theoretical framework for proper scoring rules in separable Hilbert spaces, by proving that the energy score is a strictly proper scoring rule in separable Hilbert spaces and, therefore, leads to desirable properties as a loss function.
The resulting predictions of the network are well-calibrated and can be\textemdash{}due to their sample-based character\textemdash{}applied for different downstream tasks, including extreme event analysis and estimation of dependencies. We evaluate PNO on PDE datasets and a real-world data task, namely the prediction of surface temperature across Europe.
%We demonstrate the approach across several benchmark datasets with different underlying neural operator architectures and different baselines for uncertainty quantification. A focus is set on time-dependent problems with long prediction horizons, where the uncertainty accumulates through the prediction process. Our results show that PNO is preferable for most probabilistic prediction tasks, especially with regard to calibration measures such as the probabilistic coverage of the true value. Apart from simulation-based PDE datasets, we evaluate the PNO on a real-world data task, concerning the prediction of surface temperature across Europe. 
PNO requires minimal adjustment to existing architectures and leads to well-calibrated predictive distributions and adequate uncertainty representations even for long dynamical trajectories. To summarize, we make the following key contributions:
\begin{itemize}
\item We introduce the \emph{probabilistic neural operator (PNO)} as an extension of the neural operator to the probabilistic setting based on proper scoring rules.
\item We prove that the \emph{energy score} is a strictly proper scoring rule in separable Hilbert spaces and, therefore, a suitable loss function for the PNO.
\item In extensive experiments including 2- and 3-dimensional PDEs, real-world weather data, and different model architectures we show that the PNO setup improves the calibration error while maintaining the accuracy of competing approaches. 
% This is shown on multiple neural operator architecture, such as FNO \citep{li_fourier_2021}, UNO \citep{rahman2023uno}, and SFNO \citep{bonev_spherical_2023}.
\end{itemize}

\subsection{Related work}
In order to assess uncertainties in machine learning models, a variety of approaches are available; for a detailed overview compare \citet{abdar_review_2021}. The most common way to perform uncertainty quantification is under the \textbf{Bayesian paradigm}. By utilizing Monte Carlo dropout during the inference phase of a neural network, \cite{pmlr-v48-gal16} show that the corresponding predictive distribution is a Bayesian approximation of a deep Gaussian process. While the approach requires no additional training, the approximation does not hold for all architectures and no uncertainty information is used in the training process. Another method that has been popularized in deep learning recently is the Laplace approximation \citep{NEURIPS2021_a7c95857}. It builds on the idea that, in a Bayesian setting, the posterior distribution over the network weights can be approximated by a Gaussian distribution, centered around a local maximum. The advantage is that this local maximum is available by regular training, while the covariance matrix of the Gaussian can be recovered by efficient approximations of the Hessian of the loss function \citep{botev_practical_2017}. 

A different class of approaches to uncertainty quantification is given by \textbf{ensemble methods}, where the main idea is to obtain an ensemble of predictions that differ slightly from each other and act as an empirical predictive distribution. A common way to utilize this for neural networks is to use so-called deep ensembles \citep{NIPS2017_9ef2ed4b}, where the ensemble is created by training several networks separately, with each optimization leading to a different local minimum. Although this has shown to perform well in practice, it is hindered by the computational demands of training multiple neural networks. Another method to obtain ensembles is by perturbing the input data before the forward pass through the model, which leads to an ensemble-based distribution in the output. This approach is used mainly in dynamical systems and weather forecasting \citep{bi_accurate_2023, bulte_uncertainty_2024}. However, this requires additional fine-tuning of the input perturbations and can lead to instabilities in the model predictions.

Finally, another way of uncertainty quantification is provided by \textbf{scoring rule}-based approaches. A scoring rule \citep{Gneiting.2007} assigns a numerical score to a distribution-observation pair and can be minimized as a loss function, in order to fit a predictive distribution. Scoring rule approaches have been combined with nonparametric distributional regression \citep{walz_easy_2024}, or neural networks \citep{rasp_neural_2018}, including multivariate cases \citep{JMLR:v25:23-0038, chen_generative_2024}. However, to the best of our knowledge, scoring rule approaches have not yet been transferred to the infinite-dimensional setting.

Since neural operators map to function spaces, the previously mentioned approaches need to be adjusted for \textbf{uncertainty quantification in infinite-dimensional spaces}. \citet{weber2024uncertainty} and \citet{magnani_linearization_2024} extend the Laplace approximation to Fourier neural operators, by performing the approximation in the last layers of the Fourier domain. While this approach is efficient to implement and requires no additional training, the assumption of a Gaussian can be restrictive, especially when it comes to calibration. Furthermore, the method is only applied post-hoc and no uncertainty information is used in the training process. \citet{guo_ib-uq_2023} propose an uncertainty quantification method that utilizes an information bottleneck and a Gaussian distribution to recover mean and standard deviations in the output space. However, this approach only leads to an estimate of the mean and standard deviation and does not allow for other measures of uncertainty. \citet{ma_calibrated_2024} propose a quantile neural operator that is based on conformal prediction and provides a risk-controlling prediction set that includes the true solution with a user-specified probability. While this approach leads to a risk control for the error of the neural operator, it does not lead to a predictive distribution that can be used for assessing the full uncertainty in the prediction. In a different line of work, \citet{ma_calibrated_2024} utilize the deep ensemble and Monte Carlo dropout methods as baselines, as they can be extended to the infinite-dimensional setting in a straightforward manner, leading to a functional ensemble prediction. However, the usability of the deep ensemble approach is limited by the computational overhead of training multiple models independently, especially if a large number of ensembles are required.

To the best of our knowledge, there are no other methods than those mentioned above that are specifically designed to combine uncertainty quantification with neural operators. Our work overcomes the previously discussed limitations, as it provides a predictive distribution that is directly incorporated into the training process by combining neural operators with scoring rule training.

\subsection{Outline}
This article is structured as follows: Section~\ref{sec:neural_operators} provides preliminaries regarding the problem setting and neural operators, while Section~\ref{sec:pno} introduces neural network training via scoring rule minimization in separable Hilbert spaces. Section~\ref{sec:experiments} is dedicated to the numerical experiments for several benchmark tasks, as well as a real-world application. The article finishes with the conclusion in Section \ref{sec:discussion}.


\section{Operator learning}
\label{sec:neural_operators}
Let $\mathcal{A} = \mathcal{A}(\mathcal{D}, \Ree^{d_a}) $ and $\mathcal{U} = \mathcal{U}(\mathcal{D}; \Ree^{d_u})$ denote separable Banach spaces of functions over a bounded domain $\mathcal{D} \subset \mathbb{R}^d$ with values in $\Ree^{d_a}$ and $\Ree^{d_u}$, respectively. In this work, we consider parametric partial differential equations of the form:
\begin{alignat}{2}
    \mathcal{L}_a[u](x) &= f(x),   \qquad & x &\in \mathcal{D},  \\
    u(x) &= u_b(x),  \qquad & x &\in \partial D, \nonumber
\end{alignat}
for some $a \in \mathcal{A}, u \in \mathcal{U}, f \in \mathcal{U}$ and boundary condition $u_b \in \mathcal{U}$. 
We assume that the solution $u\in \mathcal{U}$ exists and is bounded for every $u_b$ and that $\mathcal{L}_a \in\mathcal{L}(\mathcal{U}, \mathcal{U})$ is a mapping from the parameter space $\mathcal
A$ to the (possibly unbounded) space of linear operators mapping $\mathcal{U}$ to its dual. This gives rise to the nonlinear solution operator $G^\dagger: (a,u_b) \mapsto u$ mapping the parameters to the corresponding solution. Note that this includes initial condition as well as time-dependent problems, as $\mathcal{D}$ can be chosen as a spatio-temporal domain. We typically assume that we have observations $\{ (a_j, u_j) \}_{j=1}^N$, where $a_j \sim \mu$ is generated from some probability measure $\mu$ supported on $\mathcal{A}$ and $u_j = \mathcal{G}^\dagger(a_j)$.

\subsection{Neural operators}
The aim of neural operators \citep{JMLR:v24:21-1524} is to generalize deep neural networks to the infinite-dimensional operator setting in order to learn an approximation of $\mathcal{G}^\dagger$ via a parametric map
\begin{equation}
    \mathcal{G}_\theta : \mathcal{A} \to \mathcal{U}, \quad \theta \in \Theta,
\end{equation}
with parameters from a finite-dimensional space $\Theta \subseteq \mathbb{R}^p$. By minimizing a suitable loss function one wants to choose $\theta^* \in \Theta$ such that $\mathcal{G}_{\theta^*} \approx \mathcal{G}^\dagger$. Following the framework of \cite{li_neural_2020}, neural operators are constructed using a sequence of layers consisting of learnable integral operators combined with pointwise non-linearities. For an input function $v_i \in \mathcal{U}(\mathcal{D};\mathbb{R}^{d_v})$ at the $i^{th}$ layer, the map $G_{\phi} : \mathcal{U}(\mathcal{D};\mathbb{R}^{d_v}) \to \mathcal{U}(\mathcal{D};\mathbb{R}^{d_v}),  v_i \mapsto v_{i+1}$ is computed as
\[
G_{\phi} v_i(x) \coloneq \sigma \Big( \left(\mathcal{K}(a; \phi)v_i \right)(x) + W_{i, \phi} v_i(x)  \Big), \quad \forall x \in \mathcal{D}.
\]
Here, $W_{i, \phi}: \mathbb{R}^{d_v} \to \mathbb{R}^{d_v}$ is a linear transformation, $\sigma: \mathbb{R} \to \mathbb{R}$ denotes a pointwise nonlinearity, and $\mathcal{K}:\mathcal{A} \times \Theta_\mathcal{K} \to \mathcal{L}\left(\mathcal{U}(\mathcal{D}; \mathbb{R}^{d_v}), \mathcal{U}(\mathcal{D};\mathbb{R}^{d_v}) \right)$ is a map to the bounded linear operators on $\mathcal{U}(\mathcal{D};\mathbb{R}^{d_v})$, which is parameterized by $\phi \in \Theta_\mathcal{K}\subseteq \mathbb{R}^p$. Typically, $\mathcal{K}(a;\phi)$ is chosen as a kernel integral transformation that is parameterized by a neural network. 

More specifically, in this article we mainly focus on the Fourier neural operator (FNO) \citep{li_fourier_2021}, which specifies the integral kernel as a convolution operator defined in Fourier space as
\[
G_{ \phi} v_i(x) = \sigma \Big( \mathcal{F}^{-1} (R_{i,\phi} \cdot \mathcal{F}(v_i))(x) + W_{i, \phi} v_i(x) \Big), \quad \forall x \in \mathcal{D},
\]
where $\mathcal{F}$ is the Fourier transform of a function $f:\mathcal{D} \to \mathbb{R}^{d_v}$, $\mathcal{F}^{-1}$ its inverse, and $R_\phi= \mathcal{F}(\kappa_\phi)$, where $\kappa_\phi: \mathcal{D} \to \mathbb{R}^{d_v}$ is a periodic function parameterized by neural network parameters $\phi \in \Theta_\mathcal{K}$. This means that in each layer $i$, $R_{i,\phi}$ is directly parameterized in the Fourier domain by a neural network with parameters $\phi \in \Theta_\mathcal{K}$. Typically, several of these layers are combined together with additional pointwise lifting and projection operators \citep{li_fourier_2021}. Several other variants and extensions of Fourier neural operators have been proposed, such as U-shaped neural operators \citep{rahman2023uno}, spherical Fourier neural operators \citep{bonev_spherical_2023} or Wavelet neural operators \citep{tripura_wavelet_2023}.



\section{A probabilistic neural operator}
\label{sec:pno}
In the neural operator setting introduced above, the data pairs can be corrupted by noise and in time-dependent PDEs long prediction horizons might lead to unstable predictions. Moving to a probabilistic setting, instead of mapping directly to the solution, we require the neural operator to output a probability distribution over the function space $\mathcal{U}$. Assume that we have observational data $\{ (a_j, u_j) \}_{j=1}^N$ with prior distribution $a_j \sim P_\mathcal{A}$ and conditional distribution $u_j \sim P_\mathcal{U}^{*}(\, \cdot \mid  a_j\,)$ and let $\mathcal{P}_\mathcal{U}$ denote a set of probability measures over $\mathcal{U}$. We consider a probabilistic neural operator to be a map
\[
\mathcal{G}_\theta: \mathcal{A} \times \mathcal{Z} \to \mathcal{U}, \quad \theta \in \Theta,
\]
that aims to learn a distribution $P_\theta(\, \cdot \mid  a_j\,) \coloneq \mathcal{G}_\theta(a_j, \cdot) \in \mathcal{P}_\mathcal{U}$ parameterized by $\theta \in \Theta$ that fulfills $P_\theta(\, \cdot \mid  a_j\,) \approx P_\mathcal{U}^*(\, \cdot \mid  a_j\,)$. Samples from the conditional distribution are generated via $\mathcal{G}_\theta(a, z)$, where $z \sim \mu_\mathcal{Z}$ and $\mu_\mathcal{Z}$ is a probability distribution over some space $\mathcal{Z}$, usually chosen as a simple tractable distribution such as a Gaussian. For a fixed initial condition $a$, we denote the approximate (empirical) conditional distribution parameterized by the neural network as $P_\theta^M \coloneq \{\mathcal{G}_\theta(a, z_m)\}_{m=1}^M$ with $z_m \sim \mu_\mathcal{Z}, \, m = 1, ..., M$, where we drop the dependence on the initial condition $a$ from the notation. In this article, we restrict ourselves to data from separable Hilbert spaces, which include most solution spaces of PDEs, such as the Sobolev spaces $H^k, k \in \mathbb{N}$.

\subsection{Proper scoring rules for neural operators}
Our methodology builds around the theory of proper scoring rules and its extensions to separable Hilbert spaces \citep{steinwart_strictly_2021, ziegel_characteristic_2024}. Let $(\mathcal{X}, \mathcal{A})$ be a measurable space and let $\mathcal{M}_1(\mathcal{X})$ denote the class of all probability measures on $\mathcal{X}$.
For $\mathcal{P} \subseteq \mathcal{M}_1(\mathcal{X})$, a \textit{scoring rule} is a function $S : \mathcal{P} \times \mathcal{X} \to [-\infty, \infty]$ such that the integral $\int S(P,x) dQ(x)$ exists for all $P,Q \in \mathcal{P}$. Define the so-called \textit{expected score} as $S(Q,P) \coloneq \int S(Q, x) \, dP(x) = \e_{X \sim P} [S(Q, X)].$ Then $S$ is called \textit{proper} relative to the class $\mathcal{P}$ if
\begin{equation}
    \label{eq:proper_sr}
    S(P,P) \leq S(Q,P), \quad \mathrm{for \ all\ } P,Q \in \mathcal{P},
\end{equation}
and it is called \textit{strictly proper} if equality in (\ref{eq:proper_sr}) implies $P=Q$ \citep{Gneiting.2007}. In other words, for a strictly proper scoring rule, $P$ and $Q$ coincide if and only if $Q$ is the unique minimizer of the expected score. This allows for efficient and simple training of neural networks via strictly proper scoring rules, as shown later. 
% In addition, strictly proper scoring rules are preferable as a minimization objective due to the unique minimum.

% Proper scoring rules are closely connected to characteristic kernels, in fact, \citet{steinwart_strictly_2021} showed that the so-called \emph{kernel score} is strictly proper if and only if the underlying kernel is characteristic.
% \begin{definition}[Kernel score]
% The kernel score $S_k$ associated with a measurable kernel $k$ on $\mathcal{X}$ is the scoring rule $S_k : \mathcal{M}_1^k \times \mathcal{X} \to \mathbb{R}$ defined by
% \[
% S_k(P,x) = \frac{1}{2} \e_P \bracks*{k(X, X')} - \e_P\bracks*{k(X,x)} + \frac{1}{2} k(x,x),
% \]
% where $x \in \mathcal{X}$ and $X,X' \overset{i.i.d}{\sim} P \in \mathcal{M}_1^k$.
% \end{definition}
% Here, we follow the definition by \citet{ziegel_characteristic_2024}, which includes the additional last term, ensuring that the kernel score $S_k$ is non-negative. By \citet[][Theorem 1.1]{steinwart_strictly_2021} $S_k$ is a proper scoring rule with respect to $\mathcal{M}_1^k(X)$ and a strictly proper scoring rule if $k$ is characteristic. Furthermore, the kernel score is directly related to the maximum mean discrepancy \citep{gretton_kernel_2012}. The task of finding a strictly proper scoring rule in general spaces is therefore equivalent to finding a corresponding characteristic kernel. While this is a non-trivial task, recent results have provided a variety of characteristic kernels on non-standard input spaces. More specifically, \cite{ziegel_characteristic_2024} show that radial kernels, including the Gaussian and Laplace kernel, are characteristic for separable Hilbert spaces and therefore lead to strictly proper scoring rules $S_k$ relative to the set $\mathcal{M}_1^k(\mathcal{H})$.

In this article, we focus on the so-called \emph{energy score} \citep{Gneiting.2007}, which is a commonly used scoring rule for the multivariate finite-dimensional setting that has been successfully used in combination with neural networks \citep{JMLR:v25:23-0038, chen_generative_2024}. In the following theorem we prove that the energy score can be extended to a strictly proper scoring rule over separable Hilbert spaces.


\begin{theorem}[Energy score]
Let $\mathcal{H}$ denote a separable Hilbert space and $x \in \mathcal{H}$. The energy score $\mathrm{ES}: \mathcal{M}_1^E(\mathcal{H}) \times \mathcal{H} \to \mathbb{R}$ defined as
\begin{equation}
\label{eq:energy_score}
\mathrm{ES}(P, x) \coloneq \e_P \bracks*{\norm*{X - x}_\mathcal{H}} - \frac{1}{2}\e_P \bracks*{\norm*{X - X'}_\mathcal{H}},
\end{equation}
with $X, X' \overset{i.i.d}{\sim} P \in \mathcal{M}_1^E(\mathcal{H})\coloneq \braces*{P \in \mathcal{M}_1(\mathcal{H}) \mid \int_\mathcal{H} ||x||_\mathcal{H} dP(x) < \infty}$ is strictly proper relative to the class $\mathcal{M}_1^E(\mathcal{H})$
%and $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is the symmetric and positive definite \textit{kernel} defined by $k(x,y) = d(x, z_0) + d(y, z_0) - d(x,y)$
.
\end{theorem}

\begin{proof}


As a first step, we show that a separable Hilbert space induces a positive definite kernel that is characteristic.  A measurable and bounded kernel $k$ is called \emph{characteristic} if the kernel embedding defined by $\Phi(P)\coloneq \int k(\cdot, \omega) dP(\omega)$, with $P \in \mathcal{M}_1^k(\mathcal{H})\coloneq \braces*{P \in \mathcal{M}_1(\mathcal{H}) \mid \int_\mathcal{H} \sqrt{k(x,x)} dP(x) < \infty}$, is injective \citep{steinwart_strictly_2021}. 

Let $(\mathcal{X}, d)$ be a metric space and define $\mathcal{M}_1^d(\mathcal{X}) \coloneq \left\{P \in \mathcal{M}_1(\mathcal{X}) \mid \exists o \in \mathcal{X} \text{ such that }\int d(o,x) \, dP(x) < \infty \right\}$. It is said to be of \emph{negative type} if $d$ is negative definite, i.e. for all $n \in \mathbb{N}$, $x_1, ..., x_n \in \mathcal{X}$ and $\alpha_1, ..., \alpha_n \in \mathbb{R}$ with $\sum_{i=1}^n \alpha_i = 0$ it holds that $\sum_{i,j=1}^n \alpha_i \alpha_j d(x_i, x_j) \leq 0$ \citep[compare][Definition 3.1.1]{berg_harmonic_1984}. Furthermore, let $P_1, P_2 \in \mathcal{M}_1^d(\mathcal{X})$. Then, following \cite{lyons_distance_2013}, if $\mathcal{X}$ has negative type, it holds that
\begin{equation}   
\label{eq.negative_type}
\int d(x_1, x_2) \, d(P_1 - P_2)^2(x_1, x_2) \leq 0.
\end{equation}
Furthermore, the space $(\mathcal{X},d)$ is said to be of \emph{strong negative type} if it is of negative type and equality in (\ref{eq.negative_type}) holds if and only if $P_1 = P_2$, which is the case for separable Hilbert spaces \citep[][Theorem 3.25]{lyons_distance_2013}. Furthermore, a metric space $(\mathcal{X},d)$ of negative type induces a positive definite kernel $k: \mathcal{X}\times \mathcal{X} \to \mathbb{R}$ given as $k(x,y) = d(x, z_0) + d(y, z_0) - d(x,y)$ for some fixed $z_0 \in \mathcal{X}$. Finally, \citet[][Proposition 29]{sejdinovic_equivalence_2013} state that for a metric space of strong negative type the corresponding induced kernel $k$ is characteristic.

Next, we cite \citet[][Theorem 1.1]{steinwart_strictly_2021}, who prove that the \emph{kernel score} $S_k: \mathcal{M}_1^k \times \mathcal{X} \to \mathbb{R}$, defined as 
$
S_k(P,x) \coloneq \frac{1}{2} \e_P \bracks*{k(X, X')} - \e_P\bracks*{k(X,x)} + \frac{1}{2} k(x,x),
$
where $x \in \mathcal{X}$ and $X,X' \overset{i.i.d}{\sim} P \in \mathcal{M}_1^k$, is a strictly proper scoring rule if $k$ is a characteristic kernel. 
Therefore, it only remains to show that the energy score corresponds to the kernel score $S_{k_d}$ given by the induced kernel $k_d \coloneq d(x, z_0) + d(y, z_0) - d(x,y)$ in the Hilbert space $\mathcal{H}$ with metric $d(x,x') \coloneq \norm*{x-x'}_\mathcal{H}$ and for some fixed $z_0 \in \mathcal{H}$. We obtain
\[
\begin{split}
    S_{k_d}(P,x) & = - \e_P \bracks*{k_d(X,x)} + \frac{1}{2} \e_P \bracks*{ k_d(X, X')} + \frac{1}{2} k_d(x,x) \\
    & = - \e_P \bracks*{ d(X,z_0) + d(x, z_0) - d(X,x)} + \frac{1}{2}\e_P \bracks*{d(X,z_0) + d(X', z_0) - d(X,X')}\\
    & +  \frac{1}{2} \parens*{d(x,z_0) + d(x, z_0) - d(x,x)}\\
    & = \e_P \bracks*{d(X,x)} - \frac{1}{2} \e_P \bracks*{d(X,X')} - \frac{1}{2} \e_P[d(X,z_0)] + \frac{1}{2} \e_P[d(X', z_0)] - \e_P[d(x, z_0)] + d(x, z_0) \\
    & = \e_P[d(X,x)] - \frac{1}{2} \e_P[d(X,X')] = \mathrm{ES}(P,x).
\end{split}
\]
Since $\mathcal{H}$ is of strong negative type, $k_d$ is characteristic and, thus, by \cite{steinwart_strictly_2021}, the energy score is strictly proper relative to the class $\mathcal{M}_1^{k_d}(\mathcal{H})$.
\end{proof}


\subsection{Training neural operators via the energy score}
\label{ssec:training}
After introducing the underlying concepts and showing that the energy score is strictly proper in separable Hilbert spaces, we now introduce the corresponding training regime for neural networks. This follows the general framework of \citet{JMLR:v25:23-0038}, adapted to operator learning. Given a strictly proper scoring rule, the minimization objective is given as
\begin{equation}
\argmin_\theta \e_{ a \sim P_{\mathcal{A}}} \e_{ u \sim P_\mathcal{U}^*(\, \cdot \mid  a\,)} S(P_\theta(\, \cdot \mid  a\, ),  u)
\end{equation}
and leads to $P_\theta^*(\, \cdot \mid  a\, ) = P_\mathcal{U}^*(\, \cdot \mid  a\,)$ almost everywhere \citep{JMLR:v25:23-0038}. In the finite data setting, the objective can be approximated with a Monte-Carlo estimator. While closed-form expressions of $S$ are not always available, the energy score has a representation that admits an unbiased estimator, which requires the output from our neural network to consist of multiple samples of the predictive distribution, i.e., $M > 1$.\footnote{Note that this setting still includes the classical neural operator as the special case, where the predictive distribution is a Dirac measure. The objective function then becomes the $\| \cdot \|_\mathcal{H}$ loss.} The minimization objective for the neural network with the energy score is then given by
\begin{equation}
    \label{eq:final_objective}
    \argmin_\theta \frac{1}{N} \sum_{i=1}^N \left( \frac{1}{M}\sum_{j=1}^M \|u^j_i - u_i \|_\mathcal{H} - \frac{1}{2M(M-1)} \sum_{\substack{j,h = 1 \\ j \neq h}}^M \|u^j_i - u^h_i\|_\mathcal{H}  \right).
\end{equation}

Different methods are available to specify a neural network to output a conditional (empirical) distribution. Here, we focus on two main approaches. One way to obtain samples is to utilize the reparametrization trick \citep{NIPS2015_bc731692}, which learns a pointwise normal distribution in the last layer of the network, by including two projection layers corresponding to the mean and standard deviation of a normal distribution. Although this has a computational advantage as only one forward pass is required, the predictive distribution is fixed to a Gaussian, which might not be expressive enough for every task. A different way to obtain samples is via stochastic dropout \citep{pmlr-v48-gal16}. By adding Bernoulli or Normal random samples to specific weights of the network, a forward pass becomes stochastic and, therefore, leads to a predictive distribution. While this approach has higher computational demands due to the need to perform multiple forward passes, the partial stochasticity of the network leads to a higher expressivity with regard to the conditional distribution \citep{sharma_bayesian_2023}. We add additional dropout in the Fourier domain, setting random frequency parts as zero with the pre-specified dropout probability, making use of the global structure of the Fourier domain. Since both approaches are quite different and come with different strengths and weaknesses, we included them both in the experiments. For the remainder of this article, we will refer to the methods as PNO\textsubscript{D} and PNO\textsubscript{R} for the dropout variant and the parametrization variant, respectively.


%
% Experiments
%
\section{Numerical experiments}
\label{sec:experiments}
This section describes our experimental setup. In Subsection \ref{ssec:baselines} we introduce the baseline uncertainty quantification methods for comparisons, while Subsection \ref{ssec:eval} includes the evaluation metrics. In the remaining subsections we present the different experiments. All experiments are evaluated on a previously unseen test set with predictions aggregated over ten training runs with different random seeds. The different datasets as well as the corresponding grids are normalized before they are used as input to the network. As architectures, for our experiments, we focus on the Fourier Neural Operator (FNO) \citep{li_fourier_2021} and provide additional results for the U-shaped Neural Operator (UNO) \citep{rahman2023uno} architecture in \autoref{app:uno}. For the spherical shallow water equation we employ the spherical FNO \citep{bonev_spherical_2023}. The training process and the corresponding hyperparameters are explained in detail in \autoref{app:hyperparameters}. Although all uncertainty quantification methods use the same underlying architecture in the experiments, the dropout rate has been tuned separately, as it has a significant impact on the shape of the predictive distribution for the different methods. While we keep the dropout rate in the Fourier layers and the standard layers equal during the experiments in \autoref{sec:experiments}, we assess the effect of both changes independently in \autoref{app:fourier-dropout}. In order to evaluate our approach beyond the pure performance, we include an additional runtime analysis of the different methods in \autoref{app:runtime_analysis}. In \autoref{app:pno_comparison} we assess the influence of the number of samples $M$ during training for PNO. Finally, in \autoref{app:model_comparison} we provide a more detailed comparison of PNO\textsubscript{D} and PNO\textsubscript{R}, highlighting differences and providing guidelines on which approach to choose.


\subsection{Baseline models}
\label{ssec:baselines}
As a basic comparison and a sanity check, we use a deterministic neural operator (DET), where uncertainty is not directly accounted for. Here, the predictive distribution is a degenerate Dirac distribution with
\begin{equation}
    P_\theta^M = \{\mathcal{G}_\theta^* (a) \}_{m=1}^M,
\end{equation}
i.e. the predictive are all a constant value independent of $m$. In addition, we compare our method to different uncertainty quantification methods, adapted to the task of operator learning. \citet{pmlr-v48-gal16} show that a neural network with dropout before each layer is mathematically equivalent to a variational approximation of a Gaussian process. This leads to a simple and efficient way of creating a predictive distribution, known as MCDropout (MCD), which has been extended to neural operators by \cite{ma_calibrated_2024}. In our setting, the predictive distribution for a fixed input $a$ is given as \begin{equation}
    P_\theta^M = \{ \mathcal{G}_\theta^*(a, \bm z_m)  \}_{m=1}^M,
\end{equation}
where $\bm z_m$ is the random dropout mask and $\mathcal{G}_\theta^*$ denotes the trained neural operator. As opposed to the PNO, the MCDropout model is trained on the $L^p$ loss and no uncertainty information is used in the training process.

\citet{weber2024uncertainty, magnani_linearization_2024} propose to utilize the Laplace approximation (LA) for FNOs, which is based on building a second-order approximation of the weights around the maximum a posteriori (MAP) estimate $\theta_\text{MAP}$. By assuming a Gaussian weight prior, the weight-space uncertainty of the LA is given by 
\begin{equation}
p(\theta, \mathcal{C}) \approx \mathcal{N}(\theta; \theta_{\mathrm{MAP}}, \Sigma), \quad \Sigma \coloneq - (\nabla_\theta^2 \mathcal{L}(\mathcal{C};\theta)|_{\theta_\mathrm{MAP}})^{-1},
\end{equation}
where $\mathcal{C}=\{ (a_n, u_n) \}_{n=1}^N$. By sampling from this Gaussian we obtain the predictive distribution $P_\theta^M$. We utilize a last-layer Laplace approximation, which leads to an analytically available Gaussian predictive distribution in function space. Since the Laplace approximation is built around the MAP estimate, it also does not utilize uncertainty information during the training process. 

\subsection{Evaluation}
\label{ssec:eval}
To thoroughly assess the performance of the prediction methods, we use a range of evaluation metrics, each focusing on a different aspect of the probabilistic prediction. Denote by $\mathcal{D}$ the spatio-temporal domain of the governing equation, by $P_\theta^M = \{u^m \}_{m=1}^M$ the predictive distribution, and by $u$ the corresponding true observation. Furthermore, let $\Bar{u}_M \coloneq \frac{1}{M} \sum_{m=1}^M u^m$ denote the mean prediction, $\hat{\sigma}^2 \coloneq \frac{1}{M-1} \sum_{m=1}^M\left(u^m - \Bar{u}_M \right)^2$ denote the estimate of the (pointwise) variance and $\hat{q}_\theta^{\alpha}$ denote the empirical (pointwise) quantiles of $P_\theta^M$ at the level $\alpha$. We utilize the following evaluation metrics:
\begin{align}
    \mathcal{L}_{L^2}(P_\theta^M, u) & \coloneq  \left\| \Bar{u}_M - u \right\|_{L^2},\\
    \mathrm{ES}(P_\theta^M, u) &\coloneq \frac{1}{M} \sum_{m=1}^M \norm*{\hat{u}^m - u}_{L^2}  - \frac{1}{2M(M-1)} \sum_{\substack{m,h = 1 \\m \neq h}}^M \|\hat{u}^m - \hat{u}^h\|_{L^2},\\
    \mathrm{CRPS}(P_\theta^M, u) & \coloneq \frac{1}{|\mathcal{D}|}\sum_{(x,t) \in \mathcal{D}} \left(\int_0^1 \mathrm{QS}_\alpha \left( \hat{q}_{\theta}^{\alpha} (x,t), y \right) \, d\alpha \right),\\
    \mathrm{NLL}(P_\theta^M, u) &\coloneq \frac{1}{2 |\mathcal{D}|}\sum_{(x,t) \in \mathcal{D}} \log \left( 2\pi \hat{\sigma}^2(x,t)\right) + \frac{\left(\overline{u}_M(x,t) - u(x,t)\right)^2}{\hat{\sigma}^2(x,t)} ,\\
    \mathcal{C}_\alpha(P_\theta^M, u) &\coloneq \frac{1}{|\mathcal{D}|} \sum_{(x,t) \in \mathcal{D}} \mathbbm{1} \left\{u(x,t) \in [\hat{q}_\theta^{\alpha/2}(x,t) , \hat{q}_\theta^{1-\alpha/2}(x,t)] \right\},\\
    \left|\mathcal{C}_\alpha (P_\theta^M, u)\right| &\coloneq \frac{1}{|\mathcal{D}|} \sum_{(x,t) \in \mathcal{D}} \left| \hat{q}_\theta^{1-\alpha/2}(x,t) - \hat{q}_\theta^{\alpha/2}(x,t) \right|.
\end{align}
The $\mathcal{L}_{L^2}$ loss evaluates the match between the mean prediction $\Bar{u}_M$ and the observation, while the energy score (ES) evaluates the match for the predictive distribution as a whole. The continuous ranked probability score (CRPS) \citep{Gneiting.2007} evaluates the predictive distribution at a pointwise level and can be represented as an integral of the quantile score $QS_\alpha(q_\alpha, y)\coloneq 2(\alpha - \mathbbm{1}\{y < q_\alpha \})(y - q_\alpha)$ over all quantile levels $\alpha \in (0,1)$. The CRPS therefore assesses whether the predicted uncertainty fits the observations at all quantile levels at each gridpoint $(x,t)$, i.e., whether the predictive distribution is well-calibrated. Furthermore, we analyze the negative log-likelihood (NLL) of a predictive pointwise Gaussian distribution $\mathcal{N}(\overline{u}_M(x,t), \hat{\sigma}^2(x,t))$. Although not all methods produce Gaussian output, this criterion is commonly used and describes the fit of the predictive distribution in terms of the first two estimated moments. The NLL is also a strictly proper scoring rule \citep{Gneiting.2007}. Finally, we report the coverage $\mathcal{C}_\alpha$ of the predictive distribution for selected $\alpha$-quantile levels, as well as the width of the corresponding prediction interval $\left| \mathcal{C}_\alpha \right|$. Following the notions of calibration and sharpness \citep{10.1111/j.1467-9868.2007.00587.x}, in an ideal setting, an estimator should obtain $\mathcal{C}_{\alpha} \approx 1 - \alpha$, while simultaneously predicting the interval $\left| \mathcal{C}_\alpha \right|$ as small as possible. To make the baselines comparable, $M=100$ predictive samples are drawn from the respective predictive distributions at test time, and all metrics are averaged over the test dataset. For the deterministic model, the energy score reduces to the $\mathcal{L}_{L^2}$ loss and the CRPS reduces to the pointwise mean absolute error, while the negative log-likelihood and the coverage are not applicable.



\subsection{Darcy flow}
First, we consider a steady-state solution of the two-dimensional Darcy flow over the unit square, which is defined by a second-order linear elliptic equation of the form
\begin{alignat*}{2}
- \nabla (a(x) \nabla u(x)) &= f(x), \qquad  &&x \in (0,1)^2,\\
u(x) &= 0, \qquad  &&x \in \partial (0,1)^2.
\end{alignat*}
Here, $a \in L^\infty ((0,1)^2; \mathbb{R}_+)$ is the diffusion coefficient, $u \in H_0^1((0,1)^2, \mathbb{R})$ is the velocity function, and $f \in L^2((0,1)^2; \mathbb{R})$ is the forcing function. The aim is to learn the solution operator $G^*:L^\infty ((0,1)^2; \mathbb{R}_+) \to  H_0^1((0,1)^2; \mathbb{R})$, $a \mapsto u$. The PDE is usually used for modeling fluid flow in a porous medium. We use data from \citet{NEURIPS2022_0a974713}, with the forcing function fixed as $f(x)=1$, which corresponds to the setting in \citet{li_fourier_2021}. The provided data consists of 10000 samples with a resolution of $128 \times 128$. We train the networks on a spatial resolution of $64 \times 64$ and evaluate on $128 \times 128$. The results are presented in \autoref{tab:darcy_flow_results}. The PNO\textsubscript{D} obtains the best scores across all metrics when using the FNO architecture. Specifically, it is worth noting that it obtains a lower $L^2$ loss, although the deterministic model is directly trained on that metric, indicating that learning the whole predictive distribution can also improve the mean prediction. While both the PNO\textsubscript{D} and the Laplace approach achieve the coverage goal, the prediction interval of the PNO\textsubscript{D} is significantly narrower despite the coverage of PNO\textsubscript{D} being better as well.
% For the UNO architecture, the performance seems to be a bit worse in general, with the PNO\textsubscript{D} still performing best on the distributional metrics.
A visualization of the different predictions can be found in \autoref{app:additional_figures}.

\begin{table}[t]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{FNO}} &  DET  & \makecell{0.0972 \\ ($\pm$ 0.0076)} & \makecell{0.0972 \\ ($\pm$ 0.0076)} & \makecell{0.0783 \\ ($\pm$ 0.0062)} & - & -& -\\

\cline{2-8} &  PNO\textsubscript{D}  & \makecell{\textbf{0.0772} \\ ($\pm$0.0015)} & \makecell{\textbf{0.0569} \\ ($\pm$0.0009)} & \makecell{\textbf{0.0462} \\ ($\pm$0.0007)} & \makecell{\textbf{-1.3444 }\\ ($\pm$0.0355)} & \makecell{\textbf{0.9813} \\ ($\pm$0.0032)} & \makecell{0.3682 \\ ($\pm$0.0185)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.1031 \\ ($\pm$0.0069)} & \makecell{0.0817 \\ ($\pm$0.0056)} & \makecell{0.0707 \\ ($\pm$0.0040)} & \makecell{685.15 \\ ($\pm$913.21)} & \makecell{0.4702 \\ ($\pm$0.0559)} & \makecell{0.1249 \\ ($\pm$0.0191)} \\

\cline{2-8} & MCD & \makecell{0.1002 \\ ($\pm$0.0088)} & \makecell{0.0830 \\ ($\pm$0.0080)} & \makecell{0.0672 \\ ($\pm$0.0066)} & \makecell{1.5973 \\ ($\pm$0.8223)} & \makecell{0.5187 \\ ($\pm$0.0608)} & \makecell{0.1203 \\ ($\pm$0.0031)} \\

\cline{2-8}& LA  & \makecell{0.0985 \\ ($\pm$0.0074)} & \makecell{0.0767 \\ ($\pm$0.0040)} & \makecell{0.0621 \\ ($\pm$0.0033)} & \makecell{-0.8851 \\ ($\pm$0.0531)} & \makecell{\textbf{0.9762} \\ ($\pm$0.0085)} & \makecell{0.5713 \\ ($\pm$0.0076)} \\
% \hline
% \hline
% \multirow{7}{2em}{\textbf{UNO}} &  PNO\textsubscript{D}  & \makecell{0.1894 \\ ($\pm$0.0066)} & \makecell{\textbf{0.1354 }\\ ($\pm$0.0050)} & \makecell{\textbf{0.1097} \\ ($\pm$0.0041)} & \makecell{\textbf{-0.2856} \\ ($\pm$0.0614)} & \makecell{\textbf{0.9069} \\ ($\pm$0.0243)} & \makecell{0.6998 \\ ($\pm$0.0467)} \\


% \cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.2005 \\ ($\pm$0.0060)} & \makecell{0.1473 \\ ($\pm$0.0055)} & \makecell{0.1275 \\ ($\pm$0.0044)} & \makecell{20.018 \\ ($\pm$11.926)} & \makecell{0.5765 \\ ($\pm$0.0698)} & \makecell{0.4218 \\ ($\pm$0.0559)} \\

% \cline{2-8} & MCD & \makecell{\textbf{0.1875} \\ ($\pm$0.0058)} & \makecell{0.1491 \\ ($\pm$0.0076)} & \makecell{0.1233 \\ ($\pm$0.0069)} & \makecell{2.2782 \\ ($\pm$1.4980)} & \makecell{0.5468 \\ ($\pm$0.0600)} & \makecell{0.2822 \\ ($\pm$0.0259)} \\

% \cline{2-8}& LA & \makecell{0.1954 \\ ($\pm$0.0074)} & \makecell{0.1530 \\ ($\pm$0.0052)} & \makecell{0.1229 \\ ($\pm$0.0040)} & \makecell{0.9996 \\ ($\pm$0.2835)} & \makecell{0.7477 \\ ($\pm$0.0411)} & \makecell{0.4374 \\ ($\pm$0.0643)} \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the Darcy Flow equation using the FNO architecture with the different baselines. The best model is highlighted in bold for all metrics, except for the interval width, as it is only comparable for models with the same coverage. The standard deviation across the different experiment runs is given in the brackets.}
\label{tab:darcy_flow_results}
\end{table}


\subsection{Kuramoto-Sivashinsky equation}
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{FNO}} & DET & \makecell{0.9028 \\ ($\pm$ 0.0086)} & \makecell{0.9028 \\ ($\pm$ 0.0086)} & \makecell{0.7100 \\ ($\pm$ 0.0106)} & - & - & - \\

\cline{2-8}&  PNO\textsubscript{D}  & \makecell{0.8793 \\ ($\pm$0.0072)} & \makecell{0.6195 \\ ($\pm$0.0049)} & \makecell{0.5496 \\ ($\pm$0.0097)} & \makecell{3.0389 \\ ($\pm$0.5872)} & \makecell{0.7640 \\ ($\pm$0.0191)} & \makecell{3.0852 \\ ($\pm$0.0422)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.8640 \\ ($\pm$0.0038)} & \makecell{\textbf{0.6081 }\\ ($\pm$0.0027)} & \makecell{\textbf{0.4743 }\\ ($\pm$0.0037)} & \makecell{\textbf{1.2268} \\ ($\pm$0.0083)} & \makecell{\textbf{0.9401} \\ ($\pm$0.0030)} & \makecell{3.2594 \\ ($\pm$0.0294)} \\

\cline{2-8} & MCD & \makecell{\textbf{0.8635} \\ ($\pm$0.0048)} & \makecell{0.7541 \\ ($\pm$0.0049)} & \makecell{0.5974 \\ ($\pm$0.0058)} & \makecell{107.10 \\ ($\pm$22.941)} & \makecell{0.3600 \\ ($\pm$0.0082)} & \makecell{0.6046 \\ ($\pm$0.0164)} \\


\cline{2-8}& LA  & \makecell{0.8772 \\ ($\pm$0.0055)} & \makecell{0.7332 \\ ($\pm$0.0103)} & \makecell{0.6081 \\ ($\pm$0.0077)} & \makecell{43.9302 \\ ($\pm$10.8764)} & \makecell{0.3955 \\ ($\pm$0.0189)} & \makecell{0.8175 \\ ($\pm$0.0669)} \\

% \hline
% \hline
% \multirow{7}{2em}{\textbf{UNO}} &  PNO\textsubscript{D} & \makecell{\textbf{0.9315} \\ ($\pm$0.0046)} & \makecell{\textbf{0.6560} \\ ($\pm$0.0032)} & \makecell{0.6014 \\ ($\pm$0.0088)} & \makecell{\textbf{2.0875} \\ ($\pm$0.2564)} & \makecell{0.7828 \\ ($\pm$0.0315)} & \makecell{3.3892 \\ ($\pm$0.1214)} \\

% \cline{2-8} &  PNO\textsubscript{R} & \makecell{0.9371 \\ ($\pm$0.0022)} & \makecell{0.6595 \\ ($\pm$0.0015)} & \makecell{\textbf{0.5326} \\ ($\pm$0.0023)} & \makecell{5.4528 \\ ($\pm$6.2975)} & \makecell{\textbf{0.9110 }\\ ($\pm$0.0150)} & \makecell{3.4039 \\ ($\pm$0.0442)} \\

% \cline{2-8} & MCD & \makecell{0.9444 \\ ($\pm$0.0014)} & \makecell{0.8500 \\ ($\pm$0.0034)} & \makecell{0.7163 \\ ($\pm$0.0026)} & \makecell{449.08 \\ ($\pm$42.839)} & \makecell{0.1965 \\ ($\pm$0.0050)} & \makecell{0.4052 \\ ($\pm$0.0109)} \\

% \cline{2-8}& LA & \makecell{0.9494 \\ ($\pm$0.0250)} & \makecell{0.8544 \\ ($\pm$0.0265)} & \makecell{0.6928 \\ ($\pm$0.0249)} & \makecell{54.338 \\ ($\pm$37.019)} & \makecell{0.2560 \\ ($\pm$0.0473)} & \makecell{0.5770 \\ ($\pm$0.1251)} \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the Kuramoto-Sivashinsky equation. The best model is highlighted in bold and the standard deviation is given in the brackets.}
\label{tab:ks_results}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figs/ks_predictions.pdf}
    \caption{The figure shows the ground truth, mean prediction, absolute error, standard deviation, and coverage for the different methods on a sample of the Kuramoto-Sivashinsky equation.}
    \label{fig:ks_predictions}
\end{figure}

The Kuramoto-Sivashinsky (KS-)equation is a fourth-order parabolic PDE with the following form in one spatial dimension:
\begin{alignat*}{2}  
\partial_t u(x,t)+ u \partial_x u(x,t) + \partial_x^2 u(x,t) + \partial_x^4 u(x,t) &= 0, \qquad &&x \in \mathcal{D}, t \in (0,T]\\
u(x,0) &= u_0(x), \qquad &&x \in \mathcal{D}
\end{alignat*}

where $\mathcal{D} \subseteq \mathbb{R}$, $u \in C((0,T]; H_{\mathrm{per}}^4(\mathcal{D}; \mathbb{R}))$, and $u_0 \in L_{\mathrm{per}}^2(\mathcal{D};\mathbb{R})$\footnote{$H^k_{\text{per}}(\mathcal{D}; \mathbb{R}), L^2_{\text{per}}(\mathcal{D}; \mathbb{R})$ denote the periodic Sobolev and $L^2$ spaces, respectively.}. Furthermore, $u_0$ is $L$-periodic, where $L$ is the size of the specific domain. The PDE exhibits strong temporal chaotic behavior and is used for modeling instabilities in a laminar flame front, reaction-diffusion systems, or other chemical reactions. We simulate the KS-equation from random uniform noise $\mathcal{U}(-1,1)$ on a periodic domain $\mathcal{D} = [0,100]$ using the py-pde package \citep{zwicker_py-pde_2020}. We generate 10000 samples with a resolution of $256 \times 300$ and train a 2D (spatio-temporal) FNO, which takes the first twenty timesteps as input and solves the equation for the next twenty timesteps with step size $\Delta t = 2$. The results for the KS-equation can be found in \autoref{tab:ks_results}. Here, the MCDropout approach leads to the best $L^2$ loss, while the PNO\textsubscript{R} leads to the best distributional metrics. Although no method achieves a coverage of $95\%$, the PNO\textsubscript{R} is closest.
% For the UNO architecture, the results differ, with the PNO\textsubscript{D} obtaining the best score on most of the metrics. Again the PNO\textsubscript{R} leads to the best coverage. 
A corresponding visualization can be found in \autoref{fig:ks_predictions}.





\subsection{Spherical shallow water equation}
\citet{bonev_spherical_2023} have extended the FNO to convolutions on the sphere using spherical harmonic basis functions, leading to the so-called Spherical Fourier Neural Operator (SFNO). Following their experiments, we test our method on the shallow water equation on the rotating sphere. The shallow water equations (SWE) are a set of hyperbolic partial differential equations, derived from the general Navier-Stokes equations and present a framework for modeling free-surface flow problems. In the spherical setting, these are given as
\begin{alignat*}{2}
    \partial_t h(x,t) + \nabla h(x,t) \bm u &= 0, \qquad &&  x \in S^2, t \in (0,T] \\
    \partial_t h(x,t) \bm u + \nabla \left( (\bm u \odot \bm u) h(x,t) \right) &= S, \qquad && x \in S^2, t \in (0,T],
\end{alignat*}
with the vector of directional velocities $\bm u = (u,v)^\top$. Here, $h \in C((0,T]; H_0^1(S^2; \mathbb{R}))$ is the geopotential of the water and $S$ contains flux terms, such as the Coriolis force. Similarly to \citet{bonev_spherical_2023}, we simulate 5000 training samples with resolution $128 \times 256$ with PDE parameters that model the Earth and an additional 500 evaluation samples. Gaussian random fields are used as initial conditions with a burn-in period of three hours for the numerical solver. The model is trained and evaluated by using the corresponding losses in a weighted version, adjusted to the latitude weighting of the sphere. Furthermore, \citet{bonev_spherical_2023} propose a fine-tuning step, where the model is trained with an additional autoregressive step, which leads to more stable results for long rollout times, which we additionally include in our experiments. For evaluation, we report both training versions with one-hour and ten-hour prediction horizons each. The results for the two-step autoregressive training can be found in \autoref{tab:sswe_results_two_steps} and the results for the single-step training in \autoref{tab:sswe_results_one_steps} in \autoref{app:additional_results_sswe}. For the one hour prediction, the PNO\textsubscript{D} obtains the best metrics across all models, and especially a good coverage. For the ten-hour prediction task, the deterministic model performs best, although the PNO\textsubscript{R} and PNO\textsubscript{D} perform quite similar. It is notable that the PNO\textsubscript{R} leads to very large values for the negative log-likelihood, due to the predicted variance collapsing zero, which could indicate an optimization failure. However, the method is still competitive in the remaining metrics. This might be due to overconfident pointwise predictions, which lead to a small variance and, therefore, to a large NLL. This behavior is also visible in the visualization in \autoref{fig:sswe_predictions}. The MCD approach does not lead to stable predictions for the ten-hour prediction task, as is reflected in all metrics. However, for the single-step training, the performance is much better (compare \autoref{tab:sswe_results_one_steps}), indicating that the method cannot be combined with the autoregressive training approach.

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{$\Delta t$} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{1h}} & DET & \makecell{0.3150 \\ ($\pm$ 0.0129)} & \makecell{0.3150 \\ ($\pm$ 0.0129)} & \makecell{0.1244 \\ ($\pm$ 0.0046)} & - & - & - \\

\cline{2-8} &  PNO\textsubscript{D}  & \makecell{\textbf{0.2970} \\ ($\pm$0.0232)} & \makecell{\textbf{0.2216} \\ ($\pm$0.0142)} & \makecell{\textbf{0.0903} \\ ($\pm$0.0056)} & \makecell{\textbf{-0.4561} \\ ($\pm$0.0447)} & \makecell{\textbf{0.9828} \\ ($\pm$0.0031)} & \makecell{0.8959 \\ ($\pm$0.0440)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.3871 \\ ($\pm$0.0126)} & \makecell{0.2740 \\ ($\pm$0.0088)} & \makecell{0.1561 \\ ($\pm$0.0050)} & \makecell{2.3$\times 10^{10}$ \\ ($\pm$1.6$\times 10^{9}$)} & \makecell{0.0274 \\ ($\pm$0.0206)} & \makecell{0.0844 \\ ($\pm$0.0551)} \\

\cline{2-8} & MCD & \makecell{0.3022 \\ ($\pm$0.0204)} & \makecell{0.2368 \\ ($\pm$0.0196)} & \makecell{0.0944 \\ ($\pm$0.0066)} & \makecell{0.8878 \\ ($\pm$0.2608)} & \makecell{0.6023 \\ ($\pm$0.0250)} & \makecell{0.2550 \\ ($\pm$0.0072)} \\

\cline{2-8}& LA  & \makecell{0.4262 \\ ($\pm$0.0522)} & \makecell{0.3159 \\ ($\pm$0.0299)} & \makecell{0.1400 \\ ($\pm$0.0120)} & \makecell{0.2374 \\ ($\pm$0.2229)} & \makecell{0.9096 \\ ($\pm$0.0762)} & \makecell{1.0866 \\ ($\pm$0.3472)} \\

\hline
\hline
\multirow{9}{2em}{\textbf{10h}}&  DET & \makecell{\textbf{0.9215 }\\ ($\pm$ 0.0678)} & \makecell{\textbf{0.9215 }\\ ($\pm$ 0.0678)} & \makecell{\textbf{0.3933 }\\ ($\pm$ 0.0254)} & - & -& - \\

\cline{2-8}&  PNO\textsubscript{D}  & \makecell{1.2421 \\ ($\pm$0.0650)} & \makecell{0.9915 \\ ($\pm$0.0535)} & \makecell{0.4330 \\ ($\pm$0.0215)} & \makecell{\textbf{5.4385} \\ ($\pm$0.5448)} & \makecell{0.4935 \\ ($\pm$0.0200)} & \makecell{0.9210 \\ ($\pm$0.0793)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{1.1803\\ ($\pm$0.0345)} & \makecell{0.9535 \\ ($\pm$0.0413)} & \makecell{0.4995 \\ ($\pm$0.0130)} & \makecell{2.1$\times 10^{11}$ \\ ($\pm$1.4$\times 10^{10}$)} & \makecell{0.0212 \\ ($\pm$0.0200)} & \makecell{0.0850 \\ ($\pm$0.0546)} \\

\cline{2-8} & MCD & \makecell{9.1$\times 10^{5}$\\ ($\pm$2.7$\times 10^{6}$)} & \makecell{7.5$\times 10^{5}$ \\ ($\pm$2.2$\times 10^{6}$)} & \makecell{2.8$\times 10^{5}$ \\ ($\pm$8.3$\times 10^{5}$)} & \makecell{22.292 \\ ($\pm$19.623)} & \makecell{0.3530 \\ ($\pm$0.1364)} & \makecell{6.7$\times 10^{5}$ \\ ($\pm$2.0$\times 10^{6}$)} \\

\cline{2-8}& LA  & \makecell{1.5405 \\ ($\pm$0.3697)} & \makecell{1.2612 \\ ($\pm$0.3338)} & \makecell{0.5299 \\ ($\pm$0.1336)} & \makecell{6.4958 \\ ($\pm$5.5154)} & \makecell{\textbf{0.5759} \\ ($\pm$0.1480)} & \makecell{1.1925 \\ ($\pm$0.4394)} \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the spherical shallow water equation with two-step autoregressive training. The best model is highlighted in bold and the standard deviation is given in the brackets.}
\label{tab:sswe_results_two_steps}
\end{table}

\begin{figure}[htb]
    \centering    \includegraphics[width=\linewidth]{figs/sswe_predictions_1h.pdf}
    \caption{The figure shows the ground truth, mean prediction, absolute error, standard deviation, and coverage for the different methods on a sample of the spherical shallow water equation. The shown variable is the water geopotential and the prediction horizon is one hour with predictions obtained via two-step autoregressive training.}
    \label{fig:sswe_predictions}
\end{figure}



\subsection{Forecasting of 2m surface temperature}
The experiments so far have focused on systems in which the underlying equations are known and computationally tractable. We now want to demonstrate our method on a more complex large-scale dynamical system with real data. More concretely, we use our approach to create probabilistic medium-range forecasts of the Earth's surface temperature. Recently, neural networks have shown huge success in medium-range weather prediction tasks, with models such as FourCastNet \citep{pathak_fourcastnet_2022}, or PanguWeather \citep{bi_accurate_2023}. However, while some approaches are available that directly learn probabilistic predictions \citep{price_gencast_2024}, most models are either deterministic or require an additional post-processing step \citep{bulte_uncertainty_2024}.

Here, we want to demonstrate the performance of our model in predicting the 2-meter surface temperature across Europe. For computational reasons we restrict the prediction task to only one input variable. We use the ERA5 dataset \citep{hersbach_ERA5_2020}, provided via the benchmark dataset WeatherBench \citep{rasp_weatherbench_2024}. The data has a spatial resolution of $0.25^\circ \times 0.25^\circ$ and a time resolution of $6h$. For computational reasons, we restrict ourselves to a spatial domain over Europe. We use a 3D spatio-temporal neural operator that takes the last 10 timesteps (60 hours) as the input and predicts the next 10 timesteps. Therefore, we directly predict several future timesteps, instead of using an autoregressive model. The results of the temperature prediction task can be found in \autoref{tab:ERA5_results}. All models obtain a fairly similar and consistent $L^2$ loss, whereas PNO\textsubscript{D} exceeds the alternative methods in the remaining metrics. However, none of the methods obtain an optimal coverage. It is noticeable that the PNO\textsubscript{R} leads to a very poor NLL score, while the PNO\textsubscript{D} outperforms the other methods significantly with regard to that metric. \autoref{fig:ERA_spatial_statistics} shows the CRPS and coverage across the spatial domain, aggregated over the test data. The CRPS shows a clear performance distinction between land and sea, which is supported by the findings in \citet{bulte_uncertainty_2024}. While for MCDropout and the PNO\textsubscript{R}, the coverage is clearly different between land and sea, for Laplace and PNO\textsubscript{D} this difference is negligible. The latter is preferable, as the models should provide adequate uncertainty even if the underlying spatial domain makes predictions more difficult. \autoref{fig:ERA5_predictions} shows an additional visualization of the corresponding model predictions.
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/era5_spatial_metrics_24h.pdf}
    \caption{The figure shows the spatial distribution of the CRPS and coverage, aggregated over the test samples for the different methods on the two-meter surface temperature prediction task with a prediction horizon of 24h.}
    \label{fig:ERA_spatial_statistics}
\end{figure}

\begin{table}
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{FNO}} & DET & \makecell{0.0250 \\ ($\pm$ 0.0001)} & \makecell{0.0250 \\ ($\pm$ 0.0001)} & \makecell{0.0267 \\ ($\pm$ 0.0001)} & - & - & - \\

\cline{2-8} & PNO\textsubscript{D} & \makecell{0.0242 \\ ($\pm$0.0000)} & \makecell{\textbf{0.0174} \\ ($\pm$0.0000)} & \makecell{\textbf{0.0187 }\\ ($\pm$0.0001)} & \makecell{\textbf{-1.9587 }\\ ($\pm$0.0111)} & \makecell{\textbf{0.8623} \\ ($\pm$0.0033)} & \makecell{0.0975 \\ ($\pm$0.0012)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.0241 \\ ($\pm$0.0001)} & \makecell{0.0187 \\ ($\pm$0.0001)} & \makecell{0.0204 \\ ($\pm$0.0004)} & \makecell{616.98 \\ ($\pm$1.2$\times 10^{3}$)} & \makecell{0.6536 \\ ($\pm$0.0222)} & \makecell{0.0576 \\ ($\pm$0.0022)} \\

\cline{2-8} & MCD & \makecell{\textbf{0.0240} \\ ($\pm$0.0001)} & \makecell{0.0202 \\ ($\pm$0.0000)} & \makecell{0.0214 \\ ($\pm$0.0001)} & \makecell{2.3623 \\ ($\pm$0.0237)} & \makecell{0.4456 \\ ($\pm$0.0015)} & \makecell{0.0351 \\ ($\pm$0.0001)} \\

\cline{2-8}& LA & \makecell{0.0251 \\ ($\pm$0.0001)} & \makecell{0.0195 \\ ($\pm$0.0006)} & \makecell{0.0210 \\ ($\pm$0.0005)} & \makecell{-0.1502 \\ ($\pm$1.0099)} & \makecell{0.6956 \\ ($\pm$0.1169)} & \makecell{0.0802 \\ ($\pm$0.0344)} \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the ERA5 surface temperature prediction task. The best model is highlighted in bold and the standard deviation is given in the brackets.}
\label{tab:ERA5_results}
\end{table}



\begin{figure}%[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/ERA5_predictions_60h.pdf}
    \caption{The figure shows the ground truth, mean prediction, absolute error, standard deviation, and coverage for the different methods on a sample of ERA5 dataset. The prediction horizon is 60 hours.}
    \label{fig:ERA5_predictions}
\end{figure}





\FloatBarrier
\section{Discussion}
\label{sec:discussion}
% \begin{itemize}
%     \item General discussion
%     \item Compare advantages/disadvantages
%     \item Works with different architectures, dimensions, autoregressive methods, etc.
%     \item Mention theoretical soundness
%     \item Address differences between reparam / dropout
%     \item Address challenges, such as computational time or autoregressive rollouts
%     \item Future outlook, e.g. Extension to other models, for example FourCastNet.
% \end{itemize}

In this work, we introduce PNO, a framework for learning probability distributions over the output function space of neural operators. Our approach combines neural operators with generative modeling utilizing strictly proper scoring rules. We provide a theoretical justification of the approach and analyze how scoring rules can be extended to separable Hilbert spaces. More specifically, we demonstrate how the energy score can be utilized to create a predictive distribution in the output function space of a neural operator. We present and examine different ways of generating a probability distribution using neural operators that are agnostic to the underlying architecture. Our framework enables more accurate uncertainty quantification while maintaining consistent performance in the respective mean prediction task. These advantages are especially noticeable when the underlying system is highly uncertain or chaotic.

We evaluate our approach across several experiments, including stationary PDE solutions, dynamical systems, and weather forecasting. The experiments demonstrate that PNO is transferrable to different network architectures, as well as different domains. Furthermore, PNO performs well in spatio-temporal as well as autoregressive settings. Compared to the baseline methods, PNO shows the best performance across a variety of metrics, especially with regard to the fit between the predictive distribution and the observations. Surprisingly, the mean prediction of the PNO is an improvement against the deterministic model, indicating that modeling the predictive distribution can remove bias in the mean prediction. Finally, the predictions are better calibrated in terms of the coverage of corresponding $1-\alpha$ prediction intervals.

\subsection{Limitations}
The main limitation of the PNO is the increased computational complexity and memory requirements. These are mainly induced by the energy score, as it requires multiple samples to evaluate. In the case of the PNO\textsubscript{D} this also requires multiple forward passes of the underlying model. A runtime analysis shows that the training time per epoch of the PNO is around 1.5 - 3 times higher than for the comparing methods. 
Another limiting factor of PNO is that it inherits the expressivity and (lack of) understanding of the underlying generative model.
While PNO\textsubscript{R} corresponds to a predictive Gaussian in output space, for PNO\textsubscript{D} the exact distribution and its properties are not available. A better understanding of the predictive distribution could allow for convergence or coverage guarantees. Finally, the results show that PNO leads to unstable predictions for autoregressive inference with long rollout times.

\subsection{Future work}
One of the most important tasks for future research revolves around reducing the computational complexity of our method. As a first step, analyzing the capacities of training a deterministic model and fine-tuning the PNO afterwards appears to be a promising direction. In \autoref{app:fine_tuning} we provide first results in that direction, showing that utilizing fine-tuning can significantly reduce computational costs. In a similar manner, the amount of generated samples could be adapted throughout the training process, increasing performance while reducing training time simultaneously. In addition, other parameters of the training regime could be adjusted to further reduce training and inference time. Another promising research direction is the application of alternative suitable scoring rules such as the Gaussian kernel score. Alternative kernels might induce different characteristics in the predictive distribution and the optimal kernel might be dependent on the underlying data scenario. Future work could also revolve around extending PNO to more complex architectures and larger models. Especially in the context of weather forecasting, models such as the adaptive Fourier neural operator \citep{pathak_fourcastnet_2022} have shown great success and a combination with proper scoring rules might provide a practical method for uncertainty quantification for large-scale systems. In addition, implementing PNO into large-scale models for physical applications such as weather forecasting can lead to improvements in the corresponding uncertainty quantification and extreme event identification, ultimately leading to a deeper understanding of the prediction of such surrogate models.



\clearpage
\subsection*{Acknowledgements}
C. BÃ¼lte and G. Kutyniok acknowledge support by the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research.

P. Scholl and G. Kutyniok acknowledges support by the project "Genius Robot" (01IS24083), funded by the Federal Ministry of Education and Research (BMBF), as well as the ONE Munich Strategy Forum (LMU Munich, TU Munich, and the Bavarian Ministery for Science and Art).

G. Kutyniok acknowledges partial support by the Munich Center for Machine Learning (BMBF), as well as the German Research Foundation under Grants DFG-SPP-2298, KU 1446/31-1 and KU 1446/32-1. 
Furthermore, G. Kutyniok is supported by the GAIn project, which is funded by the Bavarian Ministry of Science and the Arts (StMWK Bayern) and the Saxon Ministry for Science, Culture and Tourism (SMWK Sachsen). G. Kutyniok also acknowledges support from
LMUexcellent, funded by the Federal Ministry of Education and Research (BMBF) and the Free State of Bavaria under the Excellence Strategy of the Federal Government and the LÃ¤nder as well as by the Hightech Agenda Bavaria.
\bibliographystyle{tmlr}
\bibliography{references.bib}

\clearpage
\appendix

% \section{Proper scoring rules in separable Hilbert spaces}
% %\label{app:scoring_rules}
% This section aims to provide more detailed insights into proper scoring rules and extensions over separable Hilbert spaces to be able to prove that the energy score is a strictly proper scoring rule in separable Hilbert spaces. The results and notations draw mainly on \cite{ziegel_characteristic_2024,steinwart_strictly_2021}. Let $(\mathcal{X}, \mathcal{A})$ be a measurable space and let $\mathcal{M}_1(\mathcal{X})$ denote the class of all probability measures on $\mathcal{X}$. Let $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ be a symmetric and positive function referred to as \textit{kernel} and define $\mathcal{M}_1^k(\mathcal{X}) \coloneq \braces*{P \in \mathcal{M}_1(X) \mid \int_X \sqrt{k(x,x)} dP(x) < \infty}$. A measurable and bounded kernel is called \emph{characteristic} if the kernel embedding defined by $\Phi(P)\coloneq \int k(\cdot, \omega) dP(\omega)$, with $P \in \mathcal{M}_1^k(\mathcal{H})$ is injective \citep{steinwart_strictly_2021}.


% For $\mathcal{P} \subseteq \mathcal{M}_1(\mathcal{X})$, a \textit{scoring rule} is a function $S : \mathcal{P} \times \mathcal{X} \to [-\infty, \infty]$ such that the integral $\int S(P,x) dQ(x)$ exists for all $P,Q \in \mathcal{P}$. Define the so-called \textit{expected score} as $S(Q,P) \coloneq \int S(Q, x) \, dP(x) = \e_{X \sim P} [S(Q, X)].$ Then $S$ is called \textit{proper} with respect to $\mathcal{P}$ if
% \begin{equation}
%     \label{eq:proper_sr_app}
%     S(P,P) \leq S(Q,P), \quad \mathrm{for \ all\ } P,Q \in \mathcal{P},
% \end{equation}
% and it is called \textit{strictly proper} if equality in (\ref{eq:proper_sr}) implies $P=Q$. For a more detailed overview compare \citet{Gneiting.2007}. Any proper scoring rule $S$ can be associated with a corresponding \textit{score divergence} via
% \[
% d_S(P,Q) = S(Q,P) - S(P,P), \quad P,Q \in \mathcal{P}.
% \]
% The divergence function is nonnegative and strictly positive unless $P=Q$ if $S$ is strictly proper. From the above construction, it is clear that the scoring rule and its corresponding divergence function both obtain their unique minimum at $P=Q$. This highlights the importance of finding proper scoring rules. Proper scoring rules are closely connected to characteristic kernels, in fact \citet{steinwart_strictly_2021} showed that the \emph{kernel score} is strictly proper if and only if the underlying kernel is characteristic.
% \begin{definition}[Kernel score]
% The kernel score $S_k$ associated with a measurable kernel $k$ on $\mathcal{X}$ is the scoring rule $S_k : \mathcal{M}_1^k \times \mathcal{X} \to \mathbb{R}$ defined by
% \[
% S_k(P,x) = \frac{1}{2} \e_P \bracks*{k(X, X')} - \e_P\bracks*{k(X,x)} + \frac{1}{2} k(x,x),
% \]
% where $x \in \mathcal{X}$ and $X,X' \overset{i.i.d}{\sim} P \in \mathcal{M}_1^k$.
% \end{definition}
% Here, we follow the definition by \citet{ziegel_characteristic_2024}, which includes the additional last term, ensuring that the kernel score $S_k$ is non-negative. By \citet[][Theorem 1.1]{steinwart_strictly_2021} $S_k$ is a proper scoring rule with respect to $\mathcal{M}_1^k(X)$ and a strictly proper scoring rule if $k$ is characteristic. The task of finding a strictly proper scoring rule is therefore equivalent to finding a characteristic kernel in the corresponding space. While this is a non-trivial task, recent results have provided a variety of characteristic kernels on non-standard input spaces. We will present a few chosen ones and the corresponding strictly proper scoring rules that are relevant to this work, mainly focusing on separable Hilbert spaces, denoted by $\mathcal{H}$.

% \paragraph{Gaussian and Laplace kernels}
% \citet{ziegel_characteristic_2024} investigate characteristic kernels and their properties on Hilbert and Banach spaces, as well as kernels over general sets of measures. We focus on the results most relevant to this work, which are with regard to the Gaussian and Laplace kernels in separable Hilbert spaces. The Gaussian kernel is defined by
% \[
% k(z,z') = \exp \parens*{- \frac{\norm*{z-z'}^2_{\mathcal{H}}}{2 \gamma^2}},
% \]
% with scalar bandwidth $\gamma$. By \citet[][Theorem 3.1]{ziegel_characteristic_2024} the Gaussian kernel is characteristic and therefore leads to a strictly proper scoring rule $S_k$ relative to the set $\mathcal{M}_1^k(\mathcal{H})$. Similarly, the Laplace kernel, given by
% \[
% k(z,z') = \exp \parens*{- \frac{\norm*{z-z'}_{\mathcal{H}}}{2 \gamma^2}},
% \]
% with scalar bandwidth $\gamma$ leads to a strictly proper scoring rule $S_k$ relative to the set $\mathcal{M}_1^k(\mathcal{H})$. \citet{ziegel_characteristic_2024} more generally show these results for general metric spaces of strong negative type, which includes separable Hilbert spaces \citep{lyons_distance_2013}. In addition, they further investigate radial kernel on Hilbert and Banach spaces.

% \paragraph{Energy score}
% In addition to the Gaussian and Laplace kernel, we want to generalize the so-called energy score \citep{Gneiting.2007}, as it is one of the most commonly used scoring rules in the multivariate finite dimensional setting. The energy score is closely related to the energy distance statistic \citep{Szekely.2013}, which itself has shown to be a special case of the maximum mean discrepancy \citep{sejdinovic_equivalence_2013}. 

% \begin{theorem}[Energy score]
% Let $\mathcal{H}$ denote a separable Hilbert space. The energy score $\mathrm{ES}: \mathcal{M}_1^k(\mathcal{H}) \times \mathcal{H} \to \mathbb{R}$ defined as
% \[
% \mathrm{ES}(P, x) \coloneq \e_P \bracks*{\norm*{X - x}_\mathcal{H}} - \frac{1}{2}\e_P \bracks*{\norm*{X - X'}_\mathcal{H}},
% \]
% with $x \in \mathcal{H}$ and $X, X' \overset{i.i.d}{\sim} P \in \mathcal{M}_1^k$ is strictly proper relative to the class $\mathcal{M}_1^k(\mathcal{H})$.
% \end{theorem}

% \begin{proof}
% \citet[][Theorem 3.25]{lyons_distance_2013} states that every separable Hilbert space is of so-called strong negative type, which implies the existence of a positive definite \emph{distance kernel} induced by the metric $\norm{\cdot}_\mathcal{H}$ given as $k(z, z') = \norm{z- z_0}_\mathcal{H} + \norm{z'- z_0}_\mathcal{H} - \norm{z - z'}_\mathcal{H}$ for some fixed $z_0 \in \mathcal{H}$. Furthermore, \citet[][Proposition 29]{sejdinovic_equivalence_2013} state that the corresponding kernel $k$ is characteristic.
% % A metric space $(\mathcal{X}, \rho)$ is said to be of \emph{negative type} if the metric $\rho$ is negative definite \citep{bergHarmonicAnalysisSemigroups1984}. Such a metric induces a positive definite \emph{distance kernel} $k(z, z') = \rho(z, z_0) + \rho(z', z_0) - \rho(z, z')$ for some fixed $z_0 \in \mathcal{X}$. \cite[][Proposition 29]{sejdinovicEquivalenceDistancebasedRKHSbased2013} states that a metric is of strong negative type if and only if the associated distance kernel $k$ is characteristic. Finally, \cite[][Theorem 3.25]{lyonsDistanceCovarianceMetric2013} states that every separable Hilbert space is of strong negative type.
% Defining $k_d(z,z') \coloneqd(z,z_0) + d(z', z_0) - d(z,z')$, with $d(x,x') \coloneq \norm*{x-x'}_\mathcal{H}$ and $z_0 \in \mathcal{H}$ leads to
% \[
% \begin{split}
%     S_{k_d}(P,x) & = - \e_P \bracks*{k_d(X,x)} + \frac{1}{2} \e_P \bracks*{ k_d(X, X')} + \frac{1}{2} k_d(x,x) \\
%     & = - \e_P \bracks*{ d(X,z_0) + d(x, z_0) - d(X,x)} + \frac{1}{2}\e_P \bracks*{d(X,z_0) + d(X', z_0) - d(X,X')}\\
%     & +  \frac{1}{2} \parens*{d(x,z_0) + d(x, z_0) - d(x,x)}\\
%     & = \e_P \bracks*{d(X,x)} - \frac{1}{2} \e_P \bracks*{d(X,X')} - \frac{1}{2} \e_P[d(X,z_0)] + \frac{1}{2} \e_P[d(X', z_0)] - \e_P[d(x, z_0)] + d(x, z_0) \\
%     & = \e_P[d(X,x)] - \frac{1}{2} \e_P[d(X,X')] = \mathrm{ES}(P,x)
% \end{split}
% \]
% Since $k_d$ is characteristic, by \cite{steinwart_strictly_2021} the energy score is strictly proper relative to the class $\mathcal{M}_1^{k_d}(\mathcal{H})$.
% \end{proof}

\clearpage
\section{Model training and hyperparameters}
\label{app:hyperparameters}
Across the experiments, all uncertainty quantification methods share the same underlying neural operator architecture. The architectures for the Darcy-Flow experiments are taken from \citet{li_fourier_2021} and \citet{rahman2023uno} for the FNO and UNO, respectively. An overview of relevant model parameters can be found in table \autoref{tab:hyperparameters}. As the underlying function space, we use the $L^2$ space and its corresponding norm. The models are trained for a maximum of 1000 epochs and we use early stopping, to stop the training process, if the validation loss does not improve for ten epochs in order to avoid overfitting. For optimization, we employ the popular Adam optimizer \citep{kingma_adam_2017} with gradient clipping. Due to its size, we use an additional learning rate scheduler for the ERA5 dataset, which halves the learning rate if the validation loss does not improve for five epochs. The PNO methods are trained with $3$ generated samples, while we use an ensemble size of $M=100$ across all methods for evaluation. All experiments were implemented in PyTorch and run on an NVIDIA RTX A6000 with 48 GB of RAM. The accompanying code can be found at \url{https://github.com/cbuelt/pfno}.
\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
          & Model & \makecell{Batch \\ size} & \makecell{Learning \\ rate} & \makecell{Hidden \\ channels} & \makecell{Projection\\ channels} & \makecell{Lifting \\ channels} & Modes  \\
        \hline
        \multirow{3}{3em}{Darcy Flow}& FNO & 64 & 0.001 & 32 & 256 & 256 & (12,12) \\
        & UNO  & 64 & 0.001 & \makecell{[64,128,128,\\ 64,32]} & 32 & 32 & \makecell{[(18,18),(8,8),(8,8), \\ (8,8),(18,18)]} \\
        \hline
        \multirow{3}{3em}{KS}& FNO & 64 & 0.001 & 20 & 128 & 128 & (10,12) \\
        & UNO  & 32 & 0.001 & \makecell{[16,32,64,128,\\ 64,32,16]} & 32 & 32 & \makecell{[(4,20),(4,14),(4,6),(7,6), \\ (7,6),(10,14),(10,20)]} \\
        \hline
        \multirow{1}{3em}{SSWE}& SFNO & 64 & 0.005 & 32 & 256 & 256 & (32,32) \\
        \hline
        \multirow{1}{3em}{ERA5}& SNO & 16 & 0.005 & 20 & 128 & 128 & (10,12,12) \\
        \hline
    \end{tabular}
    \vspace{0.2cm}
    \caption{Overview of relevant model hyperparameters.}
    \label{tab:hyperparameters}
\end{table}



% \clearpage
% \section{Additional experiments and studies}
% \begin{enumerate}
%     \item Runtime analysis: All methods runtime per epoch for one experiment and number of epochs
%     \item Comparison of PFNO with Dropout and reparametrization layer, performance and runtime
%     \item Dependence of PFNO error against Dropout/Fourier dropout, Heatmap with different dropout values (e.g. 6 times 6)
%     \item Dependence of PNO performance on number of generated samples. Runtime, performance and number of epochs. Only Reparametrization.    
%     \item Maybe: Training time comparison and pre-training of PFNO
%     \item Maybe: Superresolution on SSWE
% \end{enumerate}

%UNO
\clearpage
\section{Results for the U-shaped neural operator}
\label{app:uno}
This section provides additional results for our method using the U-shaped neural operator \citep{rahman2023uno}, highlighting the transferability across different architectures. While the model generally performs a bit worse than the FNO on our experiments, the performance of the uncertainty quantification methods shows a similar ordering as before. The PNO\textsubscript{D} leads to the best performance on most metrics for Darcy flow and the Kuramoto-Sivashinsky equation. Only once, for the $L^2$ loss for Darcy flow the MCD approach is preferable and PNO\textsubscript{R} for the CRPS and the coverage on the KS equation.
\begin{table}[ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Experiment} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{Darcy Flow}}&DET & \makecell{0.1949 \\ ($\pm$ 0.0073)} & \makecell{0.1949 \\ ($\pm$ 0.0073)} & \makecell{0.1565 \\ ($\pm$ 0.0060)} & - & - & - \\

\cline{2-8} &  PNO\textsubscript{D}  & \makecell{0.1894 \\ ($\pm$0.0066)} & \makecell{\textbf{0.1354 }\\ ($\pm$0.0050)} & \makecell{\textbf{0.1097} \\ ($\pm$0.0041)} & \makecell{\textbf{-0.2856} \\ ($\pm$0.0614)} & \makecell{\textbf{0.9069} \\ ($\pm$0.0243)} & \makecell{0.6998 \\ ($\pm$0.0467)} \\

\cline{2-8} &  PNO\textsubscript{R}  & \makecell{0.2005 \\ ($\pm$0.0060)} & \makecell{0.1473 \\ ($\pm$0.0055)} & \makecell{0.1275 \\ ($\pm$0.0044)} & \makecell{20.018 \\ ($\pm$11.926)} & \makecell{0.5765 \\ ($\pm$0.0698)} & \makecell{0.4218 \\ ($\pm$0.0559)} \\

\cline{2-8} & MCD & \makecell{\textbf{0.1875} \\ ($\pm$0.0058)} & \makecell{0.1491 \\ ($\pm$0.0076)} & \makecell{0.1233 \\ ($\pm$0.0069)} & \makecell{2.2782 \\ ($\pm$1.4980)} & \makecell{0.5468 \\ ($\pm$0.0600)} & \makecell{0.2822 \\ ($\pm$0.0259)} \\

\cline{2-8}& LA & \makecell{0.1954 \\ ($\pm$0.0074)} & \makecell{0.1530 \\ ($\pm$0.0052)} & \makecell{0.1229 \\ ($\pm$0.0040)} & \makecell{0.9996 \\ ($\pm$0.2835)} & \makecell{0.7477 \\ ($\pm$0.0411)} & \makecell{0.4374 \\ ($\pm$0.0643)} \\
\hline
\hline
\multirow{9}{2em}{\textbf{Kuramoto-Sivashinsky}} & DET & \makecell{0.9492 \\ ($\pm$ 0.0250)} & \makecell{0.9492 \\ ($\pm$ 0.0250)} & \makecell{0.7652 \\ ($\pm$ 0.0241)} & - & -& -\\

\cline{2-8}&  PNO\textsubscript{D} & \makecell{\textbf{0.9315} \\ ($\pm$0.0046)} & \makecell{\textbf{0.6560} \\ ($\pm$0.0032)} & \makecell{0.6014 \\ ($\pm$0.0088)} & \makecell{\textbf{2.0875} \\ ($\pm$0.2564)} & \makecell{0.7828 \\ ($\pm$0.0315)} & \makecell{3.3892 \\ ($\pm$0.1214)} \\

\cline{2-8} &  PNO\textsubscript{R} & \makecell{0.9371 \\ ($\pm$0.0022)} & \makecell{0.6595 \\ ($\pm$0.0015)} & \makecell{\textbf{0.5326} \\ ($\pm$0.0023)} & \makecell{5.4528 \\ ($\pm$6.2975)} & \makecell{\textbf{0.9110 }\\ ($\pm$0.0150)} & \makecell{3.4039 \\ ($\pm$0.0442)} \\

\cline{2-8} & MCD & \makecell{0.9444 \\ ($\pm$0.0014)} & \makecell{0.8500 \\ ($\pm$0.0034)} & \makecell{0.7163 \\ ($\pm$0.0026)} & \makecell{449.08 \\ ($\pm$42.839)} & \makecell{0.1965 \\ ($\pm$0.0050)} & \makecell{0.4052 \\ ($\pm$0.0109)} \\

\cline{2-8}& LA & \makecell{0.9494 \\ ($\pm$0.0250)} & \makecell{0.8544 \\ ($\pm$0.0265)} & \makecell{0.6928 \\ ($\pm$0.0249)} & \makecell{54.338 \\ ($\pm$37.019)} & \makecell{0.2560 \\ ($\pm$0.0473)} & \makecell{0.5770 \\ ($\pm$0.1251)} \\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the Darcy Flow and Kuramoto-Sivashinsky equation using the UNO architecture. The best model is highlighted in bold for all metrics, except for the interval width, as it is only comparable for models with the same coverage. The standard deviation across the different experiment runs is given in the brackets.}
\end{table}


% Fourier dropout
\clearpage
\section{Effect of Fourier dropout} \label{app:fourier-dropout}
For the PNO\textsubscript{D} stochastic forward passes are required to generate the predictive distribution as the output of the neural operator. As mentioned in Subsection~\ref{ssec:training}, we use an additional dropout by masking random components in the Fourier domain (Fourier dropout) of each Fourier block in addition to the standard dropout (weight dropout). In the main paper, we always set the same value for the weight dropout as for the Fourier dropout. Here, we provide an additional study to examine the dependence between the different dropout rates and the performance of the PNO\textsubscript{D}. An FNO architecture is trained for the Kuramoto-Sivashinsky equation with different combinations of the dropout. The corresponding results can be found in \autoref{tab:fourier_dropout}, while \autoref{fig:fourier_dropout} shows a corresponding visualization. The dropout rates seem to have a structured effect on the predictions, as the $L^2$ loss and the energy score show better performance with higher dropout rates. However, the NLL seems to be smaller with lower dropout rates. Generally, several of the metrics are minimized by a weight dropout of 0.05 and a higher ($L^2$, ES) or lower (CRPS) Fourier dropout. However, detecting a significant effect is difficult due to the stochastic nature of the approach and the large computational demand for performing detailed experiments. Still, the experiment suggests that different types of dropout seem beneficial, as they induce more stochasticity in the model that could lead to a higher expressivity with regard to the predictive distribution.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/fourier_dropout.pdf}
    \caption{The figure shows the different metrics in dependence of the Fourier and weight dropout for the Kuramoto-Sivashinsky equation. The axes are shown on a log-scale.}
    \label{fig:fourier_dropout}
\end{figure}




\begin{table}[htb]
\centering
\begin{tabular}{|c|l|c|c|c|c|c|c|c|}
\hline
\makecell{\textbf{Weight} \\ \textbf{dropout}} & \makecell{\textbf{Fourier} \\ \textbf{dropout}} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{5}{2em}{0.01} & 0.01 & 0.8998 & 0.6337 & 0.5666 & 1.4866 & 0.8505 & 3.4649 \\

\cline{2-8} & 0.02 & 0.8942 & 0.6296 & 0.5469 & \textbf{1.4281} & 0.8746 & 3.4825 \\

\cline{2-8} & 0.05 & 0.9032 & 0.6361 & 0.5315 & 1.5714 & 0.8366 & 3.2629 \\

\cline{2-8} & 0.1 & 0.8899 & 0.6266 & 0.5236 & 1.5835 & 0.8336 & 3.2151 \\

\cline{2-8} & 0.2 & 0.8858 & 0.6237 & 0.5263 & 1.6129 & 0.8294 & 3.2427 \\

\hline
\hline
\multirow{5}{2em}{0.02} & 0.01 & 0.9042 & 0.6367 & 0.5462 & 1.4577 & 0.8983 & 3.6815 \\

\cline{2-8} & 0.02 & 0.9054 & 0.6376 & 0.5410 & 1.4299 & 0.9051 & 3.6425 \\

\cline{2-8} & 0.05 & 0.8887 & 0.6259 & 0.5223 & 1.5018 & 0.8667 & 3.3519 \\

\cline{2-8} & 0.1 & 0.8942 & 0.6298 & 0.5296 & 1.6132 & 0.8294 & 3.2363 \\

\cline{2-8} & 0.2 & 0.8848 & 0.6231 & 0.5276 & 1.6191 & 0.8279 & 3.2578 \\

\hline
\hline
\multirow{5}{2em}{0.05} & 0.01 & 0.8907 & 0.6274 & \textbf{0.5181} & 1.4798 & 0.8712 & 3.2865 \\

\cline{2-8} & 0.02 & 0.8922 & 0.6284 & 0.5221 & 1.4968 & 0.8660 & 3.3190 \\
\cline{2-8} & 0.05 & 0.8908 & 0.6273 & 0.5342 & 1.6776 & 0.8198 & 3.2360 \\

\cline{2-8} & 0.1 & 0.8900 & 0.6268 & 0.5466 & 1.9742 & 0.7766 & 3.1370 \\

\cline{2-8} & 0.2& \textbf{0.8806} & \textbf{0.6201} & 0.5327 & 1.7949 & 0.8078 & 3.2122 \\

\hline
\hline

\multirow{5}{2em}{0.1} & 0.01& 0.9072 & 0.6390 & 0.5357 & 1.5733 & 0.8418 & 3.2286 \\

\cline{2-8} & 0.02 & 0.8914 & 0.6280 & 0.5344 & 1.6708 & 0.8151 & 3.1668 \\

\cline{2-8} & 0.05 & 0.8910 & 0.6276 & 0.5425 & 1.8126 & 0.7929 & 3.1337 \\

\cline{2-8} & 0.1 & 0.8877 & 0.6255 & 0.5643 & 3.1770 & 0.7391 & 3.0874 \\

\cline{2-8} & 0.2 & 0.8852 & 0.6234 & 0.5537 & 2.7249 & 0.7638 & 3.0821 \\

\hline
\hline
\multirow{5}{2em}{0.2} & 0.01 & 0.8891 & 0.6264 & 0.5520 & 2.2083 & 0.7598 & 2.9729 \\

\cline{2-8} & 0.02 & 0.8870 & 0.6249 & 0.5563 & 2.5560 & 0.7431 & 2.9557 \\

\cline{2-8} & 0.05 & 0.8895 & 0.6266 & 0.5561 & 2.4287 & 0.7527 & 3.0111 \\

\cline{2-8} & 0.1 & 0.8869 & 0.6247 & 0.5640 & 3.4221 & 0.7352 & 2.9904 \\

\cline{2-8} & 0.2 & 0.8828 & 0.6217 & 0.5675 & 6.7351 & 0.7319 & 3.0469 \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Metrics for different combinations of dropout for training on the Kuramoto-Sivashinsky equation.}
\label{tab:fourier_dropout}
\end{table}

\clearpage
\section{Runtime analysis}
\label{app:runtime_analysis}

Since using a scoring rule as the loss function requires predicting multiple samples for each optimization step, a runtime analysis of PNO is of high interest. In Table~\ref{tab:runtime-analysis}, we show the estimated runtime of MCD, LA, PNO\textsubscript{D}, and PNO\textsubscript{R}. As expected, both PNO methods require significantly more time per epoch, with PNO\textsubscript{D} taking about three times as long as the baseline methods and PNO\textsubscript{R} about 50\% longer. The main bottleneck is the evaluation of the energy score, which requires to calculate a distance matrix for all predictive samples. Furthermore, PNO\textsubscript{D} requires several forward passes per optimization step, increasing the runtime even further. PNO\textsubscript{R} on the other hand requires more memory and had to be trained with a smaller batch size (16 instead of 32), which also increases the computing time per epoch compared to MCD and LA, which are both trained without uncertainty.

\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Method} & \textbf{Avg. Number of Epochs} & \textbf{Time per Epoch (s)} & \textbf{Total Time (s)} \\
\hline
MCD           & \textbf{112.89}      & 22.36           & \textbf{22718}         \\
LA           & 140.00      & \textbf{21.56 }           & 27161         \\
PNO\textsubscript{D} & 132.33   & 58.71            & 69920         \\
PNO\textsubscript{R} & 209.78   & 31.92            & 60272         \\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{Runtime comparison on SSWE with one-step autoregressive training. The number of epochs is averaged across the ten training runs using early stopping with 10 epochs of patience. The timings are averaged over 25 training epochs on an RTXA6000 and the best models are highlighted in bold.}
\label{tab:runtime-analysis}
\end{table}


% Computational efficiency
\clearpage
\section{Improving computational efficiency}
\label{app:fine_tuning}
As previously mentioned, the PNO requires substantially more memory and computing time than the deterministic neural operator. In this section, we briefly want to present a strategy to mitigate this behavior. The most straightforward way to improve computing time is to consider a given pre-trained (deterministic) model, which is a realistic scenario in a practical setting. In that case, the PNO\textsubscript{D} can be used to fine-tune that model, as it only requires stochastic dropout with nonzero probability. \autoref{tab:fine_tuning} compares the runtime and metrics for the deterministic model, the PNO\textsubscript{D} and the fine-tuned PNO\textsubscript{D}. The results show that the pre-trained model requires substantially fewer epochs and training time, while at the same time leading to an improved performance. \autoref{fig:finetuning} shows an additional visualization of the training and validation loss against the training time of the model. While training a deterministic model and a PNO\textsubscript{D} together, does not yield significant improvements in the computing time, the pre-trained PNO\textsubscript{D} requires significantly fewer epochs to converge and therefore can be efficiently used if one has an already trained deterministic model.
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
 & \textbf{PNO\textsubscript{D}} & \textbf{DET} & \textbf{PNO\textsubscript{D} (pre-trained)} \\
\hline
 $t$ & \makecell{7043.38 \\ ($\pm$ 895.79)} & \makecell{4689.33 \\ ($\pm$ 408.54)} & \makecell{\textbf{2404.10} \\ ($\pm$ 700.00)} \\
 \hline
$\varnothing$ epochs  & \makecell{150.78 \\ ($\pm$ 19.18)} & \makecell{112.33 \\ ($\pm$ 9.79)} & \makecell{\textbf{46.33} \\ ($\pm$ 13.49)} \\
 \hline
$\mathcal{L}_{L^2}$ & \makecell{0.8793 \\ ($\pm$ 0.0072)} & \makecell{\textbf{0.8635 }\\ ($\pm$ 0.0048)} & \makecell{0.8702 \\ ($\pm$ 0.0046)} \\
 \hline
ES & \makecell{0.6195 \\ ($\pm$ 0.0049)} & \makecell{0.7541 \\ ($\pm$ 0.0049)} & \makecell{\textbf{0.6130} \\ ($\pm$ 0.0033)} \\
 \hline
CRPS & \makecell{0.5496 \\ ($\pm$ 0.0097)} & \makecell{0.5974 \\ ($\pm$ 0.0058)} & \makecell{\textbf{0.5343} \\ ($\pm$ 0.0080)} \\
 \hline
NLL & \makecell{\textbf{3.0389 }\\ ($\pm$ 0.5872)} & \makecell{107.1038 \\ ($\pm$ 22.9412)} & \makecell{6.0401 \\ ($\pm$ 1.9573)} \\
 \hline
$\mathcal{C}_{0.05}$ & \makecell{0.7640 \\ ($\pm$ 0.0191)} & \makecell{0.3600 \\ ($\pm$ 0.0082)} & \makecell{\textbf{0.7950} \\ ($\pm$ 0.0154)} \\
 \hline
$|\mathcal{C}_{0.05}|$  & \makecell{3.0852 \\ ($\pm$ 0.0422)} & \makecell{0.6046 \\ ($\pm$ 0.0164)} & \makecell{3.1327 \\ ($\pm$ 0.0288)} \\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{Performance comparison of the pre-trained PNO\textsubscript{D} against the deterministic model and the regular PNO\textsubscript{D} using ten different model runs. The standard deviation across the runs is given in the brackets and the best model is highlighted in bold.}
\label{tab:fine_tuning}
\end{table}



\begin{figure}[ht]
    \centering
    \includegraphics[width = \linewidth]{figs/finetuning.pdf}
    \caption{The figure shows the training time against the energy score for the regular PNO\textsubscript{D} and the pre-trained PNO\textsubscript{D} for a specific experiment run. The solid and dashed lines show the training and validation loss, respectively.} 
    \label{fig:finetuning}
\end{figure}



\clearpage
\section{Influence of the number of predictive samples on the performance of PNO}
\label{app:pno_comparison}
Our PNO approach depends on utilizing an unbiased estimator of the energy score in \autoref{eq:energy_score}, which requires the model to generate $M>1$ samples. While in practice we found that even for only $M=3$ generated samples, the model shows improved performance, increasing the sample size is mainly hindered by computational reasons. In order to further analyze the model training in dependence of the sample size $M$, we trained both PNO approaches on the Kuramoto-Sivashinsky equation with different training sample sizes $M$. For evaluation, all models still use an ensemble size of one hundred members across the test data. \autoref{tab:pno_samples_comparison} shows the corresponding results, which are again aggregated over ten runs. For the PNO\textsubscript{R}, the $L^2$ loss and energy score already perform well for $M=3$ samples, while the other metrics are improved by increasing the sample size. While the training time per epoch increases, the differences seem negligible for less than twenty samples. On the contrary, the PNO\textsubscript{D} shows the best performance, if $M\geq10$ samples are already used during the training process. Especially the negative log-likelihood improves significantly. However, for the PNO\textsubscript{D}, the training time per epoch increases drastically, as the model also requires $M$ stochastic forward passes. On the other hand, for the PNO\textsubscript{D}, the number of training epochs decreases with the sample size. A solution might be using different training regimes, starting with a low sample size and increasing it during the training process, to reduce the computational demand.

\begin{table}[htb]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{$M$} & $t$& $\varnothing$ epochs & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ \\
\hline
\multirow{9}{2em}{\textbf{PNO\textsubscript{D}}} & 3 & \textbf{37.72s} &151 & \makecell{0.8793 \\ ($\pm$0.0072)} & \makecell{0.6195 \\ ($\pm$0.0049)} & \makecell{0.5496 \\ ($\pm$0.0097)} & \makecell{3.0389 \\ ($\pm$0.5872)} & \makecell{0.7640 \\ ($\pm$0.0191)}  \\

\cline{2-9} &  5  & 44.58s &154 & \makecell{0.8790 \\ ($\pm$0.0059)} & \makecell{0.6191 \\ ($\pm$0.0038)} & \makecell{0.5577 \\ ($\pm$0.0080)} & \makecell{5.2872 \\ ($\pm$1.8802)} & \makecell{0.7541 \\ ($\pm$0.0189)} \\

\cline{2-9} & 10  & 59.94s & 126  & \makecell{0.8799 \\ ($\pm$0.0021)} & \makecell{0.6196 \\ ($\pm$0.0015)} & \makecell{\textbf{0.5348} \\ ($\pm$0.0038)} & \makecell{2.3471 \\ ($\pm$0.2235)} & \makecell{0.8001 \\ ($\pm$0.0095)}  \\

\cline{2-9}& 20&  90.64s & 135 & \makecell{\textbf{0.8784} \\ ($\pm$0.0091)} & \makecell{\textbf{0.6188} \\ ($\pm$0.0061)} & \makecell{0.5566 \\ ($\pm$0.0088)} & \makecell{5.3147 \\ ($\pm$1.5163)} & \makecell{0.7592 \\ ($\pm$0.0170)}  \\

\cline{2-9}& 50&181.45s & \textbf{98} & \makecell{0.8857 \\ ($\pm$0.0050)} & \makecell{0.6239 \\ ($\pm$0.0036)} & \makecell{0.5356 \\ ($\pm$0.0030)} & \makecell{\textbf{1.9174} \\ ($\pm$0.1097)} & \makecell{\textbf{0.8036} \\ ($\pm$0.0092)}  \\


\hline
\hline
%%%%%%%%%%%%%%PNOR
\multirow{9}{2em}{\textbf{PNO\textsubscript{R}}} & 3 & 31.08s  & 163 &\makecell{\textbf{0.8640} \\ ($\pm$0.0038)} & \makecell{\textbf{0.6081} \\ ($\pm$0.0027)} & \makecell{0.4743 \\ ($\pm$0.0037)} & \makecell{1.2268 \\ ($\pm$0.0083)} & \makecell{0.9401 \\ ($\pm$0.0030)}  \\

\cline{2-9} & 5 &  \textbf{29.75s}& 146 & \makecell{0.8661 \\ ($\pm$0.0036)} & \makecell{0.6096 \\ ($\pm$0.0025)} & \makecell{0.4751 \\ ($\pm$0.0032)} & \makecell{1.3768 \\ ($\pm$0.4510)} & \makecell{0.9421 \\ ($\pm$0.0027)}  \\

\cline{2-9} & 10 & 31.35s & 152 & \makecell{0.8649 \\ ($\pm$0.0036)} & \makecell{0.6088 \\ ($\pm$0.0025)} & \makecell{0.4737 \\ ($\pm$0.0038)} & \makecell{1.2180 \\ ($\pm$0.0158)} & \makecell{0.9437 \\ ($\pm$0.0020)} \\

\cline{2-9}& 20  & 35.06s & \textbf{135} & \makecell{0.8680 \\ ($\pm$0.0053)} & \makecell{0.6110 \\ ($\pm$0.0037)} & \makecell{0.4756 \\ ($\pm$0.0051)} & \makecell{1.2198 \\ ($\pm$0.0181)} & \makecell{\textbf{0.9453} \\ ($\pm$0.0027)}  \\

\cline{2-9}& 50 & 43.43s & 181 & \makecell{0.8649 \\ ($\pm$0.0035)} & \makecell{0.6088 \\ ($\pm$0.0025)} & \makecell{\textbf{0.4733} \\ ($\pm$0.0027)} & \makecell{\textbf{1.2138} \\ ($\pm$0.0085)} & \makecell{0.9449 \\ ($\pm$0.0028)}  \\


\hline
\end{tabular}
\vspace{0.2cm}
\caption{Comparison of the performance of PNO\textsubscript{D} and PNO\textsubscript{R} in dependence of the training sample size $M$ for the Kuramoto-Sivashinsky equation. The best results are highlighted in bold, where $t$ denotes the average training time per epoch.}
\label{tab:pno_samples_comparison}
\end{table}

% Introduce section adressing the changes of the first reviewer regarding comparison of PNO_D and PNO_R
\clearpage
\section{Comparison of PNO\textsubscript{D} and PNO\textsubscript{R}}
\label{app:model_comparison}
In this section we compare the two presented approaches PNO\textsubscript{D} and PNO\textsubscript{R} in more detail and provide some guidance as to which method might be more useful for a specific problem set. While \autoref{sec:experiments} evaluates both approaches with respect to different metrics and experiments, these absolute values make it difficult to see which method is generally preferable. For that purpose we here also report the skill scores across different metrics, which are a common tool for assessing relative improvements over a reference model. The skill score for a performance metric over a test set is given by
\begin{equation*}
    \mathrm{SS}_F = \frac{\mathrm{\overline{\mathrm{S}}_{\mathrm{ref}}}-\overline{\mathrm{S}}_F}{ \mathrm{S}_{\mathrm{opt}} - \mathrm{\overline{\mathrm{S}}_{\mathrm{ref}}}},
\end{equation*}
where $\overline{\mathrm{S}}_F$ denotes the average performance of the method of interest, $\overline{\mathrm{S}}_{\mathrm{ref}}$ denotes the corresponding average metric of a reference method and $\mathrm{S}_{\mathrm{opt}}$ denotes the optimal achievable value. For the $L^2$, ES and CRPS metrics, $\mathrm{S}_{\mathrm{opt}}$ corresponds to zero. For the other metrics, the skill score is not applicable (for the negative log-likelihood $\mathrm{S}_{\mathrm{opt}}$ would be minus infinity). The skill score is positively oriented with a maximum of one, negative values indicating worse performance than the reference, and zero indicating no improvement. \autoref{tab:skill_scores} shows the skill scores of all methods with respect to the deterministic baseline. The results show that in the cases where PNO\textsubscript{D} outperforms PNO\textsubscript{R}, the relative improvement is quite high, while in the opposite direction, PNO\textsubscript{R} only achieves a relatively small improvement against PNO\textsubscript{D}. Averaged over all experiments, it is evident that PNO\textsubscript{D} is preferable to all metrics, as it generally leads to an improvement against PNO\textsubscript{R}.
% \begin{table}[htb]
% \centering
% \begin{tabular}{|l|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{\textbf{Experiment}} & \multicolumn{3}{c|}{\textbf{PNO\textsubscript{D}}} & \multicolumn{3}{c|}{\textbf{PNO\textsubscript{R}}} \\
% \cline{2-7}
%  & $\mathcal{L}_{L^2}$ & ES & CRPS & $\mathcal{L}_{L^2}$ & ES & CRPS \\
% \hline
% Darcy Flow &  0.2160 & 0.2572 & 0.2566 & \red{-0.0466} & \red{-0.0657} & \red{-0.1387} \\
% KS &  \red{-0.0025} & 0.1550 & 0.0961 & 0.0151 & 0.1706 & 0.2200 \\
% SSWE (1h) &  0.3032 & 0.2984 & 0.3549 & 0.0918 & 0.1328 & \red{-0.1152} \\
% ERA5 &  0.0343 & 0.1102 & 0.1066 & 0.0399 & 0.0427 & 0.0250 \\
% \hline
% \hline
% $\Sigma$ & 0.1377 & 0.2052& 0.2036 &0.025 & 0.0701 & \red{-0.0022} \\
% \hline
% \end{tabular}
% \caption{Skill scores for the different experiments with respect to the Laplace approximation as a baseline.}
% \label{tab:skill_scores}
% \end{table}

\begin{table}[htb]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
 \textbf{Experiment} & $\mathcal{L}_{L^2}$ & ES & CRPS \\
\hline
Darcy Flow & 0.2509 & 0.303 & 0.3471 \\
KS & \red{-0.0178} & \red{-0.0188} & \red{-0.1588} \\
SSWE & 0.2328 & 0.191 & 0.4216 \\
ERA & \red{-0.0059} & 0.0704 & 0.0836 \\
\hline
$\varnothing$ & 0.115 & 0.1364 & 0.1734 \\
\hline
\end{tabular}
\vspace{0.2cm}
\caption{Skill scores for the different experiments and PNO\textsubscript{D} with respect to PNO\textsubscript{R} as a baseline. The last row shows the average over all experiments.}
\label{tab:skill_scores}
\end{table}

While from a performance perspective, PNO\textsubscript{D} is preferable, there might be other considerations determining which model to choose. If one has a fixed architecture or an already trained model, PNO\textsubscript{D} might be a better choice, as it is easier to incorporate into a given model, requiring only a nonzero dropout probability, while PNO\textsubscript{R} also requires additional weights. On the other hand, PNO\textsubscript{D} comes with a higher computational cost, thus if the computational resources are limited, PNO\textsubscript{R} might be preferable. Finally, one might choose the method based on knowledge of the underlying data distribution and the corresponding expressivity of the method. Both methods, PNO\textsubscript{R} and PNO\textsubscript{D} only offer a crude approximation to the posterior predictive distribution \citep{pmlr-v80-hron18a}. While for PNO\textsubscript{D} it is in general very difficult to make any statements about the expressivity and approximation quality of the resulting distribution, for the PNO\textsubscript{R} case, the resulting distribution is a standard Gaussian. Therefore, if the data distribution is known to be similar to a Gaussian, PNO\textsubscript{R} might be a suitable method. On the other hand, if the data distribution is known to be complex or multimodal, PNO\textsubscript{D} might perform better, although no guarantees can be made that it matches the underlying distribution.







\clearpage
\section{Additional figures}
\label{app:additional_figures}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/darcy_flow_predictions.pdf}
    \caption{The figure shows the ground truth, mean prediction, absolute error, standard deviation, and coverage for the different methods on a sample of the Darcy-Flow equation.}
    \label{fig:darcy_flow_predictions}
\end{figure}


% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\linewidth]{figs/ks_predictions.pdf}
%     \caption{The figure shows the ground truth, mean prediction, standard deviation, and coverage for the different methods on a sample of the Kuramoto-Sivashinsky equation. The mean coverage percentages from left to right are 36.19\%, 45.74\%, 76.27\% and 95.47\%.}
%     \label{fig:ks_predictions}
% \end{figure}

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\linewidth]{figs/ERA5_statistics_60h.pdf}
%     \caption{The figure shows the squared error, standard deviation, and coverage for the different methods on the 2m surface temperature prediction task with a lead time of 60h. The mean coverage percentages from left to right are 46.17\%, 82.22\%, 91.68\% and 63.36\%.}
%     \label{fig:ERA5_statistics}
% \end{figure}

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=\linewidth]{figs/sswe_statistics_1h.pdf}
%     \caption{The figure shows the squared error, standard deviation, and coverage for the different methods on a sample of the spherical shallow water equation. The shown variable is the water geopotential and the prediction horizon is one hour with predictions obtained via two-step autoregressive training. The mean coverage percentages from left to right are 62.16\%, 98.70\%, 96.73\% and 3.36\%.}
%     \label{fig:sswe_statistics}
% \end{figure}


\clearpage
\section{Additional results for SSWE}
\label{app:additional_results_sswe}

\begin{table}[ht]
\centering
\begin{tabular}{|l|l|c|c|c|c|c|c|c|}
\hline
\textbf{$\Delta t$} & \textbf{Method} & $\mathcal{L}_{L^2}$ & ES & CRPS & NLL &$\mathcal{C}_{0.05}$ & $|\mathcal{C}_{0.05}|$ \\
\hline
\multirow{9}{2em}{\textbf{1h}} & DET & \makecell{0.3215 \\ ($\pm$ 0.0106)} & \makecell{0.3215 \\ ($\pm$ 0.0106)} & \makecell{0.1260 \\ ($\pm$ 0.0035)} & -& - & -\\


\cline{2-8} & PNO\textsubscript{D}  & \makecell{\textbf{0.2955} \\ ($\pm$0.0207)} & \makecell{\textbf{0.2202 }\\ ($\pm$0.0132)} & \makecell{\textbf{0.0896 }\\ ($\pm$0.0051)} & \makecell{\textbf{-0.4609} \\ ($\pm$0.0463)} & \makecell{\textbf{0.9840} \\ ($\pm$0.0032)} & \makecell{0.8923 \\ ($\pm$0.0467)} \\

\cline{2-8} &  PNO\textsubscript{R} & \makecell{0.3132 \\ ($\pm$0.0244)} & \makecell{0.2208 \\ ($\pm$0.0170)} & \makecell{0.1048 \\ ($\pm$0.0069)} & \makecell{2.5$\times 10^{7}$\\ ($\pm$3.6$\times 10^{7}$)} & \makecell{0.6912 \\ ($\pm$0.0897)} & \makecell{0.4971 \\ ($\pm$0.0842)} \\

\cline{2-8} & MCD & \makecell{0.3845 \\ ($\pm$0.0206)} & \makecell{0.2972 \\ ($\pm$0.0194)} & \makecell{0.1193 \\ ($\pm$0.0102)} & \makecell{0.4516 \\ ($\pm$0.5340)} & \makecell{0.7063 \\ ($\pm$0.0496)} & \makecell{0.3819 \\ ($\pm$0.0123)} \\

\cline{2-8}& LA & \makecell{0.4222 \\ ($\pm$0.0511)} & \makecell{0.3063 \\ ($\pm$0.0345)} & \makecell{0.1346 \\ ($\pm$0.0135)} & \makecell{0.1821 \\ ($\pm$0.2472)} & \makecell{0.9024 \\ ($\pm$0.0694)} & \makecell{0.9606 \\ ($\pm$0.2178)} \\

\hline
\hline
\multirow{9}{2em}{\textbf{10h}}& DET & \makecell{\textbf{1.1346} \\ ($\pm$ 0.1861)} & \makecell{\textbf{1.1346} \\ ($\pm$ 0.1861)} & \makecell{\textbf{0.4771} \\ ($\pm$ 0.0665)} & - & - & - \\

\cline{2-8} & PNO\textsubscript{D} & \makecell{2.8$\times 10^{3}$ \\ ($\pm$8.3$\times 10^{3}$)} & \makecell{2.7$\times 10^{3}$ \\ ($\pm$8.0$\times 10^{3}$)} & \makecell{22.3235 \\ ($\pm$65.581)} & \makecell{\textbf{6.0553} \\ ($\pm$0.7769)} & \makecell{0.4695 \\ ($\pm$0.0207)} & \makecell{43.754 \\ ($\pm$128.49)} \\

\cline{2-8} &  PNO\textsubscript{R} & \makecell{9.4$\times 10^{3}$\\ ($\pm$2.0$\times 10^{4}$)} & \makecell{7.6$\times 10^{3}$\\ ($\pm$1.6$\times 10^{4}$)} & \makecell{3.6$\times 10^{3}$\\ ($\pm$7.7$\times 10^{3}$)} & \makecell{1.7$\times 10^{20}$ \\ ($\pm$4.6$\times 10^{20}$)} & \makecell{0.3056 \\ ($\pm$0.1644)} & \makecell{1.3$\times 10^{4}$\\ ($\pm$3.2$\times 10^{4}$)} \\


\cline{2-8} & MCD & \makecell{1.3324 \\ ($\pm$0.2282)} & \makecell{1.2066 \\ ($\pm$0.2177)} & \makecell{0.5043 \\ ($\pm$0.0739)} & \makecell{25.394 \\ ($\pm$3.3598)} & \makecell{0.2511 \\ ($\pm$0.0047)} & \makecell{0.4104 \\ ($\pm$0.0304)} \\


\cline{2-8}& LA & \makecell{1.5445 \\ ($\pm$0.2696)} & \makecell{1.2835 \\ ($\pm$0.2493)} & \makecell{0.5412 \\ ($\pm$0.1041)} & \makecell{7.2994 \\ ($\pm$6.0597)} & \makecell{\textbf{0.5254} \\ ($\pm$0.1033)} & \makecell{1.0291 \\ ($\pm$0.2616)} \\

\hline
\end{tabular}
\vspace{0.2cm}
\caption{Results for the spherical shallow water equation with single-step training. The best model is highlighted in bold and the standard deviation is given in the brackets.}
\label{tab:sswe_results_one_steps}
\end{table}


\end{document}
