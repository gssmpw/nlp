@article{luDeepONetLearningNonlinear2021,
  title = {{{DeepONet}}: {{Learning}} Nonlinear Operators for Identifying Differential Equations Based on the Universal Approximation Theorem of Operators},
  shorttitle = {{{DeepONet}}},
  author = {Lu, Lu and Jin, Pengzhan and Karniadakis, George Em},
  year = {2021},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {3},
  number = {3},
  eprint = {1910.03193},
  primaryclass = {cs},
  pages = {218--229},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00302-5},
  urldate = {2024-12-18},
  abstract = {While it is widely known that neural networks are universal approximators of continuous functions, a less known and perhaps more powerful result is that a neural network with a single hidden layer can approximate accurately any nonlinear continuous operator [5]. This universal approximation theorem is suggestive of the potential application of neural networks in learning nonlinear operators from data. However, the theorem guarantees only a small approximation error for a sufficient large network, and does not consider the important optimization and generalization errors. To realize this theorem in practice, we propose deep operator networks (DeepONets) to learn operators accurately and efficiently from a relatively small dataset. A DeepONet consists of two sub-networks, one for encoding the input function at a fixed number of sensors xi, i = 1, . . . , m (branch net), and another for encoding the locations for the output functions (trunk net). We perform systematic simulations for identifying two types of operators, i.e., dynamic systems and partial differential equations, and demonstrate that DeepONet significantly reduces the generalization error compared to the fully-connected networks. We also derive theoretically the dependence of the approximation error in terms of the number of sensors (where the input function is defined) as well as the input function type, and we verify the theorem with computational results. More importantly, we observe high-order error convergence in our computational tests, namely polynomial rates (from half order to fourth order) and even exponential convergence with respect to the training dataset size.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/chris/Zotero/storage/HA3GWFM3/Lu et al. - 2021 - DeepONet Learning nonlinear operators for identifying differential equations based on the universal.pdf}
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-10-23},
    doi = {10.48550/arXiv:1412.6980},
	journal = {arXiv preprint},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	year = {2017},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{sharma_bayesian_2023,
	title = {Do {Bayesian} {Neural} {Networks} {Need} {To} {Be} {Fully} {Stochastic}?},
	abstract = {We investigate the benefit of treating all the parameters in a Bayesian neural network stochastically and find compelling theoretical and empirical evidence that this standard construction may be unnecessary. To this end, we prove that expressive predictive distributions require only small amounts of stochasticity. In particular, partially stochastic networks with only n stochastic biases are universal probabilistic predictors for n-dimensional predictive problems. In empirical investigations, we find no systematic benefit of full stochasticity across four different inference modalities and eight datasets; partially stochastic networks can match and sometimes even outperform fully stochastic networks, despite their reduced memory costs.},
	language = {en},
	urldate = {2024-10-23},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Sharma, Mrinank and Farquhar, Sebastian and Nalisnick, Eric and Rainforth, Tom},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {7694--7722},
}

@article{hersbach_era5_2020,
	title = {The {ERA5} global reanalysis},
	volume = {146},
	issn = {0035-9009, 1477-870X},
	doi = {10.1002/qj.3803},
	abstract = {Abstract
            Within the Copernicus Climate Change Service (C3S), ECMWF is producing the ERA5 reanalysis which, once completed, will embody a detailed record of the global atmosphere, land surface and ocean waves from 1950 onwards. This new reanalysis replaces the ERA‐Interim reanalysis (spanning 1979 onwards) which was started in 2006. ERA5 is based on the Integrated Forecasting System (IFS) Cy41r2 which was operational in 2016. ERA5 thus benefits from a decade of developments in model physics, core dynamics and data assimilation. In addition to a significantly enhanced horizontal resolution of 31 km, compared to 80 km for ERA‐Interim, ERA5 has hourly output throughout, and an uncertainty estimate from an ensemble (3‐hourly at half the horizontal resolution). This paper describes the general set‐up of ERA5, as well as a basic evaluation of characteristics and performance, with a focus on the dataset from 1979 onwards which is currently publicly available. Re‐forecasts from ERA5 analyses show a gain of up to one day in skill with respect to ERA‐Interim. Comparison with radiosonde and PILOT data prior to assimilation shows an improved fit for temperature, wind and humidity in the troposphere, but not the stratosphere. A comparison with independent buoy data shows a much improved fit for ocean wave height. The uncertainty estimate reflects the evolution of the observing systems used in ERA5. The enhanced temporal and spatial resolution allows for a detailed evolution of weather systems. For precipitation, global‐mean correlation with monthly‐mean GPCP data is increased from 67\% to 77\%. In general, low‐frequency variability is found to be well represented and from 10 hPa downwards general patterns of anomalies in temperature match those from the ERA‐Interim, MERRA‐2 and JRA‐55 reanalyses.},
	language = {en},
	number = {730},
	urldate = {2024-09-19},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Hersbach, Hans and Bell, Bill and Berrisford, Paul and Hirahara, Shoji and Horányi, András and Muñoz‐Sabater, Joaquín and Nicolas, Julien and Peubey, Carole and Radu, Raluca and Schepers, Dinand and Simmons, Adrian and Soci, Cornel and Abdalla, Saleh and Abellan, Xavier and Balsamo, Gianpaolo and Bechtold, Peter and Biavati, Gionata and Bidlot, Jean and Bonavita, Massimo and De Chiara, Giovanna and Dahlgren, Per and Dee, Dick and Diamantakis, Michail and Dragani, Rossana and Flemming, Johannes and Forbes, Richard and Fuentes, Manuel and Geer, Alan and Haimberger, Leo and Healy, Sean and Hogan, Robin J. and Hólm, Elías and Janisková, Marta and Keeley, Sarah and Laloyaux, Patrick and Lopez, Philippe and Lupu, Cristina and Radnoti, Gabor and De Rosnay, Patricia and Rozum, Iryna and Vamborg, Freja and Villaume, Sebastien and Thépaut, Jean‐Noël},
	month = jul,
	year = {2020},
	pages = {1999--2049},
}

@article{zwicker_py-pde_2020,
	title = {py-pde: {A} {Python} package for solving partial differential equations},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2475-9066},
	shorttitle = {py-pde},
	doi = {10.21105/joss.02158},
	number = {48},
	urldate = {2024-09-19},
	journal = {Journal of Open Source Software},
	author = {Zwicker, David},
	month = apr,
	year = {2020},
	pages = {2158},
}

@misc{pic_proper_2024,
	title = {Proper {Scoring} {Rules} for {Multivariate} {Probabilistic} {Forecasts} based on {Aggregation} and {Transformation}},
	url = {http://arxiv.org/abs/2407.00650},
	abstract = {Proper scoring rules are an essential tool to assess the predictive performance of probabilistic forecasts. However, propriety alone does not ensure an informative characterization of predictive performance and it is recommended to compare forecasts using multiple scoring rules. With that in mind, interpretable scoring rules providing complementary information are necessary. We formalize a framework based on aggregation and transformation to build interpretable multivariate proper scoring rules. Aggregationand-transformation-based scoring rules are able to target specific features of the probabilistic forecasts; which improves the characterization of the predictive performance. This framework is illustrated through examples taken from the literature and studied using numerical experiments showcasing its benefits. In particular, it is shown that it can help bridge the gap between proper scoring rules and spatial verification tools.},
	language = {en},
	urldate = {2024-08-16},
	publisher = {arXiv},
	author = {Pic, Romain and Dombry, Clément and Naveau, Philippe and Taillardat, Maxime},
	month = jun,
	year = {2024},
	note = {arXiv:2407.00650 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Applications, Statistics - Methodology},
}

@inproceedings{bonev_spherical_2023,
	address = {Honolulu, Hawaii, USA},
	series = {{ICML}'23},
	title = {Spherical {Fourier} neural operators: learning stable dynamics on the sphere},
	volume = {202},
	shorttitle = {Spherical {Fourier} neural operators},
	abstract = {Fourier Neural Operators (FNOs) have proven to be an efficient and effective method for resolution-independent operator learning in a broad variety of application areas across scientific machine learning. A key reason for their success is their ability to accurately model long-range dependencies in spatio-temporal data by learning global convolutions in a computationally efficient manner. To this end, FNOs rely on the discrete Fourier transform (DFT), however, DFTs cause visual and spectral artifacts as well as pronounced dissipation when learning operators in spherical coordinates by incorrectly assuming flat geometry. To overcome this limitation, we generalize FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators on spherical geometries. We apply SFNOs to forecasting atmospheric dynamics, and demonstrate stable autoregressive rollouts for a year of simulated time (1,460 steps), while retaining physically plausible dynamics. The SFNO has important implications for machine learning-based simulation of climate dynamics that could eventually help accelerate our response to climate change.},
	urldate = {2024-09-09},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Bonev, Boris and Kurth, Thorsten and Hundt, Christian and Pathak, Jaideep and Baust, Maximilian and Kashinath, Karthik and Anandkumar, Anima},
	month = jul,
	year = {2023},
	pages = {2806--2823},
}

@article{rasp_weatherbench_2024,
author = {Rasp, Stephan and Hoyer, Stephan and Merose, Alexander and Langmore, Ian and Battaglia, Peter and Russell, Tyler and Sanchez-Gonzalez, Alvaro and Yang, Vivian and Carver, Rob and Agrawal, Shreya and Chantry, Matthew and Ben Bouallegue, Zied and Dueben, Peter and Bromberg, Carla and Sisk, Jared and Barrington, Luke and Bell, Aaron and Sha, Fei},
title = {WeatherBench 2: A Benchmark for the Next Generation of Data-Driven Global Weather Models},
journal = {Journal of Advances in Modeling Earth Systems},
volume = {16},
number = {6},
keywords = {machine learning, NWP, artificial intelligence, benchmark},
doi = {https://doi.org/10.1029/2023MS004019},
abstract = {Abstract WeatherBench 2 is an update to the global, medium-range (1–14 days) weather forecasting benchmark proposed by (Rasp et al., 2020, https://doi.org/10.1029/2020ms002203), designed with the aim to accelerate progress in data-driven weather modeling. WeatherBench 2 consists of an open-source evaluation framework, publicly available training, ground truth and baseline data as well as a continuously updated website with the latest metrics and state-of-the-art models: https://sites.research.google/weatherbench. This paper describes the design principles of the evaluation framework and presents results for current state-of-the-art physical and data-driven weather models. The metrics are based on established practices for evaluating weather forecasts at leading operational weather centers. We define a set of headline scores to provide an overview of model performance. In addition, we also discuss caveats in the current evaluation setup and challenges for the future of data-driven weather forecasting.},
year = {2024}
}


@article{price_gencast_2024,
  title = {Probabilistic Weather Forecasting with Machine Learning},
  author = {Price, Ilan and {Sanchez-Gonzalez}, Alvaro and Alet, Ferran and Andersson, Tom R. and {El-Kadi}, Andrew and Masters, Dominic and Ewalds, Timo and Stott, Jacklynn and Mohamed, Shakir and Battaglia, Peter and Lam, Remi and Willson, Matthew},
  year = {2025},
  month = jan,
  journal = {Nature},
  volume = {637},
  number = {8044},
  pages = {84--90},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-08252-9},
  urldate = {2025-03-25},
  abstract = {Weather forecasts are fundamentally uncertain, so predicting the range of probable weather scenarios is crucial for important decisions, from warning the public about hazardous weather to planning renewable energy use. Traditionally, weather forecasts have been based on numerical weather prediction (NWP)1, which relies on physics-based simulations of the atmosphere. Recent advances in machine learning (ML)-based weather prediction (MLWP) have produced ML-based models with less forecast error than single NWP simulations2,3. However, these advances have focused primarily on single, deterministic forecasts that fail to represent uncertainty and estimate risk. Overall, MLWP has remained less accurate and reliable than state-of-the-art NWP ensemble forecasts. Here we introduce GenCast, a probabilistic weather model with greater skill and speed than the top operational medium-range weather forecast in the world, ENS, the ensemble forecast of the European Centre for Medium-Range Weather~Forecasts4. GenCast is an ML weather prediction method, trained on decades of reanalysis data. GenCast generates an ensemble of stochastic 15-day global forecasts, at 12-h steps and 0.25{$^\circ$} latitude--longitude resolution, for more than 80 surface and atmospheric variables, in 8\,min. It has greater skill than ENS on 97.2\% of 1,320 targets we evaluated and better predicts extreme weather, tropical cyclone tracks and wind power production. This work helps open the next chapter in operational weather forecasting, in which crucial weather-dependent decisions are made more accurately and efficiently.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Atmospheric dynamics,Computer science,Natural hazards},
  file = {/home/chris/Zotero/storage/7WTCXGAK/Price et al. - 2025 - Probabilistic weather forecasting with machine learning.pdf}
}


@article{tripura_wavelet_2023,
	title = {Wavelet {Neural} {Operator} for solving parametric partial differential equations in computational mechanics problems},
	volume = {404},
	issn = {0045-7825},
	doi = {10.1016/j.cma.2022.115783},
	abstract = {With massive advancements in sensor technologies and Internet-of-things (IoT), we now have access to terabytes of historical data; however, there is a lack of clarity on how to best exploit the data to predict future events. One possible alternative in this context is to utilize an operator learning algorithm that directly learns the nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator (WNO) that blends integral kernel with wavelet transformation. WNO harnesses the superiority of the wavelets in time–frequency localization of the functions and enables accurate tracking of patterns in the spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, WNO can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed WNO are illustrated on a wide array of problems involving Burger’s equation, Darcy flow, Navier–Stokes equation, Allen–Cahn equation, and Wave advection equation. A comparative study with respect to existing operator learning frameworks is presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth’s air temperature based on available historical data.},
	urldate = {2024-08-07},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Tripura, Tapas and Chakraborty, Souvik},
	month = feb,
	year = {2023},
	keywords = {Nonlinear mappings, Operator learning, Scientific machine learning, Wavelet, Wavelet neural operator},
	pages = {115783},
}

@article{scheuerer_variogram-based_2015,
	title = {Variogram-{Based} {Proper} {Scoring} {Rules} for {Probabilistic} {Forecasts} of {Multivariate} {Quantities}},
	volume = {143},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/143/4/mwr-d-14-00269.1.xml},
	doi = {10.1175/MWR-D-14-00269.1},
	abstract = {Proper scoring rules provide a theoretically principled framework for the quantitative assessment of the predictive performance of probabilistic forecasts. While a wide selection of such scoring rules for univariate quantities exists, there are only few scoring rules for multivariate quantities, and many of them require that forecasts are given in the form of a probability density function. The energy score, a multivariate generalization of the continuous ranked probability score, is the only commonly used score that is applicable in the important case of ensemble forecasts, where the multivariate predictive distribution is represented by a finite sample. Unfortunately, its ability to detect incorrectly specified correlations between the components of the multivariate quantity is somewhat limited. In this paper the authors present an alternative class of proper scoring rules based on the geostatistical concept of variograms. The sensitivity of these variogram-based scoring rules to incorrectly predicted means, variances, and correlations is studied in a number of examples with simulated observations and forecasts; they are shown to be distinctly more discriminative with respect to the correlation structure. This conclusion is confirmed in a case study with postprocessed wind speed forecasts at five wind park locations in Colorado.},
	language = {EN},
	number = {4},
	urldate = {2024-08-01},
	journal = {Monthly Weather Review},
	author = {Scheuerer, Michael and Hamill, Thomas M.},
	month = apr,
	year = {2015},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	keywords = {Ensembles, Forecast verification/skill},
	pages = {1321--1334},
}

@book{berg_harmonic_1984,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Harmonic {Analysis} on {Semigroups}},
	volume = {100},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4612-7017-1 978-1-4612-1128-0},
	publisher = {Springer},
	author = {Berg, Christian and Christensen, Jens Peter Reus and Ressel, Paul},
	year = {1984},
	doi = {10.1007/978-1-4612-1128-0},
	keywords = {Fourier transform, Functions, Halbgruppe, Harmonische Analyse, Hilbert space, Positiv definite Funktion, Vector space, harmonic analysis},
}

@article{wynne_kernel_2022,
	title = {A {Kernel} {Two}-{Sample} {Test} for {Functional} {Data}},
	volume = {23},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v23/20-1180.html},
	abstract = {We propose a nonparametric two-sample test procedure based on Maximum Mean Discrepancy (MMD) for testing the hypothesis that two samples of functions have the same underlying distribution, using kernels defined on function spaces. This construction is motivated by a scaling analysis of the efficiency of MMD-based tests for datasets of increasing dimension. Theoretical properties of kernels on function spaces and their associated MMD are established and employed to ascertain the efficacy of the newly proposed test, as well as to assess the effects of using functional reconstructions based on discretised function samples. The theoretical results are demonstrated over a range of synthetic and real world datasets.},
	number = {73},
	urldate = {2024-07-26},
	journal = {Journal of Machine Learning Research},
	author = {Wynne, George and Duncan, Andrew B.},
	year = {2022},
	pages = {1--51},
}

@article{magnani_linearization_2024,
	title = {Linearization {Turns} {Neural} {Operators} into {Function}-{Valued} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2406.05072},
	abstract = {Modeling dynamical systems, e.g. in climate and engineering sciences, often necessitates solving partial differential equations. Neural operators are deep neural networks designed to learn nontrivial solution operators of such differential equations from data. As for all statistical models, the predictions of these models are imperfect and exhibit errors. Such errors are particularly difficult to spot in the complex nonlinear behaviour of dynamical systems. We introduce a new framework for approximate Bayesian uncertainty quantification in neural operators using functionvalued Gaussian processes. Our approach can be interpreted as a probabilistic analogue of the concept of currying from functional programming and provides a practical yet theoretically sound way to apply the linearized Laplace approximation to neural operators. In a case study on Fourier neural operators, we show that, even for a discretized input, our method yields a Gaussian closure–a structured Gaussian process posterior capturing the uncertainty in the output function of the neural operator, which can be evaluated at an arbitrary set of points. The method adds minimal prediction overhead, can be applied post-hoc without retraining the neural operator, and scales to large models and datasets. We showcase the efficacy of our approach through applications to different types of partial differential equations.},
	language = {en},
	urldate = {2024-07-25},
	journal = {arXiv preprint},
    doi = {10.48550/arXiv:2406.05072},
	author = {Magnani, Emilia and Pförtner, Marvin and Weber, Tobias and Hennig, Philipp},
	year = {2024},
	keywords = {Computer Science - Machine Learning, G.1.0, I.2.6, G.3, G.1.8, Statistics - Machine Learning},
}

@inproceedings{botev_practical_2017,
	title = {Practical {Gauss}-{Newton} {Optimisation} for {Deep} {Learning}},
	abstract = {We present an efficient block-diagonal approximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is competitive against state-of-the-art first-order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a laborious process, our approach can provide good performance even when used with default settings. A side result of our work is that for piecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
	language = {en},
	urldate = {2024-07-25},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {557--565},
}

@article{rasp_neural_2018,
	title = {Neural {Networks} for {Postprocessing} {Ensemble} {Weather} {Forecasts}},
	volume = {146},
	issn = {0027-0644, 1520-0493},
	doi = {10.1175/MWR-D-18-0187.1},
	abstract = {Abstract
            Ensemble weather predictions require statistical postprocessing of systematic errors to obtain reliable and accurate probabilistic forecasts. Traditionally, this is accomplished with distributional regression models in which the parameters of a predictive distribution are estimated from a training period. We propose a flexible alternative based on neural networks that can incorporate nonlinear relationships between arbitrary predictor variables and forecast distribution parameters that are automatically learned in a data-driven way rather than requiring prespecified link functions. In a case study of 2-m temperature forecasts at surface stations in Germany, the neural network approach significantly outperforms benchmark postprocessing methods while being computationally more affordable. Key components to this improvement are the use of auxiliary predictor variables and station-specific information with the help of embeddings. Furthermore, the trained neural network can be used to gain insight into the importance of meteorological variables, thereby challenging the notion of neural networks as uninterpretable black boxes. Our approach can easily be extended to other statistical postprocessing and forecasting problems. We anticipate that recent advances in deep learning combined with the ever-increasing amounts of model and observation data will transform the postprocessing of numerical weather forecasts in the coming decade.},
	number = {11},
	urldate = {2024-07-25},
	journal = {Monthly Weather Review},
	author = {Rasp, Stephan and Lerch, Sebastian},
	month = nov,
	year = {2018},
	pages = {3885--3900},
}

@article{taillardat_calibrated_2016,
	title = {Calibrated {Ensemble} {Forecasts} {Using} {Quantile} {Regression} {Forests} and {Ensemble} {Model} {Output} {Statistics}},
	volume = {144},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/144/6/mwr-d-15-0260.1.xml},
	doi = {10.1175/MWR-D-15-0260.1},
	abstract = {Ensembles used for probabilistic weather forecasting tend to be biased and underdispersive. This paper proposes a statistical method for postprocessing ensembles based on quantile regression forests (QRF), a generalization of random forests for quantile regression. This method does not fit a parametric probability density function (PDF) like in ensemble model output statistics (EMOS) but provides an estimation of desired quantiles. This is a nonparametric approach that eliminates any assumption on the variable subject to calibration. This method can estimate quantiles using not only members of the ensemble but any predictor available including statistics on other variables. The method is applied to the Météo-France 35-member ensemble forecast (PEARP) for surface temperature and wind speed for available lead times from 3 up to 54 h and compared to EMOS. All postprocessed ensembles are much better calibrated than the PEARP raw ensemble and experiments on real data also show that QRF performs better than EMOS, and can bring a real gain for human forecasters compared to EMOS. QRF provides sharp and reliable probabilistic forecasts. At last, classical scoring rules to verify predictive forecasts are completed by the introduction of entropy as a general measure of reliability.},
	language = {EN},
	number = {6},
	urldate = {2024-07-25},
	journal = {Monthly Weather Review},
	author = {Taillardat, Maxime and Mestre, Olivier and Zamo, Michaël and Naveau, Philippe},
	month = jun,
	year = {2016},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	keywords = {Ensembles, Forecast verification/skill, Forecasting, Mathematical and statistical techniques, Model output statistics, Models and modeling, Probability forecasts/models/distribution, Statistical techniques, Statistics},
	pages = {2375--2393},
}

@article{walz_easy_2024,
	title = {Easy {Uncertainty} {Quantification} ({EasyUQ}): {Generating} {Predictive} {Distributions} from {Single}-{Valued} {Model} {Output}},
	volume = {66},
	issn = {0036-1445},
	shorttitle = {Easy {Uncertainty} {Quantification} ({EasyUQ})},
	doi = {10.1137/22M1541915},
	abstract = {Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library (ŭlhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs.},
	number = {1},
	urldate = {2024-07-25},
	journal = {SIAM Review},
	author = {Walz, Eva-Maria and Henzi, Alexander and Ziegel, Johanna and Gneiting, Tilmann},
	month = feb,
	year = {2024},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {91--122},
}

@article{bi_accurate_2023,
	title = {Accurate medium-range global weather forecasting with {3D} neural networks},
	volume = {619},
	copyright = {2023 The Author(s)},
	issn = {1476-4687},
	doi = {10.1038/s41586-023-06185-3},
	abstract = {Weather forecasting is important for science and society. At present, the most accurate forecast system is the numerical weather prediction (NWP) method, which represents atmospheric states as discretized grids and numerically solves partial differential equations that describe the transition between those states1. However, this procedure is computationally expensive. Recently, artificial-intelligence-based methods2 have shown potential in accelerating weather forecasting by orders of magnitude, but the forecast accuracy is still significantly lower than that of NWP methods. Here we introduce an artificial-intelligence-based method for accurate, medium-range global weather forecasting. We show that three-dimensional deep networks equipped with Earth-specific priors are effective at dealing with complex patterns in weather data, and that a hierarchical temporal aggregation strategy reduces accumulation errors in medium-range forecasting. Trained on 39 years of global data, our program, Pangu-Weather, obtains stronger deterministic forecast results on reanalysis data in all tested variables when compared with the world’s best NWP system, the operational integrated forecasting system of the European Centre for Medium-Range Weather Forecasts (ECMWF)3. Our method also works well with extreme weather forecasts and ensemble forecasts. When initialized with reanalysis data, the accuracy of tracking tropical cyclones is also higher than that of ECMWF-HRES.},
	language = {en},
	number = {7970},
	urldate = {2024-07-25},
	journal = {Nature},
	author = {Bi, Kaifeng and Xie, Lingxi and Zhang, Hengheng and Chen, Xin and Gu, Xiaotao and Tian, Qi},
	month = jul,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Atmospheric dynamics, Computer science},
	pages = {533--538},
}

@article{abdar_review_2021,
	title = {A review of uncertainty quantification in deep learning: {Techniques}, applications and challenges},
	volume = {76},
	issn = {1566-2535},
	shorttitle = {A review of uncertainty quantification in deep learning},
	doi = {10.1016/j.inffus.2021.05.008},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.},
	urldate = {2024-07-25},
	journal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	month = dec,
	year = {2021},
	keywords = {Artificial intelligence, Bayesian statistics, Deep learning, Ensemble learning, Machine learning, Uncertainty quantification},
	pages = {243--297},
}

@misc{ayzel_rydl_2020,
	title = {{RYDL}: the sample data of the {RY} product for deep learning applications},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	shorttitle = {{RYDL}},
	url = {https://zenodo.org/record/3629950},
	doi = {10.5281/ZENODO.3629950},
	abstract = {RYDL is the sample data of the RY product provided by the German Weather Service (DWD). This data has been utilized for training the RainNet model -- a deep convolutional neural network for radar-based precipitation nowcasting. You can find the latest RY product data on DWD open data repository: https://opendata.dwd.de/weather/radar/. The RainNet model is available on GitHub: https://github.com/hydrogo/rainnet.},
	urldate = {2024-07-22},
	publisher = {Zenodo},
	author = {Ayzel, Georgy},
	month = jan,
	year = {2020},
	keywords = {radar, RY, Germany, DWD, RainNet, deep learning},
}

@article{ayzel_rainnet_2020,
	title = {{RainNet} v1.0: a convolutional neural network for radar-based precipitation nowcasting},
	volume = {13},
	issn = {1991-959X},
	shorttitle = {{RainNet} v1.0},
	url = {https://gmd.copernicus.org/articles/13/2631/2020/},
	doi = {10.5194/gmd-13-2631-2020},
	abstract = {In this study, we present RainNet, a deep convolutional neural network for radar-based precipitation nowcasting. Its design was inspired by the U-Net and SegNet families of deep learning models, which were originally designed for binary segmentation tasks. RainNet was trained to predict continuous precipitation intensities at a lead time of 5\&thinsp;min, using several years of quality-controlled weather radar composites provided by the German Weather Service (DWD). That data set covers Germany with a spatial domain of 900 km×900\&thinsp;km and has a resolution of 1\&thinsp;km in space and 5\&thinsp;min in time. Independent verification experiments were carried out on 11 summer precipitation events from 2016 to 2017. In order to achieve a lead time of 1\&thinsp;h, a recursive approach was implemented by using RainNet predictions at 5\&thinsp;min lead times as model inputs for longer lead times. In the verification experiments, trivial Eulerian persistence and a conventional model based on optical flow served as benchmarks. The latter is available in the rainymotion library and had previously been shown to outperform DWD's operational nowcasting model for the same set of verification events.

 RainNet significantly outperforms the benchmark models at all lead times up to 60\&thinsp;min for the routine verification metrics mean absolute error (MAE) and the critical success index (CSI) at intensity thresholds of 0.125, 1, and 5\&thinsp;mm h−1. However, rainymotion turned out to be superior in predicting the exceedance of higher intensity thresholds (here 10 and 15\&thinsp;mm h−1). The limited ability of RainNet to predict heavy rainfall intensities is an undesirable property which we attribute to a high level of spatial smoothing introduced by the model. At a lead time of 5\&thinsp;min, an analysis of power spectral density confirmed a significant loss of spectral power at length scales of 16\&thinsp;km and below. Obviously, RainNet had learned an optimal level of smoothing to produce a nowcast at 5\&thinsp;min lead time. In that sense, the loss of spectral power at small scales is informative, too, as it reflects the limits of predictability as a function of spatial scale. Beyond the lead time of 5\&thinsp;min, however, the increasing level of smoothing is a mere artifact – an analogue to numerical diffusion – that is not a property of RainNet itself but of its recursive application. In the context of early warning, the smoothing is particularly unfavorable since pronounced features of intense precipitation tend to get lost over longer lead times. Hence, we propose several options to address this issue in prospective research, including an adjustment of the loss function for model training, model training for longer lead times, and the prediction of threshold exceedance in terms of a binary segmentation task. Furthermore, we suggest additional input data that could help to better identify situations with imminent precipitation dynamics. The model code, pretrained weights, and training data are provided in open repositories as an input for such future studies.},
	language = {English},
	number = {6},
	urldate = {2024-07-22},
	journal = {Geoscientific Model Development},
	author = {Ayzel, Georgy and Scheffer, Tobias and Heistermann, Maik},
	month = jun,
	year = {2020},
	note = {Publisher: Copernicus GmbH},
	pages = {2631--2644},
}

@misc{fanaskov_spectral_2024,
	title = {Spectral {Neural} {Operators}},
	url = {http://arxiv.org/abs/2205.10573},
	abstract = {A plentitude of applications in scientific computing requires the approximation of mappings between Banach spaces. Recently introduced Fourier Neural Operator (FNO) and Deep Operator Network (DeepONet) can provide this functionality. For both of these neural operators, the input function is sampled on a given grid (uniform for FNO), and the output function is parametrized by a neural network. We argue that this parametrization leads to 1) opaque output that is hard to analyze and 2) systematic bias caused by aliasing errors in the case of FNO. The alternative, advocated in this article, is to use Chebyshev and Fourier series for both domain and codomain. The resulting Spectral Neural Operator (SNO) has transparent output, never suffers from aliasing, and may include many exact (lossless) operations on functions. The functionality is based on well-developed fast, and stable algorithms from spectral methods. The implementation requires only standard numerical linear algebra. Our benchmarks show that for many operators, SNO is superior to FNO and DeepONet.},
	language = {en},
	urldate = {2024-07-22},
	publisher = {arXiv},
	author = {Fanaskov, V. and Oseledets, I.},
	month = apr,
	year = {2024},
	note = {arXiv:2205.10573 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}


@article{bulte_uncertainty_2024,
      author = "Christopher Bülte and Nina Horat and Julian Quinting and Sebastian Lerch",
      title = "Uncertainty quantification for data-driven weather models",
      journal = "Artificial Intelligence for the Earth Systems",
      year = "2025",
      publisher = "American Meteorological Society",
      address = "Boston MA, USA",
      doi = "10.1175/AIES-D-24-0049.1",
      url = "https://journals.ametsoc.org/view/journals/aies/aop/AIES-D-24-0049.1/AIES-D-24-0049.1.xml"
}

@misc{bulte_estimation_2024,
	title = {Estimation of spatio-temporal extremes via generative neural networks},
	url = {http://arxiv.org/abs/2407.08668},
	abstract = {Recent methods in modeling spatial extreme events have focused on utilizing parametric max-stable processes and their underlying dependence structure. In this work, we provide a unified approach for analyzing spatial extremes with little available data by estimating the distribution of model parameters or the spatial dependence directly. By employing recent developments in generative neural networks we predict a full sample-based distribution, allowing for direct assessment of uncertainty regarding model parameters or other parameter dependent functionals. We validate our method by fitting several simulated max-stable processes, showing a high accuracy of the approach, regarding parameter estimation, as well as uncertainty quantification. Additional robustness checks highlight the generalization and extrapolation capabilities of the model, while an application to precipitation extremes across Western Germany demonstrates the usability of our approach in real-world scenarios.},
	urldate = {2024-07-13},
	publisher = {arXiv},
	author = {Bülte, Christopher and Leimenstoll, Lisa and Schienle, Melanie},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08668 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_solving_2023,
	title = {Solving {Seismic} {Wave} {Equations} on {Variable} {Velocity} {Models} {With} {Fourier} {Neural} {Operator}},
	volume = {61},
	issn = {1558-0644},
	doi = {10.1109/TGRS.2023.3333663},
	abstract = {In the study of subsurface seismic imaging, solving the acoustic wave equation is a pivotal component in existing models. The advancement of deep learning (DL) enables solving partial differential equations (PDEs), including wave equations, by applying neural networks to identify the mapping between the inputs and the solution. This approach can be faster than traditional numerical methods when numerous instances are to be solved. Previous works that concentrate on solving the wave equation by neural networks consider either a single velocity model or multiple simple velocity models, which is restricted in practice. Instead, inspired by the idea of operator learning, this work leverages the Fourier neural operator (FNO) to effectively learn the frequency domain seismic wavefields under the context of variable velocity models. We also propose a new framework paralleled FNO (PFNO) for efficiently training the FNO-based solver given multiple source locations and frequencies. Numerical experiments demonstrate the high accuracy of both FNO and PFNO with complicated velocity models in the OpenFWI datasets. Furthermore, the cross-dataset generalization test verifies that PFNO adapts to out-of-distribution velocity models. Finally, PFNO admits higher computational efficiency on large-scale testing datasets than the traditional finite-difference method. The aforementioned advantages endow the FNO-based solver with the potential to build powerful models for research on seismic waves.},
	urldate = {2024-07-22},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Li, Bian and Wang, Hanchen and Feng, Shihang and Yang, Xiu and Lin, Youzuo},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Computational modeling, Data models, Fourier neural operator (FNO), Frequency-domain analysis, Helmholtz equation, Mathematical models, Numerical models, Propagation, Training, operator learning, seismic wave},
	pages = {1--18},
}

@article{renn_forecasting_2023,
	title = {Forecasting subcritical cylinder wakes with {Fourier} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2301.08290},
	abstract = {We apply Fourier neural operators (FNOs), a state-of-the-art operator learning technique, to forecast the temporal evolution of experimentally measured velocity fields. FNOs are a recently developed machine learning method capable of approximating solution operators to systems of partial differential equations through data alone. The learned FNO solution operator can be evaluated in milliseconds, potentially enabling faster-than-real-time modeling for predictive flow control in physical systems. Here we use FNOs to predict how physical fluid flows evolve in time, training with particle image velocimetry measurements depicting cylinder wakes in the subcritical vortex shedding regime. We train separate FNOs at Reynolds numbers ranging from Re = 240 to Re = 3060 and study how increasingly turbulent flow phenomena impact prediction accuracy. We focus here on a short prediction horizon of ten non-dimensionalized time-steps, as would be relevant for problems of predictive flow control. We find that FNOs are capable of accurately predicting the evolution of experimental velocity fields throughout the range of Reynolds numbers tested (L2 norm error {\textless} 0.1) despite being provided with limited and imperfect flow observations. Given these results, we conclude that this method holds significant potential for real-time predictive flow control of physical systems.},
	language = {en},
	urldate = {2024-07-22},
	journal = {arXiv preprint},
    doi = {10.48550/arXiv:2301.08290},
	author = {Renn, Peter I. and Wang, Cong and Lale, Sahin and Li, Zongyi and Anandkumar, Anima and Gharib, Morteza},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Physics - Fluid Dynamics},
}

@article{bartolucci_are_nodate,
	title = {Are {Neural} {Operators} {Really} {Neural} {Operators}? {Frame} {Theory} {Meets} {Operator} {Learning}},
	abstract = {Recently, there has been signiﬁcant interest in operator learning, i.e. learning mappings between inﬁnite-dimensional function spaces. This has been particularly relevant in the context of learning partial differential equations from data. However, it has been observed that proposed models may not behave as operators when implemented on a computer, questioning the very essence of what operator learning should be. We contend that in addition to deﬁning the operator at the continuous level, some form of continuous-discrete equivalence is necessary for an architecture to genuinely learn the underlying operator, rather than just discretizations of it. To this end, we propose to employ frames, a concept in applied harmonic analysis and signal processing that gives rise to exact and stable discrete representations of continuous signals. Extending these concepts to operators, we introduce a unifying mathematical framework of Representation equivalent Neural Operator (ReNO) to ensure operations at the continuous and discrete level are equivalent. Lack of this equivalence is quantiﬁed in terms of aliasing errors. We analyze various existing operator learning architectures to determine whether they fall within this framework, and highlight implications when they fail to do so.},
	language = {en},
	author = {Bartolucci, Francesca and de Bézenac, Emmanuel and Raonic, Bogdan and Molinaro, Roberto and Mishra, Siddhartha and Alaifari, Rima},
}

@misc{li_geometry-informed_2023,
	title = {Geometry-{Informed} {Neural} {Operator} for {Large}-{Scale} {3D} {PDEs}},
	url = {http://arxiv.org/abs/2309.00583},
	abstract = {We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function (SDF) and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretizations of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a 26, 000× speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches.},
	language = {en},
	urldate = {2024-07-22},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola Borislavov and Choy, Chris and Li, Boyi and Kossaifi, Jean and Otta, Shourya Prakash and Nabian, Mohammad Amin and Stadler, Maximilian and Hundt, Christian and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	month = sep,
	year = {2023},
	note = {arXiv:2309.00583 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{gopakumar_plasma_2024,
	title = {Plasma surrogate modelling using {Fourier} neural operators},
	volume = {64},
	issn = {0029-5515},
	doi = {10.1088/1741-4326/ad313a},
	abstract = {Predicting plasma evolution within a Tokamak reactor is crucial to realizing the goal of sustainable fusion. Capabilities in forecasting the spatio-temporal evolution of plasma rapidly and accurately allow us to quickly iterate over design and control strategies on current Tokamak devices and future reactors. Modelling plasma evolution using numerical solvers is often expensive, consuming many hours on supercomputers, and hence, we need alternative inexpensive surrogate models. We demonstrate accurate predictions of plasma evolution both in simulation and experimental domains using deep learning-based surrogate modelling tools, viz., Fourier neural operators (FNO). We show that FNO has a speedup of six orders of magnitude over traditional solvers in predicting the plasma dynamics simulated from magnetohydrodynamic models, while maintaining a high accuracy (Mean Squared Error in the normalised domain ). Our modified version of the FNO is capable of solving multi-variable Partial Differential Equations, and can capture the dependence among the different variables in a single model. FNOs can also predict plasma evolution on real-world experimental data observed by the cameras positioned within the MAST Tokamak, i.e. cameras looking across the central solenoid and the divertor in the Tokamak. We show that FNOs are able to accurately forecast the evolution of plasma and have the potential to be deployed for real-time monitoring. We also illustrate their capability in forecasting the plasma shape, the locations of interactions of the plasma with the central solenoid and the divertor for the full (available) duration of the plasma shot within MAST. The FNO offers a viable alternative for surrogate modelling as it is quick to train and infer, and requires fewer data points, while being able to do zero-shot super-resolution and getting high-fidelity solutions.},
	language = {en},
	number = {5},
	urldate = {2024-07-22},
	journal = {Nuclear Fusion},
	author = {Gopakumar, Vignesh and Pamela, Stanislas and Zanisi, Lorenzo and Li, Zongyi and Gray, Ander and Brennand, Daniel and Bhatia, Nitesh and Stathopoulos, Gregory and Kusner, Matt and Deisenroth, Marc Peter and Anandkumar, Anima and Team, the JOREK and Team, MAST},
	month = apr,
	year = {2024},
	note = {Publisher: IOP Publishing},
	pages = {056025},
}

@article{wen_real-time_2023,
	title = {Real-time high-resolution {CO2} geological storage prediction using nested {Fourier} neural operators},
	volume = {16},
	issn = {1754-5706},
	doi = {10.1039/D2EE04204E},
	abstract = {Carbon capture and storage (CCS) plays an essential role in global decarbonization. Scaling up CCS deployment requires accurate and high-resolution modeling of the storage reservoir pressure buildup and the gaseous plume migration. However, such modeling is very challenging at scale due to the high computational costs of existing numerical methods. This challenge leads to significant uncertainties in evaluating storage opportunities, which can delay the pace of large-scale CCS deployment. We introduce Nested Fourier Neural Operator (FNO), a machine-learning framework for high-resolution dynamic 3D CO2 storage modeling at a basin scale. Nested FNO produces forecasts at different refinement levels using a hierarchy of FNOs and speeds up flow prediction nearly 700 000 times compared to existing methods. By learning the solution operator for the family of governing partial differential equations, Nested FNO creates a general-purpose numerical simulator alternative for CO2 storage with diverse reservoir conditions, geological heterogeneity, and injection schemes. Our framework enables unprecedented real-time modeling and probabilistic simulations that can support the scale-up of global CCS deployment.},
	language = {en},
	number = {4},
	urldate = {2024-07-22},
	journal = {Energy \& Environmental Science},
	author = {Wen, Gege and Li, Zongyi and Long, Qirui and Azizzadenesheli, Kamyar and Anandkumar, Anima and Benson, Sally M.},
	month = apr,
	year = {2023},
	note = {Publisher: The Royal Society of Chemistry},
	pages = {1732--1741},
}

@inproceedings{pathak_fourcastnet_2022,
author = {Kurth, Thorsten and Subramanian, Shashank and Harrington, Peter and Pathak, Jaideep and Mardani, Morteza and Hall, David and Miele, Andrea and Kashinath, Karthik and Anandkumar, Anima},
title = {FourCastNet: Accelerating Global High-Resolution Weather Forecasting Using Adaptive Fourier Neural Operators},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592979.3593412},
doi = {10.1145/3592979.3593412},
abstract = {Extreme weather amplified by climate change is causing increasingly devastating impacts across the globe. The current use of physics-based numerical weather prediction (NWP) limits accuracy and resolution due to high computational cost and strict time-to-solution limits.We report that a data-driven deep learning Earth system emulator, FourCastNet, can predict global weather and generate medium-range forecasts five orders-of-magnitude faster than NWP while approaching state-of-the-art accuracy. FourCastNet is optimized and scales efficiently on three supercomputing systems: Selene, Perlmutter, and JUWELS Booster up to 3,808 NVIDIA A100 GPUs, attaining 140.8 petaFLOPS in mixed precision (11.9\% of peak at that scale). The time-to-solution for training FourCastNet measured on JUWELS Booster on 3,072 GPUs is 67.4 minutes, resulting in an 80,000 times faster time-to-solution relative to state-of-the-art NWP, in inference.FourCastNet produces accurate instantaneous weather predictions for a week in advance and enables enormous ensembles that could be used to improve predictions of rare weather extremes.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {13},
numpages = {11},
keywords = {climate change, extreme weather, transformer, fourier neural operator, deep learning},
location = {Davos, Switzerland},
series = {PASC '23}
}


@article{JMLR:v24:21-1524,
	title = {Neural operator: {Learning} maps between function spaces with applications to pdes},
	volume = {24},
	url = {http://jmlr.org/papers/v24/21-1524.html},
	number = {89},
	journal = {Journal of Machine Learning Research},
	author = {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	year = {2023},
	pages = {1--97},
}

@inproceedings{weber2024uncertainty,
	title = {Uncertainty quantification for fourier neural operators},
	url = {https://openreview.net/forum?id=knSgoNJcnV},
	booktitle = {{ICLR} 2024 workshop on {AI4DifferentialEquations} in science},
	author = {Weber, Tobias and Magnani, Emilia and Pförtner, Marvin and Hennig, Philipp},
	year = {2024},
}

@inproceedings{NIPS2015_bc731692,
	title = {Variational dropout and the local reparameterization trick},
	volume = {28},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Kingma, Durk P and Salimans, Tim and Welling, Max},
	editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
	year = {2015},
}

@article{rahman2023uno,
	title = {U-{NO}: {U}-shaped neural operators},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=j3oQF9coJd},
	journal = {Transactions on Machine Learning Research},
	author = {Rahman, Md Ashiqur and Ross, Zachary E and Azizzadenesheli, Kamyar},
	year = {2023},
}

@inproceedings{NIPS2017_9ef2ed4b,
	title = {Simple and scalable predictive uncertainty estimation using deep ensembles},
	volume = {30},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@inproceedings{vahidi2024probabilistic,
	title = {Probabilistic self-supervised representation learning via scoring rules minimization},
	url = {https://openreview.net/forum?id=skcTCdJz0f},
	booktitle = {The twelfth international conference on learning representations},
	author = {Vahidi, Amirhossein and Schosser, Simon and Wimmer, Lisa and Li, Yawei and Bischl, Bernd and Hüllermeier, Eyke and Rezaei, Mina},
	year = {2024},
}

@article{JMLR:v25:23-0038,
	title = {Probabilistic forecasting with generative networks via scoring rule minimization},
	volume = {25},
	number = {45},
	journal = {Journal of Machine Learning Research},
	author = {Pacchiardi, Lorenzo and Adewoyin, Rilwan A. and Dueben, Peter and Dutta, Ritabrata},
	year = {2024},
	pages = {1--64},
}

@inproceedings{NEURIPS2022_0a974713,
	title = {{PDEBench}: {An} extensive benchmark for scientific machine learning},
	volume = {35},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Takamoto, Makoto and Praditia, Timothy and Leiteritz, Raphael and MacKinlay, Daniel and Alesiani, Francesco and Pflüger, Dirk and Niepert, Mathias},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {1596--1611},
}

@inproceedings{NEURIPS2021_a7c95857,
	title = {Laplace redux - effortless bayesian deep learning},
	volume = {34},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {20089--20103},
}

@inproceedings{pmlr-v48-gal16,
	address = {New York, New York, USA},
	series = {Proceedings of machine learning research},
	title = {Dropout as a bayesian approximation: {Representing} model uncertainty in deep learning},
	volume = {48},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.},
	booktitle = {Proceedings of the 33rd international conference on machine learning},
	publisher = {PMLR},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	editor = {Balcan, Maria Florina and Weinberger, Kilian Q.},
	month = jun,
	year = {2016},
	pages = {1050--1059},
}

@inproceedings{NEURIPS2019_5103c358,
	title = {Conformalized quantile regression},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf},
	booktitle = {Advances in neural information processing systems},
	publisher = {Curran Associates, Inc.},
	author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and dAlché-Buc, F. and Fox, E. and Garnett, R.},
	year = {2019},
}

@misc{diquigiovanni_importance_2021,
	title = {The {Importance} of {Being} a {Band}: {Finite}-{Sample} {Exact} {Distribution}-{Free} {Prediction} {Sets} for {Functional} {Data}},
	shorttitle = {The {Importance} of {Being} a {Band}},
	url = {http://arxiv.org/abs/2102.06746},
	abstract = {Functional Data Analysis represents a field of growing interest in statistics. Despite several studies have been proposed leading to fundamental results, the problem of obtaining valid and efficient prediction sets has not been thoroughly covered. Indeed, the great majority of methods currently in the literature rely on strong distributional assumptions (e.g, Gaussianity), dimension reduction techniques and/or asymptotic arguments. In this work, we propose a new nonparametric approach in the field of Conformal Prediction based on a new family of nonconformity measures inducing conformal predictors able to create closed-form finite-sample valid or exact prediction sets under very minimal distributional assumptions. In addition, our proposal ensures that the prediction sets obtained are bands, an essential feature in the functional setting that allows the visualization and interpretation of such sets. The procedure is also fast, scalable, does not rely on functional dimension reduction techniques and allows the user to select different nonconformity measures depending on the problem at hand always obtaining valid bands. Within this family of measures, we propose also a specific measure leading to prediction bands asymptotically no less efficient than those with constant width.},
	language = {en},
	urldate = {2024-06-11},
	publisher = {arXiv},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	month = apr,
	year = {2021},
	note = {arXiv:2102.06746 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{angelopoulos_gentle_2022,
	title = {A {Gentle} {Introduction} to {Conformal} {Prediction} and {Distribution}-{Free} {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantiﬁcation to avoid consequential model failures. Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-speciﬁed probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the ﬁelds of computer vision, natural language processing, deep reinforcement learning, and so on.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	month = dec,
	year = {2022},
	note = {arXiv:2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{li_physics-informed_2024,
	title = {Physics-{Informed} {Neural} {Operator} for {Learning} {Partial} {Differential} {Equations}},
	volume = {1},
	doi = {10.1145/3648506},
	abstract = {In this article, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric Partial Differential Equations (PDE). PINO is the first hybrid approach incorporating data and PDE constraints at different resolutions to learn the operator. Specifically, in PINO, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. The resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, that is, being able to predict beyond the resolution of training data. PINO uses the Fourier neural operator (FNO) framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed, while previous approaches, such as the Physics-Informed Neural Network (PINN), fail due to optimization challenges, for example, in multi-scale dynamic systems such as Kolmogorov flows.PROBLEM STATEMENTMachine learning methods have recently shown promise in solving partial differential equations (PDEs) raised in science and engineering. They can be classified into two broad categories: approximating the solution function  and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this paper, we consider a new learning paradigm, aiming to overcome the optimization challenge in PINN and relieve the data requirement in FNO.METHODSIn this paper, we propose physics-informed neural operators (PINO) that combine training data and physics constraints to learn the solution operator of a given family of parametric PDEs.In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family using training data and physics constraints. In the instance-wise fine-tuning phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE using the physics constraints only.Specifically, we combine coarse-resolution training data with PDE constraints imposed at a higher resolution. By adding PDE constraints to FNO at a higher resolution, we obtain a high-fidelity reconstruction of the ground-truth operator.RESULTSThe resulting PINO model can accurately approximate the ground-truth solution operator for many popular PDE families and shows no degradation in accuracy even under zero-shot super-resolution, i.e., being able to predict beyond the resolution of training data.Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. With the equation constraints, PINO requires few to no data to learn the Burgers, Darcy, and Navier-Stokes equation. In particular, PINO accurately solves long temporal transient flows and  Kolmogorov flows where other baseline methods fail to converge.SIGNIFICANCEPINO uses the neural operator framework that is guaranteed to be a universal approximator for any continuous operator and discretization convergent in the limit of mesh refinement. Moreover, PINO succeeds in settings where no training data is available and only PDE constraints are imposed. These advantages could lead to applications such as weather forecast, airfoil designs, and turbulence control.},
	number = {3},
	urldate = {2024-07-16},
	journal = {ACM / IMS J. Data Sci.},
	author = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	month = may,
	year = {2024},
	pages = {9:1--9:27},
}

@article{ziegel_characteristic_2024,
	title = {Characteristic kernels on {Hilbert} spaces, {Banach} spaces, and on sets of measures},
	volume = {30},
	issn = {1350-7265},
	doi = {10.3150/23-BEJ1639},
	abstract = {We present new classes of positive definite kernels on non-standard spaces that are integrally strictly positive definite or characteristic. In particular, we discuss radial kernels on separable Hilbert spaces, and introduce broad classes of kernels on Banach spaces and on metric spaces of strong negative type. The general results are used to give explicit classes of kernels on separable Lp spaces and on sets of measures.},
	number = {2},
	urldate = {2024-07-16},
	journal = {Bernoulli},
	author = {Ziegel, Johanna and Ginsbourger, David and Dümbgen, Lutz},
	month = may,
	year = {2024},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {Characteristic kernel, integrally strictly positive definite kernel, kernel on Banach space, kernel on Hilbert space, kernel on measures},
	pages = {1441--1457},
}

@article{szekely_new_2005,
	title = {A new test for multivariate normality},
	volume = {93},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X03002124},
	doi = {10.1016/j.jmva.2003.12.002},
	abstract = {We propose a new class of rotation invariant and consistent goodness-of-fit tests for multivariate distributions based on Euclidean distance between sample elements. The proposed test applies to any multivariate distribution with finite second moments. In this article we apply the new method for testing multivariate normality when parameters are estimated. The resulting test is affine invariant and consistent against all fixed alternatives. A comparative Monte Carlo study suggests that our test is a powerful competitor to existing tests, and is very sensitive against heavy tailed alternatives.},
	number = {1},
	urldate = {2024-07-16},
	journal = {Journal of Multivariate Analysis},
	author = {Székely, Gábor J. and Rizzo, Maria L.},
	month = mar,
	year = {2005},
	keywords = {BHEP test, Goodness-of-fit, Henze–Zirkler test, Multivariate kurtosis, Multivariate skewness, Projection pursuit, Strictly negative definite},
	pages = {58--80},
}

@article{Szekely.2013,
	title = {E-statistics: {Energy} of statistical samples},
	urldate = {2023-10-09},
	author = {Szekely, Gabor},
	year = {2013},
}

@article{steinwart_strictly_2021,
	title = {Strictly proper kernel scores and characteristic kernels on compact spaces},
	volume = {51},
	issn = {1063-5203},
	doi = {10.1016/j.acha.2019.11.005},
	abstract = {Strictly proper kernel scores are well-known tools in probabilistic forecasting, while characteristic kernels have been extensively investigated in machine learning. We show that both notions coincide, so insights from one part of the literature can be used in the other. We show that the metric induced by a characteristic kernel cannot reliably distinguish between distributions that are far apart in total variation norm as soon as the underlying space of measures is infinite dimensional. We describe characteristic kernels in terms of eigenvalues and eigenfunctions and apply this characterization to continuous kernels on (locally) compact spaces. In the compact case, we show that characteristic kernels exist if and only if the space is metrizable. As special cases we investigate translation-invariant kernels on compact Abelian groups and isotropic kernels on spheres. The latter are of interest for forecast evaluation of probabilistic predictions on spherical domains as encountered in meteorology and climatology.},
	urldate = {2024-07-16},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Steinwart, Ingo and Ziegel, Johanna F.},
	month = mar,
	year = {2021},
	keywords = {Characteristic kernels, Hilbert space embeddings, Isotropic positive definite functions on spheres, Kernel scores, Machine learning, Probabilistic forecasts, Strictly proper scoring rules, Translation-invariant kernels on compact Abelian groups},
	pages = {510--542},
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of distance-based and {RKHS}-based statistics in hypothesis testing},
	volume = {41},
	issn = {0090-5364, 2168-8966},
	doi = {10.1214/13-AOS1140},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	number = {5},
	urldate = {2024-07-16},
	journal = {The Annals of Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	month = oct,
	year = {2013},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {46E22, 62G10, 62H20, 68Q32, Independence testing, distance covariance, reproducing kernel Hilbert spaces, two-sample testing},
	pages = {2263--2291},
}

@misc{pop_deep_2018,
	title = {Deep {Ensemble} {Bayesian} {Active} {Learning} : {Addressing} the {Mode} {Collapse} issue in {Monte} {Carlo} dropout via {Ensembles}},
	shorttitle = {Deep {Ensemble} {Bayesian} {Active} {Learning}},
	url = {http://arxiv.org/abs/1811.03897},
	doi = {10.48550/arXiv.1811.03897},
	abstract = {In image classification tasks, the ability of deep CNNs to deal with complex image data has proven to be unrivalled. However, they require large amounts of labeled training data to reach their full potential. In specialised domains such as healthcare, labeled data can be difficult and expensive to obtain. Active Learning aims to alleviate this problem, by reducing the amount of labelled data needed for a specific task while delivering satisfactory performance. We propose DEBAL, a new active learning strategy designed for deep neural networks. This method improves upon the current state-of-the-art deep Bayesian active learning method, which suffers from the mode collapse problem. We correct for this deficiency by making use of the expressive power and statistical properties of model ensembles. Our proposed method manages to capture superior data uncertainty, which translates into improved classification performance. We demonstrate empirically that our ensemble method yields faster convergence of CNNs trained on the MNIST and CIFAR-10 datasets.},
	urldate = {2024-07-16},
	publisher = {arXiv},
	author = {Pop, Remus and Fulop, Patric},
	month = nov,
	year = {2018},
	note = {arXiv:1811.03897 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{
ma_calibrated_2024,
title={Calibrated Uncertainty Quantification for Operator Learning via Conformal Prediction},
author={Ziqi Ma and David Pitt and Kamyar Azizzadenesheli and Anima Anandkumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=cGpegxy12T},
note={}
}


@article{lyons_distance_2013,
	title = {Distance covariance in metric spaces},
	volume = {41},
	issn = {0091-1798, 2168-894X},
	doi = {10.1214/12-AOP803},
	abstract = {We extend the theory of distance (Brownian) covariance from Euclidean spaces, where it was introduced by Székely, Rizzo and Bakirov, to general metric spaces. We show that for testing independence, it is necessary and sufficient that the metric space be of strong negative type. In particular, we show that this holds for separable Hilbert spaces, which answers a question of Kosorok. Instead of the manipulations of Fourier transforms used in the original work, we use elementary inequalities for metric spaces and embeddings in Hilbert spaces.},
	number = {5},
	urldate = {2024-07-16},
	journal = {The Annals of Probability},
	author = {Lyons, Russell},
	month = sep,
	year = {2013},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {30L05, 51K99, 62G20, 62H15, 62H20, Brownian covariance, Distance correlation, Hypothesis testing, Negative type, independence},
	pages = {3284--3305},
}

@article{guo_ib-uq_2023,
	title = {{IB}-{UQ}: {Information} bottleneck based uncertainty quantification for neural function regression and neural operator learning},
	shorttitle = {{IB}-{UQ}},
	url = {http://arxiv.org/abs/2302.03271},
	abstract = {We propose a novel framework for uncertainty quantification via information bottleneck (IB-UQ) for scientific machine learning tasks, including deep neural network (DNN) regression and neural operator learning (DeepONet). Specifically, we incorporate the bottleneck by a confidence-aware encoder, which encodes inputs into latent representations according to the confidence of the input data belonging to the region where training data is located, and utilize a Gaussian decoder to predict means and variances of outputs conditional on representation variables. Furthermore, we propose a data augmentation based information bottleneck objective which can enhance the quantification quality of the extrapolation uncertainty, and the encoder and decoder can be both trained by minimizing a tractable variational bound of the objective. In comparison to uncertainty quantification (UQ) methods for scientific learning tasks that rely on Bayesian neural networks with Hamiltonian Monte Carlo posterior estimators, the model we propose is computationally efficient, particularly when dealing with large-scale data sets. The effectiveness of the IB-UQ model has been demonstrated through several representative examples, such as regression for discontinuous functions, real-world data set regression, learning nonlinear operators for partial differential equations, and a large-scale climate model. The experimental results indicate that the IB-UQ model can handle noisy data, generate robust predictions, and provide confident uncertainty evaluation for out-of-distribution data.},
	language = {en},
    doi = {10.48550/arXiv.2302.03271},
	urldate = {2024-06-05},
	journal = {arXiv preprint},
	author = {Guo, Ling and Wu, Hao and Zhou, Wenwen and Wang, Yan and Zhou, Tao},
	year = {2023},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{gretton_kernel_2012,
	title = {A kernel two-sample test},
	volume = {13},
	issn = {1532-4435},
	abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD).We present two distribution free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	number = {null},
	journal = {The Journal of Machine Learning Research},
	author = {Gretton, Arthur and Borgwardt, Karsten M. and Rasch, Malte J. and Schölkopf, Bernhard and Smola, Alexander},
	year = {2012},
	keywords = {hypothesis testing, integral probability metric, kernel methods, schema matching, two-sample test, uniform convergence bounds},
	pages = {723--773},
}

@article{Gneiting.2007,
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	issn = {0162-1459},
	doi = {10.1198/016214506000001437},
	number = {477},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E.},
	year = {2007},
	pages = {359--378},
}

@article{diquigiovanni_conformal_2022,
	title = {Conformal prediction bands for multivariate functional data},
	volume = {189},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001573},
	doi = {10.1016/j.jmva.2021.104879},
	abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.},
	urldate = {2024-07-16},
	journal = {Journal of Multivariate Analysis},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	month = may,
	year = {2022},
	keywords = {Conformal Prediction, Distribution-free prediction set, Exact prediction set, Finite-sample prediction set, Functional data, Prediction band},
	pages = {104879},
}

@article{chen_generative_2024,
	title = {Generative machine learning methods for multivariate ensemble postprocessing},
	volume = {18},
	issn = {1932-6157, 1941-7330},
	doi = {10.1214/23-AOAS1784},
	abstract = {Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require postprocessing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate postprocessing have been proposed where ensemble predictions are first postprocessed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular, the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate postprocessing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper scoring rule, which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. Our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. In two case studies on multivariate temperature and wind speed forecasting at weather stations over Germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.},
	number = {1},
	urldate = {2024-07-16},
	journal = {The Annals of Applied Statistics},
	author = {Chen, Jieyu and Janke, Tim and Steinke, Florian and Lerch, Sebastian},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Generative machine learning, ensemble postprocessing, multivariate postprocessing, probabilistic forecasting, weather forecasting},
	pages = {159--183},
}

@inproceedings{
li_fourier_2021,
title={Fourier Neural Operator for Parametric Partial Differential Equations},
author={Zongyi Li and Nikola Borislavov Kovachki and Kamyar Azizzadenesheli and Burigede liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=c8P9NQVtmnO}
}

@misc{magnani_approximate_2022,
	title = {Approximate {Bayesian} {Neural} {Operators}: {Uncertainty} {Quantification} for {Parametric} {PDEs}},
	shorttitle = {Approximate {Bayesian} {Neural} {Operators}},
	url = {http://arxiv.org/abs/2208.01565},
	abstract = {Neural operators are a type of deep architecture that learns to solve (i.e. learns the nonlinear solution operator of) partial differential equations (PDEs). The current state of the art for these models does not provide explicit uncertainty quantiﬁcation. This is arguably even more of a problem for this kind of tasks than elsewhere in machine learning, because the dynamical systems typically described by PDEs often exhibit subtle, multiscale structure that makes errors hard to spot by humans. In this work, we ﬁrst provide a mathematically detailed Bayesian formulation of the “shallow” (linear) version of neural operators in the formalism of Gaussian processes. We then extend this analytic treatment to general deep neural operators using approximate methods from Bayesian deep learning. We extend previous results on neural operators by providing them with uncertainty quantiﬁcation. As a result, our approach is able to identify cases, and provide structured uncertainty estimates, where the neural operator fails to predict well.},
	language = {en},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Magnani, Emilia and Krämer, Nicholas and Eschenhagen, Runa and Rosasco, Lorenzo and Hennig, Philipp},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01565 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@article{li_neural_2020,
    title = {Neural {Operator}: {Graph} {Kernel} {Network} for {Partial} {Differential} {Equations}},
    shorttitle = {Neural {Operator}},
    url = {http://arxiv.org/abs/2003.03485},
    abstract = {The classical development of neural networks has been primarily for mappings between a ﬁnite-dimensional Euclidean space and a set of classes, or between two ﬁnite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between inﬁnite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between inﬁnite-dimensional spaces and between different ﬁnitedimensional approximations of those spaces. We formulate approximation of the inﬁnite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as ﬁnite difference or ﬁnite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments conﬁrm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers.},
    language = {en},
    urldate = {2024-03-11},
    journal = {arXiv preprint},
    doi = {10.48550/arXiv:2003.03485 },
    author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
    year = {2020},
    keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@article{10.1111/j.1467-9868.2007.00587.x,
    author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
    title = {Probabilistic Forecasts, Calibration and Sharpness},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {69},
    number = {2},
    pages = {243-268},
    year = {2007},
    month = {03},
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2007.00587.x},
    url = {https://doi.org/10.1111/j.1467-9868.2007.00587.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/69/2/243/49794500/jrsssb\_69\_2\_243.pdf},
}


@InProceedings{pmlr-v80-hron18a,
  title = 	 {Variational {B}ayesian dropout: pitfalls and fixes},
  author =       {Hron, Jiri and Matthews, Alex and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2019--2028},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/hron18a/hron18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/hron18a.html},
}




