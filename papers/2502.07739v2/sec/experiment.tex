
\begin{table*}
\label{glue}
    \centering
    \caption{Results with T5-base on tasks from a subset of GLUE benchmark. We report the Matthews correlation coefficient for CoLA, Pearson correlation coefficient for STS-B, and accuracy for the remaining tasks. Results are averaged over 3 seeds and standard deviations are given in the subscript. \textcolor{blue}{\textbf{Blue bold}} marked denotes the best result across different LoRA initialization, and \textcolor{red}{\textbf{red bold}} marked denotes the best result overall baseline.}
\begin{tabular}{llllllll}
\toprule
 & CoLA & MRPC & QNLI & RTE & SST-2 & STS-B & Avg. \\
\midrule
AdaLoRA & $22.44_{\pm 17.33}$ & $68.06_{\pm 0.46}$ & $88.23_{\pm 0.18}$ & $51.02_{\pm 2.64}$ & $91.70_{\pm 0.19}$ & $25.53_{\pm 7.21}$ & $57.83_{\pm 2.49}$ \\
rsLoRA & $43.84_{\pm 1.22}$ & $70.10_{\pm 0.20}$ & $86.53_{\pm 0.32}$ & $55.11_{\pm 2.13}$ & $91.09_{\pm 0.48}$ & $20.59_{\pm 8.65}$ & $61.21_{\pm 1.19}$ \\
DoRA & $38.79_{\pm 3.16}$ & $69.53_{\pm 0.31}$ & $87.19_{\pm 0.10}$ & $55.23_{\pm 2.84}$ & $91.74_{\pm 0.37}$ & $13.38_{\pm 3.03}$ & $59.31_{\pm 0.72}$ \\
FPFT & $51.17_{\pm 2.00}$ & \textcolor{red}{$\mathbf{86.27_{\pm 0.60}}$} & $89.65_{\pm 0.22}$ & \textcolor{red}{$\mathbf{64.62_{\pm 2.70}}$} & $92.13_{\pm 0.38}$ & \textcolor{red}{$\mathbf{89.22_{\pm 0.28}}$} & \textcolor{red}{$\mathbf{78.84_{\pm 0.91}}$} \\\hline
LoRA(Gauss) & $54.00_{\pm 0.53}$ & $73.45_{\pm 3.01}$ & $90.89_{\pm 0.10}$ & $57.76_{\pm 0.78}$ & $92.89_{\pm 0.50}$ & $85.68_{\pm 0.38}$ & $75.78_{\pm 0.49}$ \\
LoRA(Orth) & $53.39_{\pm 1.10}$ & $73.77_{\pm 0.53}$ & $90.83_{\pm 0.21}$ & $57.64_{\pm 1.12}$ & $92.89_{\pm 0.16}$ & $85.87_{\pm 0.48}$ & $75.73_{\pm 0.47}$ \\
PiSSA & $53.12_{\pm 0.21}$ & $76.63_{\pm 5.60}$ & $91.34_{\pm 0.13}$ & $56.80_{\pm 0.34}$ & \textcolor{red}{$\mathbf{93.35_{\pm 0.32}}$} & $86.53_{\pm 0.37}$ & $76.29_{\pm 0.90}$ \\
LoRA-GA & $52.62_{\pm 1.34}$ & $78.27_{\pm 0.70}$ & $89.90_{\pm 0.60}$ & $58.48_{\pm 1.35}$ & $93.00_{\pm 0.28}$ & $86.48_{\pm 0.86}$ & $76.46_{\pm 0.58}$ \\
HRP(ours) & \textcolor{red}{$\mathbf{55.47_{\pm 0.97}}$} & \textcolor{blue}{$\mathbf{86.19_{\pm 0.64}}$} & \textcolor{red}{$\mathbf{91.55_{\pm 0.22}}$} & \textcolor{blue}{$\mathbf{59.09_{\pm 3.60}}$} & $93.31_{\pm 0.14}$ & \textcolor{blue}{$\mathbf{87.06_{\pm 0.13}}$} & \textcolor{blue}{$\mathbf{78.78_{\pm 0.65}}$} \\\hline
AsymLoRA & $28.22_{\pm 0.70}$ & $68.14_{\pm 0.35}$ & $85.12_{\pm 0.70}$ & $51.26_{\pm 2.34}$ & $91.90_{\pm 0.44}$ & $8.02_{\pm 1.51}$ & $55.44_{\pm 0.54}$ \\
PiSSA & $43.19_{\pm 0.52}$ & $69.53_{\pm 0.42}$ & $88.33_{\pm 0.23}$ & $54.63_{\pm 0.85}$ & $92.58_{\pm 0.05}$ & $34.74_{\pm 12.65}$ & $63.83_{\pm 1.89}$ \\
HRP(ours) & \textcolor{blue}{$\mathbf{52.07_{\pm 2.27}}$} & \textcolor{blue}{$\mathbf{77.86_{\pm 2.58}}$} & \textcolor{blue}{$\mathbf{90.05_{\pm 0.14}}$} & \textcolor{blue}{$\mathbf{59.81_{\pm 1.36}}$} & \textcolor{blue}{$\mathbf{92.93_{\pm 0.24}}$} & \textcolor{blue}{$\mathbf{83.37_{\pm 0.33}}$} & \textcolor{blue}{$\mathbf{76.01_{\pm 0.15}}$} \\
\bottomrule
\end{tabular}


\end{table*}

\section{Experiments}
\label{exp}

In this section, we validate the effectiveness of HRP through experiments. We evaluate the performance of HRP and other LoRA variants under the neural language understanding (NLU) tasks (GLUE \cite{wang2018glue}) on T5-base model \cite{2020t5} and the neural language generation (NLG) tasks (MetaMathQA \cite{yu2023metamath}, GSM8k \cite{cobbe2021gsm8k}, and MATH \cite{hendrycks2021measuring}) on Llama2-7B model \cite{touvron2023llama}, Qwen2-7B model \cite{qwen2}, and Falcon3-7B model \cite{Falcon3}. 

We compare HRP with several baselines to demonstrate its effectiveness:
\begin{enumerate}
    \item Full-Parameter Fine-Tuning (FPFT): the straightforward fine-tuning method, which updates model parameters from pre-trained weights. 
    \item Classic LoRA with different initialization methods (zero+random initialization is settled in RSI): 1) kaiming normal initialization, 2) orthogonal initialization \cite{zhu2024asymmetry}, 3) PiSSA \cite{meng2024pissa}: first $r$ right singular vectors of $W^{\operatorname{pre}}$, 4) LoRA-GA \cite{wang2024lora}: initializing $A$ and $B$ with first $2r$ left and right singular vectors of gradient approximation, and 5) our proposed HRP with 200 steps preheating. 
    \item Asymmetric LoRA \cite{zhu2024asymmetry} with different initialization methods (zero+random initialization is settled in RSI): orthogonal initialization, PiSSA, and HRP. 
    \item Other LoRA variants including: a) DoRA \cite{liu2024dora}: with additional learnable magnitudes, b) rsLoRA \cite{kalajdzievski2023rank}: with a scaling factor for stability, and c) AdaLoRA \cite{zhang2023adalora}: with dynamically adjusted rank allocation. 
\end{enumerate}

\subsection{Experiments on NLU tasks}

In NLU tasks, we fine-tune the T5-base model \cite{2020t5} by AdamW \cite{loshchilov2019decoupledweightdecayregularization} on a subset of GLUE \cite{wang2018glue} benchmark, including CoLA, MRPC, QNLI, RTE, SST-2, and STS-B. Performance is evaluated on the Matthews correlation coefficient for CoLA, Pearson correlation coefficient for STS-B, and accuracy for the remaining tasks. 

During fine-tuning, we fixed the learning rate to $4\times 10^{-4}$ for all tasks and fine-tuned 5 epochs on the CoLA task while others for 2 epochs. Each experiment is conducted with 3 different random seeds (fixed seeds across different methods), and both the average and standard deviation are reported. For all variants of LoRA, we inject LoRA blocks for all query and value sub-modules with low rank $r=4$ and $\alpha=2r$. For HRP, we set the preheating rank $R=128$ with $S=200$ steps in the same training dataset with the AdamW optimizer under a constant learning rate.  We present more implication detail in Appendix \ref{apd-nlu-detail}. 


As demonstrated in Table \ref{glue}, HRP yields remarkable improvements in classic LoRA's performance, enabling it to surpass all other initialization methods and other variants while achieving results comparable to FPFT. When examining Asymmetric LoRA specifically, we observed that both orthogonal and PiSSA initialization schemas initially exhibited suboptimal performance under our experimental setting of low rank $r=4$. However, the introduction of just a few steps of HRP dramatically transformed the effectiveness of these same updating schemas, elevating their performance to levels comparable with classic LoRA.

\subsection{Experiments on NLG tasks}

\begin{table*}
\label{llm}
    \centering
    \caption{Results with LLMs on math reasoning tasks. \textcolor{blue}{\textbf{Blue bold}} marked denotes the best result across different LoRA initialization, and \textcolor{red}{\textbf{red bold}} marked denotes the best result overall baseline.}
\begin{tabular}{l|cccc|cccc}
\hline
 & \multicolumn{4}{c}{GSM8K} & \multicolumn{4}{|c}{MATH} \\\hline
 & ~~Llama2~~ & ~~Qwen2~~ & ~~Falcon3~~ & ~~Avg.~~ & ~~Llama2~~ & ~~Qwen2~~ & ~~Falcon3~~ & ~~Avg.~~ \\\hline
rsLoRA & 43.75 & 74.07 & 79.08 & \textcolor{red}{\textbf{65.63}} & \textcolor{red}{\textbf{6.76}} & 30.28 & 37.88 & 24.97 \\
DoRA & 42.53 & 74.60 & 79.08 & 65.40 & 6.22 & 29.84 & 37.88 & 24.65 \\
FPFT & \textcolor{red}{\textbf{43.75}} & 71.72 & 79.68 & 65.05 & 6.34 & 31.00 & \textcolor{red}{\textbf{38.06}} & \textcolor{red}{\textbf{25.13}} \\\hline
LoRA(Gauss) & 28.89 & 74.22 & \textcolor{red}{\textbf{79.98}} & 61.03 & 3.90 & \textcolor{blue}{\textbf{32.32}} & 33.72 & 23.31 \\
LoRA(Orth) & 29.72 & 73.31 & 78.92 & 60.65 & 3.40 & 29.42 & 32.48 & 21.76 \\
PiSSA & 31.24 & 72.86 & 75.44 & 59.85 & 3.80 & 29.32 & 34.62 & 22.58 \\
LoRA-GA & 23.58 & 72.86 & 78.92 & 58.45 & 2.56 & 29.32 & 32.34 & 21.41 \\
HRP(ours) & \textcolor{blue}{\textbf{34.65}} & \textcolor{blue}{\textbf{75.36}} & 79.15 & \textcolor{blue}{\textbf{63.05}} & \textcolor{blue}{\textbf{3.98}} & 30.74 & \textcolor{blue}{\textbf{36.80}} & \textcolor{blue}{\textbf{23.84}} \\\hline
AsymLoRA & 22.44 & 75.21 & 79.30 & 58.98 & 1.36 & \textcolor{red}{\textbf{36.64}} & 32.60 & 23.53 \\
PiSSA & 28.58 & 72.71 & \textcolor{blue}{\textbf{79.68}} & 60.32 & 2.50 & 31.88 & 33.30 & 22.56 \\
HRP(ours) & \textcolor{blue}{\textbf{32.07}} & \textcolor{red}{\textbf{77.18}} & 76.19 & \textcolor{blue}{\textbf{61.81}} & \textcolor{blue}{\textbf{4.06}} & 30.82 & \textcolor{blue}{\textbf{35.96}} & \textcolor{blue}{\textbf{23.61}} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{pic/llama-curve.pdf}
    \caption{Training loss trajectories for Llama2-7B fine-tuned on MetaMathQA using different initialization strategies in classic LoRA. The inset figure (top right) highlights the loss dynamics during the first 500 optimization steps.}
    \label{loss-curve}
\end{figure}

In NLG tasks, we fine-tune the Llama2-7B model \cite{touvron2023llama}, Qwen2-7B model \cite{qwen2}, and Falcon3-7B model \cite{Falcon3} by AdamW \cite{loshchilov2019decoupledweightdecayregularization} on a 50K subset of MetaMathQA dataset \cite{yu2023metamath} for 1 epoch. Then, we evaluate the fine-tuned models on the test set of GSM8K \cite{cobbe2021gsm8k} and MATH \cite{hendrycksmath2021}. 

During fine-tuning, we fix the learning rate to $5\times 10^{-5}$ and fix the same random seed in all baselines. For all variants of LoRA, we inject LoRA blocks for all query, key, value, attention output, and all fully connected weight matrices, with rank $r=8$ and $\alpha=2r$ in the main fine-tuning process. For HRP preheating stage, we set the preheating rank $R=256$ with $S=50$ steps in the same training dataset with the AdamW optimizer under a constant learning rate. Our model is fine-tuned using standard supervised learning fine-tuning schema for language modeling where the loss for the input prompt is set to zero. We present more implication detail in Appendix \ref{apd-nlg-detail}. 

We report the evaluated results in Table \ref{llm}, which demonstrate the effectiveness of HRP across various large language models. Similar to results in NLU tasks, HRP yields substantial improvements for both classic LoRA and Asymmetric LoRA. 
It is crucial to note that HRP demonstrates more pronounced benefits when applied to the relatively under-trained Llama2 model, indicating that HRP's effectiveness does not heavily depend on the performance of the pre-trained model. 

Unlike results in NLU tasks, Asymmetric LoRA demonstrates sufficient expressiveness in these NLG tasks, despite having about half the trainable parameters compared to classic LoRA. HRP further enhances its fine-tuning capabilities with a few steps of fine-tuning steps, outperforming random orthogonal initialization and PiSSA initialization. 

To provide deeper insights into the training dynamics, we present the loss curves of fine-tuning Llama2 in Figure \ref{loss-curve}. The visualization convincingly validates our theoretical analysis, demonstrating that HRP indeed achieves superior converged results compared with random initialization. Besides, the loss curves reveal that HRP also accelerates the convergence of classic LoRA, especially in the beginning stage of fine-tuning. Aside from Llama2, we also present the loss curves of fine-tuning other models in Appendix \ref{nlg-more}. 






