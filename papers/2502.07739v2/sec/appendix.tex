\section{Proof for Theorems}

In this section, we present the proof for the theorems above. We begin with some useful lemmas and their proofs. 

\begin{lemma}
    \label{asymlora-close-form}
    Gradient flow \ref{xflow} under problem \ref{matrix-sensing} with $\eta_A=\eta, \eta_B=0$, $X_t$ has the following closed form:
    \begin{align}
        X_t=\left[I-e^{-\eta Z_0t}\right]M,
    \end{align}
    with $\eta_A=0, \eta_B=\eta$, $X_t$ has the following closed form:
    \begin{align}
        X_t=M\left[I-e^{-\eta Y_0t}\right]
    \end{align}
    where $e^{A}$ denotes exponential operation on matrix. 
\end{lemma}
\begin{proof}
    When $\eta_A=\eta, \eta_B=0$, the gradient flow becomes
    \begin{align}
        \begin{cases}
            \dot{X_t}=-\eta Z_tG_t,\\
            \dot{Y_t}=-\eta X_t^\top G_t-\eta G_t^\top X_t,\\
            \dot{Z_t}=0,
        \end{cases}
    \end{align}
    which means $Z_t\equiv Z_0$ for all $t$, and $\dot{X_t}=-\eta Z_0(X_t-M)$. Then $X_t$ has analytic solution 
    \begin{align}
        X_t=\left[I-e^{-\eta Z_0t}\right]M+X_0=\left[I-e^{-\eta Z_0t}\right]M.
    \end{align}

    When $\eta_A=0, \eta_B=\eta$, the gradient flow becomes
    \begin{align}
        \begin{cases}
            \dot{X_t}=-\eta G_tY_t,\\
            \dot{Y_t}=O_{a\times a}\\
            \dot{Z_t}=-\eta X_t G_t^\top-\eta G_t X_t^\top,,
        \end{cases}
    \end{align}
    which means $Y_t\equiv Y_0$ for all $t$, and $\dot{X_t}=-\eta (X_t-M)Y_t$. Then $X_t$ has analytic solution 
    \begin{align}
        X_t=M\left[I-e^{-\eta Y_0}\right]+X_0=M\left[I-e^{-\eta Y_0}\right].
    \end{align}
\end{proof}
\begin{lemma}
    \label{lossineq}
    For any matrix $A\in\mathbb{R}^{d_1\times d_2}$ and any orthogonal matrix $U\in\mathbb{R}^{d_1\times d_1}$, and $r\leq d_2$, for
    \begin{align}
        X=U\begin{pmatrix}
            I_r\\&O_{(d_1-r)\times (d_1-r)}
        \end{pmatrix}U^\top M,
    \end{align}
    we have
    \begin{align}
        \|M-X\|_F^2=\|M\|_F^2-\|X\|_F^2.
    \end{align}
\end{lemma}
\begin{proof}
    We have
    \begin{align}
        \|M\|_F^2&=\|X\|_F^2+\|M-X\|_F^2+2\operatorname{Trace}(X^\top(M-X)).
    \end{align}
    Then it is sufficient to prove $\operatorname{Trace}(X^\top(M-X))=0$. In fact, we have
    \begin{align}
        \operatorname{Trace}\left(X^\top(M-X)\right)
        &=\operatorname{Trace}\left(\left[U\begin{pmatrix}
            I_r\\&O_{(d_1-r)\times (d_1-r)}
        \end{pmatrix}U^\top M\right]^\top\left[M-U\begin{pmatrix}
            I_r\\&O_{(d_1-r)\times (d_1-r)}
        \end{pmatrix}U^\top M\right]\right)\\
        &=\operatorname{Trace}\left(\left[U\begin{pmatrix}
            I_r\\&O_{(d_1-r)\times (d_1-r)}
        \end{pmatrix}U^\top M\right]^\top\left[U\begin{pmatrix}
            O_{r\times r}\\&I_{d_1-r}
        \end{pmatrix}U^\top M\right]\right)\\
        &=\operatorname{Trace}\left(M^\top U\begin{pmatrix}
            I_r\\&O_{(d_1-r)\times (d_1-r)}
        \end{pmatrix}U^\top U\begin{pmatrix}
            O_{r\times r}\\&I_{d_1-r}
        \end{pmatrix}U^\top M\right)\\
        &=\operatorname{Trace}(O_{d_1\times d_1})=0.
    \end{align}
\end{proof}
\begin{theorem}[Gronwall's Theorem]
    \label{gronwall}
    For $x_t\geq 0$ and inequality
    \begin{align}
        dx_t\leq [ax_t+bt]dt,~~~~~x_0=0,
    \end{align}
    we have
    \begin{align}
        x_t=O(bt^2)
    \end{align}
\end{theorem}
\begin{proof}
    Consider $x_te^{-at}\geq 0$ and
    \begin{align}
        d\left[x_te^{-at}\right]\leq e^{-at}[ax_t+bt]dt-ax_te^{-at}dt=bte^{-at}dt
    \end{align}
    thus
    \begin{align}
        x_te^{-at}&\leq x_0+\int_0^tbse^{-as}ds=\frac{b}{a^2}[-(1+at)e^{-at}+1]\\
        x_t&\leq \frac{b}{a^2}[e^{at}-at-1]=O(bt^2)
    \end{align}
\end{proof}
\begin{remark}
    Theorem \ref{gronwall} presents a simplified version of the Gronwall inequality \cite{howard1998gronwall}. In the original inequality, the terms $ax_t$ and $bt$ have more complex forms, and the result is not expressed in big-O notation. We use this simplified version to establish bounds on the difference between Asymmetric LoRA and classical LoRA in the proof that follows.
\end{remark}

\subsection{Proof for Theorem \ref{ms-loss}}
\label{ms-loss-proof}
\begin{theorem}[Restatement of theorem \ref{ms-loss}]
    For Asymmetric LoRA under problem \ref{matrix-sensing} with Gaussian initialization and orthogonal initialization in LSI and RSI in zero+random schema, we have:
    \begin{align*}
        \mathbb{E}_{LSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{b-r}{2b}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2,\\
        \mathbb{E}_{RSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{a-r}{2a}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2,
    \end{align*}
    where $\mathbb{E}$ represents the expectation for randomness in initialization. 
\end{theorem}
\begin{proof}
    For LSI, $\eta_A=\eta,\eta_B=0$, according to Lemma \ref{asymlora-close-form} we have
    \begin{align}
        X_t=\left[I-e^{-\eta Z_0t}\right]M=\left[I-e^{-\eta B_0B_0^\top t}\right]M.
    \end{align}
    For the SVD decomposition of $B_0=U_B\Sigma_BV_B^\top$, we have $Z_0=U_B(\Sigma_B)^2U_B^\top$ where $\Sigma_B$ is a diagonal matrix with only first $r$ elements non-zero. Then, consider $t\to\infty$, we have
    \begin{align}
        \lim_{t\to\infty}X_t&=\lim_{t\to\infty}\left[\left[I-e^{-\eta Z_0t}\right]M\right]\\
        &=\left[I-\lim_{t\to\infty}e^{-\eta Z_0t}\right]M\\
        &=U_B\left[I-\lim_{t\to\infty}e^{-\eta \Sigma_B^2t}\right]U_B^\top M=U_B\begin{pmatrix}
            I_r\\&O_{b-r}
        \end{pmatrix}U_B^\top M.
        \label{asym-close-form}
    \end{align}
    Gaussian initialization and orthogonal initialization share the same probability for the same $U_B$, thus their properties at converged results are the same. The loss of the converged result satisfies:
    \begin{align}
        \mathcal{L}(\lim_{t\to\infty}X_t)&=\|\lim_{t\to\infty}X_t-M\|_F^2\\
        &=\frac{1}{2}\left\|U_B\begin{pmatrix}
            I_r\\&O_{b-r}
        \end{pmatrix}U_B^\top M-M\right\|_F^2\\
        &=\frac{1}{2}\left\|U_B\begin{pmatrix}
            O_r\\&I_{b-r}
        \end{pmatrix}U_B^\top M\right\|_F^2\\
        &=\frac{1}{2}\left\|\begin{pmatrix}
            O_r\\&I_{b-r}
        \end{pmatrix}U_B^\top M\right\|_F^2\\
        &=\frac{1}{2}\sum_{i=r}^{b}\left\|U_{B,i}^\top M\right\|_F^2,
    \end{align}
    where $U_{B,i}^\top$ is the $i$-th column of $U_B$ thus the $i$-th row of $U_B^\top$ and orthogonal with each other.
    For any orthogonal matrix $U$, consider $U^{(i)}=[U_{[:,:i]},U_{[:,i:]}]$. 
    Due to Gaussian initialization of $B$, the p.d.f at $U_B=U^{(i)}$ is the same as p.d.f at $U_B=U^{(j)}$ for all $i, j$. Thus, we have
    \begin{align}
        \mathbb{E}_B\mathcal{L}(\lim_{t\to\infty}X_t)&=
        \mathbb{E}_B\frac{1}{2}\sum_{i=r}^{b}\left\|U_{B,i}^\top M\right\|_F^2,\\
        &=\mathbb{E}_U\left.\frac{1}{2b}\sum_{j=1}^b \sum_{i=r}^{b}\left\|U_{B,i}^\top M\right\|_F^2\right|_{U_B=U^{(j)}},\\
        &=\frac{1}{2b}\sum_{i=r}^{b}\mathbb{E}_U\sum_{j=1}^b\left\|U_{j}^\top M\right\|_F^2,\\
        &=\frac{1}{2b}\sum_{i=r}^{b}\mathbb{E}_U\left\|U^\top M\right\|_F^2,\\
        &=\frac{1}{2b}\sum_{i=r}^{b}\left\|M\right\|_F^2=\frac{b-r}{2b}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2.
    \end{align}
    For RSI in matrix sensing, it is equivalent to LSI with $X^\top$ approximating $M^\top$, thus for RSI we have
    \begin{align}
        \mathbb{E}_{RSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{a-r}{2a}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2
    \end{align}
\end{proof}

\subsection{Proof for Theorem \ref{ms-pr}}
\label{ms-pr-proof}
\begin{theorem}[Restatement of theorem \ref{ms-pr}]
    For Asymmetric LoRA under problem \ref{matrix-sensing} with LSI or RSI, we have
    \begin{align*}
        \operatorname{Pr}\left[\mathcal{L}(\lim_{t\to\infty}X_t)=\mathcal{L}^*\right]=0,
    \end{align*}
    where $\operatorname{Pr}$ represents the probability for initialization. 
\end{theorem}
\begin{proof}
    According to \ref{asym-close-form} and Eckart-Young Theorem \cite{eckart1936approximation}, we have for LSI:
    \begin{align}
        \operatorname{Pr}\left[\mathcal{L}(\lim_{t\to\infty}X_t)=\mathcal{L}^*\right]
        &=\operatorname{Pr}\left[\forall i\in[r]:U_B\begin{pmatrix}
            I_r\\&O_{b-r}
        \end{pmatrix}U_B^\top MV_{M,i}=\sigma_i(M)U_{M,i}\right]\\
        &=\operatorname{Pr}\left[\forall i\in[r]:U_B\begin{pmatrix}
            I_r\\&O_{b-r}
        \end{pmatrix}U_B^\top U_{M,i}=U_{M,i}\right]\\
        &=\operatorname{Pr}\left[\forall i>r,j<r: (U_B^\top U_{M})[i,j]=0\right]
    \end{align}
    Since $U_B$ is random and independent from $U_M$, it is zero-probability for making left-down $(b-r)\times r$ matrix zero. 

    For RSI in matrix sensing, it is equivalent to LSI with $X^\top$ approximating $M^\top$, thus for both LSI and RSI, we have
    \begin{align*}
        \operatorname{Pr}\left[\mathcal{L}(\lim_{t\to\infty}X_t)=\mathcal{L}^*\right]=0,
    \end{align*}
\end{proof}


\subsection{Proof for Theorem \ref{lora-bad}}
\label{lora-bad-proof}
\begin{theorem}[Restatement of theorem \ref{lora-bad}]
    For problem \ref{matrix-sensing}, if there exists $i\leq r$ making $A,B$ with $A^\top v_i=O_a,B^\top u_i=O_b$. Then classic LoRA under LSI with $A_0=A,B_0=O_{b\times r}$ or RSI with $A_0=O_{a\times r},B_0=B$, we have for any $t$:
    \begin{align*}
        X_tv_i=O_b,~~~\text{and}~~~X_t^\top u_i=O_a,
    \end{align*}
    resulting in
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*\geq \frac{1}{2}[\sigma_i(M)^2-\sigma_{r+1}(M)^2]>0,
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$. 
\end{theorem}
\begin{proof}
    LSI with $A_0=A,B_0=O_{b\times r}$ means that initialization of $X,Y,Z$ satisfies
    \begin{align}
        X_0=O_{a\times b},~~~~Y_0=AA^\top ,~~~~Z_0=O_{b\times b},
    \end{align}
    indicating for $t=0$, we have:
    \begin{align}
        X_tv_i=O_b,~~~~Y_tv_i=O_a,~~~~X_t^\top u_i=O_a,~~~~Z_t^\top u_i=O_b. 
        \label{lorabad-base}
    \end{align}
    RSI with $A_0=O_{a\times r},B_0=B$ means that initialization of $X,Y,Z$ satisfies
    \begin{align}
        X_0=O_{b\times a},~~~~Y_0=O_{a\times a} ,~~~~Z_0=BB^\top,
    \end{align}
    which also indicates \ref{lorabad-base} true for $t=0$. We then prove \ref{lorabad-base} true for all $t>0$ to prove Theorem \ref{lora-bad}. According to induction, it is sufficient to prove each gradient to $t$ in \ref{lorabad-base} to be zero for all $t$. In fact, we have
    \begin{align}
        \frac{d(X_tv_i)}{dt}&=\dot{X_t}v_i=-\eta Z_t(X_t-M)v_i-\eta (X_t-M)Y_tv_i=\eta Z_tMv_i=\eta\sigma_i(M) Z_tu_i=O_b,\\
        \frac{d(Y_tv_i)}{dt}&=\dot{Y_t}v_i=-\eta X_t^\top (X_t-M)v_i-\eta (X_t-M)^\top X_tv_i=\eta X_t^\top Mv_i=\eta\sigma_i(M) X_t^\top u_i=O_a,\\
        \frac{d(X_t^\top u_i)}{dt}&=\dot{X_t}^\top u_i=-\eta \left[Z_t(X_t-M)\right]^\top u_i-\eta \left[(X_t-M)Y_t\right]^\top u_i=\eta Y_tM^\top u_i=\eta\sigma_i(M) Y_tv_i=O_a,\\
        \frac{d(Z_tu_i)}{dt}&=\dot{Z_t}u_i=-\eta X_t(X_t-M)^\top u_i -\eta (X_t-M)X_t^\top u_i=\eta X_tM^\top u_i=\eta\sigma_i(M) X_t v_i=O_b.
    \end{align}
    This means that the composition $\sigma_i(M)u_iv_i^\top$ will not emerge from $X_t$ as $t$ grows up. However, the best low-rank approximation of $M$ requires $\sigma_i(M)u_iv_i^\top$ to be included, which makes the optimization never converge to the best low-rank result. For loss, consider $M'=M-\sigma_i(M)u_iv_i^\top$, we have
    \begin{align}
        \mathcal{L}(X_t)&=\frac{1}{2}\|X-M\|_F^2=\frac{1}{2}\operatorname{Trace}((X-M'-\sigma_i(M)u_iv_i^\top)(X-M'-\sigma_i(M)u_iv_i^\top)^\top)\\
        &=\frac{1}{2}\|X-M'\|_F^2+\frac{1}{2}\|\sigma_i(M)u_iv_i^\top\|_F^2\\
        &\geq \frac{1}{2}\sum_{j=r+2}^{\min\{a,b\}}\sigma_j(M)^2+\frac{1}{2}\sigma_i(M)^2\\
        &=\mathcal{L}^*+\frac{1}{2}\left[\sigma_i(M)^2-\sigma_{r+1}(M)^2\right]
    \end{align}
\end{proof}


\subsection{Proof for Theorem \ref{lora-similar}}
\label{lora-similar-proof}
\begin{theorem}[Restatement of theorem \ref{lora-similar}]
    Assume $\{X_s\}_{0\leq s\leq t}$ is bounded by $R$ and the computed gradient $X_t\to G_t$ is Lipschitz in the Frobenius norm (same gradient calculator for classic LoRA and Asymmetric LoRA), then the difference between the dynamic of Asymmetric LoRA ($\tilde{X_t}$) and the dynamic of classic LoRA ($X_t$) is upper bounded by
    \begin{align}
        \|X_t-\tilde{X}_t\|_F\leq O(\eta R^3t^2),
    \end{align}
    when they have the same initialization in LSI or RSI. 
\end{theorem}
\begin{proof}
    We use Gronwall's Theorem \ref{gronwall} to bound the difference $\|X_t-\tilde{X}_t\|_F$. Recall $X_t$ and $\tilde{X_t}$ follows the differential equations:
    \begin{align}
        \begin{cases}
            \dot{X_t}=-\eta_A Z_tG_t-\eta_B G_tY_t,\\
            \dot{Y_t}=-\eta_A X_t^\top G_t-\eta_A G_t^\top X_t,\\
            \dot{Z_t}=-\eta_B X_tG_t^\top -\eta_B G_tX_t^\top,
        \end{cases}
        ~~~~~~~~\text{and}~~~~~~~~
        \dot{\tilde{X_t}}=-\eta_A Z_0\tilde{G_t}-\eta_B \tilde{G_t}Y_0.
    \end{align}
    Taking the assumption $X_t\to G_t$ to be $L$ -Lipschitz and bounded $X_t$, we have 
    \begin{align}
        \|G_t\|_F\leq LR+\|G_0\|_F,~~~~\text{and}~~~~\|\tilde{G_t}-G_t\|_F\leq L\|X_t-\tilde{X_t}\|_F.
    \end{align}
    Then the difference of $Y_t$ and $Z_t$ can be bounded through
    \begin{align}
        \|Y_t-Y_0\|_F&\leq \int_0^t\|dY_s\|_F\leq \eta \int_0^t 2\|X_t\|_F\|G_t\|_Fdt\leq 2\eta R(\|G_0\|_F+LR)t,\\
        \|Z_t-Z_0\|_F&\leq \int_0^t\|dZ_s\|_F\leq \eta \int_0^t 2\|X_t\|_F\|G_t\|_Fdt\leq 2\eta R(\|G_0\|_F+LR)t.
    \end{align}
    We then calculate the difference between $\dot{\tilde{X_t}}$ and $\dot{X_t}$:
    \begin{align}
        \dot{X_t}-\dot{\tilde{X}_t}&=-\eta Z_tG_t-\eta G_tY_t-[-\eta Z_0\tilde{G_t}-\eta \tilde{G_t}Y_0]\\
        &=-\eta(Z_t-Z_0)G_t-\eta Z_0(G_t-\tilde{G_t})-\eta G_t(Y_t-Y_0)-\eta(G_t-\tilde{G_t})Y_0.
    \end{align}
    Taking Frobenius norm of LHS and RHS we have:
    \begin{align}
        \frac{d\left\|X_t-\tilde{X_t}\right\|_F}{dt}&\leq \left\|\frac{d[X_t-\tilde{X_t}]}{dt}\right\|_F\\
        &\leq \left\|-\eta(Z_t-Z_0)G_t-\eta Z_0(G_t-\tilde{G_t})-\eta G_t(Y_t-Y_0)-\eta(G_t-\tilde{G_t})Y_0\right\|_F\\
        &\leq \eta \|(Z_t-Z_0)G_t\|_F +\eta\|Z_0(G_t-\tilde{G_t})\|_F+\eta \|G_t(Y_t-Y_0)\|_F+\eta \|(G_t-\tilde{G_t})Y_0\|_F\\
        &\leq \eta \|Z_t-Z_0\|_F\|G_t\|_F +\eta\|Z_0\|_F\|G_t-\tilde{G_t}\|_F+\eta \|G_t\|_F\|Y_t-Y_0\|_F+\eta \|G_t-\tilde{G_t}\|_F\|Y_0\|_F\\
        &\leq \eta\left[\|G_t\|_F(\|Y_t-Y_0\|_F+\|Z_t-Z_0\|_F) + (\|Z_0\|_F+\|Y_0\|_F)\|G_t-\tilde{G_t}\|_F\right]\\
        &\leq \eta\left[4R(\|G_0\|+LR)^2t + L(\|Z_0\|_F+\|Y_0\|_F)\left\|X_t-\tilde{X_t}\right\|_F\right].
    \end{align}
    Thus according to Gronwall theorem \ref{gronwall}, we have
    \begin{align}
        \left\|X_t-\tilde{X_t}\right\|_F\leq O(4R(\|G_0\|+LR)^2t^2)= O(\eta R^3t^2).
    \end{align}
\end{proof}


\subsection{Proof for Theorem \ref{asym-wise}}
\label{asym-wise-proof}
\begin{theorem}[Restatement of theorem \ref{asym-wise}]
    For Asymmetric LoRA under problem \ref{matrix-sensing} in RSI with $A_0=V_M[:,:r],B_0=O_{b\times r}$ or LSI with $A_0=O_{a\times r},B_0=U_M[:,:r]$, we have
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*=O(\exp\{-\eta t\}),
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$. 
\end{theorem}
\begin{proof}
    According to Lemma \ref{asymlora-close-form}, with LSI and initialization $A_0=V_M[:,:r],B_0=O_{b\times r}$, we have 
    \begin{align}
        X_t=M[I-e^{-\eta A_0A_0^\top t}]=(1-e^{-\eta t})MV_M\begin{pmatrix}
            I_r\\&O_{(a-r)\times (a-r)}
        \end{pmatrix}V_M^\top,
    \end{align}
    which has loss
    \begin{align}
        \mathcal{L}(X_t)&=\frac{1}{2}\|X_t-M\|_F^2\\
        &=\frac{1}{2}\left\|M-(1-e^{-\eta t})MV_M\begin{pmatrix}
            I_r\\&O_{(a-r)\times (a-r)}
        \end{pmatrix}V_M^\top\right\|_F^2\\
        &=\frac{1}{2}\left\|U_M\Sigma_MV_M^\top -(1-e^{-\eta t})U_M\Sigma_M\begin{pmatrix}
            I_r\\&O_{(a-r)\times (a-r)}
        \end{pmatrix}V_M^\top \right\|_F^2\\
        &=\frac{1}{2}\left\|\Sigma_M-(1-e^{-\eta t})\Sigma_M\begin{pmatrix}
            I_r\\&O_{(a-r)\times (a-r)}
        \end{pmatrix}\right\|_F^2\\
        &=\frac{1}{2}\left\|\Sigma_M\begin{pmatrix}
            e^{-\eta t}I_r\\&I_{a-r}
        \end{pmatrix}\right\|_F^2\\
        &=\frac{1}{2}e^{-\eta t}\sum_{i=1}^r\sigma_i(M)^2+\frac{1}{2}\sum_{i=r+1}^{\min\{a,b\}}\sigma_i(M)^2\\
        &=O(\exp\{-\eta t\})+\mathcal{L}^*.
    \end{align}
\end{proof}

\subsection{Proof for Theorem \ref{lora-wise}}
\label{lora-wise-proof}
\begin{theorem}[Restatement of theorem \ref{lora-wise}]
    For classic LoRA under problem \ref{matrix-sensing} in RSI with $A_0=V_M[:,:r],B_0=O_{b\times r}$ or LSI with $A_0=O_{a\times r},B_0=U_M[:,:r]$, we have
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*=O(\exp\{-(1+k\sigma_r(M))\eta t\}),
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$ and $k=\frac{\sqrt{1+4\sigma_1(M)}-1}{\sigma_1(M)}>0$.
\end{theorem}
\begin{proof}
    We consider
    \begin{align}
        \hat{X_t}:=U^\top X_tV,~~~~\hat{Y_t}:=V^\top Y_tV,~~~~\hat{Z_t}:=U^\top Z_tU,
    \end{align}
    and first prove 
    \begin{align}
        \forall r, \hat{X_t},\hat{Y_t},\hat{Z_t}~\text{are diagonal matrices with only the first $r$ elements non-zero.}\label{lora-wise-cl}
    \end{align}If initialized with $A_0=V_{:,:r},B_0=O_{b\times r}$, then we have 
    \begin{align}
        \hat{X_t}:=U^\top X_0V=O_{b\times a},~~~~\hat{Y_t}:=V^\top Y_0V=\begin{pmatrix}
            I_r\\&O_{a-r}
        \end{pmatrix},~~~~\hat{Z_t}:=U^\top Z_0U=O_{b\times b}.
    \end{align}
    If initialized with $A_0=O_{a\times r},B_0=U_{:,:r}$, then we have 
    \begin{align}
        \hat{X_t}:=U^\top X_0V=O_{b\times a},~~~~\hat{Y_t}:=V^\top Y_0V=O_{a\times a},~~~~\hat{Z_t}:=U^\top Z_0U=\begin{pmatrix}
            I_r\\&O_{b-r}
        \end{pmatrix}.
    \end{align}
    Thus, claim \ref{lora-wise-cl} is true for $t=0$. If it is true for $t$, then their gradient satisfies
    \begin{align}
        \dot{\hat{X_t}}&=U^\top\left[-\eta Z_t(X_t-M)-\eta (X_t-M)Y_t\right]V=-\eta \hat{Z_t}(\hat{X_t}-\Sigma)-\eta (\hat{X_t}-\Sigma)\hat{Y_t},\\
        \dot{\hat{Y_t}}&=V^\top\left[-\eta X_t^\top (X_t-M)-\eta (X_t-M)^\top X_t\right]V= -\eta \hat{X_t}^\top (\hat{X_t}-\Sigma)-\eta (\hat{X_t}-\Sigma)^\top \hat{X_t},\\
        \dot{\hat{Z_t}}&=U^\top\left[-\eta X_t(X_t-M)^\top -\eta (X_t-M)X_t^\top\right]U= -\eta \hat{X_t}(\hat{X_t}-\Sigma)^\top -\eta (\hat{X_t}-\Sigma)\hat{X_t}^\top,
    \end{align}
    which are all diagonal matrices with only the first $r$ elements non-zero. Thus, claim \ref{lora-wise-cl} is true for all $t$. We denote the diagonal elements for $\hat{X_t},\hat{Y_t},\hat{Z_t}$ are $x_{t,i},y_{t,i},z_{t,i}$. Then for each $i$, we have
    \begin{align}
        \dot{x_{t,i}}&=-\eta z_{t,i}(x_{t,i}-\sigma_i(M))-\eta (x_{t,i}-\sigma_i(M))y_{t,i}=-\eta(y_{t,i}+z_{t,i})(x_{t,i}-\sigma_i(M)),\\
        \dot{y_{t,i}}=\dot{z_{t,i}}&= -\eta x_{t,i}(x_{t,i}-\sigma_i(M)) -\eta (x_{t,i}-\sigma_i(M))x_{t,i}=-2\eta x_{t,i}(x_{t,i}-\sigma_i(M)),
    \end{align}
    and they are independent with other $j\neq i$. 
    Consider
    \begin{align}
        d\left[(y_{t,i}+z_{t,i})^2\right]&=-4\eta(y_{t,i}+z_{t,i})x_{t,i}(x_{t,i}-\sigma_i(M))\\
        &=-4\eta x_{t,i}(y_{t,i}+z_{t,i})(x_{t,i}-\sigma_i(M))=4d\left[x_{t,i}^2\right],
    \end{align}
    resulting in
    \begin{align}
        y_{t,i}+z_{t,i}&=\sqrt{(y_{0,i}+z_{0,i})^2+4x_{t,i}^2-4x_{0,i}^2}=\sqrt{1+4x_{t,i}^2}. 
    \end{align}
    So, the dynamic of $x_{t,i}$ is
    \begin{align}
        \dot{x_{t,i}}&=-\eta(y_{t,i}+z_{t,i})(x_{t,i}-\sigma_i(M))=-\eta\sqrt{1+4x_{t,i}^2}(x_{t,i}-\sigma_i(M)).
    \end{align}
    This means $0\leq x_{t,i}\leq \sigma_i(M)\leq \sigma_1(M)$ and $x_{t,i}$ are monotonically increasing with respect to $t$. Due to the convex property of $\sqrt{1+4x^2}$, we have for $k=\frac{\sqrt{1+4\sigma_1(M)^2}-1}{\sigma_1(M)}$:
    \begin{align}
        \sqrt{1+4x_{t,i}^2}&\geq 1+kx_{t,i}\\
        \dot{x_{t,i}}&\geq -\eta\left[1+kx_{t,i}\right](x_{t,i}-\sigma_i(M))
    \end{align}
    Consider another flow 
    \begin{align}
        \dot{\tilde{x_{t,i}}}=-\eta\left[1+k\tilde{x_{t,i}}\right](\tilde{x_{t,i}}-\sigma_i(M)),~~~~\tilde{x_{0,i}}=x_{0,i}=0,
    \end{align}
    we have $\tilde{x_{t,i}}\leq x_{t,i}\leq \sigma_i(M)$, and $(\sigma_i(M)-\tilde{x_{t,i}})^2=O(e^{-(1+k\sigma_i(M))\eta t})$. So, for $X_t$, we have
    \begin{align}
        \mathcal{L}(X_t)-\mathcal{L}^*&=\|U\hat{X_t}V^\top-M\|_F^2-\mathcal{L}^*\\
        &=\|\hat{X_t}-\Sigma\|_F^2-\mathcal{L}^*\\
        &=\sum_{i=1}^r(\sigma_i(M)-x_{t,i})^2\\
        &\leq\sum_{i=1}^r(\sigma_i(M)-\tilde{x_{0,i}})^2\\
        &=\sum_{i=1}^rO(\exp\{-(1+k\sigma_i(M))\eta t\})\\
        &\leq O(\exp\{-(1+k\sigma_r(M))\eta t\})
    \end{align}
    
\end{proof}




\subsection{Proof for Theorem \ref{hrp-theory}}
\label{hrp-theory-proof}
\begin{theorem}[Restatement of theorem \ref{hrp-theory}]
    For HRP initialized Asymmetric LoRA with one preheating step and the same updating rules (LSI preheating with LSI fine-tuning or RSI preheating with RSI fine-tuning) under problem \ref{matrix-sensing}, if $\operatorname{rank}(M)\leq r$ we have
    \begin{align*}
        &\mathbb{E}_{LSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{b-R}{2b}\sum_{i=1}^r\sigma_i(M)^2,\\
        &\mathbb{E}_{RSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{a-R}{2a}\sum_{i=1}^r\sigma_i(M)^2,
    \end{align*}
    where $R$ is the rank at the preheating stage while $r$ is the rank at the real optimizing stage. 
\end{theorem}
\begin{proof}
    According to Theorem \ref{asym-close-form}, for LSI we have
    \begin{align}
        \hat{X_t}=\left[I-e^{-\eta \hat{Z_0}t}\right]M=(1-e^{-\eta t})U_B\begin{pmatrix}
            I_R\\&O_{(b-R)\times (b-R)}
        \end{pmatrix}U_B^\top M.
    \end{align}
    This means that for all $t$, the SVD decomposition of $X_t$ is the same. Then with one-step HRP, calculated $X_t=U\Sigma V^\top$ is the SVD decomposition of 
    \begin{align}
        \hat{X_\infty}:=\lim_{t\to\infty}\hat{X_t}=U_B\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U_B^\top M.
    \end{align}. 
    With assuming $M$ to be low-rank, $\hat{X_\infty}$ has rank
    \begin{align}
        rank(\hat{X_\infty})\leq rank(M)\leq r.
    \end{align}
    which means according to Theorem \ref{asym-wise} we have
    \begin{align}
        \hat{X_\infty}^*:=U\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U^\top \hat{X_\infty}=\hat{X_\infty}.
    \end{align}
    Then, we calculate the Frobenius norm of the converged result after HRP. In fact, we have
    \begin{align}
        \|\lim_{t\to\infty}X_t\|_F^2=&\left\|U\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U^\top M\right\|_F^2\\
        =&\left\|\hat{X_\infty}^*\right\|_F^2+\left\|U\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U^\top (M-\hat{X_\infty})\right\|_F^2\\        &+2\operatorname{Trace}\left(\hat{X_\infty}^{*\top}U\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U^\top (M-\hat{X_\infty})\right)\\
        =&\left\|\hat{X_\infty}^*\right\|_F^2+\left\|U\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U^\top (M-\hat{X_\infty})\right\|_F^2+2\operatorname{Trace}\left(\hat{X_\infty}^{*\top}(M-\hat{X_\infty})\right)\\
        \geq &\left\|\hat{X_\infty}\right\|_F^2+2\operatorname{Trace}\left(\hat{X_\infty}^{\top}(M-\hat{X_\infty})\right)\\
        =&\left\|\hat{X_\infty}\right\|_F^2+2\operatorname{Trace}\left(M^\top U_B\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U_B^\top \left(M-U_B\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U_B^\top M\right)\right)\\
        =&\left\|\hat{X_\infty}\right\|_F^2+2\operatorname{Trace}\left(M^\top U_B\begin{pmatrix}
            I_r\\&O_{(b-r)\times (b-r)}
        \end{pmatrix}U_B^\top U_B\begin{pmatrix}
            O_{r\times r}\\&I_{b-r}
        \end{pmatrix}U_B^\top M\right)\\
        =&\left\|\hat{X_\infty}\right\|_F^2.
    \end{align}
    According to Theorem \ref{lossineq}, we have
    \begin{align}
        \mathcal{L}(\lim_{t\to\infty}X_t)=\mathcal{L}(\hat{X_\infty}).
    \end{align}
    According to Theorem \ref{ms-loss}, we have in expectation:
    \begin{align}
        \mathbb{E}_{LSI}\mathcal{L}(\lim_{t\to\infty}X_t)=\frac{b-R}{2b}\|M\|_F^2.
    \end{align}
    For RSI in matrix sensing, it is equivalent to LSI with $X^\top$ approximating $M^\top$, thus for RSI we have
    \begin{align}
        \mathbb{E}_{RSI}\mathcal{L}(\lim_{t\to\infty}X_t)=\frac{a-R}{2a}\|M\|_F^2.
    \end{align}
\end{proof}



\section{Experiment Details}
\subsection{Detail for NLU tasks}
\label{apd-nlu-detail}
For the GLUE benchmark using T5-base model, we run experiments on a single NVIDIA L40 GPU and report the detailed hyperparameters in Table \ref{apd-nlu-tab}. 

\begin{table*}[htb]
    \label{apd-nlu-tab}
    \centering
    \caption{Hyperparameter settings for fine-tuning T5-base on GLUE.}
\begin{tabular}{lcccccc}
\toprule
 & CoLA & MRPC & QNLI & RTE & SST-2 & STS-B \\
\midrule
Optimizer & \multicolumn{6}{c}{AdamW} \\
Batch size & \multicolumn{6}{c}{8} \\
Learning rate & \multicolumn{6}{c}{$4\times 10^{-4}$} \\
Epochs & 5 & 2 & 2 & 2 & 2 & 2 \\
Dropout & \multicolumn{6}{c}{0.05} \\
LR Scheduler & \multicolumn{6}{c}{linear} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Detail for NLG tasks}
\label{apd-nlg-detail}
For the math reasoning tasks using large language model, we run experiments on four NVIDIA H100 GPU and report the detailed hyperparameters in Table \ref{apd-nlg-tab}. For prompt in the fine-tuning stage and the inference stage, we use the given prompt template provided by the model. 

\begin{table*}[htb]
    \label{apd-nlg-tab}
    \centering
    \caption{Hyperparameter settings for fine-tuning math reasoning tasks. }
\begin{tabular}{lccc}
\toprule
 & Llama2 & Qwen2 & Falcon3 \\
\midrule
Optimizer & \multicolumn{3}{c}{AdamW} \\
Batch size & \multicolumn{3}{c}{32} \\
Learning rate & \multicolumn{3}{c}{$5\times 10^{-5}$} \\
Epochs & \multicolumn{3}{c}{1} \\
Dropout & \multicolumn{3}{c}{0.05} \\
LR Scheduler & \multicolumn{3}{c}{cosine} \\
Training data type & float32 & bfloat16 & bfloat16 \\
Inference data type & \multicolumn{3}{c}{bfloat16} \\
Inference temperature & \multicolumn{3}{c}{0.8} \\
Inference top p & \multicolumn{3}{c}{0.95} \\
Inference max new tokens & \multicolumn{3}{c}{512} \\
\bottomrule
\end{tabular}
\end{table*}

\section{More Experiment Results}
\label{nlg-more}
For fine-tuning models on NLG tasks, we present the loss curve for Llama2 in \ref{llama-apd}, Falcon3 in \ref{Falcon3-apd}, and Qwen2 in \ref{qwen2-apd}. 

\begin{figure}
    \centering
    \includegraphics[width=.45\textwidth]{pic/Llama-2-7b-chat-hf.hA.pdf}
    \includegraphics[width=.45\textwidth]{pic/Llama-2-7b-chat-hf.A.pdf}
    \caption{Loss curves for fine-tuning meta-llama/Llama-2-7b-chat-hf on the MetaMathQA. Left: classic LoRA in different initialization strategies. Right: Asymmetric LoRA in different initialization strategies. 
    \label{llama-apd}}
    \includegraphics[width=.45\textwidth]{pic/Falcon3-7B-Instruct.hA.pdf}
    \includegraphics[width=.45\textwidth]{pic/Falcon3-7B-Instruct.A.pdf}
    \caption{Loss curves for fine-tuning tiiuae/falcon-7b-instruct on the MetaMathQA. Left: classic LoRA in different initialization strategies. Right: Asymmetric LoRA in different initialization strategies. 
    \label{Falcon3-apd}}
    \includegraphics[width=.45\textwidth]{pic/Qwen2-7B-Instruct.hA.pdf}
    \includegraphics[width=.45\textwidth]{pic/Qwen2-7B-Instruct.A.pdf}
    \caption{Loss curves for fine-tuning Qwen/Qwen2-7B-Instruct on the MetaMathQA. Left: classic LoRA in different initialization strategies. Right: Asymmetric LoRA in different initialization strategies. 
    \label{qwen2-apd}}
\end{figure}
