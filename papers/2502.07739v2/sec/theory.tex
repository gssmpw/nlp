\section{Theory: Why Initialization Matters}
\label{sec:theory}
In this section, we investigate LoRA's gradient flow, demonstrating how its initialization influences convergence. 
% Specifically, we show: 1) the random initialized matrix is sometimes almost NOT optimized, yielding the dynamic of LoRA close to AsymLoRA \cite{zhu2024asymmetry}, 2) with high probability of initialization, AsymLoRA can NOT converge to global minima in linear regression, and 3) at the beginning of LoRA, a better direction leads faster growth. 

\subsection{Framework}
\label{framework}

LoRA \cite{hu2021lora} adapts pre-trained models by updating weights through the product of two low-rank matrices scaled by a multiplier. Specifically, for a sub-module $W^{\operatorname{pre}}\in \mathbb{R}^{b\times a}$ in the pre-trained model, $r$-rank LoRA uses $A\in\mathbb{R}^{a\times r}$ and $B\in\mathbb{R}^{b\times r}$ as adapters, and the weight adaption is given by 
$$W^{\operatorname{pre}}\to W^{\operatorname{pre}} + \frac{\alpha}{r} BA^\top,$$ 
where $\alpha$ denotes the scaling factor. During the fine-tuning process, the original weights $W^{\operatorname{pre}}$ are kept frozen, while the parameters of $A$ and $B$ are updated through optimization algorithms. 

In this paper, we study the gradient flow of $A$ and $B$, which approximates gradient descent. For loss function $\mathcal{L}$, the update rule of gradient descent is given by:
\begin{align*}
    &A_{t+1}=A_t-\frac{\eta_A\alpha}{r} \nabla_W\mathcal{L}(W+\frac{\alpha}{r}B_t^\top A_t)^\top B_t,\\
    &B_{t+1}=B_t-\frac{\eta_B\alpha}{r} \nabla_W\mathcal{L}(W+\frac{\alpha}{r}B_t^\top A_t) A_t,
\end{align*}
where $\eta_A$ is the learning rate for optimizing $A$ while $\eta_B$ is for $B$. When the learning rate is sufficiently small, the update rule is a first-order approximation of the following gradient flow:
\begin{align}
    \begin{cases}
        \dot{A_t}=-\frac{\eta_A\alpha}{r} \nabla_W\mathcal{L}(W+\frac{\alpha}{r}B_t^\top A_t)^\top B_t,\\
        \dot{B_t}=-\frac{\eta_B\alpha}{r} \nabla_W\mathcal{L}(W+\frac{\alpha}{r}B_t^\top A_t) A_t,
    \end{cases}
    \label{abflow}
\end{align}
where the notion $\dot{A_t}$ denotes $\frac{dA_t}{dt}$ and $\dot{B_t}$ is defined similarly. 

LoRA is widely used with a \textbf{zero+random initialization schema}, where one adapter is initialized to a zero matrix and another to a random matrix. Every element in the random initialized matrix is independently and identically distributed from a Gaussian distribution $\mathcal{N}(0,\sigma^2)$. We call the $A_0=O_{a\times r}$ case left sketch initialization (LSI) and the $B_0=O_{b\times r}$ case right sketch initialization (RSI). \citet{zhu2024asymmetry} suggest orthogonal initialization, which replaces the Gaussian matrix with its $r$ singular vectors and demonstrates similar performance to Gaussian initialization. 

Updating $A$ and $B$ with different learning rates is suggested by \citet{hayou2024lora}, however, $A$ and $B$ are updated with the same learning rate in classic LoRA. Another variant is Asymmetric LoRA, as proposed by \citet{zhu2024asymmetry}, suggests freezing the random-initialized matrix while only updating the zero-initialized matrix for better generalization. In this paper, we consider two settings: 1) \textbf{classic LoRA} where $\eta_A=\eta_B=\eta$, and 2) \textbf{Asymmetric LoRA} where $\eta_A=0,\eta_B=\eta$ for RSI and $\eta_A=\eta,\eta_B=0$ for LSI. 

We note that for LoRA, what matters fine-tuning performance is the after-multiplied adapter $X_t=\frac{\alpha}{r} B_tA_t^\top$, rather than exact values of $A_t$ or $B_t$. For analyzing the dynamic of $X_t$, we further consider auxiliary matrices $Y_t=\frac{\alpha}{r}A_tA_t^\top $, $Z_t=\frac{\alpha}{r}B_tB_t^\top $, and $G_t=\nabla_W\mathcal{L}(W+X_t)$. Then, optimizing $A$ and $B$ via differential equations \ref{abflow} is equivalent to optimizing $X,Y,Z$ via the following differential equations:
\begin{align}
    \begin{cases}
        \dot{X_t}=-\eta_A Z_tG_t-\eta_B G_tY_t , &X_0=\frac{\alpha}{r}B_0A_0^\top,\\
        \dot{Y_t}=-\eta_A X_t^\top G_t-\eta_A G_t^\top X_t , &Y_0=\frac{\alpha}{r}A_0A_0^\top,\\
        \dot{Z_t}=-\eta_B X_tG_t^\top -\eta_B G_tX_t^\top  , &Z_0=\frac{\alpha}{r}B_0B_0^\top.
    \end{cases}
    \label{xflow}
\end{align}
% where $\init_X, \init_Y ,\init_Z$ are initialization methods for $X_t, Y_t, Z_t$ respectively. 

% In the following section, we study the flow of \ref{xflow} theoretically and find: 1) sometimes the random initialized matrix is almost NOT optimized, yieling the dynamic of LoRA close to AsymLoRA \cite{zhu2024asymmetry}, 2) the convergence of LoRA is sensitive to initialization, and 3) the better initialization leads faster growth at the beginning stage of fine-tuning. We note that though the widely used optimization algorithm for LoRA is not exactly the same as the one in \ref{xflow}, the conclusion we find in the following theory is still observed in practice.

% We note that though the widely used optimization algorithm for LoRA is not exactly the same as the one in \ref{xflow}, the conclusion we find in the following theory is still observed in practice. 

\subsection{Random initialization leads bad convergence}
\label{assymetric-bad}

As studied in \cite{zeng2023expressive}, the expressive power of LoRA for fully connected neural networks and the Transformer architecture requires the capability of achieving the best low-rank approximation to some well-trained sub-modules during optimization (the analysis is optimization-free and thus also applicable for Asymmetric LoRA). Formally, for a target model $W^{\operatorname{target}}$, adapter $X_t=\frac{\alpha}{r}B_tA_t^\top$ is expected to achieve the best rank-$r$ approximation of $W^{\operatorname{target}}-W^{\operatorname{pre}}$. 

Here we analyze the setting of matrix sensing, where the loss function is the Frobenius norm toward the target. With no loss to generality, we consider $W^{\operatorname{pre}}=O_{b\times a}$ while treating $M=W^{\operatorname{target}}-W^{\operatorname{pre}}$ as target for $X_t$. Then, the loss function $\mathcal{L}$ and gradient $G_t$ (w.r.t. $X_t=\frac{\alpha}{r}B_tA_t^\top$) can be expressed as
\begin{align}
    \label{matrix-sensing}
    \mathcal{L}=\frac{1}{2}\left\|\frac{\alpha}{r}BA^\top-M\right\|_F^2,~~\text{and}~~G_t=X_t-M.
\end{align}
According to Eckart-Young Theorem \cite{eckart1936approximation}, the global minima to this problem is the best rank-$r$ approximation of $M$, with minimum loss as follows:
\begin{align*}
    \mathcal{L}^*=\frac{1}{2}\sum_{i=r+1}^{\min\{a,b\}}\sigma_i(M)^2
\end{align*}
where $\sigma_i(M)$ is the $i$-th large singular value of $M$ and we assume non-zero singular values of $M$ are different from each other. For expressive power in more complex tasks, LoRA is expected to at least converge to some points with loss similar to $\mathcal{L}^*$ in the setting of matrix sensing. Unfortunately, we show that fine-tuned results of Asymmetric LoRA and classic LoRA are likely to have a significantly higher loss. 

\paragraph{Asymmetric LoRA struggles to converge well.}
We begin by examining the optimization landscape of Asymmetric LoRA \cite{zhu2024asymmetry}, which distinguishes itself from the classic LoRA by keeping the random-initialized matrix from being updated. Compared to the classic LoRA with an equivalent rank, Asymmetric LoRA results in a reduction of trainable parameters, thereby enhancing generalization capabilities. Its success in experiments demonstrates that the frozen parameters do not substantially compromise the convergence of classic LoRA. 

Through analysis of Asymmetric LoRA, we show that under zero+random initialization schema, it is likely to have converged results with loss much higher than $\mathcal{L}^*$. 
Specifically, we have the following theorems. 
\begin{theorem}
    \label{ms-loss}
    Consider Asymmetric LoRA under objective \ref{matrix-sensing} with Gaussian initialization and orthogonal initialization. With LSI and RSI in zero+random schema, we have:
    \begin{align*}
        \mathbb{E}_{LSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{b-r}{2b}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2,\\
        \mathbb{E}_{RSI}\left[\mathcal{L}(\lim_{t\to\infty}X_t)\right]=\frac{a-r}{2a}\sum_{i=1}^{\min\{a,b\}}\sigma_i(M)^2,
    \end{align*}
    where $\mathbb{E}$ represents the expectation with respect to randomness in initialization. 
\end{theorem}
Proof is included in Appendix \ref{ms-loss-proof}. Theorem \ref{ms-loss} shows that under zero+random initialization schema, the expected loss of converged result is the loss of a random rank-$r$ approximation to $M$, rather than that of best rank-$r$ approximation. 
This is severe when the target is also of low rank because singular values diverge a lot for low-rank matrices. As observed by \citet{wang2023cuttlefish}, the rank-dimension ratio of well-trained neural networks is usually relatively small, indicating a low-rank structure for $W^{\operatorname{pre}}$ and $W^{\operatorname{target}}$, thus also for $M=W^{\operatorname{target}}-W^{\operatorname{pre}}$. Therefore, under random initialization schema, the loss of converged results of Asymmetric LoRA is high in expectation. 

Theorem \ref{ms-loss} also demonstrates that when Asymmetric LoRA is configured with higher ranks, the loss for converged results decreases and performance is enhanced. 
Specifically, when using Asymmetric LoRA with full rank ($r=\min{a,b}$), it can converge to zero-loss minima in expectation. However, in practical applications, LoRA typically uses much smaller ranks to reduce memory usage and improve generalization, which results in poor convergence. 

\begin{theorem}
    \label{ms-pr}
    Under the conditions of Theorem \ref{ms-loss}, with the additional assumption that $r < \min{a,b}$, we have:
    \begin{align*}
        \operatorname{Pr}\left[\mathcal{L}(\lim_{t\to\infty}X_t)=\mathcal{L}^*\right]=0,
    \end{align*}
    where $\operatorname{Pr}$ represents the probability with respect to randomness in initialization. 
\end{theorem}
Proof is included in Appendix \ref{ms-pr-proof}. Aside from the high expected loss, Theorem \ref{ms-pr} further reveals that Asymmetric LoRA has zero probability of converging to the optimal rank-$r$ approximation of $M$ in matrix sensing problems. This indicates that the high expected loss is not merely due to occasional poor outcomes, but rather stems from the inherent nature of zero+random initialization Asymmetric LoRA to converge to arbitrary low-rank solutions. 

Compared with other theoretical works in matrix sensing \cite{tarmoun2021understanding, min2021explicit, ye2021global, wind2023asymmetric}, which demonstrates that the gradient flow of matrix sensing converges to global minima, our approach suggests a different conclusion because of difference in the following ways: 1) we allow the target matrix $M$ to be of high rank, which may not be fully approximate-able by a low-rank matrix $X$; 2) we consider the case where adapter matrices are initialized as zero and random, respectively, whereas the aforementioned works assume both matrices are initialized with small random values; and 3) our objective is Asymmetric LoRA, where only one matrix is optimized, which is different from their approaches. 


\paragraph{Classic LoRA has similar properties.}
\label{lora-approx-assymetric}
Different from Asymmetric LoRA, classic LoRA lets the random-initialized to be updated, thus has more trainable parameters and is thus more expressive. Though theory papers in matrix sensing \cite{tarmoun2021understanding, min2021explicit, ye2021global, wind2023asymmetric} suggest that optimizing both $A$ and $B$ results in convergence to $M$ with high probability under initialization where both $A$ and $B$ are initialized from small random matrices, we demonstrate that under zero+random initialization and fine-tuning schema, classic LoRA also struggles to converge well. First, we show that for some special initialization, classic LoRA also fails to converge to the best low-rank result in matrix sensing. 
\begin{theorem}
    \label{lora-bad}
    For objective \ref{matrix-sensing}, if there exists $i\leq r$ making $A,B$ with $A^\top v_i=O_a,B^\top u_i=O_b$. Then classic LoRA under LSI with $A_0=A,B_0=O_{b\times r}$ or RSI with $A_0=O_{a\times r},B_0=B$, we have for any $t$:
    \begin{align*}
        X_tv_i=O_b,~~~\text{and}~~~X_t^\top u_i=O_a,
    \end{align*}
    resulting in
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*\geq \frac{1}{2}[\sigma_i(M)^2-\sigma_{r+1}(M)^2]>0,
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$ and $u_i,v_i$ are the $i$-th column of $U,V$. 
\end{theorem}
Proof is included in Appendix \ref{lora-bad-proof}. Theorem \ref{lora-bad} shows that when the initialization of LoRA lies on a subspace orthogonal to some $u_iv_i^\top$ in the target, the orthogonal property will always hold in $X_t$, leading to a failure in converging to the best low-rank result. Target is always unknown in the initializing stage, thus these bad directions can not be easily avoided from random initialization. 

Aside from these specific initialization directions that cause poor convergence, we also show that in many fine-tuning cases, classic LoRA has a similar optimization dynamic with Asymmetric LoRA, thus sharing similar properties about bad convergence. This similar dynamic is empirically supported by experiments \cite{zhu2024asymmetry} where Asymmetric LoRA obtains similar results with classic LoRA in real-world scenarios. 

\begin{theorem}
    \label{lora-similar}
    For the gradient flow of classic LoRA $\{X_s\}_{0\leq s\leq t}$ and the gradient flow of Asymmetric LoRA $\tilde{X_t}$, assume $\{X_s\}_{0\leq s\leq t}$ is bounded by $R$ and the computed gradient $X_t\to G_t$ is Lipschitz in the Frobenius norm (same gradient calculator for classic LoRA and Asymmetric LoRA), then the difference between $\tilde{X_t}$ and $X_t$ is upper bounded by
    \begin{align}
        \|X_t-\tilde{X}_t\|_F=O(\eta R^3t^2),
    \end{align}
    when they have the same initialization in LSI or RSI. 
\end{theorem}
Proof is included in Appendix \ref{lora-similar-proof}. 
In fine-tuning tasks, pre-trained models have already acquired powerful capabilities from training on other tasks, enabling their effectiveness in real-world applications. Therefore, the fine-tuning stage is expected to make only modest modifications to the model parameters, making the assumption of bounded $X_t$ reasonable. Additionally, assuming the gradient calculator is Lipschitz in the Frobenius norm is justified because the backward propagation process of neural networks exhibits this property in bounded domains. Besides, this Lipschitz assumption is widely adopted in other theoretical works \cite{patel2022global}.

Theorem \ref{lora-similar} tells that in fine-tuning tasks, classic LoRA has a similar dynamic of Asymmetric LoRA, especially when training is insufficient (with small $t$). As addressed above, Asymmetric LoRA is likely to get random low-rank results rather than the best low-rank result. With a similar dynamic, classic LoRA also suffers from it. 

Theorem \ref{lora-similar} tells that in fine-tuning tasks, classic LoRA exhibits dynamics similar to those of Asymmetric LoRA, particularly during the early stages of training (small $t$). As addressed above, Asymmetric LoRA tends to converge to arbitrary low-rank solutions rather than the optimal low-rank approximation under zero+random initialization schema. Given this dynamic similarity, classic LoRA inherits the same limitation. 

























\subsection{Wise initialization leads good convergence}
\label{theory-observation}
Then, we illustrate that the observed limitation in convergence is mainly attributed to initialization, rather than the training methods. In fact, we show that by merely altering the random initialization to a more informed one, both Asymmetric LoRA and classic LoRA exhibit exponential convergence in the matrix sensing problem. Specifically, we present the following theorems. 
\begin{theorem}
    \label{asym-wise}
    For Asymmetric LoRA under objective \ref{matrix-sensing} in RSI with $A_0=V_M[:,:r],B_0=O_{b\times r}$ or LSI with $A_0=O_{a\times r},B_0=U_M[:,:r]$, we have
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*=O(\exp\{-\eta t\}),
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$. 
\end{theorem}
Proof is included in Appendix \ref{asym-wise-proof}. 
Theorem \ref{asym-wise} shows that when initialized with the principal singular vectors of the target, optimizing with Asymmetric LoRA has exponential convergence to the best low-rank approximation of $M$. This result contrasts with Theorem \ref{ms-loss} and Theorem \ref{ms-pr} solely in terms of the initialization method. Therefore, the problem of convergence limitation of Asymmetric LoRA is attributed to random initialization. Furthermore, when the initialization is appropriately configured, optimizing a single LoRA adapter while frozen another one is sufficient to ensure convergence in matrix sensing.
\begin{theorem}
    \label{lora-wise}
    For classic LoRA under objective \ref{matrix-sensing} in RSI with $A_0=V_M[:,:r],B_0=O_{b\times r}$ or LSI with $A_0=O_{a\times r},B_0=U_M[:,:r]$, we have
    \begin{align*}
        \mathcal{L}(X_t)-\mathcal{L}^*=O(\exp\{-(1+k\sigma_r(M))\eta t\}),
    \end{align*}
    where $M=U_M\Sigma_MV_M^\top$ is the SVD decomposition of $M$ and $k=\frac{\sqrt{1+4\sigma_1(M)}-1}{\sigma_1(M)}>0$.
\end{theorem}
Proof is included in Appendix \ref{lora-wise-proof}. 
Theorem \ref{lora-wise} demonstrates that with the same initialization, the classic LoRA method is also guaranteed to converge to the best low-rank approximation of $M$, thereby avoiding the unfavorable scenario described in Theorem \ref{lora-bad}. It is also noteworthy that in this initialization, classic LoRA has a higher convergence rate than Asymmetric LoRA in the context of matrix sensing, while both converging to the best low-rank result.

In practice with pre-trained model modules $W^{\operatorname{pre}}$ and its well fine-tuned version $W^{\operatorname{target}}$, LoRA is expected to make each $\frac{\alpha}{r}B_iA_i^\top$ a best low-rank approximation of $W_i^{\operatorname{target}}-W_i^{\operatorname{pre}}$. However, calculating the SVD decomposition of $W_i^{\operatorname{target}}-W_i^{\operatorname{pre}}$ is always impossible because $(W_i^{\operatorname{target}})$ is unknown. 

Previous work proposes some initialization methods for LoRA, such as PiSSA and LoRA-GA \cite{meng2024pissa,wang2024lora}. PiSSA \cite{meng2024pissa} considers using SVD decomposition of $W^{\operatorname{pre}}$ as initialization while LoRA-GA \cite{wang2024lora} suggests using SVD decomposition of $\nabla_{W^{\operatorname{pre}}}\mathcal{L}(W^{\operatorname{pre}})$. Their methods rely heavily on $W_i^{\operatorname{pre}}$ and is not theoretically guaranteed for better convergence. 
% However, their methods rely on the similarity between $W_i^{\operatorname{target}}-W_i^{\operatorname{pre}}$ and $W_i^{\operatorname{pre}}$. 










