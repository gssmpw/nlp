\section{Introduction}

Recent advances in foundational models, especially large language models, have achieved remarkable success in a diverse range of applications \cite{bommasani2021opportunities,touvron2023llama,achiam2023gpt,fu2024championing,fu2024towards}. Nevertheless, owing to their substantial scale, the conventional full-parameter fine-tuning (FPFT) approach, in which all the model's parameters are updated for specialized tasks, has become progressively more formidable and inefficient. Parameter-efficient fine-tuning methods concentrate on selectively updating smaller parameter subsets or integrating lightweight adapters, thus substantially diminishing computational and storage demands \cite{hu2021lora,liu2022few,kumar2022fine} . This transition not only renders the fine-tuning process more tractable but also unlocks new prospects for deploying these potent models in resource-constrained settings. 

% PEFT to LoRA
A leading technique in this area is Low-Rank Adaptation (LoRA) \cite{hu2021lora}, which introduces lightweight low-rank adapters to the pre-trained weight matrices. LoRA has been extensively applied and has manifested substantial achievements in tailoring large language models \cite{hu2021lora,mao2025survey} and image generation models \cite{filatov2023low,ji2024advlora} for a variety of downstream applications. Although LoRA presents significant computational benefits in practical scenarios, it still proves less effective than FPFT when efficiency is not a primary consideration \cite{biderman2024lora}. 

% LoRA enhancement
To enhance LoRA's effectiveness, many variants have emerged, with initialization improvement being one line of approach. In classic LoRA, one adapter is initialized with a zero matrix and the other with a random matrix. This causes the fine-tuning process to commence from the pre-trained weights. However, due to the random-initialized adapter, the fine-tuning process begins with a random direction. Methods like PiSSA \cite{meng2024pissa} and LoRA-GA \cite{wang2024lora} use Singular Value Decomposition (SVD) of pre-trained weights and gradients for initialization, proving its significance in LoRA's performance. However, these methods rely heavily on pre-trained models and lack theoretical guarantees for better performance, calling for more research to optimize LoRA.

% Main of this paper
Delving into the optimization landscape of LoRA, this paper theoretically demonstrates that initialization plays a crucial role in achieving optimal performance. Nevertheless, random initialization methods cause the adapters to be initialized from a random direction, which leads to sub-optimal fine-tuning results with high probability. 
To address this issue, we propose High-Rank Preheating (HRP), an initialization enhancement algorithm. It uses a few steps of high-rank LoRA as a preheating step for wise initialization before the real low-rank LoRA optimization. HRP not only inherits the convergence advantages related to high-rank LoRA but also keeps good generalization properties from low-rank LoRA by keeping the number of trainable parameters small.
% Experiments on GLUE show the benefits HRP brings. 

Our contributions can be summarized as follows:

\begin{enumerate}
    \item On the theory side, we demonstrate that initialization is important for LoRA to converge to optimal results. We analyze the gradient flow of classic LoRA and Asymmetric LoRA, one LoRA variant that only updates the zero-initialized matrices. We demonstrate that 1) with random initialization, Asymmetric LoRA hardly converges to the best low-rank approximation when approximating a matrix, 2) classic LoRA has a similar dynamic with Asymmetric LoRA in fine-tuning schema and also can not converge well from some initialization, and 3) with wise initialization, both Asymmetric LoRA and classic LoRA converges exponentially to the best low-rank approximation. 
    \item On the algorithm side, we propose High-Rank Preheating (HRP), a LoRA initialization algorithm to approach the wise initialization suggested in theory. In addition to the main LoRA with low rank, HRP further employs a few steps of Asymmetric LoRA optimization as preheating and treats the SVD decomposition of preheated adapters as approximations of targets' SVD decomposition, which is guaranteed to converge well. With only modification in initialization, HRP is theory-guaranteed to make LoRA achieve convergence power comparable to high-rank LoRA (preheating stage) while preserving generalization properties due to maintaining the same number of trainable parameters. 
    \item On the experimental side, we conducted experiments on neural language understanding (NLU) tasks and neural language generation (NLG) tasks across various models to evaluate the effectiveness of HRP. In NLU tasks, classic LoRA with HRP outperforms its other variants and achieves comparable performance with full-parameter fine-tuning, Asymmetric LoRA with HRP outperforms other initialization methods and achieves comparable performance with classic LoRA while holding about half trainable parameters in main optimization. 
\end{enumerate}









