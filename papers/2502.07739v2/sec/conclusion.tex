\section{Conclusion}

In this paper, we first theoretically show the important role of LoRA initialization for convergence, where widely-used random initialization is likely to make Asymmetric LoRA converge to random low-rank results, rather than the best low-rank results and classic LoRA has similar properties. 

Then, to address the problem, we propose HRP, an initialization algorithm that makes LoRA with low rank have better convergence properties (comparable with high-rank LoRA) while maintaining the small number of trainable parameters (thus holds generalization). HRP utilizes a few steps of high-rank LoRA optimization as the preheating stage and uses main singular vectors as initialization for low-rank LoRA. 

We further evaluate the effectiveness of HRP through experiments on NLU and NLG tasks and various models, where HRP makes LoRA achieve better performance compared with other initialization strategies. 



\section*{Impact Statement}

This paper presents work whose goal is to advance the initialization of LoRA. A potential impact of this paper is guiding practitioners on more effective initialization for fine-tuning deep learning models using LoRA. After thorough consideration and analysis, it can be firmly stated that this research has no ethical aspects that could raise concerns. 







