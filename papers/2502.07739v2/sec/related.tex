\section{Related Work}
\label{sec:related}
In this section, we provide an overview of the related work, including works on the analysis of initialization, LoRA variations, and theory results about matrix sensing. 


\subsection{Role of initialization}
Parameter initialization is one of the initial elements that largely account for the final model performance \cite{glorot2010understanding,mishkin2015all}.
Existing initialization methods are designed to control the norms of network parameters via Gaussian initialization \cite{he2015delving} or orthonormal matrix initialization \cite{saxe2013exact} with different variance patterns.
Currently, learning-based initialization methods are explored: \citet{dauphin2019metainit} propose to optimize the curvature, \citet{zhu2021gradinit} suggest optimizing the loss reduction of the first stochastic step, while \citet{yang2022towards} optimize the cosine similarity of sample-wise gradients. 

The initialization for LoRA is also a hot topic in previous research. 
\citet{hayou2024impact} study the difference between the left sketch and the right sketch from a stability perspective. \citet{buyukakyuz2024olora} leverage orthonormal matrix initialization through QR decomposition. \citet{meng2024pissa,wang2024lora} initialize adapters with the principal components of the weight matrices and their gradients in pre-trained models. \citet{li2024crucial} bring Nystr√∂m initialization to LoRA for better convergence. Compared to these works, our method does not require further knowledge about the pre-trained weights or the gradient. 

\subsection{LoRA variations}
Since the introduction of the original LoRA technique \cite{hu2021lora}, there are various efforts to enhance LoRA further. \citet{zhang2023adalora} adaptively allocate the parameter budget among weight matrices. \citet{zhu2024asymmetry} freeze the random-initialized matrices for better generalization. \citet{xia2024chain,malinovsky2024randomized} suggest using a chain of LoRA for better expressive power. 
To further decrease the number of trainable parameters, \citet{balazy2024lora,ponkshe2024initialization} suggest injecting small matrices between LoRA blocks and \citet{kopiczko2023vera,renduchintala2023tied,song2024sharelora} suggest sharing LoRA weights across different modules. Compared to these works, this paper focuses on enhancing LoRA from the perspective of initialization. 

\subsection{Matrix sensing}
We also note some works about matrix sensing here. Matrix sensing considers approximating a low-rank matrix by two multiplied matrices \cite{chi2019nonconvex}. Theoretical works show that this training paradigm converges in both symmetric \cite{tarmoun2021understanding,min2021explicit} and asymmetric \cite{ye2021global,wind2023asymmetric} settings when adapters are initialized to small random matrices. Compared to these works, this paper focuses more on the realistic setting for LoRA, where the target may have a high rank and initialization is not small. 








