\section{Problem Formulation and Notation}

\input{figs_tex/overview_fig}

% In this work, we address the challenge of efficiently evaluating multi-task robot policies in the real world.
The objective of this work is to design an efficient strategy to evaluate robot policies across tasks while balancing the cost of experimentation. 
Consider a fixed set of $M$ robot policies, denoted by $\mathcal{P} = \{\pi_1, \pi_2, \ldots, \pi_M\}$ and a set of $N$ tasks $\mathcal{T} = \{T_1, T_2, ..., T_N\}$. 
% Due to practical constraints, only a subset of $N$ tasks $A \subset \mathcal{T}$ is used.
Each task $T_j \in \mathcal{T}$ is a finite-horizon MDP defined by states, actions, and a high-level natural language instruction $L_i$.
% Since robot policies can be sensitive to changes in language instructions~\cite{}, we assume a one-to-one mapping between language instructions and tasks.

Our framework is policy-agnostic and does not assume access to policy model weights, and can be applied to engineered robot \textit{systems} in addition to end-to-end models.

\textbf{Population Parameter Estimation}. 
We formulate the problem as population parameter estimation, similar to probabilistic matrix factorization~\cite{mnih2007probabilistic}.
Let the performance of a policy ${\pi_i \in \mathcal{P}}$ on a task $T_j \in \mathcal{T}$ be represented by the random variable $X_{ij}$ with distribution $P_{ij}$, from which we can sample evaluations $\trial \sim \truedist$. Here, $\truedist$ represents the ``true'' performance distribution.
Since the underlying distribution $\truedist$ is unknown, the goal of population parameter estimation is to estimate a distribution $\learneddist$ that models real-world evaluation outcomes from $\truedist$.
% , which is the distribution corresponding to a random variable $\hat{X}_{ij}$, 
We use $\theta_{ij}$ to represent the parameters of the learned distribution $\learneddist$.
For example, $\theta_{ij}=[\mu, \sigma]$ if $\learneddist$ is a Gaussian distribution.
Given a limited number of observed samples from the true distribution, $\trial^1, ..., \trial^n \sim \truedist$, the goal is to estimate the parameters of an estimated distribution $\theta_{ij}$.
Our setting also has samples from other random variables, $X_{kl}$ corresponding to different policy-task pairs. 
Therefore, in this work we want to estimate $\Theta = \{\theta_{ij}\}_{i, j = 1}^{i=M, j = N}$ for all policy-task pairs given a dataset $\mathcal{D}=\{\trial^k\}$.
These distributions can be visualized as a grid of policy-task pairs as shown in Figure~\ref{fig:overview}.

The aim is to estimate the parameters of $\learneddist$ of all policy-task combinations by leveraging shared information across this matrix.
However, it is infeasible to directly evaluate all policy-task pairs due to cost constraints. 
Therefore, we adopt an active testing approach, where the objective is to iteratively select the most informative experiments $(\pi_i, T_j)$ to efficiently learn $\Theta$. 
% Given a limited evaluation budget, we develop active experiment selection strategies to efficiently samples evaluations that improve the accuracy of these parameter estimates.

\textbf{Active Testing.}
% Since it is difficult to sample a large set of outcomes $\mathcal{D}$ due to experimenter effort, we take an active testing approach.
We apply an active learning paradigm to learn a population parameter estimator $f(\pi_i, T_j)$.
As such, we define acquisition functions to guide the selection of task-policy pairs or tasks alone, and then sample experiments that are most informative.
First, we define an acquisition function $a(\pi_i, T_j)$, and the next experiment is selected by maximizing this function over all possible experiments:
\begin{equation}
    (\pi_i^*, T_j^*) = \arg\max_{(\pi_i, T_j)} a(\pi_i, T_j).
\end{equation}
Although these acquisition functions are informative, we want a balance between selecting informative experiments and their costs.

\textbf{Evaluation Cost}. In real-world evaluation, each policy-task evaluation incurs a cost. 
Let $c_{\text{eval}}(T_j)$ denote the cost of a single evaluation of a policy on task $T_j$.
We make a simplifying assumption that this cost is agnostic to changes in the policy under evaluation, that often being a configurable software option.
This cost could include the time required to execute the policy, the resources consumed during evaluation, or the manual supervision required to reset the scene.
Furthermore, switching between tasks typically incurs a larger cost involving a reconfiguring the scene or the robot. 
We define this switching cost $c_{\text{switch}}(T_j, T_k)$ as the cost associated with transitioning from task $T_j$ to $T_k$.
% This cost depends on the specific tasks being evaluated.
For a sequence of tasks that have been evaluated $T_{i_1}, \ldots, T_{i_L}$ (where each $i_j \in N$), we compute the total cost of evaluation as:

% Do we need this part? wrote it like this to take up more space
$$c_{\text{total}} = \sum_{j=1}^N c_{\text{eval}}(T_{i_j}) + \sum_{j=1}^{N-1} c_{\text{switch}}(T_{i_j}, T_{i_{j+1}})$$

Given these costs, the problem is to design an evaluation strategy that minimizes the total cost of evaluation while learning the population parameters of test instances.




\section{Method}

We aim to design a framework for sampling experiments for multi-task robot policies.
Our framework consists of two parts: (1) learning a surrogate model to estimate the population parameters of a test instance and (2) designing strategies to sample experiments in a cost-efficient manner.
The surrogate model leverages task and policy representations that define an experiment to have a better estimate of the overall performance distributions.
Then, we use this surrogate model to compute the expected information gain of different experiments.
We then use the expected information gain along with the cost of switching tasks to conduct active testing.

% \textbf{Surrogate Model.}
\subsection{Surrogate Model}
As we evaluate our robot policies across tasks, we track the outcomes of each trial to aggregate a dataset $\mathcal{D}$ over time.
% In the process of evaluating multi-task robot policies, we collect a dataset of previous outcomes from trials $\trial \in \mathcal{D}$.
Each of these outcomes are realizations of a true underlying distribution $\truedist$.
Our goal is to learn a surrogate model from $\mathcal{D}$ that predicts the population parameters $\theta_{ij}$ of a performance distribution  $\learneddist$. 
As more evaluation rollouts are conducted, we add the outcomes to $\mathcal{D}$ and continue training the surrogate model.
% A single experiment can be defined by the policy $\pi_i$ being executed on task $t_j$.

%The surrogate model should be able to capture the performance relationship between tasks and policies.
To train an effective surrogate model,  we use notions of similarity between tasks and policies. 
% For instance, tasks that involve similar skills, such as "picking up an apple" and "picking up an orange," should result in similar performance distributions for a given robot policy.
Thus, we need a representation that captures the similarities between policies and tasks with respect to their performance distributions.
We define a policy embedding $e_{\pi_i}$ and task embedding $e_{T_j}$, where similar performance distributions in task and policy can be captured based on the embeddings.
These policy and task representations are then provided as input to an MLP that predicts the estimated population parameters:
\begin{equation}
    \hat{\theta}_{ij} = f(\pi_i, T_j) = \text{MLP}(e_{\pi_i}, e_{T_j}).
\end{equation}

\textbf{Task and Policy Representation.} 
\label{sec:method}
To define the task and policy embeddings $e_{\pi_i}, e_{T_j}$, we design various types of embeddings.
In practice, we cannot know the relationship between policies in advance as we are conducting evaluation.
Therefore, we define the policy embedding to be a fixed, randomly initialized embedding to act as an identifier for the policy in a given experiment.

For the task embedding $e_{\pi_i}$, we leverage language embeddings from MiniLMv2~\cite{gu2024minillm} which we reduce to 32 dimensions using PCA over all tasks.
However, we found that language embeddings overly focus on nouns as opposed to verbs, which causes issues as actions with similar nouns but different verbs would be closer together verbs with the same nouns. 
Thus, we apply the following procedure to mitigate this issue.
We (1) use part-of-speech tagging to extract all verbs and verb phrases, (2) compute a language embedding for the verb $e_{T_j}^{\text{verb}}$ and for the entire task description  $e_{t_j}^{\text{task}}$, and then (3) compute the task embedding 
\begin{equation}
     e_{T_j} =  0.8 \cdot e_{T_j}^{\text{verb}} +  0.2\cdot e_{T_j}^{\text{task}} + 0.1\cdot \mathcal{N}(0,1).
\end{equation}
We also found that the embeddings were often too close across multiple tasks, and we found that adding a slight noise term helped separate close embeddings.
Experiments on this result are in Section~\ref{sec:task_results}.
% These embeddings were them 

\textbf{Population Parameter Estimation.}
Outcomes in robot learning can take the form of continuous values like rewards, time to completion, or task progress, and binary values like task success.
Thus, the underlying distribution from the surrogate model depends on the type of task.
We consider two types of underlying distributions.
When $X_{ij}$ is continuous, $\learneddist$ takes the form of a mixture of Gaussians with $K$ components,
\begin{equation}
    % P(X_{ij}; \theta_{ij}) = \sum_{k=1}^{K}\pi_k\mathcal{N}(\mu_k, \sigma_k),\\
    \sample \sim \learneddist = \sum_{k=1}^{K}p_k\mathcal{N}(\mu_k, \sigma_k),
\end{equation}
where $\pi_k, \mu_k,$ and $\sigma_k$ are the mixing coefficients, means, and standard deviations of the Gaussian components respectively that are predicted from the surrogate model $\theta_{ij} = f(\pi_i, T_j)$.
% The parameters $\theta_{ij}=\{\pi_k, \mu_k, \sigma_k\}^{K}_{k=1}$ are predicted using the surrogate model.
We thus train the surrogate model with a mixture density loss~\cite{bishop1994mixture,ha2018world} to minimize the negative log-likelihood of the observed data under the mixture model.
In our experiments on continuous outcome distributions, we use $K=2$ Gaussian components, as robotics performance is often bimodal; robots either fail catastrophically or they maintain non-zero performance.

In the case where $X_{ij}$ is binary, indicating success or failure, $\learneddist$ takes the form of a Bernoulli distribution:
\begin{equation}
    % P(X_{ij}; \theta_{ij}) = p^{x_{ij}}(1-p)^{1-x_{ij}}
    \sample \sim \learneddist =  p^{x_{ij}}(1-p)^{1-x_{ij}},
\end{equation}
where $\theta_{ij} = \{p \in [0,1] \}$ represents the success probability predicted by the surrogate model trained using cross-entropy loss. 



\subsection{Cost-aware Active Experiment Selection}

% We can now use this surrogate model for selecting policy-task robot experience to execute. 
We explore cost-aware, active-experiment acquisition functions that guide selection of experiments based on their expected utility while considering associated costs.
To define the acquisition function, we first focus on how to measure the informativeness of a policy-task evaluation, which we capture through expected information gain.

\textbf{Expected Information Gain.}
Expected Information Gain (EIG) quantifies the value of an experiment by estimating how much it reduces the predictive uncertainty of the performance distribution for a policy-task pair. 
Since the surrogate model estimates performance \textit{distributions}, we define the EIG of a policy-task pair using a Bayesian Active Learning by Disagreement (BALD)~\cite{houlsby2011bayesian} formulation for probabilistic models
\begin{equation}
    \mathcal{I}(\pi_i, T_j) = \underbrace{\mathbb{H}[\learneddist]}_{\text{marginal entropy}} - \underbrace{\mathbb{E}_{\theta_{ij} \sim f(\theta_{ij}|\mathcal{D})} [\mathbb{H}[\learneddist | \theta_{ij}]]}_{\text{expected conditional entropy}}.
\end{equation}
% The first term is the entropy over the marginal predictive distribution
The first term represents the marginal entropy over $Q_{ij}$, which quantifies the total uncertainty in $Q_{ij}$. 
The second term corresponds to the expected conditional entropy over multiple samples of parameters $\theta_{ij}$.
% where $P(\learneddist|\mathcal{D})$ is the marginal predictive distribution over the performance distribution $\learneddist,$ while $P(\learneddist | \theta_{ij})$ is the conditional predictive distribution given a sampled set of parameters $\theta_{ij}$. 
% Thus, the first term is the entropy over the marginal predictive distribution and captures the epistemic and aleatoric uncertainties of the model's predictions, while the second term captures the expected aleatoric uncertainty.
Thus, $\mathcal{I}(\pi_i, T_j)$ captures the disagreement between multiple samples of distributions.
For example, if 10 sampled parameters for a Gaussian have very different distributions, then their disagreement will be high.
Since the entropy of a mixture of Gaussians generally lacks a closed-form solution, we estimate the entropy by discretizing the empirical distribution into $n=25$ bins for which to compute entropy over.

BALD ensures the EIG score is higher in test instances where there is disagreement in the predicted distributions across sampled parameters.
In this case, we define the acquisition functions $a(\pi_i,T_j)=\mathcal{I}(\pi_i,T_j)$.



To compute the expected information gain, we require multiple samples of $\Theta_{ij}$; however, we only train a single MLP.
Inspired by Monte Carlo dropout~\cite{gal2016dropout} and past literature~\cite{loquercio2020general,ledda2023dropout}, we apply dropout only at test-time to compute multiple samples of $\theta_{ij}$ from the surrogate model $f(\cdot)$.
% We found that dropout early in the sampling process would lead to overfitting on the small dataset, leading to low entropy in the samples of $\Theta_{ij}$.

% We use Monte Carlo (MC) dropout~\cite{gal2016dropout} to estimate sampling from $f(\Theta_{i,j} | \mathcal{D})$. 
% We found that Monte Carlo (MC) dropout~\cite{gal2016dropout}, where dropout is applied during training time, then at test time to compute multiple samples of $\theta_{ij}$ from the surrogate model $f(\cdot)$, leads to overfitting on the small amounts of training data available early in the evaluation process. 
% This overfitting lead to the disagreement between sampled parameters to be nearly zero, causing the EIG scores to be useful after hundreds of expensive evaluations.


% However, in the setting of robot evaluation where experiments are expensive, our surrogate model is initially trained with minimal data.
% We found that dropout during training time early in the evaluation process led to overfitting, as few examples had been seen, which caused the disagreement between sampled parameters to be essentially zero.
% Thus, the EIG scores would only be useful after hundreds of expensive evaluations.
% Past work in the active learning literature~\cite{tosh2022targeted} avoids this cold start problem by initializing their model with hundreds or thousands of training instances, which is impractical for an evaluation framework.
% To address this cold start problem, we found that restricting dropout to test time only effectively allowed our model to provide useful EIG scores earlier in the evaluation process.


\textbf{Cost-Aware EIG}.
While EIG effectively quantifies the informativeness of an experiment, it does not consider the costs of conducting evaluation.
To make EIG cost-aware, we design the following acquisition function based on prior work that simply integrates cost with a multiplicative factor~\cite{paria2020cost,lee2020cost}:
\begin{equation}
    a_{\text{cost-aware}}(\pi_i, T_j, T_\text{current}) = \dfrac{\mathcal{I}(\pi_i, T_j)}{(\lambda \cdot c_{\text{switch}}(T_{\text{current}}, T_j))+1},
    \label{eq:cost_aware_eig}
\end{equation}
where $\mathcal{I}(\pi_i, T_j)$ represents EIG for the policy $\pi_i$ on task $T_j$, $c_{\text{switch}}(T_{\text{current}}, T_j))$ is the cost of switching from the current task $T_{\text{current}}$ to a new task $T_j$, and $\lambda$ is a hyperparameter that controls the cost sensitivity.

\input{figs_tex/algo}


\textbf{Active Experiment Selection}.
% As shown in Algorithm~\ref{alg:short_pseudocode}, we use this acquisition function to iteratively sample experiments.
We use this acquisition function to iteratively sample experiments, as shown in Algorithm~\ref{alg:short_pseudocode}.
To mitigate the cold-start problem in active learning, we initialize the dataset $\mathcal{D}$ with a a single randomly-selected task, for which every policy is evaluated 3 times.
We then train the surrogate model on this data.
At each query step, the acquisition function $a(\pi_i, T_j)$ is computed for all policy-task pairs, which quantifies their informativeness weighted by the cost.
To compute the entropy over model parameters for the EIG metric, we use MC dropout to sample 10 predicted outcome distributions.
To balance exploration and exploitation, we use an epsilon-greedy strategy with a rate of $\epsilon=0.1$.
The selected experiment $(\pi_i, T_j)$ is then executed 3 times, and the observed outcomes are added to the dataset $\mathcal{D}$.
We found in preliminary experiments that 3 trials per selected experiment was often better for cost-efficient population parameter estimation.
Given these new outcomes in the dataset, we keep training the surrogate model on the updated dataset improve its predictions over time.
% In our experiments, we continue this process for a fixed number of queries; however, 

\section{Evaluation on Offline Datasets}



To evaluate our active testing framework, we leverage evaluations that have already been conducted which we then sample offline.
We use experiments from the HAMSTER paper~\cite{li2025hamster}, the  OpenVLA paper~\cite{kim24openvla}, and from MetaWorld~\cite{yu2020meta}, as visualized in Figure~\ref{fig:datasets}.
For MetaWorld, we train two versions, one focused on understanding our framework's ability in evaluating different policies and another on evaluating multiple checkpoints of a single policy.
Each of these datasets can be modeled with different underlying distributions and have varying costs, semantic diversity, and skills.
More details on training for MetaWorld, switching costs for the datasets, and other details can be found in Appendix~\ref{app:offline_datasets}.

\input{figs_tex/exp_pics}


\textbf{HAMSTER.}
We use evaluations from the HAMSTER paper~\cite{li2025hamster}, which evaluates a hierarchical VLA model against 4 other policies such as OpenVLA~\cite{kim24openvla} and Octo~\cite{octo_2023} across 81 tasks. 
These 81 tasks are of varying complexity, with diverse task types, objects, and linguistic variation that were evaluated once each.
Their work uses a continuous task progress metric; however, since they only evaluated each policy-task pair once, we treat the single continuous value as the mean of a Gaussian distribution with a fixed standard deviation.
For switching cost, we add an additional cost if the policy switches from one task type to another. 
More details on this cost can be found in Appendix~\ref{app:offline_datasets}.
% The success of that one policy-task pair evaluation is treated as the mean success metric for that experiment. 

\textbf{OpenVLA.}
We use evaluations from the OpenVLA paper~\cite{kim24openvla}, which compares 4 policies over 29 tasks. 
In their paper, some tasks allow for partial success (0.5). 
For simplicity, we round the partial successes down to maintain a binary success metric.
OpenVLA also provides results across two embodiments.
Therefore, in addition to a higher cost term to switching tasks that require a large scene reset, we add an additional cost term to switch between embodiments. More details in Appendix~\ref{app:offline_datasets}.

Given these datasets, we show that the types of policy and task representations that are useful for active learning, and then we can leverage the surrogate model for cost-aware active experiment selection. 
\input{figs_tex/embed_model_exps}

\textbf{MetaWorld Policies.}
MetaWorld~\cite{yu2020meta} is an open-source simulated benchmark containing a set of 50 different manipulation environments for multi-task learning.
We train 10 policies on every environment with different policy architecture sizes and varying amounts of noise in the robot's state to create robot policies with diverse behaviors.
We then collected 100 trajectories of each policy-task pair to serve as an approximation of the true performance population distribution.
By using the MetaWorld simulator, we can estimate performance distributions for binary success rate and a continuous reward normalized between 0 and 1.
The switching cost is set based on whether the target object of the scene, such as a drawer, is swapped out for another object, like a lever.
This dataset allows us to understand how our framework can learn the performance distributions across diverse policies.

\textbf{MetaWorld Checkpoints.}
Evaluation on a robot is not only used for comparing policies, but also to find the best checkpoints.
As such, we train a single state-based MetaWorld policy, store 11 checkpoints over the training process, and then evaluate them.
In preliminary experiments, we found that the checkpoint-based setting has a lower-rank structure in terms of the performance distributions.
This offline dataset allows us to exploit the shared information across policies.

Given these datasets, we will discuss two experiments in the next two sections: that shows that language is an informative prior in modeling the performance relationships between tasks, and that our surrogate model can be used for cost-aware experiment selection.

% . Given an evaluation score of 7.5, for example, we generate a sequence of seven successes (1s) and three failures (0s).


% This trajectory data was made of pairs of a state and the action taken from that state. Then, for each environment, we generated a natural language query that describes the task in the environment. We then trained an MLP-based model using the natural language query, embedded as a 768-dimensional vector, and the 39-dimensional state vector in the trajectory to output the 4-dimensional action vector. The resulting model is a language-conditioned behavior-cloned policy trained on all the language instructions and environment rollouts. This model is effectively trained to be a single generalist language-conditioned policy for the Metaworld environment.

% We trained 10 of these generalist policies for Metaworld, varying the training data with randomness to ensure each policy was fairly distinct from another, and then rolled out each of them 100 times each on each of the 50 multi-task learning environments. The success and reward data is used in our parameter estimations for policy evaluation. 



