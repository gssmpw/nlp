\appendices

\section{Offline Dataset Details}
\label{app:offline_datasets}

\subsection{HAMSTER}
For HAMSTER, we have a cost of 0.5 per execution of an experiment, then an additional switching cost of +1 if a task is of the same task type but requires adding/removing objects.
If a new task type is selected, we then add a cost of +2 for requiring new, often large, objects to be brought into the scene.

\subsection{OpenVLA}
For OpenVLA evaluation, we have a cost of 0.5 per execution of an experiment. 
If a task is changed, such as moving an eggplant to lifting a battery, a cost of 1 is applied.
OpenVLA also has multiple embodiments available, Bridge and the Google Robot.
If there is an embodiment change, we set the changing cost to 3, as this change is relatively large.

\subsection{MetaWorld Policies/Checkpoints}
For MetaWorld evaluation, we have a cost of 0.5 per execution of an experiment.
In MetaWorld tasks, some tasks keep the same objects in the same scene such as opening or closing a window, while others would require new objects like a faucet or a door.
Because these changes are easier to enumerate, we apply only a task switching cost of +1 if the primary object changes, and a switching cost of 0 in the case of the same object being manipulated.

In MetaWorld, we rollout an expert policy for 100 episodes for the 50 tasks to build our training set.
We then train a state-based, language-conditioned behavior cloning policy.
The policy takes in a 768-dimensional language embedding, a 39-dimensional state vector, and outputs a 4-dimensional action. 
For MetaWorld Checkpoints, we train a single MLP-based policy for 100 epochs, recording the policy performance at epoch $1, 10, 20, ..., 100$ for a total of 11 checkpoints. 
For MetaWorld Policies, we instead train 10 policies on random MLP architecture sizes and also apply different amounts of noise to the proprioceptive inputs to the policy to mimic a noisy understanding of state information.
We do this procedure to produce policies that vary more in performance while still having a systematic ``flaw" in understanding the scene, which we hope would be captured in our policy embeddings.
Then, for each policy and environment, we sample 50 evaluations each and store them offline for sampling.