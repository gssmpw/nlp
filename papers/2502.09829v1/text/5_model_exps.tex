
\section{Task and Policy Representation}
\label{sec:task_results}
As we define an experiment based on a task and a policy, we must design different embedding strategies for each of them.
We first discuss baselines and upper bounds on task and policy representations, then we show results on how these representations impact our surrogate model.

\subsection{Experimental Setup}
As it is unclear what an ideal representation for a policy or task is, we compute an upper bound for a task and policy representation by taking all the pre-evaluated outcomes, and then training learnable embeddings on the task of estimating performance. 
Thus, these task and policy representations have specifically been tuned for this prediction task.
We can then use these learned embeddings as optimal representations of the task and policy.

However, this optimal approach requires all the data a priori. 
Thus, we need a way to represent both a task and a policy.
The most direct way to represent a task is based on the language description of a task.
As described in Section~\ref{sec:method}, we define our task representation as a weighted sum between the language embeddings of the task description and the verbs.
We call this approach \textcolor{verb}{\textbf{Verb}}.
Overall, we consider the following task representation types as upper bounds and baselines:
\begin{enumerate}
    \item \textcolor{optimal}{\textbf{Optimal:}} Leverage all the data a priori to learn embeddings that are useful for predicting performance;
    \item \textcolor{verb}{\textbf{Verb:}} Use a weighted sum of the language embedding of the task and the language embedding of its verbs;
    \item \textcolor{lang}{\textbf{Language:}} Use a language embedding of the task as its representation; and
    \item \textcolor{random}{\textbf{Random:}} Assume no relationship between policies and tasks by using random embeddings.
\end{enumerate}

\input{figs_tex/cost_exps}

Unlike a task representation through language, there is no clear representation for a policy. 
We leave the exploration of new policy representations to future work and focus on two policy representations: \textbf{Optimal} and \textbf{Random}.

All experiments in this section were run for 750 evaluation steps over three seeds.
To evaluate how much these embeddings improve the performance of population parameter estimation during active experiment selection, we look at the log likelihood of all the outcomes in our offline dataset against a probability distribution represented by the predicted population parameters from the surrogate model.
Each experiment is sampled similar to how researchers typically evaluate: we select a random task and test each policy three times.
% \lipsum[2]


\subsection{Results}

We evaluate the effectiveness of different task representations by computing the average log likelihood of the full dataset against the predicted distribution across multiple datasets, including MetaWorld Policies, MetaWorld Checkpoints, OpenVLA, and HAMSTER, as shown in Figure \ref{fig:model_exps}.


\textbf{Random representations do not share information across policies and tasks.}
Our results indicate that random embeddings consistently perform worse, as they fail to capture any meaningful structure or shared information between tasks. 
In contrast, optimal embeddings, which used the entire dataset to tune its representation, outperforms all baselines. 
We found that the increasing performance of random performance is due to new experiments being sampled; however, minimal interpolation of outcomes occurred.

\textbf{Task representations vary depending on the kinds of tasks.}
We find that the types of tasks matter. 
The HAMSTER evaluations consist of many changes to objects rather than changes to the type of task itself such as ``pickup the milk\dots" and ``pickup the shrimp\dots"
This structure leads to clearer benefits when using language-based representations.
In contrast, OpenVLA has less separable tasks, thus it shows a much smaller separation between random, optimal, and language-based embeddings.
Metaworld Checkpoints, however, show a more stable improvement of \textbf{Verb} as opposed to simply \textbf{Lang} since there are many more tasks.
% A ``pickup orange mug" task and a ``push orange mug" task would have similar performance even if they are different verbs. 
% We find that the verb-specific information in the \textbf{Verb} representation is a better



\textbf{Language does not explain all the shared information between tasks.}
Despite the improvement from using language or verbs as a task representation, they do not fully bridge the gap to optimal embeddings.
The difference between the optimal embeddings and language embeddings indicates that task descriptions, even when focused on the verbs, do not capture all the information to describe a task's relationship to its performance.
Our approach does not include the observations of the trajectory, and this difference between optimal and language embeddings may be explained by the lack of the initial image.
We leave it to future work to explore this direction.
% We believe it would be interesting future work to  remaining information is likely captured in the observation itself, which our framework does not include.

\textbf{Optimal policy embeddings do not provide meaningful gains.}
While task embeddings provide a meaningful way to represent tasks, we found that random or optimal policy embeddings do not provide any significant improvements compared to one another.
This result may be due to the procedure for learning the optimal embeddings overly relying on the task embeddings during their training, or may be caused by the relatively small number of policies that were evaluated, which ranged from 4 to 11. 
In contrast, there were between 29 to 81 tasks that were evaluated against, so there was higher overlap between some tasks.

