\input{figs_tex/l1_exps}

\section{Cost-Aware Experiment Selection}

To evaluate the effectiveness of our cost-aware active experiment selection methods, we assess the population parameter estimation capability of our framework across various datasets using continuous and binary performance distributions.


\subsection{Experimental Setup}

\textbf{Sampling Strategies.}
To select the most informative experiment based on an acquisition function $a(\pi_i, T_j)$, we must design acquisition functions to define our sampling strategy.
We consider two types of sampling strategies. 
The first is to select both a policy and a task to run an evaluation on. 
Given the EIG formulation in Section~\ref{sec:method}, we define three sampling strategies with this approach:
\begin{itemize}
    \item \textcolor{rand}{\textbf{Random Sampling:}} Select a task-policy pair uniformly at random $a(\pi_i, T_j)=1/ (|\mathcal{P}| \times |\mathcal{T}|)$;
    \item  \textcolor{eig}{\textbf{EIG:}} Select a task-policy pair $(\pi_i,t_j)$ with the highest EIG: $a(\pi_i, T_j)=\mathcal{I}(\pi_i,T_j)$;
    \item  \textcolor{cost_eig}{\textbf{Cost-aware EIG:}} Select a task-policy pair that maximizes the cost-aware EIG according to Equation~\ref{eq:cost_aware_eig}.
\end{itemize}


The second type of sampling strategy is to select a task, and then evaluate every policy in that task $d=3$ times.

\begin{itemize}
    \item  \textcolor{rand_task}{\textbf{Random Task:}} Select a task uniformly at random and evaluate all policies on that task: $a(t_j) = 1 / |\mathcal{T}|$
    \item  \textcolor{task_eig}{\textbf{Task EIG:}} Select a task $T_j$ that maximizes the summed EIG across all policies: $a(t_j)=\sum_{i} \mathcal{I}(\pi_i,T_j)$
    \item  \textcolor{cost_task_eig}{\textbf{Cost-aware Task EIG:}} Select a task $T_j$ that maximizes the summed cost-aware EIG across all policies: $a(T_j)=\sum_{i} a_{\text{cost-aware}}(\pi_i,T_j, T_\text{current})$
\end{itemize}

The task-based sampling strategies is more realistic to how experimenters evaluate their robots today, as experimenters typically select a task and then evaluate every policy.
% However, it may be optimal to evaluate policy-task sampling as it may require less execution cost.

We evaluated each method for 1500 evaluation steps over three seeds using \textbf{Random} policy embeddings and \textbf{Verb} task embeddings.
To evaluate these methods, we consider two metrics: (1) the log likelihood of all the outcomes in our offline dataset against the predicted population parameters of the model, and (2) the L1 error between the mean from all the data for a policy-task pair against the mean derived from the estimated population parameters. 


\subsection{Results}


% Describe the importance of language, etc. Discuss hypothesis.
% \lipsum[2]
% \lipsum[2]


% \subsection{Cost over time}

% Discuss results of Figure \ref{fig:cost_exps} and Figure~\ref{fig:l1_exps}.
\input{figs_tex/pred_dists}

\textbf{EIG-based approaches struggle to learn population parameters that represent all the data, but better estimate the mean.}
In Figure~\ref{fig:cost_exps}, we show the average log likelihood of all the outcomes in our offline dataset against the probability distribution represented by the predicted population parameters from the surrogate model.
In both task- and policy-task sampling approaches, we find that EIG-based approaches fit the original data marginally better than random baselines.
In some cases, such as for MetaWorld Policies with success rate, cost-aware EIG is able to maintain a larger improvement; however, this result is not consistent across other datasets.
This result indicates that learning this full underlying distribution remains challenging, particularly in the early stages of evaluation when data is sparse. 
However, in Figure~\ref{fig:l1_exps}, EIG-based approaches clearly dominate when estimating the mean of these distributions, and often are able to estimate the mean at a lower cost compared to random baselines.
If the cost is fixed at a lower value, as if it was a maximum cost-budget, then we find that EIG-based approaches better estimate the means.
% We find that at costs that are earlier, EIG-based approaches better estimate the means at a lower cost.
% Even if the full distribution is difficult to capture, the mean estimates are useful.

\textbf{Tradeoffs between task- and policy-task sampling.}
Both Figure~\ref{fig:cost_exps} and Figure~\ref{fig:l1_exps} show that task-based sampling is generally better in OpenVLA and HAMSTER, but cost-aware EIG is generally estimates the L1 error better than its task-based counterpart on MetaWorld.
Policy-task sampling approaches are likely more efficient in MetaWorld experiments as there are a large number of experiments where there is a high cost to switch, and evaluating 10 policies over a single task may not be as informative.
In contrast, HAMSTER and OpenVLA have fewer policies, meaning the cost of evaluating all policies for a single task is lower.
% Since we assume a small uniform cost to evaluating a policy within the same task, there is a negligible penalty to task-based sampling methods.
Additionally, we found that policy-task sampling methods are more likely to switch tasks, causing a faster accumulation of cost.



\textbf{Learning the Performance Landscape.}
Figure~\ref{fig:pred_dists} illustrates how our formulation of sequentially sampling experiments progressively refines the predictions of the performance landscape. 
Early in the evaluation process, predictions are generally around the mean and are misaligned with the true distribution.
As more experiments are selected, the means begin to resemble the true mean distribution. 



% \subsection{Noise Robustness}
% Discuss noisy gaussian setting \ref{fig:noisy_gaussian}
% \lipsum[1]

% \textbf{Noisiness affects stuff somehow}
% \lipsum[2]

% \input{figs_tex/noisy_gaussian}



