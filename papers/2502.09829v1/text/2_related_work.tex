\section{Background and Related Work}
% We methods used for evaluating machine learning models, methods for evaluating robot policies, and introduce the concept of contrast sets.

% Past work in machine learning model evaluation has used perturbations as a method to probe model performance; however, evaluation in robotics typically focuses on a small number of pre-defined tasks. In this section, we discuss contrast sets from NLP, and its relevance to evaluation for robotics.
Past work in machine learning model evaluation and active learning have considered how to compare model performance; however, as more robot policies become easier to develop, it is critical to develop better strategies for evaluating robot strategies.
We discuss approaches for testing models in machine learning and its relevance to evaluation for robotics.

% commented these sections out since i don't think it added much!
% \textbf{Language-Conditioned Robot Learning.}
% Several works have focused on instructing robots with natural language, mostly focusing on navigation or manipulation. Some works deconstruct instructions by separating task planning and action generation~\cite{singh2023progprompt,liang2023code,ahn2022can}, while others have unified, end-to-end architectures \cite{shridhar2022cliport, shridhar2023perceiver}.
% Within robot learning, the role of language is typically to specify the task, define the desired end state, or describe instructions on how to interact with specific objects.


% One of the tasks we focus on in this work is vision-and-language navigation, which uses fine-grained instructions to control a navigation agent from visual observations \cite{gu2022vision}.
% Some work adds the ability for navigation agents to manipulate objects, further increasing complexity \cite{yenamandra2023homerobot, shridhar2020alfred}.
% SayCan \cite{ahn2022can} takes a modular approach and separates language understanding from taking actions to execute household tasks in the real world.
% Methods that are modular can often evaluate each component separately, such as a language model's output generation and each individual policy's performance.
% However, for end-to-end policies, it is expensive to evaluate a real-world robot hundreds of times and understand which component contributes to performance improvement or degradation.





% It is evident in these examples that simulated benchmarks are popular for training and evaluating robot policies as it is easy to scale.
% However, there is a need for a systematic and efficient approach to exploring the large space of possible environments and instructions a robot can encounter.
% However, there is a need for well-defined probing criteria to effectively understand the capabilities of end-to-end language-guided navigation policies.

% \textbf{Vision-and-Language Navigation.}
% Vision-and-Language Navigation (VLN) \cite{anderson2018vision, gu2022vision} is a task for language-guided navigation, where an agent receives instructions on how to reach a goal location.
% There are multiple variations of the VLN task based on different task objectives. 
% In this work, we focus on fine-grained navigation, where an agent is given step-by-step descriptions of its route.
% The Room-to-Room (R2R) dataset \cite{anderson2018vision} contains these instructions in the Matterport3D simulator, where an agent must traverse the edges of a navigation graph.
% The RxR dataset \cite{ku2020room} builds a larger dataset with English, Hindi, and Telegu instructions.
% Since these datasets use navigation graphs, it is not well-designed for physical robots.
% VLN-CE \cite{krantz2020beyond} converts trajectories in R2R and RxR on the navigation graph into a continuous environment in Habitat \cite{savva2019habitat} that is more suitable for robots. 
% In this work, we pretrain our policy on the RxR VLN-CE task and then finetune the policy on a robot.

% \input{figs_tex/training}
% \subsection{Evaluation}

\textbf{Evaluation in Machine Learning.}
% \textcolor{red}{TODO: cite sriram's lab's paper, LM eval papers, etc. differentiate ourseles from those}
% As machine learning models become more prevalent in real-world settings, research has begun to shift towards evaluating these models.
In fields such as computer vision or NLP, it is common to characterize the out-of-distribution performance of a single model~\cite{wang2021cline, hendrycks2020pretrained, recht2019imagenet, hendrycks2019benchmarking,liang2022holistic, chang2024survey, gardner2020evaluating}, some of which create a standard for comparing different models as well~\cite{liang2022holistic}. 
These approaches allow for experimenters to quickly understand the performance of their model, and in some cases compare between models.
However, in robotics, each task is expensive to evaluate and each policy evaluation is difficult. 
In this work, we look at use methods from active learning to improve experiment selection during evaluation.
% A large, sampled i.i.d. test set may not capture the span of expected situations a machine learning model could encounter in the real world. 
% To address this, researchers have designed out-of-distribution evaluation techniques in the vision~\cite{wang2021cline, hendrycks2020pretrained, recht2019imagenet, hendrycks2019benchmarking} and NLP~\cite{liang2022holistic, chang2024survey, gardner2020evaluating} communities. 
% In computer vision, perturbations of images have been used to generate counterfactual examples to test a model~\cite{zemni2023octet, jeanneret2023adversarial, jeanneret2022diffusion, sauer2021counterfactual, prabhu2024lance}.
% In NLP, contrast sets~\cite{gardner2020evaluating} perturb the original test set to accurately evaluate a model's true linguistic capabilities.
% These approaches allow experimenters to stress test their models and have confidence in their model during deployment.
% However, in robotics, testing requires physical deployment and takes considerable efforts to compute such metrics.
% In this work, we focus on designing contrast sets for language-guided robot policies.

\textbf{Active Testing.}
% \lipsum[2]
Similar to active learning which aims to select training labels, active testing approaches~\cite{sawade2010active, rainforth2024modern, yilmaz2021sample} focus on selecting test instances to evaluate to better predict model performance. 
Though these settings focus on classification or regression labeling tasks, this formulation is important to robotics as evaluation is expensive.
Various Bayesian optimization, active learning, and active testing approaches use surrogate models to estimate the value of a training or test instance~\cite{eggensperger2015efficient, brochu2010tutorial, shahriari2015taking, cozad2014learning,qian2006building, kossen2021active}, often incorporating cost-aware sampling~\cite{lee2020cost, paria2020cost}.
In robotics, surrogate models have been used to predict outcomes of a human-robot interaction scenarios in simulation for policy learning~\cite{bhatt2023surrogate}; however, that past work did not consider the cost evaluating each scenario.
Additionally, most of these works focus on active learning and active testing for regression models.
Since robot evaluation can have high variance, we take inspiration from past work~\cite{tosh2022targeted} to focus on active learning of probablistic models using a surrogate model.
We then apply these cost-aware active testing strategies on multi-task, multi-policy robot evaluation by learning a task and policy conditioned surrogate model.
% In this work, we build on active testing approaches to actively select experiments that are most informative for learning a surrogate model, and we show that this surrogate model is able to accurately estimate the multi-task, multi-policy performance distributions.



\textbf{Evaluation of Robot Policies.}
% Sim2real transfer strategies encompass various approaches, often employing domain randomization~\cite{tobin2017domain} or using generative adversarial networks to shift the target domain observation closer to the source domain~\cite{rao2020rl}. 
% However, this work's focus is not on the actual sim2real transfer process.
% Instead, our aim is to evaluate policies more effectively in the real world.
% Simulation is a common way to train and evaluate robot policies~\cite{nasiriany2024robocasa, mandlekar2021matters, james2020rlbench}. 
Simulation is often used to evaluate the performance of a real-robot system \cite{deitke2020robothor, anderson2021sim, kadian2020sim2real, gervet2023navigating} by recreating a simulated counterpart to a real environment, but shows ineffective direct sim2real performance without domain randomization or real-world finetuning strategies.
There exist correlations between simulation and real-world performance even if they do not exactly match~\cite{wilbert_colloseum, simpler_env}; however there are no guarantees about real-world performance.
% These works also pre-define a set number of tasks in simulation; but it is not scalable to engineer simulators for every new task.
Other recent work focuses on real world evaluation such as carefully selecting the initial conditions of an experiment~\cite{kress2024robot}, evaluating LLM-based task planners~\cite{hu2024deploying}, active capability assessment of black-box symbolic planners~\cite{verma2021discovering, verma2023autonomous, nayyar2022differential}, or providing bounds on policy performance by assuming some underlying distribution for outcomes~\cite{that_tri_paper}.
Other work has investigated how changes to these initial conditions can provide information about policy sensitivity~\cite{parekh2024investigating,xie2024decomposing,anwar2024contrast} or has used factors of the initial conditions and naive sampling strategies to more efficiently collect data~\cite{gao2024}.
In this work, we consider the setting of evaluating multiple policies across various tasks while also learning the parameters of an underlying distribution. 
We then leverage this learned distribution to more efficiently sample experiments for evaluation.
% The RoboEval benchmark~\cite{hu2024deploying} evaluates the effectiveness of language model-generated programs for robotics by using temporal logic to determine performance.
% Unlike past work, we focus on evaluating robot policies across the domain of language instructions.
% Additionally, it is not scalable to evaluate by engineering simulators for new tasks.
% With i.i.d. test sampling, the generalizability of a behavior cloned policy can be bounded~\cite{that_tri_paper}, but, as the number of possible test instructions and scenes scale, that bounding becomes difficult and time-consuming. 
% Additionally, RL frameworks such as autonomous RL~\cite{sharma2021autonomous} and others~\cite{chen2022you, gupta2021reset} focus on minimizing the number of human interventions during deployment.
% Additionally, single-life RL~\cite{chen2022you}, reset-free RL~\cite{gupta2021reset}, and autonomous RL~\cite{sharma2021autonomous} focus on minimizing the number of human interventions during training a real-world RL training.
% Contemporary work have begun to investigate how different visual or linguistic attributes impact evaluation performance \cite{xie2024decomposing, jin2023video, parekh2024investigating} and efficient data collection \cite{gao2024}.
% We find our work complementary to these, and focus on evaluating the space of possible test instructions and scenes in an efficient manner.
% In this work, we explore the space of possible test instructions and scene variations in an efficient manner that covers the domain of possible test instances.
% In this work, we want to \textit{know} how well a policy performs in the real world while saving time for experimenters.
% Thus, we propose systematically perturbing different components of a test set to efficiently evaluate robot policies. 


% ADDING CONTRAST SET STUFF FROM SECTION 4 HERE
% \subsection{Background: Contrast Sets}
% \subsection{Background}
% \textbf{Contrast sets in NLP.} 
% Contrast sets are perturbed variants of the test set that help characterize the decision boundary of a classification model.
% % These perturbations are often crafted by experts.
% They are constructed by perturbing the input text and/or the output label.
% For example, in a sentiment classification task, a perturbation to test model robustness to sarcasm could append ``Yeah, right!'' to the text of a positive review, indicating a change from a positive label to a negative label.
% We design contrast sets for robotics that alter accompanying language instructions of test instances and potentially the expected behavior.
% When an example is perturbed several times, the perturbed instances are tightly clustered in input space around the original test instance since these perturbations are often small.
% Evaluating on a contrast set allows one to probe the ideal decision boundary around a test instance. 
% Concretely, the text embeddings of a movie review with the words ``good" and ``bad" swapped out are likely similar, but the model should be able to discriminate between these perturbations.


% For image-based perturbation strategies, images in the NVLR2~\cite{suhr2018corpus} contrast set is perturbed by finding a new image that makes one minimal change in some concrete aspect.
% In this work, we design perturbation strategies for robot evaluation in the language, scene, and expected behavior axes.


% \textbf{Contrast sets with robots.}
% ~\label{sec:back:contrast}
% % Recall that a test instance is defined with respect to the language instruction $l$, the scene $s$, and the expected behavior $b$ (Figure~\ref{fig:overview}).
% % In comparison to a standard test set evaluation strategy $\mathcal{I}({X})$, where an experimenter sequentially executes i.i.d. random test instances with no consideration of the cost, a contrast set evaluation allows the experimenter to move around the test domain and cover more of this domain efficiently.
% Our work proposes to adapt contrast sets for robotics.
% Robotics contains visual environments, language instructions, and expected behavior. 
% Perturbations to the language and scene can lead to changes in the expected behavior, so we define our own scene and language perturbation functions that may or may not modify the expected behavior.
% We define four types of perturbation functions, denoted by the symbol $\Delta$ and a letter for the axes they modify:
% \begin{itemize}[nosep] %[noitemsep,topsep=0pt]
%     \item  $\Delta S (x)$ perturbs the environment such that the expected behavior is the same
%     \item $\Delta S B (x)$ perturbs the environment such that the expected behavior is different
%     \item $\Delta L (x)$ perturbs the language instruction such that the expected behavior is the same
%     \item $\Delta L B (x)$ perturbs the language instruction such that the expected behavior is different
% \end{itemize}
% The set of perturbation functions $\mathcal{P}$ consist of these functions defined above and depend on the tasks being evaluated.


% VLN-CE poses unique challenges, including language grounding, compositionality of instructions, and diverse scenes.
% We find that a policy trained only on simulation does not perform well, and finetuning on real-world demonstrations improves performance.

% However, it is intractible to \textit{know} how well a policy performs in the real world

% Therefore, these works on simulated evaluation for real-world transfer are not sufficient to \textit{know} how well a policy performs in the real world.
% In this work, we investigate robot policies effectively in the real world, bypassing this step of simulated evaluation.

% While these environments are effective in evaluating task performance in simulation, they do not comprehensively probe the diverse capabilities that language allows for.


% \textbf{Evaluating Physical Navigation Policies.}
% \lipsum[1]

% \input{figs_tex/num_demos}


