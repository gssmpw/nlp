% \section{Practical Advice on Using Active Testing}
% \abrar{write this!}
% \textbf{How you do modelling maters a lot}
% \lipsum[2]

% \textbf{Designing your text encoder matters a lot}
% \lipsum[2]


\section{Conclusion and Limitations}

We present a framework for the efficient evaluation of multitask robot policies. 
By framing evaluation as an active testing problem, we develop techniques that use relationships between tasks to predict policy performance distributions. 
In particular, we focus on methods that select experiments based on the expected information gain.
Our experiments demonstrate that task similarities can indeed be used to predict policy performance in an efficient manner, compared to standard evaluation approaches.
As evaluation settings and policy comparisons continue to scale in size, our methods for active testing can help lower the cost of effective evaluation without sacrificing too much information about policy performance. 


\textbf{Future Work.}
To properly be cost-aware, a single look-ahead step is often not enough, as it may be beneficial to plan future evaluations with respect to cost and potential information gain. 
Future work can extend our methods by developing look-ahead algorithms that can select longer sequences of experiments at a time.
In addition, other types of acquisition functions, such as those that batch experiments at once, can be explored.
We focused on ensuring that our surrogate model is able to estimate the landscape of performance across tasks and policies, but future work can focus on other types of comparison, such as finding the best average policy, finding a ranked ordering of policies, or finding the worst performing tasks. 
Each of these would require different active sampling strategies.
Additionally, learning policy embeddings may better predict performance, and policy embedding priors might be formed by encoding the training data of those policies, analogous to ``task embeddings'' in multi-task learning. 
There are also hierarchical relationships between tasks such as ``pour milk" likely depending on being able to ``pick up the milk" that would be exciting to explore in future work.

\textbf{Limitations.}
Though our approach to mitigating the cold-start problem with test-time dropout appears to have improved performance during sampling, this approach has not been rigorously tested by the Bayesian optimization community.
We had also tried other approaches, such as ensembling and variational prediction, but these approaches also overfit to the small size of the dataset early in the evaluation procedure.
We also represented execution costs naively at a fixed cost; however, different tasks may have different execution costs that may depend on whether a policy fails on its task or not, such as having to clean up spilled milk.
Additionally, we chose to use a simple MLP to learn our surrogate model; however, other work often used Bayesian neural networks and Gaussian processes. 
We made this decision because these alternative approaches typically do not scale to larger inputs; however, we did not consider the state-of-the-art for those approaches.
% Though we were skeptical about the scalability of these approaches to language embeddings, we believe such results would have more significant statistical importance.

\section{Acknowledgments}
This work was supported in part by a grant from the Army Research Lab (ARL) Army AI Innovations Institute (A2I2), award number W911NF-23-2-0010.
The claims and findings of this work do not necessarily represent the views of the ARL.

% Qualitatively we saw ensembling overfit to the small amounts of data. Cite some cold start active learning papers.

% Why L1 distance 

% \lipsum[2]

% Cost-Aware Multi-Task Robot Policy Evaluation
% Cost-Aware Evaluation of Robot Policies
% Cost aware robot policy evaluation enables efficient learning of where policies suceced/fail
% Efficiently Estimating Multi-Task Multi-Policy Robot Performance
% Efficient Multi-Task Performance Evaluation

% create a big channel
