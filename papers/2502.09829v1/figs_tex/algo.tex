% \begin{algorithm}
% \caption{Active Testing Pseudocode.}
% \label{alg:short pseudocode}
% \begin{algorithmic}[1]
% %\Require Dataset $\primdata$ w/ language instruction labels, LLM, Skill Library $Z$, Time limit $T$, max chain length $M$ % $V^\pi$, LLM, Embed $\leftarrow$ Sentence Embedding Model, Threshold $\delta$
% \State Train $\rewardgeneral$ on $\demos$ + $\openx$ w/ video rewinding and LLM augmentation
% \State Train policy $\pi$ on $demos$ with offline rewards $\offlinerewards$ from \Cref{eq:offline_labeled_rewards}
% \State
% \State Rollout $\pi$ for warmup steps conditioned on new task instr. 
% \For{Online Episode} %\Comment{\Cref{sec:method:skill_bootstrapping}}
%     \State Rollout $\pi$ to get new interaction data
%     \State Train $\pi$ with $\onlinerewards$ from \Cref{eq:online_rewards}
%     \State Update policy $\pi$ on new interaction data
% \EndFor
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Active Testing Pseudocode}
% \label{alg:short_pseudocode}
% \begin{algorithmic}[1]
% \Require A set of policies $\pi_i \in \mathcal{P}$ to evaluate over tasks $T_j \in \mathcal{T}$, an empty dataset of outcomes $\mathcal{D}$, and an untrained surrogate model $p(\pi_i, T_j)$
% \State Randomly sample a single task $T_j$ and evaluate every policy 3 times. Add outcomes to $\mathcal{D}$
% \State Train the surrogate model $p(\cdot)$ on $\mathcal{D}$ for $k$ epochs
% \For{each query step}
%     \State Compute score $s_{ij}=a(\pi_i, T_j)$ for every policy-task pair according to Eq.~\ref{eq:cost_aware_eig}
%     \State Sample $(\pi_i, T_j)$ according to the maximum score
%     \State Add the outcome of $(\pi_i, T_j)$ to $\mathcal{D}$
%     \State Train $p(\cdot)$ for $k$ epochs
% \EndFor
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{Active Testing Pseudocode}
% \label{alg:short_pseudocode}
% \begin{algorithmic}[1]
% \Require A set of policies $\pi_i \in \mathcal{P}$ to evaluate over tasks $T_j \in \mathcal{T}$, an empty dataset of outcomes $\mathcal{D}$, an untrained surrogate model $p(\pi_i, T_j)$, exploration rate $\epsilon = 0.1$
% \State Randomly sample a single task $T_j$ and evaluate every policy 3 times. Add outcomes $x_{ij}^k \in X_{ij}$ to $\mathcal{D}$
% \State Set $T_{\text{current}} = T_j$
% \State Train the surrogate model $p(\cdot)$ on $\mathcal{D}$ for $k$ epochs
% \For{each query step}
%     \State Use MC dropout to sample 10 predicted distributions from the surrogate model for every policy-task pair
%     \State Use sampled distribtuions to compute scores $s_{ij}=a(\pi_i, T_j, T_\text{current})$ according to Eq.~\ref{eq:cost_aware_eig}
%     \State With probability $\epsilon$, select a random $(\pi_i, T_j)$
%     \State Otherwise, select $(\pi_i, T_j) = \arg\max_{(\pi_i, T_j)} s_{ij}$
%     \State Evaluate the selected $(\pi_i, T_j)$ 3 times and observe outcomes $x_{ij}^1, x_{ij}^2, x_{ij}^3$
%     \State Add these outcomes to $\mathcal{D}$
%     \State Update $T_{\text{current}} = T_j$
%     \State Train $p(\cdot)$ on $\mathcal{D}$ for $k$ epochs
% \EndFor
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}
\caption{Active Experiment Selection Procedure}
\label{alg:short_pseudocode}
\begin{algorithmic}[1]
\Require A set of policies $\pi_i \in \mathcal{P}$ to evaluate over tasks $T_j \in \mathcal{T}$, an empty dataset of outcomes $\mathcal{D}$, an untrained surrogate model $p(\pi_i, T_j)$, exploration rate $\epsilon = 0.1$
\State Randomly sample a single task $T_j$ and evaluate every policy 3 times. Add outcomes $x_{ij}^k$ to $\mathcal{D}$
\State Set $T_{\text{current}} = T_j$
\State Increment $C_\text{total} = C_\text{eval} + c_\text{eval} \cdot |\mathcal{P}| \cdot 3$ 
\State Train the surrogate model $p(\cdot)$ on $\mathcal{D}$ for $k$ epochs
\For{each query step}
    \State Use MC dropout to sample 10 predicted distributions from the surrogate model for every policy-task pair
    \State Use sampled distributions to compute scores $s_{ij}=a(\pi_i, T_j, T_\text{current})$ according to Eq.~\ref{eq:cost_aware_eig}
    \State With probability $\epsilon$, select a random $(\pi_i, T_j)$
    \State Otherwise, select $(\pi_i, T_j) = \arg\max_{(\pi_i, T_j)} s_{ij}$

    \State Conduct 3 evaluations and observe  $\trial^1, \trial^2, \trial^3 \sim \truedist$
    \State Add these outcomes to $\mathcal{D}$
    \State Train $f(\cdot)$ on $\mathcal{D}$ for $k$ epochs

    \State Increment $C_\text{total} = C_\text{total} + c_\text{eval} \cdot 3$ 
    \If{$T_j \neq T_{\text{current}}$} \Comment{Task switching cost applies}
        \State Increment $C_\text{total} = C_\text{total} + c_\text{switch}(T_{\text{current}}, T_j)$
        \State Update $T_{\text{current}} = T_j$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
