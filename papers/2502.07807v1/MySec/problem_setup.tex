\vspace{-3mm}
\section{Preliminaries}
\label{sec:preliminaries}
\subsection{Formulation of Collaborative Perception}
\label{sec:formulation}

% \vspace{-3mm}
In this section, we formulate collaborative perception and give the pipeline of our CP system. Specifically, let $\mathcal{X}^N$ denote the set of $N$ CAVs in the CP system. CAVs in $\mathcal{X}$ can be divided into two categories: the ego CAV and helping CAVs. The ego CAV is the one that needs to perceive its surrounding environment, while helping CAVs are the ones that send their complementary sensing information to the ego CAV to help it enhance its perception performance.
Thus, each CAV can be an ego one and helping one, depending on its role in a perception process. We assume that each CAV is equipped with a feature encoder $f_\mathtt{{encoder}}(\cdot)$, a feature aggregator $f_\mathtt{{agg}}(\cdot)$, and a feature decoder $f_\mathtt{{decoder}}(\cdot)$. For the $i$-th CAV in the set $\mathcal{X}$, the raw observation is denoted as $\mathbf{O}_i$ (such as camera images and LiDAR point clouds), and the final perception results are denoted as $\mathbf{Y}_i$. The CP pipeline of the $i$-th CAV can be described as follows.
\begin{enumerate}
    % \vspace{-3mm}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item \textit{Observation Encoding}: Each CAV encodes its raw observation $\mathbf{O}_j$ into an initial feature map $\mathbf{F}_j = f_\mathtt{{encoder}}(\mathbf{O}_j)$, where $j \in \mathcal{X}^N$.
    \item \textit{Intermediate Feature Transmission}: Helping CAVs transmit their intermediate features to the ego CAV: $\mathbf{F}_{j\rightarrow i}=\mathbf{\Gamma}_{j\rightarrow i}(\mathbf{F}_j),\  j\in \mathcal{X}^N, j\neq i,$
    where $\mathbf{\Gamma}_{j\rightarrow i}(\cdot)$ denotes a transmitter that conveys the $j$-th CAV's intermediate feature $\mathbf{F}_j$ to the ego CAV, while performing a spatial transformation. $\mathbf{F}_{j\rightarrow i}$ is the spatially aligned feature in the $i$-th CAV's coordinate.
    \item \textit{Feature Aggregation}: The ego CAV receives all the intermediate features and fuses them into a unified observational feature $\mathbf{F}_\mathtt{fused}=f_\mathtt{agg}(\mathbf{F}_{0\rightarrow i}, \{\mathbf{F}_{j\rightarrow i}\}_{j\neq i,\  j\in \mathcal{X}^N})$.
    \item \textit{Perception Decoding}: Finally, the ego CAV decodes the unified observational feature $\mathbf{F}_\mathtt{fused}$ into the final perception results $\mathbf{Y}=f_\mathtt{decoder}(\mathbf{F}_\mathtt{fused})$.
    % \vspace{-4mm}
\end{enumerate}


\begin{figure*}[t]
    % \vspace{-5mm}
    \centering
    % \fbox{\rule{0pt}{1.8in} \rule{0.9\linewidth}{0pt}}
    \includegraphics[width=.9\linewidth]{fig/CPDataGenerationPipeline.png}
    \vspace{-3mm}
    \caption{\textbf{Automatic Data Generation and Annotation Pipeline.} We first train a robust LiDAR collaborative object detector. Then, we discard the detection head and decoder and only keep the backbone as the intermediate feature generator. The data generation pipeline is shown in (a), (b), and (c), where (a) is the intermediate feature generation, (b) is the attack implementation, and (c) is the pair generation and saving.}
    \label{fig:data_generation}
    \vspace{-5mm} 
\end{figure*}



\subsection{Adversarial Threat Model}

Our focus is on the operation of an  intermediate-fusion collaboration scheme, where an attacker introduces designed adversarial perturbations into the intermediate features to mislead the perception of the ego CAV. Since an attacker participates in the collaborative system with local perception model installation, we assume they have white-box access to the model parameters. The attack procedure in each frame follows four sequential phases.
\begin{enumerate}
    \vspace{-4mm}
    \setlength{\itemsep}{0pt}
    \setlength{\parskip}{0pt}
    \setlength{\parsep}{0pt}
    \item \textit{Local Perception Phase}: All agents, including the malicious one, process their sensing data independently and extract intermediate features using feature encoders. 
    \vspace{-1mm}
    \begin{equation}
    \mathbf{F}_k = f_\mathtt{encoder}(\mathbf{O}_k), \quad k \in \mathcal{X}^N
    \end{equation}
    \vspace{-1mm}
    This phase operates in parallel without inter-agent communication.

    \item \textit{Feature Communication Phase}: All agents broadcast their extracted features through the network. Malicious agent $k$ collects feature information $\{\mathbf{F}_{j\rightarrow i}\}$ from other agents. Feature-level transmission ensures minimal communication overhead compared to raw sensor data exchange.

    \item \textit{Attack Generation Phase}: A malicious agent executes the attack by first perturbing its local features and then propagating them through the collaborative perception pipeline described in Section \ref{sec:formulation}.
    The attacker aims to optimize the perturbation $\delta$ through an iterative process. The optimization objective is formulated as:
    \vspace{-1mm}
    \begin{equation}
        \vspace{-2mm}
        \begin{aligned}
        \mathop{\arg\max}_{\delta} \mathcal{L}(\mathbf{Y}^\delta, \mathbf{Y}^\mathtt{gt}),
        \quad \mathtt{s.t.}\quad  \|\delta\|\leq \Delta
        \end{aligned}
    \end{equation}
    where $\Delta$ bounds the perturbation magnitude to maintain attack stealthiness. The total loss function is designed to aggregate adversarial losses over all object proposals, targeting both classification and localization aspects:
    \vspace{-1mm}
    \begin{equation}
        \vspace{-2mm}
        \mathcal{L}(\mathbf{Y}^\delta, \mathbf{Y}^\mathtt{gt}) = \sum_{p \in \mathbf{Y}^\delta} \mathcal{L}_\mathtt{adv}(p, p^\mathtt{gt})
    \end{equation}
    For each proposal $p$ with the highest confidence class $c = \mathop{\arg\max}\{p_i\}$, we leverage a class-specific adversarial loss following \citep{tuAdversarialAttacksMultiAgent2021}:
    \begin{equation*}
        % \vspace{-2mm}
        \mathcal{L}_\mathtt{adv}(p', p) = \begin{cases}
            -\log(1 - p'_c)\cdot\eta & c \neq k,\ p_c > \tau_1\\
            -\lambda p'_c\log(1 - p'_c) & c = k,\ p_c > \tau_2\\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    where $\eta$ represents the IoU between perturbed and original proposals to consider spatial accuracy, $\tau_1$ and $\tau_2$ are confidence thresholds for different attack scenarios, $\lambda$ balances the importance of different attack objectives, and $k$ denotes the background class.

    \item \textit{Defense and Final Perception Phase}: The ego vehicle integrates all received feature information, including potentially corrupted ones, to complete the final object detection task. Note that we focus exclusively on CP-specific vulnerabilities, excluding physical sensor attacks (e.g., LiDAR or GPS spoofing), which are general threats to CAVs. We also assume communication channels are secured with proper cryptographic protection.
\end{enumerate}
