\section{Related Work}
\noindent\textbf{Video captioning.}
The goal of video captioning is to describe a video across several key aspects, aiding understanding~\cite{doveh2023dense}, retrieval~\cite{ma2024drvideo}, and motion control~\cite{wang2024motionctrl}.
In T2V generation, accurate and detailed video captions can enhance semantic alignment during model training~\cite{polyak2024movie}.
Naive captioning models adopt free-form descriptions~\cite{chen2024panda,wang2024koala}.
To enhance controllability, MiraData~\cite{ju2024miradata}, VDC~\cite{chai2024auroracap}, and Vript~\cite{yang2024vript} emphasize specific aspects like subjects, background, and shots, significantly benefiting T2V generation.
Other methods describe videos from an event perspective~\cite{wang2024tarsier,he2024storyteller}, capturing temporal information more effectively.
Despite advancements in caption controlling~\cite{wang2023caption,hua2024finecaption}, 
% metrics on evaluating the caption model often remain limited.
evaluations with omissions may lead to a seesaw effect where gains in one dimension come at the cost of others, limiting the utility of the captioning model.

\noindent\textbf{Evaluation methods for video captioning.}
% While the effective evaluation of video captions was previously underemphasized, the advancement of text-to-video generation has spurred the development of increasingly sophisticated evaluation metrics, constantly evolving alongside changes in caption formats.
The advancement of T2V generation has spurred the development of evaluation approaches for video captioning.
Traditional approaches~\cite{xu2017video,xu2016msr} for short captions rely on legacy metrics like CIDEr and BLEU.
For dense captions, inspired by image captioning evaluation~\cite{liu2024playground,prabhu2024trust,tu2024automatic}, many approaches employ question answering (QA) followed by natural language inference (NLI) with a critic model.
Existing evaluation schemes of video captions are often tied to specific caption formats and suffer from instability in automatic evaluation.
In this context, VidCapBench emerges as a more robust solution, offering a comprehensive and stable evaluation framework that aligns with the controllable T2V evaluation~\cite{rawte2024vibe,huang2024vbench++,he2024videoscore}, providing better guidance for T2V model training.