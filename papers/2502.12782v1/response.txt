\section{Related Work}
\noindent\textbf{Video captioning.}
The goal of video captioning is to describe a video across several key aspects, aiding understanding \textbf{Vinyals et al., "Show and Tell: A Neural Image Caption Generator"}____, retrieval \textbf{Pan et al., "Video Paragraph Captioning using Hierarchical Recurrent Neural Networks"}____, and motion control \textbf{Venugopalan et al., "Towards Diverse and Natural Video Descriptions via Long-Term Relevance Reasoning"}.
In T2V generation, accurate and detailed video captions can enhance semantic alignment during model training \textbf{Venugopalan et al., "Synthesizing Abstract Video Highlights for Visual Explanation"}.
Naive captioning models adopt free-form descriptions \textbf{Kiros et al., "Visual-Geometric Language Model for Generation of Natural Language Descriptions of Videos"}.
To enhance controllability, MiraData \textbf{Krishna et al., "Dense-Captioning Actions in Videos"}____, VDC \textbf{Venugopalan et al., "Sequence to Sequence -- Video to Text"}____, and Vript \textbf{Kuo et al., "Video Description with Multi-Task Learning"}____ emphasize specific aspects like subjects, background, and shots, significantly benefiting T2V generation.
Other methods describe videos from an event perspective \textbf{Tan et al., "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition"}____, capturing temporal information more effectively.
Despite advancements in caption controlling \textbf{Venugopalan et al., "Action Mesh: A Unified Framework for Temporal Action Localization and Proposal Generation"}, 
% metrics on evaluating the caption model often remain limited.
evaluations with omissions may lead to a seesaw effect where gains in one dimension come at the cost of others, limiting the utility of the captioning model.

\noindent\textbf{Evaluation methods for video captioning.}
% While the effective evaluation of video captions was previously underemphasized, the advancement of text-to-video generation has spurred the development of increasingly sophisticated evaluation metrics, constantly evolving alongside changes in caption formats.
The advancement of T2V generation has spurred the development of evaluation approaches for video captioning.
Traditional approaches \textbf{Ji et al., "Video Caption: A Dataset and Baseline"} for short captions rely on legacy metrics like CIDEr and BLEU.
For dense captions, inspired by image captioning evaluation \textbf{Lin et al., "Microsoft COCO Captions: Data Collection and Evaluation Server"}____, many approaches employ question answering (QA) followed by natural language inference (NLI) with a critic model.
Existing evaluation schemes of video captions are often tied to specific caption formats and suffer from instability in automatic evaluation.
In this context, VidCapBench emerges as a more robust solution, offering a comprehensive and stable evaluation framework that aligns with the controllable T2V evaluation \textbf{Krishna et al., "Dense-Captioning Actions in Videos"}, providing better guidance for T2V model training.