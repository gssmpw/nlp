% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@inproceedings{wu2023tune,
  title={Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Stan Weixian and Gu, Yuchao and Shi, Yufei and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7623--7633},
  year={2023}
}

@article{zhang2023controlvideo,
  title={Controlvideo: Training-free controllable text-to-video generation},
  author={Zhang, Yabo and Wei, Yuxiang and Jiang, Dongsheng and Zhang, Xiaopeng and Zuo, Wangmeng and Tian, Qi},
  journal={arXiv preprint arXiv:2305.13077},
  year={2023}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}

@article{blattmann2023stable,
  title={Stable video diffusion: Scaling latent video diffusion models to large datasets},
  author={Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and others},
  journal={arXiv preprint arXiv:2311.15127},
  year={2023}
}

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{weng2024art,
  title={ART-V: Auto-Regressive Text-to-Video Generation with Diffusion Models},
  author={Weng, Wenming and Feng, Ruoyu and Wang, Yanhui and Dai, Qi and Wang, Chunyu and Yin, Dacheng and Zhao, Zhiyuan and Qiu, Kai and Bao, Jianmin and Yuan, Yuhui and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7395--7405},
  year={2024}
}

@article{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@misc{genmo2024mochi,
      title={Mochi 1},
      author={Genmo Team},
      year={2024},
      publisher = {GitHub},
      journal = {GitHub repository},
      howpublished={\url{https://github.com/genmoai/models}}
}

@article{zhou2024allegro,
  title={Allegro: Open the black box of commercial-level video generation model},
  author={Zhou, Yuan and Wang, Qiuyue and Cai, Yuxuan and Yang, Huan},
  journal={arXiv preprint arXiv:2410.15458},
  year={2024}
}

@article{kong2024hunyuanvideo,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}

@article{song2024both,
  title={Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination},
  author={Song, Dingjie and Lai, Sicheng and Chen, Shunian and Sun, Lichao and Wang, Benyou},
  journal={arXiv preprint arXiv:2411.03823},
  year={2024}
}

@article{bansal2024videophy,
  title={VideoPhy: Evaluating Physical Commonsense for Video Generation},
  author={Bansal, Hritik and Lin, Zongyu and Xie, Tianyi and Zong, Zeshun and Yarom, Michal and Bitton, Yonatan and Jiang, Chenfanfu and Sun, Yizhou and Chang, Kai-Wei and Grover, Aditya},
  journal={arXiv preprint arXiv:2406.03520},
  year={2024}
}

@article{nan2024openvid,
  title={Openvid-1m: A large-scale high-quality dataset for text-to-video generation},
  author={Nan, Kepan and Xie, Rui and Zhou, Penghao and Fan, Tiehan and Yang, Zhenheng and Chen, Zhijie and Li, Xiang and Yang, Jian and Tai, Ying},
  journal={arXiv preprint arXiv:2407.02371},
  year={2024}
}

@inproceedings{chen2025sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={370--387},
  year={2025},
  organization={Springer}
}

@article{wang2024koala,
  title={Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content},
  author={Wang, Qiuheng and Shi, Yukai and Ou, Jiarong and Chen, Rui and Lin, Ke and Wang, Jiahao and Jiang, Boyuan and Yang, Haotian and Zheng, Mingwu and Tao, Xin and others},
  journal={arXiv preprint arXiv:2410.08260},
  year={2024}
}

@inproceedings{onoe2025docci,
  title={Docci: Descriptions of connected and contrasting images},
  author={Onoe, Yasumasa and Rane, Sunayana and Berger, Zachary and Bitton, Yonatan and Cho, Jaemin and Garg, Roopal and Ku, Alexander and Parekh, Zarana and Pont-Tuset, Jordi and Tanzer, Garrett and others},
  booktitle={European Conference on Computer Vision},
  pages={291--309},
  year={2025},
  organization={Springer}
}

@article{awadalla2024blip3,
  title={BLIP3-KALE: Knowledge Augmented Large-Scale Dense Captions},
  author={Awadalla, Anas and Xue, Le and Shu, Manli and Yan, An and Wang, Jun and Purushwalkam, Senthil and Shen, Sheng and Lee, Hannah and Lo, Oscar and Park, Jae Sung and others},
  journal={arXiv preprint arXiv:2411.07461},
  year={2024}
}

@article{li2024densefusion,
  title={Densefusion-1m: Merging vision experts for comprehensive multimodal perception},
  author={Li, Xiaotong and Zhang, Fan and Diao, Haiwen and Wang, Yueze and Wang, Xinlong and Duan, Ling-Yu},
  journal={arXiv preprint arXiv:2407.08303},
  year={2024}
}

@article{wang2024dreamrunner,
  title={DreamRunner: Fine-Grained Storytelling Video Generation with Retrieval-Augmented Motion Adaptation},
  author={Wang, Zun and Li, Jialu and Lin, Han and Yoon, Jaehong and Bansal, Mohit},
  journal={arXiv preprint arXiv:2411.16657},
  year={2024}
}

@article{zhou2024motion,
  title={Motion Control for Enhanced Complex Action Video Generation},
  author={Zhou, Qiang and Zhang, Shaofeng and Yang, Nianzu and Qian, Ye and Li, Hao},
  journal={arXiv preprint arXiv:2411.08328},
  year={2024}
}

@article{wei2024dreamvideo,
  title={DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control},
  author={Wei, Yujie and Zhang, Shiwei and Yuan, Hangjie and Wang, Xiang and Qiu, Haonan and Zhao, Rui and Feng, Yutong and Liu, Feng and Huang, Zhizhong and Ye, Jiaxin and others},
  journal={arXiv preprint arXiv:2410.13830},
  year={2024}
}

@article{xiong2024lvd,
  title={LVD-2M: A Long-take Video Dataset with Temporally Dense Captions},
  author={Xiong, Tianwei and Wang, Yuqing and Zhou, Daquan and Lin, Zhijie and Feng, Jiashi and Liu, Xihui},
  journal={arXiv preprint arXiv:2410.10816},
  year={2024}
}

@article{guo2024trace,
  title={Trace: Temporal grounding video llm via causal event modeling},
  author={Guo, Yongxin and Liu, Jingyu and Li, Mingda and Tang, Xiaoying and Liu, Qingbin and Chen, Xi},
  journal={arXiv preprint arXiv:2410.05643},
  year={2024}
}

@inproceedings{yang2023vid2seq,
  title={Vid2seq: Large-scale pretraining of a visual language model for dense video captioning},
  author={Yang, Antoine and Nagrani, Arsha and Seo, Paul Hongsuck and Miech, Antoine and Pont-Tuset, Jordi and Laptev, Ivan and Sivic, Josef and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10714--10726},
  year={2023}
}

@article{cheng2024videgothink,
  title={VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI},
  author={Cheng, Sijie and Fang, Kechen and Yu, Yangyang and Zhou, Sicheng and Li, Bohao and Tian, Ye and Li, Tingguang and Han, Lei and Liu, Yang},
  journal={arXiv preprint arXiv:2410.11623},
  year={2024}
}

@article{jin2024pyramidal,
  title={Pyramidal flow matching for efficient video generative modeling},
  author={Jin, Yang and Sun, Zhicheng and Li, Ningyuan and Xu, Kun and Jiang, Hao and Zhuang, Nan and Huang, Quzhe and Song, Yang and Mu, Yadong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2410.05954},
  year={2024}
}

@article{luo2024videoautoarena,
  title={VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation},
  author={Luo, Ziyang and Wu, Haoning and Li, Dongxu and Ma, Jing and Kankanhalli, Mohan and Li, Junnan},
  journal={arXiv preprint arXiv:2411.13281},
  year={2024}
}

@article{qin2024worldsimbench,
  title={WorldSimBench: Towards Video Generation Models as World Simulators},
  author={Qin, Yiran and Shi, Zhelun and Yu, Jiwen and Wang, Xijun and Zhou, Enshen and Li, Lijun and Yin, Zhenfei and Liu, Xihui and Sheng, Lu and Shao, Jing and others},
  journal={arXiv preprint arXiv:2410.18072},
  year={2024}
}

@inproceedings{kim2023dense,
  title={Dense text-to-image generation with attention modulation},
  author={Kim, Yunji and Lee, Jiyoung and Kim, Jin-Hwa and Ha, Jung-Woo and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={7701--7711},
  year={2023}
}

@article{liu2024improving,
  title={Improving Long-Text Alignment for Text-to-Image Diffusion Models},
  author={Liu, Luping and Du, Chao and Pang, Tianyu and Wang, Zehan and Li, Chongxuan and Xu, Dong},
  journal={arXiv preprint arXiv:2410.11817},
  year={2024}
}

@article{wang2024tarsier,
  title={Tarsier: Recipes for training and evaluating large video description models},
  author={Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao},
  journal={arXiv preprint arXiv:2407.00634},
  year={2024}
}

@article{he2024storyteller,
  title={StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification},
  author={He, Yichen and Lin, Yuan and Wu, Jianchao and Zhang, Hanchong and Zhang, Yuchen and Le, Ruicheng},
  journal={arXiv preprint arXiv:2411.07076},
  year={2024}
}

@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@article{hong2024cogvlm2,
  title={Cogvlm2: Visual language models for image and video understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}

@article{zheng2024videogen,
  title={VideoGen-of-Thought: A Collaborative Framework for Multi-Shot Video Generation},
  author={Zheng, Mingzhe and Xu, Yongqi and Huang, Haojian and Ma, Xuran and Liu, Yexin and Shu, Wenjie and Pang, Yatian and Tang, Feilong and Chen, Qifeng and Yang, Harry and others},
  journal={arXiv preprint arXiv:2412.02259},
  year={2024}
}

@article{chai2024auroracap,
  title={AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark},
  author={Chai, Wenhao and Song, Enxin and Du, Yilun and Meng, Chenlin and Madhavan, Vashisht and Bar-Tal, Omer and Hwang, Jeng-Neng and Xie, Saining and Manning, Christopher D},
  journal={arXiv preprint arXiv:2410.03051},
  year={2024}
}

@article{ju2024miradata,
  title={Miradata: A large-scale video dataset with long durations and structured captions},
  author={Ju, Xuan and Gao, Yiming and Zhang, Zhaoyang and Yuan, Ziyang and Wang, Xintao and Zeng, Ailing and Xiong, Yu and Xu, Qiang and Shan, Ying},
  journal={arXiv preprint arXiv:2407.06358},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{yang2024vript,
  title={Vript: A Video Is Worth Thousands of Words},
  author={Yang, Dongjie and Huang, Suyuan and Lu, Chengqiang and Han, Xiaodong and Zhang, Haoxin and Gao, Yan and Hu, Yao and Zhao, Hai},
  journal={arXiv preprint arXiv:2406.06040},
  year={2024}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@inproceedings{wang2019vatex,
  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4581--4591},
  year={2019}
}

@article{ma2024drvideo,
  title={DrVideo: Document Retrieval Based Long Video Understanding},
  author={Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei},
  journal={arXiv preprint arXiv:2406.12846},
  year={2024}
}

@article{wang2023caption,
  title={Caption anything: Interactive image description with diverse multimodal controls},
  author={Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
  journal={arXiv preprint arXiv:2305.02677},
  year={2023}
}

@article{hua2024finecaption,
  title={FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity},
  author={Hua, Hang and Liu, Qing and Zhang, Lingzhi and Shi, Jing and Zhang, Zhifei and Wang, Yilin and Zhang, Jianming and Luo, Jiebo},
  journal={arXiv preprint arXiv:2411.15411},
  year={2024}
}

@article{doveh2023dense,
  title={Dense and aligned captions (dac) promote compositional reasoning in vl models},
  author={Doveh, Sivan and Arbelle, Assaf and Harary, Sivan and Herzig, Roei and Kim, Donghyun and Cascante-Bonilla, Paola and Alfassy, Amit and Panda, Rameswar and Giryes, Raja and Feris, Rogerio and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={76137--76150},
  year={2023}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13320--13331},
  year={2024}
}

@article{polyak2024movie,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{chen2024motionllm,
  title={MotionLLM: Understanding Human Behaviors from Human Motions and Videos},
  author={Chen, Ling-Hao and Lu, Shunlin and Zeng, Ailing and Zhang, Hao and Wang, Benyou and Zhang, Ruimao and Zhang, Lei},
  journal={arXiv preprint arXiv:2405.20340},
  year={2024}
}

@inproceedings{wang2024motionctrl,
  title={Motionctrl: A unified and flexible motion controller for video generation},
  author={Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Li, Yaowei and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@article{liu2024playground,
  title={Playground v3: Improving text-to-image alignment with deep-fusion large language models},
  author={Liu, Bingchen and Akhgari, Ehsan and Visheratin, Alexander and Kamko, Aleks and Xu, Linmiao and Shrirao, Shivam and Lambert, Chase and Souza, Joao and Doshi, Suhail and Li, Daiqing},
  journal={arXiv preprint arXiv:2409.10695},
  year={2024}
}

@article{prabhu2024trust,
  title={Trust but Verify: Programmatic VLM Evaluation in the Wild},
  author={Prabhu, Viraj and Purushwalkam, Senthil and Yan, An and Xiong, Caiming and Xu, Ran},
  journal={arXiv preprint arXiv:2410.13121},
  year={2024}
}

@article{tu2024automatic,
  title={Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark},
  author={Tu, Rong-Cheng and Ma, Zi-Ao and Lan, Tian and Zhao, Yuehao and Huang, Heyan and Mao, Xian-Ling},
  journal={arXiv preprint arXiv:2411.15488},
  year={2024}
}

@article{he2024videoscore,
  title={Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation},
  author={He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and others},
  journal={arXiv preprint arXiv:2406.15252},
  year={2024}
}

@article{rawte2024vibe,
  title={ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models},
  author={Rawte, Vipula and Jain, Sarthak and Sinha, Aarush and Kaushik, Garv and Bansal, Aman and Vishwanath, Prathiksha Rumale and Jain, Samyak Rajesh and Reganti, Aishwarya Naresh and Jain, Vinija and Chadha, Aman and others},
  journal={arXiv preprint arXiv:2411.10867},
  year={2024}
}

@article{huang2024vbench++,
  title={VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models},
  author={Huang, Ziqi and Zhang, Fan and Xu, Xiaojie and He, Yinan and Yu, Jiashuo and Dong, Ziyue and Ma, Qianli and Chanpaisit, Nattapol and Si, Chenyang and Jiang, Yuming and others},
  journal={arXiv preprint arXiv:2411.13503},
  year={2024}
}

@inproceedings{zhang2022bytetrack,
  title={Bytetrack: Multi-object tracking by associating every detection box},
  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
  booktitle={European conference on computer vision},
  pages={1--21},
  year={2022},
  organization={Springer}
}

@article{wei2024general,
  title={General ocr theory: Towards ocr-2.0 via a unified end-to-end model},
  author={Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},
  journal={arXiv preprint arXiv:2409.01704},
  year={2024}
}

@inproceedings{khirodkar2025sapiens,
  title={Sapiens: Foundation for human vision models},
  author={Khirodkar, Rawal and Bagautdinov, Timur and Martinez, Julieta and Zhaoen, Su and James, Austin and Selednik, Peter and Anderson, Stuart and Saito, Shunsuke},
  booktitle={European Conference on Computer Vision},
  pages={206--228},
  year={2025},
  organization={Springer}
}

@article{ren2024dino,
  title={DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding},
  author={Ren, Tianhe and Chen, Yihao and Jiang, Qing and Zeng, Zhaoyang and Xiong, Yuda and Liu, Wenlong and Ma, Zhengyu and Shen, Junyi and Gao, Yuan and Jiang, Xiaoke and others},
  journal={arXiv preprint arXiv:2411.14347},
  year={2024}
}

@article{ravi2024sam,
  title={Sam 2: Segment anything in images and videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

@article{cho2023davidsonian,
  title={Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-image generation},
  author={Cho, Jaemin and Hu, Yushi and Garg, Roopal and Anderson, Peter and Krishna, Ranjay and Baldridge, Jason and Bansal, Mohit and Pont-Tuset, Jordi and Wang, Su},
  journal={arXiv preprint arXiv:2310.18235},
  year={2023}
}

@inproceedings{liu2025grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={European Conference on Computer Vision},
  pages={38--55},
  year={2025},
  organization={Springer}
}

@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{zhang2024video,
  title={Video instruction tuning with synthetic data},
  author={Zhang, Yuanhan and Wu, Jinming and Li, Wei and Li, Bo and Ma, Zejun and Liu, Ziwei and Li, Chunyuan},
  journal={arXiv preprint arXiv:2410.02713},
  year={2024}
}

@article{zhang2024llava,
  title={Llava-next: A strong zero-shot video understanding model},
  author={Zhang, Y and Li, B and Liu, H and Lee, Y and Gui, L and Fu, D and Feng, J and Liu, Z and Li, C},
  year={2024}
}

@article{zhang2024longcontexttransferlanguage,
      title={Long Context Transfer from Language to Vision}, 
      author={Peiyuan Zhang and Kaichen Zhang and Bo Li and Guangtao Zeng and Jingkang Yang and Yuanhan Zhang and Ziyue Wang and Haoran Tan and Chunyuan Li and Ziwei Liu},
      journal={arXiv preprint arXiv:2406.16852},
      year={2024},
}

@article{ye2024mplug,
  title={mplug-owl3: Towards long image-sequence understanding in multi-modal large language models},
  author={Ye, Jiabo and Xu, Haiyang and Liu, Haowei and Hu, Anwen and Yan, Ming and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2408.04840},
  year={2024}
}

@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={arXiv preprint arXiv:2404.16821},
  year={2024}
}

@article{agrawal2024pixtral,
  title={Pixtral 12B},
  author={Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and De Monicault, Baudouin and Garg, Saurabh and Gervet, Theophile and others},
  journal={arXiv preprint arXiv:2410.07073},
  year={2024}
}

@article{li2024aria,
  title={Aria: An open multimodal native mixture-of-experts model},
  author={Li, Dongxu and Liu, Yudong and Wu, Haoning and Wang, Yue and Shen, Zhiqi and Qu, Bowen and Niu, Xinyao and Wang, Guoyin and Chen, Bei and Li, Junnan},
  journal={arXiv preprint arXiv:2410.05993},
  year={2024}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{unterthiner2019fvd,
  title={FVD: A new metric for video generation},
  author={Unterthiner, Thomas and van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Rapha{\"e}l and Michalski, Marcin and Gelly, Sylvain},
  year={2019}
}