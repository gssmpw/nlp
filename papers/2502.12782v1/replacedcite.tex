\section{Related Work}
\noindent\textbf{Video captioning.}
The goal of video captioning is to describe a video across several key aspects, aiding understanding____, retrieval____, and motion control____.
In T2V generation, accurate and detailed video captions can enhance semantic alignment during model training____.
Naive captioning models adopt free-form descriptions____.
To enhance controllability, MiraData____, VDC____, and Vript____ emphasize specific aspects like subjects, background, and shots, significantly benefiting T2V generation.
Other methods describe videos from an event perspective____, capturing temporal information more effectively.
Despite advancements in caption controlling____, 
% metrics on evaluating the caption model often remain limited.
evaluations with omissions may lead to a seesaw effect where gains in one dimension come at the cost of others, limiting the utility of the captioning model.

\noindent\textbf{Evaluation methods for video captioning.}
% While the effective evaluation of video captions was previously underemphasized, the advancement of text-to-video generation has spurred the development of increasingly sophisticated evaluation metrics, constantly evolving alongside changes in caption formats.
The advancement of T2V generation has spurred the development of evaluation approaches for video captioning.
Traditional approaches____ for short captions rely on legacy metrics like CIDEr and BLEU.
For dense captions, inspired by image captioning evaluation____, many approaches employ question answering (QA) followed by natural language inference (NLI) with a critic model.
Existing evaluation schemes of video captions are often tied to specific caption formats and suffer from instability in automatic evaluation.
In this context, VidCapBench emerges as a more robust solution, offering a comprehensive and stable evaluation framework that aligns with the controllable T2V evaluation____, providing better guidance for T2V model training.