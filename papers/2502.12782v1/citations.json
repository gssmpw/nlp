[
  {
    "index": 0,
    "papers": [
      {
        "key": "doveh2023dense",
        "author": "Doveh, Sivan and Arbelle, Assaf and Harary, Sivan and Herzig, Roei and Kim, Donghyun and Cascante-Bonilla, Paola and Alfassy, Amit and Panda, Rameswar and Giryes, Raja and Feris, Rogerio and others",
        "title": "Dense and aligned captions (dac) promote compositional reasoning in vl models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ma2024drvideo",
        "author": "Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei",
        "title": "DrVideo: Document Retrieval Based Long Video Understanding"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wang2024motionctrl",
        "author": "Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Li, Yaowei and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying",
        "title": "Motionctrl: A unified and flexible motion controller for video generation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "polyak2024movie",
        "author": "Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others",
        "title": "Movie gen: A cast of media foundation models"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2024panda",
        "author": "Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others",
        "title": "Panda-70m: Captioning 70m videos with multiple cross-modality teachers"
      },
      {
        "key": "wang2024koala",
        "author": "Wang, Qiuheng and Shi, Yukai and Ou, Jiarong and Chen, Rui and Lin, Ke and Wang, Jiahao and Jiang, Boyuan and Yang, Haotian and Zheng, Mingwu and Tao, Xin and others",
        "title": "Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "ju2024miradata",
        "author": "Ju, Xuan and Gao, Yiming and Zhang, Zhaoyang and Yuan, Ziyang and Wang, Xintao and Zeng, Ailing and Xiong, Yu and Xu, Qiang and Shan, Ying",
        "title": "Miradata: A large-scale video dataset with long durations and structured captions"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chai2024auroracap",
        "author": "Chai, Wenhao and Song, Enxin and Du, Yilun and Meng, Chenlin and Madhavan, Vashisht and Bar-Tal, Omer and Hwang, Jeng-Neng and Xie, Saining and Manning, Christopher D",
        "title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "yang2024vript",
        "author": "Yang, Dongjie and Huang, Suyuan and Lu, Chengqiang and Han, Xiaodong and Zhang, Haoxin and Gao, Yan and Hu, Yao and Zhao, Hai",
        "title": "Vript: A Video Is Worth Thousands of Words"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "wang2024tarsier",
        "author": "Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao",
        "title": "Tarsier: Recipes for training and evaluating large video description models"
      },
      {
        "key": "he2024storyteller",
        "author": "He, Yichen and Lin, Yuan and Wu, Jianchao and Zhang, Hanchong and Zhang, Yuchen and Le, Ruicheng",
        "title": "StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "wang2023caption",
        "author": "Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan",
        "title": "Caption anything: Interactive image description with diverse multimodal controls"
      },
      {
        "key": "hua2024finecaption",
        "author": "Hua, Hang and Liu, Qing and Zhang, Lingzhi and Shi, Jing and Zhang, Zhifei and Wang, Yilin and Zhang, Jianming and Luo, Jiebo",
        "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xu2017video",
        "author": "Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting",
        "title": "Video question answering via gradually refined attention over appearance and motion"
      },
      {
        "key": "xu2016msr",
        "author": "Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong",
        "title": "Msr-vtt: A large video description dataset for bridging video and language"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2024playground",
        "author": "Liu, Bingchen and Akhgari, Ehsan and Visheratin, Alexander and Kamko, Aleks and Xu, Linmiao and Shrirao, Shivam and Lambert, Chase and Souza, Joao and Doshi, Suhail and Li, Daiqing",
        "title": "Playground v3: Improving text-to-image alignment with deep-fusion large language models"
      },
      {
        "key": "prabhu2024trust",
        "author": "Prabhu, Viraj and Purushwalkam, Senthil and Yan, An and Xiong, Caiming and Xu, Ran",
        "title": "Trust but Verify: Programmatic VLM Evaluation in the Wild"
      },
      {
        "key": "tu2024automatic",
        "author": "Tu, Rong-Cheng and Ma, Zi-Ao and Lan, Tian and Zhao, Yuehao and Huang, Heyan and Mao, Xian-Ling",
        "title": "Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "rawte2024vibe",
        "author": "Rawte, Vipula and Jain, Sarthak and Sinha, Aarush and Kaushik, Garv and Bansal, Aman and Vishwanath, Prathiksha Rumale and Jain, Samyak Rajesh and Reganti, Aishwarya Naresh and Jain, Vinija and Chadha, Aman and others",
        "title": "ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models"
      },
      {
        "key": "huang2024vbench++",
        "author": "Huang, Ziqi and Zhang, Fan and Xu, Xiaojie and He, Yinan and Yu, Jiashuo and Dong, Ziyue and Ma, Qianli and Chanpaisit, Nattapol and Si, Chenyang and Jiang, Yuming and others",
        "title": "VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models"
      },
      {
        "key": "he2024videoscore",
        "author": "He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and others",
        "title": "Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation"
      }
    ]
  }
]