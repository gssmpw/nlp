@article{chai2024auroracap,
  title={AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark},
  author={Chai, Wenhao and Song, Enxin and Du, Yilun and Meng, Chenlin and Madhavan, Vashisht and Bar-Tal, Omer and Hwang, Jeng-Neng and Xie, Saining and Manning, Christopher D},
  journal={arXiv preprint arXiv:2410.03051},
  year={2024}
}

@inproceedings{chen2024panda,
  title={Panda-70m: Captioning 70m videos with multiple cross-modality teachers},
  author={Chen, Tsai-Shien and Siarohin, Aliaksandr and Menapace, Willi and Deyneka, Ekaterina and Chao, Hsiang-wei and Jeon, Byung Eun and Fang, Yuwei and Lee, Hsin-Ying and Ren, Jian and Yang, Ming-Hsuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13320--13331},
  year={2024}
}

@article{doveh2023dense,
  title={Dense and aligned captions (dac) promote compositional reasoning in vl models},
  author={Doveh, Sivan and Arbelle, Assaf and Harary, Sivan and Herzig, Roei and Kim, Donghyun and Cascante-Bonilla, Paola and Alfassy, Amit and Panda, Rameswar and Giryes, Raja and Feris, Rogerio and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={76137--76150},
  year={2023}
}

@article{he2024storyteller,
  title={StoryTeller: Improving Long Video Description through Global Audio-Visual Character Identification},
  author={He, Yichen and Lin, Yuan and Wu, Jianchao and Zhang, Hanchong and Zhang, Yuchen and Le, Ruicheng},
  journal={arXiv preprint arXiv:2411.07076},
  year={2024}
}

@article{he2024videoscore,
  title={Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation},
  author={He, Xuan and Jiang, Dongfu and Zhang, Ge and Ku, Max and Soni, Achint and Siu, Sherman and Chen, Haonan and Chandra, Abhranil and Jiang, Ziyan and Arulraj, Aaran and others},
  journal={arXiv preprint arXiv:2406.15252},
  year={2024}
}

@article{hua2024finecaption,
  title={FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity},
  author={Hua, Hang and Liu, Qing and Zhang, Lingzhi and Shi, Jing and Zhang, Zhifei and Wang, Yilin and Zhang, Jianming and Luo, Jiebo},
  journal={arXiv preprint arXiv:2411.15411},
  year={2024}
}

@article{huang2024vbench++,
  title={VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models},
  author={Huang, Ziqi and Zhang, Fan and Xu, Xiaojie and He, Yinan and Yu, Jiashuo and Dong, Ziyue and Ma, Qianli and Chanpaisit, Nattapol and Si, Chenyang and Jiang, Yuming and others},
  journal={arXiv preprint arXiv:2411.13503},
  year={2024}
}

@article{ju2024miradata,
  title={Miradata: A large-scale video dataset with long durations and structured captions},
  author={Ju, Xuan and Gao, Yiming and Zhang, Zhaoyang and Yuan, Ziyang and Wang, Xintao and Zeng, Ailing and Xiong, Yu and Xu, Qiang and Shan, Ying},
  journal={arXiv preprint arXiv:2407.06358},
  year={2024}
}

@article{liu2024playground,
  title={Playground v3: Improving text-to-image alignment with deep-fusion large language models},
  author={Liu, Bingchen and Akhgari, Ehsan and Visheratin, Alexander and Kamko, Aleks and Xu, Linmiao and Shrirao, Shivam and Lambert, Chase and Souza, Joao and Doshi, Suhail and Li, Daiqing},
  journal={arXiv preprint arXiv:2409.10695},
  year={2024}
}

@article{ma2024drvideo,
  title={DrVideo: Document Retrieval Based Long Video Understanding},
  author={Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei},
  journal={arXiv preprint arXiv:2406.12846},
  year={2024}
}

@article{polyak2024movie,
  title={Movie gen: A cast of media foundation models},
  author={Polyak, Adam and Zohar, Amit and Brown, Andrew and Tjandra, Andros and Sinha, Animesh and Lee, Ann and Vyas, Apoorv and Shi, Bowen and Ma, Chih-Yao and Chuang, Ching-Yao and others},
  journal={arXiv preprint arXiv:2410.13720},
  year={2024}
}

@article{prabhu2024trust,
  title={Trust but Verify: Programmatic VLM Evaluation in the Wild},
  author={Prabhu, Viraj and Purushwalkam, Senthil and Yan, An and Xiong, Caiming and Xu, Ran},
  journal={arXiv preprint arXiv:2410.13121},
  year={2024}
}

@article{rawte2024vibe,
  title={ViBe: A Text-to-Video Benchmark for Evaluating Hallucination in Large Multimodal Models},
  author={Rawte, Vipula and Jain, Sarthak and Sinha, Aarush and Kaushik, Garv and Bansal, Aman and Vishwanath, Prathiksha Rumale and Jain, Samyak Rajesh and Reganti, Aishwarya Naresh and Jain, Vinija and Chadha, Aman and others},
  journal={arXiv preprint arXiv:2411.10867},
  year={2024}
}

@article{tu2024automatic,
  title={Automatic Evaluation for Text-to-image Generation: Task-decomposed Framework, Distilled Training, and Meta-evaluation Benchmark},
  author={Tu, Rong-Cheng and Ma, Zi-Ao and Lan, Tian and Zhao, Yuehao and Huang, Heyan and Mao, Xian-Ling},
  journal={arXiv preprint arXiv:2411.15488},
  year={2024}
}

@article{wang2023caption,
  title={Caption anything: Interactive image description with diverse multimodal controls},
  author={Wang, Teng and Zhang, Jinrui and Fei, Junjie and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan},
  journal={arXiv preprint arXiv:2305.02677},
  year={2023}
}

@article{wang2024koala,
  title={Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content},
  author={Wang, Qiuheng and Shi, Yukai and Ou, Jiarong and Chen, Rui and Lin, Ke and Wang, Jiahao and Jiang, Boyuan and Yang, Haotian and Zheng, Mingwu and Tao, Xin and others},
  journal={arXiv preprint arXiv:2410.08260},
  year={2024}
}

@inproceedings{wang2024motionctrl,
  title={Motionctrl: A unified and flexible motion controller for video generation},
  author={Wang, Zhouxia and Yuan, Ziyang and Wang, Xintao and Li, Yaowei and Chen, Tianshui and Xia, Menghan and Luo, Ping and Shan, Ying},
  booktitle={ACM SIGGRAPH 2024 Conference Papers},
  pages={1--11},
  year={2024}
}

@article{wang2024tarsier,
  title={Tarsier: Recipes for training and evaluating large video description models},
  author={Wang, Jiawei and Yuan, Liping and Zhang, Yuchen and Sun, Haomiao},
  journal={arXiv preprint arXiv:2407.00634},
  year={2024}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@inproceedings{xu2017video,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@article{yang2024vript,
  title={Vript: A Video Is Worth Thousands of Words},
  author={Yang, Dongjie and Huang, Suyuan and Lu, Chengqiang and Han, Xiaodong and Zhang, Haoxin and Gao, Yan and Hu, Yao and Zhao, Hai},
  journal={arXiv preprint arXiv:2406.06040},
  year={2024}
}

