\section{Related Work}
\label{sec:related_works}

\paragraph{Aggregate LLM performance prediction} Previous studies explored aggregate performance prediction across computational scales (\textit{scaling laws}, \citealp{kaplan2020scaling, hernandez2020measuring}) and predicted  LLMs' accuracy on BIG-Bench \citep{srivastava2023imitationgamequantifyingextrapolating} tasks using factors such as parameters or compute usage \citep{ye2023how,owen2024predictable}. Relatedly, \citet{ruan2024observational} %
predicted aggregate task performance using latent factors derived from benchmark performance and compute usage of multiple LLMs. 
In contrast, \predbench focuses on instance-level predictability.

\paragraph{How human users predict LLM performance}
Humans were found 
to only marginally beat random guess in predicting GPT-4's performance \citep{carlini_gpt4_challenge}. 
Relatedly, \citet{vafa2024largelanguagemodelsperform} showed humans overestimate LLM future performance based on prior interactions, especially with larger models in high-stakes contexts. They argue that ``the best LLM is the one that allows humans to make the most reliable inferences about where it will succeed'', closely aligning with \predbench's motivation. \citet{zhou2024larger} indicated that human predictions become unreliable as AI systems become more capable 
and \citet{steyvers2025large} found LLM-produced explanations supporting a statement do not lead humans to reliably assess whether that statement is correct, even when the LLM's token-level probabilities are calibrated. %
Finally, in a specific use case, \citet{bansalchallenges} reported that the failures of an LLM software engineer agent cannot be reliably anticipated.

\paragraph{Instance-level LLM performance prediction}
Various disciplines offer approaches for instance-level performance prediction. For instance, \citet{drapal2024MetaLearningNoveltyDetection} combined novelty detection with meta-learning to reject instances that are likely to cause %
AI failure. Additionally, Item Response Theory (IRT), originally developed to predict human performance %
\cite{embretson2013item}, has been adapted for machine learning and NLP \cite{martinez2019item,lalor2016building,kipnis2024metabench,polo2024tinybenchmarks,vania-etal-2021-comparing}, although it requires previously processed instances, limiting predictability for new inputs. %


Some works trained ``assessors'' to predict instance-level LLM performance. %
\citet{kadavath2022language} trained LLMs to predict the probability of succeeding on a question without reference to %
a specific answer, %
which performed satisfactorily but struggled with novel tasks. 
\citet{schellaert2024analysing} effectively predicts the performance of LLMs on 100+ BIG-bench tasks, outperforming subject systems in confidence, and maintaining predictability across different model sizes, suggesting scalability.
\citet{zhou2022reject}  showed that smaller LLMs can %
predict performance of %
larger models %
on certain tasks, almost halving errors and computational costs. %
\citet{DRAPAL2024112351} obtained explainable meta-rules from trained assessors to identify regions of predictable performance. 
Finally, while assessors allow to avoid prompting models on task instances causing failures, their training requires generating a failure/correctness dataset specific to each LLM. Single assessors  sharing information across many LLMs \citet{pacchiardi2024100instancesneedpredicting} reduce the instances on which each LLM must be tested.
All these techniques can be used as assessors for 
\predbench.


\looseness=-1
\paragraph{Machine learning with reject options} %
Surveyed by \citet{hendrickx2024machine}, these models are closely related %
to the subjects of \predbench, with ``rejectors'' analogous to our interpretation of assessors. \citet{hendrickx2024machine} categorise rejectors according to their reliance on the ``predictor'' model.  %
Their ``separated'' rejectors are trained without involving the predictor, yet more powerful assessors %
can be trained using the observed LLM validity %
In any case, \citet{hendrickx2024machine} advocates for the %
development of benchmarks for %
ML with reject options, exemplified by %
\predbench, which can evaluate all kinds of rejectors and assessors.%

\paragraph{LLM Uncertainty Quantification (UQ)}
\looseness=-1
\citet{shorinwa} splits UQ methods for LLMs into token-level \citep{kadavath2022language}, verbalisation \citep{lin2022teaching, kapoor} and ``semantic similarity'' (prompting the model multiple times and grouping answers with the same meaning, \citealp{kuhn2023semantic}). 

White-box approaches that rely on model activations also exist \citep{ferrando2025do}.
However, in general, the performance of UQ methods is debated \citep{kapoor}; for instance, \citet{pawitan2024confidence} found  different methods to extract confidence to be poorly correlated and only partly indicative of correctness.
Additionally, in contrast to the anticipative assessors we use as baselines, %
UQ methods require inputs to be passed through the LLM, making them close to the ``integrated rejectors'' described in \cite{hendrickx2024machine}. Nevertheless, \predbench can be used to evaluate these methods too.



\paragraph{LLM routing}
LLM routers \citep{lee2023orchestrallm,vsakota2024fly,lu2023routing,shnitzer2023LargeLanguageModel,ding2024hybrid}
direct task instances to the most appropriate LLM from a pool 
trading off performance, cost, response time or other factors\footnote{See \href{https://withmartian.com/}{Martian} for a commercially-available router.}.
This mechanism %
is similar to %
\textit{delegating classifiers}  where initial classifiers delegate difficult tasks to %
specialised ones 
\citep{ferri2004delegating}. Although assessors can act %
as routers by predicting LLM-specific probabilities of success, routers often bypass this step by directly selecting the model most likely to succeed. %
RouterBench \citep{hu2024routerbench} ranks routers based on their selection from a fixed set of LLMs, whereas \predbench evaluates each LLM paired with an  assessor.  %
While \predbench focuses on metrics %
measuring the size of operating conditions,  %
RouterBench uses an aggregate metric of %
quality and cost to optimise the use of %
LLMs in low-risk scenarios.


\looseness=-1
\paragraph{Model behaviour analysis} Various disciplines  %
help %
to understand the performance of %
AI models. %
 Surrogate modelling %
(\citealp{ilyas2022datamodels}) %
anticipates model behaviour from training data, while %
error analysis methods \cite{amershi2015modeltracker} %
identify %
weaknesses and %
incorrect predictions. Out-of-Distribution (OOD) detection \cite{hendrycks2016baseline,liang2017enhancing} targets %
unpredictable input behaviour (e.g., outliers). Unlike OOD methods, %
\predbench focuses on %
anticipating performance for inputs both in and out-distribution.





\paragraph{Guaranteed/SafeGuarded AI} Different research agendas \citep{dalrymple2024guaranteedsafeaiframework, darpa2024} propose the development of ``safeguarded'' AI systems, that come with (possibly probabilistic, \citealp{bengio2024bayesianoraclepreventharm}) performance guarantees in particular operating regions. \predbench can empirically test these methods.