\section{Related Work}
\label{sec:related_works}

\paragraph{Aggregate LLM performance prediction} Previous studies explored aggregate performance prediction across computational scales (\textit{scaling laws}, ____) and predicted  LLMs' accuracy on BIG-Bench ____ tasks using factors such as parameters or compute usage ____. Relatedly, ____ %
predicted aggregate task performance using latent factors derived from benchmark performance and compute usage of multiple LLMs. 
In contrast, \predbench focuses on instance-level predictability.

\paragraph{How human users predict LLM performance}
Humans were found 
to only marginally beat random guess in predicting GPT-4's performance ____. 
Relatedly, ____ showed humans overestimate LLM future performance based on prior interactions, especially with larger models in high-stakes contexts. They argue that ``the best LLM is the one that allows humans to make the most reliable inferences about where it will succeed'', closely aligning with \predbench's motivation. ____ indicated that human predictions become unreliable as AI systems become more capable 
and ____ found LLM-produced explanations supporting a statement do not lead humans to reliably assess whether that statement is correct, even when the LLM's token-level probabilities are calibrated. %
Finally, in a specific use case, ____ reported that the failures of an LLM software engineer agent cannot be reliably anticipated.

\paragraph{Instance-level LLM performance prediction}
Various disciplines offer approaches for instance-level performance prediction. For instance, ____ combined novelty detection with meta-learning to reject instances that are likely to cause %
AI failure. Additionally, Item Response Theory (IRT), originally developed to predict human performance %
____, has been adapted for machine learning and NLP ____, although it requires previously processed instances, limiting predictability for new inputs. %


Some works trained ``assessors'' to predict instance-level LLM performance. %
____ trained LLMs to predict the probability of succeeding on a question without reference to %
a specific answer, %
which performed satisfactorily but struggled with novel tasks. 
____ effectively predicts the performance of LLMs on 100+ BIG-bench tasks, outperforming subject systems in confidence, and maintaining predictability across different model sizes, suggesting scalability.
____  showed that smaller LLMs can %
predict performance of %
larger models %
on certain tasks, almost halving errors and computational costs. %
____ obtained explainable meta-rules from trained assessors to identify regions of predictable performance. 
Finally, while assessors allow to avoid prompting models on task instances causing failures, their training requires generating a failure/correctness dataset specific to each LLM. Single assessors  sharing information across many LLMs ____ reduce the instances on which each LLM must be tested.
All these techniques can be used as assessors for 
\predbench.


\looseness=-1
\paragraph{Machine learning with reject options} %
Surveyed by ____, these models are closely related %
to the subjects of \predbench, with ``rejectors'' analogous to our interpretation of assessors. ____ categorise rejectors according to their reliance on the ``predictor'' model.  %
Their ``separated'' rejectors are trained without involving the predictor, yet more powerful assessors %
can be trained using the observed LLM validity %
In any case, ____ advocates for the %
development of benchmarks for %
ML with reject options, exemplified by %
\predbench, which can evaluate all kinds of rejectors and assessors.%

\paragraph{LLM Uncertainty Quantification (UQ)}
\looseness=-1
____ splits UQ methods for LLMs into token-level ____, verbalisation ____ and ``semantic similarity'' (prompting the model multiple times and grouping answers with the same meaning, ____). 

White-box approaches that rely on model activations also exist ____.
However, in general, the performance of UQ methods is debated ____; for instance, ____ found  different methods to extract confidence to be poorly correlated and only partly indicative of correctness.
Additionally, in contrast to the anticipative assessors we use as baselines, %
UQ methods require inputs to be passed through the LLM, making them close to the ``integrated rejectors'' described in ____. Nevertheless, \predbench can be used to evaluate these methods too.



\paragraph{LLM routing}
LLM routers ____
direct task instances to the most appropriate LLM from a pool 
trading off performance, cost, response time or other factors\footnote{See \href{https://withmartian.com/}{Martian} for a commercially-available router.}.
This mechanism %
is similar to %
\textit{delegating classifiers}  where initial classifiers delegate difficult tasks to %
specialised ones 
____. Although assessors can act %
as routers by predicting LLM-specific probabilities of success, routers often bypass this step by directly selecting the model most likely to succeed. %
RouterBench ____ ranks routers based on their selection from a fixed set of LLMs, whereas \predbench evaluates each LLM paired with an  assessor.  %
While \predbench focuses on metrics %
measuring the size of operating conditions,  %
RouterBench uses an aggregate metric of %
quality and cost to optimise the use of %
LLMs in low-risk scenarios.


\looseness=-1
\paragraph{Model behaviour analysis} Various disciplines  %
help %
to understand the performance of %
AI models. %
 Surrogate modelling %
(____) %
anticipates model behaviour from training data, while %
error analysis methods ____ %
identify %
weaknesses and %
incorrect predictions. Out-of-Distribution (OOD) detection ____ targets %
unpredictable input behaviour (e.g., outliers). Unlike OOD methods, %
\predbench focuses on %
anticipating performance for inputs both in and out-distribution.





\paragraph{Guaranteed/SafeGuarded AI} Different research agendas ____ propose the development of ``safeguarded'' AI systems, that come with (possibly probabilistic, ____) performance guarantees in particular operating regions. \predbench can empirically test these methods.