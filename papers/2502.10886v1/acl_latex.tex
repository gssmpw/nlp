% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% Added packages
\usepackage{subcaption}
\usepackage{booktabs} % for professional tables

\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{MET-Bench: Multimodal Entity Tracking for Evaluating \\ the Limitations of Vision-Language and Reasoning Models}


\author{Vanya Cohen \and Raymond Mooney \\
        Department of Computer Science\\
        The University of Texas at Austin\\
        \texttt{\{vanya,mooney\}@utexas.edu}}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}

Entity tracking is a fundamental challenge in natural language understanding, requiring models to maintain coherent representations of entities. Previous work has benchmarked entity tracking performance in purely text-based tasks. We introduce MET-Bench, a multimodal entity tracking benchmark designed to evaluate the ability of vision-language models to track entity states across modalities. Using two structured domains, Chess and the Shell Game, we assess how effectively current models integrate textual and image-based state updates. Our findings reveal a significant performance gap between text-based and image-based tracking and that this performance gap stems from deficits in visual reasoning rather than perception. We further show that explicit text-based reasoning strategies improve performance, yet substantial limitations remain, especially in long-horizon multimodal scenarios. Our results highlight the need for improved multimodal representations and reasoning techniques to bridge the gap between textual and visual entity tracking.

\end{abstract}

\section{Introduction}

Natural language understanding requires the ability to track and update information about entities as they evolve through text. From coreference resolution \citep{HOBBS1978311, lappin-leass-1994-algorithm} and discourse processing to narrative comprehension, computational linguistics has long grappled with the challenge of maintaining coherent entity representations across textual contexts \citep{bunescu-pasca-2006-using, Schank1977}.

While significant progress has been made in tasks like coreference resolution and entity linking \cite{liu2023brief, papalampidi2022towards}, the broader challenge of tracking entity states—understanding how entities change through sequences of actions or events—remains an open challenge \citep{fagnou-etal-2024-chain, kim-schuster-2023-entity, toshniwal2022chess}. This limitation becomes particularly apparent in tasks requiring integration of information across multiple modalities, an increasingly important frontier in computational linguistics as language processing systems are asked to reason about content that combines text with other forms of communication like images and video.

Our work examines this challenge through the lens of multimodal entity state tracking, where changes to entity states must be understood from both textual descriptions and visual observations. This setting provides a natural extension to classical NLP problems like discourse processing and situated language understanding, while also connecting to emerging research in multimodal dialogue and human-AI interaction. We focus specifically on scenarios where language models must reason about world-state changes described through a combination of text and images.
Consider the task of understanding assembly instructions that combine written steps with supporting diagrams: while text might specify "Attach panel A to the base using the provided screws," accompanying images show the precise alignment and orientation. Accurate language understanding in such contexts requires maintaining a coherent mental model that integrates both linguistic and visual information about how entities' states evolve.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/chess_state.pdf}
        \caption{Chess State}
        \label{fig:chess_state}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.20\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/chess_action.pdf}
        \caption{Chess Action}
        \label{fig:chess_action}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/shell_state.pdf}
        \caption{Shell Game State}
        \label{fig:shell_state}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/shell_action.pdf}
        \caption{Shell Game Action}
        \label{fig:shell_action}
    \end{subfigure}
    \caption{Illustration of the two domains used in this work. 
    (a) An example Chess board state. 
    (b) The chess move (action) rendered as an image. 
    (c) An initial shell game state with the blue ball under a shell.
    (d) Image action representing shells at positions one and three being swapped.}
    \label{fig:domains}
\end{figure*}

To systematically evaluate models' capabilities in this multimodal language understanding setting, we introduce two complementary benchmarks: \textit{multimodal Chess} and the \textit{Shell Game}. Through these domains, we assess how effectively current language models can track entity states when updates are conveyed through both text and images. Our analysis reveals substantial disparities in how models process text-based and image-based entity-state updates, highlighting fundamental limitations in their multimodal language understanding.

While Chess and the Shell Game provide structured testbeds for evaluating entity tracking, real-world applications often involve more ambiguous and dynamic environments. However, by isolating state-tracking performance in controlled settings, we establish a clear baseline for assessing multimodal reasoning, one that provides a straightforward means of evaluation and can scale in difficulty with minimal changes.

We hypothesize that current language models struggle with multimodal entity tracking not due to low-level perceptual failures but because they lack representations (learned or otherwise engineered) for maintaining entity coherence across sequential visual observations. This suggests a fundamental limitation in how these models integrate and update state representations from different modalities.

We make the following contributions:
\begin{itemize}
\item We introduce the multimodal entity tracking benchmark (MET-Bench) that extends traditional NLP entity tracking evaluation to the multimodal setting for two domains: \textit{multimodal Chess} and \textit{Shell Game}.
\item We demonstrate that current models, despite strong performance on pure text tasks, struggle to maintain accurate entity representations when processing mixed text and image inputs.
% \item We quantify a substantial performance gap between text-based and image-based entity tracking, highlighting specific challenges in multimodal language understanding.
\item Through probing experiments, we show that these limitations stem from higher-level reasoning challenges rather than low-level perception issues.
\item We evaluate various approaches to improving multimodal entity tracking, finding that techniques emphasizing explicit reasoning outperform traditional NLP methods like fine-tuning when generalizing to novel domains.
\end{itemize}

\section{Background}

We formulate the problem of \textit{multimodal entity tracking} as a sequential state estimation task, where an agent must infer the final state of a system given an initial state and a series of observed actions. Formally, at each time step \( t \), the environment is in state \( \bm{S_t} \), and transitions occur according to an action sequence \( \bm{A} = (\bm{a_1}, \bm{a_2}, \dots, \bm{a_T}) \). The agent receives observations \( \bm{A_t} \) corresponding to each action, which may be textual (\( \bm{A_t^{\text{text}}} \)) or visual (\( \bm{A_t^{\text{image}}} \)). The objective is to infer the final state,
\[
\bm{S_T} = f(\bm{S_0}, \bm{A_1}, \bm{A_2}, \dots, \bm{A_T}),
\]
where \( f \) is the function modeling entity state updates.

MET-Bench represents the initial and final states of each domain as text but evaluate the models' ability to track entity state changes through images. This approach isolates the multimodal entity tracking challenge by ensuring that models begin and end with well-defined textual representations while processing state transitions visually. By doing so, we assess their capacity to maintain coherent entity representations across modalities while minimizing confounding errors from perceptual failures, which remain a known limitation of current vision-language models \cite{sharma2024vision}.

\subsection{Chess Domain}
Chess is a well-studied domain for testing entity tracking of deep learning models \cite{toshniwal2022chess}. The state \( S_t \) represents an 8×8 board configuration expressed in Forsyth–Edwards Notation (FEN) notation, actions correspond to legal chess moves from real games, and action observations consist of either symbolic (UCI notation) or visual (board images) descriptions of moves. We likewise adopt chess as an entity tracking testbed where the task is to maintain a correct representation of the board state across a sequence of moves. This distinction foregrounds how well a model can integrate and track piece locations as they change over time in potentially complicated board configurations.

Utilizing real Chess games from the Millionbase dataset\footnote{\url{https://rebel13.nl/rebel13/rebel\%2013.html}} used in \citet{toshniwal2022chess}, we generate sequences of states and actions (moves) using standard chess notation: Universal Chess Interface (UCI) for actions and FEN for board states.
\begin{itemize}[leftmargin=1.5em]
    \item \textbf{Text-Encoded Moves}: Each action is provided as a short UCI textual description (e.g., \texttt{``e2e4''} for moving a piece from e2 to e4).
    \item \textbf{Image-Encoded Moves}: Each action is accompanied by a rendered image that serves as a visual representation of the move (see Fig.\ \ref{fig:domains}).
\end{itemize}

In both cases, the final output is the FEN-encoded location of each piece on the board after a sequence of moves. The dataset includes multiple game trajectories of varying length, capturing a variety of piece types and board states.

\subsection{Shell Game Domain}
The second domain in MET-Bench is the Shell Game, a classic demonstration of hidden-state tracking. A ball is placed under one of three cups (or shells), which are then swapped pairwise in succession. The goal is tracking which cup currently hides the ball as shells are swapped. The state \( S_t \) tracks the hidden position of a ball under three shells, actions correspond to swaps between pairs of shells. Other works have explored shell-game-like domains with varying levels of added complexity \citep{li-etal-2021-implicit, long-etal-2016-simpler, kim-schuster-2023-entity}.

This domain  has a simpler entity-state and action space than Chess. However, while many frontier models have been trained on UCI/FEN encoded chess games, the Shell Game is, to the best of our knowledge, not present in the training data of these models. There may however be analogous tasks in the pre-training data.

We simulate repeated Shell Game swaps to create a set of Shell Game trajectories. Swap actions are either:
\begin{itemize}
    \item \textbf{Text-Encoded Swap}: Denoted as \texttt{``x swap y''}, where \(\{x,y\}\) are in \(\{1,2,3\}\).
    \item \textbf{Image-Encoded Swap}: An image depicting the shells being swapped, with the ball visually hidden (see Fig.\ \ref{fig:domains}).
\end{itemize}

The ground truth entity state after the game finishes is a single number indicating the final shell position of the ball.

\subsection{Models}

We use MET-Bench to evaluate the limitations of frontier models, including vision-language models (VLMs) which accept images and text as input, and newer reasoning models like OpenAI's o1 that are trained using reinforcement learning and utilize test-time search algorithms to improve their reasoning abilities on domains like mathematics and coding. A full list of models and their capabilities is shown in Appendix \ref{sec:appendix}, Table \ref{tab:ai_models}.

\section{Methods}
\label{sec:methods}

\begin{figure}[ht]
\begin{small}
\begin{tabularx}{\columnwidth}{p{0.15\columnwidth} X}
\toprule
\textbf{Role} & \textbf{Messages} \\
\midrule
\textbf{User} &
\texttt{\detokenize{You are a helpful assistant that tracks chess moves in a game and produces the final FEN.
The initial state is:
}}
\newline
{\scriptsize
\texttt{
rnbqkbnr/pppppppp/8/8/8/8/
PPPPPPPP/RNBQKBNR w KQkq - 0 1
}}
\newline
\texttt{\detokenize{
Here are the moves played:
}}
\\[6pt]

 &
\texttt{\detokenize{e2e4}}
\\[2pt]

 &
\texttt{\detokenize{e7e5}}
\\[2pt]

 &
\texttt{\detokenize{Now what is the final FEN? Only output the FEN.}}
\\[6pt]

\textbf{Assistant} &
{\scriptsize
\texttt{rnbqkbnr/pppp1ppp/8/4p3/4P3/
8/PPPP1PPP/RNBQKBNR w KQkq - 2 2
}}
\\
\bottomrule
\end{tabularx}
\end{small}
\caption{An example zero-shot user--assistant exchange in the \textbf{Chess} domain, showing the initial board state as FEN, two UCI moves (e2e4, e7e5) and the final state. For image actions, the UCI moves are replaced with their visual representations and a description of how to interpret these images. The FEN is line-broken for readability.}
\label{fig:chess-prompt}
\end{figure}

We utilize the standard chat-based schema exposed by current frontier models that consists of interleaved user-provided and assistant (model) provided messages. Figures \ref{fig:chess-prompt} shows the prompting strategy used for the Chess domain. Similarly, Appendix \ref{sec:appendix}, \ref{fig:shell-prompt} shows the prompting strategy used for the Shell Game domain. Text actions are represented using simple notation on which the models have been trained, UCI for Chess and a simple domain-specific-language for Shell Game.  

In the case of image-action input, the text actions are replaced with their image-rendered versions as Base64 encoded PNG images and a text description of how to interpret the image-actions is provided. Fig.\ \ref{fig:domains} shows the image representations used for the Chess and Shell Game actions. These image representations were created through visual-prompt engineering to maximize the classification accuracy of actions depicted. Various common image representations were explored including arrows, bounding boxes, and symbolic markers. The image depiction of the game actions is explained to the language model every time images are provided using the prompts in Appendix \ref{sec:appendix}, Figures \ref{fig:chess-image-prompt} \& \ref{fig:shell-game-image-prompt}.

\section{Experiments}
\label{sec:experiments}

\begin{table*}[ht]
\centering
\begin{subtable}{0.45\textwidth}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Chess & \textbf{Text} & \textbf{Image} \\
\midrule
\multicolumn{3}{l}{\textbf{Baseline}} \\
Game Start       & 74.4 & 74.4 \\
\midrule
\multicolumn{3}{l}{\textbf{Zero-Shot}} \\
Claude 3.5 Sonnet & 96.8 & 66.2 \\
MiniMax-VL-01        & 86.3 & 65.8 \\
Gemini-2.0-Flash  & 93.3 & 74.7 \\
GPT-4o mini       & 68.3 & 60.6 \\
GPT-4o            & 89.6 & 73.3 \\
\midrule
\multicolumn{3}{l}{\textbf{Few-Shot (n=5)}} \\
Claude 3.5 Sonnet & 99.6 & 77.9 \\
MiniMax-VL-01        & 88.0 & 75.9 \\
Gemini-2.0-Flash  & 96.6 & 78.7 \\
GPT-4o mini       & 75.1 & 74.9 \\
GPT-4o            & 94.9 & 77.7 \\
\midrule
\multicolumn{3}{l}{\textbf{Chain-of-Thought}} \\
Claude 3.5 Sonnet & 97.9 & 68.4 \\
MiniMax-VL-01        & 62.7 & 58.0 \\
Gemini-2.0-Flash  & 93.3 & 74.5 \\
GPT-4o mini       & 72.0 & 69.6 \\
GPT-4o            & 91.8 & 74.9 \\
\midrule
\multicolumn{3}{l}{\textbf{Reasoning}} \\
o1-mini           & 65.1 & - \\
o3-mini           & 99.6 & - \\
o1                & 98.2 & 83.5 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Chess with ten moves in each sequence.}
\label{tab:chess-text-image-results}
\vskip -0.1in
\end{subtable}
\hfill
\begin{subtable}{0.45\textwidth}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Shell Game & \textbf{Text} & \textbf{Image} \\
\midrule
\multicolumn{3}{l}{\textbf{Baseline}} \\
Random & 33.3 & 33.3 \\
\midrule
\multicolumn{3}{l}{\textbf{Zero-Shot}} \\
Claude 3.5 Sonnet & 34.4 & 36.2 \\
MiniMax-VL-01        & 34.4 & 35.2 \\
Gemini-2.0-Flash  & 30.0 & 33.4 \\
GPT-4o mini       & 30.6 & 32.8 \\
GPT-4o            & 36.0 & 32.4 \\
\midrule
\multicolumn{3}{l}{\textbf{Few-Shot (n=5)}} \\
Claude 3.5 Sonnet & 34.0 & 30.6 \\
MiniMax-VL-01        & 36.4 & 32.0 \\
Gemini-2.0-Flash  & 37.0 & 31.4 \\
GPT-4o mini       & 34.4 & 31.2 \\
GPT-4o            & 37.2 & 31.0 \\
\midrule
\multicolumn{3}{l}{\textbf{Chain-of-Thought}} \\
Claude 3.5 Sonnet & 97.4 & 94.2 \\
MiniMax-VL-01        & 92.6 & 32.8 \\
Gemini-2.0-Flash  & 76.8 & 33.8 \\
GPT-4o mini       & 84.4 & 35.0 \\
GPT-4o            & 99.8 & 84.2 \\
\midrule
\multicolumn{3}{l}{\textbf{Reasoning}} \\
o1-mini           & 99.8 & - \\
o3-mini           & 100.0 & - \\
o1                & 100.0 & 92.8 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Shell Game with five swap moves in each sequence.}
\label{tab:shell-text-image-results}
\end{subtable}
\caption{Entity tracking accuracy in Chess and Shell Game for text and image actions. In the Few-Shot setting $N=5$ in-context examples are used. Methods and models which employ explicit reasoning perform best (chain-of-thought and reasoning models).}
\vskip -0.1in
\end{table*}

We perform experiments across a wide range of models and settings to evaluate different aspects of frontier-model entity-tracking performance. For all experiments, the models are sampled with a temperature of zero.

\subsection{Tracking Entities with Text and Image Actions}

We evaluate difference in accuracy when tracking images from text and image actions in the zero-shot, few-shot, and chain-of-thought settings. These results are presented in Section \ref{sec:image_vs_text_results}.

\subsubsection*{Zero-shot}

\paragraph{Chess}
In the Chess domain, we evaluate on a set of $100$ games selected at random from the test set, each with a sequence length of ten actions. The model must predict the FEN string for the final state. If the FEN string contains syntax errors such that it cannot be parsed, the accuracy for that instance is zero. We report the per-square accuracy of the predicted board, that is the ratio of correctly predicted pieces (or absence of a piece) to the total number of board tiles. The `Game Start' baseline is the accuracy of predicting the initial board configuration. After only ten actions, most of the board configuration remains unchanged, so this is a strong baseline.

\paragraph{Shell Game}
We evaluate Shell Game using a set of $500$ games generated at random, each with a sequence length of five actions.  The final state is a single number $n \in \{1, 2, 3\}$ that gives the position of the ball, and we measure the accuracy of predicting it. The naive baseline picks a position uniformly at random.

\subsubsection*{Few-Shot and Chain-of-Thought}
In these settings, the evaluation and procedure remain largely unchanged from the zero-shot Chess. For the few-shot experiments, $N=5$ examples are selected at random from the training set and prepended to the test example. The few-shot examples have the same number of actions as the test-set examples. To evaluate the effect of chain-of-thought reasoning, we prompt the model to `think step by step before producing a final answer.'

\subsection{Sequence-Length Variation}
The sequence length of the actions is varied to quantify the effect of compounding errors on the models. These sequences range from zero to $100$ actions in both the image and text action modality. This serves to quantify the relative drop-off in performance among models in the zero-shot setting. These results are presented in Section \ref{sec:seq_len_results}.

\subsection{Image-Action Classification}
\label{sec:image_action_exp}
We perform an experiment to demonstrate that VLMs  have the ability to accurately interpret the actions depicted in the image-action representations. Figures \ref{fig:chess-image-prompt} \& \ref{fig:shell-game-image-prompt} in Appendix \ref{sec:appendix} show the prompting strategy used to get a VLM to predict (zero-shot) a text action from an image action. We perform this evaluation on a test set of $1000$ image depictions of actions for each game. The results are presented in Section \ref{sec:image2text_results}.

\subsection{Cascaded Inference}
Using the text actions predicted from the images in the image-action classification task, we devise an ablation to test the effect of cascading (performing multimodal inference in two steps) in the zero-shot setting. Given a sequence of image-based observations \( \bm{A_t^{\text{image}}} \), a vision-language model (VLM) first predicts the corresponding text-based action sequence \( \hat{\bm{A}}^{\text{text}} = g(\bm{A}^{\text{image}}) \), where \( g \) maps images to text using the procedure in Section \ref{sec:image_action_exp}. The text actions are then used for zero-shot inference to estimate the final state as \( \bm{S_T} = f(\bm{S_0}, \hat{\bm{A}}^{\text{text}}) \). This removes the need for the model to perform entity tracking directly in the image modality, isolating the effect of perceptual failures. These results are presented in Section \ref{sec:cascaded_results}.


\subsection{Fine-Tuning Experiments}
We fine-tune frontier OpenAI models using their fine-tuning API on a small training set. We evaluate these in the Chess and Shell Game domains using both image and text modalities. For Chess, we select a random set of $100$ training examples of sequence length ten. For Shell Game we use $20$ training examples of sequence length $3$. Results are available in Section \ref{sec:finetune_results}. Hyperparameters were determined through grid search and are available in Appendix \ref{sec:finetune_results}, Table \ref{tab:hyperparams}.

\subsection{Mixed-Modality Experiments}
We perform an ablation where the image-action and text-action modalities are mixed. At each action step, one action modality is randomly selected with a probability varying from $100\%$ text actions to $100\%$ image actions. This serves to quantify the effect of forcing the model to reason over multiple input modalities simultaneously. These results are presented in Section \ref{sec:mixed_modality_results}.

\section{Results}
\label{sec:results}

\subsection{Tracking in Text Outperforms Images}
\label{sec:image_vs_text_results}
\paragraph{Chess}
Table \ref{tab:chess-text-image-results} reports the accuracy for text and image actions in the chess domain. Performance in the text modality is significantly better than in the image modality. In the zero-shot setting, the best-performing text model, Claude 3.5 Sonnet, achieves an impressive $96.8\%$ accuracy, while its image-based counterpart drops sharply to $66.2\%$. A similar trend holds across models, indicating that current models can perform entity tracking in text but fail to integrate visual updates as effectively.

Both in-context learning with few-shot prompting and chain-of-thought reasoning lead to performance improvements. In the image modality, the accuracy is not significantly greater than the naive baseline. Only the o1 reasoning model achieves an accuracy greater than $80\%$ in the image modality.

\paragraph{Shell Game}

\begin{figure*}[t]
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[height=2.5in]{figures/chess_text_sequence_length_vs_accuracy_multi_model.pdf}
  \caption{Chess zero-shot accuracy with text actions.}
  \label{fig:chess_text_seq}
\end{subfigure}
\quad
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[height=2.5in]{figures/chess_image_sequence_length_vs_accuracy_multi_model.pdf}
  \caption{Chess zero-shot accuracy with image actions.}
  \label{fig:chess_image_seq}
\end{subfigure}
\caption{In the text action setting, the reasoning models, o1 and o3-mini, maintain the highest accuracy at longer sequence lengths. o1-mini and the other models begin to output invalid and inaccurate board representations. All models struggle to maintain accurate board representations in the image action setting, with o1 performing the best.}
\label{fig:chess_side_by_side}
\vskip -0.2in
\end{figure*}

Table \ref{tab:shell-text-image-results} reports the accuracy for text and image actions for Shell Game. The results follow a pattern similar to that of Chess, with text-based tracking outperforming image-based tracking. However, unlike in Chess, the performance in the zero-shot setting is close to random. But the best  model, o1, attains accuracies of $100.0\%$ and $92.8\%$ for the text and image modalities, respectively.

Few-shot prompting provides only marginal improvements, but chain-of-thought gives large performance increases.  GPT-4o's accuracy jumps from $36.0\%$ to $99.8\%$ in text, and from $32.4\%$ to $84.2\%$ in image tracking. These results suggest that when guided to decompose the task step-by-step, models can reason more effectively using image inputs, a finding that complements the results of performing cascaded inference in Section \ref{sec:cascaded_results}.

\subsection{Reasoning Aids Long Sequence Accuracy}
\label{sec:seq_len_results}

\paragraph{Chess} Figure \ref{fig:chess_text_seq} plots model accuracy against increasing sequence lengths of text actions. The models are evaluated in the zero-shot setting for sequence lengths of zero to $100$ text actions. Reasoning models like o1 and o3-mini are able to handle longer sequence lengths with a smaller decrease in accuracy. However, o1-mini performs worse than the other models as it produces more invalid FEN board representations at longer sequence lengths. In contrast, the non-reasoning models experience sharp decreases in accuracy after only a few actions. Figure \ref{fig:chess_image_seq}  plots model accuracy against increasing sequence lengths of image actions. The models are evaluated in the zero-shot setting for sequence lengths of zero to $20$ image actions. While the reasoning models attain higher accuracies, the performance differences are smaller than in the text-action setting.


\paragraph{Shell Game} 
%The Shell Game domain is simpler than the Chess domain. 
In the text modality in Figure \ref{fig:shell_text_seq}, the reasoning models o1, o1-mini, and o3-mini attain the highest accuracies. o1 performs perfectly at a sequence length of 50 actions, where the non-reasoning models' performance is significantly degraded. In the image modality results in Figure \ref{fig:shell_image_seq}, o1 performs better than the other models, but sees a rapid decrease in accuracy with sequence lengths longer than five actions. By $20$ actions, the performance of all models has degraded to random.

The superior performance of reasoning models like o1 suggests that structured inference mechanisms, such as test-time search or chain-of-thought, are crucial for maintaining coherent entity states over long sequences. This aligns with prior findings that models trained on structured reasoning tasks (e.g., mathematics, coding) develop stronger implicit state-tracking capabilities \citep{kim2024codepretrainingimprovesentity}, whereas standard vision-language models struggle with entity persistence beyond short contexts.

\subsection{Models Understand Image Actions}
\label{sec:image2text_results}

\begin{figure*}[ht]
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[height=2.5in]{figures/shell_text_sequence_length_vs_accuracy_multi_model.pdf}
  \caption{Shell Game zero-shot accuracy with text actions.}
  \label{fig:shell_text_seq}
\end{subfigure}
\quad
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[height=2.5in]{figures/shell_image_sequence_length_vs_accuracy_multi_model.pdf}
  \caption{Shell Game zero-shot accuracy with image actions.}
  \label{fig:shell_image_seq}
\end{subfigure}
\caption{In the text action setting, the reasoning models o1, o1-mini, and o3-mini achieve the highest performance over long action sequences, but performance degrades for all models as sequence length increases. In the image action setting, o1 performs the best, but achieves an accuracy no better than guessing the starting state by $20$ actions.}
\label{fig:shell_side_by_side}
\vskip -0.2in
\end{figure*}

\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Start} & \textbf{End} & \textbf{Overall} \\
\midrule
\multicolumn{4}{l}{\textbf{Chess}} \\
GPT-4o-mini & 62.1 & 55.2 & 41.0 \\
GPT-4o      & 97.3 & 97.0 & 94.5 \\
\midrule
\multicolumn{4}{l}{\textbf{Shell Game}} \\
GPT-4o-mini & 100.0 & 100.0 & 100.0 \\
GPT-4o      & 100.0 & 100.0 & 100.0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Percent image-action classification accuracy for various models. We report the accuracy of predicting the action start, end, and overall/UCI action for both Chess and Shell Game on $1{,}000$ image actions.}
\label{tab:image-action-accuracy}
\end{table}

\begin{table}[ht]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
 & \multicolumn{2}{c}{\textbf{Cascaded}} \\
\cmidrule(lr){2-3}
\textbf{Method} & \textbf{Chess} & \textbf{Shell} \\
\midrule
\multicolumn{3}{l}{\textbf{Baseline}} \\
Game Start & 74.4 & 33.3 \\
\midrule
\multicolumn{3}{l}{\textbf{Zero-Shot}} \\
GPT-4o mini      & 63.0 & 28.0 \\
GPT-4o           & 89.8 & 34.0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Cascaded entity tracking accuracy for Chess and Shell (Image → Text). In cascaded inference, the model is first used to map each image action to the text representation of the action. Then model is prompted to perform the entity tracking task as in the text-action setting.}
\label{tab:chess-shell-image2text}
\vskip -0.1in
\end{table}

Table \ref{tab:image-action-accuracy} shows the performance of GPT-4o and GPT-4o-mini on classifying the text action represented by each image action. For Chess, the action `start' is the square of the piece being moved, and `end' is the destination square. For Shell Game, the `start' is the first shell to be swapped and `end' is the second. `Overall' is the accuracy of classifying the entire action (start and end) correctly. While GPT-4o-mini struggles to recognize actions in chess, GPT-4o achieves an accuracy of $94.5\%$. Both models attain perfect accuracy on the simpler Shell Game domain. This indicates that perception of the image-actions is not the fundamental limiting factor for effective entity tracking with image inputs.

A potential concern is whether the observed failures stem from poor image representation rather than reasoning deficiencies. However, our cascaded inference experiments (Section \ref{sec:cascaded_results}) demonstrate that when models first translate image actions into text, they achieve near-text-level accuracy. This suggests that models can correctly parse image-based actions but struggle with integrating them into coherent state updates, a limitation in reasoning rather than perception.

\subsection{Cascading Matches Text-Only Tracking}
\label{sec:cascaded_results}

Table \ref{tab:chess-shell-image2text} shows the accuracy of cascaded inference in the zero-shot setting for GPT-4o and GPT-4o-mini. In this setting, the image actions are first translated into text actions, and then run through the text-based entity tracking pipeline. The performance in the cascaded setting is similar to the text-action performance, showing that the model has the task-knowledge needed to perform entity tracking in both domains, but cannot reason effectively in the image modality.

\section{Discussion}

Our evaluation of frontier model performance on MET-Bench provides several insights into the current state and remaining challenges of multimodal entity tracking. We demonstrate a significant performance gap between text-based and image-based entity tracking across all evaluated models, with even state-of-the-art vision-language-reasoning models struggling to maintain accurate entity states when processing visual inputs. This disparity persists across both the Chess and Shell Game domains, suggesting a fundamental limitation in current architectures' ability to reason about entity states through visual observations.

This finding is particularly noteworthy given that our image-action classification results (Table \ref{tab:image-action-accuracy}) demonstrate that models can accurately perceive and classify individual visual actions. The gap between perception and reasoning suggests that the challenge lies not in processing visual inputs, but in maintaining and updating coherent entity information across sequential visual observations.

Our cascaded inference experiments provide further evidence for this interpretation. When models first translate visual inputs to text before performing entity tracking, they achieve performance comparable to pure text-based tracking. This indicates that the models possess the relevant task knowledge and reasoning capabilities, but struggle to apply them directly in the visual domain.

Further, the effectiveness of chain-of-thought prompting, particularly in the Shell Game domain where it improved GPT-4o's accuracy from $36.0\%$ to $99.8\%$ for text and $32.4\%$ to $84.2\%$ for images, highlights the importance of explicit reasoning for entity tracking. This improvement indicates that current models can perform complex entity tracking when guided to decompose the task into smaller steps, even in novel domains not present in their training data. However, the fact that such prompting was necessary suggests that models do not implement robust tracking, particularly in multimodal settings. Lastly, the performance of specialized reasoning models like o1 and o3-mini on longer sequences demonstrates the potential of architectures explicitly trained for sequential reasoning to maintain coherent entity states despite the challenges of accumulating errors over extended sequences.

\section{Related Work}

Entity tracking has been extensively studied in textual domains, with a focus on probing and improving language models' abilities to maintain representations of entity states. For instance, \citet{toshniwal2022chess} evaluates chess as an entity tracking domain, employing fine-tuned models \citep{radford2019language} to assess performance. Similarly, \citet{kim-schuster-2023-entity} examine the impact of model size and fine-tuning on entity tracking in textual settings similar to our Shell Game domain. \citet{tandon-etal-2020-dataset} construct a benchmark for understanding entity state changes in procedural texts. \citet{shirai-etal-2022-visual} construct the Visual Recipe Flow corpus and evaluate the ability of multimodal embedding models to properly sequence images depicting recipe states. In contrast, our work requires predicting entity state changes from actions specified in images and involves larger state spaces.

Several studies explore the implicit representations of entity states in language models. \citet{li-etal-2021-implicit} and \citet{long-etal-2016-simpler} use semantic probing to reveal that Transformer-based models \citep{vaswani2017attention} capture entity state representations implicitly during textual reasoning. Building on this, \citet{prakash2023fine} demonstrate that fine-tuning language models for entity tracking tasks enhances pre-existing internal mechanisms rather than learning entirely new representations. \citet{li2023emergent} find that Transformers trained on Othello games form internal representations of the game state.

Efforts to improve textual entity tracking beyond domain-specific fine-tuning include \citet{fagnou-etal-2024-chain}, which establishes theoretical limitations of the Transformer architecture in tracking entities. They propose a novel attention mechanism to enhance entity tracking in Transformers. \citet{gupta-durrett-2019-effective} fine-tunes small Transformer-based models for tracking entity state in instructional texts. \citet{kim2024codepretrainingimprovesentity} investigates how code pretraining improves language models' abilities to track entities in text, while \citet{yoneda2024statler} introduce Stalter, a prompting method designed to maintain accurate state representations in text-based robotics planning.

These works focus on entity tracking as a unimodal, text-based reasoning task. While unimodal approaches have achieved substantial progress, there remains a gap in evaluating models' ability to integrate multimodal inputs for entity tracking. Our work extends these evaluations to the multimodal setting and quantifies the performance improvement of reasoning models for entity tracking.

\section{Conclusion}

Our findings suggest that the primary bottleneck in multimodal entity tracking is not visual recognition but sequential reasoning over visual updates. Unlike text-based representations, which align with the models’ training paradigms, visual updates require implicit state reconstruction—a task that current architectures do not perform reliably. Future work should explore the effect of additional visual-reasoning post-training, explicit memory structures, or hybrid symbolic representations to mitigate this gap. Additional research directions include investigating the role of entity tracking in world-modeling, narrative understanding, and expanding MET-Bench to include more complex domains beyond games. We believe addressing these challenges will be crucial for developing AI systems capable of robust reasoning for real-world tasks.

\section{Limitations}

Our benchmark is restricted to two synthetic domains—Chess and the Shell Game. While these domains are well-structured and offer a clear framework for assessing entity state tracking, they may not fully capture the complexity of real-world multimodal reasoning tasks that require entity tracking.

Our study highlights a substantial performance gap between text-based and image-based entity tracking, yet the exact causes of this disparity remain unclear. While our cascaded inference experiments suggest that models struggle to reason directly over visual updates rather than simply perceiving them, further investigation is needed to pinpoint the underlying source of the problem and whether it's possible to explicitly ameliorate it.

Additionally, our fine-tuning experiments demonstrate that performance can be improved with domain-specific adaptation, but we do not investigate the trade-offs between fine-tuning and generalization across unseen multimodal datasets. Understanding whether these improvements transfer to novel multimodal reasoning tasks is an important avenue for future work.

\section*{Ethical Considerations}
Vision-language models may inherit biases from their training data or other aspects of their development, leading to disparities in performance across different languages, cultures, or other categories that intersect with textual and visual representations. The evaluations in this work can be used to improve the capabilities of frontier models, which if deployed in contexts requiring multimodal entity tracking, could further expose end-users to the biases present in this work including our choice of game domains and language.

\section*{Acknowledgments}
This paper and associated code were created with the assistance of ChatGPT for proofreading and debugging.

% Bibliography entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}
% Custom bibliography entries only
% \bibliography{custom}

\clearpage

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Models Struggle to Integrate Mixed Modalities}
\label{sec:mixed_modality_results}

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figures/mix_ratio.pdf}}
\caption{Chain-of-thought entity tracking accuracy for Chess and Shell Game with GPT-4o. The data splits range from 100\% text-encoded actions to 100\% image-encoded actions. The plot illustrates the change in accuracy as actions shift between modalities.}
\label{fig:mix_ratio}
\end{center}
\vskip -0.2in
\end{figure}

To examine how well models can integrate mixed-modality information, we evaluate performance as we vary the proportion of text and image-based action representations. As shown in Figure~\ref{fig:mix_ratio}, for Chess performance degrades smoothly as the fraction of image actions increases, rather than exhibiting an abrupt collapse. However for Shell Game, the opposite is true and mixes of text and image actions are challenging for the model to reason over.

\subsection{Fine-Tuning Improves Multimodal Entity Tracking}
\label{sec:finetune_results}

\begin{table}[ht]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
\textbf{Fine-tuned} & \textbf{Text} & \textbf{Image} \\
\midrule
\multicolumn{3}{l}{\textbf{Chess}} \\
GPT-4o mini       & 89.2 & - \\
GPT-4o            & 97.0 & 86.4 \\
\midrule
\multicolumn{3}{l}{\textbf{Shell Game (S=3)}} \\
GPT-4o mini & 32.0 & - \\
GPT-4o      & 74.0 & 32.0 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\caption{Fine-tuned model entity tracking accuracy for the Chess and Shell Game domains (Text actions and Image actions).GPT-4o-mini does not support image finetuning. A training set of $100$ action sequences of length ten were used for Chess and $20$ action sequences of length three and five for Shell Game.}
\label{tab:finetune}
\end{table}


Fine-tuning using the OpenAI fine-tuning API substantially improves model performance across both text and image modalities, as shown in Table~\ref{tab:finetune}. In Chess, fine-tuned models outperform even the strongest zero-shot reasoning models, achieving 97.0\% accuracy in the text domain and a significant boost to 86.4\% in the image domain. This suggests that even with a relatively small dataset, fine-tuning allows the model to learn entity tracking representations that generalize better in both modalities. Notably, fine-tuning leads to a larger improvement in the image modality than in the text modality. This reinforces the idea that pretrained models already encode strong textual reasoning capabilities, whereas multimodal reasoning requires additional adaptation.

In contrast, improvements on a simplified version of Shell Game with only three moves are minimal in case of image-encoded actions. The Shell Game task is not present in the training data and it's harder for the model to generalize, even when exposed to a large fraction of the possible games of the given length. This may indicate that the Shell Game domain is simply too challenging for the model to learn in both the image and text settings from a limited number of examples. A more complex training curriculum involving fine-tuning over multiple game lengths may be required.

\begin{table}[ht]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model Name} & \textbf{Image} & \textbf{Reasoning} \\
        \midrule
        Claude 3.5 Sonnet & \checkmark & \\
        \cite{Anthropic2024ClaudeSonnet} & & \\
        
        Gemini-2.0-Flash & \checkmark & \\
        \cite{Hassabis2024Gemini2Flash} & & \\
        
        GPT-4o mini & \checkmark & \\
        \cite{OpenAI2024GPT4oMini} & & \\
        
        GPT-4o & \checkmark & \\
        \cite{OpenAI2024HelloGPT4o} & & \\

        Minimax-VL-01 & \checkmark & \\
        \cite{minimax2025minimax01scalingfoundationmodels}  & & \\
        
        o1-mini & & \checkmark \\
        \cite{OpenAI2024o1mini} & & \\
        
        o1 & \checkmark & \checkmark \\
        \cite{OpenAI2024o1} & & \\
        
        o3-mini & & \checkmark \\
        \cite{OpenAI2025o3mini} & & \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of capabilities of language models evaluated using the MET benchmark. All evaluated models support text input and output. The total API cost of experiments run is \$$2340.00$.}
    \label{tab:ai_models}
\end{table}

\begin{table}[t]
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
\textbf{Configuration} & \textbf{Epochs} & \textbf{LRM} & \textbf{Batch} \\
\midrule
\multicolumn{4}{l}{\textbf{Chess}} \\
\quad \textbf{Text:} \\
\quad\quad GPT-4o-mini  & 3            & 1.8                 & 1                \\
\quad\quad GPT-4o       & 3            & 2                 & 1                \\
\quad \textbf{Image:} \\
\quad\quad GPT-4o       & 3             & 2                 & 1                \\
\midrule
\multicolumn{4}{l}{\textbf{Shell Game}} \\
\quad \textbf{Text:} \\
\quad\quad GPT-4o-mini  & 3            & 1.8                 & 1                 \\
\quad\quad GPT-4o       & 3            & 2                 & 1                 \\
\quad \textbf{Image:} \\
\quad\quad GPT-4o       & 5            & 2                 & 1                \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\caption{Hyperparameters used for fine-tuning across domains and modalities. The training epochs, learning rate multiplier (LRM), and batch size are reported.}
\label{tab:hyperparams}
\vskip -0.1in
\end{table}


\begin{figure}[ht]
\begin{small}
\begin{tabularx}{\columnwidth}{p{0.15\columnwidth}X}
\toprule
\textbf{Role} & \textbf{Messages} \\
\midrule
\textbf{User} &
\texttt{\detokenize{The shell game is a classic game where a ball is hidden under one of three shells.
You are a helpful assistant that tracks the position of the ball.
The ball starts under shell 2. Here are the moves played:
}}
\\[6pt]

 &
\texttt{\detokenize{1 swap 3}}
\\[2pt]

 &
\texttt{\detokenize{2 swap 3}}
\\[2pt]

 &
\texttt{\detokenize{Now what is the final position of the ball? Only output the number 1, 2, or 3.}}
\\[6pt]

\textbf{Assistant} &
\texttt{\detokenize{3}}
\\
\bottomrule
\end{tabularx}
\end{small}
\caption{An example zero-shot user--assistant exchange in the \textbf{Shell Game} domain, illustrating how the system tracks swaps to determine the ball’s final shell.}
\label{fig:shell-prompt}
\end{figure}


\begin{figure}[ht]
\begin{small}
\begin{tabularx}{\columnwidth}{p{0.15\columnwidth} X}
\toprule
\textbf{Role} & \textbf{Messages} \\
\midrule
\textbf{User} &
\texttt{\detokenize{You are a helpful assistant that interprets image-based actions in chess.}}
\newline
\texttt{\detokenize{Here is an image representing a move:}}
\newline
\textbf{[Image Input]}
\newline
\texttt{\detokenize{
In UCI notation, what move does the arrow on the chessboard represent? The move is from the green square to the red square. (e.g., `e2e4'). Only output the move and nothing else.
}} 
\\[6pt]

\textbf{Assistant} &
\texttt{\detokenize{e2e4}}
\\
\bottomrule
\end{tabularx}
\end{small}
\caption{An example user--assistant exchange in the \textbf{Chess} domain, where the assistant identifies the move represented in the image.}
\label{fig:chess-image-prompt}
\end{figure}

\begin{figure}[ht]
\begin{small}
\begin{tabularx}{\columnwidth}{p{0.15\columnwidth} X}
\toprule
\textbf{Role} & \textbf{Messages} \\
\midrule
\textbf{User} &
\texttt{\detokenize{You are a helpful assistant that interprets image-based actions in the shell game.}}
\newline
\texttt{\detokenize{Here is an image representing a swap:}}
\newline
\textbf{[Image Input]}
\newline
\texttt{\detokenize{
In shell game notation, which shells are being swapped in the image? Shells are labeled `1', `2', `3' and the shells being swapped have their numbers highlighted in green. Only output a dash-separated pair like `1 swap 3' and nothing else.
}} 
\\[6pt]

\textbf{Assistant} &
\texttt{\detokenize{1 swap 3}}
\\
\bottomrule
\end{tabularx}
\end{small}
\caption{An example user--assistant exchange in the \textbf{Shell Game} domain, where the assistant identifies the shell swap represented in the image.}
\label{fig:shell-game-image-prompt}
\end{figure}

\subsection{Models}
The models evaluated using MET-Bench are listed in Table \ref{tab:ai_models}.

\paragraph{Minimax-VL-01}
This model is released under the license: \url{https://github.com/MiniMax-AI/MiniMax-01/blob/main/LICENSE}. The model is 465 billion parameters and is trained on a ``diverse [dataset] incorporating diverse sources including academic literature, books, web content, and programming code'' and post-training dataset encompassing many multimodal and NLP tasks of 512 billion tokens  \citep{minimax2025minimax01scalingfoundationmodels}.

\subsubsection{Proprietary Models}
These models have limited information about their training and development. Like Minimax-VL-01, these models are likely trained on diverse, web-scale corpora spanning many domains and tasks. We provide links to the current terms of their use.

\paragraph{Claude 3.5 Sonnet} \url{https://www.anthropic.com/legal/consumer-terms}.

\paragraph{Gemini-2.0-Flash} \url{https://ai.google.dev/gemini-api/terms}

\paragraph{GPT-4o mini, GPT-4o, o1, o1-mini, o3-mini} \url{https://openai.com/policies/}

\subsection{Datasets}

The Chess dataset is adapted from \citet{toshniwal2022chess} which is adapted from the MillionBase dataset, available for download at \url{https://rebel13.nl/rebel13/rebel\%2013.html}. To the best of our knowledge, no license or terms of use are currently listed for either the original MillionBase dataset or dataset of \citet{toshniwal2022chess}. Our usage of this dataset is consistent with the description of its use by \citet{toshniwal2022chess}.

MET-Bench is intended for evaluating and improving the ability of VLMs to perform entity tracking. It is released under the MIT License.

% \begin{figure*}[ht]
% \centering
% \begin{minipage}{0.9\textwidth}
% \begin{small}
% \begin{tabular}{p{0.12\textwidth} p{0.8\textwidth}}
% \toprule
% \textbf{Role} & \textbf{Message} \\
% \midrule
% \textbf{System} & \texttt{\detokenize{The shell game is a classic game where a ball is hidden under one of three shells. You are a helpful assistant that tracks the position of the ball during the swaps and determines the final ball position. We label the shells 1, 2, 3. The ball starts under one of the numbered shells which we call the initial state, and each move is shell swap of shell x and y written 'x swap y'. The initial state is: 3}} \\
% \midrule
% \textbf{User}   & \texttt{\detokenize{1 swap 2}} \\
% \textbf{User}   & \texttt{\detokenize{2 swap 3}} \\
% \textbf{User}   & \texttt{\detokenize{Now what is the final position of the ball? Answer 1, 2, or 3 and then explain why.}} \\
% \midrule
% \textbf{Assistant} & \texttt{\detokenize{1. The ball starts under shell 3. After the first move "1 swap 2", the ball is still under shell 3 because shells 1 and 2 are swapped, and the ball was not under either of those shells. After the second move "2 swap 3", the ball moves from shell 3 to shell 2 because shells 2 and 3 are swapped. Therefore, the final position of the ball is under shell 1.}} \\
% \bottomrule
% \end{tabular}
% \end{small}
% \end{minipage}
% \caption{A GPT-4o Shell Game trajectory. The \emph{ground truth} final shell is \textbf{2}, but the assistant incorrectly concludes \textbf{1}.}
% \label{fig:shell-game-error}
% \end{figure*}

\end{document}
