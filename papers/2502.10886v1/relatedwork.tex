\section{Related Work}
Entity tracking has been extensively studied in textual domains, with a focus on probing and improving language models' abilities to maintain representations of entity states. For instance, \citet{toshniwal2022chess} evaluates chess as an entity tracking domain, employing fine-tuned models \citep{radford2019language} to assess performance. Similarly, \citet{kim-schuster-2023-entity} examine the impact of model size and fine-tuning on entity tracking in textual settings similar to our Shell Game domain. \citet{tandon-etal-2020-dataset} construct a benchmark for understanding entity state changes in procedural texts. \citet{shirai-etal-2022-visual} construct the Visual Recipe Flow corpus and evaluate the ability of multimodal embedding models to properly sequence images depicting recipe states. In contrast, our work requires predicting entity state changes from actions specified in images and involves larger state spaces.

Several studies explore the implicit representations of entity states in language models. \citet{li-etal-2021-implicit} and \citet{long-etal-2016-simpler} use semantic probing to reveal that Transformer-based models \citep{vaswani2017attention} capture entity state representations implicitly during textual reasoning. Building on this, \citet{prakash2023fine} demonstrate that fine-tuning language models for entity tracking tasks enhances pre-existing internal mechanisms rather than learning entirely new representations. \citet{li2023emergent} find that Transformers trained on Othello games form internal representations of the game state.

Efforts to improve textual entity tracking beyond domain-specific fine-tuning include \citet{fagnou-etal-2024-chain}, which establishes theoretical limitations of the Transformer architecture in tracking entities. They propose a novel attention mechanism to enhance entity tracking in Transformers. \citet{gupta-durrett-2019-effective} fine-tunes small Transformer-based models for tracking entity state in instructional texts. \citet{kim2024codepretrainingimprovesentity} investigates how code pretraining improves language models' abilities to track entities in text, while \citet{yoneda2024statler} introduce Stalter, a prompting method designed to maintain accurate state representations in text-based robotics planning.

These works focus on entity tracking as a unimodal, text-based reasoning task. While unimodal approaches have achieved substantial progress, there remains a gap in evaluating models' ability to integrate multimodal inputs for entity tracking. Our work extends these evaluations to the multimodal setting and quantifies the performance improvement of reasoning models for entity tracking.