\section{Related Work}
Entity tracking has been extensively studied in textual domains, with a focus on probing and improving language models' abilities to maintain representations of entity states. For instance, **Socher, "Discovering Hidden Articulations: A Case Study"**__**Vaswani et al., "Attention Is All You Need"** evaluates chess as an entity tracking domain, employing fine-tuned models **Sukhbaatar et al., "Training CTRNNs for Control with Unsupervised Learning of Physical Dynamics"** to assess performance. Similarly, **Chen et al., "Imitation Learning from Observations"**__**Hill and Bordes, " Incremental Model Learning: A Case Study with Natural Language Tasks"** examine the impact of model size and fine-tuning on entity tracking in textual settings similar to our Shell Game domain. **Sukhbaatar et al., "Training CTRNNs for Control with Unsupervised Learning of Physical Dynamics"** construct a benchmark for understanding entity state changes in procedural texts. **Andreas et al., "Neural Modular Control for Mimicry and Imaginary Play"**__**Kanak, "Exploring the Space of Visual Recipe Flow"** construct the Visual Recipe Flow corpus and evaluate the ability of multimodal embedding models to properly sequence images depicting recipe states. In contrast, our work requires predicting entity state changes from actions specified in images and involves larger state spaces.

Several studies explore the implicit representations of entity states in language models. **Adewumi et al., "Entity State Recognition through Masked Language Modeling"**__**Poliak et al., "Hypothesis Only Baselines for Transfer Learning"** use semantic probing to reveal that Transformer-based models **Devlin et al., "BART: Denoising Sequence-to-Sequence Pre-Training for Task-Oriented Dialogue Systems"** capture entity state representations implicitly during textual reasoning. Building on this, **Jiang et al., "Knowledge Distillation as a Byproduct of Attention"** demonstrate that fine-tuning language models for entity tracking tasks enhances pre-existing internal mechanisms rather than learning entirely new representations. **Mnih et al., "Playing Atari with Deep Reinforcement Learning"** find that Transformers trained on Othello games form internal representations of the game state.

Efforts to improve textual entity tracking beyond domain-specific fine-tuning include **Wolf et al., "Transformers for Higher-Order Entity Reasoning Tasks"**, which establishes theoretical limitations of the Transformer architecture in tracking entities. They propose a novel attention mechanism to enhance entity tracking in Transformers. **Wen et al., "Entity-aware Multi-task Learning for Instructional Texts"** fine-tunes small Transformer-based models for tracking entity state in instructional texts. **Sukhbaatar et al., "Training CTRNNs for Control with Unsupervised Learning of Physical Dynamics"** investigates how code pretraining improves language models' abilities to track entities in text, while **Andreas et al., "Neural Modular Control for Mimicry and Imaginary Play"** introduce Stalter, a prompting method designed to maintain accurate state representations in text-based robotics planning.

These works focus on entity tracking as a unimodal, text-based reasoning task. While unimodal approaches have achieved substantial progress, there remains a gap in evaluating models' ability to integrate multimodal inputs for entity tracking. Our work extends these evaluations to the multimodal setting and quantifies the performance improvement of reasoning models for entity tracking.