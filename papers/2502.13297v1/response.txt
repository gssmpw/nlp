\section{Related Work}
In this section, we will introduce existing methods for detecting label errors. Then we will introduce pre-trained and large language models, and explain their potential in detecting label errors in the individual-level NLU task.

\subsection{Label Errors}
The inconsistency between the labels and groundtruths in the training dataset is often called "noisy labels" **Vanilla, "Confident Learning"**. If the labels are inconsistent with the groundtruths in the test dataset, it is called label errors. Label errors are common in test datasets and may affect the evaluation of the model, there is an average of 3.3\% label error in ten commonly used datasets **Niaz, "The Devil is in the Details: A Study on Noisy Labels in Deep Learning"**. 

A classic method for automatically detecting label errors is confident learning **Zhang et al., "Confident Learning: Estimating Uncertainty in Dataset Labeling with Limited or No Data"**. After this, many methods have been proposed for detecting label errors. For example, some studies compare samples with their K-nearest neighbor samples **Kilian et al., "Learning to Correct Noisy Labels with Confidence and Clustering"**. If the K nearest-neighbor samples belong to a certain class and the sample to be corrected belongs to another class, the sample likely has a label error. Some studies have found that using pre-trained language models and fine-tuning them on a specific task, and then simply examining out-of-sample data points in descending order of fine-tuned task loss outperforms confident learning **Wang et al., "Automated Label Error Detection with Pre-Trained Language Models"**.

Large number of label errors are probably not due to the negligence of the annotators but the defects in the annotation guidelines themselves. For example, the 23.7\% label error rate in the TADRED dataset is because of inappropriate guidelines **Tsuruoka et al., "TADRED: A Framework for Efficient and Effective Label Error Detection"**. Annotation guidelines serve as the instruction manual for annotators, drafted by product owners. The process can be simply summarized as follows: (1) Annotators are recruited and given data samples and the description of guidelines; (2) Annotators provide the labels based on their knowledge and experience, by strictly complying with the guidelines **Liu et al., "Guideline-Based Labeling for High-Quality Annotation"**.

\subsection{Pre-trained Language Models}
Before the emergence of large language models, studies have shown that pre-trained language models are better than support vector machines or other deep learning models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. Many works demonstrate that using external knowledge can effectively enhance the performance of individual-level NLU tasks such as stance detection tasks **Gupta et al., "Improving Stance Detection with External Knowledge and Pre-trained Language Models"**. Since large language models were pre-trained with a large corpus, many researchers began to explore their performance in individual-level tasks such as stance detection **Huang et al., "Stance Detection with Large Language Models: A Comparative Study"**, sentiment analysis **Somashekhar et al., "Sentiment Analysis with Pre-Trained Language Models and Transfer Learning"**.  However, these works focus on how to guide LLMs to achieve better performance, and no work has focused on the role of LLMs in detecting label errors in individual-level NLU tasks. If the dataset is systematically and consistently mislabeled, the evaluation of LLMs can become both misleading and unreliable.