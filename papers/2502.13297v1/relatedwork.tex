\section{Related Work}
In this section, we will introduce existing methods for detecting label errors. Then we will introduce pre-trained and large language models, and explain their potential in detecting label errors in the individual-level NLU task.

\subsection{Label Errors}
The inconsistency between the labels and groundtruths in the training dataset is often called "noisy labels" \cite{song2022learning}. If the labels are inconsistent with the groundtruths in the test dataset, it is called label errors. Label errors are common in test datasets and may affect the evaluation of the model, there is an average of 3.3\% label error in ten commonly used datasets \cite{northcutt2021pervasive}. 

A classic method for automatically detecting label errors is confident learning \cite{northcutt2021confident}. After this, many methods have been proposed for detecting label errors. For example, some studies compare samples with their K-nearest neighbor samples \cite{zhu2022detecting,zhu2023unmasking}. If the K nearest-neighbor samples belong to a certain class and the sample to be corrected belongs to another class, the sample likely has a label error. Some studies have found that using pre-trained language models and fine-tuning them on a specific task, and then simply examining out-of-sample data points in descending order of fine-tuned task loss outperforms confident learning \cite{chong2022detecting}.

Large number of label errors are probably not due to the negligence of the annotators but the defects in the annotation guidelines themselves. For example, the 23.7\% label error rate in the TADRED dataset is because of inappropriate guidelines \cite{stoica2021re}. Annotation guidelines serve as the instruction manual for annotators, drafted by product owners. The process can be simply summarized as follows: (1) Annotators are recruited and given data samples and the description of guidelines; (2) Annotators provide the labels based on their knowledge and experience, by strictly complying with the guidelines \cite{klie2024analyzing}.

\subsection{Pre-trained Language Models}
Before the emergence of large language models, studies have shown that pre-trained language models are better than support vector machines or other deep learning models \cite{ghosh2019stance}. Many works demonstrate that using external knowledge can effectively enhance the performance of individual-level NLU tasks such as stance detection tasks \cite{he2022infusing, hanawa2019stance, li2021improving}. Since large language models were pre-trained with a large corpus, many researchers began to explore their performance in individual-level tasks such as stance detection \cite{zhang2022would, cruickshank2023use, lan2024stance, li2024advancing, gatto2023chain}, sentiment analysis \cite{zhang2023sentiment, korkmaz2023analyzing}.  However, these works focus on how to guide LLMs to achieve better performance, and no work has focused on the role of LLMs in detecting label errors in individual-level NLU tasks. If the dataset is systematically and consistently mislabeled, the evaluation of LLMs can become both misleading and unreliable.