% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{float}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.
\usepackage{textcomp,booktabs}
\usepackage{colortbl}
\definecolor{mygray}{gray}{.9}
\usepackage{makecell}
\usepackage{subfigure}

\newcommand{\youpeng}[1]{{\textbf{\color{orange}#1}}}

\title{Understanding and Tackling Label Errors in Individual-Level Nature Language Understanding}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Yunpeng Xiao \\
  Emory University
  \\Atlanta, US \\
  \texttt{yxia326@emory.edu} \\\And
  Youpeng Zhao \\
  University of Central Florida\\
  Orlando, US \\
  \texttt{youpeng.zhao@ucf.edu} \\\And
  Kai Shu\\
  Emory University\\
  Atlanta, US \\
  \texttt{kai.shu@emory.edu}
  }

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Natural language understanding (NLU) is a task that enables machines to understand human language. 
Some tasks, such as stance detection and sentiment analysis, are closely related to individual subjective perspectives, thus termed individual-level NLU. 
Previously, these tasks are often simplified to text-level NLU tasks, ignoring individual factors. 
This not only makes inference difficult and unexplainable but often results in a large number of label errors when creating datasets. 
To address the above limitations, we propose a new NLU annotation guideline based on individual-level factors. 
Specifically, we incorporate other posts by the same individual and then annotate individual subjective perspectives after considering all individual posts. We use this guideline to expand and re-annotate the stance detection and topic-based sentiment analysis datasets. We find that error rates in the samples were as high as 31.7\% and 23.3\%. 
We further use large language models to conduct experiments on the re-annotation datasets and find that the large language models perform well on both datasets after adding individual factors. Both GPT-4o and Llama3-70B can achieve an accuracy greater than 87\% on the re-annotation datasets. We also verify the effectiveness of individual factors through ablation studies. We call on future researchers to add individual factors when creating such datasets. Our re-annotation dataset can be found at \href{https://github.com/24yearsoldstudent/Individual-NLU}{https://github.com/24yearsoldstudent/Individual-NLU}.

\end{abstract}

\section{Introduction}

\begin{figure}[t]
	\centering
	\subfigure[The original dataset contains text, target and label. LLM judge on the text is different from the label.]{
		\begin{minipage}{7.5cm}
                        \includegraphics[width=\textwidth]{latex/figure1_a.pdf} \\
			%\includegraphics[width=\textwidth]{fig/beforePAA.eps} 
		\end{minipage}
	}

	\subfigure[Expanded dataset, add other posts of the same individual (user) for the same target, and re-annotate the user's stance. The LLM Judge is consistent with the re-annotation label.]{
		\begin{minipage}{7.5cm}%[b]%{0.2\textwidth}
			\includegraphics[width=\textwidth]{latex/figure1_b.pdf} \\
			
		\end{minipage}
	}
    \caption{A typical example of potential label error in stance detection.}
    \label{label_errors}
    \vspace{-3mm}
\end{figure}


Natural language understanding (NLU) refers to the task of determining whether a natural language hypothesis can be reasonably inferred from a given natural language premise \cite{maccartney2009natural}. Common natural language understanding tasks include fake news detection \cite{shu2017fake}, sentiment analysis \cite{wankhade2022survey}, stance detection \cite{aldayel2021stance}, toxicity detection \cite{pavlopoulos2020toxicity}, and sarcasm detection \cite{joshi2017automatic}. Existing NLU datasets are predominantly text-based, relying solely on short text information without accounting for social factors. While text-level NLU simplifies many tasks, its limitations begin to be recognized, such as poor inference performance \cite{hovy2021importance,bhattacharya2025rethinking}.
So, some researchers have propose frameworks integrating social factors into NLU \cite{hovy2021importance}. Additionally, various studies have incorporated different social factors such as user information and background knowledge into specific tasks to improve NLU accuracy \cite{yang2017overcoming, aldayel2019your}.

However, current research has not explored in depth which NLU tasks will have huge deficiencies when using only textual information (without any other factors). In this paper, we define a type of NLU tasks as individual-level NLU tasks, where the labels reflect the identity or perspective of the individual (typically the web user who posts the text) rather than the content of the text itself. 
We argue that inference only with short texts is flawed in such tasks. Tasks that fall under individual-level NLU include sentiment analysis, sarcasm detection, and stance detection, etc. 
A key characteristic of these tasks is that their labels are inherently tied to the publishers rather than the readers. 
An NLU task that does not incorporate an individual's perspective is not considered individual-level NLU. Such tasks are usually annotated based on social consensus or objective facts.
For example, in tasks such as nature language inference, the labels usually represent a broadly accepted interpretation rather than an individual user's perspective \cite{bowman2015large}. 
In fake news detection \cite{shu2017fake} and authorship detection \cite{huang2024can}, the label remains unchanged regardless of whether one or multiple individuals share or endorse it. 

This deficiency is reflected in the creation of the datasets. Current research often implicitly assumes that the labels in original datasets are accurate. 
However, individual-level NLU datasets are often created using text-level guidelines, and annotators' interpretations may differ from those of the original publishers. 
Such misalignment can lead to a significant number of labeling errors. 
Prior works have attempted to mitigate this issue by leveraging the individual factors in the datasets. 
For instance, datasets like Amazon reviews \cite{zhang2015character} and IMDB \cite{maas2011learning} assign labels directly based on user scores, reducing the likelihood of annotation inconsistencies. However, many individual-level NLU datasets, such as the Twitter stance detection dataset \cite{mohammad2016semeval} and the Twitter sentiment analysis dataset \cite{rosenthal2019semeval}, depend mostly on manual annotation, since social media posts do not come with explicit "scores" and must be annotated manually or inferred through hashtags instead. A recent study demonstrates that large language models (LLMs) perform well when human annotators do but fail in cases where human annotators struggle to reach consensus \cite{li2024advancing}. This suggests that inconsistencies among annotators stem from the inherent ambiguity of the text rather than annotator negligence. 

From a sociolinguistic perspective, the attitude of an individual should be tied to the original publisher’s intent at the time of posting \cite{kockelman2004stance}, rather than being subject to the variability of annotator interpretations. Annotation inconsistencies often arise due to insufficient information and poor data quality. This phenomenon is referred to as systematic label errors \cite{cabrera2014systematic}. A recent study \cite{garg2024stanceformer} identifies potential label errors in the SemEval-2016 stance detection dataset, with error rates reaching up to 22.7\% for the Atheism category (see Figure \ref{label_errors} for sample case). To address the limitations of text-level NLU, some sarcasm detection datasets have attempted to make annotators the original publishers—meaning they generate and annotate their own posts \cite{farha2022semeval, oprea2019isarcasm}. However, the volume of such intentionally created data remains limited, making it difficult to scale for large individual-level NLU tasks. 

To address this research gap, we propose guidelines for two NLU subtasks: stance detection and topic-based sentiment analysis. These guidelines aim to identify and mitigate systematic labeling errors that may exist in text-level NLU datasets. Specifically, building on the prior finding that a user's stance on a specific perspective tends to remain consistent over time \cite{aldayel2019your}, we incorporate additional posts from the same user within a similar timeframe to assess the accuracy of dataset labels. Our analysis reveals a substantial number of labeling errors. To further evaluate these errors, we employ three mainstream large language models (LLMs) to evaluate the datasets. Our findings indicate that LLMs achieve exceptionally high accuracy on the re-annotated datasets using only simple prompts, demonstrating the necessity of introducing individual-level NLU and individual factors. 

We summarize our contributions as follows:
\begin{itemize}
    \item We propose a novel guideline to reduce labeling errors in individual-level NLU. 
    \item We identify that individual-level NLU datasets often rely on text-level annotation methods, leading to a high error rate, which even exceeds 30\% on the most commonly used stance detection dataset.
    \item We evaluate the newly re-annotated datasets using LLMs. Our results demonstrate that LLMs hold significant potential for individual-level NLU tasks, even surpassing crowdsourced annotators in domains requiring specialized knowledge.
\end{itemize}


\section{Related Work}
In this section, we will introduce existing methods for detecting label errors. Then we will introduce pre-trained and large language models, and explain their potential in detecting label errors in the individual-level NLU task.

\subsection{Label Errors}
The inconsistency between the labels and groundtruths in the training dataset is often called "noisy labels" \cite{song2022learning}. If the labels are inconsistent with the groundtruths in the test dataset, it is called label errors. Label errors are common in test datasets and may affect the evaluation of the model, there is an average of 3.3\% label error in ten commonly used datasets \cite{northcutt2021pervasive}. 

A classic method for automatically detecting label errors is confident learning \cite{northcutt2021confident}. After this, many methods have been proposed for detecting label errors. For example, some studies compare samples with their K-nearest neighbor samples \cite{zhu2022detecting,zhu2023unmasking}. If the K nearest-neighbor samples belong to a certain class and the sample to be corrected belongs to another class, the sample likely has a label error. Some studies have found that using pre-trained language models and fine-tuning them on a specific task, and then simply examining out-of-sample data points in descending order of fine-tuned task loss outperforms confident learning \cite{chong2022detecting}.

Large number of label errors are probably not due to the negligence of the annotators but the defects in the annotation guidelines themselves. For example, the 23.7\% label error rate in the TADRED dataset is because of inappropriate guidelines \cite{stoica2021re}. Annotation guidelines serve as the instruction manual for annotators, drafted by product owners. The process can be simply summarized as follows: (1) Annotators are recruited and given data samples and the description of guidelines; (2) Annotators provide the labels based on their knowledge and experience, by strictly complying with the guidelines \cite{klie2024analyzing}.

\subsection{Pre-trained Language Models}
Before the emergence of large language models, studies have shown that pre-trained language models are better than support vector machines or other deep learning models \cite{ghosh2019stance}. Many works demonstrate that using external knowledge can effectively enhance the performance of individual-level NLU tasks such as stance detection tasks \cite{he2022infusing, hanawa2019stance, li2021improving}. Since large language models were pre-trained with a large corpus, many researchers began to explore their performance in individual-level tasks such as stance detection \cite{zhang2022would, cruickshank2023use, lan2024stance, li2024advancing, gatto2023chain}, sentiment analysis \cite{zhang2023sentiment, korkmaz2023analyzing}.  However, these works focus on how to guide LLMs to achieve better performance, and no work has focused on the role of LLMs in detecting label errors in individual-level NLU tasks. If the dataset is systematically and consistently mislabeled, the evaluation of LLMs can become both misleading and unreliable.

\section{Methodology of Re-annotation}
In this section, we illustrate the process of mitigating label errors in individual-level NLU tasks. We begin by highlighting the unique characteristics of individual-level tasks. Next, we present our methods, using representative tasks such as stance detection and topic-based sentiment analysis.

\begin{figure*}
    \centering 
    \includegraphics[width=1\textwidth]{latex/Figure_2.pdf} 
    \caption{The process of manual re-annotation and LLMs judges. In the manual re-annotation, after finding other posts related to the topic/target by individuals (network users), three annotators follow the guidelines to annotate individual-level labels. In LLMs Judges, the input is divided into two parts: data and prompts.} 
    ~\label{multi_tweets} 
    \vspace{-5mm}
\end{figure*}

\vspace{-2mm}

\subsection{Tasks and Dataset Selection}
According to the definition of individual-level NLU, annotators cannot directly infer a publisher's perspectives but can only approximate them using indirect contextual information about the user. Relying solely on a single piece of text often results in inaccurate annotations. This highlights the critical need for a more comprehensive understanding of an individual’s background in NLU tasks, including physiological attributes (e.g., gender, age) and social factors (e.g., interests, occupation, and community affiliations). However, collecting such sensitive information from social media presents significant challenges, particularly regarding privacy concerns. Therefore, it is essential to simplify the problem by focusing on specific individual-level NLU tasks while minimizing privacy risks.

Therefore, we focus on two representative tasks: topic-based sentiment analysis and stance detection. One advantage of these tasks is that they have clearly defined topics or targets, making it easier to collect relevant posts from the users. Other tasks such as sarcasm detection lack a specific target and often require a deep understanding of an individual’s speaking style, interests, and other contextual factors, making data collection significantly more challenging. Additionally, previous studies have shown that users' attitudes toward specific perspectives tend to remain stable over short periods \cite{borge2015content,aldayel2019your}. For example, in topic-based sentiment analysis, if the topic is Arsenal, a dedicated Arsenal fan is expected to maintain a positive sentiment toward the team over time. For our study, we select two datasets: the SemEval-2016 Task 4 topic-based sentiment analysis dataset \cite{nakov2019semeval} and the SemEval-2016 Task 6 stance detection dataset \cite{mohammad2016semeval}. The basic statistics of both datasets are presented in Table \ref{demographic}. Stance detection has three classes: $Favor$, $Against$, and $None$. In topic-based sentiment analysis, the creators discard the $Neutral$ and only keep the $Positive$ and $Negative$ classes.

\begin{table}[!t]
\centering
\begin{tabular}{llll}
\hline
Dataset & Topics/Targets & Tweets      \\ \hline
Semeval Stance & 5 & 1,129  \\
\hline
Semeval Sentiment & 60 & 4,346    \\
\hline
\end{tabular}
\caption{Statistics of two datasets.} ~\label{demographic}
\vspace{-5mm}
\end{table}

\subsection{Data Expansion}
To expand the dataset, we collect user posts related to the specified topic or target. We make the following assumption: given a set of posts $X = {x_1, x_2, ..., x_n}$ authored by a user about a topic or target $t$ over a certain period, these posts should exhibit the same sentiment or stance. We further validate this assumption from a clustering perspective. Previous research has clustered posts based on textual features at the text level \cite{samih2021few}, where posts with similar textual characteristics are positioned closer together and are more likely to share the same class label. At the individual level, drawing from prior studies \cite{borge2015content, aldayel2019your}, we extend this idea by assuming that users and their posts should be each other's nearest neighbors. In other words, if a dataset contains only one post $x_1$ from a given user, and we add $k$ additional posts ${x_1,...x_k}$, forming a cluster of nearest neighbors that share the same label. Previous studies have shown that detecting label errors requires as few as two nearest neighbor samples (2-NN) \cite{zhu2022detecting}.  so we set $k=2$ for the dataset creation (we also conduct experiments to demonstrate the impact of $k$ in section 6.2). However, certain edge cases must be considered—such as when a user has only one post related to $t$, or when the original post itself isn't directly related to $t$. We provide specific guidelines for handling the cases in Section 3.3.

We start from the existing dataset, find the users corresponding to these posts, and then use the Twitter API to crawl other tweets from the same user within a certain period of time based on the corresponding keywords. This period is usually no more than two years. For example, for the stance dataset, we crawl the user’s tweets from January 2015 to December 2016. The keywords (also called search queries) corresponding to different targets are given in the appendix. If more than three tweets are collected from a user, we filter the tweets: We first keep the tweets that explicitly contained the target (e.g., the target was Legalization of Abortion and the tweet explicitly contained abortion). If there are not enough tweets (less than three tweets), we manually collect the user's tweets in the following order: (1) tweets posted by the user related to the target or topic (regardless of time, the closer to the original tweet, the better); (2) tweets retweeted by the user related to the target or topic (regardless of time, the closer to the original tweet, the better); (3) tweets posted by the user closest to the original tweet. 

Because some tweets have been deleted or restricted, we can only obtain the users corresponding to a part of the tweets. This is also the case in previous studies \cite{aldayel2019your}. In the stance detection dataset, we selected four targets: Atheism (AT), Climate Change is a Real Concern (CC), Feminist Movement (FM), and Legalization of Abortion (LA) for data expansion. In topic-based sentiment analysis, we select two tweets for each topic, giving priority to one with the label $Positive$ and one with the label $Negative$. However, if all tweets of a certain class of a certain topic are inaccessible, we select two tweets of the same class. The data statistics are shown in Table \ref{Error Rates}.

\subsection{Manual Re-annotation Guidelines}
After collecting user tweets, three annotators independently annotated them. Considering that we use LLMs for data evaluation and that the annotators may not understand some background knowledge, we allow the annotators to use search engines to assist in the annotation work but prohibit the use of LLMs. 

In the annotation, we first followed the guidelines for constructing the SemEval-2016 datasets. According to the characteristics of expanded data, we propose a new guideline: for individuals whose sentiment/stance is difficult to determine, we use the following rules to annotate: (1) If none of the three posts can determine the individual's stance/sentiment on the target, it is annotated as $None$ ($Neutral$). (2) If one tweet clearly states that the stance is $Favor$ ($Positive$) or $Against$ ($Negetive$), and the remaining two tweets have unclear stances or are irrelevant to the target, the stance is still annotated as $Favor$ ($Positive$) or $Against$ ($Negetive$). (3) If more than half of the tweets are $Favor$ ($Positive$)/$Against$ ($Negetive$), please identify the user's stance/sentiment as $Favor$ ($Positive$)/$Against$ ($Negetive$). Our goal is to re-annotate the labels of the original dataset, we cannot simply discard the samples when the three annotators are inconsistent. Therefore, if an inconsistency is found, the annotators will re-search the Twitter user's information and discuss it until they reach a consensus. Since the topic-based sentiment analysis dataset discards neutral samples and only has positive and negative samples, the samples we finally annotate also only have positive and negative samples.
\begin{figure*}[!t]
    \centering 
    \includegraphics[width=1\textwidth]{latex/error_samples.pdf} 
    \caption{Some examples of correcting label errors. Using multiple posts from the same publisher can more accurately determine the user's sentiment or stance, and can effectively explain why this label is given.} 
    ~\label{error_simples} 
\end{figure*}
\section{Experiments}
In this section, we introduce the large language models used to evaluate Individual-level NLU performance and then our evaluation metrics. 

\subsection{LLM Judges}

We use three representative large language models: GPT-4o \cite{achiam2023gpt}, Llama3-70B \cite{dubey2024llama} and PHI-4 \cite{abdin2024phi} to evaluate the performance of the datasets after expansion and correction. 

For each user, we repeated the experiment three times to demonstrate more robust results due to the non-deterministic nature of LLMs \cite{xiong2023can}. Finally, we selected the category with the highest probability as the predicted label. 

To demonstrate that multiple posts are more effective than one post, we also conduct two ablation experiments. The first ablation experiment compares the performance when using the original tweet and two newly collected tweets and the performance when using the original tweet. The second ablation experiment verifies the performance of LLM when using different numbers of tweets. In the second ablation experiment, not all users can collect more than three tweets, so we only use users with more than or equal to five tweets collected in the stance detection dataset, and then randomly select one to five tweets from these users to input into LLM to evaluate the performance. In the one-tweet experiment, the input tweet can be different from the tweets in the original dataset.

\subsection{Evaluation Metrics}
Similar to previous work, we calculate the label error rate $R_e$ according to (1). 
\begin{equation}
R_e = S_e/S_t
\end{equation}
$S_e$ is the number of error samples, and $S_t$ is the total number of samples. Previous work \cite{mohammad2016semeval, nakov2019semeval} uses the average F1 value of positive and negative samples to evaluate model performance. However, we find that in some targets, the number of positive or negative samples that can still be accessed is very small, and directly using the average F1 value will cause a large bias. Thus we use the $Accuracy$ for evaluation. However, in the appendix, we also give the average F1 value of each model.
\begin{equation}
Accuracy = S_c/S_t
\end{equation}

$S_e$ is the number of samples predicted correctly. Since a user may have multiple tweets in the dataset, each one may be annotated with a different label, we calculate $R_e$ and $Accuracy$ based on the number of tweets rather than the number of users. 

\section{Assessing Label Errors}

According to our guidelines, we evaluated the labeling errors of the two datasets. In the stance detection dataset, there were 156 tweets with incorrect labels. The error rate was as high as 31.7\%. Among them, the error rates of Atheism, Feminism, and Legalization of Abortion were as high as 29.3\%, 43.9\% and 41.5\% respectively. In the topic-based sentiment dataset, the error rate is 23.3\%.  Table \ref{Error Rates} shows the tweets and error rates in different targets or topics in the two datasets.

\begin{table}[!t]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cccc}
        \hline
        Dataset & Target & Posts & Error & Error Rate\\
        \hline
        \multirow{5}{*}{Stance} & AT & 133 & 30 & 29.3\% \\
        ~ & CC & 114 & 22 & 19.3\% \\
        ~ & FM & 114 & 50 & 43.9\% \\
        ~ & LA & 130 & 54 & 41.5\% \\
        ~ & ALL & 491 & 156 & 31.7\% \\
        \hline
        Sentiment & All Topic & 120 & 28 & 23.3\% \\
        \hline
    \end{tabular}
    }
\caption{Error rate statistics of different datasets} ~\label{Error Rates}
\vspace{-5mm}
\end{table}


We then perform a qualitative analysis of the errors in these labels. In the stance detection dataset, the target of Legalization of Abortion, a hashtag $\#repealthe8th$ repeatedly appears, which often means that the user is Irish and opposes the Eighth Amendment to the Irish Constitution. The Eighth Amendment to the Irish Constitution is a law against the Legalization of Abortion. Opposing the law means that the user's stance on the Legalization of Abortion is $Favor$. However, in the original dataset, a large number of tweets are annotated as $Against$. This is most likely because the annotators are not Irish and do not understand Irish culture and politics. This further illustrates the complexity of annotations. More examples are given in Figure \ref{error_simples}. We also give more examples in the appendix figure.

The sentiment analysis dataset only provides the Tweet ID but not the original text. The stance detection dataset provides both the Tweet ID and the original text. We also conducted case studies on the tweets in SemEval-2016 that are no longer available on the Internet and found that many tweets have vague content and opinions without specific context. It is difficult to infer the publisher's stance based on just one tweet. This shows that even if the dataset is expanded, similar situations will occur.

\section{Evaluation on Expanded Datasets}

\subsection{Quantitative Analysis}
We first evaluate the performance of the three models on the new datasets. As shown in Table \ref{Correction}, GPT-4o has an accuracy of more than 90\% for each target on the stance detection dataset, and Llama3-70B has an accuracy of more than 80\% on each dataset. PHI-4 performs slightly worse, with an accuracy of only 68\% on some targets. In terms of overall accuracy, GPT-4o and LLama3-70B reached 92\% and 88\% respectively, and PHI-4 was slightly worse, but also 79\%. However, if we use the original labels (uncorrected dataset labels, OL) for evaluation, the accuracy of the three models will drop to 65\%, 64\%, and 62\% respectively. This shows that label errors in the original dataset will seriously affect the evaluation of model performance, and also shows that most LLMs can already make accurate predictions for the two tasks.

\begin{table*}[!t]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccccc|cc}
        \hline
        ~ & \multicolumn{6}{c|}{Stance Detection} & \multicolumn{2}{c}{Sentiment Analysis}\\
        \hline
        Model & AT & CC & FM & LA & Total & Total (OL) & Total & Total (OL) \\
        \hline
        GPT-4o & 92.48 & 92.98 & 92.98 & 91.53 & 92.46 & 64.77 & 89.17 & 78.33 \\
        LLama3-70B & 86.47 & 91.52 & 83.33 & 89.23 & 87.58 & 64.56 & 92.50 & 78.33 \\
        PHI-4 & 68.42 & 83.33 & 84.21 & 81.54 & 79.02 & 61.51 & 92.50 & 76.67 \\
        \hline
    \end{tabular}
    }
\caption{The performance of different models on the dataset after label correction. OL means original labels, which is the label of the original datasets without correction.} ~\label{Correction}
\end{table*}

\subsection{Ablation Studies}

We conduct two ablation experiments to evaluate the validity of multiple tweets from the same user. Table \ref{multi} shows the results of the first ablation experiment. When using only the original tweet, the accuracy of all LLMs drops. This proves the necessity of using individual factors. Different posts from the same individual can complement each other and enhance the accuracy of prediction.

\begin{table}[!b]
    \centering
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{c|cc|cc}
        \hline
        ~ & \multicolumn{2}{c|}{Stance Detection} & \multicolumn{2}{c}{Sentiment Analysis}\\
        \hline
        Model & MT & ST & MT & ST \\
        \hline
        GPT-4o & 92.48 & 69.25 & 89.17 & 74.17 \\
        \hline
        LLama3 & 87.58 & 74.13 & 92.50 & 72.50 \\
        \hline
        PHI-4 & 79.02 & 60.29 & 92.50 & 71.67 \\
        \hline
    \end{tabular}
    }
    \vspace{-2mm}
\caption{Comparison of results using multiple tweets and a single tweet. MT: Multiple Tweets. ST: Single Tweet} ~\label{multi}
\end{table}

Then we input LLM with one to five tweets from the same user. We collected 281 users with more than five tweets, so we evaluated the effectiveness of multiple tweets on these 281 users. Since PHI-4 performed poorly before, we used LLama3-70B and GPT-4o for experiments. Figure \ref{multi_tweets} shows that three tweets can achieve good accuracy. Although the performance can continue to improve by increasing the number of tweets, the improvement is significantly reduced. Therefore, using three tweets is a choice that takes both performance and efficiency into consideration.


% \vspace{-8mm}

\begin{figure}[!t]
    \centering 
    \includegraphics[width=0.5\textwidth]{latex/figure_multi.pdf} 
    % \vspace{-mm}
    \caption{Performance on the Semeval stance detection dataset using different numbers of tweets as LLM input.} 
    ~\label{multi_tweets}
    % \vspace{-10mm}
\end{figure}

\begin{figure*}[!htb]
    \centering 
    \includegraphics[width=1\textwidth]{latex/multi_tweets.pdf} 
    \caption{Multi-posts example. In this case, although the user's stance is "Against" in both the new and original datasets, it is difficult or even impossible to infer the user's stance from the text in the original dataset. After adding other tweets from the user, LLM gives an accurate prediction.} ~\label{Case}
\end{figure*}

\subsection{Case Study}
We also conduct case studies of the results given by LLMs. We focus on two types of samples: The first type is samples where the new label is different from the original label. The second type is samples where the new label is the same as the original label, but the prediction results are different when using multiple tweets and a single tweet. We find that LLMs' explanations were basically consistent with the annotators' cognition.  Figure \ref{Case} shows a typical example. The sample is annotated "Against" in both the new and original datasets, but even humans find it difficult to judge the user's stance on Legalization of Abortion through the original tweets. All three annotators also believe that the original tweet did not mention abortion at all, nor did it contain any clues supporting or opposing abortion. When only one tweet is used for stance detection, LLMs give a prediction result of "None", which is consistent with the annotator's cognition. After using the newly added two tweets, a total of three tweets for prediction, LLMs give the result of "Against". The three annotators also give the label "Against" based on the newly added tweets. This proves that expanding the dataset and increasing the information of the same user in the dataset is crucial for individual-level NLU. More samples are given in the appendix.

\section{Discussion and Conclusion}
Our research demonstrates the limitations of reducing individual-level NLU tasks to text-level tasks. The information lost in the reduction process not only leads to poor model performance but also causes annotators to misunderstand semantic information, resulting in a large number of label errors. Therefore, we call on dataset creators to fully consider social factors and reasonably choose guidelines to reduce systematic label errors when creating individual-level datasets in the future.

Past studies have shown that online users' perspectives of a topic or target do not change over time. We draw inspiration from these conclusions and propose an individual-level annotation guideline for stance detection and topic-based sentiment analysis. We collect posts related to topics/targets from online users over a period of time, use the consistency of the posts for cross-validation, and finally judge the stance or sentiment of the online user. Case studies show that our method avoids the ambiguous semantics of a single post, allowing for more accurate annotation, and the labels we give are more explainable. At the same time, our method only collects posts to avoid collecting a large amount of invalid user information.

We used the re-annotated dataset to conduct zero-shot experiments on different LLMs. Comparing the labels with the original datasets, we found that incorrect labels seriously affect the evaluation model's performance on the individual-level NLU task; the current LLM performs exceptionally well in stance detection and topic-based semantic analysis. Through ablation experiments and case studies, we demonstrated the effectiveness of multiple posts compared to a single post and also showed that LLMs have human thinking patterns when facing single and multiple tweets.

\section{Limitations}
Our study also has some limitations. First, in individual-level NLU, the user's perspectives can be determined not only through the tweets posted by the user but also by using other information of the user. For example, the user's retweets, likes, follows, and profile. Although these data are rich, they are highly heterogeneous compared to the tweets posted by the user. For example, some user profiles may contain information to determine the user's stance, while some users may not even have profiles. This may be because different users have different habits when using social media. Effectively utilizing and modeling this information is one of our future directions.

Secondly, our current information retrieval methods are only applicable to tasks that involve determining topics, such as stance detection and topic-based sentiment analysis. The characteristic of this type of task is that we can use keywords to retrieve user posts. Some other individual-level NLU tasks, such as sarcasm detection, do not have similar characteristics and cannot find corresponding user tweets by keywords. This means that when facing this type of NLU task, we need new information retrieval methods and models. This is also the direction we need to explore.

Finally, our annotations are relatively small. Individual-level annotations require full consideration of each post, which greatly increases the annotation cost. To verify the robustness of our method, we will increase the number of annotation samples and build a larger dataset in the future.

\section{Ethics Statement}
Our work on the datasets is conducted with a strong commitment to ethical principles. We prioritize privacy by collecting only publicly available tweets and strictly adhering to relevant guidelines for annotation and dataset sharing. In our research, we comply with the X Developer Agreement and Policy, ensuring that all content is used solely for academic research purposes. Tweets can only identify online users, not real individuals. Furthermore, we respect diverse religious beliefs and political perspectives.

Additionally, our research does not diminish the contributions of previous dataset creators; rather, we deeply appreciate their efforts. The datasets they developed serve as the foundation of our work.

\bibliography{custom}

\appendix

\section{Keywords in searching}
In topic-based sentiment analysis, we directly use topics as the keyword for search. In stance detection, the keywords are:
\begin{itemize}
    \item Atheism: $Atheism$, $God$, $Pray$
    \item Climate Change is a Real Concern: $Climate$, $Globalwarming$
    \item Feminist Movement: $Women$, $Feminism$, $Feminist$
    \item Legalization of Abortion: $Abortion$, $Women$, $Legal$
\end{itemize}

\section{Prompts Design}
We use a very simple prompt:

\begin{itemize}
    \item For stance detection:

    Read the question, provide your answer, and your confidence in this answer. Please make sure that the confidence level of your answers adds up to 1. Only output confidence levels. Do not output any other things. Please decide the following users' stance on the $Target$: Is it FAVOR, AGAINST, or NONE? These tweets are from the same user, so please assume they have the same stance. $Tweet1$ $Tweet2$ $Tweet3$  
    
    \item For topic-based sentiment analysis:

    Read the question, provide your answer, and your confidence in this answer. Please make sure that the confidence level of your answers adds up to 1. Only output confidence levels. Do not output any other things. Please decide the following users' sentiment on the $Topic$: Is it POSITIVE or NEGATIVE? These tweets are from the same user, so please assume they have the same sentiment. $Tweet1$ $Tweet2$ $Tweet3$  
    
\end{itemize}

\section{Average F1 value}
In previous work, the average F1 value of positive and negative samples was often used to evaluate model performance. The formula is as follows:
\begin{equation}
F_{avg} = \frac{F_P+F_N}{2}
\end{equation}
$F_P$ is the F1 value of the positive sample, and $F_N$ is the F1 value of the negative sample. In stance detection, positive samples are samples with the label $Favor$, and negative samples are samples with the label $Against$.

\begin{table}[H]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|cc|cc}
        \hline
        ~ & \multicolumn{2}{c|}{Stance Detection} & \multicolumn{2}{c}{Sentiment Analysis}\\
        \hline
        Model & Total & Total (OL) & Total & Total (OL) \\
        \hline
        GPT-4o & 94.36 & 68.61 & 88.44 & 77.08 \\
        LLama3-70B & 90.29 & 68.94 & 92.06 & 76.67 \\
        PHI-4 & 78.87 & 65.96 & 92.06 & 74.88 \\
        \hline
    \end{tabular}
    }
\caption{The average F1 value of different models on the dataset after label correction. OL means original labels, which is the label of the original datasets without correction.} 
\end{table}

\clearpage
\onecolumn
\section{More Error Samples and LLM Responses}

\begin{figure*}[!htb]
    \centering 
    \includegraphics[width=1\textwidth]{latex/appendix.pdf} 
\end{figure*}


\end{document}


