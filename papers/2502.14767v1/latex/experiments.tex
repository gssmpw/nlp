\section{Experimental Design}
\par We choose \texttt{Llama-3.1-Nemotron-70B-Instruct-\\HF}, an \textit{open-source model}, as the base model for all experiments. We sample from the top 1\% of the tokens and use the same temperature settings across all samples (details on setting and hardware provided in Appendix \ref{appendix: settings}).

\subsection{Dataset}
\label{sec:dataset}
\par No datasets currently exist for comparing \textit{non-citing} pairs of scientific papers--- an overlooked setting, especially given the growing scale of literature where not all relevant work can be cited. Consequently, we aimed to construct a dataset with papers that both cite and do not cite each other, in order to test the robustness of Tree-of-Debate. However, novelty comparison between papers is a highly specialized and expensive task, requiring rich domain expertise to verify-- especially if such papers do not explicitly cite one another. Thus, we gathered five domain expert researchers (detailed provided in Appendix \ref{appendix: annotators}) to construct a dataset of 100 paper pairs across natural language processing, data mining, electrical engineering, and aerospace engineering (further details provided in Appendix \ref{appendix:dataset}). Each researcher identified at least five papers they were highly familiar with, such that they could perform a detailed and informed human evaluation. They were instructed to annotate each pair with a root topic and whether: (1) the papers roughly focus on the same task but differ in \textbf{methodology}, or (2) they work on different \textbf{tasks} that are applied to \textit{similar} motivations. Furthermore, they noted if the papers explicitly \textbf{cited} each other \textbf{or not}. Table \ref{tab:dataset}  shows the dataset distribution.
%Table \ref{tab:dataset} in Appendix \ref{appendix:dataset} shows the dataset distribution. 30 paper pairs cite each other and 70 do not; 45 differ by methodology and 55 differ by task.

\begin{table}[h!]
    \footnotesize
    \centering
    \begin{tabular}{l|cc|c}
        \toprule
         \textbf{Category} & \textbf{Method} & \textbf{Task} & \textbf{Total}\\
         \midrule
         \textbf{Cited} & 15 & 15 & 30 \\
         \textbf{Not Cited} & 30 & 40 & 70 \\
         \midrule
         \textbf{Total} & 45 & 55 & 100 \\
         \bottomrule
    \end{tabular}
    \caption{\# of paper pairs/summaries per category.}
    \label{tab:dataset}
    \vspace{-0.5cm}
\end{table}

\subsection{Baselines}
\label{sec:baselines}
\par Given that our primary goal is to demonstrate the difference in inference-time comparative reasoning capabilities between ToD and current LLMs, we design the following prompting-based baselines: (1) \textbf{Single Stage} uses the title, abstract and introduction sections of both papers to directly generate a comparative summary \cite{MartinBoyle2024ShallowSO}; (2) \textbf{Two Stage} first individually summarizes each paper based on the title, abstract and introductions, and then uses the generated summaries to generate a comparative summary \cite{Zhang2024FromCT}.

\par To contextualize improvements from each component in ToD, we construct the following ablative methods: (1) \textbf{ToD (No Tree)} removes the tree structure by merging child arguments into one and considering the combined subtopic as the debate node topic; (2) \textbf{ToD (No SD)} removes self-deliberation (SD) to test the impact of iterative retrieval based on debate progression. No SD relies on the title, abstract, and introduction of each paper instead of retrieving based on the subtopic. We use the same LLM for all comparisons. Complete baseline and ablation details are provided in Appendix \ref{appendix:baselines}.

\subsection{Evaluation Metrics}
\par The same domain-experts from Section \ref{sec:dataset} manually evaluate each of their chosen pairs in-depth, assessing various qualities of the 100 summaries. We normalize each of the scores below and scale them by 100 for the final results in Table \ref{tab:method_results} (full metric guidelines provided in Appendix \ref{appendix:eval_metrics}):
\begin{itemize}[leftmargin=*]
    \itemsep-0.3em
    \item \textbf{Factuality}: \textit{How factual is the summary?} Each sentence is given a 1/0 binary score for factuality, and the scores are averaged across the summary.
    \item \textbf{Breadth}: \textit{Is the summary comprehensive and complete?} Each summary is rated from 0-4 (``\textit{not at all}'' to ``\textit{very}'').
    \item \textbf{Contextualization}: 
    \textit{Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them?} Each summary is rated from 0-4 (``\textit{not at all}'' to ``\textit{very}'').
    \vspace{-0.5em}
\end{itemize}

\input{latex/main_tables}

\section{Experimental Results}

\par{\textbf{Overall Performance \& Analysis.}} Table \ref{tab:method_results} shows the performance of Tree-of-Debate (ToD) compared with the baselines on factuality, breadth of comparison (completeness), and contextualization. We observe that the domain-experts found ToD summaries $6.85\%$ more complete and $25.98\%$ more contextualized compared to the most competitive baseline. This observation indicates that multi-persona debate trees help analyze pairs of papers to uncover more fine-grained contributions, as well as identifying \textit{connections} between the papers. Given that all samples were carefully annotated and evaluated by domain-experts, we are able to draw several interesting insights, which we list below:

\par{\textbf{\underline{Structured} debate is necessary for eliciting \underline{contextualized} comparative summaries.}} Our results show that \textsc{Tree-of-Debate} significantly improves contextualization, achieving an average score of $95.21\%$ across all settings, compared to $75.57\%$ for the strongest baseline (Two-Stage). Our domain-expert evaluators frequently observed that the LLM-generated summaries (Single and Two Stage) at face value mention a breadth of \textit{specific} contributions made by each paper, noting them as either similarities or differences. However, these tend to resemble \textit{\textbf{extractive}} summaries, where phrases that are \textit{semantically similar or dissimilar} are extracted from the papers' abstracts and introduction and are posed as similarities and differences respectively, with no context provided on \textit{why} this is the case. We further analyze this finding in Section \ref{sec:summary_case_study}. Moreover, we see that without the tree-structure, the debate's analysis quality deteriorates. Specifically, for our ``No Tree'' ablation study (Table \ref{tab:method_results}), we modify our method to combine all proposed subtopics by the moderator (Section \ref{sec:self-deliberate}) into a single high-level topic. This leads to lower contextualization and breadth of contributions discussed, due to reasoning difficulty in disentangling the contributions during the debate. Thus, \textit{\underline{structuring} the debate is necessary to experience its contextualization benefits}.

\begin{table*}[h!]
    \centering
    \small
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{|p{0.75cm}|p{14.5cm}|}
        \hline
        \textbf{Model} & \textbf{Comparative Summary Excerpts} \\ 
        \hline
        \textbf{Two Stage} & MEGClass focuses on mutually-enhancing text granularities, iteratively refining its model through feedback between document, sentence, and word levels, and \textit{\textcolor{red}{requires an initial set of weakly labeled data}}. In contrast, LOTClass \textit{\textcolor{red}{innovates by relying solely on label names}} to train its model, employing a language model self-training approach that \textit{\textcolor{red}{obviates the need for any labeled documents}}, achieving high accuracy without human-labeled data. \\ 
        \hline
        \textbf{ToD} & \textit{\textcolor{ForestGreen}{Both papers leverage limited labeled data}} to achieve robust performance. Both MEGClass and LOTClass also employ \textit{\textcolor{ForestGreen}{iterative refinement techniques}}, with MEGClass using iterative feedback and LOTClass utilizing self-training on unlabeled data. MEGClass \textit{\textcolor{ForestGreen}{uniquely leverages mutually-enhancing text granularities}}, capturing both coarse- and fine-grained context signals to provide a more comprehensive understanding of class labels. In contrast, LOTClass relies \textit{\textcolor{ForestGreen}{solely on label names}}, leveraging pre-trained language models to drive a self-training mechanism. Furthermore, MEGClass's adaptive granularity balancing approach provides robustness to label name selection, whereas LOTClass's \textit{\textcolor{ForestGreen}{reliance on label names alone may introduce biases.}} \\ 
        \hline
    \end{tabular}
    \caption{Case study on two weakly supervised text classification works \cite{kargupta-etal-2023-megclass,meng-etal-2020-text}.}
    \label{tab:summary}
\end{table*}

\par{\textbf{\underline{Retrieval-augmented} debate ensures factuality and breadth of comparison.}} In Table \ref{tab:method_results}, we compare the performance between ToD and ``No SD'', where the latter ablation replaces the iterative retrieval step with simply providing the paper's title, abstract, and introduction as in-context evidence across the entire debate tree (same setting as the two baselines). We observe that No SD still experiences the strong benefits of the structured debate, evident through its similarly high contextualization score ($91.37\%$, compared to ToD's $95.47\%$ and Two Stage's $75.57\%$). We note a drop in the factuality and breadth of the summary. The information present in the abstract and introduction is not detailed or deep enough to facilitate fine-grained discussion of the paper. This leads to more noise, and hallucinations, which negatively impacts No SD's breadth and factuality scores. Therefore, \textit{\underline{iterative evidence retrieval} is necessary for exploring in breadth and minimizing hallucinations}.


\par{\textbf{\textsc{ToD} is \underline{robust} to all comparison settings.}} We analyze four different comparison settings (as detailed in Section \ref{sec:dataset}), where two papers may or may not cite each other and differ in either their \textit{task} or \textit{method}. We can see through Table \ref{tab:method_results} that regardless of their comparison type, ToD demonstrates consistently high performance-- with only an average standard deviation $\sigma=2.49$ across breadth and soundness, while Single Stage's $\sigma = 5.32$ and Two Stage's $\sigma=3.39$. While there are no consistent trends across the different settings, studying them allows us to \textit{ensure the \underline{consistency} of ToD}.




\subsection{Qualitative Case Study}
\label{sec:summary_case_study}
\par{\textbf{Evolution via Critical Reasoning.}} Our approach enables paper personas to refine their comparisons by addressing counterarguments elicited through debate. Table \ref{tab:evolution} illustrates this through a debate round between Tree of Thoughts (ToT) and Chain-of-Thought (CoT) on their generalizability and flexibility. ToT initially highlights its \textit{flexibility} through deliberate reasoning, exploring multiple paths with lookahead/backtracking. However, CoT counters that ToT's complexity hinders broad applicability, whereas CoT can easily generalize to a multitude of tasks with minimal setup while still achieving state-of-the-art results. In the revision stage, both refine their arguments: ToT emphasizes adaptive exploration for \textit{complex reasoning}, while CoT maintains its simplicity, now citing empirical success in complex tasks. It is interesting to note that during the \textit{Respond} stage, CoT \textbf{\textit{suggests a future study}} that would help validate the claims made in the debate.

\par{\textbf{Contextualized Summaries.}} The baselines' summaries tend to be extractive, not explaining why certain comparisons are important and also mistaking similarities for differences if their wording is semantically dissimilar. Table \ref{tab:summary} demonstrates this finding; Two Stage \textit{mistakenly states} that MEGClass ``requires an initial set of weakly labeled data'' while LOTClass ``innovates by relying solely on label names''. However, this is false-- both methods only require the class label names. This similarity is identified by ToD, which uses it to contextualize the methods' differences: both use label names, however MEGClass considers all three levels of text granularity, while LOTClass solely relies on the label names. ToD also provides \textit{further insight} into LOTClass's over-reliance on label names potentially introducing biases. ToD's debate format elicits critical reasoning and allows for a more insightful, contextualized comparison. Appendices \ref{sec:tree_example} and \ref{appendix:summary} include in-depth qualitative analyses on an additional sample's tree and output summaries.