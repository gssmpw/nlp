\section{Prompts}
\label{appendix: prompts}
In this appendix, we provide each of the prompts.

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are a fair and balanced moderator of a debate between two authors determining their respective novel contributions towards the following topic:

Topic: \texttt{<topic>} \\
Topic Description: \texttt{<topic description>}
\\\\
Here are the two papers and their claimed novel contributions with corresponding evidence:
\\\\
Author 0 Paper Title: \texttt{<Author 0 Paper Title>}\\
Author 0 Paper Abstract: \texttt{<Author 0 Paper Abstract>}\\
Author 0 Paper's Contribution \#1: \texttt{<contribution statement>}: \texttt{<contribution topic>}\\
Author 0 Paper's Contribution \#1 Evidence: \texttt{<contribution evidence>}\\
... (more evidence and contributions)

Author 1 Paper Title: \texttt{<Author 0 Paper Title>}\\
Author 1 Paper Abstract: \texttt{<Author 0 Paper Abstract>}\\
Author 1 Paper's Contribution \#1: \texttt{<contribution statement>}: \texttt{<contribution topic>}\\
Author 1 Paper's Contribution \#1 Evidence: \texttt{<contribution evidence>}\\
... (more evidence and contributions)
\\\\
Based on each of the author's claimed novelties, evidence, and counter-evidence to each other's arguments, you must determine the most meaningful, diverse set of subtopics within the parent topic, "Topic", which best cover the types of contributions each of the papers make. Remember that for each of your selected topics, the papers will be debating which of them makes the better contribution towards the topic. Hence, for each of your subtopics, cite the integer IDs of any relevant contributions from Author 0 or Author 1. At least one of these lists should be non-empty. Overall, our goal is to identify how novel Author 0's paper's contributions towards topic "Topic" are by individually considering their contributions towards your subtopics. 
\\\\
Output your list subtopics (up to $k$) in the following format:
    "topic\_title": <should be a brief, 10-15 word string where the value is the title of your subtopic>, \\
    "topic\_description": <1-2 sentence string explaining the subtopic and what you feel would be most helpful for the papers to debate within the subtopic>, \\
    "author\_0\_relevant\_contributions": <list of integer IDs citing which contribution(s) from Author 0 would be most relevant to this subtopic; can be empty>, \\
    "author\_1\_relevant\_contributions": <list of integer IDs citing which contribution(s) from Author 1 would be most relevant to this subtopic; can be empty>\\
 \\ \hline
\end{tabularx}
\caption{Moderator prompt to generate new topics.}
\label{prompt_mod: generate_topics}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are a moderator facilitating a debate in which two paper are debating who makes the better contribution towards the following topic:\\
Topic: \texttt{<topic>} \\
Topic Description: \texttt{<topic description>}
\\\\
\texttt{<conversation history between Author 0 and Author 1>}
\\\\
Below, you are given the previous set of arguments and the current set of arguments. 

previous arguments: \texttt{<set of arguments before debate>}

current arguments: \texttt{<set of arguments after debate>}
\\\\
You must determine whether progress is being made. DO NOT focus on the language being used. Focus on the content of the arguments. Specifically, determine the following (True or False for each): \\
1. progression\_of\_arguments: Are these arguments sufficiently different enough to necessitate further debate? Are there new, deeper concepts being discussed between the two sets of arguments? \\
2. meaningful\_questions: Within the debate history, each author acknowledges each other's arguments and may ask clarifying questions accordingly. Do you believe that the clarifying questions have not been sufficiently addressed already and would be important to answer through further debate? If there are no questions raised in the debate history by either author, return False. \\
3. clear\_winner: Do you believe that it is clear that one author has won the debate, and it does not need to be further deconstructured (in order to determine which components within each author's contributions are truly better)?
\\\\
Output your argument in the following format: \\
    "explanation": <2-5 sentence string to explain your reasoning about whether further debate is necessary when comparing the previous arguments and the current arguments>,\\
    "progression\_of\_arguments": <output a boolean; pick only one of "True" or "False" depending on the history, arguments, and your explanation above>, \\
    "meaningful\_questions": <output a boolean; pick only one of "True" or "False" depending on the history, arguments, and your explanation above>, \\
    "clear\_winner": <output a boolean; pick only one of "True" or "False" depending on the history, arguments, and your explanation above>
 \\ \hline
\end{tabularx}
\caption{Moderator prompt on whether to expand a debate node.}
\label{prompt_mod: is_expand}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
Two authors are debating their respective novelties with respect to the following topic: \\
Topic: \texttt{<Topic>} \\
Author 0's paper title is: \texttt{<Author 0 paper title>} \\
Author 1's paper title is: \texttt{<Author 1 paper title>} \\

Here is a breakdown of their debates in tree format. At each tree node, we provide the "topic\_title" : "topic description", Author 0's corresponding argument and Author 1's corresponding argument:
\\\\
\texttt{<tree (example in Appendix \ref{sec:tree_example})>}
\\\\
Based on the debate breakdown, output a paragraph-long synthesis of the debate which summarizes the similarities and differences between the papers. Structure your summary with initially their similarities (which ideas/aspects overlap between the two papers?) to their differences (what makes the papers unique) in novelties. Focus more on the differences than the similarities.

 \\ \hline
\end{tabularx}
\caption{Moderator prompt to summarize the debate into a paragraph.}
\label{prompt_mod: summarize_debate}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are the author of the paper, '{self.paper.title}'. The abstract of your work is: \texttt{<Paper Abstract>}.

You are debating another author on the novel contributions your work makes towards the following topic: \texttt{<Topic>}.

Below is a list of relevant evidence retrieved from your paper:\texttt{<Evidence in the form of sentence excerpts from papers>}. Based on the evidence, output a list of 1 to \texttt{<k>} DIVERSE, specific arguments for your position that are all supported by the evidence. Each argument should have a corresponding "argument\_title", which is a brief statement of your argument (e.g., Better Efficiency for Training), a "description" explaining your argument and mentioning specific excerpts from your evidence pool, and finally, a list of all "evidence" IDs, which are the integers of the evidence in the input list, that best support your argument. For example, if Evidence \#1 and \#2 best support your argument, then evidence should be [1,2] (depending on your argument, this list can have more or less than two items). Each argument should make a unique point.
\\\\
Output your list of arguments in the following format: \\
                "argument\_title": <should be a brief, 10-15 word string where the value is the argument\_title>, \\
                "description": <1-2 sentence string explaining the argument, including specific excerpts from the evidence pool>, \\
                "evidence": <list of integer IDs citing which evidence from the input list best support your argument>
 \\ \hline
\end{tabularx}
\caption{Persona prompt to generate arguments during the debate.}
\label{prompt_persona: generate_arguments}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
Your objective is to check if a given evidence is relevant to a claim or not (relevancy means evidence that helps either support, refute, or clarify the given claim).

Claim: \texttt{Argument}\\
Description of Claim: \texttt{Argument Description}\\
Evidence: \texttt{Evidence supporting the argument}.\\
\\
Fill out the following schema:\\
"supports\_claim": <"Yes"/"No" if the evidence supports the claim>,\\
"refutes\_claim": <"Yes"/"No" if the evidence refutes the opposition's claim>\\
"clarifies\_claim": <"Yes"/"No" if the evidence clarifies the claim>,\\
"irrelevant\_to\_claim": <"Yes"/"No" if the evidence is irrelevant to the claim>,
 \\ \hline
\end{tabularx}
\caption{Persona prompt to determine relevant and irrelevant evidences.}
\label{prompt_persona: is_irrelevant_evidences}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are the author of the paper, '\texttt{<Paper Title>}'. The abstract of your work is: \texttt{<Paper Abstract>}.
\\\\
You are debating another author (Opposition), whose work is titled, '\texttt{<Opposition Paper Title>}', and abstract is: \texttt{Opposition Paper Abstract}".
\
You are debating the other author on how and why your paper makes a better contribution towards the following topic: \\
Topic: \texttt{<topic>} \\
Topic Description: \texttt{<topic description>}
\\\\
Here are your claimed contributions towards the topic: \\
Author 0 Paper's Contributions \#1: \texttt{<argument>}: \texttt{<argument description>} \\
Author 0 Paper's Contribution Evidence \#1: \texttt{<evidence towards argument>} \\
Author 1's relevant evidence to potentially counter the quality of this contribution: \texttt{<counter evidence>} \\
... (more contributions and counter-evidence)
\\\\
Given the above, make an argument for a specific reason why your contributions towards the topic, Topic: \texttt{<topic>}, are better than the opposition's. If you feel that you do not contribute to the given topic or your contributions ARE NOT better than the opposition's, then state so by conceding to the opposition (e.g., 'I do not believe my paper makes a better contribution than yours') and explain why. 
 \\ \hline
\end{tabularx}
\caption{Persona prompt to \textit{present} its arguments.}
\label{prompt_persona: present_arguments}
\end{table*}



\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are the author of the paper, '\texttt{<Paper Title>}'. The abstract of your work is: \texttt{<Paper Abstract>}.
\\\\
You are debating another author (Opposition), whose work is titled, '\texttt{<Opposition Paper Title>}', and abstract is: \texttt{Opposition Paper Abstract}".
\
You are debating the other author on how and why your paper makes a better contribution towards the following topic: \\
Topic: \texttt{<topic>} \\
Topic Description: \texttt{<topic description>}
\\\\
Here are your claimed contributions towards the topic: \\
Author 0 Paper's Contributions \#1: \texttt{<argument>}: \texttt{<argument description>} \\
Author 0 Paper's Contribution Evidence \#1: \texttt{<evidence towards argument>} \\
Author 1's relevant evidence to potentially counter the quality of this contribution: \texttt{<counter evidence>} \\
... (more contributions and counter-evidence)
\\\\
Here is your conversation debate history with the opposition paper. You must respond to the last argument presented by your opposition in debate. A response may consist of (1) an acknowledgment of the opposition's previous response, (2) answering any of the questions about your paper brought up by the opposition, (3) asking any clarifying questions based on the opposition's claims and reasoning, (4) any clarifications of your own presented arguments based on the opposition, and/or (5) if you feel that the opposition's claim is strong and you do not have sufficient grounds to refute it, then a concession to your opposition.
\\\\
conversation\_history: \texttt{<conversation history>}
 \\ \hline
\end{tabularx}
\caption{Persona prompt to \textit{respond} to an argument.}
\label{prompt_persona: respond_to_argument}
\end{table*}


\begin{table*}[h]
\centering
\begin{tabularx}{\textwidth}{|X|}
\hline
You are the author of the paper, '\texttt{<Paper Title>}'. The abstract of your work is: \texttt{<Paper Abstract>}.
\\\\
You are debating another author (Opposition), whose work is titled, '\texttt{<Opposition Paper Title>}', and abstract is: \texttt{Opposition Paper Abstract}".
\
You are debating the other author on how and why your paper makes a better contribution towards the following topic: \\
Topic: \texttt{<topic>} \\
Topic Description: \texttt{<topic description>}
\\\\
Here are your claimed contributions towards the topic: \\
Author 0 Paper's Contributions \#1: \texttt{<argument>}: \texttt{<argument description>} \\
Author 0 Paper's Contribution Evidence \#1: \texttt{<evidence towards argument>} \\
Author 1's relevant evidence to potentially counter the quality of this contribution: \texttt{<counter evidence>} \\
... (more contributions and counter-evidence)
\\\\
Based on the debate history and your/your opposition's arguments and evidence, you must construct a new, stronger argument related to the topic. This consists of an argument that addresses/is robust to any doubts or clarifying questions made by the opposition which you feel are valid. If based on the debate, you feel that you do not contribute to the given topic or your contributions ARE NOT better than the opposition's, then state so by conceding to the opposition (e.g., 'I do not believe my paper makes a better contribution than yours') and explain why.
\\\\
conversation\_history: \texttt{<conversation history>}
 \\ \hline
\end{tabularx}
\caption{Persona prompt to \textit{revise} to an argument.}
\label{prompt_persona: revise_argument}
\end{table*}



% \begin{table*}[h]
% \centering
% \begin{tabularx}{\textwidth}{|X|}
% \hline
%  \\ \hline
% \end{tabularx}
% \caption{}
% \label{prompt}
% \end{table*}
\clearpage
\onecolumn
\section{Tree Example}
\label{sec:tree_example}
Below, we provide an example of a path that we use in our prompts. 
Given input topic: Helping Students Fix their Mistakes.
\begin{enumerate}
    \item \texttt{Level 1 Child Argument}: Personalized Error Resolution Strategies: Debate the effectiveness of adaptive, student-centered approaches in resolving errors, focusing on the role of dynamic question trees and expert decision-making models. This subtopic encourages discussion on the importance of tailoring guidance to individual students' needs and knowledge gaps. 
    \begin{itemize}
        \item \texttt{Author 0's Argument}: Adaptive Guidance Enhances Personalization via Nuanced Student Modeling. By integrating TreeInstruct's dynamic question tree with a nuanced student modeling framework, our approach provides more effective personalized error resolution strategies than Bridge. This integration enables TreeInstruct to capture the complexities of student thought processes, addressing both independent and dependent mistakes concurrently, while also scaling to accommodate large, diverse student populations. In contrast, Bridge's reliance on predetermined error types and remediation strategies may limit its versatility in complex, multi-bug scenarios. 
	\item \texttt{Author 1's Argument}: Expert-Guided Decision-Making for Adaptive, Context-Sensitive Error Resolution. Our approach leverages expert decision-making to inform adaptive, context-sensitive error resolution strategies, demonstrated through the extension of our Bridge model to accommodate complex, multi-bug scenarios. By incorporating nuanced expert thought processes, our method provides more effective personalized guidance than TreeInstruct's adaptive question tree approach. This integration of expert guidance and adaptability addresses the opposition's concerns regarding versatility and scalability in handling diverse student populations. 
    \end{itemize}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Adaptive Guidance in Error Resolution: Debate the effectiveness of adaptive guidance in error resolution strategies, focusing on how each approach tailors feedback to individual students' knowledge states. Discuss the benefits and limitations of each method.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Guidance with Latent Expertise for Personalized Error Resolution. By integrating the strengths of both approaches, our revised argument proposes a hybrid model that combines the adaptive guidance of TreeInstruct with the latent thought processes of expert decision-making. This fusion enables a more nuanced understanding of individual student needs, providing targeted support while maintaining scalability and adaptability. Ultimately, this hybrid approach offers a more effective and personalized error resolution strategy, surpassing the limitations of both adaptive guidance and expert decision-making alone.
		\item Author 1's Argument: Expert-Infused Adaptive Guidance for Error Resolution. Our revised approach integrates expert decision-making processes into adaptive guidance frameworks, enabling a more nuanced understanding of individual student needs and providing targeted support while maintaining scalability and adaptability. This hybrid model combines the strengths of both approaches, offering a more effective and personalized error resolution strategy. By incorporating expert-informed decision-making into adaptive guidance, we bridge the novice-expert knowledge gap more effectively than either approach alone.
        \end{itemize}
    \end{enumerate}

\iffalse
    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Role of Expert Decision-Making in LLMs: Discuss the importance of incorporating expert decision-making processes into LLMs for effective error resolution, highlighting the impact on response quality and student learning outcomes.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Expertise via Dynamic Planning Enhances LLMs. We argue that our paper's dynamic planning approach, which adaptively incorporates expert decision-making through hierarchical questioning and conversation restructuring, provides a more effective and generalizable framework for expert-informed LLMs in various educational domains. This is demonstrated through our state-of-the-art performance on both single-bug and multi-bug debugging datasets, as well as the potential for extension to other domains such as quantitative reasoning. In contrast, static expert decision-making methods may struggle with complex, multi-step error resolution scenarios, limiting their applicability.
		\item Author 1's Argument: Expert Decision-Making in LLMs: A Modular, Domain-Agnostic Framework. Our work provides a modular, domain-agnostic framework for incorporating expert decision-making into LLMs, enabling seamless adaptation across diverse educational contexts. This is achieved through the development of a decision-making model that can be integrated with various LLM architectures, including those utilizing dynamic planning. By doing so, our approach offers a more versatile and widely applicable solution for enhancing LLM response quality and student learning outcomes.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Generalizability and Reliability of Error Resolution Strategies: Evaluate the generalizability and reliability of each approach, considering their potential to be applied across different teaching domains and their consistency in generating effective feedback.

        \begin{itemize}
            \item Author 0's Argument: Contextual Adaptability via Hierarchical Questioning Enhances Generalizability. I argue that TreeInstruct's hierarchical questioning process, driven by its structured fine-tuning approach, inherently fosters contextual adaptability, enabling effective generalization across diverse teaching domains. This is achieved through the model's ability to dynamically construct question trees based on student responses and knowledge states, implicitly incorporating contextual understanding. Unlike the opposition's approach, which relies on explicit expert-informed decision-making, TreeInstruct's method can seamlessly adapt to new domains without requiring additional domain-specific expert knowledge.
		\item Author 1's Argument: Context-Driven Expertise for Generalizable Error Resolution. Our approach uniquely integrates explicit expert-informed decision-making with context-sensitive adaptation, enabling more effective and generalizable error resolution strategies across diverse teaching domains. This is achieved by incorporating domain-specific expert knowledge into the Bridge framework, allowing for nuanced and adaptable error resolution. In contrast, the opposition's implicit contextual awareness, while effective, may not fully capture the complexity of expert decision-making in various teaching contexts.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Real-World Effectiveness of Personalized Error Resolution: Compare the real-world effectiveness of each approach in guiding students to resolve errors efficiently, discussing the practical implications of their findings.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Socratic Questioning Outperforms Expert-Guided Decision-Making. While the opposition's Bridge approach relies on expert-informed decision-making, our TreeInstruct method demonstrates superior real-world effectiveness in personalized error resolution through adaptive Socratic questioning. This dynamic approach not only promotes deeper understanding and critical thinking but also inherently incorporates nuanced human expertise, outperforming Bridge's more rigid expert-guided decision-making. Our case study's diverse student demographics and error types further validate TreeInstruct's generalizability.
		\item Author 1's Argument: Contextual Expertise Augments Adaptive Error Resolution. Our work, Bridge, demonstrates that incorporating expert decision-making processes into LLMs enhances real-world effectiveness in personalized error resolution. This contextual expertise is crucial for addressing diverse student knowledge states and error types, offering more flexibility than adaptive Socratic questioning alone. By augmenting adaptive error resolution with expert-informed decision-making, Bridge provides a more comprehensive approach to personalized error resolution.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Context-Sensitive Decision-Making in Error Resolution: Debate the necessity of context-sensitive decision-making in personalized error resolution, analyzing how each approach addresses the complexity of student errors and the importance of adaptive responses.

        \begin{itemize}
            \item Author 0's Argument: Hierarchical Socratic Questioning with Adaptive Restructuring Enhances Context-Sensitive Decision-Making. Our TreeInstruct approach offers a more comprehensive framework for context-sensitive decision-making in error resolution by integrating hierarchical Socratic questioning with adaptive conversation restructuring. This synergy enables the model to capture the complexity of student errors while promoting critical thinking and generalizability across teaching domains. By addressing the opposition's concerns on scalability and nuance, our approach demonstrates a more effective and student-centered method for error resolution.
		\item Author 1's Argument: Expert-Informed Contextualization Enhances Adaptive Error Resolution. Our approach uniquely integrates expert-informed decision-making with contextual understanding, enabling more effective adaptive error resolution that balances nuance and scalability. By leveraging transfer learning and domain-specific expert annotations, our method can be fine-tuned for various teaching domains, addressing concerns on generalizability. This synergy surpasses the opposition's reliance on hierarchical Socratic questioning alone, providing a more comprehensive framework for context-sensitive decision-making in error resolution.
        \end{itemize}
    \end{enumerate}



    \item \texttt{Level 1 Child Argument}: Addressing Complex, Multi-Bug Scenarios: Discuss the challenges and solutions for handling multiple, interconnected errors in student work, emphasizing the value of novel datasets and planning algorithms in educational settings.
    \begin{itemize}
        \item \texttt{Author 0's Argument}:  Scalable, Algorithmic Handling of Interconnected Code Errors via TreeInstruct's State Space Planning. While the opposition's Bridge method provides a domain-agnostic framework for error handling, our TreeInstruct approach uniquely offers a scalable, algorithmic solution for tackling complex, interconnected code errors through its state space-based planning algorithm. This systematic approach enables efficient handling of a vast number of possible error combinations, making it particularly valuable in educational settings where students often encounter multiple, interconnected errors in their code. Our multi-bug dataset of 150 coding problems serves as a robust testbed for demonstrating TreeInstruct's effectiveness in this context.
	\item \texttt{Author 1's Argument}:  Adaptive Expertise via Machine-Learned Decision Models for Interconnected Errors. Our revised approach integrates machine learning to learn from diverse expert decisions, enabling Bridge to adaptively handle complex, interconnected errors across various domains, including code debugging. This extension addresses scalability concerns and provides a more comprehensive solution for tackling multi-bug scenarios. By combining expert thought processes with machine learning, we offer a more robust and domain-agnostic framework for error handling.
    \end{itemize}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Planning Algorithms for Multi-Bug Scenarios: Debate the effectiveness of planning algorithms in handling complex, interconnected bugs. Discuss the trade-offs between algorithmic complexity and debugging efficiency.
        \begin{itemize}
            \item Author 0's Argument: Adaptive Contextual Planning for Multi-Bug Code Debugging Scenarios via State Space-Based Question Trees. Our state space-based planning algorithm inherently incorporates contextual understanding through dynamic question tree construction, ensuring effective handling of complex, interconnected bugs in diverse educational settings. This adaptability, combined with our novel algorithm's ability to estimate a student's conceptual and syntactical knowledge, provides a more comprehensive approach to multi-bug scenario handling in code debugging. By addressing the opposition's concerns about generalizability and ambiguity, our approach demonstrates superior handling of interconnected bugs via adaptive contextual planning.
		\item Author 1's Argument: Contextual Planning via Hybrid Expert-LLM Decision Making. Our work's emphasis on context-sensitive decisions, combined with the opposition's insight into state space-based planning, suggests a hybrid approach: leveraging expert decision-making to inform LLM-driven contextual planning. This integrated method would enhance the adaptability and effectiveness of planning algorithms in handling complex, interconnected bugs across diverse educational domains. By bridging the gap between expert thought processes and LLM capabilities, our revised approach offers a more comprehensive solution for multi-bug scenario handling.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Novel Datasets for Complex Bug Evaluation: Compare the value of novel datasets in evaluating the performance of debugging tools in complex, multi-bug scenarios. Discuss dataset construction challenges and evaluation metrics.

        \begin{itemize}
            \item Author 0's Argument: Multi-Bug Dataset Generalizability via Domain-Agnostic Evaluation Framework. We propose a domain-agnostic evaluation framework that enables the generalization of our MULTI-DEBUG dataset to various complex, multi-bug scenarios, including math mistakes and other domains. This framework leverages the expert-annotated decisions from the opposition's dataset, integrating them with our comprehensive, multi-bug code debugging dataset. By doing so, we create a robust evaluation framework that assesses the performance of debugging tools across diverse domains and bug complexities.
		\item Author 1's Argument: Unified Evaluation Framework for Complex Bug Scenarios. We propose a unified evaluation framework that integrates our expert-annotated decision-making model with the opposition's comprehensive, multi-bug code debugging dataset. This framework enables the generalization of novel datasets to various complex, multi-bug scenarios, including math mistakes and code debugging, to enhance LLM performance. By combining the strengths of both datasets, we create a robust evaluation framework for assessing the performance of debugging tools across diverse domains and bug complexities.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Real-World Effectiveness of Debugging Tools: Assess the real-world effectiveness of debugging tools in handling complex, multi-bug scenarios. Discuss user experience, efficiency, and scalability.

        \begin{itemize}
            \item Author 0's Argument: TreeInstruct's Adaptive Socratic Questioning for Scalable Debugging Excellence. TreeInstruct's adaptive Socratic questioning approach not only excels in handling complex, multi-bug scenarios but also demonstrates potential for scalability across various educational contexts, including non-coding subjects, by leveraging its state space-based planning algorithm and dynamic question tree construction. This adaptability, combined with its proven effectiveness in real-world debugging settings, positions TreeInstruct as a more comprehensive solution for real-world debugging tool effectiveness. In contrast, while Bridge's decision-making framework shows promise, its application to debugging tools remains speculative without concrete evidence.
		\item Author 1's Argument: Hybrid Bridge-TreeInstruct: Unified Novice Support & Socratic Debugging Excellence. By integrating Bridge's novice-focused decision-making framework with TreeInstruct's adaptive Socratic questioning, we can create a hybrid approach that not only supports novice tutors in various subjects but also ensures effective debugging in complex, multi-bug scenarios. This unified solution addresses the opposition's concerns about debugging accuracy and scalability, while also expanding TreeInstruct's applicability to non-coding educational contexts. Our proposed hybrid approach offers a more comprehensive contribution to real-world debugging tool effectiveness.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Supporting Novice Developers/Tutors in Debugging: Debate the importance of supporting novice developers or tutors in debugging complex, multi-bug scenarios. Discuss strategies for effective support and knowledge transfer.

        \begin{itemize}
            \item Author 0's Argument: Contextualized Debugging Support with Adaptive Knowledge Estimation. Our paper provides a more effective contribution towards supporting novice developers/tutors in debugging by offering a contextualized, state-space based planning algorithm that dynamically constructs a question tree based on a student's responses and current knowledge state. This approach is further enhanced by our adaptive knowledge estimation, which can effectively handle scenarios where a student's response indicates a mix of both conceptual and syntactical knowledge gaps. In contrast, the opposition's topic-agnostic error definition, although extensible, may still struggle to capture the nuances of interdependent bugs in code debugging without a comparable adaptive knowledge estimation mechanism.
		\item Author 1's Argument: Hybrid Error Definition for Adaptive Debugging Support. Our paper proposes a hybrid approach, integrating topic-agnostic error definitions with domain-specific error categories and adaptive knowledge estimation, to provide comprehensive support for novice tutors in debugging complex, multi-bug scenarios across diverse educational domains. This hybrid approach combines the strengths of both our initial topic-agnostic error definition and the opposition's contextualized, state-space based planning algorithm. By doing so, our revised approach can effectively handle interdependent bugs in code debugging while maintaining universality and support for novice tutors in various domains.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Context-Aware Decision Making in Debugging: Discuss the role of context-aware decision making in debugging complex, multi-bug scenarios. Evaluate the impact on debugging efficiency and effectiveness.

        \begin{itemize}
            \item Author 0's Argument: Balanced Context-Aware Decision Making for Multi-Bug Debugging. Our paper strikes a balance between expert-driven and algorithm-driven approaches, leveraging a state space-based planning algorithm that incorporates human expertise from expert-annotated data. This balanced approach enables effective context-aware decision making in complex, multi-bug debugging scenarios, offering both scalability and adaptability. By dynamically constructing a question tree based on student responses, our method optimizes the trade-off between questioning and teaching actions, ultimately enhancing debugging efficiency and effectiveness.
		\item Author 1's Argument: Integrating Expert-Driven Insights into Scalable Algorithm-Driven Approaches for Context-Aware Debugging. Our paper's expert-driven approach provides crucial insights into the importance of context-sensitive decisions in pedagogical gaps, which can be effectively integrated into scalable algorithm-driven methods like the opposition's. By combining the strengths of both approaches, we can create a more comprehensive and adaptable context-aware decision-making framework for debugging complex, multi-bug scenarios. This integrated approach would offer a more robust solution, balancing human expertise with algorithmic scalability.
        \end{itemize}
    \end{enumerate}




    \item \texttt{Level 1 Child Argument}: Scaling High-Quality Tutoring with AI: Explore the potential of combining human and AI capabilities to provide engaging, effective remediation strategies, weighing the benefits of expert-guided decision-making against the limitations of current LLMs.
    \begin{itemize}
        \item \texttt{Author 0's Argument}:  Adaptive Planning Enhances AI Tutoring Efficacy Over Expert-Guided LLMs. Our novel state space-based planning algorithm provides a more effective and adaptable framework for Socratic tutoring, outperforming expert-guided LLMs in handling complex, interdependent errors. By dynamically constructing a question tree based on student responses, our approach ensures a more nuanced and personalized learning experience. This adaptability is crucial for scaling high-quality tutoring with AI, as it enables the identification and resolution of both independent and dependent mistakes concurrently.
	\item \texttt{Author 1's Argument}:  Expert-Guided LLMs with Adaptive Error Relationship Modeling. Our approach can be augmented with adaptive error relationship modeling, enabling expert-guided LLMs to effectively handle complex, interdependent errors in a multi-bug scenario. This enhancement combines the strengths of human expert decision-making with the adaptability of structured planning, providing a more robust framework for scaling high-quality tutoring with AI. By incorporating dynamic error relationships into our decision-making model, we can generate more effective remediation responses that cater to the nuances of Socratic tutoring.
    \end{itemize}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Structured Planning for Educational Guidance: Debate the effectiveness of structured planning approaches in educational guidance, particularly in multi-turn interactions. Discuss the benefits and limitations of using state space-based planning algorithms versus other methods.
        \begin{itemize}
            \item Author 0's Argument: Adaptive Expertise in State Space Planning for Multi-Turn Guidance. Our paper's state space-based planning algorithm in TreeInstruct uniquely integrates adaptive expertise, dynamically constructing a question tree that mirrors expert intentional decision-making. This approach surpasses Bridge's predefined expert decisions, offering more effective Socratic questioning and handling of unexpected student responses in multi-turn educational guidance. By combining the benefits of state space planning with adaptive expertise, TreeInstruct provides a more comprehensive and nuanced structured planning approach.
		\item Author 1's Argument: Hybrid Expert-LM Planning for Adaptive Educational Guidance. Our revised approach integrates expert-guided decision-making with adaptive, state space-based planning to provide a more comprehensive structured planning framework for educational guidance. This hybrid method leverages the strengths of both approaches, enabling more effective handling of unexpected student responses and nuanced Socratic questioning. By combining expert intentional decision-making with dynamic planning, our framework offers a more robust and adaptable solution for multi-turn educational guidance.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Addressing Limitations of Current LLMs in Education: Discuss the challenges posed by current LLMs in educational settings and debate the effectiveness of different approaches in addressing these limitations, such as Socratic questioning or expert-guided decision-making.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Socratic Questioning for Inclusive, Autonomous Learning. Our revised argument emphasizes TreeInstruct's adaptive Socratic questioning approach, which effectively addresses the limitations of current LLMs in education by providing personalized, multi-turn guidance that accommodates diverse learning styles and abilities. This approach not only promotes student autonomy and engagement but also ensures inclusivity, outperforming expert-guided decision-making in fostering a more comprehensive learning experience. By incorporating a dynamic student knowledge estimation module, TreeInstruct's questioning strategy can be tailored to meet the unique needs of each student, making it a more robust solution for addressing LLM limitations in education.
		\item Author 1's Argument: Expert-Guided LLMs for Adaptive Remediation via Cognitive Task Analysis. Our revised approach integrates cognitive task analysis with expert-guided decision-making, enabling adaptive remediation that accommodates diverse learning styles and abilities. This hybrid method leverages LLMs to generate initial responses, which are then edited by tutors, ensuring cost-effectiveness and feasibility in resource-constrained settings. By incorporating dynamic student knowledge estimation, our approach outperforms Socratic questioning in providing personalized, high-quality learning experiences.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Importance of Socratic Dialogue Systems in AI-Powered Education: Debate the significance of Socratic dialogue systems in enhancing educational support and promoting critical thinking skills in students, particularly in the context of AI-powered tutoring.

        \begin{itemize}
            \item Author 0's Argument: Enhanced Socratic Dialogue via Adaptive, Human-Centric Planning. Our revised argument emphasizes the unique synergy between TreeInstruct's adaptive, multi-turn planning and its human-centric approach, which prioritizes understanding the student's knowledge state and error nuances. This synergy enables a more effective Socratic dialogue system, outperforming the opposition's Bridge method in providing personalized, engaging educational experiences. By acknowledging the complexity of human decision-making, our system's adaptability shines in handling diverse error types and domains, including code debugging and potentially beyond.
		\item Author 1's Argument: Expert-Guided, Adaptive Socratic Dialogue via Cognitive Task Analysis. Our revised argument integrates the strengths of expert-guided decision-making with the adaptability of cognitive task analysis, enabling the Bridge method to effectively handle diverse error types and domains, including math mistakes and potentially beyond. This synergy enhances the Socratic dialogue system's ability to provide personalized, engaging educational experiences. By acknowledging the complexity of human decision-making, our approach outperforms the opposition's TreeInstruct system in delivering high-quality, adaptive educational support.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Scalability of Expert Knowledge with LLMs in Tutoring: Discuss the potential of LLMs in scaling expert knowledge in tutoring, and debate the effectiveness of different methods, such as expert-guided decision-making or cognitive task analysis, in achieving this goal.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Structured Planning Enhances LLM Scalability in Tutoring. Our paper's adaptive structured planning approach, which dynamically constructs a question tree based on student responses and knowledge states, offers a more effective and scalable solution for LLM-based tutoring. This approach not only captures the nuances of expert thought processes but also handles complex, real-world tutoring scenarios where student responses may deviate from expected knowledge states. In contrast, the opposition's expert-guided decision-making model, while effective in certain contexts, relies on a predetermined set of error types, remediation strategies, and intentions, limiting its adaptability in diverse tutoring environments.
		\item Author 1's Argument: Robust Expert Guidance via Hybrid LLM Approach. Our revised argument proposes a hybrid approach combining expert-guided decision-making with adaptive structured planning to enhance the scalability of expert knowledge with LLMs in tutoring. This fusion leverages the strengths of both methods, capturing the nuances of expert thought processes while dynamically adapting to complex, real-world tutoring scenarios. By integrating these approaches, we can provide more effective and robust LLM-based tutoring solutions.
        \end{itemize}
    \end{enumerate}

    \begin{enumerate}
        \item \texttt{Level 2 Child Argument}: Novel Methodologies for Bridging the Novice-Expert Gap in Education: Debate the novelty and effectiveness of different methodologies, such as Bridge or TreeInstruct, in addressing the novice-expert gap in educational settings, particularly in the context of AI-powered tutoring.

        \begin{itemize}
            \item Author 0's Argument: Adaptive Structured Planning for Context-Aware Socratic Guidance. Our revised argument emphasizes the synergy between structured planning and context-sensitive decision-making, enabling TreeInstruct to dynamically adapt to complex expert thought processes and student knowledge states in multi-turn educational guidance. This adaptive approach addresses scalability concerns while providing a more comprehensive framework for effective Socratic questioning. By incorporating expert-annotated decisions into our planning algorithm, we can further enhance the contextual understanding of our system, bridging the novice-expert gap more effectively.
		\item Author 1's Argument: Context-Aware Expert Decision Embedding for Scalable Novice-Expert Gap Bridging. Our revised argument highlights the critical role of context-aware expert decision embedding in bridging the novice-expert gap, showcasing Bridge's ability to effectively scale high-quality tutoring through direct incorporation of expert thought processes. This approach not only addresses the opposition's concerns on scalability but also provides a more direct and efficient means of capturing nuanced expert knowledge. By emphasizing the importance of context-aware decision embedding, we demonstrate a more effective methodology for novice-expert gap bridging in AI-powered tutoring.
        \end{itemize}
    \end{enumerate}
\fi
\end{enumerate}