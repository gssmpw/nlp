\section{Related Work}
\subsection{Persona Creation \& Debate} Similar to how a person's background shapes their abilities, recent work has explored assigning personas to LLMs to capture diverse perspectives and extract unique capabilities \cite{self-play}. For instance, \citet{bridger} creates author personas for author recommendation by applying named entity recognition to papers and matches authors based on shared terminology. As we aim to highlight specific novelties and incremental contributions between two papers, we instead define a \textit{paper persona}. While \citet{bridger}'s personas represent the author's multiple works and are used solely for comparison, ours represent the paper, actively debating for and defending it. Other studies have also leveraged persona-driven debate by assigning multiple personas-- such as affirmative and negative debaters along with a judge--- to synthesize diverse reasoning steps for tasks like commonsense generation and arithmetic, thereby reducing confirmation bias inherent in self-reflection methods \cite{liang2023encouraging}. Although our objective differs, we similarly employ debate--- not to serve as a means to improve the final output but, rather, as the outcome itself--- using the tree directly to generate refined summaries of differences between research papers.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=0.9\textwidth]{tods_framework.pdf}
    \caption{
    \label{fig:tree} We propose Tree-of-Debate, a novel framework which guides a multi-persona debate using a retrieval-augmented tree. $A \rightarrow B$ 
within the diagram translates to the statement, ``Given $A$, a persona arrives at $B$''.}
\end{figure*}

\subsection{Comparative Summarization} Generating comparative summaries is challenging due to the diverse ways that differences between two entities can be represented. Traditional graph-based methods \cite{comparative_graph, Strhle2023} classify sentences as a claim, similarity, or difference and score them to produce extractive summaries. While we use extractive summarization questions for self-deliberation, ultimately we aim to generate an \textit{abstractive} summary that synthesizes the debate results. More recent works \cite{explainingrelationships} fine-tune models to generate explanation sentences by first extracting in-text citation sentences that compare a principal document with a cited one, then maximizing the probability of generating the explanation given the two documents; however, this approach typically yields only a single sentence, which may not fully capture the nuanced differences between papers.

\subsection{Generation of Related Works Sections} 
\par Multi-document summarization consolidates information from various sources, a task that grows in importance as scientific literature expands \cite{comparative_graph}. Certain works within the HCI space \cite{palani2023relatedly,lee2024paperweaver}, which have designed off-the-shelf, interactive systems. However, from a methodological standpoint, one approach \cite{ur3wg} expands a paper's abstract into semantically similar sentences to form search queries for retrieving relevant papers, and then uses in-context examples to generate related work sections. On the other hand, DIR \cite{dir} employs a structured fine-tuning process by prompting a language model to extract commonalities and differences from candidate summaries compared to a gold standard. However, these methods face limitations: the former is highly dependent on the quality of its in-context example without clear guidelines on how to structure the related work. The latter requires fine-tuning the model for each dataset and relies on similarity matching that can restrict summary content \cite{contribution_sum, whats_new}. In contrast, our method uses debate rounds to guide the summary structure, operates at inference-time without training-- making it domain-agnostic-- and leverages the reasoning capabilities of language models to identify isomorphic properties of ideas beyond mere semantic similarity.