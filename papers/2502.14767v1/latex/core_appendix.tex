\section{Experimental Settings}
\label{appendix: settings}
\par We choose \texttt{Llama-3.1-Nemotron-70B-Instruct-\\HF}, an \textit{open-source model}, as the base model for all experiments. We sample from the top 1\% of the tokens and set the temperature between $(0,0.5)$ to trade-off between deterministic and creative generation based on the nature of the given task (same setting across all samples):
\begin{itemize}
    \item \underline{Paper} generates arguments: $0.3$
    \item \underline{Paper} whether evidence is relevant: $0$
    \item \underline{Paper} presents its argument: $0.1$
    \item \underline{Paper} responds to the opposition's argument: $0.4$
    \item \underline{Paper} revises its argument: $0.4$
    \item \underline{Moderator} generating subtopics: $0.3$
    \item \underline{Moderator} determines whether to expand the debate note: $0.1$
    \item \underline{Moderator} summarizes a debate path: $0.4$
\end{itemize}

\par We set the number of retrieved segments $\delta = 5$ so that we can gather a sufficient amount of evidence while not overwhelming the debate with long-context. We set the number of generated subtopics $k = 3$, for covering a reasonable breadth of topics while minimizing redundancy. Finally, we set the maximum debate tree depth $l=3$ for adequate exploration. We use vLLM \cite{kwon2023efficient} for distributed and constrained generation on four \texttt{NVIDIA A100} GPUs.

\section{Baselines}
\label{appendix:baselines}
\par We compare Tree-of-Debate (ToD) with the following prompting-based baseline methods. 
We use the same base language model for all comparisons. 
\begin{itemize}[leftmargin=*]
    \item \textbf{Single-stage}: We prompt an LLM with the title, abstract and introduction sections of both focus and opposition papers. We prompt the model to directly generate a contrastive summary of the two papers \cite{MartinBoyle2024ShallowSO}. 
    \item \textbf{Two-stage}: We first instruct an LLM to individually summarize each paper based on the title, abstract and introductions. We then use the generated summaries to prompt the model to generate a contrastive summary \cite{Zhang2024FromCT}. 
\end{itemize}

To contextualize improvements from each component in Tree-of-Debate we construct the following ablative methods:
\begin{itemize}
    \item \textbf{ToD (No Tree)}: We remove the tree structure from Tree-of-Debate by merging child arguments into one. We do so by concatenating the topics and descriptions of the child subtopics and tag them to distinguish the topics. In each debate round, the model is prompted with the combined subtopic and its corresponding description. 
    \item \textbf{ToD (No SD)}: We remove self-deliberation (SD) to test the impact of iterative retrieval based on debate progression. We do so by prompting the model with title, abstract, and introduction of each paper instead of retrieving based on the subtopic. 
\end{itemize}


\section{Evaluation Metrics}
\label{appendix:eval_metrics}
\par The same domain-experts from Section \ref{sec:dataset} manually evaluate each sample in-depth, assessing various qualities of the summaries:

\begin{itemize}
    \item \textbf{Factuality}: \textit{How factual is the summary?} Each sentence is given a 1/0 binary score for factual or not, and the scores are averaged across the summary.
    \item \textbf{Completeness}: \textit{Is the summary comprehensive and complete?} This is evaluated using the following Likert scale:
    \begin{enumerate}
        \setcounter{enumi}{-1}
        \item Not at all, the summary misses (MULTIPLE) major points.
        \item No, the summary misses a (SINGULAR) major point.
        \item Somewhat, the summary misses minor points.
        \item Yes, the summary covers the major points, but still is not what I would expect.
        \item Very comprehensive, the summary covers the major points.
    \end{enumerate}
    \item \textbf{Contextualization}: 
    \textit{Does the summary explain and/or justify the posed similarities/differences between the papers, as opposed to just mentioning them?}
    \begin{enumerate}
        \setcounter{enumi}{-1}
        \item Not at all, the summary is simply extractive-- it just seems to take different subtopics from each paper and doesn’t synthesize them– no justification behind similarities and differences.
        \item No, the summary attempts at some level of synthesis, but it is not meaningful.
        \item Somewhat, the summary attempts at synthesizing at most one point.
        \item Yes, the summary contains meaningful synthesis but only for a minority of points.
        \item  Strongly contextualized, the summary contains meaningful synthesis across all major points.
    \end{enumerate}
\end{itemize}

\section{Domain-Expert Profiles}
\label{appendix: annotators}
\par Given that novelty comparison between papers is a highly specialized and expensive task, we gather five domain experts to both collect and annotate our dataset, as well as evaluate Tree-of-Debate's generated summaries based on their respective samples. Each domain expert is a graduate student with $3+$ years of research experience in a specialized area:
\begin{enumerate}
    \item \textbf{Domain Expert \#1:} A third-year PhD in computer science with ten publications; research expertise is text mining and data mining.
    \item \textbf{Domain Expert \#2:} A third-year PhD in aerospace engineering with two publications; research expertise is in electric propulsion.
    \item \textbf{Domain Expert \#3:} A second-year PhD (with two years of a research-track Masters) in electrical engineering with four publications; research expertise is in in-memory computing and wireless communications.
    \item \textbf{Domain Expert \#4:} A first-year PhD (with two years of a research-track Masters) in computer science with six publications; research expertise is data-efficient natural language processing.
    \item \textbf{Domain Expert \#5:} A first-year PhD (with two years of a research-track Masters) in computer science with twenty-five publications; research expertise is large language model training and efficiency.

\input{latex/dataset}
\input{latex/case_study_summary_appendix}

\end{enumerate}