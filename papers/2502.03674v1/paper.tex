\documentclass{article}

\leftmargin=0in \rightmargin=0in 
\oddsidemargin=-0in 
\textwidth=6in \textheight=9in 
\topmargin=0in \headheight=0in \headsep=0in

\renewcommand{\baselinestretch}{1.0}
\providecommand{\keywords}[1]{\textbf{\textit{Keywords --}} #1}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage{comment}
%\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage{graphicx}
\usepackage{lineno}
%\usepackage{natbib}
%\usepackage[table,xcdraw]{xcolor}
\usepackage{url}
\usepackage{tabularray}
\usepackage[utf8]{inputenc}
\usepackage{subcaption}
%\usepackage{longtable}



\definecolor{dr}{rgb}{0.7,0,0}
\definecolor{dg}{rgb}{0,0.4,0}
\definecolor{db}{rgb}{0,0.2,0.8}
\newcommand{\cmt}[1] {\noindent{\color{db} #1}}
\newcommand{\XY}[1] {\noindent{\color{dr} {\bf XY}: #1}}
\newcommand{\resp}[1] {\noindent{\color{dg} #1}}


%\setlength{\bibsep}{1pt}



\title{An Empirical Study of Methods for Small Object Detection from Satellite Imagery}

\author{Xiaohui Yuan, Aniv Chakravarty, Lichuan Gu, Zhenchun Wei, Elinor Lichtenberg, \\Tian Chen}
\date{}



\begin{document}

\maketitle
\begin{abstract}
This paper reviews object detection methods for finding small objects from remote sensing imagery and provides an empirical evaluation of four state-of-the-art methods to gain insights into method performance and technical challenges. In particular, we use car detection from urban satellite images and bee box detection from satellite images of agricultural lands as application scenarios. Drawing from the existing surveys and literature, we identify several top-performing methods for the empirical study. Public, high-resolution satellite image datasets are used in our experiments.
\end{abstract}


%\linenumbers



\section{Introduction}
Object detection provides cues for many computer vision applications such as object recognition and tracking movements of targets in videos~\cite{2021Yuan,2017Zhou,2011Yuan}. There are many learning-based methods developed in recent years such as You Only Look Once (YOLO)~\cite{2016Redmon}, Single Shot MultiBox Detection (SSD)~\cite{2016Liu}, and derivations of Region-Based Convolutional Neural Networks (R-CNN)~\cite{2014Ross} (e.g., Faster R-CNN~\cite{2016Shaoqing} and Cascade R-CNN~\cite{2018Cai}). However, the application of object detection methods for satellite imagery analysis faces unique challenges. One of them being small object detection. More specifically, relatively small objects in a large field of view and multiple instances with ambiguous spectral features make the object detection task challenging. 

Detecting objects from images and videos is a long-standing computer vision problem and many methods have been developed in the past decades, from sliding window strategies to deep learning methods. To understand the technical landscape and the open challenges, surveys were conducted and several papers were published on the review of the methods~\cite{2019Zou, 2023Amjoub}. Most recently, Sun et al.~\cite{2024Sun} reviewed deep learning-based object detection methods including Convolution Neural Network-based and transformer-based methods. Garcia et al.~\cite{2023Garcia} reviewed deep network methods focused on the iterative labeling step in model training. Huang et al.~\cite{2023Huang} focused on the few-shot and self-supervised learning methods that deal with learning from unlabeled data. Gui et. al.~\cite{2024Gui} reviewed deep learning methods for object detection, emphasizing approaches addressing data and label limitations. Liu et al.~\cite{2021Liu} surveyed deep learning methods for small object detection from aerial imagery. 
These surveys provide a broad view of the recent technological aspects of object detection methods, as well as their chronological and geographical evolution. Yet, benchmarking these methods is missing. In particular, evaluating how the existing methods perform for the emerging problem of detecting small but critical objects in complex and large views of satellite imagery requires investigation. In addition, such small object detection tasks are often complicated by the lack of examples. 

In this paper, we review object detection methods for finding small objects from remote sensing imagery and provide an empirical evaluation to gain insights into method performance and technical challenges. In particular, we use car detection from urban satellite images and bee box detection from satellite images of agricultural lands as application scenarios. Drawing from the existing surveys and literature, we identify several top-performing methods for the empirical study. Two public, high-resolution satellite image datasets (xView and SkySat imagery) are used in our experiments. 

The rest of this paper is organized as follows:
Section~\ref{sec:RelatedWork} reviews the related studies that document recent object detection methods and provide a rationale of top performing methods for the empirical study. 
Section~\ref{sec:Method} presents our methodology of empirical study. 
Section~\ref{sec:Results} discusses our experimental results and highlights the open issues of detecting small objects. 
Section~\ref{sec:Conclusion} concludes this paper with a summary. 




\section{Related Work
\label{sec:RelatedWork}}


Since many object detection methods have been developed in recent years due to the explosive advances of deep networks, survey studies were conducted that cover many aspects of this field. Liu et al.~\cite{2021Liu} reviewed deep learning methods for small object detection in aerial imagery, where single convolution layers tend to struggle with small objects due to restrictions in relevant information obtained from deeper feature layers. Such features also lack contextual information at low resolutions. In addition, small object detection data often suffer from imbalanced foreground and background instances and insufficient positive examples. 
Kang et al.~\cite{2022Kang} opted for more generalized aerial and satellite imagery. The study mentioned the relation between spatial resolution and average precision score performance.
This paper focuses more on the details of the various datasets rather than any empirical evaluation of the performance of the different small object detection methods.
Li et al.~\cite{2022Li} provided an overview of deep learning methods for remote sensing images. Apart from the typical challenges mentioned in previous surveys, objects, such as cross-sea bridges and slender roads, may have extreme aspect ratios that hinder the detection. Over-cluttered background scenes and human annotation errors also contribute to performance loss.
Zou et al.~\cite{2023Zou} traced the chronological evolution of object detection methods. %While many methods were described, comparative observations with quantitative evaluations were not discussed.
Gui et. al.~\cite{2024Gui} reviewed deep learning methods for object detection emphasizing approaches addressing data and label limitations.

Besides documentation and categorization of methods, evaluation is included in several recent surveys. Huang et al.~\cite{2023Huang} focused on the few-shot and self-supervised learning methods that deal with learning from unlabeled data. Methods were evaluated on a backbone pre-trained on ImageNet for generic object detection.
Cheng et al.~\cite{2023Cheng} provide benchmarks for methods on large-scale datasets. Apart from the one and two-stage methods covered in previous review studies, they also consider anchor-free and query-based methods such as evaluating small objects on oriented boxes from the satellite imagery inclusive DOTA dataset. 
Sun et al.~\cite{2024Sun} reviewed the technical issues of deep learning-based object detection with CNN-based and transformer-based methods. The survey goes into detail with categorizing the different techniques from region proposal and bounding box regression-based methods in the CNN-based model to vision and end-to-end methods for transformers.
While the study includes comparative results for small-scale objects, the list of datasets included in their evaluation does not cover nadir satellite imagery.

Despite the extensive surveys on object detection methods, the review and discussion of detecting small objects from remote sensing imagery are missing. There are many real world applications such as detecting fixtures along urban streets and managed pollination near crop fields. The performance and challenges faced by the existing methods require an investigation. Small objects pose unique challenges in the design and training models. In fact, the definition of small objects is vague. %The paper has limited scope in terms of the number of models considered as well as a lack of criteria in selecting these methods for small object detection from remote sensing imagery. %There is also no mention of noise specific to satellite imagery such as surface reflectance and cloud coverage nor any evaluation of any real-world inference.
%These studies provide no comparative qualitative and quantitative evaluation.
Based on the limited empirical evaluation benchmarks for small object detection and the lack of study on the implication of model transferability in real-world applications, we conduct an empirical evaluation of the state-of-the-art object detection methods using satellite imagery and assess the generalization ability between datasets and applications.



\subsection{Preliminaries}

Convolution-based methods have proven to be capable of learning from features from complex datasets. The addition of multi-scale feature extraction through methods such as feature pyramid networks (FPNs) further enables the retention of information on lower-level features. These methods have been well-established for general object detection from images~\cite{2024Sun}. These methods also offer a decent balance between speed and accuracy. Additionally, anchor-based methods provide additional improvements for detecting smaller objects.

Among the one-stage anchor-free approaches, YOLO~\cite{2016Redmon} models have been improving in terms of accuracy and computation time for small object detection with each successive version and are effective due to feature fusion at multi-scale. The recent version implements an attention method akin to transformer models to further improve their detection capabilities. The second single-stage approach is the well-established SSD~\cite{2016Liu} model that offers a grid-based approach with default anchored boxes at different scales that offer a balance between accuracy and speed. The SSD model still maintains relevance for small object detection due to its simplicity and speed in terms of fine-tuning a model for small object detection at a lower computational overhead from additional clustering operations. For two-stage approaches, we look at two variants of region proposal-based R-CNN. Faster R-CNN~\cite{2016Shaoqing} offers high accuracy potential balanced with speed and Cascade R-CNN~\cite{2018Cai} employs an adaptive threshold with its multi-stage approach to handle misclassifications at lower thresholds in images with dense objects and noise.

\begin{table}[!htb]
\begin{center}
\caption{Properties of the comparative methods}
\begin{tabular}{c|cccccc}
\hline
Method&Localization&Anchored&Type&Architecture \\
\hline\hline
YOLOv11&one-stage&free&Box Regression&Backbone,Neck,Head\\
SSD&one-stage&anchored&Box Regression&Backbone, Extra convolutions\\
Faster R-CNN&two-stage&anchored&Region Proposal&Backbone,RPN,ROI Head\\
Cascade R-CNN&two-stage&anchored&Region Proposal&Backbone,RPN,Cascaded Head\\
\hline
\end{tabular}\label{tb:architecture overview}
\end{center}
\end{table}



\subsubsection{YOLO}
\label{sec:YOLO}

The YOLO models frame object detection as a regression problem where a single convolutional network predicts objects' bounding boxes and probabilities. While the main architecture initially consisted of convolutional and max pooling layers, it has since expanded to a more modular block structure to improve performance. The models from version 4 onward, adopted a backbone-neck-head structure. \cref{fig:yoloarch} depicts the general structure used since YOLOv5. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3in]{Yolo_arch.png}
\caption{YOLO Backbone-Neck-Head structure.}
\label{fig:yoloarch}
\end{figure}

While there have been further alterations to the internal blocks of the architecture, the general structure remains relatively the same. The backbone is customized from the DarkNet53 structure that has convolutional layers having kernels of varying sizes and strides. The convolutional layers represent blocks with batch normalization and SiLU activation. The k3 blocks have a kernel size of 3, stride 2, and padding of 1. The k6 blocks have a kernel size 6, stride 2, and padding 2. The main blocks in the YOLO model are the cross-stage partial connection (CSP) blocks that split convolution feature maps to be processed in parallel before aggregation. A c3 block consists of a series of bottleneck layers between the convolution layers. These bottleneck layers are added to reduce computation costs. Two different bottleneck configurations were used for the backbone and neck layers. The c3 block was later replaced with the more efficient c2f block in subsequent YOLO versions. The extracted features from the backbone go to the neck for refinement. The neck hosts refinement modules such as fast spatial pyramid pooling (SPPF) and the path aggregation network (PAN) added in version 10. Intermediate features obtained in the neck from the k1 layers are upsampled before concatenation. The head obtains the refined features and predicts feature maps at three scales based on objectness and confidence in its bounding boxes and classes. The latest update to the model is version 11 with improvements in computational efficiency and accuracy performance for object detection based on adaptability~\cite{2024Rahima}. The model architecture retains its backbone-neck-head structure replacing the c2f block with the computationally efficient transformer-based c3k2 block having a smaller kernel size. The c3k2 block consists of initial convolution passing intermediate features through a series of c3k layers. The output feature from the final c3k layer is concatenated with the initial convolution layer before applying a 1 $\times$ 1 convolution. The SPPF block extracts features through multiple scales for detecting objects of varying sizes. Additionally, this version introduces the cross-stage partial with spatial attention (CSPSA) block after the SPPF and before the upsampling layer to improve spatial attention with enhanced retention of specific regions of interest for accuracy across objects of varying sizes and positions.
%loss:varifocal loss(imbalance and uncertainty), dfl loss(estimate object categorization),CIOU loss(bounding box regressor)
The current version of YOLOv11 uses the same IoU based cross-entropy loss functions from YOLOv8 consisting of varifocal loss (VFL)~\cite{2021Zhang2}, distribution focal loss and CIoU for box regression.





\subsubsection{SSD}
\label{sec:SSD}

The SSD model employs multiple defined bounding boxes with anchor points in a grid format to predict their positions with convolutions across different scales. The overall architecture of SSD is shown in \cref{fig:ssdarch}. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{SSD_arch.png}
\caption{SSD model architecure}
\label{fig:ssdarch}
\end{figure}

The backbone used for the original version of SSD was the VGG-16 configuration. The reliance of such a model on predefined anchor boxes has limitations with new objects or small objects that do not conform to predefined boundaries.
SSD uses the sum of its localization loss through smooth L1  and confidence loss from softmax to determine the overall loss.


\subsubsection{Variants of R-CNN}
\label{sec:R-CNN}

Another approach to object detection involves region proposals based on the "recognition from regions" paradigm. These methods are derived from the R-CNN method which involves obtaining several possible proposed regions. The proposals come from the features extracted from the input image by passing them through the CNN backbone and then integrating class-specific linear support vector machines (SVMs) for classifying the regions. \cref{fig:rcnn} shows the general architecture of R-CNN. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{RCNN_arch.png}
\caption{R-CNN architecture}
\label{fig:rcnn}
\end{figure}

The base R-CNN method uses the selective search algorithm for obtaining region proposals. The proposals are then passed to an O-Net model having pre-trained VGG-16 weights. The object regions initially proposed are a rough estimate and typically require further refinement. Additionally, utilization of the O-Net comes with a higher computational overhead for improved accuracy compared to the alternative T-Net.


\subsubsection{Faster R-CNN}
\label{subsubsec: Faster R-CNN}

Since the first proposal of R-CNN, many derived models have been developed to reduce computational costs. Fast R-CNN~\cite{2015Ross} introduced a more streamlined approach with region of interest (RoI) based max pooling layers that provide fixed dimension feature maps to be passed to the network. Truncated single-value decomposition is added for faster computations of these RoIs. The training stage uses a multi-scale approach for approximate scale invariance while the testing and inference stages use the image pyramid approach. Extending Fast R-CNN, Faster R-CNN uses a GPU-based region proposal network (RPN) with a training scheme that alternates between region proposal and fine-tuning tasks. The RPN works by using a sliding window with a base assumption that both Fast R-CNN and RPN share a common convolution layer structure. Reference boxes called anchors are generated at the center of each sliding window.~\cref{fig:fasterrcnn} shows the overall workflow of the Faster R-CNN method. The loss function is a multitask loss function that involves classification and regression losses of predicted proposal bounding boxes with the ground truth.

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{FasterRCNN_wrkflow.png}
\caption{Faster R-CNN workflow}
\label{fig:fasterrcnn}
\end{figure}


\subsubsection{Cascade R-CNN}
\label{subsubsec: Cascade R-CNN}

One major drawback of these methods is the handling of thresholds based on overlap with intersection over union (IoU) metric. This leads to a trade-off between noise and degradation in detection performance. This method extends Faster R-CNN's two-stage to a multi-stage approach with specialized regressors and sequential refinement where each consecutive pooling operation takes the refined proposals from box regression of the previous stage with the feature maps obtained from the backbone in a cascaded fashion shown in \cref{fig:cascadercnn}. RPN generally favors low-quality proposals which nullify any learning on high-quality class proposals. Resampling based on the cascade regression helps avoid such issues. 

The loss function on Cascade-RCNN is based on optimizing the threshold used with cross entropy as classification loss and smoothing L1 as localization loss.

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{CascadeRCNN_arch.png}
\caption{Cascade R-CNN workflow}
\label{fig:cascadercnn}
\end{figure}

\section{Small Object Detection and Method}
\label{sec:Method}


A small object is considered to have a coverage area consisting of a few pixels. The threshold considered in terms of the area is relative to the dataset and task. For example, the definition of small objects according to the MS COCO~\cite{2014Lin} dataset that consists of an average image size of 640 $\times$ 480 is any object smaller than their defined smallest bounding box of 32 $\times$ 32 pixels. However, a small object can also be defined relative to the image size~\cite{2020Nguyen}. Depending on the spatial resolution of the satellite imagery, these objects may be smaller than one pixel at minimum or a cluster of four to five pixels if each pixel is sub-1m resolution. Detecting such small objects becomes a challenge due to the inability of convolutional kernels to pick up on any distinct features from such objects that may be ignored as noise or part of the background leading to misclassifications. Defining a small object based on area coverage is also not ideal as we obtain more refined, high-resolution images that would increase the pixel count and make objects easier to identify. For example, we might have a hard time finding vehicles in 0.5m resolution images but they can be easily identified in 0.3m resolution images~\cref{fig:smallObject} due to their distinct features being more prominent. 
\begin{figure}[!htb]
\centering
\begin{tabular}{@{}cc}
\includegraphics[height=1.5in]{SkySat_car_05.png}&
\includegraphics[height=1.5in]{xView_car_03.png}
\end{tabular}
\caption{Automobiles at different spatial resolutions. Left: a SkySat image at 0.5-meter spatial resolution; Right: an xView image at 0.3-meter spatial resolution. 
\label{fig:smallObject}}
\end{figure}

Therefore, we define a small object as an object in the image within a few pixels within 1$\%$ of overall pixel count relative to the spatial resolution to the point where no discernible features can be extracted and the object cannot be distinguished from other similar-sized objects without any additional external information.



\subsection{Evaluation Strategy
\label{subsec:evaluation}}

To determine the impact of these proposed multi-scale algorithms on detecting, we would need to consider our evaluation from various aspects based on the challenges faced in the task. 




\subsubsection{Feature Extraction}


Implementing an effective feature extraction approach across multiple resolutions provides contextual and semantic information to be fused back into the model enriching the deeper-level features. The different multi-scale algorithms determine the extent of contextual information that can be retained in the final feature and would thereby affect the accuracy model. Efficient feature extraction will lead to better average precision.

The feature extraction methods are typically represented by the backbone architecture of the model. For this study, we evaluate the effectiveness of feature extraction of the default backbones and the strategies of these methods. A good backbone selection determines the amount of information that can be obtained from the final feature layer as successive convolutions lead to the down-sampling of the image. Having a balance between its down-sampling strategy and feature information retention.

We correlate the feature maps extracted from the outputs of the backbone layer.
Base models will be trained with baseline parameters to consider for 3 scenes having different landscapes. The results from the different models 


\subsubsection{Anchor Size}


The use of predefined anchors of certain sizes and anchor-less approaches have significant effects on the learning loss of models. YOLOv11 uses an anchor-less approach and performs better than the other three approaches. The size of the anchor based on the dataset further improves precision at the cost of additional computation overhead.
300 iterations


Faster R-CNN  was trained on the same parameters of iterations 10000, learning rate $1\times10^{-4}$, head score threshold of 0.5, batch size per image as 1 ,and anchor generation range is kept between 2 and 4 for small and 32 to 64 for large 

\subsubsection{Region Proposals}

For region proposal-based techniques the number and quality of the region proposals affect the coverage of the detection of small objects. Faster R-CNN and Cascade R-CNN both use region proposals from the RPN. Having more proposals may increase accuracy at the cost of computation. 

\subsubsection{Generalization Ability}

Provided the objects meet the definition, we observe how well these models can adapt to a different resolution dataset such as SkySat for finding bee boxes.



\subsubsection{Training}

Evaluation of these methods can be determined through experiments that showcase the impact of their respective methods. The key property common across all four methods is the ability to perform multi-scale feature extraction.
First, by performing small object detection on the targeted class of small vehicles named cars on the xView dataset, we evaluate the overall performance to determine anchor-free approaches.
Next we observe the effects of anchor box size on small object detection.
We then evaluate the quality of the region proposal between Faster R-CNN and Cascade R-CNN.

YOLOv11 was trained using 532 training images and 86 validation images trained for 600 epochs with an image size of 1120, batch size of 2, and default learning rate of 0.01.  

SSD model was trained on 532 images with 82 validation samples, image size of 300, learning rate of $10^{-4}$, momentum 0.9, weight decay $5 \times 10^{-4}$, gamma 0.1 and overlap threshold of 0.5. The anchor box size was kept between 2 to 351. Number of iterations 500

Faster R-CNN was trained on 532 images with 82 validation samples, head score threshold of 0.2, bounding box sizes of 2 and 4 augmented with an aspect ratio of 0.1,1 and 2. learning rate of $10^{-4}$, $10^5$ iterations with a batch size of 1.

Cascade R-CNN used the same training and validation split as SSD and Faster R-CNN with a base learning rate of $5 \times 10^{-4}$, RoI head batch size of 128 trained for 500 iterations.

All models use their respective default backbones of  c3k2, VGG-16, and ResNet-50.




\subsection{Datasets}
\label{subsec:datasets}
%single table for dataset distributions only if they have any significance in the evaluation process


Our evaluation conducted using two datasets: xView and SkySat.
The xView dataset~\cite{2018lam} is a collection of satellite imagery from WorldView-3 at 0.3m pixel resolution and offers a standard and publicly accessible high resolution dataset with over a million objects for tasks related to small object detection. While xView consists of 60 classes of objects, we use xView as our baseline dataset for initial object detection to identify cars in the satellite image. The 16 types of different cars from small vehicles to trucks with and without backs get pushed to the same class making it a binary class object detection with the background. Data quality assessment for the dataset was conducted by the authors through three stages to maintain the quality of the dataset.

\begin{table}[!htb]
\begin{center}
\caption{dataset description}
\begin{tabular}{c|cccccc}
\hline
Dataset& \# images& resolution& \# train & \# val. \\
\hline\hline
xView&1413&0.3&847&282\\
SkySat&30&0.5&25&5\\
%SkySat (syn) &&0.5\\
\hline
\end{tabular}\label{tb:xView description}
\end{center}
\end{table}

The SkySat program imagery data are obtained from high-resolution Earth imaging satellites maintained by Planet Labs. These images have a pixel resolution of 0.5m obtained from 21 satellites that orbit around the planet at different altitudes. Our target objects from these images are small bee boxes that appear in dimensions from around 5 to 18 inches clustered close to each other in lines next to fields or within barren fields. From the satellite imagery perspective, our criteria would be around 5 to 8 pixels next to farmlands. The region selected for this study is Van Buren County, Michigan in 2018. This was due to an abundance of blueberry crops grown in their fields, which provided an improved chance of locating apiary bee boxes next to fields for pollination from April to May.

With the limited number of images we split the dataset into 25 images for the training set and 5 images for the validation set. We then perform 6-fold cross-validation to evaluate the performance of the trained models on the new dataset with results listed in \cref{tb:quantitative results}.



\section{Results and Discussion} 
\label{sec:Results}
%


The baseline results from the trained models show that the YOLOv11, SSD, Faster R-CNN, and Cascade R-CNN fine tuned on xView and SkySat datasets are shown in~\cref{tb:quantitative results}. We observe that the anchor-free approaches such as Cascade R-CNN and Yolov11 achieve better at $50\%$ overlap. The YOLOv11 model can detect more objects above the $75\%$ overlap. The grid-based SSD suffers from the overall range of the bounding boxes and threshold selection from multiple predictions. Faster R-CNN provides better accuracy than SSD due to the region proposals which can be observed from the qualitative results in~\cref{fig:qualitative results}


Hardware used is a 24GB VRAM RTX 3090 GPu, 16 Intel core i7-10700K CPU, 31GB RAM, Labelstudio~\cite{2025Tkachenko} was used to annotate SkySat images and Python with pytorch was the main platform for building the networks. Common metrics of intersection over union (IoU) based on mean average precision at different thresholds of 50 and 75 are used to determine how well the boxes align in terms of detection.


\subsection{Feature extraction}

\begin{table}[!htb]
\centering
\caption{Performance of feature extraction techniques.}
\begin{tabular}{c|ccccc}
\hline
Method&Backbone&$AP_aS$&$AP_bS$&$AP_cS$&$AP_N$\\
\hline\hline
SPPF+C2PSA&c3k2&&&&\\
convolution filter&VGG-16&&&&\\
FPN&ResNet-50&&&&\\
\hline
\end{tabular}\label{tb:Effective feature extraction}
\end{table}



\subsection{Anchor size}

\begin{table}[!htb]
\centering
\caption{Impact of anchor size on the detection performance}
\begin{tabular}{c|cccc}
\hline
Model&Anchor Size&AP&$AP_{50}$&$AP_{75}$\\
\hline\hline
SSD&30-315&&&\\
SSD&5-35&0.02&&\\
Faster R-CNN+FPN&32-64&0.01&0.04&0.004\\
Faster R-CNN+FPN&2-4&0.01&0.03&0.002\\
\hline
\end{tabular}\label{tb:Anchor sizes}
\end{table}



\subsection{Region proposal}

\begin{table}[!htb]
\centering
\caption{Quality of region proposals}
\begin{tabular}{c|cccc}
\hline
Model&Method&AP&$AP_{50}$&$AP_{75}$\\
\hline\hline
Faster R-CNN&RPN&&&\\
Cascade R-CNN&RPN+refinement&&&\\
\hline
\end{tabular}\label{tb:Effective feature extraction}
\end{table}



\subsection{Generalization across Datasets and Applications}

\begin{table}[!htb]
\centering
\caption{Performance}
\begin{tabular}{c|c|ccccccc}
\hline
Dataset& Method&Backbone&Precision&Recall&F1&mAP&$mAP_{50}$&$mAP_{75}$  \\
\hline\hline
%one stage
& YOLOv11~\cite{yolo11_ultralytics}&C3k2&0.43&0.11&0.16&-&0.25&0.09\\
xView & SSD&VGG-16&-&-&-&0.03&0.00&0.00\\
& Faster R-CNN&ResNet-50&-&-&-&0.03&0.14&0.009\\
& Cascade R-CNN&ResNet-50&-&-&-&0.15&0.49&0.008\\
\hline
& YOLOv11~\cite{yolo11_ultralytics}&C3k2&0.07$\pm$0.12&0.10&0.07&-&0.06&0.02\\
SkySat& SSD&VGG-16&&&&&\\
%two stage
& Faster R-CNN&ResNet-50&&&&&&\\
& Cascade R-CNN&ResNet-50&&&&&\\
\hline
\end{tabular}\label{tb:quantitative results}
\end{table}

\begin{figure}[!htb]
\centering
\begin{tabular}{@{}p{1in}p{1in}p{1in}p{1in}p{1in}}
\includegraphics[height=2.1in]{gt_20.png}&
\includegraphics[height=2.1in]{Yolov11_xView_1120_20.png}&
\includegraphics[height=2.1in]{SSD300_xView_500_1_315_34.png}&
\includegraphics[height=2.1in]{FasterRCNN_xView_2_4_41.png}&
\includegraphics[height=2.1in]{Cascade10k0_5_41.png}
\end{tabular}
\caption{Results of car detection. From left to right: Ground Truth, results of Yolo v11, SSD 300, Faster R-CNN, and Cascade R-CNN. 
\label{fig:qualitative results}}
\end{figure}






\section{Conclusion \label{sec:Conclusion}}





%\bibliographystyle{unsrt}
\bibliography{ref}
\begin{thebibliography}{10}

\bibitem{2021Yuan}
Xiaohui Yuan, Jianfang Shi, and Lichuan Gu.
\newblock A review of deep learning methods for semantic segmentation of remote sensing imagery.
\newblock {\em Expert Systems with Applications}, 169:114417, 2021.

\bibitem{2017Zhou}
Yun Zhou, Jianghong Han, Xiaohui Yuan, Zhenchun Wei, and Richang Hong.
\newblock Inverse sparse group lasso model for robust object tracking.
\newblock {\em IEEE Transactions on Multimedia}, 19:1798--1810, 2017.

\bibitem{2011Yuan}
Xiaohui Yuan and Vaibhav Sarma.
\newblock Automatic urban water-body detection and segmentation from sparse alsm data via spatially constrained model-driven clustering.
\newblock {\em IEEE Geoscience and Remote Sensing Letters}, 8(1):73--77, 2011.

\bibitem{2016Redmon}
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 779--788, 2016.

\bibitem{2016Liu}
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander~C. Berg.
\newblock {\em SSD: Single Shot MultiBox Detector}, page 21–37.
\newblock Springer International Publishing, 2016.

\bibitem{2014Ross}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic segmentation, 2014.

\bibitem{2016Shaoqing}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal networks, 2016.

\bibitem{2018Cai}
Zhaowei Cai and Nuno Vasconcelos.
\newblock Cascade r-cnn: Delving into high quality object detection.
\newblock In {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6154--6162, 2018.

\bibitem{2019Zou}
Xinrui Zou.
\newblock A review of object detection techniques.
\newblock In {\em 2019 International Conference on Smart Grid and Electrical Automation (ICSGEA)}, pages 251--254, 2019.

\bibitem{2023Amjoub}
Ayoub~Benali Amjoud and Mustapha Amrouch.
\newblock Object detection using deep learning, cnns and vision transformers: A review.
\newblock {\em IEEE Access}, 11:35479--35516, 2023.

\bibitem{2024Sun}
Yibo Sun, Zhe Sun, and Weitong Chen.
\newblock The evolution of object detection methods.
\newblock {\em Engineering Applications of Artificial Intelligence}, 133:108458, 2024.

\bibitem{2023Garcia}
Dibet Garcia, JoÃ£o Carias, Telmo AdÃ£o, Rui Jesus, Antonio Cunha, and Luis~G. MagalhÃ£es.
\newblock Ten years of active learning techniques and object detection: A systematic review.
\newblock {\em Applied Sciences}, 13(19), 2023.

\bibitem{2023Huang}
Gabriel Huang, Issam Laradji, David Vázquez, Simon Lacoste-Julien, and Pau Rodríguez.
\newblock A survey of self-supervised and few-shot object detection.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(4):4071--4089, 2023.

\bibitem{2024Gui}
Shengxi Gui, Shuang Song, Rongjun Qin, and Yang Tang.
\newblock Remote sensing object detection in the deep learning era—a review.
\newblock {\em Remote Sensing}, 16(2), 2024.

\bibitem{2021Liu}
Yang Liu, Peng Sun, Nickolas Wergeles, and Yi~Shang.
\newblock A survey and performance evaluation of deep learning methods for small object detection.
\newblock {\em Expert Systems with Applications}, 172:114602, 2021.

\bibitem{2022Kang}
Junhyung Kang, Shahroz Tariq, Han Oh, and Simon~S. Woo.
\newblock A survey of deep learning-based object detection methods and datasets for overhead imagery.
\newblock {\em IEEE Access}, 10:20118--20134, 2022.

\bibitem{2022Li}
Zheng Li, Yongcheng Wang, Ning Zhang, Yuxi Zhang, Zhikang Zhao, Dongdong Xu, Guangli Ben, and Yunxiao Gao.
\newblock Deep learning-based object detection techniques for remote sensing images: A survey.
\newblock {\em Remote Sensing}, 14(10), 2022.

\bibitem{2023Zou}
Zhengxia Zou, Keyan Chen, Zhenwei Shi, Yuhong Guo, and Jieping Ye.
\newblock Object detection in 20 years: A survey.
\newblock {\em Proceedings of the IEEE}, 111(3):257--276, 2023.

\bibitem{2023Cheng}
Gong Cheng, Xiang Yuan, Xiwen Yao, Kebing Yan, Qinghua Zeng, Xingxing Xie, and Junwei Han.
\newblock Towards large-scale small object detection: Survey and benchmarks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(11):13467--13488, 2023.

\bibitem{2024Rahima}
Rahima Khanam and Muhammad Hussain.
\newblock Yolov11: An overview of the key architectural enhancements, 2024.

\bibitem{2021Zhang2}
Haoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sunderhauf.
\newblock Varifocalnet: An iou-aware dense object detector.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 8514--8523, June 2021.

\bibitem{2015Ross}
Ross Girshick.
\newblock Fast r-cnn, 2015.

\bibitem{2014Lin}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, {\em Computer Vision -- ECCV 2014}, pages 740--755, Cham, 2014. Springer International Publishing.

\bibitem{2020Nguyen}
Nhat-Duy Nguyen, Tien Do, Thanh~Duc Ngo, and Duy-Dinh Le.
\newblock An evaluation of deep learning methods for small object detection.
\newblock {\em Journal of Electrical and Computer Engineering}, 2020(1):3189691, 2020.

\bibitem{2018lam}
Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and Brendan McCord.
\newblock xview: Objects in context in overhead imagery, 2018.

\bibitem{2025Tkachenko}
Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov.
\newblock Label studio: Data labeling software, 2020-2025.
\newblock Open source software available from https://github.com/HumanSignal/label-studio.

\bibitem{yolo11_ultralytics}
Glenn Jocher and Jing Qiu.
\newblock Ultralytics yolo11, 2024.

\end{thebibliography}


\end{document}
