\section{Related Work
\label{sec:RelatedWork}
}


Since many object detection methods have been developed in recent years due to the explosive advances of deep networks, survey studies were conducted that cover many aspects of this field. Liu et al.~\cite{2021Liu} reviewed deep learning methods for small object detection in aerial imagery, where single convolution layers tend to struggle with small objects due to restrictions in relevant information obtained from deeper feature layers. Such features also lack contextual information at low resolutions. In addition, small object detection data often suffer from imbalanced foreground and background instances and insufficient positive examples. 
Kang et al.~\cite{2022Kang} opted for more generalized aerial and satellite imagery. The study mentioned the relation between spatial resolution and average precision score performance.
This paper focuses more on the details of the various datasets rather than any empirical evaluation of the performance of the different small object detection methods.
Li et al.~\cite{2022Li} provided an overview of deep learning methods for remote sensing images. Apart from the typical challenges mentioned in previous surveys, objects, such as cross-sea bridges and slender roads, may have extreme aspect ratios that hinder the detection. Over-cluttered background scenes and human annotation errors also contribute to performance loss.
Zou et al.~\cite{2023Zou} traced the chronological evolution of object detection methods. %While many methods were described, comparative observations with quantitative evaluations were not discussed.
Gui et. al.~\cite{2024Gui} reviewed deep learning methods for object detection emphasizing approaches addressing data and label limitations.

Besides documentation and categorization of methods, evaluation is included in several recent surveys. Huang et al.~\cite{2023Huang} focused on the few-shot and self-supervised learning methods that deal with learning from unlabeled data. Methods were evaluated on a backbone pre-trained on ImageNet for generic object detection.
Cheng et al.~\cite{2023Cheng} provide benchmarks for methods on large-scale datasets. Apart from the one and two-stage methods covered in previous review studies, they also consider anchor-free and query-based methods such as evaluating small objects on oriented boxes from the satellite imagery inclusive DOTA dataset. 
Sun et al.~\cite{2024Sun} reviewed the technical issues of deep learning-based object detection with CNN-based and transformer-based methods. The survey goes into detail with categorizing the different techniques from region proposal and bounding box regression-based methods in the CNN-based model to vision and end-to-end methods for transformers.
While the study includes comparative results for small-scale objects, the list of datasets included in their evaluation does not cover nadir satellite imagery.

Despite the extensive surveys on object detection methods, the review and discussion of detecting small objects from remote sensing imagery are missing. There are many real world applications such as detecting fixtures along urban streets and managed pollination near crop fields. The performance and challenges faced by the existing methods require an investigation. Small objects pose unique challenges in the design and training models. In fact, the definition of small objects is vague. %The paper has limited scope in terms of the number of models considered as well as a lack of criteria in selecting these methods for small object detection from remote sensing imagery. %There is also no mention of noise specific to satellite imagery such as surface reflectance and cloud coverage nor any evaluation of any real-world inference.
%These studies provide no comparative qualitative and quantitative evaluation.
Based on the limited empirical evaluation benchmarks for small object detection and the lack of study on the implication of model transferability in real-world applications, we conduct an empirical evaluation of the state-of-the-art object detection methods using satellite imagery and assess the generalization ability between datasets and applications.



\subsection{Preliminaries}

Convolution-based methods have proven to be capable of learning from features from complex datasets. The addition of multi-scale feature extraction through methods such as feature pyramid networks (FPNs) further enables the retention of information on lower-level features. These methods have been well-established for general object detection from images~\cite{2024Sun}. These methods also offer a decent balance between speed and accuracy. Additionally, anchor-based methods provide additional improvements for detecting smaller objects.

Among the one-stage anchor-free approaches, YOLO~\cite{2016Redmon} models have been improving in terms of accuracy and computation time for small object detection with each successive version and are effective due to feature fusion at multi-scale. The recent version implements an attention method akin to transformer models to further improve their detection capabilities. The second single-stage approach is the well-established SSD~\cite{2016Liu} model that offers a grid-based approach with default anchored boxes at different scales that offer a balance between accuracy and speed. The SSD model still maintains relevance for small object detection due to its simplicity and speed in terms of fine-tuning a model for small object detection at a lower computational overhead from additional clustering operations. For two-stage approaches, we look at two variants of region proposal-based R-CNN. Faster R-CNN~\cite{2016Shaoqing} offers high accuracy potential balanced with speed and Cascade R-CNN~\cite{2018Cai} employs an adaptive threshold with its multi-stage approach to handle misclassifications at lower thresholds in images with dense objects and noise.

\begin{table}[!htb]
\begin{center}
\caption{Properties of the comparative methods}
\begin{tabular}{c|cccccc}
\hline
Method&Localization&Anchored&Type&Architecture \\
\hline\hline
YOLOv11&one-stage&free&Box Regression&Backbone,Neck,Head\\
SSD&one-stage&anchored&Box Regression&Backbone, Extra convolutions\\
Faster R-CNN&two-stage&anchored&Region Proposal&Backbone,RPN,ROI Head\\
Cascade R-CNN&two-stage&anchored&Region Proposal&Backbone,RPN,Cascaded Head\\
\hline
\end{tabular}\label{tb:architecture overview}
\end{center}
\end{table}



\subsubsection{YOLO}
\label{sec:YOLO}

The YOLO models frame object detection as a regression problem where a single convolutional network predicts objects' bounding boxes and probabilities. While the main architecture initially consisted of convolutional and max pooling layers, it has since expanded to a more modular block structure to improve performance. The models from version 4 onward, adopted a backbone-neck-head structure. \cref{fig:yoloarch} depicts the general structure used since YOLOv5. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3in]{Yolo_arch.png}
\caption{YOLO Backbone-Neck-Head structure.}
\label{fig:yoloarch}
\end{figure}

While there have been further alterations to the internal blocks of the architecture, the general structure remains relatively the same. The backbone is customized from the DarkNet53 structure that has convolutional layers having kernels of varying sizes and strides. The convolutional layers represent blocks with batch normalization and SiLU activation. The k3 blocks have a kernel size of 3, stride 2, and padding of 1. The k6 blocks have a kernel size 6, stride 2, and padding 2. The main blocks in the YOLO model are the cross-stage partial connection (CSP) blocks that split convolution feature maps to be processed in parallel before aggregation. A c3 block consists of a series of bottleneck layers between the convolution layers. These bottleneck layers are added to reduce computation costs. Two different bottleneck configurations were used for the backbone and neck layers. The c3 block was later replaced with the more efficient c2f block in subsequent YOLO versions. The extracted features from the backbone go to the neck for refinement. The neck hosts refinement modules such as fast spatial pyramid pooling (SPPF) and the path aggregation network (PAN) added in version 10. Intermediate features obtained in the neck from the k1 layers are upsampled before concatenation. The head obtains the refined features and predicts feature maps at three scales based on objectness and confidence in its bounding boxes and classes. The latest update to the model is version 11 with improvements in computational efficiency and accuracy performance for object detection based on adaptability~\cite{2024Rahima}. The model architecture retains its backbone-neck-head structure replacing the c2f block with the computationally efficient transformer-based c3k2 block having a smaller kernel size. The c3k2 block consists of initial convolution passing intermediate features through a series of c3k layers. The output feature from the final c3k layer is concatenated with the initial convolution layer before applying a 1 $\times$ 1 convolution. The SPPF block extracts features through multiple scales for detecting objects of varying sizes. Additionally, this version introduces the cross-stage partial with spatial attention (CSPSA) block after the SPPF and before the upsampling layer to improve spatial attention with enhanced retention of specific regions of interest for accuracy across objects of varying sizes and positions.
%loss:varifocal loss(imbalance and uncertainty), dfl loss(estimate object categorization),CIOU loss(bounding box regressor)
The current version of YOLOv11 uses the same IoU based cross-entropy loss functions from YOLOv8 consisting of varifocal loss (VFL)~\cite{2021Zhang2}, distribution focal loss and CIoU for box regression.





\subsubsection{SSD}
\label{sec:SSD}

The SSD model employs multiple defined bounding boxes with anchor points in a grid format to predict their positions with convolutions across different scales. The overall architecture of SSD is shown in \cref{fig:ssdarch}. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{SSD_arch.png}
\caption{SSD model architecure}
\label{fig:ssdarch}
\end{figure}

The backbone used for the original version of SSD was the VGG-16 configuration. The reliance of such a model on predefined anchor boxes has limitations with new objects or small objects that do not conform to predefined boundaries.
SSD uses the sum of its localization loss through smooth L1  and confidence loss from softmax to determine the overall loss.


\subsubsection{Variants of R-CNN}
\label{sec:R-CNN}

Another approach to object detection involves region proposals based on the "recognition from regions" paradigm. These methods are derived from the R-CNN method which involves obtaining several possible proposed regions. The proposals come from the features extracted from the input image by passing them through the CNN backbone and then integrating class-specific linear support vector machines (SVMs) for classifying the regions. \cref{fig:rcnn} shows the general architecture of R-CNN. 

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{RCNN_arch.png}
\caption{R-CNN architecture}
\label{fig:rcnn}
\end{figure}

The base R-CNN method uses the selective search algorithm for obtaining region proposals. The proposals are then passed to an O-Net model having pre-trained VGG-16 weights. The object regions initially proposed are a rough estimate and typically require further refinement. Additionally, utilization of the O-Net comes with a higher computational overhead for improved accuracy compared to the alternative T-Net.


\subsubsection{Faster R-CNN}
\label{subsubsec: Faster R-CNN}

Since the first proposal of R-CNN, many derived models have been developed to reduce computational costs. Fast R-CNN~\cite{2015Ross} introduced a more streamlined approach with region of interest (RoI) based max pooling layers that provide fixed dimension feature maps to be passed to the network. Truncated single-value decomposition is added for faster computations of these RoIs. The training stage uses a multi-scale approach for approximate scale invariance while the testing and inference stages use the image pyramid approach. Extending Fast R-CNN, Faster R-CNN uses a GPU-based region proposal network (RPN) with a training scheme that alternates between region proposal and fine-tuning tasks. The RPN works by using a sliding window with a base assumption that both Fast R-CNN and RPN share a common convolution layer structure. Reference boxes called anchors are generated at the center of each sliding window.~\cref{fig:fasterrcnn} shows the overall workflow of the Faster R-CNN method. The loss function is a multitask loss function that involves classification and regression losses of predicted proposal bounding boxes with the ground truth.

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{FasterRCNN_wrkflow.png}
\caption{Faster R-CNN workflow}
\label{fig:fasterrcnn}
\end{figure}


\subsubsection{Cascade R-CNN}
\label{subsubsec: Cascade R-CNN}

One major drawback of these methods is the handling of thresholds based on overlap with intersection over union (IoU) metric. This leads to a trade-off between noise and degradation in detection performance. This method extends Faster R-CNN's two-stage to a multi-stage approach with specialized regressors and sequential refinement where each consecutive pooling operation takes the refined proposals from box regression of the previous stage with the feature maps obtained from the backbone in a cascaded fashion shown in \cref{fig:cascadercnn}. RPN generally favors low-quality proposals which nullify any learning on high-quality class proposals. Resampling based on the cascade regression helps avoid such issues. 

The loss function on Cascade-RCNN is based on optimizing the threshold used with cross entropy as classification loss and smoothing L1 as localization loss.

\begin{figure}[!htb]
\centering
\includegraphics[width=3.25in]{CascadeRCNN_arch.png}
\caption{Cascade R-CNN workflow}
\label{fig:cascadercnn}
\end{figure}