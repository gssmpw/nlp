\section{Related Work}
\label{sec:related}
The most relevant related work is discussed within context throughout the manuscript. Here we provide details about connections to (and differences from) additional work on MST algorithms. 

\paragraph{MST algorithms with predictions.}
Our work shares high-level similarities with other work on learning-augmented and query-minimizing algorithms for MSTs. Erlebach et al.~\cite{erlebach2022learning} considered a setting where a (possibly erroneous) prediction is given for each edge weight in a graph (not necessarily a metric graph) and the goal is to minimize the number of (non-erroneous) edge weight queries in order to compute an exact MST. 
Berg et al.~\cite{berg2023online} considered an online setting where edge-weight predictions are all provided a priori but true weights are only revealed in an online fashion. After revealing a true weight, an irrevocable decision must be made as to whether to include the edge in the spanning tree or not. 
Bateni et al.~\cite{bateni2024metric} recently considered MST computations in metric graphs, in settings where one has access both to a weak oracle (providing a similar type of edge-weight prediction) and a strong oracle giving true distances. The authors focus on bounding the number of strong oracle queries needed to find an exact or approximate MST. These prior works share similarities with our work in their goal to minimize certain types of queries. However, they differ from our work in that the learning-augmented information takes the form of edge weight predictions, rather than an initial forest. These works also perform $\Omega(n^2)$ queries in the worst case, which is prohibitive for large $n$.
Our work is distinctive in its focus on subquadratic algorithms for approximate MSTs.

\paragraph{Algorithms for metric MST.}
Many previous papers focus on improving algorithmic guarantees for variants of the metric MST problem, especially for the special case of Euclidean distances. For $d$-dimensional Euclidean space when $d = O(1)$, an exact MST can be computed in $O(n^{2 - 2/(\lceil d/2 \rceil + 1) + \varepsilon})$ time~\cite{agarwal1990euclidean}, and a $(1+\varepsilon)$-approximate solution can be found in time $O(n \log n + (\varepsilon^{-2} \log^2 \frac{1}{\varepsilon})n)$ time, where the big-$O$ notation hides constants of the form $O(1)^d$~\cite{arya2016fast}. For high dimensional spaces, there are also known approaches for obtaining $c$-approximate Euclidean MSTs where subquadratic runtimes depend on the value of the desired approximation factor $c > 1$~\cite{har2013euclidean,harpeled2012ann}. There are also many recent improved theoretical results for parallel and streaming variants of the metric MST problem~\cite{jayaram2024massively,chen2022new,chen2023streamingemst,wang2021fast,azarmehr2024massively,march2010fast}, most of which again focus on the Euclidean case.

\paragraph{Partitioning heuristics for MSTs.}
There are several existing divide-and-conquer heuristics for finding an approximate Euclidean MST~\cite{chen2013clustering,zhong2015fast,mishra2020efficient,jothi2018fast}. Similar to our approach, these begin by partitioning the data into smaller components, using $k$-means~\cite{zhong2015fast,jothi2018fast}, finding components of a $k$-NN graph~\cite{chen2013clustering}, or via recursive partitioning~\cite{mishra2020efficient}. An MST for the entire dataset is obtained using various heuristics for connecting components internally and then connecting disjoint components. This often involves computing an MST on some form of coarsened graph as a substep. Despite high-level similarities, these methods differ from our forest completion framework in that they focus exclusively on Euclidean space, an assumption that is essential for the partitioning schemes used and the techniques for connecting components. The other major difference is that these approaches do not attempt to provide any type of approximation guarantee, which is our main focus.

\paragraph{Alternate uses of the acronym MFC.}
To avoid possible confusion, we highlight two concepts that relate to minimum spanning trees and use the acronym MFC to denote something distinct from \textsc{Metric Forest Completion}. Liu et al.~\cite{liu2014supervised} used MFC to denote \textit{MST-based Feature Clustering}, a technique they used for feature selection. Although this involved computing a minimum spanning tree, it did not consider any notion of forest completion. Prior to this, Kor et al.~\cite{kor2011tight} studied distributed algorithms for verifying whether a tree is a minimum spanning tree of a graph. For this problem, an input graph is assumed to be partitioned among many processors, and the candidate tree consists of a collection of marked edges that are partitioned into a so-called \textit{MST fragment collection} (MFC). These fragments bear a cursory resemblance with the forest used as input to our \textsc{Metric Forest Completion} problem. However, Kor et al.~\cite{kor2011tight} do not focus on metric graphs or completing initial forests. Furthermore, their problem and approach rely on evaluating a full graph in memory, which is intractable in our setting.

