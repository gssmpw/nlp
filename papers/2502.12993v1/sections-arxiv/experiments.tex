\section{Experiments}
We run a large number of numerical experiments on synthetic and real-world datasets to show the practical utility of our MFC framework.
The overall utility of our approach relies both on the performance of \textsf{MFC-Approx} as well as our ability to obtain good initial forests. We therefore evaluate both aspects in practice. Our experiments are designed to address the following questions:
\begin{itemize}[leftmargin=30pt,itemsep = 0pt]
	\item[\textbf{Q1}.] How does this framework perform in terms of runtime and spanning tree cost?
	\item[\textbf{Q2}.] What $\gamma$-overlap (see Eq.~\ref{eq:gamma}) can be achieved in practice by scalable partitioning heuristics?
	\item[\textbf{Q3}.] How does practical performance compare with theoretical bounds in Theorem~\ref{thm:learning}?
	\item[\textbf{Q4}.] How do differences in the structure of the data and choice of distance metric affect performance?
\end{itemize}
As a summary of our findings, we obtain significant reductions in runtime and memory use over exact methods while obtaining very good approximation guarantees (addressing \textbf{Q1}), the $\gamma$ values tend to be very small and typically bounded by a small constant (addressing \textbf{Q2}), and the true approximation to the optimal solution is even better than our theory predicts (addressing \textbf{Q3}). Regarding \textbf{Q4}, these findings persist across a wide range of dataset types (e.g., point clouds, set data, strings) and metrics (Euclidean, Jaccard distance, Hamming distance, and edit distance) on both synthetic and real-world data. One of our key findings is that our framework performs especially well (in terms of $\gamma$-overlap and approximation ratios) on datasets where there is some inherent underlying clustering structure. This is particularly valuable given that clustering applications are a primary motivating use case for large-scale MST computations.


\subsection{Implementation details and experimental setup}
We implemented our algorithms in templated C++ code which allows for easy specialization of different distance metrics. Experiments were run on a research server with two AMD EPYC 7543 32-Core Processors and 1 TB of ram running Ubuntu 20.04.1. The code was compiled with clang version 20.0.0 with O3 using libc++ version 20.0.0. Our code is publicly available on GitHub.\footnote{\url{https://github.com/tommy1019/MetricForestCompletion}}

\paragraph{Generating initial forests.}
To generate initial forests, we first partition data points using the standard greedy 2-approximation algorithm for $k$-center clustering~\cite{gonzalez1985clustering}, as this is simple, fast, and works for arbitrary metrics. We consider results for a range of different 
 component numbers $t = k$.
After partitioning the data, we compute an exact minimum spanning tree for each component, mirroring an approach used by previous divide-and-conquer algorithms for large-scale Euclidean MST approximations~\cite{jothi2018fast,zhong2015fast,mishra2020efficient}. Computing exact MSTs for all $t$ components is often the most expensive step of our MSTC framework as it requires generating all edges inside each component. Even so, we find this leads to significant runtime improvements over applying the exact algorithm to the entire dataset, while achieving extremely good approximation ratios. 

\paragraph{Baseline and evaluation criteria.}
We compare against an exact MST computed by generating all ${n \choose 2}$ edge weights then running Kruskal's algorithm. The bottleneck in Kruskal's algorithm is sorting the $O(n^2)$ edge weights, leading to an $O(n^2 \log n)$ runtime. Although more sophisticated algorithms could achieve a runtime of $O(n^2)$ for arbitrary metrics, Kruskal's algorithm is still fast (optimal up to a log factor in terms of runtime) and has the added benefit of being simple to implement and use in practice. We note furthermore that any speed-up in our baseline algorithm for the full MST would also instantly improve the runtime of our MFC approach, since we use the same solver to find MSTs for the partitions in the initial forest. 

\begin{figure}[hbt!]
	\begin{center}
		\resizebox{\columnwidth}{!}{
			\includegraphics[page=1, width=0.4\paperwidth]{figures/unif_all_4_to_256.pdf}
		}
        \vspace{-12pt}
		$\begin{alignedat}{10}
			\textcolor[RGB]{229, 116, 100}{\bullet} \text{~$t=16$~} &~~~&
			\textcolor[RGB]{153, 154, 35}{\blacktriangle} \text{~$t=32$~} &~~~&
			\textcolor[RGB]{73, 178, 117}{\blacksquare} \text{~t=64~} &~~~&
			\textcolor[RGB]{67, 164, 241}{\boldsymbol{+}} \text{~$t=128$~} &~~~&
			\textcolor[RGB]{211, 108, 236}{\boldsymbol{\boxtimes}} \text{~$t=256$~} 
		\end{alignedat}$
	\end{center}
	% \vspace{-12pt}
	\caption{Results on synthetic uniform random data for dimensions $d \in \{4, 8, 16, 32, 256\}.$ Each point in each plot represents an average over $16$ sampled point clouds for a fixed $n$ and choice of component number $t$. Runtime ratio is the ratio between the runtime for the optimal MST algorithm divided by the runtime of our MFC framework (including initial forest generation). Cost ratio is the ratio between the spanning tree weight for our method and the optimal MST weight. The $\gamma$ upper bound is computed by comparing the initial forest overlap with the one optimal MST computed.}
	\label{fig:unif_grid_aux}
\end{figure}

We evaluate our MST pipeline (both the initial forest generation and the forest completion step) in terms of runtime, a bound on $\gamma$-overlap, and cost ratio (i.e., the spanning tree cost divided by the weight of an optimal MST). We find a bound $\bar{\gamma} \geq \gamma(\mathcal{P})$ on $\gamma$-overlap for our initial forest, obtained by computing the overlap between the initial forest and the MST found by the baseline.
This amounts to computing the ratio in Eq.~\eqref{eq:gamma}, except without optimizing over all possible minimum spanning trees in the denominator. When all edge weights are unique, this produces the exact value of $\gamma$. We therefore expect this to be an especially good approximation of $\gamma$ for our Euclidean point cloud datasets, since distances between random points tend to be unique (or nearly all unique).

\subsection{Experiments on Uniform Random Data}
\label{sec:nincreases}
We run a large number of experiments on uniform random data to illustrate the strong performance of our MFC framework in terms of runtime and spanning tree quality (addressing \textbf{Q1} and \textbf{Q2}) in a simple controlled setting. We consider random points clouds of size $n$ in $d$-dimensional Euclidean space where the value for a point in each dimension is drawn uniformly from $[-1,1]$. Figure~\ref{fig:unif_grid_aux} show results for $d \in \{4,8,16,32,256\}$ as $n$ increases from 1000 to 30000, using a different color for each choice of component number $t \in \{16,32,64,128,256\}$ for which we ran our MFC framework. 



% \begin{figure}[t!]
% 	\begin{center}
% 		\resizebox{\columnwidth}{!}{
% 			\includegraphics[page=1, width=0.4\paperwidth]{figures/unif_all_4_to_256.pdf}
% 		}
% 		$\begin{alignedat}{10}
% 			\textcolor[RGB]{229, 116, 100}{\bullet} \text{~$t=16$~} &~~~&
% 			\textcolor[RGB]{153, 154, 35}{\blacktriangle} \text{~$t=32$~} &~~~&
% 			\textcolor[RGB]{73, 178, 117}{\blacksquare} \text{~t=64~} &~~~&
% 			\textcolor[RGB]{67, 164, 241}{\boldsymbol{+}} \text{~$t=128$~} &~~~&
% 			\textcolor[RGB]{211, 108, 236}{\boldsymbol{\boxtimes}} \text{~$t=256$~} 
% 		\end{alignedat}$
% 	\end{center}
% 	\vspace{-10pt}
% 	\caption{Results on synthetic uniform random data for dimensions $d \in \{4, 8, 16, 32, 64, 256\}.$ Each point in each plot represents an average over $16$ sampled point clouds for a fixed $n$ and choice of component number $t$. Runtime ratio is the ratio between the runtime for the optimal MST algorithm divided by the runtime of our MFC framework (including initial forest generation). Cost ratio is the ratio between the spanning tree weight for our method and the optimal MST weight. The $\gamma$ upper bound is computed by comparing the initial forest overlap with the one optimal MST computed.}
% 	\label{fig:unif_grid_aux}
% \end{figure}

As $t$ increases, we see improvements in asymptotic speedups over the exact baseline algorithm (up to a 800x speedup for $d = 4$ and 35x speedup for $d = 128$). As $t$ increases the quality of the approximate spanning tree also decreases slightly, but the cost approximation ratio still remains very good (i.e., close to 1) in all cases. The $\gamma$-overlap bound $\bar{\gamma}$ tend to be small for low dimensions, but gets large as $d$ increases (e.g., $\bar{\gamma}$ close to 30 for $d = 256$). For a given $\bar{\gamma}$, Theorem~\ref{thm:learning} guarantees roughly a $(2\bar{\gamma} + 1)$-approximation. For $d = 4$ this approximation ranges from $3$ to $5$, while for $d = 256$ it ranges from $3$ to around $60$. 
Despite this, the cost ratios obtained in practice remain far below these bounds (always very close to 1), and even improve slightly as $d$ increases. This tells us first of all that in practice our approach greatly exceeds our theoretical bounds (addressing \textbf{Q3}). We conjecture that for these high-dimensional point clouds (which lack any underlying structure), there are a large number of spanning trees that are \textit{nearly} optimal in terms of weight but are structurally very different from an optimal spanning tree (which may be unique), which could lead to high ${\gamma}$ values despite our good cost ratios in practice.

\subsection{Improved Results on Clustered Data}
\begin{figure}[t!]
	\begin{center}
		Fashion-MNIST $~~d=784$
		\includegraphics[page=1, width=0.7\paperwidth]{figures/fashion_aux.pdf}
		$\begin{alignedat}{10}
			\textcolor[RGB]{229, 116, 100}{\bullet} \text{~$t=16$~} &~~~&
			\textcolor[RGB]{153, 154, 35}{\blacktriangle} \text{~$t=32$~} &~~~&
			\textcolor[RGB]{73, 178, 117}{\blacksquare} \text{~t=64~} &~~~&
			\textcolor[RGB]{67, 164, 241}{\boldsymbol{+}} \text{~$t=128$~} &~~~&
			\textcolor[RGB]{211, 108, 236}{\boldsymbol{\boxtimes}} \text{~$t=256$~} 
		\end{alignedat}$
	\end{center}
	\vspace{-10pt}
	\caption{Results for Fashion-MNIST. Each point is the average of 16 samples for fixed $n$ and $t$.}
	\label{fig:fashion_aux}
\end{figure}
\begin{figure}[t!]
	\begin{center}
		\resizebox{\columnwidth}{!}{
			\includegraphics[page=1, width=0.45\paperwidth]{figures/gauss_grid.pdf}
		}
		$\begin{alignedat}{10}
			\textcolor[RGB]{229, 116, 100}{\bullet} \text{~$t=16$~} &~~~&
			\textcolor[RGB]{153, 154, 35}{\blacktriangle} \text{~$t=32$~} &~~~&
			\textcolor[RGB]{73, 178, 117}{\blacksquare} \text{~t=64~} &~~~&
			\textcolor[RGB]{67, 164, 241}{\boldsymbol{+}} \text{~$t=128$~} &~~~&
			\textcolor[RGB]{211, 108, 236}{\boldsymbol{\boxtimes}} \text{~$t=256$~} 
		\end{alignedat}$
	\end{center}
	\vspace{-10pt}
	\caption{Experimental results on synthetic data with clustering structure. Each synthetic dataset is drawn from a mixture of $g$ Gaussians in $d$-dimensional Euclidean space, with $\lfloor20000/g\rfloor$ points per Gaussian. We show results for $d \in \{4,8,32\}$  Cost ratio and $\gamma$-overlap bound $\bar{\gamma}$ are typically minimized when the number of Gaussians $g$ roughly matches the number of components $t$ used by our MSTC framework. Compared with results for uniform data (Figure~\ref{fig:unif_grid_aux}), we see our MFC framework performs even better on datasets with inherent clustering structure, especially when one has a good estimate for the number of underlying true clusters. Results shown are averages over 16 dataset samples for each $g$.}
	\label{fig:gaussian_plots}
\end{figure}
Although $\bar{\gamma}$ is large for high-dimensional uniform random data, the lack of structure in this synthetic data is atypical for most applications. For example, metric spanning trees are often computed as the first step in clustering data. Therefore, as part of our answers to Questions \textbf{Q2} (on $\gamma$ values) and \textbf{Q4} (on the structure of the data), we explore how our framework performs on datasets with some level of clustering structure. We begin by running a similar set of experiments as in Figure~\ref{fig:unif_grid_aux}, but we instead sample points uniformly at random from the Fashion-MNIST dataset~\cite{fashion_mnist}, where points represent images of clothing items. Although images do not perfectly cluster into different classes, there is still clear underlying structure (e.g., we expect images of sneakers to look different from images of trousers). Figure~\ref{fig:fashion_aux} shows that for this dataset, our MFC framework achieves slightly better cost ratios and far better $\gamma$-overlap bounds than for uniform random data (Figure~\ref{fig:unif_grid_aux}), even though the dimension of Fashion-MNIST points ($d = 784$) is much larger than the dimensions we considered for uniform data. We also observe similar trends in runtime improvements as in Figure~\ref{fig:unif_grid_aux}.

We further examine the effect of clustering structure in a controlled setting by generating mixtures of Gaussians. In more detail, we generate a mixture of $g$ Gaussians in $d$-dimensional Euclidean space (for $d \in \{4, 8,32\}$) with $g$ ranging from 8 to 300. For each Gaussian cluster we generate $\floor{20000 / g}$ points, so that $n \approx 20000$ for each dataset. The mean of each Gaussian is chosen uniformly at random from an $8$-dimensional box where each axis ranges from $-5$ to $5$. The standard deviation of each dimension of each Gaussian is chosen uniformly at random between $0.5$ and $0.8$. In practice these parameters generate good (but not perfect) clustering structure. We then run our MFC framework for each choice of initial component number $t \in \{16,32,64,128,256\}$.  Figure~\ref{fig:gaussian_plots} shows $\bar{\gamma}$ values and cost ratios for a range of component numbers $t$ as $g$ varies. Significantly, the quality of our spanning trees gets better and better as the number of components $t$ gets closer to the number of Gaussians $g$ (i.e., true number of clusters in the data). This is illustrated by bowl-shaped curves for $\bar{\gamma}$ and cost ratio that are minimized when $t \approx g$. Furthermore, even when $t$ and $g$ are not close to each other, $\bar{\gamma}$ values and cost ratios are still smaller than the ones obtained on uniform random data with the same dimension (see Figure~\ref{fig:unif_grid_aux}). This indicates that clustering structure helps our framework even when the number of underlying clusters is unknown. 


\subsection{Additional Results on Real-world Data}
\begin{table}[t]
	\centering
	\caption{List of different datasets used for real world experiments}
	\label{tab:data}
	%\resizebox{\columnwidth}{!}{
	\begin{tabular}{ lll } 
		\toprule
		\textbf{Dataset} & \textbf{Type of Data} & \textbf{Distance Metric} \\
		\cmidrule(lr){1-3}
		Kosarak\cite{annBenchmarks, kosarak} & Sets & Jaccard distance \\
		MovieLens-10M\cite{annBenchmarks, movie_lens} & Sets & Jaccard distance \\
		Fashion-MNIST\cite{annBenchmarks, fashion_mnist} & $784$ dimensional points & Euclidean distance \\
		Name Dataset\cite{nameDataset2021} & Strings & Levenshtein edit distance \\
		GreenGenes\cite{greengenesDataset} & Strings & Levenshtein edit distance \\
		GreenGenes Aligned\cite{greengenesDataset} & 7682 character strings & Hamming distance\\
		\bottomrule
	\end{tabular}
	%}
\end{table}
We continue to address \textbf{Q4} by running experiments on several real-world datasets that have often been used as benchmarks for similarity search algorithms and clustering algorithms. These involve a variety of different types of data (e.g., point cloud data, sets, strings) and distance functions (e.g., Euclidean, Jaccard, Hamming, Levenshtein edit distance). For some datasets, we are unable to compute an optimal minimum spanning tree in a reasonable amount of time and space, so we subsample the data 16 times for a fixed value of $n$ and report mean results and standard deviations. We run experiments on implicit graphs with up to $n = 39774$ nodes, which means we are considering graphs with 700 million edges. Our MFC framework in fact scales up to even larger graphs, but we restrict to graphs with under 1 billion edges since in this regime we are still able to compute an optimal MST to use as a point of comparison. We summarize the dataset type and distance metric in Table~\ref{tab:data}, and provide more details for each dataset below.

\begin{itemize}[label = {},leftmargin = 10pt, itemsep = 0pt]
	\item \textbf{Cooking.}\footnote{\url{https://www.cs.cornell.edu/~arb/data/cat-edge-Cooking/}} Each data object is a set of food ingredients defining a recipe. There are 6714 ingredients and 39774 recipes. We use Jaccard distance. The original dataset comes from the \emph{What's Cooking?} Kaggle challenge~\cite{kaggle2015cooking}. We use the parsed dataset of Amburg et al.~\cite{amburg2020clustering}.
	\item \textbf{MovieLens-10M.}\footnote{\url{https://github.com/erikbern/ann-benchmarks}} This is a set of sets derived from movie ratings~\cite{movie_lens}. We consider a subset of the data provided on the ANN Benchmarks Repository~\cite{annBenchmarks}, restricting to sets with 64 items or more, in order to work with a dataset where $n \approx 30000$. We apply Jaccard distance.
	\item \textbf{Kosarak.}\footnote{\url{http://fimi.uantwerpen.be/data/kosarak.dat}} This dataset is derived from click-stream data from a Hungarian news portal~\cite{kosarak}. The original data comprises 990002 sets defined over a collection of 41270 items. We restricted to sets of size at least 40, leading to a set of 32295 sets. We apply Jaccard distance.
	\item \textbf{Fashion-MNIST.}\footnote{\url{https://github.com/zalandoresearch/fashion-mnist}} Each point in the Fashion-MNIST dataset represents a $28 \times 28$ grayscale image of an item of clothing (e.g., sneakers, trousers), encoded as a vector of size $d = 784$. The total number of class labels is 10. We use Euclidean distance for this dataset.
	\item \textbf{Names-US.}\footnote{\url{https://github.com/philipperemy/name-dataset}} Each data object is a last name for someone in the United States. This provides a natural benchmark for computing spanning trees on short sequence data using Levenshtein edit distance. We treat each UTF-8 code point as a separate character. Last names in the dataset vary in length from $0$ to $252$ characters where $0$ indicates no last name. The average length of the last names is $6.67$ with a standard deviation of $2.470$. Computing an exact minimum spanning tree for the entire dataset is infeasible, so we consider samples of names of size $n = 30000$.
	
	\item \textbf{GreenGenes-Unaligned and GreenGenes-Aligned.} GreenGenes is a chimera-checked 16S megagenomic sequence dataset~\cite{greengenesDataset}, which is frequently used as benchmarks for sequence clustering. We use version 13.5 of the data, accessed by following instructions on the USEARCH benchmarks website.\footnote{\url{https://www.drive5.com/usearch/benchmark_ggclust.html}} The data is provided in two formats: as unaligned variable length sequences (ranging in length from $1111$ to $2368$; mean length is $1401.06$ with a standard deviation of $57.083$), and as aligned sequences of a fixed length of 7682. We use Levenshtein edit distance for the unaligned sequences and Hamming distance for the aligned sequences. 
\end{itemize}


\begin{table}[t!]
	\centering
	\caption{Results for real-world datasets. Runtimes are in minutes. For the last 4 datasets we report averages and standard deviations for 16 different samples of size $n$. The last three rows for each dataset report the proportion of time spent on each step of the MFC framework.}
	\vspace{-10pt}
	\label{tab:realdata_detailed}
	\resizebox{\columnwidth}{!}{
		\begin{tabular}{ llllllll } 
			\toprule
			\textbf{Dataset} & & \textbf{OPT} & $\mathbf{t=16}$ & $\mathbf{t=32}$ & $\mathbf{t=64}$ & $\mathbf{t=128}$ & $\mathbf{t=256}$ \\
			\cmidrule(lr){1-8}
			Cooking & $ \bar{\gamma} $ & - & $ 1.770  $ & $ 2.014  $ & $ 2.222  $ & $ 2.421  $ & $ 3.144  $ \\
			& Cost Ratio & 1 & $ 1.040  $ & $ 1.051  $ & $ 1.059  $ & $ 1.069  $ & $ 1.089  $ \\
			$ n=39774$ & Runtime Ratio & 1 & $ 4.391  $ & $ 6.524  $ & $ 8.689  $ & $ 9.966  $ & $ 15.598  $ \\
			& Runtime (mins) & $24.2 \scriptstyle{ \pm 0.61 } $ & $ 5.3  $ & $ 3.5  $ & $ 2.7  $ & $ 2.3  $ & $ 1.5  $ \\
			& k-centering \% & - & $ 0.006  $ & $ 0.016  $ & $ 0.038  $ & $ 0.082  $ & $ 0.321  $ \\
			& Sub-MST \% & - & $ 0.989  $ & $ 0.971  $ & $ 0.923  $ & $ 0.826  $ & $ 0.355  $ \\
			& MFC-approx \% & - & $ 0.004  $ & $ 0.013  $ & $ 0.038  $ & $ 0.092  $ & $ 0.324  $ \\
			\cmidrule(lr){1-8}
			MovieLens & $ \bar{\gamma} $ & - & $ 1.395  $ & $ 1.513  $ & $ 1.690  $ & $ 1.955  $ & $ 2.283  $ \\
			& Cost Ratio & 1 & $ 1.010  $ & $ 1.012  $ & $ 1.017  $ & $ 1.022  $ & $ 1.028  $ \\
			$ n=33240$ & Runtime Ratio & 1 & $ 4.026  $ & $ 5.014  $ & $ 6.287  $ & $ 9.337  $ & $ 9.529  $ \\
			& Runtime (mins) & $536.0 \scriptstyle{ \pm 15.58 } $ & $ 120.3  $ & $ 96.6  $ & $ 77.0  $ & $ 51.9  $ & $ 50.8  $ \\
			& k-centering \% & - & $ 0.016  $ & $ 0.028  $ & $ 0.054  $ & $ 0.130  $ & $ 0.238  $ \\
			& Sub-MST \% & - & $ 0.978  $ & $ 0.959  $ & $ 0.912  $ & $ 0.765  $ & $ 0.508  $ \\
			& MFC-approx \% & - & $ 0.006  $ & $ 0.013  $ & $ 0.034  $ & $ 0.104  $ & $ 0.253  $ \\
			\cmidrule(lr){1-8}
			Kosarak & $ \bar{\gamma} $ & - & $ 1.289  $ & $ 1.737  $ & $ 2.281  $ & $ 2.788  $ & $ 3.129  $ \\
			& Cost Ratio & 1 & $ 1.007  $ & $ 1.013  $ & $ 1.020  $ & $ 1.025  $ & $ 1.029  $ \\
			$ n=32295$ & Runtime Ratio & 1 & $ 2.682  $ & $ 5.994  $ & $ 15.159  $ & $ 18.174  $ & $ 12.583  $ \\
			& Runtime (mins) & $182.2 \scriptstyle{ \pm 0.26 } $ & $ 68.0  $ & $ 30.4  $ & $ 12.0  $ & $ 10.0  $ & $ 14.5  $ \\
			& k-centering \% & - & $ 0.010  $ & $ 0.036  $ & $ 0.146  $ & $ 0.363  $ & $ 0.464  $ \\
			& Sub-MST \% & - & $ 0.985  $ & $ 0.935  $ & $ 0.725  $ & $ 0.339  $ & $ 0.138  $ \\
			& MFC-approx \% & - & $ 0.005  $ & $ 0.028  $ & $ 0.128  $ & $ 0.297  $ & $ 0.397  $ \\
			\cmidrule(lr){1-8}
			Names-US & $ \bar{\gamma} $ & - & $ 1.020  \scriptstyle{ \pm  0.02  } $ & $ 1.059  \scriptstyle{ \pm  0.02  } $ & $ 1.139  \scriptstyle{ \pm  0.04  } $ & $ 1.219  \scriptstyle{ \pm  0.05  } $ & $ 1.322  \scriptstyle{ \pm  0.07  } $ \\
			& Cost Ratio & 1 & $ 1.005  \scriptstyle{ \pm  0.00  } $ & $ 1.015  \scriptstyle{ \pm  0.01  } $ & $ 1.034  \scriptstyle{ \pm  0.01  } $ & $ 1.051  \scriptstyle{ \pm  0.01  } $ & $ 1.071  \scriptstyle{ \pm  0.01  } $ \\
			$ n=30000$ & Runtime Ratio & 1 & $ 0.923  \scriptstyle{ \pm  0.16  } $ & $ 0.954  \scriptstyle{ \pm  0.18  } $ & $ 0.939  \scriptstyle{ \pm  0.17  } $ & $ 1.027  \scriptstyle{ \pm  0.19  } $ & $ 1.138  \scriptstyle{ \pm  0.21  } $ \\
			& Runtime (mins) & $2.0 \scriptstyle{ \pm 0.28 } $ & $ 2.1  \scriptstyle{ \pm  0.2  } $ & $ 2.1  \scriptstyle{ \pm  0.2  } $ & $ 2.1  \scriptstyle{ \pm  0.2  } $ & $ 1.9  \scriptstyle{ \pm  0.2  } $ & $ 1.7  \scriptstyle{ \pm  0.1  } $ \\
			& k-centering \% & - & $ 0.003  \scriptstyle{ \pm  0.00  } $ & $ 0.005  \scriptstyle{ \pm  0.00  } $ & $ 0.009  \scriptstyle{ \pm  0.00  } $ & $ 0.019  \scriptstyle{ \pm  0.00  } $ & $ 0.038  \scriptstyle{ \pm  0.00  } $ \\
			& Sub-MST \% & - & $ 0.995  \scriptstyle{ \pm  0.00  } $ & $ 0.992  \scriptstyle{ \pm  0.00  } $ & $ 0.985  \scriptstyle{ \pm  0.00  } $ & $ 0.970  \scriptstyle{ \pm  0.01  } $ & $ 0.937  \scriptstyle{ \pm  0.01  } $ \\
			& MFC-approx \% & - & $ 0.001  \scriptstyle{ \pm  0.00  } $ & $ 0.003  \scriptstyle{ \pm  0.00  } $ & $ 0.005  \scriptstyle{ \pm  0.00  } $ & $ 0.011  \scriptstyle{ \pm  0.00  } $ & $ 0.024  \scriptstyle{ \pm  0.01  } $ \\
			\cmidrule(lr){1-8}
			GreenGenes-Unalign. & $ \bar{\gamma} $ & - & $ 1.388  \scriptstyle{ \pm  0.08  } $ & $ 1.460  \scriptstyle{ \pm  0.07  } $ & $ 1.460  \scriptstyle{ \pm  0.06  } $ & $ 1.402  \scriptstyle{ \pm  0.04  } $ & $ 1.314  \scriptstyle{ \pm  0.02  } $ \\
			& Cost Ratio & 1 & $ 1.092  \scriptstyle{ \pm  0.02  } $ & $ 1.104  \scriptstyle{ \pm  0.01  } $ & $ 1.095  \scriptstyle{ \pm  0.01  } $ & $ 1.075  \scriptstyle{ \pm  0.01  } $ & $ 1.055  \scriptstyle{ \pm  0.00  } $ \\
			$ n=2500$ & Runtime Ratio & 1 & $ 2.555  \scriptstyle{ \pm  0.55  } $ & $ 3.270  \scriptstyle{ \pm  0.52  } $ & $ 3.156  \scriptstyle{ \pm  0.36  } $ & $ 2.116  \scriptstyle{ \pm  0.08  } $ & $ 1.159  \scriptstyle{ \pm  0.04  } $ \\
			& Runtime (mins) & $239.3 \scriptstyle{ \pm 6.24 } $ & $ 98.0  \scriptstyle{ \pm  23.0  } $ & $ 74.9  \scriptstyle{ \pm  11.9  } $ & $ 76.7  \scriptstyle{ \pm  8.8  } $ & $ 113.2  \scriptstyle{ \pm  4.0  } $ & $ 206.5  \scriptstyle{ \pm  6.0  } $ \\
			& k-centering \% & - & $ 0.068  \scriptstyle{ \pm  0.01  } $ & $ 0.172  \scriptstyle{ \pm  0.03  } $ & $ 0.331  \scriptstyle{ \pm  0.03  } $ & $ 0.448  \scriptstyle{ \pm  0.01  } $ & $ 0.492  \scriptstyle{ \pm  0.01  } $ \\
			& Sub-MST \% & - & $ 0.885  \scriptstyle{ \pm  0.03  } $ & $ 0.692  \scriptstyle{ \pm  0.06  } $ & $ 0.377  \scriptstyle{ \pm  0.08  } $ & $ 0.134  \scriptstyle{ \pm  0.01  } $ & $ 0.047  \scriptstyle{ \pm  0.01  } $ \\
			& MFC-approx \% & - & $ 0.047  \scriptstyle{ \pm  0.01  } $ & $ 0.136  \scriptstyle{ \pm  0.03  } $ & $ 0.292  \scriptstyle{ \pm  0.05  } $ & $ 0.418  \scriptstyle{ \pm  0.01  } $ & $ 0.461  \scriptstyle{ \pm  0.01  } $ \\
			\cmidrule(lr){1-8}
			GreenGenes-Aligned & $ \bar{\gamma} $ & - & $ 1.224  \scriptstyle{ \pm  0.07  } $ & $ 1.368  \scriptstyle{ \pm  0.10  } $ & $ 1.466  \scriptstyle{ \pm  0.06  } $ & $ 1.512  \scriptstyle{ \pm  0.05  } $ & $ 1.531  \scriptstyle{ \pm  0.03  } $ \\
			& Cost Ratio & 1 & $ 1.074  \scriptstyle{ \pm  0.02  } $ & $ 1.117  \scriptstyle{ \pm  0.03  } $ & $ 1.143  \scriptstyle{ \pm  0.02  } $ & $ 1.147  \scriptstyle{ \pm  0.01  } $ & $ 1.141  \scriptstyle{ \pm  0.01  } $ \\
			$ n=30000$ & Runtime Ratio & 1 & $ 1.604  \scriptstyle{ \pm  0.51  } $ & $ 2.375  \scriptstyle{ \pm  0.82  } $ & $ 3.918  \scriptstyle{ \pm  1.99  } $ & $ 5.849  \scriptstyle{ \pm  2.83  } $ & $ 7.026  \scriptstyle{ \pm  1.69  } $ \\
			& Runtime (mins) & $33.1 \scriptstyle{ \pm 0.17 } $ & $ 22.1  \scriptstyle{ \pm  4.9  } $ & $ 15.1  \scriptstyle{ \pm  3.7  } $ & $ 9.9  \scriptstyle{ \pm  3.1  } $ & $ 6.7  \scriptstyle{ \pm  2.6  } $ & $ 5.0  \scriptstyle{ \pm  1.3  } $ \\
			& k-centering \% & - & $ 0.003  \scriptstyle{ \pm  0.00  } $ & $ 0.010  \scriptstyle{ \pm  0.00  } $ & $ 0.033  \scriptstyle{ \pm  0.02  } $ & $ 0.098  \scriptstyle{ \pm  0.05  } $ & $ 0.232  \scriptstyle{ \pm  0.05  } $ \\
			& Sub-MST \% & - & $ 0.994  \scriptstyle{ \pm  0.00  } $ & $ 0.983  \scriptstyle{ \pm  0.01  } $ & $ 0.940  \scriptstyle{ \pm  0.03  } $ & $ 0.817  \scriptstyle{ \pm  0.09  } $ & $ 0.550  \scriptstyle{ \pm  0.12  } $ \\
			& MFC-approx \% & - & $ 0.002  \scriptstyle{ \pm  0.00  } $ & $ 0.007  \scriptstyle{ \pm  0.00  } $ & $ 0.027  \scriptstyle{ \pm  0.02  } $ & $ 0.084  \scriptstyle{ \pm  0.05  } $ & $ 0.216  \scriptstyle{ \pm  0.07  } $ \\
			\cmidrule(lr){1-8}
			Fashion-MNIST & $ \bar{\gamma} $ & - & $ 1.173  \scriptstyle{ \pm  0.02  } $ & $ 1.237  \scriptstyle{ \pm  0.03  } $ & $ 1.320  \scriptstyle{ \pm  0.05  } $ & $ 1.405  \scriptstyle{ \pm  0.03  } $ & $ 1.527  \scriptstyle{ \pm  0.05  } $ \\
			& Cost Ratio & 1 & $ 1.013  \scriptstyle{ \pm  0.00  } $ & $ 1.017  \scriptstyle{ \pm  0.00  } $ & $ 1.023  \scriptstyle{ \pm  0.00  } $ & $ 1.029  \scriptstyle{ \pm  0.00  } $ & $ 1.036  \scriptstyle{ \pm  0.00  } $ \\
			$ n=30000$ & Runtime Ratio & 1 & $ 4.675  \scriptstyle{ \pm  1.42  } $ & $ 7.176  \scriptstyle{ \pm  2.55  } $ & $ 10.027  \scriptstyle{ \pm  2.85  } $ & $ 12.129  \scriptstyle{ \pm  2.64  } $ & $ 12.698  \scriptstyle{ \pm  1.83  } $ \\
			& Runtime (mins) & $11.4 \scriptstyle{ \pm 0.31 } $ & $ 2.7  \scriptstyle{ \pm  0.8  } $ & $ 1.8  \scriptstyle{ \pm  0.7  } $ & $ 1.2  \scriptstyle{ \pm  0.4  } $ & $ 1.0  \scriptstyle{ \pm  0.2  } $ & $ 0.9  \scriptstyle{ \pm  0.1  } $ \\
			& k-centering \% & - & $ 0.008  \scriptstyle{ \pm  0.00  } $ & $ 0.024  \scriptstyle{ \pm  0.01  } $ & $ 0.067  \scriptstyle{ \pm  0.02  } $ & $ 0.162  \scriptstyle{ \pm  0.03  } $ & $ 0.339  \scriptstyle{ \pm  0.05  } $ \\
			& Sub-MST \% & - & $ 0.986  \scriptstyle{ \pm  0.00  } $ & $ 0.963  \scriptstyle{ \pm  0.01  } $ & $ 0.896  \scriptstyle{ \pm  0.03  } $ & $ 0.746  \scriptstyle{ \pm  0.06  } $ & $ 0.462  \scriptstyle{ \pm  0.07  } $ \\
			& MFC-approx \% & - & $ 0.004  \scriptstyle{ \pm  0.00  } $ & $ 0.013  \scriptstyle{ \pm  0.01  } $ & $ 0.036  \scriptstyle{ \pm  0.01  } $ & $ 0.090  \scriptstyle{ \pm  0.02  } $ & $ 0.197  \scriptstyle{ \pm  0.03  } $ \\
			\bottomrule
		\end{tabular}
	}
\end{table}

Table~\ref{tab:realdata_detailed} displays cost ratios, runtimes, and $\gamma$-overlap bound $\bar{\gamma}$ for our MFC framework. The table includes details for the proportion of time spent on the three main steps of our approach: 
\begin{enumerate}[itemsep=0pt,leftmargin=10pt,label = {}]
	\item \textbf{$k$-centering}: obtaining the partitioning $\mathcal{P}$ using a greedy $2$-approximaton for $k$-centering~\cite{gonzalez1985clustering}.
	\item \textbf{Sub-MST}: Using Kruskal's algorithm to find optimal MSTs $\{T_i\}_{i=1}^t$ for each component in $\mathcal{P}$.
	\item \textbf{MFC-Approx}: Approximating the MFC problem using \textsf{MFC-Approx}.
\end{enumerate}
As anticipated, Sub-MST is typically the most expensive step. However, as $t$ increases, the $k$-centering step and the \textsf{MFC-Approx} algorithm become a significant portion of the overall runtime, and even dominate in extreme cases. We remark that the \textsf{MFC-Approx} step is always dominated by the time it takes to form the coarsened graph $G_\mathcal{P}$. The time spent finding an MST of $G_\mathcal{P}$ is negligible for all datasets and choices of $t$ we considered. 

We note several trends in the performance of our MFC framework from Table~\ref{tab:realdata_detailed}. Across all datasets (and corresponding distance metrics), the $\bar{\gamma}$ values tend to be very good: usually below 2 and never above 3.2. As expected, $\bar{\gamma}$ values get slightly worse as $t$ increases. This is also true for cost ratios, but cost ratios nevertheless remain very good in all cases (always below 1.2 and typically much better). In general, runtimes improve as $t$ increases up to a certain point. Once $t$ becomes too large, computing MSTs for the components found by $k$-centering is no longer the bottleneck. When the number of components becomes too large, the $k$-centering step and building the coarsened graph become too expensive. This is especially clear on the GreenGenes-Unaligned dataset. For this dataset we consider small subsets of size $n = 2500$ since data objects are long strings, and computing Levenshtein distances is expensive. When $t = 256$, the average partition size for the initial forest is roughly 10 points. In this case, \textsf{MFC-Approx} is extremely expensive. This algorithm is designed for situations where $t$ is asymptotically much smaller than $n$, hence Table~\ref{tab:realdata_detailed} illustrates the decrease in performance we would naturally expect when $t$ is too large.

One other exception to the general runtime trends is that for small values of $t$, running the MFC framework on the Names-US dataset is in fact slower than computing an optimal MST. For this dataset, we found that the $k$-centering step tends to form a disproportionately large component, and finding an MST of a very large component takes nearly as long as finding an MST of the entire dataset.
Combining this with the time taken on other steps in our MFC framework can lead to a larger overall runtime. This behavior (having a single large cluster) is likely tied to the fact that we are considering Levenshtein distances for short strings. This means that distances will tend to be small integer values, and many pairs of points will have the exact same distance. Our simple $k$-centering implementation can lead to imbalanced clusters in this setting where many points can be equidistant from multiple cluster center, and then one cluster is given all the points that could have just as easily gone to another cluster. Even despite this behavior, we note that runtimes do continue to increase as $t$ increases, and our MFC framework is faster for the largest $t$ values. These results could also be improved using simple strategies to avoid cluster imbalance.