\section{Approximate Completion Algorithm}
\label{sec:algs}
\begin{algorithm}[tb]
	\caption{\textsf{MFC-Approx}}
	\label{alg:mfcapprox}
	\begin{algorithmic}[5]
		\State{\bfseries Input:} $\mathcal{X} = \{x_1, x_2, \hdots , x_n\}$, components $\mathcal{P} = \{P_1, P_2, \hdots, P_t\}$, spanning trees $\{T_1, T_2, \hdots, T_t\}$
		\State {\bfseries Output:} Spanning tree for implicit metric graph of $\mathcal{X}$
		\For{$i = 1, 2, \hdots t$}
		\State Select arbitrary component representative $s_i \in P_i$
		\EndFor
		\For{$i = 1, 2, \hdots t-1$}
		\For{$j = i+1, \hdots , t$}
		\State $w_{i \rightarrow j} = \min_{x_i \in P_i} d(x_i, s_j)$  \quad \hfill \texttt{ // closest a $P_i$ node comes to $s_j$}
		\State $w_{j \rightarrow i} = \min_{x_j \in P_j} d(x_j, s_i)$ \quad \hfill  \texttt{ // closest a $P_j$ node comes to $s_i$}
		\State $\hat{w}_{ij} = \min \{w_{i \rightarrow j}, w_{j \rightarrow i}\}$ \quad  \hfill \texttt{// set weight for edge $(v_i, v_j)$} 
		\EndFor
		\EndFor
		\State $\hat{T}_{\mathcal{P}} = \textsc{OptimalMST}(\{\hat{w}_{ij}\}_{i,j \in [t]})$ \hfill \texttt{ // find optimal MST on complete $t$-node graph}
		\State Return spanning tree $\hat{T}$ of $G_\mathcal{X}$ by combining $\bigcup_{i=1}^t T_i$ with edges from $\hat{T}_{\mathcal{P}}$.
	\end{algorithmic}
\end{algorithm}

We now present an algorithm that approximates MFC to within a factor $c < 2.62$. We also prove it can be viewed as a learning-augmented algorithm for metric MST, where the approximation factor depends on the $\gamma$-overlap of the initial forest. 
%Pseudocode for our algorithm is provided in the appendix. Here in the main text we give a full description of the algorithm along with visual aids in Figure~\ref{fig:mfcapprox}. Due to space constraints, proofs are relegated to the appendix.



\begin{figure*}[t]
	\centering
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/wijhat.pdf}
		%        \vspace{-10pt}
		\caption{Computing $\hat{w}_{ij}$}
		\label{fig:wijhat}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/componentsgraphhat.pdf}
		%        \vspace{-10pt}
		\caption{$G_\mathcal{P}$ w.r.t.\ $\hat{w}$}
		\label{fig:componentsgraphhat}
	\end{subfigure}\hfill
	\begin{subfigure}[b]{0.33\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/approx_mstc.pdf}
		%         \vspace{-10pt}
		\caption{\textsf{MFC-Approx} output}
		\label{fig:approx_mfc}
	\end{subfigure}
	\caption{(a) Finding the minimum distance between components $P_i$ and $P_j$ (dashed line) is an expensive bichromatic closest pair problem. \textsf{MFC-Approx} instead performs a cheaper nearest neighbor query for a \emph{representative} point in each component ($s_i$ and $s_j$, shown as stars). The algorithm finds the closest point to each representative from the opposite cluster, then takes the minimum of the two distances. 
		(b) Applying this to each pair of components produces a weight function $\hat{w}$ for the coarsened graph $G_\mathcal{P}$. Finding an MST of $G_\mathcal{P}$ with respect to $\hat{w}$ yields (c) a 2.62-approximation for MFC.}
	\label{fig:mfcapprox}
\end{figure*}


%\subsection{Algorithm description} 
Pseodocode for our method, which we call \textsf{MFC-Approx}, is shown in Algorithm~\ref{alg:mfcapprox}. This algorithm starts by choosing an arbitrary point $s_i \in P_i$ for each $i \in \{1,2, \hdots, t\}$ to be the component's \emph{representative} (starred nodes in Figure~\ref{fig:mfcapprox}). 
The algorithm computes the distance between every point $x \in \mathcal{X}$ and all of the other representatives. For every pair of distinct components $i$ and $j$ we compute the weights:
\begin{align}
	w_{i \rightarrow j} &= \min_{x_i \in P_i} d(x_i, s_j)  	&\text{(the closest $P_i$ node to $s_j$)} \\
	w_{j \rightarrow i} &= \min_{x_j \in P_j} d(x_j, s_i)  &\text{(the closest $P_j$ node to $s_i$).} 
\end{align}
We then define the approximate edge weight between component nodes $v_i$ and $v_j$ to be:
\begin{align}
	\label{eq:approxweight}
	\hat{w}_{ij} = \min \{w_{i \rightarrow j}, w_{j \rightarrow i}\}.
\end{align}
This upper bounds the minimum distance $w_{ij}^*$ between the two components (Figure~\ref{fig:wijhat}).
Computing this for all pairs of components creates a new weight function $\hat{w}$ for the
coarsened graph $G_\mathcal{P}$ (Figure~\ref{fig:componentsgraphhat}). The algorithm keeps track of the points in $\mathcal{X}$ that define these edge weights in $G_\mathcal{P}$. It then computes an MST in $G_\mathcal{P}$ with respect to $\hat{w}$, then identifies the corresponding edges in $G_\mathcal{X}$, to produce a feasible solution for MFC (Figure~\ref{fig:approx_mfc}). 





\subsection{Bounded and unbounded edges in the coarsensed graph}
A key step of our approximation analysis is to separate edges of the coarsened graph $G_\mathcal{P} = (V_\mathcal{P},E_\mathcal{P})$ into two categories, depending on the relationship between $\hat{w}$ and $w^*$.
% \begin{definition}
Formally, for an arbitrary constant $\beta \geq 1$, we say edge $(v_i,v_j) \in E_\mathcal{P}$ is \textit{$\beta$-bounded} if $\hat{w}_{ij}  \leq \beta w^*_{ij}$, otherwise it is \textit{$\beta$-unbounded}. 
If all edges in $G_\mathcal{P}$ were $\beta$-bounded, finding an MST in $G_\mathcal{P}$ with respect to $\hat{w}$ would produce a $\beta$-approximation for the MST problem in $G_\mathcal{P}$ with respect to $w^*$. In general we cannot guarantee all edges will be $\beta$-bounded, as this would imply Algorithm~\ref{alg:mfcapprox} is a subquadratic $\beta$-approximation algorithm for MFC, contradicting Theorem~\ref{thm:hard}. Nevertheless, any $\beta$-bounded edge that Algorithm~\ref{alg:mfcapprox} includes in its MST of $G_\mathcal{P}$ is easy to bound in terms of the optimal edge weights $w^*$. 
%
If we include a $\beta$-unbounded edge in our MST of $G_\mathcal{P}$, we can no longer bound its weight in terms of an optimal solution to MFC. However, the following lemma shows that its weight can be bounded in terms of the weight of the initial forest.
\begin{lemma}
	\label{lem:unboundededges}
	Let $P_i$ and $P_j$ be an arbitrary pair of components and let $\beta > 1$. If $\hat{w}_{ij} > \beta w^*_{ij}$, then 
	\begin{align}
		\hat{w}_{ij} <
		%		\frac{\beta}{\beta - 1} \min \{\alpha_i, \alpha_j\} \leq 
		\frac{\beta}{\beta - 1}  \min\{w_\mathcal{X}(T_i), w_\mathcal{X}(T_j)\}.
	\end{align}
\end{lemma}
\begin{proof}
	For each $i \in \{1, 2, \hdots, t\}$, we denote the maximum distance between a point in $P_i$ and its component representative $s_i$ by $\alpha_i = \max_{x \in P_i} d(x, s_i)$.
	Let $x_i^* \in P_i$ and $x_j^* \in P_j$ be points satisfying $d(x_{i}^*, x_{j}^*) = d(P_i, P_j) = w_{ij}^*$.
	We use the (reverse) triangle inequality and the definition of $\alpha_i$ to see that:
	\begin{align*}
		d(x_i^*, x_j^*) = \min_{x_j \in P_j} d(x_i^*, x_j) \geq  \min_{x_j \in P_j} d(s_i, x_j) - d(s_i, x_i^*) \geq  \min_{x_j \in P_j} d(s_i, x_j) - \alpha_i = w_{j \rightarrow i} - \alpha_i \geq \hat{w}_{ij} - \alpha_i.
	\end{align*}
	Similarly we can show that 
	\begin{align*}
		d(x_i^*, x_j^*) = \min_{x_i \in P_i} d(x_i, x_j^*) \geq  \min_{x_i \in P_i} d(s_j, x_i) - d(s_j, x_j^*)  \geq  \min_{x_i \in P_i} d(s_j, x_i) - \alpha_j = w_{i \rightarrow j} - \alpha_j \geq \hat{w}_{ij} - \alpha_j.
	\end{align*}
	In other words, we have the bound $w^*_{ij} \geq \hat{w}_{ij} - \min\{\alpha_i, \alpha_j\}$. Combining this with the assumption that $\hat{w}_{ij} > \beta w_{ij}^*$ gives
	\begin{align*}
		\hat{w}_{ij} &> \beta w_{ij}^* \geq \beta\hat{w}_{ij} - \beta\min \{\alpha_i, \alpha_j\} \implies \frac{\beta}{\beta -1 } \min \{\alpha_i, \alpha_j\}> \hat{w}_{ij}.
	\end{align*}
	The proof follows from the observation that $\alpha_i \leq w_\mathcal{X}(T_i)$. To see why, note that there exists some ${x} \in P_i$ such that $d({x}, s_i) = \alpha_i$. Since $T_i$ is a spanning tree of $P_i$, it must contain a path from $s_i$ to ${x}$ with sum of edge weights at least $\alpha_i$.
\end{proof}
Our main approximation guarantees rely on Lemma~\ref{lem:unboundededges}, as well as two other simple supporting observations. This first amounts to the observation that a tree has arboricity and degeneracy 1.
\begin{observation}
	\label{lem:treeorient}
	If $T = (V,E_T)$ is an undirected tree, there is a way to orient edges in such a way that every node has at most one outgoing edge.
\end{observation}
\begin{proof}
	The proof is constructive. Define an iterative algorithm that removes a minimum degree node at each step and deletes all its incident edges. Orient the deleted edges so that they start at the node that was removed. Note that a tree always contains a node of degree 1, and removing such a node leads to another tree with one fewer node. Thus, this procedure will orient edges of the original graph in such a way that each node has at most one outgoing edge.
\end{proof}
The other supporting result deals with MSTs in a graph that includes edges of weight zero.
\begin{observation}
	\label{obs:Z}
	Let $w^{(1)} \colon E \rightarrow \mathbb{R}^+$ and $w^{(2)} \colon E \rightarrow \mathbb{R}^+$ be two nonnegative weight functions for an undirected graph $G = (V,E)$. Assume there exists an edge set $Z \subseteq E$ such that 
	\begin{equation*}
		w^{(1)}(i,j) = w^{(2)}(i,j) = 0 \text{ for every $(i,j) \in Z$}.
	\end{equation*}
	Then there exist spanning trees $M_1$ and $M_2$ for $G$ such that $M_i$ is an MST for $G$ with respect to $w^{(i)}$ for $i \in \{1,2\}$, and $M_1 \cap Z = M_2 \cap Z$.
\end{observation}
\begin{proof}
	The proof is constructive. Recall that Kruskal's algorithm finds an MST by ordering edges by weight (starting with the smallest and breaking ties arbitrarily in the ordering) and then greedily adds each edge a growing spanning tree if and only if it connects two previously disconnected components. 
	Fix an arbitrary ordering $\sigma_Z$ of edges in $Z$. When applying Kruskal's algorithm to find minimum spanning trees of $G$ with respect to $w^{(1)}$ and $w^{(2)}$, we can choose orderings for these functions that exactly coincide for the first $|Z|$ edges visited. Namely, we place edges in $Z$ first, using the order given by $\sigma_Z$. 
	% The remaining edges $E\backslash Z$ may be ordered differently in the orderings for $w^{(1)}$ and $w^{(2)}$. 
	The first $|Z|$ steps of Kruskal's algorithm will be identical when building MSTs with respect to $w^{(1)}$ and $w^{(2)}$. Thus, if $M_1$ and $M_2$ are the spanning trees obtained for $w^{(1)}$ and $w^{(2)}$ respectively using this approach, we know these trees will include the same set of edges from $Z$ and discard the same set of edges from $Z$, i.e., $M_1\cap Z = M_2 \cap Z.$
\end{proof}


\subsection{Main approximation guarantees}
Let $T^*_\mathcal{P}$ represent an MST of $G_\mathcal{P}$ with respect to the optimal weight function $w^* \colon E_\mathcal{P} \rightarrow \mathbb{R}^+$ and $\hat{T}_\mathcal{P}$ represent an MST of $G_\mathcal{P}$ with respect to the approximate weight function $\hat{w} \colon E_\mathcal{P} \rightarrow \mathbb{R}^+$. The edges of $T^*_\mathcal{P}$ map to a set of edges $M^*$ in $G_\mathcal{X}$ that optimally solves the metric MST completion problem, and the edges in $\hat{T}_\mathcal{P}$ map to an edge set $\hat{M}$. The weight of these edges is given by:
\begin{align*}
	w_\mathcal{X}(M^*) &= w^*(T^*_\mathcal{P}) \\
	w_\mathcal{X}(\hat{M}) &= \hat{w}(\hat{T}_\mathcal{P}).
\end{align*}
Let $T^*$ be the spanning tree of $G_\mathcal{X}$ defined by combining $\bigcup_{i = 1}^t T_i$ with $M^*$ and $\hat{T}$ be the spanning tree (returned by Algorithm~\ref{alg:mfcapprox}) that combines $\bigcup_{i = 1}^t T_i$ with $\hat{M}$. These have weights given by
\begin{align}
	\label{eq:Tstar}
	w_\mathcal{X}(T^*) &= w^*(T_\mathcal{P}^*) +  \sum_{i = 1}^t w_\mathcal{X}(T_i) \\
	\label{eq:That}
	w_\mathcal{X}(\hat{T}) &= \hat{w}(\hat{T}_\mathcal{P}) +  \sum_{i = 1}^t w_\mathcal{X}(T_i) .
\end{align}
We are now ready to prove the approximation guarantee for \textsf{MFC-Approx}.
\begin{theorem}
	\label{thm:main}
	The spanning tree $\hat{T}$ returned by Algorithm~\ref{alg:mfcapprox} satisfies
	\begin{equation*}
		w_\mathcal{X}(T^*) \leq w_\mathcal{X}(\hat{T}) \leq \beta w_\mathcal{X}(T^*)
	\end{equation*}
	for $\beta = (3 + \sqrt{5})/2 < 2.62$. 
\end{theorem}
\begin{proof}
	For our analysis we consider two hypothetical weight functions $w^*_0$ and $\hat{w}_0$ for $G_\mathcal{P} = (V_\mathcal{P}, E_\mathcal{P})$, defined by zeroing out the $\beta$-unbounded edges in $w^*$ and $\hat{w}$:
	\begin{align*}
		w^*_0(v_i, v_j) &= \begin{cases} 
			w^*_{ij} & \text{ if $(v_i,v_j)$ is $\beta$-bounded, i.e., $\hat{w}_{ij} \leq \beta w^*_{ij}$} \\
			0 & \text{ otherwise} 
		\end{cases}\\
		\hat{w}_0(v_i, v_j) &= \begin{cases} 
			\hat{w}_{ij} & \text{ if $(v_i,v_j)$ is $\beta$-bounded, i.e., $\hat{w}_{ij} \leq \beta w^*_{ij}$} \\
			0 & \text{ otherwise}. 
		\end{cases}
	\end{align*}
	By Observation~\ref{obs:Z}, there exist spanning trees $T_0^*$ and $\hat{T}_0$ for $G_\mathcal{P}$ that are optimal with respect to $w_0^*$ and $\hat{w}_0$, respectively, which contain the same exact set of $\beta$-unbounded edges. Let $U$ represent this set of $\beta$-unbounded edges in $\hat{T}_0$ and $T^*_0$. Let $B^*$ be the set of $\beta$-bounded edges in $T^*_0$ and $\hat{B}$ be the set of $\beta$-bounded edges in $\hat{T}_0$. Because $\hat{T}_\mathcal{P}$ is an MST with respect to $\hat{w}$ we know that:
	\begin{align}
		\label{eq:hats}
		\hat{w}(\hat{T}_\mathcal{P}) &\leq \hat{w}(\hat{T}_0) = \hat{w}(U) + \hat{w}(\hat{B}).
	\end{align}
	We will use this to upper bound the weight of $\hat{T}_\mathcal{P}$ in terms of $T^*$. First we claim that
	\begin{equation}
		\label{eq:Lhatbound}
		\hat{w}(\hat{B}) \leq \beta w^*(T_\mathcal{P}^*).
	\end{equation}
	This follows from the following sequence of inequalities:
	\begin{align*}
		\hat{w}(\hat{B}) 
		&= \hat{w}_0(\hat{B}) & \text{ since $\hat{w}$ and $\hat{w}_0$ coincide on $\beta$-bounded edges} \\
		&= \hat{w}_0(\hat{B}) + \hat{w}_0(U) & \text{ since $\hat{w}_0$ is zero on $\beta$-unbounded edges} \\
		&= \hat{w}_0(\hat{T}_0)  & \text{ since $\hat{T}_0 = \hat{B} \cup U$} \\
		& \leq \hat{w}_0(T_0^*) & \text{ since $\hat{T}_0$ is optimal for $\hat{w}_0$} \\
		& = \hat{w}_0(B^*) & \text{ since $\hat{w}_0$ is zero on $\beta$-unbounded edges} \\
		& \leq \beta w^*_0(B^*) & \text{ since $\hat{w}_0 \leq \beta w^*_0$ on $\beta$-bounded edges}\\
		%			&= w^*_0(B^*) + w^*_0(U) & \text{since $w_0^*$ is zero on $\beta$-unbounded}  \\
		&= \beta w^*_0(T_0^*) & \text{since $T_0^* = B^* \cup U$ and $w^*_0(U) = 0$}  \\
		&\leq \beta w^*_0(T_\mathcal{P}^*) & \text{since $T_0^*$ is optimal for $w_0^*$}  \\
		& \leq \beta w^*(T_\mathcal{P}^*) & \text{ since $w^*_0 \leq w^*$ for all edges.}
	\end{align*}	
	Next we bound $\hat{w}(U)$. From Lemma~\ref{lem:unboundededges}, we know that $\hat{w}_{ij} \leq {\beta}/({\beta-1}) \min \{w_\mathcal{X}(T_i), w_\mathcal{X}(T_j)\}$ for every $(v_i, v_j) \in U$. Because $\hat{T}_\mathcal{P}$ is a tree on $G_\mathcal{P}$, we know by Observation~\ref{lem:treeorient} that we can orient its edges in such a way that each node in $V_\mathcal{P} = \{v_1, v_2, \hdots, v_t\}$ has at most one outgoing edge. 
	We can therefore assign each $(v_i, v_j) \in U$ to one of its nodes in such a way that each node in $V_\mathcal{P}$ is assigned at most one edge from $U$. Assume without loss of generality that we write edges in such a way that edge $(v_i, v_j) \in U$ is assigned to node $v_i$. Thus,
	\begin{equation}
		\label{eq:small}
		\hat{w}(U) = \sum_{(v_i, v_j) \in U} \hat{w}_{ij} \leq \sum_{(v_i, v_j) \in U} \frac{\beta}{\beta-1} w_\mathcal{X}(T_i) \leq \frac{\beta}{\beta-1} \sum_{i = 1}^t w_\mathcal{X}(T_i).
	\end{equation}
	%
	Combining these gives our final bound
	\begin{align*}
		w_\mathcal{X}(\hat{T}) 
		&= \hat{w}(\hat{T}_\mathcal{P}) + \sum_{i = 1}^t w_\mathcal{X}(T_i)  & \text{by Eq.~\eqref{eq:That}}\\
		&\leq \hat{w}(\hat{B}) + \hat{w}(U) +  \sum_{i = 1}^t w_\mathcal{X}(T_i)  & \text{by Eq.~\eqref{eq:hats}}\\
		& \leq \beta w^*(T^*_\mathcal{P}) + \left(\frac{\beta}{\beta-1} + 1\right) \sum_{i = 1}^t w_\mathcal{X}(T_i) & \text{by Eqs.~\eqref{eq:Lhatbound} and~\eqref{eq:small}}\\
		&\leq \max \left\{ \beta, \frac{\beta}{\beta - 1} + 1 \right\} \left(w^*(T_\mathcal{P}^*) +  \sum_{i = 1}^k w_\mathcal{X}(T_i) \right)  \\
		%			&\leq \max \left\{ \beta, 1 + \frac{\beta}{\beta - 1} \right\} \left(w^*(T_\mathcal{P}^*) +  \sum_{i = 1}^k w_\mathcal{X}(T_i) \right)  \\
		&= \beta w_\mathcal{X}(T^*) &\text{ by Eq.~\eqref{eq:Tstar} and our choice of $\beta$.}
	\end{align*}
	For the last step that we have specifically chosen $\beta = (3+\sqrt{5})/2$ to ensure that $\beta = 1 + \beta/(\beta-1)$, as this leads to the best approximation guarantee using the above inequalities. 
\end{proof}

Using a similar proof technique as Theorem~\ref{thm:main} we obtain the following result, showing that Algorithm~\ref{alg:mfcapprox} is a learning-augmented algorithm for metric MST whose performance depends on the $\gamma$-overlap of the initial forest. 
\begin{theorem}
	\label{thm:learning}
	Let $G_\mathcal{X}$ be an implicit metric graph and $\mathcal{P}$ be an initial partitioning with $\gamma$-overlap $\gamma = \gamma(\mathcal{P})$. Algorithm~\ref{alg:mfcapprox} returns a spanning tree of $\hat{T}$ of $G_\mathcal{X}$ that satisfies
	\begin{equation}
		w_\mathcal{X}(T_\mathcal{X}) \leq w_\mathcal{X}(\hat{T}) \leq \beta w_\mathcal{X}(T_\mathcal{X})
	\end{equation}
	where $T_\mathcal{X}$ is an MST of $G_\mathcal{X}$ and $\beta = \frac{1}{2}\left(2\gamma + 1 + \sqrt{4\gamma + 1} \right) \leq 2\gamma+ 1$.
\end{theorem}
\begin{proof}
	We use the same terminology and notation as in the proof of Theorem~\ref{thm:main}. The only difference is that we do not necessarily use $\beta = (3+ \sqrt{5})/2$. For an arbitrary $\beta \geq 1$, we can still prove in the same way that
	\begin{align}
		\label{eq:start}
		w_\mathcal{X}(\hat{T}) \leq  \beta w^*(T^*_\mathcal{P}) + \left(\frac{\beta}{\beta-1} + 1\right) \sum_{i = 1}^t w_\mathcal{X}(T_i).
	\end{align}
	The $\gamma$-overlap of the initial forest implies there exists an MST $T_\mathcal{X}$ of $G_\mathcal{X}$ satisfying:
	\begin{equation}
		\label{eq:ieratio}
		\sum_{i = 1}^t w_\mathcal{X}(T_i) = \gamma w_\mathcal{X}(I_\mathcal{X}),
	\end{equation}
	where $I_\mathcal{X}$ is the set of edges of $T_\mathcal{X}$ inside components $\mathcal{P}$ of the initial forest. Let $B_\mathcal{X}$ be the set of edges in $T_\mathcal{X}$ that cross between components, so that $w_\mathcal{X}(T_\mathcal{X}) = w_\mathcal{X}(B_\mathcal{X}) + w_\mathcal{X}(I_\mathcal{X})$. Since $T_\mathcal{X}$ is a spanning tree, $B_\mathcal{X}$ must contain a path between every pair of components, meaning that $B_\mathcal{X}$ corresponds to a spanning subgraph of the coarsened graph $G_\mathcal{P}$. Since $T_\mathcal{P}^*$ defines an MST of $G_\mathcal{P}$ with respect to $w^*$, which captures the minimum distances between pairs of components, we know
	\begin{equation}
		w^*(T_\mathcal{P}^*) \leq w_\mathcal{X}(B_\mathcal{X}).
	\end{equation}
	Putting the pieces together we see that
	\begin{equation}
		w_\mathcal{X}(\hat{T}) \leq  \beta w_\mathcal{X}(B_\mathcal{X}) + \left( 1 + \frac{\beta}{\beta-1}\right) \gamma w_\mathcal{X}(I_\mathcal{X}) \leq \max \left\{\beta, \gamma\left(1 + \frac{\beta}{\beta -1}\right)  \right\} w_\mathcal{X}(T_\mathcal{X}).
	\end{equation}
	This will hold for any choice of $\beta \geq 1$. In order to prove the smallest approximation guarantee, we choose $\beta$ satisfying:
	\begin{equation*}
		\beta = \gamma\left(1 + \frac{\beta}{\beta -1}\right).
	\end{equation*}
	The solution for this equation under constraint $\beta \geq 1$ and $\gamma \geq 1$ is 
	\begin{equation*}
		\beta = \frac{1}{2}\left(2\gamma + 1 + \sqrt{4\gamma + 1} \right) \leq 2\gamma+ 1.
	\end{equation*}
\end{proof}


\subsection{Runtime analysis and practical considerations}
Algorithm~\ref{alg:mfcapprox} finds the distance between each point in $\mathcal{X}$ and each of the $t$ component representatives, for a total of $O(nt)$ distance queries. It then finds an MST of a dense graph with ${t \choose 2}$ edges, which has runtime and space requirements of $\tilde{O}(t^2)$. Thus, the algorithm has subquadratic memory and query complexity as long as $t = o(n)$. The runtime is $\tilde{O}(nt\texttt{Q}_\mathcal{X} + t^2)$ where $\texttt{Q}_\mathcal{X}$ is the complexity for one distance query in $\mathcal{X}$, which also is subquadratic as long as $t\texttt{Q}_\mathcal{X} = o(n)$. In settings where $\texttt{Q}_\mathcal{X} = \tilde{O}(1)$, the memory, runtime, and query complexity are all subquadratic as long as $t = o(n)$.


\paragraph{Full runtime using $k$-center initialization.} The practical utility of our full MST pipeline also depends on the time it takes to find an initial forest, which depends on various design choices and trade-offs when using any strategy. For intuition we provide a rough complexity analysis for the $k$-center strategy assuming an idealized case of balanced clusters. The simple $2$-approximation for $k$-center chooses an arbitrary first cluster center, and chooses the $i$th cluster center to be the point with maximum distance from the first $i - 1$ centers~\cite{gonzalez1985clustering}. This requires $O(nt)$ distance queries. We can use the cluster centers as the component representatives for \textsf{MFC-Approx}, which allows us to compute $\hat{w}$ without any additional queries. If clusters are balanced in size, we can compute minimum spanning trees for all clusters using $O(n^2/t)$ queries and memory and a runtime of $\tilde{O}(\texttt{Q}_\mathcal{X}n^2/t)$, simply by querying all inner-cluster edges and running a standard MST algorithm. In this balanced-cluster case, combining the initial forest complexity with the complexity of our MFC algorithm, the entire pipeline for finding a spanning tree takes $\tilde{O}(\texttt{Q}_\mathcal{X}(n^2/t + nt) + t^2)$ time. This is minimized by choosing $t = \sqrt{n}$ clusters, leading to a complexity that grows as $n^{1.5}$. For unbalanced clusters, one must consider different trade-offs for cluster-balancing strategies, which could be beneficial for runtime but may affect initial cluster quality.
We could also improve the runtime at the expense of initial forest quality by not computing an exact MST for each cluster. For example, we could recursively apply our entire MFC framework to find a spanning tree of each cluster. 


\paragraph{Practical improvements.} There are several ways to relax our MFC framework to make our approach faster while still satisfying strong approximation guarantees. For metrics with high query complexity, we can use approximate queries with only minor degradation in approximation guarantees. For example, for high-dimensional Euclidean distance we can apply Johnson-Lindenstrauss transformations to reduce the query complexity while approximately maintaining distances. As another relaxation, we can replace the exact nearest neighbor search subroutine in \textsf{MFC-Approx} with an approximate nearest neighbor search. If for some $\varepsilon > 0$ we find a $(1+\varepsilon)$-approximate nearest neighbor in each component for every component representative $s_i$, this will make our approximation guarantees worse by at most a factor $(1+\varepsilon)$. There are also numerous opportunities for parallelization, such as parallelizing distance queries and MST computation for components. 

We can also incorporate heuristics to improve the spanning tree quality of our algorithm with little effect on runtime. As a specific example, when approximating the distance between components $P_i$ and $P_j$ of the initial forest, we could compute $\tilde{x}_i = \argmin_{x \in P_i} d(x,s_j)$ and $\tilde{x}_j = \argmin_{x \in P_j} d(x,s_i)$ and then use the following weight for the coarsened graph:
\begin{equation*}
	\tilde{w}_{ij} = \min \{d(\tilde{x}_i, s_j),d(\tilde{x}_j, s_i), d(\tilde{x}_j,\tilde{x}_i)\}.
\end{equation*}
This differs from Algorithm~\ref{alg:mfcapprox} only in that it additionally checks the distance between $d(\tilde{x}_j,\tilde{x}_i)$ to see if this provides an even closer pair of points between $P_i$ and $P_j$. Although this does not always improve results, it can never be worse in terms of approximations. Figure~\ref{fig:wijhat} provides an example where this strategy would find the optimal distance $w_{ij}^*$, which is strictly better than $\hat{w}_{ij}$. An interesting future direction is to implement this and also explore other heuristics that could improve the practical performance of our method without affecting our theoretical guarantees.
