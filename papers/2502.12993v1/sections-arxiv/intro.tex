\section{Introduction}
\label{sec:intro}
Finding a minimum spanning tree of a graph is a classical combinatorial problem with well-known algorithms dating back to the early and mid 1900s~\cite{boruvka1926jistem,prim1957shortest,kruskal1956shortest}. A widely-studied special case in theory and practice is the \emph{metric} MST problem, where each node corresponds to a point in a metric space and every pair of points defines an edge with weight equal to the distance between points. Finding a metric MST has widespread applications including network design~\cite{loberman1957formal}, approximation algorithms for traveling salesman problems~\cite{held1970traveling}, and feature selection~\cite{labbe2023dendrograms}. The problem also has a very well-known connection to hierarchical clustering~\cite{gower1969minimum} and has been used as a key step in clustering astronomical data~\cite{barrow1985minimal,march2010fast}, analyzing gene expression data~\cite{xu2002clustering}, document clustering~\cite{xu19972d}, and various image segmentation and classification tasks~\cite{xu19972d,an2000fast,la2022ocmst}.


This paper is motivated by challenges in efficiently computing metric MSTs in modern machine learning (ML) and data mining applications. The most fundamental challenge is simply the massive size of modern datasets. In theory one can always find an MST for $n$ points by computing all $O(n^2)$ distances and running an existing MST algorithm. However, this quadratic complexity is far too expensive both in terms of runtime and memory for massive datasets. 
A second challenge is handling complicated metric spaces. Most existing algorithms for metric MSTs are designed for Euclidean distances or other simple metric spaces, often with a particular focus on small-length feature vectors~\cite{march2010fast,agarwal1990euclidean,shamos1975closest,vaidya1988minimum,arya2016fast,wang2021fast}.
While useful in certain settings, these are limited in their applicability to high-dimensional feature spaces and complex distance functions. Many modern ML tasks focus on non-Cartesian data (e.g., videos, images, text, nodes in a graph, or even entire graphs) which must be classified, clustered, or otherwise compared, possibly after being embedded into some metric space. In these settings, even querying the distance score between two data points becomes non-trivial and often involves some level of uncertainty. 
Computing distances for popular non-Euclidean metrics like Levenshtein distance or graph kernels is far more expensive than computing Euclidean distances.
This motivates a body of research on minimizing the number of queries needed to find a minimum spanning tree~\cite{bateni2024metric,erlebach2022learning,hoffman2008computing,megow2017randomization}. 
%
% Another challenge is simply the massive size of modern datasets, which poses a significant challenge even when querying distances is not a bottleneck. In theory one can always find an MST for $n$ points by computing all $O(n^2)$ distances and running an existing MST algorithm. However, this quadratic complexity is far too expensive both in terms of runtime and memory for massive datasets. 

These applications and challenges motivate new approaches for efficiently finding good spanning trees for a set of points in an arbitrary metric space. Ideally, we would like an algorithm whose memory, runtime, and distance query complexity are all $o(n^2)$, while still being able to find spanning trees with strong theoretical guarantees in arbitrary metric spaces. 
Unfortunately,
there are known hardness results that pose challenges for obtaining meaningful subquadratic algorithms. In particular, it is known that finding any constant factor approximation for an MST in an arbitrary metric space requires 
knowing $\Omega(n^2)$ edges in the underlying metric graph~\cite{indyk1999sublinear}. Overcoming this inherent challenge requires exploring additional assumptions and alternative types of theoretical approximation guarantees.

\paragraph{The present work: metric forest completion.} 
To achieve our design goals and overcome existing hardness results, we take our inspiration from the nascent field of learning-augmented algorithms, also known as algorithms with predictions~\cite{mitzenmacher2022algorithms}. The learning-augmented paradigm assumes access to an ML heuristic that provides a prediction or ``warm start'' that is useful in practice but does not come with a priori theoretical guarantees. The goal is to design an algorithm that (1) comes with improved theoretical guarantees when the ML heuristic performs well, and (2) recovers similar worst-case guarantees if the ML heuristic performs poorly. Improved theoretical guarantees can take various forms, including faster runtimes (e.g., for sorting~\cite{bai2023sorting}, binary search~\cite{lin2022learning} or maximum $s$-$t$ flows~\cite{davies2023predictive}), improved approximation factors (e.g., for NP-hard clustering problems~\cite{ergun2022learning,nguyen2023improved}), better competitive ratios for online algorithms (e.g., for ski rental problems~\cite{shin2023improved}), or some combination of the above (e.g., better trade-offs for space requirements vs.\ false positive rates for Bloom filters~\cite{kraska2018case}). These improved guarantees are given in terms of some parameter measuring the quality of the prediction, which is typically not known in practice but provides a concrete measure of error that can be used in theoretical analysis.

In this paper we specifically assume access to fragments of a spanning tree for a metric MST problem (called the \emph{initial forest}), that takes the form of a spanning tree for each component of some partitioning of the data objects. This input can be interpreted as a heuristic approximation for the forest that would be obtained by running a few iterations of a classical MST algorithm such as Kruskal's~\cite{kruskal1956shortest} or Boruvka's~\cite{boruvka1926jistem}. Given this input, we formalize the \textsc{Metric Forest Completion} problem (MFC), whose goal is to find a minimum-weight set of edges that connects disjoint components to produce a full spanning tree.
% An initial forest can be efficiently obtained in a number of different ways, including fast clustering algorithms or by finding connected components in an approximate $k$-nearest neighbors graph. 
Although optimally solving MFC takes $\Omega(n^2)$ distance queries, we design a subquadratic approximation algorithm for MFC and prove a learning-augmented style approximation guarantee for the original metric MST problem. To summarize, we have the following contributions. 
\begin{itemize}[leftmargin=10pt,itemsep=0pt]
	\item \textbf{New algorithmic framework.} We introduce \textsc{Metric Forest Completion} for large-scale metric MST problems, along with strategies for computing an initial forest and a discussion of how our problem fits into the recent framework of learning-augmented algorithms.
	\item \textbf{Approximate completion algorithm.}  We prove that optimally solving MFC requires $\Omega(n^2)$ edge queries, but provide an approximation algorithm with approximation factor $\approx 2.62$. Our algorithm has a query complexity of $o(n^2)$ as long as the initial forest has $o(n)$ components. 
	\item \textbf{Learning-augmented approximation guarantees.} We prove that if the initial forest $\gamma$-overlaps with an optimal MST, then our algorithm is a $(2\gamma+1)$-approximation algorithm for the metric MST problem. If $\gamma = 1$, this means all edges in the forest are contained in an optimal MST. A precise definition for $\gamma > 1$ is given in Section~\ref{sec:mfc}.
	\item \textbf{Experiments.} We show that our method is extremely scalable and obtains very good results on synthetic and real datasets. We also show that simple heuristics provide initial forests with $\gamma$-overlap values that are typically smaller than 2 on various datasets and distance functions, including many non-Euclidean metrics.
\end{itemize}
% We remark finally that although our approximation proofs are somewhat technical, the algorithm itself is simple to implement and is highly parallelizable. 
% This highlights promising opportunities for future research on fast parallelized methods for finding spanning trees in arbitrary metric spaces.