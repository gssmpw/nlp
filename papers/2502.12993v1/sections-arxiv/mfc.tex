\section{Metric Forest Completion}
\label{sec:mfc}
We now formalize our \mfc{} (MFC) framework\footnote{This is distinct from two other MST-related concepts that also use the acronym MFC; see Section~\ref{sec:related} for details.}
which assumes access to an initial forest that is then grown into a full spanning tree.

 \subsection{Formalizing the MFC problem}
\label{sec:definemfc}
As a starting point for the metric MST problem on $(\mathcal{X},d)$, we assume access to a partitioning $\mathcal{P} = \{P_1, P_2, \hdots , P_t\}$ where $\mathcal{X} = \bigcup_{i = 1}^t P_i$ and $P_i \cap P_j = \emptyset$ for $i \neq j$. For each component $P_i$ we have a partition spanning tree $T_i = (P_i, E_{T_i})$ for that component.
% , which may or may not be an optimal MST for $P_i$. 
See Figure~\ref{fig:warmstart} for an illustration. Let $G_t = (\mathcal{X}, E_t)$ represent the union of these trees, which has the same node set as $G_\mathcal{X}$, and edge set $E_t = \bigcup_{i = 1}^t E_{T_i}$. Each set $P_i$ for $i \in [t]$ defines a group of points in $\mathcal{X}$ as well as a connected component of $G_t$. We refer to this as the \emph{initial forest} for MFC. To provide intuition, the initial forest can be viewed as a proxy for the forest obtained at an intermediate step of Kruskal's or Boruvka's algorithm (see Figure~\ref{fig:partial_MST_1}). While this serves as a useful analogy, we stress that the partitioning will typically be obtained using much cheaper methods and will not satisfy any formal approximation guarantees. Section~\ref{sec:initialforest} covers practical considerations about strategies, runtimes, and quality measures for an initial forest. For now we simply assume it is given as a ``good enough'' starting point, that will ideally overlap, even if not perfectly, with some true MST (see Figure~\ref{fig:overlap}). 
%The goal is to efficiently grow or \emph{complete} this partial tree into a spanning tree for $G_\mathcal{X}$. 

%Our theoretical analysis requires very few assumptions about the partitioning $\mathcal{P}$ and partition spanning trees (initial forest) $\{T_i \colon i = 1,2, \hdots t\}$ that are given as input to the \mfc{} problem. 
%In particular, we need not assume that $T_i$ is a minimum spanning tree or even a good spanning tree of $P_i$. Similarly, the components are not required to be sets of points that are close together in the metric space in order for the problem to be well-defined. Nevertheless, \textit{in practice} the hope is that $\mathcal{P}$ identifies groups of nearby points in $\mathcal{X}$ and that $T_i$ is a reasonably good spanning tree for $P_i$ for each $i \in [t] = \{1,2, \hdots, t\}$. Appendix~\ref{sec:initialsubtree} covers two practical strategies and corresponding runtimes for finding an initial forest. In summary these are:
%
%\textit{Strategy 1: $k$-centering.} Apply a fast $k$-centering heuristic to $\mathcal{X}$ to form $\mathcal{P}$, e.g., using a simple 2-approximation~\cite{gonzalez1985clustering} or fast distributed methods~\cite{malkomes2015fast,mcclintock2016efficient}.
%
%\textit{Strategy 2: $k$-NN graph.}  Compute an approximate $k$-nearest neighbor graph of $\mathcal{X}$ for a reasonably small $k$, e.g., via the scalable $k$-NN descent algorithm~\cite{dong2011efficient} or its distributed generalization~\cite{iwabuchi2023towards}.
%
%Similar but more restrictive strategies have been used by previous heuristics for Euclidean MSTs (see Appendix~\ref{sec:initialsubtree}).  

%\textbf{Defining the MFC problem.}
% Given an initial forest, the goal of 
\mfc{} seeks to connect the initial forest into a spanning tree for $G_\mathcal{X}$. Let $P(x) \in \mathcal{P}$ denote the component that $x \in \mathcal{X}$ belongs to. The set of inter-component edges is
\begin{equation*}
	\label{eq:intercomponent}
	\mathcal{I} = \{ (x, y) \in \mathcal{X} \times \mathcal{X} \colon P(x) \neq P(y) \}.
\end{equation*}
We wish to find a minimum weight set of edges $M \subseteq \mathcal{I}$ so that $M\cup E_t$ defines a connected graph on $\mathcal{X}$. If $M$ satisfies these constraints we say it is a valid \emph{completion set} and that $M$ \emph{completes} $\mathcal{P}$. 
The MFC problem can then be written as
\begin{equation}
	\label{eq:mfc}
	\begin{array}{ll}
		\minimize & w_\mathcal{X}(M) + w_\mathcal{X}(E_t)\\
		\text{subject to} & M \emph{ completes } \mathcal{P}.
	\end{array}
\end{equation}
Let $M^*$ denote an optimal completion set. The graph $T^* = (\mathcal{X}, E_t \cup M^*)$ is then guaranteed to be a tree (see Figure~\ref{fig:opt_mfc}); if not we could remove edges to decrease the weight while still spanning $\mathcal{X}$. If the initial forest $G_t$ is in fact contained in some optimal spanning tree of $G_\mathcal{X}$ (which would be the case if it were obtained by running a few iterations of Kruskal's or Boruvka's algorithm), then solving MFC would produce an MST of $G_\mathcal{X}$. In practice this will typically not be the case, but the problem remains well-defined regardless of any assumptions about the quality of the initial forest.

\begin{figure}[t]
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/true_MST.pdf}
		\caption{True metric MST}
		\label{fig:truemst}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/warm-start.pdf}
		\caption{Partial spanning tree ($\mathcal{P}$; $\{T_i\})$}
		\label{fig:warmstart}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/true_MST_clusters.pdf}
		\caption{True MST overlap with $\mathcal{P}$}
		\label{fig:overlap}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/opt_mstc.pdf}
		\caption{MFC solution ($M^*$ in orange)}
		\label{fig:opt_mfc}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/componentgraph.pdf}
		\caption{Coarsened graph $G_\mathcal{P}$ w.r.t.\ $w^*$}
		\label{fig:coarsenedgraph}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/component_mst.pdf}
		\caption{MST of $G_\mathcal{P}$ w.r.t.\ $w^*$}
		\label{fig:mstcoarsened}
	\end{subfigure}
	\caption{(a) We display an optimal metric MST for a toy example with $|\mathcal{X}| = 75$ points. Our framework and algorithm apply to general metric spaces, but for visualization purposes our figures focus on 2-dimensional Euclidean space. (b) The \mfc{} problem is given a partitioning $\mathcal{P}$ and spanning trees $\{T_i\}$ for components of the partition. For this illustration we used a $k$-means algorithm with $k = 5$ computed optimal spanning trees of components using the naive approach. (c)~The true MST overlaps significantly with the initial partial spanning tree, but its induced subgraph on each component is not necessarily connected. For this example, the $\gamma$-overlap (see Section~\ref{sec:learningaugmented}) is $\gamma \leq 1.12$. (d)~The optimal completion set $M^*$ is shown in orange; combining it with the spanning trees of the partial spanning tree produces a spanning tree for all of $\mathcal{X}$. (e)~The coarsened graph $G_\mathcal{P}$ has a node $v_i$ for each component $P_i \in \mathcal{P}$. Solving $O(t^2)$ bichromatic closest pair problems identifies the closest pair of points between each pair of clusters, defining an optimal weight function $w^*$ on $G_\mathcal{P}$. (f)~Finding the minimum-weight completion set $M^*$ amounts to finding the MST of $G_\mathcal{P}$ with respect to weight function $w^*$. }
	\label{fig:three_figures}
\end{figure}


The objective function in Eq.~\eqref{eq:mfc} includes the weight of the initial forest $w_\mathcal{X}(E_t)$. Although this is constant with respect to $M$ and does not affect optimal solutions, there are several reasons to incorporate this term explicitly. Most importantly, our ultimate goal is to obtain a good spanning tree for all of $G_\mathcal{X}$, and thus the weight of the full spanning tree (i.e., the objective in Eq.~\ref{eq:mfc}) is a more natural measure. Considering the weight of the full spanning tree also makes more sense in the context of our learning-augmented algorithm analysis, where the goal is to approximate the original metric MST problem on $G_\mathcal{X}$, under different assumptions about the initial forest. We note finally that excluding the term $w_\mathcal{X}(E_t)$ rules out the possibility of any meaningful approximation results. 
We prove the following result using a reduction from BCP to MFC, combined with a slight variation of a simple lower bound for monochromatic closest pair that was shown in Section 9 of Indyk~\cite{indyk1999sublinear}.
\begin{theorem}
\label{thm:hard}
Every optimal algorithm for MFC has $\Omega(n^2)$ query complexity. Furthermore, for any multiplicative factor $p \geq 1$ (not necessarily a constant), any algorithm that finds a set $M \subseteq \mathcal{I}$ that is feasible for~\eqref{eq:mfc} and satisfies $w_\mathcal{X}(M) \leq p \cdot w_{\mathcal{X}}(M^*)$ requires $\Omega(n^2)$ queries.
\end{theorem}
\begin{proof}
	Let $\mathcal{X}$ be a set of $n$ points that are partitioned into two sets $P_1$ and $P_2$ of size $n/2$. Define a distance function $d$ such that $d(a,b) = 1$ for a randomly chosen pair $(a,b) \in P_1 \times P_2$, and such that $d(x,y) = 2p$ for all other pairs $(x,y) \in {\mathcal{X} \choose 2} \backslash \{(a,b)\}$. Note that this $d$ is a metric. The MFC problem on this instance is identical to solving BCP on $P_1$ and $P_2$. The unique optimal solution is exactly $M^* = (a,b)$, and no other choice of $M \subseteq \mathcal{I}$ comes within a factor $p$ of this solution. Thus, any $p$-approximation algorithm must find the pair $(a,b)$ with distance 1 among a collection of $\Omega(n^2)$ pairs, where all other pairs have distance $2p$. This requires $\Omega(n^2)$ queries.
\end{proof}
Theorem~\ref{thm:hard} shows that it is impossible in general to find optimal solutions for MFC, or multiplicative approximations for $w_\mathcal{X}(M^*)$, in $o(n^2)$ time. However, this does not rule out the possibility of approximating the more relevant objective $w_\mathcal{X}(M) + w_\mathcal{X}(E_t)$.

\paragraph{The MFC coarsened graph.} The  MFC problem is equivalent to finding a minimum spanning tree in a \textit{coarsened graph} $G_\mathcal{P} = (V_\mathcal{P}, E_\mathcal{P})$ with node set $V_\mathcal{P} = \{v_1, v_2, \hdots, v_t\}$ where $v_i$ represents component $P_i$. We refer to $v_i$ as the $i$th \textit{component node}. This graph is complete: $E_\mathcal{P}$ includes all pairs of component nodes. Figure~\ref{fig:coarsenedgraph} provides an illustration of the coarsened graph. Finding an MFC solution $M^* \subseteq \mathcal{I}$ is equivalent to finding an MST in $G_\mathcal{P}$ (see Figure~\ref{fig:mstcoarsened}) with respect to the weight function $w^* \colon E_\mathcal{P} \rightarrow \mathbb{R}^+$ defined for every $i, j \in \{1,2, \hdots, t\}$ by
\begin{equation}
	\label{eq:wstar}
	w^*_{ij} = w^*(v_i, v_j) = d(P_i, P_j) = \min_{x \in P_i, y\in P_j} d(x,y).
	%, \quad \text{ for every $P_i, P_j \in \mathcal{P} \times \mathcal{P}.$}
\end{equation}
Computing $w_{ij}^*$ exactly requires solving a bichromatic closest pair problem over sets $P_i$ and $P_j$. A straightforward approach for computing $w_{ij}^*$ is to check all $|P_i|\cdot |P_j|$ pairs of points in $P_i \times P_j$. The number of distance queries needed to apply this simple strategy to form all of $w^*$ is
\begin{equation*} 
	\frac{1}{2}\sum_{i = 1}^t |P_i| \cdot (n - |P_i|) = \frac{n^2}{2} - \frac{1}{2} \sum_{i = 1}^t |P_i|^2.
\end{equation*}
In a worst-case scenario where component sizes are balanced, we would need
$\Omega\left({t \choose 2} \frac{n}{t} \frac{n}{t}\right) = \Omega(n^2)$ 
queries, which is not surprising given Theorem~\ref{thm:hard}. Nevertheless, this notion of a coarsened graph will be very useful in developing approximation algorithms for MFC.

\subsection{Computing an initial forest}
\label{sec:initialforest}
Our theoretical analysis requires very few assumptions about the partitioning $\mathcal{P}$ and partition spanning trees $\{T_i \colon i = 1,2, \hdots t\}$ that are given as input to the \mfc{} problem. In particular, we need not assume that $T_i$ is a minimum spanning tree or even a good spanning tree of $P_i$. Similarly, the components are not required to be sets of points that are close together in the metric space in order for the problem to be well-defined. Nevertheless, \textit{in practice} the hope is that $\mathcal{P}$ identifies groups of nearby points in $\mathcal{X}$ and that $T_i$ is a reasonably good spanning tree for $P_i$ for each $i \in [t] = \{1,2, \hdots, t\}$. 

In order for our approach to be meaningful for large-scale metric MST problems, we must be able to obtain a reasonably good initial forest without this dominating our overall algorithmic pipeline.
Here we discuss two specific strategies for initial forest computations, both of which are fast, easy to parallelize, and motivated by techniques that are already being used in practice in large-scale clustering pipelines. In particular, there are already a number of existing heuristics for finding MSTs and hierarchical clusters that rely in some way on partitioning an initial dataset and then connecting or merging components~\cite{zhong2015fast,jothi2018fast,mishra2020efficient,chen2013clustering}. See Section~\ref{sec:related} for more details. These typically focus only on point cloud data, do not apply to arbitrary metric spaces, and do not come with any type of approximation guarantee. Nevertheless, they provide examples of fast heuristics for large-scale metric spanning tree problems, and serve as motivation for our more general strategies.

\paragraph{Strategy 1: Components of a $k$-NN graph.}
A natural way to obtain an initial forest for $G_\mathcal{X}$ is to compute an approximate $k$-nearest neighbor graph for a reasonably small $k$, which can be accomplished with the $k$-NN descent algorithm~\cite{dong2011efficient} or a recent distributed generalization of this method~\cite{iwabuchi2023towards}. This efficiently connects a large number of points using small-weight edges. The $k$-NN graph will often be disconnected, and we can use the set of connected components as our initial components $\mathcal{P} = \{P_1, P_2, \hdots , P_t\}$. The exact number of components will depend on the distribution of the data and the number of nearest neighbors computed. For larger values of $k$, the $k$-NN graph is more expensive to compute, but then there are fewer components to connect, so there are trade-offs to consider. The components of the $k$-NN graph will typically not be trees but will be sparse (each node has at most $O(k)$ edges), so we can use classical MST algorithms to find spanning trees for all components in $\sum_{i = 1}^t \tilde{O}(k \cdot |P_i|) = \tilde{O}(kn)$ time. The exact runtime of the $k$-NN descent algorithm depends on various parameters settings, but prior work reports an empirical runtime of $O(n^{1.14})$~\cite{dong2011efficient}, with strong empirical performance across a range of different metrics and dataset sizes. We remark that $k$-NN computations have already been used elsewhere as subroutines for large-scale Euclidean MST computations~\cite{almansoori2024fast,chen2013clustering}.

\paragraph{Strategy 2: Fast clustering heuristics.}
Another approach is to form components of the initial forest $\mathcal{P} = \{P_1, P_2, \hdots, P_t\}$ by applying a fast clustering heuristic to $\mathcal{X}$ such as a distributed $k$-center algorithm~\cite{malkomes2015fast,mcclintock2016efficient}. Even the simple sequential greedy 2-approximation algorithm for $k$-center can produce an approximate clustering using $O(nk)$ queries~\cite{gonzalez1985clustering}. Approximate or exact minimum spanning trees for each $P_i$ can be found in parallel. The remaining step is to find a good way to connect the forest. Similar approaches that partition the initial dataset using $k$-means clustering also exist~\cite{zhong2015fast,jothi2018fast}, though this inherently forms clusters based on Euclidean distances. For all of these clustering-based approaches, the number of components $t$ for the initial forest is easy to control since it exactly corresponds to the number of clusters $k$. Smaller $k$ leads to larger clusters, and hence finding an MST of each $P_i$ is more expensive. However, there are then fewer components to connect, so there is again a trade-off to consider. 
%
We remark that it may seem counterintuitive to use a clustering method as a subroutine for finding an MST, since one of the main reasons to compute an MST is to perform clustering. We stress that Strategy 2 uses a cheap and fast clustering method that identifies sets of points that are somewhat close in the metric space, without focusing on whether they are good clusters for a downstream application. This speeds up the search for a good spanning tree, which can be used as one step of a more sophisticated hierarchical clustering pipeline. \\


\subsection{MFC as a learning-augmented framework}
\label{sec:learningaugmented}
Our approach fits the framework of learning-augmented algorithms in that the initial forest can be viewed as a prediction for a partial metric MST, such as the forest obtained by running several iterations of a classical MST algorithm. 
In an ideal setting, the initial forest would be a subset of an optimal MST. If so, then an optimal solution to MFC would produce an optimal metric MST. We relax this by introducing a more general way to measure how much an optimal MST ``overlaps'' with initial forest components. 
Let $\mathcal{T}_\mathcal{X}$ represent the set of minimum spanning trees of $G_\mathcal{X}$, and $T \in \mathcal{T}_\mathcal{X}$ denote an arbitrary MST.
For components $\mathcal{P} =  \{P_1, P_2, \hdots, P_t\}$, let $T(P_i)$ denote the induced subgraph of $T$ on $P_i$, and let $T(\mathcal{P}) = \bigcup_{i = 1}^t T(P_i)$ denote the edges of $T$ contained inside components of $\mathcal{P}$. 
% Then, $w(T(\mathcal{P}))$ is the total weight that an optimal MST $T$ places inside components of $\mathcal{P}$. 
Larger values of $w_\mathcal{X}(T(\mathcal{P}))$ indicate better initial forests, since this means an optimal MST places a larger weight of edges inside these components. 
% Given components $\mathcal{P} =  \{P_1, P_2, \hdots, P_t\}$ and corresponding spanning trees $\{T_1, T_2, \hdots, T_t$\}, 
We define the $\gamma$-overlap for the initial forest to be the ratio
\begin{equation}
\label{eq:gamma}
\gamma(\mathcal{P})  = \frac{w_\mathcal{X}(E_t)}{\max_{T \in \mathcal{T}_\mathcal{X}} w_\mathcal{X}(T(\mathcal{P}))}.
\end{equation}
This measures the weight of edges that the initial forest places inside $\mathcal{P}$, relative to the weight of edges an optimal MST places inside $\mathcal{P}$. 
When $\mathcal{P}$ is clear from context we will simply write $\gamma$. 
Lower ratios for $\gamma$ are better. 
In the denominator, we maximize $w_\mathcal{X}(T(\mathcal{P}))$ over all optimal spanning trees since any MST of $G_\mathcal{X}$ is equally good for our purposes; hence we are free to focus on the MST that overlaps most with $\mathcal{P}$. The optimality of $T$ implies that $w_\mathcal{X}(T(P_i)) \leq w_\mathcal{X}(T_i)$ for every $i \in \{1,2, \hdots, t\}$, which in turn implies that $\gamma(\mathcal{P}) \geq 1$ always. Even if $T_i$ is a minimum spanning tree for $P_i$, it is possible to have $w_\mathcal{X}(T(P_i)) < w_\mathcal{X}(T_i)$ since the induced subgraph $T(P_i)$ is not necessarily connected (see Figure~\ref{fig:overlap} for an example). We achieve the lower bound $\gamma = 1$ exactly in the idealized setting where the initial forest is contained in some optimal metric MST. In practice, we expect the clusters $P_i$ and spanning trees $T_i$ to be imperfect in the sense that connecting them will likely not provide an optimal MST for $G_X$. However, for an initial forest where each $P_i$ is a set of nearby points and $T_i$ is a reasonably good spanning tree for $P_i$, we would expect $\gamma$ to be larger than 1 but still not too large. Figure~\ref{fig:overlap} provides an example where we can certify that $\gamma \leq 1.12$ by comparing against one optimal MST. We later show experimentally that we can quickly obtain initial forests with small $\gamma$-overlap for a wide range of datasets and metrics. 

\paragraph{Learning-augmented algorithm guarantees.}
 We typically would not compute the ratio $\gamma$ for large-scale applications as this would be even more computationally expensive than solving the original metric MST problem. However, this serves as a theoretical measure of initial forest quality when proving approximation guarantees, following the standard approach in the analysis of learning-augmented algorithms. 
In the worst case, no algorithm making $o(n^2)$ queries can obtain a constant factor approximation for metric MST in arbitrary metric spaces~\cite{indyk1999sublinear}.  Following the standard goal of learning-augmented algorithms, we will improve on this worst-case setting when the initial forest is good; otherwise we will recover the worst-case behavior. Formally, we will consider the initial forest to be good when $\gamma$ is finite and the number of components is $t = o(n)$. For this setting, Section~\ref{sec:algs} will present a learning-augmented $(2\gamma+1)$-approximation algorithm for metric MST with subquadratic complexity. There are two ways to see that this is no worse than the worst-case guarantee using no initial forest, depending on whether we focus on worst-case runtimes or wort-case approximation factors. From the perspective of runtimes, if $t = \Omega(n)$ we can default to the worst-case quadratic complexity algorithms that provide an optimal MST. From the perspective of approximation factors, we certainly do no worse than the worst-case approximation factor, which is infinite for subquadratic algorithms. We will state our approximation results and runtimes more precisely in Section~\ref{sec:algs}.

