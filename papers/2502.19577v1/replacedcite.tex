\section{Related work}
The development of Prototypical Part Network research was initiated by the ProtoPNet architecture ____. ProtoPNet works by comparing parts of an image to learned prototypes, which are meant to represent semantically coherent concepts. This approach allows the model to make predictions based on the similarity of image parts to these prototypes, offering a form of reasoning that is qualitatively similar to human decision-making.

Several challenges with the original ProtoPNet have led to the development of more advanced models. The original ProtoPNet used a fixed number of class-specific prototypes. This means that each class had a pre-determined number of prototypes associated with it, which could lead to large explanations and redundant prototypes____. ProtoPShare was developed to address this issue by enabling prototypes to be shared across different classes, reducing the total number of prototypes needed and allowing the model to find similarities between classes____. ProtoPool built on this work and introduced a fully differentiable assignment of prototypes to classes, allowing end-to-end training of the model____. 

Other approaches have explored different prototype representations. Deformable ProtoPNet introduced spatially flexible prototypes____. ST-ProtoPNet aimed to improve classification accuracy by learning support and trivial prototypes, drawing an analogy with Support Vector Machine theory____. On a different note, ProtoTree looked at replacing the final linear layer with a decision tree____, while PIP-Net focused on producing sparse explanations____. Most of the presented works described above use different variations of CNN backbones, but the ProtoVit architecture recently looked at leveraging Vision Transformers (ViT)____.

\begin{table*}[t]
\centering
\caption{\textbf{Properties and associated metrics for evaluating interpretability}. $^*$ denotes metrics from the FunnyBirds framework. $^\dagger$ indicates metrics from the literature adapted to the FunnyBirds dataset}
\label{tab:interpretability_metrics}
\rowcolors{2}{gray!15}{white} % Alternating row colors
\begin{tabular}{m{0.15\linewidth}m{0.55\linewidth}m{0.25\linewidth}}
\hline
\textbf{Property} & \textbf{Definition} & \textbf{Associated Metrics} \\ \midrule
\textbf{Correctness} &  The explanation faithfully represents the model’s behavior. & SD$^*$ \\ 
\textbf{Completeness} & The model’s behavior is fully captured by the explanation. & CSDC$^*$, PC$^*$, DC$^*$, D$^*$ \\ 
\textbf{Contrastivity} &  Discriminative parts are correctly captured by the explanations & TS$^*$ \\ 
\textbf{Consistency} &  Prototypes are consistent in the input space. & Consistency$^\dagger$ \\ 
\textbf{Stability} & Prototype attribution should be stable under small perturbations. & Stability$^\dagger$ \\
\textbf{Compactness} &  The explanation is compact to be intelligible by the user. & Global$^\dagger$ and Local size$^\dagger$ \\
\textbf{Composition} & The explanation presentation should reflect the model’s behavior. &  SEC  \\ \bottomrule
\end{tabular}
\end{table*}

While part-prototype networks are theoretically designed to be interpretable, researchers have investigated their interpretability and identified several limitations that can undermine this promise. Studies have shown that such models may exhibit: (i) a \textit{semantic gap}, where prototypes fail to consistently represent the same concepts across different images____, and (ii) \textit{spatial misalignment}, where the pixels used by the model for predictions are not correctly localized____.


While several studies have examined specific aspects of interpretability evaluation____, none have provided a holistic and quantitative assessment of prototypical models' interpretability that addresses its multifaceted nature. The Co-12 properties were recently introduced as a comprehensive framework for evaluating explanation quality____. These properties have been designed for interpretable methods in general, including both post-hoc interpretability methods and SEMs. We summarize the most important properties in~\cref{tab:interpretability_metrics}. They encompass many desiderata that have been independently formulated in the literature focused on SEMs with metrics such as \textit{Prototypical part Location Change (PLC)}____ or \textit{Relevance Ordering Test (ROT)}____ which aim to evaluate the spatial alignment and fall under the \textit{Correctness property}. Metrics to evaluate the consistency, stability and compactness desiderata have also been proposed in the literature____.   

However, the metrics and their evaluation introduced to evaluate Prototypical models suffer from two main issues: i) Distribution shift under pixel ablation and ii) Lack of precise part annotations. Regarding the first issue, interpretability evaluations are typically conducted by removing pixels that are considered important by an interpretability method and assessing the resulting change in the model's prediction. However, this ablation creates a distribution shift between the training dataset and the dataset used for the interpretability evaluation which might alter the evaluation____. The second aspect is related more specifically to the evaluation of SEM which often aims to measure the consistency of the prototype used for the classification based on part annotations (e.g. beak, wing in the CUB dataset)____. However, the datasets used for these evaluations do not include precise annotations for individual parts. As a result, the evaluation relies on estimating part localization by placing a fixed-size box around a point that represents the part's position.Evaluation performed with these annotations considers only the top patches activated by a prototype which is then compared to the fixed-size box. This approach has been criticized as not correctly reflecting the model's decision process____. 

The FunnyBirds dataset has been developed to evaluate a three of the Co-12 properties, namely correctness, completeness and contrastivity. We list the associated metrics from FunnyBirds in~\cref{tab:interpretability_metrics} and refer the reader to the paper introducing these metrics for more details____. The developed metrics and dataset address the issues stated above and unify the evaluation of post-hoc interpretability methods and SEM models under a single framework. As part of their benchmark, 24 combinations of interpretability methods and models are evaluated. However, the only prototypical model evaluated is ProtoPNet. Recently, the methodology to compute the metrics presented in FunnyBirds for prototypical models was slightly modified, but again only ProtPNet was evaluated____. 

Next, we present the novel architecture along with a set of metrics to improve the overall explainability of self-explainable models and thoroughly evaluate the explanations provided by these models.