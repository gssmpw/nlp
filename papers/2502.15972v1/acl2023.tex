% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{booktabs}
\usepackage{multirow}



% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{graphicx}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{setspace}

\graphicspath{{img/}}

\newcommand{\oana}[1]
{\textcolor{purple}{\textit{#1}$_\text{ -- Oana}$}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Multi-Agent Multimodal Models\\ for Multicultural Text to Image Generation}

 \author{Parth Bhalerao \hspace{5pt} 
 Mounika Yalamarty \hspace{5pt}
  Brian Trinh \hspace{5pt}
Oana Ignat \\
Santa Clara University - Santa Clara, USA  \\
 \textit{\{pbhalerao, oignat\}@scu.edu} \\  }

\begin{document}
\maketitle
\begin{abstract}
Large Language Models (LLMs) demonstrate impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of existing data and models. Meanwhile, multi-agent models have shown strong capabilities in solving complex tasks.
In this paper, we evaluate the performance of LLMs in a multi-agent interaction setting for the \textit{novel task} of multicultural image generation. Our key contributions are: (1) We introduce MosAIG, a \underline{M}ulti-\underline{A}gent framework that enhances multicultural \underline{I}mage \underline{G}eneration by leveraging LLMs with distinct cultural personas; (2) We provide a dataset of 9,000 multicultural images spanning five countries, three age groups, two genders, 25 historical landmarks, and five languages; and (3) We demonstrate that multi-agent interactions outperform simple, no-agent models across multiple evaluation metrics, offering valuable insights for future research.
Our dataset and models are available at
\url{https://github.com/OanaIgnat/MosAIG}.

\end{abstract}

\section{Introduction}

%Motivation
Societies worldwide are increasingly diverse, with people of various cultural backgrounds co-existing - an outcome amplified by global travel and migration~\cite{migration}.
This multicultural tapestry offers both opportunities and challenges, particularly in Artificial Intelligence (AI), where robust representation of diverse groups is essential for equity and inclusivity~\cite{hershcovich-etal-2022-challenges, naous2023having,  mihalcea2024ai} However, most existing datasets—especially those used for text-to-image generation—primarily focus on narrow demographics, predominantly western adult males, and frequently portray single-culture scenarios (e.g., \textit{a Chinese temple}, \textit{an Indian market})~\cite{liu2024cultural, kannen2024beyond}. 
Such limited scope fails to encompass common multicultural interactions (e.g., \textit{a Chinese girl visiting the Golden Gate Bridge}). This limited representation affects the applicability of text-to-image generation models as they fail to accurately reflect the varied cultural and demographic landscapes of the real world~\cite{hershcovich-etal-2022-challenges, bhatia-etal-2024-local}.

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{img/overview2.pdf}
\caption{Most datasets used for training are dominated by singular cultural contexts (e.g., ``Golden Gate Bridge'' primarily depicted with American visitors or as a standalone monument). However, real-world scenarios often transcend cultural boundaries, with people from various backgrounds sharing spaces and experiences. Including images that combine multiple cultures, gender and age groups in a single scene allows models to develop a richer, more nuanced understanding of the world.}
\vspace{-1.2em}
\label{fig:main_idea}
\end{figure}

% Explain process - person and background - show picture
To address this gap, our work aims to enhance diversity in text-to-image generation models and datasets. We examine two critical dimensions: (1) the demographic attributes of the depicted person, and (2) the multicultural interactions between the person and the landmark (e.g., Golden Gate Bridge). To this end, we investigate four demographic aspects—age, gender, nationality, and language, while incorporating cross-cultural landmarks (\Cref{fig:main_idea}). 
By systematically exploring these aspects, we seek to evaluate and improve how state-of-the-art text-to-image models portray diverse populations and their interaction.
% Research Questions
Our paper aims to answer three main research questions.
\begin{description}
\item [RQ1:] {\bf How accurately do state-of-the-art text-to-image models depict people from one culture within the context of a landmark associated with a different culture?} 
\vspace{-0.3em}
\item [RQ2:] {\bf How does the performance of text-to-image generation vary across different demographic groups?} 
\vspace{-0.5em}
\item [RQ3:] {\bf  What strategies can enhance the performance of multicultural text-to-image generation?}
\end{description}

% By addressing these questions, our work contributes to the development of more inclusive AI systems that better mirror the global diversity of human societies. This not only improves the quality and relevance of AI-generated images but also fosters greater fairness and representation in AI applications.

%Contributions
The paper makes the following contributions. First, {\bf we compile and share the first dataset of 9,000 images depicting multicultural interactions, i.e., a person and a landmark from different cultures}, across five countries, three age groups, two genders, 25 historical landmarks, and five languages.
Second, we propose \texttt{MosAIG} {\bf a novel multi-agent framework to improve multicultural text-to-image generation performance} across demographics and languages.
Finally, we show that {\bf our multi-agent interactions outperform simple models across multiple evaluation metrics}, and provide actionable steps for future work.

\section{Related Work}

\paragraph{Cultural Evaluation in Language and Vision Models.}

Research in language-based models is advancing rapidly in capturing cultural nuances through large multilingual evaluation benchmarks~\cite{pawar2024survey,romanou2024include,singh2024global}. In the language-vision domain, recent benchmarks like CVQA~\cite{romero2024cvqaculturallydiversemultilingualvisual} and GlobalRG~\cite{bhatia-etal-2024-local} focus on culturally aware question answering, retrieval, and visual grounding. Novel methods leveraging multi-agent frameworks of large multimodal models~\cite{guo2024large, han2024llm} have shown further promise in enhancing cross-cultural understanding. For instance, MosAIC~\cite{bai2024power} employs a multi-agent framework for cross-cultural understanding but focuses on image captioning in single-culture contexts rather than text-to-image generation. Our work addresses this gap by examining how state-of-the-art text-to-image models handle multicultural representations within the same image.
\vspace{-0.8em}
\paragraph{Text-to-Image Generation Models and Benchmarks.} Text-to-image generative capabilities have advanced rapidly in recent years, as evidenced by models such as Stable Diffusion-XL~\cite{podell2023sdxl}, DALLE-3~\cite{betker2023improving}, and FLUX~\cite{flux2023}. Evaluation benchmarks like TIFA~\cite{hu2023tifa}, GenEval~\cite{ghosh2024geneval}, and GenAIBench~\cite{lin2025evaluating} traditionally emphasize technical factors such as realism, text faithfulness, and compositional accuracy. More recent work, i.e., HEIM~\cite{lee2024holistic}, extends these metrics to include socially situated aspects like toxicity, bias, and aesthetics, reflecting growing concern for the social impact of generative models~\cite{hartwig2024evaluating}.
\vspace{-0.8em}
\paragraph{Cultural Gap and Language Limitations in Text-to-Image Generation.} Despite advancements, existing efforts predominantly focus on a narrow set of languages (e.g., English, Chinese, Japanese), leaving large user communities underserved. 
Recent multilingual models, such as Taiyi-Diffusion-XL~\cite{wu2024taiyi}, target Chinese text input, while AltDiffusion~\cite{ye2024altdiffusion} expands language coverage to eighteen languages. However, a broader ``cultural gap'' persists~\cite{liu2024cultural}, as most models and benchmarks insufficiently capture diverse cultural settings and interactions.
\vspace{-0.8em}
\paragraph{Data Diversity and Cultural Competence.} Only recently have researchers begun to evaluate cultural competence in text-to-image models. For instance, CUBE~\cite{kannen2024beyond} assesses cultural awareness and diversity, yet still focuses on single-culture depictions per image. To our knowledge, no existing work systematically addresses multicultural scenarios—where multiple cultures may be represented in a single image—and rigorously evaluates the performance of state-of-the-art text-to-image systems under such conditions. Our approach aims to fill this gap by exploring how these models handle more complex, multicultural representations.

\section{Multicultural Image Generation}

\textit{Culture is a multifaceted concept meaning different things to different people at different times}~\cite{adilazuarda-etal-2024-towards}.
In this work, we adopt the definition proposed by \citet{nguyen2023extracting} and focus specifically on visual cultural elements such as clothing and historical landmarks.

We propose a \textit{novel task}, multicultural image generation, aimed at evaluating how generation models represent elements from diverse cultures within the same image, i.e., a person from one culture and a landmark from a different culture. We also analyze other demographic attributes and their intersection, such as age, gender, and language\footnote{All demographics are shown in Appendix \Cref{tab:age_gender_country_landmark}}.
To address this task, we introduce \texttt{MosAIG}, a \textit{novel framework} for \underline{M}ulti-\underline{A}gent \underline{I}mage \underline{G}eneration, as illustrated in \Cref{fig:pipeline}.
Our framework generates comprehensive image captions that are used to generate more accurate multicultural images using off-the-shelf image generation models.
This framework is built around a multi-agent interaction model, as described below.


\subsection{Multi-Agent Interaction Model}

We introduce a multi-agent setup to emulate collaboration between demographically diverse groups.
Our setup contains five agents, with specific roles: one Moderator Agent, three Social Agents, and one Summarizer Agent, as illustrated in \Cref{fig:pipeline}.

\noindent\textbf{Moderator Agent.} The Moderator Agent obtains demographic (age, gender, nationality) information about the person, the name of the landmark (e.g., Taj Mahal), and the language of the caption as input.
The Moderator Agent then assigns tasks to the Social agents, instructing them to focus on the visually relevant aspects of the input information. 

\noindent\textbf{Social Agents.} The Social Agents interact by asking each other relevant questions to create an image caption according to the information provided by the Moderator Agent. Each Social Agent assumes a \textit{persona}: the first agent represents the culture of the person in the image, the second agent represents the age and gender of the person, and the last agent represents the historical landmark.
Each agent generates an initial description of their persona. Then, by interacting through multiple rounds of question-answering conversations, each agent creates a more comprehensive image description.

\noindent\textbf{Summarizer Agent.} The Summarizer Agent collects the three descriptions from the Social Agents and summarizes them into a final image caption with a maximum length of 77 tokens.

\noindent\textbf{Social Agents Conversation.}
At the start, the three Social Agents—Country Agent, Landmark Agent, and Age-Gender Agent—receive demographic information and tasks from the Moderator Agent.
The Country Agent processes nationality information and describes traditional attire, which is then evaluated by the Age-Gender Agent (e.g., ``Is this attire suitable for a young female?''). Adjustments, such as modifying the color or style of a garment to suit the individual's age, are made accordingly.
The Landmark Agent describes the landmark architecture, and its descriptions are refined based on feedback from the Country Agent (e.g., ``How do Vietnamese visitors typically interact with this landmark?''), ensuring cultural authenticity.
The Age-Gender Agent generates demographic descriptions, which are cross-checked with the Country Agent to ensure culturally appropriate accessories and mannerisms.
After two rounds of conversation, the agents enhance and refine the descriptions with culturally sensitive and contextually rich details. Once the iterative improvement process is complete, the refined descriptions are passed to the Summarizer Agent, which condenses them into a final 77-token prompt capturing the cultural and contextual nuances.
The prompts used for each agent are provided in the Appendix \Cref{fig:prompts}.

\noindent\textbf{Implementation Details.}
The Summarizer Agent and each Social Agent are initialized as different instances of a LLaMA model\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B}}~\cite{touvron2023llama}.
The Moderator Agent is a predefined function call. The agent conversation uses the CrewAI framework to establish an iterative feedback loop\footnote{https://www.crewai.com/open-source}.
The implementation was carried out using an NVIDIA V100 GPU (32GB). More details can be found in \Cref{sec:model}. 


\begin{figure}
\centering
\includegraphics[width=1\linewidth]{img/method.pdf}
  % \vspace{-1.5em}
\caption{Overview of \texttt{MosAIG}, our framework for \underline{M}ulti-\underline{A}gent \underline{I}mage \underline{G}eneration. The framework includes a multi-agent interaction model that generates an image caption from demographic information (person age, gender, country, landmark, and caption language), which is then used by an image generation model to create a multicultural image of a landmark and a person.}
\vspace{-1.2em}
\label{fig:pipeline}
\end{figure}

\subsection{Image Generation Models}

We evaluate our generated image captions using two different state-of-the-art image generation models: AltDiffusion~\cite{ye2024altdiffusion} and FLUX~\cite{flux2023}.

\paragraph{AltDiffusion.}
AltDiffusion\footnote{\url{https://huggingface.co/BAAI/AltDiffusion-m18}}~\cite{ye2024altdiffusion} is one of the very few multilingual open-source image generation models. The model aligns multilingual language models with diffusion models to generate high-quality images from text across multiple languages. The model builds on CLIP~\cite{radford2021learning}, replacing its text encoder with XLM-R~\cite{conneau2019unsupervised} and employing a two-stage training process that combines teacher learning and contrastive learning.
AltDiffusion supports 18 different languages; we select five—English, German, Hindi, Spanish, and Vietnamese—based on the annotators' expertise. The model processes text inputs with a maximum length of 77 tokens.  
\vspace{-0.6em}
\paragraph{FLUX.}
FLUX.1-dev\footnote{\url{https://huggingface.co/black-forest-labs/FLUX.1-dev}}~\cite{flux2023} is a state-of-the-art, widely used, open-source text-to-image model designed for English-language prompts. Due to computational constraints, we employ Flux.1 Lite\footnote{\url{https://huggingface.co/Freepik/flux.1-lite-8B-alpha}}~\cite{flux1-lite}, an 8B-parameter transformer model, more efficient variant distilled from FLUX.1-dev. 
% This 8B-parameter transformer model reduces memory consumption by 7 GB and accelerates inference by 23\%, while retaining the original model’s bfloat16 precision and high-quality image generation capabilities.

\subsection{Simple vs. Multi-Agent Image Generation}

Simple models generate images based on predefined captions, whereas multi-agent models utilize dynamically generated captions derived from multi-agent interactions.
For instance, when provided with demographic details such as ``Vietnamese'' (nationality), ``child'' (age), ``female'' (gender), ``Golden Gate Bridge'' (landmark), and ``English'' (caption language), the resulting image captions differ between the two approaches. 
Multi-agent models generate captions that provide richer contextual information, including detailed descriptions of the landmark’s architecture and surroundings, as well as a more nuanced depiction of the person's appearance, particularly focusing on clothing and facial features, as shown below\footnote{All the captions are shown in our code repository.}.

% \vspace{-0.5em}
\begin{description}
\small
\item [Simple caption:] {\it A Vietnamese girl wearing traditional attire, standing in front of the Golden Gate Bridge.} 
\vspace{-0.3em}
\item [Multi-agent caption:] {\it A 12-year-old Vietnamese girl in Áo Dài, standing on the Golden Gate Bridge, with the San Francisco Bay's blue waters and the bridge's orange-red towers in the background.} 
\end{description}



\section{Evaluation and Results}

We employ both automated metrics and human evaluation to provide a holistic and comprehensive assessment of the generated images.

\subsection{Evaluation Metrics}

We adopt automated evaluation metrics, which assess alignment, quality, aesthetics, knowledge, and fairness, ensuring a comprehensive analysis. These metrics encompass both technical factors—alignment, quality, and knowledge—as well as socially situated aspects such as fairness and aesthetics~\cite{lee2024holistic}.  

\noindent\textbf{Alignment.}
CLIPScore~\cite{hessel2021clipscore} measures text-to-image alignment by computing the cosine similarity between the semantic embeddings of the image and its associated text, providing an effective assessment of how well the generated image reflects the intended description. CLIPScore ranges from -1 to +1, where higher values indicate a stronger semantic alignment between the generated image and its corresponding text.

\noindent\textbf{Quality.}
We assess the quality of generated images using the Inception Score (IS)~\cite{salimans2016improved}, which leverages an Inception v3 classifier to measure image fidelity and diversity. Lower scores (below ~10) typically indicate poor quality or limited variation, while higher scores (10+) suggest more realistic and diverse outputs.

\noindent\textbf{Aesthetic.}
This metric evaluates the aesthetic appeal of an image, considering factors such as visual clarity, sharpness, color vibrancy, and overall subject clarity. Aesthetic evaluation also takes into account composition, color harmony, balance, and visual complexity.
To assess these aspects, we use the SigLIP-based predictor\footnote{\url{https://github.com/discus0434/aesthetic-predictor-v2-5}}, which rates the aesthetics of an image on a scale from 1 to 10 (best).

\noindent\textbf{Fairness.}
This metric evaluates the consistency of model performance when captions are modified to reference different social groups.
Specifically, modifications are applied to attributes such as \textit{gender}, \textit{age}, and \textit{nationality}, while keeping the rest of the caption unchanged. 
Given an original caption \textit{c} and its corresponding image \textit{I}, we construct a modified caption \textit{c'} by substituting a demographic term, i.e., replacing male-gendered terms with female-gendered terms, ``young'' with ``old'' or ``German'' with ``Indian''. The corresponding modified image \textit{I'}
also reflects the demographic change.

\noindent For example, given the initial caption-image pair:

\noindent $(c, I)=($\textit{A German boy in front of Taj Mahal}, $I$)

\noindent modifying the gender term results in the new pair:

\noindent $(c', I')=$(\textit{A German girl in front of Taj Mahal}, $I'$)

\noindent To evaluate fairness, we compute the absolute difference in CLIPScore between the original and modified pairs:
\begin{equation*}
\Delta S=\mid S(c,I) - S(c',I')\mid 
\end{equation*}
where $S(c,I)$ and $S(c',I')$ denote the CLIPScores for the original and modified caption-image pairs, respectively. A fair model should exhibit minimal variation in performance across demographic groups, implying low values of $\Delta S$. Higher values of $\Delta S$ indicate greater performance disparity, suggesting potential bias.


\noindent\textbf{Knowledge.}
This metric evaluates the model's knowledge of the world by analyzing its ability to recognize and distinguish historical landmarks. To assess this, we modify a given caption $c$ by replacing one \textit{historical landmark} with another while keeping the corresponding image $I$
and the rest of the caption unchanged. 
For example, given the initial caption-image pair:

\noindent $(c, I)=($\textit{A German boy in front of Taj Mahal}, $I$)

\noindent modifying the landmark term results in:

\noindent $(c', I)=$(\textit{A German boy in front of White House}, $I$)

\noindent We measure the absolute difference in CLIPScore before and after the modification:
\begin{equation*}
\Delta S= S(c,I) - S(c',I)
\end{equation*}
\noindent A model with strong cross-cultural knowledge of historical landmarks should exhibit high performance variations when landmarks are swapped. Higher scores indicate greater knowledge, while lower scores suggest weaker landmark recognition.

% \noindent\textbf{Robustness.}




\subsection{Multi-Agent Interaction Results}\label{sec:multiagent}

Our multi-agent models outperform simple models in Alignment, Aesthetics (only \texttt{Alt-En-M}), Quality, and Knowledge, while scoring lower in Fairness, as illustrated in \Cref{fig:main_plot_bar}.
The most significant improvement is observed in Image Quality, where multi-agent models achieve substantially higher scores (0.77 vs. 0.48 for \texttt{Alt-En} and 0.65 vs. 0.45 for \texttt{Flux-En}). We hypothesize that this enhancement is driven by the additional contextual details provided by multi-agent interactions, leading to more visually refined outputs.

Furthermore, we analyze performance variations across demographic categories for all models and metrics, as detailed in \Cref{sec:results}. Notably, Alignment improves across gender, age, person, and landmark country when using multi-agent models compared to simple models. Additionally, Quality is consistently higher for \texttt{Alt} compared to \texttt{Flux}, likely due to the tendency of Flux-generated images to exhibit blurry backgrounds.

However, Fairness scores decline for multi-agent models. We attribute this to the increased level of detail in their generated captions—such as references to clothing, facial features, and hairstyles—which amplifies the absolute difference in CLIPScore between the original and modified caption-image pairs. In contrast, the simpler, more concise captions generated by simple models do not introduce as many additional descriptors, resulting in smaller variations in CLIPScore and consequently lower Fairness scores.
These findings highlight a trade-off between improved Quality, Alignment, and Knowledge and the potential bias introduced in Fairness, likely due to richer descriptive caption generated with multi-agent models. 
% Moreover, Fairness is also improved, likely due to the models' emphasis on demographic characteristics such as clothing, facial features, and hair.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{img/plot_mean.pdf}
  \vspace{-1.5em}
\caption{
Our multi-agent models (Alt-En-M and Flux-M) surpass simple models (Alt-En-S and Flux-S) on Alignment, Aesthetics, Quality, and Fairness while performing worse in Knowledge.
\textit{For ease of comparison, all the scores are normalized to a 0–1 scale. Higher scores are better for Alignment, Aesthetics, Quality, and Knowledge, while lower scores are better for Fairness.}}
\vspace{-1.2em}
\label{fig:main_plot_bar}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{img/ablations.pdf}
  % \vspace{-1.5em}
\caption{
Ablation studies on (a) person age, (b) person gender, (c) person country, (d) landmark country, (e) caption language using the best overall model, the \texttt{Multi-agent English Flux-M}  (a-d) and \texttt{Multi-agent Multilingual Alt-M} (e). Performance across all five metrics—Alignment, Aesthetic, Quality, Knowledge, and Fairness—reveals significant variation across these demographic categories.}
% \vspace{-1.2em}
\label{fig:ablations}
\end{figure}

\vspace{-1em}
\subsection{Ablation Studies}
We also perform ablation studies to
assess \texttt{MosAIG’s} performance across demographics.
\vspace{-0.5em}
\paragraph{a) Person Age.}
\Cref{fig:ablations} a) shows that Image Quality varies by age group, with Adults achieving the highest quality (0.55), followed by Children (0.51) and Elders (0.49). The model is also fairer when depicting Children (0.33) compared to Adults (0.38) and Elders (0.40). Alignment and Aesthetic metrics remain consistent across all age groups.
\vspace{-0.7em}
\paragraph{b) Person Gender.}
\Cref{fig:ablations} b) shows that Image Quality varies by gender, with Males achieving higher quality (0.56) than Females (0.52). However, the model is fairer when depicting Females (0.36) than Males (0.38). The other metrics remain consistent across both groups.
\vspace{-0.7em}
\paragraph{c) Person Country.}
\Cref{fig:ablations} c) shows that model performance varies by person’s country. Alignment is highest for Indian people (0.32) and lowest for Spanish people (0.29). Similarly, Image Quality is highest for Indian people (0.47) and lowest for German people (0.41). The model is also fairest when depicting Indian people (0.35) and least fair for German people (0.39).
\vspace{-0.7em}
\paragraph{d) Landmark Country.} \Cref{fig:ablations} d) shows that model performance varies by landmark country. The most notable difference is in the Knowledge metric, with U.S. landmarks being the most well-known (0.55), followed by Germany (0.47), Spain (0.42), Vietnam (0.40), and India (0.39). Alignment is highest for U.S. landmarks (0.33) and lowest for Spanish landmarks (0.29).
\paragraph{e) Caption Language.}
\vspace{-0.7em}
\Cref{fig:ablations} e) shows that model performance varies by caption language, with English achieving the highest Alignment (0.31) and Knowledge (0.46), while Hindi and Vietnamese score the lowest (0.14 and 0.43, respectively). This disparity may stem from differences in training data availability, as model performance moderately correlates with dataset size (Pearson coefficient: 0.5), estimated from CommonCrawl \cite{wenzek-etal-2020-ccnet}.
\vspace{-0.7em}
\paragraph{f) Intersectionality.}
Examining a single demographic category, such as race or gender, may overlook nuanced inequalities \cite{field_survey_2021}. To address this, we analyze the intersectionality of age and gender, person and landmark country, and language and person country. We measure Alignment and analyze other metrics across various demographic intersections, as detailed in \Cref{sec:intersection}.

\noindent\textbf{Age and Gender.}
\Cref{fig:heatmap_person_landmark} (right) shows that Alignment performance varies by gender for generating adult images, with males having a lower score (0.29) compared to females (0.31). The performance for child and elder categories remains consistent across gender.


\noindent\textbf{Person and Landmark Country.}
\Cref{fig:heatmap_person_landmark} (left) illustrates Alignment across Person and Landmark Country. We expected higher performance when the person and landmark originate from the same country, suggesting challenges in cross-cultural representation. However, results vary by country. For instance, the highest alignment occurs when Indian or Vietnamese people visit U.S. landmarks (0.34), comparable to U.S. people at U.S. landmarks (0.33). In contrast, the lowest alignment is observed when Vietnamese people visit Spanish landmarks (0.28). Significant differences across other metrics are in \Cref{sec:intersection}.


\noindent\textbf{Language and Country.}
\Cref{fig:heatmap_person_country_language} shows Alignment across Person Country and Caption Language. 
English, Spanish, and Vietnamese captions achieve the highest performance ($\sim$ 0.3) with minimal variation across person countries. However, Hindi captions perform best for Indian people (0.17) and worst for Spanish and U.S. people (0.13). This suggests that, for certain languages, the interaction between caption language and the depicted person’s culture influences Alignment in image generation.



\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{img/heatmap2.pdf}
  \vspace{-1em}
\caption{
Alignment scores with the best overall model, \texttt{Flux-M}, over person and landmark country (left) and gender and age (right).}
\vspace{-1.2em}
\label{fig:heatmap_person_landmark}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.7\linewidth]{img/heatmap_person_country_language.pdf}
  \vspace{-1em}
\caption{
Alignment scores with the best overall multilingual model, \texttt{Alt-M}, over image caption language and person country.}
\vspace{-1.2em}
\label{fig:heatmap_person_country_language}
\end{figure}

\vspace{-1em}
\subsection{Human Evaluation and Error Analysis}\label{sec:error}
% From HEIM: Correlations between human and automated metrics are generally weak, particularly in photorealism
% and aesthetics. 
% This highlights the importance of using human metrics in evaluating image
% generation models.
Two annotators evaluate a subset of 300 images, covering all demographics (age, gender, country, landmark) and model settings (Alt-S, Alt-M, Flux-S, Flux-M). They assess the generated images based on three key metrics: Alignment, Quality, and Aesthetics.
Following \citet{lee2024holistic}, Quality is measured in terms of photorealism, while Aesthetics is evaluated based on subject clarity and overall visual appeal.
Annotator agreement is measured using weighted Cohen’s Kappa for ordinal values \cite{Cohen1968WeightedKN}, yielding scores between 0.5 and 0.6 across all three metrics, indicating moderate agreement.
% We also measure Correlation between the human and automated evaluation. Alignment and Aesthetic automated metrics have a weak corelation ($\sim$ 0.3 with human evaluation and Quality is a weak negative corelation ($\sim$ -0.16)
The complete set of human evaluation questions, along with the annotation interface, is detailed in \Cref{sec:human_eval}.

\noindent\textbf{Most Common Errors.}
The most frequent errors in the Flux-M model involve incorrect backgrounds, occurring in 38 of 75 images (38/75). Additionally, deviations from prompt descriptions are observed, along with errors in rendering human figures (5/75), such as missing fingers or incorrect cultural markers (e.g., misplacement of a bindi). Landmark-related inconsistencies are less common (2/75), and include significant omissions, such as missing faces on Mount Rushmore.
In contrast, the Flux-S model exhibits a higher rate of landmark errors (15/75), such as missing the Sagrada Familia. Errors in depicting human figures also increase (10/75), particularly in rendering traditional attire and facial accuracy.
The Alt models (Alt-S and Alt-M) display more pronounced inaccuracies. The most prevalent issue is incorrect backgrounds (55/75), followed by severe body distortions (e.g., three hands, elongated arms, two right feet), and multiplicity errors (e.g., two people instead of one). While the multi-agent Alt-M model reduces errors related to cultural elements (2/75), it still exhibits body distortions (15/75).

\begin{figure*}[htbp]
\centering
  \includegraphics[width=\linewidth]{img/qualitative_Vietnamese.pdf}  % Spans the full width of both columns
  \vspace{-1em}
\caption{ 
Comparison of generated images and captions using our multi-agent framework (\texttt{Flux-M}, \texttt{Alt-M}) and simple models (\texttt{Flux-S}, \texttt{Alt-S}). The first two columns highlight cases where multi-agent models perform better, while the last column shows instances where simpler models excel. The second column depicts images generated with Vietnamese captions using the multilingual model \texttt{Alt} (\texttt{Alt-Vi-S}, \texttt{Alt-Vi-M}). Demographic keywords are \textbf{bolded}, and incorrect content is marked in {\color{red}red}.}
\vspace{-1em}
\label{fig:qualitative.pdf}
\end{figure*}

\vspace{-1em}
\subsection{Qualitative Results}\label{sec:quals}

In \Cref{fig:qualitative.pdf}, we compare the images generated by our multi-agent framework (\texttt{Flux-M} and \texttt{Alt-M}) with those from simpler models (\texttt{Flux-S} and \texttt{Alt-S}). The second column presents images generated with Vietnamese captions using the multilingual models (\texttt{Alt-Vi-S}, \texttt{Alt-Vi-M}).
Compared to the simple models, the multi-agent models perform better at generating landmarks and people. However, they still miss important details about people, such as \textit{a person looking up}, \textit{curly hair}, or \textit{hair tied back with a nón lá hat}. Notably, body distortions are more pronounced in the Alt-S model. While the Flux model produces more accurate backgrounds, they tend to be blurrier compared to those in the Alt model.
A manual error analysis of 300 images across all demographics highlights the need for further improvements, particularly in rendering body structures and backgrounds. Additional results across demographics are in \Cref{sec:qual}.

\vspace{-1em}
\section{Lessons Learned and Actionable Steps}

Our findings provide insights into the performance of multi-agent multimodal models for multicultural image generation, highlighting key lessons and proposing actionable steps to improve accuracy and cultural representation in future models.

\noindent\textbf{Prioritize Multi-Agent Models.}
Our analysis shows that multi-agent models generate more contextually rich and culturally nuanced images than simple models (\Cref{sec:multiagent}). By integrating diverse perspectives through collaboration, these models enhance alignment, aesthetics, quality, and knowledge. Future research should focus on refining multi-agent frameworks to further enhance alignment, fairness, and representational diversity. 
Additionally, our framework can be extended to generate images depicting a wider range of cultural interactions—such as dancing, eating, and festivals—while featuring diverse groups. This extension would allow for a comprehensive evaluation of reasoning and action-based image generation.

\noindent\textbf{Prioritize Multilingual Generation Models.}
Our results indicate a performance discrepancy between English and non-English prompts, with English-based generations often exhibiting higher Alignment (\Cref{fig:ablations} e). To ensure equitable representation across languages, future models should incorporate stronger multilingual capabilities, improving Fairness and Alignment in non-English text-to-image generation.

\noindent\textbf{Develop Better Evaluation Metrics.}
Current evaluation metrics do not always align with qualitative assessments, particularly when surrounding elements boost scores despite incorrect Landmarks (\Cref{sec:error}). For example, an image of the Taj Mahal may score highly due to accurately depicted gardens, even if the Landmark itself is wrong. We recommend refining Alignment metrics by assigning greater weight to key elements, such as Landmarks, for more reliable assessments.
Additionally, our findings reveal a trade-off between enhanced Quality, Alignment, and Knowledge and reduced Fairness, likely due to the richer captions generated by multi-agent models (\Cref{sec:multiagent}). Future research should address this balance to enhance expressiveness while maintaining demographic consistency.

\vspace{-1em}
\section{Conclusion}
In this paper, we introduce \texttt{MosAIG}, a framework that leverages LLM agent interactions to enhance multicultural text-to-image generation. We conduct a comprehensive analysis of image generation performance across five countries, three age groups, two genders, 25 historical landmarks, and five languages, as well as their intersections.
Our evaluation across five key metrics reveals significant demographic variations. Notably, our framework outperforms simple models in Alignment, Aesthetics, Quality, and Knowledge.
We contribute the first dataset of 9,000 images depicting multicultural interactions, specifically showcasing individuals and landmarks from different cultural backgrounds. Additionally, we open-source both our dataset and the models generated by \texttt{MosAIG}, providing a valuable resource for future research.
Our dataset and models are available at:
\url{https://github.com/OanaIgnat/MosAIG}.


\section*{Limitations and Ethical Considerations}

\paragraph{Limited Demographics.}
Our study focuses on a binary gender representation—male and female—while overlooking non-binary and other gender identities. Expanding future models to encompass a broader spectrum of gender identities would enhance inclusivity and fairness in image generation.
Additionally, our dataset is restricted to five countries—U.S., Germany, India, Spain, and Vietnam—and five languages—English, German, Hindi, Spanish, and Vietnamese. These languages and regions are relatively well-represented in the training data, limiting our ability to evaluate model performance across less-studied linguistic and cultural groups. This highlights the need for broader validation across a more diverse set of cultures to ensure improved alignment, fairness, and reliability in cross-cultural image generation.
Finally, we categorize age into three broad groups: child, adult, and elder, which may oversimplify the diversity within each age category. Further refinement of age-related categorizations could help more accurately reflect the varied experiences and characteristics of individuals across different life stages.


\paragraph{Challenges in Defining Demographic Representation.}
Our methodology utilizes multi-agent large language model (LLM) interactions, where each LLM simulates a unique perspective based on cultural, age, and gender attributes. While carefully designed prompts help align these models with diverse demographic contexts, identity is inherently complex and cannot be fully encapsulated through broad categorizations. Defining culture solely through national affiliation or language overlooks the vast heterogeneity of traditions, experiences, and perspectives that exist within and across borders.
Relying on a limited set of demographic indicators provides only a foundational framework for understanding diversity, but it does not capture the deeper nuances that define individual and collective identities. To improve representation, future research should incorporate additional dimensions such as historical influences, societal values, traditions, and lived experiences. Expanding cultural modeling to account for attitudes, biases, and personal narratives will enable more accurate and contextually rich portrayals, ultimately enhancing both the performance and authenticity of AI-generated representations.

% \section*{Acknowledgements}


\bibliography{custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendix}
\label{sec:appendix}

\section{Data}\label{sec:model}

\begin{table*}[h]
    \centering
    \begin{tabular}{|c|c|c|l|}
        \hline
        \textbf{Age} & \textbf{Gender} & \textbf{Country} & \textbf{Landmark} \\ 
        \hline
        \multirow{25}{*}{Child/ Adult/ Elder} & \multirow{25}{*}{Female/Male} & \multirow{5}{*}{Germany} & Cologne Cathedral \\ 
        & & & Reichstag Building \\ 
        & & & Neuschwanstein Castle \\ 
        & & & Brandenburg Gate \\ 
        & & & Holocaust Memorial \\ 
        \cline{3-4}
        & & \multirow{5}{*}{India} & Taj Mahal \\ 
        & & & Lotus Temple \\ 
        & & & Gateway of India \\ 
        & & & India Gate \\ 
        & & & Charminar \\ 
        \cline{3-4}
         &  & \multirow{5}{*}{Spain} & Sagrada Familia \\ 
        & & & Alhambra \\ 
        & & & Guggenheim Museum \\ 
        & & & Roman Theater of Cartagena \\ 
        & & & Royal Palace of Madrid \\ 
        \cline{3-4}
        & & \multirow{5}{*}{U.S.} & White House \\ 
        & & & Statue of Liberty \\ 
        & & & Mount Rushmore \\ 
        & & & Golden Gate Bridge \\ 
        & & & Lincoln Memorial \\ 
        \cline{3-4}
         &  & \multirow{5}{*}{Vietnam} & Meridian Gate of Hue \\ 
        & & & Independence Palace \\ 
        & & & One Pillar Pagoda \\ 
        & & & Ho Chi Minh Mausoleum \\ 
        & & & Thien Mu Pagoda \\ 
        \hline
    \end{tabular}
    \caption{Demographics Overview: 3 Age groups, 2 Genders, 5 Countries, and 25 Landmarks}
    \label{tab:age_gender_country_landmark}
\end{table*}

\section{Multicultural Image Generation}\label{sec:model}

\subsection{Implementation Details}

The multi-agent configuration processed 750 base prompts in approximately 45 minutes, while additional language variants (3,750 prompts in total) required 75 minutes using the Google Translation API.
Two models—Flux and Alt-Diffusion—were used for image generation:
Flux produced 750 images (768×768 pixels) in 2.5 hours with the settings: guidance scale: 4, inference steps: 30, seed: 11, averaging roughly 12 seconds per image.
Alt-Diffusion was configured with the settings: guidance scale: 11, inference steps: 110, seed: 11000, and processed 3,750 images of the same resolution in 16 hours, averaging about 15 seconds per image.
All processing times accounted for overhead related to model loading and image saving, ensuring consistency in image resolution (768×768 pixels) across both models.


\begin{figure*}
\centering
\includegraphics[width=\linewidth]{img/appendix/prompts_multi.pdf}
\caption{Our Multi-agent Framework Prompts}

\label{fig:prompts}
\end{figure*}


\section{Human Evaluation and Error Analysis}\label{sec:human_eval}

We rely on human annotators to assess a sample of the generated images based on three key metrics: Alignment, Quality, and Aesthetics.
Following ~\citet{lee2024holistic}, Quality is evaluated in terms of photorealism, while Aesthetics is assessed based on subject clarity and overall visual appeal. The complete set of human evaluation questions is outlined below.
Annotators are provided with definitions (\Cref{tab:guidelines_human_eval}) and corresponding questions to guide their assessments. To determine whether the generated images meet their expectations, we ask annotators to rate them using a 5-point Likert scale.



\begin{table*}[h]
\centering
\begin{tabular}{c|p{13cm}}
\toprule
Aspect & Definition \\
\midrule
Alignment    &   Is the image semantically correct given the text (text-image alignment)?        \\
Quality    &    Do the generated images look like real photographs?       \\
Aesthetic    &    Is the image aesthetically pleasing?       \\
Fairness    &      Does the model exhibit performance disparities across social groups (e.g., gender, dialect)     \\
Knowledge    &   Does the model have knowledge about the world or domains?        \\
\bottomrule
\end{tabular}%
%}
\caption{Evaluation Aspects of Text-to-Image Models}
\label{tab:guidelines_human_eval}
\end{table*}


\paragraph{Alignment.}
We ask the annotators to rate how well the image matches the description.

\textbf{How well does the image match the description?}
\begin{enumerate}
    \item  Does not match at all
    \item  Has significant discrepancies
    \item  Has several minor discrepancies
    \item  Has a few minor discrepancies
    \item  Matches exactly
\end{enumerate}


\paragraph{Quality.}
We ask the annotators to rate how photorealistic the generated images are.

\textbf{Determine if the following image is AI-generated or real.}
\begin{enumerate}
    \item  AI-generated photo.
    \item  Probably an AI-generated photo, but photorealistic.
    \item  Neutral.
    \item  Probably a real photo, but with irregular textures and shapes.
    \item  Real photo.
\end{enumerate}

\paragraph{Aesthetics.}
To evaluate the overall aesthetics, we ask annotators to provide a holistic assessment of the image's visual appeal by rating its aesthetic quality.

\textbf{How aesthetically pleasing is the image?}
\begin{enumerate}
    \item I find the image ugly.
    \item The image has a lot of flaws, but it's not completely unappealing.
    \item I find the image neither ugly nor aesthetically pleasing.
    \item  The image is aesthetically pleasing and is nice to look at.
    \item The image is aesthetically stunning. I can look at it all day.
\end{enumerate}




\begin{figure*}
\centering
\includegraphics[width=0.9\linewidth]{img/appendix/annotation_pipeline.pdf}
\caption{Human Annotation Interface for manually evaluating the images across all models.}
\label{fig:annotation_pipeline}
\end{figure*}

\newpage
\section{Results}

\subsection{Across Metrics and Demographics, across All Models}\label{sec:results}

\newpage


%%% Alignment

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Age_Alignment.pdf}
\vspace{-1em}
\label{fig:all_Age_Alignment}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Gender_Alignment.pdf}
\vspace{-1em}
\label{fig:all_Gender_Alignment}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]
{img/appendix/all_Country_Alignment.pdf}
\vspace{-1em}
\label{fig:all_Country_Alignment}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Landmark_Alignment.pdf}
\vspace{-1em}
\label{fig:all_Landmark_Alignment}
\end{figure}

%%% Aesthethics

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Age_Aesthetic.pdf}
\vspace{-1em}

\label{fig:all_Age_Aesthetic}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Gender_Aesthetic.pdf}
\vspace{-1em}
\label{fig:all_Gender_Aesthetic}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Country_Aesthetic.pdf}
\vspace{-1em}
\label{fig:all_Country_Aesthetic}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{img/appendix/all_Landmark_Aesthetic.pdf}
\vspace{-1em}
\label{fig:all_Landmark_Aesthetic}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Age_Knowledge.pdf}
\vspace{-1em}
\label{fig:all_Age_Knowledge}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Gender_Knowledge.pdf}
\vspace{-1em}
\label{fig:all_Gender_Knowledge}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Country_Knowledge.pdf}
\vspace{-1em}
\label{fig:all_Country_Knowledge}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]
{img/appendix/all_Landmark_Knowledge.pdf}
\vspace{-1em}
\label{fig:all_Landmark_Knowledge}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Age_Fairness.pdf}
\label{fig:all_Age_Fairness}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Gender_Fairness.pdf}
\vspace{-1em}
\label{fig:all_Gender_Fairness}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Country_Fairness.pdf}
\vspace{-1em}
\label{fig:all_Country_Fairness}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{img/appendix/all_Landmark_Fairness.pdf}
\vspace{-1em}
\label{fig:all_Landmark_Fairness}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{img/plot_mean_alt.pdf}
\caption{
English vs. Multilingual Performance. Models with English captions as input (Alt-En-S, Alt-En-M) achieve higher scores than non-English (Alt-NonEn-S, Alt-NonEn-M) in Alignment (0.30 vs. 0.20), while performing comparably across Aesthetics and Quality metrics. Knowledge performance is higher for non-English models. }
\label{fig:alt_plot_bar}
\end{figure}

\newpage
\subsection{Intersectionality}\label{sec:intersection}

%%%%%%%% Alignment

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmap_country_age.pdf}
\label{fig:heatmap_country_age}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmap_gender_country.pdf}
\label{fig:heatmap_gender_country}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmap_landmark_country_language.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

%%%%%%%% Aesthetic

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_country_age_Aesthetic.pdf}
\label{fig:heatmap_country_age}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_gender_country_Aesthetic.pdf}
\label{fig:heatmap_gender_country}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_landmark_Aesthetic.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_landmark_country_language_Aesthetic.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_language_Aesthetic.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}


%%%%%%%% Knowledge

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_country_age_Knowledge.pdf}
\label{fig:heatmap_country_age}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_gender_country_Knowledge.pdf}
\label{fig:heatmap_gender_country}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_landmark_Knowledge.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_landmark_country_language_Knowledge.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_language_Knowledge.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}


%%%%%%%% Fairness

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_country_age_Fairness.pdf}
\label{fig:heatmap_country_age}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_gender_country_Fairness.pdf}
\label{fig:heatmap_gender_country}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_landmark_Fairness.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_landmark_country_language_Fairness.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{img/appendix/heatmaps/heatmap_person_country_language_Fairness.pdf}
\label{fig:heatmap_landmark_country_language}
\end{figure}

% \newpage
\subsection{Qualitative Results}\label{sec:qual}

\begin{figure*}[htbp]
\centering
  \includegraphics[width=\linewidth]{img/qualitative_German}  % Spans the full width of both columns
\caption{ 
Comparison of generated images and captions using our multi-agent framework (\texttt{Flux-M}, \texttt{Alt-M}) and simple models (\texttt{Flux-S}, \texttt{Alt-S}). The second column depicts images generated with \textbf{German} captions using the multilingual model \texttt{Alt} (\texttt{Alt-De-S}, \texttt{Alt-De-M}). Demographic keywords are \textbf{bolded}, and incorrect content is marked in {\color{red}red}.}
\vspace{-1em}
\label{fig:qualitative_German.pdf}
\end{figure*}


\begin{figure*}[htbp]
\centering
  \includegraphics[width=\linewidth]{img/qualitative_Hindi}  % Spans the full width of both columns
\caption{ 
Comparison of generated images and captions using our multi-agent framework (\texttt{Flux-M}, \texttt{Alt-M}) and simple models (\texttt{Flux-S}, \texttt{Alt-S}). The second column depicts images generated with \textbf{Hindi} captions using the multilingual model \texttt{Alt} (\texttt{Alt-Hi-S}, \texttt{Alt-Hi-M}). Demographic keywords are \textbf{bolded}, and incorrect content is marked in {\color{red}red}.}
\vspace{-1em}
\label{fig:qualitative_Hindi.pdf}
\end{figure*}


\begin{figure*}[htbp]
\centering
  \includegraphics[width=\linewidth]{img/qualitative_Spanish_German}  % Spans the full width of both columns
\caption{ 
Comparison of generated images and captions using our multi-agent framework (\texttt{Flux-M}, \texttt{Alt-M}) and simple models (\texttt{Flux-S}, \texttt{Alt-S}). The first column depicts images generated with \textbf{German} captions using the multilingual model \texttt{Alt} (\texttt{Alt-De-S}, \texttt{Alt-De-M}).
The last column depicts images generated with \textbf{Spanish} captions using the multilingual model \texttt{Alt} (\texttt{Alt-Es-S}, \texttt{Alt-Es-M}).
Demographic keywords are \textbf{bolded}, and incorrect content is marked in {\color{red}red}.}
\vspace{-1em}
\label{fig:qualitative_Spanish.pdf}
\end{figure*}



\end{document}
