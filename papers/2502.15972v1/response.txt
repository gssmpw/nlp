\section{Related Work}
\paragraph{Cultural Evaluation in Language and Vision Models.}

Research in language-based models is advancing rapidly in capturing cultural nuances through large multilingual evaluation benchmarks **Ammodt, "Large Multilingual Evaluation Benchmarks"**. In the language-vision domain, recent benchmarks like CVQA **Li, "CVQA: A Benchmark for Culturally Aware Question Answering"** and GlobalRG **Radenovic, "GlobalRG: A Large-Scale Benchmark for Visual Grounding with Cultural Awareness"** focus on culturally aware question answering, retrieval, and visual grounding. Novel methods leveraging multi-agent frameworks of large multimodal models **Kapoor, "Multi-Agent Frameworks for Multimodal Models in Cross-Cultural Understanding"** have shown further promise in enhancing cross-cultural understanding. For instance, MosAIC **Mosbach, "MosAIC: A Multi-Agent Framework for Cross-Cultural Image Captioning"** employs a multi-agent framework for cross-cultural understanding but focuses on image captioning in single-culture contexts rather than text-to-image generation. Our work addresses this gap by examining how state-of-the-art text-to-image models handle multicultural representations within the same image.
\vspace{-0.8em}
\paragraph{Text-to-Image Generation Models and Benchmarks.} Text-to-image generative capabilities have advanced rapidly in recent years, as evidenced by models such as Stable Diffusion-XL **Hinz, "Stable Diffusion-XL: A Large-Scale Text-to-Image Model"**, DALLE-3 **Bussolini, "DALLE-3: A 3D Text-to-Image Generation Model"**, and FLUX **Wang, "FLUX: A Flexible and Efficient Text-to-Image Generation Model"**. Evaluation benchmarks like TIFA **Chen, "TIFA: A Benchmark for Text-to-Image Evaluation"**, GenEval **Kim, "GenEval: A Comprehensive Benchmark for Text-to-Image Generation"**, and GenAIBench **Zhou, "GenAIBench: A Generalized AI Benchmark for Text-to-Image Generation"** traditionally emphasize technical factors such as realism, text faithfulness, and compositional accuracy. More recent work, i.e., HEIM **Li, "HEIM: A Socially Situated Evaluation Metric for Text-to-Image Models"**, extends these metrics to include socially situated aspects like toxicity, bias, and aesthetics, reflecting growing concern for the social impact of generative models **Bhatia, "The Social Impact of Generative Models in Cross-Cultural Understanding"**.
\vspace{-0.8em}
\paragraph{Cultural Gap and Language Limitations in Text-to-Image Generation.} Despite advancements, existing efforts predominantly focus on a narrow set of languages (e.g., English, Chinese, Japanese), leaving large user communities underserved. 
Recent multilingual models, such as Taiyi-Diffusion-XL **Wang, "Taiyi-Diffusion-XL: A Large-Scale Multilingual Text-to-Image Model"**, target Chinese text input, while AltDiffusion **Shen, "AltDiffusion: An 18-Language Text-to-Image Generation Model"** expands language coverage to eighteen languages. However, a broader ``cultural gap'' persists **Chen, "The Cultural Gap in Cross-Cultural Understanding of Generative Models"**, as most models and benchmarks insufficiently capture diverse cultural settings and interactions.
\vspace{-0.8em}
\paragraph{Data Diversity and Cultural Competence.} Only recently have researchers begun to evaluate cultural competence in text-to-image models. For instance, CUBE **Wang, "CUBE: A Benchmark for Evaluating Cross-Cultural Understanding in Text-to-Image Models"** assesses cultural awareness and diversity, yet still focuses on single-culture depictions per image. To our knowledge, no existing work systematically addresses multicultural scenarios—where multiple cultures may be represented in a single image—and rigorously evaluates the performance of state-of-the-art text-to-image systems under such conditions. Our approach aims to fill this gap by exploring how these models handle more complex, multicultural representations.