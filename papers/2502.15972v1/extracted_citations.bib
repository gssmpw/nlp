@article{bai2024power,
  title={The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning},
  author={Bai, Longju and Borah, Angana and Ignat, Oana and Mihalcea, Rada},
  journal={arXiv preprint arXiv:2411.11758},
  year={2024}
}

@article{betker2023improving,
  title={Improving image generation with better captions},
  author={Betker, James and Goh, Gabriel and Jing, Li and Brooks, Tim and Wang, Jianfeng and Li, Linjie and Ouyang, Long and Zhuang, Juntang and Lee, Joyce and Guo, Yufei and others},
  journal={Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf},
  volume={2},
  number={3},
  pages={8},
  year={2023}
}

@inproceedings{bhatia-etal-2024-local,
    title = "From Local Concepts to Universals: Evaluating the Multicultural Understanding of Vision-Language Models",
    author = "Bhatia, Mehar  and
      Ravi, Sahithya  and
      Chinchure, Aditya  and
      Hwang, EunJeong  and
      Shwartz, Vered",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.385/",
    doi = "10.18653/v1/2024.emnlp-main.385",
    pages = "6763--6782",
    abstract = "Despite recent advancements in vision-language models, their performance remains suboptimal on images from non-western cultures due to underrepresentation in training datasets. Various benchmarks have been proposed to test models' cultural inclusivity. Still, they have limited coverage of cultures and do not adequately assess cultural diversity across universal and culture-specific local concepts. To address these limitations, we introduce the GlobalRG benchmark, comprising two challenging tasks: retrieval across universals and cultural visual grounding. The former task entails retrieving culturally diverse images for universal concepts from 50 countries, while the latter aims at grounding culture-specific concepts within images from 15 countries. Our evaluation across a wide range of models reveals that the performance varies significantly across cultures {--} underscoring the necessity for enhancing multicultural understanding in vision-language models."
}

@misc{flux2023,
    author={Black Forest Labs},
    title={FLUX},
    year={2023},
    howpublished={\url{https://github.com/black-forest-labs/flux}},
}

@article{ghosh2024geneval,
  title={Geneval: An object-focused framework for evaluating text-to-image alignment},
  author={Ghosh, Dhruba and Hajishirzi, Hannaneh and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{guo2024large,
  title={Large language model based multi-agents: A survey of progress and challenges},
  author={Guo, Taicheng and Chen, Xiuying and Wang, Yaqi and Chang, Ruidi and Pei, Shichao and Chawla, Nitesh V and Wiest, Olaf and Zhang, Xiangliang},
  journal={arXiv preprint arXiv:2402.01680},
  year={2024}
}

@article{han2024llm,
  title={LLM multi-agent systems: Challenges and open problems},
  author={Han, Shanshan and Zhang, Qifan and Yao, Yuhang and Jin, Weizhao and Xu, Zhaozhuo and He, Chaoyang},
  journal={arXiv preprint arXiv:2402.03578},
  year={2024}
}

@article{hartwig2024evaluating,
  title={Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics},
  author={Hartwig, Sebastian and Engel, Dominik and Sick, Leon and Kniesel, Hannah and Payer, Tristan and Ropinski, Timo and others},
  journal={arXiv preprint arXiv:2403.11821},
  year={2024}
}

@inproceedings{hu2023tifa,
  title={Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering},
  author={Hu, Yushi and Liu, Benlin and Kasai, Jungo and Wang, Yizhong and Ostendorf, Mari and Krishna, Ranjay and Smith, Noah A},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20406--20417},
  year={2023}
}

@article{kannen2024beyond,
  title={Beyond aesthetics: Cultural competence in text-to-image models},
  author={Kannen, Nithish and Ahmad, Arif and Andreetto, Marco and Prabhakaran, Vinodkumar and Prabhu, Utsav and Dieng, Adji Bousso and Bhattacharyya, Pushpak and Dave, Shachi},
  journal={arXiv preprint arXiv:2407.06863},
  year={2024}
}

@article{lee2024holistic,
  title={Holistic evaluation of text-to-image models},
  author={Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah and Bellagente, Marco and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lin2025evaluating,
  title={Evaluating text-to-visual generation with image-to-text generation},
  author={Lin, Zhiqiu and Pathak, Deepak and Li, Baiqi and Li, Jiayao and Xia, Xide and Neubig, Graham and Zhang, Pengchuan and Ramanan, Deva},
  booktitle={European Conference on Computer Vision},
  pages={366--384},
  year={2025},
  organization={Springer}
}

@incollection{liu2024cultural,
  title={On the cultural gap in text-to-image generation},
  author={Liu, Bingshuai and Wang, Longyue and Lyu, Chenyang and Zhang, Yong and Su, Jinsong and Shi, Shuming and Tu, Zhaopeng},
  booktitle={ECAI 2024},
  pages={930--937},
  year={2024},
  publisher={IOS Press}
}

@article{pawar2024survey,
  title={Survey of cultural awareness in language models: Text and beyond},
  author={Pawar, Siddhesh and Park, Junyeong and Jin, Jiho and Arora, Arnav and Myung, Junho and Yadav, Srishti and Haznitrama, Faiz Ghifari and Song, Inhwa and Oh, Alice and Augenstein, Isabelle},
  journal={arXiv preprint arXiv:2411.00860},
  year={2024}
}

@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}

@article{romanou2024include,
  title={INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge},
  author={Romanou, Angelika and Foroutan, Negar and Sotnikova, Anna and Chen, Zeming and Nelaturu, Sree Harsha and Singh, Shivalika and Maheshwary, Rishabh and Altomare, Micol and Haggag, Mohamed A and Amayuelas, Alfonso and others},
  journal={arXiv preprint arXiv:2411.19799},
  year={2024}
}

@misc{romero2024cvqaculturallydiversemultilingualvisual,
      title={CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark}, 
      author={David Romero and Chenyang Lyu and Haryo Akbarianto Wibowo and Teresa Lynn and Injy Hamed and Aditya Nanda Kishore and Aishik Mandal and Alina Dragonetti and Artem Abzaliev and Atnafu Lambebo Tonja and Bontu Fufa Balcha and Chenxi Whitehouse and Christian Salamea and Dan John Velasco and David Ifeoluwa Adelani and David Le Meur and Emilio Villa-Cueva and Fajri Koto and Fauzan Farooqui and Frederico Belcavello and Ganzorig Batnasan and Gisela Vallejo and Grainne Caulfield and Guido Ivetta and Haiyue Song and Henok Biadglign Ademtew and Hernán Maina and Holy Lovenia and Israel Abebe Azime and Jan Christian Blaise Cruz and Jay Gala and Jiahui Geng and Jesus-German Ortiz-Barajas and Jinheon Baek and Jocelyn Dunstan and Laura Alonso Alemany and Kumaranage Ravindu Yasas Nagasinghe and Luciana Benotti and Luis Fernando D'Haro and Marcelo Viridiano and Marcos Estecha-Garitagoitia and Maria Camila Buitrago Cabrera and Mario Rodríguez-Cantelar and Mélanie Jouitteau and Mihail Mihaylov and Mohamed Fazli Mohamed Imam and Muhammad Farid Adilazuarda and Munkhjargal Gochoo and Munkh-Erdene Otgonbold and Naome Etori and Olivier Niyomugisha and Paula Mónica Silva and Pranjal Chitale and Raj Dabre and Rendi Chevi and Ruochen Zhang and Ryandito Diandaru and Samuel Cahyawijaya and Santiago Góngora and Soyeong Jeong and Sukannya Purkayastha and Tatsuki Kuribayashi and Thanmay Jayakumar and Tiago Timponi Torrent and Toqeer Ehsan and Vladimir Araujo and Yova Kementchedjhieva and Zara Burzo and Zheng Wei Lim and Zheng Xin Yong and Oana Ignat and Joan Nwatu and Rada Mihalcea and Thamar Solorio and Alham Fikri Aji},
      year={2024},
      eprint={2406.05967},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.05967}, 
}

@article{singh2024global,
  title={Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation},
  author={Singh, Shivalika and Romanou, Angelika and Fourrier, Cl{\'e}mentine and Adelani, David I and Ngui, Jian Gang and Vila-Suero, Daniel and Limkonchotiwat, Peerat and Marchisio, Kelly and Leong, Wei Qi and Susanto, Yosephine and others},
  journal={arXiv preprint arXiv:2412.03304},
  year={2024}
}

@article{wu2024taiyi,
  title={Taiyi-Diffusion-XL: advancing bilingual text-to-image generation with large vision-language model support},
  author={Wu, Xiaojun and Zhang, Dixiang and Gan, Ruyi and Lu, Junyu and Wu, Ziwei and Sun, Renliang and Zhang, Jiaxing and Zhang, Pingjian and Song, Yan},
  journal={arXiv preprint arXiv:2401.14688},
  year={2024}
}

@inproceedings{ye2024altdiffusion,
  title={Altdiffusion: A multilingual text-to-image diffusion model},
  author={Ye, Fulong and Liu, Guang and Wu, Xinya and Wu, Ledell},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={7},
  pages={6648--6656},
  year={2024}
}

