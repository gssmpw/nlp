\section{Related Work}
\paragraph{Cultural Evaluation in Language and Vision Models.}

Research in language-based models is advancing rapidly in capturing cultural nuances through large multilingual evaluation benchmarks~\cite{pawar2024survey,romanou2024include,singh2024global}. In the language-vision domain, recent benchmarks like CVQA~\cite{romero2024cvqaculturallydiversemultilingualvisual} and GlobalRG~\cite{bhatia-etal-2024-local} focus on culturally aware question answering, retrieval, and visual grounding. Novel methods leveraging multi-agent frameworks of large multimodal models~\cite{guo2024large, han2024llm} have shown further promise in enhancing cross-cultural understanding. For instance, MosAIC~\cite{bai2024power} employs a multi-agent framework for cross-cultural understanding but focuses on image captioning in single-culture contexts rather than text-to-image generation. Our work addresses this gap by examining how state-of-the-art text-to-image models handle multicultural representations within the same image.
\vspace{-0.8em}
\paragraph{Text-to-Image Generation Models and Benchmarks.} Text-to-image generative capabilities have advanced rapidly in recent years, as evidenced by models such as Stable Diffusion-XL~\cite{podell2023sdxl}, DALLE-3~\cite{betker2023improving}, and FLUX~\cite{flux2023}. Evaluation benchmarks like TIFA~\cite{hu2023tifa}, GenEval~\cite{ghosh2024geneval}, and GenAIBench~\cite{lin2025evaluating} traditionally emphasize technical factors such as realism, text faithfulness, and compositional accuracy. More recent work, i.e., HEIM~\cite{lee2024holistic}, extends these metrics to include socially situated aspects like toxicity, bias, and aesthetics, reflecting growing concern for the social impact of generative models~\cite{hartwig2024evaluating}.
\vspace{-0.8em}
\paragraph{Cultural Gap and Language Limitations in Text-to-Image Generation.} Despite advancements, existing efforts predominantly focus on a narrow set of languages (e.g., English, Chinese, Japanese), leaving large user communities underserved. 
Recent multilingual models, such as Taiyi-Diffusion-XL~\cite{wu2024taiyi}, target Chinese text input, while AltDiffusion~\cite{ye2024altdiffusion} expands language coverage to eighteen languages. However, a broader ``cultural gap'' persists~\cite{liu2024cultural}, as most models and benchmarks insufficiently capture diverse cultural settings and interactions.
\vspace{-0.8em}
\paragraph{Data Diversity and Cultural Competence.} Only recently have researchers begun to evaluate cultural competence in text-to-image models. For instance, CUBE~\cite{kannen2024beyond} assesses cultural awareness and diversity, yet still focuses on single-culture depictions per image. To our knowledge, no existing work systematically addresses multicultural scenarios—where multiple cultures may be represented in a single image—and rigorously evaluates the performance of state-of-the-art text-to-image systems under such conditions. Our approach aims to fill this gap by exploring how these models handle more complex, multicultural representations.