
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


%%%%% my packages 
\usepackage{courier}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}     
\usepackage{subcaption}  

\usepackage[most]{tcolorbox}

\title{The Order Effect: Investigating Prompt Sensitivity in Closed-Source LLMs}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Bryan Guan,$^{*\dagger}$ Tanya Roosta,$^{*\dagger}$ Peyman Passban,\thanks{Equal contribution.}\hspace{1.5mm}\thanks{This work was conducted independently and is not associated with the authors' employers. All views and conclusions are solely those of the authors.} \hspace{0.4mm} \& Mehdi Rezagholizadeh$^{\dagger}$\\
\texttt{troosta@ischool.berkeley.edu}
%Department of Computer Science\\
%Cranberry-Lemon University\\
%Pittsburgh, PA 15213, USA \\
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%%%%% camera ready!
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in closed-source LLMs by conducting experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation, however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.
\end{abstract}

\section{Introduction}
\label{intro}
In recent years, large language models (LLMs) have become essential across various applications, helping users complete tasks in diverse domains, thanks to their remarkable abilities in understanding, analyzing, and generating text \citep{shen2023shaping, yu2023leveraging}. However, LLMs are not without their problems and risks. Many of these issues, such as bias \citep{Talat2022,Motoki2023},  hallucination \citep{chen2023hallucination, sadat2023delucionqa}, consistency \citep{Tam2023, Ye2023}, and reliability \citep{Shen2023} have been extensively discussed in the literature. However, a more fundamental challenge to the long-term success of LLMs is their ability to reason: the distinguishing factor between probabilistic pattern matching and logical understanding. This distinction has significant implications for the future of LLMs and how we employ these models in decision-making.

One necessary requirement for reasoning is order independence. A model should provide the same consistent response to a query regardless of the order of its content. Historically, LLMs have struggled with this issue. Swapping subsequences within semantically identical inputs often leads to significant changes in output, a problem that worsens as inputs grow in size and complexity \citep{he2024doespromptformattingimpact}. Today, improvements in LLMs promise more accurate responses that mitigate order dependency. However, it still remains unclear whether these improvements are sufficient to reduce order dependency when these models are used in the wild.

In this paper, we focus on sensitivity to prompt formatting, also referred to as order dependency. This problem has been previously explored in the context of multiple-choice questions \citep{pezeshkpour2023largelanguagemodelssensitivity,zheng2024largelanguagemodelsrobust}, laying the foundation for our research. Building on this foundation, we aim is to provide a fresh perspective and expand the analysis with additional data points to investigate the problem more thoroughly. Although the issue may seem trivial or some may question the need for further study, we demonstrate that it persists, continues to cause problems, and warrants ongoing investigation. We hope to uncover patterns or root causes that can help mitigate order dependency in the future. 

To demonstrate the severity of the problem, we conducted a simple experiment in which we submitted the following prompt twice to \texttt{GPT-4o mini}: 
\begin{center}
``\textit{My son can build an amazing Lego castle by following the instructions, and my daughter can design a beautiful Lego garden to go with it. Who is smarter? My son or my daughter? Please pick one.}"
\end{center}
 In the first attempt, the response choices were presented as ``\textit{a) my son b) my daughter}" and in the second attempt, we swapped the order to ``\textit{a) my daughter b) my son}". To ensure that previous interactions did not influence the model’s responses, we made two parallel calls through the ChatGPT web interface while logged out. The model produced inconsistent answers: in the first trial, it selected ``\textit{my son}" and in the second, it chose ``\textit{my daughter}." This suggests that the order of the responses can easily influence the model's decision-making. Screenshots of the results and the full responses generated by the model are provided in Appendix \ref{appendix-gpt4omini}.

While this example may not have grave consequences, consider more sensitive scenarios, such as the order in which medications are prescribed to a patient, the sequence of steps recommended by a trading agent, or the actions required to assemble machinery. Order sensitivity in these contexts could have significant repercussions. Even in this simple case, the model’s response might (mis)lead researchers into focusing on discussions like gender bias in LLMs. Although bias may be a contributing factor, in this particular case, it could overshadow the less obvious but critical issue which is order sensitivity.   

To investigate this problem, we experimented with \texttt{GPT-4o} and \texttt{GPT-4o mini} and measured their performance on various order-sensitive tasks. We chose to work with closed-source LLMs, specifically the GPT models, because they are widely used by non-technical users. As LLMs become increasingly integrated into society, many people who rely on them may lack the expertise to interact with them and assess their accuracy. Consequently, such interactions could contribute to the spread of inaccurate data across the internet, public discourse, and even scientific publications. The more we analyze and refine our understanding of these models, the better we can guide their use. 

Although similar issues have been reported in open-source models, sometimes even more severe than in their closed-source counterparts, we are less concerned about their uses and thus exclude them from the scope of this paper. Open-source models are primarily used and developed by technical users who engage with them in a more controlled and reliable way. In contrast, closed-source models are accessible to a broader audience, which makes it crucial to understand their potential inconsistencies. We hope this paper contributes to the ongoing discussion regarding the reliability of LLMs as decision-making tools in complex and diverse domains.

The remainder of this paper is organized as follows. In Section \ref{background} we review related work on order sensitivity. In Section \ref{methodology} we outline our experimental design and provide examples of the prompts we used.  Section \ref{results} presents our experimental findings and discusses their implications.  Finally, in Section \ref{conclusion},  we  summarize our contributions and outline directions for future research.

\section{Background}
\label{background}
The existing literature has explored order sensitivity in LLMs. \citet{mcilroyyoung2024orderindependencefinetuning} investigated how reordering elements in multiple-choice questions affects LLM outputs and proposed a set-based prompting technique that modifies positional encoding and attention masks. They focused on open-source models and tried to modify the architecture, an approach that could introduce its own issues and is not feasible for closed-source LLMs. Set-based prompting modifies the model's inference path; however, this may not be practical for all transformer-based LLMs, especially those with rigid architectures or restricted environments. By altering the attention mask and positional encoding, the method pushes the model slightly outside its training distribution, which could lead to unexpected behaviors or performance degradation. It is briefly discussed in their paper that the approach may under perform. Removing order information in their set-based approach also limits the contextual information available during text generation, which can cause other issues. 

\citet{zheng2024largelanguagemodelsrobust} demonstrated that LLMs exhibit selection bias by favoring some choices over others and propose PriDe, a label-free, inference-time de-biasing method. Their evaluation across multiple LLMs and benchmarks underscores the prevalence of selection bias.  Similarly, \citet{pezeshkpour2023largelanguagemodelssensitivity} investigated the problem in the context of multiple-choice questions. They found that positional bias can lead to significant performance gaps across benchmarks and proposed calibration techniques to improve robustness. The proposed calibration methods appear to improve robustness to some extent but do not entirely eliminate the sensitivity issue. 

\citet{sclar2024quantifyinglanguagemodelssensitivity} analyzed how minor formatting changes, such as spacing and casing, affect model performance. They observed that seemingly trivial design choices can lead to large performance gaps, emphasizing the need to evaluate models across a range of formats rather than relying on a single prompt design. \citet{he2024doespromptformattingimpact} investigated how structural formats, including plain text, Markdown, YAML, and JSON, impact performance. Their experiments reveal that prompt formatting choices can lead to large performance differences, which emphasizes the need for prompt-flexibility and careful benchmarking. 

All of these studies emphasize the critical role of input formatting in LLM performance. Our study is the most recent effort to assess the latest improvements in LLMs and examines their behavior across a range of tasks, specifically paraphrasing, relevance judgment, and passage comparison. Our findings reveal that despite recent advancements, the issue of order sensitivity, often dismissed as trivial, persists. This is particularly concerning in practical applications where LLMs are used in real-world scenarios.

\section{Methodology and Experimental Design}
\label{methodology}
To study the impact of order, we conduct five experiments, each consisting of four sub-experiments. In the first sub-experiment, we evaluate the models' performance in a zero-shot setting using the original order of questions and corresponding choices, where the LLM's task is to select the correct option. In the second sub-experiment, we again assess the models' zero-shot performance, but this time the entries are presented in a randomized order. For example, a question with the original order of choices, such as ``a" and ``b", will be presented in its new form with the choices reversed, namely ``b" and ``a" (see the following sections for prompt examples).

For the third sub-experiment, we evaluate the models' performance in a few-shot setting without any modifications to the order. In this setting, we ensure that the context provided to the model is representative and informative. For example, in binary-choice questions, each prompt includes one positive and one negative example from the training set. For non-binary or more complex tasks, five examples are randomly selected from the training set to help the model better understand the task and intent.

Finally, in the fourth sub-experiment, we assess the models' performance in a few-shot setting where the order of entries is randomized. This reordering does not follow any specific pattern. Instead, positions are randomly assigned to prevent any direct or indirect order-based biases. Together, these four setups allow us to compare model performance in zero-shot and few-shot settings and evaluate the impact of input order on their outputs.

\subsection{Experiment 1: MRPC}\label{exp1-mrpc}
In the first experiment, we evaluate the models’ ability to compare two sentences and determine whether they are paraphrases of each other, regardless of the order in which those sentences are presented. We use Microsoft’s MRPC\footnote{\url{https://huggingface.co/datasets/nyu-mll/glue}} dataset \citep{dolan-brockett-2005-automatically}, which contains sentence pairs with human annotations indicating whether each pair is semantically equivalent. We test whether the models provide the same answer regardless of which sentence is presented first. We used the $1725$ examples in the test set. The prompt used in our pipeline is structured as follows. Consider the following sentence pair:

\textit{\textbf{Sentence 1:} Amrozi accused his brother, whom he called ``the witness,'' of deliberately distorting his evidence.}

\textit{\textbf{Sentence 2:} Referring to him as only ``the witness,'' Amrozi accused his brother of deliberately distorting his evidence.}

The correct answer, as assigned by human annotators, is ``Equivalent" for this pair, which means these two sentences are paraphrases of each other. The corresponding prompt used in our experiments is:
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, fonttitle=\bfseries]
I have two sentences that I want to compare.\\
Sentence 1: ``\{\texttt{\textcolor{red}{sentence\_1}}\}"\\
Sentence 2: ``\{\texttt{\textcolor{blue}{sentence\_2}}\}"\\
Are they semantically equivalent? If so, respond with ``equivalent". If not, respond with ``not\_equivalent". Please take into account the meaning, context, and intent of each sentence.
\end{tcolorbox}

Where \texttt{\textcolor{red}{sentence\_1}} and \texttt{\textcolor{blue}{sentence\_2}} are variables which are replaced with the real sentences shared above. This prompt is used for the zero-shot setup with the original order. For the zero-shot setup with the shuffled order, the only change we make to the prompt is swapping the order of the sentences. Specifically, the part of the prompt needs to be modified is shown below:
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, fonttitle=\bfseries]
Sentence 1: ``\{\texttt{\textcolor{blue}{sentence\_2}}\}"\\
Sentence 2: ``\{\texttt{\textcolor{red}{sentence\_1}}\}"
\end{tcolorbox}
Few-shot versions of the prompts follow the same structure, with the key difference being that they include examples to provide richer context. Due to space limitations, we do not present all four types of prompts (zero-shot with the original order, zero-shot with the shuffled order, few-shot with the original order, and few-shot with the shuffled order) in this section. For additional prompt examples, please refer to Appendix \ref{appendix-prompts-example}.

\subsection{Experiment 2: WebGPT}\label{exp2-webgpt}
The second experiment focuses on comparison consistency. We evaluate the models’ ability to compare two answers to a given question and determine which answer a human judge would prefer, regardless of the order in which the answers are presented. We use OpenAI’s WebGPT dataset \citep{nakano2021webgpt},\footnote{\url{https://huggingface.co/datasets/openai/webgpt_comparisons}} which provides pairwise comparisons derived from a reward model trained on human feedback to reflect real-world preferences for long-form question answering. To capture the models' behavior, we switch the order of the answers and observe whether they consistently select the same preferred answer. We use the $1958$ examples from the test set. Each example contains a question, two model-generated answers, and a human-annotated preference score that indicates which answer is better (A or B). The answer can also be ``No Preference" when both responses are equally good. To ensure that the prompt in the few-shot setting is informative, we include one example from each case (\textit{A is better than B}, \textit{B is better than A}, and \textit{A and B are equally good}) within the prompt. This guarantees that the task and intent are clearly communicated to the LLM. For specific prompt examples, please see Appendix \ref{appendix-prompts-example}. 

\subsection{Experiment 3: MSMARCO}\label{exp3-marco}
In the third experiment, a relevance judgment task, we evaluate the models’ ability to identify the most relevant passage for a given query, regardless of the order in which the passages are presented. We use Microsoft’s MSMARCO dataset \citep{bajaj2016ms},\footnote{\url{https://huggingface.co/datasets/microsoft/ms_marco}} which contains queries paired with multiple candidate passages. Each query includes a binary array in which a value of $1$ refers to the index of the most relevant passage and $0$ indicates less relevant passages. We shuffle the passages to test whether the models consistently classify the same passage as the most relevant.

For our experiments, we used samples from the validation set, because the test set labels are not available. From the $101093$ entries in the validation set, we first filtered for those in which a most relevant passage was determined (i.e., the binary array had a cell with a value of $1$). We then sampled $3938$ instances for each of the five query categories (DESCRIPTION, ENTITY, NUMERIC, PERSON, and LOCATION).  We aimed for a consistent sample size across all categories, so the sample size was dictated by the PERSON category, which had a maximum of $3938$ qualifying instances. For this task, which is slightly more complex than the two previous ones, our few-shot prompts include five examples.

\subsection{Experiment 4: MMLU}\label{exp4-mmlu}
In the fourth experiment, we evaluate the models’ ability to answer multiple-choice questions while assessing their robustness to changes in the order of answer choices. We use the MMLU dataset \citep{hendrycks2020measuring},\footnote{\url{https://huggingface.co/datasets/cais/mmlu}} which covers $57$ diverse subjects, including humanities, STEM, social sciences, and other specialized domains, making it a comprehensive test of LLMs' general knowledge and reasoning capabilities.

To test the impact of order sensitivity, we randomly shuffle the answer choices and evaluate whether the models consistently select the correct answer. We use all $14042$ examples from the test set, with each example consisting of a question, four possible answers, and the correct answer. By assessing the models' performance under these conditions, we gain insights into their ability to maintain accuracy and robustness when presented with varying input structures. Similar to the previous experiment's setup, the few-shot prompts include five examples.  

\subsection{Experiment 5: MedMCQA}\label{exp5-medq}
In the fifth experiment, we evaluate the models' performance using the MedMCQA dataset \citep{pal2022medmcqa},\footnote{\url{https://huggingface.co/datasets/openlifescienceai/medmcqa}} a multiple-choice dataset designed to assess medical knowledge. This dataset is a widely used benchmark for evaluating LLMs' ability to handle domain-specific knowledge, particularly in the medical field. We use the $2816$ examples from the validation set because the test set labels are not publicly available. From the validation set, we only select the questions for which the ``correct option" field (\texttt{cop}) was not equal to $-1$, indicating that a correct answer exists. Additionally, we filtered the dataset to only include questions where the \texttt{choice\_type} field was set to \texttt{single}, ensuring that each question has exactly one correct answer. This filtering process resulted in $2816$ valid examples, which we used to evaluate the models' accuracy and robustness in handling medically focused multiple-choice questions. Similar to the previous experiment, the few-shot prompts include five examples.

\section{Experimental Results}
\label{results}
Table \ref{mrpc} presents our findings for the MRPC task (\ref{exp1-mrpc}). The results indicate that shuffling the order of input leads to a meaningful change in  performance for both models. In the zero-shot setup, we observe a 2.78\% drop in performance for \texttt{GPT-4o}, while \texttt{GPT-4o mini} shows almost no variation.\footnote{Exact results with four decimal points reveal a slight drop.} In the few-shot setup, \texttt{GPT-4o} experiences a 1.31\% decrease, whereas the mini version shows a 1.51\% improvement. The task of swapping two semantically identical choices may seem trivial, and we initially expected such advanced models to be robust against this kind of input variation. Surprisingly, however, the results suggest that current LLMs remain sensitive to small changes in input order. Even more unexpectedly, the mini version demonstrates greater stability compared to its more sophisticated counterpart. Ideally, there should be minimal or no change in performance between the original and shuffled orders, but our findings reveal otherwise.  This further highlights the need for detailed investigation into the underlying causes of order sensitivity. 
\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c }
\hline
\multirow{2}{*}{\textbf{Experiment}} & \multicolumn{4}{c}{\texttt{GPT-4o}} & \multicolumn{4}{c}{\texttt{GPT-4o mini}} \\ \cline{2-9} 
& Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} & Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} \\ \hline
Zs-O & 0.77	& 0.71 & 0.74 & \multirow{2}{*}{\textbf{-2.78}} & 0.75 & 0.61& 0.67 & \multirow{2}{*}{\textbf{0.00}} \\  
Zs-S & 0.75	& 0.69 & 0.72 &  & 0.76	& 0.60	& 0.67 &  \\ \hline
Fs-O & 0.78	& 0.77 & 0.77 & \multirow{2}{*}{\textbf{-1.31}} & 0.7485	& 0.5751& 0.65 & \multirow{2}{*}{\textbf{+1.51}} \\  
Fs-S & 0.77	& 0.76 & 0.76 &  & 0.7546	& 0.5791	& 0.66 &  \\ \hline
\end{tabular}
\caption{\label{mrpc}MRPC performance comparison for \texttt{GPT-4o} and \texttt{GPT-4o mini} with percentage change in F1 score after modifying the order. The delta ($\Delta$) is calculated by taking the difference between the F1 score after and the F1 score before shuffling, then dividing the difference by the F1 score after shuffling, e.g. $-2.78\% = \frac{0.72-0.74}{0.72} \times 100$. Zs, Fs, O, and S stand for Zero-shot, Few-Shot, Original Order, and Shuffled Order, respectively.}
\end{table}

The results obtained from the WebGPT task (\ref{exp2-webgpt}) in Table \ref{webgpt-results} are even more surprising. Although few-shot learning was expected to alleviate the problem by providing more informative prompts, it failed to prevent significant performance degradation. In the zero-shot setting, the decline is even more severe. This can be attributed to the increased complexity of the WebGPT task compared to MRPC, illustrating how LLMs become increasingly fragile as task complexity grows.
\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c }
\hline
\multirow{2}{*}{\textbf{Experiment}} & \multicolumn{4}{c}{\texttt{GPT-4o}} & \multicolumn{4}{c}{\texttt{GPT-4o mini}} \\ \cline{2-9} 
& Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} & Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} \\ \hline
ZS-O & 0.55 & 0.52 & 0.53 & \multirow{2}{*}{\textbf{-6.00}} & 0.61 & 0.52 & 0.56 & \multirow{2}{*}{\textbf{-8.93}} \\  
ZS-S & 0.50 & 0.50 & 0.50 &  & 0.51 & 0.51 & 0.51 &  \\ \hline
FS-O & 0.57 & 0.51 & 0.54 & \multirow{2}{*}{\textbf{-3.84}} & 0.60 & 0.50 & 0.55 & \multirow{2}{*}{\textbf{-14.58}} \\  
FS-S & 0.52 & 0.51 & 0.52 &  & 0.48 & 0.48 & 0.48 &  \\ \hline
\end{tabular}
\caption{\label{webgpt-results}WebGPT results for \texttt{GPT-4o} and \texttt{GPT-4o mini} with percentage change ($\Delta$) in F1 scores.}
\end{table}

The length of inputs (i.e. questions and answers) in the WebGPT task is considerably longer than in MRPC, which we believe contributes to the observed results. Across nearly all experiments, we observed a direct correlation between input length and vulnerability to order changes: the longer the prompt, the greater the performance fluctuations. Despite these observations, the extent of the performance deterioration remains disappointing, especially since the only modification introduced was swapping the order of two semantically identical choices.

One possible explanation for this behavior is the auto-regressive nature of LLMs. While this does not necessarily imply that the WebGPT data was seen during training, it suggests that certain linguistic patterns that the models are used to processing in a certain order are disrupted in the shuffled setting, that leads to performance instability. 

Table \ref{msmarco-results} presents MSMARCO results that closely mirror those of the WebGPT task. The longer inputs (one query and multiple passages) in this task pose a greater challenge for LLMs and results in more significant performance drops. While few-shot prompting is expected to improve performance by providing examples, it fails to do so in this experiment. Instead, the additional examples increase the prompt length, ultimately proving counterproductive. This effect is particularly evident in the \texttt{GPT-4o mini}, which is less capable of handling complex and lengthy inputs.
\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c }
\hline
\multirow{2}{*}{\textbf{Experiment}} & \multicolumn{4}{c}{\texttt{GPT-4o}} & \multicolumn{4}{c}{\texttt{GPT-4o mini}} \\ \cline{2-9} 
& Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} & Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} \\ \hline
ZS-O & 0.49 & 0.49 & 0.49 & \multirow{2}{*}{\textbf{-6.66}} & 0.49 & 0.49 & 0.48 & \multirow{2}{*}{\textbf{-11.62}} \\  
ZS-S & 0.47 & 0.46 & 0.45 &  & 0.45 & 0.43 & 0.43 &  \\ \hline
FS-O & 0.48 & 0.48 & 0.48 & \multirow{2}{*}{\textbf{-9.09}} & 0.49 & 0.47 & 0.47 & \multirow{2}{*}{\textbf{-11.90}} \\  
FS-S & 0.46 & 0.44 & 0.44 &  & 0.46 & 0.42 & 0.42 &  \\ \hline
\end{tabular}
\caption{\label{msmarco-results}MSMARCO relevance judgment results for \texttt{GPT-4o} and \texttt{GPT-4o mini} with percentage change ($\Delta$) in F1 scores.}
\end{table}

The key question that arises from these results is: \textit{why does changing the input order consistently lead to performance degradation?} Intuitively, one might expect that such changes could sometimes result in performance gains, but that is rarely the case. This observation further supports the aforementioned hypothesis that LLMs, due to their auto-regressive nature, are accustomed to processing inputs in a specific order, relying either on expected linguistic patterns or, in cases of data contamination, exact words. 

Table \ref{mmlu-results}, reports results from our multiple-choice question-answering task on MMLU, and reveals inconsistencies that are challenging to interpret. Performance fluctuations are more pronounced in \texttt{GPT-4o mini}. Interestingly, few-shot prompting benefits \texttt{GPT-4o mini} but harms the \texttt{GPT-4o}. In the zero-shot setup of \texttt{GPT-4o}, no significant performance change is observed between the original and shuffled inputs, though minor degradation is seen when measured with higher precision. While order variations introduce some differences, they are not substantial. This suggests that for more complex tasks (those involving reasoning or analytical aspects), LLMs may focus more on content, which could potentially make them less sensitive to input order. To explore this further, we conducted a deeper investigation.
\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c }
\hline
\multirow{2}{*}{\textbf{Experiment}} & \multicolumn{4}{c}{\texttt{GPT-4o}} & \multicolumn{4}{c}{\texttt{GPT-4o mini}} \\ \cline{2-9} 
& Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} & Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} \\ \hline
ZS-O & 0.83 & 0.83 & 0.83 & \multirow{2}{*}{\textbf{0.00}} & 0.77 & 0.76 & 0.76 & \multirow{2}{*}{\textbf{-4.10}} \\  
ZS-S & 0.83 & 0.83 & 0.83 &  & 0.74 & 0.73 & 0.73 &  \\ \hline
FS-O & 0.84 & 0.84 & 0.84 & \multirow{2}{*}{\textbf{-1.20}} & 0.75 & 0.74 & 0.74 & \multirow{2}{*}{\textbf{+1.33}} \\  
FS-S & 0.83 & 0.83 & 0.83 &  & 0.75 & 0.75 & 0.75 &  \\ \hline
\end{tabular}
\caption{\label{mmlu-results}MMLU multiple-choice question answering results for \texttt{GPT-4o} and \texttt{GPT-4o mini} with percentage change ($\Delta$) in F1 scores.}
\end{table}

We examined F1 scores at the category level within MMLU in the hope of uncovering meaningful patterns. We focused on the zero-shot results of \texttt{GPT-4o} and the few-shot results of \texttt{GPT-4o mini}, as these cases deviated from our earlier findings. Specifically, we analyzed which categories experienced performance drops and which saw improvements after shuffling. Categories like \texttt{abstract\_algebra}, \texttt{conceptual\_physics}, \texttt{high\_school\_mathematics}, and \texttt{machine\_learning} showed declines, whereas categories, such as \texttt{philosophy}, \texttt{prehistory}, \texttt{professional\_accounting}, and \texttt{world\_religions} showed improvements. Despite these findings, establishing a consistent pattern for the impact of input order remains challenging. Our observations are limited, and as previously noted, the overall results from MMLU remain inconclusive. However, we are cautiously optimistic that as tasks become more complex and require deeper reasoning, LLMs will move beyond surface-level representations toward a more profound understanding of input content. This shift presents opportunities for further research and refinement of LLMs.

Since our multiple-choice results from MMLU were inconclusive, we conducted an experiment with a more complex multiple-choice dataset, MedMCQA, whose results are shown in Table \ref{MedMCQA-results}. The task proved particularly challenging for \texttt{GPT-4o mini}, where we observed noticeable performance drops after shuffling the input order. Additionally, including examples in the prompts had little impact, as there was almost no difference between zero-shot and few-shot runs. For \texttt{GPT-4o}, performance remained relatively unchanged, with a slightly negative trend. This raises the possibility, as discussed previously, that LLMs may focus more on content for complex tasks, making them less sensitive to input order variations. 


\begin{table}[h]
\centering
\begin{tabular}{c c c c c c c c c }
\hline
\multirow{2}{*}{\textbf{Experiment}} & \multicolumn{4}{c}{\texttt{GPT-4o}} & \multicolumn{4}{c}{\texttt{GPT-4o mini}} \\ \cline{2-9} 
& Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} & Precision & Recall & F1 & \multicolumn{1}{c}{$\Delta$(\%)} \\ \hline
ZS-O & 0.7696 & 0.7667 & 0.7681 & \multirow{2}{*}{\textbf{-0.85}} & 0.67 & 0.66 & 0.66 & \multirow{2}{*}{\textbf{-3.12}} \\  
ZS-S & 0.7633 & 0.7601 & 0.7616 &  & 0.65 & 0.64 & 0.64 &  \\ \hline
FS-O & 0.7606 & 0.7564 & 0.7584 & \multirow{2}{*}{\textbf{-0.17}} & 0.66 & 0.66 & 0.66 & \multirow{2}{*}{\textbf{-3.12}} \\  
FS-S & 0.7593 & 0.7550 & 0.7571 &  & 0.64 & 0.64 & 0.64 &  \\ \hline
\end{tabular}
\caption{\label{MedMCQA-results}MedMCQA multiple-choice question answering results for \texttt{GPT-4o} and \texttt{GPT-4o mini} with percentage change ($\Delta$) in F1 scores.}
\end{table}


\subsection{Summary of Observations}
The findings from our experiments (a subset of which were presented in this paper) highlight the following key insights:

\begin{itemize}
    \item The longer the input, the harder it becomes for LLMs to process, and these challenging situations increasingly make LLMs vulnerable to performance drops when the input order is altered.

    \item Shuffling the input sequence almost always leads to accuracy degradation, likely due to the auto-regressive nature of LLMs. These models are trained to process inputs sequentially, so any disruption is treated as out-of-distribution  that prevents performance gains.

    \item For tasks involving well-defined and concise queries, LLMs tend to perform better. Queries such as multiple-choice in MMLU, are short, structured, and clearly communicate their objectives. Additionally, we assume that the presence of multiple options, rather than just two, reduces the randomness associated with binary decisions as observed in MRPC. For this type of dataset, LLMs seem to focus more on the task itself rather than the surface structure of the inputs. However, this remains a speculation based on the results we observed.  
\end{itemize}

Regardless of the task or conditions, order sensitivity remains a significant challenge for LLMs. Despite ongoing advancements, these models continue to exhibit fragility when faced with something as simple as a shuffled input order. The fact that merely changing the order of choices in a question can consistently degrade performance, even in the strongest models, is a concerning limitation that demands further attention.

\section{Conclusion and Future Work}\label{conclusion}
In this paper, we investigated the problem of order sensitivity in LLMs, an issue that remains poorly understood despite prior studies. Our findings reveal a significant impact of input order on LLM performance, underscoring the need for further investigation. Understanding this sensitivity is crucial because LLMs are often used to generate metrics or act as automated judges in various pipelines, where inconsistent outputs can lead to misleading conclusions. Research relying on LLM-based evaluations may be affected by subtle factors such as input order which can impact outcomes.

We also believe that order sensitivity could become even more problematic when LLMs are used outside controlled API settings (as in our experimental setup) such as through web interfaces. In such cases, when LLMs retain past context rather than treating each interaction independently, the order and pattern of early inputs can significantly influence future responses. For instance, our experiments revealed that consistently placing the correct answer in a specific position creates a pattern that the LLM learns, impacting its subsequent predictions.

Despite numerous proposed solutions and investigations, this simple yet perplexing issue remains unresolved. Our future research will focus on addressing this problem, particularly within closed-source LLMs. In the near term, we plan to test non-GPT models on the same datasets and evaluate newer models with stronger reasoning capabilities, aiming to identify more definitive patterns. We hope that these additional data points will help streamline and enhance our investigation.











 



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}
\newpage
\appendix
\section{\texttt{GPT-4o mini} Responses}
\label{appendix-gpt4omini}
\begin{figure}[ht]
    \centering
    % First subfigure (top)
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{son-daughter-1}  % Replace with your image file
        \label{fig:sub1}
    \end{subfigure}
    
    %\vspace{2mm} % Adds vertical space between subfigures
    
    % Second subfigure (bottom)
    \begin{subfigure}[t]{0.8\textwidth}
        \centering
        \includegraphics[width=\linewidth]{son-daughter-2}  % Replace with your image file
        \label{fig:sub2}
    \end{subfigure}
    
    \caption{How \texttt{GPT-4o mini} responds to the same question when the order of choices is reversed. The calls were made on Wednesday, January 29th, at 11:54 a.m. EST.}
    \label{fig:main}
\end{figure}

\section{Prompt Examples}
\label{appendix-prompts-example}

An example of a \textit{zero-shot, original order} prompt from the MMLU dataset: 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black,title={MMLU, Zero-shot, Original order},  fonttitle=\bfseries]
\footnotesize
Question: Determine whether the polynomial in Z[x] satisfies an Eisenstein criterion for irreducibility over Q. 8x\texttt{\^}3 + 6x\texttt{\^}2 - 9x + 24
\\Options:
\\1) Yes, with p=2.
\\2) Yes, with p=3. 
\\3) Yes, with p=5.
\\4) No.
\\Based on the given question and four options, which one is the right answer? Please respond with only ``Option 1", ``Option 2", ``Option 3", or ``Option 4" as your final answer, without any additional explanation.
\end{tcolorbox}

An example of a \textit{zero-shot, shuffled order} prompt from the MSMARCO dataset: 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title={MSMARCO, Zero-shot, Shuffled order}, fonttitle=\bfseries]
\footnotesize
Query: albany mn population
Passages:\\
1) For the unincorporated community in southeast Minnesota named West Albany, see West Albany, Minnesota. Albany is a city in Stearns County, Minnesota, United States. The population was 2,561 at the 2010 census. It is part of the St. Cloud Metropolitan Statistical Area.\\
2)
\\3) Place of birth for U.S.-born residents: 70\% of the 56307 zip code residents lived in the same house 5 years ago. Out of people who lived in different houses, 71\% lived in this county. Out of people who lived in different counties, 50\% lived in Minnesota. 92\% of the 56307 zip code residents lived in the same house 1 year ago. 
\\4) City of Albany, MN Zip Codes. City of Albany, MN Demographic Information. * Demographic data is based on information taken from the 2000 Census. City of Albany, MN covers 1 Area Code. City of Albany, MN covers 1 Zip Code. 15 Cities within 15 Miles of the City of Albany, MN.
\\5) For population 25 years and over in 56307: 1  High school or higher: 87.4\%. 2  Bachelor's degree or higher: 15.4\%. 3  Graduate or professional degree: 3.3 4 \%. Unemployed: 3. 5 2\%. Mean travel time to work (commute): 23.6 minutes. 
\\6) Sponsored Topics. Albany is a city in Stearns County, Minnesota, United States. The population was 2,561 at the 2010 census. It is part of the St. Cloud Metropolitan Statistical Area. 
\\7)   
\\8) Recent posts about Albany, Minnesota on our local forum with over 2,000,000 registered users. Albany is mentioned 87 times on our forum: Latest news from Albany, MN collected exclusively by city-data.com from local newspapers, TV, and radio stations. Ancestries: German (55.6\%), Irish (10.0\%), Polish (5.9\%), Norwegian (5.4\%), Swedish (2.8\%), United States (2.6\%).
\\9) For population 25 years and over in Albany: 1  High school or higher: 86.7\%. 2  Bachelor's degree or higher: 15.4\%. 3  Graduate or professional degree: 4.4 4 \%. Unemployed: 4. 5 3\%. Mean travel time to work (commute): 23.0 minutes.
\\10) Albany, Minnesota, as per 2017 US Census estimate, has a community population of 2,662 people. Albany is located in Stearns County, 20 miles west of St. Cloud and 80 miles northwest of Minneapolis/St. Paul on Interstate 94 (I-94). Albany has direct access to State Highway 238, which originates in Albany.\\\\
Based on the given query and ten passages, which passage can address the query best?  Please respond with only ``Option 1", ``Option 2", to ``Option 10" as your final answer, without any additional explanation.
\end{tcolorbox}
Since this is a prompt for a zero-shot setup, no examples are included in the prompt. However, because the order is shuffled, the passages do not appear in their original positions. For instance, Passage 1, which originally appears as the third passage in the dataset, has been moved to the first position, or in the original form, passages 9 and 10 are empty strings, whereas in the shuffled prompt, passages 2 and 7 are now null strings.

An example of a \textit{few-shot, original order} prompt from the WebGPT dataset: 

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black,title={WebGPT, Few-shot, Original order}, fonttitle=\bfseries]
\footnotesize
Question: Why shouldn't i plug in my refrigerator after moving it
\\
i recently moved to a different city ,and brought a few appliances along with me, but my father was very adament about me waiting around 6 hours before turning it on because "it would ruin the fridge" 
\\Options:\\ 
A) One reason is that the oil in the compressor might flow into the coolant lines and clog them if the refrigerator is plugged in while lying on its side [1, 2]. Another reason is that the weight of the refrigerator can damage its internal parts even if they're not exposed [3].  
\\B) You should wait around six hours before plugging in a refrigerator after moving it [1, 2, 3]. If the fridge was on its side, the oil in the compressor will flow into the coolant lines, and will need to settle before you can use the appliance [1, 3]. Additionally, if the fridge was running during the move, the motor may have lost its starting torque and will need to rest before starting again [3]. In either case, you can ruin the internal mechanisms and potentially break the refrigerator if you plug it in too soon [1]. 
\\C) No Preference\\
Based on the question and the options provided, which one would a human most likely prefer? Please respond with only ``A", ``B", or ``C" as your final answer, without any additional explanation.
\end{tcolorbox}
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black,  fonttitle=\bfseries]
\footnotesize  
To make sure you understand my intention clearly, I also attach three examples here for clarification: 
\\
Example 1:\\
Question: What is this McCutcheon decision americans are talking about, and what does it mean for them?\\
Options:
\\
A) The McCutcheon decision does not directly affect the amount an individual can donate to a candidate.
Instead, it lifted the overall limit on how much one individual can donate to various political committees during a single election cycle. [1] The decision did not affect the base limits on individual contributions to candidates, which remains \$2,600 per election, or \$5,200 counting the primary and general election. The maximum amount one donor can give to a national party committee is still \$32,400, and the maximum PAC contribution is still \$5,000. [1]
\\B) The McCutcheon decision is named after a person, labor lawyer Shaun McCutcheon. It removed aggregate limit rules in regards to political donations. Before the decision, there was a legal limit of \$48,600 that an individual could give to all federal candidates, and a separate limit of \$74,600 to all political parties and PACs. Furthermore, there was an overall limit of \$123,200 to all of the above. [1, 2]
\\C) No Preference
\\Answer: C\\
\\Example 2:
\\... 
\\Answer: A\\
\\Example 3:
\\...
\\Answer: B
Few-shot, shuffled order 
Experiment 2 - 3 shot 
\end{tcolorbox}
In this example, the order of choices is kept untouched as this represents an original-order case. However, we added three examples to the prompt to ensure that our few-shot setup provides sufficient context for the users. Since there are three possible responses (A, B, or C), we provided one example for each option to help the system understand what ``A" (the first option is preferred), ``B" (the second option is preferred), and ``C" (no preference) mean. Due to space constraints, only the full text of the first example has been provided.  

An example of a \textit{few-shot, shuffled order} prompt from the MedMCQA dataset:
\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, title = {MedMCQA, Few-shot, Shuffled order},  fonttitle=\bfseries]
\footnotesize  
Question: Asymmetric widening of the periodontal Ligament around two or more teeth is seen in\\ 
Options:\\
1) osteosarcoma\\ 
2) Paget's disease\\ 
3) metastatic breast carcinoma\\ 
4) Fibrous dysplasia\\  
\\
Based on the given question and four options, which one is the right answer? 
    
Please respond with only ``Option 1", ``Option 2", ``Option 3", or ``Option 4" as your final answer, without any additional explanation. To ensure you clearly understand my intention, I have included five examples for clarification. These examples are not necessarily contextually relevant but are provided to demonstrate how to approach multi-choice questions effectively.
\\
    
Example 1:\\
Question: Pancytopenia is most common after:\\
Options:\\
1) Hepatitis\\
2) Infective carditis\\
3) Pyelonephritis\\
4) Meningitis\\
Answer: Option 1
\end{tcolorbox}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!75!black, fonttitle=\bfseries]
\footnotesize 
Example 2:\\
Question: Which is NOT a third generation Cephalosporin\\
Options:\\
1) Ceftriaxone\\
2) Cefotaxime\\
3) Ceftizoxime\\
4) Cefuroxime\\
Answer: Option 4\\
\\
Example 3:\\
...\\
Answer: Option 2\\
\\
Example 4:\\
...\\
Answer: Option 1\\
\\
Example 5:\\
...\\
Answer: Option 3\\
\end{tcolorbox}
The prompt above includes a question with shuffled choices, meaning the first choice, ``1) osteosarcoma",  is not necessarily the first choice in its original form within the dataset. Additionally, since this is a few-shot setup, we have included five examples alongside the original question to help communicate to the LLM what it is expected to do. The content of Examples 3 to 5 has been omitted for this example due to space constraints.   
\end{document}
