\vspace{-1em}
\section{Introduction}
\vspace{-1em}

% % \lipsum[1-5]


% Sanjiban minor comments
% merge contribution 3 and 4
% say various baselines, not 3.
% get rid of provide insight

% histogram stepes to goal
% ratio between horizon length and steps to goal is what people do

% steps to goal can just be optimality
% histogram (look at searchformer) optimality / success steps
% searchformer had a metric
% excess number of steps
% how does shape change for synchronous to asynchronous
% stacked histogram show failures, etc.

% repeated only shows failures (maybe can show with successes)
% red and green bars repeated all data

% how do things become harder from synchronous to asynchronous
% what is missing?
% in results, what if i wanted 4 stories to tell
% things become harder with asychronous planning (level 0)
% this is a failure mode of ReAct in async vs sync
% more async vs sync failures
% 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Gonzalo introduction draft 2 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% Consider a home robot that helps you prepare dinner. You may command it to take care of all the steps: setting the table, preparing then serving the food, and finally cleaning up. The robot would then consider the dependencies in these tasks to produce an optimal plan to prepare your dinner; for instance, it could identify that the table could be setup while the dinner is cooking or that the dinner should be prepared before preparing the dishwasher. Alternatively, you may also work alongside the robot and split up independent tasks or work together to speed up tasks that other tasks depend on. This scenario outlines the need for robots that can (1) solve diverse long-horizon tasks, (2) plan efficiently and (3) collaborate with humans.

% Consider a home robot designed to assist with dinner preparation. One essential capability for such a robot is (1) \textbf{asynchronous planning}. For instance, when deciding between setting the table or cooking dinner, itâ€™s more efficient to leave the dinner cooking while setting the table. Another key requirement is the ability to solve (2) \textbf{diverse long-horizon tasks}. To reduce human effort, the robot ideally takes care of a long sequence of unique steps including food preparation, serving dinner, and cleaning up afterward. Lastly, the robot must have the ability to (3) \textbf{collaborate with others}. This enables it to work alongside humans or other robots to accelerate the overall process of dinner preparation.

% Sanjiban suggestions
% Suggested flow in place of the above (crude): 
% Large language models (LLMs) have demonstrated impressive reasoning and task planning capabilities in controlled, single-agent environments where tasks are clearly defined and synchronized (cite, cite). However, decision-making in the real world introduces a more intricate array of challenges. Consider a home robot preparing dinner. It must handle (1) \emph{multiple-objectives}: like such and such (2) \emph{time-delays}: from such and such (3) \emph{long-horizon}: such and such
% (4) \emph{multi-agent}: ...
% To tackle these challenges, an agent must be able to do \textbf{\emph{asynchronous planning}} (cite) -- (define it). 

Large language models (LLMs) have demonstrated impressive reasoning and task planning capabilities in short-horizon single-agent environments with clearly defined sequential tasks \citep{yao2022react,yao2023treethoughtsdeliberateproblem,shinn2023reflexionlanguageagentsverbal}; however, decision-making in the real world introduces a more intricate array of challenges. Consider an assistant that helps you with cooking a recipe. It must be able to handle (1) \emph{time delays} such as boiling spaghetti, which takes time to complete. An efficient agent would move onto other steps instead of waiting for the spaghetti to fully cook. It should also handle (2) \emph{diverse long-horizon tasks} that require the assistant to satisfy multiple objectives and reason about dependencies between different actions. Finally, the assistant should handle (3) \emph{multiple agents} by coordinating with others or distributing tasks based on each agent's capability. To tackle these challenges, an agent must be capable of \textbf{asynchronous planning}, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially. With this capability, an agent can coordinate time delays, break down long horizon tasks into subtasks, and efficiently assign subtasks to multiple agents. 
% \GG{return to this}

% To tackle these challenges, blah blah blah past definitoin. then list out 3 ways async planning solves (1), (2), and (3)
% Compared to (2) and (3) [cite? benchmarks], (1) is relatively unexplored...

% [1] is relatively unexplored... (what about 2 and 3?)

% [1,2,3] are solved by async planning (1 sure, 2 and 3?)

% [1] is a necessary pre-req [strong assumption / do comparisons with multi-agent and long-horizon so not our only focus]
% ^ ^ ^
% To tackle these challenges, time delays must first be solved. 


% A base capability that agents require to solve time delays in short-horizon single-agent scenarios is \textbf{\emph{asynchronous planning}} \cite{lin2024graphenhancedlargelanguagemodels}, or the ability to efficiently reason and plan over states and actions that must happen in parallel and in sequence for multiple agents.
% To tackle these challenges, an agent must be able to do \textbf{\emph{asynchronous planning}}: the ability to  these actions in parallel or in sequence or among multiple agents

% definition 1 more time, give 3 reasons how async planning solves (1) (2) (3)

% talk about how most work focuses on 2 and 3 but 1 is an important aspect of real-world planning for any real-world planning.

% (Table~\ref{tab:related-works})

% By improving this capability, we can enable agents to handle time delays in real-world long-horizon multi-agent scenarios.

% To tackle these challenges, an agent must be capable of \textbf{\emph{asynchronous planning}} \cite{lin2024graphenhancedlargelanguagemodels}, or managing finite resources efficiently by parallelizing tasks when possible to complete a task.

% Existing robot benchmarks [cite robosuite, franka kitchen, behavior, robomimic, metaworld, libero, etc] focus on low-level skill acquisition in physics simulation for learning policies capable of dexterous manipulation. An equally important part of robotics involves high-level task planning to generate a sequence of low-level skills to achieve a goal. Traditionally this is done with STRIPS classical planning where states are represented by a set of predicates (i.e. \texttt{on(lettuce, table)}) and actions (i.e. \texttt{move(table, stove, player)}) that have predicate preconditions and effects that add or remove predicates from the state set. With a domain specification that contains state and action definitions and a problem instance describing the objects, initial state, and goal, it is straightforward to generate a graph and run an A* search for an optimal plan; however, this assumes access to a predefined domain which are difficult to create for the real world and must be updated for new tasks.

% On the other hand, leveraging large language models (LLMs) for task planning serves as a flexible alternative to STRIPS that can be readily applied to robot systems [cite saycan, inner monologue, socratic models, code as policies, etc.]. There are many existing LLM agent benchmarks [cite planbench, alfworld, webshop, virtualhome, cuisineworld, etc.] that test LLM's task planning ability; however, these benchmarks evaluate on short-horizon non-temporal tasks. We are interested in a benchmark that tests LLM's temporal planning ability on long-horizon tasks.

% We are interested in a benchmark that stress tests each of these capabilities. Existing robot benchmarks~\cite{zhu2020robosuite, gupta2019relay, srivastava2022behavior, robomimic2021, yu2020meta, liu2024libero} focus on low-level skill acquisition in physics simulation for learning policies capable of dexterous manipulation. While important towards the goal of actualizing home robots, these benchmarks do not focus on higher-level task planning. Closer to this focus are large language model (LLM) agent benchmarks~\cite{valmeekam2022large, shridhar2020alfworld, yao2022webshop, puig2018virtualhome, gong2023mindagent} which test LLM's task planning ability; however, these benchmarks tend to evaluate on short-horizon tasks and lack asynchronous planning.

% We are interested in a benchmark that stress tests asynchronous planning capability by evaluating how it handles time delays, diverse long-horizon tasks, and multiple agents (Table~\ref{tab:related-works}). 

To improve asynchronous planning capability, we are interested in a benchmark (Table~\ref{tab:related-works}) that stress tests LLM agents using time delays.
% AsyncHow \citep{lin2024graphenhancedlargelanguagemodels} benchmarks asynchronous planning but makes a strong assumption that there are enough agents available to achieve an optimal asynchronous plan in short-horizon tasks (up to 9 steps).
AsyncHow \cite{lin2024graphenhancedlargelanguagemodels} benchmarks asynchronous planning but does not use an interactive environment, lacking support for closed-loop planning agents.
ALFWorld \citep{shridhar2021alfworldaligningtextembodied}, WebShop \citep{yao2023webshopscalablerealworldweb} and PlanBench \citep{valmeekam2023planbenchextensiblebenchmarkevaluating} offer long-horizon diverse tasks (up to 50, 48 and 90 steps respectively) but evaluate with a single agent and no time delays. VirtualHome \citep{puig2018virtualhomesimulatinghouseholdactivities} offers long-horizon (up to 96 steps) and multi-agent tasks with procedural generation for extra diversity but also lacks time delays.

% We introduce \texttt{Robotouille}, a cooking environment designed to stress test LLM agents' temporal planning on diverse long-horizon tasks. Robotouille's fully customizable JSON backend is based off PDDL, a generalization of STRIPS, and introduces custom action effects to simplify domain creation. Figure 1 [todo pretty diagram on temporal planning task that clearly shows the complexity of our benchmark] provides an overview of Robotouille. We also implement LLM baselines and show that the top-performing approach only achieves XX.XX\% on our temporal task dataset demonstrating the need for improving LLM's temporal planning ability in future work.

To address these gaps, we introduce \robotouille, a simulator for cooking diverse recipes designed to stress test LLM agents (Figure~\ref{fig:main}). \robotouille tests asynchronous planning through tasks that take time like cooking meat for burgers or sandwiches or filling up a pot with water to cook soup. Its fully customizable JSON backend allows for the addition of new states, actions, and goals simplifying the creation of diverse long-horizon tasks. Finally, \robotouille supports turn-based and real-time multi-agent execution either locally or on the network.

In addition, we provide 3 datasets to test LLM agents' synchronous, asynchronous, and multi-agent planning capabilities. We implement 3 baselines for benchmarking the synchronous and asynchronous datasets, leaving multi-agent for future work, and provide analyses on the failure modes to provide insights for future work. Our hope is for the research community to engage with \robotouille to create an ecosystem of environments and methods that increase the diversity of our testbed and the capabilities of LLM agents.

\input{figures/main/fig_main_yuki}

Our key contributions include the following
\begin{enumerate}
    \item We present a new environment, \robotouille, for stress testing LLM agents' ability to perform asynchronous planning to handle time delays, diverse long-horizon tasks, and multi-agent.
    \item We curate 3 datasets for synchronous, asynchronous, and multi-agent settings, each containing 10 unique tasks each with 10 procedurally generated instances.
    \item We implement various LLM baselines, evaluate them on the synchronous and asynchronous datasets, and provide quantitative and qualitative analyses on failure modes.
\end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sanjiban feedback draft 1 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% Enumerate 1 2 3, call out the 3 things in the 3rd paragraph. its the most important thing in the introduction and needs to be frontloaded. why are these the most important things and why.

% 1
% enumerate list
% 1 temporal 
% 2 long horizon diverse
% 3 multi-agent

% 1st sentence of 2nd paragraph should immediately say there is a gap

% what are you proposing to work on
% what is it that you're trying to do
% before 1 2 3 we propose to develop a benchmark simulator game, etc. that tests these 3 characteristics
% first say very clearly what we do
% say what the gap is that we are filling in
% lots and lots of simulator that work on low leve skill learning, there are relatively fewer for higher level task planning

% STRIPS explanation is too long and subtle point
% lots of simulation that work on low level skills but much fewer skills that work on higher level behavior. of the ones that exist, many of these are setup to
% don't want to alienate people with STRIPS already from the get go. need to
% while these things exist they don't test the temporal aspect (temporal planning)

% nicer and more compact to bring in that LLMs makes the decisions while we are interested in testing just LLM agents
% can't talk about testing the agents until the agents themselves are talked about
% LLMs can easily be 

% 3 desiderata and there is a clear gap but only one space for another argument

% LLMs both promising but a challenge - maybe in
% not trivially used for classical planning, LLMs may be useful, but still issues. cannot be packed in the introduction but needs to be heart of the paper

% looking for a filler between 2nd paragraph this is the gap and 4th paragraph

% temporal planning related works

% need to work on tables and graphs





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Gonzalo introduction draft 1 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% Consider a home robot that helps you prepare dinner. You may command it to take care of all the steps: setting the table, preparing then serving the food, and finally cleaning up. The robot would then consider the dependencies in these tasks to produce an optimal plan to prepare your dinner; for instance, it could identify that the table could be setup while the dinner is cooking or that the dinner should be prepared before preparing the dishwasher. Alternatively, you may also work alongside the robot and split up independent tasks or work together to speed up tasks that other tasks depend on. This scenario outlines the need for robots that can (1) solve diverse long-horizon tasks, (2) plan efficiently and (3) collaborate with humans.

% Existing robot benchmarks [cite robosuite, franka kitchen, behavior, robomimic, metaworld, libero, etc] focus on low-level skill acquisition in physics simulation for learning policies capable of dexterous manipulation. An equally important part of robotics involves high-level task planning to generate a sequence of low-level skills to achieve a goal. Traditionally this is done with STRIPS classical planning where states are represented by a set of predicates (i.e. \texttt{on(lettuce, table)}) and actions (i.e. \texttt{move(table, stove, player)}) that have predicate preconditions and effects that add or remove predicates from the state set. With a domain specification that contains state and action definitions and a problem instance describing the objects, initial state, and goal, it is straightforward to generate a graph and run an A* search for an optimal plan; however, this assumes access to a predefined domain which are difficult to create for the real world and must be updated for new tasks.

% On the other hand, leveraging large language models (LLMs) for task planning serves as a flexible alternative to STRIPS that can be readily applied to robot systems [cite saycan, inner monologue, socratic models, code as policies, etc.]. There are many existing LLM agent benchmarks [cite planbench, alfworld, webshop, virtualhome, cuisineworld, etc.] that test LLM's task planning ability; however, these benchmarks evaluate on short-horizon non-temporal tasks. We are interested in a benchmark that tests LLM's temporal planning ability on long-horizon tasks.

% We introduce \texttt{Robotouille}, a cooking environment designed to stress test LLM agents' temporal planning on diverse long-horizon tasks. Robotouille's fully customizable JSON backend is based off PDDL, a generalization of STRIPS, and introduces custom action effects to simplify domain creation. Figure 1 [todo pretty diagram on temporal planning task that clearly shows the complexity of our benchmark] provides an overview of Robotouille. We also implement LLM baselines and show that the top-performing approach only achieves XX.XX\% on our temporal task dataset demonstrating the need for improving LLM's temporal planning ability in future work.


% Our key contributions include the following
% \begin{enumerate}
%     \item We present a new benchmark, \texttt{Robotouille}, for stress testing temporal planning for LLMs. The environment is designed to be fully customizable with JSONs to simplify creating new tasks or domains
%     \item We curate 3 datasets for non-temporal, temporal, and multi-agent settings, each containing 10 unique tasks each with 10 procedurally generated instances
%     \item We implement 3 LLM baselines and evaluate them on the non-temporal and temporal datasets showing that LLMs struggle on both
%     % don't be colloquail with struggle on both
%     \item We provide an in-depth quantitative and qualitative analysis on failure modes to provide insight for future work
% \end{enumerate}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Gonzalo introduction outline %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% First talk about what the problem is. We want robots in households. To make this happen, they need to do general tasks that involve various different dependencies, do them efficiently, and be able to collaborate as well.

% Then we need to talk about existing work that explores this problem. Robosuite, etc. explore lower level robot interaction but what needs to be focused on is the higher level. Talk about how one way to do this is utilizing a pre-constructed PDDL. Mention that many LLM benchmarks do this like PlanBench and other benchmarks like AlfWorld, VirtualHome, CuisineWorld fall under STRIPS-like dynamics. These are insufficient to capture dynamics like leaving in the middle of an action or background state changes.

% Then we introduce a fully customizable JSON backend intended for creating complex environments for LLM agents. We demonstrate this with a cooking environment, Robotouille, that includes complex effects that cannot be represented with STRIPS such as: repeated effects, delayed effects, creation, deletion.

% Say we're curated 3 datasets (1) sequential (2) parallelization (3) multi-agent and implement baseline LLM agents to test on the first 2 (skip 3 since performance is already poor on first 2). Each dataset has 10 tasks with 10 procedurally generated instances for a total of 300 unique instances.

% Contributions include (1) a user-friendly JSON backend for creating complex LLM agent environments (2) Robotouille, an instance of said backend, along with 3 curated datasets for benchmarking (3) baseline empirical results on various LLMs, open and closed source (4) a quantitative and qualitative analysis on failure modes to drive future work


%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Sanjiban's mental model %
%%%%%%%%%%%%%%%%%%%%%%%%%%%




% We want robots to (1) do general tasks (2) do them efficiently and (3) do them with humans

% A lot of the focus on robotics is currently on low-level skill acquisition. be it learning, policies for pick and place or dexterous manipulation, physics simulation, very low level. lots of benchmarks like robosuite, catering towards low level behavior. however, an equally important part of robotics is higher level reasoning and these higher level reasoning we want robots to handle complex tasks, do these tasks efficiently, do them with humans. To test this, we provide a new benchmark the goal testing the high level reasoning of robots and we specifically look at the domain of cooking that exhibits all of these complexities notably (1) complex tasks where recipes have steps with dependencies, preconditions, etc. (2) -- tasks are complex is too big a term. long horizon and higher level reasoning is baked into there. one of the things that makes these tasks complex is - robot needs to reason about delays and dependencies are good and is the 2nd part.

% only about success and not about efficiency. making dinner in 5 hours versus 1 hour. not just looking at success, looking at time it took to finish the task, etc. delays in the environment. complexity (dependencies - right order, diversity combinatorial number of recipes) and efficiency (parallel and number of steps) and collaboration.

% models and prompting strategies table

% drive home that there is a need for this dataset. a homerun for my work that  doing better on this benchmark would result in progress for robotics (transfer to robot).

% lots of robot setups that already use perception modules to convert to PDDL-like states where actions are given to low-level policies.

% be careful with citing Demo2Code.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Gonzalo's initial thoughts %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% What do I want to convey with this paper?
% \begin{enumerate}
%     \item (1) Insights about how to test LLM's decision-making abilities (need to improve this)
%     \begin{enumerate}
%         \item No need to express it's fully observable, can say its easily partially observable later
%         \item Fully observable environments makes the transition function and ambiguous language (i.e. goal) the only source of uncertainty in LLM decision-making  
%         \item This transition function can be described through an exhaustive set of rule in the prompt
%         \begin{enumerate}
%             \item An example of an exhaustive set of rules for a transition function are the action definitions in a PDDL
%         \end{enumerate}
%         \item LLMs must extrapolate (1) the valid actions and (2) a valid path to the goal with these rules
%     \end{enumerate}
%     \item (2) Insights about how current LLMs perform on the Robotouille benchmark with simple baselines
%     \begin{enumerate}
%         \item Results on closed-source and open-source LLMs for I/O, I/O CoT, ReAct
%         \item Performance should increase I/O < I/O CoT < ReAct because we are forcing the LLM's extrapolations to be more and more explicit
%         \begin{enumerate}
%             \item Extrapolations involve (a) valid actions and (b) valid path to goal
%             \item I/O does (a) and (b) implicitly
%             \item I/O CoT forces (a) to be explicit through state estimation but does (b) implicitly
%             \item ReAct receives (a) through prompt and (b) is explicitly built step by step with feed-back
%         \end{enumerate}
%     \end{enumerate}
%     \item (3) Insights about why (the best) LLMs fail on Robotouille
%     \begin{enumerate}
%         \item Quantitative analyses of results
%         \begin{enumerate}
%             \item Steps left to goal - How close to the goal did the LLM make it?
%             \item Repeated actions - How many times are actions repeated (capture cycling)
%         \end{enumerate}
%         \item Qualitative analyses of results
%         \begin{enumerate}
%             \item Classify failures into a taxonomy of failure cases related to uncertainty in the transition function and goal
%             \item Uncertainty in the transition function, uncertainty in the goal specification
%         \end{enumerate}
%     \end{enumerate}
% \end{enumerate}