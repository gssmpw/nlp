% \textbf{Question 1.} \textit{How successful are baselines?}
\textbf{Finding 1.} Closed-loop baselines outperform open-loop baselines.

Table~\ref{tab:successes} shows the success rates of various LLMs baselines on the synchronous and asynchronous datasets. Table~\ref{tab:tasks-results} shows the task-specific success rates of baselines using \gptfo{}. Success rate is determined by reaching the goal within 1.5 times the optimal number of steps for the given instance. Baselines exceeding this step limit are terminated.
% TODO: Include open-source
% \NS{and Table~\ref{tab:async-baselines}}

Among all the LLM baselines, \react{} with the \gptfo{} model performs the best on the synchronous and asynchronous datasets. \io{} performs worst for most LLMs while \iocot{} improves performance. We qualitatively observed \texttt{gemini-1.5-flash} failing with \react{} since it attempts to solve the few-shot example goal rather than the current environment goal. This is likely due to the long context examples, aligning with findings in \cite{liu2023lostmiddlelanguagemodels} where LLMs struggle with simple tasks in long contexts. We investigate PLaG (BaG) \cite{lin2024graphenhancedlargelanguagemodels} and Reflexion \cite{shinn2023reflexionlanguageagentsverbal} performance on the asynchronous dataset in Appendix~\ref{app:async-baselines} and achieved small performance improvements.
%\NS{However, it is important to note that the Gemini model notably performs poorly with \react{}. We discuss more about this in Section~\ref{app:gemini-discussion}.}

When considering task-specific success over \gptfo{} baselines, \react{} generally achieves higher performance per task. While we list the horizon length as a crude difficulty metric, it is evident that success rate is not solely dependent on it. We investigate this further in Appendix~\ref{app:horizon_complexity}. We also investigate different agent failure modes in more depth in Section~\ref{sec:failures}.

\input{figures/table_1_accuracies}
\input{figures/table_2_task_performance}

% \textbf{Question 2.} \textit{How close to optimal are successes?}
\textbf{Finding 2}. Asynchronous successes are less optimal than synchronous ones.

Fig.~\ref{fig:histogram_optimality} shows a histogram of the binned optimality rates on the successful runs of \gptfo{} \react{} on the synchronous and asynchronous datasets. Optimality rate is $\frac{\|\hat\tau\|}{\|\tau^*\|}$ where $\|\hat\tau\|$ is the number of steps taken by an agent and $\|\tau^*\|$ is the optimal number of steps to reach the goal from the initial state.

For the synchronous dataset, 55.3\% of successful attempts are optimal compared to the asynchronous dataset where only 9.1\% of successful attempts are optimal. We expect this since the order that tasks are done in the synchronous setting does not affect optimality compared to the asynchronous setting. We also see for the asynchronous dataset that 63.6\% of successful attempts are suboptimal in the $(1, 1.25]$ bucket. We qualitatively observe that while the LLM agent usually prioritizes asynchronous subtasks, suboptimal runs were due to inefficient actions, such as waiting while cooking. We further investigate the agent's subtask prioritization in Section~\ref{sec:follow-ups}.

\input{figures/final/fig_histogram_optimality}

% \textbf{Question 3.} \textit{How far off are failures from the goal?}
\textbf{Finding 3.} Asynchronous failures make little progress toward the goal.

Fig.~\ref{fig:histogram_steps_to_go} shows a histogram of the binned normalized steps to go on the failed runs of \gptfo{} \react{} on the synchronous and asynchronous datasets. Steps to go is $\frac{\|\tau^*_{\text{left}}\|}{\|\tau^*\|}$ where $\|\tau^*_{\text{left}}\|$ are the optimal number of steps left to reach the goal from the final state in a failed run and normalization factor $\|\tau^*\|$ is the optimal number of steps to reach the goal from the initial state.

% The synchronous and asynchronous datasets included 7 and 16 attempts in the $(0, 0.5]$ bucket respectively that made progress to the goal but exceeded the step limit. 
% % \GG{TODO: Su yean can we say anything special here about asynchronous having async fails? Answer: not necessarily which is what we expect. typically async bad with soup.} 
% There were 24 and 52 attempts in the $(0.5, 1.0]$ bucket respectively. The largest portion of asynchronous failures were in this bucket showing that most attempts made little to no progress towards the goal. The attempts in buckets beyond $1.0$ made progress away from the goal. Both datasets account for 22 and 19 attempts, where 3 of the synchronous attempts were excessively away from the goal, 2 of which were twice as far from where they started.

\input{figures/final/fig_histogram_steps_to_go}

For the asynchronous dataset, about 58.6\% of the failures are in the $(0.5, 1.0]$ bucket, which shows that most attempts made little to no progress towards the goal. We also see this in the synchronous dataset, with 41.5\% of failures in the $(0.5, 1.0]$ bucket. We show quantitative results of \gptfo{} \react{}'s ineffective failure recovery in Section~\ref{sec:failures} suggesting that failures on the asynchronous dataset are mainly due to little progress being made. In contrast, we see 45.3\% failures in the synchronous dataset from $(1.0, \infty)$ which show that most attempts make progress away from the goal. The asynchronous dataset only has 25.3\% failures from $(1.0, \infty)$. We present qualitatively annotated failures in Section~\ref{sec:failures} that suggest failures on the synchronous dataset are due to misunderstanding the goal. 