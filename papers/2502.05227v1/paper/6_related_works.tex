\subsection{Related Works}
\label{sec:related_works}
% In this section we will focus on differentiating from other related works (Table~\ref{tab:related-works}) and group work related to asynchronous planning. The field of LLM agent benchmarks for task planning performance consists of a large literature. \robotouille{} aims to extend the existing work with a LLM assessment environment that uses multi-agent asynchronous planning on long-horizon tasks.

In this section we will focus on our desiderata for LLM assistants and how \robotouille{} is different from other related works (Table~\ref{tab:related-works}).

% \input{figures/table_3_related_works}

\textbf{Asynchronous Planning}
% While there exist many benchmarks that test the task planning abilities of LLMs agents (\cite{shridhar2021alfworldaligningtextembodied}; \cite{gong2023mindagent}; \cite{liu2018reinforcementlearningwebinterfaces}; \cite{valmeekam2023planbenchextensiblebenchmarkevaluating}; \cite{yao2024taubenchbenchmarktoolagentuserinteraction}; \cite{zhou2024webarenarealisticwebenvironment};\cite{yao2022webshop}), very few test the ability to plan asynchronously. The field of asynchronous planning benchmarks begins with benchmarks testing the temporal logic abilities of LLMs like TRAM (\cite{wang2024trambenchmarkingtemporalreasoning}). Of those benchmarks which test asynchronous planning, many use graph-based algorithms like GoT or GNNs (\cite{wu2024graphlearningimprovetask}; \cite{Besta_2024}). In fact, AsyncHow (\cite{lin2024graphenhancedlargelanguagemodels}), which extends both asynchronous and graph-based planning, proposes Plan Like a Graph (PLaG).
% When planning to bake potatoes or cut onions, it is important to realize that it is more efficient to cut onions while the potatoes are baking. Thus, a temporal component in tasks can be critical in task planning. While Overcooked AI (\cite{carroll2020utilitylearninghumanshumanai}) includes time delays for cooking in their environment, it has only 1 objective-based task so it is not able to test situations where multiple temporal tasks occur asynchronously. AsyncHow (\cite{lin2024graphenhancedlargelanguagemodels}) creates a benchmark for asynchronous planning with LLMs but uses GPT-4 to calculate estimates for each task. In contrast, \robotouille{} tests asynchronous task planning in a kitchen environment using temporal tasks like cutting and frying. In \robotouille{}, multiple tasks can occur sequentially or simultaneously which mimics real world interactions and planning.
Many benchmarks evaluate the task planning abilities of LLM agents \citep{shridhar2021alfworldaligningtextembodied, gong2023mindagent,liu2018reinforcementlearningwebinterfaces,valmeekam2023planbenchextensiblebenchmarkevaluating,yao2024taubenchbenchmarktoolagentuserinteraction,zhou2024webarenarealisticwebenvironment,yao2023webshopscalablerealworldweb} but few test the ability to plan asynchronously. Existing work relevant to asynchronous planning evaluate LLM capabilities on temporal logic \citep{wang2024trambenchmarkingtemporalreasoning} or use graph-based techniques \citep{wu2024graphlearningimprovetask}; \citep{Besta_2024}) but do not focus on it. \citep{lin2024graphenhancedlargelanguagemodels} proposes the Plan Like a Graph technique and a benchmark AsyncHow that focuses on asynchronous planning but makes a strong assumption that infinite agents exist. \citep{carroll2020utilitylearninghumanshumanai} proposes a benchmark, Overcooked-AI, that involves cooking onion soup which has time delays but has limited tasks and focuses on lower-level planning without LLM agents. \robotouille{} has a dataset focused on asynchronous planning that involves actions including cooking, frying, filling a pot with water, and boiling water.

% \textbf{Diverse Long-Horizon Task Planning}
% LLMs have a great store of semantic knowledge about the world but it is difficult for them to make decisions as agents because they have not experienced the real world. Works like SayCan (\cite{ahn2022icanisay}) have tried to solve this problem by grounding the LLM in real-world, long horizon tasks. Inner Monologue (\cite{huang2022innermonologueembodiedreasoning}) highlights the application of LLMs to embodied environments like robots where closed-loop language feedback allows better planning for robotic situations. Because of their wealth of knowledge about high-level tasks, reasoning capabilities, and ability to turn natural language into plans, LLMs can make proficient planners (\cite{singh2022progpromptgeneratingsituatedrobot}; \cite{liu2023llmpempoweringlargelanguage}; \cite{Lin_2023}; \cite{Wang_2024}; \cite{huang2024understandingplanningllmagents}). In many papers, LLMs shine as zero-shot generalizers, planners, and reasoners (\cite{huang2022languagemodelszeroshotplanners}; \cite{zeng2022socraticmodelscomposingzeroshot}). There have also been improvements in few-shot prompting with LLMs for the purpose of planning (\cite{yang2023couplinglargelanguagemodels}; \cite{liang2023codepolicieslanguagemodel}; \cite{song2023llmplannerfewshotgroundedplanning}). Now that LLMs can be used as agents in planners, how can we evaluate the effectiveness of the agents?

% Existing LLM agent benchmarks evaluate agents on only short-horizon tasks (\cite{shridhar2020alfworld}; \cite{puig2018virtualhome}; \cite{gong2023mindagent}). Firstly, many benchmarks contain very few goal-based tasks at all. The benchmarks AgentBench (\cite{liu2023agentbenchevaluatingllmsagents}), ARA (\cite{kinniment2024evaluatinglanguagemodelagentsrealistic}), and MLAgentBench (\cite{huang2024mlagentbenchevaluatinglanguageagents}) have 8, 12, 11 tasks respectively. Highlighting long-horizon tasks, the majority of the agent benchmarks in (Table~\ref{tab:related-works}) have less than 50 steps for their longest horizon plans. However, \robotouille{} creates a benchmark with 30 diverse long-horizon tasks with 80 steps as its longest plan, focused on cooking and better evaluates agents’ ability to solve plans with long sequences of steps.

% In addition, \robotouille{} leverages procedural generation. While other agent benchmarks like MiniWoB++ (\cite{liu2018reinforcementlearningwebinterfaces}), PlanBench (\cite{valmeekam2023planbenchextensiblebenchmarkevaluating}), $\tau$-bench (\cite{yao2024taubenchbenchmarktoolagentuserinteraction}), WebArena (\cite{zhou2024webarenarealisticwebenvironment}), and MAgIC (\cite{xu2023magicinvestigationlargelanguage}) support procedural generation, they contain only short-horizon tasks. \robotouille{}’s combination of long-horizon tasks and procedural generation gives it the ability to provide thorough testing of LLM agents on the randomization of tasks and environments.

\textbf{Diverse Long-Horizon Task Planning}
There is vast amount of work that use LLMs to plan \citep{ahn2022icanisay,huang2022innermonologueembodiedreasoning,zeng2022socraticmodelscomposingzeroshot,liang2023codepolicieslanguagemodel,singh2022progpromptgeneratingsituatedrobot,song2023llmplannerfewshotgroundedplanning,yang2023couplinglargelanguagemodels,song2023llmplannerfewshotgroundedplanning} but they tend to evaluate on short-horizon tasks with limited diversity in tasks. We present the number of tasks, longest plan horizon, and procedural generation capability of various benchmarks in Table~\ref{tab:related-works} to capture these axes. Notable LLM agent benchmarks that capture these axes include PlanBench \citep{valmeekam2023planbenchextensiblebenchmarkevaluating}, WebShop \citep{yao2023webshopscalablerealworldweb}, and VirtualHome \citep{puig2018virtualhomesimulatinghouseholdactivities}. \robotouille{} provides a focused set of diverse long-horizon tasks that can be procedurally generated.

% \textbf{Multi-agent Planning}
% Recently, after the development of using a singular LLM as an autonomous agent, multi-agent planners have made advancements in complex decision making and task planning (\cite{guo2024largelanguagemodelbased}; \cite{xi2023risepotentiallargelanguage}). Because of a better division of labor, having multiple agents can break down complex tasks and efficiently finish tasks. Agents can specialize in their roles, allowing for more collaboration. AutoGen (\cite{wu2023autogenenablingnextgenllm}) and ProAgent (\cite{zhang2024proagentbuildingproactivecooperative}) are frameworks used to build LLM-based multi-agent applications in conversation-based and cooperative task settings respectively.

% Many benchmarks have used LLMs as agents for collaboration in a shared environment (\cite{gong2023mindagent}; \cite{carroll2020utilitylearninghumanshumanai}; \cite{yao2024taubenchbenchmarktoolagentuserinteraction}; \cite{liu2023agentbenchevaluatingllmsagents}; \cite{xu2023magicinvestigationlargelanguage}; \cite{ma2024agentboardanalyticalevaluationboard}). However, many of these benchmarks focus on two-agent interactions with uneven task distribution like Overcooked AI (\cite{carroll2020utilitylearninghumanshumanai}). In response, CUISINEWORLD (\cite{gong2023mindagent}) tries to address this problem by assuming equal responsibilities across agents for tasks with competing resources. \robotouille{} instead extends this approach with turn-based environments where multiple LLM agents can control multiple agents to reach an objective within an environment. T-Eval (\cite{chen2024tevalevaluatingtoolutilization}) uses a three pronged approach to multi-agent frameworks with a planner, executor, and reviewers in which each agent can finish its job without switching the roles as dictated by the current plan. But, T-Eval uses multiple agents to annotate solutions which is different from the multi-agent planning of \robotouille{}. While MAgIC (\cite{xu2023magicinvestigationlargelanguage}) also investigates LLM powered multi-agents, it focuses on classic decision-making games like the Prisoner’s Dilemma and Chameleon. \robotouille{}’s cooking-based temporal tasks allow for better evaluation of a complex multi-agent environment.

\textbf{Multi-agent Planning} LLM agent benchmarks like \citep{liu2023agentbenchevaluatingllmsagents,xu2023magicinvestigationlargelanguage,ma2024agentboardanalyticalevaluationboard, gong2023mindagent} evaluate multi-agent interactions but do not involve time delays. OvercookedAI \citep{carroll2020utilitylearninghumanshumanai}, while not an LLM agent benchmark, incorporates time delays which brings the complexity of asynchronous planning to multi-agent settings. \robotouille{} provides a multi-agent dataset for 2-4 agents, a choice between turn-based or realtime planning, and incorporates asynchronous tasks for added complexity.