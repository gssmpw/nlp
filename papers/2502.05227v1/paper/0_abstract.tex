\begin{abstract}

% \GG{1) home robots require the 3 qualities mentioned in the paper. 2) existing benchmarks either focus on low-level skill acquisition or agent benchmarks which evaluate on short-horizon tasks and lack async planning 3) We present Robotouille to stress test LLMs on these 3 axes 4) We show that \gptfo{} \react{} only achieves 47\% and 11\% in sync and async settings. 5) we also show that failure modes are similar in sync and async. 6) follow up studies convey that we need agents that incorporate feedback more effectively and can audit their reasoning.}

% Home robots require the ability to perform asynchronous planning, like deciding to leave dinner cooking while setting the table, solve diverse long-horizon tasks, and collaborate with others. Existing large language model (LLM) agent benchmarks that focus on high-level task planning tend to evaluate on short-horizon tasks and do not test asynchronous planning. We present \robotouille{}, a kitchen environment for stress testing LLM agents to be compatible to plan for home robots with 3 datasets: synchronous, asynchronous, and multi-agent. We show that our best baseline, \gptfo{} \react{}, only achieves 47\% on the synchronous dataset and 11\% on the asynchronous dataset. We further show that the dominant failure modes in these domains are similar to one another. Our findings in follow-up studies express the need to improve LLM agents' abilities to incorporate long-horizon feedback and to self-audit its reasoning for mistakes.

Effective asynchronous planning, or the ability to efficiently reason and plan over states and actions that must happen in parallel or sequentially, is essential for agents that must account for time delays, reason over diverse long-horizon tasks, and collaborate with other agents. While large language model (LLM) agents show promise in high-level task planning, current benchmarks focus primarily on short-horizon tasks and do not evaluate such asynchronous planning capabilities. We introduce \robotouille{}, a challenging benchmark environment designed to test LLM agents' ability to handle
% asynchronous, long-horizon, and multi-agent scenarios.
long-horizon asynchronous scenarios.
% These datasets
Our synchronous and asynchronous datasets
capture increasingly complex planning challenges that go beyond existing benchmarks,
% particularly in their requirement for agents 
requiring agents 
% to manage overlapping tasks, interruptions, and collaboration. 
to manage overlapping tasks and interruptions.
Our results show that \react{} (\gptfo{}) achieves 47\% on synchronous tasks but only 11\% on asynchronous tasks, highlighting significant room for improvement. We further analyze failure modes, demonstrating the need for LLM agents to better incorporate long-horizon feedback and self-audit their reasoning during task execution. Code is available \href{https://github.com/portal-cornell/robotouille}{here}.

\end{abstract}