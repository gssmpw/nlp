\vspace{-1em}
\section{Discussion}
\vspace{-1em}

In this paper we propose a new benchmark, \robotouille{}, for stress testing LLM agents on synchronous, asynchronous, and multi-agent settings. We evaluate state-of-the-art LLMs and expose their dominant failure modes are similar across synchronous and asynchronous settings. We perform follow-up studies to bring up performance and uncover the need for improvements in LLM agents that we discuss below. 
% \lipsum[1-3]

\textbf{Feedback Incorporation}
\label{dis:feedback}
% \GG{talk about lost in the middle, RAG, also qualitative example about how planning/reasoning is important and its not all about following rules and paying attention to environment (appendix on example where follow rules but doesn't think ahead}
% The most general method to incorporate long-horizon planning feedback into an LLM agent is to include all interactions in the context history. This is reasonable with models with large context windows or that use near-infinite context attention mechanisms \cite{liu2023ringattentionblockwisetransformers,munkhdalai2024leavecontextbehindefficient}; however, LLMs have been shown to make ineffective use of long-contexts \cite{liu2023lostmiddlelanguagemodels}. An alternative is to retrieve relevant interactions using RAG \cite{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} but this shifts the difficulty to the retrieval process. One promising approach from our explorations in Section~\ref{sec:follow-ups} is using an agent to acquire knowledge to collapse its uncertainty of an environment by summarizing its interactions succinctly into facts which increase it priors. The agent should also reason about future states to avoid myopic behaviors with strong priors as we qualitatively show in Appendix~\ref{app:qualitative-planning}.
A general method to incorporate long-horizon planning feedback in LLM agents is to include all interactions in the context history. This works well for models with large context windows or near-infinite attention mechanisms \citep{liu2023ringattentionblockwisetransformers,munkhdalai2024leavecontextbehindefficient}, but LLMs often struggle with long-contexts \citep{liu2023lostmiddlelanguagemodels}. An alternative is RAG \citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp}, yet this shifts the complexity to retrieval. As explored in Section~\ref{sec:follow-ups}, a promising approach is for the agent to summarize interactions into facts to reduce uncertainty and strengthen priors. It should also reason about future states to avoid myopic behaviors, as shown qualitatively in Appendix~\ref{app:qualitative-planning}. Another underexplored yet effective approach is finetuning LLM agents \citep{chen2023fireactlanguageagentfinetuning} with methods such as TD learning and value propagation \citep{putta2024agentqadvancedreasoning, gehring2024rlefgroundingcodellms}.


\textbf{Self-Verification}
\label{dis:correct}
% \GG{talk about how self-verification sucks but integration with external planners (llm+p, llm-modulo) and finetuning on specialized planning datasets (plansformer, searchformer) can help with this}
An LLM agent should be able to audit but LLMs are unreliable at self-verification \citep{valmeekam2023largelanguagemodelsreally}. Other approaches use LLMs to create a representation for external planners \citep{liu2023llmpempoweringlargelanguage,guan2023leveragingpretrainedlargelanguage} or finetune on planning datasets \citep{pallagani2022plansformergeneratingsymbolicplans,lehnert2024abetterplanningtransformers} but these methods are difficult to debug and lack guarantees respectively. One approach is to combine code-use with language \citep{wang2024executablecodeactionselicit}; reasoning in language and verifying understanding with code and APIs would allow us stronger guarantees that are easier to debug.

\textbf{Real-World Application}
% \GG{talk about limitations and also how we should test with real humans and how robotouille is gonna be a platform for this through multi-agent and games whatever}
To effectively deploy LLM agents on real-world agents, the cost and inference time of LLMs must be brought down to make them affordable and quick. This is especially problematic for long-horizon task planning since cost and inference time increases as context grows. These system must also be evaluated with real humans; one future direction for Robotouille is serving as an online platform to test agents with humans through collaboration.

% \begin{enumerate}
%     \item Limitations
%     \begin{enumerate}
%         \item To prepare LLM agents for home robots, we should evaluate against real humans and the systems should be real-time. One future direction for Robotouille is serving as an online platform to test agents with humans
%         \item It is expensive to use LLMs for long-horizon tasks. Closing the gap between closed-source and open-source models through finetuning or RAG can make inference cheaper.
%     \end{enumerate}
% \end{enumerate}