From the previous experiments, we conclude that LLM agents struggle in the asynchronous dataset due to simple failures that arise in the synchronous dataset. In order to have a better understanding of how to improve LLM agent capabilities on asynchronous planning, we look into asynchronous subtask prioritization and boosting performance.

% \textbf{Question 6.} \textit{Does asynchronous subtask prioritization affect performance?}
\textbf{Finding 6.} Proper asynchronous prioritization boosts performance.

% In addition to observing performance changes, we annotate the asynchronous setting runs with stronger rule priors on Tasks 1 to 3 with whether the asynchronous tasks (like cooking) are done first or later. For the 12 successes, 9 runs prioritize asynchronous tasks and 3 runs do not. For the 18 failures, there was an even split between prioritization. We expect most successes to be prioritizing asynchronous tasks since this aligns with the in-context example provided; however despite the example, there are still instances where asynchronous tasks are not prioritized. Further increasing priors on which tasks to prioritize would easily fix this issue; however, an agent should be capable of reasoning about and auditing whether its trajectory is optimal without excess domain-specific knowledge. We discuss methods for self-correcting agents in Section~\ref{dis:correct}.

% If we assume that the synchronous planning capabilities of LLM agents is perfect, then performance on the asynchronous dataset entirely depends on subtask prioritization. We expect that runs where the agent prioritizes asynchronous subtasks will succeed while those where it doesn't will be suboptimal or be a failure in the worst case. We annotated the subtask prioritization of \gptfo{} \react{} in the initial experiments and found that in 72.7\% of successes, the agent prioritizes asynchronous subtasks while in 52.8\% of failures, the agent doesn't prioritize asynchronous subtasks. The former result aligns closely to what we expect while the latter does not. Since the agent is imperfect, it can correctly prioritize asynchronous subtasks and still encounter failure cases related to synchronous planning. We also expect the agent to prioritize asynchronous subtasks in successful runs even though it is imperfect because the in-context example focuses on this prioritization; however, it is undesirable that some successes do not prioritize asynchronous subtasks despite an example that does so. An agent should be capable of auditing its own reasoning and plan to ensure that it is optimal. We discuss methods for reliable self-verification in Section~\ref{dis:correct}.

Efficient asynchronous planning requires prioritizing subtasks that can be performed asynchronously. We investigate how success rate changes with asynchronous task prioritization to understand the impact of asynchronous planning on the results. Our hypothesis is that prioritizing asynchronous subtasks leads to higher success rates because the planned trajectory is shorter and reaches the goal within the maximum step limit. We find that the success rate conditioned on prioritization is 16\% compared to 6\% without, supporting that prioritization achieves higher success rate. An agent should be capable of auditing its own reasoning and plan to ensure that its prioritization correctly targets asynchronous subtasks. We discuss methods for reliable self-verification in Section~\ref{dis:correct}.

% \textbf{Question 7.} \textit{Would asynchronous performance improve by increasing priors over the transition function?}
\textbf{Finding 7.} Stronger priors improve asynchronous performance.

The dominant failures of \gptfo{} \react{} on the asynchronous dataset were transition failures. We investigate how we can improve performance by increasing the priors over the transition function. We create an augmented method, \reactp{}, that prompts \react{} with more details about the rules of \robotouille{}. 
%See Appendix~\ref{app:prompts} for differences in prompting.


Fig.~\ref{fig:piechart_followup} shows nested pie charts of the failure modes on Tasks 1 to 3 of the asynchronous dataset from the \gptfo{} \react{} experiments in Table~\ref{tab:tasks-results} and from \gptfo{} \reactp{}.

We observe a statistically insignificant change in performance, where the success rate for \gptfo{} \react{} is $0.30 \pm 0.085$ and \gptfo{} \reactp{} is $0.40 \pm 0.050$. We also observe failures relating to violating the 'one item at station' rule decrease from $38.1\%$ for \gptfo{} \react{} (8 failures) to $22.2\%$ for \gptfo{} \reactp{} (4 failures) accounting for a $50\%$ decrease in these transition failures. While this shows that increasing priors over rules decreases transition failures as expected, overall performance did not improve due to other failures that arose. We note that state failures increase from $23.8\%$ for \gptfo{} \react{} (5 failures) to $38.9\%$ for \gptfo{} \reactp{} (7 failures). These failures are due to misunderstandings with the state description provided; specifically, the agent assumes that meat on a stove always implies it is cooked. Augmenting \reactp{} over state priors would presumably improve performance but is impractical because it requires excessive effort from a domain-expert and wouldn't generalize to new domains. We discuss methods for incorporating state feedback in Section~\ref{dis:feedback}.

\input{figures/final/fig_piechart_followup}