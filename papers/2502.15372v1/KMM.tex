\section{Discussing  KMM guarantees}\label{sec:KMMComparison}
\paragraph{Description of the Kernel Mean Matching algorithm}
The KMM algorithm in the realizable scenario assumes that the unknown function $\ff$ is bounded in some RKHS $\cH$, $\|\ff\|_{\cH}\leq M$. If the kernel corresponding to $\cH$ is also bounded, say $K(\xx, \xx) \leq 1$ for all $\xx$. 

In this case, with enough samples we can guarantee that $\|n_{tr}^{-1} \sum_i \Phi_{\xx_i} - \E_{\xx \sim \ptr} \Phi_{\xx}\|_{\cH} \leq \varepsilon/M$ (and similarly for $\tilde{\xx} \sim \pte$). Then by solving a linear program, we can find some weights $\hat{\beta}_i$ s.t. the empirical averages of the reproducing kernel map are close after reweighting, i.e., $\|\sum_i \hat{\beta}_i \Phi_{\xx_i}/n_{tr} - \sum_{i} \Phi_{\tilde{\xx}_i} /n_{te}\|_{\cH} \leq \varepsilon/M$ (these weights exist, because in particular $\beta_i = \pte(\xx_i) / \ptr(\xx_i)$ is a feasible solution). Now it is easy to show that $\hat{\beta}$ can be used as importance weights to solve the covariate-shifted mean estimation:
\begin{align*}
    \sum_i \frac{1}{n} \hat{\beta}_i \ff(\xx_i)&  = \langle \sum_i \frac{1}{n} \hat{\beta}_i \Phi_{\xx_i}, \ff \rangle_{\cH} \\
    & = \langle \E_{\tilde{\xx}\sim \pte} \Phi_{\tilde{\xx}} , \ff \rangle_{\cH} + \langle \Delta, \ff \rangle_{\cH} \\
    & = \E_{\tilde{\xx}\sim \pte} \ff(\xx) \pm \|\Delta\|_{\cH} \|\ff\|_{\cH},
\end{align*}
where $\Delta := \sum_i n^{-1} \hat{\beta}_i \Phi_{\xx_i} - \E_{\tilde{\xx}\sim \pte} \Phi_{\tilde{\xx}}$ satisfies $\|\Delta\|_{\cH} \leq \varepsilon / M$.

As discussed above, collecting $n_{tr} \gtrsim (MB/\varepsilon)^2$ samples from $\ptr$ and $n_{te} \gtrsim (M/\varepsilon)^2$ samples from $\pte$ is enough to guarantee the corresponding closeness of three relevant empirical averages to their population averages. 

\paragraph{Analysis of the plug-in method}
Alternative, naive approach, to solve the covariate shift mean estimation problem in the same setup as above, is to find $\hat{\ff}$ with a sufficiently small distance $L_1$ from $\ff$ with respect to $\ptr$, and use it as a proxy for $\ff$ when computing the averages with respect to $\pte$. 

\begin{lemma}
    When $\ff \in \cH$ is bounded by $\|\ff\| \leq M$, using $O((M/\varepsilon')^2)$ samples $(\xx, \ff(\xx))$ for $\xx\sim\ptr$ we can find a function $\hat{\ff}$ s.t.
    \begin{equation*}
        \E_{\xx \sim \ptr} |\ff(\xx) - \hat{\ff}(\xx)| \leq \varepsilon'.
    \end{equation*}
\end{lemma}
\begin{proof} 
    Consider the family of function $\cF' := \{ \xx \mapsto |\boldsymbol{h}(x)| : \boldsymbol{h} \in \cH, \|\boldsymbol{h}\| \leq 2M \}$. Since $\xx \mapsto |\xx|$ is 1-Lipschitz, and we have a Rademacher complexity bound for a radius $M$-ball in the kernel space~(\Cref{lem:rademacher-kernel}), the Rademacher complexity of $\mathcal{R}_{n}(\cF') \leq O(M/\sqrt{n})$.
    
    Using~\Cref{thm:rademacher-generalization-bound} we obtain a generalization bound: after collecting $n  = O(M/\varepsilon')^2$ samples, with probability $9/10$ we get a uniform bound on the generalization error for every $\boldsymbol{h} \in \cH$ with $\|\boldsymbol{h}\| \leq 2$.  
    \begin{equation*}
        \sup_{\|\boldsymbol{h}\| \leq 2M} \frac{1}{n} \sum_i |\boldsymbol{h}(\xx_i)| = \E_{\xx} |\boldsymbol{h}(\xx)| \pm \varepsilon'/2.
    \end{equation*}
    In particular, taking $\boldsymbol{h}$ of form $\ff - \ff'$ for some $\|\ff'\|_{\cH} \leq M$, we get
        \begin{equation*}
        \sup_{\|\ff'\| \leq M} \frac{1}{n} \sum_i |\ff(\xx_i) - \ff'(\xx_i)| = \E_{\xx} |\ff'(\xx) - \ff(\xx)| \pm \varepsilon'/2.
    \end{equation*}
    It is enough therefore to approximately minimize the empirical loss: given samples $(\xx_i, \ff(\xx_i))$, we need to find 
    \begin{equation*}
        \min_{\|\ff'\|_{\cH} \leq M} \frac{1}{n} \sum_{i} |\ff'(\xx_i) - \ff(\xx_i)|.
    \end{equation*}
    By the Representer Theorem~(\Cref{thm:representer}), this minimum is realized by some $\ff'$ of form $\sum_{i} \Phi_{\xx_i} \lambda_i$, and the problem becomes just a minimization
    \begin{equation*}
        \min_{\lambda \in \bR^n} \| K \lambda - y\|_1,
    \end{equation*}
    where $K$ is a kernel matrix $K_{ij} = K(\xx_i, \xx_j)$, and $y$ is a vector of observations $y_i = \ff(\xx_i)$. This minimization is just a linear program and can be solved efficiently.
\end{proof}

Note that if we now find $\hat{\ff}$ with $L_1$ error $\varepsilon':= \varepsilon/B$ with respect to $\ptr$, since the density ratio $\pte/\ptr$ is bounded by $B$ we can also bound the $L_1$ error with respect to $\pte$. This clearly implies closeness of the expectations
\begin{equation*}
    \E_{\xx \sim \pte} \ff(\xx) - \E_{\xx \sim \pte} \hat{\ff}(\xx) \leq \E_{\xx \sim \pte} |\ff(\xx) - \hat{\ff}(\xx)| \leq B \E_{\xx \sim \ptr} |\ff(\xx) - \hat{\ff}(\xx)| \leq \varepsilon.
\end{equation*}

We can now use $(M/\varepsilon)^2$ samples from $\pte$ to estimate $\E_{\xx \sim \pte} \hat{\ff}(\xx)$ by a sample average. This leads to the final sample complexity $n_{tr} \gtrsim (MB/\varepsilon)^2$ samples from $\ptr$ and $n_{te} \gtrsim (M/\varepsilon)^2$ samples from $\pte$, matching the guarantees of the KMM method.
