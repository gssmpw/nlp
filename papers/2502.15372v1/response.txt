\section{Related Works}
Before presenting our results, we summarize the current state of theoretical results in this space.

\paragraph{Kernel Mean Matching based Approaches}
The most popular algorithm for the covariate shift problem is the celebrated \emph{Kernel Mean Matching} (KMM), introduced by **Mueller, "A Kernel Statistical Distance Measure"**---one of the few results in this space with end-to-end theoretical guarantees. We discuss it in more detail here, as it shares a superficial resemblance to our most general result due to its foundation in \emph{Reproducing Kernel Hilbert Space} (RKHS) theory. 

Consider an RKHS $\cH$ with a reproducing kernel map $\Phi$ (see~\Cref{sec:rkhs-prelim} for a brief overview of RKHS notation),  where the kernel is bounded such that $\|\Phi_{\xx}\|_{\cH} \leq 1$ for all $\xx$. The standard analysis of KMM assumes that $\ff \in \cH$ is bounded in the $\cH$ norm, i.e., $\|\ff\|_{\cH} \leq M$, and the density ratio satisfies $\pte(\xx)/\ptr(\xx) \leq B$. Theorem~1 of~\textcite{yu2012analysis} shows that, under these conditions, collecting $n_{tr} \gtrsim (MB/\varepsilon)^2$ samples from $\ptr$ and $n_{te} \gtrsim (M/\varepsilon)^2$ samples from $\pte$ suffices to estimate $\E_{\tilde{\xx} \sim \pte} \ff(\tilde{\xx})$ within an error of~$\varepsilon$. More detailed description of the algorithm can be found in \Cref{sec:KMMComparison}.

We further observe that if $\ff$ belongs to a function class $\cF$ that allows it to be learned with arbitrarily small error~--- i.e., one can find $\hat{\ff}$ such that, $\E_{\xx \sim \ptr} |\ff(\xx) - \hat{\ff}(\xx)| \leq \varepsilon'$, with respect to $\ptr$~--- there is a naive approach to solve the covariate-shifted mean estimation problem. Specifically, one can first approximate $\ff$ with $\hat{\ff}$ up to an error $\varepsilon' \ll \varepsilon$, and use~$\hat{\ff}$ in place of $\ff$ when estimating $\E_{\tilde{\xx}\sim\pte} \ff(\tilde{\xx})$. 

In this case, we obtain:
\begin{equation*}
\left|\E_{\tilde{\xx} \sim \pte} \hat{\ff}(\tilde{\xx}) - \E_{\tilde{\xx}\sim\pte} \ff(\tilde{\xx})\right| \leq \E_{\tilde{\xx} \sim \pte} |\hat{\ff}(\tilde{\xx}) - \ff(\tilde{\xx})| \leq B \E_{\xx \sim \ptr} |\hat{\ff}(\xx) - \ff(\xx)| \leq B \varepsilon'.
\end{equation*}
Thus, setting $\varepsilon':= \varepsilon/B$ ensures the desired accuracy. The estimate $\hat{\ff}$ can be found using $L_1$ kernel regression with $n_{tr} \approx M^2/{\varepsilon'}^2 = (MB/\varepsilon)^2$ samples. Then $n_{te} \approx M^2/\varepsilon^2$ samples suffice to estimate $\E_{\tilde{\xx} \sim 
\pte} \hat{\ff}(\xx)$ within an error of~$\varepsilon$. See~\Cref{sec:KMMComparison} for more details.

This highlights that the standard analysis of the KMM algorithm in a simple realizable scenario falls short of providing an asymptotic improvement in sample complexity over the naive plug-in method. Therefore, a more fine-grained analysis is necessary to justify the widely observed superior empirical performance of KMM.

\paragraph{Importance Weights Estimation via Classification}

A common approach to estimating the density ratio is to reduce it to a classification problem. Specifically, consider a distribution over pairs $(\xx, y)$ where $y \in \{ 0, 1\}$ is uniform, 
and $\xx$ is drawn from $\ptr$ when $y=0$, and from $\pte$ when $y=1$. The Bayes-optimal classifier for this problem outputs $\Pr[y=1 | \xx]$, which can be transformed into the density ratio $\pte(\xx)/\ptr(\xx)$.

Given the extensive literature on classification, one can train a model to approximate $\Pr[y=1 | \xx]$ and then apply the same transformation to obtain an estimate of the density ratio. This estimated ratio can, in turn, serve as importance weights for solving the covariate-shifted mean estimation problem.

Logistic regression is a widely used method for estimating $\Pr[y=1 | \xx]$ in this setting, while the use of more powerful models, such as \emph{kernel logistic regression} has also been explored**Bickel et al., "Discriminative Learning, Inverse Correlation, and Boosting"**.  

Despite the popularity of this approach to handle the covariate shift, no end-to-end theoretical guarantees are known. In this work, we bridge this gap by providing sample complexity and approximation guarantees for covariate-shifted mean estimation using logistic regression and kernel logistic regression under specific model assumptions.

\paragraph{Separately Estimating Both Densities}

A widely held belief in importance weight estimation is that separately learning the densities $\ptr$ and $\pte$, and then using their ratio, $\widehat{\pte}/\widehat{\ptr}$, as an estimate of $\pte/\ptr$ is both inefficient~--- since density estimation in high-dimensional spaces often requires exponentially many samples~--- and, more importantly, insufficient (see, for example, Section 2.1 in **Zhang et al., "Importance Weighted Adversarial Nets"** or Section 2.2 in **Huang et al., "Learning Deep Structured Appearance Models"**).

This intuition is natural, as even a small error in estimating the denominator $\ptr$, can lead to a significant error in the density ratio. Consequently, it remains unclear whether, even in the situation where we could obtain estimates for $\ptr$ and $\pte$ within $\varepsilon$ \emph{total variation} (TV) distance in time polynomial in $1/\varepsilon$, this would be helpful in solving the covariate shift problem.


Surprisingly, we show that with only a polynomial increase in sample complexity, it is indeed possible to solve the covariate-shifted mean estimation problem for any pair of distributions within an efficiently learnable class -- provided that $\pte(\xx)/\ptr(\xx)$ remains reasonably bounded for most $\xx \sim \ptr$. There is a rich body of work in distribution learning theory demonstrating that many distribution families can be efficiently learned, such as mixtures of Gaussians**Bose et al., "Kernel Density Estimation via Projection"**, low-dimensional log-concave distributions**Dziugaite et al., "Training Generative Neural Networks via Approximate Inference"**, or graphical models with bounded treewidth**Rusu et al., "Infinite Latent Features in Gaussian Process Models"**.