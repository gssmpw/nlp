\section{Covariate Shift for Gaussian Distributions \label{sec:gaussians-optimal}}

In this section, we present our results for Gaussian distributions. We prove that it is sufficient to learn the means and variances of the training and test distributions to within an $\varepsilon$ error in order to solve the covariate-shifted mean estimation problem with $\varepsilon$ accuracy. In particular, we prove,
\begin{theorem}\label{thm:CVGauss}
Let $\ptr = \cN(\mtr,\str)$ and $\pte = \cN(\mte,\ste)$, where $\|\mtr-\mte\|_2\leq O(1)$ and $\|\str^{-1}-\ste^{-1}\|_F \leq O(1)$. Also, let $\|\mtr\|_2\leq O(1), \|\str^{-1}\|_{op}\leq O(1)$. For any $\ff$ such that $\sup_{\xx\in \mathbb{R}^d}|\ff(\xx)|\leq 1$, Algorithm~\ref{alg:CVGauss} returns $Z$ such that with probability at least $1-\delta$,
\begin{equation}\label{eq:ResGauss}
    |Z-\E_{\xx\in \pte}\ff(\xx)|\leq \varepsilon.
\end{equation}


Furthermore, Algorithm~\ref{alg:CVGauss} requires sampling at most $O\left(\frac{d^2}{\varepsilon^2 }\log\frac{1}{\delta}\right)$ samples of $(\xx_i,\ff(\xx_i)), \xx_i \sim \ptr$ and $\widetilde{\xx_i}\sim\pte$. 
\end{theorem}

For isotropic Gaussians, we can prove a tighter bound on the sample complexity.
\begin{theorem}\label{thm:CVGaussIso}
Let $\ptr = \cN(\mtr,\II)$ and $\pte = \cN(\mte,\II)$, where $\|\mtr-\mte\|_2\leq O(1)$. For any $\ff$ such that $\sup_{\xx\in \mathbb{R}^d}|\ff(\xx)|\leq 1$, Algorithm~\ref{alg:CVGaussIso} returns $Z$ such that with probability at least $1-\delta$,
\begin{equation}\label{eq:ResGaussIso}
    |Z-\E_{\xx\in \pte}\ff(\xx)|\leq \varepsilon.
\end{equation}


Furthermore, Algorithm~\ref{alg:CVGaussIso} requires sampling at most $O\left(\frac{d}{\varepsilon^2 }\log\frac{1}{\delta}\right)$ samples of $(\xx_i,\ff(\xx_i)), \xx_i \sim \ptr$ and $\widetilde{\xx_i}\sim\pte$.
\end{theorem}

\begin{algorithm}
\caption{Covariate shift for General Gaussian $\pte$ and $\ptr$}
 \label{alg:CVGauss}
  \begin{algorithmic}[1]
\STATE {\bfseries Input:} $d,\varepsilon,\delta$
 \STATE $K = O\left(\frac{d^2}{\varepsilon^2}\log \frac{1}{\delta}\right)$
\STATE Generate $K$ samples $\xx_i$ from $\ptr$ and $K$ samples $\widetilde{\xx}_i$ from $\pte$.
 \STATE Compute $\widehat{\mtr} = \frac{2}{K}\sum_{i\in [K/2]} \xx_i$
 \STATE Compute $\widehat{\mte} = \frac{2}{K}\sum_{i\in [K/2]} \widetilde{\xx}_i$
 \STATE Compute $\widehat{\str} = \frac{1}{K}\sum_{i\in [K/2]}(\xx_{2i}-\xx_{2i-1})(\xx_{2i}-\xx_{2i-1})^{\top}$
 \STATE Compute $\widehat{\ste} = \frac{1}{K}\sum_{i\in [K/2]}(\widetilde{\xx}_{2i}-\widetilde{\xx}_{2i-1})(\widetilde{\xx}_{2i}-\widetilde{\xx}_{2i-1})^{\top}$
\STATE $\widehat{\ptr} = \cN(\widehat{\mtr},\widehat{\str})$, $\widehat{\pte} = \cN(\widehat{\mte},\widehat{\ste})$
\FOR{$t = 1,\cdots,O(\log(1/\delta))$}
\STATE Generate $m= O\left(\varepsilon^{-2}\right)$ samples $(\yy_i,\ff(\yy_i)),$ with $\yy_i \sim \ptr.$
 \FOR{$i\in [m]$}
 \STATE $Z_i = \frac{\widehat{\pte}(\yy_i)}{\widehat{\ptr}(\yy_i)}\ff(\yy_i)$
 \ENDFOR
 \STATE $\overline{Z}_t = \frac{1}{m}\sum_{i=1}^m Z_i$
 \ENDFOR
 \STATE  {\bfseries Return:} Median of $\overline{Z}_t$'s
 \end{algorithmic}
 \end{algorithm}

 \begin{algorithm}
\caption{Covariate shift for Isotropic Gaussian $\pte$ and $\ptr$}
 \label{alg:CVGaussIso}
  \begin{algorithmic}[1]
\STATE {\bfseries Input:} $d,\varepsilon,\delta$
 \STATE $K = O\left(\frac{d}{\varepsilon^2}\log\frac{1}{\delta}\right)$
\STATE Generate $K$ samples $\xx_i$ from $\ptr$ and $K$ samples $\widetilde{\xx}_i$ from $\pte$.
 \STATE Compute $\widehat{\mtr} = \frac{2}{K}\sum_{i\in [K/2]} \xx_i$
\STATE Compute $\widehat{\mte} = \frac{2}{K}\sum_{i\in [K/2]} \widetilde{\xx}_i$
\STATE $\widehat{\ptr} = \cN(\widehat{\mtr},\II)$, $\widehat{\pte} = \cN(\widehat{\mte},\II)$
\FOR{$t = 1,\cdots,O(\log(1/\delta))$}
\STATE Generate $m= O\left(\varepsilon^{-2}\right)$ samples $(\yy_i,\ff(\yy_i)),$ with $\yy_i \sim \ptr.$
 \FOR{$i\in [m]$}
 \STATE $Z_i = \frac{\widehat{\pte}(\yy_i)}{\widehat{\ptr}(\yy_i)}\ff(\yy_i)$
 \ENDFOR
 \STATE $\overline{Z}_t = \frac{1}{m}\sum_{i=1}^m Z_i$
 \ENDFOR
 \STATE  {\bfseries Return:} Median of $\overline{Z}_t$'s
 \end{algorithmic}
 \end{algorithm}

 Our analysis proves that the estimator $X = \frac{\widehat{\pte}}{\widehat{\ptr}}\ff$ is a good estimator. Note that this estimator is biased. We divide the analysis into two parts: in the first part, we show that our estimator has a small bias, and in the second part, we provide a bound on its variance. Finally, we apply concentration bounds to establish the total sample complexity. 

 For the first part, we prove the following result. The proofs of these lemmas are deferred to Appendix~\ref{sec:ProofsGauss}.

 \begin{restatable}[Bound for Isotropic Gaussian Distributions]{lemma}{IsoGaussBias}
\label{lem:isotropic-gaussians-result}
Let $\ptr = \cN(\mtr,\II)$ and $\pte = \cN(\mte,\II)$ such that $\|\mtr\|_2\leq O(1)$ and $\|\mtr-\mte\|_2\leq B\leq O(1)$. Further, let $\widehat{\ptr}$ and $\widehat{\pte}$ be as defined in Algorithm~\ref{alg:CVGauss}. Then for any function $\ff$ such that $\sum_{\xx\in \mathbb{R}^d}|\ff(\xx)|\leq 1$,
\[
\left|\E_{\xx\sim \ptr} \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)} \ff(\xx) - \E_{\xx \sim \pte} \ff(\xx)\right| \leq O(\varepsilon).
\]
\end{restatable}

\begin{restatable}[Bound for General Gaussian Distributions]{lemma}{GaussBias}
\label{lem:Q_gauss_non_iso}
    Let $\pte := \cN(\mte, \ste), \ptr := \cN(\mtr, \str)$ and $\widehat{\ptr} = \cN(\widehat{\mtr} , \widehat{\str}),\widehat{\pte} = \cN(\widehat{\mte} , \widehat{\ste})$ be as defined in Algorithm~\ref{alg:CVGauss}. If $\|\mtr-\mte\|_2 \leq B\leq O(1)$, $\|\str^{-1}\|_{op}\leq O(1)$ then, for any function $\ff$, such that $\sup_{\xx\in \mathbb{R}^d}|\ff(\xx)|\leq 1$,
    \[
\left|\E_{\xx\sim \ptr} \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)} \ff(\xx) - \E_{\xx \sim \pte} \ff(\xx)\right| \leq O(\varepsilon).
\]
\end{restatable}

In the next part, we show that the variance of our estimator is small. The proof is again deferred to Appendix~\ref{sec:ProofsGauss}.

\begin{restatable}[Variance of our Estimator]{lemma}{VarianceGauss}\label{lem:Var}
     If $\|\str^{-1}\|_{op}, \|\ste^{-1}\|_{op} \leq O(1)$, $\|\str^{-1}-\ste^{-1}\|_F\leq O(1)$, and $\|\mtr\|_2, \|\mtr-\mte\|_2\leq O(1)$, then, the variance of our estimator can be bounded as,
    \[
    \E_{\xx\sim \ptr} \left[\left(\frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)}\ff(\xx) - \E_{\xx\sim\pte}\ff(\xx)\right)^2\right] \leq O(1).
    \]
\end{restatable}

Having established that our estimator has a small bias, i.e., $O(\epsilon)$, and a small variance, $O(1)$, the result follows directly from standard Chebyshev's inequality. 

For the high-probability statement, we apply the standard technique: given an estimator $Z$ that lies within the desired interval $I$ with probability $2/3$, we can take the median of $O(\log(1/\delta))$ independent realizations of this estimator to reduce the failure probability to $\delta$, using the Chernoff bound.

\subsection*{Proof of Theorems~\ref{thm:CVGauss} and~\ref{thm:CVGaussIso}}
\begin{proof}[Proof of Theorem~\ref{thm:CVGauss}]

From Lemmas~\ref{lem:MeanGauss} and \ref{lem:CovGauss}, it is sufficient to use $O(d^2/\varepsilon^2 \log1/\delta)$ samples to obtain $\widehat{\ptr}$ and $\widehat{\pte}$ to the required accuracy.

Now, from Lemma~\ref{lem:Q_gauss_non_iso} our estimator $X = \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)}\ff(\xx)$ for $\xx \sim \ptr$ satisfies 
\begin{equation}\label{eq:bias}
    |\E_{\xx\sim \ptr} X - \E_{\xx\sim \pte}\ff(\xx)| \leq \varepsilon.
\end{equation}


Further, from Lemma~\ref{lem:Var}, 
\[
\Var(X) = \E_{\xx\sim \ptr} \left[\left(\frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)}\ff(\xx) - \E_{\xx\sim\pte}\ff(\xx)\right)^2\right] \leq O(1).
\]

We now use Chebyshev's inequality to obtain our sample complexity bounds. Since the variance of the estimator is at most $O(1)$ the variance of $Z =\frac{1}{m}\sum_i \frac{\widehat{\pte}}{\widehat{\ptr}}\ff(\xx_i)$ is at most $O(1/m)$. Now from Chebyshev's inequality and Eq.~\eqref{eq:bias}, for $m\geq O(1/\epsilon^2)$,
\[
\Pr\left[\left|\frac{1}{m}\sum_i \frac{\widehat{\pte}}{\widehat{\ptr}}\ff(\xx_i)-\E_{\xx\sim \pte}\ff(\xx)\right|\geq \epsilon\right] \leq \frac{Var(Z)}{\epsilon^2}\leq 0.1.
\]

We can now use the standard median trick to boost the probability, i.e., repeat the sampling process of sampling $m$ points $O(\log (1/\delta))$ times. Let $\overline{X}$ denote the average of the $m$ samples above. Now, the median trick says generate $\overline{X}_i$ for $i =1,\cdots, O(\log(1/\delta))$. The median $Y$ of these $\overline{X}_i$'s satisfies with probability at least $1-\delta$,
\[
|Y - \E_{\xx\sim\pte}\ff(\xx)|\leq \epsilon
\]
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:CVGaussIso}]
From Lemma~\ref{lem:MeanGauss} it is sufficient to use $O(d/\epsilon^2\log 1/\delta)$ samples to obtain $\widehat{\ptr}$ and $\widehat{\pte}$ to the required accuracy.

Now, from Lemma~\ref{lem:isotropic-gaussians-result} our estimator $X = \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)}\ff(\xx)$ for $\xx \sim \ptr$ satisfies 
\begin{equation*}
    |\E_{\xx\sim \ptr} X - \E_{\xx\sim \pte}\ff(\xx)| \leq \varepsilon.
\end{equation*}
The remaining proof follows the same as in the proof of Theorem~\ref{thm:CVGauss}.
\end{proof}


\section{TV Learnability implies Covariate Shift}\label{sec:TV}

In this section, we extend our algorithms to more general distributions. We show that, under mild conditions on the training and test distributions, it is sufficient to learn both distributions to a small total variation distance. In particular, we prove the following:

\begin{theorem}
\label{thm:shift-TV}
    Let $\ff$ be a bounded function, $\ptr, \pte$ be a pair of distributions, such that for $B>0$, $\Pr_{\xx \sim \pte}(\pte(\xx) / \ptr(\xx) > B/4) \leq \varepsilon$. Let $\widehat{r}$, be s.t. $\widehat{r} = \widehat{\pte}/\widehat{\ptr}$ for some $\widehat{\pte}$, and $\widehat{\ptr}$ satisfying $d_{TV}(\pte, \widehat{\pte}) \leq \varepsilon$ and $d_{TV}(\ptr, \widehat{\ptr}) \leq \varepsilon / B$. Then Algorithm~\ref{alg:CV} returns $Z$ such that, with probability at least $0.9$, we get
    \begin{equation*}
        |Z -\E_{\xx\sim \pte} \ff(\xx)| \leq O(\varepsilon).
    \end{equation*}
    Furthermore, Algorithm~\ref{alg:CV} uses only $O(B^2 / \varepsilon^2)$ samples $(\xx_i, \ff(\xx_i))$, $\xx_i\sim \ptr$.
\end{theorem}

 \begin{algorithm}
 \caption{Covariate shift for any $\pte$ and $\ptr$ with bounded tails}
 \label{alg:CV}
 \begin{algorithmic}[1]
 \STATE {\bfseries Input:} $\widehat{r} =\widehat{\pte}/\widehat{\ptr},B,\varepsilon$
 \STATE $K = O(B^2/\varepsilon^2)$
 \STATE Generate $K$ samples $(\xx_i,\ff(\xx_i))$ from $\ptr$.
 \IF{$\widehat{r}(\xx_i)\leq B$} 
 \STATE $Z_i = \widehat{r}(\xx_i)\ff(\xx_i)$
\ELSE
\STATE $Z_i = 0$
 \ENDIF
 \STATE {\bfseries Return:} $Z = \frac{1}{K}\sum_{i=1}^K Z_i$
 \end{algorithmic}
 \end{algorithm}
In the next section, we will show how to compute the density ratio $\widehat{r}$ used in Algorithm~\ref{alg:CV}, satisfying the conditions of the above theorem, for certain classes of probability distributions, without explicitly estimating $\ptr$ and $\pte$.

\begin{remark}
    \label{rem:lower-bound}
    This sample complexity upper bound should be contrasted with the following simple lower bound: let $B$ be such that $\Pr_{\xx\sim \pte}(\pte(\xx)/ \ptr(\xx)> B) = 2 \varepsilon$. Even with exact knowledge of $\pte$ and $\ptr$, we require $\Omega(B/\varepsilon)$ samples to solve the covariate-shifted mean estimation problem up to error $\varepsilon$. A simple proof of this lower bound can be found in~\Cref{sec:ProofsTV}.
\end{remark}
A relatively common assumption in the covariate shift literature, which is stronger than the one we use, is that the density ratio $\pte(\xx)/\ptr(\xx)$ is bounded everywhere by $B$. We chose to relax this assumption because it is not even satisfied for two non-equal univariate Gaussian distributions.

To put the sample complexity in~\Cref{thm:shift-TV} into context, note that we could apply it when $\ptr$ and $\pte$ are isotropic Gaussian distributions, as in~\Cref{thm:CVGaussIso}, with $|\mu_1 - \mu_2| \leq O(1)$. In this case, for the desired accuracy $\varepsilon$, we would need to set $B := (1/\varepsilon)^{1 + o(1)}$ and learn an isotropic Gaussian $\ptr$ up to an error of $(1/\varepsilon)^{2 + o(1)}$ in total variation distance. This can be done using $O(d/\varepsilon^{4 + o(1)})$ samples. While the dependency on the dimension $d$ matches that of~\Cref{thm:CVGauss}, the dependency on the accuracy $\varepsilon$ is polynomially worse.

The following technical lemma will be crucial in our proof.

\begin{restatable}{lemma}{ApproximationBiasTV}
\label{lem:approximation_2}
Let $\ptr, \pte$ be a pair of distributions, and $S \subseteq U$ be such that $\Pr_{\xx\sim \pte}(S) \geq 1-\varepsilon$ and $\Pr_{\xx\sim \ptr}(S) \geq 1-\varepsilon$. Let $\widehat{\ptr}$ and $\widehat{\pte}$ be a pair of estimations of $\ptr,\pte$ respectively with $\supp (\widehat{\pte}) \subseteq \supp (\widehat{\ptr}) \subseteq S$. Moreover, assume that $\forall \xx\in S$, $\widehat{\pte}(\xx) / \widehat{\ptr}(\xx) \leq B$. Then
\begin{equation*}
    \left|\E_{\xx\sim \ptr} \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)} \ff(\xx) - \E_{\xx\sim \pte} \ff(\xx)\right|  \leq B d_{TV}(\ptr, \widehat{\ptr}) + d_{TV}(\pte, \widehat{\pte}).
\end{equation*}
\end{restatable}
The proof of the above is in Appendix~\ref{sec:ProofsTV}. We now prove the main result of the section.
\begin{proof}[Proof of \Cref{thm:shift-TV}]
    Let $S := \{ \xx : \widehat{\pte}(\xx) / \widehat{\ptr}(\xx) \leq B \}$, and $S' := \{\xx : \pte(\xx) / \ptr(\xx) \leq B/4\}$. Moreover, define sets $A_{\mathrm{te}} := \{ \xx : \widehat{\pte}(\xx) \leq 2 \pte(\xx) \}$, and $A_{\mathrm{tr}} := \{ \xx : \ptr(\xx) \leq 2 \ptr(\xx)\}$.

    We would first prove that $\pte(S) \geq 1 - O(\varepsilon)$ and $\ptr(S) \geq 1 - O(\varepsilon)$. Note that since $\frac{\widehat{\pte}}{\widehat{\ptr}} =  \frac{\pte}{\ptr}\cdot \frac{\widehat{\pte}}{\pte} \cdot \frac{\ptr}{\widehat{\ptr}}$, $A_{\mathrm{te}} \cap A_{\mathrm{tr}} \cap S' \subseteq S$. Let $U$ denote the entire space. We have
    \begin{itemize}
        \item $\pte(U \setminus A_{\mathrm{te}}) \leq \widehat{\pte}(U \setminus A_{\mathrm{te}}) + \varepsilon \leq O(\varepsilon)$,
        \item $\pte(S' \setminus A_{\mathrm{tr}}) \leq \frac{B}{4} \cdot \ptr(S' \setminus A_{\mathrm{tr}}) \leq \frac{B}{4}\cdot \ptr(U \setminus A_{\mathrm{tr}}) \leq O(\varepsilon)$, 
        \item $\pte(S') \leq \varepsilon$ by assumption.
    \end{itemize}
    This yields that $\pte(S) \geq \pte(A_{\mathrm{te}} \cap A_{\mathrm{tr}} \cap S') \geq 1 - O(\varepsilon)$ and therefore $\widehat{\pte}(S) \geq 1 - O(\varepsilon)$. Furthermore, from the definition of $S$ we also have $\widehat{\ptr}(S^c) < \widehat{\pte}(S^c)/B \leq(\pte(S^c) + \varepsilon)/B \leq O(\varepsilon/B)$. Therefore, we also have, ${\ptr}(S)\geq 1-O(\varepsilon)$.

Now, consider the conditional distributions $\widehat{\pte}|_S$ and $\widehat{\ptr}|_S$, which are defined as, $p|_S(\xx) = p(\xx)$, if $\xx\in S$ and $0$ if $\xx\notin S$. We get the desired bound on the density ratio on the set $S$.
    \begin{equation*}
        \frac{\widehat{\pte}|_S(\xx)}{\widehat{\ptr}|_S(\xx)} \leq  B 
    \end{equation*}

    
    Note that conditioning on $S$ doesn't introduce too much error, i.e., if $d_{TV}(\pte, \widehat{\pte}) \leq \varepsilon$ and $\pte(S) \geq 1-\varepsilon$ then $d_{TV}(\pte, \widehat{\pte}|_S) \leq O(\varepsilon)$ by triangle inequality. Similarly, $d_{TV}(\widehat{\ptr}|_S, \ptr) \leq O(\varepsilon/B)$ and we can apply the lemma~\Cref{lem:approximation_2} to deduce that,
    the bias is at most $\varepsilon$, i.e.,
     \[
        \left|\E_{\xx\sim \ptr} \frac{\widehat{\pte}(\xx)}{\widehat{\ptr}(\xx)} \ff(\xx) - \E_{\xx\sim \pte} \ff(\xx)\right| \leq O(\varepsilon).
\]
    
    Finally, the random variable $Z :=\ff(\xx)\mathbf{1}_S(\xx) \frac{\widehat{\pte}|_S(\xx)}{\widehat{\ptr}|_S(\xx)}$ for $\xx \sim \pte$ is always bounded by $B$, hence $\Var(Z) \leq B$. The final bound now follows directly from the application of the Chebyshev inequality. Further, we can boost the probability using the standard median trick.
    
This concludes the proof of Theorem~\ref{thm:shift-TV}.
\end{proof}


\section{Learning Density Ratio by Classification}\label{sec:logistic}

In this section, we present an algorithm based on logistic regression that approximates the density ratio $\widehat{r}$ such that $\widehat{r} = \widehat{\pte}/\widehat{\ptr}$ for some pair of distributions, with the guarantees that $d_{TV}(\widehat{\ptr}, \ptr) \leq \varepsilon$ and $d_{TV}(\widehat{\pte}, \pte) \leq \varepsilon$. This holds when both $\pte$ and $\ptr$ belong to the same exponential family (Definition~\ref{def:exp}), i.e., have the same values of the functions $\hh$ and $\boldsymbol{T}$. Moreover, the probabilities $\widehat{\ptr}$ and $\widehat{\pte}$ provide guarantees for the density ratio, i.e., $\widehat{r} = \frac{\widehat{\pte}}{\widehat{\ptr}}$ is a good approximation to $r = \frac{\pte}{\ptr}$, making it suitable for use as importance weights in the covariate shift problem (after appropriate truncation). Although logistic regression has been applied in practice for estimating density ratios in several previous works, such theoretical guarantees remained unknown. We prove the following:

\begin{theorem}
\label{thm:logistic-regression-result}
    Assume that $\ptr$ and $\pte$ belong to the same exponential family, with $\pte(\xx) = p(\xx | \tte)$ and $\ptr(\xx) = p(\xx | \ttr)$. Let $\str = \E_{\xx \sim \ptr} \boldsymbol{T}(\xx) \boldsymbol{T}(\xx)^{T}$ and similarly for $\ste$. Define $B_{1} := \Tr (\str + \ste)$ and assume, in addition, that $\|\ttr - \tte\|_2 + |A(\ttr) - A(\tte)| \leq B_{2}$.
    
    As long as $R_2(\ptr || \pte) \leq B_3$, we can solve the covariate-shifted mean eastimation problem using $O(B_1 B_2^2 B_3^4 \varepsilon^{-8})$ samples.
\end{theorem}

We now present the high-level idea of our approach. The details and proof of \Cref{thm:logistic-regression-result} is in Appendix~\ref{sec:ProofsLogistic}.

\paragraph{General scheme.}
Consider a distribution $\mathcal{D}_0$ over pairs $(\xx,y)$ where $\xx \in U$ and $y \in \{ \pm 1 \}$, s.t. $\Pr(y = 1) = 1/2$ and $\Pr(\xx | y=1) = \pte(\xx)$ and $\Pr(\xx | y=-1) = \ptr(\xx)$. Then by Bayes rule
\begin{align*}
    \Pr(y = -1 | \xx) &= \frac{\Pr(\xx | y=-1)}{\Pr(\xx|y=1) + \Pr(\xx|y=-1)} \\
    & = \frac{\ptr(\xx)}{\pte(\xx) + \ptr(\xx)} \\
    &= \frac{1}{1 + \pte(\xx)/\ptr(\xx)}.
\end{align*}

Hence, if we can find a Bayes optimal classifier for the distribution $(\xx,y)$ calculating $\Pr(y=-1 | \xx)$, we could invert it to get a density ratio:
\begin{equation}
\label{eq:classification-dr}
    \frac{\pte(\xx)}{\ptr(\xx)} = \frac{1}{\Pr(y=-1 | \xx)} - 1.
\end{equation}

A known method (see for instance \cite{BS06}) of estimating the ratio $\pte(\xx)/\ptr(\xx)$ is then training a classifier for the binary classification problem (to predict $y$ given features $\xx$ from distribution $\mathcal{D}_0$), and using its output in place of $\Pr(y=-1 | \xx)$ in \Cref{eq:classification-dr}.

We show that if we use the negative log-likelihood as a loss function for classification training (also known as the cross-entropy loss), to any classifier $\hat{r}(\xx)$ we have found, we can associate a pair of distributions $\widehat{\pte}$ and $\widehat{\ptr}$, s.t. $\widehat{\pte}/\widehat{\ptr} = \frac{1}{\hat{r}(x)} - 1$, and the regret of the classifier $\hat{r}$ (difference between the population loss of the classifier, and the population loss of the Bayes optimal classifier $\Pr_{\cD_0}[y=-1 | x]$) is an upper bound for the sum of KL-divergences $D_{KL}(\pte || \widehat{\pte}) + D_{KL}(\ptr || \widehat{\ptr})$; see~\Cref{lem:population-regret-is-kl}.

In particular, whenever we can guarantee that this regret is low, we know that the estimated density ratio corresponds to the density ratio of a pair of distributions $\widehat{\ptr}, \widehat{\pte}$ that are both KL-close to the actual distributions $\ptr, \pte$. Using the Pinskers inequality, this can be translated into TV-distance guarantees, and via~\Cref{thm:shift-TV}, after appropriate truncation, such a density ratio can be used as importance weight for the covariate shift problem.

Interestingly, this method never explicitly recovers the densities $\widehat{\pte}$ and $\widehat{\ptr}$ which are shown to exist~--- it only recovers the density ratio.

\paragraph{Using logistic regression.}

When the distributions $\pte$ and $\ptr$ are from the same exponential family, using logistic regression to find the classifier provides the right density ratio estimate. In particular, for parameters $\tte$ and $\ttr$, and functions $\hh,\boldsymbol{T}$, $\pte = \hh(\xx) \cdot \exp\left(\tte^{\top}\boldsymbol{T}(\xx)-A(\tte)\right)$, and $\ptr = \hh(\xx) \cdot \exp\left(\ttr^{\top}\boldsymbol{T}(\xx)-A(\ttr)\right)$.

Now, for these distributions, 
\begin{align*}
    & \Pr_{(\xx,y) \sim \mathcal{D}_0}[y=-1 | \xx]  = \frac{1}{1 + \pte(\xx)/\ptr(\xx)}\\
    & = \frac{1}{1 + \exp\left(\boldsymbol{T}(\xx)^{\top}(\tte-\ttr) + A(\tte)-A(\ttr)\right)}
\end{align*}
The logistic model is given by a joint distribution $(\xx, y)$, parameterized by $\boldsymbol{\theta}^* \in \bR^d$ and $s^* \in \bR$, where
\begin{equation}
    \Pr[y = -1 | \xx] = \frac{1}{1 + \exp(\langle \boldsymbol{\theta}^*, \xx\rangle + s^*)}.
    \label{eq:logistic}
\end{equation}
Hence, the distribution $\mathcal{D}_1$ of $(\boldsymbol{T}(\xx), y)$ for $(\xx, y) \sim \mathcal{D}_0$ indeed follows the logistic model, Eq.\eqref{eq:logistic} with parameters $\ttheta^* = \tte - \ttr$ and $s^* = A(\tte) - A(\ttr)$.
Note that for the distribution following the logistic model we have
\begin{align*}
    \Pr[y=1 | \xx] &= \frac{1}{1 + \exp(-\langle \boldsymbol{\theta}, \xx \rangle - s)},
\end{align*}
and more succinctly
\begin{align*}
    \Pr[y | \xx] & = (1 + \exp(-y(\langle \boldsymbol{\theta}, \xx \rangle +s)))^{-1}.
\end{align*}


Since the empirical minimization of negative log-likelihood for logistic regression is a convex problem, we can efficiently find a minimizer on a finite sample. Using a Rademacher complexity bound for the associated class of loss functions, the minimizer on a finite sample will be an approximate minimizer with respect to the population loss. Concretely we can find $(\hat{\ttheta}, \hat{s})$ with population loss $\varepsilon$ close to optimal, i.e., regret bounded by $\varepsilon$, where the loss is the negative log-likelihood. This regret bound, by~\Cref{lem:population-regret-is-kl}, lets us control the total KL-divergence $D_{KL}(\pte || \widehat{\pte}) + D_{KL}(\ptr || \widehat{\ptr})$. By Pinsker inequality, this implies TV-closeness between the respective distributions and our approximations. We can then invoke~\Cref{thm:shift-TV}, to conclude that after truncation, the density ratio $\widehat{\pte}/\widehat{\ptr}$ can be used as importance weights for the covariate-shifted mean estimation problem.

\paragraph{Error bounds on the estimated parameter $\hat{\theta}$ in the Gaussian case.}

In the case where both distributions are $d$-dimensional Gaussian vectors with known covariance, the sample complexity bound obtained by~\Cref{thm:logistic-regression-result} is easily seen to be suboptimal. For instance, when $\ptr = \cN(\mtr, \II)$, $\pte = \cN(\mte, \II)$, s.t. $\|\mtr - \mte\| \leq O(1)$ according to~\Cref{thm:logistic-regression-result}, we can solve the covariate-shifted mean estimation problem using $O(d/\varepsilon^8)$ samples (in this case we would have $B_1 \approx d, B_2 \leq O(1)$, and $B_3 \leq O(1)$). In contrast~\Cref{thm:CVGaussIso} shows that by learning the means of both Gaussian distributions separately, we can solve the covariate shift problem using $O(d/\varepsilon^{2})$ samples.

  
As it turns out, specifically in the case of Gaussians, we can improve the sample complexity in~\Cref{thm:logistic-regression-result}, and we can recover the importance weights $\hat{r}$, using only $O(\max\{d \log d, d/\varepsilon^{2}\})$ samples using logistic regression, nearly matching the guarantees of~\Cref{thm:CVGaussIso}.

This is a consequence of new results for the error bounds on the \emph{estimated parameter} $\|\hat{\ttheta} - \ttheta^*\|$ in logistic regression, as opposed to classical bounds showing only that the (population) loss at $\hat{\ttheta}$ is not much larger than the minimum value attained at $\ttheta^*$. Bounds of this form are only known when covariate $\xx$ itself is a sub-Gaussian random variable, so this statement is much less general then~\Cref{thm:logistic-regression-result}, but provides much stronger results in this scenario.  We present these improvements in Appendix~\ref{sec:LogisticGauss}

\section{Reproducing Kernel Hilbert Spaces~---~Kernel Trick for Density Estimation}\label{sec:RKHS}
We now present a generalization of logistic regression beyond exponential families.

Specifically, we consider the case where $\ln (\ptr/\pte)$ lies in a \emph{Reproducing Kernel Hilbert Space} $\mathcal{H}$. We show that using \emph{kernel logistic regression}~\cite{zhu2005kernel}, we can find an approximation to the density ratio $\ptr/\ptr$, which is good enough to solve the covariate shift problem. 

Before stating the main theorem, let us briefly review the theory behind RKHS. For a more detailed exposition, refer to~\cite{ghojogh2021reproducing}.

\subsection{Quick RKHS Preliminary \label{sec:rkhs-prelim}}

Consider a Hilbert space $\cH$ of functions over $U$ (not necessarily finitely dimensional), together with an inner product $\langle f,g \rangle_\cH$. If for each $x \in U$ the evaluation functional $\mathrm{ev}_x(f) := f(x)$ is bounded (i.e. for each $x \in U$, there is a bound $T_{x}$, such that for each $f \in \cH$ we have $f(x) \leq T_x \|f\|_{\cH}$), then we say that $(\cH, \langle \cdot, \cdot\rangle_{\cH})$ is a \emph{Reproducing Kernel Hilbert Space (RKHS)}. 

Using standard functional-analytic tools, this implies the existence of a \emph{feature map} $\Phi: U \to \cH$ --- i.e., with any point $x \in U$, we can associate a function $\Phi_x \in \cH$ satisfying, for every $f \in \cH$, $\langle f, \Phi_x \rangle_{\cH} = f(x)$. We can further associate with this RKHS a kernel $K : U\times U \to \mathbb{R}$, given as $K(x_1, x_2) := \langle \Phi_{x_1}, \Phi_{x_2}\rangle_{\cH}$. As it turns out, every such kernel is positive semidefinite, and in fact, this is a complete characterization of RKHS --- every positive semidefinite kernel $K: U \times U \to \mathbb{R}$ corresponds to some RKHS on $U$. For a given kernel $K$, the corresponding feature map is $\Phi_x := K(x, \cdot)$, and the subspace of finite sums
$\sum_i \lambda_i K(x_i, \cdot)$ is dense in $\cH$.


\paragraph{Why are RKHS good --- the Kernel trick}
The key tool related to RKHS is the so-called 'kernel trick' and, more specifically, the relatively simple yet powerful Representer Theorem. This theorem states that the solution to a general optimization problem over the (potentially infinitely dimensional) $\cH$ can be found by reducing it to a finite-dimensional problem. For example,  consider a sample $(x_1, y_1), (x_2, y_2), \ldots (x_n, y_n)$ where $x_i \in \mathcal{U}$ and $y_i \in \mathbb{R}$. 
\begin{theorem}[Representer Theorem\cite{ghojogh2021reproducing}]
\label{thm:representer}
    Let $\cH$ be an RKHS of functions over $U$, together with the feature map $\Phi: U \to \cH$. Let $E : (U \times \bR \times \bR)^n \to \bR$ be an arbitrary error map,  $(x_1, y_1), \ldots (x_n, y_n) \in U \times \bR$ be a sample, and $B \geq 0$ a non-negative bound. Then the minimizer $\hat{\theta}$ of
    \begin{equation}
        \min_{\boldsymbol{\theta} \in \cH ~ : ~ \|\boldsymbol{\theta}\|_{\cH} \leq B} E((x_1, y_1, \boldsymbol{\theta}(x_1)), \ldots (x_n, y_n, \boldsymbol{\theta}(y_n))
    \end{equation}
    is a linear combination
    \begin{equation}
        \hat{\boldsymbol{\theta}} = \sum_i \gamma_i \Phi_{x_i}.
    \end{equation}
\end{theorem}
\paragraph{Kernel Logistic Regression}
Most of the content in this section is well-known. However, for completeness, we provide a detailed analysis of kernel logistic regression. Kernel logistic regression assumes that we observe samples $(\xx_i, y_i)$ where $\xx_i \in U$ is drawn from some distribution and $y_i|x_i \in \{\pm 1\}$ follows the logistic distribution,
\begin{equation*}
    \Pr[y| \xx] = (1 + \exp(y \boldsymbol{\theta}^*(\xx)))^{-1},
\end{equation*}
where $\boldsymbol{\theta}^* \in \cH$ for some RKHS $\cH$. Let us call the joint distribution of $(\xx,y)$ drawn this way as $\mathcal{D}_{\boldsymbol{\theta}^*}$. Note that as long as $\|\boldsymbol{\theta}^*\|_{\cH} \leq B$,  $\ttheta^*$ is a unique minimizer of the population loss function given by negative log-likelihood,
\begin{equation*}
\mathcal{L}_{\boldsymbol{\theta}}(\cD_{\ttheta^*}) := \E_{(x,y) \sim \cD_{\boldsymbol{\theta}^*}} \log(1 + \exp(y \boldsymbol{\theta}(\xx)).
\end{equation*}
That is $\ttheta_* = \text{arg min}_{\|\ttheta\|_{\cH} \leq B} \mathcal{L}_{\ttheta}(\cD_{\ttheta^*})$. 
Using a standard argument based on Rademacher complexity, we bound the error between the population loss and the empirical loss for all $\|\ttheta\|_{\cH} \leq B$. Combined with the fact that empirical loss minimization for kernel logistic regression can be efficiently solved (after applying the representer theorem, it reduces to an $n$-dimensional convex problem). We show the following.
\begin{lemma}
\label{lem:kernel-logistic-regression-regret-bound}
    Using $n \approx \E_{\xx}[K(\xx, \xx)] B^2/\varepsilon^2$ samples from $\cD_{\boldsymbol{\theta}^*}$ we can efficiently find $\hat{\boldsymbol{\theta}}$ satisfying regret bound
    \begin{equation*}
        \cL_{\hat{\boldsymbol{\theta}}}(\cD_{\ttheta^*})  - \cL_{\boldsymbol{\theta}^*}(\cD_{\ttheta^*}) \leq \varepsilon.
    \end{equation*}
\end{lemma}

See~\Cref{sec:kernel-logistic-regression-regret-bound} for a full proof.

\subsection{Covariate Shift via Kernel Logistic Regression}
We now give the main result of this section. If the log of the density ratio of the training and test distribution lies in some RKHS $\cH$, we can efficiently solve the covariate shift problem for these distributions. This result is obtained by combining the regret bounds from~\Cref{lem:kernel-logistic-regression-regret-bound}, which imply the KL-divergence bounds by~\Cref{lem:population-regret-is-kl}, with the Pinsker inequality and~\Cref{thm:shift-TV}.

\begin{theorem}
\label{thm:covariate-shift-kernel-logistic-regression}
    Let $\cH$ be a RKHS of functions over $U$, and $\ptr, \pte$ a pair of distributions over $U$, such that $\boldsymbol{\theta}^*(x) := \ln (\ptr(x) / \pte) \in \cH$.

    Assume that the kernel $K$ associated with $\cH$ satisfies $\sqrt{\E_{\xx \sim \ptr} K(x,x)} + \sqrt{\E_{\xx \sim \pte} K(\xx, \xx)} \leq D$ for all $x \in U$,  $\|\boldsymbol{\theta}^*\| \leq B_1$, and $R_2(\pte || \ptr) \leq B_2.$

    Then, we can solve the covariate-shifted mean estimation problem from $\ptr$ to $\pte$ using at most $O(D B_1^2 B_2^4 / \varepsilon^8)$ samples.
\end{theorem}
The full proof can be found in~\Cref{sec:covariate-shift-kernel-logistic-regression}.

Again, similar to the argument that using \Cref{thm:covariate-shift-kernel-logistic-regression} directly in the special case of univariate Gaussian distributions gives a suboptimal sample complexity, i.e., when $\ptr = \cN(\mu_{tr}, 1), \pte = \cN(\mu_{te}, 1)$  s.t. $\|\mu_1 - \mu_2\| \leq O(1)$, this result also gives a suboptimal sample complexity of $O(\varepsilon^{-8})$ instead of $O(\varepsilon^{-2})$.