\section{Introduction}

A common assumption in several machine learning algorithms is that training and test data come from the same distribution, which does not hold in many real-world applications. {\em Covariate shift}, first introduced by~\textcite{shimodaira2000improving}, is a widely used assumption for addressing such distribution shifts. It assumes that the conditional distribution of labels given the features remains the same across training and test data, i.e., $\ptr(\yy|\xx) = \pte(\yy|\xx)$, while the marginal distributions of the features differ, i.e., $\ptr(\xx) \neq \pte(\xx)$. Covariate shift has applications across various domains, including Natural Language Processing~\cite{yamada2010semi}, Signal Processing~\cite{yamada2012no}, Brain-computer interfacing~\cite{li2010application}, Spam Filtering~\cite{bickel2006dirichlet}, Human activity recognition~\cite{hachiya2012importance}, speaker identification~\cite{yamada2010semi}, and biomedical engineering~\cite{li2010application}.


In this paper, we capture the core of covariate shift and introduce the following general problem, which we refer to as \emph{covariate-shifted mean estimation}. Let $\ff: \bR^d \to [-1,1]$ be a bounded function unknown to the algorithm. Given $\epsilon>0$,  along with $n_{tr}$ labeled samples $(\xx_i,\ff(\xx_i))$ for $\xx_i\sim \ptr$ and $n_{te}$ unlabeled samples $\widetilde{\xx_i}\sim\pte$, the goal is to compute $Z$ such that $|Z-\E_{\xx\in \pte}\ff(\xx)|\leq \epsilon$. The question is: for a target accuracy $\varepsilon$, how many samples $n_{tr}, n_{te}$ are sufficient to achieve this guarantee? Notably, if both $\ptr$ and $\pte$ were known exactly, solving this problem would be significantly easier. In this case \begin{equation*}
    \E_{\xx\sim \ptr} \frac{\pte(\xx)}{\ptr(\xx)}\ff(\xx) = \E_{\xx\sim \pte}\ff(\xx),
\end{equation*}
providing direct access to an unbiased estimator of the desired quantity. Moreover, if the ratio $\pte(\xx)/\ptr(\xx)$ is bounded for a typical $\xx \sim \ptr$, we can control the variance of this estimator\footnote{A variant of this condition is necessary. For instance, if $\pte$ and $\ptr$ have disjoint support, estimating $\E_{\xx\in \pte}\ff(\xx)$ is impossible, even if we know both densities $\pte$ and $\ptr$ exactly since we have no information about $\ff$ on the support of $\pte$.}, and therefore estimate $\E_{\xx \sim \pte} \ff(\xx)$ from a finite sample drawn from $\ptr$. As a result, most of the interest is focused on approximately learning the density ratio $\pte(\xx)/\ptr(\xx)$, also called {\it importance weights}. Independently, there is a rich body of work on importance weight estimation in both the theoretical and applied literature, for example, ~\cite{CYM10,wen2015correcting,hachiya2012importance,gopalan2022multicalibrated,sugiyama2012density}. However, surprisingly, relatively few studies attempt to address the following two fundamental questions systematically.
\begin{enumerate}
    \item What approximation guarantees can be obtained for a specific algorithm attempting to estimate the density ratio with a given number of samples?
    \item What kind of approximation guarantees do we need for the estimated density ratio to serve as a good proxy of the true density ratio for covariate shift?
\end{enumerate}

Since the covariate-shifted mean estimation problem cannot be efficiently solved in full generality, certain regularity conditions must be imposed~--- either on the unknown function $\ff$ or on the unknown densities $\ptr, \pte$. Most prior work has focused on the development of algorithms under specific restrictions on $\ff$, 
with the most significant theoretical results based on \emph{kernel mean matching}. In this work, we primarily focus on the latter approach, where we assume  constraints on the densities instead.




\subsection{Related Works}
Before presenting our results, we summarize the current state of theoretical results in this space.

\paragraph{Kernel Mean Matching based Approaches}
The most popular algorithm for the covariate shift problem is the celebrated \emph{Kernel Mean Matching} (KMM), introduced by \cite{huang2006correcting}~---~one of the few results in this space with end-to-end theoretical guarantees. We discuss it in more detail here, as it shares a superficial resemblance to our most general result due to its foundation in \emph{Reproducing Kernel Hilbert Space} (RKHS) theory. 
 

Consider an RKHS $\cH$ with a reproducing kernel map $\Phi$ (see~\Cref{sec:rkhs-prelim} for a brief overview of RKHS notation),  where the kernel is bounded such that $\|\Phi_{\xx}\|_{\cH} \leq 1$ for all $\xx$. The standard analysis of KMM assumes that $\ff \in \cH$ is bounded in the $\cH$ norm, i.e., $\|\ff\|_{\cH} \leq M$, and the density ratio satisfies $\pte(\xx)/\ptr(\xx) \leq B$. Theorem~1 of~\textcite{yu2012analysis} shows that, under these conditions, collecting $n_{tr} \gtrsim (MB/\varepsilon)^2$ samples from $\ptr$ and $n_{te} \gtrsim (M/\varepsilon)^2$ samples from $\pte$ suffices to estimate $\E_{\tilde{\xx} \sim \pte} \ff(\tilde{\xx})$ within an error of~$\varepsilon$. More detailed description of the algorithm can be found in \Cref{sec:KMMComparison}.

We further observe that if $\ff$ belongs to a function class $\cF$ that allows it to be learned with arbitrarily small error~--- i.e., one can find $\hat{\ff}$ such that, $\E_{\xx \sim \ptr} |\ff(\xx) - \hat{\ff}(\xx)| \leq \varepsilon'$, with respect to $\ptr$~--- there is a naive approach to solve the covariate-shifted mean estimation problem. Specifically, one can first approximate $\ff$ with $\hat{\ff}$ up to an error $\varepsilon' \ll \varepsilon$, and use~$\hat{\ff}$ in place of $\ff$ when estimating $\E_{\tilde{\xx}\sim\pte} \ff(\tilde{\xx})$. 

In this case, we obtain:
\begin{equation*}
\left|\E_{\tilde{\xx} \sim \pte} \hat{\ff}(\tilde{\xx}) - \E_{\tilde{\xx}\sim\pte} \ff(\tilde{\xx})\right| \leq \E_{\tilde{\xx} \sim \pte} |\hat{\ff}(\tilde{\xx}) - \ff(\tilde{\xx})| \leq B \E_{\xx \sim \ptr} |\hat{\ff}(\xx) - \ff(\xx)| \leq B \varepsilon'.
\end{equation*}
Thus, setting $\varepsilon':= \varepsilon/B$ ensures the desired accuracy. The estimate $\hat{\ff}$ can be found using $L_1$ kernel regression with $n_{tr} \approx M^2/{\varepsilon'}^2 = (MB/\varepsilon)^2$ samples. Then $n_{te} \approx M^2/\varepsilon^2$ samples suffice to estimate $\E_{\tilde{\xx} \sim 
\pte} \hat{\ff}(\xx)$ within an error of~$\varepsilon$. See~\Cref{sec:KMMComparison} for more details.

This highlights that the standard analysis of the KMM algorithm in a simple realizable scenario falls short of providing an asymptotic improvement in sample complexity over the naive plug-in method. Therefore, a more fine-grained analysis is necessary to justify the widely observed superior empirical performance of KMM.

\paragraph{Importance Weights Estimation via Classification}

A common approach to estimating the density ratio is to reduce it to a classification problem. Specifically, consider a distribution over pairs $(\xx, y)$ where $y \in \{ 0, 1\}$ is uniform, 
and $\xx$ is drawn from $\ptr$ when $y=0$, and from $\pte$ when $y=1$. The Bayes-optimal classifier for this problem outputs $\Pr[y=1 | \xx]$, which can be transformed into the density ratio $\pte(\xx)/\ptr(\xx)$.

Given the extensive literature on classification, one can train a model to approximate $\Pr[y=1 | \xx]$ and then apply the same transformation to obtain an estimate of the density ratio. This estimated ratio can, in turn, serve as importance weights for solving the covariate-shifted mean estimation problem.

Logistic regression is a widely used method for estimating $\Pr[y=1 | \xx]$ in this setting, while the use of more powerful models, such as \emph{kernel logistic regression} has also been explored~\cite{bickel2009discriminative,sugiyama2012density}.  

Despite the popularity of this approach to handle the covariate shift, no end-to-end theoretical guarantees are known. In this work, we bridge this gap by providing sample complexity and approximation guarantees for covariate-shifted mean estimation using logistic regression and kernel logistic regression under specific model assumptions.

\paragraph{Separately Estimating Both Densities}

A widely held belief in importance weight estimation is that separately learning the densities $\ptr$ and $\pte$, and then using their ratio, $\widehat{\pte}/\widehat{\ptr}$, as an estimate of $\pte/\ptr$ is both inefficient~--- since density estimation in high-dimensional spaces often requires exponentially many samples~--- and, more importantly, insufficient (see, for example, Section 2.1 in \cite{huang2006correcting}, or Section 2.2 in \cite{yu2012analysis}).

This intuition is natural, as even a small error in estimating the denominator $\ptr$, can lead to a significant error in the density ratio. Consequently, it remains unclear whether, even in the situation where we could obtain estimates for $\ptr$ and $\pte$ within $\varepsilon$ \emph{total variation} (TV) distance in time polynomial in $1/\varepsilon$, this would be helpful in solving the covariate shift problem.


Surprisingly, we show that with only a polynomial increase in sample complexity, it is indeed possible to solve the covariate-shifted mean estimation problem for any pair of distributions within an efficiently learnable class -- provided that $\pte(\xx)/\ptr(\xx)$ remains reasonably bounded for most $\xx \sim \ptr$. There is a rich body of work in distribution learning theory demonstrating that many distribution families can be efficiently learned, such as mixtures of Gaussians~\cite{moitra2010settling,liu2022clustering}, low-dimensional log-concave distributions~\cite{diakonikolas2017learning}, or graphical models with bounded treewidth~\cite{narasimhan2004paclearning}.

\subsection{Our Results}

In this paper, we introduce several algorithms for the covariate-shifted mean estimation problem, providing theoretical guarantees on approximation bounds and sample complexity. Our first result is an algorithm with small sample complexity when both $\ptr$ and $\pte$ are multivariate $d$-dimensional Gaussian distributions. Surprisingly, this is the first work to establish formal bounds for covariate shift, even in the case of Gaussian distributions 

\begin{theorem}[Informal Statement of Theorem~\ref{thm:CVGauss}]\label{thm:GaussIntro}
    Let $\ptr$ and $\pte$ be $d$-dimensional Gaussian distributions which are close to each other.  There is an algorithm (Algorithm~\ref{alg:CVGauss}), such that for any $\ff$ satisfying $\sup_{\xx\in \mathbb{R}^d}|\ff(\xx)|\leq 1$,  requires at most $O\left(d^2/\varepsilon^2 \log\frac{1}{\delta}\right)$ samples of $(\xx_i,\ff(\xx_i)), \xx_i \sim \ptr$ and $\widetilde{\xx_i}\sim\pte$, and returns $Z$ such that with probability at least $1-\delta$,
\[
|Z-\E_{\xx\in \pte}\ff(\xx)|\leq \varepsilon.
\]
In the case when $\pte,\ptr$ are isotropic Gaussians, $O\left(d/\varepsilon^2 \log(1/\delta)\right)$ suffice (Algorithm~\ref{alg:CVGaussIso}).
\end{theorem}


Our algorithms indicate that it is sufficient to learn the training and test Gaussian distributions to within an $\varepsilon$ {\it total variation distance}. Given such estimates, $\widehat{\ptr}$ and $\widehat{\pte}$, the estimator $\frac{\widehat{\pte}}{\widehat{\pte}}\ff$ is good enough for our purposes. 

Next, we extend our approach to a broader class of probability distributions. Specifically, we show that if $\ptr$ and $\pte$ belong to any efficiently learnable class of distributions, the covariate-shifted mean estimation problem can be solved with polynomially many samples by learning both distributions individually.

\begin{theorem}[Informal Statement of~\Cref{thm:shift-TV}]\label{thm:TV-Intro}
Let $\ff$ be a bounded function, $\ptr, \pte$ be a pair of distributions, and let $B>0$, be such that $\Pr_{\xx\sim\pte}(\pte(\xx)/ \ptr(\xx) > B/4) \leq \varepsilon$. Let $\widehat{r}$ be s.t. $\widehat{r} = \widehat{\pte}/\widehat{\ptr}$ for some $\widehat{\pte}$, and $\widehat{\ptr}$ satisfying $d_{TV}(\pte, \widehat{\pte}) \leq \varepsilon$ and $d_{TV}(\ptr, \widehat{\ptr}) \leq \varepsilon / B$.

Then given access to $\hat{r}$ and using $O(B^2/\varepsilon^2\log(1/\delta))$ samples samples $(\xx_i, \ff(\xx_i))$, $\xx_i\sim \ptr$ we can return $Z$ (Algorithm~\ref{alg:CV}) such that, with probability at least $1-\delta$,
    \begin{equation*}
        |Z -\E_{\xx\sim \pte} \ff(\xx)| \leq O(\varepsilon).
    \end{equation*}
\end{theorem}

Our algorithm again employs the estimator $\frac{\widehat{\pte}}{\widehat{\ptr}}\ff$ where $\widehat{\pte},\widehat{\ptr}$, are approximations of $\pte$ and $\ptr$ respectively~--- except of dismissing all samples for which $\frac{\widehat{\pte}}{\widehat{\ptr}}$ exceeds a threshold $B$. This challenges the common belief that separately estimating both densities is insufficient for accurately estimating the density ratio. We prove that when the tail of the density ratio is bounded, independently estimating the training and test distributions to a small error is sufficient to solve the covariate-shifted mean estimation problem. While Algorithm~\ref{alg:CV} applies to a broad class of distributions, for Gaussian distributions, Algorithm~\ref{alg:CVGauss} still achieves better sample complexity (Theorem~\ref{thm:GaussIntro}).


Our remaining generalizations build on Algorithm~\ref{alg:CV} and Theorem~\ref{thm:TV-Intro}, focusing on efficiently computing the approximate ratio $\widehat{r}$. As previously mentioned, logistic regression is a popular method for estimating such quantities, yet no theoretical guarantees exist for its performance. We prove that when $\pte$ and $\ptr$ belong to the same exponential family, logistic regression successfully computes the required $\widehat{r}$. Thus, we provide the first theoretical analysis of logistic regression-based density estimation methods. Our analysis, combined with Theorem~\ref{thm:TV-Intro}, leads to the following result.


\begin{theorem}[Informal Statement of Theorem~\ref{thm:logistic-regression-result}]
      Assume that $\ptr$ and $\pte$ belong to the same exponential family parameterized by $\theta \in \bR^D$. If the distance between $\pte$ and $\ptr$ is at most $B$, we can solve the covariate-shifted mean estimation problem using at most $O(B^4 D \varepsilon^{-8})$ samples.
\end{theorem}


Finally, we present our most general result for the covariate-shifted mean estimation problem. We prove that if $\ln (\ptr/\pte)$ belongs to a \emph{Reproducing Kernel Hilbert Space}, the problem can be solved with polynomial sample complexity using an efficient algorithm. Specifically, we show that \emph{kernel logistic regression} can be used to obtain the required estimate $\widehat{r}$, which can then be used in Algorithm~\ref{alg:CV} to obtain the required solution. Notably, our result only assumes regularity of the density ratio $\ptr/\pte$ with either of these densities can potentially be pathological by itself.

\begin{theorem}[Informal Statement of Theorem~\ref{thm:covariate-shift-kernel-logistic-regression}]
    Let $\cH$ be a RKHS of functions over $U$, and $\ptr, \pte$ a pair of distributions over $U$, such that $\ln (\ptr(\xx) / \pte(\xx)) \in \cH$. Assume that the kernel $K$ associated with $\cH$ is bounded by $D$ and the distance between $\ptr$ and $\pte$ is at most $B$. We can then solve the covariate-shifted mean estimation problem using at most $O(B^4 D \varepsilon^{-8})$ samples.
\end{theorem}

