
\section{Proofs from Section~\ref{sec:logistic}}\label{sec:ProofsLogistic}
\begin{lemma}
\label{lem:population-regret-is-kl}
Let $\cD_0$ be a distribution described above, $r^*(\xx) := \Pr_{\cD_0}[y=-1 | \xx]$ and consider a classifier $\hat{r} : U \to (0, 1)$, intended to estimate $r^*$.

Let $\cL_{\cD_0, \hat{r}}$ be an expected cross-entropy loss of classifier $\hat{r}$, namely
\begin{equation*}
    \cL_{\cD, \hat{r}} := - \E_{(\xx,y) \sim \cD} [ \mathbf{1}_{y=-1} \log \hat{r}(\xx) + \mathbf{1}_{y=1} \log (1-\hat{r}(\xx))].
\end{equation*}
Then there exist a pair of distributions $\widehat{\ptr}, \widehat{\pte}$ such that the population regret for a classifier $\hat{r} $ is equal
\begin{equation*}
    \mathcal{R}_{\cD, \hat{r}} := \mathcal{L}_{\cD, \hat{r}} - \mathcal{L}_{\cD, r^*} = \frac{1}{2} (D_{KL}(\pte || \widehat{\pte}) + D_{KL}(\ptr || \widehat{\ptr})) + (1 - H(y')),
\end{equation*}
where $H(y') \leq 1$ is a Shannon entropy of some binary random variable. In particular
\begin{equation*}
    D_{KL}(\pte || \widehat{\pte}) + D_{KL}(\ptr || \widehat{\ptr}) \leq 2 \mathcal{R}_{\hat{r}}.
\end{equation*}
\end{lemma}


\begin{proof}
Consider a joint distribution $\cD_1$ over $(\xx, y')$, s.t. the marginal distribution of $\xx$ is the same as in $\cD$, i.e. $\xx \sim \frac{1}{2} \ptr + \frac{1}{2} \pte$, and $\Pr[y' = -1 | \xx] := \hat{r}(\xx)$.


    We have
    \begin{align}
        \mathcal{L}_{\hat{r}} - \mathcal{L}_{r^*} & = \E_{(\xx,y) \sim \mathcal{D}_0} \mathbf{1}_{y=-1} (\log(\hat{r}(\xx)) - \log(r^*(\xx))) + \mathbf{1}_{y=1} ( \log(1 - \hat{r}(\xx)) - \log(1- r^*(\xx)))  \notag \\
        & = \frac{1}{2} \E_{x \sim \ptr} \log\frac{\hat{r}(\xx)}{r^*(\xx)} + \frac{1}{2} \E_{x\sim \pte} \log \frac{1-\hat{r}(\xx)}{1-r^*(\xx)}. \label{eq:5.2.1}
    \end{align}
    Now $\hat{r}(\xx) = \Pr[y'=-1 | \xx] = \Pr[y'=-1, \xx] / Pr[\xx]$, and similarly $r(\xx) = \Pr[y'=-1, \xx] / \Pr[\xx]$. Hence \begin{equation*}
\frac{\hat{r}(\xx)}{r^*(\xx)} = \frac{\Pr[y'=-1, \xx]}{\Pr[y=-1, \xx]} = \frac{\Pr[y'=-1]}{\Pr[y=-1]}\frac{\ptr(\xx)}{\widehat{\ptr}(\xx)} = 2 \Pr[y'=-1] \frac{\ptr(\xx)}{\widehat{\ptr}(\xx)},
    \end{equation*}
    and analogously
    \begin{equation*}
        \frac{1- \hat{r}(\xx)}{1-r^*(\xx)} = 2 \Pr[y'=1] \frac{\pte(\xx)}{\widehat{\pte}(\xx)}.
    \end{equation*}

    Plugging this back to~\eqref{eq:5.2.1}, we get
    \begin{align*}
    \mathcal{L}_{\hat{r}} - \mathcal{L}_{r^*} & = \frac{1}{2} \E_{\xx \sim \ptr} \log \frac{\ptr(\xx)}{\widehat{\ptr}(\xx)} + \E_{\xx \sim \pte} \log \frac{\pte(\xx)}{\widehat{\pte}(\xx)} + \frac{1}{2} \log \Pr[y'=-1] + \frac{1}{2} \log \Pr[y'=-1] +1 \\
    & =
    \frac{1}{2} D_{KL}(\ptr || \widehat{\ptr}) + D_{KL}(\pte || \widehat{\pte}) + D_{KL}(y || y').
    \end{align*}
\end{proof}



As we discussed above, in order to show this theorem, we will bound the Rademacher complexity of the class $\{ \ell_{\boldsymbol{\theta}}: \|\boldsymbol{\theta}\| \leq B_2, s \leq B_2\}$, where
\begin{equation}
    \ell_{\boldsymbol{\theta},s}(\xx, y) = -\log(1 + \exp(-y(\langle \boldsymbol{\theta}, \xx\rangle + s))).
\end{equation}
We show the following lemma using methods that are now standard, but many of the proofs of similar statements in the literature use stronger assumptions on the distribution of $\xx$ (for example, assuming that almost surely $\|\xx\| \leq C \sqrt{\|\boldsymbol{\Sigma}\|_{op}}$), in order to provide better concentration results. We include a full proof of the following lemma (which needs only second moment assumptions on $\xx$) in~\Cref{sec:rademacher-complexity-log-reg}.
\begin{lemma}
    \label{lem:rademacher-complexity-log-reg}
    Let $\cD$ be a distribution over pairs $(\xx, y)$ with $\xx \in U, y \in \{\pm 1\}$, and let $\boldsymbol{\Sigma} = \E \xx \xx^T$ be a covariance matrix of the marginal $\xx$. Consider a sample $S = ((\xx_1, y_1), \ldots (\xx_n, y_n))$ drawn i.i.d. from $\cD$.  Then
    \begin{equation*}
        \E_{S \sim \cD^n} \sup_{\|\boldsymbol{\theta}\| \leq B, s \leq B} |\widehat{\cL}_{\boldsymbol{\theta}, s}(S) - \cL_{\boldsymbol{\theta}, s}(\cD)|\leq \frac{B \sqrt{\Tr\boldsymbol{\Sigma}}}{\sqrt{n}}.
    \end{equation*}
    where $\cL_{\boldsymbol{\theta}, s}(\cD) := \E_{(\xx, y)\sim \cD} \ell_{\boldsymbol{\theta}, s}$ and $\widehat{\cL}_{\boldsymbol{\theta}, s}(S) := \frac{1}{n} \sum_i \ell_{\boldsymbol{\theta}, s}(\xx_i, y_i)$.
\end{lemma}

Finally, since for a given sample $S$, a function $(\boldsymbol{\theta}, s) \mapsto \widehat{\cL}_{\boldsymbol{\theta}, s}(S)$ is convex, we can efficiently find an approximate minimum of this function. That is,
\begin{fact}
\label{fact:log-reg-optimization}
    Given a sample $S = (\xx_1, y_1), \ldots (\xx_n, y_n)$ and $\varepsilon'$ we can find in polynomial time $\hat{\boldsymbol{\theta}}, \hat{s}$ such that
    \begin{equation*}
        \widehat{\cL}_{\hat{\boldsymbol{\theta}}, \hat{s}}(S) \leq \min_{\boldsymbol{\theta}, s} \widehat{\cL}_{\boldsymbol{\theta}, S} + \varepsilon'.
    \end{equation*}
\end{fact}

We are now ready to prove~\Cref{thm:logistic-regression-result}.

\begin{proof}[Proof of~\Cref{thm:logistic-regression-result}]
Take $\varepsilon'$, depending on $\varepsilon, B_1, B_2, B_3$, which we will fix later. Using~\Cref{fact:log-reg-optimization}, we can find $\hat{\boldsymbol{\theta}}, \hat{s}$ s.t. $\widehat{\cL}_{\hat{\boldsymbol{\theta}}, \hat{s}}(S) \leq \varepsilon' + \min_{\boldsymbol{\theta}, s} \widehat{\cL}_{\boldsymbol{\theta}, s}(S)$. Using Markov inequality for $n \gtrsim {\varepsilon'}^{-2} B_2 \Tr (\str + \ste)$, \Cref{lem:rademacher-complexity-log-reg} implies that with probability $9/10$ simultaneously for all $\boldsymbol{\theta},s$ we have $\cL_{\boldsymbol{\theta}, s}(\cD) = \widehat{\cL}_{\boldsymbol{\theta}, s}(S) \pm \varepsilon'$.
Combining those inequalities, we get
\begin{equation*}
    \cL_{\hat{\boldsymbol{\theta}}, \hat{s}}(\cD) \leq \varepsilon' 
    + \widehat{\cL}_{\hat{\boldsymbol{\theta}}, \hat{s}}(S) \leq 2\varepsilon' 
    + \widehat{\cL}_{\boldsymbol{\theta}^*, s^*}(S) \leq 3 \varepsilon' + \cL_{\boldsymbol{\theta}^*, s^*}(\cD).
\end{equation*}
This implies bound on population regret
\begin{equation*}
     \cL_{\hat{\boldsymbol{\theta}}, \hat{s}}(\cD) - \cL_{\boldsymbol{\theta}^*, s^*}(\cD) \leq 3 \varepsilon',
\end{equation*}
and by~\Cref{lem:population-regret-is-kl}, this implies
\begin{equation*}
    \max(D_{KL}(\pte || \widehat{\pte}), D_{KL}(\ptr || \widehat{\ptr})) \lesssim \varepsilon'.
\end{equation*}
By Pinsker inequality~(\Cref{thm:pinsker}), we can deduce $d_{TV}(\pte, \widehat{\pte}) \lesssim \sqrt{\varepsilon'}$ and similarly $d_{TV}(\pte, \widehat{\ptr}) \lesssim\sqrt{\varepsilon'}$. If $R_2(\ptr || \pte) \leq B_3$, then $\E_{\xx \sim \pte} \pte(\xx)/\ptr(\xx) = \E_{\xx\sim\ptr} (\pte(\xx)/\ptr(\xx))^2 \leq B_3$ and by Markov inequality $\Pr_{\xx \sim \pte}(\pte(\xx) / \ptr(\xx) > B_3 / \varepsilon) \leq \varepsilon$. We can now apply~\Cref{thm:shift-TV}, with $B = B_3 / \varepsilon$ to deduce the desired statement. To this end, we need to choose $\varepsilon' \approx (\varepsilon/B)^2 \lesssim \varepsilon^{4} / B_3^2$, such that the assumption of~\Cref{thm:shift-TV} are satisfied.

This choice of $\varepsilon'$, yields sample complexity used in the logistic regression phase of the argument as $n \approx B_2^2 \Tr (\Sigma_{tr} + \Sigma_{te}) (\varepsilon')^{-2} = B_1 B_2^2 B_3^4 \varepsilon^{-8}$.

We need additional $O(B^2/\varepsilon^2) = O(B_3^2 \varepsilon^3)$ samples to apply estimation in Algorithm~\ref{alg:CV}, which is negligible. The total sample complexity is then $O(B_1 B_2^2 B_3^4 \varepsilon^{-6})$ as desired.
\end{proof}



\subsection{Improved Bounds for Gaussians}\label{sec:LogisticGauss}


\begin{theorem}[\cite{Bblog,OB18}]
    \label{thm:logistic-regression-parameter-closeness}
    Let $(\xx, y)$ follow the logistic model with parameter $\ttheta^*$, s.t. $\|\ttheta^*\| \leq O(1)$. Then using logistic regression by running an empirical minimization of the negative log-likelihood in parameter space $\|\ttheta\| \leq C$, after observing $n \gtrsim d \log d$ samples, the estimated parameter $\hat{\ttheta}$ satisfies \begin{equation*}
        \|\hat{\ttheta} - \ttheta^*\|^2 \leq \frac{d}{n}.
    \end{equation*}
\end{theorem}
In our case, the covariate $\xx$ follows a mixture of two Gaussian distributions. We will show that such a random variable is indeed subgaussian, and therefore we can apply~\Cref{thm:logistic-regression-parameter-closeness}
\begin{fact}
\label{fact:mixture-is-subgaussian}
    Let $\xx$ be distributed according to the mixture of two Gaussian distributions $\xx \sim \frac{1}{2} \cN(\boldsymbol{\mu}_1, \II) + \frac{1}{2} \cN(\boldsymbol{\mu}_2, \II)$, where $\|\boldsymbol{\mu}_1\|, \|\boldsymbol{\mu}_2\| \leq O(1)$. Then $\xx$ is $O(1)$-subgaussian.
\end{fact}
\begin{proof}
Taking $Z = \langle \xx, v\rangle$ for a unit vector $v$, we reduce to one-dimensional case: $Z$ is a mixture of two univariate gaussians.

Then
\begin{align*}
    \E \exp(\lambda(Z - \E Z)) & = \exp(-\lambda(\mu_1 + \mu_2)/2) \frac{\E\exp(\lambda Z_1) + \E\exp(\lambda Z_2)}{2} \\
    & = \exp(-\lambda(\mu_1 + \mu_2)/2) \frac{\exp(\lambda \mu_1 + O(\lambda^2)) + \exp(\lambda \mu_2 + O(\lambda^2)}{2} \\
    & = \exp(O(\lambda^2)) \frac{\exp(\lambda \Delta) + \exp(-\lambda \Delta)}{2},
\end{align*}
where $\Delta = (\mu_1 - \mu_2)/2$. If $\sigma$ is a Rademacher random variable independent of $Z$, then 
\begin{equation*}
    \frac{\exp(\lambda \Delta) + \exp(-\lambda \Delta)}{2} = \E \exp(\lambda \Delta \sigma) = \exp(O(\lambda^2 \Delta^2),
\end{equation*}
using a well-known fact that Rademacher random variables are $O(1)$-subgaussian. This implies
\begin{equation*}
    \E \exp(\lambda(Z - \E Z)) \leq \exp(O(\lambda^2)) \exp(O(\lambda^2 \Delta^2) = \exp(O(\lambda^2)),
\end{equation*}
when $\Delta = O(1)$.
\end{proof}


We can now show that a bound on covariate shift, using $\hat{\ww}(x) := \exp(\langle \hat{\ttheta}, \xx \rangle)$ as an estimate for the true density ratio $\pte(\xx)/\ptr(\xx) = \exp(\langle \ttheta^*, \xx \rangle)$.

\begin{lemma}
    Taking $\hat{\ww}(\xx)$ as above, if $\|\hat{\ttheta} - \ttheta^*\| \leq \varepsilon$, then for any bounded function $\ff$, we have
    \begin{equation*}
        | \E_{\xx\sim \ptr} \hat{\ww}(\xx) \ff(\xx) - \E_{\xx\sim \pte} \ff(\xx)| \lesssim \varepsilon.
    \end{equation*}
\end{lemma}
\begin{proof}
    Taking $w(x) := p_1(x)/p_2(x)$, we can bound 
    \begin{align*}
        | \E_{x\sim p_2} \hat{w}(x) f(x) - \E_{x\sim p_1} f(x)| 
        & \leq \E_{x\sim p_2} |\hat{w}(x) - w(x)|  \\
        & = \E_{x\sim p_2} w(x) \left|1 - \frac{\hat{w}(x)}{w(x)}\right| \\
        & = \E_{x\sim p_1} \left| 1 - \frac{\hat{w}(x)}{w(x)}\right|.
    \end{align*}
    Now, just by definition $\hat{w}(x) / w(x) = \exp(\langle \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}^*, x\rangle)$. Taking $Z := \langle \hat{\boldsymbol{\theta}} - \boldsymbol{\theta}^*, x \rangle$, since $x\sim p_1$ is a multivariate Gaussian with mean $\boldsymbol{\mu}_1$ and covariance $\II$, $Z$ is a univariate gaussian with mean $\E Z = \langle \boldsymbol{\theta}^* - \hat{\boldsymbol{\theta}}, \boldsymbol{\mu}_1\rangle \leq O(\|\boldsymbol{\theta}^* - \hat{\boldsymbol{\theta}}\|_2) \leq O(\varepsilon)$ and variance $\Var(Z) = \|\boldsymbol{\theta}^* - \hat{\boldsymbol{\theta}}\|_2^2 \leq \varepsilon^2$.

    The bound on $\E |1 - \exp(Z)| \leq O(\varepsilon)$ is a simple calculation proven after this proof.
\end{proof}
\begin{lemma}
\label{lem:abs_one_minus_mgf}
    Let $Z$ be a univariate gaussian with $| \E Z | \leq \varepsilon$ and $\Var(Z) \leq \varepsilon^2$. Then
    \begin{equation*}
        \E |1 - \exp(Z)| \lesssim \varepsilon.
    \end{equation*}
\end{lemma}
\begin{proof}
    Let $\mu := \E Z$, $s := \exp(\mu) - 1$, $\tilde{Z} = Z - \E Z$ and $\sigma := \sqrt{\E \tilde{Z}^2}$. We have
    \begin{align*}
        \E | 1 - \exp(Z)| = \E |1 - \exp(\tilde{Z})( 1 + s)| \\
        & = \E |1 - \exp(\tilde{Z})| + s \E |\exp(\tilde{Z})| \\
        & \leq \E | 1 - \exp(\tilde{Z}|) + s \sqrt{\E \exp(2 \tilde{Z})} \\
        & \leq \E | 1 - \exp(\tilde{Z})| + O(\varepsilon).
    \end{align*}

    We can now bound
    \begin{align*}
        \E (1 - \exp(\tilde{Z}))^2 & = 1 + \exp(2 \tilde{Z}) - 2 \exp(\tilde{Z}) \\
        & = 1 + \exp(4 \sigma^2) - 2 \exp(\sigma^2) \\
        & = 1 + (1 + O(\sigma^2)) - 2 (1 + O(\sigma^2))\\
        & \leq O(\sigma^2).
    \end{align*}
    Which gives
    \begin{equation}
        \E | 1 - \exp(\tilde{Z})| \leq \| 1 - \exp(\tilde{Z})\|_2 \leq O(\sigma).
    \end{equation}
Hence $\E | 1 - \exp(Z)| \leq O(\sigma) + O(\varepsilon) = O(\varepsilon)$

\end{proof}

\begin{lemma}
    Let $f$ be a bounded function, and $Z := \hat{w}(\xx) \ff(\xx)$ for $\xx \sim \ptr$ and $\hat{w}$ as above. Then $\Var(Z) \leq O(1)$.
\end{lemma}
\begin{proof}
    Indeed,
    \begin{equation*}
        \Var(Z) \leq \E Z^2 \leq \E_{x\sim p_1} \hat{w}(x) = \E_{x\sim p_1} \exp(\langle 2 \hat{\theta}, x\rangle) = \exp(2 \hat{\theta} \mu_1 + 4 \|\hat{\theta}\|^2) = O(1).
    \end{equation*}
\end{proof}

\subsection{Proof of~\Cref{lem:rademacher-complexity-log-reg} \label{sec:rademacher-complexity-log-reg}}


\begin{lemma}
\label{lem:rademacher-linear}
     The Radeamcher complexity of the class $\mathcal{F} := \{ \xx \mapsto \langle \boldsymbol{\theta}, \xx \rangle : \|\boldsymbol{\theta}\| \leq B \}$ is at most
    $B \sqrt{\Tr \boldsymbol{\Sigma}}/\sqrt{n}$ where $\boldsymbol{\Sigma} := \E_{\xx \sim p} \xx \xx^T$ is the covariance matrix.
\end{lemma}
\begin{proof}
  We can bound the Rademacher complexity of $\cF$ (taking $\sigma_i$ to be independent Rademacher valued random variables) as follows
    \begin{align*}
        n \mathcal{R}_{n, \cD}(\cF) & =\E_{\xx, \sigma} \sup_{\|\boldsymbol{\theta}\| \leq B} \sum_i \sigma_i \langle \xx_i, \boldsymbol{\theta} \rangle \\
        & = \E_{\xx, \sigma} \sup_{\|\theta\| \leq B} \left\langle \sum_i \sigma_i \xx_i, \theta \right\rangle \\
        & = \E_{\xx, \sigma} B \left\|\sum_i \sigma_i \xx_i \right\| \leq B \sqrt{\E\left\|\sum \sigma_i x_i\right\|^2}
    \end{align*}
    Now, since $\E \sigma_i \sigma_j = 0$ for $i\not=j$, we get
    \begin{align*}
    \E \left\|\sum_i \sigma_i x_i \right\|^2 =\sum_i \E \|\xx_i\|^2 = n \Tr \boldsymbol{\Sigma}.
    \end{align*}
    Combining those two, we get
    \begin{equation*}
        \mathcal{R}_{n, \cD}(\cF) \leq B \frac{\sqrt{\Tr \boldsymbol{\Sigma}}}{n},
    \end{equation*}
\end{proof}
Since composition with a Lipschitz function does not increase the Rademacher complexity~(\Cref{thm:rademacher-lipschitz-composition}) we get for any Lipschitz function $\gamma$, the class $\{ \xx \mapsto \gamma(\langle \boldsymbol{\theta}, \xx\rangle) : \|\boldsymbol{\theta}\| \leq B\}$ is also bounded by $B \sqrt{\Tr \boldsymbol{\Sigma}} / \sqrt{n}$.

We can now lift this bound to a bound on the class of relevant loss functions.

\begin{lemma}
    \label{lem:rademacher-y}
    Let $\cF'$ be any family of functions from $U \to \bR$. Consider a family of functions $\cF \subset U \times \{0, 1\} \to \bR$ s.t. for every $f \in \cF$ and every $y \in \{0, 1\}$ we have $f(\cdot, y) \in \cF'$. 
    Let $\cD$ be a distribution over $U \times \{ 0, 1\}$, s.t. $\cD_{\xx}$ is the marginal of $\cD$ on $U$. Then
    \begin{equation*}
        \mathcal{R}_{n, \cD}(\cF) \lesssim \mathcal{R}_{n, \cD_{\xx}}(\cF').
    \end{equation*}
\end{lemma}
\begin{proof}
    We can write arbitrary function $f \in \cF$ as  $f(\xx, y) = y f_1(\xx) + (1-y) f_0(\xx)$, 
    where $f_0, f_1 \in \cF'$. Then
    \begin{equation*}
        \mathcal{R}_{n, \cD}(\cF) = \E_{(\xx, y), \sigma} \sup_{f \in \cF} \sum_i \sigma_i f(\xx_i, y_i) \leq \E_{(\xx, y), \sigma} \sup_{f_1 \in \cF'} \sum_i \sigma_i y_i f_0(x_i) + \E_{(\xx, y), \sigma}\sup_{f_0 \in \cF'} \sum_i \sigma_i (1-y_i) f_1(x_i).
    \end{equation*}

    Now, all we need to show is that for a fixed sample $(\xx_1, y_1), \ldots (\xx_n, y_n)$ we have
    \begin{equation*}
        \E_{\sigma} \sup_{f \in \cF'} \sigma_i y_i f(\xx_i) \leq \E_{\sigma} \sup_{f \in \cF'} \sum \sigma_i f(\xx_i).
    \end{equation*}
    That is a special case of Talagrand's contraction principle~(Theorem 4.12 in \cite{ledoux2013probability}), since for each $i$ we have $|y_i f(\xx_i)| \leq |f(\xx_i)|$.
\end{proof}
Combining~\Cref{lem:rademacher-linear} and~\Cref{lem:rademacher-y}, together with the observation that $t \mapsto \log(1 + \exp(t + s))$ is a 1-Lipschitz function, we obtain the following corollary, where as a reminder, we consider a family of loss functions $\ell_{\theta, s}(\xx, y) := \log (1 + \exp(y\langle \theta, \xx\rangle + s))$.
\begin{corollary}
Family of functions $\cF_{\ell} := \{\ell_{\theta, s}(\xx, y) : \|\boldsymbol{\theta}\| \leq B_2, |s| \leq B_2 \}$, with respect to the distribution $\cD_0$ as in \Cref{sec:logistic}, has Rademacher complexity bounded as
\begin{equation*}
    \mathcal{R}_{n, \cD_0}(\cF_{\ell}) \lesssim B_2 \frac{\sqrt{\Tr (\str + \ste)}}{\sqrt{n}}.
\end{equation*}
\end{corollary}

This, together with~\Cref{thm:rademacher-generalization-bound} and Markov inequality completes the proof of~\Cref{lem:rademacher-complexity-log-reg}.



