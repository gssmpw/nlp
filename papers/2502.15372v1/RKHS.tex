\section{{Proofs from Section~\ref{sec:RKHS}}}
\subsection{Proof of~\Cref{lem:kernel-logistic-regression-regret-bound} \label{sec:kernel-logistic-regression-regret-bound}}
As usual, instead of minimizing the population loss directly, we minimize the empirical loss. For a sample $S = ((\xx_1, y_1), \ldots (\xx_n, y_n))$ drawn i.i.d. from $\cD_{\boldsymbol{\theta}^*}$ the empirical loss is defined as
\begin{equation*}
\widehat{\mathcal{L}}_{\boldsymbol{\theta}, S} := \frac{1}{n}\sum_i \log(1 + \exp(y_i \boldsymbol{\theta}(\xx_i))),
\end{equation*}

\begin{lemma}
\label{lem:kernel-log-regression-easy}

Given sample $S = ((\xx_1, y_1), \ldots (\xx_n, y_n))$ and desired accuracy $\varepsilon'$ we can efficiently  find  $\hat{\boldsymbol{\theta}}$, s.t.
\begin{equation*}
\label{lem:rkhs-optimization-easy}
    \widehat{\mathcal{L}}_{\hat{\boldsymbol{\theta}}}(S) - \min_{\boldsymbol{\theta}} \widehat{\mathcal{L}}_{\boldsymbol{\theta}}(S) \leq \varepsilon'.
\end{equation*}
\end{lemma}
\begin{proof}
By the Representer Theorem, the optimal $\theta$ can be written as a linear combination
\begin{equation}
\label{eq:kernel-emp-loss}
    \hat{\theta}^* := \sum_i \gamma_i \Phi_{x_i}.
\end{equation}
Finding the minimizer of this form of the empirical loss function~\eqref{eq:kernel-emp-loss} is just a finite-dimensional convex problem
\begin{equation*}
    \min_{\gamma \in \bR^n} \sum_i \log(1 + \exp(y_i \langle \gamma, K_{i} \rangle)),
\end{equation*}
where $K_{i}$ is the $i$-th row of the kernel matrix  $K_{i,j} := K(x_i, x_j)$, and as such it can be solved efficiently.
\end{proof}


When the sample size is large enough, the standard Rademacher-complexity based considerations leads to a generalization bound. The proof is almost identical to the one of~\Cref{lem:rademacher-complexity-log-reg}. Instead of direct calculation for the Rademacher complexity of the class of all bounded linear forms, we need to show the corresponding bound for Rademacher complexity of the kernel classes. Variants of this lemma appear for example in~\cite{bartlett2002rademacher}.
\begin{lemma}
\label{lem:rademacher-kernel}
    If $\cH$ is an RKHS of functions over $U$ with kernel $K : U\times U \to \bR$, and $\mathcal{F} := \{ \boldsymbol{\theta} \in \cH : \|\boldsymbol{\theta}\|_{\cH} \leq B \}$, then
    \begin{equation*}
        \mathcal{R}_{n, \cD}(\mathcal{F}) \leq \sqrt{\E_{\xx\sim \cD} K(\xx, \xx)} B / \sqrt{n}.
    \end{equation*}
\end{lemma}
\begin{proof}
    For given $\xx$, let $\Phi_{\xx} \in \cH$ be such that $\langle \Phi_{\xx}, \theta\rangle_{\cH} = \boldsymbol{\theta}(\xx)\rangle_{\cH}$. Sampling $\xx_1, \ldots \xx_n$ i.i.d. from $\cD$ and, $\sigma_1, \ldots \sigma_n$ independent Rademacher random variables, we have
\begin{align*}
\E_{\xx, \sigma} \sup_{\boldsymbol{\theta}} \sum_i \sigma_i \boldsymbol{\theta}(x_i) & = \E_{x, \sigma} \sup_{\boldsymbol{\theta}} \langle \sum_i \sigma_i \Phi_{x_i}, \boldsymbol{\theta} \rangle \\
& \leq B \E_{x, \sigma} \|\sum_i \sigma_i\Phi_{x_i}\|_{\cH}
\end{align*}
    By Jensen inequality
    \begin{equation*}
        \E_{\xx, \sigma} \|\sum_i \sigma_i\Phi_{x_i}\|_{\cH} \leq \sqrt{\E_{\xx, \sigma}\|\sum_i \sigma_i \Phi_{\xx_i}\|^2} = \sqrt{\E_{\xx} \sum_i \langle \Phi_{\xx_i}, \Phi_{\xx_i}\rangle} = \sqrt{n} \sqrt{\E_{\xx \sim \cD} K(\xx, \xx)}.
    \end{equation*}
    Combining those two we get
    \begin{equation*}
        \frac{1}{n}\E_{\xx, \sigma} \sup_{\theta} \sum_i \sigma_i \theta(x_i) \leq \frac{B}{\sqrt{n}} \sqrt{\E_{\xx \sim \cD} K(\xx, \xx)}.
    \end{equation*}
\end{proof}

The Rademacher complexity bound above, as usual, implies the following uniform bound for the error between population loss an empirical loss of the kernel logistic regression classifier. 

\begin{lemma}
\label{lem:kernel-log-regression-generalization}
    Let $\cH$ be a RKHS with kernel $K$, and assume that $\E_{\xx} K(\xx,\xx) \leq R^2$. Then when $n \gtrsim R/\varepsilon^2$, with probability at least $9/10$ with respect to random sample $S$ we have
    \begin{equation*}
        \sup_{\|\boldsymbol{\theta}\|_{\cH} \leq B} |\mathcal{L}_{\boldsymbol{\theta}}(S) - \widehat{\mathcal{L}}_{\boldsymbol{\theta}}(S)| \leq \varepsilon.
    \end{equation*}
\end{lemma}
\begin{proof}
Combining~\Cref{lem:rademacher-kernel} and~\Cref{lem:rademacher-y}, together with the observation that $t \mapsto \log(1 + \exp(t + s))$ is a 1-Lipschitz function, we get that for an RKHS $\cH$, the family $\{ (\xx, y) \mapsto \ell_{\boldsymbol{\theta}}(\xx, y) : \theta \in \cH, \|\boldsymbol{\theta}\|_{\cH} \leq B\}$ has Rademacher complexity at most $O(B\sqrt{\E_{\xx \sim \cD} K(\xx, \xx)} / \sqrt{n})$, where $\ell_{\boldsymbol{\theta}}(\xx, y) := \log (1 + \exp(\boldsymbol{\theta}(\xx))).$ 
This, together with~\Cref{thm:rademacher-generalization-bound} and Markov inequality completes the proof of~\Cref{lem:rademacher-complexity-log-reg}.
\end{proof}


As usual, those two lemmas together give the concrete regret bound for the kernel logistic regression

\begin{proof}[Proof of \Cref{lem:kernel-logistic-regression-regret-bound}]
We will invoke~\Cref{lem:kernel-log-regression-easy} and~\Cref{lem:kernel-log-regression-generalization} with $\varepsilon' := \varepsilon/3$

We can decompose
\begin{align*}
      \cL_{\hat{\theta}}(\cD_{\theta^*}) - \cL_{\theta^*}(\cD_{\theta^*}) & = (\cL_{\hat{\theta}}(\cD_{\theta^*}) - \widehat{\cL}_{\hat{\theta}}(S) + (\widehat{\cL}_{\hat{\theta}}(S) \\
      & - \widehat{\cL}_{\theta^*}(S)) + (\widehat{\cL}_{\hat{\theta}}(S) - \cL_{\theta^*}(\cD_{\theta^*})).
\end{align*}
By~\Cref{lem:kernel-log-regression-generalization}, the first and the last summand here are each at most $\varepsilon'$. On the other hand, by~\Cref{lem:kernel-log-regression-easy}, we have $\widehat{\cL}_{\hat{\theta}}(S) - \widehat{\cL}_{\theta^*}(S) \leq \varepsilon'$, leading to
\begin{equation*}
    \cL_{\hat{\theta}}(\cD_{\theta^*}) - \cL_{\theta^*}(\cD_{\theta^*}) \leq 3 \varepsilon' \leq \varepsilon.
\end{equation*}
\end{proof}

\subsection{Proof~of~\Cref{thm:covariate-shift-kernel-logistic-regression} \label{sec:covariate-shift-kernel-logistic-regression}}
\begin{proof}

With~\Cref{lem:kernel-logistic-regression-regret-bound}, this proof is identical to the proof of~\Cref{thm:logistic-regression-result}.

Consider a distribution $\cD$ given by sampling $y \in \{\pm 1\}$, and then $x$ from $\ptr$ if $y=1$ and $x$ from $\pte$ if $y=-1$.

We can apply the kernel logistic regression to a random sample from $\cD$~--- according to~\Cref{thm:covariate-shift-kernel-logistic-regression}, we will be able to find a function $\widehat{\theta} \in \cH$ with bounded regret with respect to distribution $\cD$. According to~\Cref{lem:population-regret-is-kl}, this $\widehat{\theta}$ is of form $\ln(\widehat{\pte}/\widehat{\ptr})$ for some $\widehat{\ptr}$ and $\widehat{\pte}$ both $(\varepsilon')^2$-close in KL-divergence to $\ptr$ and $\pte$ respectively. Using now Pinsker we get the TV-distance bounds on $d_{TV}(\widehat{\ptr}, \ptr) \leq \varepsilon'$ and $d_{TV}(\widehat{\pte}, \pte) \leq \varepsilon'$. The bound $R_2(\ptr || \pte) \leq B_2$, together with Markov inequality implies that $\Pr_{\xx \sim \pte}[\pte(\xx)/\ptr(\xx) > B] \leq \varepsilon$ for $B = B_2/\varepsilon$. 

We can now apply~\Cref{thm:shift-TV} with this $B$ to deduce that we can use the truncated version of $\exp(\theta(x))$ as importance weights to solve the covariate shift problem; to this end we need $\varepsilon' = \varepsilon/B = \varepsilon^2 / B_2$; so the desired regret error in~\Cref{lem:kernel-logistic-regression-regret-bound} is of order~$\varepsilon^4/B_2^2$, and according to this lemma we can get it with $O(D B_1^2 B_2^4 /\varepsilon^8)$ samples.
\end{proof}
