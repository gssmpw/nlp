 %\section{Extended preliminaries}


\begin{definition}[Distance Measure]\label{def:Q}
For any probability distributions $p,q,\widehat{p}$, define 
\begin{equation*}
    \Qdiv{1}{\hat{p}}{p}{q} := \E_{\xx\sim q} \left|\frac{p(\xx)}{\widehat{p}(\xx)} - 1 \right|,
\end{equation*}
and more generally
\begin{equation*}
    \Qdiv{s}{\widehat{p}}{p}{q} := \left(\E_{\xx\sim q} \left|\frac{p(\xx)}{\widehat{p}(\xx)} - 1\right|^s \right)^{1/s}.
\end{equation*}
\end{definition}
By standard norm comparison for any $1\leq t_1 \leq t_2$, $Q_{t_1} \leq Q_{t_2}$. 

\begin{definition}[KL Divergence]
For any two probability distributions $p$ and $q$, the KL-divergence, $D_{KL}(p \, || \, q)$, between $p$ and $q$ is defined as
\begin{equation*}
    D_{KL}(p \, || \, q) := \E_{\xx\sim p} \ln \frac{ p(\xx)}{q(\xx)}.
\end{equation*}
    
\end{definition}

\begin{definition}[TV Distance] 
For a pair $p, q$ of probability distributions that are absolutely continuous with respect to some common measure, define the total variation distance between them as,
\[
d_{TV}(p, q) := \int |p(\xx) - q(\xx)|\, \mathrm{d}\xx
\]
    
\end{definition}

\begin{theorem}[Pinsker's Inequality]
\label{thm:pinsker}
    The famous Pinsker inequality sates that for any pair $p, q$ of probability distributions that are absolutely continuous with respect to some common measure, $d_{TV}(p, q) \leq \sqrt{2 D_{KL}(p\, || \, q)}$.
\end{theorem}

\subsection*{Gaussian Random Variables}
The following are well-known results about learning $d$-dimensional Gaussian random variables. Let $\xx_1,\cdots,\xx_{2m}\sim \cN(\boldsymbol{\mu},\boldsymbol{\Sigma})$ and define,
    \begin{equation}\label{eq:samples}
          \widehat{\boldsymbol{\mu}} = \frac{1}{m}\sum_{i\in[m]}\xx_i, \quad \widehat{\boldsymbol{\Sigma}} = \frac{1}{2m}\sum_{i\in [m]}(\xx_{2i}-\xx_{2i-1})(\xx_{2i}-\xx_{2i-1})^{\top}.
    \end{equation}
  

\begin{lemma}[TV Learning Gaussians, Theorem C.1,~\cite{ashtiani2020near}]
    There exists an absolute constant $C$ such that if we take $2m = 2C(d^2+d\log(1/\delta))/\epsilon^2$ samples $\xx_1,\cdots,\xx_{2m}\sim \cN(\boldsymbol{\mu},\boldsymbol{\Sigma})$, then for $  \widehat{\boldsymbol{\mu}}, \widehat{\boldsymbol{\Sigma}}$ as defined in Eq.~\eqref{eq:samples}, with probability $1-\delta$,
\[
d_{TV}\left(\cN(\boldsymbol{\mu},\boldsymbol{\Sigma}),\cN(\widehat{\boldsymbol{\mu}},\widehat{\boldsymbol{\Sigma}})\right) \leq \epsilon/2.
    \]
\end{lemma}



\begin{lemma}[Estimating Mean of Gaussians, Lemma C.2~\cite{ashtiani2020near}]\label{lem:MeanGauss}
    If $m\geq (2d + 6\sqrt{d\log(2/\delta)})/\epsilon^2$, then we have for $\widehat{\boldsymbol{\mu}}$ as defined in Eq.~\eqref{eq:samples},
    \[
    \Pr\left[(\widehat{\boldsymbol{\mu}}-\boldsymbol{\mu})^{\top}\boldsymbol{\Sigma}^{-1}(\widehat{\boldsymbol{\mu}}-\boldsymbol{\mu}) \geq \frac{\epsilon^2}{2}\right] \leq \frac{\delta}{2}.
    \]
\end{lemma}

\begin{lemma}[Estimating the Covariance of Gaussians, Lemma C.3~\cite{ashtiani2020near}]\label{lem:CovGauss}
    There exists an absolute constant $C$ such that if $m \geq C(d^2+d\log(1/\delta))/\epsilon^2$, then for $  \widehat{\boldsymbol{\Sigma}}$ as defined in Eq.~\eqref{eq:samples} with probability at least $1-\delta/2$,
    \[
\left\|\boldsymbol{\Sigma}^{-1/2}\widehat{\boldsymbol{\Sigma}}\boldsymbol{\Sigma}^{-1/2} -\II\right\|_{op}\leq \frac{\epsilon}{\sqrt{2d}}.
    \]
\end{lemma}

Note that since for any positive semi-definite matrix $\AA$, it holds that $\|\AA\|_F\leq \|\AA\|_{op}\cdot \sqrt{d}$, the above guarantee is stronger than the standard on the Frobenius norms of the covariance matrix.


\subsection*{Subexponential Random Variables}

\begin{definition}[Subesponential Random Variables]
    We say that a random variable $Z$ over $\bR$ is $(\sigma^2, B)$-subexponential, if and only if for every $\lambda \leq 1/B$, $\E \exp(\lambda (Z - \E Z)) \leq \exp(\lambda^2\sigma^2/2)$.
\end{definition}
The following fact is easy to derive directly from the definition of subexponential random variables. 
\begin{fact}
    If $Z_1, \ldots Z_n$ are $(\sigma_i, B_i)$-subexponential independent random variables, then $Z = \sum_i Z_i$ is $(\sigma, B)$-subexponential, where $\sigma = \sqrt{\sum \sigma_i^2}$, and $B = \max_i(B_i)$.
\end{fact}
The following are well known facts about Gaussian random variables.
\begin{fact}\label{fact:quad-gauss}
    If $Z = \mathcal{N}(\mu, 1)$, then $Z^2$ is $(16, 1/4)$-subexponential. Further, if $Z$ is a Gaussian vector $Z \sim \cN(\boldsymbol{\mu}, \boldsymbol{\Sigma})$, with $\|\boldsymbol{\Sigma}\|_{op}\leq O(1)$, and $A$ is a symmetric matrix, then $Z^T A Z$ is $(O(\|A\|_F), O(\|A\|_{op}))$-subgaussian, where $\|A\|_{op}$ is the largest eigenvalue of $A$.
\end{fact}


\subsection*{Rademacher complexity}
\begin{definition}
Rademacher complexity of a family $\mathcal{F}$ of functions $\Theta \to \bR$, with respect to the distribution $\cD$ is defined as
\begin{equation*}
    \mathcal{R}_{n, \cD} (\mathcal{F}) := \E_{\xx_1, \ldots \xx_n \sim \cD} \E_{\sigma_1, \ldots \sigma_n} \frac{1}{n} \sup_{f \in \mathcal{F}} \sum_i \sigma_i f(\xx_i),
\end{equation*}
where $\sigma_1, \ldots \sigma_n$ are independent $\{\pm 1\}$ Rademacher random variables.
\end{definition}
We will repeatedly use that left-composition of the entire function class with a single univariate Lipschitz function does not increase the Rademacher complexity. The following is a corollary of Lemma 5 in \cite{meir2003generalization}, see also Theorem 4.12 in \cite{ledoux2013probability}.
\begin{theorem}
\label{thm:rademacher-lipschitz-composition}
If $\ell : \bR \to \bR$ is a 1-Lipschitz function,  $\mathcal{F}'$ is arbitrary family of functions from $\Theta$ to $\bR$, $\cD$ is a distribution on $\Theta$ and $\mathcal{F} := \{ \ell \circ f : f \in \mathcal{F}'\}$, then
\begin{equation*}
    \mathcal{R}_{n, \cD}(\cF) \leq \mathcal{R}_{n, \cD}(\cF').
\end{equation*}
\end{theorem}

The main reason Rademacher complexity is useful is to provide generalization bounds: using a finite sample, we can estimate simultanously expectation of all functions in the class with bounded Rademacher complexity.
\begin{theorem}
\label{thm:rademacher-generalization-bound}
    Let $\xx_1, \ldots \xx_n\sim \cD$ be a sequence of i.i.d. random variables, and let $\mathcal{F}$ be a family of functions. Then
    \begin{equation}
        \E_{\xx} \sup_{f \in \mathcal{F}} \left|\frac{1}{n} \sum_{i \leq n}f(\xx_i) - \E_{\xx \sim \cD} f(\xx)\right| \leq 2 \mathcal{R}_{n, \cD}(\cF). \label{eq:rademacher-gen-bound}
    \end{equation}
\end{theorem}
Note that if $\mathcal{F}$ is in addition a family of functions bounded by $c$, a stronger concentration guarantees are known for~\eqref{eq:rademacher-gen-bound}. That is, with probability $1-\delta$, we have
\begin{equation*}
    \sup_{f \in \mathcal{F}} \left|\frac{1}{n} \sum_{i \leq n}f(\xx_i) - \E_{\xx} f(\xx)\right| \lesssim \mathcal{R}_{n, \cD}(\cF) + \frac{c \sqrt{\log (1/\delta)}}{n},
\end{equation*}
but we will not be using this bound explicitly.



\subsection*{Concentration of Random Variables}

\begin{lemma}[Hoeffding's Inequality]\label{lem:hoeff}
    Let $X_1,\cdots, X_n$ be independent random variables such that $a\leq X_i\leq b$, and $\E[X_i] = \mu$ for all $i$.
Consider the average of these random variables, $\overline{X} = \frac{1}{n}\sum_i X_i$. Then, for all $t>0$,
\[
\Pr\left[|\overline{X} - \mu|\geq t\right] \leq 2\exp\left\{-\frac{2nt^2}{(b-a)^2}\right\}.
\]
\end{lemma}

