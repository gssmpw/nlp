%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{adjustbox}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{enumitem}


% Comment out this line in the camera-ready submission
%\linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Wisdom of the Crowds in Forecasting: Forecast Summarization for Supporting Future Event Prediction}


% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Anisha Saha$^1$
\and
Adam Jatowt$^2$\\
\affiliations
$^1$Max Planck Institute for Informatics, Saarland Informatics Campus\\
$^2$University of Innsbruck\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation\\
\emails
ansaha@mpi-inf.mpg.de,
adam.jatowt@uibk.ac.at
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
Future Event Prediction (FEP) is an essential activity whose demand and application range across multiple domains. While traditional methods like simulations, predictive and time-series forecasting have demonstrated promising outcomes, their application in forecasting complex events is not entirely reliable due to the inability of numerical data to accurately capture the semantic information related to events. One forecasting way is to gather and aggregate collective opinions on the future to make predictions as cumulative perspectives carry the potential to help estimating the likelihood of upcoming events. In this work, we organize the existing research and frameworks that aim to support future event prediction based on crowd wisdom through aggregating individual forecasts. We discuss the challenges involved, available datasets, as well as the scope of improvement and future research directions for this task. We also introduce a novel data model to represent individual forecast statements.
\end{abstract}

\section{Introduction}

The ability to correctly predict the future is of utmost importance to foresee and plan for likely outcomes. Not only does it allow people to take necessary measures, but also concerned authorities can formulate required policies and make informed decisions. 
%For example, a company's business strategy and profits depend largely on how competent they are at predicting future trends. 
% Examples include forecasting the weather \cite{singh2019weather}, stock-market trends \cite{park2022stock}, geo-political unrests \cite{mellers2023human}, resource crises \cite{parolin2020forecasting} or a pandemic outbreak \cite{ma2022hierarchical} like COVID-19 
Examples include forecasting the weather, stock-market trends, geo-political unrests, resource crises or pandemic outbreaks like COVID-19.
%has proven to be of great social and economic importance over the years. 
%Future can be predicted by either analyzing the current situation and the latest trends or by examining and extrapolating the past, usually by finding previous occurrences similar to the present ones. 
However, understandably, accurately forecasting the future is an extremely challenging task owing to its inherent uncertainty. While the reasoning capability and the acquired experience give humans an edge over machines when it comes to forecasting likely events, the vast amount of potentially relevant information requires computational approaches. With the recent advancement in artificial intelligence and large language models in particular, automated forecasting systems with reasoning abilities matching those of humans or beyond are not unimaginable.

There are many ways in which future forecasting could be realized.  Often, approaches essentially rely on the history of a phenomenon such as past records of stock prices, previous weather conditions over time, etc.
For example, time series-based prediction is commonly done in weather forecasting, stock markets trend analysis like options trading and in pricing. Scholars also use simulation techniques like Monte Carlo \cite{mooney1997monte} to model a sequence of events occurring in time. 
Starting from ancient times when predictions were based on trajectories of celestial bodies or astrology, the approaches to anticipate occurrence of events have slowly progressed to more refined methods like scientific modeling and advanced computational approaches. 
There is even a field of studies called 
Futurology or Future Science that aims to study past and present trends for forecasting future scenarios. 
%The term "Futurology" was coined by Ossip K. Flechtheim, a German professor with the vision of a new probabilistic branch of knowledge. 
%The interdisciplinary nature of the subject arises from its application across diverse domains including business, economics, technology, climate and the society at large. 



In real life, people often resort to the opinions of others for future forecasting, such as friends, relatives or professionals to leverage collective wisdom and compare diverse perspectives. Collective judgment and verdict given by a group of people is often more reliable and accurate compared to that of an individual. ``Wisdom of the Crowd" in the forecasting context refers to the opinions and expectations that people share about a certain future event or state.
%, as shown in Figure \ref{fig:1}. 
%\cite{surowiecki2005wisdom} leverages this concept to mitigate any errors caused due to singular inherent bias or misinformation, by aggregating individual viewpoints and averaging out diverse perspectives. 
%It takes advantage of the contextual understanding ability of humans that may not be captured by purely data-driven approaches. 
On the web, one can find abundant information related to future such as forecasts, opinions, discussions related to the future, etc. With the advent of social media, this data is more easily accessible than ever, amplifying the reach of public opinion concerning the future. Due to the relative abundance of future-related data available online, researchers started investigating approaches to aggregate individual predictions for formulating reliable forecasts. The underlying intuition is that the more often a similar event is predicted by different users, the more likely this event is to happen.

Our survey outlines the computational approaches aiming to harness the wisdom of the crowd for future prediction. We overview the concept of \emph{Future Event Prediction based on Crowd Wisdom} (\emph{FEP-CW}), which we also call \emph{Forecast Aggregation} or \emph{Forecast Summarization}, the types and sources of data collected for this specific task, the various approaches undertaken by researchers to extract temporal information from text, different methods of aggregating future-related information and finally the techniques applied to formulate and visualize predictions.

%An event can be defined as a condition, fulfilling three key characteristics: it happens in the real world, is associated with a geographical location and a temporal specification \cite{das2017estimating}. 
%In the past, humans have tried to anticipate the occurrence of events in advance through a variety of ways. 
In total, 36 relevant papers were selected for our study through a keyword search in online publication databases followed by a careful analysis. We retained only those works that perform future forecasting by aggregating multiple future-related statements expressed in text. This means that researches that study patterns of historical events (e.g., frequencies, chronology, causality, etc.) to extrapolate them to the future were not included in our survey, same as works which utilize time series for making the predictions.  Our main criterion was that the relevant researches must utilize future-referring statements in text for producing the forecasts.

To our knowledge, this is the first survey on FEP-CW, despite the importance and the inherent complexity of this task, which surpasses the complexity of traditional multi-document summarization. Our focus is to provide a comprehensive deep-dive into the underlying concepts and techniques developed to target the problem of FEP-CW. Figure \ref{flow} briefly outlines the basic steps involved in FEP-CW, most of which will be discussed in detail in different sections of the paper.
%Our aim was to build a collection of papers on the wisdom of the crowd concept for FEP. We made sure that papers based on other methods like time-series forecasting, simulations, etc, were filtered out from the final list. 
Additionally, we also provide in Sec. 5 a novel data model of future-related statements useful to single out core elements that matter for the FEP-CW task. We finally list in Sec. 6 multiple different avenues for further research.

%\begin{figure}[!tbp]
 % \centering
 %   \includegraphics[width=0.3\textwidth]{FEPCW.jpg}
 %   \caption{Future Event Prediction Based on Crowd Wisdom} \label{fig:1}
%\end{figure}  

%\subsection{Related Surveys and Areas}\label{related}
%As for the related works, surveys like \cite{gmati2019taxonomy} and \cite{zhao2021event} overview more mathematically oriented methods of event, time, and location forecasting. There also exist surveys that are either specific application-based prediction surveys like weather \cite{cloke2009ensemble,doswell1993tornado}, civil unrest \cite{ramakrishnan2014beating} or ones that utilize data from a particular domain like Twitter for analyzing whether sentiments extracted from tweets improve forecasts of social, commercial or economic indicators \cite{arias2014forecasting}, the pros and cons of social media for event forecasting \cite{phillips2017using} and the realms of future human-related events that can be predicted from the opinions expressed in social media platforms \cite{yu2012survey}. However, none of those works approach reviewing the methods of harnessing the knowledge of the crowd. We also put forward the proposal for a data model of an individual future-related statement and single out the components that are crucial to predicting a future event. So far, none of the works in this area have highlighted the utility of such a data model.

%FEP-CW shares similarities with Multi-Document Summarization (MDS) \cite{cui2021topic,zhou2021entity,christensen2013towards,celikyilmaz2010hybrid}, while having some key differences. In FEP-CW, the goal is to \textit{gather future-related statements} from multiple sources, \textit{aggregate them}, and \textit{obtain a concise summary of possible future events}, a process akin to MDS. However, FEP-CW also entails \textit{conflicting expert opinions and viewpoints}. Resolving these divergent views is an extra step over MDS which assures relevant predictions. While understanding the text semantics is of utmost importance in both tasks, FEP-CW additionally requires \textit{temporal interpretation of information} to make credible forecasts. On top of that, it places more stress on leveraging recent information, unlike MDS, whose aim is to aggregate the majority of the information available into a concise snippet. Lastly, while MDS is solely based on the available evidence, FEP-CW is characterized by \textit{varying levels of uncertainty associated with individual predictions}. Some predictions might be backed by direct implications of available information, others might be more speculative in nature, lacking the sureness quotient.

\begin{figure}
    \includegraphics[width=0.45\textwidth]{Flow_diagram_2_cropped.pdf}
    \caption{General Flow of Predicting Future Event} \label{flow}
\end{figure}

% \begin{table*}
% \centering
% \small
% % \adjustbox{max width=\textwidth}{
% \vskip\baselineskip 
% \begin{tabular}{|p{2cm}|p{1.5cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
% % \begin{tabular}{|p|p|p|p|p|p|}
% \hline
% \textbf{Dataset} & \textbf{Language(s)} & \textbf{Contents} & \textbf{Count} & \textbf{Source} & \textbf{Format}\\
% \hline
% \cite{jatowt2011extracting} & English & News & 3.6M & & \\
% \hline
% \cite{dias2011future} & English & Web contents & 508 & & \\
% \hline
% \cite{jatowt2013multi} & English (EN), Japanese (JP), Polish (PL) & Web contents, News & 1.04M (EN Web), 2.99M (JP Web), 714K (PL Web), 196K (EN News) \\
% \hline
% \cite{hurriyetouglu2013estimating} & Dutch & Twitter & 94,285 \\
% \hline
% \cite{hu2017happens} & Chinese & News & 155,358 \\
% \hline
% \cite{goyal2019embedding} & English & News & 5,000 \\
% \hline
% \textbf{FORECASTQA} \cite{jin2020forecastqa} & English & News & 10,392 \\
% \hline
% \textbf{Autocast} \cite{zou2022forecasting} & English & Articles on Economy, Politics, Science, Social, and Others from forecasting tournaments. & 6,707 \\
% \hline
% \textbf{IntervalQA} \cite{zou2022forecasting} & English & Generic articles & 30K \\
% \hline
% \textbf{ExpTime} \cite{yuan2023back} & English & Compilation of event forecasting datasets & 26K \\
% \hline
% \cite{regev2024future} & English & News & 6,800 \\
% \hline
% \end{tabular}
% \caption{The datasets used in crowdsource-based future event prediction research.}
% \label{table:1}
% \end{table*}

\begin{table*}
    \centering
    \adjustbox{max width=\textwidth}{%
    \begin{tabular}{lrrrrr}
        \toprule
        Dataset & Types  & Language(s) & Structured & Source & Count \\
        \midrule
        \textbf{\cite{jatowt2011extracting}} & News  & English &  No & Google News Archive & 3.6M \\
        \textbf{\cite{dias2011future}} & Web Contents & English & Yes & Web & 508 \\
        \textbf{\cite{jatowt2013multi}} & News/Web Contents & Multilingual & Yes & Web & 1,436K \\
        \textbf{\cite{hurriyetouglu2013estimating}} & Tweets & Dutch & Yes &  Twitter & 94,285 \\
        \textbf{\cite{hu2017happens}} & News & Chinese & Yes & Sina News & 155,358 \\
        \textbf{\cite{goyal2019embedding}} & News  & English &  Yes & Gigaword & 5,000 \\
        FORECASTQA \textbf{\cite{jin2020forecastqa}} &  News & English & Yes  &  LexisNexis & 10,392\\
        Autocast \textbf{\cite{zou2022forecasting}} & News Articles & English & Yes & Public forecasting tournaments &  6,707 \\
        IntervalQA \textbf{\cite{zou2022forecasting}} & News Articles & English & Yes & Various NLP datasets & 30k \\
        ExpTime \textbf{\cite{yuan2023back}} & News & English & Yes&Compilation of event forecasting datasets &26K \\
        \textbf{\cite{regev2024future}} & News& English
                &     Yes     &Longbets, Horizons, ChatGPT, New York Times &   6,800     \\
        \textbf{\cite{mutschlechner2025analyzing}} & News & English & Yes & Metaculus, Google News & 614 \\
        \bottomrule
    \end{tabular}}
    \caption{The datasets used in crowdsource-based future event prediction research.}
    %\label{tab:booktabs}
    \label{table:1}
\end{table*}

\section{Data and Sources} \label{data}
 In this section we discuss used document genres and datasets created for FEP-CW  task. We summarize the relevant datasets in Table \ref{table:1}. Most of FEP-CW methods leverage news articles, tweets and web pages. Some approaches make use of dedicated websites that collect predictions from diverse sources or invite users to express their opinions or cast votes. Commonly used forecasting websites like \textit{Sigma Scan}\footnote{http://www.sigmascan.org/}, \textit{Metaculus}\footnote{https://www.metaculus.com/} and \textit{Future Timeline}\footnote{http://seikatsusoken.jp/futuretimeline/} contain future information and predictions collected from news articles, research papers, government reports or through community activity.



%These are further filtered on attributes like topic, event timeline, presence or absence of explicit temporal expressions, etc.  
%Table \ref{table:1} outlines the metadata of the major datasets that use the wisdom of the crowd for future event prediction.

The majority of works that perform FEP-CW opt however for news articles as their data source, likely due to their availability and up-to-date information contained about events happening around the world. An investigation by \cite{kanhabua2011ranking} revealed that nearly one-third of sentences in news articles consist of some kind of reference to the future. News articles have been then commonly used for creating datasets for FEP-CW. Additionally, some researchers \cite{nakajima2014investigation,radinsky2013mining,nakajima2020future} advocate directly retrieving Web content instead of relying on any prepared datasets or collections.
%The amount of information available across various time frames help in training a predictive model on past details and validating its predictions for an event for a later time stamp, whose updated status has already been published in the news.  

We discuss below a few of the commonly used dataset formats. In the FORECASTQA dataset \cite{jin2020forecastqa}, English news articles are converted to ${<Question, Answer, Timestamp>}$ triples for answering binary and multiple-choice forecasting questions. However, FORECASTQA suffers from ambiguity and the lack of context, owing to it being crowdsourced. The unstructured nature of this dataset requires models to additionally link relevant events to answer forecasting questions. Addressing these challenges, \cite{zou2022forecasting} released the Autocast (forecasting questions of type True/False, Multiple-Choice or Numerical) and IntervalQA (questions having numerical answers only) datasets. Recently, \cite{mutschlechner2025analyzing} curated a binary forecasting questions dataset to evaluate performance of LLMs. \cite{goyal2019embedding} built a dataset containing temporally ordered event pairs - $<sentence1> <sentence2> <event1> <event2> <temporal relation>$. The TimeLlaMA \cite{yuan2023back} model for explainable event prediction is based on a multi-source instruction tuning dataset, ExpTime with documents, forecasting questions, predictions and explanations for these predictions. \cite{regev2024future} built a dataset containing manually labeled future and non-future related sentences. Besides English news and tweets, some works \cite{hu2017happens,jatowt2015mapping,hurriyetouglu2013estimating,tops2013predicting,jatowt2013multi} harvested multilingual data. 

% Some works like \cite{dias2011future}, \cite{jatowt2010analyzing} and \cite{jatowt2009supporting} choose to focus on the Web, collecting snippets containing temporal expressions and future mentions, rather than limiting information from just news and tweets. For example, \cite{radinsky2008predicting} mine Google Trends to gather training data which includes the history of 5 years of user search queries. 

%While most of the works focus on English news or tweets, few tasks have been accomplished on datasets in other languages like Chinese, Japanese, Dutch, Polish, etc. \cite{hu2017happens} crawls a Chinese news event dataset containing 15,254 news series from Sina News while \cite{jatowt2015mapping} collects both English and Japanese tweets with explicit temporal references, structuring them as \textit{<Users, Time-referring messages, Subjects>}. \cite{hurriyetouglu2013estimating} and \cite{tops2013predicting} specifically harvested tweets from a Dutch database.
%Datasets like \cite{jatowt2013multi} contain both English and non-English (Japanese and Polish) contents queried from Bing web and news search.

Curating datasets for FEP-CW involves often extensive preprocessing, e.g., with commonly available toolkits like OpenNLP for sentence splitting, tokenization, part-of-speech tagging, and shallow parsing, TARSQI Toolkit for annotating documents with TimeML, SuperSense for named-entity recognition, etc. Documents retrieved from the Web may need to be also filtered out for removing duplicates. 

An important problem with FEP-CW datasets is that they can quickly become obsolete. Not only the patterns of events might change over time causing models trained on outdated datasets exhibit lower accuracy, but there is a risk of future information leakage. Expecially, when it comes to Large Language Models (LLMs), their training cutoff dates need to be carefully compared against the dates of events to be forecasted to minimize the risk of models already knowing the results of these events \cite{mutschlechner2025analyzing}.
Otherwise, the evaluation would not be fair.  %Questions about future events become useless after the resolution dates of these events, as the outcomes become known on the Web or within large language models (LLMs) trained afterwards. A related challenge lies in the fact that 
In fact, accumulating future-related content could be considered as an integral part of the task as in some working systems \cite{regev2024future}. This however makes scientific reproducibility more challenging.

% While some works have built a refined dataset of their own, others like \cite{
% radinsky2012learning,nakajima2014investigation,radinsky2013mining,radinsky2008predicting,nakajima2020future} just retrieve Web content to suit their use case.

\section{Extracting future-related information} \label{temporal}
%Information retrieval involves retrieving documents or text from documents and databases which can be achieved by supplying user queries, specific questions, particular dates or a timeline, etc. 
Usually, documents are not purely about the future, but rather future-related content is mixed with the one referring to the past or present. For example, in news articles, statements about planned events or forthcoming course of actions appear together with the content reporting ongoing or recent events.
While the forecast summarization can, in principle, be done on entire documents (e.g., by prompting LLMs to focus on future information only), often a common approach is to first extract future-related statements to be later used in aggregation and summarization. 
%Content referring to the future is necessary to be first discovered, extracted and collected. %for forecasting the unfolding of future events. They provide the relevant context and background knowledge about a future event in question. 

A simple way to collect relevant data is by using standard search techniques by issuing pre-specified sets of keywords.
In general, conventional search engines have been used for time-oriented ranking since already some time ago \cite{DBLP:conf/cikm/AlonsoGB09,jatowt2005temporal,campos2017identifying,dias2011future}, and this also applies to ranking results so that future-related ones get returned at the top of search results list. \cite{baeza2005searching} was one of the first to introduce the concept of a search engine for extracting future temporal expressions from news articles. They investigated almost half a million future-referring sentences from Google News and inferred that the closer the event forecasted in the prediction is to the prediction's creation time, the higher is the confidence and accuracy of the prediction. 


%Below is an example by \cite{nakajima2018future} of how future-referring sentences support answering questions about the future. 

% \vskip0.1cm
%\textit{Question}: Predict whether in 2020 nursing care fee will be applying AI in Care Planning.

% \vskip0.15cm
%\textit{Options}:
%\begin{enumerate}[label=(\roman*)]
%    \item AI will be applied in Care Planning.
 %   \item The situation will not change.
  %  \item Care Planning will be created using new method other than AI.
%\end{enumerate}

%Example relevant sentences that refer to future:
%\begin{enumerate}
 %   \item The government plans to revise the nursing care fee in future.
  %  \item The government announced that it would develop the legal system and rules for society with AI.
   % \item A model in which people and artificial intelligence work together will be constructed.
%\end{enumerate}

%Based on $(1)$ and $(2)$, we can eliminate the options $(ii)$ and $(iii)$ respectively and $(3)$ supports the possibility of (i).
\subsection{Extraction based on Time Expressions} 

When the publication time of a document is known (as in news articles), it is usually easy to find future-related information if it is accompanied by explicit dates. Sentences containing embedded temporal expressions or references that point to the time later from the document publication date are then identified as future-referring instances wrt. the document. We call these \emph{per-document future-related statements}.
Whether they are actually future-related at a given inference time needs to be determined by comparing them to the time point of ``now". Per-document future-related statements that are no longer referring to the actual future  can however be still useful. For example, they can be used for training extraction models. 

 % To determine if an event will happen in the future, the first step involves extracting temporal expressions from the data.  
 %A simple way for collecting future related statements is by searching for sentences, paragraphs, or other text units that contain temporal expressions pointing to the future. 
 A temporal expression can be defined as a sequence of words that denote a point in time, a time interval or the frequency of occurrence of an event.  %Often there is a high connection between the reliability of an event and its temporal distance. 
 %To retrieve time-scoped information from unstructured text, 
 Temporal expressions after being detected in text may need to be normalized. There can be explicit, relative, or implicit temporal expressions. Explicit or absolute temporal expressions directly indicate a time point or an interval, for example, \textit{``1st April 2024"}. Relative temporal expressions refer indirectly to a point of time, by means of expressions like \textit{``two weeks later"}, \textit{"a month ago"}, etc. Implicit temporal expressions refer to events like, "The Asian Games" that signify a time event. 
 %Besides, temporal expressions can have different granularity levels like hours, days, months, seasons, years, etc. 
 %Detecting and extracting temporal expressions is a simple procedure in FEP-CW as it allows to segregate future events from past events using rules or regular expressions, enable chronological ordering of events and even support understanding causality between events. However, 
 
 We emphasize that due to the multiple senses of words and overall language ambiguity, extracting and normalizing time expressions is not always trivial and often needs to be done using specialized temporal taggers \cite{mani2000robust,strotgen2010extraction} if high accuracy is important. \cite{mani2005language} created the TimeML framework through annotations of temporal events, marking the commencement of frequent use of text data for understanding the underlying relations between time and events. 
 Built on top of the TimeML framework are commonly used temporal taggers like GUTime \cite{mani2000robust}, HeidelTime \cite{strotgen2010extraction}, Stanford CoreNLP tagger and SUTime. However, these taggers possess some limitations. \cite{regev2024future}, for example, reported that SUTime failed to perform in ambiguous cases. For instance, it maps \textit{"On Tuesday"} to the upcoming Tuesday, rather than the past Tuesday referred to in the sentence \textit{"On Tuesday, he revealed his decision, which will become official soon."}.  

%
% \subsubsection{Time-referenced events}
% This section deals with future event extraction methods which are structured around the concept of time, like dates, hours, months, years, etc. We discuss methods as early as temporal taggers to the latest time expressions proposed by researchers.

Simple patterns or regular expressions involving explicit temporal expressions can be used as an alternative to temporal taggers, especially when one wants to extract large number of future-related statements without high computational cost. \cite{jatowt2010analyzing} in their exploratory analysis collected over a million time-referenced future-related statements from the Web in English, Japanese and Polish by searching using structured queries with temporal expressions defined as \textit{$temp_{modifier}+(the)year(s)+yyyy$} (where, \textit{$temp_{modifier}$} is a preposition) for retrieval by years, which translates to expressions like \textit{“in year 2027”, “in the year 2035”, “by the year 2049”} and so on. Similarly, for retrieval by months, the expressions was based on \textit{“$month\_{reference}+yyyy$”} where \textit{$month\_{reference}$} is a full or shortened form of the name of a month (like, February or Feb). 

In a similar way, ChronoSeeker \cite{kawai2010chronoseeker} retrieves both past and future time-referenced events that are relevant to the user's query and filters noisy events using support vector machines. The system has five modules: search (a search API returns year expressions $y$ relevant to a user query $q$), extraction (extracts candidate sentences containing $y$), filtering (a machine-learning technique which filters these candidates), clustering (groups sentences related to the same event) and visualizations (timelines of chronological events).
 To analyze the impact of temporal features on the classification of different types of Web snippets, \cite{dias2011future} employed a pattern-matching methodology as proposed in \cite{campos2011temporal} which focuses on years. They investigated whether a unigram model combined with temporal features can classify future-related web snippets into one of the genres (informative, scheduled or rumours) for five different classifiers, only to conclude that depending on the learning algorithm temporal features might improve future-related text classification to a great extent. %The unavailability of a Japanese tagger led \cite{manning2014stanford} to build their own tagger by matching tokens in the tweet content with a dictionary of lexical expressions related to time in Japanese. A set of regular expressions was also defined to capture mentions of hours, days, weekdays, months, seasons, national holidays, and years. 

While the time expressions-based extraction is characterized by a relatively high precision, the main problem with this approach is low recall. This is because many future-related statements lacking explicit temporal pointers are omitted which causes bias in the data collection towards more established or concrete forecasts and plans.

\subsection{Extraction based on Future Indicating Keywords}
%Time-unreferenced events do not contain any explicit reference to time and there could be numerous ways to refer to the future. 
%This is also known to be the most common scenario in news collections. 
Forecasts with specific time markers may be for relatively nearer and more certain events in the future. The absence of an explicit mention of any date or even a vague time pointer makes it more difficult to retrieve future mentions. The implicit future-related statements need then to be found through other means.  

 One way is to look for the occurrence of certain markers of future-relatedness like \textit{``will"} or \textit{``future"}, although many other phrases like \textit{``...plans to"} or \textit{``...hope to"} might serve as well. 
%Below are some of the methods developed to query future events without explicit time references.
Searching with the keywords like exemplified above might yield some results but carries a high risk of returning more false-positive cases. For example, a sentence like \textit{"He has a strong will to pursue higher studies"} might be misleading. Additionally, the list of keywords needs to be extensive for this approach as there can be many different ways one can refer to the future, including the use of tenses other than the future tense.

\cite{kanazawa2011improving} proposes a simple technique to determine future orientation markers. %based on the intuition that the topics of a future-related content are discussed in future documents and similarly for past-referring sentences. To classify a future-referring sentence in a large corpus, 
They employ statistical test to detect characteristic terms that frequently appear in a large collection of future-related statements while being infrequent in the collection containing content related to the past. The two content collections are formed by relying on temporal expressions as discussed in the previous section.
Having determined characteristic terms that often appear in future-related sentences, they proceed to extracting implicit future-related statements.
The sentences are ranked based on the average weights of the characteristic terms in their content. A sentence that contains many highly-scored future characteristic terms and few highly-scored past characteristic terms is more likely to refer to a future event. 

A better way than the statistical analysis is to use trained neural network classifiers like DistilRoBERTa as proposed in \cite{regev2024future}. However, the problem lies in the fact that this approach is not very efficient when searching for large amounts of future-related content, but rather as a post-processing tool to filter out non-future-related content. 


\subsection{Extraction based on Morphological Patterns}
The complexity of natural language might mislead a model to perceive future information as a past reference. Consider the sentence, \textit{"Carol has decided to open a new bakery"}. Despite the use of past tense in \textit{"decided"}, the sentence indicates an upcoming event. To parse complex future expressions as this example, sentences can be represented in a morphosemantic structure \cite{levin2017morphology} which is a combination of semantic role labeling and morphological information.
%and can extract future reference sentences containing both explicit and critical cases of implicit expressions and context-dependent information. 

\cite{nakajima2014investigation} introduce a sentence extraction system \textit{SPEC} based on semantic role annotations for extracting frequent sentence patterns from a corpus. The approach is based on the intuition that there are multiple possible future expressions while some words or phrases are used as future expressions only in a specific context. Hence, there must exist characteristic patterns (grammatic or semantic) appearing in future-related sentences that distinguish them from non-future referring sentences. 
\cite{10.5555/2832747.2832885} investigates patterns in multiple news corpora, that are semantically and grammatically consistent, yet lexically different. They propose a method for automatically extracting such frequent patterns and establish its effectiveness on the downstream task of identifying future-related sentences. \cite{nakajima2020future} then use frequent combinations of patterns for classifying future-referring sentences in Japanese by using MeCab analyzer to extract morphological information for Japanese words. \cite{ni2015computational} and \cite{al2018automatic} apply similar approaches to analyze future-referring expressions in English and Arabic, respectively. 

\subsection{Extraction using Language Models}
LLMs have excelled recently in natural language understanding (contextual, semantic, etc.) and generation. 
As trained on huge text corpora, the models naturally   
%This information aids an LLM to 
observe a variety of ways in which opinions on future are expressed throughout diverse document genres (news articles, blogs, scientific papers, etc.). 
%In addition, LLMs can be used as a summarization tool for scrapped web pages. 
\cite{regev2024future} propose using encoder only language models, such as the DistilRoBERTa model \cite{liu2019roberta}, as classifiers to distinguish future and non-future referring sentences. The authors use 6,800 manually labeled sentences for training to extract positive sentences with a strict confidence threshold of 0.9. 
 In case of larger language models, special prompts need to be designed to elucidate information about the future. A study by \cite{yuan2023back} shows that directly prompting ChatGPT \cite{ouyang2022training} to identify future-related content leads to suboptimal accuracy. 
%Hence, LLMs provide multiple avenues for future event extraction. 
%It is essential to note the training cut-off dates of these LLMs to avoid data contamination.

%, given a user query about a topic or entity whose future needs to be analyzed.

% \begin{figure}
%     \centering
%     \includegraphics[scale = 0.35]{Flowchart.pdf}

    
%     \caption{Atomic Components of Individual Future Prediction that can be harnessed for FEP}
%     \label{fig:comp}
% \end{figure}

\subsection{Post-Filtering Past Future}
A potential subsequent step in the data collection/extraction process is to validate the predictions by ensuring they have not already occurred. The predictions that are no longer valid should be removed from the collection. This filtering step, often skipped, could play a crucial role in assuring the quality of generated forecast summaries.
\cite{kanazawa2011improving} proposed a simple two-stage method for estimating validity of the predictions by searching for relevant events that have already actually occurred (or their forecasted horizon have already passed) and calculating cosine similarity between the forecasts and the gathered events.
 
 Note that the above concept of the "already passed future" is different from the case when an earlier prediction gets invalidated or contradicted by a later one (e.g., a company officially canceling its previous plans). Those resolutions may need to be accounted for during the aggregation stage.

\section{Aggregating Forecasts} \label{event}
%Often, humans tend to analyze past events, aggregate insights, and make a highly probable assumption about the possibility of a future event.
% In making these predictions, people use widely available resources like the contents and news available over the internet, articles, blog posts, tweets, etc. 
%Determining probable global events in politics, economics, and business requires extensive common sense and knowledge, which are possessed by human experts belonging to the related domain. 
%To predict a future event, a model must possess the above capabilities, understand the present state and reason about the future state. 
%However, this approach might be quite challenging due to difficulty in collecting relevant data, potential misinformation, lack of understanding of the true cause driving an event, diversity in opinions about how the future could be expressed, conflicts between competing forecasts, amount of uncertainty expressed in forecasts, provided conditions for events to happen, lack of explanations or relevant dates of events or absence of any patterns in past information archives.
Event prediction methodologies vary due to the underlying heterogeneity of the prediction scope, which might range from the prediction of only a single attribute of an event like date, time, location, actors, etc. or their combination, to the prediction of an event itself or even a sequence of events. 
Additionally, the actual form of output might vary from a phrase, a sentence, a collection of sentences, binary answer (True or False) such as for questions whether an event will happen, a timeline, etc.  
Some works even formulate the problem of event forecasting as a problem of link prediction in knowledge graphs or temporal knowledge graphs \cite{ma2023context,lee2023temporal,ma2023structured,deng2020dynamic}. 

 

% \begin{figure}[!tbp]
%   \centering
%   % \begin{minipage}[b]{0.41\textwidth}
%     \includegraphics[width=0.5\textwidth]{Flowchart_cropped.pdf}
%     \caption{Atomic Components of Individual Future Prediction that can be harnessed for FEP-CW.} \label{comp}

%     % \hfill
%   % \begin{minipage}[b]{0.4\textwidth}
%   %   \includegraphics[width=\textwidth]{Eval_pie.png}
%   %   \caption{Distribution of evaluation metrics across papers.} \label{fig:eval-pie}
%   % \end{minipage}
% \end{figure}


\subsection{Frequency-based Future Analysis}
Certain approaches do not predict individual events but rather focus on presenting the overall statistics of future-related content such as frequency, average sentiment, common words, etc. \cite{jatowt2013multi} conduct a large-scale exploratory analysis of future-related information on the web studying their temporal horizons, common keywords, and sentiment degrees in three different languages. \cite{jatowt2015mapping} analyze a large collection of tweets to quantify temporal attention and related temporal characteristics expressed by users by representing time mentions as a probability distribution over a time duration. 

%\subsection{Prediction via probabilities}
% Some works do not resort to any kind of aggregation or summarization but simply determine individual predictions without performing ranking or clustering. These approaches are often probabilistic. 
%\cite{radinsky2012learning} leverage causal reasoning to generate possible future occurrences of events. They collect event pairs from news articles that are related through a causal relationship which results in a semantically structured causality graph containing 300 million fact nodes and over 1 billion edges. 
%The Pundit algorithm then applies semantic natural language modeling techniques and a vast amount of world knowledge ontologies to generate an abstraction tree that is used to predict causality between unseen events. \cite{radinsky2013mining} undertakes a probabilistic approach by building an inferential model of the form $P(ev_j(\tau + \Delta)|ev_i(\tau))$ for a future event $ev_j$ at time $\tau + \Delta$ and past event $ev_i$ happening at time $\tau$. This approach extracts storylines using topic tracking and detection algorithms, clusters similar texts together, enriches these storylines with information extracted from Web knowledge sources, estimates the above probability, and builds a real-time classifier that indicates the likelihood of a forthcoming event, following a past sequence of events. \cite{hu2017happens} develops an end-to-end model called  Context-aware Hierarchical Long Short-Term Memory (CH-LSTM) for future sub-event prediction, even if they do not exist in the training data. The model does this by encoding the word sequence for each subevent, the temporal order of sub-events, and the topic of past sub-events for generating context-aware predictions. The evaluation is based on the actual occurrence of the target event and the mean time between the prediction and occurrence. 

%Some of the works predict attributes of a future event like date, time, location, etc. instead of what the whole event would look like. \cite{radinsky2013mining} predicts numerical attributes like the number of deaths given an event concerning death is predicted by undertaking a binning approach and learning predictors that estimate a probability. \cite{tops2013predicting} undertakes a classifier-based approach to estimate when an event mentioned in a stream of Twitter micro texts is about to happen, by mapping unseen tweets onto discrete time segments (mainly into broad categories: ‘before’, ‘during’, and ‘after’). They validate the fact that it is comparatively difficult to predict the time of an event when it is far away, both for a classifier as well as for humans.  \cite{hurriyetouglu2013estimating} extends this approach for providing hourly forecasts by mapping clusters of tweets to time-to-event. \cite{jin2020forecastqa} poses the problem of future event forecasting as a restricted-domain multiple-choice question-answering task on large-scale unstructured data. Time is constrained such that the answer is not present with certainty in the available text, rather temporal understanding and commonsense are required to forecast it.

\subsection{Forecasting through Clustering}
Due to the lack of dedicated and annotated datasets, many approaches resort to unsupervised ways of forecast aggregation such as clustering.
%Clustering indicates grouping together related content, in this case, to summarize or forecast future events. 
The clusters could be formed based on semantic similarity of input forecasts, their temporal similarity, which typically denotes the agreement between the expected dates of predicted events, or a combination of both. \cite{jatowt2009supporting} clusters predictions related to input query using k-means containing future temporal expressions based on the mixture of content and time similarities and then selects the best clusters to summarize probable future events. The later post-clustering step is needed to discard semantically incoherent or temporally elongated clusters to eliminate noise. 

\cite{jatowt2011extracting} undertook a mixture-model-based clustering approach to estimate the probability of a future event by grouping related predictions. When a user queries a topic, a cluster is created based on the related events' textual and the forecast horizons are used to form a probability distribution over time. The authors use logarithmic timeline which collapses time further ahead mimicking human style to think about the future in which distant events are represented by coarser time granularity than near events. A more recent approach by \cite{regev2024future} uses BERTopic library to extract topics based on similarity of forecast embeddings.

\subsection{Forecasting through Ranking}

Ranking future predictions can be considered as another way of forecast aggregation and selection.
%plotting the predictions in a timeline. obtained from Web content
\cite{kanhabua2011ranking} proposes an approach for automatically generating queries from a news story read by a user to retrieve predictions related to that news article. 
The predictions are then ranked by their relevance (future information) and are returned to the user. The ranking algorithm is built on a support vector machine framework, based on features like term, entity, topic and temporal similarity which captures the correspondence between the generated query and predictions. This work is actually an example of an interesting application of FEP-CW as the trigger in this case is a document being read by a user unlike other approaches that use entities as their input. 

\subsection{Forecasting using LLMs}
%\cite{jin2020forecastqa} was the first to pose the problem of future event forecasting as a restricted-domain multiple-choice question-answering task on large-scale unstructured data. 
%Time is constrained in their approach such that the answer is not present with certainty in the available text, rather temporal understanding and commonsense are required to forecast it.
%Predictions based on questions...likelihood, binary, choice scope...
%Instruction: Tell me about the future, drawn future timeline..rationaly

With LLMs becoming very usefil, it is natural to study the effectiveness of LLMs in forecasting, effectively, using them as a kind of aggregation and reasoning tool. With this intention, \cite{schoenegger2023large} evaluated GPT-4's \cite{achiam2023gpt} capability of future prediction. The authors concluded that GPT-4’s forecasts are significantly less accurate compared to the median human-crowd forecasts.  They also hypothesized that a potential reason for the underperformance is that in making real-world forecasts the actual answers are unknown, unlike in other benchmark tasks where the strong performance of an LLM could be (at least partially) due to the answers being memorized from the training data. %Again, in contrast to humans, GPT-4 is not aware of current affairs due to its training cut-off date. 

\cite{mutschlechner2025analyzing} found out that supplementing background context in the form of related news articles or future-related question's resolution criteria combined with an appropriate prompt structure can improve the performance of even older LLMs like GPT-3.5 turbo.
%In future, to mitigate this issue, LLM forecasters could be provided access to the internet for updates information.
\cite{pernegger2024evaluating} investigated the performance of BERT \cite{devlin2019bertpretrainingdeepbidirectional}, RoBERTa \cite{liu2019robertarobustlyoptimizedbert} and GLM \cite{du2022glmgenerallanguagemodel} and concluded that zero-shot learning did not improve the language models' forecasting abilities and that retrieval-augmented generation (RAG) offers more chance for substantial improvement. Recently, \cite{zhang2024largelanguagemodelsevent} formulated the problem of multi-event forecasting as predicting occurrences of multiple unique relations in a temporal knowledge graph (TKG). They design a prompt template such that it does not exceed the maximum permitted token limit for open-source LLMs while including as much historical information as possible. Further, they utilize the pre-trained LLM, RoBERTa-large to encode this prompt and employ a self-attention-based prediction head to handle the output embeddings and predict the relation indicating the future event occurrence. \cite{nako2025navigatingtomorrowreliablyassessing} investigated the effectiveness of LLMs for likelihood questions by asking if an event is likely to happen instead of if it will happen. For better accuracy, the authors also created fake events probing how these will be predicted by LLMs.
%It would also be interesting to investigate the integration of the wisdom of the crowd with LLM forecasts.
\vskip \baselineskip



\begin{figure}[!tbp]
  \centering
  % \begin{minipage}[b]{0.41\textwidth}
    \includegraphics[width=0.4\textwidth]{Flowchart_cropped.pdf}
    \caption{Atomic Components of Individual Future Prediction that can be harnessed for FEP-CW.} \label{comp}

    % \hfill
  % \begin{minipage}[b]{0.4\textwidth}
  %   \includegraphics[width=\textwidth]{Eval_pie.png}
  %   \caption{Distribution of evaluation metrics across papers.} \label{fig:eval-pie}
  % \end{minipage}
\end{figure}

%\section{Prediction Visualization} \label{viz}
%Visualizing predictive insights aids the interpretability of results obtained from complex models. Time-series graphs, cluster mapping, word clouds and interactive timelines can be used for displaying results. 
%In this section, we explore the various visualization tools and their applications. 
%
 %\cite{matthews2010searching} improves the visualization capabilities of timelines derived from \textit{Simile Timeline}, \textit{Google Trends} and \textit{Google Timeline}. The timeline is split into a ``Trend Graph" which shows the change in the frequency of documents containing a query over \textit{n} years and a ``Topic Timeline" which displays the titles of the top-ranked news articles. 
%For example, \cite{jatowt2009supporting} builds a query-dependent system that generates graph-based visual summaries of the most probable future outcomes of entities such as a person, place, organization, event, etc. This is achieved by analyzing future temporal expressions, detecting periodical patterns in historical documents, clustering these documents based on time and content similarity and finally selecting the best clusters for summarization. The events are positioned  %\cite{jatowt2013multi} analyzes the sentiment degrees of time-referenced future-related information and the amount of information in each dataset across three languages and arranges it on a timeline.  

%While the previous works tend to use graphs or static timelines, advancement in future-event prediction visualization has led to a surge in interactive timelines for better user experience. Time Explorer described in \cite{von2004living} directly portrays all the future references found in news articles on a timeline.
%Chronoseeker \cite{kawai2010chronoseeker} generates timelines of chronologically arranged future events subject to accurate filtering method for removing noise. \cite{regev2024future} designs a system that takes a user query as input, downloads and preprocesses a large volume of news articles concerned with the query, classifies them to either future-related or non-future-related, clusters them topic-wise and finally displays the centroid future-related statement from each cluster on an interactive timeline. Upon clicking, a data point on the timeline can display a ranked topic-wise collection of sentences belonging to its date.   
%Finally, \cite{jatowt2015mapping} builds an interactive system that helps in collectively visualizing both historical and future attention span of microblogging users using heat map and logarithmic timelines. 
%They claim that by observing the visualization graphs, researchers can improve the feature engineering for classifiers in the future.

\begin{table*}
\centering
\small
\vskip\baselineskip
\begin{tabular}{|p{5cm}|p{7cm}|}
\hline
\multicolumn{2}{|c|}{\parbox{12cm}{\vspace{0.5\baselineskip}\textbf{FRS}: (July 31, 2024) Japanese National Bank will try to increase the value of Yen after October since the country needs to import many goods and their current price is too high. This however would depend on the actions of FED and the changes in the interest rate in the USA.\vspace{0.7\baselineskip}}} \\
\hline
\textbf{Component} & \textbf{Example}\\
\hline
Predicted Future Event & Increase the value of Yen.\\
\hline
Predicted Attribute of Future Event &  Date: After October 2024.\\
\hline
Genre of Statement & News article\\
\hline
Credibility of a Source & High\\
\hline
Conditions & Actions of FED and the changes in the interest rate in the USA.\\
\hline
Rationale & The country needs to import many goods and their current price is too high.\\
\hline
Modality & "will try to" (moderately plausible)\\
\hline
Timestamp & July 31, 2024\\
\hline
\end{tabular}
\caption{Example of the proposed data model components}
\label{table:2}
\end{table*}

\vspace{-1em}
\section{Data Model of Future-related Statements}
%Before we proceed with discussing approaches for FEP-CW based on aggregating individual forecasts, 
We outline in this section our proposal of a data model that decomposes a future-related statement into several distinct components. These components can be useful for a more effective aggregation/summarization process using refined methods. Figure \ref{comp} shows the breakdown of a future-related statement into components that can play key roles in FEP-CW, described as below. Table \ref{table:2} also highlights these components through an example. 
\begin{itemize}
    \item \textbf{Predicted Future Event} refers to the description of an event expected to occur by the statement creator.
    %, the probability of which can be derived from a future-related statement.
    \item \textbf{Predicted Attribute of Future Event} are any associated information about the future event that the statement creator forecasts in her future-related statement (e.g., actors, location, result, consequences). Of particular importance is here the attribute that specifies the \textbf{date/time of the forecasted event} or a forecast horizon. 
    \item \textbf{Genre of Statement} defines the type of a document in which the future-related statement appears setting up the general context for understanding the statement. News, web and social media are a few of the common sources.
    \item \textbf{Credibility of a Source} determines the quality and reliability of the statement creator (an expert or average user, an institution, etc.) which helps to assess the quality and veracity of the future prediction. Note that the credibility is strongly affected by the relation of a source to the forecasted events (e.g., a company expressing its own future plans vs. an investor or a customer making the prediction about this company).
    \item \textbf{Conditions} are usually specific circumstances or constraints that need to occur or be satisfied for an event to happen. These constitute other future events expressed as embedded future-related statements, potentially, with their own components such as respective dates, modality, or even their own conditions\footnote{This essentially leads to a sequence or hierarchy of conditions.}. Note that there could be multiple conditions in a single future-related statement.
    \item \textbf{Rationale} defines the logic behind why a given future event is expected to happen, usually in the form of an explanation supporting why the predicted event will happen. 
    %It may explain the causation behind an upcoming event, providing a background of the event that would support the particular future course of actions, or other. 
    Sometimes however rationale may only contain expressions of modality or subjective confidence without backing them by any logic or reasoning. 
    \item \textbf{Modality} indicates the certainty level of a future event made by the statement maker and can be expressed using words like \textit{"will", "likely", "might",} etc. Any future-related information has some level of uncertainty and the modality determines how the statement creator assesses the likelihood of the forecast to happen. Modality can be also in the form of a  specific expression of confidence (e.g., 60\%).
    \item \textbf{Timestamp} denotes the date or time when the particular future-related statement has been made. A general assumption is that the older the statement, the less probable is that it will be still correct (or still valid). 
    %It has been evident that recent documents contain more dependable future-related information than older documents, thus making 
    The more recent predictions can be thus preferred in case of \emph{forecast conflicts}, or they may have  higher weights assigned for the aggregation step. %Despite these challenges, more refined methods are being developed.
\end{itemize}

%Aggregating individual components or combinations thereof...

The above-described model may form the base for training dedicated classifier(s) that would allow extracting individual components. These can be then utilized in various ways when aggregating individual forecasts (e.g., weighting them by timestamps, credibility, expressed modalities, a presence or argumentativeness of associated rationale or likelihood of necessary conditions to be fulfilled). Note that not all of the above-listed elements may be stated in a particular future-related statement or may be known. For example, when expressing predictions, one may not mention (or even know) the date when the event will occur or there might be no particular prerequisites given in the form of conditions. However, when available, some of these elements can be quite useful for creating the final forecast summaries such as the rationale or credibility of a statement producer which can help determine the accuracy of predictions. 



%So far, datasets have been curated from news collections, web, or social media which provides generalized content. 

%Since future events are very much industry-dependent, generalized data for training models fails to capture the nitty-gritty specific to a domain, yielding wrong predictions. Besides, the lack of benchmarking in the forecasting community has prevented direct comparison of state-of-the-art models for various categories of forecasting tasks. Aggregating wisdom of the crowd for future event forecasting entails many challenges such as resolving conflict of opinions, retrieving only relevant information and summarizing them in the form of a prediction and its expected date.

\section{Future Directions} \label{challenges}
    % \setlength\itemsep{0.01em}
 In this section, we identify potential future research directions. 

\begin{itemize}
    \item \textbf{Approaches using Future-related Data Model}: Currently, few approaches make use of the individual elements explicated in the data model in Sec. 5. Most just treat predictions as atomic units or consider only more obvious elements such as timestamps. The model can be used to more accurately understand predictions by individual actors and can lead to their better combination. For example, predictions could be grouped not only by the similarity of the forecasted events but also by the similarity of their conditions or rationale to more accurately reflect their support within the community. Another way is to assign weights to individual forecasts based on their source credibility, genres, timestamps, complexity of conditions, logic of rationale, etc.
    
    \item \textbf{Multilingual datasets}: Creating datasets across languages from different regions will enable incorporating local context and lead to more nuanced understanding of model capabilities. 
    %Besides, augmenting existing forecasting models to adapt to multilingual settings would save a huge amount of time and resources. 
    \item \textbf{Domain-specific datasets}: Collecting domain-specific data will add knowledge from particular areas like agriculture, business, and health and help in generating specialized real-time forecasts.
    \item  \textbf{Resolving forecast conflicts}: Since future is inherently uncertain one can expect many forecast conflicts which can be automatically detected and resolved, e.g., with the help of assessing prediction validity and timeliness.
    \item \textbf{Evaluation and metrics}: There is a need to formulate evaluation benchmarks, both generalized and domain-specific and formulate context-aware metrics that will help in better evaluation. Currently, evaluation is either missing, or based on a manual verification if the predicted events happened.
    \item \textbf{LLMs for forecasting}: GPT-4 has been found to underperform in real-world-forecasting, event if supplied with background information about an event \cite{yuan2023back}. In the future, LLMs' forecasting capabilities can be improved by accessing real-time information (RAG) from the web for mitigating knowledge cutoffs, integrating human-in-the-loop, adapting chain-of-thought prompting, or investigating better prompting techniques and the role of supporting, contextual information  \cite{mutschlechner2025analyzing}.
    %\item \textbf{Integrating additional aspects}: It is important to consider other dimensions of future-related statements besides just event description as shown in Figure \ref{comp}. For example, one could measure the confidence level of the predictions, evaluate the provided rationale and errors in determining expected dates of forecasted events.
    %as they are based on human assumptions and verdicts which might or might not be exactly accurate.
    \item \textbf{Integrating wisdom of the crowd approach with other forecasting methods}: Combining public opinion analysis and other forecasting approaches such as time series based forecasting, or causal analysis is a promising area.
    \item \textbf{Bias removal}: Future forecasting faces the risk of bias. For example, statements originating from a single source may be skewed towards certain event outcome or certain interpretation of the future. It is important to maintain carefully the diversity of used sources and consider application of bias detection techniques 
%\cite{10140917}.
\cite{farber2020multidimensional,10140917,gallegos2024bias}.
    \item \textbf{Multimodality}: Other modalities in addition to text can be incorporated in the prediction task (e.g., images, time series, videos).
    \item \textbf{Creating future timelines}: Timeline summarization techniques \cite{yu2021multi,chen2019learning} 
    %\cite{chen2019learning} 
    can be adapted to future forecasting outputting chronologically arranged sequences of events potentially with explained causal relations between them (e.g., connecting future events by event-conditions relations).
\end{itemize}


\section{Conclusion}\label{conclusion}
Future forecasting is inherently challenging but, at the same time, it is a very common activity that humans do on a daily basis both in their private and professional lives. Forecasting is also of great importance for any domains and business. It is then important and natural to study how to harness computational approaches for enhancing future forecasting, especially, considering the impressive advancements in NLP methods we recently witness.

We are the first to survey previous research works that follow a particular way to generate predictions about the future - by attempting to detect, aggregate and summarize individual forecasts issued by multiple users. The application of the Wisdom-of-the-Crowd concept for future forecasting is appealing given the abundance of opinions shared online including the expressed opinions on likely future. 

In our survey, we overview the techniques, challenges, data sources and promising avenues of research in the task of future event forecasting using the wisdom of the crowd. Our work details the data sources and types, methodologies, and future directions. We also introduce a novel data model that decomposes future-related statements into separate elements that carry potential for effective aggregation and for subsequent generation of effective predictions. In the future, we plan to extend this data model  and conduct experiments to determine the extraction performance. 

\textbf{Challenges}. 
%The survey highlights also several old and new approaches undertaken to predict the future by aggregating the crowd's opinion.
Our survey could be useful for scholars as the first step when starting to approach this challenging task or practitioners who want to develop intelligent systems for providing real-time applications.
While problem-solving techniques in NLP have evolved over time, a comparatively under-explored domain as this faces numerous challenges. A key problem is with the evaluation. While a natural way is to verify the predictions based on real outcomes, this means that either one needs to wait to check if the forecasts' results were indeed correct, or to use past forecasts with already known event outcomes. The latter approach carries the risk of data contamination especially when using LLMs.
%, for example, LLMs may have been trained on data that already includes the results of predictions. 
This means that datasets quickly become obsolete.
The task suffers also from the lack of structured datasets not only in English but across other languages.
%retrieving only relevant information and summarizing them in the form of a prediction and its expected date.

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{v2}

\end{document}

