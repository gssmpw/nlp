\section{Gradient of loss functions}
\label{sec: app_proof}
\paragraph{Gradient of the original DPO loss function.}
Given the input $(x, y^w, y^l)\sim\mathcal{D}$, the loss of DPO is as follows.
\begin{equation}
    \mathcal{L}=-\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}[\log \sigma (\beta \log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)})]
\end{equation}
Let $h_\theta(x,y_w,y_l)\triangleq \beta \log\frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)}$ and $f(x,y_w,y_l)\triangleq\beta(1- \sigma(h_\theta(x,y_w,y_l)))$, then
\begin{equation}
\begin{aligned}
    \frac{\partial \mathcal{L}(x,y_w,y_l)}{\partial \theta}
    & = \frac{\partial -\log \sigma(h_\theta(x,y_w,y_l))}{\partial \theta} \\
    & = -\frac{1}{\sigma(h_\theta(x,y_w,y_l))}\frac{\partial \sigma(h_\theta(x,y_w,y_l))}{\partial \theta} \\
    & = -\frac{1}{\sigma(h_\theta(x,y_w,y_l))}\frac{\partial \sigma(h_\theta(x,y_w,y_l))}{\partial h_\theta(x,y_w,y_l)}\frac{\partial h_\theta(x,y_w,y_l)}{\partial \theta} \\
    & = -\frac{1}{\sigma(h_\theta(x,y_w,y_l))}\sigma(h_\theta(x,y_w,y_l))(1-\sigma(h_\theta(x,y_w,y_l)))\frac{\partial h_\theta(x,y_w,y_l)}{\partial \theta} \\
    & = -f(x,y_w,y_l)\frac{\partial [\log\pi_\theta(y_w|x)- \log\pi_\text{ref}(y_w|x) - \log\pi_\theta(y_l|x)+ \log\pi_\text{ref}(y_l|x)]}{\partial \theta}\\
    & = -f(x,y_w,y_l)(\frac{\partial \log\pi_\theta(y_w|x)}{\partial \theta} - \frac{\partial \log\pi_\theta(y_l|x)}{\partial \theta})
\end{aligned}
\end{equation}

\paragraph{Gradient of the loss function of D3PO.}
To study the generative distribution in the denoising process of diffusion models, let $x\triangleq (x_t,c), y\triangleq x_{t-1}$, then we have
\begin{equation}
    \pi_\theta(y|x)=\pi_\theta(x_{t-1}|x_t,c)=
\frac{1}{(2\pi\sigma_t^2)^{d/2}}\exp(-\frac{\Vert x_{t-1}-\mu_\theta(x_t)\Vert_2^2}{2\sigma_t^2})
\end{equation}
In this case, the gradient of the loglikelihood $\log \pi_\theta(x_{t-1}|x_t,c)$ \emph{w.r.t.} $\theta$ is given as follows.
\begin{equation}
\begin{aligned}
    \frac{\partial\log\pi_\theta(x_{t-1}|x_t,c)}{\partial \theta}
    & = (\frac{\partial \mu_\theta(x_t)}{\partial \theta})^T \frac{\partial (-\frac{\Vert x_{t-1}-\mu_\theta(x_t)\Vert_2^2}{2\sigma_t^2} - \log((2\pi\sigma_t^2)^{d/2}))}{\partial \mu_\theta(x_t)}  \\
    & = (\frac{\partial \mu_\theta(x_t)}{\partial \theta})^T\frac{(x_{t-1}-\mu_\theta(x_t))}{\sigma_t^2}
\end{aligned}
\end{equation}


Then, we consider the gradient of the D3PO loss \emph{w.r.t.} the model output $\mu_\theta$.
\begin{equation}
\begin{aligned}
    \frac{\partial\mathcal{L}(x^w_t,x_{t-1}^w, x^l_t, x_{t-1}^l)}{\partial \theta}
    & = -f_t(\frac{\partial\log\pi_\theta(x^w_{t-1}|x^w_t,t,c)}{\partial \theta} - \frac{\partial\log\pi_\theta(x^l_{t-1}|x^l_t,t,c)}{\partial \theta}) \\
    & = -\frac{f_t}{\sigma_t^2}\left[(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})^T(x^w_{t-1}-\mu_\theta(x^w_t)) - (\frac{\partial \mu_\theta(x^l_t)}{\partial\theta})^T(x^l_{t-1}-\mu_\theta(x^l_t))\right]
\end{aligned}
\end{equation}
Suppose $\Delta\theta=-\eta\frac{\partial\mathcal{L}(x^w_t,x^w_{t-1},x^l_t,x^l_{t-1})}{\partial\theta}$.
After the update of $\theta^\prime\leftarrow\theta+\Delta\theta$, 
$\Delta \mu_\theta(x^w_t)\approx\eta\frac{f_t}{\sigma_t^2}[(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})^T (x^w_{t-1}-\mu_\theta(x^w_t))] - \eta\frac{f_t}{\sigma_t^2}[(\frac{\partial\mu_\theta(x^w_t)}{\partial\theta})(\frac{\partial\mu_\theta(x^l_t)}{\partial\theta})^T (x^l_{t-1}-\mu_\theta(x^l_t))]$.
If $x^w_{t}$ and $x^l_{t}$ are located in the same linear subspace of the model, \emph{i.e.,} $\frac{\partial\mu_\theta(x^w_t)}{\partial\theta}\approx \frac{\partial\mu_\theta(x^l_t)}{\partial\theta}$, 
then the gradient can be written as follows.
\begin{equation}
\begin{aligned}
    \frac{\partial\mathcal{L}(x^w_t,x_{t-1}^w, x^l_t, x_{t-1}^l)}{\partial \theta}
    & = -\frac{f_t}{\sigma_t^2}\left[(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})^T(x^w_{t-1}-\mu_\theta(x^w_t)) - (\frac{\partial \mu_\theta(x^l_t)}{\partial\theta})^T(x^l_{t-1}-\mu_\theta(x^l_t))\right] \\
    & \approx -\frac{f_t}{\sigma_t^2}\left[(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})^T(x^w_{t-1}-\mu_\theta(x^w_t)) - (\frac{\partial \mu_\theta(x^w_t)}{\partial\theta})^T(x^l_{t-1}-\mu_\theta(x^l_t))\right] \\
    & \approx -\frac{f_t}{\sigma_t^2}(\frac{\partial\mu_\theta(x^w_t)}{\partial \theta})^T\left[(x^w_{t-1}-x^l_{t-1}) + (\mu_\theta(x^l_t) - \mu_\theta(x^w_t))\right] \\
\end{aligned}
\end{equation}
Suppose $\Delta\theta=-\eta\frac{\partial\mathcal{L}(x^w_t,x^w_{t-1},x^l_t,x^l_{t-1})}{\partial\theta}$.
After the update of $\theta^\prime\leftarrow\theta+\Delta\theta$, 
$\Delta\mu_\theta(x^w_t)\approx \eta\frac{f_t}{\sigma^2_t} (\frac{\partial\mu_\theta(x^w_t)}{\partial\theta})(\frac{\partial\mu_\theta(x^w_t)}{\partial\theta})^T[(x^w_{t-1}-x^l_{t-1}) + (\mu_\theta(x^l_t)-\mu_\theta(x^w_t))]$.

\paragraph{Gradient of our loss function.}

Then, we consider the gradient of our loss function \emph{w.r.t.} the model output $\mu_\theta$.
\begin{equation}
\begin{aligned}
    \frac{\partial\mathcal{L}(x_t,x_{t-1}^w,x_{t-1}^l)}{\partial \theta}
    & = -f_t(\frac{\partial \mu_\theta(x_t)}{\partial \theta})^T(\frac{\partial\log\pi_\theta(x^w_{t-1}|x_t,t,c)}{\partial \mu_\theta(x_t)} - \frac{\partial\log\pi_\theta(x^l_{t-1}|x_t,t,c)}{\partial \mu_\theta(x_t)}) \\
    & = -f_t(\frac{\partial \mu_\theta(x_t)}{\partial \theta})^T(\frac{x^w_{t-1}-\mu_\theta(x_t)}{\sigma_t^2} - \frac{x^l_{t-1}-\mu_\theta(x_t)}{\sigma_t^2}) \\
    & = -\frac{f_t}{\sigma_t^2}(\frac{\partial \mu_\theta(x_t)}{\partial \theta})^T(x^w_{t-1} - x^l_{t-1})
\end{aligned}
\end{equation}
Suppose $\Delta\theta=-\eta\frac{\partial\mathcal{L}(x_t,x^w_{t-1},x^l_{t-1})}{\partial\theta}$.
After the update of $\theta^\prime\leftarrow\theta+\Delta\theta$, $\Delta \mu_\theta(x_t)\approx (\frac{\partial \mu_\theta(x_t)} {\partial \theta})\Delta\theta
 = \eta\frac{f_t}{\sigma^2_t} (\frac{\partial \mu_\theta(x_t)}{\partial\theta})(\frac{\partial \mu_\theta(x_t)}{\partial\theta})^T  (x^w_{t-1}-x^l_{t-1})$.


\section{TailorPO and TailorPO-G}
\label{sec: app_tailorpo-g}

In this section, we provide another formulation for the loss function of TailorPO, and then discuss the difference between TailorPO and TailorPO-G from the perspective of gradient.

First, Eq.~(\ref{eq: loss}) only shows a classic loss formulation of DPO, and does not reflect the preference selection procedure in TailorPO. To this end, we provide another formulation of the loss function, which incorporates the preference selection based on step-wise reward $r_t$.
\begin{equation}
\begin{aligned}
    \mathcal{L}(\theta) &= -\mathbb{E}_{(c,x_{t},x^{(0)}_{t-1}, x^{(1)}_{t-1})} \left[\log \sigma \left((-1)^{\mathbbm{1}(r_t(c,x^{(0)}_{t-1})<r_t(c,x^{(1)}_{t-1}))} \cdot \Delta\right)\right] ,\\
    \Delta &= \beta\log \frac{\pi_\theta(x^{(0)}_{t-1}|x_{t}, c)}{\pi_\text{ref}(x^{(0)}_{t-1}|x_{t}, c)} - \beta\log \frac{\pi_\theta(x^{(1)}_{t-1}|x_{t}, c)}{\pi_\text{ref}(x^{(1)}_{t-1}|x_{t}, c)}  
\end{aligned}
\end{equation}
where $\mathbbm{1}(\cdot)$ is the indicator function. The term $(-1)^{\mathbbm{1}(r_t(c,x^{(0)}_{t-1})<r_t(c,x^{(0)}_{t-1})}$ represents the step-level preference ranking procedure. 

Furthermore, we compare TailorPO and TailorPO-G from the perspective of gradient, in order to understand their difference in effectiveness.
In Eq.~(\ref{eq: gradient of ours}), we have shown that the gradient of the TarilorPO loss function can be written as follows.
\begin{equation*}
\nabla_\theta \mathcal{L}(\theta) = -\mathbb{E}\left[(f_t/{\sigma^2_{t}})\cdot\nabla^T_\theta\mu_\theta(x_{t})(x^w_{t-1} - x^l_{t-1})\right]
\end{equation*}
For TarlorPO-G, the term $x^w_{t-1}$ is modified by adding the gradient term $\nabla_{x^w_{t-1}}\log p(r_{\text{high}}|x^w_{t-1})$. Therefore, we can derive its gradient term as follows.
\begin{equation}
\begin{aligned}
\nabla_\theta \mathcal{L}_{TailorPO-G}(\theta) & = -\mathbb{E}\left[(f_t/{\sigma^2_{t}})\cdot\nabla^T_\theta\mu_\theta(x_{t})((x^w_{t-1} +  \nabla_{x^w_{t-1}}\log p(r_\text{high} | x^w_{t-1}))- x^l_{t-1})\right] \\
&= -\mathbb{E}\Big[(f_t/{\sigma^2_{t}})\cdot\nabla^T_\theta\mu_\theta(x_{t})(\underbrace{\nabla_{x^w_{t-1}}\log p(r_\text{high} | x^w_{t-1})}_{\text{pushing towards high reward values}} + (x^w_{t-1} - x^l_{t-1})\Big]
\end{aligned}
\end{equation}
The gradient term pushes the model towards the high-reward regions in the reward models. Therefore, TarlorPO-G further improves the effectiveness of TailorPO. 


\section{Experimental settings and ethics statement for the user study}
\label{sec: app_user_study}

To verify that our framework generates more human-preferred images, we conducted a user study by requesting ten human users to label their preference for generated images from the perspective of visual appeal and general preference.

\textbf{Ethics statement. }
We collect feedback from ten annotators. All annotators acknowledge and agree that their efforts will be used to evaluate the performance of different methods in this paper.

\textbf{Task description.}
Given each prompt in the set of 45 animal prompts, we sampled five images from the fine-tuned model and obtained a total of 225 images per model.
For comparison, for each pair of fine-tuned model, we organized their generated images into 225 pairs. 
Each human annotator was given several triplets of ($c, x^{(a)}_1, x^{(b)}_0$), where $c$ is the text prompt and $x^{(a)}_1$ and $x^{(b)}_0$ represent the paired image generated by the model finetuned by method $a$ and method $b$, respectively.
In order to avoid user bias, \textit{we hid the source of $x^{(a)}_1$ and $x^{(b)}_0$ and randomly placed their order to annotators}.
Then, the annotator was asked to compare the two images from the perspective of alignment, aesthetics, and visual pleasantness. If both images in a pair looked very similar or were both unappealing, then they should label ``draw'' for them. Otherwise, they should label each image with a ``win'' or ``lose'' tag. In this way, for each pair of comparing methods, we had 225 triplets of ($c, x^{(a)}_1, x^{(b)}_0$) and each annotator labeled 225 ``win/lose'' or ``draw'' tags.


Then, we computed the ratio of pairs where TailorPO and TailorPO-G received ``win,'' ``draw,'' and ``lose'' labels, respectively.
Figure~\ref{fig: user study} reports the win-lose percentage results of our method versus other baseline methods, our method exhibits a clear advantage in aligning with human preference.


\section{More experimental results}

\subsection{Experiments on complex prompts}
\label{subsec: app_pick}

We fine-tuned Stable Diffusion v1.5 on various reward models using 4k prompts in the training set of the Pick-a-Pic validation set~\citep{kirstain2023pick}, selected by \citet{liang2024spo}.
We followed the same setting with Section~\ref{sec: experiments} of the main text for TailorPO and TailPO-G.
Then, we evaluated the fine-tuned model on 500 prompts from the Pick-a-Pic validation set.
Table~\ref{tab: pick-a-pic} compares our method with Diffusion-DPO~\citep{wallace2024diffusiondpo} and SPO~\citep{liang2024spo}\footnote{Results of Diffusion-DPO and SPO on prompts in the Pick-a-Pic dataset are from \citep{liang2024spo}.}.
For these complex prompts, our methods also achieved the highest reward values.
Visual demonstrations are shown in Figure~\ref{fig: vis pickapic}.

\begin{figure}[h]
    \centering
    \captionof{table}{Reward values of images generated by diffusion models fine-tuned using different methods. The prompts are from the Pick-a-Pic dataset.}
    \vspace{-5pt}    
    \label{tab: pick-a-pic}
    \resizebox{0.7\linewidth}{!}{
    \centering
    \begin{tabular}{c|c c c c}
        \hline
         & Diffusion-DPO & SPO & TailorPO & TailorPO-G \\
         \hline
      Aesthetic scorer &  5.505 & 5.887 & 6.050 & 6.242 \\
      ImageReward & 0.1115 & 0.1712  & 0.3820 & 0.3791 \\
      \hline
    \end{tabular}
    }
\end{figure}


\subsection{Verification of the estimation for step-wise rewards}
\label{subsec: verify step-wise reward}

In this section, we conducted experiments to verify the reliability of the estimation in Eq.~(\ref{eq: reward}) for step-wise rewards. We compared the estimated value $r(c,\hat{x}_0(x_t))$ with  $r_t(c,x_t)\triangleq \mathbb{E}[r(c,x_0)|c,x_t]$ at different training checkpoints. For the fine-tuned model $\epsilon_{\theta'}$, we sampled images with 20 DDIM steps and randomly sampled 100 pairs of $(c,x_t)$ at each timestep $t\in\{12,8,4,1\}$. Give each pair of $(c,x_t)$, we sampled 100 images $x_0$ based on $x_t$ and then computed $r_t(c,x_t)=\mathbb{E}[r(c,x_0)|c,x_t]$ as the ground truth of the step-wise reward. Then, we computed the estimated value $r(c,\hat{x}_0(x_t))$ based on the fine-tuned parameters $\theta'$. Table~\ref{tab: verify rewards aes} and Table~\ref{tab: verify rewards jpeg} report the average relative error $\mathbb{E}[\vert\frac{r_t(c,x_t) - r(x, \hat{x}_0(x_t))}{r_t(c,x_t)}\vert]$ at different timesteps $t$ in different models (we used the aesthetic scorer and JPEG compressibility as the reward model, respectively).

\begin{table}[h]
    \centering
    \caption{Average relative error of the estimated aesthetic score.
    \label{tab: verify rewards aes}
    \vspace{-3pt}}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c c}
        \hline
        timestep $t$ & 12 & 8 & 4 & 1 \\
        \hline
        pre-trained model $\epsilon_\theta$ &  0.0545{\small $\pm$0.0427} & 0.0378{\small $\pm$0.0287} & 0.0132{\small $\pm$0.0089} & 0.0047{\small $\pm$0.0051} \\ 
        $\epsilon_{\theta'}$ after training on 10k samples & 0.0353{\small $\pm$0.0345} & 0.0176{\small $\pm$0.0160} & 0.0106{\small $\pm$0.0080} & 0.0033{\small $\pm$0.0029} \\
        $\epsilon_{\theta'}$ after training on 40k samples & 0.1330{\small $\pm$0.0320} & 0.0283{\small $\pm$0.0231} & 0.0132{\small $\pm$0.0084} & 0.0070{\small $\pm$0.0047} \\        
        \hline
    \end{tabular}
    }
\end{table}

\begin{table}[h]
    \centering
    \captionof{table}{Average relative error of the estimated JPEG compressibility.
    \label{tab: verify rewards jpeg}
    \vspace{-3pt}}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c c}
        \hline
        timestep $t$ & 12 & 8 & 4 & 1 \\
        \hline
        pre-trained model $\epsilon_\theta$ &  0.2263{\small $\pm$0.0524} & 0.1259{\small $\pm$0.0333} & 0.0390{\small $\pm$0.0101} & 0.0070{\small $\pm$0.0039} \\
        $\epsilon_{\theta'}$ after training on 10k samples & 0.2492{\small $\pm$0.0390} & 0.1440{\small $\pm$0.0279} & 0.0425{\small $\pm$0.0071} & 0.0074{\small $\pm$0.0016} \\
        $\epsilon_{\theta'}$ after training on 40k samples & 0.1566{\small $\pm$0.0925} & 0.0341{\small $\pm$0.0221} & 0.0113{\small $\pm$0.0077} & 0.0066{\small $\pm$0.0016} \\ 
         \hline
    \end{tabular}
    }
\end{table}

These results demonstrate that after fine-tuning, the model $\epsilon_{\theta'}$ achieved a small error as the pre-trained model $\epsilon_\theta$ does. Moreover, our DPO-based loss function does not require an accurate reward value, but only needs the preference order of samples. Even if there is a small estimation error for the step-wise reward, it does not affect the preference order between paired samples, thus having little effect on training. Therefore, the estimation for step-wise rewards is reliable.



\subsection{Experiments on other base models}

We also fine-tuned Stable Diffusion v2.1\footnote{https://huggingface.co/stabilityai/stable-diffusion-2-1-base} (SD v2.1-base) to demonstrate the effectiveness of our method.
Taking the aesthetic scorer as the reward model, we fine-tuned SD v2.1-base using prompts of animals, and then evaluated the model with the same prompts.
After fine-tuning with TailorPO, the aesthetic score of images generated by SD v2.1-base was improved from 5.95 to 7.60.
In comparison, DDPO only reached the value of 7.06.



\subsection{Generations given different reward models and prompts.}
In this section, we provide some examples of generated images given different reward models and prompts from the main text.
For different models, Figure~\ref{fig: vis-pick} shows images generated by SD v1.5 fine-tuned on the PickScore reward model.
For different prompts, we designed and selected\footnote{We selected several prompts from \url{https://openai.com/index/dall-e-3/}.} several real-world prompts, which were not presented in the training set of prompts. Figure~\ref{fig: real-world prompts} shows that the model generated natural and beautiful images accordingly.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/vis_pick.pdf}
    \vspace{-5pt}
    \caption{Images generated by the model fine-tuned on the PickScore reward model.}
    \vspace{-5pt}
    \label{fig: vis-pick}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figs/vis_real.pdf}
    \vspace{-5pt}
    \caption{Images generated given real-world prompts: (1) A chair in the corner on a boat. (2) A table of delicious food. (3) A dog playing a ball. (4) A warm and comfortable room with a table, a chair, and a bed. (5) A kid riding a bike. (6) A cat sleeping next to the window. (7) A modern architectural building with large glass windows, situated on a cliff overlooking a serene ocean at sunset. (8) Illustration of a chic chair with a design reminiscent of a pumpkinâ€™s form, with deep orange cushioning, in a stylish loft setting.
    }
    \label{fig: real-world prompts}
    \vspace{-5pt}
\end{figure}




\section{Ablation studies}
\label{sec: app_ablation}
In this section, we performed ablation studies to verify the effect of hyper-parameters on performance, including the number of steps used for optimization and the strength of gradient guidance.
Furthermore, we investigated the impact of each component in our framework.


\textbf{Effect of steps used for training.}
We first investigate the effect of the number of steps $T_\text{fine-tune}$ used for fine-tuning in TailorPO.
In Section~\ref{sec: experiments}, We generated images with $T=20$ sampling timesteps and uniformly sampled only $T_\text{fine-tune}=5$ steps for training to boost the training efficiency.
Here, we compared the results of setting $T_\text{fine-tune}=3,5,10$ in Table~\ref{tab: effect of steps}, and it shows that while the fine-tuning performance is relatively stable to the setting of $T_\text{fine-tune}$, fine-tuning on five steps achieved a better trade-off between performance and efficiency.


\textbf{Effect of the strength of gradient guidance.}
We also verify the effect of gradient guidance in TailorPO-G by applying gradient guidance with different strengths at intermediate steps.
Specifically, we used different settings of $\eta_t$ in Eq.~(\ref{eq: gradient guidance}) for fine-tuning.
The result in Table~\ref{tab: effect of gradient guidance} shows that the varying strength $\eta_t$ for different steps $t$ better enhances the fine-tuning performance.

\begin{figure}[t]
    \centering
    \begin{minipage}{0.51\linewidth}
    \centering
    \captionof{table}{Effect of the number of steps used in TailorPO.
    For each setting of $T_\text{fine-tune}$, we uniformly sampled $T_\text{fine-tune}$ steps for fine-tuning.
    \vspace{-5pt}}
    \label{tab: effect of steps}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c }
         \hline
         $T_\text{fine-tune}$ & Aesthetic Scorer & HPSv2 & compressibility \\
         \hline
         $10$ & 6.61 & 28.14 & -20.62\\
         $5$ & \textbf{6.74} & \textbf{28.43} & \textbf{-4.76} \\
         $3$ & 6.40 & 28.15 & -9.97\\
         \hline
    \end{tabular}
    }
\end{minipage}
\hfill
\begin{minipage}{0.47\linewidth}
    \centering
    \captionof{table}{Effect of strength $\eta_t$ of gradient guidance in TailorPO-G.
    [0.1,0.2] represents we set $\eta_t$ ranging from 0.1 to 0.2 for different $t$.
    \vspace{-5pt}}
    \label{tab: effect of gradient guidance}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c c}
         \hline
         $\eta_t$ & Aesthetic Scorer & ImageReward & HPSv2 \\
         \hline
         $0.1$ & 5.82 & 1.22 & 28.10 \\
         $0.2$ & 6.97 & \textbf{1.35} & 28.18 \\
         $0.5$ & 7.07 & 0.71 & 27.48 \\
         $[0.1, 0.2]$ & \textbf{7.11} & 1.25 & \textbf{28.43} \\
         \hline
    \end{tabular}
    }
\end{minipage}        
\end{figure}


\textbf{Effects of each component in our methods.}
There are three key components in our methods: (1) step-level preference ranking, (2) the same input condition at each step, and (3) gradient guidance of reward models. Therefore, we fine-tuned SD v1.5 based on the aesthetic scorer using  (1), (1)+(2), (1)+(2)+(3). Here we set the same random seed for a fair comparison, so the results of (1)+(2) and (1)+(2)+(3) were slightly different from Table~\ref{tab: animal results} (where we averaged results of three runs with different random seeds). Table~\ref{tab: effect of components} shows that all these components improved the aligning effectiveness.

\begin{table}[h]
    \centering
    \caption{Effect of each component in our framework.
    \vspace{-5pt}}
    \label{tab: effect of components}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{c|c c}
    \hline
         &  Aesthetic scores & ImageReward \\
    \hline
      SD v1.5   & 5.79 & 0.65\\
      (1) step-level preference ranking & 6.40 & 0.98\\
      (1) step-level preference ranking + (2) same input condition at each step & 6.69 & 1.16 \\
      (1) step-level preference ranking + (2) same input condition at each step + (3) gradient guidance & 6.78 & 1.25 \\
    \hline
    \end{tabular}
    }
    \vspace{-5pt}
\end{table}

\section{Limitations and discussions}
\label{sec: app_limitation}

In this section, we discuss the potential limitations of our method.
Like other methods based on an explicit pre-trained reward model, including DDPO, D3PO, and SPO, TailorPO has the potential of being prone to reward hacking~\citep{skalse22defining}, if we fine-tune the model on very simple prompts for too many iterations. It means that the generative model is overoptimized to improve the score of the reward model but fails to maintain the original output distribution of natural images. We provide some examples in Figure~\ref{fig: reward hacking} to demonstrate this phenomenon. 
\begin{figure}[h]
    \centering
    \begin{minipage}{0.38\linewidth}
        \caption{Fine-tuning the diffusion model based on pre-trained reward models may introduce some bias into the generated images. For example, when taking JPEG compressibility as the reward model, DDPO, D3PO, and our methods all generate images with a blank background.}
    \label{fig: reward hacking}
    \end{minipage}
    \hfill
    \begin{minipage}{0.6\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/failure_case.pdf}
    \end{minipage}
    \vspace{-5pt}
\end{figure}


The problem of reward hacking is related to the quality of reward models. Given the fact that these pre-trained reward models are usually trained on a finite training set, they cannot \textit{perfectly} fit the human preference for natural and visually pleasing images. Therefore, the optimization of generative models towards these reward models may lead to an unnatural distribution of images.

In order to alleviate the reward hacking problem, TailorPO can be further improved from the following perspectives.

\begin{itemize}
    \item Using a better reward model that well captures the distribution of natural and visually pleasing images. A better reward model can avoid guiding model optimization towards unnatural images.
    \item Utilizing the ensemble of multiple reward models to alleviate the bias of a single reward model. While each single reward model has its own preference bias, considering multiple reward models altogether may be able to alleviate the risk of falling into a single model. To this end, \citet{coste24reward} have shown that the reward model ensembles can effectively address reward hacking in RLHF-based fine-tuning of language models. Therefore, we are hopeful that the reward model ensembles are also effective for diffusion models.
    \item Searching for a better setting of the hyperparameter $\beta$ in the loss function to strike a balance between natural images and high reward scores. In DPO-style methods, the coefficient $\beta$ controls the deviation from the original generative distribution (the KL regularization). In this way, we can search for a better value of $\beta$ to avoid the model being fine-tuned far away from the original base model. For example, \citet{wu24beta} have provided a method to dynamically adjust the value of $\beta$.
\end{itemize}
