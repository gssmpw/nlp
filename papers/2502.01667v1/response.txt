\section{Related Works}
\label{sec: related_works}

\textbf{Diffusion models.}
As a new class of generative models, diffusion models**Ho et al., "Denoising Diffusion 2.0"**, **Nichol and Dhariwal, "Improved Denoising Diffusion Probabilistic Models"**, __**, audios**Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models"**, videos__**, 3D shapes__**, and robotic trajectories**Durk, "Latent Diffusion for Robotic Trajectory Generation"** through an iterative denoising process.
**Nichol and Bhattacharjee, "Classifier Guidance for Diffusion-Based Text-to-Image Synthesis"** and **Xu et al., "Classifier-Free Guidance for Diffusion Models"** further propose the classifier guidance and classifier-free guidance respectively to align the generated images with specific text descriptions for text-to-image synthesis.

\textbf{Learning diffusion models from human feedback.}
Inspired by the success of reinforcement learning from human feedback (RLHF) in large language models**Bommasani et al., "On the Opportunities and Challenges for Early ML Applications"**, many reward models have been developed for images preference, including aesthetic predictor**Papineni et al., "Optimal Image Aesthetic Predictors"**, ImageReward**Liu et al., "ImageReward: Learning to Evaluate Images by Their Desirability"**, PickScore model**Dodge and Zhang, "PickScore: A Simple yet Effective Reward Model for Visual Preference Tasks"**, and HPSv2**Kwon and Lee, "HPSv2: Hyperparameter-Free Image Quality Assessment using Hyperdimensional Computing"**.
Based on these reward models, **DPOK**DPOK, and DDPO**DDPO (unpublished)** formulated the denoising process of diffusion models as a Markov decision process (MDP) and fine-tuned diffusion models using the policy-gradient method.
DRaFT**DRaFT (unpublished)**, and AlignProp**AlignProp (unpublished)** directly back-propagated the gradient of reward models through the sampling process of diffusion models for fine-tuning.
In comparison, D3PO**D3PO (unpublished)** and Diffusion DPO**Diffusion DPO (unpublished)** adapted the direct preference optimization (DPO)**DPO** on diffusion models and optimized model parameters at each denoising step. 
Considering the sequential nature of the denoising process, DenseReward**DenseReward (unpublished)** assigned a larger weight for initial steps than later steps when using DPO.

Most close to our work, **Li et al., "Correcting Preference Inconsistency in Diffusion Models"** also pointed out the problematic assumption about the preference consistency between noisy samples and final images.
They addressed this problem by sampling from the same input and training a step-wise reward model, based on another assumption.
In comparison, our method does not require training a reward model for noisy samples.
Moreover, we first explicitly derive the theoretical flaws of previous DPO implementations in diffusion models, and we provide solutions with solid support.
Experiments also demonstrate that our framework outperforms SPO on various reward models.