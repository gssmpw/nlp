\section{Related Works}
\label{sec: related_works}

\textbf{Diffusion models.}
As a new class of generative models, diffusion models____ transform Gaussian noises into images____, audios____, videos____, 3D shapes____, and robotic trajectories____ through an iterative denoising process.
____ and ____ further propose the classifier guidance and classifier-free guidance respectively to align the generated images with specific text descriptions for text-to-image synthesis.

\textbf{Learning diffusion models from human feedback.}
Inspired by the success of reinforcement learning from human feedback (RLHF) in large language models____, many reward models have been developed for images preference, including aesthetic predictor____, ImageReward____, PickScore model____, and HPSv2____.
Based on these reward models, ____, DPOK____ and DDPO____ formulated the denoising process of diffusion models as a Markov decision process (MDP) and fine-tuned diffusion models using the policy-gradient method.
DRaFT____, and AlignProp____ directly back-propagated the gradient of reward models through the sampling process of diffusion models for fine-tuning.
In comparison, D3PO____ and Diffusion DPO____ adapted the direct preference optimization (DPO)____ on diffusion models and optimized model parameters at each denoising step. 
Considering the sequential nature of the denoising process, DenseReward____ assigned a larger weight for initial steps than later steps when using DPO.

Most close to our work, ____ also pointed out the problematic assumption about the preference consistency between noisy samples and final images.
They addressed this problem by sampling from the same input and training a step-wise reward model, based on another assumption.
In comparison, our method does not require training a reward model for noisy samples.
Moreover, we first explicitly derive the theoretical flaws of previous DPO implementations in diffusion models, and we provide solutions with solid support.
Experiments also demonstrate that our framework outperforms SPO on various reward models.