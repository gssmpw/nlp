\section{Related Works}
\label{sec: related_works}

\textbf{Diffusion models.}
As a new class of generative models, diffusion models~\citep{shol2015deep,ho2020DDPM,song2021DDIM} transform Gaussian noises into images~\citep{dhariwal2021diffusion,ho2022cascaded,nichol2022glide,rombach2022LDM}, audios~\citep{liu2023audioldm}, videos~\citep{ho2022imagenvideo,singer2023video}, 3D shapes~\citep{zeng2022lion,poole2023dreamfusion,gu2023nerfdiff}, and robotic trajectories~\citep{janner2022planning,chen2024simple} through an iterative denoising process.
\citet{dhariwal2021diffusion} and \citet{ho2022classifier} further propose the classifier guidance and classifier-free guidance respectively to align the generated images with specific text descriptions for text-to-image synthesis.

\textbf{Learning diffusion models from human feedback.}
Inspired by the success of reinforcement learning from human feedback (RLHF) in large language models~\citep{ouyang2022rlhf,bai2022anthropic,openai2023gpt4}, many reward models have been developed for images preference, including aesthetic predictor~\citep{schuhmann2022laion}, ImageReward~\citep{xu2023imagereward}, PickScore model~\citep{kirstain2023pick}, and HPSv2~\citep{wu2023hps}.
Based on these reward models, \citet{lee2023aligning}, DPOK~\citep{ying2023dpok} and DDPO~\citep{black2024ddpo} formulated the denoising process of diffusion models as a Markov decision process (MDP) and fine-tuned diffusion models using the policy-gradient method.
DRaFT~\citep{clark2024draft}, and AlignProp~\citep{prabhudesai2023alignprop} directly back-propagated the gradient of reward models through the sampling process of diffusion models for fine-tuning.
In comparison, D3PO~\cite{yang2024d3po} and Diffusion DPO~\citep{wallace2024diffusiondpo} adapted the direct preference optimization (DPO)~\citep{rafailov2023DPO} on diffusion models and optimized model parameters at each denoising step. 
Considering the sequential nature of the denoising process, DenseReward~\citep{yang2024densereward} assigned a larger weight for initial steps than later steps when using DPO.

Most close to our work, \citet{liang2024spo} also pointed out the problematic assumption about the preference consistency between noisy samples and final images.
They addressed this problem by sampling from the same input and training a step-wise reward model, based on another assumption.
In comparison, our method does not require training a reward model for noisy samples.
Moreover, we first explicitly derive the theoretical flaws of previous DPO implementations in diffusion models, and we provide solutions with solid support.
Experiments also demonstrate that our framework outperforms SPO on various reward models.