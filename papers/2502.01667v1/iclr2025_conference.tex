\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{diagbox}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{bbm}

\usepackage[linesnumbered, ruled]{algorithm2e}
\usepackage{theorem}
\newtheorem{proposition}{Proposition}

\title{Refining Alignment Framework for Diffusion Models with Intermediate-Step Preference Ranking}

\author{Jie Ren\textsuperscript{1}\thanks{Equal contribution.}~~, Yuhang Zhang\textsuperscript{2}\footnotemark[1]~~\thanks{Project lead.}~~, Dongrui Liu\textsuperscript{1}, Xiaopeng Zhang\textsuperscript{2}, Qi Tian\textsuperscript{2}
\\
\textsuperscript{1}Shanghai Jiao Tong University
~~\textsuperscript{2}Huawei Inc.
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
Direct preference optimization (DPO) has shown success in aligning diffusion models with human preference.
Previous approaches typically assume a consistent preference label between final generations and noisy samples at intermediate steps, and directly apply DPO to these noisy samples for fine-tuning. 
However, 
we theoretically identify inherent issues in this assumption and its impacts on the effectiveness of preference alignment.
We first demonstrate the inherent issues from two perspectives: \textit{gradient direction} and \textit{preference order}, and then propose a \textbf{Tailor}ed \textbf{P}reference \textbf{O}ptimization (TailorPO) framework for aligning diffusion models with human preference, underpinned by some theoretical insights.
Our approach directly ranks intermediate noisy samples based on their step-wise reward, and effectively resolves the gradient direction issues through a simple yet efficient design.
Additionally, we incorporate the gradient guidance of diffusion models into preference alignment to further enhance the optimization effectiveness.
Experimental results demonstrate that our method significantly improves the model's ability to generate aesthetically pleasing and human-preferred images.
\end{abstract}

\section{Introduction}
\label{sec: introduction}

Direct preference optimization (DPO), which fine-tunes the model on paired data to align the model generations with human preferences, has demonstrated its success in large language models (LLMs) \citep{rafailov2023DPO}. Recently, researchers generalized this method to diffusion models for text-to-image generation ~\citep{black2024ddpo,yang2024d3po, wallace2024diffusiondpo}. Given a pair of images generated from the same prompt and a ranking of human preference for them, DPO aims to increase the probability of generating the preferred sample while decreasing the probability of generating another sample, which enables the model to generate more visually appealing and aesthetically pleasing images that better align with human preferences.


Specifically, previous researchers \citep{yang2024d3po} leverage the \textit{trajectory-level} preference to rank the generated samples. As shown in Figure~\ref{fig: d3po_framework}(a), given a text prompt $c$, they first sample a pair of denoising trajectories $[x^0_T, \ldots, x^0_0]$ and $[x^1_T,  \ldots, x^1_0]$ from the diffusion model, and then rank them according to the human preference on the final generated images $x^0_0$ and $x^1_0$. It is assumed that \emph{the preference order of $(x^0_0, x^1_0)$, at the end of the generation trajectory, can consistently represent the preference order of $(x^0_t, x^1_t)$ at all intermediate steps $t$.} Then, the DPO loss function is implemented using the generation probabilities $p(x^0_{t-1}|x^0_{t},c)$ and $p(x^1_{t-1}|x^1_{t},c)$ at each step $t$ to fine-tune the diffusion model, which is called the \textit{step-level} optimization.

However, we notice that the above trajectory-level preference ranking and the step-level optimization are not fully compatible in diffusion models. 
\textbf{First}, the trajectory-level preference ranking (\emph{i.e.,} the preference order of final outputs $(x^0_0, x^1_0)$ of trajectories) does not accurately reflect the preference order of $(x^0_t, x^1_t)$ at intermediate steps.
Considering the inherent randomness in the denoising process, simply assigning the preference of final outputs to all the intermediate steps will detrimentally affect the preference optimization performance.
\textbf{Second}, the generation probabilities $p(x^0_{t-1}|x^0_{t},c)$ and $p(x^1_{t-1}|x^1_{t},c)$ in two different trajectories are conditioned on different inputs, and this causes the optimization direction to be significantly affected by the difference between the inputs.
In particular, if $x^0_{t}$ and $x^1_{t}$ are located in the same linear subspace of the diffusion model, then the optimization of DPO probably increases the output probability of the dis-preferred samples. We conducted a detailed theoretical analysis of these issues in Section~\ref{subsec: mismatch}.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/figure1.pdf}
    \vspace{-20pt}
    \caption{Framework overview of (a) previous method and (b) TailorPO. 
    In the previous method, the preference order is determined based on final outputs and used to guide the optimization of intermediate noisy samples in different generation trajectories.
    In contrast, we generate noisy samples from the same input $x_t$ and directly rank their preference order for optimization.}
    \vspace{-5pt}
    \label{fig: d3po_framework}
\end{figure}


Therefore, in this paper, we propose a \textbf{Tailor}ed \textbf{P}reference \textbf{O}ptimization (TailorPO) framework to fine-tune diffusion models with DPO, which addresses the aforementioned challenges.
As Fig.~\ref{fig: d3po_framework}(b) shows, we generate noisy samples $(x^0_{t-1},x^1_{t-1})$ from the same input $x_{t}$ at each step. Then, we directly obtain the preference ranking of noisy samples based on their step-wise reward.
To this end, the most straightforward approach is directly evaluating the reward of noisy samples using a reward model. However, existing reward models are trained on natural images and do not apply to noisy samples.
To address this challenge, we formulate the denoising process as a Markov decision process (MDP) and derive a simple yet effective measurement for the reward of noisy samples.
Then, we utilize $p(x^0_{t-1}|x_{t},c)$ and $p(x^1_{t-1}|x_{t},c)$ to compute the DPO loss function for fine-tuning.
In this way, the gradient direction is proven to increase the generation probability of preferred samples while decreasing the probability of dis-preferred samples.

Moreover, we notice that TailorPO generates paired samples from the same $x_t$, potentially causing two samples to be similar in late denoising steps with large $t$. Such similarity may reduce the diversity of paired samples, thereby impacting the effectiveness of the DPO-based method.
To overcome this limitation, we propose to enhance the diversity of noisy samples by increasing their reward gap.
Specifically, we employ gradient guidance \citep{guo2024gradient} to generate paired samples, leveraging the gradient of differentiable reward models to increase the reward of preferred noisy samples. This strategy, termed \textit{TailorPO-G}, further improves the effectiveness of our TailorPO framework.

In experiments, we fine-tune Stable Diffusion v1.5 using TailorPO and TailorPO-G to enhance its ability to generate images that achieve elevated aesthetic scores and align with human preference. Additionally, we evaluate TailorPO on user-specific preferences, such as image compressibility. The experimental results indicate that diffusion models fine-tuned with TailorPO and TailorPO-G yield higher reward scores compared to those fine-tuned with other RLHF and DPO-style methods.


\textbf{Contributions} of this paper can be summarized as follows.
(1) Through theoretical analysis and experimental validation, we demonstrate the mismatch between the trajectory-level ranking and the step-level optimization in existing DPO methods for diffusion models. To the best of our knowledge, this is the first study that explicitly proves flaws in existing DPO frameworks for diffusion models.
(2) Based on these insights, we propose TailorPO, a framework tailored to the unique denoising structure of diffusion models.
Experimental results have demonstrated that TailorPO significantly improves the model's ability to generate human-preferred images.
(3) Furthermore, we incorporate gradient guidance of differentiable reward models in TailorPO-G to increase the diversity of training samples for fine-tuning to further enhance performance.


\section{Related Works}
\label{sec: related_works}

\textbf{Diffusion models.}
As a new class of generative models, diffusion models~\citep{shol2015deep,ho2020DDPM,song2021DDIM} transform Gaussian noises into images~\citep{dhariwal2021diffusion,ho2022cascaded,nichol2022glide,rombach2022LDM}, audios~\citep{liu2023audioldm}, videos~\citep{ho2022imagenvideo,singer2023video}, 3D shapes~\citep{zeng2022lion,poole2023dreamfusion,gu2023nerfdiff}, and robotic trajectories~\citep{janner2022planning,chen2024simple} through an iterative denoising process.
\citet{dhariwal2021diffusion} and \citet{ho2022classifier} further propose the classifier guidance and classifier-free guidance respectively to align the generated images with specific text descriptions for text-to-image synthesis.

\textbf{Learning diffusion models from human feedback.}
Inspired by the success of reinforcement learning from human feedback (RLHF) in large language models~\citep{ouyang2022rlhf,bai2022anthropic,openai2023gpt4}, many reward models have been developed for images preference, including aesthetic predictor~\citep{schuhmann2022laion}, ImageReward~\citep{xu2023imagereward}, PickScore model~\citep{kirstain2023pick}, and HPSv2~\citep{wu2023hps}.
Based on these reward models, \citet{lee2023aligning}, DPOK~\citep{ying2023dpok} and DDPO~\citep{black2024ddpo} formulated the denoising process of diffusion models as a Markov decision process (MDP) and fine-tuned diffusion models using the policy-gradient method.
DRaFT~\citep{clark2024draft}, and AlignProp~\citep{prabhudesai2023alignprop} directly back-propagated the gradient of reward models through the sampling process of diffusion models for fine-tuning.
In comparison, D3PO~\cite{yang2024d3po} and Diffusion DPO~\citep{wallace2024diffusiondpo} adapted the direct preference optimization (DPO)~\citep{rafailov2023DPO} on diffusion models and optimized model parameters at each denoising step. 
Considering the sequential nature of the denoising process, DenseReward~\citep{yang2024densereward} assigned a larger weight for initial steps than later steps when using DPO.

Most close to our work, \citet{liang2024spo} also pointed out the problematic assumption about the preference consistency between noisy samples and final images.
They addressed this problem by sampling from the same input and training a step-wise reward model, based on another assumption.
In comparison, our method does not require training a reward model for noisy samples.
Moreover, we first explicitly derive the theoretical flaws of previous DPO implementations in diffusion models, and we provide solutions with solid support.
Experiments also demonstrate that our framework outperforms SPO on various reward models.
\section{Method}
\label{sec: method}

\subsection{Preliminaries}
\label{subsec: preliminaries}

\textbf{Diffusion models.}
Diffusion models contain a forward process and a reverse denoising process. 
In the forward process, given an input $x_0$ sampled from the real distribution $p_\text{data}$, diffusion models gradually add Gaussian noises to $x_0$ at each step $t\in[1,T]$, as follows:
\begin{equation}
\label{eq: add_noise}
    x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_{t-1} = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon
\end{equation}
where $\epsilon_t\sim\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$ denotes the Gaussian noise at step $t$.
$\alpha_{1:T}$ denotes the variance schedule and $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$.

In the reverse denoising process, the diffusion model is trained to learn $p(x_{t-1}|x_t)$ at each step $t$. Specifically, following~\citep{song2021DDIM}, the denoising step at step $t$ is formulated as
\begin{equation}
\label{eq: DDIM}
    x_{t-1} = \sqrt{\bar{\alpha}_{t-1}}\underbrace{\left(\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t,t)}{\sqrt{\bar{\alpha}_t}}\right)}_{\hat{x}_{0}(x_t)\text{, predicted }x_0} + \underbrace{\sqrt{1-\bar{\alpha}_{t-1}-\sigma^2_t}\epsilon_\theta(x_t,t)}_{\text{direction pointing to }x_t} + \underbrace{\sigma_t\epsilon'_t}_{\text{random noise}}
\end{equation}
where $\epsilon_\theta(\cdot)$ is a noise prediction network with trainable parameters $\theta$, which aims to use $\epsilon_\theta(x_t,t)$ to predict the noise $\epsilon$ in Eq.~(\ref{eq: add_noise}) at each step $t$.
$\epsilon'_t\sim\mathcal{N}(\boldsymbol{0,I})$ is sampled from the standard Gaussian distribution.
In fact, $x_{t-1}$ is sampled from the estimated distribution $\mathcal{N}(\mu_\theta(x_t), \sigma_t^2\boldsymbol{I})$.
According to the reverse process, $\hat x_0(x_t)=(x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_\theta(x_t,t)/\sqrt{\bar{\alpha}_t}$ represents the predicted $x_0$ at step $x$.

\textbf{Direct preference optimization (DPO)~\citep{rafailov2023DPO}.} The DPO method was originally proposed to fine-tune large language models to align with human preferences based on paired datasets.
Given a prompt $x$, two responses $y_0$ and $y_1$ are sampling from the generative model $\pi_\theta$, \emph{i.e.,} $y_0,y_1\sim\pi_\theta(y|x)$.
Then, $y_0$ and $y_1$ are ranked based on human preferences or the outputs $r(x,y_0)$ and $r(x,y_1)$ of a pre-trained reward model $r(\cdot)$.
Let $y_w$ denote the preferred response in $(y_0,y_1)$ and $y_l$ denote the dis-preferred response.
DPO optimizes parameters $\theta$ in $\pi_\theta$ by minimizing the following loss function.
\begin{equation}
\label{eq: DPO}
\mathcal{L}_\text{DPO}(\theta) = -\mathbb{E}_{(x,y_w,y_l)}\left[\log \sigma\left(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)}\right)\right]
\end{equation}
where $\sigma$ is the sigmoid function, and $\beta$ is a hyper-parameter. $\pi_\text{ref}$ represents the reference model, usually set as the pre-trained models before fine-tuning.
The gradient of the above loss function on each pair of $(x, y_w, y_l)$ with respect to the parameters $\theta$ is as follows~\citep{rafailov2023DPO}.
\begin{equation}
\label{eq: gradient of DPO}
    \nabla_\theta \mathcal{L}_\text{DPO}(\theta,x,y_w,y_l) = -f(x,y_w,y_l)\left(\nabla_\theta \log \pi_\theta(y_w|x) - \nabla_\theta \log \pi_\theta(y_l|x) \right)
\end{equation}
where $f(x,y_w,y_l)\triangleq\beta(1-\sigma(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_\text{ref}(y_l|x)}))$.
Therefore, the gradient of the DPO loss function increases the likelihood of the preferred response $y_w$ and decreases the likelihood of the dis-preferred response $y_l$.


\subsection{Mismatch between trajectory-level ranking and step-level optimization}
\label{subsec: mismatch}
In this section, we first revisit how existing works implement DPO for diffusion models, using D3PO~\citep{yang2024d3po} as an example for explanation. Then, we identify the mismatch between their trajectory-level ranking and step-level optimization from two perspectives.

For a text-to-image diffusion model $\pi_\theta$ parameterized by $\theta$, given a text prompt $c$, D3PO first samples a pair of generation trajectories $[x^0_T,\ldots, x^0_0]$ and $[x^1_T, \ldots, x^1_0]$. Then, they compare the reward scores $r(c,x^0_0)$ and $r(c,x^1_0)$ of generated images, using the reward model $r(\cdot)$, and rank their preference order.
The preferred image is denoted by $x^w_0$ and the dis-preferred image is denoted by $x^l_0$.
Then, as Figure~\ref{fig: d3po_framework}(a) shows, it is assumed that the preference order of final images $(x^0_0, x^1_0)$ represents the preference order of $(x^0_t, x^1_t)$ at all intermediate steps $t$.
Subsequently, the diffusion model is fine-tuned by minimizing the following DPO-like loss function at the step level.
\begin{equation}
\label{eq: D3PO}
\mathcal{L}_\text{D3PO}(\theta) = -\mathbb{E}_{(c,x^w_{t}, x^l_{t},x^w_{t-1}, x^l_{t-1})} \left[\log \sigma \left(\beta\log \frac{\pi_\theta(x^w_{t-1}|x^w_{t}, c)}{\pi_\text{ref}(x^w_{t-1}|x^w_{t}, c)} - \beta\log \frac{\pi_\theta(x^l_{t-1}|x^l_{t}, c)}{\pi_\text{ref}(x^l_{t-1}|x^l_{t}, c)} \right) \right]
\end{equation}

We argue that there are two critical issues in the aforementioned process and loss function, which we will elaborate on and prove through theoretical analysis in the following sections.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/step_reward.pdf}
    \vspace{-15pt}
    \caption{The preference order of intermediate noisy samples is not always consistent with the preference order of final output images, from both perspectives of the aesthetic score (red) and ImageReward score (blue).}
    \vspace{-5pt}
    \label{fig: step reward}
\end{figure}

\textbf{Inaccurate preference order.}
The first obvious issue is that the preference order of final images $x_0$ at the end of the trajectory cannot accurately reflect the preference order of noisy samples $x_t$ at intermediate steps.
\citet{liang2024spo} demonstrated that early steps in the denoising process tend to handle layout, while later steps focus more on detailed textures. However, the preference order based on final images primarily reflects layout and composition preferences, misaligning with the function of later steps.
Beyond these visual discoveries, we rethink this problem from another perspective and theoretically formulate the reward at intermediate steps.

Similar to \citep{yang2024d3po}, we formulate the denoising process in a diffusion model as a Markov decision process (MDP), as follows.
\begin{equation}
\begin{aligned}
    S_t\triangleq (c,x_{T-t}),~
    A_t\triangleq x_{T-t-1},~
    R_t = R(S_t, A_t) \triangleq R((c,x_{T-t}), x_{T-t-1})\\    P(S_{t+1}|S_t,A_t)\triangleq(\delta_c,\delta_{x_{T-t-1}}),~ \pi(A_t|S_t)\triangleq \pi_\theta(x_{T-t-1}|x_{T-t},c)
\end{aligned}
\end{equation}
where $S_t, A_t, R_t, P(S_{t+1}|S_t,A_t)$, and $\pi(A_t|S_t)$ denote the state, action, reward, state transition probability, and the policy in MDP, respectively.
Based on the above MDP, we aim to maximize the action value function at time $t$, \emph{i.e.,} $Q(s,a)=\mathbb{E}[G_t|S_t=s,A_t=a]$, where $G_t$ denotes the cumulative return at step $t$. We define $G_t$ using the $TD(1)$ formulation and assume $R_t=0$ for $t<T$ in diffusion models. Then, we obtain $G_t=R_t=r(c,x_0)$, which evaluates the reward value of the generated image.
In this way, the action value function is simplified as follows.
\begin{equation}
    Q(s,a)=\mathbb{E}[r(c,x_0)|S_t=(c, x_{T-t}),A_t=x_{T-t-1}] = \mathbb{E}[r(c,x_0)|c, x_{T-t-1}]
\end{equation}
In other words, the quality of noisy samples $x_{T-t-1}$ can be determined by the expected reward of all possible generation trajectories originating from $x_{T-t-1}$.
In contrast, the reward $r(c,x_0)$ of an image from a single trajectory is insufficient to represent the quality of the intermediate denoising action.
Based on this analysis, we demonstrate that \textit{the preference order of final images cannot accurately represent the preference order of intermediate noisy samples.}


To better illustrate this issue, we first propose a method for evaluating the quality of intermediate noisy samples, followed by an experimental validation using this method. The results shown in Figure~\ref{fig: step reward} demonstrate that the preference order between a pair of intermediate samples $x_t$ can sometimes conflict with the preference order between the corresponding denoised images $x_0$. This finding likewise provides evidence against the validity of the assumption employed in previous methods. The proposed evaluation method and our framework will be elaborated in the subsequent sections.



\textbf{Disturbed gradient direction.}
Moreover, even if we obtain an accurate preference order of noisy samples at intermediate steps, the loss function in Eq.~(\ref{eq: D3PO}) still has limitations from the gradient perspective.
To gain a mechanistic understanding of the above loss function, we compute its gradient with respect to parameters $\theta$ as follows (please refer to Appendix~\ref{sec: app_proof} for the proof).
\begin{equation}
\label{eq: gradient of d3po}
\begin{aligned}
    \!\!\nabla_\theta \mathcal{L}_\text{D3PO}(\theta) &=-\mathbb{E}\left[(f_t/{\sigma^2_{t}})\cdot[\nabla^T_\theta \mu_\theta(x^w_{t})(x^w_{t-1}-\mu_\theta(x^w_{t}))-\nabla^T_\theta \mu_\theta(x^l_{t})(x^l_{t-1}-\mu_\theta(x^l_{t}))]\right]\\
    f_t & \triangleq\beta(1-\sigma(\beta\log \frac{\pi_\theta(x^w_{t-1}|x^w_{t}, c)}{\pi_\text{ref}(x^w_{t-1}|x^w_{t}, c)} - \beta\log \frac{\pi_\theta(x^l_{t-1}|x^l_{t}, c)}{\pi_\text{ref}(x^l_{t-1}|x^l_{t}, c)}))
\end{aligned}
\end{equation}
In the above equation, the gradient is significantly affected by the relationship between inputs $x^w_{t}$ and $x^l_{t}$ from the previous step. This is because the input conditions ($x^w_{t}, x^l_t$) of generation probabilities for preferred sample $x^w_{t-1}$ and dis-preferred sample $x^l_{t-1}$ in Eq.~(\ref{eq: D3PO}) are different.
Therefore, the choice of $x^w_{t}$ and $x^l_{t}$ disturbs the original optimization direction of DPO.
In particular, if $\nabla_\theta \mu_\theta(x^w_{t})\approx\nabla_\theta \mu_\theta(x^l_{t})$, then the gradient term can be written as:
\begin{equation}
    \!\!\!\!\nabla_\theta \mathcal{L}_\text{D3PO}(\theta) \!\approx\! -\mathbb{E}\left[(f_t/{\sigma^2_{t}})\cdot\nabla^T_\theta \mu_\theta(x^w_{t})[(x^w_{t-1}-x^l_{t-1}) + (\mu_\theta(x^l_{t})-\mu_\theta(x^w_{t}))]\right]
\end{equation}
It shows that if $x^w_{t}$ and $x^l_{t}$ are located in the same linear subspace, then the optimization direction of the model shifts towards the direction $\mu_\theta(x^l_{t}) - \mu_\theta(x^w_{t})$, which points to the dis-preferred samples.
Thus, the fine-tuning effectiveness of DPO is significantly weakened.


\subsection{Tailored preference optimization framework for diffusion models}
\label{subsec: framework}

\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figs/our_framework_new.pdf}
    \vspace{-5pt}
    \caption{Framework of TailorPO. At each step $t$, we start from the same $x_t$ to generate two noisy samples $x^0_{t-1}$ and $x^1_{t-1}$. Subsequently, we compare their step-wise reward to determine their preference order. For the preferred sample, if the reward model is differentiable, we employ the gradient guidance to further increase its reward to obtain $x^+_{t-1}$. Then, we optimize the generating probability of preferred and dis-preferred samples. After the optimization at step $t$, the preferred sample is taken as the input $x_{t-1}$ of the next step for later sampling and optimization.}
    \label{fig: framework}
\end{figure}

To address the aforementioned issues, considering the characteristics of diffusion models, we propose a \textbf{Tailor}ed \textbf{P}reference \textbf{O}ptimization (TailorPO) framework for fine-tuning diffusion models in this section.
Specifically, given a text prompt $c$ and the time step $t$, we always start from the \textit{\textbf{same $x_t$}} to generate the next time-step noisy samples, \emph{i.e.,} $x^0_{t-1}$ and $x^1_{t-1}$.
Then, we estimate the step-wise reward of intermediate noisy samples $x^0_{t-1}$ and $x^1_{t-1}$ to directly rank their preference order.
The sample with the higher reward value is represented by $x^w_{t-1}$, and the sample with the lower reward is denoted as $x^l_{t-1}$.
Furthermore, if the reward function is differentiable, we apply the gradient guidance of the reward function (introduced in Section~\ref{subsec: gradient guidance}) to increase the reward of the preferred sample $x^w_{t-1}$, which enlarges the reward gap between $x^w_{t-1}$ and $x^l_{t-1}$ and enhances the fine-tuning effectiveness.
At the next denoising step $(t-1)$, the preferred sample $x^w_{t-1}$ is taken as $x_{t-1}$ for further sampling and training. 
Our framework is illustrated in Figure~\ref{fig: framework}, and the loss function is given as follows.
\begin{equation}
\label{eq: loss}
\mathcal{L}(\theta) = -\mathbb{E}_{(c,x_{t},x^w_{t-1}, x^l_{t-1})} \left[\log \sigma \left(\beta\log \frac{\pi_\theta(x^w_{t-1}|x_{t}, c)}{\pi_\text{ref}(x^w_{t-1}|x_{t}, c)} - \beta\log \frac{\pi_\theta(x^l_{t-1}|x_{t}, c)}{\pi_\text{ref}(x^l_{t-1}|x_{t}, c)} \right) \right]
\end{equation}

We will subsequently elucidate and substantiate the advantages of our proposed TailorPO framework for diffusion models from the following perspectives.


\textbf{Consistency between gradient direction and preferred samples.}
First, TailorPO addresses the problem of the gradient direction by always generating paired samples from the same $x_t$.
We theoretically analyze the underlying mechanism behind its effectiveness and prove that it better aligns the gradient directions with human preference.
Specifically, this simple operation ensures that the generation probabilities in Eq.~(\ref{eq: loss}) are all based on the same condition, aligning with the original formulation of DPO in Eq.~(\ref{eq: DPO}).
In this way, the gradient of our loss function is given as follows (please refer to Appendix~\ref{sec: app_proof} for the proof).
\begin{equation}
\label{eq: gradient of ours}
    \nabla_\theta \mathcal{L}(\theta) = -\mathbb{E}\left[(f_t/{\sigma^2_{t}})\cdot\nabla^T_\theta\mu_\theta(x_{t})(x^w_{t-1} - x^l_{t-1})\right]
\end{equation}
Notably, the gradient direction of our loss function clearly points towards the preferred samples.
Therefore, the model is effectively encouraged to generate preferred samples.

\textbf{Intermediate-step preference ranking.}
Instead of performing preference ranking on final images, we directly rank the preference order of noisy samples at intermediate steps.
Different from \citep{liang2024spo}, which trained a step-wise reward model, we directly evaluate the preference of noisy samples $x_t$ without training a new model.
As discussed in Section~\ref{subsec: mismatch}, the denoising process of a diffusion model can be formulated as an MDP, where the action value function for generating $x_t$ simplifies to the expected reward of images over all trajectories starting from $x_t$.
Therefore, we define the step-wise reward value of the noisy sample $x_t$ as follows.
\begin{equation}
\label{eq: reward}
    r_t(c, x_t) \triangleq \mathbb{E}[r(c,x_0)|c, x_t] \approx r(c, \hat{x}_0(x_t))
\end{equation}
However, computing the above expectation over all trajectories is intractable. Therefore, we employ an approximation to the expectation value.
Previous studies~\citep{chung2023diffusion,guo2024gradient} have proven that $\mathbb{E}[x_0|c, x_t] = \hat{x}_0(x_t)$, which represents the predicted $x_0$ at step $t$ (defined in Eq.~(\ref{eq: DDIM})). Furthermore, \citet{chung2023diffusion} prove the following Proposition~\ref{prop: expected reward}, which ensures that the expectation of image rewards $\mathbb{E}[r(c,x_0)|c,x_t]$ can be approximated by the reward of the expected image $r(c, \mathbb{E}[x_0|c,x_t])$.
Therefore, we compute $r_t(c, x_t) \approx r(c, \hat{x}_0(x_t))$ to estimate the step-wise reward of $x_t$ for preference ranking.
In Appendix~\ref{subsec: verify step-wise reward}, we verify that the estimation error is small through the training process, thus the obtained preference ranking is reliable.

\begin{proposition}[proven by \citet{chung2023diffusion}]
\label{prop: expected reward}
Let a measurement $g(x_0)=\mathcal{A}(x_0)+n$, where $\mathcal{A}(\cdot)$ is a measure operator defined on images $x_0$ and $n\sim\mathcal{N}(0, \sigma^2 I)$ is the measurement noise.
The Jensen gap between $\mathbb{E}[g(x_0)|c, x_t]$ and $g(\mathbb{E}[x_0|c,x_t])$, \emph{i.e.,} $\mathcal{J}=\mathbb{E}[g(x_0)|c, x_t]-g(\mathbb{E}[x_0|c,x_t])$ is bounded by
$\mathcal{J} \le \frac{d}{\sqrt{2\pi \sigma^2}}e^{-1/2\sigma^2} \Vert \nabla_x \mathcal{A}(x)\Vert m_1$, where $\nabla_x \mathcal{A}(x) \triangleq \max_x \Vert \nabla_x \mathcal{A}(x)\Vert$, $m_1\triangleq\int \Vert x_0 - \hat{x}_0 \Vert p(x_0|c,x_t) dx_0$, and $\hat{x}_0=\mathbb{E}[x_0|c,x_t]$.
The Jensen gap can approach 0 as $\sigma$ increases.
\vspace{-5pt}
\end{proposition}

By obtaining the preference order of noisy samples immediately at intermediate steps, we can fine-tune the model using Eq.~(\ref{eq: loss}).
Then, the preferred sample $x^w_{t-1}$ is assigned as the input for the next step, enabling sampling and optimization in subsequent steps.


\subsection{Gradient guidance of reward model for fine-tuning}
\label{subsec: gradient guidance}


\begin{figure}[t]
    \centering
    \begin{minipage}{0.34\linewidth}
        \centering
        \captionof{table}{Gradient guidance successfully increased/decreased the reward of most samples.}
        \label{tab: gradient guidance}
    \end{minipage}
    \hfill
    \begin{minipage}{0.63\linewidth}
        \vspace{-7pt}
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{ c|c c c c c}
        \hline
        $t$ & 20 & 16 & 12 & 8 & 4 \\
        \hline
        ratio of $r_t(c,x^+_{t-1})>r_t(c,x_{t-1})$ & 0.83 & 0.97 & 0.98 & 0.99 & 0.99  \\
        ratio of $r_t(c,x^-_{t-1})<r_t(c,x_{t-1})$ &  0.87 & 0.98 & 1.00 & 0.98 & 1.00 \\
        \hline
        \end{tabular}
        }
    \end{minipage}
\end{figure}

\begin{algorithm}[t]
    \KwIn{Diffusion model $\pi_\theta(\cdot)$, reference model $\pi_\text{ref}(\cdot)$, reward model $r(\cdot)$}
    Sample a text prompt $c$\;
    Initialize $x_T\sim\mathcal{N}(0,\boldsymbol{I})$\;
    \For{$t=T,\ldots,1$}{
        Sample $x^0_{t-1}$, $x^1_{t-1}$ from $\pi_\theta(\cdot|x_t,c)$\;
        Rank $x^0_{t-1}$ and $x^1_{t-1}$ based on their step-wise rewards to obtain $x^w_{t-1}$ and $x^l_{t-1}$\;
        Inject gradient guidance to compute $x^+_{t-1}=x^w_{t-1} - \eta_t \nabla_{x^w_{t-1}}(r_\text{high}-r_t(c,x^w_{t-1}))^2$\;
        \If{$r_t(c,x^+_{t-1}) >r_t(c,x^w_{t-1})$}{
        $x^w_{t-1} \leftarrow x^+_{t-1}$
        }
        Optimize $\pi_\theta(\cdot)$ using Eq.~(\ref{eq: loss})\;
        $x_{t-1} \leftarrow x^{w}_{t-1}$\;
    }
    \KwOut{The fine-tuned diffusion model $\pi_\theta(\cdot)$.}
\caption{The TailorPO-G framework for aligning diffusion models with human preference.}
\label{alg: gradient guidance}
\end{algorithm}

In TailorPO, since noisy samples ($x^0_{t-1}, x^1_{t-1}$) are generated from the same $x_t$, their similarity increases as $t$ decreases.
This increasing similarity potentially reduces the diversity of paired samples for training.
On the other hand, \citet{khaki2024rsdpo} have shown that a large difference between paired samples is beneficial to the DPO effectiveness.
Therefore, to enhance the DPO performance in this case, we propose enlarging the difference between two noisy samples from the reward perspective.

To this end, we consider how to adjust the reward of a noisy sample $x_{t-1}$.
Similar to~\citep{guo2024gradient}, we use $r_\text{high}$ to represent an expected higher reward.
Then, the gradient of the conditional score function is $\nabla_{x_{t-1}} \log p(x_{t-1}|r_\text{high})=\nabla \log p(x_{t-1}) + \nabla_{x_{t-1}}\log p(r_\text{high}|x_{t-1})$, where the first term $\nabla\log p(x_{t-1})$ is estimated by the diffusion model itself, and the second term is to be estimated by the guidance.
\citet{guo2024gradient} further prove the following relationship for estimation.
\begin{equation}
    \nabla_{x_{t-1}}\log p(r_\text{high}|x_{t-1}) \propto \nabla_{x_{t-1}}\log p(r_\text{high}|\hat{x}_0(x_{t-1})) \propto -\eta_t \nabla_{x_{t-1}}(r_\text{high} - r_t(c,x_{t-1}))^2
\end{equation}
Therefore, we can inject the gradient term $\nabla_{x_{t-1}}(r_\text{high} - r_t(c,x_{t-1}))^2$ as the guidance to the generation of $x_{t-1}$ to adjust its reward. 
Specifically, we update the noisy samples as follows.
\begin{equation}
\begin{aligned}
\label{eq: gradient guidance}
    & x^{+}_{t-1}\leftarrow x_{t-1} - \eta_t \nabla_{x_{t-1}}(r_\text{high} - r_t(c,x_{t-1}))^2,~ \text{to increase reward}\\
    & x^{-}_{t-1}\leftarrow x_{t-1} + \eta_t \nabla_{x_{t-1}}(r_\text{high} - r_t(c,x_{t-1}))^2, ~\text{to decrease reward}
\end{aligned}
\end{equation}
To demonstrate that the above gradient guidance is able to adjust the reward of noisy samples as expected, we compared the step-wise rewards of the original sample $x_{t-1}$, the increased sample $x^+_{t-1}$, and the decreased sample $x^-_{t-1}$. Specifically, we generated $100$ noisy samples $x_{t-1}$ from Stable Diffusion v1.5~\citep{rombach2022LDM}, and then computed the corresponding $x^+_{t-1}$ and $x^-_{t-1}$.
We set $\eta_t=0.2$ and $r_\text{high}=r_t(c,x_{t-1}) + \delta$ following~\citet{guo2024gradient}, where the constant $\delta=0.5$ specified the expected increment of the reward value.

Then, we computed the ratio of increased samples (satisfying $r_t(c,x^+_{t-1}) > r_t(c, x_{t-1})$) and the ratio of decreased samples (satisfying $r_t(c,x^-_{t-1}) < r_t(c, x_{t-1})$).
Table~\ref{tab: gradient guidance} shows that for almost all samples, the gradient guidance successfully increased or decreased their reward as expected, demonstrating its effectiveness in adapting the reward of samples.


Finally, we apply this method in our training process to enlarge the reward gap between a pair of noisy samples and develop the \textit{TailorPO-G} framework.
As shown in Figure~\ref{fig: framework} and Algorithm~\ref{alg: gradient guidance}, we first modify the preferred sample $x^w_{t-1}$ to increase its reward value, and then use the modified sample for fine-tuning and subsequent sampling. 
In Appendix~\ref{sec: app_tailorpo-g}, we analyze the gradient of TailorPO-G and demonstrate that the gradient guidance of reward models pushes the model generations towards the high-reward regions in the reward model.

\section{Experiments}
\label{sec: experiments}

\textbf{Experimental settings.}
In our experiments, we evaluate the effectiveness of our method in fine-tuning Stable Diffusion v1.5~\citep{rombach2022LDM}.
We compared our TailorPO method with the RLHF method, DDPO~\citep{black2024ddpo}, and DPO-style methods, including D3PO~\citep{yang2024d3po} and SPO~\citep{liang2024spo}.
For all methods, we used the aesthetic scorer~\citep{schuhmann2022laion}, ImageReward~\citep{xu2023imagereward}, PickScore~\citep{kirstain2023pick}, HPSv2~\citep{wu2023hps}, and JPEG compressibility measurement~\citep{black2024ddpo} as reward models.
Considering that some reward models are non-differentiable, we evaluate both the effectiveness of TailorPO and TailorPO-G, respectively.

Following the settings in D3PO~\citep{yang2024d3po} and SPO~\citep{liang2024spo}, we applied the DDIM scheduler~\citep{song2021DDIM} with $\eta=1.0$ and $T=20$ inference steps.
The generated images were of resolution of $512\times 512$.
We employed LoRA~\citep{hu2022lora} to fine-tune the UNet parameters on a total of 10,000 samples with a batch size of 2.
The reference model was set as the pre-trained Stable Diffusion v1.5 itself.
For SPO, we ran the officially released code by using the same hyper-parameters as in its original paper, and for other methods, we used the same hyper-parameters as in \citep{yang2024d3po}, except that we set a smaller batch size for all methods.
In particular, for all our frameworks, we generated images with $T=20$ and uniformly sampled $T_\text{fine-tune}=5$ steps for fine-tuning, \emph{i.e.,} we only fine-tuned the model at steps $t=20,16,12,8,4$. 
In addition, we set the coefficient $\eta_t$ in gradient guidance using a cosine scheduler in the range of $[0.1,0.2]$, which assigned a higher coefficient to smaller $t$ (samples closer to output images).
We have conducted ablation studies in Appendix~\ref{sec: app_ablation} to show that our method is relatively stable with respect to the setting of $T_\text{fine-tune}$ and $\eta_t$.
We have also conducted ablation studies on each component in our framework in Appendix~\ref{sec: app_ablation}.

\subsection{Effectiveness of aligning diffusion models with preference}

In this section, we demonstrate that our frameworks outperform previous methods in aligning diffusion models with various preferences, from both quantitative and qualitative perspectives.


\begin{table}[t]
    \centering
    \caption{Reward values of images generated by diffusion models fine-tuned using different methods. The prompts are related to common animals.
    \vspace{-5pt}}
    \label{tab: animal results}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c c c c c}
        \hline
        {} & Aesthetic scorer & ImageReward & HPSv2 & PickScore & Compressibility \\
        \hline
        Stable Diffusion v1.5 & 5.79 & 0.65 & 27.51 & 20.20 & -105.51 \\
        DDPO~\citep{black2024ddpo} & 6.57 & 0.99 & 28.00 & 20.24 & -37.37 \\
        D3PO~\citep{yang2024d3po} & 6.46 & 0.95 & 27.80 & 20.40 & -29.31 \\
        SPO~\citep{liang2024spo} & 5.89 & 0.95 & 27.88 & 20.38 & -- \\
        TailorPO & 6.66 & 1.20 & \textbf{28.37} & 20.34 & \textbf{-6.71} \\
        TailorPO-G & \textbf{6.96} & \textbf{1.26} & 28.03 &  \textbf{20.68} & --\\
        \hline
    \end{tabular}
    }
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/training_curve.pdf}
    \vspace{-15pt}
    \caption{The change curve of reward values during the fine-tuning process. Experiments were conducted for three runs and we plot the average value and standard deviation of the reward.}
    \vspace{-5pt}
    \label{fig: training curve}
\end{figure}

\textbf{Quantitative evaluation.}
We fine-tuned SD v1.5 on various reward models using a set of prompts of animals released by \citet{black2024ddpo} and a set of complex prompts in the Pick-a-Pic dataset~\citep{kirstain2023pick}, respectively.
For quantitative evaluation, we randomly sampled five images for each prompt and computed the average reward value of all images. For the animal-related prompts, Table~\ref{tab: animal results} demonstrates that both TailorPO and TailorPO-G outperform other methods across all reward models.
On the other hand, Figure~\ref{fig: training curve} shows curves of reward values throughout the fine-tuning process. It can be observed that our methods rapidly increase the reward of generations in early iterations.
Appendix~\ref{subsec: app_pick} compares results on prompts in the Pick-a-Pic dataset and shows that our method also effectively improved the reward values, surpassing SPO and the state-of-the-art offline method, Diffusion-DPO~\citep{wallace2024diffusiondpo}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/vis_animals.pdf}
    \vspace{-15pt}
    \caption{Visualization of images generated by diffusion models fine-tuned using different methods. For these animal-related prompts, diffusion models fine-tuned by TailorPO and TailorPO-G generated more colorful and visually pleasing images.}
    \vspace{-5pt}
    \label{fig: vis animals}
\end{figure}


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/vis_pickapic.pdf}
    \vspace{-10pt}
    \caption{Visualization of images generated by diffusion models fine-tuned on complex prompts in the Pick-a-Pic dataset. Prompts are given on the right with missing elements in SD v1.5 highlighted.}
    \vspace{-5pt}
    \label{fig: vis pickapic}
\end{figure}


\textbf{Qualitative comparison.}
For qualitative comparison, we first visualize the generated samples given simple prompts of animals in Figure~\ref{fig: vis animals}. 
It is obvious that after fine-tuning using TailorPO and TailorPO-G, the model generated more colorful and visually appealing images with fine-grained details. In addition, we fine-tuned Stable Diffusion v1.5 on more complex prompts, using prompts in the Pick-a-Pic training dataset~\citep{kirstain2023pick}.
Figure~\ref{fig: vis pickapic} shows that both TailorPO and TailorPO-G encourage the model to generate more aesthetically pleasing images, and these images were better aligned with the given prompts.
For example, in the third row of Figure~\ref{fig: vis pickapic}, the 5th and 6th images contained more consistent and aligned subjects, scenes, and elements with the prompts.


\begin{figure}[t]
    \centering
    \begin{minipage}{0.36\linewidth}
    \centering        
    \includegraphics[width=\linewidth]{figs/user_study.pdf}
    \caption{User-labeled win-lose ratio of TailorPO and TailorPO-G versus other baseline methods.}
    \label{fig: user study}
    \end{minipage}
    \hfill
    \begin{minipage}{0.61\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/prompt_generalization.pdf}
    \caption{Diffusion model fine-tuned on simple prompts generalized well to complex prompts. Prompts from left to right are: (1) cinematic still of a stainless steel robot swimming in a pool. (2) A cat that is riding a horse without a leg. (3) Crazy frog, on one wheel, motorcycle, dead. (4) A panda riding a motorcycle. (5) Fantasy castle on a hilltop.
    }
    \label{fig: prompt generalization}
    \end{minipage}
\end{figure}

\textbf{User study.}
Additionally, we conducted a user study by requesting ten users to label their preference for generated images from the perspective of visual appeal and general preference.
For each fine-tuned model, we generated images for each animal-related prompt and asked users to compare and annotate images generated by different models to indicate their preferences.
Figure~\ref{fig: user study} reports the win-lose percentage results of our method versus other baseline methods, where our method exhibits a clear advantage in aligning with human preference. More experimental details and the ethics statement about the user study can be seen in Appendix~\ref{sec: app_user_study}.



\subsection{Generalization to different prompts and reward models}

\begin{table}[t]
    \centering
    \caption{Prompt generalization: the model fine-tuned on simple prompts also exhibited higher reward values for unseen complex prompts.
    \vspace{-5pt}}
    \label{tab: prompt generalization}
    \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{c|c c c c c}
       \hline
        {} & Aesthetic scorer & ImageReward & HPSv2 & PickScore & Compressibility \\
        \hline
        SD v1.5 &  5.69 & -0.04 & 25.79 & 17.74 & -98.95 \\
        DDPO & 5.94 & 0.06 & 26.24 & 17.74 & -49.94 \\
        D3PO & 6.14  & 0.11 & 26.09 & 17.77 & -38.92 \\
        SPO & 5.79 & 0.15 & 26.28 & 17.16 & -- \\
        TailorPO & 6.26 & 0.11 & \textbf{26.64} & 17.85 & \textbf{-7.32} \\
        TailorPO-G & \textbf{6.45} & \textbf{0.25} & 26.25 & \textbf{17.93} & -- \\
        \hline
    \end{tabular}
    }
\end{table}

\begin{table}[t]
\centering
    \caption{Reward generalization: the model fine-tuned towards a reward model also exhibited higher reward values on other different but related reward models.
    \vspace{-5pt}}
    \label{tab: reward generation}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}{c|c c c c}
       \hline
        \diagbox{Train}{Evaluate} & Aesthetic scorer & ImageReward & HPSv2 & PickScore \\
        \hline
        SD v1.5 & 5.79 & 0.65 & 27.51 & 20.20 \\
        Aesthetic scorer &  \textbf{6.96} & \underline{1.04} & 27.63 & \underline{20.34} \\
        ImageReward & \underline{6.01} & \textbf{1.26} & \underline{28.01} & 20.21 \\
        HPSv2 & 5.45 & 0.92 & \textbf{28.03} & 20.04 \\
        PickScore & 5.94 & 0.83 & 27.71 & \textbf{20.68} \\
        \hline
    \end{tabular}
    }
\end{table}


In this section, we investigate the generalization ability of the fine-tuned model using our method. Here, we consider two types of generalization mentioned in~\citep{clark2024draft}: prompt generalization and reward generalization.

\textbf{Prompt generalization} refers to the model's ability to generate high-quality images for prompts beyond those used in fine-tuning.
To evaluate this, we fine-tuned Stable Diffusion v1.5 on 45 prompts of simple animal~\citep{black2024ddpo} and evaluated its performance on 500 complex prompts ~\citep{kirstain2023pick}. As shown in 
Table~\ref{tab: prompt generalization}, the model fine-tuned on simple prompts exhibited higher reward values on complex prompts than the original SD v1.5, with our approach achieving the highest performance.
Figure~\ref{fig: prompt generalization} presents examples of images generated from complex prompts, demonstrating that despite being fine-tuned on simple prompts, the model was also capable of generating high-quality images given complex prompts. This highlights the effectiveness of our method in enhancing the model's generalization to human-preferred images across various prompts, rather than overfitting to simple prompts.

\textbf{Reward generalization} refers to the phenomenon where fine-tuning the model towards a specific reward model can also enhance its performance on another different but related reward model.
We selected one reward model from the aesthetic scorer, ImageReward, HPSv2, and Pickscore for fine-tuning, and used the other three reward models for evaluation.
Table~\ref{tab: reward generation} shows that after being fine-tuned towards the aesthetic scorer, ImageReward, and PickScore, the model usually exhibited higher performance on all these four reward models.
In other words, our method boosted the overall ability of the model to generate high-quality images.



\section{Conclusions}
In this study, we rethink the existing DPO framework for aligning diffusion models and identify the potential flaws in these methods. We analyze these issues from both perspectives of preference order and gradient direction.
To address these issues, we consider the distinctive characteristics of diffusion models and introduce a tailored preference optimization framework for aligning diffusion models with human preference. Specifically, at each denoising step, our approach generates noisy samples from the same input and directly ranks their preference order for optimization.
Furthermore, we propose integrating gradient guidance into the training framework to enhance the training effectiveness.
Experimental results demonstrate that our approach significantly improved the reward scores of generated images, and exhibited good generalization over different prompts and different reward models.


\newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\include{appendix}


\end{document}
