\section{Architecture Details}
\subsection{Transformer}
\label{sm:transformer_architecture}
We use a vanilla encoder-decoder Transformer architecture for all Transformer experiments.
The decoder input is the last 48 time-steps in the sequence concatenated by an array of zeros with a length of the prediction window.
For our baseline experiment, the decoder self-attention has a causal mask, which is common in Transformer architectures, while the encoder self-attention and decoder cross-attention do not have masks.
Our WCMHA variant of the Transformer applies WCMHA to both the encoder and decoder self-attention, but not the decoder cross-attention.
We do not apply WCMHA to the decoder cross-attention because this process is already causal as it forecasts future points based on the given sequence.
We present the Transformer hyperparameters used in this work in Table~\ref{tab:powerformer_params}.


\subsection{PatchTST}
\label{sm:patchtst_architecture}
Here, we review PatchTST's patching mechanism, as PatchTST is otherwise similar to a traditional Transformer with a linear readout head that simultaneously predicts all future points.
For a detailed description of PatchTST, see Ref.~\cite{nie.patchtst.2023a}.
Let us consider an input time-series $\mathbf{X}$ with dimensions $[B, T_\text{seq}, D]$, where $B$ is batch size, $T_\text{seq}$ is the time interval, and $D$ is the number of input dimensions.
The multivariate input is first normalized to zero mean and unit variance along the time axis and flattened into $D$ univariate inputs, resulting in a dimensionality of $[B\times D, T_\text{seq}]$.
PatchTST batches these time series via a convolution with $N$ filters, a stride $s$, and patch size $p$, resulting in a dimensionality of $[B \times D, P, N]$.
Here, $P = (T_\text{seq}-p)//s + 1$ is the number of patches, and $//$ is integer division.
At this point, the patched data is fed into the transformer where a positional embedding is added and the patched embeddings are further contextualized by vanilla MHA~\cite{vaswani.transformer.2017}.
The linear output head takes the $[B \times D, P, N]$ output of the Transformer, flattens the last two dimensions, and applies a linear matrix multiplication (of shape $[P \times N, T_\text{pred}]$) to predict the output time series of length $T_\text{pred}$.
The linear head returns a matrix of shape $[B \times D, T_\text{pred}]$.
This matrix is reshaped into the original multivariate dimensionality $([B, T_\text{pred}, D])$ and renormalized.
We note that the patching mechanism is performed before applying the Transformer and is independent of it.

\subsection{Powerformer}
\label{sm:powerformer_architecture}
\emph{Powerformer} forecasts univariate time-series by combining WCMHA, the commonly used encoder-only Transformer architecture, and patching~\cite{nie.patchtst.2023a}.
We selected PatchTST as the base of \emph{Powerformer} since PatchTST already includes decomposes multivariate time-series into univariate channels and performs the patching.
Additionally, similar to other modern time-series Transformer-based models~\cite{zhou2023OFA, liu2024itransformer, talukder2024totem}, PatchTST~\cite{nie.patchtst.2023a} uses an encoder-only architecture with a linear readout head.
\emph{Powerformer} uses a standard Transformer encoder architecture with the primary difference being the replacement of the vanilla MHA by weighted causal WCMHA, described in Eqs.~\ref{eq:mask_causal}-\ref{eq:weight_ME}.
We additionally remove the dropout applied to the attention weights because our implicit biases enforce a deterministic temporal dependency on pairwise correlations.
Dropout enforces redundancy between pairwise correlations and is therefore in conflict with our implicit biases.
The input data comes in with $D$ variates and shape $[B, D, T_\text{seq}]$, where $B$ is the batch size and $T_\text{seq}$ is the input sequence length.
The PatchTST patching mechanism (described in Appendix~\ref{sm:patchtst_architecture}) returns the patched and normalized univariate embeddings with shape $[B \times D, P, N]$, where $P$ is the number of patches and $N$ is the size of the embeddings.
These embeddings are encoded through multiple WCMHA Transformer encoder layers (see Appendix~\ref{sm:hyperparams} for hyperparameter details).
The encoder output (of shape $[B \times D, P, N]$) is flattened along the last two dimensions ($[B \times D, P \times N]$) in preparation for the linear readout head.
The linear readout matrix has the shape $[P \times N, T_\text{pred}]$ and after acting on the flattened encoder output produces the univariate forecasts of shape $[B \times D, T_\text{pred}]$.
Finally, these univariate forecasts are renormalized and reshaped into the multivariate input structure $[B, D, T_\text{pred}]$ and returned to the user.
Consequently, \emph{Powerformer} is a simple Transformer-based model with causal and local masks added to the attention scores $\mathbf{S}$, and patching applied to the input data.
We present the \emph{Powerformer} hyperparameters used in this work in Table~\ref{tab:powerformer_params}.


\subsection{Model Tuning Parameters}
\label{sm:hyperparams}
To reproduce our results, we list our hyperparameters used on each dataset for \emph{Powerformer} (Table~\ref{tab:powerformer_params}) and Transformer (Table~\ref{tab:transformer_params}).
For each dataset and model, we trained with the following 3 random seeds: 2021, 1776, and 1953.
\begin{table}[!ht]
    \centering
    \caption{We provide \emph{Powerformer} architecture and training parameters used for each dataset.}
    \label{tab:powerformer_params}
    \vskip 0.1in
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \toprule
        \textbf{Parameters} & \textbf{ETTh1} & \textbf{ETTh2} & \textbf{ETTm1} & \textbf{ETTm2} & \textbf{Weather} & \textbf{Electricity} & \textbf{Traffic} \\
        \midrule
        Sequence Length     & 336 / 512   & 336 / 512   & 336 / 512   & 336 / 512   & 336 / 512 & 336 / 512   & 336 / 512  \\
        Patch Length        & 16    & 16    & 16    & 16    & 16    & 16    & 16    \\
        Patch Stride        & 8     & 8     & 8     & 8     & 8     & 8     & 8     \\
        Encoder Layers      & 3     & 3     & 3     & 3     & 3     & 3     & 3     \\
        Embedding Size      & 16    & 16    & 128   & 128   & 128   & 128   & 128   \\
        MHA Heads           & 4     & 4     & 16    & 16    & 16    & 16    & 16    \\
        MHA Feed Forward    & 128   & 128   & 256   & 256   & 256   & 256   & 256   \\
        Dropout \%          & 30    & 30    & 20    & 20    & 20    & 20    & 20    \\
        Linear Dropout \%   & 30    & 30    & 20    & 20    & 20    & 20    & 20    \\
        Train Epochs        & 100   & 100   & 100   & 100   & 100   & 100   & 100   \\
        Patience            & --    & --    & 20    & 20    & 20    & 10    & 10    \\
        Learning Rate       & $10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$ \\
        Batch Size          & 128   & 128   & 128   & 128   & 128   & 32    & 24\\
        \bottomrule
    \end{tabular}}
\end{table}


\begin{table}[!ht]
    \centering
    \caption{We provide Transformer architecture and training parameters used for each dataset.}
    \label{tab:transformer_params}
    \vskip 0.1in
    \resizebox{0.97\textwidth}{!}{
    \begin{tabular}{c|c|c|c|c|c|c|c}
        \toprule
        \textbf{Parameters} & \textbf{ETTh1} & \textbf{ETTh2} & \textbf{ETTm1} & \textbf{ETTm2} & \textbf{Weather} & \textbf{Electricity} & \textbf{Traffic} \\
        \midrule
        Sequence Length     & 256   & 256   & 256   & 256   & 256   & 256   & 256  \\
        Decoder Input Length        & 48    & 48    & 48    & 48    & 48    & 48    & 48 \\
        Encoder Layers      & 2     & 2     & 2     & 2     & 2     & 2     & 2     \\
        Decoder Layers      & 1     & 1     & 1     & 1     & 1     & 1     & 1     \\
        Embedding Size      & 512    & 512    & 512   & 512   & 512   & 512   & 512   \\
        MHA Heads           & 8     & 8     & 8    & 8    & 8    & 8    & 8    \\
        MHA Feed Forward    & 2048   & 2048   & 2048   & 2048   & 2048   & 2048   & 2048   \\
        Dropout \%          & 5    & 5    & 5    & 5    & 5    & 5    & 5    \\
        Linear Dropout \%   & 5    & 5    & 5    & 5    & 5    & 5    & 5    \\
        Train Epochs        & 100   & 100   & 100   & 100   & 100   & 100   & 100   \\
        Patience            & --    & --    & --    & --    & --    & --    & --    \\
        Learning Rate       & $10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$&$10^{-4}$ \\
        Batch Size          & 128   & 128   & 128   & 128   & 128   & 128   & 128   \\
        \bottomrule
        
    \end{tabular}}
\end{table}
