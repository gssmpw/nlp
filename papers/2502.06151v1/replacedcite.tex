\section{Related Work}
The importance of time-series data has led to decades of work, resulting in a myriad of methods: early statistical methods ____ (such as ARIMA ____); multilayer perceptions (MLP) ____ (such as NHITS ____, TSMixer ____, and N-BEATS ____); convolutional neural networks (CNNs) ____ (such as TimesNet ____ and SCINet ____); recurrent neural networks (RNNs) ____ (such as DeepAR ____); Transformers ____ (such as PatchTST ____, Autoformer ____, FEDformer ____, and iTransformer ____); and state space models (SSMs) ____ (such as MAMBA ____).
Of these methods, we focus on Transformers.

The success of LLMs and the introduction of powerful foundation models, like GPT-2 ____, inspired models built atop them and others looking to improve upon the Transformer architecture.
For example, Time-GPT ____ uses the Transformer encoder-decoder structure, and One-Fits-All ____ is built atop a pretrained GPT-2 and fine-tunes the attention layer normalization and temporal embedding.
Some models like AutoTimes ____ leverage LLM's in-context learning capabilities and employ textual prompt engineering to feed into LLMs.
Other models have adapted the attention mechanism to time-series data: some models strongly rely on, or partially include, Fourier representations when calculating attention weights ____; AutoFormer ____ leverages temporal autocorrelations as a similarity metric; Reformer ____ uses a locality-sensitive hashing mechanism to reduce the attention's complexity; Informer ____ selects only high attention weight values, increasing prediction capacity; FlowFormer ____ uses flow conservation to achieve linear complexity attention without imposing locality implicit biases; and iTransformer ____ inverts the temporal and embedding dimensions, applying attention along the embedding.  

Transformers were developed for discrete inputs, unlike the continuous valued time-series inputs.
Patching along the time domain (PatchTST) shortens the context window which allows for longer lookback windows and significantly improves performance over more complicated Transformer methods ____.
This simple yet powerful improvement has made patching ubiquitous.
Other works look to convert time-series into discretized tokens similar to word vectors.
Chronos ____ normalizes the input sequences and matches these continuous values to tokens made from discretizing a sufficiently large domain.
TOTEM ____ similarly discretizes the input time series, but instead VQVAE ____ encodes the input and matches it with the most similar token.