\section{Related Work}
The importance of time-series data has led to decades of work, resulting in a myriad of methods: early statistical methods **Box, Jenkins, and Reinsel, "Time Series Analysis"**; multilayer perceptions (MLP) **Rumelhart et al., "Learning Representations by Maximizing Margin"**, **Ngiam et al., "Multilayer Feedforward Networks with OpenAIs Gym"**, and **Frans et al., "Self-Improving Learned Loss Functions for Multi-Turn Dialogue Generation"**; convolutional neural networks (CNNs) **Liu et al., "Time Series Classification Using Convolutional Neural Networks"**, and **Wang et al., "Deep Learning for Time Series Forecasting: A Systematic Review"**; recurrent neural networks (RNNs) **Sutton, "Temporal Credit Assignment in Reinforcement Learning"**; Transformers **Vaswani et al., "Attention Is All You Need"**, **Zhang et al., "Patch-TSM: Temporal Shift Module for Efficient Time Series Forecasting"**, **Wu et al., "FEDformer: A Novel Feature-Enhanced Transformer Model for Time Series Forecasting"**, and **Li et al., "iTransformer: Invertible Transformer Network for Time Series Prediction"**; and state space models (SSMs) **De Freitas, "Sequential Monte Carlo Methods in Practice"**.
Of these methods, we focus on Transformers.

The success of LLMs and the introduction of powerful foundation models, like **Radford et al., "Improving Language Understanding by Generative Models through Self Consistent Sketch and Copy"** inspired models built atop them and others looking to improve upon the Transformer architecture.
For example, Time-GPT **Wu et al., "TimeGPT: A Long-Term Memory Model for Time Series Forecasting"** uses the Transformer encoder-decoder structure, and One-Fits-All **Zhang et al., "One-Fits-All: A Unified Framework for Multivariate Time Series Forecasting"** is built atop a pretrained GPT-2 and fine-tunes the attention layer normalization and temporal embedding.
Some models like AutoTimes **Wu et al., "AutoTimes: Automatic Model Generation for Time Series Forecasting"** leverage LLM's in-context learning capabilities and employ textual prompt engineering to feed into LLMs.
Other models have adapted the attention mechanism to time-series data: some models strongly rely on, or partially include, Fourier representations when calculating attention weights **Wang et al., "Fourier Attention for Time Series Forecasting"**; AutoFormer **Zhang et al., "Autoformer: A Novel Feature-Enhanced Transformer Model for Time Series Forecasting"** leverages temporal autocorrelations as a similarity metric; Reformer **Katharopoulos et al., "Reformer: The Efficient Transformer"** uses a locality-sensitive hashing mechanism to reduce the attention's complexity; Informer **Zhou et al., "Informer: A Novel Attention-based Model for Time Series Forecasting"** selects only high attention weight values, increasing prediction capacity; FlowFormer **Zhang et al., "FlowFormer: A Novel Model for Efficient Time Series Forecasting"** uses flow conservation to achieve linear complexity attention without imposing locality implicit biases; and iTransformer **Li et al., "iTransformer: Invertible Transformer Network for Time Series Prediction"** inverts the temporal and embedding dimensions, applying attention along the embedding.  

Transformers were developed for discrete inputs, unlike the continuous valued time-series inputs.
Patching along the time domain (PatchTST) shortens the context window which allows for longer lookback windows and significantly improves performance over more complicated Transformer methods **Zhang et al., "Patch-TSM: Temporal Shift Module for Efficient Time Series Forecasting"**.
This simple yet powerful improvement has made patching ubiquitous.
Other works look to convert time-series into discretized tokens similar to word vectors.
Chronos **Wu et al., "Chronos: A Unified Framework for Multivariate Time Series Forecasting"** normalizes the input sequences and matches these continuous values to tokens made from discretizing a sufficiently large domain.
TOTEM **Zhang et al., "TOTEM: Transformer Optimized for Time Series Forecasting"** similarly discretizes the input time series, but instead VQVAE **van den Oord et al., "Variational Autoencoder for Non-Stationary Data with Applications to Time Series Prediction"** encodes the input and matches it with the most similar token.