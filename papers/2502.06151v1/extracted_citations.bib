@article{Chen.mlp_vs_transformer.2023,
author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
title={Are Transformers Effective for Time Series Forecasting?},
volume={37},
url={https://ojs.aaai.org/index.php/AAAI/article/view/26317},
DOI={10.1609/aaai.v37i9.26317},
number={9},
year={2023},
month={Jun.},
pages={11121-11128}
}

@article{He_2019,
doi = {10.1088/1742-6596/1213/4/042050},
url = {https://dx.doi.org/10.1088/1742-6596/1213/4/042050},
year = {2019},
month = {jun},
publisher = {IOP Publishing},
volume = {1213},
number = {4},
pages = {042050},
author = {Yangdong He and Jiabao Zhao},
title = {Temporal Convolutional Networks for Anomaly Detection in Time Series},
journal = {Journal of Physics: Conference Series},
}

@inproceedings{Kitaev2020Reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@article{SALINAS20201181,
title = {DeepAR: Probabilistic forecasting with autoregressive recurrent networks},
journal = {International Journal of Forecasting},
volume = {36},
number = {3},
pages = {1181-1191},
year = {2020},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301888},
author = {David Salinas and Valentin Flunkert and Jan Gasthaus and Tim Januschowski},
keywords = {Probabilistic forecasting, Neural networks, Deep learning, Big data, Demand forecasting},
abstract = {Probabilistic forecasting, i.e., estimating a time series’ future probability distribution given its past, is a key enabler for optimizing business processes. In retail businesses, for example, probabilistic demand forecasts are crucial for having the right inventory available at the right time and in the right place. This paper proposes DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an autoregressive recurrent neural network model on a large number of related time series. We demonstrate how the application of deep learning techniques to forecasting can overcome many of the challenges that are faced by widely-used classical approaches to the problem. By means of extensive empirical evaluations on several real-world forecasting datasets, we show that our methodology produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.}
}

@article{Wang2024IsME,
  title={Is Mamba Effective for Time Series Forecasting?},
  author={Zihan Wang and Fanheng Kong and Shi Feng and Ming Wang and Han Zhao and Daling Wang and Yifei Zhang},
  journal={ArXiv},
  year={2024},
  volume={abs/2403.11144},
  url={https://api.semanticscholar.org/CorpusID:268513477}
}

@article{Zhou.informer.2021,
title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
volume={35},
url={https://ojs.aaai.org/index.php/AAAI/article/view/17325},
DOI={10.1609/aaai.v35i12.17325},
number={12}, journal={Proceedings of the AAAI Conference on Artificial Intelligence},
author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
year={2021},
month={May},
pages={11106-11115}
}

@misc{ahamed2024tscmambamambameetsmultiview,
      title={TSCMamba: Mamba Meets Multi-View Learning for Time Series Classification}, 
      author={Md Atik Ahamed and Qiang Cheng},
      year={2024},
      eprint={2406.04419},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04419}, 
}

@misc{ansari2024chronoslearninglanguagetime,
      title={Chronos: Learning the Language of Time Series}, 
      author={Abdul Fatir Ansari and Lorenzo Stella and Caner Turkmen and Xiyuan Zhang and Pedro Mercado and Huibin Shen and Oleksandr Shchur and Syama Sundar Rangapuram and Sebastian Pineda Arango and Shubham Kapoor and Jasper Zschiegner and Danielle C. Maddix and Hao Wang and Michael W. Mahoney and Kari Torkkola and Andrew Gordon Wilson and Michael Bohlke-Schneider and Yuyang Wang},
      year={2024},
      eprint={2403.07815},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.07815}, 
}

@article{bai2018empirical,
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  eprint = {1803.01271},
  journal = {CoRR},
  keywords = {SequenceModelling TCN TemporalConvolutionalNeuralNetwork},
  title = {An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling.},
  url = {http://arxiv.org/abs/1803.01271},
  volume = {abs/1803.01271},
  year = 2018
}

@book{box.TSA_forecasting.1970,
    author = {Box, George E. P. and Jenkins, Gwilym M.},
    title = {Time Series Analysis: Forecasting and Control},
    publisher = {Holden-Day},
    year = {1970}
}

@article{challu.NHITS.2023,
author={Challu, Cristian and Olivares, Kin G. and Oreshkin, Boris N. and Garza Ramirez, Federico and Mergenthaler Canseco, Max and Dubrawski, Artur},
title={NHITS: Neural Hierarchical Interpolation for Time Series Forecasting},
journal={Proceedings of the AAAI Conference on Artificial Intelligence},
volume={37},
url={https://ojs.aaai.org/index.php/AAAI/article/view/25854},
DOI={10.1609/aaai.v37i6.25854},
number={6},
year={2023},
month={Jun.},
pages={6989-6997}
}

@article{chen2023tsmixer,
title={{TSM}ixer: An All-{MLP} Architecture for Time Series Forecast-ing},
author={Si-An Chen and Chun-Liang Li and Sercan O Arik and Nathanael Christian Yoder and Tomas Pfister},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=wbpxTuXgm0},
note={}
}

@article{das2023longterm,
title={Long-term Forecasting with Ti{DE}: Time-series Dense Encoder},
author={Abhimanyu Das and Weihao Kong and Andrew Leach and Shaan K Mathur and Rajat Sen and Rose Yu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=pCbC3aQB5W},
note={}
}

@misc{eisenach2022mqtransformermultihorizonforecastscontext,
      title={MQTransformer: Multi-Horizon Forecasts with Context Dependent and Feedback-Aware Attention}, 
      author={Carson Eisenach and Yagna Patel and Dhruv Madeka},
      year={2022},
      eprint={2009.14799},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2009.14799}, 
}

@inproceedings{franceschi.unsupervised.2019,
 author = {Franceschi, Jean-Yves and Dieuleveut, Aymeric and Jaggi, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Unsupervised Scalable Representation Learning for Multivariate Time Series},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/53c6de78244e9f528eb3e1cda69699bb-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{garza2024timegpt1,
      title={TimeGPT-1}, 
      author={Azul Garza and Cristian Challu and Max Mergenthaler-Canseco},
      year={2024},
      eprint={2310.03589},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03589}, 
}

@misc{gu2024mambalineartimesequencemodeling,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@article{hochreiter.LSTM_TS.1997,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@book{holt1957forecasting,
  title={Forecasting Seasonals and Trends by Exponentially Weighted Moving Averages},
  author={Holt, Charles C. },
  series={O.N.R. research memorandum},
  url={https://books.google.com/books?id=SAFFNwAACAAJ},
  year={1957},
  publisher={Defense Technical Information Center}
}

@book{hyndman2018forecasting,
  title={Forecasting: principles and practice},
  author={Hyndman, R.J. and Athanasopoulos, G.},
  isbn={9780987507112},
  url={https://books.google.com/books?id=_bBhDwAAQBAJ},
  year={2018},
  publisher={OTexts}
}

@inproceedings{li.enhancing_locality.2019,
  author = {Li, Shiyang and Jin, Xiaoyong and Xuan, Yao and Zhou, Xiyou and Chen, Wenhu and Wang, Yu-Xiang and Yan, Xifeng},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché Buc, Florence and Fox, Edward A. and Garnett, Roman},
  keywords = {dblp},
  pages = {5244-5254},
  title = {Enhancing the Locality and Breaking the Memory Bottleneck of Transformer on Time Series Forecasting.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf},
  year = 2019
}

@misc{li2023revisitinglongtermtimeseries,
      title={Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping}, 
      author={Zhe Li and Shiyi Qi and Yiduo Li and Zenglin Xu},
      year={2023},
      eprint={2305.10721},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.10721}, 
}

@inproceedings{liu.nonstationary.2022,
 author = {Liu, Yong and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {9881--9893},
 publisher = {Curran Associates, Inc.},
 title = {Non-stationary Transformers: Exploring the Stationarity in Time Series Forecasting},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/4054556fcaa934b0bf76da52cf4f92cb-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{liu2022pyraformer,
title={Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting},
author={Shizhan Liu and Hang Yu and Cong Liao and Jianguo Li and Weiyao Lin and Alex X. Liu and Schahram Dustdar},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=0EXmFzUn5I}
}

@inproceedings{liu2022scinet,
title={{SCIN}et: Time Series Modeling and Forecasting with Sample Convolution and Interaction},
author={Minhao Liu and Ailing Zeng and Muxi Chen and Zhijian Xu and Qiuxia Lai and Lingna Ma and Qiang Xu},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=AyajSjTAzmg}
}

@misc{liu2024autotimesautoregressivetimeseries,
      title={AutoTimes: Autoregressive Time Series Forecasters via Large Language Models}, 
      author={Yong Liu and Guo Qin and Xiangdong Huang and Jianmin Wang and Mingsheng Long},
      year={2024},
      eprint={2402.02370},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.02370}, 
}

@inproceedings{liu2024itransformer,
title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=JePfAI8fah}
}

@inproceedings{nie.patchtst.2023a,
title={A Time Series is Worth 64 Words:  Long-term Forecasting with Transformers},
author={Yuqi Nie and Nam H Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Jbdc0vTOcol}
}

@inproceedings{oreshkin2020N-BEATS,
title={N-BEATS: Neural basis expansion analysis for interpretable time series forecasting},
author={Boris N. Oreshkin and Dmitri Carpov and Nicolas Chapados and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1ecqn4YwB}
}

@misc{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
}

@inproceedings{shen.timeseries.2020,
 author = {Shen, Lifeng and Li, Zhuocong and Kwok, James},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {13016--13026},
 publisher = {Curran Associates, Inc.},
 title = {Timeseries Anomaly Detection using Temporal Hierarchical One-Class Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/97e401a02082021fd24957f852e0e475-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{talukder2024totem,
      title={TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis}, 
      author={Sabera Talukder and Yisong Yue and Georgia Gkioxari},
      year={2024},
      eprint={2402.16412},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{taylor.forecasting.2018,
author = {Sean J. Taylor and Benjamin Letham},
title = {Forecasting at Scale},
journal = {The American Statistician},
volume = {72},
number = {1},
pages = {37--45},
year = {2018},
publisher = {ASA Website},
doi = {10.1080/00031305.2017.1380080},
URL = {https://doi.org/10.1080/00031305.2017.1380080},
eprint = {https://doi.org/10.1080/00031305.2017.1380080}
}

@inproceedings{vandenoord.vqvae.2017,
 author = {van den Oord, Aaron and Vinyals, Oriol and kavukcuoglu, koray},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Discrete Representation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{winters.TS_forecasting.1960,
 ISSN = {00251909, 15265501},
 URL = {http://www.jstor.org/stable/2627346},
 abstract = {The growing use of computers for mechanized inventory control and production planning has brought with it the need for explicit forecasts of sales and usage for individual products and materials. These forecasts must be made on a routine basis for thousands of products, so that they must be made quickly, and, both in terms of computing time and information storage, cheaply; they should be responsive to changing conditions. The paper presents a method of forecasting sales which has these desirable characteristics, and which in terms of ability to forecast compares favorably with other, more traditional methods. Several models of the exponential forecasting system are presented, along with several examples of application.},
 author = {Peter R. Winters},
 journal = {Management Science},
 number = {3},
 pages = {324--342},
 publisher = {INFORMS},
 title = {Forecasting Sales by Exponentially Weighted Moving Averages},
 urldate = {2024-10-07},
 volume = {6},
 year = {1960}
}

@misc{woo2022etsformerexponentialsmoothingtransformers,
      title={ETSformer: Exponential Smoothing Transformers for Time-series Forecasting}, 
      author={Gerald Woo and Chenghao Liu and Doyen Sahoo and Akshat Kumar and Steven Hoi},
      year={2022},
      eprint={2202.01381},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.01381}, 
}

@InProceedings{wu.flowformer.2022,
  title = 	 {Flowformer: Linearizing Transformers with Conservation Flows},
  author =       {Wu, Haixu and Wu, Jialong and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {24226--24242},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/wu22m/wu22m.pdf},
  url = 	 {https://proceedings.mlr.press/v162/wu22m.html},
  abstract = 	 {Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.}
}

@inproceedings{wu2021autoformer,
title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},
author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=J4gRj6d5Qm}
}

@inproceedings{wu2023timesnet,
title={TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis},
author={Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=ju_Uqw384Oq}
}

@inproceedings{xu2022anomaly,
title={Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy},
author={Jiehui Xu and Haixu Wu and Jianmin Wang and Mingsheng Long},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=LzQQ89U1qm_}
}

@misc{zhang2022morefastmultivariatetime,
      title={Less Is More: Fast Multivariate Time Series Forecasting with Light Sampling-oriented MLP Structures}, 
      author={Tianping Zhang and Yizhuo Zhang and Wei Cao and Jiang Bian and Xiaohan Yi and Shun Zheng and Jian Li},
      year={2022},
      eprint={2207.01186},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2207.01186}, 
}

@inproceedings{zhang2023effectively,
title={Effectively Modeling Time Series with Simple Discrete State Spaces},
author={Michael Zhang and Khaled Kamal Saab and Michael Poli and Tri Dao and Karan Goel and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=2EpjkjzdCAa}
}

@InProceedings{zhou.fedformer.2022,
  title = 	 {{FED}former: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting},
  author =       {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27268--27286},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zhou22g/zhou22g.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zhou22g.html},
  abstract = 	 {Long-term time series forecasting is challenging since prediction accuracy tends to decrease dramatically with the increasing horizon. Although Transformer-based methods have significantly improved state-of-the-art results for long-term forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in a well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, Fedformer can reduce prediction error by 14.8% and 22.6% for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.}
}

@inproceedings{zhou2023OFA,
title={One Fits All: Power General Time Series Analysis by Pretrained {LM}},
author={Tian Zhou and Peisong Niu and Xue Wang and Liang Sun and Rong Jin},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=gMS6FVZvmF}
}

