\section{Experiments}

\subsection{Experiment Details}
\label{sm:exp_details}

All experiments are evaluated 3 times with different initialization (\ref{sm:hyperparams}).
For each dataset, we select a single input sequence based on the best-performing input sequence length across all prediction lengths.
We choose a single input sequence length for each dataset because the strength of the correlations between time steps is independent of the prediction length, but the required input sequence length to capture the information necessary to forecast is unique for each dataset.
For each prediction length, we treat the attention mask as a hyperparameter and select the best mask and time constant pair.
We select attention masks for each prediction length because the forecasting timescale will likely emphasize short or long-term correlations for shorter or longer forecasting windows.
We make our selections based on the MSE error.

\subsection{Powerformer}
\label{sm:experiments_powerformer}
We evaluate the weight power-law (\fpl) and similarity power-law (\fspl) filters on all 7 datasets.
For \fpl{} we evaluate $\alpha \in \{ 0.1, 0.25, 0.5, 0.75, 1.0 \}$ and for \fspl{} we evaluate $\alpha \in \{ 0.1, 0.5, 1.0, 2.0 \}$.
Similar to PatchTST~\cite{nie.patchtst.2023a} we evaluate the power-law filters with two input lengths: 336 and 512.

We evaluate both Butterworth filters $\left( \fbwomm \right.$ and $\left. \fbwtmm \right)$ on all the ETT datasets and the Weather datasets.
For each Butterworth filter, we evaluated $\alpha \in \{ 2, 5, 10, 15, 20 \}$ with an input length of 336.
Due to poor performance, we did not evaluate the Butterworth filters on the larger Electricity and Traffic datasets, nor with the longer 512 input length.


\subsection{Transformer}
\label{sm:experiments_transformer}
We train a standard encoder-decoder Transformer architecture both with and without WCMHA.
Generic Transformer models often incorporate causality through masking.
To account for this, our Transformer benchmark uses masking for the decoder MHA but not for the encoder MHA nor the decoder cross-attention.
We investigate WCMHA's effects by replacing the encoder and decoder self-attention, but not the decoder cross-attention as it is already causal.
Section~\ref{sm:transformer_architecture} provides more architectural details.

We train Transformer models, with and without WCMHA, using the weight power-law (\fpl) filter on all 7 datasets.
We explore the following decay constants: $\alpha \in [0.1, 0.25, 0.5, 0.75, 1.0]$ and provide further experimental detail in sections~\ref{sec:experiments} and \ref{sm:exp_details}.

\subsection{Baseline Selection}
We compare \emph{Powerformer} against multiple Transformer-based state-of-the-art methods.
We populate Table~\ref{tab:performance} with values taken from the models' respective papers.
All models except TOTEM~\cite{talukder2024totem} and iTransformer~\cite{liu2024itransformer} treat the input length as a hyperparameter and report the best results.
PatchTST~\cite{nie.patchtst.2023a} provides results for two input sequence lengths (336 and 512).
In Table~\ref{tab:performance} we report the best result between these input sequence lengths for each dataset based on the MSE.
We employ the same input sequence length selection for \emph{Powerformer}.
