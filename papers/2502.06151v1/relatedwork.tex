\section{Related Work}
The importance of time-series data has led to decades of work, resulting in a myriad of methods: early statistical methods \cite{holt1957forecasting,hyndman2018forecasting,taylor.forecasting.2018,winters.TS_forecasting.1960} (such as ARIMA \cite{box.TSA_forecasting.1970}); multilayer perceptions (MLP) \cite{Chen.mlp_vs_transformer.2023, li2023revisitinglongtermtimeseries,das2023longterm, zhang2022morefastmultivariatetime} (such as NHITS \cite{challu.NHITS.2023}, TSMixer \cite{chen2023tsmixer}, and N-BEATS \cite{oreshkin2020N-BEATS}); convolutional neural networks (CNNs) \cite{He_2019, franceschi.unsupervised.2019, bai2018empirical} (such as TimesNet \cite{wu2023timesnet} and SCINet \cite{liu2022scinet}); recurrent neural networks (RNNs) \cite{shen.timeseries.2020, hochreiter.LSTM_TS.1997} (such as DeepAR \cite{SALINAS20201181}); Transformers \cite{zhou2023OFA, garza2024timegpt1, woo2022etsformerexponentialsmoothingtransformers, zhou.fedformer.2022, liu.nonstationary.2022, wu.flowformer.2022, xu2022anomaly, liu2022pyraformer, Zhou.informer.2021, Kitaev2020Reformer, li.enhancing_locality.2019, talukder2024totem, eisenach2022mqtransformermultihorizonforecastscontext} (such as PatchTST \cite{nie.patchtst.2023a}, Autoformer \cite{wu2021autoformer}, FEDformer \cite{zhou.fedformer.2022}, and iTransformer \cite{liu2024itransformer}); and state space models (SSMs) \cite{ahamed2024tscmambamambameetsmultiview, Wang2024IsME, zhang2023effectively} (such as MAMBA \cite{gu2024mambalineartimesequencemodeling}).
Of these methods, we focus on Transformers.

The success of LLMs and the introduction of powerful foundation models, like GPT-2 \cite{radford2019language}, inspired models built atop them and others looking to improve upon the Transformer architecture.
For example, Time-GPT \cite{garza2024timegpt1} uses the Transformer encoder-decoder structure, and One-Fits-All \cite{zhou2023OFA} is built atop a pretrained GPT-2 and fine-tunes the attention layer normalization and temporal embedding.
Some models like AutoTimes \cite{liu2024autotimesautoregressivetimeseries} leverage LLM's in-context learning capabilities and employ textual prompt engineering to feed into LLMs.
Other models have adapted the attention mechanism to time-series data: some models strongly rely on, or partially include, Fourier representations when calculating attention weights \cite{zhou.fedformer.2022, woo2022etsformerexponentialsmoothingtransformers}; AutoFormer \cite{wu2021autoformer} leverages temporal autocorrelations as a similarity metric; Reformer \cite{Kitaev2020Reformer} uses a locality-sensitive hashing mechanism to reduce the attention's complexity; Informer \cite{Zhou.informer.2021} selects only high attention weight values, increasing prediction capacity; FlowFormer \cite{wu.flowformer.2022} uses flow conservation to achieve linear complexity attention without imposing locality implicit biases; and iTransformer \cite{liu2024itransformer} inverts the temporal and embedding dimensions, applying attention along the embedding.  

Transformers were developed for discrete inputs, unlike the continuous valued time-series inputs.
Patching along the time domain (PatchTST) shortens the context window which allows for longer lookback windows and significantly improves performance over more complicated Transformer methods \cite{nie.patchtst.2023a}.
This simple yet powerful improvement has made patching ubiquitous.
Other works look to convert time-series into discretized tokens similar to word vectors.
Chronos \cite{ansari2024chronoslearninglanguagetime} normalizes the input sequences and matches these continuous values to tokens made from discretizing a sufficiently large domain.
TOTEM \cite{talukder2024totem} similarly discretizes the input time series, but instead VQVAE \cite{vandenoord.vqvae.2017} encodes the input and matches it with the most similar token.