\documentclass[a4paper,11pt,twocolumn]{article}

\usepackage[margin=20mm]{geometry}


\usepackage{microtype}
\usepackage[pdftex]{graphicx}
%\usepackage{placeins}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}

\usepackage{xspace}


\usepackage{microtype}      % microtypography


\usepackage{parskip}

%\usepackage{natbib}

\usepackage{hyperref}
%\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={magenta},
	urlcolor={blue!80!black}
}


\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}


\input{commands}


\title{\bf Powerformer: A Transformer with Weighted Causal Attention for Time-series Forecasting\vspace{+0.5cm}}


\author{Kareem Hegazy$^{1,2 *}$ \and  Michael W. Mahoney$^{1,2,3}$  \and N. Benjamin Erichson$^{2,3}$\thanks{Corresponding authors: Kareem Hegazy (\url{khegazy@berkeley.edu}), and N. Benjamin Erichson (\url{erichson@icsi.berkeley.edu}).%, rnakata@eri.u-tokyo.ac.jp})
}}


\date{ \vspace{+0.1cm} 
$^1$Department of Statistics, University of California Berkeley \\%
$^2$International Computer Science Institute, Berkeley \\% 
$^3$Lawrence Berkeley National Laboratory \\%
}


\newenvironment{acknowledgements}{%
	\renewcommand{\abstractname}{Acknowledgements}% Rename Abstract to Acknowledgements
	\begin{abstract}
	}{%
	\end{abstract}
}


\sloppy


\begin{document}


\maketitle





\begin{abstract}
Transformers have recently shown strong performance in time-series forecasting, but their all-to-all attention mechanism overlooks the (temporal) causal and often (temporally) local nature of data. We introduce Powerformer, a novel Transformer variant that replaces noncausal attention weights with causal weights that are reweighted according to a smooth heavy-tailed decay. This simple yet effective modification endows the model with an inductive bias favoring temporally local dependencies, while still allowing sufficient flexibility to learn the unique correlation structure of each dataset. Our empirical results demonstrate that Powerformer not only achieves state-of-the-art accuracy on public time-series benchmarks, but also that it offers improved interpretability of attention patterns. Our analyses show that the modelâ€™s locality bias is amplified during training, demonstrating an interplay between time-series data and power-law-based attention. These findings highlight the importance of domain-specific modifications to the Transformer architecture for time-series forecasting, and they establish Powerformer as a strong, efficient, and principled baseline for future research and real-world applications.
\end{abstract}


\begin{figure*}[!t]
	\centering
	\includegraphics[width=0.99\textwidth]{figures/arch_fig.pdf}
	\vspace{0.1cm}
	\caption{Illustration of \emph{Powerformer} and the Weighted Causal Multihead Attention (WCMHA) architecture, as well as their effects on attention weights. Panel (a) shows the \emph{Powerformer} architecture (left) and the WCMHA (right). Panels (b) and (c) show the attention weights without and with our local-causal mask, respectively. Here, $\Sigma$ corresponds to the softmax function.}
	\label{fig:architecture}
\end{figure*}

\section{Introduction}

With the recent popularity of Transformer models \cite{vaswani.transformer.2017}, large language models (LLMs), and foundation models~\cite{Bommasani2021FoundationModels}, the Transformer architecture gained popularity in time-series forecasting, leading to a number of Transformer based state-of-the-art models: Autoformer~\cite{wu2021autoformer}, FEDformer~\cite{zhou.fedformer.2022}, and PatchTST~\cite{nie.patchtst.2023a}.

The multihead attention (MHA) mechanism is at the core of these models, which is an all-to-all process that enriches each embedding via a weighted combination of other embeddings based on a similarity calculation.
For natural language processing (NLP) tasks, each embedding represents a word.
Here, an all-to-all embedding mechanism is natural since correlations between words can be only weakly dependent on their ordering and the number of words between them.
However, for time-series data, where each embedding is a temporal measurement, the relation between embeddings is both causal and dependent on the time delay.
This disparity between the attention mechanism's all-to-all structure motivates the open question: how well-suited is the Transformer's attention mechanism for time-series data?
For example, \cite{Chen.mlp_vs_transformer.2023} shows that linear models outperform Transformers on common time-series benchmarks.
Still, Transformers have seen success in time-series forecasting.

Previous methods sought to impose causal and local implicit biases on Transformer. Before Transformer's widespread adoption, methods such as WaveNet~\cite{vandenoord.wavenet.2016} and \cite{franceschi.unsupervised.2019} used causal convolutions.
Imparting local and causal implicit biases to MHA requires altering its architecture.
Reformer~\cite{Kitaev2020Reformer} calculates the attention weights via a causal and locality-aware hash map.
LogSparse self-attention~\cite{li.enhancing_locality.2019} uses convolutions to encode local variations and feeds them into a causally sparse self-attention mechanism.
The work most similar to ours is ETSformer~\cite{woo2022etsformerexponentialsmoothingtransformers}, which develops exponential smoothing attention that replaces the similarity-based attention weights with exponential decaying temporal weights.
These temporal weights are no longer dependent on a similarity metric like the key-query projection.
ETSformer also employs a frequency attention scheme alongside their exponentially weighted attention.

When enforcing a locality bias, previous methods imposed temporal length scales on the dynamics and/or the functional form of the temporal correlations.
However, the temporal correlations between data points vary in both functional form and time scale between datasets.
Temporal convolutions, and their dilations, are examples of imposing such a time scale.
Exponentially weighted attention imposes exponentially decaying temporal correlations but lacks the similarity-weighted MHA that captures varying temporal dependencies. 
Previous successes from adding local and causal biases~\cite{woo2022etsformerexponentialsmoothingtransformers, Kitaev2020Reformer, li.enhancing_locality.2019, vandenoord.wavenet.2016, franceschi.unsupervised.2019}
show their importance, but the application method is crucial.



In this work, we develop \emph{Powerformer} (illustrated in Fig.~\ref{fig:architecture}), a Transformer-based model that uses Weighted Causal Multihead Attention (WCMHA) to learn temporal dependencies unique to each dataset.
%
Our model is motivated by the observation that many physical systems exhibit heavy-tailed autocorrelations, e.g., the pairwise correlation strength may decay as a power law distribution, as the time delay grows \cite{clauset.powerlaw.2009}.
%
Our goal is to impose this power-law temporal dependence as an inductive bias, while allowing \emph{Powerformer} the flexibility to perturb it sufficiently to capture the temporal pairwise correlations unique to each dataset.
To do so, we add a temporally decaying mask to the attention mechanism, specifically to the key-query overlap (referred to as the similarity score).
The mask decays attention weights and pairwise dependencies to resemble a power law, but its additive nature maintains the key and query similarity score.
This maintained similarity score allows the model to learn data-dependent perturbations to the power-law pairwise dependency.
The mask acts as a regularizer that guides the attention weight temporal dependence towards a power law, but still maintains sufficient expressivity for MHA to learn pairwise dependencies unique to each dataset.
Moreover, \emph{Powerformer's} simplicity promotes interpretability studies that provide evidence of learned heavy-tailed dependencies in the attention weights.

We build \emph{Powerformer} by combining WCMHA, the commonly used encoder-only Transformer architecture, and patching~\cite{nie.patchtst.2023a}.
\emph{Powerformer} outperforms previous state-of-the-art models on typical tasks despite its simplicity compared to other attention mechanisms.
This suggests that these biases and our implementation better align Transformer with natural time-series structures.

Our main contributions are the following.

\begin{itemize}[leftmargin=*] %noitemsep,nolistsep
	\item  We develop \emph{Powerformer}, a Transformer-based model for time-series, with WCMHA to impose causal and local implicit biases that shape attention's pairwise correlation to better respect the data's unique temporal structure.
	\item \emph{Powerformer} is powered by various weighting schemes: power-law decays and Butterworth filters. The former resembles naturally occurring time-series, while the latter acts analogously to a step function. 
	\item  Our empirical results demonstrate that \emph{Powerformer} obtains state-of-the-art performance when compared to similar Transformer models on a range of public times-series benchmarks. Moreover, we provide interpretability studies that show how our implicit biases shape \emph{Powerformer's} learned attention representations.
	\item \emph{Powerformer} can reduce the attention complexity from quadratic to linear by defining a cutoff time after which the attention weighting removes their contribution.
\end{itemize}


\section{Related Work}

The importance of time-series data has led to decades of work, resulting in a myriad of methods: early statistical methods \cite{holt1957forecasting,hyndman2018forecasting,taylor.forecasting.2018,winters.TS_forecasting.1960} (such as ARIMA \cite{box.TSA_forecasting.1970}); multilayer perceptions (MLP) \cite{Chen.mlp_vs_transformer.2023, li2023revisitinglongtermtimeseries,das2023longterm, zhang2022morefastmultivariatetime} (such as NHITS \cite{challu.NHITS.2023}, TSMixer \cite{chen2023tsmixer}, and N-BEATS \cite{oreshkin2020N-BEATS}); convolutional neural networks (CNNs) \cite{He_2019, franceschi.unsupervised.2019, bai2018empirical} (such as TimesNet \cite{wu2023timesnet} and SCINet \cite{liu2022scinet}); recurrent neural networks (RNNs) \cite{shen.timeseries.2020, hochreiter.LSTM_TS.1997} (such as DeepAR \cite{SALINAS20201181}); Transformers \cite{zhou2023OFA, garza2024timegpt1, woo2022etsformerexponentialsmoothingtransformers, zhou.fedformer.2022, liu.nonstationary.2022, wu.flowformer.2022, xu2022anomaly, liu2022pyraformer, Zhou.informer.2021, Kitaev2020Reformer, li.enhancing_locality.2019, talukder2024totem, eisenach2022mqtransformermultihorizonforecastscontext} (such as PatchTST \cite{nie.patchtst.2023a}, Autoformer \cite{wu2021autoformer}, FEDformer \cite{zhou.fedformer.2022}, and iTransformer \cite{liu2024itransformer}); and state space models (SSMs) \cite{ahamed2024tscmambamambameetsmultiview, Wang2024IsME, zhang2023effectively} (such as MAMBA \cite{gu2024mambalineartimesequencemodeling}).
Of these methods, we focus on Transformers.

The success of LLMs and the introduction of powerful foundation models, like GPT-2 \cite{radford2019language}, inspired models built atop them and others looking to improve upon the Transformer architecture.
For example, Time-GPT \cite{garza2024timegpt1} uses the Transformer encoder-decoder structure, and One-Fits-All \cite{zhou2023OFA} is built atop a pretrained GPT-2 and fine-tunes the attention layer normalization and temporal embedding.
Some models like AutoTimes \cite{liu2024autotimesautoregressivetimeseries} leverage LLM's in-context learning capabilities and employ textual prompt engineering to feed into LLMs.
Other models have adapted the attention mechanism to time-series data: some models strongly rely on, or partially include, Fourier representations when calculating attention weights \cite{zhou.fedformer.2022, woo2022etsformerexponentialsmoothingtransformers}; AutoFormer \cite{wu2021autoformer} leverages temporal autocorrelations as a similarity metric; Reformer \cite{Kitaev2020Reformer} uses a locality-sensitive hashing mechanism to reduce the attention's complexity; Informer \cite{Zhou.informer.2021} selects only high attention weight values, increasing prediction capacity; FlowFormer \cite{wu.flowformer.2022} uses flow conservation to achieve linear complexity attention without imposing locality implicit biases; and iTransformer \cite{liu2024itransformer} inverts the temporal and embedding dimensions, applying attention along the embedding.  

Transformers were developed for discrete inputs, unlike the continuous valued time-series inputs.
Patching along the time domain (PatchTST) shortens the context window which allows for longer lookback windows and significantly improves performance over more complicated Transformer methods \cite{nie.patchtst.2023a}.
This simple yet powerful improvement has made patching ubiquitous.
Other works look to convert time-series into discretized tokens similar to word vectors.
Chronos \cite{ansari2024chronoslearninglanguagetime} normalizes the input sequences and matches these continuous values to tokens made from discretizing a sufficiently large domain.
TOTEM \cite{talukder2024totem} similarly discretizes the input time series, but instead VQVAE \cite{vandenoord.vqvae.2017} encodes the input and matches it with the most similar token.


\section{Method}

We propose Weighted Causal Multihead Attention (WCMHA) as a foundation for \emph{Powerformer}, a Transformer-based architecture designed for time-series forecasting.
In this section, we review standard Multihead Attention (MHA) and demonstrate how we introduce locality and causality into it.
We then present various weighting functional forms and insights on how WCMHA modifies attention distributions.
We then introduce the \emph{Powerformer} architecture that builds upon these principles. 
Figure~\ref{fig:architecture} provides a high-level illustration of WCMHA and its effect on attention weights.

\subsection{Weighted Causal Multihead Attention}
\label{sec:wcma}

Transformers typically leverage MHA to compute new embeddings from weighted sums of input embeddings. Let $\mathbf{X} \in \mathbb{R}^{T \times d}$ be a sequence of $T$ input vectors, each of dimension $d$. For each attention head $h$, MHA first projects $\mathbf{X}$ into \emph{query}, \emph{key}, and \emph{value} matrices, respectively denoted by $\mathbf{Q}_h$, $\mathbf{K}_h$, and $\mathbf{V}_h$. Concretely,
%
\begin{equation}
	\label{eq:KQV}
	\mathbf{Q}_h = \mathbf{X}\,\mathbf{W}_h^{(\text{Q})}, 
	\quad
	\mathbf{K}_h = \mathbf{X}\,\mathbf{W}_h^{(\text{K})}, 
	\quad
	\mathbf{V}_h = \mathbf{X}\,\mathbf{W}_h^{(\text{V})},
\end{equation}
%
where each projection matrix $\mathbf{W}_h^{(\cdot)} \in \mathbb{R}^{d \times d_k}$ is learnable and $d_k$ is the dimensionality of each head. The parameters $\mathbf{Q}_h$, $\mathbf{K}_h$, and $\mathbf{V}_h$ thus each have shape $T \times d_k$.

The \emph{unnormalized attention similarity scores} $\mathbf{S}_h$ measure how each query interacts with all keys:
%
\begin{equation}
	\label{eq:Sh}
	\mathbf{S}_h = \frac{\mathbf{K}_h \,\mathbf{Q}_h^\top}{\sqrt{d_k}}.
\end{equation}
%
The factor of $1/\sqrt{d_k}$ helps stabilize gradients by normalizing the dot products according to the head dimension. We then apply a softmax function to obtain the \emph{normalized attention weights}:
%
\begin{equation}
	\label{eq:Ch}
	\mathbf{C}_h = \text{Softmax}\bigl(\mathbf{S}_h\bigr),
\end{equation}
%
where each row of $\mathbf{C}_h$ sums to 1. Finally, each head's output $\tilde{\mathbf{X}}_h$ is computed as a weighted sum of the values:
%
\begin{equation}
	\label{eq:Xhtilde}
	\tilde{\mathbf{X}}_h = \mathbf{C}_h \,\mathbf{V}_h.
\end{equation}
%
To form the final MHA output, we concatenate the outputs from all $H$ heads and apply a linear projection:
%
\begin{equation}
	\label{eq:MHAOutput}
	\tilde{\mathbf{X}} 
	\;=\;
	[\tilde{\mathbf{X}}_1,\dots,\tilde{\mathbf{X}}_H]
	\,\mathbf{W}^{(\text{A})},
\end{equation}
%
where $\mathbf{W}^{(\text{A})} \in \mathbb{R}^{(H\,d_k)\times d}$ is another learnable matrix. The result $\tilde{\mathbf{X}}$ has the same dimensionality as the input $\mathbf{X}$ (i.e., $T \times d$), allowing it to be passed into subsequent Transformer layers or decoded for downstream tasks.


\subsubsection{Enforcing Causality}
\label{sec:causal_mask}

To ensure that information at time $t$ does not leak from future time steps $t' > t$, we apply a causal mask $\mathbf{M}^{(\text{C})}$:
%
\begin{equation}
	\mathbf{M}^{(\text{C})}_{i,j} =
	\begin{cases}
		-\infty & j > i,\\
		0       & j \leq i.
	\end{cases}
	\label{eq:mask_causal}
\end{equation}
%
This mask is added to the attention score $\mathbf{S}_h$, forcing any attention weight for $j>i$ to be zero after the softmax:
%
\[
\mathbf{S}_h^{(\text{C})}
= \mathbf{S}_h + \mathbf{M}^{(\text{C})},
\quad
\mathbf{C}_h^{(\text{C})}
= \text{Softmax}\bigl(\mathbf{S}_h^{(\text{C})}\bigr).
\]
%
Hence, $\mathbf{C}_{t,t'}=0$ for all $t'>t$, ensuring causal structure for time-series data.

\subsubsection{Incorporating Locality via Decaying Masks}
\label{sec:local_mask}

Time-series often exhibit stronger correlations between nearby time steps compared to distant ones \cite{clauset.powerlaw.2009}. To bias the model toward local dependencies, we introduce a second mask $\mathbf{M}^{(\text{D})}$ that decays attention weights as the time gap $|\Delta t|$ increases:
%
\begin{equation}
	\mathbf{M}^{(\text{D})}_{i,j} 
	= \begin{cases}
		0 & j > i, \\
		f(t_i - t_j) & j \leq i,
	\end{cases}
	\label{eq:mask_local}
\end{equation}
%
where $f(\Delta t)\leq0$ is a non-positive function of the time difference. Adding $\mathbf{M}^{(\text{D})}$ to $\mathbf{S}_h^{(\text{C})}$ yields
%
\begin{equation}
	\mathbf{S}_h^{(\text{C,D})}
	= \mathbf{S}_h + \mathbf{M}^{(\text{C})} + \mathbf{M}^{(\text{D})}.
	\label{eq:attnSCD}
\end{equation}
%
We thus obtain the \emph{Weighted Causal Attention Weights}
%
\begin{align}
	\mathbf{C}_h^{(\text{C,D})}
	= \text{Softmax}\bigl(\mathbf{S}_h^{(\text{C,D})}\bigr),
\end{align}
%
where, 
\begin{align}
	C_{i,j}^{(\text{C,D})} \propto 
	\begin{cases}
		0 & j > i, \\
		\exp\bigl[f(t_i - t_j)\bigr] & j \le i.
	\end{cases}
	\label{eq:weight_ME}
\end{align}
We refer to this mechanism as WCMHA. By combining causal masking and smoothly decaying weights, WCMHA captures both the temporal ordering and localized dependencies inherent to many real-world processes.

\subsection{Reweighting Functions for Locality}
\label{sec:decay_funcs}

We explore multiple choices for the reweighting function~$f(\Delta t)$:

\begin{itemize}[leftmargin=*]
	\item \textbf{Similarity Power Law:} 
	\(
	\fsplmm(\Delta t) = -(\Delta t)^\alpha.
	\)
	
	\item \textbf{Weight Power Law:}
	\(
	\fplmm(\Delta t) = -\alpha\,\log(\Delta t).
	\)
	
	\item \textbf{Butterworth Filter:} 
	A frequency-based filter with a smooth transition resembling a step function, parameterized by its cutoff time $t_c$ and order $n$.
\end{itemize}
\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/powerlaw_filters.pdf}
	%\vspace{-0.5cm}
	\caption{We show the weight power-law (solid line) and similarity power-law (dashed line) masks for varying $\alpha$. 
		Panel (a) shows the contribution added to the attention scores and Panel (b) shows the subsequent effects on the attention weights after applying Softmax.}
	\label{fig:maskPL}
\end{figure}


Each function has a tunable hyperparameter (e.g., $\alpha$ or $t_c$) that governs the decay rate of attention as $|\Delta t|$ grows. Figure~\ref{fig:architecture}c illustrates the conceptual shift in attention scores when applying these decays.
Figures~\ref{fig:maskPL} and \ref{fig:maskBF} illustrate the masks' contributions to the attention score and weights for \fpl{} and \fspl, and \fbwn, respectively.
The \fpl{} mask imparts a power-law envelope over the attention weight distribution, whereas the \fspl{} envelope is the exponential of a power-law.
Therefore, \fspl, which applies the power-law in the similarity score space, decays faster than \fpl{} in weight space (as seen in Fig.~\ref{fig:maskPL}).


\subsection{Insights into WCMHA}
\label{sec:empirical_wcmha}

Before integrating WCMHA into a state-of-the-art Transformer-based model, we investigate how causal and local masks reshape attention distributions in a vanilla Transformer.
See Appendix~\ref{sm:experiments_transformer} for details. 
Figure~\ref{fig:transformer_attn_general} compares attention score and weight distributions (encoder self-attention, decoder self-attention, and decoder cross-attention) on the Electricity dataset.
We observe:

\begin{itemize}[leftmargin=*]
	\item \textbf{Encoder Self-Attention} (modified by WCMHA) shifts toward higher positive values, emphasizing local contexts.
	\item \textbf{Decoder Cross-Attention} (unchanged) retains a heavy-tailed distribution, indicating some long-range dependencies remain relevant.
	\item \textbf{Decoder Self-Attention} (already causal) exhibits minimal distributional change.
\end{itemize}

These shifts consistently improve forecasting performance on real-world datasets (see Appendix~\ref{sm:experiments_transformer}).
In many cases, WCMHA significantly outperforms standard MHA, especially for datasets with slowly decaying long-range dependencies.
Interestingly, when WCMHA outperforms MHA it does so by much larger margins than when it underperforms.
For example, on the ETTh2 dataset, WCMHA improves MSE and MAE by 16\% and underperforms by 2\%.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/Transformer_Electricity_powerLaw/attn_general_effects_sq256_pl96.pdf}
	\vspace{-0.5cm}
	\caption{We show the attention score and weight distributions for both the benchmark Transformer (dotted black line) with MHA and our modified Transformer with WCMHA and \fpl{} (solid colored lines). Panels (a), (b), and (c) correspond to the last encoder self-attention, decoder self-attention, and decoder cross-attention layers, respectively. The colored lines correspond to different mask decay times $(\alpha)$. These results are from the Electricity dataset with a 96 prediction length and 512 input length.}
	\label{fig:transformer_attn_general}
\end{figure}

\subsection{Powerformer: Locality, Causality, and Patching}
\label{sec:powerformer}

Having shown that WCMHA provides a powerful mechanism for time-series, leveraging its full potential in practical scenarios requires an effective approach to represent temporal data.
While incorporating WCMHA into modern Transformer time-series models, we wish to isolate the effects of WCMHA and its local and causal biases.
To do so requires a simple architecture, and we focus on univariate time-series.
Thus, we introduce \emph{Powerformer} (see Fig.~\ref{fig:architecture}), a model that integrates Transformer, WCMHA, and PatchTST's~\cite{nie.patchtst.2023a} successful patching framework.

Patching is a common tool in modern time-series models, and similar to \maskL{} it acts as a regularizer to remove high-frequency noise components.
We decompose multivariate time-series into univariate channels before applying \emph{convolutional patching}, similar to PatchTST~\cite{nie.patchtst.2023a}:
\[
\mathbf{X}^{(\text{Patched})}_{s,w} 
\;=\;
\mathbf{X} * \mathbf{p}\Bigl.\Bigr|_s,
\]
with patch length $w$ and convolution stride~$s$.
Patching captures local temporal contexts by converting segments of the raw series into compact embeddings for the Transformer. 


Many state-of-the-art models only use the Transformer encoder~\cite{nie.patchtst.2023a, zhou2023OFA, talukder2024totem} with a linear decoder head.
\emph{Powerformer} combines these insights with our previous Transformer experiments by using only the Transformer encoder with WCMHA and a similar linear readout head.
\emph{Powerformer} integrates two modern design choices while injecting causal masks and smoothly decaying attention weights, allowing it to better model nearby past time points.


Our design integrates two complementary strengths:
\begin{itemize}
	\item \textbf{Locality and Causality:} WCMHA introduces both temporal ordering and tailored decay, making it naturally suited for real-world time-series with power-law or exponential correlations.
	\item \textbf{Robust Patch Embeddings:} Patching~\cite{nie.patchtst.2023a} efficiently captures short-term trends and reduces input length, mitigating quadratic complexity issues that can impede Transformers on long time-series.
\end{itemize}

\emph{Powerformer} is expressive, computationally efficient, and sufficiently simple to interpret.
%
Appendix~\ref{sm:powerformer_architecture} provides further details.
Our empirical results confirm that imposing explicit locality and causality boosts forecasting accuracy and the interpretability of learned temporal dependencies. 

\section{Empirical Evaluation}
\label{sec:experiments}

We evaluate \emph{Powerformer}\footnote[1]{https://github.com/khegazy/Powerformer}, investigating the effects of WCMHA on the attention score and weight distributions, using the mean squared error (MSE) and mean absolute error (MAE) to quantify aggregate performance.


\subsection{Datasets}

We evaluate \emph{Powerformer} on 7 popular publicly-available real-world datasets \cite{Zhou.informer.2021, wu2021autoformer}: ETT (4 subsets), Weather, Electricity, and Traffic.
We are particularly interested in the ETT subsets which measure the electricity transformer oil temperature at 2 different time scales (every 15 minutes and hourly) for two regions.
Analyzing the same measurement in different locations and different time scales will illustrate how sensitive WCMHA is to sampling changes and distribution shifts.
Section~\ref{sec:datasets} provides a detailed description of these datasets, available at \cite{wu2021autoformer}.
We note that Weather, Electricity, and Traffic have much larger timescales, spanning from one to multiple years (Table~\ref{tab:data_size}).
These 7 datasets are large enough to capture slowly decaying temporal correlations, shown in Appendix~\ref{sec:datasets}.
Illness \cite{Zhou.informer.2021} is traditionally used as a long-term forecasting benchmark.
We omit this dataset as its temporal measurements (Table~\ref{tab:data_size}) and evaluation look-back window are more than an order of magnitude smaller than the other datasets.
This short look-back window is typically the size of our filters.





\subsection{Evaluation setup}

Following previous works \cite{nie.patchtst.2023a, wu2021autoformer, zhou.fedformer.2022, woo2022etsformerexponentialsmoothingtransformers, liu2024itransformer, talukder2024totem, Kitaev2020Reformer, Zhou.informer.2021}, we train for multiple prediction and input lengths.
The prediction lengths are 96, 192, 336, and 720, while the input sequence lengths are 336 and 512.
Each model is trained three times for each setting.
Further details on our experimental process and hyperparameters are provided in Appendices~\ref{sm:exp_details} and \ref{sm:hyperparams}, respectively.


%\setlength{\tabcolsep}{3.65pt}
\begin{table*}[!t]
	\centering
	\caption{We compare \emph{Powerformer's} performance on standard publicly-available time-series datasets to state-of-the-art Transformer models. The best results are bolded and second best are underlined. \emph{Powerformer} and PatchTST are evaluated with look-back windows of 336 and 512, see Section~\ref{sm:exp_details} for selection criteria. TOTEM \cite{talukder2024totem} and iTransformer \cite{liu2024itransformer} were evaluated with $T_\text{seq}=96$. All other models were evaluated with multiple look-back windows with the best results selected.}
	\label{tab:performance}
	\vskip 0.1in
	\resizebox{0.98\textwidth}{!}{
		\begin{tabular}{c|c|cc|cc|cc|cc|cc|cc|cc}
			\toprule
			\multicolumn{2}{c|}{} & \multicolumn{2}{c|}{Powerformer} & \multicolumn{2}{c|}{PatchTST} & \multicolumn{2}{c|}{One-Fits-All} & \multicolumn{2}{c|}{TOTEM} & \multicolumn{2}{c|}{iTransformer} & \multicolumn{2}{c|}{FEDformer} & \multicolumn{2}{c}{ETSformer} \\
			\midrule
			\multicolumn{2}{c|}{Metric} & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{ETTh1}}}
			& 96 &  \textbf{0.369} & 0.399 &  \underline{0.370} & 0.400 &  0.376 & \underline{0.397} &  0.380 & \textbf{0.394} &  0.386 & 0.405 &  0.376 & 0.415 &  0.494 & 0.479 \\
			& 192 &  \textbf{0.402} & \textbf{0.418} &  \underline{0.413} & 0.429 &  0.416 & \textbf{0.418} &  0.434 & \underline{0.427} &  0.441 & 0.436 &  0.423 & 0.446 &  0.538 & 0.504 \\
			& 336 &  \textbf{0.414} & \textbf{0.428} &  \underline{0.422} & 0.440 &  0.442 & \underline{0.433} &  0.490 & 0.459 &  0.487 & 0.458 &  0.444 & 0.462 &  0.574 & 0.521 \\
			& 720 &  \textbf{0.439} & \underline{0.459} &  \underline{0.447} & 0.468 &  0.477 & \textbf{0.456} &  0.539 & 0.513 &  0.503 & 0.491 &  0.469 & 0.492 &  0.562 & 0.535 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{ETTh2}}}
			& 96 &  \textbf{0.274} & \textbf{0.335} &  \textbf{0.274} & \underline{0.336} &  \underline{0.285} & 0.342 &  0.293 & 0.338 &  0.297 & 0.349 &  0.332 & 0.374 &  0.340 & 0.391 \\
			& 192 &  \textbf{0.338} & \textbf{0.379} &  \underline{0.339} & \textbf{0.379} &  0.354 & \underline{0.389} &  0.375 & 0.390 &  0.380 & 0.400 &  0.407 & 0.446 &  0.430 & 0.439 \\
			& 336 &  \textbf{0.325} & \textbf{0.379} &  \underline{0.331} & \underline{0.380} &  0.373 & 0.407 &  0.422 & 0.431 &  0.428 & 0.432 &  0.400 & 0.447 &  0.485 & 0.479 \\
			& 720 &  \textbf{0.376} & \textbf{0.419} &  \underline{0.379} & \underline{0.422} &  0.406 & 0.441 &  0.610 & 0.567 &  0.427 & 0.445 &  0.412 & 0.469 &  0.500 & 0.497 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{ETTm1}}}
			& 96 &  \textbf{0.290} & \textbf{0.342} &  \textbf{0.290} & \textbf{0.342} &  \underline{0.292} & \underline{0.346} &  0.320 & 0.347 &  0.334 & 0.368 &  0.326 & 0.390 &  0.375 & 0.398 \\
			& 192 &  \textbf{0.329} & \textbf{0.369} &  \underline{0.332} & \textbf{0.369} &  \underline{0.332} & \underline{0.372} &  0.379 & 0.382 &  0.377 & 0.391 &  0.365 & 0.415 &  0.408 & 0.410 \\
			& 336 &  \textbf{0.359} & \textbf{0.389} &  \underline{0.366} & \underline{0.392} &  \underline{0.366} & 0.394 &  0.406 & 0.402 &  0.426 & 0.420 &  0.392 & 0.425 &  0.435 & 0.428 \\
			& 720 &  \textbf{0.412} & \textbf{0.421} &  0.420 & \underline{0.424} &  \underline{0.417} & \textbf{0.421} &  0.471 & 0.438 &  0.491 & 0.459 &  0.446 & 0.458 &  0.499 & 0.462 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{ETTm2}}}
			& 96 &  \textbf{0.162} & \textbf{0.252} &  \underline{0.165} & 0.255 &  0.173 & 0.262 &  0.176 & \underline{0.253} &  0.180 & 0.264 &  0.180 & 0.271 &  0.189 & 0.280 \\
			& 192 &  \underline{0.222} & \textbf{0.292} &  \textbf{0.220} & \textbf{0.292} &  0.229 & \underline{0.301} &  0.247 & 0.302 &  0.250 & 0.309 &  0.252 & 0.318 &  0.253 & 0.319 \\
			& 336 &  \textbf{0.275} & \textbf{0.327} &  \underline{0.278} & \underline{0.329} &  0.286 & 0.341 &  0.317 & 0.348 &  0.311 & 0.348 &  0.324 & 0.364 &  0.314 & 0.357 \\
			& 720 &  \textbf{0.351} & \textbf{0.380} &  \underline{0.367} & \underline{0.385} &  0.378 & 0.401 &  0.426 & 0.410 &  0.412 & 0.407 &  0.410 & 0.420 &  0.414 & 0.413 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{Weather}}}
			& 96 &  \textbf{0.147} & \textbf{0.198} &  \underline{0.149} & \textbf{0.198} &  0.162 & 0.212 &  0.165 & \underline{0.208} &  0.174 & 0.214 &  0.238 & 0.314 &  0.197 & 0.281 \\
			& 192 &  \textbf{0.191} & \textbf{0.239} &  \underline{0.194} & \underline{0.241} &  0.204 & 0.248 &  0.207 & 0.250 &  0.221 & 0.254 &  0.275 & 0.329 &  0.237 & 0.312 \\
			& 336 &  \textbf{0.243} & \textbf{0.279} &  \underline{0.245} & \underline{0.282} &  0.254 & 0.286 &  0.257 & 0.291 &  0.278 & 0.296 &  0.339 & 0.377 &  0.298 & 0.353 \\
			& 720 &  \textbf{0.310} & \textbf{0.329} &  \underline{0.314} & \underline{0.334} &  0.326 & 0.337 &  0.326 & 0.340 &  0.358 & 0.347 &  0.389 & 0.409 &  0.352 & 0.388 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{Electricity}}}
			& 96 &  \textbf{0.129} & \underline{0.223} &  \textbf{0.129} & \textbf{0.222} &  \underline{0.139} & 0.238 &  0.178 & 0.263 &  0.148 & 0.240 &  0.186 & 0.302 &  0.187 & 0.304 \\
			& 192 &  \textbf{0.145} & \textbf{0.240} &  \underline{0.147} & \textbf{0.240} &  0.153 & \underline{0.251} &  0.187 & 0.272 &  0.162 & 0.253 &  0.197 & 0.311 &  0.199 & 0.315 \\
			& 336 &  \textbf{0.162} & \textbf{0.257} &  \underline{0.163} & \underline{0.259} &  0.169 & 0.266 &  0.199 & 0.285 &  0.178 & 0.269 &  0.213 & 0.328 &  0.212 & 0.329 \\
			& 720 &  \underline{0.198} & \textbf{0.289} &  \textbf{0.197} & \underline{0.290} &  0.206 & 0.297 &  0.236 & 0.318 &  0.225 & 0.317 &  0.233 & 0.344 &  0.233 & 0.345 \\
			\midrule
			\multirow{4}{*}{\rotatebox[origin=c]{90}{\text{Traffic}}}
			& 96 &  \underline{0.365} & \underline{0.251} &  \textbf{0.360} & \textbf{0.249} &  0.388 & 0.282 &  0.523 & 0.303 &  0.395 & 0.268 &  0.376 & 0.415 &  0.607 & 0.392 \\
			& 192 &  \underline{0.382} & \underline{0.258} &  \textbf{0.379} & \textbf{0.256} &  0.407 & 0.290 &  0.530 & 0.303 &  0.417 & 0.276 &  0.423 & 0.446 &  0.621 & 0.399 \\
			& 336 &  \textbf{0.391} & \textbf{0.264} &  \underline{0.392} & \textbf{0.264} &  0.412 & 0.294 &  0.549 & 0.311 &  0.433 & \underline{0.283} &  0.444 & 0.462 &  0.622 & 0.396 \\
			& 720 &  \textbf{0.430} & \textbf{0.285} &  \underline{0.432} & \underline{0.286} &  0.450 & 0.312 &  0.598 & 0.331 &  0.467 & 0.302 &  0.469 & 0.492 &  0.632 & 0.396 \\
			\midrule
			\multicolumn{2}{c|}{$1^\text{st} / 2^\text{nd}$} & \multicolumn{2}{c|}{\textbf{47 / 8}} & \multicolumn{2}{c|}{\underline{17 / 33}} & \multicolumn{2}{c|}{3 / 13} & \multicolumn{2}{c|}{1 / 3} & \multicolumn{2}{c|}{0 / 1} & \multicolumn{2}{c|}{0 / 0} & \multicolumn{2}{c}{0 / 0} \\
			\bottomrule
	\end{tabular}}%
\end{table*}
%\setlength{\tabcolsep}{6pt}

\subsection{Empirical Results}

\emph{Powerformer} achieves state-of-the-art performance as we compare it against other state-of-the-art models in Table~\ref{tab:performance}.
\emph{Powerformer} achieves the best performance in 47 forecasting tasks compared to the next best model (PatchTST) which achieves 17. 
We note that \emph{Powerformer} outperforms ETSformer in all tasks, further supporting our claim that how and where locally decaying attention weights are applied is crucial.
Notably, \emph{Powerformer} outperforms One-Fits-All~\cite{zhou2023OFA} which is more than twice \emph{Powerformer's} size and is built from pre-trained GPT-2~\cite{radford2019language}.


\emph{Powerformer} is sensitive to the mask type and aligns $f(t)$ with the dataset's natural pairwise correlation distribution, which we observe as \fpl{} and \fspl{} consistently outperforming \fbwn.
See Tables~\ref{tab:powerformer_powerLaw_results}, \ref{tab:powerformer_simPowerLaw_results}, \ref{tab:powerformer_butter1_results}, and \ref{tab:powerformer_butter2_results} for full \fpl, \fspl, \fbwo, and \fbwt{} evaluation results.
The \fbwn{} filters resemble step functions and similarly weight all the values within the window.
Alternatively, \fpl{} and \fspl{} allow for longer-time couplings but significantly diminish their contribution; see Fig.~\ref{fig:architecture}c.
The superior performance of \fpl{} and \fspl{} over \fbwn{} further supports that the method for enforcing locality is important and that power-law correlation distributions are naturally suited to our datasets.
We perform further ablation studies and report detailed results in Appendix~\ref{sm:ablation}.

In addition to distinguishing between masks that naturally align with the data distribution, \emph{Powerformer} is also sensitive to the pairwise correlation timescale.
The ETTh and ETTm datasets test \emph{Powerformer's} ability to adapt the timescale $\alpha$ of \maskL{} for datasets sampled from the same distribution but with different temporal resolution.
Since ETTh and ETTm are sampled on the hour and minute timescales, respectively, ETTm requires a slowly decaying mask to access the same information as a quickly decaying mask for ETTh. 
As expected, \emph{Powerformer} with \fspl{} (Table~\ref{tab:powerformer_simPowerLaw_results}) favors quickly decaying masks on ETTh while selecting slowly decaying masks on ETTm.
This illustrates \emph{Powerformer's} sensitivity to both the temporal information timescale, the timescale covering information needed to accurately forecast, and the temporal sampling rate.

To learn the dataset's temporal information timescale, we consider a learnable $\alpha$ instead of treating it as a hyperparameter.
Section~\ref{sm:learnable_decay_times} outlines method and experimental details, as well as the full results.
During training, we observe that $\alpha$ consistently and monotonically drops.
This is expected, as it facilitates overfitting by providing the model with extraneous information.
Ultimately, we observe very little difference between learnable and constant $\alpha$ (see Table~\ref{tab:powerformer_learnable_time}).
This indicates that WCMHA acts as a regularizer during training, removing superfluous information to avoid overfitting and improve generalization.



\subsection{Interpretability}

The simplicity of \emph{Powerformer} is a key strength and indicator that causal and local biases are important for time-series Transformer-based models.
Previous models employ more complicated attention schemes using autocorrelations~\cite{wu2021autoformer} or Fourier decompositions~\cite{zhou.fedformer.2022}; or they induce causal and local implicit biases via various mechanisms \cite{woo2022etsformerexponentialsmoothingtransformers, li.enhancing_locality.2019, Kitaev2020Reformer}.
Some models, like FEDformer~\cite{zhou.fedformer.2022}, purposely avoid such implicit biases.
The success and simplicity of WCMHA and \emph{Powerformer} over other models and their attention techniques bolsters the importance of prioritizing embeddings with causal correlation structures natural to the dataset.
Moreover, \emph{Powerformer's} simplicity, along with the local and causal biases, promotes interpretability and provides insight into the dependencies of each forecast.

\begin{figure}[!t]
	\centering
	\includegraphics[width=0.99\linewidth]{figures/Powerformer_Weather_powerLaw/attn_general_effects_sq512_pl96.pdf}
	
	\caption{We show \emph{Powerformer's} attention score and weight distributions with MHA (dotted line) and with WCMHA (solid lines) for \fpl. The colored lines correspond to different mask decay times $(\alpha)$. These results are for the Weather dataset with a 96 prediction length and 512 input length.}
	\label{fig:powerformer_attn_general}
\end{figure}
\begin{figure}[t!]
	\centering
	\includegraphics[width=0.99\linewidth]{figures/Powerformer_Weather_powerLaw/Powerformer_single_scores_sq512_pl96.pdf}
	
	\caption{We show the causal and local biases' implicit and explicit effects on \emph{Powerformer's} attention score and weight distributions. The reference (dotted line) has no mask (MHA), the solid line uses WCMHA and has the mask applied, and the dashed-dotted line is the WCMHA distribution calculated before applying the mask. This result are for the Weather dataset with a 96 prediction length and 512 input length.}
	\label{fig:powerformer_attn_w_wo_mask}
\end{figure}


By leveraging \emph{Powerformer's} simplicity, we interprate it's behavior through the attention weight and score distributions. 
As the power-law dampening grows (Fig.~\ref{fig:powerformer_attn_general}), an emergent bimodal distribution in attention weights appears.
%
The score distributions (inset in Fig.~\ref{fig:powerformer_attn_general}) shift towards lower values, while the positive heavy-tail increases as the decay constant grows.
This shift in scores to larger magnitude values produces an emergent bimodal distribution in the attention weights.
%
This bimodal behavior is well aligned with our implicit biases as the right mode (larger weights) corresponds to our amplified local interactions and the left mode (smaller weights) corresponds to the diminished non-local interactions.
The bimodal \weightCL{} distributions in Fig.~\ref{fig:powerformer_attn_general} indicates that \maskCL{} achieves our stated goal of amplifying local couplings, removing causally inconsistent couplings, and diminishing unphysically long-time couplings.

Remarkably, we observe \emph{Powerformer} further enforcing our local bias beyond \maskL{}, and we consider this a key insight into the local bias' importance.
Because our masks are constant, \emph{Powerformer} has the potential to mitigate the effects of \maskL{} by learning to reweight the attention scores.
Instead, we observe the opposite, \emph{Powerformer} further enforces our locality bias during training by imposing a bimodal attention weight distribution before \maskCL{} is applied.
Figure~\ref{fig:powerformer_attn_w_wo_mask} shows the attention score and weight distributions with and without applying \maskCL.
The dashed-dotted line in the attention weight indicates that \emph{Powerformer} imposed a local bias before the locally biasing \maskCL{} mask is applied.
This learned effect amplifies the impact of \maskCL{} as seen by the further separation in modes between the solid and dashed-dotted distributions in Fig.~\ref{fig:powerformer_attn_w_wo_mask}.
Ultimately, \emph{Powerformer} learns embeddings that further strengthen the effects of our implicit bias.
We consider \emph{Powerformer's} learned amplification of our causal and local implicit biases to be strong evidence these biases are crucial in time-series forecasting.

Our local biasing power-law \maskCL{} endows \emph{Powerformer} with an implicit temporal multiscale structure as its embeddings change as the timescale increases.
%
Let us consider the case when \maskL{} mask has a long tail (e.g., \fpl{} for $\alpha \leq 0.5$ in Fig.~\ref{fig:maskPL}).
The mask weighting in this tail begins to asymptote and similarly scales the attention weights at large $\Delta t$.
Summing over many similarly weighted time steps washes out high-frequency information and amplifies dynamic information on the time scale of the summation.
For example, if one added a fast oscillating sinusoid to a line and summed this signal over many periods of the sinusoid, the high-frequency oscillations would cancel and one would see a linear signal.
Conversely, in temporally local regions the mask quickly changes to amplify local high-frequency components.
These local high-frequency components thus play a more prominent role than high-frequency components from longer windows.
The mask imposes a temporal multiscale attention structure by capturing low-frequency information from longer-time summations and high frequency from local interactions.
In Appendix~\ref{sm:visualizations}, we observe this phenomena as \emph{Powerformer} better predicts higher frequency contributions fast-varying signals than PatchTST. 

By removing longer-term pairwise contributions, WCMHA has the potential to improve the attention complexity cost from quadratic to linear.
Moreover, since multiple timescales can provide similar results (Tables~\ref{tab:powerformer_powerLaw_results}, \ref{tab:powerformer_simPowerLaw_results}, \ref{tab:powerformer_butter1_results}, and \ref{tab:powerformer_butter2_results}), such speed improvements may come at a small cost to accuracy.
Each mask type and timescale has a cutoff length $\tau$, after which the contributions from data are negligible: e.g., for \fpl{} with $\alpha=1$, points beyond $\tau = 100$ are diminished by two orders of magnitude and may be considered negligible.
When WCMHA is only applied within $\tau$, it scales linearly O$(\tau T)$ with input sequence length $T$, instead of quadratically (O$(T^2)$) like MHA.
Because $\alpha$ varies between datasets and can be lowered to trade speed for small changes in accuracy, we expect a large variance in potential speedup times, and so we do not provide results here.
For example, \fpl{} with $\alpha=1$ provides a factor of 3 and 5 speedup for typical look-back window of 336 and 512, respectively.

WCMHA leverages the strengths of both statistical models and MHA.
The induced power-law decay in attention weights, and thus temporal correlations, resembles previous statistical models.
However, the WCMHA attention mechanism is not fully overwhelmed by this decay and still amplifies important longer-time correlations, as shown in Fig.~\ref{fig:architecture}b and Fig.~\ref{fig:architecture}c where the attention mask clearly selects regions of strong correlations near and past the tail of the power-law decay.
We believe this balance between power-law temporal correlations and maintaining WCMHA's ability to attend to highly correlated input is key to WCMHA's success.


\section{Conclusion}
We develop \emph{Powerformer} to impose and investigate the importance of causal and local biases for Transformer-based time-series models.
\emph{Powerformer} outperforms state-of-the-art time series models on standard tasks often with a simpler architecture.
\emph{Powerformer's} simplicity promotes interpretability studies that elucidate the effects of our biases.
\emph{Powerformer} strengthens our local bias by learning to decrease attention scores for long-time interactions.

In addition to outperforming previous models on forecasting accuracy, \emph{Powerformer} also has the potential to significantly increase attention efficiency.
The decaying nature of $f(t)$ determines a cutoff time after which \maskL{} effectively removes contributions.
Evaluating within this cutoff time will result in O($\tau T$) rather than O($T^2$) complexity.

\section*{Acknowledgements}

NBE would like to acknowledge the Laboratory Directed Research and Development Program of Lawrence Berkeley National Laboratory under U.S. Department of Energy Contract No. DE-AC02-05CH11231, and NERSC
(DE-AC02-05CH11231) for providing compute resources. 



\bibliography{references}
\bibliographystyle{plain}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX

\newpage
\appendix
\onecolumn

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{page}{1}
\setcounter{section}{0}
\makeatletter
\renewcommand \thesection{A\@arabic\c@section}
\renewcommand{\theequation}{A\arabic{equation}}
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}

\input{S01_butterworth}
\input{S02_datasets}
\input{S03_architecture}
\input{S09_visualizations}
\input{S04_experiments}
\input{S05_ablation}


\end{document}

