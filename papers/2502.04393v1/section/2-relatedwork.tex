\section{Related Work}
\label{sec:related_work}
\begin{figure*}[t] % t表示将图形放在页面顶端
    \centering
    % 根据需要调整图片宽度，如width=\textwidth表示与双栏总宽度相同
    \includegraphics[width=\textwidth]{figures/insight.pdf} 
    \caption{Visualization of attention differences in Open-Sora. (a) Conventional U-shaped error distribution and sudden error spikes; (b) Error accumulation in regions with sudden spikes: the left side employs the EDCW strategy, while the right side uses manually set cache window sizes; (c) Similarity of attention maps in different blocks; (d) Original attention map and sliced attention map following PCAS.}
    \label{fig:Observations}
    \vspace{-5mm}
\end{figure*}

\subsection{Video Generation}
Video generation focuses on creating realistic videos that are visually appealing and exhibit seamless motion.
The foundational technologies include GAN-based approaches \cite{saito2017temporal,wang2018video}, auto-regressive models \cite{weissenborn2019scaling}, UNet-based diffusion models \cite{blattmann2023stable,ho2022video}, and Transformer-based diffusion models \cite{ma2024latte,hong2022cogvideo,yang2024cogvideox,xu2024vasa}. Among these, diffusion models are widely applied in generating multimodal data, such as video and images \cite{rombach2022high,esser2024scaling}, due to their powerful data distribution modeling capabilities. 
In video generation, Transformer-based diffusion models, specifically those based on DiT, are highly favored for their efficient scalability in an era of increasing computational power.
The computational challenges in Transformer-based frameworks primarily stem from attention mechanisms, where video generation employs three main types: spatial, temporal, and cross attention \cite{ma2024latte,yang2024cogvideox,blattmann2023stable,kong2024hunyuanvideo}. PAB \cite{zhao2024pab} highlights that differences between adjacent diffusion steps are most pronounced in the early and late stages, forming a U-shaped pattern, with significant variations in spatial and temporal attention computations.
This paper specifically addresses the acceleration within the DiT-based video generation framework.


\subsection{Accelerating Diffusion Models}
Video diffusion models have achieved impressive quality in generation, yet their speed is often limited by the sampling mechanisms used during inference. Approaches to accelerate inference can be classified into three main categories: 
(1) Developing enhanced solvers for SDE/ODE equations \cite{lu2022dpm}, which offer limited speed gains and suffer from quality degradation when sampling steps are reduced due to accumulated discretization errors. 
(2) Utilizing diffusion distillation techniques \cite{yin2024onestep}, which apply 2D distillation methods to video generation within a unified diffusion model framework. 
(3) Modifying the architecture of pre-trained models to address computational bottlenecks in the inference process, using techniques such as caching, reuse, and post-training methods like model compression, pruning (\textit{e.g.,} matrix decomposition and dimensionality reduction) \cite{ashkboos2024slicegpt}, and quantization.

Faster Diffusion \cite{li2023faster} caches self-attention early on and then leverages cross-attention for enhancing fidelity in later stages.
PAB \cite{zhao2024pab} eliminates attention computation during the diffusion process by broadcasting attention output in the stable middle phase of diffusion.
$\Delta$-DiT \cite{chen2024delta} leverages the correlation between DiT blocks and image generation by caching backend blocks in early sampling stages and frontend blocks in later stages to achieve faster generation.
Unlike these methods, we focus on accelerating the computation of temporal and spatial attention by utilizing caching and post-training techniques (in our paper, \textit{i.e.,} PCA dimensionality reduction), which are commonly used in the Natural Language Processing field \cite{ashkboos2024slicegpt}.





