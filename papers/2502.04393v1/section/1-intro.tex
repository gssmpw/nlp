\section{Introduction}
\label{sec:intro}
% Diffusion transformers (DiTs) \cite{Peebles2022DiT} have recently gained significant traction in video generation, often surpassing CNN-based methods \cite{rombach2022high,blattmann2023stable} in terms of output quality. However, this improvement comes at a high cost, demanding more memory, computation, and inference time. Consequently, finding an efficient approach for DiT-based video generation has become an urgent priority for broadening the reach of GenAI applications.
Diffusion transformers (DiTs) \cite{Peebles2022DiT,yuan2024ditfastattn} have recently become prominent in video generation, often exceeding the output quality of unet-based methods \cite{ho2020denoising,rombach2022high,blattmann2023stable}. However, this advancement requires substantial memory, computational resources, and inference time. Therefore, developing an efficient method for DiT-based video generation is crucial for expanding the scope of generative AI applications.



% Unlike traditional UNet \cite{ho2020denoising,song2020denoising} in diffusion models, the DiT has a unique isotropic architecture that lacks encoders, decoders, and different depth skip connections. 
Unlike traditional unet architectures used in diffusion models \cite{ho2020denoising,song2020denoising}, the DiT employs a distinctive isotropic design that omits encoders, decoders, and skip connections of varying depths.
This causes the existing feature reuse mechanism such as DeepCache \cite{ma2023deepcache} and Faster Diffusion \cite{li2023faster}, may result in the loss of information when applied for DiT. PAB \cite{zhao2024pab} discovered that the attention differences between adjacent diffusion steps follow a U-shaped pattern, and in response developed a pyramidal caching strategy tailored to this observation. $\Delta-$DIT \cite{chen2024delta} discovered that the front blocks primarily handle low-level details, while the back blocks focus more on semantic information, and accordingly designed a two-stage error caching strategy tailored to these insights. DITFastAttn \cite{yuan2024ditfastattn} analyzes redundancy within attention blocks, implementing targeted caching strategies for both the attention outputs and the conditional/unconditional settings. However, these methods typically depend on the U-shaped error curve and manually selected step sizes for caching. As a result, they offer no effective strategies for handling the high-error regions at the ends of the curve or the sudden spikes at its bottom.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/visual.pdf}
    \caption{Accelerating video generation methods like OpenSora, Latte, CogVideoX.}
    \label{fig:visual}
    \vspace{-7mm}
\end{figure}
To achieve a more flexible and impactful acceleration solution, we introduce UniCP—an error-aware framework that integrates caching and pruning strategies to accelerate the process across both temporal and spatial dimensions. Specifically: (1) To address sudden error spikes at the bottom of the U-shaped error distribution, UniCP employs an Error-Aware Dynamic Cache Window (EDCW). This mechanism dynamically adjusts caching intervals and strategies based on real-time error feedback. (2) To mitigate large discrepancies at the two ends of the U-shaped error curve, we present a PCA-based Slicing (PCAS) strategy for pruning, further reducing the network’s computational complexity. (3) To unify caching and pruning, we devise a Dynamic Weight Shift (DWS) strategy, seamlessly integrating both approaches across temporal and spatial domains. Our approach delivers up to a 1.6× speedup on a single GPU without compromising video quality. The main contributions of our paper are as follows:

\begin{itemize}
    \item We present UniCP, the first framework to jointly integrate caching and pruning strategies, providing a more flexible and comprehensive approach to accelerating video generation.

    \item An Error-Aware Dynamic Cache Window (EDCW) strategy is proposed to prevent sudden error spikes at the bottom of the U-shaped error distribution.

    \item A PCA-based Slicing (PCAS) strategy is introduced to reduce computational overhead in the attention modules during time steps characterized by large errors that cannot be effectively cached.

    \item A Dynamic Weight Shift (DWS) strategy is proposed to integrate caching and pruning approaches, optimizing the generation process across both spatial and temporal dimensions.
\end{itemize}


