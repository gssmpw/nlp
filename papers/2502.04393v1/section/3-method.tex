\section{Method}
\label{sec:method}

\begin{figure}[t] % t表示将图形放在页面顶端
    \centering
    % 根据需要调整图片宽度，如width=\textwidth表示与双栏总宽度相同
    \includegraphics[width=0.48\textwidth]{figures/EDCW.pdf} 
    \caption{Visualization of the cache routine in EDCW. EDCW dynamically adjusts the cache window size and caching strategy based on the error threshold.}
    \label{fig:cache}
    \vspace{-3mm}
\end{figure}
\begin{figure}[t] % t表示将图形放在页面顶端
    \centering
    % 根据需要调整图片宽度，如width=\textwidth表示与双栏总宽度相同
    \includegraphics[width=0.48\textwidth]{figures/slice.pdf} 
    \caption{Visualization of the PCAS. PCAS reduces the computational cost of the attention mechanism by pruning redundant dimensions in the query and key matrices.}
    \label{fig:slice}
    \vspace{-4mm}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figures/cachemap.pdf}
    \caption{After acquiring the spatial-temporal cache map, the DWS strategy enables dynamic switching between caching and pruning strategies, allowing both processes to operate within a unified framework.}
    \label{fig:Cache map}
\vspace{-4mm}
\end{figure}
% As shown in Fig.~\ref{fig:Observations}, even at the bottom of the U-shaped error curve, sudden error spikes can still occur. Manually setting the caching interval leads to unstable results. Moreover, significant errors on both sides of the U-shaped error distribution result in poor performance when using a caching strategy. To address these issues, we propose three targeted optimization strategies. In Section \ref{chap:EDCW}, we introduce Error-Aware Dynamic Cache Windows to dynamically adjust the caching window in response to the observed error. In Section \ref{chap:PCAS}, we introduce PCA-based Slicing to further reduce computation in timesteps that cannot be cached due to error distribution. In Section \ref{chap:DWS}, we introduce a Dynamic Weight Shift strategy that can integrate pruning and caching strategies into a single framework.
As illustrated in Fig.~\ref{fig:Observations}, even at the bottom of the U-shaped error curve, sudden error spikes can still occur. Manually setting the caching interval results in unstable outcomes. Moreover, significant errors on both sides of the U-shaped error distribution lead to poor performance when employing a caching strategy. To address these challenges, we propose three targeted optimization strategies. In Section \ref{chap:EDCW}, we introduce Error-Aware Dynamic Cache Windows, which dynamically adjust the caching window in response to observed errors. In Section \ref{chap:PCAS}, we present PCA-based Slicing to further reduce computation during time steps that cannot be cached due to error distribution. In Section \ref{chap:DWS}, we describe a Dynamic Weight Shift strategy that integrates pruning and caching strategies into a unified framework.
\subsection{Error-Aware Dynamic Cache Window (EDCW)}
\label{chap:EDCW}
As shown in Fig.~\ref{fig:Observations}, the error distribution within each attention block during inference does not strictly form a U-shaped pattern. Instead, there are larger errors at both ends and sudden spikes at the bottom. We contend that the caching interval should be determined by the error itself. Given a certain attention block $i$ under timestep $j$, the cache step $t_{i,j} $ can be defined as:
\begin{equation}
    t_{i,j}=\omega(\delta , o_{i}, o_{i+k}, a_{i}, a_{i+k})
\end{equation} 
Where \(\delta\) is the user-defined error threshold, and \(k=K,K-1,\ldots,1\) denotes the size of the dynamic search window. The parameter \(\omega\) specifies a dynamic caching strategy: we begin by measuring the error between the attention outputs \((o_i)\) and \((o_j)\). If the computed error fails to meet the threshold \(\delta\), an alternative caching approach is then applied to the attention map \((a_i)\). The first strategy offers greater computational savings but allows for higher error, whereas the second approach preserves more accuracy at the cost of reduced computational gains. By employing predefined error thresholds, EDCW can dynamically adjust both the caching window and the chosen caching strategy. 
% To be specific, given the attention output we cache the whole output, given the attention map, we


% Firstly, we introduce the Error-Aware Dynamic Cache Window strategy. In the algorithm process, the loss threshold curve and the Cache Map are two-dimensional arrays with a length of \(T\) and a width of \(L\). \(T\) represents the number of diffusion timesteps , and \(L\) represents the depth of the stack of transformer blocks.

% Simply , the model caches the output of the attention layers for all blocks in the first timestep. Each time the module processes a timestep, it compares the cached attention values with the attention output values of this timestep. The comparison is performed by calculating the Mean Squared Error (MSE) values for different timesteps and the same block. The MSE loss is a commonly used metric to measure the difference between two images. Specifically, it calculates the average of the squared differences between corresponding pixel values.It can also be used to calculate the difference in the attention output of the model.
% The module is pre-configured with an error threshold curve, which indicates the maximum permissible error for the cached module. A simple formula for calculating the thereshold of timestep is:
% \begin{align}
% \text{threshold}_{ij}= \frac{\text{block}_i + 1}{\text{depth}} \times \text{threshold}_j
% \end{align}
% where \( threshold_{ij} \) represents the i-th block of the transformer model , \( depth \) indicates the depth of the transformer model, and the \( threshold_j \) represents the maximum threshold for manual design in timestep j.

% In the model pre-inference stage, by comparing the calculated MSE value with the predesigned loss threshold curve of the module, if the MSE value is less than the corresponding value on the threshold curve, this position is marked as True on the Cache Map, indicating that this transformer block can be cached. If it exceeds the threshold curve, it is not marked. A diagram of Cache Map can be found in Fig.\ref{fig:Cache map}.


% The loss threshold curve stores the maximum allowable precision loss for each block, while Cache Map contain data which type is bool, indicating which model blocks are cached and which are not. If the j-th block in the i-th row in the Cache Map is marked as False, it means that the i-th block in transformer model at the j-the timestep will not be cached, and the model will continue to execute the attention layer and cache its output. Conversely, if it is marked as True, it indicates that caching will occur at the i-th block during the j-th timestep. In this case, the attention operation will be skipped and replaced by the attention output from the closest preceding block in the same row where Cache Map is marked as False in column n (where \( n < j \)) and the computation of the attention process is skipped.


\subsection{PCA-based Slicing (PCAS)}
\label{chap:PCAS}
At the far ends of the U-shaped error distribution — encompassing roughly 30\% of all steps — attention block outputs diverge significantly. In such situations, attempting to cache results may actually degrade the method’s performance. To mitigate this, we introduce a pruning approach tailored for those uncachable blocks, focusing on the query and key transformations, as well as the corresponding linear layer parameters, to further alleviate computational burden.

Principal component analysis (PCA) commonly aims to transform an original data matrix \(\mathbf{X} \in \mathbb{R}^{m \times m}\) into a compact representation \(\mathbf{\overline{Z}} \in \mathbb{R}^{m \times n}\) (where \(n < m\)) and a reconstructed approximation \(\overline{\mathbf{X}} \in \mathbb{R}^{m \times m}\). The core operation of an attention map involves multiplying the query and the key matrices, expressed as: $\text{softmax}(\mathbf{Q}\mathbf{K}^{\top})$. Given an attention input $\mathbf{X}$ and the corresponding weight for the query and key, it can be length defined as: $\text{softmax}((\mathbf{XW_{q}})(\mathbf{XW_{k}}^{\top}))$. Letting \(\mathbf{R} \in \mathbb{R}^{m \times m}\) represent an eigenvector matrix of the attention input \(\mathbf{X}^{\top}\mathbf{X}\), the compressed expression for query and key can be formulated as:
\begin{equation}
        \mathbf{Z_{q}} = \mathbf{(XW_{q})RD}, \quad \quad \overline{\mathbf{Q}} = \mathbf{Z_{q} D}^{\top}\mathbf{R}^{\top}.
\end{equation}
\begin{equation}
        \mathbf{Z_{k}} = \mathbf{(XW_{k})RD}, \quad \quad \overline{\mathbf{K}} = \mathbf{Z_{k} D}^{\top}\mathbf{R}^{\top}.
\end{equation}
Here, \(\mathbf{D} \in \mathbb{R}^{m \times n}\) is a selection matrix derived from the identity matrix, retaining only \(n\) thin columns to reduce dimensionality while preserving critical structure. This reconstruction is \(L_2\)-optimal in the sense that the chosen linear mapping $\mathbf{RD}$ minimizes \(\|\mathbf{X} - \overline{\mathbf{X}}\|\). 




 



% Note that multiplying a vector $x$ by $Q$ does not change the norm of the vector, since ∥Qx∥ = px⊤Q⊤Qx = √x⊤x = ∥x∥. In this work, the dimensions of Q will always match the embedding dimension of the transformer D.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% In previous methods, a single conversion  based on PCA techniques was applied to the model weights using PCA decomposition techniques, maintaining the model output unchanged based on the computation invariance of the RMSNorm layer. However, in the video generation model based on the DiT model, the computational invariance cannot be maintained due to the existence of an access control mechanism.We innovatively apply PCA techniques to the attention architecture of the model for information compression: Despite becoming dense, PCA effectively compresses data, reducing dimensionality when preserving important information.


% Simply, the process of computing the Attention output in the model can be summarized as follows:
% \begin{equation}
% \text{Attention} = \left[ \text{softmax}\left( \frac{\text{Q}  \text{K}^T}{\sqrt{\text{K}_{\text{dim}}}} \right)  \text{V} \right]  \text{O}
% \end{equation}



% During model inference, we collect the data flow from the query and key linear layers of the Attention module and compute the covariance matrix. Based on this matrix, we perform an eigenvalue decomposition to obtain the principal component matrix \( R \) of the Attention module query and key. The resulting matrix \( R \) has dimensions of \([\text{in\_feature}, \text{in\_feature}]\). We can remove a portion of the model weights from the back, assuming that the dimension of the data to be retained is \(\text{dim}\) (where \(\text{dim} < \text{in\_feature}\)). This trimmed matrix is then multiplied (i.e., multiplied by right) with the model's query and key weights. 

% According to the model's computation rules, we can obtain:
% \begin{small}
% \begin{align*}
% \text{Attention} &= \left( \text{softmax}\left(\frac{(Q  R)  (K  R)^T}{\sqrt{K_{\text{dim}}}}\right)  V \right)  O \\
% &= \left( \text{softmax}\left(\frac{Q  R  R^T  K^T}{\sqrt{K_{\text{dim}}}}\right)  V \right)  O
% \end{align*}
% \end{small}
% Thus, the final weight matrix obtained has dimensions of \([\text{hidden\_size}, \text{dim}]\). Since the principal component matrix obtained from PCA is an orthogonal matrix, by the mathematical properties of orthogonal matrices, we know that:
% \[
% R^T R = I
% \]
% so we can obtain:
% \begin{small}
% \begin{align*}
% \text{Attention} &= \left( \text{softmax}\left(\frac{Q  R  R^T  K^T}{\sqrt{K_{\text{dim}}}}\right)  V \right)  O \\
% &= \left( \text{softmax}\left(\frac{Q  K^T}{\sqrt{K_{\text{dim}}}}\right)  V \right)  O
% \end{align*}
% \end{small}
% where \( R \) is the orthogonal matrix and \( I \) is the identity matrix.

% As a result, the pruned model retains almost the same output as the original model while reducing its dimensionality, thereby decreasing computational overhead during inference. In practical applications, the attention mechanism of the model is often composed of multi-head attention. As shown in Figure 2, the weights of attention query layer or key layer can be divided into  parts. Each part can independently undergo eigenvalue decomposition using the data flow extracted from the model. Based on the dimension  \(head-dim\) of the multi-head attention, these parts are split, and then concatenated back together to reduce the computational loads of the model.
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=\columnwidth]{ICME/figures/Attn Head.png}
%     \caption{Attention weight slice and rotate}
%     \label{fig:Attn Head Slice}
% \end{figure} 

\begin{algorithm}
\caption{Detailed Caching and Pruning Strategy Given an Attention Block under Certain Timestep}
\begin{algorithmic}
\Require $X, \text{δ}$ \Comment{Attention Input, Error Threshold}
\Require $K, c$ \Comment{Search Window, Cache Status}
\Require $a, o$ \Comment{Attention Map, Attention Output}
\Require $i$ \Comment{Current Step}

\State Initialize Cache Status $c=F$

% \State $S \gets \sigma(S)$
% \State $\text{Cache-Attn} = S$

\If{$c=F:$}
    \For{$k=K$ to $1$:}
\State \textbf{if }{$\begin{Vmatrix}o_{i} - o_{i+k}\end{Vmatrix} _{2} \le \delta_{i+k}:$}
        cache attention output; $c=T$; Break;
    \EndFor
\For{$k=K$ to $1$:}
\State \textbf{if }{$\begin{Vmatrix}a_{i} - a_{i+k}\end{Vmatrix} _{2} \le \delta_{i+k}:$}
        cache attention map; $c=T$; Break;
    \EndFor
\State Apply PCA-based Slicing
\State Update c as processed; Break;
\EndIf
% \State \textbf{if }{$\begin{Vmatrix}a_{i} - a_{i+j}\end{Vmatrix} _{2} \le \delta:$}
%         cache attention output; $C_{i,j}=T$;

%     \For{$j = K$ to $1$}
%         \State \textbf{if }{$\begin{Vmatrix}o_{i} - o_{i+j}\end{Vmatrix} _{2} \le \delta:$}
%         cache attention output;
%         $C_{i,j}=T$;
%     \State
%         \If{$\text{MSE}_{ij} > \text{δ}_{ij}$}
%             \State $S' = \text{PCAS}(S)$
%         \EndIf
%         \State $\text{Cache-Attn} = S'$
%         \State $S = \sigma(S')$
%     \EndFor
% \EndFor
\end{algorithmic}
\end{algorithm}


\subsection{Dynamic Weight Shift (DWS)}
\label{chap:DWS}

EDCW and PCAS enhance the network from spatial (individual attention block processing) and temporal (caching across denoising steps) dimensions, but applying both simultaneously can cause interference. To unify them, we propose a Dynamic Weight Shift (DWS) strategy. Guided by a cache map Fig.~\ref{fig:Cache map}, DWS identifies uncachable blocks, prunes them selectively, and stores both pre-pruning and post-pruning weights, allowing for a dynamic integration of pruning and caching. As shown in Algorithm 1, we initially skip partitioning to preserve the original output. Then, we gradually increase the attention head partition size until the loss approaches the threshold curve, recording a suitable dimension \(k\) that remains below this threshold. Once the adaptive algorithm finishes, we compile all recorded \(k\) values for that block and select the smallest one as the final pruning dimension.

% Previously introduced methods optimize the network from both spatial (processing individual attention blocks) and temporal (caching across denoising steps) dimensions. However, directly combining these two approaches can lead to mutual interference. To unify them into a single framework, we propose a Dynamic Weight Shift (DWS) strategy. By tracking the cache map (\ref{fig:Cache map}) of each block throughout the entire denoising process, we selectively prune those modules that cannot be cached and store both the pre- and post-pruning weights. This allows us to dynamically adjust pruning and caching strategies in an integrated manner.

% As shown in Algorithm \ref{alg:1}. Initially, we do not perform any partitioning on the module, ensuring that the output remains consistent with the original output. Subsequently, we begin to increase the dimensionality of the attention head partitioning from one dimension, gradually bringing the loss closer to the threshold curve, and determining a pruning dimension \( k \) that does not exceed the threshold. This \( k \) is not used directly for partitioning, but is saved. After the adaptive algorithm completes its run in the model, all \( k \)  for that block are collected, and the minimum \( k \)  is selected as the final pruning dimension for the model.

