\section{Related Work}
\subsection{Multi-Teacher Knowledge Distillation}
	The conventional KD frameworkHuang, "Deep Neural Networks for Object Detection" often improves a student network taught by a high-capacity teacher. However, the student performance may be bounded by a single teacher due to limited knowledge. Many works attempted to rely on multiple teacher networks for knowledge transfer. The core question of multi-teacher KD is how to balance the weights among various teacher networks. A straightforward methodSrinivas, "Multi-Task Deep Learning" is to assign an equal weight to each teacher, but it may ignore the diversity of teachers and lead to suboptimum performance. 
	
	Some subsequent works explore better multi-teacher KD algorithms with meaningful balancing weights.  AE-KDVanish et al., "Learning Where to Look" formulates multi-teacher KD as a multi-objective optimization problem and derives the weight solutions from the perspective of gradient space. AMTML-KDTseng et al., "Attention-based Multi-Task Learning for Knowledge Distillation" introduces a latent factor for each teacher and calculates teacher importance by making an element-wise product with the student max-pooled features. EB-KDGuo et al.,  "Entropy-Based Aggregation for Knowledge Distillation" computes aggregation weights according to information-entropy, since the entropy measures the confidence of a teacher predictive distribution. However,  EB-KD may assign an inaccurate weight when the teacher's predictive category is not correct. To address this problem, CA-MKDQiao et al., "Cross-Entropy Complemented Multi-Task Learning for Knowledge Distillation" introduces cross-entropy complemented by ground-truth labels to guarantee the correct category guidance. Instead of exploiting probability-level information, the recent MMKDVanhoucke et al., "Meta-Learning for Multi-Teacher Knowledge Distillation" introduces a meta-learning mechanism with a hard buffer to optimize aggregation weights of both the teachers' features and logits. Although both MOBAZhang et al., "Multi-Agent Reinforcement Learning" and RMTSSun et al., "Rewarding Multitask Supervision Learning" involve RL and distillation, they have essential distinctions compared to MTKD-RL in \emph{solved tasks}. MOBA boosts RL models in continuous control tasks, \emph{e.g.} games and robotics. RMTS enhances BERT models for natural language processing tasks. Our MTKD-RL improves CNN or ViT models for visual recognition tasks. More references about multi-teacher KD works could refer to this surveyQiao et al., "A Survey on Multi-Teacher Knowledge Distillation". 
	
	Although previous multi-teacher KD methods achieve good performance gains, they often consider a single level to optimize teacher weights.  Moreover, they neglect the interaction between ensemble teachers and the student, especially student accuracy as a critical indicator. Therefore, the generated weights may not reflect the comprehensive capability of teachers and lack compatibility with a specific student. By contrast, our MTKD-RL method  constructs both the teacher performance and teacher-student gaps as the state, optimized by the student performance as rewards, leading to more meaningful teacher weights.
	
	
	\subsection{Reinforcement Learning}
	Reinforcement Learning (RL) has achieved great success in decision-makingSutton et al., "Introduction to Reinforcement Learning". The basic framework of the RL systemSutton and Barto, "Reinforcement Learning: An Introduction" is to maximize the reward achieved by an agent when interacting with the environment. During RL training, the agent outputs an action using the observed state from the environment.  After action execution, the RL system updates the agent according to the returned reward. The RL loop iterates on episodes until the agent converges.
	
	The RL optimization can be divided into value-based (\emph{e.g. }DQN Watkins and Dayan, "Q-learning" and DDQN Van Hasselt et al., "Deep Reinforcement Learning with Double Q-Learning") and policy-based (\emph{e.g. } PG Sutton et al.,  "Policy Gradient Methods for Reinforcement Learning" and PPO Schulman et al., "Trust Region Policy Optimization") algorithms. Value-based RL often selects an action with the maximum Q-value.  Policy-based RL constructs a probability distribution to sample an action. Compared with value-based RL, policy-based RL has two advantages: (1) it is more compatible with continuous action space; (2) it avoids policy degradation since it does not have value function errors in value-based RL. Based on the above analyses, policy-based RL is more suitable for optimizing continuous multi-teacher weights as action space. Policy Gradient (PG)Sutton et al.,  "Policy Gradient Methods for Reinforcement Learning" uses reward from the observed state to optimize the action policy. In the conventional RL system, the original PG may be difficult to converge due to the large gradient variance. Some advanced variants are proposed to improve PG based on actor-critic (\emph{e.g. }DPG Silver et al., "Deterministic Policy Gradient Algorithms" and DDPG Lillicrap et al., "Continuous Control with Deep Reinforcement Learning") and trust region (\emph{e.g. }PPO Schulman et al., "Trust Region Policy Optimization"). 
	
	As shown in Table~\ref{ablation}, we found that the multi-teacher decision is not sensitive to PG algorithms. One possible reason is that the gradient optimization of multi-teacher decisions with a fixed visual dataset is more stable than the conventional RL environment. Therefore, we choose the original PG to optimize multi-teacher weights. Notice that this paper focuses on a unified framework of multi-teacher KD with RL instead of proposing a new RL algorithm.