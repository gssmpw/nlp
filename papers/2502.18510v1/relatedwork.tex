\section{Related Work}
\subsection{Multi-Teacher Knowledge Distillation}
	The conventional KD framework~\cite{hinton2015distilling,yang2021hierarchical,yang2022knowledge,yang2022cross,yang2024clip} often improves a student network taught by a high-capacity teacher. However, the student performance may be bounded by a single teacher due to limited knowledge. Many works attempted to rely on multiple teacher networks for knowledge transfer. The core question of multi-teacher KD is how to balance the weights among various teacher networks. A straightforward method~\cite{you2017learning} is to assign an equal weight to each teacher, but it may ignore the diversity of teachers and lead to suboptimum performance. 
	
	Some subsequent works explore better multi-teacher KD algorithms with meaningful balancing weights.  AE-KD~\cite{du2020agree} formulates multi-teacher KD as a multi-objective optimization problem and derives the weight solutions from the perspective of gradient space. AMTML-KD~\cite{liu2020adaptive} introduces a latent factor for each teacher and calculates teacher importance by making an element-wise product with the student max-pooled features. EB-KD~\cite{kwon2020adaptive}  computes aggregation weights according to information-entropy, since the entropy measures the confidence of a teacher predictive distribution. However,  EB-KD may assign an inaccurate weight when the teacher's predictive category is not correct. To address this problem, CA-MKD~\cite{zhang2022confidence} introduces cross-entropy complemented by ground-truth labels to guarantee the correct category guidance. Instead of exploiting probability-level information, the recent MMKD~\cite{zhang2023adaptive} introduces a meta-learning mechanism~\cite{finn2017model} with a hard buffer to optimize aggregation weights of both the teachers' features and logits. Although both MOBA~\cite{kang2022moba} and RMTS~\cite{yuan2021reinforced} involve RL and distillation, they have essential distinctions compared to MTKD-RL in \emph{solved tasks}. MOBA boosts RL models in continuous control tasks, \emph{e.g.} games and robotics. RMTS enhances BERT models for natural language processing tasks. Our MTKD-RL improves CNN or ViT models for visual recognition tasks. More references about multi-teacher KD works could refer to this survey~\cite{yang2023categories}. 
	
	Although previous multi-teacher KD methods achieve good performance gains, they often consider a single level to optimize teacher weights.  Moreover, they neglect the interaction between ensemble teachers and the student, especially student accuracy as a critical indicator. Therefore, the generated weights may not reflect the comprehensive capability of teachers and lack compatibility with a specific student. By contrast, our MTKD-RL method  constructs both the teacher performance and teacher-student gaps as the state, optimized by the student performance as rewards, leading to more meaningful teacher weights.
	
	
	\subsection{Reinforcement Learning}
	Reinforcement Learning (RL) has achieved great success in decision-making~\cite{kaelbling1996reinforcement,liu2020new,chengqing2023multi}. The basic framework of the RL system~\cite{mnih2013playing,van2016deep} is to maximize the reward achieved by an agent when interacting with the environment. During RL training, the agent outputs an action using the observed state from the environment.  After action execution, the RL system updates the agent according to the returned reward. The RL loop iterates on episodes until the agent converges.
	
	The RL optimization can be divided into value-based (\emph{e.g. }DQN~\cite{mnih2013playing} and DDQN~\cite{van2016deep}) and policy-based (\emph{e.g. } PG~\cite{sutton1999policy} and PPO~\cite{schulman2017proximal}) algorithms. Value-based RL often selects an action with the maximum Q-value.  Policy-based RL constructs a probability distribution to sample an action. Compared with value-based RL, policy-based RL has two advantages: (1) it is more compatible with continuous action space; (2) it avoids policy degradation since it does not have value function errors in value-based RL. Based on the above analyses, policy-based RL is more suitable for optimizing continuous multi-teacher weights as action space. Policy Gradient (PG)~\cite{sutton1999policy} uses reward from the observed state to optimize the action policy. In the conventional RL system, the original PG may be difficult to converge due to the large gradient variance. Some advanced variants are proposed to improve PG based on actor-critic (\emph{e.g. }DPG~\cite{silver2014deterministic} and DDPG~\cite{lillicrap2015continuous}) and trust region (\emph{e.g. }PPO~\cite{schulman2017proximal}). 
	
	As shown in Table~\ref{ablation}, we found that the multi-teacher decision is not sensitive to PG algorithms. One possible reason is that the gradient optimization of multi-teacher decisions with a fixed visual dataset is more stable than the conventional RL environment. Therefore, we choose the original PG to optimize multi-teacher weights. Notice that this paper focuses on a unified framework of multi-teacher KD with RL instead of proposing a new RL algorithm.