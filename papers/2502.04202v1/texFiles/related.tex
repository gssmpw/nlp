\section{Related Work}
\label{sec:related}

In this section, we discuss the works related to our paper.

\subsection{Empirical studies on Mobile Performance}
%Detecting performance issues in mobile applications is critical for maintaining a smooth user experience, as such issues directly influence user satisfaction. Common challenges include identifying janky frames, slow image rendering, and screen freezes, all of which negatively impact an app’s responsiveness. Some empirical studies have extensively examined these challenges. 

Several empirical studies~\cite{ 2014_ICSE_Characterizing_and_detecting_performance_bugs, 2023_empirical_study_on_mobile_performance, 2020_EMSE_statically_detectable_performance_issues} have investigated and categorized performance issues in mobile applications.
Liu et al.~\cite{2014_ICSE_Characterizing_and_detecting_performance_bugs} categorized performance issues into three main categories: GUI lag, memory bloat, and energy leaks. Notably, GUI lag accounted for over 75\% of the performance issues, significantly reducing app responsiveness.
Rua et al.~\cite{2023_empirical_study_on_mobile_performance} examined how semantic code changes impact performance metrics like energy usage, runtime, and memory consumption. Their findings revealed inconsistencies between these metrics and the priorities set by Android Lint. 
%Das et al.~\cite{2020_EMSE_statically_detectable_performance_issues} explored the evolution of performance issues detected by Android Lint, identifying that resource recycling issues are common and often unresolved. 
These studies highlight the need for more advanced tools and techniques to accurately detect and address performance issues in mobile applications, ultimately improving user experience and app reliability.

\begin{comment}
Liu et al.~\cite{2014_ICSE_Characterizing_and_detecting_performance_bugs} conducted an empirical analysis of performance bugs in large-scale Android applications, uncovering recurring problems such as GUI lag, memory bloat, and energy leaks, with GUI lag accounting for over 75\% of the identified bugs, significantly diminishing app responsiveness.
Following this, Su et al.~\cite{Su2021Benchmarking} evaluated the capabilities of automated GUI testing tools, revealing that many of these tools struggle to detect real-world performance issues like crashes and lag in mobile apps. 
Rua et al.~\cite{2023_empirical_study_on_mobile_performance} expanded the investigation by assessing the impact of semantic changes, including feature additions and bug fixes, on energy usage, runtime, and memory consumption across 1,322 versions of 215 Android apps. Their findings revealed discrepancies between performance indicators and the priority assigned by Android Lint. 
Das et al.~\cite{2020_EMSE_statically_detectable_performance_issues} explored the evolution of performance issues in 316 open-source Android apps, focusing on the frequency, resolution rates, and duration of these problems as detected by Android Lint. Their study showed that resource recycling issues are common and often unresolved, while algorithmic problems are typically addressed more swiftly. These studies collectively emphasize the need for more advanced tools and techniques capable of accurately detecting and addressing the diverse and evolving performance issues in mobile applications to enhance user experience and app reliability.
\end{comment}


%Detecting performance issues in mobile applications is crucial for ensuring a smooth user experience, particularly when such issues directly impact user satisfaction. Common challenges include identifying problems like janky frames, slow image rendering, and frozen screens, significantly affecting an app’s responsiveness. Various tools and methods have been developed to tackle these issues, ranging from dynamic monitoring tools to static analysis approaches. For example, Su et al.~\cite{Su2021Benchmarking} conducted a comprehensive evaluation of automated GUI testing tools, revealing that many existing tools struggle to detect real-world performance problems, such as crashes and lag, commonly encountered in mobile apps. Liu et al.~\cite{2014_ICSE_Characterizing_and_detecting_performance_bugs} conducted an empirical study of performance bugs in large-scale Android applications and identified several common patterns of performance problems, such as GUI lagging, memory bloat, and energy leaks. The study found that over 75\% of performance bugs were related to GUI lagging, significantly reducing app responsiveness. These findings highlight the need for more specialized tools and techniques that can detect such recurring issues in mobile apps. 
%Rua et al.~\cite{2023_empirical_study_on_mobile_performance} conducted a large-scale empirical study on mobile software performance, focusing on energy, runtime, and memory usage across 1,322 versions of 215 Android applications. Using black-box testing frameworks, they assessed the impact of semantic changes like feature additions and bug fixes on energy performance, revealing discrepancies between performance indicators and the priority assigned by Android Lint.

%Das et al.~\cite{2020_EMSE_statically_detectable_performance_issues}examined the evolution of performance issues detected by Android Lint in 316 open-source Android apps, focusing on issue frequency, resolution rates, and survival time. Their findings indicate that resource recycling issues are prevalent and often unresolved, while algorithmic issues are typically addressed swiftly.
\subsection{Detecting Mobile Performance Issues}
Many studies have used static analysis of source code to detect performance issues in mobile applications~\cite{2014_ICSE_Characterizing_and_detecting_performance_bugs, 2019_EMSE_iPerfDetector,2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues, 2020_EMSE_statically_detectable_performance_issues, 2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects, 2023_ASE_Detection_Thread_Misuses}. Specifically, some studies \cite{2014_ICSE_Characterizing_and_detecting_performance_bugs, 2019_EMSE_iPerfDetector, 2023_ASE_Detection_Thread_Misuses} focus on statically analyzing the source code to identify performance issues based on performance patterns, while others \cite{2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues, 2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects} target inefficient image displaying issues. Static analysis tools like Android Lint~\cite{Android_Lint}, FindBugs~\cite{FindBugs}, PMD~\cite{PMD} are also used for detecting performance issues.
However, the issues identified through static analysis may not accurately reflect the user's experiences of app unresponsiveness. Moreover, static analysis tools rely heavily on known performance patterns, reducing their effectiveness in real-world scenarios.

Other profiling tools, such as Android Debug Bridge (adb)~\cite{adb} and Perfetto~\cite{perfetto}, monitor  system metrics like CPU and memory usage to detect performance issues. While these metrics provide valuable insights into system performance, they do not always reflect the responsiveness of the user interface (UI)~\cite{2022_IST_resource_influences_UI_responsiveness}, making it difficult to identify user-perceived issues such as GUI lags. 
Researchers have also developed dynamic instrumentation tools like AppInsight~\cite{2012_OSDI_AppInsight}, PerfProbe~\cite{2019_MOBILESoft_PerfProbe}, AppSPIN~\cite{2022_EMSE_AppSPIN} to collect system performance metrics and program events (e.g., user inputs, network events, UI thread activity) to detect performance issue. They still struggle to capture the full scope of user-perceived responsiveness. In contrast, our approach analyzes mobile screencasts, which provide direct insight into how applications behave from the end user’s perspective, making them ideal for identifying performance issue, particularly those related to UI responsiveness.


\begin{comment}
Building on the insights gained from these empirical studies, several works have developed tools and techniques aimed at detecting and resolving these problems in mobile applications. Deka et al.~\cite{deka2017zipt} introduced ZIPT, a zero-integration performance testing platform for mobile app designs. This platform collects and analyzes user interaction data from third-party Android apps, leveraging crowd-sourcing to provide designers with metrics like task completion rates and usability insights. Notably, ZIPT operates without requiring access to the app’s code, making it an efficient solution for large-scale, comparative design evaluations. Similarly, Kim et al.~\cite{kim2009PerformanceTesting} proposed a unit-level performance testing approach using a benchmark database within an emulator-based environment. Their method proved effective for optimizing product quality while adhering to the constraints of mobile development.

In addition to dynamic testing methods, some studies focused on static analysis. Afjehei et al.~\cite{2019_EMSE_iPerfDetector} developed iPerfDetector, a tool designed to detect performance anti-patterns in iOS applications, including inefficient UI designs and memory management issues. Expanding on this static analysis approach, Cui et al.~\cite{2023_ASE_Detection_Thread_Misuses} introduced Leopard, which identifies thread misuses in Java applications that lead to performance degradation, such as resource leaks and unresponsive threads.

Transitioning to GUI-related performance issues, Song et al.~\cite{2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects} developed IMGDroid, targeting image loading defects that degrade app responsiveness—a common source of performance bottlenecks. Likewise, Li et al.~\cite{2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues} explored solutions for inefficient image rendering in Android apps, emphasizing how improper image loading contributes to overall performance lags. Following this focus on graphical elements, Zhao et al.~\cite{2020_ICSE_Seenomaly_vision_based_linting_of_GUI_animation_effects_against_guidelines} addressed performance degradation caused by GUI animations with their tool, Seenomaly, which employs vision-based techniques to ensure animations comply with design guidelines and do not overload system resources. Furthermore, Mirzaei et al.~\cite{Mirzaei2016ReducingCombinatorics} presented an approach to manage the combinatorial explosion in GUI testing by optimizing test case numbers while maintaining coverage, thereby improving the efficiency of performance testing in complex Android apps.

While these advancements have made detecting performance issues more effective, \tool presents a novel approach by analyzing screencasts of mobile applications during GUI testing. By focusing on GUI lags, \tool enhances the ability to identify and resolve critical performance bottlenecks that might otherwise go unnoticed, pushing the boundaries of mobile performance testing further.
\end{comment}




%Some studies employ static analysis of source code to identify performance issues in mobile applications.
%Other studies may use profiling, testing, GUI-based analysis (Hierarchy viewer of analysis of layers), logs to detect performance issues. 
%Deka et al.~\cite{deka2017zipt} introduced ZIPT, a zero-integration performance testing platform for mobile app designs, enabling designers to collect and analyze user interaction data across third-party Android apps. By leveraging crowdsourcing, ZIPT provides designers with performance metrics such as task completion rates and usability insights without requiring access to the app’s code, making it an efficient solution for comparative design evaluations at scale.

%Kim et al.~\cite{kim2009PerformanceTesting} proposed a method for performance testing of mobile applications at the unit test level, utilizing a database created through benchmark testing in an emulator-based environment. Their tool supports this approach, demonstrating the reliability and effectiveness of performance test results in optimizing product quality under the constraints of mobile development environments. 
%Afjehei et al.~\cite{2019_EMSE_iPerfDetector} introduced iPerfDetector, a static analysis tool designed to detect performance anti-patterns in iOS applications, such as inefficient UI design and memory management issues.
%Cui et al.~\cite{2023_ASE_Detection_Thread_Misuses} developed Leopard, a static analysis tool designed to detect thread misuses that can lead to performance degradation in Java applications, such as resource leaks and unresponsive threads.
%Different from prior studies, \tool focuses on analyzing user-perceived performance issues (i.e., GUI lags) by analyzing the screencasts. We also discuss how it is integrated and deployed in real-world settings. 
%Building on this, Song et al.~\cite{2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects} introduced IMGDroid, a tool aimed at detecting image loading defects that degrade app responsiveness, one of the most common sources of performance bottlenecks. Li et al.~\cite{2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues} proposed approaches for detecting inefficient image rendering issues in Android apps, shedding light on how improper image loading contributes to performance lags. Zhao et al.~\cite{2020_ICSE_Seenomaly_vision_based_linting_of_GUI_animation_effects_against_guidelines} addressed performance degradation caused by GUI animations with their tool, Seenomaly, which uses vision-based techniques to check animations against design guidelines, ensuring they do not overload the system unnecessarily.
%Mirzaei et al.~\cite{Mirzaei2016ReducingCombinatorics} proposed an approach to reduce the combinatorial explosion in GUI testing, optimizing test case numbers while maintaining coverage, thus improving the efficiency of performance testing in complex Android apps without overwhelming system resources.


%\subsection{GUI Performance Testing and Analysis}

\subsection{Computer Vision in Mobile Testing}
% \guo{try to merge D\&E}
As computer vision techniques evolve, they have been applied to enhance the efficiency and accuracy of mobile app testing. Some studies have employed these techniques to support testing by improving exploration efficiency~\cite{2023_ICSE_Efficiency_Matters_Speeding_Up_Automated_Testing_with_GUI_Rendering_Inference}, recording and replaying tests~\cite{2020_ICSE_translating_video_recordings_of_mobile_app_usages, 2023_UIST_Video2Action}, and enabling cross-platform testing~\cite{2024_TOSEM_PIRLTEST_GUI_Testing_via_Image_Embedding_and_RL, 2021_ICSE_Layout_and_Image_Recognition_Driving_Mobile_Testing}. In contrast, our approach focuses on analyzing the results (i.e., mobile screencasts) of GUI testing to detect performance issues, thus expanding the mobile testing.

Other studies have used computer vision techniques to analyze the mobile screenshot or screencast for detecting visual issues in mobile application GUIs. For example, some studies~\cite{2018_ICSE_Automated_reporting_of_GUI_design_violations, 2020_ICSE_Seenomaly_vision_based_linting_of_GUI_animation_effects_against_guidelines, 2021_ICSE_Hunting_Down_Visual_Design_Smells_in_Complex_UIs, 2024_ICSE_MotorEase} focus on identifying UI design issues by checking for compliance with design guidelines or intended designs. Additionally, other research addresses GUI-related bugs, such as UI display issues~\cite{2021_ASE_spotting_UI_display_issues}, data inconsistency bugs~\cite{2024_ICSE_Data_Inconsistency_Detection_of_Mobile_Apps}, and text glitches in game app GUIs~\cite{2023_FSE_Automated_Game_GUI_Text_Glitch_Detection}. In contrast, our work specifically applies computer vision techniques to analyze mobile screencasts to detect user-perceived GUI lags that affect user experiences.  

\begin{comment}
\subsection{Computer Vision in GUI Testing}
The rise of computer vision technologies has significantly impacted the field of mobile testing, enabling more automated and accurate detection of visual defects and performance issues that are difficult to identify. Computer vision can support testing by improving exploration efficiency~\cite{2023_ICSE_Efficiency_Matters_Speeding_Up_Automated_Testing_with_GUI_Rendering_Inference}, recording and replaying tests~\cite{2020_ICSE_translating_video_recordings_of_mobile_app_usages, 2023_UIST_Video2Action}, and enabling cross-platform testing~\cite{2024_TOSEM_PIRLTEST_GUI_Testing_via_Image_Embedding_and_RL, 2021_ICSE_Layout_and_Image_Recognition_Driving_Mobile_Testing}. This improves the overall efficiency and accuracy of mobile app testing.
One notable advancement is the work by Feng et al.~\cite{2023_ICSE_Efficiency_Matters_Speeding_Up_Automated_Testing_with_GUI_Rendering_Inference}, who used Convolutional Neural Networks (CNNs) to infer when the GUI is fully or partially rendered, reducing delays between test actions in automated testing environments. Their approach allows automated testing systems to wait until the interface is fully loaded, thus enhancing the reliability of test results.
Yu et al.~\cite{2024_TOSEM_PIRLTEST_GUI_Testing_via_Image_Embedding_and_RL} use a VGG model~\cite{2015_VGG} to extract widgets from GUI screenshots and embed them as states representing the GUI page. This approach makes the testing process applicable across different platforms like Android and Web by abstracting the GUI into a platform-independent state representation. 
Different from these approaches, our work analyzes screencasts to detect use-perceived GUI lags. 
\end{comment}


\begin{comment}
\subsection{Performance Regression Testing.} %among different Android app releases
\citet{2016_MSR_Mining_test_repositories_for_automatic_detection_of_UI_performance_regressions} proposed an approach called DUNE to automatically detect UI performance degradation, including regressions and optimizations, across different Android application releases. Specifically, the approach uses the ratio of janky frames during GUI tests on Android applications to represent their UI performance metrics. 


\subsection{Inefficient Image Display} 
\citet{2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues} conducted an empirical study on 162 inefficient image displaying issues in Android apps, identifying four types of anti-patterns responsible for these issues. Based on these findings, they developed a tool called TAPIR, which statically analyzes source code to detect image displaying issues in Android apps.
\citet{2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects}
\end{comment}



\begin{comment}
\subsection{Detecting GUI Visual Issues}
Prior research has utilized computer vision techniques to detect visual issues in mobile application GUIs. Some studies identify UI smells by checking compliance with design guidelines. 
For example,  \citet{2020_ICSE_Seenomaly_vision_based_linting_of_GUI_animation_effects_against_guidelines} detect GUI animations, while ~\citet{2021_ICSE_Hunting_Down_Visual_Design_Smells_in_Complex_UIs} identify UI design smells by evaluating them against design anti-patterns. Additionally, \citet{2024_ICSE_MotorEase} examine accessibility issues (e.g., small visual touch targets) in mobile app UIs for users with motor impairments by detecting violations of relevant design guidelines. Other research~\cite{2018_ICSE_Automated_reporting_of_GUI_design_violations} verifies whether the implementation of GUI aligns with the intended design in mock-up images to identify design violations.
Studies also address the GUI-related bugs: \citet{2021_ASE_spotting_UI_display_issues} detect UI display issues like text overlap and missing images, \citet{2024_ICSE_Data_Inconsistency_Detection_of_Mobile_Apps} detect data inconsistency bugs, 
and \citet{2023_FSE_Automated_Game_GUI_Text_Glitch_Detection} targets game app GUI to detect text glitches (e.g., text overstep). In contact, in this paper, we discuss our experience working with Company A on analyzing screencasts to detect GUI lags. 
\end{comment}