\section{Approach}
\label{sec:approach}

\begin{figure*}
	\centering
    \includegraphics[width=0.95\linewidth]{figures/overview.pdf}
	\caption{The overall architecture of \tool.}
	\label{fig_overview_approach}
\end{figure*}

Manually analyzing screencasts to identify GUI lags is a time-consuming and labor-intensive process that becomes nearly impossible as the complexity and number of mobile apps grow. With apps typically generating thousands of frames in a one-minute interaction, manually reviewing these frames to detect performance issues, such as janky, long loading, and frozen frames, is inefficient and error-prone. 
%Furthermore, the subtle nature of some GUI lags, which may not be immediately noticeable to the human eye, makes it challenging to consistently detect and prioritize critical performance problems.

To address these challenges, we collaborated with our industry partner, Company A, to propose an automated approach for analyzing screencasts to detect GUI lags efficiently and accurately. Figure~\ref{fig_overview_approach} presents the overall architecture of the framework based on our approach, \tool.
It comprises two main components: \textit{GUI Testing \& Video Capture} and \textit{Detection of GUI lags}. %The proposed solution leverages computer vision techniques and threshold-based analysis to identify and classify performance issues across thousands of frames without manual intervention. Our approach automatically flags critical issues such as janky frames, excessive load times, and frozen frames, enabling the detection of even subtle performance degradations that might be missed during manual reviews.
\tool has been fully integrated into Company A's development workflow as part of the continuous testing and quality assurance processes. The CI pipeline triggers \tool every time the automated tests finish executing to generate a GUI lag detection report. To date, \tool has successfully detected and helped resolve many performance issues. 

Below, we discuss \tool in detail. 



\subsection{GUI Testing \& Video Capture}
As illustrated in Figure~\ref{fig_overview_approach}, our approach utilizes GUI screencasts (i.e., a sequence of frames) as input for detecting GUI lags. The process begins with capturing these GUI screencasts from real mobile applications during GUI testing. During test execution, the tests automatically trigger the screen recording process. We record the screencasts at full Frames Per Second (FPS) to comprehensively analyze every screen change in response to user operations (e.g., tapping). To minimize the performance overhead during the recording process, we use specialized external hardware that captures and saves each frame as the mobile device renders it to the screen. Using such hardware minimizes the performance impact of screen recording and ensures that the recorded screencasts accurately reflect what human eyes would see. Once recorded, the screencasts are stored as video files for subsequent analysis. 

\subsection{Detection of GUI Lags}
\subsubsection{Lag Detector}
This phase aims to automatically detect GUI lags from screencasts. The output includes detailed information about the detected GUI lags, such as the screencast name, the type of GUI lags, the start and end frame indices of the lag within the screencast, and the duration of the lag-inducing frames. 

%Each screencast is recorded before and after one user operation applied on the mobile app.
%As shown in Figure~\ref{fig_overview_approach}, GUI performance lags occur during frame rendering. Therefore, we first identify the rendering segments in the GUI screencast and then locate the performance lags within these segments. The position of a performance lag is identified as a range of frames, represented as $[f_{start}, f_{end}]$, where $f_{start}$ and $f_{end}$ are the indexes of start frame and end frame within the screencasts.


%\feng{it's not consistent with fig2b. in 2b this example only have f1 to f6, but this one has f1 to f7. try to change fig2b index number, as f1 f2 ... f5 f6 f7}\wei{I think it is OK since they are in different Section.}
\begin{figure}
	\centering
    \includegraphics[width=0.98\linewidth]{figures/lag_detection.pdf}
	\caption{An illustration of GUI lag detection.}
	\label{fig_lag_detection}
 \vspace{-3mm}
\end{figure}

As illustrated in Figure~\ref{fig_lag_detection}, we first extract the static frames (i.e., frames that end users perceive as unchanged for some time) from the given screencast, which may or may not potentially be GUI lags. This approach helps focus the subsequent analysis on specific time frames of the screencast to speed up analysis. 
Inspired by prior studies~\cite{2023_UIST_Video2Action, 2023_ICSE_Efficiency_Matters_Speeding_Up_Automated_Testing_with_GUI_Rendering_Inference}, we employ the Structural Similarity Index Measure (SSIM)~\cite{2004_SSIM_Transactions_on_Image_Processing}, a highly effective image similarity technique that reflects human perception, to segment the screencasts and identify static frames. For every frame in the screencast, we calculate the similarity score relative to the previous frame. If the similarity score exceeds a predefined threshold, it indicates no human perceivable change compared to the prior frame. 
As an example, in Figure~\ref{fig_lag_detection}, frame $f_{3}$ to $f_{6}$ have a similarity score that exceeds the predefined threshold, indicating there are static frames from $f_{2}$ to $f_{6}$ (frame $f_{2}$ is included because it is the first frame that shows no subsequent change).
%This threshold helps us identify segments of frames from the screencast that remain stable for a certain duration.

After extracting these static frames, we apply rule-based performance bug detection approaches to identify the three types of GUI lags discussed in Section~\ref{sec:background}. 

\phead{Janky Frames.} Jank occurs when frames are skipped or delayed, causing the screen to display the content of a previous frame. This makes users perceive the screen as momentarily stuck, as the display fails to update as expected. %Following a prior study~\cite{2024_ASPLOS_More_Apps_Faster_Hot_Launch_on_Mobile_Devices}, 
We detect jank by identifying frames where the interval between two rendered frames exceeds 16.7 milliseconds, corresponding to a standard frame rate of 60 frames per second (FPS). To implement this detection, we traverse all static frames to retrieve their presentation times and calculate the interval between consecutive frames by subtracting their presentation times. If any interval exceeds 16.7 milliseconds, a jank is detected. Additionally, if any interval exceeds 100 milliseconds, indicating the skipping of six frames (as discussed in Section~\ref{sec:background}), we classify it as a GUI lag caused by janky frames. The indices of the consecutive frames are marked as the start and end frames of the janky frames, annotated as an interval $[f_{start}, f_{end}]$. 


\phead{Long Loading Frames.} Long loading frames indicate resource loading, which can significantly disrupt the user experience by causing noticeable delays. Typically, a placeholder in the frame represents content that has yet to be loaded, serving a visual cue to users that loading is in progress. Figure~\ref{fig_placeholder} presents examples of frames containing placeholders. To identify these loading frames, we use a fine-tuned object detection model to detect placeholders within the frames. We first build a dataset of images by collecting numerous mobile screenshots (i.e., images) and manually labeling any placeholders. Then, we fine-tune the pre-trained YOLO~\cite{yolov} model using this dataset. %The pre-trained model is trained using the specified dataset and hyperparameters. 
YOLO is a computer vision-based object detection model pre-trained with images from various sources, such as ImageNet~\cite{ImageNet}. However, its training data does not involve any context information about mobile app screens. The fine-tuning process optimizes the model's parameters with our mobile app data, enabling it to accurately detect the existence and location of placeholders in images. We evaluate the fine-tuned YOLO model and find that it achieves high precision and recall in detecting the placeholders. 

\begin{figure}
	\centering
    \includegraphics[width=0.98\linewidth]{figures/placeholder.pdf}
	\caption{Examples of frames containing placeholders.}
	\label{fig_placeholder}
 \vspace{-3mm}
\end{figure}


Specifically, we focus on the first frame of the static frames (i.e., consecutive frames with SSIM scores larger than a threshold). If a placeholder is detected by the fine-tuned YOLO model on the first frame and the duration between the first frame and the last frame exceeds one second (calculated by subtracting their respective presentation times), we classify this scenario as a GUI lag caused by long loading frames. The indices of the first and last frame in the static frames are marked as the start and end frames of the long loading frames, denoted as the interval $[f_{start}, f_{end}]$. 


\phead{Frozen Frames.} Frozen frames occur when the content displayed on the screen becomes unresponsive, resulting in a stop where no changes happen on the screen. Unlike long loading frames, frozen frames do not contain explicit placeholders indicating resource loading. In contrast to janky frames, which happen when the frame rate drops inconsistently due to app overload, frozen frames represent a loss of responsiveness caused by a hang that temporarily locks up the app. We want to categorize the detected GUI lags into the corresponding type for better bug triage. Hence, to identify frozen frames, we employ the same fine-tuned YOLO~\cite{yolov} model to check for the presence of placeholders in the first frame of the static frames. If the placeholder is absent and the duration between the first frame and last frame exceeds 100 milliseconds (calculated by subtracting their respective presentation times), we classify these static frames as frozen frames. The indices of the first and last frame in the static frames are marked as the start and end frames of the frozen frames, denoted as the interval $[f_{start}, f_{end}]$. 


\subsubsection{Prioritizing Severe GUI Lags}
Our approach can detect numerous instances of GUI lags. However, one issue is that some lags may be short (e.g., at the boundary of the 100ms threshold), while some may be long and significantly impact end users’ experiences. Moreover, the severe lags are more likely to indicate underlying performance issues.  
For instance, consider a scenario where a user clicks a button in a mobile application, but the screen transition is delayed by several seconds. Such a delay could signal inefficiencies in the rendering process or problems within the app’s underlying logic, both of which are key performance-related issues affecting the GUI. We categorize these prolonged lags as ``severe GUI lag'' as they significantly degrade the user experience and might indicate deeper performance bottlenecks that require immediate attention.




%For example, consider a scenario where a button is clicked in a mobile application in a delayed screen transition, taking several seconds to respond. This delay could indicate inefficiencies in the rendering process or issues with the app's underlying logic, both of which are considered performance-related GUI problems. Therefore, we refer to these instances as ``severe GUI lag''.


%In this step, we analyze the detected lags to prioritize severe GUI lags. 
We employ a threshold-based approach to prioritize severe GUI lags because it provides a simple yet effective method for distinguishing between minor and major delays. 
The lag duration is the key feature influencing user perception of GUI performance. Therefore, we establish duration thresholds to classify any GUI lag exceeding these limits as severe. 
After consulting with user experience experts at Company A, we set three duration thresholds: $t_{janky}$,  $t_{load}$, and $t_{frozen}$, which correspond to janky frames, long loading frames, and frozen frames, respectively.
Any detected GUI lag exceeding these thresholds is considered severe lag by \tool and will undergo in-depth investigation. Although we cannot share the exact values of these duration thresholds due to the Non-Disclosure Agreement (NDA), practitioners willing to implement and deploy \tool can set their thresholds according to their specific requirements. 
%The details of determining the threshold is discussed in next section.

\begin{comment}
\subsection{Determining threshold}
To determine the threshold, we analyze a dataset of 3,854 GUI unlabeled screencasts from mobile testing. The \textit{Threshold Determination} phase is only performed once, prior to the deployment of \tool. Specifically, we analyze the unlabeled screencasts to detect GUI lags and use statistical analysis to calculate the distribution of their duration. Based on this distribution, we set the threshold for severe lags at the 80th percentile, resulting in thresholds of 200ms, 300ms, and 1500ms for the three types of GUI lags, respectively. Any detected GUI lag exceeding these thresholds is considered as severe. \wei{@Peter This method for determining the threshold may look fancy, but it raises logical concerns. Why should the 80th percentile of duration be the best choice for identifying severe GUI lags? }
\end{comment}

\subsection{Efficiency of \tool and Recording Overhead}
\label{sec:implementation}

To minimize the overhead associated with screen recording on the mobile devices under test, we utilize external hardware for recording in controlled settings.
Due to the NDA, we cannot disclose the implementation details of the recording process. 
Nevertheless, to measure the recording overhead, we monitor performance metrics (e.g., response time, CPU utilization, memory consumption, and frame rates) to compare the performance before and after integrating the hardware for screen recording. After the integration, we observed no degradation in performance, as the performance metrics remained nearly the same (at most 10ms or so difference in response time). This indicates that the overhead is technically not human-observable, and analyzing the screencasts can accurately reflect users' perceptions of performance and GUI lags.



%The hardware retrieves a live video stream from the mobile phone via an HDMI connection and outputs it to a client machine through a USB interface. 
%The client captures the video stream and save it as a screencast. 
%Since the hardware is responsible for capturing the video signal, and the client handles the video processing and decoding, the mobile device itself remains unaffected by the recording process. 
%We monitor performance metrics---CPU utilization and memory consumption, as employed in prior study~\cite{2017_INFOCOM_UI_rendering_analysis_for_mobile_applications}---to compare mobile performance before and after integrating the hardware for screen recording. After the integration, we observed no degradation in mobile performance, as both CPU utilization and memory consumption remained stable. This indicates that the mobile device operates normally in real-world usage without additional recording overhead that could affect performance measurements.

%Although we cannot share detailed information about the hardware and client due to the NDA, the processing times for detecting GUI lags demonstrate the high efficiency of our approach.

%\begin{figure}
%	\centering
%    \includegraphics[width=0.6\linewidth]{figures/deploy.pdf}
%	\caption{An illustration of potential deployment of \tool.}
%	\label{fig_overview_deployment}
%\end{figure}


%Our approach efficiently processes screencasts, enabling quick analysis. The recorded screencast can be analyzed locally by the client or transferred to a server for GUI lag detection. On the client, the average processing time for a frame (i.e., an image) in the screencast is X ms, while on the server, it is Y ms. For a screencast representing a test case, the processing times are X ms on the client and Y ms on the server, respectively. 

% All the experiments are conducted on a Ubuntu X server with Intel X CPU, X GB RAM, 1 NVIDIA 4090 24GB GPU.