\section{Evaluation}
\label{sec:evaluation}

\subsection{Experimental setup}
\label{sec:setup}
\subsubsection{Collecting Dataset from Real-World Testing Results} 

To effectively detect and analyze GUI performance issues, we have deployed our tool, \tool, in a real-world production environment within the Continuous Integration (CI) pipeline of Company A. This integration enables automated data collection from CI test runs, which are executed regularly as part of the company’s quality assurance process. During each test run, \tool captures screencasts of the mobile application under various usage scenarios, simulating real user interactions such as tapping, scrolling, and navigation.

For each CI test run, the tool records detailed screencasts at the native frame rate of the device, typically 60 frames per second, allowing for precise detection of even subtle lags. The recorded data includes timestamps for each frame, system resource usage (e.g., CPU and memory consumption), and any detected lag instances classified into three categories: janky frames, long loading frames, and frozen frames. By capturing this comprehensive dataset, we can analyze the occurrence, duration, and frequency of different types of GUI lags in a variety of testing scenarios.

%Data is continuously collected and stored in a centralized repository for analysis, providing a comprehensive view of the application’s performance over time. This setup allows for the early identification of performance regressions, as the tool automatically flags any detected GUI lags that exceed predefined thresholds. The automated collection and analysis process significantly reduces the manual effort required for performance testing, enabling developers to respond quickly to potential issues before they reach end-users. By leveraging data from ongoing CI tests, \tool not only improves the efficiency of the testing process but also enhances the reliability of the application, ensuring that each new build meets performance standards before deployment.

%\peter{checkout 3.1 here: \url{https://petertsehsun.github.io/papers/MLASP_EMSE2021.pdf#page=7.11}}To the best of our knowledge, there are few related public datasets of mobile GUI screencast~\cite{ERICA_2016_UIST, RICO_2017_UIST}. However, these datasets primarily contain animations capturing transition effects (i.e., screenshots) rather than actual screen recordings in response to user interactions. The captured animations are stored as GIFs with low FPS and resolution. Consequently, many features of the screencast are lost, which may affect the performance of our approach based on computer vision.
%====To comprehensively evaluate our approach, we collected a dataset of GUI screencasts from a diverse selection of top-ranked Android applications, chosen based on download popularity. These apps cover a wide range of categories, including shopping, social networking, travel, tools, and more. We ran these applications on a real mobile device, simulating user operations such as tapping to mimic actual usage of the app's key features. For each test case, which represents a user interaction on the app, we recorded a screencast starting 10 seconds before the user interaction and lasting approximately 30 seconds. Thus, one screencast represents a distinct test scenario. 
%Table~\ref{tab:dataset} presents the statistics of the dataset, which contains 929 screencasts collected across the 50 most popular apps.

% \begin{table}
% 	\centering
% 	\caption{Statistics of the dataset\peter{to be removed}.}
% 	\label{tab:dataset}
%     % \setlength{\tabcolsep}{5pt}
% 	\scalebox{0.9}{
% 	\begin{tabular}{cc r r r  r r r} 
% 		\toprule
% 		\multirow{2}*{\#App} & 
% 		\multirow{2}*{\#Screencasts} & 
% 		\multicolumn{3}{c}{\#GUI Lags}&
%         \multicolumn{3}{c}{\#Lag Alerts} \\
		
% 		\cmidrule(r){3-5} \cmidrule(r){6-8}
% 		& &Janky &Load &Frozen &Janky &Load &Frozen \\
		
% 		\midrule
% 		50 &920   &56  & y  & & 5  & 20  & 19\\
		
% 		\bottomrule
% 	\end{tabular}}
% \end{table}

\subsubsection{Labeling Ground Truth Dataset} %We manually label the detected GUI lags to establish the ground truth. 
To create the ground truth dataset to evaluate \tool, we involved five user experience experts from Company A. These experts were selected based on their extensive experience in mobile application testing. 
Each expert independently watched the screencast playback and reviewed the corresponding images to identify lags. They assessed the screencast to determine if the lag was noticeable enough to affect the user experience, marking the exact start and end frames for each instance where a GUI lag was observed. 
Any GUI lag instance identified by at least one expert was classified as an issue. This approach allowed us to capture even subtle lags that may not affect all users equally but could still detract from the overall user experience. For each detected lag, the experts marked the relevant frames and provided details on the perceived severity based on both the type of lag and its duration.
%The labeled data from this process formed the ground truth used to validate \tool's automated detection algorithms, ensuring that the system could accurately identify and classify GUI lags during subsequent CI test runs. 


%A performance lag was marked if at least one expert felt the screencast gave the impression of lagging from the user's perspective. We used the threshold, rather than requiring consensus from all experts because even a single perceived lag can impact a portion of the potential end-user base. The experts also check the sequence of PNG images corresponding to the screencast to determine the start and end frame indices (donated as $[f_{start}, f_{end}]$) for each performance lag. If the experts determine two different locations for a single performance lag, they discuss until reaching a consensus. Ultimately, we identify hundreds of instances of the three types of GUI lags: janky frames, load frames, and frozen frames. We observe that a single screencast may contain multiple lags.
%The author also annotate the performance lag as $[f_{start}, f_{end}]$, where $f_{start}$ and $f_{end}$ represent the start and end frame indices in the screencast.

%For each labeled GUI lag, we also record the number of authors (ranging from 1 to 5) who agree it is an lag to indicate its severity. The more authors that agree it is an lag, the more severe the lag is considered, suggesting that more end-users are likely to perceive it as lagging. Consequently, we use GUI lags with the highest severity (i.e., severity 5) to represent severe lag. This approach is reasonable, as these lags are likely to impact a large number of end users and have been reported to the company's developers. Finally, we collect a,b, and c lag alerts for the three types of GUI lags: janky frames, load frames, and frozen frames, respectively.

% \subsection{Evaluation metrics}
