\section{Introduction}
\label{sec:introduction}

Non-functional performance characteristics, particularly the responsiveness and smoothness of the Graphical User Interface (GUI), are critical to user satisfaction with mobile applications~\cite{2015_IEEE_Software_Mobile_App_Users_Complain, 2018_WWW_Rating_for_App_Reviews, 2022_JSS_What_factors_affect_the_UX_in_mobile_apps}. As mobile devices have become more powerful and widely used, user expectations for seamless performance have risen accordingly. 
Performance issues like GUI lags can significantly and directly impact users' perceptions of an app's quality. Frequent or severe lags may cause users to abandon the app and leave extremely negative reviews, ultimately damaging the app's ratings and overall reputation. 

%When end-users experience lag in a mobile app, they are likely to give it a low rating or even abandon it entirely, directly impacting the profit of the app or system's company. 


\begin{comment}
For example, a user left a 1-star review on Google Play, citing performance issues with the WordPress mobile app:
\peter{please, don't add this quote.}\textit{``But now it's \textbf{lagging} and getting \textbf{frozen} at every stage whenever I try to update and compose my story. It's becoming so slow that I'm unable to type a story and it hangs or freezes so much that even if I press backspace or try to exit it doesn't process well. All other sites and platforms are working except this. I'm unable to type and frame my story at all.''}~\cite{WordPress} 
\end{comment}


Despite this growing emphasis on performance, conventional GUI testing tools often fall short in detecting issues that align with users' perceptions of lag. While some studies employ static analysis of source code to identify performance issues in mobile applications~\cite{2014_ICSE_Characterizing_and_detecting_performance_bugs, 2019_EMSE_iPerfDetector,2019_SANER_Characterizing_and_Detecting_Inefficient_Image_Displaying_Issues, 2020_EMSE_statically_detectable_performance_issues, 2021_ICSE_IMGDroid_Detecting_Image_Loading_Defects, 2023_ASE_Detection_Thread_Misuses}, the issues identified by these methods may not accurately reflect users' experiences of unresponsiveness. Additionally, static analysis tools depend heavily on known performance patterns, limiting their applicability in real-world scenarios. Other traditional approaches commonly focus on low-level metrics like CPU and memory usage, which do not necessarily correlate with perceived performance issues~\cite{2022_IST_resource_influences_UI_responsiveness, adb, perfetto}. %For example, an application may exhibit high CPU utilization without any noticeable impact on the user interface, or conversely, experience noticeable User Interface (UI) delays with minimal changes in system resource metrics. 

%Moreover, manually testing GUI screencasts to identify lags is labor-intensive, time-consuming, and prone to human error, making it impractical for large-scale applications.


Screencasts are recordings of a mobile device’s screen during app usage or GUI testing. Analyzing screencasts offers a direct way to identify potential GUI performance issues, such as lag, from the user's perspective. These screencasts capture the exact sequence of individual frames rendered by the mobile system, with each frame representing a static UI image at a specific time point marked by a timestamp. This creates a rich data source for performance analysis. 
%However, manually reviewing screencasts is impractical.
%Despite the widespread adoption of GUI testing to ensure mobile app quality, few approaches involve analyzing screencasts---the most direct way to identify potential GUI performance issues such as lag. 
However, manually reviewing screencasts to determine whether the app is slow or experiencing lagging is both time-consuming and labor-intensive. For a one-minute screencast recorded at 60 Hz, reviewers must examine 3,600 frames individually to pinpoint the issues for further analysis. This process becomes even more infeasible in a Continuous Integration (CI) setting, where frequent code updates and automated testing produce a large volume of screencasts.  
%Additionally, multiple reviewers are often required to reach a consensus on performance issues~\cite{Fair_Assessments,Guide_to_GUI_Testing}\peter{cite}\wei{@Peter The citation just weakly supports the statement about requiring multiple reviewers. Later, I learned that in issue tracking systems, developers typically assign a single person to handle bug reports. This practice doesn't fully align with the idea of needing multiple reviewers for consensus. Therefore, I’m considering removing the statement.  https://stackoverflow.com/questions/7340865/does-it-ever-make-sense-to-have-multiple-assignees-for-an-issue-in-an-issue-trac}, making manual reviews of screencasts unreliable or efficient for identifying GUI performance issues.
%Additionally, it often requires multiple person to review the screencast to reach a consensus on the performance issues. Hence, manual review of the screencasts is neither reliable nor efficient to identify GUI performance issues.


To address these challenges, we collaborated with Company A to develop \tool, a framework that uses computer vision techniques to automatically detect GUI lags from mobile application screencasts---lags that users perceive as performance delays or a lack of smoothness during app usage.
These screencasts provide direct insight into how applications behave from the end user’s perspective, making them ideal for identifying performance issues. While minor lags might be acceptable from a company’s standpoint, severe lags can significantly harm user experience. \tool further identifies these critical GUI lags, enabling companies to address high-impact issues before they affect a broader user base.


\tool is a framework composed of two key components: (1) \textit{GUI Testing \& Video Capture} and (2) \textit{Detection of GUI Performance lags}. In the first component, we conduct GUI tests on mobile applications to simulate user interactions, such as tapping. During these tests, we record screencasts that capture the mobile device's screen. To minimize additional performance overhead during testing and recording, we use external hardware to record the screencast efficiently. 
In the second component, we implement a rule-based approach that combines image similarity and object detection techniques to identify three types of GUI lags: (1) \textit{Janky Frames}---dropped frames that disrupt the smoothness of the UI, (2) \textit{Long Loading Frames}---frames that indicate long resource loading, causing bad user experience, and (3) \textit{Frozen Frames}---frames where the UI appears unresponsive, giving the impression of a freeze in user interaction. 

We begin by applying the image similarity techniques of Structural Similarity Index Measure (SSIM)~\cite{2004_SSIM_Transactions_on_Image_Processing} to the screencast to extract static frames (i.e., frames that do not change) to narrow the analysis scope and improve efficiency. Since users perceive these static frames as unchanged for an extended period, they may create a sense of lag and indicate potential performance issues.
Next, we classify the static frames into potentially problematic frames that may signify GUI lags. To assess these frames, we calculate their duration by measuring the intervals between their presentation times. If any interval exceeds 100 milliseconds (set based on HCI studies~\cite{1968_AFIPS_Response_time_in_man_computer, 1994_Usability_Engineering, zippy_Android_apps, 2015_MOBILESoft_performance_parameters}), we classify it as a GUI lag caused by janky frames. We then distinguish long loading frames from frozen frames based on their content. Long loading frames typically contain a placeholder representing content that has yet to be loaded, serving a visual cue to users that loading is in progress. 

To differentiate between these frame types, we fine-tune a pre-trained YOLO~\cite{yolov} object detection model using a dataset of images we collected and manually labeled for any placeholders. Using the fine-tuned YOLO model, we identify placeholders within the frames. If placeholders are detected and the total duration exceeds one second, we classify it as a GUI lag caused by long loading frames. Conversely, if no placeholders are detected and the total duration exceeds 100 milliseconds, we classify it as a GUI lag caused by frozen frames. 
By combining these techniques with predefined rules based on thresholds from prior HCI studies~\cite{1968_AFIPS_Response_time_in_man_computer, 1994_Usability_Engineering, zippy_Android_apps, 2015_MOBILESoft_performance_parameters}, \tool effectively detects the three types of GUI lags. Furthermore, \tool distinguishes severe GUI lags based on the duration of these problematic frames, highlighting those that significantly degrade the user experience.
%These static frames are then evaluated against the predefined rules such as thresholds established by prior studies. If the duration of these frames exceed the predefined limits and meets certain criteria---such as long loading frames containing a placeholder indicating resource loading---a GUI lag is detected.

To evaluate the effectiveness of \tool, we conducted experiments using a dataset of real-world testing results, specifically mobile screencasts collected from the Continuous Integration (CI) test runs at company A. 
The evaluation results show that \tool achieves an average precision of 0.91 and recall of 0.96 in detecting GUI lags. 
%Our evaluation results demonstrate that \tool achieves high precision and recall in detecting GUI lags. 
Furthermore, we deployed \tool in a real-world production environment, which is part of the CI to analyze screencasts to monitor app performance. \tool has successfully identified many severe GUI lags. Positive feedback confirms that the comprehensive bug reports generated from these lags provide developers with actionable insights for debugging and locating performance issues. The reports also help them prioritize fixing critical issues that have a larger impact on user experiences. % and address them efficiently.

The main contributions of this paper are as follows: 
\begin{itemize}
	\item We present \tool, a GUI performance testing framework that effectively detects user-centric GUI lags by analyzing mobile screencasts. % and filters out the most severe ones that impact users experiences.
 \item  \tool analyzes various factors, such as user interactions (e.g., lags during scrolling can be more impactful to human perception), duration and frequency of lags, and visual context (e.g., GUI elements experience lags), to help developers prioritize performance issues that directly impact user experience.

	\item We evaluate \tool using real-world testing results, achieving an average precision of 0.91 and recall of 0.96 in detecting GUI lags.

    %\item We deployed \tool in a real-world production environment, where the comprehensive bug reports generated from detected lags help developers focus on critical issues and debug them efficiently.

	\item We deployed and integrated \tool into the CI pipeline at company A to continuously monitor the GUI performance of mobile applications. We also discussed the positive feedback we received. 

\end{itemize}

In short, we hope our successful industry collaboration can inspire practitioners and researchers working on this direction. Future studies may consider adopting more user-centered approaches in their performance testing processes. 

\phead{Paper organization.} 
Section~\ref{sec:background} provides background on mobile screencasts and outlines three types of GUI lags. Section~\ref{sec:approach} details our approach.
Section~\ref{sec:evaluation} evaluates our approach using real-world testing results, while Section~\ref{sec:discussion} discusses its production impact. Section~\ref{sec:related} discusses related work and Section~\ref{sec:conclusion} concludes the paper.
