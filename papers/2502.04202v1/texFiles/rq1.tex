\subsection{Accuracy of \tool in Detecting GUI Lags}

%\subsection{RQ: How effectively can \tool detect GUI performance lags?} \feng{the structure of this part looks strange...}
%\subsubsection{Motivation}
%Given the variety design of mobile apps and the different types of contents on app pages, detecting GUI performance lags for the app screencast can be challenging. In this research question (RQ), we evaluate how effectively \tool can detect performance lags in screencasts.


%\subsubsection{Approach}
We use the dataset collected and labeled by the user experience experts as the ground truth for evaluation. 
In the context of a screencast, an instance GUI lag is considered correctly located by \tool if and only if it matches a lag (annotated as $[f_{start}, f_{end}]$) in the ground truth, with identical start and end frame indices (i.e, the same $f_{start}$ and $f_{end}$ values). 
To evaluate the effectiveness of \tool in detecting GUI lags, We employ precision, recall, and F1-score:

\begin{itemize}
    \item \textbf{Precision} is the fraction of correctly identified GUI lags out of all instances that \tool identified as GUI lags. It is defined as: 
\end{itemize}
\begin{equation}
Precision = \frac{\#\textit{Correctly identified GUI lags}}{\#\textit{Identified GUI lags}}
\end{equation}

\begin{itemize}
    \item \textbf{Recall} is the fraction of correctly identified GUI lags out of the total number of actual GUI lags. It is defined as: 
\end{itemize}
\begin{equation}
Recall = \frac{\#\textit{Correctly Identified GUI lags}}{\#\textit{Actual GUI lags}}
\end{equation}

\begin{itemize}
    \item The \textbf{F1-score} is the harmonic mean of precision and recall:
\end{itemize}
\begin{equation}
F_{1} = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\end{equation}


%\subsubsection{Results}
Table~\ref{tab:resutls_detecting_lags} presents the results of detecting GUI performance lags using \tool, demonstrating its high level of accuracy in detecting these lags. The precision for detecting the three types of GUI lags ranges from 0.91 to 0.92, while recall ranges from 0.95 to 0.98, and the F1-score ranges from 0.93 to 0.95. Overall, \tool achieves high average precision of 0.91 and recall of 0.96 across all lag types. 

\begin{table}
	\centering
	\caption{Results of \tool in detecting performance GUI lags across different lag types.}
	\label{tab:resutls_detecting_lags}
	\scalebox{1}{
	\begin{tabular}{lrrr}
		\toprule
		Lag type&\tabincell{r}{Precision}&\tabincell{r}{Recall}&\tabincell{r}{F1-Score}\\
		\midrule
			Janky frames          &0.92	        &0.98       &0.95\\
   	  Long Loading frames   &0.91 		  &0.96		  &0.93\\
			Frozen frames         &0.91		    &0.95	    &0.93\\
		\midrule
		\tabincell{l}{Average Across Types} 	&0.91		&0.96 		&0.94\\
		\bottomrule
	\end{tabular}}
        \vspace{-5mm}
\end{table}


Even though \tool achieves high precision and recall, there are still some missed or incorrectly detected cases. Hence, we conduct further analysis on such cases. We found that certain factors contribute to these inaccuracies. Primarily, our approach uses a threshold of 100 milliseconds based on prior HCI studies~\cite{1968_AFIPS_Response_time_in_man_computer, 1994_Usability_Engineering} to determine the shortest duration that users can typically perceive a lag caused by janky frames. While this threshold works well in most scenarios, it is still possible to lead to missed detection or false positives. 
For example, janky frames with a duration under 100 milliseconds may still be perceptible when they occur in scenarios with frequent or complex screen changes, such as during video playback or fast-paced animations. In these cases, even slight irregularities become noticeable to users, but \tool may not flag them due to the short duration. Similarly, certain long loading frames and frozen frames that exceed the thresholds might not be detected as high-severity issues when users have context-specific expectations of delay, such as when waiting for large media files or complex images to load. In these instances, users anticipate the delay, which reduces their perception of it as a performance issue, yet \tool might flag it as a severe lag based on the duration alone.
This analysis indicates that to further improve detection accuracy, \tool needs to go beyond a simple time threshold and incorporate additional factors such as the type of user interaction, the visual prominence of the lagging element, and the expected behavior of certain app functionalities. By integrating these contextual cues, \tool can more effectively capture the GUI lags that are most likely to affect user satisfaction. 


%true impact of GUI lags on user experience, providing more meaningful and actionable insights to developers. This enhancement would ensure that only the most perceptible and disruptive lags are flagged as critical, allowing developers to prioritize issues that are most likely to affect user satisfaction.




%Furthermore, we observed that not all lags are equally disruptive across different types of interactions. For example, users are less tolerant of delays during high-interaction activities, like tapping buttons or scrolling, compared to delays during passive content loading, such as image rendering in the background. This variation in tolerance suggests that the context of user interactions should also influence how \tool categorizes and prioritizes detected lags. Without accounting for these contextual nuances, \tool may sometimes incorrectly classify the severity of an issue, either overlooking significant lags or unnecessarily flagging minor ones.

%This analysis indicates that to improve detection accuracy, \tool needs to go beyond a simple time threshold and incorporate additional factors such as the type of user interaction, the visual prominence of the lagging element, and the expected behavior of certain app functionalities. By integrating these contextual cues, \tool can more effectively capture the true impact of GUI lags on user experience, providing more meaningful and actionable insights to developers. This enhancement would ensure that only the most perceptible and disruptive lags are flagged as critical, allowing developers to prioritize issues that are most likely to affect user satisfaction.

%============






%Nevertheless, some lags were either missed or incorrectly detected. The primary reason for this is our approach's reliance on the shortest time duration that humans can typically perceive---100ms (calculated as 16.6ms per frame * 5 frames for a 60Hz mobile device).
%In certain cases, users may still perceive lag even when an lag lasts for less than 100ms. For example on janky frames, when playing videos, the content of the screen changes frequently which makes the frame transition more noticeable to users. Although the lag duration is less than 100ms, some people may still perceive it as lagging, which is incorrectly detected by \tool.
%For long load frames, users may still perceive lag even when an lag lasts for less than 100ms. 
%For example on long load frames, as illustrated in Figure 4, although the lag duration is less than 100ms, the size of the image occupying a significant portion of the screen makes the frame transition more noticeable to users.
%Conversely, there are situations where users may not perceive the lag, even when it exceeds 100ms, because they expect the app to take time for specific functionalities based on their prior experiences, which is missed detected by \tool.
%These observations suggest that lag detection should not solely rely on duration but also incorporate other factors, such as visual elements within the app and the nature of specific functionalities, which influence user perception of performance.


% \subsubsection{Discussion} Given a screencast, developers may just want to know if it behaviours slow (i.e., containing any type of the performance lag) under the usage scenario. Hence, we also use the metrics to evaluate how effectively \tool can detect the slow screencasts. 

\rqbox{We find that \tool achieves an average precision and recall of 0.91 and 0.96 on detecting GUI lags from screencasts. Our analysis finds that incorporating additional app context may further improve detection results. }

% \begin{table}
% 	\centering
% 	\caption{Results of detecting and locating GUI performance lags by \tool. }
% 	\label{tab:resutls_detecting_lags}
%     \setlength{\tabcolsep}{5pt}
% 	\scalebox{1}{
% 	\begin{tabular}{c rrr rrr} 
% 		\toprule
% 		\multirow{2}*{lag type} & 
% 		\multicolumn{3}{c}{Detection}&
%         \multicolumn{3}{c}{Location}\\
		
% 		\cmidrule(r){2-4}\cmidrule(r){5-7}
% 		&Precision &Recall &F1 &Precision &Recall &F1\\
		
% 		\midrule
% 		Janky frames &	    &       &  &	    &       &\\
% 		Frozen frames&		&	    &  &	    &       &\\
% 		Load frames& 		&		&  &	    &       &\\

%   	\midrule
% 		\tabincell{l}{Avg. across }	&	& 	& &		& 		&\\
		
% 		\bottomrule
% 	\end{tabular}}
% \end{table}