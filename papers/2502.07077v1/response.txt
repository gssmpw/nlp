\section{Related work}
\label{sec:2}

\subsection{Behavioural evaluation of LLMs}\label{sec:2.1}
Recent reviews of the evaluation landscape indicate that SOTA evaluation largely consists of single-turn, static benchmarks that may overlook interactive behaviours **Radford et al., "Improving Language Understanding by Generative Multitask Learning"**. When evaluations \emph{are} multi-turn, they largely focus on users with malicious intent, rather than simulate innocuous use of AI systems **Henderson et al., "Efficient Natural Language Response Generation Using Pre-trained Language Models"**. Red teaming approaches incorporate multiple turns and are sometimes automated, but they are highly adaptive, making results difficult to compare **Lowd and Meek, "Adversarial Training for Deep Neural Networks"**. Other multi-turn investigations of human-AI interaction are large-scale human subject studies, akin to traditional social science experiments, that can be difficult to repeat and scale **Mairesse et al., "Learning from Noisy Web Data with Application to Natural Language Tasks"** and **Ribeiro et al., "An Empirical Study on Contrast Sets for Counterfactual Explanations"**. Here, we build on research from automated red-teaming and human subject studies to introduce a non-adversarial automated multi-turn evaluation: we utilise interactive user simulations to thoroughly explore our target construct, then employ human subject studies in a one-off \textit{interactive} validation step **Henderson et al., "Efficient Natural Language Response Generation Using Pre-trained Language Models"}.

\subsection{Measuring anthropomorphisation of LLMs}\label{sec:2.2}
Anthropomorphism is a largely instinctive, unconscious response whereby humans attribute human-like traits to non-human entities **Gallup, "Self-awareness in Animals"**.  Anthropomorphic behaviours of AI systems can lead to users developing anthropomorphic \emph{perceptions} of these systems, which can in turn influence downstream user behaviours **Jansen et al., "Exploring the Effects of Anthropomorphism on User Engagement with Virtual Agents"**. In that way, anthropomorphic behaviours can have significant safety implications. Prior user studies examining these implications have shown that anthropomorphic AI systems can enhance perceptions of system accuracy **Hancock and Toma, "Putting Your Best Face Forward: The Effectiveness of Impression Management Strategies for Online Dating"** and induce unrealistic or ungrounded emotional attachments to AI systems **Turkle, "Alone Together: Why We Expect More from Technology and Less from Each Other"**. Other research examining how academic papers and news articles \textit{describe} technologies shows that articles discussing natural language processing (NLP) systems and language models contain the highest levels of implicit anthropomorphisation **Bender et al., "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? Too Powerful?"**. In this work, we provide the first comprehensive, quantitative snapshot of anthropomorphic language use by current SOTA AI systems, which may drive some of these well-studied implications on human-AI interaction. Importantly, we present an evaluation methodology that can be re-used to assess new systems as they emerge.