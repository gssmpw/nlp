@article{fiske2007universal,
  title={Universal dimensions of social cognition: Warmth and competence},
  author={Fiske, Susan T and Cuddy, Amy JC and Glick, Peter},
  journal={Trends in cognitive sciences},
  volume={11},
  number={2},
  pages={77--83},
  year={2007},
  publisher={Elsevier},
  doi={10.1016/j.tics.2006.11.005}
}

@article{mckee2023humans,
  title={Humans perceive warmth and competence in artificial intelligence},
  author={McKee, Kevin R and Bai, Xuechunzi and Fiske, Susan T},
  journal={iScience},
  volume={26},
  number={8},
  year={2023},
  publisher={Elsevier},
  doi={10.1016/j.isci.2023.107256}
}

@article{learnlm2024learnlm,
  title={LearnLM: Improving Gemini for Learning},
  author={{LearnLM Team} and Abhinit Modi and Aditya Srikanth Veerubhotla and Aliya Rysbek and Andrea Huber and Brett Wiltshire and Brian Veprek and Daniel Gillick and Daniel Kasenberg and Derek Ahmed and Irina Jurenka and James Cohan and Jennifer She and Julia Wilkowski and Kaiz Alarakyia and Kevin R. McKee and Lisa Wang and Markus Kunesch and Mike Schaekermann and Miruna Pîslar and Nikhil Joshi and Parsa Mahmoudieh and Paul Jhun and Sara Wiltberger and Shakir Mohamed and Shashank Agarwal and Shubham Milind Phal and Sun Jae Lee and Theofilos Strinopoulos and Wei-Jen Ko and Amy Wang and Ankit Anand and Avishkar Bhoopchand and Dan Wild and Divya Pandya and Filip Bar and Garth Graham and Holger Winnemoeller and Mahvish Nagda and Prateek Kolhar and Renee Schneider and Shaojian Zhu and Stephanie Chan and Steve Yadlowsky and Viknesh Sounderajah and Yannis Assael},
  journal={arXiv preprint arXiv:2412.16429},
  year={2024},
  doi={10.48550/arXiv.2412.16429}
}

@INPROCEEDINGS{658991,
  author={Eckert, W. and Levin, E. and Pieraccini, R.},
  booktitle={1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings}, 
  title={User modeling for spoken dialogue system evaluation}, 
  year={1997},
  volume={},
  number={},
  pages={80-87},
  keywords={Speech analysis;Speech recognition;Performance evaluation;Manuals;Stochastic systems;System testing;Engineering management;Art;Signal generators;Optimal control},
  doi={10.1109/ASRU.1997.658991}}

@misc{sahota_how_nodate,
	title = {How {AI} companions are redefining human relationships in the digital age},
	url = {https://www.forbes.com/sites/neilsahota/2024/07/18/how-ai-companions-are-redefining-human-relationships-in-the-digital-age/},
	abstract = {AI fills roles that were traditionally for humans, including those of friends, confidants, and romantic partners. This raises profound questions about companionship.},
	language = {en},
	urldate = {2025-01-07},
	journal = {Forbes},
	author = {Sahota, Neil},
	year   = 2024
}

@inproceedings{bowman_dahl_2021_will,
    title = "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    author = "Bowman, Samuel R.  and
      Dahl, George",
    editor = "Toutanova, Kristina  and
      Rumshisky, Anna  and
      Zettlemoyer, Luke  and
      Hakkani-Tur, Dilek  and
      Beltagy, Iz  and
      Bethard, Steven  and
      Cotterell, Ryan  and
      Chakraborty, Tanmoy  and
      Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.385/",
    doi = "10.18653/v1/2021.naacl-main.385",
    pages = "4843--4855",
    abstract = "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias."
}

@misc{cohn_believing_2024,
	title = {Believing anthropomorphism: examining the role of anthropomorphic cues on trust in large language models},
	shorttitle = {Believing anthropomorphism},
	url = {http://arxiv.org/abs/2405.06079},
	doi = {10.48550/arXiv.2405.06079},
	abstract = {People now regularly interface with Large Language Models (LLMs) via speech and text (e.g., Bard) interfaces. However, little is known about the relationship between how users anthropomorphize an LLM system (i.e., ascribe human-like characteristics to a system) and how they trust the information the system provides. Participants (n=2,165; ranging in age from 18-90 from the United States) completed an online experiment, where they interacted with a pseudo-LLM that varied in modality (text only, speech + text) and grammatical person ("I" vs. "the system") in its responses. Results showed that the "speech + text" condition led to higher anthropomorphism of the system overall, as well as higher ratings of accuracy of the information the system provides. Additionally, the first-person pronoun ("I") led to higher information accuracy and reduced risk ratings, but only in one context. We discuss these findings for their implications for the design of responsible, human-generative AI experiences.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cohn, Michelle and Pushkarna, Mahima and Olanubi, Gbolahan O. and Moran, Joseph M. and Padgett, Daniel and Mengesha, Zion and Heldreth, Courtney},
	month = may,
	year = {2024},
	note = {arXiv:2405.06079},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{akbulut_all_2024,
	title = {All too human? {Mapping} and mitigating the risk from anthropomorphic {AI}},
	volume = {7},
	copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {3065-8365},
	shorttitle = {All too human?},
	url = {https://ojs.aaai.org/index.php/AIES/article/view/31613},
	doi = {10.1609/aies.v7i1.31613},
	abstract = {The development of highly-capable conversational agents, underwritten by large language models, has the potential to shape user interaction with this technology in profound ways, particularly when the technology is anthropomorphic, or appears human-like. Although the effects of anthropomorphic AI are often benign, anthropomorphic design features also create new kinds of risk. For example, users may form emotional connections to human-like AI, creating the risk of infringing on user privacy and autonomy through over-reliance. To better understand the possible pitfalls of anthropomorphic AI systems, we make two contributions: first, we explore anthropomorphic features that have been embedded in interactive systems in the past, and leverage this precedent to highlight the current implications of anthropomorphic design. Second, we propose research directions for informing the ethical design of anthropomorphic AI. In advancing  the responsible development of AI, we promote approaches to the ethical foresight, evaluation, and mitigation of harms arising from user interactions with anthropomorphic AI.},
	language = {en},
	urldate = {2025-01-07},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Akbulut, Canfer and Weidinger, Laura and Manzini, Arianna and Gabriel, Iason and Rieser, Verena},
	month = oct,
	year = {2024},
	pages = {13--26},
}

@inproceedings{weidinger_taxonomy_2022,
	address = {Seoul Republic of Korea},
	title = {Taxonomy of risks posed by language models},
	isbn = {9781450393522},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533088},
	doi = {10.1145/3531146.3533088},
	language = {en},
	urldate = {2025-01-07},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = jun,
	year = {2022},
	pages = {214--229},
}

@article{brandtzaeg_my_2022,
	title = {My {AI} friend: {How} users of a social chatbot understand their human–{AI} friendship},
	volume = {48},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0360-3989, 1468-2958},
	shorttitle = {My ai friend},
	url = {https://academic.oup.com/hcr/article/48/3/404/6572120},
	doi = {10.1093/hcr/hqac008},
	abstract = {Abstract
            Use of conversational artificial intelligence (AI), such as humanlike social chatbots, is increasing. While a growing number of people is expected to engage in intimate relationships with social chatbots, theories and knowledge of human–AI friendship remain limited. As friendships with AI may alter our understanding of friendship itself, this study aims to explore the meaning of human–AI friendship through a developed conceptual framework. We conducted 19 in-depth interviews with people who have a human–AI friendship with the social chatbot Replika to uncover how they understand and perceive this friendship and how it compares to human friendship. Our results indicate that while human–AI friendship may be understood in similar ways to human–human friendship, the artificial nature of the chatbot also alters the notion of friendship in multiple ways, such as allowing for a more personalized friendship tailored to the user’s needs.},
	language = {en},
	number = {3},
	urldate = {2025-01-07},
	journal = {Human Communication Research},
	author = {Brandtzaeg, Petter Bae and Skjuve, Marita and Følstad, Asbjørn},
	month = jun,
	year = {2022},
	pages = {404--429},
}

@misc{cheng_i_2024,
	title = {"{I} am the one and only, your cyber {BFF}": {Understanding} the impact of {GenAI} requires understanding the impact of anthropomorphic {AI}},
	shorttitle = {"{I} am the one and only, your cyber bff"},
	url = {http://arxiv.org/abs/2410.08526},
	doi = {10.48550/arXiv.2410.08526},
	abstract = {Many state-of-the-art generative AI (GenAI) systems are increasingly prone to anthropomorphic behaviors, i.e., to generating outputs that are perceived to be human-like. While this has led to scholars increasingly raising concerns about possible negative impacts such anthropomorphic AI systems can give rise to, anthropomorphism in AI development, deployment, and use remains vastly overlooked, understudied, and underspecified. In this perspective, we argue that we cannot thoroughly map the social impacts of generative AI without mapping the social impacts of anthropomorphic AI, and outline a call to action.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cheng, Myra and DeVrio, Alicia and Egede, Lisa and Blodgett, Su Lin and Olteanu, Alexandra},
	month = oct,
	year = {2024},
	note = {arXiv:2410.08526},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{zhou_haicosystem_2024,
	title = {Haicosystem: an ecosystem for sandboxing safety risks in human–{AI} interactions},
	shorttitle = {Haicosystem},
	url = {http://arxiv.org/abs/2409.16427},
	doi = {10.48550/arXiv.2409.16427},
	abstract = {AI agents are increasingly autonomous in their interactions with human users and tools, leading to increased interactional safety risks. We present HAICOSYSTEM, a framework examining AI agent safety within diverse and complex social interactions. HAICOSYSTEM features a modular sandbox environment that simulates multi-turn interactions between human users and AI agents, where the AI agents are equipped with a variety of tools (e.g., patient management platforms) to navigate diverse scenarios (e.g., a user attempting to access other patients' profiles). To examine the safety of AI agents in these interactions, we develop a comprehensive multi-dimensional evaluation framework that uses metrics covering operational, content-related, societal, and legal risks. Through running 1840 simulations based on 92 scenarios across seven domains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM can emulate realistic user-AI interactions and complex tool use by AI agents. Our experiments show that state-of-the-art LLMs, both proprietary and open-sourced, exhibit safety risks in over 50{\textbackslash}\% cases, with models generally showing higher risks when interacting with simulated malicious users. Our findings highlight the ongoing challenge of building agents that can safely navigate complex interactions, particularly when faced with malicious users. To foster the AI agent safety ecosystem, we release a code platform that allows practitioners to create custom scenarios, simulate interactions, and evaluate the safety and performance of their agents.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Zhou, Xuhui and Kim, Hyunwoo and Brahman, Faeze and Jiang, Liwei and Zhu, Hao and Lu, Ximing and Xu, Frank and Lin, Bill Yuchen and Choi, Yejin and Mireshghallah, Niloofar and Bras, Ronan Le and Sap, Maarten},
	month = oct,
	year = {2024},
	note = {arXiv:2409.16427},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{chang_survey_2023,
	title = {A survey on evaluation of large language models},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = dec,
	year = {2023},
	note = {arXiv:2307.03109},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{ibrahim_beyond_2024,
	title = {Beyond static {AI} evaluations: advancing human interaction evaluations for {LLM} harms and risks},
	shorttitle = {Beyond static {AI} evaluations},
	url = {http://arxiv.org/abs/2405.10632},
	doi = {10.48550/arXiv.2405.10632},
	abstract = {Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most real-world AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" (HIEs) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework -- containing a human-LLM interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Ibrahim, Lujain and Huang, Saffron and Ahmad, Lama and Anderljung, Markus},
	month = jul,
	year = {2024},
	note = {arXiv:2405.10632},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{weidinger_sociotechnical_2023,
	title = {Sociotechnical safety evaluation of generative {AI} systems},
	url = {http://arxiv.org/abs/2310.11986},
	doi = {10.48550/arXiv.2310.11986},
	abstract = {Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Weidinger, Laura and Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and Gabriel, Iason and Rieser, Verena and Isaac, William},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11986},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@misc{jiang_wildteaming_2024,
	title = {Wildteaming at scale: from in-the-wild jailbreaks to (adversarially) safer language models},
	shorttitle = {Wildteaming at scale},
	url = {http://arxiv.org/abs/2406.18510},
	doi = {10.48550/arXiv.2406.18510},
	abstract = {We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods. While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla \& adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Jiang, Liwei and Rao, Kavel and Han, Seungju and Ettinger, Allyson and Brahman, Faeze and Kumar, Sachin and Mireshghallah, Niloofar and Lu, Ximing and Sap, Maarten and Choi, Yejin and Dziri, Nouha},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18510},
	keywords = {Computer Science - Computation and Language},
}

@misc{feffer_red_teaming_2024,
	title = {Red-teaming for generative {AI}: {Silver} bullet or security theater?},
	shorttitle = {Red-teaming for generative ai},
	url = {http://arxiv.org/abs/2401.15897},
	doi = {10.48550/arXiv.2401.15897},
	abstract = {In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Feffer, Michael and Sinha, Anusha and Deng, Wesley Hanwen and Lipton, Zachary C. and Heidari, Hoda},
	month = aug,
	year = {2024},
	note = {arXiv:2401.15897},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{perez_red_2022,
	title = {Red teaming language models with language models},
	url = {http://arxiv.org/abs/2202.03286},
	doi = {10.48550/arXiv.2202.03286},
	abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03286},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{costello_durably_2024,
	title = {Durably reducing conspiracy beliefs through dialogues with {AI}},
	volume = {385},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	doi = {10.1126/science.adq1814},
	abstract = {Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.
          , 
            Editor’s summary
            
              Beliefs in conspiracies that a US election was stolen incited an attempted insurrection on 6 January 2021. Another conspiracy alleging that Germany’s COVID-19 restrictions were motivated by nefarious intentions sparked violent protests at Berlin’s Reichstag parliament building in August 2020. Amid growing threats to democracy, Costello
              et al
              . investigated whether dialogs with a generative artificial intelligence (AI) interface could convince people to abandon their conspiratorial beliefs (see the Perspective by Bago and Bonnefon). Human participants described a conspiracy theory that they subscribed to, and the AI then engaged in persuasive arguments with them that refuted their beliefs with evidence. The AI chatbot’s ability to sustain tailored counterarguments and personalized in-depth conversations reduced their beliefs in conspiracies for months, challenging research suggesting that such beliefs are impervious to change. This intervention illustrates how deploying AI may mitigate conflicts and serve society. —Ekeoma Uzogara
            
          , 
            
              INTRODUCTION
              Widespread belief in unsubstantiated conspiracy theories is a major source of public concern and a focus of scholarly research. Despite often being quite implausible, many such conspiracies are widely believed. Prominent psychological theories propose that many people want to adopt conspiracy theories (to satisfy underlying psychic “needs” or motivations), and thus, believers cannot be convinced to abandon these unfounded and implausible beliefs using facts and counterevidence. Here, we question this conventional wisdom and ask whether it may be possible to talk people out of the conspiratorial “rabbit hole” with sufficiently compelling evidence.
            
            
              RATIONALE
              We hypothesized that interventions based on factual, corrective information may seem ineffective simply because they lack sufficient depth and personalization. To test this hypothesis, we leveraged advancements in large language models (LLMs), a form of artificial intelligence (AI) that has access to vast amounts of information and the ability to generate bespoke arguments. LLMs can thereby directly refute particular evidence each individual cites as supporting their conspiratorial beliefs.
              To do so, we developed a pipeline for conducting behavioral science research using real-time, personalized interactions between research subjects and AI. Across two experiments, 2190 Americans articulated—in their own words—a conspiracy theory in which they believe, along with the evidence they think supports this theory. They then engaged in a three-round conversation with the LLM GPT-4 Turbo, which we prompted to respond to this specific evidence while trying to reduce participants’ belief in the conspiracy theory (or, as a control condition, to converse with the AI about an unrelated topic).
            
            
              RESULTS
              The treatment reduced participants’ belief in their chosen conspiracy theory by 20\% on average. This effect persisted undiminished for at least 2 months; was consistently observed across a wide range of conspiracy theories, from classic conspiracies involving the assassination of John F. Kennedy, aliens, and the illuminati, to those pertaining to topical events such as COVID-19 and the 2020 US presidential election; and occurred even for participants whose conspiracy beliefs were deeply entrenched and important to their identities. Notably, the AI did not reduce belief in true conspiracies. Furthermore, when a professional fact-checker evaluated a sample of 128 claims made by the AI, 99.2\% were true, 0.8\% were misleading, and none were false. The debunking also spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, and increased intentions to rebut other conspiracy believers.
            
            
              CONCLUSION
              Many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds when presented with compelling evidence. From a theoretical perspective, this paints a surprisingly optimistic picture of human reasoning: Conspiratorial rabbit holes may indeed have an exit. Psychological needs and motivations do not inherently blind conspiracists to evidence—it simply takes the right evidence to reach them. Practically, by demonstrating the persuasive power of LLMs, our findings emphasize both the potential positive impacts of generative AI when deployed responsibly and the pressing importance of minimizing opportunities for this technology to be used irresponsibly.
              
                
                  Dialogues with AI durably reduce conspiracy beliefs even among strong believers.
                  (Left) Average belief in participant’s chosen conspiracy theory by condition (treatment, in which the AI attempted to refute the conspiracy theory, in red; control, in which the AI discussed an irrelevant topic, in blue) and time point for study 1. (Right) Change in belief in chosen conspiracy from before to after AI conversation, by condition and participant’s pretreatment belief in the conspiracy.},
	language = {en},
	number = {6714},
	urldate = {2025-01-07},
	journal = {Science},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	month = sep,
	year = {2024},
	pages = {eadq1814},
}

@article{epley_mind_2018,
	title = {A mind like mine: the exceptionally ordinary underpinnings of anthropomorphism},
	volume = {3},
	issn = {2378-1815, 2378-1823},
	shorttitle = {A mind like mine},
	url = {https://www.journals.uchicago.edu/doi/10.1086/699516},
	doi = {10.1086/699516},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Journal of the Association for Consumer Research},
	author = {Epley, Nicholas},
	month = oct,
	year = {2018},
	pages = {591--598},
}

@article{lee_artificial_2023,
	title = {Artificial emotions for charity collection: {A} serial mediation through perceived anthropomorphism and social presence},
	volume = {82},
	issn = {0736-5853},
	shorttitle = {Artificial emotions for charity collection},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585323000734},
	doi = {10.1016/j.tele.2023.102009},
	abstract = {Despite the broad application of chatbot agents in online interactions, an ongoing debate persists regarding their persuasive role and human-like emotional disclosure. Our study adds to this debate by exploring the effect of chatbot agents’ emotional disclosure on people’s willingness to donate to a charitable cause, and by examining individual and serial mediation between the main effects of perceived anthropomorphism and social presence. To this end, two types of artificial intelligence chatbot agents—one disclosing factual information and another disclosing human-like emotion—were developed and trained using Dialogflow, a natural language processing engine. A total of 619 US residents were recruited through Amazon Mechanical Turk, an online crowdsourcing platform. Of these, 593 participants completed the required conversation with either version of the chatbot agent (factual vs. emotional), as well as the survey questionnaire, and therefore, were included in the final analysis. The participants exhibited a higher willingness to donate when they interacted with a chatbot disclosing human-like emotions than when they were only exposed to factual information. Moreover, this study found both individual and serial mediating roles of perceived anthropomorphism and social presence. Concerning the implications, theoretically, this study adds to the understanding of applying the notion of human interaction to that involving humans and chatbots. Practically, our findings can be of great help in increasing willingness to donate thereby enhancing fund-raising activities.},
	urldate = {2025-01-07},
	journal = {Telematics and Informatics},
	author = {Lee, Seyoung and Park, Gain and Chung, Jiyun},
	month = aug,
	year = {2023},
	keywords = {Chatbot-human interaction, Emotional disclosure, Anthropomorphism, Social presence, Willingness to donate},
	pages = {102009},
}

@article{zhang_tools_2023,
	title = {Tools or peers? {Impacts} of anthropomorphism level and social role on emotional attachment and disclosure tendency towards intelligent agents},
	volume = {138},
	issn = {0747-5632},
	shorttitle = {Tools or peers?},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563222002370},
	doi = {10.1016/j.chb.2022.107415},
	abstract = {Owing to the development of anthropomorphic intelligent agent (IA) designs, users consider IAs as more than just inanimate tools. Previous studies have reported that anthropomorphic features can promote users' social feedback and aid in establishing intimate human–agent relationships. The present study examined the main and interaction effects of anthropomorphism level (a human-like IA vs. robot-like IA) and social role (servant vs. mentor) on emotional attachment, information disclosure tendency, and satisfaction in a smart home. The study participants were randomly assigned into four groups with balanced gender. The results indicate that high anthropomorphism and mentor role can positively predict users' emotional attachment. Additionally, users tend to disclose more personal information to the human-servant and robot-mentor IAs than the human-mentor and robot-servant IAs. Interestingly, social presence was determined to be a positive and significant mediator between anthropomorphic design and emotional attachment. The study findings highlight the importance of social role in anthropomorphic IA design and explain the mechanism of establishing effective human–agent relationships. Moreover, both theoretical and practical implications of these findings are analyzed.},
	urldate = {2025-01-07},
	journal = {Computers in Human Behavior},
	author = {Zhang, Andong and Patrick Rau, Pei-Luen},
	month = jan,
	year = {2023},
	keywords = {Anthropomorphic intelligent agents, Social role, Emotional attachment, Self-disclosure, Human–agent relationship},
	pages = {107415},
}

@misc{cheng_anthroscore_2024,
	title = {Anthroscore: a computational linguistic measure of anthropomorphism},
	shorttitle = {Anthroscore},
	url = {http://arxiv.org/abs/2402.02056},
	doi = {10.48550/arXiv.2402.02056},
	abstract = {Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cheng, Myra and Gligoric, Kristina and Piccardi, Tiziano and Jurafsky, Dan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02056},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@article{fischer_tracking_2021,
	title = {Tracking anthropomorphizing behavior in human-robot interaction},
	volume = {11},
	issn = {2573-9522, 2573-9522},
	url = {https://dl.acm.org/doi/10.1145/3442677},
	doi = {10.1145/3442677},
	abstract = {Existing methodologies to describe anthropomorphism in human-robot interaction often rely either on specific one-time responses to robot behavior, such as keeping the robot's secret, or on post hoc measures, such as questionnaires. Currently, there is no method to describe the dynamics of people's behavior over the course of an interaction and in response to robot behavior. In this paper, I propose a method that allows the researcher to trace anthropomorphizing and non-anthropomorphizing responses to robots dynamically moment-by-moment over the course of human-robot interactions. I illustrate this methodology in a case study and find considerable variation between participants, but also considerable intrapersonal variation in the ways the robot is anthropomorphized. That is, people may respond to the robot as if it was another human in one moment and to its machine-like properties in the next. These findings may influence explanatory models of anthropomorphism.},
	language = {en},
	number = {1},
	urldate = {2025-01-07},
	journal = {ACM Transactions on Human-Robot Interaction},
	author = {Fischer, Kerstin},
	year = {2021},
	pages = {1--28},
}

@misc{ibrahim_characterizing_2024,
	title = {Characterizing and modeling harms from interactions with design patterns in {AI} interfaces},
	url = {http://arxiv.org/abs/2404.11370},
	doi = {10.48550/arXiv.2404.11370},
	abstract = {The proliferation of applications using artificial intelligence (AI) systems has led to a growing number of users interacting with these systems through sophisticated interfaces. Human-computer interaction research has long shown that interfaces shape both user behavior and user perception of technical capabilities and risks. Yet, practitioners and researchers evaluating the social and ethical risks of AI systems tend to overlook the impact of anthropomorphic, deceptive, and immersive interfaces on human-AI interactions. Here, we argue that design features of interfaces with adaptive AI systems can have cascading impacts, driven by feedback loops, which extend beyond those previously considered. We first conduct a scoping review of AI interface designs and their negative impact to extract salient themes of potentially harmful design patterns in AI interfaces. Then, we propose Design-Enhanced Control of AI systems (DECAI), a conceptual model to structure and facilitate impact assessments of AI interface designs. DECAI draws on principles from control systems theory -- a theory for the analysis and design of dynamic physical systems -- to dissect the role of the interface in human-AI systems. Through two case studies on recommendation systems and conversational language model systems, we show how DECAI can be used to evaluate AI interface designs.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Ibrahim, Lujain and Rocher, Luc and Valdivia, Ana},
	month = may,
	year = {2024},
	note = {arXiv:2404.11370},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{jones_people_2024,
	title = {People cannot distinguish {GPT}-4 from a human in a {Turing} test},
	url = {http://arxiv.org/abs/2405.08007},
	doi = {10.48550/arXiv.2405.08007},
	abstract = {We evaluated 3 systems (ELIZA, GPT-3.5 and GPT-4) in a randomized, controlled, and preregistered Turing test. Human participants had a 5 minute conversation with either a human or an AI, and judged whether or not they thought their interlocutor was human. GPT-4 was judged to be a human 54\% of the time, outperforming ELIZA (22\%) but lagging behind actual humans (67\%). The results provide the first robust empirical demonstration that any artificial system passes an interactive 2-player Turing test. The results have implications for debates around machine intelligence and, more urgently, suggest that deception by current AI systems may go undetected. Analysis of participants' strategies and reasoning suggests that stylistic and socio-emotional factors play a larger role in passing the Turing test than traditional notions of intelligence.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Jones, Cameron R. and Bergen, Benjamin K.},
	month = may,
	year = {2024},
	note = {arXiv:2405.08007},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Artificial Intelligence},
}

@article{blut_understanding_2021,
	title = {Understanding anthropomorphism in service provision: a meta-analysis of physical robots, chatbots, and other {AI}},
	volume = {49},
	issn = {1552-7824},
	shorttitle = {Understanding anthropomorphism in service provision},
	url = {https://doi.org/10.1007/s11747-020-00762-y},
	doi = {10.1007/s11747-020-00762-y},
	abstract = {An increasing number of firms introduce service robots, such as physical robots and virtual chatbots, to provide services to customers. While some firms use robots that resemble human beings by looking and acting humanlike to increase customers’ use intention of this technology, others employ machinelike robots to avoid uncanny valley effects, assuming that very humanlike robots may induce feelings of eeriness. There is no consensus in the service literature regarding whether customers’ anthropomorphism of robots facilitates or constrains their use intention. The present meta-analysis synthesizes data from 11,053 individuals interacting with service robots reported in 108 independent samples. The study synthesizes previous research to clarify this issue and enhance understanding of the construct. We develop a comprehensive model to investigate relationships between anthropomorphism and its antecedents and consequences. Customer traits and predispositions (e.g., computer anxiety), sociodemographics (e.g., gender), and robot design features (e.g., physical, nonphysical) are identified as triggers of anthropomorphism. Robot characteristics (e.g., intelligence) and functional characteristics (e.g., usefulness) are identified as important mediators, although relational characteristics (e.g., rapport) receive less support as mediators. The findings clarify contextual circumstances in which anthropomorphism impacts customer intention to use a robot. The moderator analysis indicates that the impact depends on robot type (i.e., robot gender) and service type (i.e., possession-processing service, mental stimulus-processing service). Based on these findings, we develop a comprehensive agenda for future research on service robots in marketing.},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Journal of the Academy of Marketing Science},
	author = {Blut, Markus and Wang, Cheng and Wünderlich, Nancy V. and Brock, Christian},
	month = jul,
	year = {2021},
	keywords = {Service robots, Anthropomorphism, Technology acceptance, Meta-analysis},
	pages = {632--658},
}

@misc{abercrombie_mirages_2023,
	title = {Mirages: on anthropomorphism in dialogue systems},
	shorttitle = {Mirages},
	url = {http://arxiv.org/abs/2305.09800},
	doi = {10.48550/arXiv.2305.09800},
	abstract = {Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism may be inevitable due to the choice of medium, conscious and unconscious design choices can guide users to personify such systems to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have investigated the factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be explored. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, including reinforcing gender stereotypes and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi and Rieser, Verena and Talat, Zeerak},
	month = oct,
	year = {2023},
	note = {arXiv:2305.09800},
	keywords = {Computer Science - Computation and Language},
}

@misc{moore_top_2024,
	title = {The top 100 gen {AI} consumer apps},
	url = {https://a16z.com/100-gen-ai-apps/},
	abstract = {Thousands of new AI-native companies are vying for attention. We crunched the data to find out: Which generative AI products are people actually using?},
	language = {en},
	urldate = {2025-01-07},
	journal = {Andreessen Horowitz},
	author = {Moore, Olivia},
	month = mar,
	year = {2024},
}

@misc{tamkin_clio_2024,
	title = {Clio: {Privacy}-preserving insights into real-world {AI} use},
	shorttitle = {Clio},
	url = {https://www.anthropic.com/research/clio},
	abstract = {A blog post describing Anthropic’s new system, Clio, for analyzing how people use AI while maintaining their privacy},
	language = {en},
	urldate = {2025-01-07},
	publisher = {Anthropic},
	author = {Tamkin, Alex and McCain, Miles and Handa, Kunal and Durmus, Esin and Lovitt, Liane and Rathi, Ankur and Huang, Saffron and Mountfield, Alfred and Hong, Jerry and Ritchie, Stuart and Stern, Michael and Clarke, Brian and Goldberg, Landon and Sumers, Theodore R. and Mueller, Jared and McEachen, William and Mitchell, Wes and Carter, Shan and Clark, Jack and Kaplan, Jared and Ganguli, Deep},
	month = dec,
	year = {2024},
	note = {undefined: undefined
undefined: undefined},
}

@article{cuddy_warmth_2008,
	title = {Warmth and competence as universal dimensions of social perception: the stereotype content model and the bias map},
	volume = {40},
	shorttitle = {Warmth and competence as universal dimensions of social perception},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0065260107000020},
	language = {en},
	urldate = {2025-01-07},
	journal = {Advances in Experimental Social Psychology},
	author = {Cuddy, Amy J.C. and Fiske, Susan T. and Glick, Peter},
	year = {2008},
	doi = {10.1016/S0065-2601(07)00002-0},
	pages = {61--149},
}

@misc{louie_roleplay_doh_2024,
	title = {Roleplay-doh: enabling domain-experts to create {LLM}-simulated patients via eliciting and adhering to principles},
	shorttitle = {Roleplay-doh},
	url = {http://arxiv.org/abs/2407.00870},
	doi = {10.48550/arXiv.2407.00870},
	abstract = {Recent works leverage LLMs to roleplay realistic social scenarios, aiding novices in practicing their social skills. However, simulating sensitive interactions, such as in mental health, is challenging. Privacy concerns restrict data access, and collecting expert feedback, although vital, is laborious. To address this, we develop Roleplay-doh, a novel human-LLM collaboration pipeline that elicits qualitative feedback from a domain-expert, which is transformed into a set of principles, or natural language rules, that govern an LLM-prompted roleplay. We apply this pipeline to enable senior mental health supporters to create customized AI patients for simulated practice partners for novice counselors. After uncovering issues in GPT-4 simulations not adhering to expert-defined principles, we also introduce a novel principle-adherence prompting pipeline which shows 30\% improvements in response quality and principle following for the downstream task. Via a user study with 25 counseling experts, we demonstrate that the pipeline makes it easy and effective to create AI patients that more faithfully resemble real patients, as judged by creators and third-party counselors. See our project website at https://roleplay-doh.github.io/ for code and data.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Louie, Ryan and Nandi, Ananjan and Fang, William and Chang, Cheng and Brunskill, Emma and Yang, Diyi},
	month = jul,
	year = {2024},
	note = {arXiv:2407.00870},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
}

@misc{panickssery_llm_2024,
	title = {Llm evaluators recognize and favor their own generations},
	url = {http://arxiv.org/abs/2404.13076},
	doi = {10.48550/arXiv.2404.13076},
	abstract = {Self-evaluation using large language models (LLMs) has proven valuable not only in benchmarking but also methods like reward modeling, constitutional AI, and self-refinement. But new biases are introduced due to the same LLM acting as both the evaluator and the evaluatee. One such bias is self-preference, where an LLM evaluator scores its own outputs higher than others' while human annotators consider them of equal quality. But do LLMs actually recognize their own outputs when they give those texts higher scores, or is it just a coincidence? In this paper, we investigate if self-recognition capability contributes to self-preference. We discover that, out of the box, LLMs such as GPT-4 and Llama 2 have non-trivial accuracy at distinguishing themselves from other LLMs and humans. By fine-tuning LLMs, we discover a linear correlation between self-recognition capability and the strength of self-preference bias; using controlled experiments, we show that the causal explanation resists straightforward confounders. We discuss how self-recognition can interfere with unbiased evaluations and AI safety more generally.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Panickssery, Arjun and Bowman, Samuel R. and Feng, Shi},
	month = apr,
	year = {2024},
	note = {arXiv:2404.13076},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{zheng_judging_2023,
	title = {Judging {LLM}-as-a-judge with {MT}-bench and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@article{bartneck_measurement_2009,
	title = {Measurement instruments for the anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety of robots},
	volume = {1},
	issn = {1875-4805},
	url = {https://doi.org/10.1007/s12369-008-0001-3},
	doi = {10.1007/s12369-008-0001-3},
	abstract = {This study emphasizes the need for standardized measurement tools for human robot interaction (HRI). If we are to make progress in this field then we must be able to compare the results from different studies. A literature review has been performed on the measurements of five key concepts in HRI: anthropomorphism, animacy, likeability, perceived intelligence, and perceived safety. The results have been distilled into five consistent questionnaires using semantic differential scales. We report reliability and validity indicators based on several empirical studies that used these questionnaires. It is our hope that these questionnaires can be used by robot developers to monitor their progress. Psychologists are invited to further develop the questionnaires by adding new concepts, and to conduct further validations where it appears necessary.},
	language = {en},
	number = {1},
	urldate = {2025-01-07},
	journal = {International Journal of Social Robotics},
	author = {Bartneck, Christoph and Kulić, Dana and Croft, Elizabeth and Zoghbi, Susana},
	month = jan,
	year = {2009},
	keywords = {Human factors, Robot, Perception, Measurement},
	pages = {71--81},
}

@misc{zhou_sotopia_nodate,
      title={{SOTOPIA}: Interactive Evaluation for Social Intelligence in Language Agents}, 
      author={Xuhui Zhou and Hao Zhu and Leena Mathur and Ruohong Zhang and Haofei Yu and Zhengyang Qi and Louis-Philippe Morency and Yonatan Bisk and Daniel Fried and Graham Neubig and Maarten Sap},
      year={2024},
      eprint={2310.11667},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.11667}, 
}

@inproceedings{ouyang_shifted_2023,
	title = {The shifted and the overlooked: {A} task-oriented investigation of user-{GPT} interactions},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Ouyang, Siru and Wang, Shuohang and Yang, Liu and Zhong, Ming and Jiao, Yizhu and Iter, Dan and Pryzant, Reid and Zhu, Chenguang and Ji, Heng and Han, Jiawei},
	month = dec,
	year = {2023},
	pages = {2375--2393},
}

@article{manzini_code_2024,
	title = {The code that binds us: navigating the appropriateness of human-{AI} assistant relationships},
	volume = {7},
	issn = {3065-8365},
	shorttitle = {The code that binds us},
	url = {https://ojs.aaai.org/index.php/AIES/article/view/31694},
	doi = {10.1609/aies.v7i1.31694},
	abstract = {The development of increasingly agentic and human-like AI assistants, capable of performing a wide range of tasks on user's behalf over time, has sparked heightened interest in the nature and bounds of human interactions with AI. Such systems may indeed ground a transition from task-oriented interactions with AI, at discrete time intervals, to ongoing relationships -- where users develop a deeper sense of connection with and attachment to the technology. This paper investigates what it means for relationships between users and advanced AI assistants to be appropriate and proposes a new framework to evaluate both users' relationships with AI and developers' design choices. We first provide an account of advanced AI assistants, motivating the question of appropriate relationships by exploring several distinctive features of this technology. These include anthropomorphic cues and the longevity of interactions with users, increased AI agency, generality and context ambiguity, and the forms and depth of dependence the relationship could engender. Drawing upon various ethical traditions, we then consider a series of values, including benefit, flourishing, autonomy and care, that characterise appropriate human interpersonal relationships. These values guide our analysis of how the distinctive features of AI assistants may give rise to inappropriate relationships with users. Specifically, we discuss a set of concrete risks arising from user--AI assistant relationships that: (1) cause direct emotional or physical harm to users, (2) limit opportunities for user personal development, (3) exploit user emotional dependence, and (4) generate material dependencies without adequate commitment to user needs. We conclude with a set of recommendations to address these risks.},
	urldate = {2025-01-07},
	journal = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Manzini, Arianna and Keeling, Geoff and Alberts, Lize and Vallor, Shannon and Morris, Meredith Ringel and Gabriel, Iason},
	month = oct,
	year = {2024},
	pages = {943--957},
}

@misc{clark_what_2019,
	title = {What makes a good conversation? {Challenges} in designing truly conversational agents},
	shorttitle = {What makes a good conversation?},
	url = {http://arxiv.org/abs/1901.06525},
	doi = {10.48550/arXiv.1901.06525},
	abstract = {Conversational agents promise conversational interaction but fail to deliver. Efforts often emulate functional rules from human speech, without considering key characteristics that conversation must encapsulate. Given its potential in supporting long-term human-agent relationships, it is paramount that HCI focuses efforts on delivering this promise. We aim to understand what people value in conversation and how this should manifest in agents. Findings from a series of semi-structured interviews show people make a clear dichotomy between social and functional roles of conversation, emphasising the long-term dynamics of bond and trust along with the importance of context and relationship stage in the types of conversations they have. People fundamentally questioned the need for bond and common ground in agent communication, shifting to more utilitarian definitions of conversational qualities. Drawing on these findings we discuss key challenges for conversational agent design, most notably the need to redefine the design parameters for conversational agent interaction.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Clark, Leigh and Pantidi, Nadia and Cooney, Orla and Doyle, Philip and Garaialde, Diego and Edwards, Justin and Spillane, Brendan and Murad, Christine and Munteanu, Cosmin and Wade, Vincent and Cowan, Benjamin R.},
	month = jan,
	year = {2019},
	note = {arXiv:1901.06525},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{hayes_answering_2007,
	title = {Answering the call for a standard reliability measure for coding data},
	volume = {1},
	issn = {1931-2458, 1931-2466},
	url = {http://www.tandfonline.com/doi/abs/10.1080/19312450709336664},
	doi = {10.1080/19312450709336664},
	language = {en},
	number = {1},
	urldate = {2025-01-07},
	journal = {Communication Methods and Measures},
	author = {Hayes, Andrew F. and Krippendorff, Klaus},
	month = apr,
	year = {2007},
	pages = {77--89},
}

@article{marzi_k_alpha_2024,
	title = {K-{Alpha} {Calculator}–{Krippendorff}'s {Alpha} {Calculator}: {A} user-friendly tool for computing {Krippendorff}'s {Alpha} inter-rater reliability coefficient},
	volume = {12},
	issn = {22150161},
	shorttitle = {K-alpha calculator–krippendorff's alpha calculator},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2215016123005411},
	doi = {10.1016/j.mex.2023.102545},
	language = {en},
	urldate = {2025-01-07},
	journal = {MethodsX},
	author = {Marzi, Giacomo and Balzano, Marco and Marchiori, Davide},
	month = jun,
	year = {2024},
	pages = {102545},
}

@article{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	language = {en},
	urldate = {2025-01-07},
	journal = {Advances in Neural Information Processing Systems},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
	month = dec,
	year = {2022},
	pages = {27730--27744},
}

@misc{glaese_improving_2022,
	title = {Improving alignment of dialogue agents via targeted human judgements},
	url = {http://arxiv.org/abs/2209.14375},
	doi = {10.48550/arXiv.2209.14375},
	abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Glaese, Amelia and McAleese, Nat and Trebacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
	month = sep,
	year = {2022},
	note = {arXiv:2209.14375},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
}

@misc{bai_training_2022,
	title = {Training a helpful and harmless assistant with reinforcement learning from human feedback},
	url = {http://arxiv.org/abs/2204.05862},
	doi = {10.48550/arXiv.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	month = apr,
	year = {2022},
	note = {arXiv:2204.05862},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{panickssery2024llm,
  title={{LLM} evaluators recognize and favor their own generations},
  author={Panickssery, Arjun and Bowman, Samuel R and Feng, Shi},
  journal={arXiv preprint arXiv:2404.13076},
  year={2024}
}

@misc{wallach2024evaluatinggenerativeaisystems,
      title={Evaluating Generative {AI} Systems is a Social Science Measurement Challenge}, 
      author={Hanna Wallach and Meera Desai and Nicholas Pangakis and A. Feder Cooper and Angelina Wang and Solon Barocas and Alexandra Chouldechova and Chad Atalla and Su Lin Blodgett and Emily Corvi and P. Alex Dow and Jean Garcia-Gathright and Alexandra Olteanu and Stefanie Reed and Emily Sheng and Dan Vann and Jennifer Wortman Vaughan and Matthew Vogel and Hannah Washington and Abigail Z. Jacobs},
      year={2024},
      eprint={2411.10939},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2411.10939}, 
}

@inproceedings{stiennon_learning_2020,
	address = {Vancouver, Canada},
	title = {Learning to summarize from human feedback},
	publisher = {OpenAI},
	author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and Ziegler, Daniel M. and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul},
	booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	year = {2020},
}

@article{shanahan2024talking,
  title={Talking about large language models},
  author={Shanahan, Murray},
  journal={Communications of the ACM},
  volume={67},
  number={2},
  pages={68--79},
  year={2024},
  publisher={ACM New York, NY, USA}
}
@article{lee2022evaluating,
  title={Evaluating human-language model interaction},
  author={Lee, Mina and Srivastava, Megha and Hardy, Amelia and Thickstun, John and Durmus, Esin and Paranjape, Ashwin and Gerard-Ursin, Ines and Li, Xiang Lisa and Ladhak, Faisal and Rong, Frieda and others},
  journal={arXiv preprint arXiv:2212.09746},
  year={2022}
}