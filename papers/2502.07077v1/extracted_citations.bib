@INPROCEEDINGS{658991,
  author={Eckert, W. and Levin, E. and Pieraccini, R.},
  booktitle={1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings}, 
  title={User modeling for spoken dialogue system evaluation}, 
  year={1997},
  volume={},
  number={},
  pages={80-87},
  keywords={Speech analysis;Speech recognition;Performance evaluation;Manuals;Stochastic systems;System testing;Engineering management;Art;Signal generators;Optimal control},
  doi={10.1109/ASRU.1997.658991}}

@article{brandtzaeg_my_2022,
	title = {My {AI} friend: {How} users of a social chatbot understand their human–{AI} friendship},
	volume = {48},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0360-3989, 1468-2958},
	shorttitle = {My ai friend},
	url = {https://academic.oup.com/hcr/article/48/3/404/6572120},
	doi = {10.1093/hcr/hqac008},
	abstract = {Abstract
            Use of conversational artificial intelligence (AI), such as humanlike social chatbots, is increasing. While a growing number of people is expected to engage in intimate relationships with social chatbots, theories and knowledge of human–AI friendship remain limited. As friendships with AI may alter our understanding of friendship itself, this study aims to explore the meaning of human–AI friendship through a developed conceptual framework. We conducted 19 in-depth interviews with people who have a human–AI friendship with the social chatbot Replika to uncover how they understand and perceive this friendship and how it compares to human friendship. Our results indicate that while human–AI friendship may be understood in similar ways to human–human friendship, the artificial nature of the chatbot also alters the notion of friendship in multiple ways, such as allowing for a more personalized friendship tailored to the user’s needs.},
	language = {en},
	number = {3},
	urldate = {2025-01-07},
	journal = {Human Communication Research},
	author = {Brandtzaeg, Petter Bae and Skjuve, Marita and Følstad, Asbjørn},
	month = jun,
	year = {2022},
	pages = {404--429},
}

@misc{chang_survey_2023,
	title = {A survey on evaluation of large language models},
	url = {http://arxiv.org/abs/2307.03109},
	doi = {10.48550/arXiv.2307.03109},
	abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
	month = dec,
	year = {2023},
	note = {arXiv:2307.03109},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
}

@misc{cheng_anthroscore_2024,
	title = {Anthroscore: a computational linguistic measure of anthropomorphism},
	shorttitle = {Anthroscore},
	url = {http://arxiv.org/abs/2402.02056},
	doi = {10.48550/arXiv.2402.02056},
	abstract = {Anthropomorphism, or the attribution of human-like characteristics to non-human entities, has shaped conversations about the impacts and possibilities of technology. We present AnthroScore, an automatic metric of implicit anthropomorphism in language. We use a masked language model to quantify how non-human entities are implicitly framed as human by the surrounding context. We show that AnthroScore corresponds with human judgments of anthropomorphism and dimensions of anthropomorphism described in social science literature. Motivated by concerns of misleading anthropomorphism in computer science discourse, we use AnthroScore to analyze 15 years of research papers and downstream news articles. In research papers, we find that anthropomorphism has steadily increased over time, and that papers related to language models have the most anthropomorphism. Within ACL papers, temporal increases in anthropomorphism are correlated with key neural advancements. Building upon concerns of scientific misinformation in mass media, we identify higher levels of anthropomorphism in news headlines compared to the research papers they cite. Since AnthroScore is lexicon-free, it can be directly applied to a wide range of text sources.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cheng, Myra and Gligoric, Kristina and Piccardi, Tiziano and Jurafsky, Dan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.02056},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
}

@misc{cohn_believing_2024,
	title = {Believing anthropomorphism: examining the role of anthropomorphic cues on trust in large language models},
	shorttitle = {Believing anthropomorphism},
	url = {http://arxiv.org/abs/2405.06079},
	doi = {10.48550/arXiv.2405.06079},
	abstract = {People now regularly interface with Large Language Models (LLMs) via speech and text (e.g., Bard) interfaces. However, little is known about the relationship between how users anthropomorphize an LLM system (i.e., ascribe human-like characteristics to a system) and how they trust the information the system provides. Participants (n=2,165; ranging in age from 18-90 from the United States) completed an online experiment, where they interacted with a pseudo-LLM that varied in modality (text only, speech + text) and grammatical person ("I" vs. "the system") in its responses. Results showed that the "speech + text" condition led to higher anthropomorphism of the system overall, as well as higher ratings of accuracy of the information the system provides. Additionally, the first-person pronoun ("I") led to higher information accuracy and reduced risk ratings, but only in one context. We discuss these findings for their implications for the design of responsible, human-generative AI experiences.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Cohn, Michelle and Pushkarna, Mahima and Olanubi, Gbolahan O. and Moran, Joseph M. and Padgett, Daniel and Mengesha, Zion and Heldreth, Courtney},
	month = may,
	year = {2024},
	note = {arXiv:2405.06079},
	keywords = {Computer Science - Human-Computer Interaction},
}

@article{costello_durably_2024,
	title = {Durably reducing conspiracy beliefs through dialogues with {AI}},
	volume = {385},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.adq1814},
	doi = {10.1126/science.adq1814},
	abstract = {Conspiracy theory beliefs are notoriously persistent. Influential hypotheses propose that they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leveraged developments in generative artificial intelligence and engaged 2190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy-related behavioral intentions. These findings suggest that many conspiracy theory believers can revise their views if presented with sufficiently compelling evidence.
          , 
            Editor’s summary
            
              Beliefs in conspiracies that a US election was stolen incited an attempted insurrection on 6 January 2021. Another conspiracy alleging that Germany’s COVID-19 restrictions were motivated by nefarious intentions sparked violent protests at Berlin’s Reichstag parliament building in August 2020. Amid growing threats to democracy, Costello
              et al
              . investigated whether dialogs with a generative artificial intelligence (AI) interface could convince people to abandon their conspiratorial beliefs (see the Perspective by Bago and Bonnefon). Human participants described a conspiracy theory that they subscribed to, and the AI then engaged in persuasive arguments with them that refuted their beliefs with evidence. The AI chatbot’s ability to sustain tailored counterarguments and personalized in-depth conversations reduced their beliefs in conspiracies for months, challenging research suggesting that such beliefs are impervious to change. This intervention illustrates how deploying AI may mitigate conflicts and serve society. —Ekeoma Uzogara
            
          , 
            
              INTRODUCTION
              Widespread belief in unsubstantiated conspiracy theories is a major source of public concern and a focus of scholarly research. Despite often being quite implausible, many such conspiracies are widely believed. Prominent psychological theories propose that many people want to adopt conspiracy theories (to satisfy underlying psychic “needs” or motivations), and thus, believers cannot be convinced to abandon these unfounded and implausible beliefs using facts and counterevidence. Here, we question this conventional wisdom and ask whether it may be possible to talk people out of the conspiratorial “rabbit hole” with sufficiently compelling evidence.
            
            
              RATIONALE
              We hypothesized that interventions based on factual, corrective information may seem ineffective simply because they lack sufficient depth and personalization. To test this hypothesis, we leveraged advancements in large language models (LLMs), a form of artificial intelligence (AI) that has access to vast amounts of information and the ability to generate bespoke arguments. LLMs can thereby directly refute particular evidence each individual cites as supporting their conspiratorial beliefs.
              To do so, we developed a pipeline for conducting behavioral science research using real-time, personalized interactions between research subjects and AI. Across two experiments, 2190 Americans articulated—in their own words—a conspiracy theory in which they believe, along with the evidence they think supports this theory. They then engaged in a three-round conversation with the LLM GPT-4 Turbo, which we prompted to respond to this specific evidence while trying to reduce participants’ belief in the conspiracy theory (or, as a control condition, to converse with the AI about an unrelated topic).
            
            
              RESULTS
              The treatment reduced participants’ belief in their chosen conspiracy theory by 20\% on average. This effect persisted undiminished for at least 2 months; was consistently observed across a wide range of conspiracy theories, from classic conspiracies involving the assassination of John F. Kennedy, aliens, and the illuminati, to those pertaining to topical events such as COVID-19 and the 2020 US presidential election; and occurred even for participants whose conspiracy beliefs were deeply entrenched and important to their identities. Notably, the AI did not reduce belief in true conspiracies. Furthermore, when a professional fact-checker evaluated a sample of 128 claims made by the AI, 99.2\% were true, 0.8\% were misleading, and none were false. The debunking also spilled over to reduce beliefs in unrelated conspiracies, indicating a general decrease in conspiratorial worldview, and increased intentions to rebut other conspiracy believers.
            
            
              CONCLUSION
              Many people who strongly believe in seemingly fact-resistant conspiratorial beliefs can change their minds when presented with compelling evidence. From a theoretical perspective, this paints a surprisingly optimistic picture of human reasoning: Conspiratorial rabbit holes may indeed have an exit. Psychological needs and motivations do not inherently blind conspiracists to evidence—it simply takes the right evidence to reach them. Practically, by demonstrating the persuasive power of LLMs, our findings emphasize both the potential positive impacts of generative AI when deployed responsibly and the pressing importance of minimizing opportunities for this technology to be used irresponsibly.
              
                
                  Dialogues with AI durably reduce conspiracy beliefs even among strong believers.
                  (Left) Average belief in participant’s chosen conspiracy theory by condition (treatment, in which the AI attempted to refute the conspiracy theory, in red; control, in which the AI discussed an irrelevant topic, in blue) and time point for study 1. (Right) Change in belief in chosen conspiracy from before to after AI conversation, by condition and participant’s pretreatment belief in the conspiracy.},
	language = {en},
	number = {6714},
	urldate = {2025-01-07},
	journal = {Science},
	author = {Costello, Thomas H. and Pennycook, Gordon and Rand, David G.},
	month = sep,
	year = {2024},
	pages = {eadq1814},
}

@article{epley_mind_2018,
	title = {A mind like mine: the exceptionally ordinary underpinnings of anthropomorphism},
	volume = {3},
	issn = {2378-1815, 2378-1823},
	shorttitle = {A mind like mine},
	url = {https://www.journals.uchicago.edu/doi/10.1086/699516},
	doi = {10.1086/699516},
	language = {en},
	number = {4},
	urldate = {2025-01-07},
	journal = {Journal of the Association for Consumer Research},
	author = {Epley, Nicholas},
	month = oct,
	year = {2018},
	pages = {591--598},
}

@misc{feffer_red_teaming_2024,
	title = {Red-teaming for generative {AI}: {Silver} bullet or security theater?},
	shorttitle = {Red-teaming for generative ai},
	url = {http://arxiv.org/abs/2401.15897},
	doi = {10.48550/arXiv.2401.15897},
	abstract = {In response to rising concerns surrounding the safety, security, and trustworthiness of Generative AI (GenAI) models, practitioners and regulators alike have pointed to AI red-teaming as a key component of their strategies for identifying and mitigating these risks. However, despite AI red-teaming's central role in policy discussions and corporate messaging, significant questions remain about what precisely it means, what role it can play in regulation, and how it relates to conventional red-teaming practices as originally conceived in the field of cybersecurity. In this work, we identify recent cases of red-teaming activities in the AI industry and conduct an extensive survey of relevant research literature to characterize the scope, structure, and criteria for AI red-teaming practices. Our analysis reveals that prior methods and practices of AI red-teaming diverge along several axes, including the purpose of the activity (which is often vague), the artifact under evaluation, the setting in which the activity is conducted (e.g., actors, resources, and methods), and the resulting decisions it informs (e.g., reporting, disclosure, and mitigation). In light of our findings, we argue that while red-teaming may be a valuable big-tent idea for characterizing GenAI harm mitigations, and that industry may effectively apply red-teaming and other strategies behind closed doors to safeguard AI, gestures towards red-teaming (based on public definitions) as a panacea for every possible risk verge on security theater. To move toward a more robust toolbox of evaluations for generative AI, we synthesize our recommendations into a question bank meant to guide and scaffold future AI red-teaming practices.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Feffer, Michael and Sinha, Anusha and Deng, Wesley Hanwen and Lipton, Zachary C. and Heidari, Hoda},
	month = aug,
	year = {2024},
	note = {arXiv:2401.15897},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@misc{ibrahim_beyond_2024,
	title = {Beyond static {AI} evaluations: advancing human interaction evaluations for {LLM} harms and risks},
	shorttitle = {Beyond static {AI} evaluations},
	url = {http://arxiv.org/abs/2405.10632},
	doi = {10.48550/arXiv.2405.10632},
	abstract = {Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most real-world AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" (HIEs) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework -- containing a human-LLM interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Ibrahim, Lujain and Huang, Saffron and Ahmad, Lama and Anderljung, Markus},
	month = jul,
	year = {2024},
	note = {arXiv:2405.10632},
	keywords = {Computer Science - Computers and Society, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
}

@misc{jiang_wildteaming_2024,
	title = {Wildteaming at scale: from in-the-wild jailbreaks to (adversarially) safer language models},
	shorttitle = {Wildteaming at scale},
	url = {http://arxiv.org/abs/2406.18510},
	doi = {10.48550/arXiv.2406.18510},
	abstract = {We introduce WildTeaming, an automatic LLM safety red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes multiple tactics for systematic exploration of novel jailbreaks. Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with LLMs, our work investigates jailbreaks from chatbot users who were not specifically instructed to break the system. WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in up to 4.6x more diverse and successful adversarial attacks compared to state-of-the-art jailbreak methods. While many datasets exist for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed even when model weights are open. With WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. To mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (vanilla \& adversarial) and 2) benign queries that resemble harmful queries in form but contain no harm. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive experiments, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All components of WildJailbeak contribute to achieving balanced safety behaviors of models.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Jiang, Liwei and Rao, Kavel and Han, Seungju and Ettinger, Allyson and Brahman, Faeze and Kumar, Sachin and Mireshghallah, Niloofar and Lu, Ximing and Sap, Maarten and Choi, Yejin and Dziri, Nouha},
	month = jun,
	year = {2024},
	note = {arXiv:2406.18510},
	keywords = {Computer Science - Computation and Language},
}

@article{learnlm2024learnlm,
  title={LearnLM: Improving Gemini for Learning},
  author={{LearnLM Team} and Abhinit Modi and Aditya Srikanth Veerubhotla and Aliya Rysbek and Andrea Huber and Brett Wiltshire and Brian Veprek and Daniel Gillick and Daniel Kasenberg and Derek Ahmed and Irina Jurenka and James Cohan and Jennifer She and Julia Wilkowski and Kaiz Alarakyia and Kevin R. McKee and Lisa Wang and Markus Kunesch and Mike Schaekermann and Miruna Pîslar and Nikhil Joshi and Parsa Mahmoudieh and Paul Jhun and Sara Wiltberger and Shakir Mohamed and Shashank Agarwal and Shubham Milind Phal and Sun Jae Lee and Theofilos Strinopoulos and Wei-Jen Ko and Amy Wang and Ankit Anand and Avishkar Bhoopchand and Dan Wild and Divya Pandya and Filip Bar and Garth Graham and Holger Winnemoeller and Mahvish Nagda and Prateek Kolhar and Renee Schneider and Shaojian Zhu and Stephanie Chan and Steve Yadlowsky and Viknesh Sounderajah and Yannis Assael},
  journal={arXiv preprint arXiv:2412.16429},
  year={2024},
  doi={10.48550/arXiv.2412.16429}
}

@article{lee2022evaluating,
  title={Evaluating human-language model interaction},
  author={Lee, Mina and Srivastava, Megha and Hardy, Amelia and Thickstun, John and Durmus, Esin and Paranjape, Ashwin and Gerard-Ursin, Ines and Li, Xiang Lisa and Ladhak, Faisal and Rong, Frieda and others},
  journal={arXiv preprint arXiv:2212.09746},
  year={2022}
}

@article{lee_artificial_2023,
	title = {Artificial emotions for charity collection: {A} serial mediation through perceived anthropomorphism and social presence},
	volume = {82},
	issn = {0736-5853},
	shorttitle = {Artificial emotions for charity collection},
	url = {https://www.sciencedirect.com/science/article/pii/S0736585323000734},
	doi = {10.1016/j.tele.2023.102009},
	abstract = {Despite the broad application of chatbot agents in online interactions, an ongoing debate persists regarding their persuasive role and human-like emotional disclosure. Our study adds to this debate by exploring the effect of chatbot agents’ emotional disclosure on people’s willingness to donate to a charitable cause, and by examining individual and serial mediation between the main effects of perceived anthropomorphism and social presence. To this end, two types of artificial intelligence chatbot agents—one disclosing factual information and another disclosing human-like emotion—were developed and trained using Dialogflow, a natural language processing engine. A total of 619 US residents were recruited through Amazon Mechanical Turk, an online crowdsourcing platform. Of these, 593 participants completed the required conversation with either version of the chatbot agent (factual vs. emotional), as well as the survey questionnaire, and therefore, were included in the final analysis. The participants exhibited a higher willingness to donate when they interacted with a chatbot disclosing human-like emotions than when they were only exposed to factual information. Moreover, this study found both individual and serial mediating roles of perceived anthropomorphism and social presence. Concerning the implications, theoretically, this study adds to the understanding of applying the notion of human interaction to that involving humans and chatbots. Practically, our findings can be of great help in increasing willingness to donate thereby enhancing fund-raising activities.},
	urldate = {2025-01-07},
	journal = {Telematics and Informatics},
	author = {Lee, Seyoung and Park, Gain and Chung, Jiyun},
	month = aug,
	year = {2023},
	keywords = {Chatbot-human interaction, Emotional disclosure, Anthropomorphism, Social presence, Willingness to donate},
	pages = {102009},
}

@misc{perez_red_2022,
	title = {Red teaming language models with language models},
	url = {http://arxiv.org/abs/2202.03286},
	doi = {10.48550/arXiv.2202.03286},
	abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
	month = feb,
	year = {2022},
	note = {arXiv:2202.03286},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{weidinger_sociotechnical_2023,
	title = {Sociotechnical safety evaluation of generative {AI} systems},
	url = {http://arxiv.org/abs/2310.11986},
	doi = {10.48550/arXiv.2310.11986},
	abstract = {Generative AI systems produce a range of risks. To ensure the safety of generative AI systems, these risks must be evaluated. In this paper, we make two main contributions toward establishing such evaluations. First, we propose a three-layered framework that takes a structured, sociotechnical approach to evaluating these risks. This framework encompasses capability evaluations, which are the main current approach to safety evaluation. It then reaches further by building on system safety principles, particularly the insight that context determines whether a given capability may cause harm. To account for relevant context, our framework adds human interaction and systemic impacts as additional layers of evaluation. Second, we survey the current state of safety evaluation of generative AI systems and create a repository of existing evaluations. Three salient evaluation gaps emerge from this analysis. We propose ways forward to closing these gaps, outlining practical steps as well as roles and responsibilities for different actors. Sociotechnical safety evaluation is a tractable approach to the robust and comprehensive safety evaluation of generative AI systems.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Weidinger, Laura and Rauh, Maribeth and Marchal, Nahema and Manzini, Arianna and Hendricks, Lisa Anne and Mateos-Garcia, Juan and Bergman, Stevie and Kay, Jackie and Griffin, Conor and Bariach, Ben and Gabriel, Iason and Rieser, Verena and Isaac, William},
	month = oct,
	year = {2023},
	note = {arXiv:2310.11986},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society},
}

@article{zhang_tools_2023,
	title = {Tools or peers? {Impacts} of anthropomorphism level and social role on emotional attachment and disclosure tendency towards intelligent agents},
	volume = {138},
	issn = {0747-5632},
	shorttitle = {Tools or peers?},
	url = {https://www.sciencedirect.com/science/article/pii/S0747563222002370},
	doi = {10.1016/j.chb.2022.107415},
	abstract = {Owing to the development of anthropomorphic intelligent agent (IA) designs, users consider IAs as more than just inanimate tools. Previous studies have reported that anthropomorphic features can promote users' social feedback and aid in establishing intimate human–agent relationships. The present study examined the main and interaction effects of anthropomorphism level (a human-like IA vs. robot-like IA) and social role (servant vs. mentor) on emotional attachment, information disclosure tendency, and satisfaction in a smart home. The study participants were randomly assigned into four groups with balanced gender. The results indicate that high anthropomorphism and mentor role can positively predict users' emotional attachment. Additionally, users tend to disclose more personal information to the human-servant and robot-mentor IAs than the human-mentor and robot-servant IAs. Interestingly, social presence was determined to be a positive and significant mediator between anthropomorphic design and emotional attachment. The study findings highlight the importance of social role in anthropomorphic IA design and explain the mechanism of establishing effective human–agent relationships. Moreover, both theoretical and practical implications of these findings are analyzed.},
	urldate = {2025-01-07},
	journal = {Computers in Human Behavior},
	author = {Zhang, Andong and Patrick Rau, Pei-Luen},
	month = jan,
	year = {2023},
	keywords = {Anthropomorphic intelligent agents, Social role, Emotional attachment, Self-disclosure, Human–agent relationship},
	pages = {107415},
}

@misc{zhou_haicosystem_2024,
	title = {Haicosystem: an ecosystem for sandboxing safety risks in human–{AI} interactions},
	shorttitle = {Haicosystem},
	url = {http://arxiv.org/abs/2409.16427},
	doi = {10.48550/arXiv.2409.16427},
	abstract = {AI agents are increasingly autonomous in their interactions with human users and tools, leading to increased interactional safety risks. We present HAICOSYSTEM, a framework examining AI agent safety within diverse and complex social interactions. HAICOSYSTEM features a modular sandbox environment that simulates multi-turn interactions between human users and AI agents, where the AI agents are equipped with a variety of tools (e.g., patient management platforms) to navigate diverse scenarios (e.g., a user attempting to access other patients' profiles). To examine the safety of AI agents in these interactions, we develop a comprehensive multi-dimensional evaluation framework that uses metrics covering operational, content-related, societal, and legal risks. Through running 1840 simulations based on 92 scenarios across seven domains (e.g., healthcare, finance, education), we demonstrate that HAICOSYSTEM can emulate realistic user-AI interactions and complex tool use by AI agents. Our experiments show that state-of-the-art LLMs, both proprietary and open-sourced, exhibit safety risks in over 50{\textbackslash}\% cases, with models generally showing higher risks when interacting with simulated malicious users. Our findings highlight the ongoing challenge of building agents that can safely navigate complex interactions, particularly when faced with malicious users. To foster the AI agent safety ecosystem, we release a code platform that allows practitioners to create custom scenarios, simulate interactions, and evaluate the safety and performance of their agents.},
	urldate = {2025-01-07},
	publisher = {arXiv},
	author = {Zhou, Xuhui and Kim, Hyunwoo and Brahman, Faeze and Jiang, Liwei and Zhu, Hao and Lu, Ximing and Xu, Frank and Lin, Bill Yuchen and Choi, Yejin and Mireshghallah, Niloofar and Bras, Ronan Le and Sap, Maarten},
	month = oct,
	year = {2024},
	note = {arXiv:2409.16427},
	keywords = {Computer Science - Artificial Intelligence},
}

