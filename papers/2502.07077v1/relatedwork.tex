\section{Related work}
\label{sec:2}

\subsection{Behavioural evaluation of LLMs}\label{sec:2.1}
Recent reviews of the evaluation landscape indicate that SOTA evaluation largely consists of single-turn, static benchmarks that may overlook interactive behaviours \cite{chang_survey_2023, weidinger_sociotechnical_2023, ibrahim_beyond_2024}. When evaluations \emph{are} multi-turn, they largely focus on users with malicious intent, rather than simulate innocuous use of AI systems \cite{jiang_wildteaming_2024, zhou_haicosystem_2024}. Red teaming approaches incorporate multiple turns and are sometimes automated, but they are highly adaptive, making results difficult to compare \cite{feffer_red_teaming_2024,perez_red_2022, lee2022evaluating}. Other multi-turn investigations of human-AI interaction are large-scale human subject studies, akin to traditional social science experiments, that can be difficult to repeat and scale \cite{costello_durably_2024, learnlm2024learnlm}. Here, we build on research from automated red-teaming and human subject studies to introduce a non-adversarial automated multi-turn evaluation: we utilise interactive user simulations to thoroughly explore our target construct, then employ human subject studies in a one-off \textit{interactive} validation step \cite{658991}. 

\subsection{Measuring anthropomorphisation of LLMs}\label{sec:2.2}
Anthropomorphism is a largely instinctive, unconscious response whereby humans attribute human-like traits to non-human entities \cite{epley_mind_2018}.  Anthropomorphic behaviours of AI systems can lead to users developing anthropomorphic \emph{perceptions} of these systems, which can in turn influence downstream user behaviours \cite{lee_artificial_2023,cohn_believing_2024}. In that way, anthropomorphic behaviours can have significant safety implications. Prior user studies examining these implications have shown that anthropomorphic AI systems can enhance perceptions of system accuracy \cite{cohn_believing_2024} and induce unrealistic or ungrounded emotional attachments to AI systems \cite{brandtzaeg_my_2022,zhang_tools_2023}. Other research examining how academic papers and news articles \textit{describe} technologies shows that articles discussing natural language processing (NLP) systems and language models contain the highest levels of implicit anthropomorphisation \cite{cheng_anthroscore_2024}. In this work, we provide the first comprehensive, quantitative snapshot of anthropomorphic language use by current SOTA AI systems, which may drive some of these well-studied implications on human-AI interaction. Importantly, we present an evaluation methodology that can be re-used to assess new systems as they emerge.