\section{Related Work}
\label{Related Work}
\subsection{Graph Contrastive Learning}
Recently, due to the significant success of contrastive learning methods for images, CL techniques have increasingly been applied to graphs. DGI **Kencana, "Deep Graph Infomax"** is a notable pioneering work inspired by Deep InfoMax ____ . The objective of DGI is to acquire node representations by maximizing the mutual information of local graph blocks. GMI **Grover, "Graph Mutual Information"** and HDMI **Hassani, "Higher-Order Contrastive Learning"** further enhanced DGI by incorporating mutual information about edges and node attributes. Inspired by SimCLR ____ , GRACE ____ , which generates two augmented graph views by randomly perturbing nodes and edges, learns node representations by aligning representations of the same nodes across two augmented graphs while maintaining separation of the other nodes. Despite these advancements, a major criticism of graph contrastive learning (GCL) methods is so-called semantic drift, where the augmentation process can lead to distorted or inconsistent node representations, thereby reducing the quality of learned embeddings ____ . Additionally, most existing GCL frameworks use two view encoders with identical architectures, which may limit their ability to learn diverse features from augmented graph views ____ . In this study, we propose a novel paradigm for GCL. Unlike existing methods, the proposed AS-GCL method employs asymmetric view encoders with identical parameters but a different number of propagation layers. This design enables the encoders to capture information from both long-range and short-range connections in the graph, resulting in richer and more diverse representations. Structural asymmetry helps enhance the quality of the learned representations and opens new avenues for advancing contrastive learning on graph data.


%\subsection{Augmentations on Graphs}
%The goal of graph augmentation in graph contrastive learning (GCL) is to generate compatible and identity-preserving positive examples of a given graph. While significant progress has been made in data augmentation for contrastive learning in computer vision and natural language processing, transferring these methods directly to graphs is challenging due to the non-Euclidean and irregular structure of graph data. The most widely used augmentation methods in GCL utilize random augmentations. For example, DGI **Kencana, "Deep Graph Infomax"** and GIC **Gao, "Graph Inference Contrastive Learning"** use random perturbations of the feature matrix for augmentation, while Wang et al. ____ augment latent space features to generate auxiliary samples with identical class labels but different semantics. GraphCL ____ constructs contrastive views by randomly discarding nodes and edges or using graph sampling, and MVGRL ____ generates augmented views through graph diffusion. While these methods have shown success, random augmentations do not account for the varying impacts of edges and/or features on the original graph, potentially leading to arbitrariness in the augmented graphs and degrading model performance.

%To better preserve graph properties that may be overlooked by uniform perturbation, domain knowledge can be employed. GCA ____ and GRADE ____ apply degree-based contrastive learning, adopting strategies for both low-degree and high-degree nodes. MoCL ____ leverages biomedical domain knowledge to augment molecular graphs by replacing substructures like functional groups. However, these methods primarily rely on local information and tend to capture the local structure of a graph, potentially overlooking its global features. Moreover, if so-called supernodes (nodes of extremely high degree) exist, these methods may lead to distortions in importance assessment. In addition, most of these methods operate in the spatial domain, but studies have shown that perturbations to nodes or edges do not have equal impact on the graph spectrum ____ , which reflects the structural properties of graphs. To capture minimal sufficient information from a graph and remove redundancy that could harm downstream performance, some researchers adopt generative or adaptive strategies. For instance, GACN ____ utilizes a graph view generator and discriminator, learning enhanced graph views via a min-max game which are further used for GCL. AdaGCL ____ employs graph generation and denoising models to create contrastive views that adapt to a data distribution, enabling adaptive contrastive views for GCL. GCARec ____ calculates the retention probability for each edge based on an attention mechanism and samples edges using Gumbel Softmax. NCLA ____ employs a multi-head attention mechanism to generate learnable graph augmentations, where each of its augmented views consists of the same nodes and edges as the original graph with different adaptive edge weights. While these methods effectively remove noise introduced by data augmentation, they typically introduce complex model architectures, such as additional generators, discriminators, or attention mechanisms. This complexity increases computational cost and memory overhead, particularly when applied to large-scale graph data, affecting training and inference efficiency. Augmentation-free approaches exist, such as LSGCL ____ , which generate positive and negative samples using anchors derived from a single view. However, this approach lacks diverse sample construction, leading the model to learn only local, context-specific representations. Without a global perspective, the model is more likely to fall into a local optimum, limiting its generalizability.

%In contrast, we propose a general framework, AS-GCL, that leverages minimal spectral variability to significantly reduce structural differences between augmented and original graphs, thereby facilitating high-quality contrastive learning. AS-GCL also employs an asymmetric encoder architecture to enhance its representations. Our spectral augmentation algorithms can be run independently and seamlessly integrated with existing GCL methods to boost performance without altering the underlying learning process.
%Compared to \textbf{domain knowledge-based methods} (which often rely on manually designed augmentations) such as GCA ____ , our approach analyzes the global properties of a graph through its Laplacian spectrum; this enables the model to efficiently capture global structural patterns such as periodicity and symmetry without requiring extensive manual tuning or domain-specific expertise. As a result, our spectral augmentation techniques demonstrate greater versatility and adaptability across various graph types.
%In contrast to \textbf{generative or adaptive methods}, such as NCLA ____ , AdaGCL ____ , and GCARec ____ , our method does not rely on complex encoders or attention-based mechanisms. Instead, it achieves similar results through simple yet powerful matrix operations; this allows for efficient implementation, reducing computational overhead while maintaining high-quality augmentations, making it a more scalable and practical solution for diverse graph learning tasks.
%Compared to \textbf{augmentation-free methods} (e.g., those that rely solely on a single view to generate positive and negative samples) such as LSGCL ____ , AS-GCL introduces spectral-based augmentations to ensure that the augmented view preserves the global structural integrity of the original graph; this provides a richer and more diverse set of training samples, reducing the risk of overfitting to local patterns and improving the model's generalization ability.

\subsection{Spectrum-based Augmentation}

Most existing augmentation methods focus on the spatial domain, with relatively few studies exploring the spectral domain. Spectral-based methods can be broadly categorized as either utilizing the graph Laplacian spectrum or the feature spectrum. Methods leveraging the graph Laplacian spectrum focus primarily on capturing global graph structure. For example, SpCo ____ aims to capture the low-frequency information common to both augmentations by adjusting the balance of high- and low-frequency components. Similarly, Amur Ghose et al. ____ proposed a method that enhances representations by rearranging various frequency components within graphs. GCIL ____ adopts a causal perspective by considering low-frequency components as causal variables and high-frequency components as noncausal variables. GCIL achieves graph augmentation by perturbing noncausal information and retaining causal information.
Feature spectrum-based methods emphasize the local features of nodes or edges. For example, COSTA ____ decomposes hidden space representations and optimizes the alignment process by adding uniform noise (perturbation) to singular values. SFA ____ balances the contribution of each component within the feature spectrum to achieve an optimal representation. These methods typically rely on spectral filters to balance low- and high-frequency components, which requires careful tuning based on specific datasets and tasks.
In contrast, our method focuses on minimizing Laplacian spectral variations to effectively mitigate the semantic drift caused by graph data augmentation. Unlike previous methods, our method does not require fine-tuning of frequency components for each dataset; rather, it automatically learns the structure with minimal variance. By reducing Laplacian spectral variation, our method ensures a more consistent global representation, leading to improved generalizability and robustness across different tasks.