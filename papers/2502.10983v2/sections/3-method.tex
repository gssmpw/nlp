\section{METHOD}
To minimize foot contact velocity in the physics simulator correlated to the footstep sound in the real, we propose a sim-to-real deep \ac{rl} framework which synergistically incorporates three key components as shown in \figref{fig:system_overview}.

The training network architecture consists of three fully connected layers, each with 128 hidden units and \ac{elu} activation functions, and utilizes the \ac{ppo}~\cite{ppo}.
Both the actor and critic networks are implemented as \ac{mlp}, adhering to the architecture and PPO hyperparameters established in prior work~\cite{legged_gym}.
The simulation and control loop frequencies are set to 400 Hz and 100 Hz, respectively, using the Isaac Gym framework~\cite{Makoviychuk2021-th}.

\begin{table}
\caption{Policy Observation}
\label{table:observation}
\begin{center}
\begin{tabular}{ccccc}
\toprule
\textbf{Entry} & \textbf{Symbol} & \textbf{Noise} & \textbf{Units} & \textbf{Dimensions} \\
 & &  \textbf{level}\\
\midrule
Joint positions & $\boldsymbol{\varphi}$ & 0.01 & $\rm{rad}$ & $12$ \\
Joint velocities & $\boldsymbol{\dot{\varphi}}$ & 1.5 & $\rm{rad/s}$ & $12$ \\
Last action (joint position) & $\boldsymbol{a}^*$ & 0.0 & $\rm{rad}$ & $12$\\
Last action (PD gain scale) & $\boldsymbol{gain}^*$ & 0.0 & $\rm{-}$ & $12$\\
Foot contact state & $\boldsymbol{f_c}$ & 0.0 & $-$ & $4$\\
Gravity orientation & $\boldsymbol{g}$ & 0.05 & $m/s^2$ & $3$\\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Actions}
As shown in \figref{fig:system_overview}, the policy outputs two types of actions for the 12 leg joints: target joint positions and PD gain scales. $\boldsymbol{a}^*$ and $\boldsymbol{gain}^*$ represent target joint positions and PD gain scales, respectively. 
We chose target joint position and PD gain scale as actions instead of directly estimating torque for two primary reasons. First, previous work~\cite{peng2017learning} reported that torque control as an action wasn't efficient for learning locomotion. Secondly, previous work~\cite{bogdanovic2020learning} found that estimating target joint positions and PD gains is more suitable for sensitive contact tasks that require soft ground contact.

In our approach, the proportional $P_i$ and derivative $D_i$ gains are calculated using the following formulas:
\begin{equation}
\begin{split}
P_i &= P^* + \alpha\sigma({x}_{i}) \\
D_i &= D^* + \beta\sigma({x}_{i})
\end{split}
\label{eq:pd_gain_scale}
\end{equation}
where $\sigma$ is the sigmoid function, $\alpha$ and $\beta$ are scaling factors, $P^*$ and $D^*$ are the nominal gains, and $i$ represents each leg joint. The specific parameter values are $\alpha = 4.0$, $\beta = 0.02$, $P^* = 3.0$, and $D^* = 0.03$.
Our locomotion policy outputs only one gain scale factor $x_i$ for both the proportional $P_i$ and derivative $D_i$ parts of the PD controller. 
While this approach of outputting a single gain scale from the policy is consistent with the previously mentioned work~\cite{bogdanovic2020learning}, this method is the first application to locomotion tasks in quadruped robots to explore the quietness.

\subsection{Observations}
Policy observations and noise levels are detailed in Table~\ref{table:observation}. One of the key elements of our approach is utilizing the switch contact sensor to determine when to stiffen the joint from its soft state during the contact phase. Since aibo is a consumer-grade small robot designed to be affordable for the average household, it does not have force and torque sensors. Without these sensors, there is a traditional way to estimate external force to detect contact using model-based methods \cite{traditional_torque_control}. However, such methods are not sufficiently accurate to determine when a small robot contacts the ground, as it is difficult to precisely identify friction and develop an accurate robot model for commercially available mass production robots. Instead of estimating external force, we utilized switch contact sensors to detect the contact directly. Although the switch contact sensors provide binary output, they are still useful for the contact sensitive task such as quiet walking.

\begin{table*}
\caption{Reward functions and scales at each curriculum learning phase}
\label{table:reward}
\begin{center}
\begin{tabular}{lllcc}
\toprule
\textbf{Category} & \textbf{Reward} & \textbf{Definition} & \textbf{Scale in noisy} & \textbf{Scale in quiet} \\
 & & & \textbf{walking phase} & \textbf{walking phase} \\
\midrule
\multirow{2}{*}{\textbf{Task Rewards}} & Linear velocity tracking & $\exp(-\frac{1}{0.06}\|\mathbf{v}_{b,xy}^* - \mathbf{v}_{b,xy}\|^2 )$ & 1.0 & 1.0  \\
 %& Angular velocity tracking & $\exp(-\frac{1}{0.06}\|\mathbf{w}_{b,z}^* - \mathbf{w}_{b,z}\|^2)$ & 1.0 & 1.0 \\
 & Angular velocity tracking & $\exp(-\frac{1}{0.06}\|{w}_{b,z}^* - {w}_{b,z}\|^2)$ & 1.0 & 1.0 \\
\midrule
\multirow{8}{*}{\textbf{Penalty Rewards}} & Joint torque & $\|\boldsymbol{\tau}\|^2$ & -0.015 & -0.015  \\
 & Base linear velocity z & $\|v_{b,z}\|^2$ & -3.0 & -3.0 \\
 & Base orientation & $\|\boldsymbol{\vartheta}_{b,xy}\|^2$ & -5.0 & -5.0 \\
 & Base angular velocity & $\|\boldsymbol{\dot\vartheta}_{b,xy}\|^2$ & -0.05 & -0.05 \\
 & Foot slippage & $\|\boldsymbol{v}_{f,xy}\|^2$ & -0.15 & -0.15 \\
 & Self-collisions & $n_{collision}$ & -10.0 & -10.0 \\
 & Foot air time & $\sum_i (\boldsymbol{t}_{f,air} - 0.2)$ & 2.0 & 2.0 \\
 & Joint target difference & $\|\boldsymbol{a}^*_{t-1} - \boldsymbol{a}^*_{t}\|^2$ & -0.02 & -0.02 \\
 & PD gain scale difference & $\|\boldsymbol{gain}^*_{t-1} - \boldsymbol{gain}^*_{t}\|^2$ & -0.005 & -0.005 \\
\midrule
\multirow{3}{*}{\textbf{Noisy Walking Penalties}}  & Foot contact velocity & $\|\boldsymbol{v}_{f,xyz}\|^2$ & -5.0 & -25.0 \\
& Joint acceleration & $\|\boldsymbol{\ddot\varphi}\|^2$ & -2e-7 & -4e-7 \\
 & Base angular acceleration & $\|\boldsymbol{\ddot\vartheta}_{b,xy}\|^2$ & -5e-5 & -1e-4 \\
\bottomrule
\end{tabular}
\end{center}
\end{table*}

\subsection{Rewards}
Reward functions and their relative scales are described in Table~\ref{table:reward}. 
For task rewards, $\boldsymbol{v}_{b}$, $\boldsymbol{v}_{b}^*$, $\boldsymbol{w}_{b}$, and $\boldsymbol{w}_{b}^*$ denote the measured and desired linear velocities in the base frame and the measured and desired angular velocities in the base frame, respectively.
Penalty rewards help suppress inappropriate or jerky motions. $\boldsymbol{\varphi}$, $\boldsymbol{\tau}$, $\boldsymbol{v}_{f}$, ${n}_{collision}$ and $\boldsymbol{t}_{f,air}$ represent joint positions, joint torques, foot velocities, number of collisions and airtime of foot, respectively.  

We incorporate three additional terms related to the footstep sound for quiet walking. 
Biomechanics research indicates that footstep sound correlates with foot contact velocity~\cite{acoustic_footstep}. When the foot contacts the ground, the kinetic energy, $\frac{1}{2} m \mathbf{v}_{f,xyz}^2$, is converted into various forms of energy, including sound energy which produces the footstep sound. 
While foot contact velocity is the dominant parameter related to footstep sound, we also include joint acceleration and base angular acceleration penalties to support this objective. We observe that sudden changes in joint angles and base orientation often follow loud step sounds, as noisy walking causes the robot to bump into the ground.

\subsection{Curriculum Learning}
Initial attempts at training without curriculum learning proved unsuccessful, resulting in either non-convergent training or the aibo learning to remain stationary, a strong local minimum that minimizes noise but fails to achieve the desired locomotion.
To address these challenges, we implemented a curriculum learning approach~\cite{curriculum1}. Inspired by a recent study proposing the division of a training episode into multiple stages to tackle the explore vs exploit dilemma~\cite{tuyls2022multi, hartmann2024deep}, we divide the training episode into two distinct stages: noisy walking and quiet walking stage. 

During the noisy walking phase, we applied lower weights to the noise-related penalties, as shown in Table~\ref{table:reward}. This strategy allowed the robot to prioritize learning basic locomotion and velocity command tracking, even if significant footstep sounds are generated. 
The transition to the quiet walking phase is triggered when the agents achieve the sum of episodic reward about linear and angular velocity tracking is more than $1.5$.
In the quiet walking phase, we increased the penalties associated with foot contact velocity, base angular acceleration, and joint acceleration.  
Given that foot contact velocity is the most critical parameter for footstep sound reduction, its penalty was increased 5 times compared to the noisy walking phase, while other penalties were only doubled as shown in Table~\ref{table:reward}. 
By combining reward shaping with this two-phase curriculum learning approach, we successfully enabled the robot to develop a quiet locomotion policy that minimizes the footstep sound while preserving its capacity to execute velocity commands effectively.

\begin{table}[!t]
\caption{Randomized Simulation Parameters}
\label{table:domainrandomizationparameters}
\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Randomized Variables} & \textbf{unit} & \textbf{min} & \textbf{max} \\
\midrule
Robot base mass & kg & -0.4 & 0.4 \\
Velocity disturbance & m/s & -0.2 & 0.2 \\
External force & N & 0.0 & 0.4 \\
External torque & Nm & 0.0 & 0.1 \\
Terrain height & m & 0.002 & 0.01 \\
Friction coefficient & [-] & 0.4 & 0.7 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\subsection{Domain Randomization}
We employ \ac{dr}~\cite{domainrandomization1} to enhance the sim-to-real transfer of the policy. This involves procedurally generating various parameters in the simulation environment, such as the height of the steps, the ground friction coefficient, and the robot base mass. Additionally, we utilize disturbances during the episodes by applying an impulse signal to the robot’s base velocity every 4 seconds and random external forces and torques to the base. The ranges for the randomized parameters are shown in \tabref{table:domainrandomizationparameters}.

The extent of randomization significantly influences the resulting policy's characteristics. While more extensive \ac{dr} typically leads to a more robust gait, it can potentially compromise the quietness of locomotion. Previous research on \ac{rl}-based racing drones~\cite{song2023reaching} has mentioned the trade-off between robustness and performance depending on the degree of \ac{dr}. Similarly, for quiet walking, it is crucial to strike an optimal balance when determining the level of \ac{dr}.

% We employ \ac{dr}~\cite{domainrandomization1} to enhance the sim-to-real transfer of the policy, as detailed in Table~\ref{table:domainrandomizationparameters}.
% The extent of randomization significantly influences the resulting policy's characteristics. While more extensive \ac{dr} typically leads to a more robust gait, it can potentially compromise the quietness of locomotion. Previous research on \ac{rl}-based racing drones~\cite{song2023reaching} has mentioned the trade-off between robustness and performance depending on the degree of \ac{dr}. Similarly, for quiet walking, it is crucial to strike an optimal balance when determining the level of \ac{dr}.

