[
  {
    "index": 0,
    "papers": [
      {
        "key": "baltruvsaitis2018multimodal",
        "author": "Baltru{\\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe",
        "title": "Multimodal machine learning: A survey and taxonomy"
      },
      {
        "key": "ramachandram2017deep",
        "author": "Ramachandram, Dhanesh and Taylor, Graham W",
        "title": "Deep multimodal learning: A survey on recent advances and trends"
      },
      {
        "key": "wang2020deep",
        "author": "Wang, Yikai and Huang, Wenbing and Sun, Fuchun and Xu, Tingyang and Rong, Yu and Huang, Junzhou",
        "title": "Deep multimodal fusion by channel exchanging"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "poria2015deep",
        "author": "Poria, Soujanya and Cambria, Erik and Gelbukh, Alexander",
        "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "tsai2019multimodal",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Multimodal transformer for unaligned multimodal language sequences"
      },
      {
        "key": "hang2021multi",
        "author": "Hang, Li and Fan, Yang and Xiaohan, Xing and others",
        "title": "Multi-modal multi-instance learning using weakly correlated histopathological images and tabular clinical information in Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021"
      },
      {
        "key": "lee2021variational",
        "author": "Lee, Changhee and Van der Schaar, Mihaela",
        "title": "A variational information bottleneck approach to multi-omics data integration"
      },
      {
        "key": "kiela2019supervised",
        "author": "Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide",
        "title": "Supervised multimodal bitransformers for classifying images and text"
      },
      {
        "key": "huang2021makes",
        "author": "Huang, Yu and Du, Chenzhuang and Xue, Zihui and Chen, Xuanyao and Zhao, Hang and Huang, Longbo",
        "title": "What makes multi-modal learning better than single (provably)"
      },
      {
        "key": "huang2020multimodal",
        "author": "Huang, Jian and Tao, Jianhua and Liu, Bin and Lian, Zheng and Niu, Mingyue",
        "title": "Multimodal transformer fusion for continuous emotion recognition"
      },
      {
        "key": "hu2021unit",
        "author": "Hu, Ronghang and Singh, Amanpreet",
        "title": "Unit: Multimodal multitask learning with a unified transformer"
      },
      {
        "key": "hong2020more",
        "author": "Hong, Danfeng and Gao, Lianru and Yokoya, Naoto and Yao, Jing and Chanussot, Jocelyn and Du, Qian and Zhang, Bing",
        "title": "More diverse means better: Multimodal deep learning meets remote-sensing imagery classification"
      },
      {
        "key": "arevalo2017gated",
        "author": "Arevalo, John and Solorio, Thamar and Montes-y-G{\\'o}mez, Manuel and Gonz{\\'a}lez, Fabio A",
        "title": "Gated multimodal units for information fusion"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2021mogonet",
        "author": "Wang, Tongxin and Shao, Wei and Huang, Zhi and Tang, Haixu and Zhang, Jie and Ding, Zhengming and Huang, Kun",
        "title": "MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification"
      },
      {
        "key": "subedar2019uncertainty",
        "author": "Subedar, Mahesh and Krishnan, Ranganath and Meyer, Paulo Lopez and Tickoo, Omesh and Huang, Jonathan",
        "title": "Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference"
      },
      {
        "key": "simonyan2014two",
        "author": "Simonyan, Karen and Zisserman, Andrew",
        "title": "Two-stream convolutional networks for action recognition in videos"
      },
      {
        "key": "natarajan2012multimodal",
        "author": "Natarajan, Pradeep and Wu, Shuang and Vitaladevuni, Shiv and Zhuang, Xiaodan and Tsakalidis, Stavros and Park, Unsang and Prasad, Rohit and Natarajan, Premkumar",
        "title": "Multimodal feature fusion for robust event detection in web videos"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "poria2015deep",
        "author": "Poria, Soujanya and Cambria, Erik and Gelbukh, Alexander",
        "title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "tsai2019multimodal",
        "author": "Tsai, Yao-Hung Hubert and Bai, Shaojie and Liang, Paul Pu and Kolter, J Zico and Morency, Louis-Philippe and Salakhutdinov, Ruslan",
        "title": "Multimodal transformer for unaligned multimodal language sequences"
      },
      {
        "key": "hang2021multi",
        "author": "Hang, Li and Fan, Yang and Xiaohan, Xing and others",
        "title": "Multi-modal multi-instance learning using weakly correlated histopathological images and tabular clinical information in Medical Image Computing and Computer Assisted Intervention--MICCAI 2021: 24th International Conference, Strasbourg, France, September 27--October 1, 2021"
      },
      {
        "key": "lee2021variational",
        "author": "Lee, Changhee and Van der Schaar, Mihaela",
        "title": "A variational information bottleneck approach to multi-omics data integration"
      },
      {
        "key": "kiela2019supervised",
        "author": "Kiela, Douwe and Bhooshan, Suvrat and Firooz, Hamed and Perez, Ethan and Testuggine, Davide",
        "title": "Supervised multimodal bitransformers for classifying images and text"
      },
      {
        "key": "huang2021makes",
        "author": "Huang, Yu and Du, Chenzhuang and Xue, Zihui and Chen, Xuanyao and Zhao, Hang and Huang, Longbo",
        "title": "What makes multi-modal learning better than single (provably)"
      },
      {
        "key": "huang2020multimodal",
        "author": "Huang, Jian and Tao, Jianhua and Liu, Bin and Lian, Zheng and Niu, Mingyue",
        "title": "Multimodal transformer fusion for continuous emotion recognition"
      },
      {
        "key": "hu2021unit",
        "author": "Hu, Ronghang and Singh, Amanpreet",
        "title": "Unit: Multimodal multitask learning with a unified transformer"
      },
      {
        "key": "hong2020more",
        "author": "Hong, Danfeng and Gao, Lianru and Yokoya, Naoto and Yao, Jing and Chanussot, Jocelyn and Du, Qian and Zhang, Bing",
        "title": "More diverse means better: Multimodal deep learning meets remote-sensing imagery classification"
      },
      {
        "key": "arevalo2017gated",
        "author": "Arevalo, John and Solorio, Thamar and Montes-y-G{\\'o}mez, Manuel and Gonz{\\'a}lez, Fabio A",
        "title": "Gated multimodal units for information fusion"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "wang2021mogonet",
        "author": "Wang, Tongxin and Shao, Wei and Huang, Zhi and Tang, Haixu and Zhang, Jie and Ding, Zhengming and Huang, Kun",
        "title": "MOGONET integrates multi-omics data using graph convolutional networks allowing patient classification and biomarker identification"
      },
      {
        "key": "subedar2019uncertainty",
        "author": "Subedar, Mahesh and Krishnan, Ranganath and Meyer, Paulo Lopez and Tickoo, Omesh and Huang, Jonathan",
        "title": "Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference"
      },
      {
        "key": "simonyan2014two",
        "author": "Simonyan, Karen and Zisserman, Andrew",
        "title": "Two-stream convolutional networks for action recognition in videos"
      },
      {
        "key": "natarajan2012multimodal",
        "author": "Natarajan, Pradeep and Wu, Shuang and Vitaladevuni, Shiv and Zhuang, Xiaodan and Tsakalidis, Stavros and Park, Unsang and Prasad, Rohit and Natarajan, Premkumar",
        "title": "Multimodal feature fusion for robust event detection in web videos"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "federici2020learning",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "han2020trusted",
        "author": "Han, Zongbo and Zhang, Changqing and Fu, Huazhu and Zhou, Joey Tianyi",
        "title": "Trusted multi-view classification"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "geng2021uncertainty",
        "author": "Geng, Yu and Han, Zongbo and Zhang, Changqing and Hu, Qinghua",
        "title": "Uncertainty-aware multi-view representation learning"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "han2022multimodal",
        "author": "Han, Zongbo and Yang, Fan and Huang, Junzhou and Zhang, Changqing and Yao, Jianhua",
        "title": "Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhang2023provable",
        "author": "Zhang, Qingyang and Wu, Haitao and Zhang, Changqing and Hu, Qinghua and Fu, Huazhu and Zhou, Joey Tianyi and Peng, Xi",
        "title": "Provable dynamic fusion for low-quality multimodal data"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zheng2023multi",
        "author": "Zheng, Xiao and Tang, Chang and Wan, Zhiguo and Hu, Chengyu and Zhang, Wei",
        "title": "Multi-level confidence learning for trustworthy multimodal classification"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zou2023dpnet",
        "author": "Zou, Xin and Tang, Chang and Zheng, Xiao and Li, Zhenglai and He, Xiao and An, Shan and Liu, Xinwang",
        "title": "DPNET: Dynamic Poly-attention Network for Trustworthy Multi-modal Classification"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhou2023calm",
        "author": "Zhou, Hai and Xue, Zhe and Liu, Ying and Li, Boang and Du, Junping and Liang, Meiyu and Qi, Yuankai",
        "title": "CALM: An Enhanced Encoding and Confidence Evaluating Framework for Trustworthy Multi-view Learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "cao2024predictive",
        "author": "Bing Cao and Yinan Xia and Yi Ding and Changqing Zhang and Qinghua Hu",
        "title": "Predictive Dynamic Fusion"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "radenovic2023filtering",
        "author": "Radenovic, Filip and Dubey, Abhimanyu and Kadian, Abhishek and Mihaylov, Todor and Vandenhende, Simon and Patel, Yash and Wen, Yi and Ramanathan, Vignesh and Mahajan, Dhruv",
        "title": "Filtering, distillation, and hard negatives for vision-language pre-training"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "gadre2024datacomp",
        "author": "Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others",
        "title": "Datacomp: In search of the next generation of multimodal datasets"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "sharma2018conceptual",
        "author": "Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu",
        "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "huang2021learning",
        "author": "Huang, Zhenyu and Niu, Guocheng and Liu, Xiao and Ding, Wenbiao and Xiao, Xinyan and Wu, Hua and Peng, Xi",
        "title": "Learning with noisy correspondence for cross-modal matching"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "li2021align",
        "author": "Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong",
        "title": "Align before fuse: Vision and language representation learning with momentum distillation"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "li2022blip",
        "author": "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
        "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "huang2023nlip",
        "author": "Huang, Runhui and Long, Yanxin and Han, Jianhua and Xu, Hang and Liang, Xiwen and Xu, Chunjing and Liang, Xiaodan",
        "title": "Nlip: Noise-robust language-image pre-training"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2020oscar",
        "author": "Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others",
        "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "zeng2023semantic",
        "author": "Zeng, Pengxin and Yang, Mouxing and Lu, Yiding and Zhang, Changqing and Hu, Peng and Peng, Xi",
        "title": "Semantic invariant multi-view clustering with fully incomplete information"
      }
    ]
  }
]