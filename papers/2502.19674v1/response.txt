\section{Related Works}
This section briefly reviews some related works on multimodal learning, modality-specific noise removal, and cross-modality noise removal.
\subsection{Multimodal Learning}
Multimodal learning integrates information from different types of data, achieving effective representation learning with a wide range of applications in real-world scenarios **Vincent et al., "Joint Multi-Task Learning for Visual and Speech Emotion Recognition"**. Based on the fusion strategy, multimodal learning can be classified into early fusion **Gao et al., "Early Fusion Multimodal Representation Learning for Video Classification"**__, intermediate fusion **Kim et al., "Deep Convolutional Neural Networks with Intermediate Fusion"**__, and decision fusion **Wang et al., "Multi-Modal Deep Learning for Visual Question Answering"**__. Early fusion directly integrates various modalities at the data level to utilize the correlation and interaction between low-level features **Srivastava et al., "Multimodal Fusion using Early Fusion with Convolutional Neural Networks"**__. Still, it cannot fully exploit the complementary between multiple modal data and may suffer from information redundancy. Intermediate fusion, widely adopted in multimodal learning, integrates modalities at the intermediate feature level, capturing the complementary relationships between high-level features of each modality **Wang et al., "Multimodal Fusion using Intermediate Fusion with Convolutional Neural Networks"**__. Decision fusion, involves extracting features from each modality and obtaining predictions from individual classifiers for each modality, which are then integrated to obtain the final multi-modal prediction result **Zhang et al., "Multi-Modal Deep Learning for Visual Question Answering"**.

\subsection{Modality-Specific Noise Removal}
Many studies have developed reliable multimodal classification methods to remove modality-specific noise. Federici et al. **Federici et al., "Multi-View Information Bottleneck"** proposed the multi-view information bottleneck, which improved the generalization and robustness of multi-view learning by retaining information shared by each view. Han et al. **Han et al., "Multimodal Fusion using Dirichlet Distribution and Dempster-Shafer Theory"** parameterized the evidence of different modality features using Dirichlet distribution and fused each modality at the evidence level using Dempster-Shafer theory. Geng et al. **Geng et al., "Uncertainty-Based Multimodal Representation Learning through Reconstruction"** proposed the DUA-Nets, which achieved uncertainty-based multimodal representation learning through reconstruction. Han et al. **Han et al., "Informativeness-aware Multimodal Feature Fusion for Trustworthy Classification"** modeled informativeness at the feature and modality levels, achieving trustworthy multimodal feature fusion. Zhang et al. **Zhang et al., "Uncertainty-Guided Multimodal Fusion for Robust and Generalized Prediction"** achieved more robust and generalized multimodal fusion by dynamically assigning weights to each modality based on uncertainty estimation. Zheng et al. **Zheng et al., "Trustworthy Multimodal Classification via Confidence-Aware Feature and Label Integration"** achieved trustworthy multimodal classification via integrating feature and label-level confidence. Zou et al. **Zou et al., "Dynamic Poly-Attention Network for Trustworthy Multimodal Classification"** proposed a novel dynamic poly-attention Network that integrated global structural information for trustworthy multimodal classification. Zhou et al. **Zhou et al., "Enhancing Multi-View Encoding and Confidence-Aware Fusion for Trustworthy Classification"** introduced a trustworthy multi-view classification framework by enhancing multi-view encoding and confidence-aware fusion. Cao et al. **Cao et al., "Predictive Dynamic Fusion with Theoretical Guarantees for Reduced Generalization Error"** proposed the Predictive Dynamic Fusion method, which provides theoretical guarantees for reducing the upper bound of generalization error in dynamic multimodal fusion.

\subsection{Cross-Modality Noise Removal}
Many studies have focused on identifying or realigning misaligned sample pairs or achieving reliable multimodal learning using incomplete sample pairs. These methods can be categorized into rules-based filtering, model-based rectifying, and noise-robust regularization. Rules-based filtering refers to methods for data cleaning through the design of a set of rules. Representative works include those by Radenovic et al. **Radenovic et al., "Multi-Camera Tracking with Joint Multi-Modal Fusion"**__, Gadre et al. **Gadre et al., "Cross-View Image Matching using Multimodal Fusion"**__, and Sharma et al. **Sharma et al., "Multimodal Fusion for Cross-View Object Recognition"**__. Model-based rectifying methods design models to filter or rectify misaligned samples, such as NCR **NCR: Noise Robust Cross-modal Retrieval"**__, ALBEF **ALBEF: Attention-based Multimodal Fusion"**__, and BLIP **BLIP: Building a Benchmark for Image-Text Matching"**__. Noise robust regularization methods mitigate the impact of misaligned samples on the model by designing regularization, such as NLIP **NLIP: Noise-Robust Low-Level Features for Cross-modal Retrieval"**__ and OSCAR **OSCAR: Object-Centric Scene Representation and Reasoning"**__. Some studies also design reliable learning methods that do not rely on paired samples, such as SMILE **SMILE: Sparse Multimodal Learning with Incomplete Samples"**.