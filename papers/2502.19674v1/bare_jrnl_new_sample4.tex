\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amssymb}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{booktabs}
% \usepackage{subcaption}
\usepackage{pifont}
\allowdisplaybreaks
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021
\captionsetup[subfloat]{font=scriptsize}

\begin{document}

\title{MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification}

\author{Tong Zhang, Shu Shen, C. L. Philip Chen}
% \author{IEEE Publication Technology,~\IEEEmembership{Staff,~IEEE,}
%         % <-this % stops a space
% \thanks{This paper was produced by the IEEE Publication Technology Group. They are in Piscataway, NJ.}% <-this % stops a space
% \thanks{Manuscript received April 19, 2021; revised August 16, 2021.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, January~2025}%
{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2021 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle

\begin{abstract}
% Reliable multimodal learning in the presence of noisy data is a widely concerned issue, especially in safety-critical applications. Many existing multimodal methods have effectively achieved reliable classification in the presence of modality-specific or cross-modality noise. However, they fail to handle the simultaneous occurrence of both types of noise efficiently. To address this issue, a reliable multimodal classification method dubbed Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is proposed. MICINet achieves the reliable removal of both types of noise by unifying them into the concept of Inter-class Confusing Information (\textit{ICI}) and eliminating it at both global and individual levels. Specifically, MICINet first reliably learns the global \textit{ICI} distribution through the proposed \textbf{\textit{Global \textbf{ICI} Learning Module (GICI)}}. Then, it introduces the \textbf{\textit{Global-guided Sample ICI Learning module}} (\textbf{\textit{SICI}}) to efficiently remove global-level \textit{ICI} from sample features utilizing the learned global \textit{ICI} distribution. Subsequently, the \textbf{\textit{Sample-adaptive Cross-modality Information Compensation module (CMIC)}} is designed to remove individual-level \textit{ICI} from each sample reliably. This is achieved through interpretable cross-modality information compensation based on the complementary relationship between discriminative features and \textit{ICI} and the perception of the relative quality of modalities introduced by the relative discriminative power. Experiments on four datasets demonstrate that MICINet outperforms other state-of-the-art reliable multimodal methods under various noise conditions. For instance, on the ROSMAP dataset with both types of noise, MICINet achieves improvements of 7.2\% in ACC, 5.9\% in F1, and 4.9\% in AUC.

Reliable multimodal learning in the presence of noisy data is a widely concerned issue, especially in safety-critical applications. Many reliable multimodal methods delve into addressing modality-specific or cross-modality noise. However, they fail to handle the coexistence of both types of noise efficiently. Moreover, the lack of comprehensive consideration for noise at both global and individual levels limits their reliability. To address these issues, a reliable multimodal classification method dubbed Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is proposed. MICINet achieves the reliable removal of both types of noise by unifying them into the concept of Inter-class Confusing Information (\textit{ICI}) and eliminating it at both global and individual levels. Specifically, MICINet first reliably learns the global \textit{ICI} distribution through the proposed \textbf{\textit{Global \textbf{ICI} Learning Module}}. Then, it introduces the \textbf{\textit{Global-guided Sample ICI Learning module}} to efficiently remove global-level \textit{ICI} from sample features utilizing the learned global \textit{ICI} distribution. Subsequently, the \textbf{\textit{Sample-adaptive Cross-modality Information Compensation module}} is designed to remove individual-level \textit{ICI} from each sample reliably. This is achieved through interpretable cross-modality information compensation based on the complementary relationship between discriminative features and \textit{ICI} and the perception of the relative quality of modalities introduced by the relative discriminative power. Experiments on four datasets demonstrate that MICINet outperforms other state-of-the-art reliable multimodal classification methods under various noise conditions. MICINet achieves up to 7.2\% improvement in ACC, 6.6\% in Weighted F1, and 14.9\% in Macro F1 on multi-class datasets, and up to 7.2\% in ACC, 5.9\% in F1, and 4.9\% in AUC on binary classification dataset.
% For instance, on the ROSMAP dataset with both types of noise, MICINet achieves improvements of 7.2\% in ACC, 5.9\% in F1, and 4.9\% in AUC.


% Specifically, MICINet unifies the modality-specific and cross-modality noise under the concept of Inter-class Confusing Information (\textit{ICI}) and designs a general paradigm for removing both of them from the perspective of \textit{ICI} elimination and discriminative power enhancement. In addition, MICINet fully considers noise at both the global and sample levels and effectively improves the reliability of sample-adaptive \textit{ICI} removal by utilizing the effectively learned global \textit{ICI} information. Furthermore, the complementary relationship between discriminative features and \textit{ICI} across modalities is leveraged to facilitate the interpretability of the noise removal process. Experimental results on four datasets demonstrate that MICINet outperforms other reliable multimodal classification methods in terms of classification performance and robustness under both types of noise.
% Multimodal machine learning has significantly advanced in a wide range of scenarios, yet its reliability is undermined by noise in multimodal data. Existing reliable multimodal classification methods can only address modality-specific or cross-modality noise but fail to handle cases where both types of noise are present effectively. To this end, a novel and reliable multimodal classification method capable of simultaneously removing two types of noise is proposed, dubbed Global-Guided Sample-Adaptive Cross-Modal Discriminative Alignment (MICINet). On the one hand, MICINet unifies the two types of noise under the concept of Inter-class Confusing Information (\textit{ICI}) and designs a general paradigm for noise removal from the perspective of \textit{ICI} elimination and discriminative power enhancement. On the other hand, MICINet fully considers noise at both the global and sample levels and effectively improves the reliability of sample-adaptive \textit{ICI} removal by utilizing the effectively learned global \textit{ICI} information. Specifically, MICINet first learns the global ICI distribution from the entire dataset. MICINet fully considers the differences in confusing information and degrees among different class pairs and designs a novel dual-path structure to achieve reliable learning. For an input sample, MICINet first separates the discriminative features and ICI from the features of each modality. The global ICI distribution is introduced as an additional supervision to enhance the reliability of the discriminative features separation. Subsequently, a sample-adaptive cross-modality information compensation mechanism is proposed, leveraging the complementary relationship between discriminative features and ICI. This mechanism interpretably compensates for the information loss caused by ICI in each modality by acquiring useful information from other modalities, thereby further mitigating the impact of ICI. The relative discriminative power is proposed to enhance the mechanism's reliability by integrating compensatory information from different modalities according to their quality. To the best of our knowledge, this paper is the first to design a general architecture capable of removing both types of noise simultaneously and leveraging global information to assist noise removal at the sample level. Experimental results on four datasets demonstrate that MICINet outperforms other reliable multimodal classification methods in terms of classification performance and robustness under both types of noise.


% Specifically, MICINet establishes a two-stage pipeline to remove \textit{ICI} in each multimodal sample. In the first stage, the discriminative features and \textit{ICI} of each modality in the input sample are reliably separated under the guidance of global \textit{ICI}, removing modality-specific noise and a portion of cross-modal noise. A novel learning module based on a dual-path structure is employed to learn the global \textit{ICI} of the data. The second stage proposes an interpretable cross-modal information compensation mechanism, leveraging the complementary relationship between discriminative features and \textit{ICI}. This mechanism enhances discriminative power across modalities and remove cross-modality noise further. Additionally, a sample-adaptive approach based on the relative discriminative power across modalities is integrated to improve the reliability of modality information compensation. Experimental results on four datasets demonstrate that MICINet outperforms other reliable multimodal classification methods in terms of classification performance and robustness under modality-specific noise, cross-modal noise, and the combined presence of both types of noise.

% To this end, a novel, reliable multimodal classification network is proposed, dubbed Global-Guided Sample-Adaptive Cross-Modal Discriminative Alignment (MICINet). On the one hand, MICINet explores a general approach for cross-modal noise removal from the perspective of discriminative power. On the other hand, it reliably removes both types of noise simultaneously by integrating global distribution information into sample-adaptive feature learning and cross-modal information compensation. Specifically, MICINet unifies the two types of noise into a concept opposite to the discriminative feature, i.e., inter-class confusion information (ICI), and establishes a two-stage noise removal pipeline. In the first stage, MICINet learns the global distribution of ICI and uses it to guide the separation of discriminative features and ICI from each modality of the samples, effectively removing modality-specific noise. In the second stage, MICINet leverages the mutually exclusive relationship between discriminative features and ICI to achieve interpretable cross-modal discriminative information compensation, effectively removing cross-modal noise. Experimental results on four datasets demonstrate that MICINet outperforms other reliable multimodal classification methods in terms of classification performance and robustness under modality-specific noise, cross-modal noise, and the combined presence of both types of noise.
\end{abstract}

\begin{IEEEkeywords}
Multimodal classification, modality-specific noise, cross-modality noise, reliable multimodal classification.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:intro}
% \IEEEPARstart{W}{ith} the rapid advancement of various sensors, multimodal data is readily accessible and widely used to enhance the performance of different applications. For example, single-cell multimodal sequencing technologies \cite{zhu2020single,baysoy2023technological,vandereyken2023methods,wang2024progress} utilizing information from multiple modalities such as DNA, mRNA, and miRNA to achieve comprehensive analyses of cells. Autonomous driving systems achieve safer operations by integrating data from various sensors \cite{yeong2021sensor,huang2022multi,wang2023multi,tian2024multi,vinoth2024multi}. However, In real-world scenarios, multimodal data often contains noise, significantly reducing the reliability of multimodal classification, a critical issue in safety-critical applications such as medicine, finance, and autonomous driving. Multimodal noise can be categorized into modality-specific and cross-modality noise \cite{zhang2024multimodal}. The former refers to noise introduced by sensor malfunctions, environmental influences, or data transmission issues, while the latter pertains to weakly aligned or unaligned multimodal samples \cite{zhang2019weakly,changpinyo2021conceptual}.

% These two types of noise often occur together in real-world scenarios (e.g., misaligned data encountering transmission errors). However, existing methods typically focus on addressing only one type of noise and fail to handle cases where both types coexist effectively. Many methods effectively remove modality-specific noise based on confidence of each modality \cite{han2020trusted,han2022trusted,geng2021uncertainty,han2022multimodal,zhou2023calm,zou2023dpnet,zhang2023provable,cao2024predictive}. However, they are weak in identifying or realigning misaligned samples, thus failing to address cross-modality noise. In contrast, numerous methods are dedicated to addressing misaligned samples \cite{radenovic2023filtering,huang2021learning,li2021align,li2022blip,huang2023nlip,li2020oscar,nakada2023understanding}. These methods employ specially designed rules, models, or regularizations to filter out, rectify, or reduce the impact of misaligned samples on the model. However, the presence of modality-specific noise severely impairs the ability of these methods to discern and learn the alignment relationships between modalities, thereby affecting their reliability. Thus, an important question is raised: \textbf{\textit{How to design a reliable multimodal classification method that simultaneously removes modality-specific and cross-modality noise?}}

\IEEEPARstart{W}{ith} the rapid advancement of various sensors, multimodal data is readily accessible and widely used to enhance the performance of different applications. For example, single-cell multimodal sequencing technologies \cite{zhu2020single,baysoy2023technological,vandereyken2023methods,wang2024progress} utilizing information from multiple modalities such as DNA, mRNA, and miRNA to achieve comprehensive analyses of cells. Autonomous driving systems achieve safer operations by integrating data from various sensors \cite{yeong2021sensor,huang2022multi,wang2023multi,tian2024multi,vinoth2024multi}. However, multimodal data often contains noise, significantly reducing the reliability of multimodal classification, a critical issue in safety-critical applications such as medicine, finance, and autonomous driving. Regarding the type, multimodal noise can be categorized into modality-specific and cross-modal noise \cite{zhang2024multimodal}. The former refers to noise introduced into each modality's data due to sensor malfunction, environmental impact, or transmission errors. The latter refers to weakly aligned or unaligned multimodal samples \cite{zhang2019weakly,changpinyo2021conceptual}. Regarding the characteristics, multimodal noise can be classified into global-level noise, such as sensor errors that introduce consistently distributed noise across all samples, and individual-level noise, such as misalignment in a particular sample due to data collection errors.

In real-world scenarios, data is often contaminated by both modality-specific and cross-modal noise simultaneously (e.g., misaligned data encountering transmission errors). However, existing methods typically focus on addressing only one type of noise and fail to handle cases where both types coexist effectively. Many methods effectively remove modality-specific noise based on confidence of each modality \cite{han2020trusted,han2022trusted,geng2021uncertainty,han2022multimodal,zhou2023calm,zou2023dpnet,zhang2023provable,cao2024predictive}. However, they are weak in identifying or realigning misaligned samples, thus failing to address cross-modality noise. In contrast, numerous methods are dedicated to addressing misaligned samples \cite{radenovic2023filtering,huang2021learning,li2021align,li2022blip,huang2023nlip,li2020oscar,nakada2023understanding,zeng2023semantic}. These methods employ specially designed rules, models, or regularizations to filter out, rectify, or reduce the impact of misaligned samples on the model. However, the presence of modality-specific noise severely impairs the ability of these methods to discern and learn the alignment relationships between modalities. Moreover, the above-mentioned methods generally fail to address global-level and individual-level noise concurrently, thereby limiting the reliability of noise removal. Among them, \cite{han2022multimodal,zou2023dpnet,zhang2023provable,cao2024predictive,radenovic2023filtering,huang2021learning,li2021align,li2022blip,huang2023nlip,li2020oscar} focus on individual-level noise removal while neglecting the learning of global-level noise. \cite{geng2021uncertainty,zeng2023semantic} remove noise from a global perspective but overlook the dynamic quality of individual samples. Thus, an important question is raised: \textbf{\textit{How to design a reliable multimodal classification method that simultaneously removes modality-specific and cross-modality noise at both global and individual levels?}}

% Considering that both types of noise introduce useless or erroneous information that reduces the discriminative power of modalities, this paper proposes to unify them under the concept of Inter-class Confusion Information (\textit{ICI}). \textit{ICI} is opposite to discriminative features, which refers to features that fail to distinguish different classes. It can be defined as the aggregation of shared information between all distinct class pairs. From the perspective of removing \textit{ICI} across modalities, a novel paradigm named Global-Guided Sample-Adaptive Cross-Modal Discriminative Alignment (MICINet) is designed to achieve the simultaneous removal of both types of noise. MICINet fully considers noise at both the global level (e.g., sensor failures causing identically distributed noise) and the individual level (e.g., a misaligned sample due to data collection errors) and designs a two-stage \textit{ICI} removal pipeline for each sample. The first stage focuses on global \textit{ICI} removal. The Global \textit{ICI} Learning Module (\textbf{\textit{GICI}}) is first introduced to learn the global \textit{ICI} distribution of the data. Subsequently, the Global-Guided Sample \textit{ICI} Learning Module (\textbf{\textit{SICI}}) module is proposed to extract features in each modality of the sample under the guidance of global \textit{ICI} distribution. The second stage further eliminates the individual \textit{ICI} by cross-modality information compensation via the proposed Sample-Adaptive Cross-Modal Information Compensation module (\textbf{\textit{CMIC}}).
% Considering that both types of noise introduce useless or erroneous information that reduces the discriminative power of modalities, this paper proposes to unify them under the concept of Inter-class Confusing Information (\textit{ICI}). \textit{ICI} is opposite to discriminative features, which refers to features that fail to distinguish different classes. It can be defined as the aggregation of shared information between all distinct class pairs. From the perspective of removing \textit{ICI} across modalities, a novel paradigm named Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is designed to achieve the simultaneous removal of both types of noise. MICINet fully considers \textit{ICI} at both the global level (e.g., sensor failures causing identically distributed noise among all samples) and the individual level (e.g., a misaligned sample due to data collection errors) and delves into their characteristics. At the global level, MICINet observes the variation of \textit{ICI} distributions and degrees of confusion among different class pairs. For individual \textit{ICI}, MICINet notes that it leads to varying information incompleteness and quality across modalities in different samples. Based on these findings, MICINet establishes a two-stage pipeline for reliable global and individual \textit{ICI} removals in each sample.
To address this question, a novel paradigm named Multi-Level Inter-Class Confusing Information Removal Network (MICINet) is designed. MICINet simultaneously removes both types of noise by unifying them under the concept of Inter-class Confusing Information (\textit{ICI}) and reliably eliminating it at both global and individual levels. \textit{ICI} is proposed based on the observation that two types of noise both introduce useless or erroneous information that leads to classification confusion. It can be intuitively learned by extracting and aggregating the shared information between different class pairs. While learning and removing \textit{ICI}, MICINet fully considers it at both the global and individual levels and delves into their characteristics. At the global level, MICINet observes the variation of \textit{ICI} distributions and degrees of confusion among different class pairs. For individual \textit{ICI}, MICINet notes that it leads to varying information incompleteness and quality across modalities in different samples. Based on these findings, a two-stage pipeline for reliable global and individual \textit{ICI} removals is established.

% MICINet fully considers noise at both the global level (e.g., sensor failures causing identically distributed noise) and the sample level (e.g., a misaligned sample due to data collection errors) and designs a global ICI-guided sample-adaptive ICI removal pipeline. MICINet first introduces the Global \textit{ICI} Learning Module (\textbf{\textit{GICI}}) to learn the global \textit{ICI} of the data. Subsequently, a Global-Guided Sample \textit{ICI} Learning Module (\textbf{\textit{SICI}}) module is proposed to extract the discriminative feature and ICI of each modality of each input sample under the guidance of Global \textit{ICI}. Additionally, the Sample-Adaptive Cross-Modal Information Compensation module (\textbf{\textit{CMIC}}) is proposed to enhance the discriminative power further and mitigate the impact of \textit{ICI} across modalities in a reliable and interpretable way.

% Based on the global nature (e.g., sensor failures causing identically distributed noise) and sample-specific cases (e.g., a misaligned sample due to data collection errors) of noise, a two-stage sample-adaptive \textit{ICI} removal pipeline with global information enhancement is established. In the first stage, MICINet introduces the Global \textit{ICI} Learning Module (\textbf{\textit{GICI}}) to learn the global \textit{ICI} of the data. Additionally, a Global-Guided Sample \textit{ICI} Learning Module (\textbf{\textit{SICI}}) module is proposed to remove modality-specific noise and a portion of cross-modality noise under the guidance of Global \textit{ICI}. In the second stage, the Sample-Adaptive Cross-Modal Information Compensation module (\textbf{\textit{CMIC}}) is introduced to further remove cross-modality noise by information compensation and discriminative power enhancement across modalities.
% MICINet first considers \textit{ICI} comprehensively from global and sample-specific perspectives. The reason is that noise may exhibit global consistency, such as a faulty sensor adding uniformly distributed noise to all samples, or it may vary by sample, such as mislabeling causing misalignment in a particular sample. Subsequently, MICINet proposes a two-stage pipeline that learns \textit{ICI} at global and sample levels and guides the sample-level adaptive \textit{ICI} removal using global information. In the first stage, MICINet introduces the Global \textit{ICI} Learning Module (\textbf{\textit{GICI}}) to learn the global \textit{ICI} of each modality using all samples. Meanwhile, it designs the Global-Guided Sample \textit{ICI} Learning Module (\textbf{\textit{SICI}}), reliably removing global-level \textit{ICI} from each sample and enhancing discriminative power leveraging the learned global \textit{ICI}. In the second stage, the Sample-Adaptive Cross-Modal Information Compensation module (\textbf{\textit{CMIC}}) is introduced to remove \textit{ICI} and unify discriminative power across modalities at the sample level.

% Specifically, the \textbf{\textit{GICI}} thoroughly considers the differences in \textit{ICI} distribution and confusion degree between different class pairs in each modality and designs a novel dual-path structure for learning of all class pairs. 

% Specifically, to enhance the reliability of global \textit{ICI} learning, the design of \textit{\textbf{GICI}} takes full account of the distinct \textit{ICI} distribution and degrees of confusion among different class pairs. A novel dual-path structure is proposed to extract the shared information between any two classes as the \textit{ICI} distribution between them and estimate their degree of confusion. It then employs the learned \textit{ICI} distribution and confusion degree of different class pairs to construct a Gaussian Mixture Model (GMM) for modeling the global \textit{ICI}. The \textbf{\textit{SICI}} reliably separates discriminative features and \textit{ICI} in each modality of an input sample by pushing discriminative features away from global \textit{ICI}. The \textbf{\textit{CMIC}} first performs interpretable cross-modality information queries by exploiting the complementary relationship between discriminative features and \textit{ICI}. Concretely, for each modality, its \textit{ICI} is used to obtain enhancement information from the discriminative features of other modalities via an attention mechanism. Subsequently, the relative discriminative power is adopted to promote reliability by integrating the obtained information from all other modalities based on their qualities, and the integrated information is then used to enhance the discriminative feature of that modality.
Specifically, in the first stage, MICINet focuses on global \textit{ICI} removal. The \textbf{\textit{Global \textit{ICI} Learning module (GICI)}} is first introduced to learn the global \textit{ICI} distribution of the data. A novel dual-path structure is proposed to extract the shared information between any two classes as their \textit{ICI} distribution and estimate their confusion degree. It then employs the learned \textit{ICI} distribution and confusion degree of different class pairs to construct a Gaussian Mixture Model (GMM) for modeling the global \textit{ICI} distribution. Subsequently, the \textbf{\textit{Global-Guided Sample \textit{ICI} Learning module (SICI)}} achieves reliable global-level \textit{ICI} removal for each sample by pushing the extracted discriminative features of each modality away from the global \textit{ICI} distribution. In the second stage, the individual \textit{ICI} is further eliminated via the proposed \textbf{\textit{Sample-Adaptive Cross-Modality Information Compensation module (CMIC)}}. \textbf{\textit{CMIC}} first performs an interpretable cross-modality compensatory information query by exploiting the complementary relationship between discriminative features and \textit{ICI}. Concretely, for each modality, its \textit{ICI} is used to obtain information from the discriminative features of other modalities via an attention mechanism. Then, the relative discriminative power is adopted to reliably integrate the obtained information from all other modalities based on their relative qualities to compensate for the information incompleteness of each modality.


% The \textbf{\textit{CMIC}} further achieves noise removal by compensating for the information disparities between modalities. The complementary relationship between discriminative features and \textit{ICI} is exploited to perform interpretable cross-modality compensation. Concretely, the \textit{ICI} of each modality is used to query useful information from the discriminative features of other modalities via an attention mechanism. Then, the relative discriminative power is proposed to integrate the queried information from all the other modalities based on their qualities before they are used to enhance the discriminative feature of a certain modality. 

% 全局层面ICI的特性：类别对间差异性
% 样本层面ICI的特性：特征层面及模态层面差异性

% The \textbf{\textit{CMIC}} exploits the complementary between discriminative features and \textit{ICI} for interpretable cross-modality information retrieval and adaptively enhances modalities based on their relative discriminative power. In this way, MICINet reliably removes modality-specific and cross-modality noise within samples simultaneously by removing \textit{ICI} and enhancing discriminative power across modalities. By incorporating global information into the sample-adaptive process, the reliability of noise removal is significantly enhanced.

The contributions of this paper can be summarized as:
\begin{itemize}
    \item The proposed MICINet is the first method to address the scenario where modality-specific and cross-modality noise coexist. Both types of noise are unified and simultaneously removed under the proposed concept of Inter-class Confusing Information (\textit{ICI}). Experiments show that MICINet significantly outperforms other state-of-the-art multimodal classification methods in accuracy and reliability under different noise settings.
    \item MICINet reliably removes \textit{ICI} by comprehensively considering it at both the global and individual levels and establishing a two-stage removal pipeline based on their respective variable characteristics.
    \item A novel dual-path network is designed for global-level \textit{ICI} learning by characterizing diverse \textit{ICI} distributions and confusion degrees between different class pairs. An interpretable information compensation with relative quality estimation across modalities is proposed to reliably remove individual-level \textit{ICI} based on the variable information incompleteness and quality of modalities across samples.
    % A two-stage pipeline is proposed by taking \textit{ICI} removal at both global and individual levels into account.
    % \item MICINet introduces a Global \textit{ICI} Learning Module (\textit{\textbf{GICI}}). This module accounts for the varying \textit{ICI} distributions and confusion degrees across different class pairs. A novel dual-path network is designed to achieve effective learning among all class pairs using a single network structure.
    % \item The proposed Sample-Adaptive Cross-Modality Information Compensation (\textbf{\textit{CMIC}}) achieves interpretable cross-modality information compensation by leveraging the complementary relationship between discriminative features and \textit{ICI}. The relative discriminative power improves the reliability of \textit{\textbf{CMIC}} by considering the variation of modality quality caused by \textit{ICI} at both global and individual levels.
\end{itemize}


% Many multimodal tasks rely highly on well-aligned multimodal data. However, in real-world scenarios, multimodal instances often contain weakly aligned or unaligned samples \cite{zhang2019weakly,changpinyo2021conceptual}.
% a phenomenon also referred to as \textbf{\textit{semantic-level noise}} \cite{zhang2024multimodal}. 

% However, in real-world scenarios, multimodal data often contain noise, generally categorized into two types: low-level noise and semantic-level noise \cite{zhang2024multimodal}. The former refers to noise introduced by sensor errors \cite{cheng2019noise}, environmental interference \cite{yang2024test}, transmission losses, and so on. The latter pertains to weakly aligned or misaligned samples within multimodal instances \cite{zhang2019weakly,changpinyo2021conceptual}.

% However, in real-world scenarios, multimodal instances often contain weakly aligned or entirely unaligned samples \cite{zhang2019weakly,changpinyo2021conceptual}, a phenomenon also referred to as \textbf{\textit{semantic-level noise}} \cite{zhang2024multimodal}. 
% Many multimodal tasks highly rely on well-aligned multimodal data. Unfortunately, in real-world scenarios, multimodal instances often exhibit weakly aligned or entirely unaligned samples, a phenomenon commonly referred to as \textbf{\textit{semantic-level noise}}. 
% Weak or unaligned data adversely affects the reliability and robustness of multimodal models, particularly in safety-critical domains such as medicine, finance, and autonomous driving. Although many methods delve into enhancing the reliability of multimodal learning \cite{han2020trusted,han2022trusted,geng2021uncertainty,han2022multimodal,zhou2023calm,zou2023dpnet,zhang2023provable,cao2024predictive}, most of them cannot distinguish unalignment or realign multimodal instances. In contrast, some unalignment-aware methods focus on detecting or realigning unaligned instances, which aids in reliable learning. Representative methods include CAT \cite{radenovic2023filtering}, which uses specific rules to select high-quality data. NCR \cite{huang2021learning}, ALBEF \cite{li2021align}, and BLIP \cite{li2022blip} employ sophisticated models to filter or rectify unaligned data. NLIP \cite{huang2023nlip}, OSCAR \cite{li2020oscar}, and MMCL \cite{nakada2023understanding} introduce noise-robust regularization terms to mitigate the impact of unaligned data. SMILE \cite{zeng2023semantic} takes a global approach to learning cross-modal semantic consistency, enabling robust learning even without aligned sample pairs. 

% However, due to sensor errors, environmental influences, and transmission losses, multimodal data tends to contain diverse and varying noise. The noise can shift the original semantics of samples and lead to varying sample quality, consequently introducing biases in learning inter-modal alignment relationships and assessing data alignment. Despite the effectiveness of unalignment-aware methods in enhancing multimodal robustness, they either lack a global distribution perspective or sample-level adaptability. As a result, they struggle to handle the bias effectively brought by multimodal noise. Many methods, including but not limited to NCR, ALBEF, and BLIP, focus on sample-level alignment within each instance, overlooking the global sample distribution. Therefore, they cannot detect semantic shifts of samples relative to the overall distribution, leading to unreliable alignment assessment. Although SMILE adopts a global perspective, it lacks dynamic adaptability to variable noisy samples, limiting its reliability in learning inter-modal alignment and semantic invariance. Furthermore, many methods focus on specific modalities and lack generalizability across different scenarios. Thus, an important question is raised: \textbf{\textit{how to design a general method with both a global perspective and sample-level adaptability to enhance the reliability of multimodal classification with unaligned data?}}

% Global perspective methods, such as SMILE, lack adaptability to the variability of samples introduced by low-level noise, which limits their reliability in learning semantic alignment across modalities.
% Despite the effectiveness of unalignment-aware methods in enhancing multimodal robustness, they share some limitations requiring further investigation. First, many methods focus on specific modalities and lack generalizability across different scenarios. Second, these methods struggle to effectively handle the bias induced by low-level noise while dealing with semantic-level noise. Low-level noise can alter or even dominance the original semantics of a sample, causing semantic shifts. These shifts lead to biased or incorrect judgments of alignment between samples. Many methods, including but not limited to NCR, ALBEF, and BLIP, focus on intra-instance relationships, overlooking the global sample distribution. As a result, they cannot detect semantic shifts relative to the overall distribution, leading to unreliable alignment judgments. Global perspective methods, such as SMILE, lack adaptability to the variability of samples introduced by low-level noise, which limits their reliability in learning semantic alignment across modalities. This raises an important question: how to design a general method with both a global perspective and sample-level adaptability to enhance the reliability of multimodal classification under semantic-level noise?


% fail to account for the global distribution of samples, focusing instead on relationships within individual instances. 

% As a result, they cannot detect semantic shifts relative to the overall distribution, leading to unreliable alignment judgments. Global perspective methods, such as SMILE, lack adaptability to the variability introduced by low-level noise, which limits their reliability in learning semantic alignment across modalities.

% semantic-level noise misjudgments caused by low-level noise. 


% \IEEEPARstart{W}{ith} the rapid advancement of various sensors and related technologies, an increasing amount of multimodal data is now more readily accessible and widely utilized to enhance the performance and efficiency of multiple applications. For instance, developing single-cell multimodal sequencing technologies enables more interpretable descriptions and studies of individual cells through multiple modalities, such as DNA, mRNA, and miRNA. Autonomous driving systems achieve safer operations by integrating data from various sensors, and community-based image-text posts are employed for precise recommendations. However, in real-world scenarios, factors such as environmental conditions, sensor states, human intervention, and inherent differences in information content across modalities often result in variations in the quality, noise levels, or even data shift of different modalities for each sample. This gives rise to weakly aligned or unaligned multimodal pairs, a phenomenon also referred to as \textbf{\textit{semantic-level noise}}. Semantic-level noise adversely affects the reliability and robustness of multimodal models, particularly in safety-critical domains such as medicine, finance, and autonomous driving.

% Many existing studies have delved into reliable multimodal learning on data with semantic-level noise. For instance, CAT use specific rules to select high-quality data. NCR, ALBEF, and BLIP employ sophisticated models to filter or rectify samples with semantic-level noise. NLIP, OSCAR, and MMCL introduce noise-robust regularization terms to mitigate the impact of semantic-level noise. SMILE takes a global approach to learn cross-modal semantic consistency, enabling robust learning without aligned sample pairs. Despite their effectiveness, these methods share some limitations requiring further investigation. First, many approaches (e.g., references) are designed for specific modality and lack generalizability across modalities. Second, existing methods often lack either a global distribution perspective or dynamic adaptability at the sample level. Many (e.g., references) focus on sample-pair-level similarity measures to determine cross-modal alignment. However, real-world scenarios frequently involve sample pairs with instance-level noise—where the instance as a whole deviates from the normal class distribution, yet its modalities retain semantic consistency. For example, during heavy rain in autonomous driving, both the camera and microphone capture samples dominated by rain-induced noise, maintaining high cross-modal consistency but deviating from the normal distribution. Sample-pair-level methods, due to insufficient global distribution information, may misclassify such instances as high-quality, well-aligned samples, compromising multimodal learning robustness. Conversely, some methods with a global perspective can effectively handle instance-level noisy sample pairs and even learn without paired samples. However, these approaches often lack sufficient sample-level adaptability to account for dynamically varying sample quality in real-world scenarios, leading to suboptimal robustness. This raises an important question: how to design a generalized method with both a global perspective and instance-level adaptability to enhance the reliability of multimodal classification under semantic-level noise?
% This paper offers a general perspective of semantic-level removal by aligning the discriminative power across modalities

% This paper provides a general paradigm for semantic-level noise removal from the novel perspective of aligning the discriminative capabilities of different modalities. 


% This paper addresses the aforementioned question by aligning the discriminative power across modalities. In classification tasks, regardless of the specific application, misalignment among modalities manifests as a common issue: differences in the discriminative power and classification confidence across modalities. Thus, aligning the discriminative power of different modalities can be a practical and generalized way to handle semantic-level noise. To achieve alignment of discriminative power across modalities, this paper focuses on the underlying factor that degrades discriminative power: \textbf{inter-class confusion information} (ICI). ICI refers to features that make it hard to distinguish different classes due to factors such as the inherent distribution of the data, noise, and so on. Differences in discriminative power are attributed to different ICI and the degree of confusion across modalities. 

% Based on this insight, we propose a novel two-stage cross-modal discriminative power alignment framework leveraging ICI. In the first stage, the framework learns and removes ICI in each modality. In the second stage, it performs inter-modal information compensation utilizing the learned ICI to achieve cross-modal alignment of discriminative power. Specifically, the framework comprises three key modules. The \textit{\textbf{instance-level ICI learning module}} separates high- and low-quality features of each modality within an instance, and then extract ICI from low-quality features. The \textit{\textbf{global ICI learning module}} utilizes low-quality features from all samples to learn confusion information distributions between different pairs of classes. The distributions are then dynamically mixed to form the global ICI distribution based on confusion degrees and used to guide the instance-level learning of ICI. The \textit{\textbf{ICI-based cross-modal information compensation module}} utilizes the ICI of each modality within an instance to extract compensatory information from other modalities. Instance-adaptive cross-modal compensation is then performed based on informativeness differences between modalities. By integrating global distribution information with instance-level adaptive cross-modal alignment, this method provides a unified approach from the perspective of discriminative power alignment. Experiments on four datasets demonstrate that our method outperforms existing approaches for reliable multimodal classification and methods targeting semantic-level noise. Additional results show that our approach effectively addresses both semantic-level and instance-level noise, exhibiting superior robustness. 
% This paper provides a general paradigm for multimodal alignment learning from the novel perspective of aligning the discriminative power of different modalities. In classification tasks, highly aligned semantics between modalities manifest as strong consistency in their discriminative power and classification confidence. Therefore, aligning each modality at high discriminative power can be a general and effective approach to achieving reliable alignment and learning for unaligned multimodal data. To this end, a novel two-stage pipeline is proposed. The first stage enhances the discriminative power of each modality, while the second stage aligns modalities at a high level of discriminative power. For discriminative power enhancement, this paper takes an inverse approach by removing factors that would lead to a decrease in discriminative power, namely, the Inter-class Confusion Information (ICI). ICI refers to features that make it hard to distinguish different classes, which may induced by inherent shifts of data, noise, and so on. To avoid learning biased and unreliable features at the sample level, the model designs a global ICI distribution learning module and utilizes the learned global ICI distribution to guide sample-level feature extraction. For discriminative power alignment, a novel sample-level adaptive cross-modality information compensation strategy is adopted. This strategy efficiently leverages the exclusivity between discriminative features and ICI, allowing for the interpretable acquisition of complementary information across modalities, thereby achieving the alignment of modal information and discriminative power. By integrating global distribution information with sample-level adaptability, this method provides a reliable and general paradigm for unbiased multimodal alignment from the perspective of discriminative power. Experiments on four datasets demonstrate that our method outperforms existing approaches for reliable multimodal classification and methods targeting semantic-level noise. Additional results show that our approach effectively addresses both semantic-level and instance-level noise, exhibiting superior robustness. 

% The contributions of this paper can be summarized as:

% Some methods developed rules or network modules to filter weakly aligned or unaligned multimodal pairs on the sample level \cite{ref1,ref2}. However, in many applications, privacy concerns or the high cost of data collection make obtaining a large number of sample pairs prohibitively expensive. On small datasets, further filtering of samples is often impractical. Additionally, in real-world scenarios, all modalities of a sample may be uniformly degraded. For instance, in autonomous driving under heavy rain, both images and audio collected by cameras and microphones may be dominated by rain-related noise. Similarly, when a pathological slide is damaged, the extracted DNA and mRNA data are also correspondingly affected. In such cases, filtering methods on the sample level may retain these low-quality samples as well-aligned due to the semantic similarity of the dominant noise across modalities. These samples undermine multimodal learning and fail to ensure the alignment of the underlying useful information (masked by noise) across modalities. Some methods rectify labels or modalities of unaligned sample pairs. 


\section{Related Works}
This section briefly reviews some related works on multimodal learning, modality-specific noise removal, and cross-modality noise removal.
\subsection{Multimodal Learning}
Multimodal learning integrates information from different types of data, achieving effective representation learning with a wide range of applications in real-world scenarios \cite{baltruvsaitis2018multimodal,ramachandram2017deep,wang2020deep}. Based on the fusion strategy, multimodal learning can be classified into early fusion \cite{poria2015deep}, intermediate fusion \cite{tsai2019multimodal,hang2021multi,lee2021variational,kiela2019supervised,huang2021makes,huang2020multimodal,hu2021unit,hong2020more,arevalo2017gated}, and decision fusion \cite{wang2021mogonet,subedar2019uncertainty,simonyan2014two,natarajan2012multimodal}. Early fusion directly integrates various modalities at the data level to utilize the correlation and interaction between low-level features \cite{poria2015deep}. Still, it cannot fully exploit the complementary between multiple modal data and may suffer from information redundancy. Intermediate fusion, widely adopted in multimodal learning, integrates modalities at the intermediate feature level, capturing the complementary relationships between high-level features of each modality \cite{tsai2019multimodal,hang2021multi,lee2021variational,kiela2019supervised,huang2021makes,huang2020multimodal,hu2021unit,hong2020more,arevalo2017gated}. Decision fusion, involves extracting features from each modality and obtaining predictions from individual classifiers for each modality, which are then integrated to obtain the final multi-modal prediction result \cite{wang2021mogonet,subedar2019uncertainty,simonyan2014two,natarajan2012multimodal}.

\subsection{Modality-Specific Noise Removal}
Many studies have developed reliable multimodal classification methods to remove modality-specific noise. Federici et al. \cite{federici2020learning} proposed the multi-view information bottleneck, which improved the generalization and robustness of multi-view learning by retaining information shared by each view. Han et al. 
 \cite{han2020trusted} parameterized the evidence of different modality features using Dirichlet distribution and fused each modality at the evidence level using Dempster-Shafer theory. Geng et al. \cite{geng2021uncertainty} proposed the DUA-Nets, which achieved uncertainty-based multimodal representation learning through reconstruction. Han et al. \cite{han2022multimodal} modeled informativeness at the feature and modality levels, achieving trustworthy multimodal feature fusion. Zhang et al. \cite{zhang2023provable} achieved more robust and generalized multimodal fusion by dynamically assigning weights to each modality based on uncertainty estimation. Zheng
et al. \cite{zheng2023multi} achieved trustworthy multimodal classification via integrating feature and label-level confidence. Zou et al. \cite{zou2023dpnet} proposed a novel dynamic poly-attention Network that integrated global structural information for trustworthy multimodal classification. Zhou et al. \cite{zhou2023calm} introduced a trustworthy multi-view classification framework by enhancing multi-view encoding and confidence-aware fusion. Cao et al. \cite{cao2024predictive} proposed the Predictive Dynamic Fusion method, which provides theoretical guarantees for reducing the upper bound of generalization error in dynamic multimodal fusion.

\subsection{Cross-Modality Noise Removal}
Many studies have focused on identifying or realigning misaligned sample pairs or achieving reliable multimodal learning using incomplete sample pairs. These methods can be categorized into rules-based filtering, model-based rectifying, and noise-robust regularization. Rules-based filtering refers to methods for data cleaning through the design of a set of rules. Representative works include those by Radenovic et al. \cite{radenovic2023filtering}, Gadre et al. \cite{gadre2024datacomp}, and Sharma et al. \cite{sharma2018conceptual}. Model-based rectifying methods design models to filter or rectify misaligned samples, such as NCR \cite{huang2021learning}, ALBEF \cite{li2021align}, and BLIP \cite{li2022blip}. Noise robust regularization methods mitigate the impact of misaligned samples on the model by designing regularization, such as NLIP \cite{huang2023nlip} and OSCAR \cite{li2020oscar}. Some studies also design reliable learning methods that do not rely on paired samples, such as SMILE \cite{zeng2023semantic}.


% \section{Preliminary}
% % In this section, a formal definition of the semantic-level noise and instance-level noise problems encountered in reliable multimodal classification is first demonstrated in Section \ref{sec:prob_form}. Then, a detailed description of the proposed framework is presented in Section \ref{sec:prop_frame}.

% This section first provides the formulation of the multimodal classification task in Section \ref{sec:task-def}. Subsequently, a formal definition of the cross-modality and modality-specific noise is provided in Section \ref{sec:noise}. Finally, the proposed inter-class confusing information ($ICI$) is introduced in Section \ref{sec:ici}.

% \subsection{Task Formulation}\label{sec:task-def}
% Given a dataset $\mathcal{D}=(\mathcal{X},\mathcal{Y})$ consists of $N$ multimodal samples $\mathcal{X}=\{x_i\}_{i=1}^N$ from $C$ classes, along with their corresponding ground truth labels $\mathcal{Y} = \{y_i\in\mathbb{R}^C\}_{i=1}^N$. Each multimodal sample $x_i=\{x_i^m\in\mathbb{R}^{d^m}\}_{m=1}^M, i\in[1,N]$ contains samples from $M$ modalities, where $d^m$ is the dimension of the $m$-th modality. Multimodal classification methods aim to learn a neural network that maps each sample $x_i$ to its corresponding class label $y_i$. 
% % However, the dataset $\mathcal{D}$ contains semantic-level noise and instance-level noise, which undermine the reliability of multimodal classification.

% \subsection{Multimodal Noise}\label{sec:noise} 
% The multimodal noise can be roughly categorized into modality-specific and cross-modality noise. The following paragraphs define these two types of noise and briefly illustrate their combined impact on model reliability.
% % Semantic-level noise refers to instances in the dataset where the samples from different modalities exhibit misalignment or weak alignment. 
% \subsubsection{Cross-Modality Noise}
% Cross-modality noise refers to the weak or unaligned multimodal samples in a dataset. Formally, the dataset $\mathcal{D}=\mathcal{\Tilde{D}}\cup\mathcal{\Bar{D}}$ consists of two subsets $\mathcal{\Tilde{D}}$ and $\mathcal{\Bar{D}}$, and $\mathcal{\Tilde{D}}\cap\mathcal{\Bar{D}}=\emptyset$. The subset $\mathcal{\Tilde{D}}=\{\Tilde{x}_i\}_{i=1}^{\Tilde{N}}$ and $\mathcal{\Bar{D}}=\{\Bar{x}_i\}_{i=1}^{\Bar{N}}$ contain all samples with and without cross-modality noise, and it meets $N=\Tilde{N}+\Bar{N}$. A function $\mathcal{J}$ is defined to judge whether an input sample has cross-modality noise (Positive) or not (Negative), which is formulated as:
% \begin{equation}
%     \mathcal{J}(x_i) = \begin{cases} 
% Positive & \text{if } \Sigma_i<M(M-1) \\
% Negative & \text{if } \Sigma_i=M(M-1), 
% \end{cases}
% \label{eq:judge-sn}
% \end{equation}
% where $\Sigma_i=\sum_{m_1}^M{\sum_{m_2\neq m_1}^M{\mathcal{S}(x_i^{m_1},x_i^{m_2})}}$. $\mathcal{S}(\cdot,\cdot)$ is an indicator function that equals 1 if and only if the two feature vectors are semantically aligned. $M$ is the number of modalities. Thus, for all samples with cross-modality noise $\Tilde{x}_i=\{\Tilde{x}_i^m\}_{m=1}^M \in \mathcal{\Tilde{D}}$: $\mathcal{J}(\Tilde{x}_i)=Positive$. For all instances without unaligned samples $\Bar{x}_i=\{\Bar{x}_i^m\}_{m=1}^M \in \mathcal{\Bar{D}}$: $\mathcal{J}(\Bar{x}_i)=Negative$.
% % The output $SN$ represents the input instance containing semantic-level noise while $w/o\, SN$ represents the input instance without semantic-level noise. 
% \subsubsection{Modality-Specific Noise}
% Modality-specific noise is attributed to noise introduced individually to each modality due to factors such as sensor malfunctions, environmental influences, and transmission issues. Formally, denote the modality-specific noise corresponding to modality $m$ as $\epsilon^m$. The set of all modalities $\mathcal{M}=\mathcal{M}_{\mathcal{N}}\cup \mathcal{M}_{\mathcal{F}}$ consists of two subsets $\mathcal{M}_{\mathcal{N}}$ and $\mathcal{M}_{\mathcal{F}}$, and $\mathcal{M}_{\mathcal{N}}\cap \mathcal{M}_{\mathcal{F}}=\emptyset$. The subset $\mathcal{M}_{\mathcal{N}}$ contains all modalities that are corrupted by modality-specific noise, and the subset $\mathcal{M}_{\mathcal{N}}$ contains all modalities without modality-specific noise. For a multimodal sample $x_i=\{x_i^m\}_{m=1}^M$, define its corruption by modality-specific noise as $(x_i)^{\prime}=\{(x_i^m)^{\prime}\}_{m=1}^M$. $(x_i^m)^{\prime}$ can be formulated as:
% \begin{equation}
%     (x_i^m)^{\prime} = \begin{cases} 
% x_i^m+\epsilon_i^m & \text{if } m\in \mathcal{M}_{\mathcal{N}} \\
% x_i^m & \text{if } m\in \mathcal{M}_{\mathcal{F}}.
% \end{cases}
% \label{eq:msn}
% \end{equation}

% \subsubsection{Coexistence of Two Types of Noise}
% The coexistence of modality-specific and cross-modality noise is common in real-world scenarios. For instance, a multimodal sample with cross-modality noise $\Tilde{x}_i\in\mathcal{\Tilde{D}}$ is corrupted by modality-specific noise: $(\Tilde{x}_i^m)^{\prime}=MSN(\Tilde{x}_i^m)$, where $MSN(\cdot)$ refers to the modality-specific noise corruption formulated in Equation \ref{eq:msn}. Methods focusing solely on modality-specific noise removal do not discern or realign samples affected by cross-modality noise using rules such as Equation \ref{eq:judge-sn}. Consequently, they fail to learn reliable inter-modal alignment relationships. For methods that focus on cross-modality noise removal, their ability to judge sample alignment and to learn alignment relationships is also compromised by modality-specific noise. In other words, the results of $\mathcal{J}(x_i)$ and $\mathcal{J}((x_i)^{\prime})$ cannot be guaranteed to be consistent, thereby leading to the presence of false positives or false negatives.

% Equation \ref{eq:judge-sn} commonly serves as a metric in sample alignment assessment or learning. However, in real-world scenarios, multimodal data tends to contain noise, which introduces bias and degrades the reliability of this metric. Specifically, for a noise-free instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$, certain modality samples may be contaminated by multimodal noise $\epsilon_i=\{\epsilon_i^m\sim\mathcal{N}^m\}_{m=1}^M$. The contaminated instance is denoted as $\textbf{x}_i^{\prime}=\{(\textbf{x}_i^m)^{\prime}\}_{m=1}^M$, and can be expressed as:
% \begin{equation}
%     (\textbf{x}_i^m)^{\prime} = \begin{cases} 
% \textbf{x}_i^m+\epsilon_i^m & \text{if } m\in \mathcal{M}_{\mathcal{N}} \\
% \textbf{x}_i^m & \text{if } m\in\mathcal{M}-\mathcal{M}_{\mathcal{N}}.
% \end{cases}
% \label{eq:judge-sn}
% \end{equation}
% $\mathcal{M}=\{1,2,...,M\}$ is the index set of all modalities, $\mathcal{M}_{\mathcal{N}}$ is the index set of all noise corrupted modalities. Equation \ref{eq:judge-sn} is used to determine whether the instance $\textbf{x}_i^{\prime}$ contains unaligned samples by $\mathcal{J}(\textbf{x}_i^{\prime})$. However, $\mathcal{J}(\textbf{x}_i^{\prime})\not\equiv\mathcal{J}(\textbf{x}_i)$, i.e., aligned noisy samples do not guarantee alignment of their original semantics before noise contamination, and vice versa. Especially when an instance is contaminated by instance-level noise, all modalities are dominated by the same semantic noise, i.e., 
% \begin{align}
%     (\mathcal{J}(\epsilon_i)&=Align) \land (\mathcal{M}_{\mathcal{N}}=\mathcal{M}) \land (\epsilon_i^m\gg \textbf{x}_i^m (\forall m\in [1,M])) \notag \\
%     &\implies \mathcal{J}(\textbf{x}_i^{\prime})=\mathcal{J}(\epsilon_i)=Align \perp \mathcal{J}(\textbf{x}_i).
% \end{align}
% Unfortunately, this scenario occurs frequently, such as in autonomous driving during heavy rain, where the semantics of the rain dominates sensors like cameras and microphones. In such cases, the original semantics of the sample (e.g., people or obstacles captured by the camera, crowd sounds collected by the microphone) are largely obscured. In such cases, the alignment of the original meaningful semantics cannot be correctly evaluated or learned; instead, the strong and aligned noise semantics misled the model, resulting in unreliable outcomes.


% In real-world scenarios, multimodal data tends to contain noise, which introduces bias and degrades the reliability of sample alignment assessment and alignment relationship learning. Specifically, for the noise-free multimodal instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$, some of its samples may corrupted by multimodal noise $\epsilon_i=\{\epsilon_i^m\sim\mathcal{N}^m\}_{m=1}^M$ and becomes $\textbf{x}_i^{\prime}=\{\textbf{x}_i^{m\prime}\}_{m=1}^M$. 

% Formally, the multimodal noise $\epsilon_i=\{\epsilon^m_i\}_{m=1}^M\sim\mathcal{N}$ following the noise distribution $\mathcal{N}$ corrupts the noise-free instance $\textbf{x}_i$ by: $(\textbf{x}_i^m)^{\prime}=\textbf{x}_i^m+\epsilon_i^m, \exists m\in M$. 

% \subsubsection{Bias in alignment assessment}
% In real-world scenarios, it is common for an entire instance to be corrupted by noise, such as in autonomous driving applications mentioned in the Introduction. In such cases, all modality samples within the instance are infused with highly consistent noise semantics $\epsilon_i=\{\epsilon^m_i\}_{m=1}^M$, i.e., $\mathcal{J}(\epsilon_i)=w/o\,SN$. $\epsilon_i$ represents the instance-level noise affecting the $i$-th instance, while $\epsilon_i^m$ denotes the noise contaminating the $m$-th modal sample within the instance. 

% Existing semantic-level noise-aware methods primarily rely on approaches based on Equation \ref{eq:judge-sn} to determine semantic consistency between sample pairs. However, the presence of instance-level noise can introduce biases in semantic consistency judgments between sample pairs. For an instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M\in\mathcal{X}$ corrupted by instance-level noise $\epsilon_i$: $\textbf{x}_i^m=(\textbf{x}_i^m)^{\prime}+\epsilon_i^m, \forall m\in [1,M]$. $(\textbf{x}_i^m)^{\prime}$ is the sample feature before polluted by instance-level noise. This introduces two issues: (i) $\mathcal{J}(\textbf{x}_i)=w/o\,SN$ does not guarantee that $\mathcal{J}((\textbf{x}_i)^{\prime})=w/o\,SN$, and vice versa. This leads to the occurrence of false positive or false negative instances in the semantic level noise judgement. (ii) When $\epsilon_i$ becomes dominant, the original feature $(\textbf{x}_i)'$ may be overwhelmed, causing the instance $\textbf{x}_i$ to satisfy $\mathcal{J}(\textbf{x}_i)=w/o\,SN$ and be retained as a clean sample with aligned modalities. However, these instances still carry strong noise, thereby adversely affecting the robustness of multimodal classification. Therefore, it is crucial to equip semantic-level noise-aware methods with the capability to perceive instance-level noise.

% \subsection{Inter-class Confusing Information ($ICI$)}\label{sec:ici}
% % It is found that both modality-specific and cross-modality noise introduce confusion information, features that fail to distinguish different classes, into the modalities of samples and lead to a decrease and inconsistency in the discriminative power across modalities. Therefore, t
% This paper proposes the concept of Inter-class Confusing Information (\textit{ICI}) to unify modality-specific and cross-modality noise. As mentioned in Section \ref{sec:intro}, noise exists at the global and individual levels. Therefore, the \textit{ICI} is analyzed on both the global and individual levels. Individual-level \textit{ICI} refers to the confusing information specific to individual samples, such as the misalignment of a certain sample caused by data collection errors. Global-level \textit{ICI} refers to the confusing information that is not specific to any particular sample, such as sensor malfunctioning that introduces noise with similar distribution to all samples. In this paper, for the dataset $\mathcal{D}$ containing samples with $M$ modalities from $C$ classes, the global-level \textit{ICI} is defined as $\mathcal{P}_g=\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$, where $\mathcal{P}_g^{m,c}=Mix(\{\beta^{m,c,c^{\prime}}\cdot\mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C)$. $\mathcal{P}_g^{m,c,c^{\prime}}\sim \mathcal{N}(\mu^{m,c,c^{\prime}},\sigma^{m,c,c^{\prime}})$ refers to the distribution of confusing information between the $c$-th and the $c^{\prime}$-th class of the $m$-th modality, and $\beta^{m,c,c^{\prime}}$ refers to their confusion degree. $\mathcal{P}_g^{m,c,c^{\prime}}$ is obtained by extracting the shared information distribution between data of class $c$ and $c^{\prime}$ in the $m$-th modality.

% \subsubsection{Sample level $ICI$}
% Sample-level \textit{ICI} refers to the confusing information specific to individual samples, such as misalignment caused by data collection errors. For an multimodal sample $x_i=\{x_i^m\}_{m=1}^M$, the sample-level $ICI$ is denoted as $ICI_i=\{ICI_i^m\}_{m=1}^M$. 

% provide a general perspective of reliable alignment under multimodal noise. For a noisy multimodal instance $\textbf{x}_i^{\prime}=\{(\textbf{x}_i^m)^{\prime}\}_{m=1}^M$, the features of each modality can be heuristically expressed as: $(\textbf{x}_i^m)^{\prime}=\textbf{h}_i^m+ICI_i^m, \forall m\in[1,M]$. $\textbf{h}_i^m$ refers to discriminative feature of the $m$-th modality sample. $ICI_i^m$ refers to those features that degrade the discriminative power of the sample feature, including noise and other factors causing unalignment. Thus, multimodal noise removal and modality alignment can be simultaneously achieved by separating $\{ICI_i^m\}_{m=1}^M$ from instance samples $\{(\textbf{x}_i^m)^{\prime}\}_{m=1}^M$ and aligning discriminative feature $\{\textbf{h}_i^m\}_{m=1}^M$.

% To reliably learn the sample-level $ICI$, the global $ICI$ distribution of samples from all instances is investigated. Through empirical analysis, the inter-class confusion information varies between different pairs of classes, with each pair exhibiting different levels of confusion. For instance, the confusion information between dogs and wolves is distinct from that between dogs and cats, and the confusion level is higher in the former case. Therefore, the study of the Global $ICI$ distribution must be conducted for each pair of classes individually. Assuming that for all samples in modality $m$, the confusion information distribution between two distinct classes $c$ and $c^{\prime}$ follows a Gaussian distribution: $\mathcal{P}^{m,c,c^{\prime}}=\mathcal{N}(\mu^{m,c,c^{\prime}},\sigma^{m,c,c^{\prime}})$. The confusion level between $c$ and $c^{\prime}$ is $\Tilde{\beta}^{m,c,c^{\prime}}$. Then, the Global $ICI$ distribution of class $c$ in modality $m$ can be intuitively modeled using a Gaussian Mixture Model (GMM) as follows:
% \begin{align}
%     \mathcal{P}_g^{m,c}=Mix(\{\beta^{m,c,c^{\prime}}\cdot\mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C).
% \end{align}
% $\beta^{m,c,c^{\prime}}$ is the normalized $\Tilde{\beta}^{m,c,c^{\prime}}$ in $\{\Tilde{\beta}^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$. Thus, the global $ICI$ distribution with respect to the dataset input $\mathcal{X}$ can be denoted as $\mathcal{P}_g=\{\{\mathcal{P}^{m,c}_g\}_{c=1}^C\}_{m=1}^M$.

% Separating $\{ICI_i^m\}_{m=1}^M$ from instance samples $\{(\textbf{x}_i^m)^{\prime}\}_{m=1}^M$ and aligning discriminative feature $\{\textbf{h}_i^m\}_{m=1}^M$ simultaneously achieves multimodal noise removal and modality alignment, offering a universally applicable modality alignment method in classification scenarios while effectively avoiding noise-induced biases. 


% For the input instance $\textbf{x}_i^{\prime}$ with multimodal noise, 


% to provide a unified perspective of both semantic- and instance-level noise removal via cross modality discriminative power alignment. Given the input instances $\mathcal{X}=\{\textbf{x}_i\}_{i=1}^N$ from $C$ classes, with each containing samples from $M$ modalities, the global $ICI$ and sample-level $ICI$ is introduced as follows.
% \subsubsection{Global $ICI$} Global $ICI$ examines the distribution of features with inter-class confusion from a global perspective of the overall distribution. Specifically, Global $ICI$ is a data structure which can be expressed as: $\mathcal{P}_{g}=\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$. Each element $\mathcal{P}_g^{m,c}$ in $\mathcal{P}_{g}$ represents the distribution of confusion features between class $c$ and all other classes within all samples of the $m$-th modality. The Gaussian Mixture Model (GMM) is employed to model each $\mathcal{P}_g^{m,c}$: 
% \begin{align}
%     \mathcal{P}_g^{m,c}=Mix(\{\beta^{m,c,c^{\prime}}\cdot\mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C).
% \end{align}
% The mixture coefficients $\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ is the estimated degree of confusion of the $m$-th modality between class $c$ and all other classes $c^{\prime}\in [1,C], c^{\prime}\neq c$. $\mathcal{P}_g^{m,c,c^{\prime}}=\mathcal{N}(\mu^{m,c,c^{\prime}},\sigma^{m,c,c^{\prime}})$ is the distribution of confusion information between class $c$ and all other classes $c^{\prime}\in [1,C]$ on the $m$-th modality.

% \subsubsection{Sample-level $ICI$} In the feature vector of each modality's sample for a given instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$, there exists a subset of features that are discriminative and related to the label. In contrast, the sample-level $ICI$ refers to another subset of features that leads to classification confusion. Sample-level $ICI$ can be caused by factors such as the inherent distribution of the samples and the introduction of noise, which degrades the discriminative power of sample's feature.
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{fig/fig_model.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The framework of the proposed Multi-Level Inter-Class Confusing Information Removal Network (MICINet).}
\label{fig_model_arch}
\end{figure*}

\section{The proposed Method} \label{sec:prop_frame}
% To perform robust multimodal classification against semantic-level and instance-level noise, this section introduces the proposed framework. The framework provides a novel and unified paradigm for semantic-level noise removal by instance-adaptive cross-modality discriminative power alignment via the proposed inter-class confusion information ($ICI$). Furthermore, the framework enhances the robustness of semantic-level noise removal and effectively eliminates instance-level noise by incorporating global distribution information into the instance-adaptive process. The following sections first introduce the overall framework including three key modules, and then elaborate on the design of each module.
\subsection{Problem Definition}
Given a dataset $\mathcal{D}=(\mathcal{X},\mathcal{Y})$ consists of $N$ multimodal samples $\mathcal{X}=\{x_i\}_{i=1}^N$ from $C$ classes, along with their corresponding ground truth labels $\mathcal{Y} = \{y_i\in\mathbb{R}^C\}_{i=1}^N$. Each multimodal sample $x_i=\{x_i^m\in\mathbb{R}^{d^m}\}_{m=1}^M, i\in[1,N]$ contains samples from $M$ modalities, where $d^m$ is the dimension of the $m$-th modality. Multimodal classification methods aim to learn a neural network that maps each sample $x_i$ to its corresponding class label $y_i$.

To remove both modality-specific and cross-modal noise in dataset $\mathcal{D}$ for reliable classification, this paper proposes MICINet based on the concept of \textit{ICI}. MICINet is a two-stage \textit{ICI} removal pipeline. The first stage includes the global \textit{ICI} learning (\textit{\textbf{GICI}}) and global-guided sample \textit{ICI} learning (\textit{\textbf{SICI}}) modules, which are elaborated respectively in Section \ref{sec:g-ici} and \ref{sec:sici}. The second stage corresponds to the sample-adaptive cross-modality information compensation module (\textbf{\textit{CMIC}}), which is detailed in Section \ref{sec:cmic}.

% The first stage focuses on eliminating global-level \textit{ICI} and enhancing the discriminative power of each modality of the sample, while the second stage primarily targets sample-level \textit{ICI} removal and unifies cross-modal discriminative power. Three key modules are proposed, including the global \textit{ICI} learning (\textit{\textbf{GICI}}), global-guided sample \textit{ICI} learning (\textit{\textbf{SICI}}), and sample-adaptive cross-modal information compensation modules (\textbf{\textit{CMIC}}). They are elaborated respectively in Secion \ref{sec:g-ici}, \ref{sec:sici}, and \ref{sec:cmic}.

% \subsection{Overall Framework}
% The proposed framework comprises three key modules: the instance-level $ICI$ learning module (\textbf{I-ICI}), the global $ICI$ learning module (\textbf{G-ICI}), and the $ICI$-based cross-modal information compensation module (\textbf{ICI-C}). At the sample level, the \textbf{I-ICI} module extracts the high- and low-quality features of all modalities samples of an input instance and further learns the $ICI$. The \textbf{ICI-C} performs cross-modality information compensation and discriminative power alignment utilizing the high-quality features and $ICI$ of each sample in an instance. From the global perspective, the \textbf{G-ICI} utilizes the low-quality features from all instances and learns the global $ICI$ distribution, which is used to guide the learning of sample-level $ICI$ in the \textbf{I-ICI} module. For clarity, the relationship of the three modules can be formulated as:
% \begin{align}
%     \textbf{h}_i,\textbf{l}_i,ICI_i=\textbf{I-ICI}(\textbf{x}_i), \notag \\
%     \textbf{f}_i=\textbf{ICI-C}(\textbf{h}_i,ICI_i),\notag \\
%     \widetilde{ICI}=\textbf{G-ICI}(\{\textbf{l}_i\}_{i=1}^N),\notag \\
%     \mathop{\arg\min}\limits_{\theta}\mathcal{L}(ICI_i,\widetilde{ICI}).
% \end{align}

% In the following section, the detailed implementation of \textbf{I-ICI} (Section \ref{sec:i-ici}), \textbf{ICI-C} (Section \ref{sec:ici-c}), and \textbf{G-ICI} (Section \ref{sec:g-ici}) will be demonstrated one by one.
% This section provides a detailed introduction to the proposed framework. The framework effectively addresses the aforementioned semantic-level and instance-level noise by performing instance-adaptive cross-modal information compensation and discriminative power alignment, while seamlessly integrating the learned global distribution information into the instance-adaptive process.

% The framework achieves the alignment of discriminative power across modalities through instance-adaptive cross-modal information compensation guided by global distribution information, effectively addressing the aforementioned semantic-level and instance-level noise.

% which consists of the instance-level ICI learning module (\textbf{I-ICI}), the global ICI learning module (\textbf{G-ICI}), and the ICI-based cross-modal information compensation module (\textbf{ICI-C}). The framework 

\subsection{Global ICI Learning Module (\textbf{GICI})} \label{sec:g-ici}
\subsubsection{Definition of Global \textit{ICI} Distribution}
In this paper, the global \textit{ICI} distribution in dataset $\mathcal{D}$ is defined as $\mathcal{P}_g=\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$, where $\mathcal{P}_g^{m,c}=Mix(\{\beta^{m,c,c^{\prime}}\cdot\mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C)$. $\mathcal{P}_g^{m,c,c^{\prime}}\sim \mathcal{N}(\mu^{m,c,c^{\prime}},\sigma^{m,c,c^{\prime}})$ refers to the distribution of confusing information between the $c$-th and the $c^{\prime}$-th class of the $m$-th modality. It is obtained by extracting the shared information distribution between data of class $c$ and $c^{\prime}$ in the $m$-th modality. $\beta^{m,c,c^{\prime}}$ refers to the confusion degree between class $c$ and $c^{\prime}$ in the $m$-th modality. $Mix$ represents the mixture of distributions of confusing information using their respective confusion degree.

\subsubsection{Overview of \textbf{GICI}}
\textbf{\textit{GICI}} is proposed to extract the global $ICI$ distribution $\mathcal{P}_g$. The learning of $\mathcal{P}_g$ consists of two parts: the confusing information distribution $\mathcal{P}_g^{m,c,c^{\prime}}$ between any two classes within each modality and their corresponding confusion degree $\beta^{m,c,c^{\prime}}$. Before learning, the input feature vectors of all samples $\mathcal{X}=\{\{x_i^m\}_{m=1}^M\}_{i=1}^N$ are first organized according to different classes and modalities, yield $\Tilde{\mathcal{X}}=\{\{\Tilde{\mathcal{X}}^{m,c}\}_{m=1}^M\}_{c=1}^C$. $\Tilde{\mathcal{X}}^{m,c}=\{x_i^m\}_{i=1}^{N^{m,c}}$ contains the feature vectors of the $m$-th modality from all samples belonging to the $c$-th class.

\textbf{\textit{GICI}} introduces $M$ modality-specific network structures $\{\textbf{U}^m\}_{m=1}^M$ to learn $\mathcal{P}_g$. Each network $\textbf{U}^m$ employs a novel dual-path to learn the confusing information distribution and estimate the confusion degree between all class pairs within the corresponding modality $m$: 
\begin{align}
   \{\{\mathcal{P}^{m,c,c^{\prime}}_g\}_{c=1}^C\}_{c^{\prime}\neq c}^C, \{\{\beta^{m,c,c^{\prime}}\}_{c=1}^C\}_{c^{\prime}\neq c}^C=\textbf{U}^m(\{\Tilde{\mathcal{X}}^{m,c}\}_{c=1}^C).  
\end{align}
The global \textit{ICI} distribution is then obtained by mixing the distributions $\{\{\mathcal{P}^{m,c,c^{\prime}}_g\}_{c=1}^C\}_{c^{\prime}\neq c}^C$ with their corresponding weights $\{\{\beta^{m,c,c^{\prime}}\}_{c=1}^C\}_{c^{\prime}\neq c}^C$ in each modality. The following paragraphs will introduce the detailed implementation of each network structure $\textbf{U}^m$.

\subsubsection{Detailed Implementation of $\textbf{U}^m$}
As demonstrated in Figure \ref{fig_model_arch}, $\textbf{U}^m$ consists of two encoders $\textbf{E}_{sp}^m, \textbf{E}_{sh}^m$, and a decoder $\textbf{D}$. Both encoders receive inputs from two batches of samples simultaneously. Encoder $\textbf{E}_{sp}^m$ learns the specific information of each batch relative to the other, while encoder $\textbf{E}_{sh}^m$ learns the shared information between the two batches. The decoder $\textbf{D}$ merges the specific and shared information of each batch and reconstructs it, thereby guiding the model's learning.

$\textbf{U}^m$ contains two separate data paths (dual-path). The first data path takes two batches of samples from different classes $c$ and $c^{\prime}$, i.e., $\Tilde{\mathcal{X}}^{m,c}$ and $\Tilde{\mathcal{X}}^{m,c^{\prime}}$ together as input. The encoder $\textbf{E}_{sp}^m$ learns the specific information $\mathcal{P}_{sp}^{m,c,c^{\prime}}$ of class $c$ relative to class $c^{\prime}$, and the specific information $\mathcal{P}_{sp}^{m,c^{\prime},c}$ of class $c^{\prime}$ relative to class $c$: $\mathcal{P}_{sp}^{m,c,c^{\prime}},\mathcal{P}_{sp}^{m,c^{\prime},c}=\textbf{E}_{sp}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}})$. The encoder $\textbf{E}_{sh}^m$ learns the shared information $\mathcal{P}_{sh}^{m,c,c^{\prime}}$ between two classes: $\mathcal{P}_{sh}^{m,c,c^{\prime}}=\textbf{E}_{sh}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}})$. The shared information distribution $\mathcal{P}_{sh}^{m,c,c^{\prime}}$ between $c$ and $c^{\prime}$ is the desired \textit{ICI} distribution $\mathcal{P}_g^{m,c,c^{\prime}}$ between them:
\begin{align}
    \mathcal{P}_g^{m,c,c^{\prime}} = \mathcal{P}_{sh}^{m,c,c^{\prime}}.
\end{align}

The second data path takes two batches of samples from the same class $c$ together as input and computes using only the encoder $\textbf{E}_{sh}^m$, resulting in a probability distribution $\mathcal{P}^{m,c,c}_{sh}$: $\mathcal{P}^{m,c,c}_{sh}=\textbf{E}_{sh}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c})$. Subsequently, the confusion degree between class $c$ and other classes $c^{\prime}$: $\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ is estimated by comparing $\{\mathcal{P}^{m,c,c^{\prime}}_g\}_{c^{\prime}\neq c}^C$ computed from the first data path with $\mathcal{P}_{sh}^{m,c,c}$ computed from the second path. This method is designed based on an intuitive idea that the shared information distribution $\mathcal{P}_{sh}^{m,c,c}$ can be regarded as the distribution of the data $\Tilde{\mathcal{X}}^{m,c}$ itself. Therefore, it encompasses the total confusing information of class $c$ and all the other classes. For any class $c^{\prime}\neq c$, the higher the similarity between the confusing information distribution $\mathcal{P}^{m,c,c^{\prime}}_g$ and $\mathcal{P}_{sh}^{m,c,c}$, the greater the proportion of confusing information related to $c^{\prime}$ in the total confusing information of $c$, and thus the higher the degree of confusion between $c$ and $c^{\prime}$.
% The higher the similarity between the confusion information distribution $\mathcal{P}^{m,c,c^{\prime}}_g$ of class $c^{\prime}$ and $\mathcal{P}_{sh}^{m,c,c}$, the greater the proportion of confusion information related to $c^{\prime}$, and thus the higher the degree of confusion. 
The estimation of $\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ can be formulated as:
\begin{align}
    \beta^{m,c,c^{\prime}}=\frac{exp(-D_{JS}(\mathcal{P}^{m,c,c}_{sh}||\mathcal{P}^{m,c,c^{\prime}}_g))}{\sum_{t\neq c}^C{exp(-D_{JS}(\mathcal{P}^{m,c,c}_{sh}||\mathcal{P}^{m,c,t}_g))}}.
    \label{eq:beta}
\end{align}
% Since the distribution $\mathcal{P}_{sh}^{m,c,c}$ retains all information of $\Tilde{\mathcal{X}}^{m,c}$, which includes its confusing information with all other classes. If the similarity between the \textit{ICI} distribution $\mathcal{P}_g^{m,c,c^{\prime}}$ and $\mathcal{P}_{sh}^{m,c,c}$ is higher, it can be inferred that the confusion information between $c$ and $c^{\prime}$ constitutes a larger proportion of the confusion information between $c$ and all other classes, implying a higher degree of confusion between $c$ and $c^{\prime}$.
% Subsequently, the confusion degree between class $c$ and other classes $\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ is estimated by comparing the \textit{ICI} distribution of $c$ between all other classes $\{\mathcal{P}^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ computed from the first data path with the distribution of class $c$  $\mathcal{P}_{sh}^{m,c,c}$ computed from the second path, which can be formulated as:
% \begin{align}
%     \beta^{m,c,c^{\prime}}=\frac{exp(-D_{JS}(\mathcal{P}^{m,c,c}_{sh}||\mathcal{P}^{m,c,c^{\prime}}_g))}{\sum_{t\neq c}^C{exp(-D_{JS}(\mathcal{P}^{m,c,c}_{sh}||\mathcal{P}^{m,c,t}_g))}}.
%     \label{eq:beta}
% \end{align}
$D_{JS}(\cdot||\cdot) \in [0,1]$ is the Jensen-Shannon Divergence, a measure of the similarity between two probability distributions, with smaller values indicating greater similarity. 

% The intuitive design behind this confusion degree estimation method is that the distribution $\mathcal{P}_{sh}^{m,c,c}$ retains all information of $\Tilde{\mathcal{X}}^{m,c}$, which includes its confusion information with all other classes. Therefore, if the similarity between the \textit{ICI} distribution $\mathcal{P}_g^{m,c,c^{\prime}}$ and $\mathcal{P}_{sh}^{m,c,c}$ is higher, it can be inferred that the confusion information between $c$ and $c^{\prime}$ constitutes a larger proportion of the confusion information between $c$ and all other classes, implying a higher degree of confusion between $c$ and $c^{\prime}$.

% and learns the common distribution $\mathcal{P}_g^{m,c,c^{\prime}}$

% % \textbf{G-ICI} learns $\mathcal{P}_{g}=\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$, which is modeled as a set of $M\times C$ Gaussian Mixture Models as mentioned in Section \ref{sec:ici}.

% \subsubsection{Model Implementation of \textbf{G-ICI}}
% \textbf{G-ICI} contains $M$ modality-specific global $ICI$ learning units $\{\textbf{U}^m\}_{m=1}^M$. Each global $ICI$ learning unit $\textbf{U}^m$ is a network structure that learns the global $ICI$ distribution for samples in the $m$-th modality, i.e., $\mathcal{P}_g^{m,:}=\{\mathcal{P}^{m,c}\}_{c=1}^C=\{Mix(\{\beta^{m,c,c^{\prime}}\cdot \mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C)\}_{c=1}^C$. To this end, a dual-path comparative network is implemented to learn the common distribution $\mathcal{P}^{m,c,c^{\prime}}$ and confusion degree $\beta^{m,c,c^{\prime}}$ between any two classes $c$ and $c^{\prime}$. 
% % The learning process of $\textbf{U}^m$ involves learning the common distribution $\mathcal{P}^{m,c,c^{\prime}}$ and confusion degree $\beta^{m,c,c^{\prime}}$ between any two classes $c$ and $c^{\prime}$. Therefore, it is crucial to design a network structure that is generalized among different class pairs. To this end, a dual-path comparative network is implemented in each $\textbf{U}^m$. 
% Specifically, the network consists of a class-specific distribution encoder $\textbf{E}_{sp}^m$, a class-shared distribution encoder $\textbf{E}_{sh}^m$, and a decoder $\textbf{D}$. 
% % encoder $\textbf{E}_{gici}^m$ and a classifier $\textbf{Cls}^m$. 
% The network contains two separate data paths. The first data path takes two batches of samples from different classes $\Tilde{\mathcal{X}}^{m,c}$ and $\Tilde{\mathcal{X}}^{m,c^{\prime}}$ together as input and learns the common distribution $\mathcal{P}_g^{m,c,c^{\prime}}$. The class-specific and -shared distribution is obtained by:
% \begin{align}
%     \mathcal{P}_{share}^{m,c},\mathcal{P}_{share}^{m,c^{\prime}}=\textbf{E}_{sh}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}}),\notag \\
%     \mathcal{P}_{spec}^{m,c}, \mathcal{P}_{spec}^{m,c^{\prime}}=\textbf{E}_{sp}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}}).
% \end{align}
% % \begin{align}
% %     \mathcal{P}_g^{m,c,c^{\prime}}=\textbf{E}_{gici}^{m}((\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}})),
% %     \label{eq:ici-cc'}
% % \end{align}
% where $\oplus$ refers to the vector stack up. Then, the common distribution $\mathcal{P}^{m,c,c^{\prime}}$ is obtained by:
% \begin{align}
%     \mathcal{P}^{m,c,c^{\prime}}_g=\frac{1}{2}(\mathcal{P}_{share}^{m,c}+\mathcal{P}_{share}^{m,c^{\prime}}).
%     \label{eq:ici-cc'}
% \end{align}
% The second data path takes two batches of samples from the same class together as input and learns their common distribution $\mathcal{P}_g^{m,c,c}$:
% \begin{align}
%     \mathcal{P}_{share}^{m,c},\mathcal{P}_{share}^{m,c}=\textbf{E}_{sh}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c}),\notag \\
%     \mathcal{P}_{spec}^{m,c}, \mathcal{P}_{spec}^{m,c}=\textbf{E}_{sp}^m(\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c}),\notag \\
%     \mathcal{P}_g^{m,c,c}=\frac{1}{2}(\mathcal{P}_{share}^{m,c}+\mathcal{P}_{share}^{m,c}).
% \end{align}
% The common distribution $\mathcal{P}_g^{m,c,c}$ between samples of the same class $c$ retains all information of $\Tilde{\mathcal{X}}^{m,c}$, which includes its confusion information with all other classes. Therefore, an intuitive method to evaluate inter-class confusion degree $\beta^{m,c,c^{\prime}}$ by contrasting $\mathcal{P}_g^{m,c,c}$ and $\mathcal{P}^{m,c,c^{\prime}}_g$ is proposed. Specifically, for the $c$-th class, its confusion degree with other classes $\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C$ can be quantified by measuring the similarity between distributions from the two data path using the Jensen-Shannon Divergence:
% \begin{align}
%    &D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,c^{\prime}}_g)= \notag \\
%    &\frac{1}{2}D_{KL}(\mathcal{P}^{m,c,c}_g||M)+\frac{1}{2}D_{KL}(\mathcal{P}^{m,c,c^{\prime}}_g||M). \notag \\
% \end{align}
% $D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,c^{\prime}}_g) \in [0,1]$ is the Jensen-Shannon Divergence between $\mathcal{P}^{m,c,c}_g$ and $\mathcal{P}^{m,c,c^{\prime}}_g$. A smaller $D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,c^{\prime}}_g)$ indicates that the confusion information distribution $\mathcal{P}^{m,c,c^{\prime}}_g$ between class $c$ and $c^{\prime}$ is closer to the distribution $\mathcal{P}^{m,c,c}_g$ containing confusion information of class $c$ with all other classes. This implies that the confusion degree between $c$ and $c^{\prime}$ is higher compared to $c$ and other classes. Therefore, the negative of $D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,c^{\prime}}_g)$ is used to estimate the confusion degree between class $c$ and $c^{\prime}$:
% \begin{align}
%     \beta^{m,c,c^{\prime}}=\frac{exp(-D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,c^{\prime}}_g))}{\sum_{t\neq c}^C{exp(-D_{JS}(\mathcal{P}^{m,c,c}_g||\mathcal{P}^{m,c,t}_g))}}.
%     \label{eq:beta}
% \end{align}
% Consequently, the global $ICI$ distribution of the $m$-th modality is obtained by mixing the learned $ICI$ distribution in Equation \ref{eq:ici-cc'} with corresponding confusion degree in Equation \ref{eq:beta}.

% An intuitive method for extracting the common distribution $\mathcal{P}^{m,c,c^{\prime}}$ between class $c$ and $c^{\prime}$ is to design a class-shared encoder and class-specific encoders for $c$ and $c^{\prime}$. Through disentanglement, the class-shared encoder can learn the common information between class $c$ and $c^{\prime}$, while the class-specific encoders learns the specific information of class $c$ w.r.t. to $c^{\prime}$ and $c^{\prime}$ w.r.t. $c$, respectively. 


% For the $c$-th class of the $m$-th modality, the global $ICI$ learning unit $\textbf{U}^{m,c}$ is designed to learn $\mathcal{P}_g^{m,c}$. $\textbf{U}^{m,c}$ is an elegantly designed network architecture featuring two distinct data pathways. One pathway is dedicated to learning the common distribution between samples of class $c$ and those of other classes $c^{\prime}, (c^{\prime}\in[1,C],c^{\prime}\neq c)$. The other pathway focuses on learning the degree of confusion between class $c$ and each of the other classes $c^{\prime}$. Specifically, $\textbf{U}^{m,c}$ consists of an encoder $\textbf{E}_{gici}^{m,c}$ and a classifier $\textbf{Cls}^m$. In the first data pathway, samples from the $c$-th class $\Tilde{\mathcal{X}}^{m,c}$ and the $c^{\prime}$-th class $\Tilde{\mathcal{X}}^{m,c^{\prime}}$ are input, the encoder $\textbf{E}_{gici}^{m,c}$ learns their common distribution $\mathcal{P}_g^{m,c,c^{\prime}}$:
% \begin{align}
%     \mathcal{P}_g^{m,c,c^{\prime}}=\textbf{E}_{gici}^{m,c}((\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c^{\prime}})),
% \end{align}
% where $\oplus$ refers to the vector stack up. In the second pathway, two identical samples from class $c$ are input:
% \begin{align}
%     \mathcal{P}_g^{m,c,c}=\textbf{E}_{gici}^{m,c}((\Tilde{\mathcal{X}}^{m,c}\oplus \Tilde{\mathcal{X}}^{m,c})).
% \end{align}
% The common distribution between the $c$-th class samples and itself $\mathcal{P}_g^{m,c,c}$ contains confusion information between $c$ and all other classes. Thus, 

% Subsequently, the confusion degree between the $c$-th class and other classes $c^{\prime}$ is estimated based on the Jensen-Shannon Divergence between the distribution between $\mathcal{P}^{m,c,c}$ and $\mathcal{P}^{m,c,c^{\prime}}$:
% \begin{align}
%     JSD^{m,c,c^{\prime}}=\frac{1}{2}D_{KL}(\mathcal{P}^{m,c,c}||M)+\frac{1}{2}D_{KL}(\mathcal{P}^{m,c,c^{\prime}}||M),
% \end{align}
% where $M=\frac{1}{2}(\mathcal{P}^{m,c,c}+\mathcal{P}^{m,c,c^{\prime}})$. Since that Jensen-Shannon Divergence becomes smaller as two distribution get closer,  

\subsubsection{Learning Objectives of $\textbf{U}^m$}
To learn the two encoders $\textbf{E}_{sp}^m$ and $\textbf{E}_{sh}^m$ in $\textbf{U}^m$, different loss functions are introduced in two data paths. In the first data path, the loss function $\mathcal{L}_{gici,1}^{m}$ is employed to maximize the distance between shared and specific information of each class. Additionally, the reconstruct loss $\mathcal{L}_{gici,re}^m$ is utilized to further constrain the learned distribution. In the second data path, $\mathcal{L}_{gici,2}^m$ is used to ensure the distribution $\mathcal{P}_{sh}^{m,c,c}$ contains all the information in $\Tilde{\mathcal{X}}^{m,c}$. Furthermore, the constrain $\mathcal{L}_{gici}^{sym,m}$ that minimize the difference between $\mathcal{P}_g^{m,c,c^{\prime}}$ and $\mathcal{P}_g^{m,c^{\prime}, c}$ is designed to ensure the symmetric of the learned \textit{ICI}. $\mathcal{L}_{gici,1}^m$, $\mathcal{L}_{gici,re}$, $\mathcal{L}_{gici,2}^m$, and $\mathcal{L}_{gici}^{sym,m}$ can be formulated as:
\begin{align}
    &\mathcal{L}_{gici,1}^m \notag \\
    &=\frac{1}{C(C-1)}\sum_{c}{\sum_{c^{\prime}}{{-D_{JS}(\mathcal{P}_{sh}^{m,c,c^{\prime}}||\mathcal{P}_{sp}^{m,c,c^{\prime}})}}} \notag \\
    &-\frac{1}{C(C-1)}\sum_{c}{\sum_{c^{\prime}}{{D_{JS}(\mathcal{P}_{sh}^{m,c,c^{\prime}}||\mathcal{P}_{sp}^{m,c^{\prime},c})}}}, \notag \\
    &\mathcal{L}_{gici,re}^{m} \notag \\
    &=\frac{1}{C(C-1)}\sum_{c}{\sum_{c^{\prime}}{||\textbf{D}(\Tilde{\mathcal{X}}_{sh}^{m,c,c^{\prime}}+\Tilde{\mathcal{X}}_{sp}^{m,c,c^{\prime}})-\Tilde{\mathcal{X}}^{m,c}||_2}} \notag \\
    &+\frac{1}{C(C-1)}\sum_{c}{\sum_{c^{\prime}}{||\textbf{D}(\Tilde{\mathcal{X}}_{sh}^{m,c,c^{\prime}}+\Tilde{\mathcal{X}}_{sp}^{m,c^{\prime},c})-\Tilde{\mathcal{X}}^{m,c^{\prime}}||_2}}, \notag \\
    &\mathcal{L}_{gici,2}^{m}=\frac{1}{C}\sum_{c}{-\log \mathcal{P}_g^{m,c,c}(\Tilde{\mathcal{X}}^{m,c})}, \notag \\
    &\mathcal{L}_{gici}^{sym,m}=\frac{1}{C(C-1)}\sum_{c}{\sum_{c^{\prime}}{D_{JS}(\mathcal{P}^{m,c,c^{\prime}}||\mathcal{P}^{m,c^{\prime},c})}}.
    \label{eq:6}
\end{align}
$\textbf{D}$ is the decoder, $\Tilde{\mathcal{X}}_{sh}^{m,c,c^{\prime}}$, $\Tilde{\mathcal{X}}_{sp}^{m,c,c^{\prime}}$, and $\Tilde{\mathcal{X}}_{sp}^{m,c^{\prime},c}$ refers to samples resampled from the distribution $\mathcal{P}_{sh}^{m,c,c^{\prime}}$, $\mathcal{P}_{sp}^{m,c,c^{\prime}}$, and $\mathcal{P}_{sp}^{m,c^{\prime},c}$, respectively. All the $\textbf{U}^m$ in $\textbf{GICI}$ is trained by minimizing the total loss $\mathcal{L}_{gici}$ formulated as:
\begin{align}
    \mathcal{L}_{gici}=\frac{1}{M}\sum_{m}{(\mathcal{L}_{gici,1}^{m}+\mathcal{L}_{gici,re}^{m}+\mathcal{L}_{gici,2}^{m}+\mathcal{L}_{gici}^{sym,m})}.
    \label{eq:l-gici}
\end{align}

% push $\mathcal{P}_{share}^{m,c}$ and $\mathcal{P}_{spec}^{m,c}$, $\mathcal{P}_{share}^{m,c^{\prime}}$ and $\mathcal{P}_{spec}^{m,c^{\prime}}$ away


% the loss function $\mathcal{L}_{gici,1}^{m}$ is employed to pull $\mathcal{P}_{share}^{m,c}$ and $\mathcal{P}_{share}^{m,c^{\prime}}$ together while push $\mathcal{P}_{share}^{m,c}$ and $\mathcal{P}_{spec}^{m,c}$, $\mathcal{P}_{share}^{m,c^{\prime}}$ and $\mathcal{P}_{spec}^{m,c^{\prime}}$ away:
% \begin{align}
%     \mathcal{L}_{gici,1}^{m,c,c^{\prime}}=D_{JS}(\mathcal{P}_{share}^{m,c}||\mathcal{P}_{share}^{m,c^{\prime}}) \notag \\
%     -D_{JS}(\mathcal{P}_{share}^{m,c}||\mathcal{P}_{spec}^{m,c})-D_{JS}(\mathcal{P}_{share}^{m,c^{\prime}}||\mathcal{P}_{spec}^{m,c^{\prime}}), \notag \\
%     \mathcal{L}_{gici,1}^{m}=\frac{1}{C(C-1)}\sum_{c=1}^C{\sum_{c^{\prime}\neq c}^C{\mathcal{L}_{gici,1}^{m,c,c^{\prime}}}}.
% \end{align}
% Additionally, the reconstruct loss $\mathcal{L}_{gici,re}^m$ is utilized to further constraint the learned distribution:
% \begin{align}
%     \mathcal{L}_{gici,re}^{m,c,c^{\prime}}=||\textbf{D}(\Tilde{\mathcal{X}}_{share}^{m,c}+\Tilde{\mathcal{X}}_{spec}^{m,c})-\Tilde{\mathcal{X}}^{m,c}||_2 \notag \\
%     +||\textbf{D}(\Tilde{\mathcal{X}}_{share}^{m,c^{\prime}}+\Tilde{\mathcal{X}}_{spec}^{m,c^{\prime}})-\Tilde{\mathcal{X}}^{m,c^{\prime}}||_2. \notag \\
%     \mathcal{L}_{gici,re}^{m}=\frac{1}{C(C-1)}\sum_{c=1}^C{\sum_{c^{\prime}\neq c}^C{\mathcal{L}_{gici,re}^{m,c,c^{\prime}}}}.
% \end{align}
% $\textbf{D}$ is the decoder, $\Tilde{\mathcal{X}}_{share}^{m,c}$, $\Tilde{\mathcal{X}}_{spec}^{m,c}$, $\Tilde{\mathcal{X}}_{share}^{m,c^{\prime}}$, $\Tilde{\mathcal{X}}_{spec}^{m,c^{\prime}}$ refers to samples resampled from the distribution $\mathcal{P}_{share}^{m,c}$, $\mathcal{P}_{spec}^{m,c}$, $\mathcal{P}_{share}^{m,c^{\prime}}$, and $\mathcal{P}_{spec}^{m,c^{\prime}}$, repectively. In the second data path, the additional constrain $\mathcal{L}_{gici,2}^m$ is used to ensure the distribution $\mathcal{P}_g^{m,c,c}$ contains all the information in $\Tilde{\mathcal{X}}^{m,c}$:
% \begin{align}
%     \mathcal{L}_{gici,2}^{m}=\frac{1}{C}\sum_{c=1}^C{-\log \mathcal{P}_g^{m,c,c}(\Tilde{\mathcal{X}}^{m,c})}.
% \end{align}
% Furthermore, the constrain between $\mathcal{P}_g^{m,c,c^{\prime}}$ and $\mathcal{P}_g^{m,c^{\prime}, c}$ is designed to ensure the symmetric of inter-class common distribution:
% \begin{align}
%     \mathcal{L}_{gici}^{sym,m}=\frac{1}{C(C-1)}\sum_{c=1}^C{\sum_{c^{\prime}\neq c}^C{D_{JS}(\mathcal{P}^{m,c,c^{\prime}}||\mathcal{P}^{m,c^{\prime},c})}}.
% \end{align}
% % To learn the encoder $\{\textbf{E}_{gici}^m\}_{m=1}^M$ in $\{\textbf{U}^m\}_{m=1}^M$, three constrains are introduced. (i) The information in the learned common distribution $\mathcal{P}^{m,c,c^{\prime}}$ must be sufficiently confusing to distinguish between classes $c$ and $c^{\prime}$. (ii) The common distribution between class $c$ and itself should be identical to its own distribution. (iii) The common distribution $\mathcal{P}^{m,c,c^{\prime}}$ between class $c$ and $c^{\prime}$ should be identical to the common distribution $\mathcal{P}^{m,c^{\prime},c}$ between class $c^{\prime}$ and $c$. To this end, a training method combining supervised and self-supervised learning is adopted and three loss functions $\mathcal{L}_{gici}^{sup}$, $\mathcal{L}_{gici}^{self}$, and $\mathcal{L}_{gici}^{sym}$ are defined to satify the above constrains respectively.
% % In the first data path of $\textbf{U}^m$, the model first resamples a set of samples $\textbf{X}^{m,c,c^{\prime}}$ from the learned $ICI$ distribution $\mathcal{P}^{m,c,c^{\prime}}$. $\textbf{X}^{m,c,c^{\prime}}$ is passed through the classifier $\textbf{Cls}^m$, and the resulting probability distribution is trained in a supervised manner against the confusion label $\Tilde{\textbf{y}}^{c,c^{\prime}}$:
% % \begin{align}
% %     \textbf{X}^{m,c,c^{\prime}}=RSample(\mathcal{P}_g^{m,c,c^{\prime}}),\notag \\
% %     pred^{m,c,c^{\prime}}=\textbf{Cls}^m(\textbf{X}^{m,c,c^{\prime}}),\notag \\
% %     \mathcal{L}_{gici}^{sup,m}=\frac{1}{C(C-1)}\sum_{c=1}^C{\sum_{c^{\prime}\neq c}^C{CE(pred^{m,c,c^{\prime}},\Tilde{\textbf{y}}^{c,c^{\prime}})}}.
% % \end{align}
% % $RSample(\cdot)$ refers to the resample operation. $CE(\cdot)$ refers to cross entropy loss. The confusion labels $\Tilde{\textbf{y}}^{c,c^{\prime}}$ is defined as:
% % \begin{equation}
% %     \Tilde{\textbf{y}}^{c,c^{\prime}}_i = \begin{cases} 
% %     0.5 & \text{if } i=c\text{ or }i=c^{\prime} \\
% %     0 & \text{if } otherwise,
% %     \end{cases}
% % \end{equation}
% % where $\Tilde{\textbf{y}}^{c,c^{\prime}}_i$ denotes the $i$-th element of the vector $\Tilde{\textbf{y}}^{c,c^{\prime}}$. In the second data path, the learned common distribution $\mathcal{P}_g^{m,c,c}$ between class $c$ and itself is optimized in a self-supervised manner by minimizing the distance between $\mathcal{P}_g^{m,c,c}$ and $\Tilde{\mathcal{X}}^{m,c}$:
% % \begin{align}
% %     \mathcal{L}_{gici}^{self,m}=\frac{1}{C}\sum_{c=1}^C{-\log \mathcal{P}_g^{m,c,c}(\Tilde{\mathcal{X}}^{m,c})}.
% % \end{align}
% % Additionally, a symmetry constraint is applied to the $ICI$ distributions $\mathcal{P}^{m,c,c^{\prime}}$ and $\mathcal{P}^{m,c^{\prime},c}$:
% % \begin{align}
% %     \mathcal{L}_{gici}^{sym,m}=\frac{1}{C(C-1)}\sum_{c=1}^C{\sum_{c^{\prime}\neq c}^C{D_{JS}(\mathcal{P}^{m,c,c^{\prime}}||\mathcal{P}^{m,c^{\prime},c})}}.
% % \end{align}
% $\textbf{G-ICI}$ is trained by minimizing the total loss $\mathcal{L}_{gici}$ formulated as:
% \begin{align}
%     \mathcal{L}_{gici}=\frac{1}{M}\sum_{m=1}^M{(\mathcal{L}_{gici,1}^{m}+\mathcal{L}_{gici,re}^{m}+\mathcal{L}_{gici,2}^{m}+\mathcal{L}_{gici}^{sym,m})}.
% \end{align}
% To further enhance the robustness of semantic-level noise removal and achieve instance-level noise elimination, the Global ICI Learning module (\textbf{G-ICI}) is proposed. This module learns the global $ICI$ distribution, which is utilized to guide the sample-level $ICI$ extraction in the \textbf{I-ICI} module introduced in Section \ref{sec:i-ici}. Specifically, the redundant features of samples in all instances $\{\{\textbf{L}_i^m\}_{m=1}^M\}_{i=1}^N$ are first organized according to different classes and yield $\Tilde{\mathcal{X}}=\{\{\Tilde{\mathcal{X}}^{m,c}\}_{m=1}^M\}_{c=1}^C$. $\Tilde{\mathcal{X}}^{m,c}=\{\textbf{L}_i^m\}_{i=1}^{N^{m,c}}$ contains the feature vectors of the $m$-th modality samples from all instances belonging to the $c$-th class. The learning of global $ICI$ distribution is performed based on the organized redundant feature set $\Tilde{\mathcal{X}}$.


\subsection{Global-Guided Sample \textit{ICI} Learning (\textbf{\textit{SICI}})}\label{sec:sici}
\subsubsection{Feature Extraction}
For each input sample $x_i=\{x_i^m\in\mathbb{R}^{d^m}\}_{m=1}^M$ in $\mathcal{D}=(\mathcal{X},\mathcal{Y})$, $M$ modality-specific encoders $\textbf{E}=\{\textbf{E}^m\}_{m=1}^M$ are employed to separate the discriminative features and $ICI$ from each modality's feature. The feature separation is designed based on the observation that given a high-dimensional feature vector, only a subset of features are informative and relevant to the class label \cite{han2022multimodal,zheng2023multi,zou2023dpnet,zheng2024global}. Those informative features are regarded as discriminative, while the others are redundant. Specifically, for the $m$-th modality, the encoder $\textbf{E}^m$ takes the input feature vector $x_i^m$ and learns a corresponding feature informativeness mask $w_i^m\in\mathbb{R}^{d^m}$: 
\begin{align}
    w_i^m=\textbf{E}^m(x_i^m).
    \label{eq:mask}
\end{align}
Each element of $w_i^m\in\mathbb{R}^{d^m}$ represents the informativeness of each feature value in the feature vector $x_i^m$. Subsequently, the learned informativeness mask $w_i^m$ is element-wise multiplied with the input sample $x_i^m$ to obtain discriminative features $h_i^m$, while its complement is element-wise multiplied with $x_i^m$ to obtain $ICI_i^m$. The calculation is formulated as follows:
\begin{align}
    h_i^m &=x_i^m\odot w_i^m, \notag \\
    ICI_i^m &=x_i^m\odot (1-w_i^m).
    \label{eq:fea_ext}
\end{align}

% pairs $\textbf{E}=\{(\textbf{E}_{info}^m,\textbf{E}_{iici}^m)\}$ are employed to extract the informative and $ICI$ features of samples from all modalities. The feature extraction is designed based on the observation that given a high-dimensional feature vector, each feature may exhibit varying levels of relevance and informativeness to the class label \cite{han2022multimodal,zheng2023multi,zou2023dpnet,zheng2024global}. Therefore, in the informative feature encoder $\textbf{E}_{info}^m$, an informativeness mask $msk^m_{info}\in\mathbb{R}^{d^m}$ is learned, where each element represents the informativeness corresponding to each feature of the input sample. Then, the informativeness mask $w^m_{info}\in\mathbb{R}^{d^m}$ is element-wise multiplied with the input sample $\textbf{x}_i^m$ to retain informative features $h_i^m$:
% \begin{align}
%     w^m_{info,i}=\textbf{E}_{info}^m(\textbf{x}_i^m),\notag \\
%     h_i^m=\textbf{x}_i^m\odot w^m_{info,i}.
% \end{align}
% On the other hand, the $ICI$ encoder $\textbf{E}_{iici}^m$ learns an opposite mask $w_{iici}^m$ which represents the degree of redundancy of each feature in the sample. The $ICI_i^m$ is extracted by retaining the redundant features in the sample $\textbf{x}_i^m$:
% \begin{align}
%     w^m_{iici,i}=\textbf{E}_{iici}^m(\textbf{x}_i^m),\notag \\
%     ICI_i^m=\textbf{x}_i^m\odot w^m_{iici,i}.
% \end{align}

\subsubsection{Global-Level $ICI$ Elimination} 
% The model learns to retain informative features in $\{h_i^m\}_{m=1}^M$ of sample $\textbf{x}_i$ under the supervision of ground-truth labels. However, empirical studies show that the learned $h_i^m$ is still biased and contains a certain amount of $ICI$. Therefore, 
Under the supervision of the ground-truth label, the informativeness mask $w_i^m$ has removed some of the redundant features from $h_i^m$. The global \textit{ICI} distribution learned by \textbf{\textit{GICI}} is utilized to guide the thorough removal of global-level \textit{ICI} from $h_i^m$ to enhance its reliability further. Specifically, the distance between $\{h_i^m\}_{m=1}^M$ and global $ICI$ distribution $\mathcal{P}_g$ is maximized, which can be achieved by minimizing $\mathcal{L}_{sici}$:
\begin{align}
    \mathcal{L}_{sici}=\frac{1}{N}\frac{1}{M}\sum_{i}{\sum_{m}{\log \mathcal{P}_g^{m,y_i}(h_i^m)}}.
    \label{eq:l-sici}
\end{align}
$y_i$ is the label of the sample $x_i$. $\mathcal{P}_g^{m,y_i}(\cdot)$ refers to the posterior probability of an input on $\mathcal{P}_g^{m,y_i}$, which can be calculated as:
\begin{align}
    \mathcal{P}^{m,y_i}(x)=\sum_{c\neq y_i}^C{\beta^{m,y_i,c}\cdot \mathcal{P}^{m,y_i,c}(x)}.
\end{align}
% By minimizing $\mathcal{L}_{info,i}$, the informative masks $\{w_i^m\}_{m=1}^M$ learned by the encoders $\textbf{E}=\{\textbf{E}^m\}_{m=1}^M$ enhance the discriminative power of $\{h_i^m\}_{m=1}^M$.

% and remove more uninformative feature into the learned $\{ICI_i^m\}_{m=1}^M$. Considering that the $\{ICI_i^m\}_{m=1}^M$ may contains sample-specific bias, as mentioned in Section \ref{sec:ins-noise}, another set of modality-specific encoders $G=\{G^m\}_{m=1}^M$ is employed to calibrate the bias in $\{ICI_i^m\}_{m=1}^M$ and align the calibrated $\{\widetilde{ICI}_i^m\}_{m=1}^M$ with the global $ICI$ distribution:
% \begin{align}
%     \widetilde{ICI}_i^m=G^m(ICI_i^m),\notag \\
%     \mathcal{L}_{iici,i}=-\frac{1}{M}\sum_{m=1}^M{\log \mathcal{P}^{m,\textbf{y}_i}(\widetilde{ICI}_i^m)}.
% \end{align}
% Thus, the unbiased learning of $\{h_i^m\}_{m=1}^M$ and $\{\widetilde{ICI}_i^m\}_{m=1}^M$ via Global $ICI$ distribution is achieved by minimizing $\mathcal{L}_{unb, i}=\mathcal{L}_{info,i}+\mathcal{L}_{iici,i}$.
% The global $ICI$ distribution learned in Section \ref{sec:g-ici} is utilized to guide the learning of the unbiased $\{ICI_i^m\}_{m=1}^M$ and informativeness feature $\{h_i^m\}_{m=1}^M$ of each sample from instance $\textbf{x}_i$. To learn the unbiased $\{ICI_i^m\}_{m=1}^M$, the model minimizes its distance $\mathcal{L}_{iici,i}^m$ from the global $ICI$ distribution to ensure the alignment with the overall $ICI$ distribution of the data:
% \begin{align}
%     \mathcal{L}{iici,i}^m=-\log \mathcal{P}^{m,\textbf{y}_i}(h_i^m).
% \end{align}
% The ground-truth labels are used to train the model to retain informative features in $h_i^m$. However, empirical studies show that the learned $h_i^m$ is still biased and contains a certain amount of $ICI$. Therefore, the global $ICI$ distribution is utilized to further seperate the $ICI$ from $h_i^m$, thereby enhancing its discriminative power. Specifically, the distance between the learned $h_i^m$ and global $ICI$ distribution is maximized by  

% For the informativeness feature $h_i^m$, its distance $Dis_{info,i}^m$ from the global $ICI$ distribution is maximized to ensure that the retained features possess discriminative power. For $ICI_i^m$, the model minimizes its distance $Dis_{iici,i}^m$ from the global $ICI$ distribution to ensure that the sample-level $ICI$ aligns with the overall $ICI$ distribution of the data. The $Dis_{info,i}^m$ and $Dis_{iici,i}^m$ are defined as:
% \begin{align}
%     Dis_{info,i}^m=\log \mathcal{P}^{m,\textbf{y}_i}(h_i^m),\notag \\
%     Dis_{iici,i}^m=-\log \mathcal{P}^{m,\textbf{y}_i}(h_i^m).
% \end{align}

% Furthermore, another set of modality-specific encoders $G=\{G^m\}_{m=1}^M$ is employed to extract inter-class confusion information $\{ICI_i^m\}_{m=1}^M$ from the redundant features of each modality by $ICI_i^m=G^m(\textbf{L}_i^m)$. 

% % 这句胡应该放introduction
% It is important to note that Inter-Class Confusion Information (ICI) represents only a subset of the information contained in low-quality features, as these features also include other redundant information that does not contribute to inter-class confusion. For example, in images used to distinguish between dogs and wolves, the confusing information may include features of the nose and ears, while other redundant information, such as the background, is irrelevant. Such redundant information is not useful for subsequent cross-modal information compensation because it lacks category-related semantic value.

\subsection{Sample-Adaptive Cross-modality Information Compensation (\textbf{\textit{CMIC}})}
\label{sec:cmic}
After the global-level $ICI$ is removed in the discriminative feature $\{h_i^m\}_{m=1}^M$ for the input sample $x_i$, the proposed \textbf{\textit{CMIC}} further eliminates the individual-level $ICI$. As analyzed in Section \ref{sec:intro}, the individual $ICI$ introduces varying information incompleteness and quality across modalities in different samples. Therefore, \textbf{\textit{CMIC}} designs a cross-modality information compensation method in a sample-adaptive way. Specifically, it first achieves interpretable cross-modality compensatory information query based on the complementary relationship between $\{h_i^m\}_{m=1}^M$ and $\{ICI_i^m\}_{m=1}^M$ across modalities. Furthermore, the discriminative feature of each modality is adaptively enhanced with the compensatory information from all the other modalities. The relative discriminative power is proposed to integrate compensatory information from modalities based on their relative qualities, thereby promoting the reliability of modality enhancement.

% After the learning of discriminative feature $\{h_i^m\}_{m=1}^M$ and $\{ICI_i^m\}_{m=1}^M$ for the input sample $x_i$, the proposed \textbf{\textit{CMIC}} further removes the cross-modality noise by aligning the discriminative power of all modalities via cross-modality compensation. \textbf{\textit{CMIC}} first achieve interpretable cross-modality compensation information query based on the complementary relationship between $\{h_i^m\}_{m=1}^M$ and $\{ICI_i^m\}_{m=1}^M$ across modalities. Subsequently, \textbf{\textit{CMIC}} adaptively enhances the discriminative features of each modality using the queried information based on the proposed relative discriminative power.
% Although the discriminative power of $\{h_i^m\}_{m=1}^M$ is enhanced under the guidance of the Global $ICI$ distribution, the discriminative power of $h_i^m$ varies across modalities. Therefore, \textbf{CMIC} aligns the discriminative power of each modality through adaptive cross-modal information compensation utilizing $\{ICI_i^m\}_{m=1}^M$. 
% % The ICI-C module performs adaptive cross-modality information compensation and discriminative power alignment utilizing the unbiased $\{\widetilde{ICI}_i^m\}_{m=1}^M$ and discriminative features $\{h_i^m\}_{m=1}^M$ of the input instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$. 
% Specifically, the compensation and alignment procedure consists of two stages.
\subsubsection{Cross-Modality Compensatory Information Query} 
As shown in Equation \ref{eq:fea_ext}, $h_i^m$ and $ICI_i^m$ are complementary for a certain modality $m$. Therefore, it is a feasible way to obtain information that $h_i^m$ lacks from the discriminative features $h_i^{m^{\prime}}$ of other modalities $m^{\prime}$ using $ICI_i^m$. To this end, a cross-modality attention mechanism is employed to query compensatory information from discriminative feature $h_i^{m^{\prime}}$ of each of the other modalities $m^{\prime},(m^{\prime}\neq m,m^{\prime}\in[1,M])$ for $h_i^m$ based on $ICI_i^m$, which can be formulated as:
% The discriminative feature $h_i^m$ and the $ICI_i^m$ together constitute all the features of modality $m$ of sample $x_i$. Therefore, compensating for the missing information corresponding to $\{ICI_i^m\}_{m=1}^M$ in each modality is a feasible way to improve alignment of discriminative power and semantic information across $\{h_i^m\}_{m=1}^M$ of all modalities. A cross-modality attention is employed to obtain features orthogonal to $ICI_i^m$ from discriminative feature $h_i^{m^{\prime}}$ of each of the other modalities $m^{\prime},(m^{\prime}\neq m,m^{\prime}\in[1,M])$ as the compensation information, which can be formulated as:
\begin{align}
    &\textbf{Q}_i^{m}=W_Q(ICI_i^m),\quad \forall m\in[1,M], \notag \\
    &\textbf{K}_i^{m^{\prime}}=W_K(h_i^{m^{\prime}}),\quad \forall m^{\prime}\in [1,M], m^{\prime}\neq m, \notag\\
    &\textbf{V}_i^{m^{\prime}}=W_V(h_i^{m^{\prime}}),\quad \forall m^{\prime}\in [1,M], m^{\prime}\neq m. \notag \\
    &A_i^{m\leftarrow m^{\prime}}=Softmax(\frac{\textbf{Q}_i^{m}(\textbf{K}_i^{m^{\prime}})^T}{\sqrt{d}}). \notag \\
    &\textbf{z}_i^{m\leftarrow m^{\prime}}=A_i^{m\leftarrow m^{\prime}}\textbf{V}_i^{m^{\prime}}.
\end{align}
$W_Q, W_K, W_V$ are three learnable mappings that project $ICI_i^m$ and discriminative features of one of the other modalities $h_i^{m^{\prime}}$ into the query, key, and value matrix. $A_i^{m\leftarrow m^{\prime}}$ is the attention map. $\textbf{z}_i^{m\leftarrow m^{\prime}}$ is the compensatory information queried from modality $m^{\prime}$ for modality $m$. 
% A loss term $\mathcal{L}_{cmic,i}^{m,m^{\prime}}=(\textbf{z}_i^{m\leftarrow m^{\prime}})^T\cdot ICI_i^m$ is introduced to guide $\textbf{z}_i^{m\leftarrow m^{\prime}}$ to be as orthogonal to $ICI_i^m$ as possible.
% that represents the similarity between $ICI_i^m$ and $h_i^{m^{\prime}}$ across each feature. To extract features from $\textbf{h}_i^{m^{\prime}}$ that have low similarity with $ICI_i^m$, $A_i^{m\leftarrow}$ is first negated and then element-wise multiplied with $\textbf{V}_i^{m^{\prime}}$:
% \begin{align}
%     \textbf{z}_i^{m\leftarrow m^{\prime}}=\neg A_i^{m\leftarrow m^{\prime}}\textbf{V}_i^{m^{\prime}}=(1-A_i^{m\leftarrow m^{\prime}})\textbf{V}_i^{m^{\prime}}.
% \end{align}
% the samples of each modality in the input instance leverage their own inter-class confusion information $ICI$ to retrieve compensation information from the discriminative features of other modalities. A cross-modal attention is employed, which computes the semantic similarity between the $ICI$ of one modality and the discriminative features of other modality samples. Discriminative features with low similarity are identified as compensation information, ensuring they contribute complementary semantic insights for robust classification. Formally, given the $ICI$ and discriminative features of the instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$: $ICI_i=\{ICI_i^m\}_{m=1}^M$ and $\textbf{H}_i=\{\textbf{H}_i^m\}_{m=1}^M$. Three learnable mappings $W_Q, W_K, W_V$ first project the $ICI$ of one modality and discriminative features of one of the other modalities into the query, key, and value matrix:
% \begin{align}
%     \textbf{Q}_i^{m}&=W_Q(ICI_i^m),\quad \forall m\in[1,M], \notag \\
%     \textbf{K}_i^{m^{\prime}}&=W_K(\textbf{H}_i^{m^{\prime}}),\quad \forall m^{\prime}\in [1,M], m^{\prime}\neq m, \notag\\
%     \textbf{V}_i^{m^{\prime}}&=W_V(\textbf{H}_i^{m^{\prime}}),\quad \forall m^{\prime}\in [1,M], m^{\prime}\neq m.
% \end{align}
% After that, the attention map can be calculated as follows:
% \begin{align}
%     A_i^{m\leftarrow m^{\prime}}=Softmax(\frac{\textbf{Q}_i^{m}(\textbf{K}_i^{m^{\prime}})^T}{\sqrt{d}}).
% \end{align}
% $A_i^{m\leftarrow}$ represents the pairwise similarity between $ICI_i^m$ and $\textbf{H}_i^{m^{\prime}}$. To extract features from $\textbf{H}_i^{m^{\prime}}$ with low similarity, $A_i^{m\leftarrow}$ is first negated and then element-wise multiplied with $\textbf{V}_i^{m^{\prime}}$:
% \begin{align}
%     \textbf{z}_i^{m\leftarrow m^{\prime}}=\neg A_i^{m\leftarrow m^{\prime}}\textbf{V}_i^{m^{\prime}}=(1-A_i^{m\leftarrow m^{\prime}})\textbf{V}_i^{m^{\prime}}.
% \end{align}
$\textbf{Z}_i^{m\leftarrow}=\{\textbf{z}_i^{m\leftarrow m^{\prime}}\}_{m^{\prime}\neq m}^M$ is the compensation information obtained by the $m$-th modality from other modalities $m^{\prime}$. 
% The attention module is trained by minimizing $\mathcal{L}_{cmic}$:
% \begin{align}
%     \mathcal{L}_{cmic}=\frac{1}{NM(M-1)}\sum_{i}{\sum_{m}{\sum_{m^{\prime}}{\mathcal{L}_{cmic,i}^{m,m^{\prime}}}}}.
% \end{align}

\subsubsection{Modality Enhancement Based On Relative Discriminative Power} 
% To achieve more accurate and reliable cross-modal information compensation and alignment of discriminative power, this work proposes to first assess the relative discriminative power between modalities. The reason is that noise at the sample level causes dynamic variability in the quality of each modality of the samples. Modalities with relatively higher quality should have a greater contribution in information compensation. Specifically, a discriminative power estimation method that considers both global and sample-level noise is first employed. This method accurately assesses each modality's discriminative power of $h_i^m$ by combining global-level feature informativeness computed via mutual information and sample-level informativeness masks $w_i^m$. The normalized mutual information is employed to evaluate the relevance between each feature of the modality $m$ and the ground-truth label:
To ensure the reliability of modality enhancement using the queried compensatory information from other modalities, this work proposes to assess the relative discriminative power between modalities. Modalities with relatively higher quality should have a greater contribution to the enhancement. To estimate the quality of each modality, a method that considers global and individual-level noise is first employed. Specifically, the mutual information is first utilized to compute the informativeness of each feature of a certain modality $m$ at the global level, which can be formulated as:
\begin{align}
    I^m=\frac{2\times I(\mathcal{X}^m,\mathcal{Y})}{H(\mathcal{X}^m)+H(\mathcal{Y})}\in\mathbb{R}^{d^m}.
\end{align}
$\mathcal{X}^m=\{\textbf{x}_i^m\}_{i=1}^N$ represents all samples from the $m$-th modality, $I(\cdot,\cdot)$ is the mutual information calculation, $H(\cdot)$ is the entropy of a variable. Then, the feature informativeness at the individual level learned by the informativeness mask $w_i^m$ in the \textbf{\textit{SICI}} module is integrated into the global-level informativeness estimation $I^m$. The discriminative power of the feature vector $h_i^m$ retained after masking by $w_i^m$ is estimated by measuring the consistency between $w_i^m$ and $I^m$:
% Since $w_i^m$ represents the relevance of each modality feature of a specific sample $x_i$ to its label, the consistency between the $w_i^m$ and mutual information $I^m$ can be used to assess the discriminative power of the features retained after masking by $w_i^m$ (i.e., $h_i^m$):
\begin{align}
    \delta_i^m=-|w_i^m-I^m|\in \mathbb{R}^{d^m}.\notag \\
    D_i^m=\sigma(W^{m+}(\delta_i^m))=\sigma(exp(\theta)\cdot\delta_i^m).
\end{align}
$|\cdot|$ denotes the element-wise absolute value of the vector. $W^{m+}$ is a learnable positive mapping that maps $\delta_i^m$ to the discriminative power estimation value $D_i^m$. $\sigma(\cdot)$ is the sigmoid function, $exp(\cdot)$ denotes the exponential function that reparameterize the learnable parameter $\theta$ to ensure the positive mapping. 
% After the discriminative power of all modalities of sample $x_i$ is obtained, 
Subsequently, the relative discriminative power $k_i^{m\leftarrow m^{\prime}}$ between modality $m$ and other modalities $m^{\prime}$ can be calculated by:
\begin{align}
    k_i^{m\leftarrow m^{\prime}}=\frac{exp(\frac{D_i^{m^{\prime}}}{D_i^m})}{\sum_{n\neq m}^M{exp(\frac{D_i^n}{D_i^m})}}.
    \label{eq:rdp}
\end{align}
% After each modality $m$ obtains compensation information from other modalities $m^{\prime}$, \textbf{\textit{CMIC}} further enhances modality $m$ using compensation information based on the relative discriminative power between modality $m$ and the other modalities $m^{\prime}$. In this way, \textbf{\textit{CMIC}} adaptively enhances each modality based on the gap in discriminative power relative to other modalities, effectively unifying the discriminative power across modalities.
% % In this stage, each modality enhances its own discriminative feature utilizing the compensation information queried from other modalities. The enhancement is performed by aggregating the compensation information based on the relative discriminative power between discriminative features of samples from different modalities. 
% To this end, the discriminative power of $\{h_i^m\}_{m=1}^M$ of each modality is first estimated. This paper employs a method that integrates mutual information with feature informativeness masks. Specifically, the normalized mutual information is first employed to evaluate the relevance between each feature of the modality samples and their corresponding labels:
% \begin{align}
%     \textbf{I}^m=\frac{2\times I(\mathcal{X}^m,\mathcal{Y})}{H(\mathcal{X}^m)+H(\mathcal{Y})}\in\mathbb{R}^{d^m}.
% \end{align}
% $\mathcal{X}^m=\{\textbf{x}_i^m\}_{i=1}^N$ represents all samples from the $m$-th modality, $I(\cdot,\cdot)$ is the mutual information calculation, $H(\cdot)$ is the entropy of a variable. Considering the presence of sample-level ICI, which implies the diversity in feature quality across different samples, mutual information alone cannot capture the feature quality differences among samples. Therefore, the informativeness mask $w_i^m$ learned in the \textbf{SICI} module by Equation \ref{eq:mask} is utilized to refine the mutual information at the sample level. Since the informativeness mask $w_i^m$ also represents the relevance of each feature to its label, the consistency $\epsilon_i^m$ between the $w_i^m$ and mutual information $\textbf{I}^m$ can be used to assess the discriminative power of the features retained after masking by $w_i^m$. In other words, the discriminative power is positively correlated with the consistency $\epsilon_i^m$. $\epsilon_i^m$ is formulated as:
% \begin{align}
%     \epsilon_i^m=-|\textbf{w}_i^m-\textbf{I}^m|\in \mathbb{R}^{d^m}.
% \end{align}
% $|\cdot|$ denotes the element-wise absolute value of the vector. Then, a learnable positive mapping $W^{m+}$ is used to map $\epsilon_i^m$ to the discriminative power estimate $D_i^m$:
% \begin{align}
%     D_i^m=\sigma(W^{m+}(\epsilon_i^m))=\sigma(exp(\theta)\epsilon_i^m).
% \end{align}
% $\sigma(\cdot)$ is the sigmoid function, $exp(\cdot)$ denotes the exponential function that reparameterize the learnable parameter $\theta$ to ensure the positive mapping. $D_i^m$ refers to the discriminative power of $h_i^m$. Subsequently, the relative discriminative power $k_i^{m\leftarrow m^{\prime}}$ between modality $m$ and other modalities $m^{\prime}$ can be obtained by:
% \begin{align}
%     k_i^{m\leftarrow m^{\prime}}=\frac{exp(\frac{D_i^{m^{\prime}}}{D_i^m})}{\sum_{n\neq m}^M{exp(\frac{D_i^n}{D_i^m})}}.
%     \label{eq:rdp}
% \end{align}
After the relative discriminative power $k_i^{m\leftarrow}=\{k_i^{m\leftarrow m^{\prime}}\}_{m^{\prime}\neq m}^M$ is estimated, the compensatory information $\textbf{Z}_i^{m\leftarrow}=\{\textbf{z}_i^{m\leftarrow m^{\prime}}\}_{m^{\prime}\neq m}^M$ is aggregated with weights of $k_i^{m\leftarrow}=\{k_i^{m\leftarrow m^{\prime}}\}_{m^{\prime}\neq m}^M$ and used to enhance the discriminative feature $h_i^m$ of the $m$-th modality:
\begin{align}
    f_i^m=h_i^m+\sum_{m^{\prime}\neq m}^M{k_i^{m\leftarrow m^{\prime}}\cdot \textbf{z}_i^{m\leftarrow m^{\prime}}}.
\end{align}
$f_i=\{f_i^m\}_{m=1}^M$ is the enhanced feature of the input sample $x_i=\{x_i^m\}_{m=1}^M$ with both modality-specific and cross-modality noise removed. The reliable classification result $\hat{y}_i$ of the sample $x_i$ is obtained by $\hat{y}_i=Classifier(Concat(\{f_i^m\}_{m=1}^M))$.

\subsection{Learning of MICINet}
In addition to $\mathcal{L}_{gici}$ (Equation \ref{eq:l-gici}) and $\mathcal{L}_{sici}$ (Equation \ref{eq:l-sici}), the training of MICINet also incorporates a loss function $\mathcal{L}_{align}$ that constrains sample feature alignment, as well as an overall classification loss $\mathcal{L}_{task}$, which can be formulated as:
\begin{align}
    &\mathcal{L}_{align}=\frac{1}{NM(M-1)}\sum_{i=1}^N{\sum_{m=1}^M{\sum_{m^{\prime}\neq m}^M{(f_i^m)^T\cdot f_i^{m^{\prime}}}}},\notag \\
    &\mathcal{L}_{task}=\sum_{i=1}^N{CrossEntropy(\hat{y}_i,y_i)}.
\end{align}
$T$ refers to the transpose of the feature vector. MICINet is trained by minimizing the total loss $\mathcal{L}$:
\begin{align}
\mathcal{L}=\mathcal{L}_{gici}+\mathcal{L}_{sici}+\mathcal{L}_{task}-\mathcal{L}_{align}.
\end{align}
% a discriminative power estimation method is proposed, integrating a mutual information-based global distribution perspective with a sample-specific informativeness mask. Specifically, the normalized mutual information is first employed to evaluate the relevance between each feature of the modality samples and their corresponding labels:
% \begin{align}
%     \textbf{I}^m=\frac{2\times I(\mathcal{X}^m,\mathcal{Y})}{H(\mathcal{X}^m)+H(\mathcal{Y})}\in\mathbb{R}^{d^m}.
% \end{align}
% $\mathcal{X}^m=\{\textbf{x}_i^m\}_{i=1}^N$ represents all samples from the $m$-th modality, $I(\cdot,\cdot)$ is the mutual information calculation, $H(\cdot)$ is the entropy of a variable. However, considering the diversity in sample-level quality, mutual information alone cannot capture the feature quality differences among samples. Therefore, the informativeness mask $\{\{\textbf{w}_i^m\}_{m=1}^M\}_{i=1}^N$ learned in the \textbf{I-ICI} module by Equation \ref{eq:mask} is utilized to refine the mutual information at the sample level. Since the informativeness mask also represents the relevance of each sample's feature to its label, the alignment between the informativeness mask and mutual information can be used to assess the discriminative power of the features retained after masking. In other words, the discriminative power is positively correlated with the degree of alignment. The alignment between the informativeness mask and mutual information is calculated by:
% \begin{align}
%     \epsilon_i^m=-|\textbf{w}_i^m-\textbf{I}^m|\in \mathbb{R}^{d^m}.
% \end{align}
% $|\cdot|$ denotes the element-wise absolute value of the vector. Then, a learnable positive mapping $W^{m+}$ is used to map $\epsilon_i^m$ to the discriminative power estimate $D_i^m$:
% \begin{align}
%     D_i^m=\sigma(W^{m+}(\epsilon_i^m))=\sigma(exp(\theta)\epsilon_i^m).
% \end{align}
% $\sigma(\cdot)$ is the sigmoid function, $exp(\cdot)$ denotes the exponential function that reparameterize the learnable parameter $\theta$ to ensure the positive mapping.

% \paragraph{Instance-adaptive cross-modality information compensation} For the instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$, the discriminative power of all its samples is obtained as $\{D_i^m\}_{m=1}^M$. For the $m$-th modality, the relative discriminative power between other modalities and the $m$-th modality is first calculated:
% \begin{align}
%     k_i^{m^{\prime}}=\frac{exp(\frac{D_i^{m^{\prime}}}{D_i^m})}{\sum_{n\neq m}^M{exp(\frac{D_i^n}{D_i^m})}}.
%     \label{eq:rdp}
% \end{align}
% Then, the compensation information $\textbf{Z}_i^{m\leftarrow}=\{\textbf{z}_i^{m\leftarrow m^{\prime}}\}_{m^{\prime}\neq m}^M$ is aggregated with weights based on the relative discriminative power and used to enhance the discriminative feature $H_i^m$ of the $m$-th modality:
% \begin{align}
%     \textbf{f}_i^m=\textbf{H}_i^m+\sum_{m^{\prime}\neq m}^M{k_i^{m^{\prime}}\cdot \textbf{z}_i^{m^{\prime}}}.
% \end{align}
% $\textbf{f}_i=\{\textbf{f}_i^m\}_{m=1}^M$ is the enhanced feature of the input instance $\textbf{x}_i=\{\textbf{x}_i^m\}_{m=1}^M$.



\section{Experiments}
This section presents experiments that comprehensively validate the effectiveness of the proposed MICINet in removing both modality-specific and cross-modality noise. Several reliable multimodal classification methods are introduced to demonstrate the improvement of MICINet. In the following sections, the experimental settings are first elaborated in Section \ref{sec:exp-set}. Subsequently, the main questions to be verified are highlighted in Section \ref{sec:ques}.


% \begin{table*}[h]
% \caption{Comparison with state-of-the-art reliable multimodal classification methods on four datasets.}
% \begin{center}
% \begin{adjustbox}{width=0.95\textwidth}
% \begin{tabular}{@{}c|c|ccc|ccc@{}}
% \toprule
%        \multirow{2}{*}{Data Type} & \multirow{2}{*}{Method} & \multicolumn{3}{c|}{BRCA}   & \multicolumn{3}{c}{ROSMAP}    \\ 
% % \cline{3-8}
%  & ACC & WeightedF1 & MacroF1 & ACC & F1 & AUC \\ 
%  \midrule 
% % ETMC \cite{han2022trusted} &static&  84.6$\pm$0.9& 84.7$\pm$1.1& 80.9$\pm$1.1  & 82.5$\pm$1.3& 82.3$\pm$0.9         & 88.6$\pm$0.9     \\ 
% \multirow{8}{*}{Complete}  MD \cite{han2022multimodal} &  87.7$\pm$0.3 & 88.0$\pm$0.5 & 84.5$\pm$0.5       & 84.2$\pm$1.3  & 84.6$\pm$0.7  & 91.2$\pm$0.7       \\ 
% MLCLNet \cite{zheng2023multi}& 86.4$\pm$1.6& 87.8$\pm$1.7& 82.6$\pm$1.8      & 84.4$\pm$1.5   & 85.2$\pm$1.5       & 89.3$\pm$1.1  \\ 
% DPNET \cite{zou2023dpnet}& 87.8$\pm$1.0& 88.4$\pm$1.2& 85.2$\pm$1.2       & 85.1$\pm$1.1   & 84.8$\pm$0.7         & 91.3$\pm$0.7      \\ 
% CALM \cite{zhou2023calm}& 88.2$\pm$0.7& 88.5$\pm$0.8& 85.1$\pm$0.8       & 85.5$\pm$1.2   & 87.9$\pm$0.9         & 91.3$\pm$1.0      \\ 

% QMF \cite{zhang2023provable}& 87.4$\pm$0.4& 87.7$\pm$0.5& 84.1$\pm$0.5       & 84.6$\pm$0.9   & 84.8$\pm$0.8         & 90.5$\pm$0.8      \\ 
% GCFANet \cite{zheng2024global}& \underline{88.6$\pm$1.5} & \underline{88.9$\pm$1.6} & \underline{85.3$\pm$1.6}     & \underline{86.3$\pm$1.4} & \underline{88.3$\pm$1.6}        & 91.5$\pm$1.2      \\ 
% PDF \cite{cao2024predictive}& 88.2$\pm$0.7 & 88.0$\pm$0.6 & 84.9$\pm$0.6     & 85.9$\pm$0.5 & 88.0$\pm$0.4        & \underline{91.6$\pm$0.5}  \\
% \midrule
% QADM-Net  &\textbf{93.7$\pm$ 0.3} & \textbf{93.7$\pm$ 0.4} & \textbf{91.5$\pm$ 0.4} & \textbf{92.1$\pm$ 0.4} & \textbf{92.7$\pm$ 0.3} & \textbf{95.2$\pm$ 0.3} \\ \bottomrule
% \toprule
% \multirow{2}{*}{Method} &  \multicolumn{3}{c|}{CUB}   & \multicolumn{3}{c}{FOOD101}    \\ 
%  & ACC & WeightedF1 & MacroF1 & ACC & WeightedF1 & MacroF1 \\ \midrule 
% % ETMC \cite{han2022trusted} &static&  90.1$\pm$1.8& 89.7$\pm$1.6& 90.2$\pm$1.7  & 92.5$\pm$1.3& 92.3$\pm$0.8         & 92.3$\pm$0.8     \\ 
% MD \cite{han2022multimodal} &  90.1$\pm$0.7 & 90.0$\pm$0.8 & 89.9$\pm$0.7       & 92.8$\pm$0.3  & 92.6$\pm$0.2  & 92.6$\pm$0.2       \\ 
% MLCLNet \cite{zheng2023multi}& 88.2$\pm$1.4& 88.3$\pm$1.7& 87.9$\pm$1.3      & 92.1$\pm$1.0   & 92.2$\pm$1.2       & 92.1$\pm$1.1  \\ 
% DPNET \cite{zou2023dpnet}& 92.1$\pm$0.9& 91.8$\pm$0.7& 92.2$\pm$1.1       & 93.1$\pm$1.2   & 93.0$\pm$0.7         & 92.8$\pm$0.8      \\ 
% CALM \cite{zhou2023calm}& 92.0$\pm$0.2& 92.1$\pm$0.4& 92.1$\pm$0.5       & 93.0$\pm$1.1   & 92.9$\pm$0.7         & 92.9$\pm$0.8      \\ 
% QMF \cite{zhang2023provable} & 89.8$\pm$0.4& 89.7$\pm$0.5& 89.1$\pm$0.5       & 92.7$\pm$0.5   & 92.3$\pm$0.4         & 92.4$\pm$0.4      \\ 
% GCFANet \cite{zheng2024global}& 92.3$\pm$1.2 & 92.4$\pm$1.1 & 92.6$\pm$1.2     & 92.9$\pm$1.2 & 93.1$\pm$1.1        & 92.7$\pm$0.9      \\ 
% PDF \cite{cao2024predictive}& \underline{93.0$\pm$0.5} & \underline{93.2$\pm$0.4} & \underline{93.2$\pm$0.4}     & \underline{93.3$\pm$0.6} & \underline{93.5$\pm$0.7}       & \underline{93.0$\pm$0.6}  \\ 

% \midrule
% QADM-Net  & \textbf{97.3$\pm$ 0.2} & \textbf{97.3$\pm$ 0.3} & \textbf{97.3$\pm$ 0.4} & \textbf{94.4$\pm$ 0.3} & \textbf{94.7$\pm$ 0.4} & \textbf{94.2$\pm$ 0.3} \\ \bottomrule
% \end{tabular}
% \end{adjustbox}
% \end{center}
% \label{sota table1}
% \end{table*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[]
\caption{Comparison of classification performance between the proposed MICINet and other reliable multimodal classification methods on the BRCA, ROSMAP, CUB, and UPMC FOOD101 datasets under different noise settings. The best results under various noise settings are indicated in bold, while the second-best results are underlined.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|ccc|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Data Type}                                          & \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{BRCA}                                            & \multicolumn{3}{c|}{ROSMAP}                              & \multicolumn{3}{c|}{CUB}                                             & \multicolumn{3}{c}{FOOD101}                                         \\ \cline{3-14} 
                                                                    &                          & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{F1} & AUC & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 \\ \hline
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=0$\\ $\epsilon=0$\end{tabular}} & MD \cite{han2022multimodal}                      & \multicolumn{1}{c|}{\underline{83.7}}   & \multicolumn{1}{c|}{\underline{83.6}}          & \underline{79.9}       & \multicolumn{1}{c|}{\underline{83.0}}   & \multicolumn{1}{c|}{\underline{83.0}}  & \underline{90.2}   & \multicolumn{1}{c|}{\underline{91.9}}   & \multicolumn{1}{c|}{\underline{92.0}}          & \underline{91.9}       & \multicolumn{1}{c|}{\underline{92.3}}   & \multicolumn{1}{c|}{\underline{92.7}}          & \underline{92.7}   \\ 
% \cline{2-14} 
                                                                    & MLCLNet \cite{zheng2023multi}                 & \multicolumn{1}{c|}{82.3}   & \multicolumn{1}{c|}{82.2}          & 78.6       & \multicolumn{1}{c|}{79.4}   & \multicolumn{1}{c|}{79.2}  & 89.3   & \multicolumn{1}{c|}{90.2}   & \multicolumn{1}{c|}{90.3}          & 89.9       & \multicolumn{1}{c|}{92.1}   & \multicolumn{1}{c|}{92.2}          & 92.1       \\ 
                                                                    % \cline{2-14} 
                                                                    & QMF \cite{zhang2023provable}                     & \multicolumn{1}{c|}{82.5}   & \multicolumn{1}{c|}{82.3}          & 79.1       & \multicolumn{1}{c|}{78.3}   & \multicolumn{1}{c|}{78.1}  & 85.1   & \multicolumn{1}{c|}{88.3}   & \multicolumn{1}{c|}{88.3}          & 87.9       & \multicolumn{1}{c|}{91.7}   & \multicolumn{1}{c|}{92.2}          & 92.1       \\ 
                                                                    % \cline{2-14} 
                                                                    & PDF \cite{cao2024predictive}                     & \multicolumn{1}{c|}{82.1}   & \multicolumn{1}{c|}{81.7}          & 76.4       & \multicolumn{1}{c|}{80.2}   & \multicolumn{1}{c|}{82.9}  & 87.1   & \multicolumn{1}{c|}{89.2}   & \multicolumn{1}{c|}{89.1}          & 88.9       & \multicolumn{1}{c|}{92.2}   & \multicolumn{1}{c|}{92.4}          & 92.3       \\ \cline{2-14} 
                                                                    & NCR \cite{huang2021learning}                     & \multicolumn{1}{c|}{77.9}   & \multicolumn{1}{c|}{75.3}          & 71.1       & \multicolumn{1}{c|}{79.2}   & \multicolumn{1}{c|}{80.0}  & 87.4   & \multicolumn{1}{c|}{83.9}   & \multicolumn{1}{c|}{83.7}          & 84.0       & \multicolumn{1}{c|}{91.1}   & \multicolumn{1}{c|}{91.5}          & 91.5       \\ 
                                                                    % \cline{2-14} 
                                                                    & ALBEF \cite{li2021align}                   & \multicolumn{1}{c|}{78.2}   & \multicolumn{1}{c|}{75.2}          & 71.3       & \multicolumn{1}{c|}{79.0}   & \multicolumn{1}{c|}{80.3}  & 86.5   & \multicolumn{1}{c|}{84.0}   & \multicolumn{1}{c|}{84.2}          & 84.2       & \multicolumn{1}{c|}{91.2}   & \multicolumn{1}{c|}{91.5}          & 91.4       \\ 
                                                                    % \cline{2-14} 
                                                                    & SMILE \cite{zeng2023semantic}                   & \multicolumn{1}{c|}{80.5}   & \multicolumn{1}{c|}{81.0}          & 78.2       & \multicolumn{1}{c|}{79.2}   & \multicolumn{1}{c|}{79.5}  & 87.4   & \multicolumn{1}{c|}{88.6}   & \multicolumn{1}{c|}{88.6}          & 88.3       & \multicolumn{1}{c|}{91.3}   & \multicolumn{1}{c|}{91.7}          & 91.6       \\ \cline{2-14} 
                                                                    & MICINet                  & \multicolumn{1}{c|}{\textbf{85.6}}   & \multicolumn{1}{c|}{\textbf{85.7}}          & \textbf{82.4}       & \multicolumn{1}{c|}{\textbf{87.7}}   & \multicolumn{1}{c|}{\textbf{88.1}}  & \textbf{93.2}   & \multicolumn{1}{c|}{\textbf{92.8}}   & \multicolumn{1}{c|}{\textbf{92.8}}          & \textbf{92.6}       & \multicolumn{1}{c|}{\textbf{93.2}}   & \multicolumn{1}{c|}{\textbf{93.4}}          & \textbf{93.2}       \\ \hline \hline
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=0$\\ $\epsilon=1$\end{tabular}} & MD \cite{han2022multimodal}                      & \multicolumn{1}{c|}{\underline{78.3}}   & \multicolumn{1}{c|}{\underline{77.4}}          & 69.0       & \multicolumn{1}{c|}{\underline{75.5}}   & \multicolumn{1}{c|}{\underline{79.0}}  & \underline{83.2}   & \multicolumn{1}{c|}{\underline{90.1}}   & \multicolumn{1}{c|}{\underline{90.1}}          & \underline{90.2}       & \multicolumn{1}{c|}{\underline{92.2}}   & \multicolumn{1}{c|}{\underline{92.5}}          & \underline{92.4}      \\ 
% \cline{2-14} 
                                                                    & MLCLNet \cite{zheng2023multi}                & \multicolumn{1}{c|}{77.0}   & \multicolumn{1}{c|}{76.6}          & 70.8       & \multicolumn{1}{c|}{74.1}   & \multicolumn{1}{c|}{78.5}  & 82.5   & \multicolumn{1}{c|}{86.4}   & \multicolumn{1}{c|}{86.5}          & 86.3       & \multicolumn{1}{c|}{88.7}   & \multicolumn{1}{c|}{88.5}          & 88.7      \\ 
                                                                    % \cline{2-14} 
                                                                    & QMF \cite{zhang2023provable}                     & \multicolumn{1}{c|}{76.4}   & \multicolumn{1}{c|}{75.8}          & \underline{71.0}       & \multicolumn{1}{c|}{70.8}   & \multicolumn{1}{c|}{75.0}  & 78.9   & \multicolumn{1}{c|}{86.5}   & \multicolumn{1}{c|}{86.5}          & 86.4       & \multicolumn{1}{c|}{91.2}   & \multicolumn{1}{c|}{91.7}          & 91.7       \\ 
                                                                    % \cline{2-14} 
                                                                    & PDF \cite{cao2024predictive}                    & \multicolumn{1}{c|}{76.0}   & \multicolumn{1}{c|}{73.6}          & 67.4       & \multicolumn{1}{c|}{68.9}   & \multicolumn{1}{c|}{73.6}  & 78.6   & \multicolumn{1}{c|}{88.2}   & \multicolumn{1}{c|}{88.2}          & 88.2       & \multicolumn{1}{c|}{89.2}   & \multicolumn{1}{c|}{90.1}          & 90.1      \\ \cline{2-14} 
                                                                    & NCR \cite{huang2021learning}                     & \multicolumn{1}{c|}{71.5}   & \multicolumn{1}{c|}{66.8}          & 53       & \multicolumn{1}{c|}{67.9}   & \multicolumn{1}{c|}{71.7}  & 70.4   & \multicolumn{1}{c|}{82.0}   & \multicolumn{1}{c|}{81.8}          & 81.9       & \multicolumn{1}{c|}{88.0}   & \multicolumn{1}{c|}{89.1}          & 89.1      \\ 
                                                                    % \cline{2-14} 
                                                                    & ALBEF \cite{li2021align}                   & \multicolumn{1}{c|}{71.2}   & \multicolumn{1}{c|}{66.5}          & 52.6       & \multicolumn{1}{c|}{68.2}   & \multicolumn{1}{c|}{71.9}  & 70.6   & \multicolumn{1}{c|}{81.7}   & \multicolumn{1}{c|}{81.8}          & 81.8       & \multicolumn{1}{c|}{87.6}   & \multicolumn{1}{c|}{88.4}          & 88.3   \\ 
                                                                    % \cline{2-14} 
                                                                    & SMILE \cite{zeng2023semantic}                   & \multicolumn{1}{c|}{71.5}   & \multicolumn{1}{c|}{67.0}          & 54.3       & \multicolumn{1}{c|}{67.6}   & \multicolumn{1}{c|}{72.3}  & 71.1   & \multicolumn{1}{c|}{82.0}   & \multicolumn{1}{c|}{81.9}          & 81.9       & \multicolumn{1}{c|}{89.7}   & \multicolumn{1}{c|}{90.1}          & 90.0       \\ \cline{2-14} 
                                                                    & MICINet                  & \multicolumn{1}{c|}{\textbf{84.4}}   & \multicolumn{1}{c|}{\textbf{84.0}}          & \textbf{80.0}       & \multicolumn{1}{c|}{\textbf{81.1}}   & \multicolumn{1}{c|}{\textbf{81.1}}  & \textbf{85.6}   & \multicolumn{1}{c|}{\textbf{92.8}}   & \multicolumn{1}{c|}{\textbf{92.8}}          & \textbf{92.6}       & \multicolumn{1}{c|}{\textbf{93.0}}   & \multicolumn{1}{c|}{\textbf{93.1}}          & \textbf{93.1}       \\ \hline \hline
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=0$\end{tabular}} & MD \cite{han2022multimodal}                      & \multicolumn{1}{c|}{\underline{72.6}}   & \multicolumn{1}{c|}{\underline{69.7}}          & \underline{54.5}       & \multicolumn{1}{c|}{\underline{72.6}}   & \multicolumn{1}{c|}{\underline{73.9}}  & \underline{78.1}   & \multicolumn{1}{c|}{69.4}   & \multicolumn{1}{c|}{69.3}          & 68.3       & \multicolumn{1}{c|}{\underline{74.1}}   & \multicolumn{1}{c|}{74.2}          & 74.2   \\ 
% \cline{2-14} 
                                                                    & MLCLNet \cite{zheng2023multi}                 & \multicolumn{1}{c|}{70.4}   & \multicolumn{1}{c|}{68.2}          & 50.5       & \multicolumn{1}{c|}{70.4}   & \multicolumn{1}{c|}{70.6}  & 76.2   & \multicolumn{1}{c|}{69.2}   & \multicolumn{1}{c|}{68.8}          & 68.1       & \multicolumn{1}{c|}{73.2}   & \multicolumn{1}{c|}{73.9}          & 74.0      \\ 
                                                                    % \cline{2-14} 
                                                                    & QMF \cite{zhang2023provable}                      & \multicolumn{1}{c|}{68.8}   & \multicolumn{1}{c|}{64.9}          & 50.3       & \multicolumn{1}{c|}{70.8}   & \multicolumn{1}{c|}{69.3}  & 73.1   & \multicolumn{1}{c|}{70.3}   & \multicolumn{1}{c|}{70.2}          & 69.5       & \multicolumn{1}{c|}{73.4}   & \multicolumn{1}{c|}{74.3}         & 74.3       \\ 
                                                                    % \cline{2-14} 
                                                                    & PDF \cite{cao2024predictive}                      & \multicolumn{1}{c|}{65.0}   & \multicolumn{1}{c|}{56.7}          & 41.7       & \multicolumn{1}{c|}{67.9}   & \multicolumn{1}{c|}{72.8}  & 77.3   & \multicolumn{1}{c|}{69.4}   & \multicolumn{1}{c|}{69.3}          & 69.0       & \multicolumn{1}{c|}{\underline{74.1}}   & \multicolumn{1}{c|}{74.2}          & 74.2       \\ \cline{2-14} 
                                                                    & NCR \cite{huang2021learning}                     & \multicolumn{1}{c|}{69.6}   & \multicolumn{1}{c|}{65.4}          & 51.1       & \multicolumn{1}{c|}{69.8}   & \multicolumn{1}{c|}{70.9}  & 76.3   & \multicolumn{1}{c|}{\underline{71.6}}   & \multicolumn{1}{c|}{\underline{71.2}}          & \underline{71.9}       & \multicolumn{1}{c|}{73.9}   & \multicolumn{1}{c|}{\underline{74.4}}          & \underline{74.4}       \\ 
                                                                    % \cline{2-14} 
                                                                    & ALBEF \cite{li2021align}                    & \multicolumn{1}{c|}{69.1}   & \multicolumn{1}{c|}{65.2}          & 50.8       & \multicolumn{1}{c|}{70.0}   & \multicolumn{1}{c|}{73.1}  & 77.6   & \multicolumn{1}{c|}{71.5}   & \multicolumn{1}{c|}{\underline{71.2}}          & 71.8       & \multicolumn{1}{c|}{74.0}   & \multicolumn{1}{c|}{74.2}          & 74.2       \\ 
                                                                    % \cline{2-14} 
                                                                    & SMILE \cite{zeng2023semantic}                    & \multicolumn{1}{c|}{68.9}   & \multicolumn{1}{c|}{64.6}          & 50.2       & \multicolumn{1}{c|}{69.1}   & \multicolumn{1}{c|}{70.0}  & 77.4   & \multicolumn{1}{c|}{62.2}   & \multicolumn{1}{c|}{62.3}          & 61.9       & \multicolumn{1}{c|}{73.2}   & \multicolumn{1}{c|}{73.4}          & 73.4       \\ \cline{2-14} 
                                                                    & MICINet                  & \multicolumn{1}{c|}{\textbf{73.4}}   & \multicolumn{1}{c|}{\textbf{73.0}}          & \textbf{69.4}       & \multicolumn{1}{c|}{\textbf{79.2}}   & \multicolumn{1}{c|}{\textbf{79.6}}  & \textbf{83.5}   & \multicolumn{1}{c|}{\textbf{73.0}}   & \multicolumn{1}{c|}{\textbf{73.3}}          & \textbf{72.6}       & \multicolumn{1}{c|}{\textbf{75.5}}   & \multicolumn{1}{c|}{\textbf{75.6}}          & \textbf{75.6}      \\ \hline \hline
\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=1$ \end{tabular}} & MD \cite{han2022multimodal}                      & \multicolumn{1}{c|}{65.8}   & \multicolumn{1}{c|}{58.5}          & 42.4       & \multicolumn{1}{c|}{68.9}   & \multicolumn{1}{c|}{\underline{72.7}}  & \underline{72.3}   & \multicolumn{1}{c|}{68.5}   & \multicolumn{1}{c|}{67.9}          & 67.2       & \multicolumn{1}{c|}{\underline{73.3}}   & \multicolumn{1}{c|}{73.4}          & 73.4   \\ 
% \cline{2-14} 
                                                                    & MLCLNet \cite{zheng2023multi}                & \multicolumn{1}{c|}{65.1}   & \multicolumn{1}{c|}{58.2}          & 42.1       & \multicolumn{1}{c|}{64.9}   & \multicolumn{1}{c|}{70.1}  & 71.5   & \multicolumn{1}{c|}{67.6}   & \multicolumn{1}{c|}{67.7}          & 68.3       & \multicolumn{1}{c|}{72.9}   & \multicolumn{1}{c|}{72.8}          & 73.1    \\ 
                                                                    % \cline{2-14} 
                                                                    & QMF \cite{zhang2023provable}                      & \multicolumn{1}{c|}{65.0}   & \multicolumn{1}{c|}{58.9}          & 44.4       & \multicolumn{1}{c|}{62.2}   & \multicolumn{1}{c|}{69.2}  & 70.1   & \multicolumn{1}{c|}{64.8}   & \multicolumn{1}{c|}{65.1}          & 64.8       & \multicolumn{1}{c|}{73.2}   & \multicolumn{1}{c|}{\underline{73.7}}          & \underline{73.7}       \\ 
                                                                    % \cline{2-14} 
                                                                    & PDF \cite{cao2024predictive}                      & \multicolumn{1}{c|}{60.5}   & \multicolumn{1}{c|}{48.1}          & 31.0       & \multicolumn{1}{c|}{66.0}   & \multicolumn{1}{c|}{72.7}  & 72.1   & \multicolumn{1}{c|}{68.5}   & \multicolumn{1}{c|}{68.0}          & 67.8      & \multicolumn{1}{c|}{72.2}   & \multicolumn{1}{c|}{72.9}          & 72.9      \\ \cline{2-14} 
                                                                    & NCR \cite{huang2021learning}                      & \multicolumn{1}{c|}{\underline{68.1}}   & \multicolumn{1}{c|}{\underline{66.2}}          & 48.2       & \multicolumn{1}{c|}{68.9}   & \multicolumn{1}{c|}{70.3}  & 68.0   & \multicolumn{1}{c|}{\underline{70.5}}   & \multicolumn{1}{c|}{70.1}          & \underline{71.0}       & \multicolumn{1}{c|}{72.0}   & \multicolumn{1}{c|}{72.8}          & 72.8      \\ 
                                                                    % \cline{2-14} 
                                                                    & ALBEF \cite{li2021align}                    & \multicolumn{1}{c|}{67.9}   & \multicolumn{1}{c|}{66.0}          & \underline{48.3}       & \multicolumn{1}{c|}{\underline{69.2}}   & \multicolumn{1}{c|}{72.1}  & 72.2   & \multicolumn{1}{c|}{70.0}   & \multicolumn{1}{c|}{\underline{70.2}}          & 70.6       & \multicolumn{1}{c|}{72.8}   & \multicolumn{1}{c|}{73.4}          & 73.2   \\ 
                                                                    % \cline{2-14} 
                                                                    & SMILE \cite{zeng2023semantic}                   & \multicolumn{1}{c|}{66.8}   & \multicolumn{1}{c|}{65.9}          & 47.8       & \multicolumn{1}{c|}{68.4}   & \multicolumn{1}{c|}{68.9}  & 70.2   & \multicolumn{1}{c|}{56.8}   & \multicolumn{1}{c|}{56.4}          & 55.9       & \multicolumn{1}{c|}{70.8}   & \multicolumn{1}{c|}{71.3}          & 71.2       \\ \cline{2-14} 
                                                                    & MICINet                  & \multicolumn{1}{c|}{\textbf{69.2}}   & \multicolumn{1}{c|}{\textbf{67.9}}          & \textbf{60.1}       & \multicolumn{1}{c|}{\textbf{76.4}}   & \multicolumn{1}{c|}{\textbf{78.6}}  & \textbf{77.2}   & \multicolumn{1}{c|}{\textbf{73.0}}   & \multicolumn{1}{c|}{\textbf{73.0}}          & \textbf{72.4}       & \multicolumn{1}{c|}{\textbf{75.3}}   & \multicolumn{1}{c|}{\textbf{75.1}}          & \textbf{75.2}       \\ \hline
\end{tabular}%
}
\label{tab:sota}
\end{table*}


\subsection{Experimental Settings} \label{sec:exp-set}

\subsubsection{Datasets}
Experiments are conducted on four commonly used multimodal datasets from different tasks including biomedical classification and image-text classification tasks.
\begin{itemize}
    \item \textbf {BRCA}: BRCA \cite{lingle9cancer} is a dataset for breast invasive carcinoma PAM50 subtype classification. The dataset comprises 875 samples, with each sample containing features from three modalities: mRNA expression data (mRNA), DNA methylation data (meth), and miRNA expression data (miRNA). These samples are categorized into five subtypes: Normal-like, Basal-like, HER2-enriched, Luminal A, and Luminal B, with 115, 131, 46, 435, and 147 samples respectively. BCRA can be obtained from The Cancer Genome Atlas program (TCGA) \footnote{\url{https://www.cancer.gov/aboutnci/organization/ccg/research/structuralgenomics/tcga}}.
    \item \textbf{ROSMAP}: ROSMAP \cite{mukherjee2015religious,a2012overview,de2018multi} is a dataset containing samples from Alzheimer's patients and normal control subjects. The dataset consists of 351 samples, including 182 Alzheimer's disease patients and 169 normal control samples. Each sample includes data from three modalities: mRNA expression data (mRNA), DNA methylation data (meth), and miRNA expression data (miRNA).
    \item \textbf{CUB}: Caltech-UCSD Birds dataset \cite{wah2011caltech} comprises 200 categories of birds. It contains a total of 11,788 samples, with each sample including data from two modalities: images of birds and their corresponding textual descriptions.
    \item \textbf{UPMC FOOD101}: The UPMC FOOD101 dataset \cite{wang2015recipe} comprises food images from 101 categories obtained through Google Image search and corresponding textual descriptions. This dataset contains 90,704 samples, where each sample's image and text are collected from uncontrolled environments, thus inherently containing noise.
\end{itemize}

\subsubsection{Compared Methods}
\paragraph{Modality-Specific Noise Removal Methods} Several state-of-the-art methods capable of reliable classification on multimodal data with modality-specific noise are introduced, including multimodal dynamics (\textbf{MD}) \cite{han2022multimodal}, multi-level confidence learning (\textbf{MLCLNet}) \cite{zheng2023multi}, quality-aware multimodal fusion (\textbf{QMF}) \cite{zhang2023provable}, and predictive dynamic fusion (\textbf{PDF}) \cite{cao2024predictive}. 
\paragraph{Cross-Modality Noise Removal Methods} Several representative approaches from this field are compared in the experiments, including Noisy Correspondence Rectifier (\textbf{NCR}) \cite{huang2021learning}, Align before Fuse (\textbf{ALBEF}) \cite{li2021align}, and Semantic Invariance Learning (\textbf{SMILE}) \cite{zeng2023semantic}.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[]
\caption{Ablation study of different components of the proposed MICINet on the BRCA, ROSMAP, CUB, and UPMC FOOD101 datasets. The best results are highlighted in bold.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|ccc|ccc|ccc|ccc|ccc}
\hline
\multirow{2}{*}{Data Type}                                         & \multicolumn{3}{c|}{Methods}                               & \multicolumn{3}{c|}{BRCA}                                            & \multicolumn{3}{c|}{ROSMAP}                              & \multicolumn{3}{c|}{CUB}                                             & \multicolumn{3}{c}{FOOD101}                                         \\ \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\textbf{SICI}} & \multicolumn{1}{c|}{\textbf{CMIC}} & \textbf{RDP} & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{F1} & AUC & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 & \multicolumn{1}{c|}{ACC} & \multicolumn{1}{c|}{WeightedF1} & MacroF1 \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=0$\end{tabular}} & \multicolumn{1}{c|}{\ding{55}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55} & \multicolumn{1}{c|}{59.7}   & \multicolumn{1}{c|}{47.3}          & 30.2       & \multicolumn{1}{c|}{71.7}   & \multicolumn{1}{c|}{74.1}  & 78.1   & \multicolumn{1}{c|}{71.2}   & \multicolumn{1}{c|}{71.1}          & 70.3       & \multicolumn{1}{c|}{72.1}   & \multicolumn{1}{c|}{72.5}          & 72.5     \\ 
% \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55}   & \multicolumn{1}{c|}{71.5} & \multicolumn{1}{c|}{70.2}   & 64.6 & \multicolumn{1}{c|}{74.5}   & \multicolumn{1}{c|}{74.8}  & 79.6   & \multicolumn{1}{c|}{72.3}   & \multicolumn{1}{c|}{72.4}          & 71.7       & \multicolumn{1}{c|}{73.4}   & \multicolumn{1}{c|}{73.5}          & 73.5     \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{55}    & \multicolumn{1}{c|}{72.4}   & \multicolumn{1}{c|}{72.1}          & 67.8       & \multicolumn{1}{c|}{78.3}   & \multicolumn{1}{c|}{78.9}  & 81.7   & \multicolumn{1}{c|}{\textbf{73.0}}   & \multicolumn{1}{c|}{72.7}          & 72.0       & \multicolumn{1}{c|}{74.8}   & \multicolumn{1}{c|}{74.7}          & 74.7     \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{51}   & \multicolumn{1}{c|}{\textbf{73.4}}   & \multicolumn{1}{c|}{\textbf{73.0}}          & \textbf{69.4}       & \multicolumn{1}{c|}{\textbf{79.2}}   & \multicolumn{1}{c|}{\textbf{79.6}}  & \textbf{83.5}   & \multicolumn{1}{c|}{\textbf{73.0}}   & \multicolumn{1}{c|}{\textbf{73.3}}          & \textbf{72.6}       & \multicolumn{1}{c|}{\textbf{75.5}}   & \multicolumn{1}{c|}{\textbf{75.6}}          & \textbf{75.6}        \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=0$\\ $\epsilon=1$\end{tabular}} & \multicolumn{1}{c|}{\ding{55}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55}   & \multicolumn{1}{c|}{67.6}   & \multicolumn{1}{c|}{33.1}          & 13.3       & \multicolumn{1}{c|}{67.9}   & \multicolumn{1}{c|}{73.0}  & 76.4   & \multicolumn{1}{c|}{84.7}   & \multicolumn{1}{c|}{84.8}          & 84.7       & \multicolumn{1}{c|}{84.5}   & \multicolumn{1}{c|}{84.2}          & 84.3       \\ 
% \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55}   & \multicolumn{1}{c|}{74.8}   & \multicolumn{1}{c|}{41.6}          & 23.6       & \multicolumn{1}{c|}{74.5}   & \multicolumn{1}{c|}{77.7}  & 83.1   & \multicolumn{1}{c|}{91.0}   & \multicolumn{1}{c|}{91.0}          & 90.6       & \multicolumn{1}{c|}{90.6}   & \multicolumn{1}{c|}{90.5}          & 90.3      \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{55}   & \multicolumn{1}{c|}{82.5}   & \multicolumn{1}{c|}{82.3}          & 78.9       & \multicolumn{1}{c|}{80.2}   & \multicolumn{1}{c|}{\textbf{81.7}}  & 85.0   & \multicolumn{1}{c|}{92.0}   & \multicolumn{1}{c|}{91.9}          & 91.9       & \multicolumn{1}{c|}{92.2}   & \multicolumn{1}{c|}{92.1}          & 92.1       \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{51}   & \multicolumn{1}{c|}{\textbf{84.4}}   & \multicolumn{1}{c|}{\textbf{84.0}}          & \textbf{80.0}       & \multicolumn{1}{c|}{\textbf{81.1}}   & \multicolumn{1}{c|}{81.1}  & \textbf{86.5}   & \multicolumn{1}{c|}{\textbf{92.8}}   & \multicolumn{1}{c|}{\textbf{92.8}}          & \textbf{92.6}       & \multicolumn{1}{c|}{\textbf{93.0}}   & \multicolumn{1}{c|}{\textbf{93.1}}          & \textbf{93.1}      \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=1$\end{tabular}} & \multicolumn{1}{c|}{\ding{55}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55}   & \multicolumn{1}{c|}{67.0}   & \multicolumn{1}{c|}{31.8}          & 12.5       & \multicolumn{1}{c|}{48.5}   & \multicolumn{1}{c|}{48.3}  & 51.2   & \multicolumn{1}{c|}{70.3}   & \multicolumn{1}{c|}{70.2}          & 69.7       & \multicolumn{1}{c|}{70.2}   & \multicolumn{1}{c|}{70.5}          & 70.6    \\ 
% \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{55}}   & \ding{55}   & \multicolumn{1}{c|}{67.7}   & \multicolumn{1}{c|}{64.5}          & 50.0       & \multicolumn{1}{c|}{68.3}   & \multicolumn{1}{c|}{68.5}  & 72.6   & \multicolumn{1}{c|}{72.0}   & \multicolumn{1}{c|}{72.1}          & 72.0       & \multicolumn{1}{c|}{73.2}   & \multicolumn{1}{c|}{73.2}          & 73.2 \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{55}   & \multicolumn{1}{c|}{69.0}   & \multicolumn{1}{c|}{66.8}          & 59.4       & \multicolumn{1}{c|}{75.5}   & \multicolumn{1}{c|}{76.0}  & 76.5   & \multicolumn{1}{c|}{72.4}   & \multicolumn{1}{c|}{72.3}          & 72.0       & \multicolumn{1}{c|}{74.4}   & \multicolumn{1}{c|}{74.5}          & 74.4   \\ 
                                                                   % \cline{2-16} 
                                                                   & \multicolumn{1}{c|}{\ding{51}}    & \multicolumn{1}{c|}{\ding{51}}   & \ding{51}   & \multicolumn{1}{c|}{\textbf{69.2}}   & \multicolumn{1}{c|}{\textbf{67.9}}          & \textbf{60.1}       & \multicolumn{1}{c|}{\textbf{76.4}}   & \multicolumn{1}{c|}{\textbf{78.6}}  & \textbf{77.2}   & \multicolumn{1}{c|}{\textbf{73.0}}   & \multicolumn{1}{c|}{\textbf{73.0}}          & \textbf{72.4}       & \multicolumn{1}{c|}{\textbf{75.3}}   & \multicolumn{1}{c|}{\textbf{75.1}}          & \textbf{75.2}      \\ \hline
% \multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}x=1\\ y=1\end{tabular}} & \multicolumn{1}{c|}{=}    & \multicolumn{1}{c|}{=}   & =   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}  & 1   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       \\ \cline{2-16} 
%                                                                    & \multicolumn{1}{c|}{=}    & \multicolumn{1}{c|}{=}   & =   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}  & 1   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       \\ \cline{2-16} 
%                                                                    & \multicolumn{1}{c|}{=}    & \multicolumn{1}{c|}{=}   & =   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}  & 1   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       \\ \cline{2-16} 
%                                                                    & \multicolumn{1}{c|}{=}    & \multicolumn{1}{c|}{=}   & =   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}  & 1   & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       & \multicolumn{1}{c|}{1}   & \multicolumn{1}{c|}{1}          & 1       \\ \hline
\end{tabular}%
}
\label{tab:abl-sdy}
\end{table*}

\subsubsection{Evaluation Metrics}
\paragraph{BRCA \& CUB \& UMPC FOOD101} BRCA, CUB and UMPC FOOD101 provide multi-class classification task. Three metrics, including accuracy (ACC), average F1 score weighted by support (WeightedF1), and macro-averaged F1 score (MacroF1), are employed to evaluate the performance of different methods.

\paragraph{ROSMAP} ROSMAP provides binary classification task. The accuracy (ACC), F1 score (F1), and area under the receiver operating characteristic curve (AUC) of different methods are reported by experiment.

\subsubsection{Implementation Details}
\paragraph{Training Details} We implement the proposed method and other comparing methods on the PyTorch 1.12.0 and cuda 11.6 platform, running on Ubuntu 20.04.2 LTS, utilizing one GPU (NVIDIA RTX A6000 with 48 GB of memory) and CPU of AMD EPYC 75F3. The Adam optimizer with learning rate decay is employed to train the model. The initial learning rate of the Adam optimizer is set to 1e-4, the weight decay is set to 1e-4, and the multiplicative factor of the learning rate decay is set to 0.2. All the quantitative results of the proposed MICINet are the average of five random seeds. 

\paragraph{Model Implementation Details} To evaluate the classification performance of \textbf{NCR} \cite{huang2021learning} and \textbf{ALBEF} \cite{li2021align}, the sample features after being filtered or rectified are utilized to train the classifier. For \textbf{SMILE} \cite{zeng2023semantic}, the sample features output by its encoders are used to train the classifier.

\paragraph{Experimental Details} Two types of noise are involved in the experiments: modality-specific and cross-modality noise. The modality-specific noise is implemented following previous works \cite{geng2021uncertainty,zhou2023calm,cao2024predictive}, and $\epsilon$ is its intensity. The implementation of cross-modality noise follows SMILE \cite{zeng2023semantic}, where unaligned samples are introduced by shuffling the features of each modality for a random subset of samples with a proportion of $\eta$.

% The cross-modality noise is implemented by adding unaligned features following \textbf{SMILE} \cite{zeng2023semantic}, and $\eta$ is the unaligned rate.

% The implementation and addition of the two types of noise in the experiments follow previous works \cite{geng2021uncertainty,zhang2023provable,cao2024predictive} and \cite{zeng2023semantic}, respectively. $\epsilon$ and $\eta$ refer to the intensity of modality-specific noise and cross-modality noise, respectively.

% Two factors are taken into account in the experiments to verify the performance of different methods: the unalignment of multimodal samples, and the noise contained in multimodal data (i.e., different data/sample qualities). The unalignment of multimodal samples is implemented following \textbf{SMILE} \cite{zeng2023semantic}, and $\eta$ is the unaligned rate. The multimodal noise is added to the dataset following previous works \cite{geng2021uncertainty,zhou2023calm,cao2024predictive}, and $\epsilon$ represents the noise intensity.

% The first stage is trained for 40 epochs on the BRCA, ROSMAP, and CUB datasets, and 50 epochs on the FOOD101 dataset. While training the informativeness-based dynamic parameters mechanism, the settings of the Adam optimizer are the same as in the first stage, while the training lasts for 100 epochs on the BRCA, ROSMAP, and CUB datasets and 200 epochs on the FOOD101 dataset.

\subsection{Questions To Be Verified} \label{sec:ques}
For clarity, the main questions to be verified in the following experiments are highlighted here:
\begin{itemize}
    \item RQ1: How does the classification performance and reliability of MICINet compare to other reliable multimodal classification methods under different noise settings?
    \item RQ2: Do all components of MICINet contribute to achieving reliable classification?
    \item RQ3: What is the effectiveness of the proposed global \textit{ICI} learning module (\textbf{\textit{GICI}})?
    \item RQ4: What is the effectiveness of the proposed sample-adaptive cross-modality information compensation module (\textbf{\textit{CMIC}})?
    \item RQ5: Are the two types of noise removed during the learning of MICINet?
    % \item RQ3: Does aligning discriminative power imply achieving cross-modal semantic alignment?
    % \item RQ4: What information is learned through the global inter-class confusion information?
    % \item RQ5: What is the contribution of global inter-class confusion information to feature learning at the sample level?
    % \item RQ6: What information is obtained through cross-modal information querying?
\end{itemize}

\section{Experimental Results And Analysis} \label{sec:exp-res}
This section presents and analyses the experiment results on four datasets and verifies the questions mentioned in Section \ref{sec:ques}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/fig_ablation_visual.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{T-SNE visualizations of sample multimodal features derived from the network with different component combinations under the noise setting of $\eta=20\%, \epsilon=5$ on the ROSMAP dataset.}
\label{fig:abl}
\end{figure}

\subsection{Classification Performance (RQ1)}
The classification performance of different methods is evaluated on the BRCA, ROSMAP, CUB, and UPMC FOOD101 datasets under different settings of noise, including ``$\eta=0,\epsilon=0$'', ``$\eta=0,\epsilon=1$'',``$\eta=20\%,\epsilon=0$'', and ``$\eta=20\%,\epsilon=1$''. The results are compared in Table \ref{tab:sota}. From the results, the following conclusion can be drawn.
\begin{itemize}
    \item \textbf{Cross-modality noise removal methods have limitations in handling modality-specific noise.} Under noise-free or modality-specific noise-only conditions, methods for modality-specific noise removal generally outperform cross-modality noise removal methods. By comparing the results under noise settings ``$\eta=0,\epsilon=0$'' and ``$\eta=0,\epsilon=1$'', it can be observed that all modality-specific noise removal methods consistently surpass cross-modality noise removal methods on most datasets.
    \item \textbf{Modality-specific noise removal methods are less effective at handling cross-modality noise.} With the introduction of cross-modality noise (unaligned samples), the advantage of modality-specific noise removal methods diminishes and is partially surpassed by cross-modality noise removal methods. Comparing the results under noise settings ``$\eta=20\%,\epsilon=0$'' and ``$\eta=20\%,\epsilon=1$'' reveals that cross-modality noise removal methods achieve comparable or superior performance to modality-specific noise removal methods on most datasets.
    \item \textbf{MICINet outperforms both modality-specific and cross-modality noise removal methods under all noise settings.} Under the modality-specific noise-only setting (``$\eta=0,\epsilon=1$''), MICINet outperforms other state-of-the-art modality-specific noise removal methods. Under the cross-modality noise-only setting (``$\eta=20\%,\epsilon=0$''), MICINet outperforms other cross-modality noise removal methods. The possible reason is that MICINet can achieve more reliable noise removal by considering noise at both the global and individual levels. Under the combined conditions of both modality-specific and cross-modality noise (``$\eta=20\%,\epsilon=1$''), MICINet outperforms both modality-specific and cross-modality noise removal methods, indicating its capability to more effectively remove both types of noise.
    
    % As shown in the Table \ref{tab:sota}, cross-modality noise removal methods generally experience a more significant performance drop when transitioning from ``$\eta=20\%, \epsilon=0$'' to ``$\eta=20\%, \epsilon=5$'' than when transitioning from ``$\eta=0, \epsilon=0$'' to ``$\eta=0, \epsilon=5$''. In contrast, MICINet almost eliminates this issue. This suggests that the introduction of noise leads to a decline in the ability of other unaligned-aware methods to handle unaligned samples. MICINet, however, achieve more reliable sample alignment under noisy conditions by leveraging global information and sample-adaptive capabilities.
\end{itemize}

% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[t]{\textwidth}
%         \centering
%         \includegraphics[width=\columnwidth]{fig/WhatGICILearn_BRCA.pdf}
%         \caption{Results on the BRCA dataset.} 
%         \label{fig:noise-free-brca}
%     \end{subfigure}
%     \vspace{1cm}
%     \begin{subfigure}[t]{\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/WhatGICILearn_ROSMAP.pdf}
%         \caption{Results on the ROSMAP dataset.}
%         \label{fig:noise-free-food}
%     \end{subfigure}
%     \caption{The correlation of the learned information in G-ICI with the noise add on the BRCA and ROSMAP datasets in each epoch of training.}
%     \label{fig:main}
% \end{figure}

\begin{figure}[h]
    \centering
    % 第一个子图
    \subfloat[Results on the BRCA dataset.]{
        \includegraphics[width=\columnwidth]{fig/WhatGICILearn_BRCA.pdf}
        \label{fig:gici-brca}
    }
    % 添加间隔
    \hfill
    % 第二个子图
    \subfloat[Results on the ROSMAP dataset.]{
        \includegraphics[width=\columnwidth]{fig/WhatGICILearn_ROSMAP.pdf}
        \label{fig:gici-rosmap}
    }
    \caption{Variation of the negative log probability of the added noise in the global \textit{ICI} distribution learned by \textbf{\textit{GICI}} during training on the BRCA and ROSMAP datasets.}
    \label{fig:gici}
\end{figure}

\begin{figure*}[t]
\centering
% \includegraphics[width=\linewidth]{fig/fig_confusion_degree_BRCA_CUB.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\subfloat[Results on the BRCA dataset.]{
        \includegraphics[width=0.57\textwidth]{fig/fig_confusion_degree_BRCA.pdf}
        \label{fig:beta-brca}
    }
    % 添加间隔
    \hfill
    % 第二个子图
    \subfloat[Results on the CUB dataset.]{
        \includegraphics[width=0.395\textwidth]{fig/fig_confusion_degree_BRCA_CUB.pdf}
        \label{fig:beta-rosmap}
    }
\caption{The visualization results of the inter-class confusion degree estimated by \textbf{\textit{GICI}} in each modality for the BRCA and CUB datasets under noise-free and noise-corrupted settings. For clarity, the CUB dataset only displays the results between the first 10 classes.}
\label{fig:conf-degree}
\end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table}[]
\caption{Comparison of results for uniform and weighted mixture of \textit{ICI} distributions using estimated confusion degree $\beta$ on the ROSMAP dataset under different noise settings. The best results are highlighted in bold.}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|c|c|c|c}
\hline
Data Type                                                          & Method & ACC & F1 & AUC \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=0$\\ $\epsilon=0$\end{tabular}} & uniform mix      & 83.0   & 83.3  & 89.1   \\ \cline{2-5} 
                                                                   & mix with $\beta$      & \textbf{87.7}   & \textbf{88.1}  & \textbf{93.2}   \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=0$\\ $\epsilon=1$\end{tabular}} & uniform mix      & 75.5   & 77.0  & 79.9   \\ \cline{2-5} 
                                                                   & mix with $\beta$      & \textbf{81.1}   & \textbf{81.1}  & \textbf{85.6}   \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=0$\end{tabular}} & uniform mix      & 72.6   & 73.4  & 77.8   \\ \cline{2-5} 
                                                                   & mix with $\beta$      & \textbf{79.2}   & \textbf{79.2}  & \textbf{83.5}   \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}$\eta=20\%$\\ $\epsilon=1$\end{tabular}} & uniform mix      & 70.8   & 72.1  & 70.3   \\ \cline{2-5} 
                                                                   & mix with $\beta$      & \textbf{76.4}   & \textbf{78.6}  & \textbf{77.2}   \\ \hline
\end{tabular}%
}
\label{tab:weight_mix}
\end{table}

\subsection{Abliation Study (RQ2)}\label{sec:abl-study}
A comprehensive ablation study is conducted on the four datasets to verify the effectiveness of different components of the proposed MICINet. Specifically, the experiments validate the effectiveness of (1) the global-guided sample \textit{ICI} learning (\textbf{SICI}); (2) sample-adaptive cross-modal information compensation (\textbf{CMIC}); (3) relative discriminative power (\textbf{RDP}) introduced in \textbf{CMIC}. The results of the model under different components and data settings are shown in Table \ref{tab:abl-sdy}. \textbf{SICI} is removed from the model by setting the loss term $\mathcal{L}_{sici}$ to 0, i.e., the global $ICI$ distribution is no longer used to guide the feature learning of samples. \textbf{CMIC} is removed by directly performing multimodal fusion on all discriminative power features $\{h_i^m\}_{m=1}^M$. \textbf{RDP} is removed by setting all relative discriminative power values $\{k_i^{m^{\prime}}\}_{m^{\prime}\neq m}^M$ equals to $\frac{1}{{M-1}}$, where $M$ is the number of modalities.

By comparing the results of different component combinations under ``$\eta=0,\epsilon=1$'' in Table \ref{tab:abl-sdy}, one can observe that all three components play a role in enhancing the robustness of MICINet under modality-specific noise. Furthermore, the results of different model versions under ``$\eta=20\%,\epsilon=0$'' demonstrate that all three components contribute to handling cross-modality noise. The results under ``$\eta=20\%,\epsilon=1$'' show that all the components enhance the performance under the coexistence of both two types of noise.

In addition, the experiment conducts t-SNE visualizations of the multimodal features computed by the model under different component combinations. Specifically, the multimodal features of each sample before being fed into the classifier are used for visualization. The results for MICINet without \textbf{SICI} and \textbf{CMIC}, without only \textbf{CMIC}, and the complete MICINet are shown in Figure \ref{fig:abl}, where the input data is polluted by modality-specific noise of $\epsilon=5$ and cross-modality noise of $\eta=20\%$. When \textbf{SICI} and \textbf{CMIC} are removed, the multimodal features of samples from different classes are largely mixed. As \textbf{SICI} and \textbf{CMIC} are progressively added, the multimodal features of samples from different classes gradually become more separated. This indicates that both \textbf{SICI} and \textbf{CMIC} contribute to enhancing the discriminative power of sample features and improving noise removal.
% Notably, the \textbf{CMP} contributes the most in handling unaligned samples under noise-free condition ($\eta=20\%,\epsilon=0$), while the \textbf{GICI} plays the most important role in dealing with unaligned samples under noise ($\eta=20\%,\epsilon=5$).

% In addition, a visualization of discriminative power enhancement through \textbf{SICI} and \textbf{CMIC} is conducted. The means of the output feature vectors $\{\{f_i^m\}_{m=1}^M\}_{i=1}^N$ from different modalities under different classes are first calculated. Then, for each modality, the cosine similarity between the mean feature vectors of different classes is computed, resulting in $M$ matrices of size $C\times C$, where $M$ is the number of modalities, and $C$ is the number of classes. These matrices reflect the discriminative power of modality features by measuring the orthogonality between classes within each modality. The result on the BRCA dataset with the setting of ``$\eta=20\%,\epsilon=5$'' is shown in Figure \ref{fig:abl}. With the addition of different components, the discriminative power of modality features gradually increases, and the consistency of discriminative power across modalities improves. The inclusion of \textbf{SICI} contributes more significantly to enhancing the discriminative power of each modality (compare the matrices in the first and second rows). The $\textbf{CMIC}$ further strengthens the alignment of discriminative power between modalities (compare the matrices in the second and third rows).

\subsection{The Effectiveness of \textbf{GICI} (RQ3)}
As elaborated in Section \ref{sec:g-ici}, \textbf{\textit{GICI}} consists of two key processes: learning the \textit{ICI} distribution between different class pairs within each modality $\{\{\{\mathcal{P}_g^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$, and estimating the degree of confusion $\{\{\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$ between them. The effectiveness of these two processes is validated separately below.
\subsubsection{The \textit{ICI} Distribution Learned in \textbf{GICI}} 
Experiments under noise are conducted to investigate whether \textbf{\textit{GICI}} learns the confusing information between different classes. Specifically, modality-specific noise $Noise$ with $\epsilon=1$ is added to all modalities of all samples in the dataset. At each epoch of the training process, the negative log probability of $Noise$ in the distribution $\mathcal{P}_g^{m,c}$ learned by \textbf{\textit{GICI}} is calculated under each modality and classes:
\begin{align}
    -\log(\mathcal{P}_g^{m,c}(Noise))=\frac{1}{C}\sum_{c^{\prime}}{-\log(\mathcal{P}_g^{m,c,c^{\prime}}(Noise))}.
\end{align}
Figure \ref{fig:gici-brca} and \ref{fig:gici-rosmap} show the results on the BRCA and ROSMAP datasets, respectively. The $-\log(\mathcal{P}_g^{m,c}(Noise))$ values across all modalities and classes gradually decreased to a lower level with the increase of training epochs. This indicates that \textbf{\textit{GICI}} successfully learned the \textit{ICI} information distribution incorporating noise information.
% To validate whether \textbf{GICI} learns the confusing information between classes, the similarity between the learned \textit{ICI} distribution $\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$ and the noise distribution $\mathcal{N}_{\epsilon}$ added to the dataset is depicted in each training epoch. The similarity is measured by $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ formulated as:
% \begin{align}
%     D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})=\frac{1}{C-1}\sum_{c^{\prime}\neq c}^C{D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c,c^{\prime}})}.
% \end{align}
% $C$ is the number of classes. $D_{JS}(\cdot||\cdot) \in[0,1]$ refers to the Jensen-Shannon Divergence of the two distributions, which is smaller when the distributions are closer and equals 0 when the distributions are identical. The results on the BRCA and ROSMAP dataset under noise with $\sigma=5$ is shown in Figure \ref{fig:gici-brca} and \ref{fig:gici-rosmap}. As training progresses, the $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ values corresponding to each class in each modality gradually decrease and quickly converge to a low level. This indicates that \textbf{GICI} effectively learns the noise information added to the dataset. Moreover, the converged $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ values are not equal to 0, as the $ICI$ contains more than just noise information.

% As mentioned in Section \ref{sec:g-ici}, \textbf{GICI} learns the inter-class confusion information $ICI$, i.e., the features that make it hard to distinguish different classes, including noise information. Therefore, in order to validate the information learned in \textbf{GICI}, noise with $\epsilon=5$ is added to the dataset. In each training epoch, the degree of similarity between the inter-class confusion information $\{\{\mathcal{P}_g^{m,c}\}_{c=1}^C\}_{m=1}^M$ extracted by \textbf{GICI} from each modality corresponding to each class and the added noise distribution $\mathcal{N}_{\epsilon}$ is measured by $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ formulated as:
% \begin{align}
%     D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})=\frac{1}{C-1}\sum_{c^{\prime}\neq c}^C{D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c,c^{\prime}})}.
% \end{align}
% $C$ is the number of classes. $D_{JS}(\cdot||\cdot) \in[0,1]$ refers to the Jensen-Shannon Divergence of the two distributions, which is smaller when the distributions are closer, and equals 0 when the distributions are identical. $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ of different classes in each modality in each epoch while training on the BRCA and ROSMAP dataset is reported in Figure \ref{fig:gici-brca} and \ref{fig:gici-rosmap}. As training progresses, the $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ values corresponding to each class in each modality gradually decrease and quickly converge to a low level. This indicates that \textbf{GICI} effectively learns the noise information added to the dataset. Moreover, the converged $D_{JS}(\mathcal{N}_{\epsilon}||\mathcal{P}_g^{m,c})$ values are not equal to 0, as the $ICI$ contains more than just noise information.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/AttenMap.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The mean of attention values corresponds to the noisy and clean halves of the meth feature in attention maps $A^{meth\leftarrow mRNA}$ and $A^{meth\leftarrow miRNA}$. Noise of ``$\eta=20\%,\epsilon=1$'' and ``$\eta=20\%,\epsilon=2$'' is added to half of the features only on the meth modality of the ROSMAP dataset. The mean values are amplified by 1000 times for clarity.}
\label{fig:atten-map-vis}
\end{figure}

\subsubsection{The Confusion Degree Estimated in \textbf{GICI}}
Extensive experiments are conducted to validate the effectiveness of the inter-class confusion degree estimation method in \textbf{\textit{GICI}}. The estimated inter-class confusion degree $\beta=\{\{\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$ is visualized under two settings: without noise and with noise (combination of modality-specific noise with $\epsilon=1$ and cross-modality noise with $\eta=20\%$). Figures \ref{fig:beta-brca} and \ref{fig:beta-rosmap} present the results under these two settings on the BRCA and CUB datasets, respectively. Under the noise-free setting (the first row of heatmaps in Figures \ref{fig:beta-brca} and \ref{fig:beta-rosmap}), different confusion degrees are exhibited between different class pairs across all modalities. Additionally, each heatmap demonstrated good diagonal symmetry, which indicates the effectiveness of the symmetric constraint $\mathcal{L}_{gici}^{sym}$ designed in Equation \ref{eq:6}. After the data are corrupted by noise (as shown in the second row of heatmaps in Figures \ref{fig:beta-brca} and \ref{fig:beta-rosmap}), the confusion degree between all class pairs across all modalities increases. This indicates that the $\beta=\{\{\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$ estimated in \textbf{\textit{GICI}} can effectively perceive the increased inter-class confusion degree caused by the introduction of noise, ensuring the reliability of the learned global \textit{ICI} distribution.

Additional experiments are conducted to verify the necessity of mixing $ICI$ distribution of different class pairs using confusion degree $\beta=\{\{\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$. As a comparison, a model version that performs a uniform mixture of \textit{ICI} distributions by setting all $\beta=\frac{1}{C-1}$ is introduced, termed ``uniform mix''. $C$ is the number of classes. Table \ref{tab:weight_mix} compares ``uniform mix'' with the method that mixes with confusion degree (``mix with $\beta$''). The results show that ``mix with $\beta$'' outperforms ``uniform mix'' under various noise conditions. This demonstrates the enhancement of model reliability by mixing the \textit{ICI} distributions with the estimated confusion degrees.

% Figure \ref{fig:conf-degree} visualizes the inter-class confusion degree $\beta=\{\{\{\beta^{m,c,c^{\prime}}\}_{c^{\prime}\neq c}^C\}_{c=1}^C\}_{m=1}^M$ in each modalities estimated in \textbf{GICI} by Equation \ref{eq:beta}. For clarity, the confusion degrees among first 10 classes of CUB is presented. The heatmaps in the first row demonstrate the confusion degree in the BRCA and CUB datasets without adding noise, illustrating that the varying confusion degree among different pairs of classes. In the heatmaps of the second row, noise is added to a randomly selected class in a modality. With the addition of noise, the confusion degree between the noisy class and other classes increases. The corresponding rows and columns in the heatmap (areas with red box) also show increased values, indicating that the $\beta$ estimated by \textbf{GICI} can effectively capture the inter-class confusion degree.

\subsection{Effectiveness of the \textbf{CMIC} (RQ4)}
As demonstrated in Section \ref{sec:cmic}, \textbf{\textit{CMIC}} consists of two parts: cross-modality compensatory information query and modality enhancement based on relative discriminative power. Extensive experiments are conducted to illustrate the effectiveness of these two parts further.
\subsubsection{The Queried Information Among Modalities}
% To demonstrate the effectiveness of the cross-modality compensatory information query mechanism, the cross-modality query attention maps $A^{m\leftarrow m^{\prime}}$ are visualized under noise. Specifically, noise is added to half of the features of an arbitrarily selected modality in the dataset, and the changes in the attention maps for querying compensatory information from the other noise-free modalities are observed before and after noise corruption. The noise includes modality-specific noise with $\epsilon=1$ and cross-modality noise with $\eta=20\%$. The cross-modality noise is implemented by shuffling the corrupted half of the modality features. The results on the ROSMAP dataset are demonstrated in Figure \ref{fig:atten-map-vis}, where the noise corruption is applied only on half of the features in the meth modality (the red-boxed regions in each heatmap). The attention maps that meth queries compensatory information from the unaffected modalities (mRNA and miRNA) are denoted as $A^{meth\leftarrow mRNA}$ and $A^{meth\leftarrow miRNA}$, respectively. Before the noise is added to the meth modality, the average values of the two halves in $A^{meth\leftarrow mRNA}$ and $A^{meth\leftarrow miRNA}$ are relatively consistent. After the noise is introduced, the average values of the regions in $A^{meth\leftarrow mRNA}$ and $A^{meth\leftarrow miRNA}$ corresponding to the half of meth features corrupted by noise significantly increase, while the unaffected half remain relatively unchanged. The results illustrate that for noise-corrupted features of a certain modality, the attention map can adaptively increase the weights to the discriminative features of other modalities to obtain more compensatory information. This indicates that the cross-modality compensatory information query mechanism can interpretably obtain information from other modalities to compensate for the information missing in a modality caused by noise, thereby enhancing the discriminative power across modalities.
To demonstrate the effectiveness of the cross-modality compensatory information query mechanism, the attention values of cross-modality query attention maps $A^{m\leftarrow m^{\prime}}$ are evaluated. Specifically, modality-specific and cross-modality noise is added to half of the features of an arbitrarily selected modality in the dataset. The cross-modality noise is implemented by shuffling the corrupted half of the modality features. The mean of attention values of attention maps for querying compensatory information from the other noise-free modalities is observed under different noise intensities. The results on the ROSMAP dataset are demonstrated in Figure \ref{fig:atten-map-vis}, where the noise corruption is applied only on half of the features in the meth modality. The attention maps that meth queries compensatory information from the unaffected modalities (mRNA and miRNA) are denoted as $A^{meth\leftarrow mRNA}$ and $A^{meth\leftarrow miRNA}$, respectively. The attention values corresponding to the noisy half of meth features are higher than those of the clean half under different noise intensities. As noise intensity increases, the attention values for the noisy half also rise, while those for the clean half remain nearly unchanged. The results illustrate that for noise-corrupted features of a certain modality, the attention map can adaptively increase the values according to noise intensity to obtain more compensatory information from discriminative features of other modalities. This indicates that the cross-modality compensatory information query mechanism can interpretably obtain information from other modalities to compensate for the information missing in a modality caused by noise, thereby enhancing the discriminative power across modalities.

% The attention map $A^{m\leftarrow m^{\prime}}$ on the CUB dataset is visualized to demonstrate the information queried between modalities. Two types of noise are applied to corrupt features of the text modality in the CUB dataset, while the image modality remains unchanged. The attention map of the text modality querying information from the image modality $A_i^{text\leftarrow image}$ before and after corruption is shown in Figure \ref{fig:atten-map-vis}, with the area boxed in red indicating the portions of features where noise and unalignment were applied. It is observed that the values in the red-boxed areas of the attention map significantly increase after the corruption is applied, indicating that these regions need to get more information from other modalities. This indicates that the cross-modality query mechanism can adaptively query the enhancing information from other modalities by considering the missing information in $h_i^m$ corresponding to $ICI_i^m$.
% The unalignment is applied by substituting the original feature values with random values. 
\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/fig-relative_dis.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The relative discriminative power of different modalities as noise with different intensities adds to one of the modalities. The result on the ROSMAP dataset is shown, where noise is added to the meth modality.}
\label{fig:rdp}
\end{figure}

\subsubsection{Relative Discriminative Power} Experiment is conducted to explore the responsiveness of relative discriminative power to noise. Specifically, the noise is added to one of the modalities of the dataset, and the relative discriminative power between modalities is investigated. The noise includes cross-modality noise with $\eta=20\%$ implemented by shuffling the features of the affected modality in 20\% of the samples. Also, the modality-specific noise with intensity $\epsilon$ is added. Figure \ref{fig:rdp} shows the variation of relative discriminative power with increasing $\epsilon$ on the ROSMAP dataset, where noise is added only to the meth modality. The left subplot shows that when the mRNA modality serves as the query modality, the relative discriminative power $k^{mRNA\leftarrow meth}$ continuously decreases with increasing noise intensity in the meth modality, while $k^{mRNA\leftarrow miRNA}$ of the unaffected miRNA modality continuously increases. Similarly, the right subplot shows that when the miRNA modality serves as the query modality, the relative discriminative power $k^{miRNA\leftarrow meth}$ continuously decreases with increasing noise intensity in the meth modality, while $k^{miRNA\leftarrow mRNA}$ of the unaffected mRNA modality continuously increases. This indicates that the relative discriminative power can capture the change in relative quality between modalities and enhance the reliability of modality enhancement by relying more on the modalities with higher quality.

% the unalignment and noise are added to one of the modalities of the dataset. The results in Figure \ref{fig:rdp} demonstrate the relative discriminative power change as noise intensity increase in the meth modality of the BRCA and ROSMAP datasets. As the noise intensity increases, the relative discriminative power of the corrupted modality decreases while that of the unaffected modalities increases. This indicates that the relative discriminative power can effectively sense the noise in modalities, enhancing the reliability of cross-modality compensation by guiding the model to rely more on the less corrupted modalities.

% \begin{figure}[t]
% \centering
% \includegraphics[width=\linewidth]{fig/WhatGICILearn_BRCA_ROS.pdf} % Reduce the figure size so that it is slightly narrower than the column.
% \caption{The correlation of the learned information in \textbf{G-ICI} with the noise add on the BRCA and ROSMAP datasets in each epoch of training.}
% \label{fig:dis-alg}
% \end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fig/fig-BRCA_Oth.pdf} % Reduce the figure size so that it is slightly narrower than the column.
\caption{The removal of modality-specific and cross-modality noise during the training of MICINet.}
\label{fig:dis-alg}
\end{figure}

% \begin{figure}[h]
%     \centering
%     \subfloat[Results on the BRCA dataset.]{\includegraphics[width=0.24\textwidth]{fig/fig-BRCA_Oth.pdf}\label{fig:sub1}}
%     \hfill  % 添加水平间隔
%     \subfloat[Results on the ROSMAP dataset.]{\includegraphics[width=0.24\textwidth]{fig/fig-rosmap_Oth.pdf}\label{fig:sub2}}
%     \caption{The removal of cross-modality noise during training as discriminative power increases.}
%     \label{fig:dis-alg}
% \end{figure}

\subsection{Effectiveness of MICINet in Removing Both Types of Noise (RQ5)} 
Additional experiments under noise are conducted to verify whether MICINet effectively removed modality-specific and cross-modal noise coexisting in the data. The experiments are performed with modality-specific noise ($\epsilon=1$) and cross-modality noise ($\eta=20\%$). The removal of cross-modality noise in the data is assessed by $L_{align}$, which indicates the alignment between modalities. The content of modality-specific noise in the data was measured by the cosine similarity between the features of each modality $\{f_i^m\}_{i=1}^N$ learned by MICINet and the noise. Figure \ref{fig:dis-alg} shows the changes in $L_{align}$ and the content of modality-specific noise during the training of MICINet on the noisy ROSMAP dataset. It can be seen that while $L_{align}$ increases to a high level, the content of modality-specific noise in each modality also decreases to a low level. This indicates that MICINet effectively removed both types of noise.

% The experiment is conducted to further demonstrate the effectiveness of noise removal by removing $ICI$ and discriminative power enhancement. $\mathcal{L}_{oth}$ is calculated to evaluate the discriminative power of the output features of each modality. Specifically, $\mathcal{L}_{oth}$ calculates the Frobenius norm between the $C\times C$ matrices of each modality mentioned in Section \ref{sec:abl-study} and the identity matrix. Figure \ref{fig:dis-alg} shows the $\mathcal{L}_{oth}$ of each modality on the BRCA dataset and the value of $\mathcal{L}_{align}$ during training on four datasets under the setting of $\eta=20\%,\epsilon=5$. The results show that as the discriminative power of each modality increases and achieves consistency at a high level, the degree of alignment between modalities also improves.

\section{Conclusion}
This paper proposes the Multi-Level Inter-Class Confusing Information Removal Network (MICINet) that reliably removes both modality-specific and cross-modality noise at the global and individual levels. Experiments on four datasets have demonstrated that MICINet achieves more robust performance than state-of-the-art reliable multimodal classification methods under various noise conditions and effectively removes both types of noise. The significant role of each proposed component in enhancing the reliability under noise is verified, and their effectiveness is fully validated in extensive experiments. Specifically, experiments have shown that the proposed Global \textit{ICI} learning module (\textit{\textbf{GICI}}) of MICINet effectively learns the inter-class confusion distribution with global noise information and reliably detects changes in inter-class confusion degree caused by noise. Additionally, more experiments have also illustrated the interpretability of the cross-modality compensation information query and the sensitivity of the proposed relative discriminative power to individual-level modality quality changes introduced in the sample-adaptive cross-modality information compensation module (\textit{\textbf{CMIC}}). Despite its effectiveness, MICINet has certain limitations in terms of time complexity, primarily because it learns the global \textit{ICI} distribution through pairwise calculations across classes. Future works will focus on reliable multimodal classification that balances computational efficiency.
% This paper proposes the Multi-Level Inter-Class Confusing Information Removal Network (MICINet), a novel, reliable multimodal classification method that focuses on an aspect commonly overlooked in existing work: the coexistence of modality-specific and cross-modality noise in multimodal data. Experiments on four datasets demonstrate that MICINet can sensitively capture the changes in data caused by both types of noise at global and individual levels and reliably remove them, achieving superior performance over state-of-the-art methods. Despite its effectiveness, MICINet has certain limitations in terms of time complexity, primarily because it learns the global \textit{ICI} distribution through pairwise calculations across classes. Future works will focus on reliable multimodal classification that balances computational efficiency.

% \begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{reference.bib}
% \bibitem{ref1}
% Radenovic F, Dubey A, Kadian A, et al. Filtering, distillation, and hard negatives for vision-language pre-training[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023: 6967-6977.

% \bibitem{ref2}
% Huang Z, Niu G, Liu X, et al. Learning with noisy correspondence for cross-modal matching[J]. Advances in Neural Information Processing Systems, 2021, 34: 29406-29419.

% \bibitem{ref3}
% F. Mittelbach and M. Goossens, {\it{The \LaTeX Companion}}, 2nd ed. Boston, MA, USA: Pearson, 2004.

% \bibitem{ref4}
% G. Gr\"atzer, {\it{More Math Into LaTeX}}, New York, NY, USA: Springer, 2007.

% \bibitem{ref5}M. Letourneau and J. W. Sharp, {\it{AMS-StyleGuide-online.pdf,}} American Mathematical Society, Providence, RI, USA, [Online]. Available: http://www.ams.org/arc/styleguide/index.html

% \bibitem{ref6}
% H. Sira-Ramirez, ``On the sliding mode control of nonlinear systems,'' \textit{Syst. Control Lett.}, vol. 19, pp. 303--312, 1992.

% \bibitem{ref7}
% A. Levant, ``Exact differentiation of signals with unbounded higher derivatives,''  in \textit{Proc. 45th IEEE Conf. Decis.
% Control}, San Diego, CA, USA, 2006, pp. 5585--5590. DOI: 10.1109/CDC.2006.377165.

% \bibitem{ref8}
% M. Fliess, C. Join, and H. Sira-Ramirez, ``Non-linear estimation is easy,'' \textit{Int. J. Model., Ident. Control}, vol. 4, no. 1, pp. 12--27, 2008.

% \bibitem{ref9}
% R. Ortega, A. Astolfi, G. Bastin, and H. Rodriguez, ``Stabilization of food-chain systems using a port-controlled Hamiltonian description,'' in \textit{Proc. Amer. Control Conf.}, Chicago, IL, USA,
% 2000, pp. 2245--2249.

% \end{thebibliography}


% \newpage

% \section{Biography Section}
% If you have an EPS/PDF photo (graphicx package needed), extra braces are
%  needed around the contents of the optional argument to biography to prevent
%  the LaTeX parser from getting confused when it sees the complicated
%  $\backslash${\tt{includegraphics}} command within an optional argument. (You can create
%  your own custom macro containing the $\backslash${\tt{includegraphics}} command to make things
%  simpler here.)
 
% \vspace{11pt}

% \bf{If you include a photo:}\vspace{-33pt}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Michael Shell}
% Use $\backslash${\tt{begin\{IEEEbiography\}}} and then for the 1st argument use $\backslash${\tt{includegraphics}} to declare and link the author photo.
% Use the author name as the 3rd argument followed by the biography text.
% \end{IEEEbiography}

% \vspace{11pt}

% \bf{If you will not include a photo:}\vspace{-33pt}
% \begin{IEEEbiographynophoto}{John Doe}
% Use $\backslash${\tt{begin\{IEEEbiographynophoto\}}} and the author name as the argument followed by the biography text.
% \end{IEEEbiographynophoto}




% \vfill

\end{document}


