\documentclass[9pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage[T1]{fontenc}
\usepackage{float} 
\usepackage{subfig}
\usepackage[export]{adjustbox} 
\usepackage{float} 
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{threeparttable}
\usepackage{stfloats}
\usepackage{xcolor}
\usepackage{pifont}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[colorlinks=true, linkcolor=red, citecolor=black, urlcolor=magenta]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}


\title{NeRF-3DTalker: Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured for https://ieeexplore.ieee.org  and
%should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
\thanks{\IEEEauthorrefmark{1}Equal Contribution. \IEEEauthorrefmark{2}Corresponding Author:~\href{mailto:bichongke@tju.edu.cn}{bichongke@tju.edu.cn}.}
}

\author{\IEEEauthorblockN{Xiaoxing Liu\IEEEauthorrefmark{1}, Zhilei 
 Liu\IEEEauthorrefmark{1}, Chongke Bi\IEEEauthorrefmark{2}}
\IEEEauthorblockA{
\textit{College of Intelligence and Computing, Tianjin University, Tianjin, China}\\
}}
% \author{\IEEEauthorblockN{Xiaoxing Liu}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{Zhilei Liu}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{Chongki Bi}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\maketitle

\begin{abstract}
Talking head synthesis is to synthesize a lip-synchronized talking head video using audio. Recently, the capability of NeRF to enhance the realism and texture details of synthesized talking heads has attracted the attention of researchers. However, most current NeRF methods based on audio are exclusively concerned with the rendering of frontal faces. These methods are unable to generate clear talking heads in novel views. Another prevalent challenge in current 3D talking head synthesis is the difficulty in aligning acoustic and visual spaces, which often results in suboptimal lip-syncing of the generated talking heads. To address these issues, we propose Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis (NeRF-3DTalker). Specifically, the proposed method employs 3D prior information to synthesize clear talking heads with free views. Additionally, we propose a 3D Prior Aided Audio Disentanglement module, which is designed to disentangle the audio into two distinct categories: features related to 3D awarded speech movements and features related to speaking style. Moreover, to reposition the generated frames that are distant from the speaker's motion space in the real space, we have devised a local-global Standardized Space. This method normalizes the irregular positions in the generated frames from both global and local semantic perspectives. Through comprehensive qualitative and quantitative experiments, it has been demonstrated that our NeRF-3DTalker outperforms state-of-the-art in synthesizing realistic talking head videos, exhibiting superior image quality and lip synchronization. Project page: \href{https://nerf-3dtalker.github.io/NeRF-3Dtalker/}{https://nerf-3dtalker.github.io/NeRF-3Dtalker/}.
\end{abstract}

\begin{IEEEkeywords}
Talking head synthesis, neural radiance fields.
\end{IEEEkeywords}

\section{Introduction}
\vspace{-3pt}
Synthesizing talking head videos using audio is a popular research topic in the fields of multimedia and computer graphics. %It has a wide range of potential applications in various areas, such as 3D gaming, virtual reality, film, and others.
In the initial phases of research, numerous methods utilized image processing techniques or Generative Adversarial Networks (GAN) to generate image frames that match the input audio~\cite{ding2017exprgan, pumarola2018ganimation, ji2021audio, lu2021live, thies2020neural}. However, GANs are challenging to train \cite{chen2021talking}, and the lip shapes synthesized by these methods are often blurry and inaccurate. This can create bottlenecks in the field of talking head synthesis. 
Recently, researchers have investigated the use of diffusion models to synthesize talking head videos \cite{zhua2023audio, shen2022learning, stypulkowski2023diffused, multomm}. However, these models demand significant training resources and do not account for the spatial structure of the head, which restricts their ability to generate facial texture and 3D realism. 
In contrast, Neural Radiance Fields (NeRF) have gained attention for their ability to synthesize high-fidelity images with \textbf{limited datasets}. NeRF \cite{2020NeRF} is a novel technique for learning continuous volumetric representations of 3D scenes.
%and several methods have successfully utilized 3D Morphable Models (3DMM) and NeRF to synthesize 3D heads with free viewpoints. [Reference] proposed an end-to-end learnable model to reconstruct 3D heads. [Reference] used external videos to provide expression parameters for reconstructing 3D talking heads, but these methods do not respond to audio cues. 
Although NeRF has been successfully applied to the talking head synthesis, most research focus on
rendering frontal faces
%has focused on rendering frontal faces
\cite{shen2022learning, bi2024nerf}. When substantial angular displacements of the head are encountered, the resultant images often exhibit a degradation in visual fidelity \cite{shen2022learning, bi2024nerf}. %This issue arises because the dataset contains only frontal views of the talking head, limiting NeRF’s ability to accurately reconstruct the talking head’s full 3D spatial geometry. 

To tackle this challenge, we propose using 3D prior information from the 3D Morphable Model (3DMM) \cite{tran2018nonlinear} and semantics features to assist NeRF in generating talking heads with free views. 
The 3DMM model is a traditional method for extracting 3D prior information of faces, representing facial structure as identity, reflectance, and facial expression. 
%In recent years, many researchers have used the 3DMM model to synthesize human heads.
%To our knowledge, we are the first to synthesize clear free-view talking heads using audio and NeRF, aided by 3D priors.
% obtained from 3DMM.
%propose to introduce 3D representations to enhance the model's ability to express three-dimensional entities. 
Furthermore, current methods for synthesizing talking heads typically input the complete audio into the model \cite{yao2022dfa,shen2022learning,bi2024nerf}. However, the abundance of information in the audio, such as rhythm and timbre, can interfere with the model's learning process, reducing its ability to learn multimodal features.
%To address these challenges, we propose Neural Radiance Field with 3D Prior Aided Audio Disentanglement for Talking Head Synthesis (NeRF-3DTalker). 
%Specifically, 
To address this challenge, we propose a 3D Prior Aided Audio Disentanglement module to disentangle the input audio into two features: \(f_{exp-aud}\) and \(f_{exp-style}\). \(f_{exp-aud}\) represents the feature related to 3D awarded speech movements, while \(f_{exp-style}\) represents the feature related to speaking style. Speaking style refers to the movement of facial muscles influenced by audio. The 3D Prior Aided Audio Disentanglement module \textbf{aligns the acoustic space with the 3D visual space}, which is crucial in the field of 3D talking head synthesis. Subsequently, we input \(f_{exp-aud}\) and \(f_{exp-style}\) along with the 3D prior information obtained from the 3DMM model into the presented conditional NeRF to synthesize talking heads with free views.
%As shown in Fig.\ref{fig:teaser}, in our paper, 
%we parameterize the face, where we extract three facial features from the identity image using the 3DMM model. These three features, along with \(f_{exp-aud}\) and \(f_{exp-style}\), are inputted into the conditional Neural Radiance Field presented in our work to generate talking head frames with free views.
%After synthesizing talking head videos, we found that introducing 3D representations can easily cause the synthesized head to deviate from the speaker's motion space, resulting in issues such as blurry head contours, blurred teeth, and imprecise lip expressions. 
Furthermore, 
%inspired by codebook concepts \cite{van2017neural}, we design a standardized space for speakers to bring back the generated frames distant from the speaker's motion space to the real space. Specifically, we model the speaker's global head motion space as a standardized domain. Subsequently, we input the generated frames into this standardized space to normalize any irregular positions.
in order to bring back the generated frames that are distant from the speaker's motion space to the real space, we draw inspiration from the concept of codebooks \cite{van2017neural} and design Standardized Space for speakers. Specifically, we model the speaker's motion space as a global Standardized Space. 
After that, we input the generated frames into this Standardized Space to normalize the irregular positions in the generated frames. 
Additionally, due to the strong correlation between the speaker's motion and speech movements, we propose the use of a semantic standardized space to express local semantics. This will effectively constrain the generated heads from both global and local perspectives. 
To summarize, this paper presents the following contributions:
\begin{itemize}
\item We provide audio semantics and visual 3D prior features for conditional NeRF to synthesize reliable multi-view talking heads.
%To the best of our knowledge, we present for the first time a method for synthesizing free-view talking heads based on audio and NeRF.
\item 
To align the acoustic space with the 3D visual space, 
we propose a 3D Prior Aided Audio Disentanglement module that disentangles the audio into features related to 3D awarded speech movements and features related to speaking style.
\item We design a Standardized Space that improves the realism and regularity of the generated images from both global and semantic perspectives.
%\item Extensive qualitative and quantitative experiments demonstrate that NeRF-3DTalker outperforms the state-of-the-art in terms of image quality and lip synchronization.
%Results of qualitative and quantitative experiments demonstrate that our proposed NeRF-ADF outperforms the state-of-the-art in terms of generating realistic and accurate talking head videos. 
\end{itemize}

\begin{figure*}
  \centering
  \includegraphics[width=0.9\textwidth]{pic/met.pdf}
  \caption{Overview of NeRF-3DTalker. 
  %The light yellow area represents the 3D Prior Extraction. The gray area represents 3D Prior Aided Audio Disentanglement module, which disentangles the input audio into features related to 3D awarded speech movements \(f_{exp-aud}\) and features related to speaking style \(f_{exp-style}\). After that, the conditional NeRF is presented to accurately render the talking head images with these five features as conditions. Finally, as shown in the green dashed area, we propose the Standardized Space, aiming to enhance the regularity of generated frames from both semantic and global perspectives.
  }
    \vspace{-16pt}
  \label{fig:teaser2}
\end{figure*}


\vspace{-8pt}
\section{Proposed method}
\vspace{-4pt}
The overall architecture of our proposed NeRF-3DTalker is shown in Fig.\ref{fig:teaser2}, which consists of four main components: the 3D Prior Extraction module (enclosed by the light yellow rectangle), the 3D Prior Aided Audio Disentanglement module (enclosed by the gray rectangle), the Neural Radiance Field Rendering module (enclosed by the orange rectangle), and the Standardized Space (enclosed by the green dashed rectangle). In the following sections, we provide detailed explanations of each component.

\subsection{3D Prior Extraction}
To generate a talking head with free views, we incorporate 3D facial priors into our model with the assistance of 3DMM. 
Inspired by \cite{egger20203d, tran2018nonlinear, hong2022headnerf}, we parameterize a head into shape and appearance. For appearance, similar to 3DMM, we use facial albedo \(f_{alb}\), and scene illumination \(f_{illu}\) to control.
In order to better reconstruct the 3D talking head, we use identity \(f_{id}\), expression related to 3D awarded speech movements \(f_{exp-aud}\), expression related to speaking style \(f_{exp-style}\) for shape representation, as shown in Eq.~\ref{Eq1}:
%we parameterize the speaker's head in this paper as identity \(f_{id}\), expression related to speech movements \(f_{exp-aud}\), expression related to speaking style \(f_{exp-style}\), facial albedo \(f_{alb}\), and scene illumination \(f_{illu}\), as shown in Eq.~\ref{Eq1}:

\begin{equation}
\label{Eq1}
    I:(f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})
\end{equation}
where \(f_{id}~\in~\mathbb{R}^{100}\), \(f_{exp-aud}~\in~\mathbb{R}^{79}\), \(f_{exp-style}~\in~\mathbb{R}^{79}\), \(f_{alb}~\in~\mathbb{R}^{100}\), \(f_{illu}~\in~\mathbb{R}^{27}\) and \(I\) represents the speaker's head. 
\(f_{exp-aud}\) and \(f_{exp-style}\) are obtained through 3D Prior Aided Audio Disentanglement Module.
%For our model, we initially utilize a nonlinear 3DMM model \cite{tran2018nonlinear, hong2022headnerf} to extract the identity, facial albedo, and scene illumination of the identity face image. 
%Subsequently, the Audio Disentanglement module is utilized to generate expressions related to speech movements and speaking style.


\subsection{3D Prior Aided Audio Disentanglement}
Since speech audio contains a lot of mixed information, including rhythm and timbre, feeding the complete audio into the NeRF would increase its learning burden and fail to learn precise speech motion features. To address this issue and align
acoustic space with 3D visual space, we propose a 3D Prior Aided Audio Disentanglement module that disentangles the speech audio into features related to 3D awarded speech movements \(f_{exp-aud}\) and features related to speaking style \(f_{exp-style}\).

As shown in the gray area of Fig.\ref{fig:teaser2}, the first step is to input the identity image \(I_{id}\) and audio into a pre-trained LipNet network \cite{zhang2022sadtalker} to generate Lip-wav images focused on the motion of the lips. 
%LipNet consists of multiple convolutional layers. 
To accomplish this, inspired by \cite{prajwal2020lip}, we also use a pre-trained SyncNet to provide lip synchronization loss for supervision. The SyncNet we use judges whether the generated images and speech are synchronized. It consists of a face encoder and an audio encoder. The lip synchronization loss provided by SyncNet can be represented as follows:
\begin{equation}
    L_{\mathrm{lip}}=\frac{1}{B}\sum_{i=1}^{B}-\log(\frac{f_{lip_i}\cdot f_{a_i}}{max(\|f_{lip_i}\|_2\cdot\|f_{a_i}\|_2,\epsilon)})
\end{equation}
where $B$ denotes the number of frames processed per iteration. \(f_{lip_i}\) denotes the feature obtained by inputting the generated Lip-wav image into the face encoder, and \(f_{a_i}\) represents the feature obtained by inputting the audio sequence into the audio encoder.

Then, we use 3D Prior Extraction module to extract the expression features \(f_{exp-aud}\) from the Lip-wav images. These expression features are only related to the speech content.
And we use a StyleNet network to extract features related to the individual's speaking style. Next, we input these two features into the fusion module (FuseNet) to generate fused expression features. We calculate the loss between the fused features \(f_{gen-exp}\) and the expression features \(f_{gt-exp}\) of the real image to optimize the entire 3D Prior Aided Audio Disentanglement module. The loss is as follows:
\begin{equation}
    L_{\mathrm{exp}}=||f_{gen-exp}-f_{gt-exp}||^2
\end{equation}

\begin{figure}[b]
  \centering
  \includegraphics[width=0.9\linewidth]{pic/ex1.pdf}
  \caption{Visual results of the comparative experiments.}
  \label{fig:teaser3}
%\vspace{-16pt}
\end{figure}

\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{pic/ex2.pdf}
  \caption{Novel view results of the comparative experiments.}
  \label{fig:teaser4}
%\vspace{-16pt}
\end{figure}

\subsection{NeRF for Talking Head Synthesis}
\label{Sec_NeRF}
% Recently, Neural Radiance Field \cite{2020NeRF} has provided a powerful framework for the 3D vision field. It can infer the color \(c\) and density \(\sigma\) of a point on the camera ray by using the point 3D location \(l=(x,y,z)\) and 2D viewpoint direction \(d=(\beta,\varphi)\). Then, the colors and densities of all 3D points on each ray are inferred and integrated to render the final image.
% Inspired by the work of \cite{shen2022learning}, we present a conditional NeRF to synthesize the talking face images with \(f_{id}\), \(f_{exp-aud}\), \(f_{exp-style}\), \(f_{alb}\), \(f_{illu}\) as conditions. 
% Then, we use these conditions along with 3D position \(l\) and view direction \(d\) as the input to the implicit function \(F_\theta\).
% Finally, the NeRF, consisting of multi-layer perceptrons, can estimate the color \(c\) and density \(\sigma\) of each 3D point on every ray. The entire implicit function can be formulated as follows:
% \begin{equation}
%     F_\theta:(l,d,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})\longrightarrow(c,\sigma)
% \end{equation}
% To calculate the color \(C_n\) of each pixel on the output image, we perform volume rendering by integrating the high-dimensional feature vectors and densities of all 3D points along each ray \(r\). This process can be represented as:
% \[C_n(r;\theta,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})=\]
% \begin{equation}
%     \int_{t_n}^{t_f}\sigma(t)\cdot c(t)\cdot T(t)dt
% \end{equation}
% where \(t_n\) and \(t_f\) denote the near and far bound of a camera ray respectively. \(T(t)=exp(-\int_{t_n}^{t}\sigma(r(\tau))d\tau)\) denotes the integral transmittance along a camera ray from near \(t_n\) bound to $t$. 
% In this process, we use the loss function \(L_{pho}\) to minimize the photo-metric reconstruction error between the generated color \(C_n\) and the ground truth color \(C_g\).
% \begin{equation}
%     L_{pho}=\sum_{r\in \mathcal{R}}||C_n(r;\theta,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})-C_g||^2
% \end{equation}
% where \(\mathcal{R}\) denotes the set of rays and \(\theta\) denotes the model parameters.


%Recently, NeRF \cite{2020NeRF} has provided a powerful framework for the 3D vision field. It can infer the color \(c\) and density \(\sigma\) of a point on the camera ray by using the point's 3D location \(l=(x,y,z)\) and 2D viewpoint direction \(d=(\beta,\varphi)\). Then, the colors and densities of all 3D points on each ray are inferred and integrated to render the final image.
Inspired by the work of \cite{shen2022learning}, we present a conditional NeRF to synthesize the talking head images with \(f_{id}\), \(f_{exp-aud}\), \(f_{exp-style}\), \(f_{alb}\), \(f_{illu}\) as conditions. 
We then use these conditions along with the 3D position \(l=(x,y,z)\) and camera parameter \(P\) as the input to the implicit function \(F_\theta\).
Finally, the NeRF, consisting of multi-layer perceptrons, can estimate a high-dimensional feature vector \(z\) and density \(\sigma\) of each 3D point on each ray. The entire implicit function can be formulated as follows:
\begin{equation}
    F_\theta:(l,P,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})\longrightarrow(z,\sigma)
\end{equation}
To generate the output image \(\hat{F}_t\), volume rendering is performed by integrating the high-dimensional feature vectors and densities of all 3D points along each ray \(r\). This process is represented as:
\[\hat{F}_t(r;\theta,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})=\]
\begin{equation}
    H_u(\int_{t_n}^{t_f}\sigma(t)\cdot z(t)\cdot T(t)dt)
\end{equation}
where \(t_n\) and \(t_f\) denote the near and far bound of a camera ray respectively. \(T(t)=exp(-\int_{t_n}^{t}\sigma(r(\tau))d\tau)\) denotes the integral transmittance along a camera ray from near \(t_n\) bound to $t$. 
It is important to note that there is significant pressure on the rendering of Neural Radiance Field due to the requirement of generating multi-view talking heads. To decrease the rendering time of NeRF, we employ the upsampling operation \(H_u\), following the inspiration of~\cite{niemeyer2021giraffe,karras2020analyzing}. 

The loss function \(L_{pho}\) is utilized in this process to minimize the photo-metric reconstruction error between the generated image \(\hat{F}_t\) and the ground truth \(F_t\).
\begin{equation}
    L_{pho}=||\hat{F}_t(r;\theta,f_{id},f_{exp-aud},f_{exp-style},f_{alb},f_{illu})-F_t||^2
\end{equation}
where \(\theta\) denotes the model parameters.

\subsection{Local-Global Standardized Space}
\label{ss}

Due to the complexity of the 3D talking head synthesis task, current methods inevitably produce some generated frames that deviate from the speaker’s motion space, which limits the visual effect of the original view.
%After obtaining the talking head, 
%it is discovered that incorporating 3D prior information frequently results in synthesized images that differ from the speaker's motion space.%resulting in some areas of blurriness in the generated image, such as teeth and chin. 
%It is crucial to normalize the generated frames. Inspired by the paper \cite{van2017neural, xing2023codetalker}, we design a Standardized Space, which significantly enhances the realism and regularity of the generated images.
In order to bring back the generated frames that are distant from the speaker's motion space to the real space, we draw inspiration from the concept of codebooks \cite{van2017neural} and design a Standardized Space for speakers.

%The fundamental idea behind the normalization space we designed is inspired by VQ-VAE \cite{van2017neural}. 
The Standardized Space we designed comprises a \textbf{global space} and a \textbf{local AU semantic space}, as illustrated in Fig.\ref{fig:teaser2}. The local AU semantic space enhances the semantic information of the generated frames from the perspective of the local AU, while the global space improves the global information of the generated frames from the perspective of the overall face. The semantic information refers to the features in the face that are highly correlated with speech movements. 
%Since there is a strong correlation between AUs and speech movements, in this paper, we use speech-related Action Units (AUs) information to represent semantic elements.
In this paper, we use speech-related Action Units (AUs) information to represent semantic elements. 

The local AU semantic space comprises an AU encoder and a semantic codebook $S$. The main objective during training is to learn how to establish the semantic codebook $S$ that stores the standardized semantic elements. This enables the replacement of semantic elements in generated frames with standardized ones using Eq.~\eqref{eq1} during testing.%, thereby enhancing the semantic information of the generated frames. 

We first input the real frames into a pre-trained AU encoder to generate AU features.
The AU encoder comprises four convolutional layers and four fully connected layers. 
%In the pre-training phase, the encoder's last fully connected layer produces a one-dimensional label. 
To train the encoder, we use binary cross-entropy loss to calculate the AU distance between the input frames and the ground truth. The specific loss function is shown below:
\begin{equation}\label{bce}
    L_{bce}=-\frac{1}{n_{AU}}\sum_{i=1}^{n_{AU}}w_i[x_{i}log\hat{x}_{i}+(1-x_{i})log(1-\hat{x}_{i})]
\end{equation}
where \(x_i\) and \(\hat{x}_{i}\) denote the $i$-th ground truth AU label and the $i$-th input frame AU label respectively. The above AU labels are extracted using OpenFace \cite{baltruvsaitis2015cross,baltrusaitis2018openface}. To reduce the impact of the correlation between AUs on model training, we adopt the method proposed by \cite{chen2021talking}, which adds weights \(w_i=\frac{\frac{1}{r_i}n_{AU}}{\sum_{i=1}^{n_{AU}}\frac{1}{r_i}}\) to Eq.~\eqref{bce}, where \(r_i\) is the probability of the $i$-th AU occurring in the dataset. Meanwhile, since some AUs appear rarely in the training set and can affect the model's prediction results, we introduce a weighted multi-label Dice coefficient loss \cite{chen2021talking, milletari2016v}.
\begin{equation}
    L_{dice}=\frac{1}{n_{AU}}\sum_{i=0}^{n_{AU}}w_i[1-\frac{2x_i\hat{x}_{i}+\varepsilon}{x_i^2+\hat{x}_{i}^2+\varepsilon}]
\end{equation}
Our total AU loss is \(L_{AU}=L_{bce}+L_{dice}\).

%After pre-training, the last fully connected layer of the network is removed and used as the AU encoder module in our framework. This is because the multidimensional AU representation obtained by the penultimate fully connected layer is more conducive to the model learning AU information than a one-dimensional label.
%the multi-dimensional AU representation obtained from the second last fully connected layer is more conducive to the model learning AU information. 
%As speech is a form of facial movement driven by multiple facial muscles in the oral area, multiple AU encoders are required to extract local facial information. This study selects five AUs related to speech based on anatomical knowledge~\cite{martinez2017automatic}: AU10, AU14, AU20, AU25, and AU26.%, as detailed in Fig.\ref{fig:teaser4}.


After obtaining the AU features \(\hat{f}_{AU}\), they are inputted into the codebook $S$. The query function $Q$ is then used to obtain the feature in the codebook $S$ that is closest to the input. The output features are the standardized AU features \(f_{AU}\). This process is defined as:
\begin{equation}
    f_{AU}=Q(\hat{f}_{AU}):=\underset{f_{{AU}_i}\in\mathcal{S}}{\arg\min}\|\hat{f}_{{AU}_i}-{f_{AU}}_i\|_2
    \label{eq1}
\end{equation}

Similar to the local AU semantic space, the global space comprises a global encoder and a global codebook $G$. The face is first inputted into the global encoder \cite{xing2023codetalker} to generate global facial features \(\hat{f}_{glo}\). \(\hat{f}_{glo}\) is then inputted into the global codebook $G$ to query and obtain standardized global features \(f_{glo}\). This process is defined as:
\begin{equation}
    f_{glo}=Q(\hat{f}_{glo}):=\underset{f_{{glo}_i}\in\mathcal{G}}{\arg\min}\|\hat{f}_{{glo}_i}-{f_{glo}}_i\|_2
    \label{eq2}
\end{equation}

Finally, the features obtained from both the local AU semantic space and the global space are inputted into the Decoder to generate the final image \(\hat{F}_{t+}\).
% \begin{equation}
%     \hat{F}_{t+}=Decoder(f_{AU}, f_{glo})
%     \label{eq3}
% \end{equation}

Constraints are imposed on the training process in standardized space from two perspectives: the angle of the generated frames and the angle of the codebook features. The loss function is shown as follows:
\[L_{S}=\|\hat{F}_{t+}-F_{t}\|_{1}\\+\|\mathrm{sg}(\hat{f}_{AU})-f_{AU}\|_{2}^{2}+\]
\begin{equation}
    \beta_1\|\hat{f}_{AU}-\mathrm{sg}(f_{AU})\|_{2}^{2}+\|\mathrm{sg}(\hat{f}_{glo})-f_{glo}\|_{2}^{2}+\beta_2\|\hat{f}_{glo}-\mathrm{sg}(f_{glo})\|_{2}^{2}
    \label{eq4}
\end{equation}
where $sg$ denotes a stop-gradient operation. This operation is defined as an identity function during forward propagation, but its gradients are not computed during backpropagation. This effectively constrains its operand to remain a non-updated constant. And \(\beta_1\) and \(\beta_2\) denote the trade-off parameters.



% \begin{figure*}
%   \centering
%   \includegraphics[width=\textwidth]{pic/ex.pdf}
%   \caption{Visual results of the comparative experiments.}
% %    \vspace{-6pt}
%   \label{fig:teaser5}
% \end{figure*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]
\resizebox{\textwidth}{!}{
\begin{tabular}{c|ccccc|ccccc|c}
\toprule
\multirow{2}{*}{Method} & \multicolumn{5}{c|}{TestA} & \multicolumn{5}{c|}{TestB} & \multirow{2}{*}{\textcolor{blue}{Ne}} \\ \cline{2-11}
                & SSIM↑  & LPIPS↓ & AU Acc↑ & LMD-79↓  & SyncNet↑   & SSIM↑   & LPIPS↓  & AU Acc↑        & LMD-79↓  & SyncNet↑   &      \\ \midrule \midrule
    Ground-truth  & -      & -      & -       & -        & 2.729      & -                   &-        &-               &-         &2.772       & -     \\
    Wav2Lip 2020\cite{prajwal2020lip}  &  0.793 &0.052    & 72.2\%    & 2.921      & \textbf{2.237}   &0.821 &0.059   &76.7\%  &3.067  &\textbf{1.971} & \ding{53} \\
    DFRF 2022\cite{shen2022learning}  & 0.818  & 0.045 & 71.4\% & 3.003 & 1.765     & 0.851  & 0.041  & 75.7\% & 2.859  & 1.694  & \ding{51}                \\
    GeneFace 2023\cite{ye2023geneface}   & 0.811 & 0.042  & 73.1\%   & 2.815  & 1.793   & 0.857  & 0.036 & 75.4\%  & 2.864 & 1.712  & \ding{51} \\ 
    NeRF-AD 2024\cite{bi2024nerf}   & 0.831 & 0.040  & 73.3\%   & 2.633  & 1.801   & 0.882  & 0.032 & 76.9\%  & 2.772 & 1.756  & \ding{51} \\ 
    Synctalk 2024\cite{synctalk}   & 0.828 & 0.043  & 73.6\%   & \textbf{2.511}  & 1.793   & \textbf{0.885}  & 0.029 & 76.9\%  & 2.852 & 1.727  & \ding{51} \\ \midrule
    \textbf{Ours}  & \textbf{0.835} & \textbf{0.037} & \textbf{75.3\%} & 2.529 & 1.956  & 0.880 & \textbf{0.025} & \textbf{78.4\%} & \textbf{2.650} & 1.868& \ding{51}  \\ \bottomrule
\end{tabular}}

\caption{Quantitative results compared with other methods. Best results are in \textbf{bold}. \textcolor{blue}{"Ne"} indicates whether this method is based on NeRF.}
\vspace{-10pt}
\label{tab:t1}
\end{table*}

\section{Experiments}
\subsection{Experimental Settings}
\textbf{Dataset.}
To compare with state-of-the-art methods, follow the approach of \cite{shen2022learning, ye2023geneface}, we collect four videos of different individuals giving speeches, interviews, or reading news from their publicly released video sets. 
%Each video has a frame rate of 25fps and an average of 4,000 frames. During pre-processing, we resize each frame to 512$\times$512. 
Additionally, we preserve the facial head segment within the dataset to facilitate the generation of 3D talking heads. Following the approach of \cite{yao2022dfa}, these videos are randomly divided into two groups for experimentation.



\textbf{Evaluation Metrics.}
The quality of the generated images is evaluated using SSIM \cite{wang2004image}, and LPIPS ~\cite{zhang2018unreasonable}. Lip synchronization is evaluated using AU Acc \cite{chen2021talking}, landmark distance (LMD)~\cite{chen2018lip}, and SyncNet confidence value \cite{prajwal2020lip}. In order to evaluate the lip shape more accurately, we have improved the LMD metric by increasing the number of calculated lip landmarks from 20 to 79, consistent with the approach of \cite{bi2024nerf}. %These landmarks are denoted as LMD-79.



% \textbf{Implementation Details.}
% NeRF-3DTalker is implemented using PyTorch and train on two NVIDIA A100s. Consistent with current talking head methods based on NeRF \cite{yao2022dfa, shen2022learning, guo2021ad, bi2024nerf}, we derive the head pose for the talking head from the camera parameters of real images. During model training, we sequentially train the 3D Prior Aided Audio Disentanglement module, Conditional Neural Radiance Field, and Standardized Space, which collectively require 48 hours. 

% The real AU labels used for model training are extracted using OpenFace \cite{baltruvsaitis2015cross,baltrusaitis2018openface}. In Sec.\ref{ss}, the semantic and global codebooks have sizes of 500$\times$256 and 8192$\times$256, respectively. The semantic features for image generation have dimensions of 200$\times$256, while the global features have dimensions of 1024$\times$256. It's worth noting that the initial output of our semantic encoder is 5$\times$256. To improve the expression of semantic features, the output dimension has been increased to 200$\times$256.


\subsection{Quantitative Results}



%In this section, we present the results of quantitative comparative experiments with state-of-the-art methods, including NeRF-AD\cite{bi2024nerf}, GeneFace \cite{ye2023geneface}, DFRF~\cite{shen2022learning}, and Wav2Lip \cite{prajwal2020lip}. 
The experimental results are shown in Table~\ref{tab:t1}. The table shows that our method outperforms the others in most of the metrics. 
%It is important to that the commonly used PSNR metric is not utilised for evaluation due to its tendency to give higher values for blurry images \cite{park2021nerfies}. 
Our method achieves a decrease of $0.016$ in LPIPS↓ and a significant improvement of 2.7\% in AU Acc↑ compared to DFRF \cite{shen2022learning} on dataset B. In terms of lip synchronization metrics, our method achieves an increase of 3\% in AU Acc↑ and a decrease of 0.021 in LMD-79↓ compared to Geneface \cite{ye2023geneface} on dataset B. 
%On dataset B, our method achieves an increase of 2.2\% in AU Acc↑ and a decrease of 0.021 in LMD-79↓. 
Our method also increases AU Acc↑ by 1.7\% on Test A and by 1.5\% on Test B compared with Synctalk \cite{synctalk}. 
Overall, compared with NeRF-based methods, our method achieves the highest level of lip synchronization, demonstrating the importance of aligning acoustic space with 3D visual space in 3D talking head synthesis.
We also compare our method with non-NeRF-based method. Our method demonstrates significant advancements in image quality compared to Wav2Lip \cite{prajwal2020lip}. Regarding lip synchronization, since Wav2Lip was trained on a large dataset specifically for this metric, it slightly outperforms ours on the SyncNet metric. %However, our method performs best on the AU Acc and LMD-79 metrics.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[b]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{1}{c|}{Method}     & SSIM↑          & LPIPS↓         & AU Acc↑        & LMD-79↓           & SyncNet↑       \\ \midrule \midrule
\multicolumn{1}{c|}{Ground-truth}      & -              & -              & -              & -              & 2.729          \\ %\cline{1-7} 
\multicolumn{1}{c|}{use ent. aud.}       & 0.796          & 0.046          & 69.1\%          & 3.001          & 1.412          \\
\multicolumn{1}{c|}{use spe. aud.}                & 0.808          & 0.044 & \textbf{72.7\%}          & 2.794          & 1.699          \\
\multicolumn{1}{c|}{use dis. aud.}         & \textbf{0.819}          & \textbf{0.041}          & 72.4\%          & \textbf{2.637}          & \textbf{1.721}          \\  \bottomrule
\end{tabular}}
\caption{The ablation study of 3D Prior Aided Audio Disentanglement module.}
\label{tab:t2}
\end{table}

%\vspace{10pt}

\subsection{Qualitative Results}
The qualitative comparison results with other methods are shown in Fig.\ref{fig:teaser3}. It is evident that our method produces mouth images that are closest to the Ground Truth, as indicated by the red region in the graph. 
%In contrast, the mouth images generated by DFRF \cite{shen2022learning}, Geneface \cite{ye2023geneface}, and NeRF-AD \cite{bi2024nerf} methods deviate significantly from reality. 
%Furthermore, with Wav2Lip \cite{prajwal2020lip} solely focused on lip synchronization, it demonstrates lower image clarity compared with other methods. Overall, our method produces visual effects that exceed those of the current state-of-the-art approaches.
In addition, by adjusting the camera poses of NeRF, we also compare our NeRF with the other NeRF models for multi-view synthesis.
%Furthermore, we also compare our NeRF with the current audio-
%driven NeRF models for multi-view synthesis.
%compared our NeRF-3Dtalker for multi-view synthesis with other NeRF models by adjusting the camera poses. 
The comparison results are depicted in Fig.~\ref{fig:teaser4}.
%The left side displays a comparison of ours and other methods using multiple views, while the right side showcases additional synthesis results of our method.
The results demonstrate that our method generates the best images under identical camera pose conditions.
This is due to datasets containing only frontal views of talking heads, which limits the ability of NeRF to accurately reconstruct a comprehensive three-dimensional space for talking heads.
In contrast, our approach integrates 3D prior knowledge and a 3D Prior Aided Audio Disentanglement module, enabling NeRF to reconstruct crisp 3D talking head features.


\subsection{Ablation Studies}





To measure the impact of the primary components of our proposed model, we conduct ablation studies, which are divided into two parts.

Table \ref{tab:t2} shows the impact of the various components of the 3D Prior Aided Audio Disentanglement module.
The entry labelled 'use ent. aud.' presents the experimental results obtained by using the entire speech audio. The entries labelled 'use spe. aud.' and 'use dis. aud.' present the experimental results obtained by using only the features related to 3D awarded speech movements \(f_{exp-aud}\) and using the two disentanglement features, respectively.
%The table shows that using undisentangled audio to predict facial expression parameters results in lower lip synchronization of generated frames.
%, particularly with AU Acc metric at only 69.1 and LMD-79 metric at 3.001. 
%However, lip synchronization of generated frames improves when we disentangle the audio and only use parameters related to 3D awarded speech movements. 
%Furthermore, the image quality and lip synchronization of generated frames are improved when using the disentanglement features \(f_{exp-aud}\) and \(f_{exp-style}\).


\begin{table}[t]
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{1}{c|}{Method}     & SSIM↑          & LPIPS↓         & AU Acc↑        & LMD-79↓           & SyncNet↑       \\ \midrule \midrule
\multicolumn{1}{c|}{Ground-truth}      & -              & -              & -              & -              & 2.279          \\ %\cline{1-7} 
\multicolumn{1}{c|}{w/o Global}       & 0.819          & 0.041          & 72.4\%          & 2.637          & 1.721          \\
\multicolumn{1}{c|}{w/o AU}                & 0.832          & 0.039 & 73.1\%          & 2.624          & 1.719          \\
\multicolumn{1}{c|}{Ours}         & \textbf{0.835} & \textbf{0.037} & \textbf{75.3\%} & \textbf{2.529} & \textbf{1.956} \\  \bottomrule
\end{tabular}}
\caption{The ablation study of Standardized Space module.}
\vspace{-20pt}
\label{tab:t3}
\end{table}

The impact of various components of the Standardized Space is presented in Table \ref{tab:t3}.
The entries labelled "w/o Global" and "w/o AU" indicate the experimental results after removing the global space and the local AU semantic space, respectively. 
The table demonstrates that removing these components step by step results in a decrease in the quality of the generated images and the lip synchronization. 
%In particular, removing the local AU semantic space results in a significant decrease in lip synchronization, with the AU Acc↑ metric decrease by 2.2\% and the SyncNet↑ metric decreasing by 0.24.

%The visualization results of the experiments are presented in Fig.\ref{fig:teaser7}. It is evident that video frames without local AU semantic space may exhibit blurry lip shapes, as indicated by the blue arrow in the figure. Furthermore, after removing the global space, more blurred lines appear on the talking heads. Finally, in the absence of the 3D Prior Aided Audio Disentanglement module, the lip synchronization in the generated images tends to deteriorate. For instance, a lip synchronization error is evident in the third frame of the second row in the figure.

\section{Conclusion}

This paper presents NeRF-3DTalker to synthesize 3D talking head with free views.%, a NeRF-based method for synthesizing 3D talking head with free views. 
To tackle the problem of blurry novel view facial images, we parameterize the face using 3D priors extracted by 3DMM.
Furthermore, we introduce a 3D Prior Aided Audio Disentanglement module to simplify the learning process of the NeRF and 
%enhance the model's ability to learn multimodal features. 
%Through this 3D Prior Aided Audio Disentanglement module, we can further 
align the acoustic space with the 3D visual space, thereby enhancing lip synchronization.  Subsequently, those 3D priors and disentangled semantic features are utilized to guide the presented conditional NeRF in synthesizing 3D talking heads.
Finally, a Standardized Space is designed to bring the generated frames distant from the speaker’s motion space back to the real space from both global and local semantic perspectives. 
%Extensive qualitative and quantitative experiments are conducted to demonstrate that our NeRF-3DTalker outperforms other methods in both image quality and lip synchronization.
Extensive qualitative and quantitative experiments demonstrate the superiority of our NeRF-3DTalker.

\textbf{Acknowledgement.} This work was supported by National Natural Science Foundation of China under Grant No. 62172294.
% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

%\section*{References}
\bibliographystyle{IEEEtran}
\bibliography{IEEEfull}



\end{document}
