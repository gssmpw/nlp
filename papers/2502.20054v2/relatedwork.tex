\section{Related Work}
\label{section: related work} 
In this section, we discuss the most related research works on priori LiDAR map-based visual localization, along with nocturnal visual state estimation and SLAM. 

\subsection{LiDAR Map-based Visual Localization}
Due to the long-term stability and rich geometric information of LiDAR maps, cross-modal localization attracts substantial research attention. Early research works focus on comparing the similarity between images and projected LiDAR data by utilizing normalized mutual information (NMI) \cite{wolcott2014visual}, normalized information distance (NID) \cite{stewart2012laps}, intensity \cite{pascoe2015direct}, and depth \cite{neubert2017sampling, zuo2019visual}. Some methods \cite{ding2019persistent, caselitz2016monocular, zhang2023cross, zuo2020multimodal} propose to transform the 2D-3D matching problem into a 3D-3D matching problem, thus the structure information can be utilized. Caselitz et al. \cite{caselitz2016monocular} employ a visual odometry algorithm to reconstruct 3D points from visual features. The sparse reconstructed points are matched with the map for tracking. In \cite{zhang2023cross}, semantic consistency between visual features and maps is introduced to improve the accuracy of point-to-plane Iterative Closest Point (ICP) problems. Structural features (e.g., lines) \cite{yu2020monocular, zhou2021visual, leng2024cross} are also utilized for sufficient matches. Recently, deep learning techniques are introduced into cross-modal localization tasks \cite{miao2023poses, shubodh2024lip, zhao2023attention, yin2021i3dloc, cattaneo2019cmrnet, chen2022i2d, wu2024lhmap_icra2024}. While these methods achieve reasonable accuracy, they necessitate dense maps and abundant visual information for cross-modal correspondences, which can be challenging to apply in nighttime scenes. 

To alleviate the huge memory cost of dense maps, common landmarks (e.g., lanes, poles, signboards) are utilized to establish lightweight maps \cite{qin2021light, pauls2020monocular, liao2020coarse, wang2021visual}. In \cite{pauls2020monocular}, the authors leverage semantic segmentation and a distance transformation method for data association. Liao et al. \cite{liao2020coarse} develop a coarse-to-fine localization method, using the coarse pose from wheel odometry to perform the data association of poles and refining the pose from matches. Wang et al. \cite{wang2021visual} propose a new data association method that exploits the local structure, global pattern, and temporal consistency. Compared to the landmarks utilized in the aforementioned works, streetlights are stable, salient, and easily detectable in low-illumination environments, making them suitable for nighttime visual localization. 

\subsection{Nocturnal Visual State Estimation and SLAM}
Conventional visual-inertial odometry (VIO) and vision-aided SLAM methods \cite{forster2016svo, engel2017dso, mur2017orb, bloesch2017rovio, qin2018vins, geneva2020openvins, campos2021orb, liu2019visual, leutenegger2015keyframe, usenko2019visual}, such as VINS-Mono \cite{qin2018vins} and Multi-State-Constraint Extended Kalman Filter (MSCKF) \cite{geneva2020openvins, mourikis2007multi},  typically rely on pixel-level features from images. Unfortunately, these methods struggle in nocturnal environments \cite{kim2020dark_synthetic_vision}, as discussed in Section \ref{subsection: crux}. Thanks to the power of data-driven machine learning techniques, methods for low-light image enhancement \cite{lore2017llnet, jiang2021enlightengan, ma2022toward, yang2023implicit, wu2023learning, wang2024zero, gomez2018learning, jung2020multi} are developed to improve feature extraction. In \cite{gomez2018learning}, a deep neural network is designed to generate more informative images for visual odometry (VO). Jung et al. \cite{jung2020multi} further improve the VO by introducing temporal and spatial constraints into the neural network. However, the issues of generalization and heavy computational burden significantly hinder the application of these methods in robotic tasks. Moreover, some studies focus on designing robust descriptors \cite{alismail2016direct, pascoe2017nid}, explicitly modeling camera parameters using photometric models \cite{engel2017dso, bergmann2017online, bloesch2017rovio, yang2018challenges}, and controlling camera attributes like exposure time and gain \cite{litvinov2005addressing, lu2010camera, zhang2017active, weidner2017underwater, kim2020dark_synthetic_vision, kim2020proactive}. In \cite{kim2020dark_synthetic_vision}, an exposure control scheme that considers inter-frame consistency is proposed, which effectively increases the matched features for visual SLAM. This work is further extended by jointly considering exposure time and gain \cite{kim2020proactive}. Nevertheless, these approaches require an in-depth understanding of the specific sensor and heuristic tuning of parameters, which impedes their generalization to different setups. In addition, these methods still essentially rely on pixel-level correspondences, which cannot resolve the fundamental \textit{insufficiency} and \textit{inconsistency} issues in nocturnal visual state estimation, as discussed in Section \ref{subsection: crux}. Few studies investigate consistent and stable visual features for nocturnal state estimation. Nelson et al. \cite{nelson2015dusk} propose NightNav, a visual localization system that uses a prior map storing streetlight images from different views and positions. However, the assumption that the area of streetlight blobs is inversely proportional to the distance of streetlights from the camera is overly stringent. When occlusion and perspective change cases occur, the distance-area function is unreliable. Besides, the pre-stored images of all streetlights in NightNav increase the storage requirement. Our previous work, Night-Rider \cite{gao2024night}, only uses the object-level visual association from streetlights, demonstrating accurate localization performance within the streetlight map. However, Night-Rider ignores historical information and suffers from degradation and degeneration cases. Additionally, Night-Rider can only be applied in nighttime scenes rather than all-day scenarios. 

In this work, a novel state estimation framework named Night-Voyager is proposed, leveraging hybrid object-level and pixel-level visual correspondences to tackle the fundamental bottlenecks of \textit{insufficiency} and \textit{inconsistency} in nocturnal visual cases, thereby addressing the critical crux inherent in mainstream pixel-level methodologies. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
% preliminary and state definition