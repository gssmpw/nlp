\section{Results}
\label{sec:experiments}

%\input{aa02-table_model}
\input{a03-result3_NEW}
\input{a02-result2_NEW}
\input{a06-result6_NEW}
\input{a01-result1_NEW}
\input{a04-result4_NEW}
\input{a05-result5_NEW}

In this section, we demonstrate that our model efficiently explores a diverse range of GUI alternatives that closely match the specified input conditions, outperforming other models in alignment and efficiency. More results are shown in the Supplementary Materials.


\subsubsection{Qualitative Results}


To evaluate our approach, we qualitatively demonstrate that our model is capable of generating diverse low-fidelity GUI samples from various input combinations. Specifically, we aim to assess: 
%
1) whether the model can produce diverse webpage and mobile GUI results solely based on the given prompt; 
%
2) whether it can accurately capture local properties given different wireframe inputs;
%
3) whether it can generate suitable GUIs styles that appropriately align with the descriptions in the text prompt;
%
4) whether it can adapt to modifications in the prompt while still following the wireframe specifications; 
%
and 5) whether it can capture global properties by aligning with the desired visual flow, both with and without wireframe inputs.

Thus, Our qualitative evaluation focuses on the aspects of diversity controllability, and adaptability.

\paragraph{Diversity} 
The model excels at generating diverse GUI designs. 
\autoref{fig:result3} demonstrates the model’s ability to generate a diverse range of webpage and mobile GUI designs based on the prompt, "a promotional site for cocktails." This highlights the model's versatility in producing varied designs from a simple text prompt, without relying on wireframe or visual flow inputs. It underscores the model’s capacity to interpret abstract concepts and deliver customized, platform-specific designs.

\paragraph{Controllability} 

The model reliably aligns its outputs with wireframe inputs and textual descriptions.
\autoref{fig:result2} demonstrates that GUIs produced from different wireframe inputs for the same prompt align well with their respective wireframes. This shows the model’s ability to reliably translate structural information from wireframes into coherent GUI designs.
%
\autoref{fig:result6} showcases the model’s capability to generate GUI styles that align with textual descriptions. However, if a wireframe is poorly suited to the specified GUI prompt, the model may produce results that are less accurate or deviate from the intended design.

\paragraph{Adaptability} 

The model shows its adaptability to varying design constraints. 
Some evidence of the model’s adaptability is shown in \autoref{fig:result1}, where, after adding a ``red-themed'' constraint to the prompt, the generated GUIs consistently exhibit a red theme while maintaining design diversity. This highlights the model's proficiency in adapting to new specifications while adhering to the provided wireframe structure.
%
In addition, \autoref{fig:result4} demonstrates the model’s ability to follow specified visual flow directions by generating varied layouts and graphic elements, regardless of the presence of wireframe inputs. This confirms the model’s flexibility in aligning with desired visual flows, reinforcing its capability to produce diverse and contextually relevant GUI designs.



\subsubsection{Comparison to Existing Models}

We provide a comparison of our model with existing state-of-the-art generative models in terms of adherence to input specifications and output quality.
In~\autoref{fig:result5}, we demonstrate that our model aligns more closely with input specifications compared to previous generative models. We compare our approach with related models, including ControlNet~\cite{controlnet}, and two commercial models, Galileo AI and GPT4o. Galileo AI is a commercial GUI generation model, while GPT4o is a state-of-the-art generative model. Since none of these models account for visual flow, our comparison is limited to prompt and wireframe inputs.
%
ControlNet excels in following wireframes but struggles to produce meaningful and realistic webpages. Conversely, Galileo AI focuses on generating realistic webpages but does not adhere to the input wireframe. Similarly, GPT4o fails to follow the wireframe and cannot generate realistic webpages. In contrast, our model effectively generates webpages that closely align with both the wireframe and prompt, producing realistic results.

\subsubsection{Efficiency}

Our model is efficient for both training and inference. Since the Stable Diffusion parameters are frozen and the adapters can be trained in parallel, the training process is efficient. Each adapter is lightweight, and the entire model can be trained in approximately 30 hours over 12 epochs on 256x256-resolution images using a single NVIDIA GeForce RTX4090 GPU. During inference, the model generates a batch of 16 GUI examples in around 15 seconds. GUIs within the same batch exhibit more similarity to each other than those across different batches.



% Importantly, our approach allows a significant relaxation in the input side. 
% The designer can provide \emph{any} combination of the three as input and will get a diverse set of low-fidelity solutions in a gallery in response. 
% In practice, hundreds of designs can be explored in the order of minutes.
% %
% The benefit is that large design spaces can be explored with very little effort in input-specification. 

% \begin{table*}[ht]
%   \caption{\textbf{Quantitative Evaluation - UI Synthesis Realism and Segmentation Map Adherence.} We compare the input semantic segmentation mask with the map obtained from the generated UI from our best-performing automatic segmentation method: BLIP-2 guided GroundingDINO-SAM. 
%   For the Mean-Opinion-Score (MOS) study $N$ assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale (1 - 5).}
%   % the lowest and the highest scores being 1 point (“Bad”) and 5 points (“Excellent”) with a step of 0.5 point.
%   % Replace N with a number.
  
%   \label{tab:quant_realism_and_segmask_adherence}
%   \begin{tabular}{lccccc}
%     \toprule
%     \multirow{2}{*}{\textbf{Methods}} & 
%     \multicolumn{3}{c}{\textbf {}}  & \multirow{2}{*}{\textbf {mIoU $\uparrow$}} & \multicolumn{1}{c}{\textbf {Human Evaluation}} \\
%     \cmidrule(lr){2-4} 
%     \cmidrule(lr){6-6}
%      & cFID $\downarrow$ & CLIP-cFID $\downarrow$ & cKID $\downarrow$ & & Likert-scale MOS $\uparrow$ \\      
%     \midrule
%     Galileo.AI & - & - & - & -  \\
%     GPT-4 & - & - & - & -  \\

%     \midrule
%   %  SPADE (Ours) & - & - & - & -\\   
%     % OASIS & - & - & - & -  \\   
%     % MakeAScene & - & - & - & -  \\
%     UniControlNet & - & - & - & -  \\  
%     MultiControlNet & - & - & - & - \\  
%     ControlNet & - & - & - & -  \\   
%     ControlNet++ & - & - & - & -  \\   
%     \midrule
%     \textbf{Ours} & - & - & - & -\\
%     \bottomrule
%   \end{tabular}
% \end{table*}







% \subsection{Ablation Study}



% \subsubsection{Inputs}
% \begin{table*}[ht]
%   \caption{\textbf{Ablation Study - The effect of Visual Saliency (VS) maps, Scan-Paths and Prompt guidance on the overall quality of UI generations.}}
%   \label{tab:ablation_inputs}
%   \begin{tabular}{lccccccc}
%   \toprule
%   \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf {Upgrades}} & \multicolumn{3}{c}{\textbf {Non-Reference Metrics}}  & \multirow{2}{*}{\textbf {mIoU $\uparrow$}} \\      
%   \cmidrule(lr){2-4} 
%   \cmidrule{5-7}
%   & Wireframe & Scan-Paths & Prompts & cFID $\downarrow$ & CLIP-FID $\downarrow$ & cKID $\downarrow$ & \\      
%   \midrule
%     % Base & \xmark & \xmark  & \xmark & - & -  & - & - \\ 
%     V1 & \cmark & \xmark  & \xmark  & - & -  & - & - \\  
%     V2  & \xmark & \cmark & \xmark & - & -  & - & - \\   
%     V3 & \xmark & \xmark & \cmark & - & -  & - & - \\   
%     V4 & \cmark & \cmark & \xmark & - & - & -  & -  \\   
%     V5 & \cmark & \xmark & \cmark & - & - & -  & -  \\   
%     V6 & \xmark & \cmark & \cmark & - & - & -  & -  \\   
%     \midrule
%     \textbf{Ours} & \cmark & \cmark & \cmark & - & - & - & - \\
%     \bottomrule
%   \end{tabular}
% \end{table*}


% 1/23
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO: 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 1. Clean-FID, CLIP-FID & Clean-KID
% 2. Scanpath validity - User study and/or comparison with other scanpath prediction models.
% 3. MIoU: Wireframe adherence - Qualitative (overlay on output)
% 4. Ablation - w/o softDTW(EyeFormer(z, cf)) (Novelty in loss can be ablated and shown that it helps (hopefully!))