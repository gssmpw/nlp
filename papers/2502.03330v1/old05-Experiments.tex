%\noindent \textbf{Unified Controller FineTuning}
%Write the final equation and the CfG Equation. 
%Then leave this section by showing the drawbacks with text-dominant GUIs. Motivating the need for our automatic text-painting module, AutoText. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{AutoText}
% \noindent \textbf{Diffusion Models as Text-Painters}     

% Still need humans to specify what to write.

% \noindent \textbf{Re-using Llava-NeXT for Automatic Painting}  

% \noindent \textbf{Iterative-drawing using the segmentation mask}      

% \noindent \textbf{Smudge Trick}

\subsection{Metrics}
Use paper: Large-Scale GAN training study (Related works) for IS, FID ... choice / usage and differences on what it captures or not.

\subsection{Baselines}
We also train models that receive the segmentation mask input at all the intermediate layers via feature
concatenation in the channel direction, which is termed as
pix2pix++ \textit{Concat.} and pix2pixHD++ \textit{Concat.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \subsection{Background}    
   
% \noindent \textbf{Visual Flow Predictors}   


% % \noindent \textbf{Sequence Modeling and Prediction - Visual Flow}     


% \noindent \textbf{Latent Diffusion Models (LDMs)}    


% \noindent \textbf{Vision Language Models (VLMs)}

          
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \subsection{Unified Resolution-Agnostic GUI Detection}
% Finding and understanding the composition of UI elements or their building blocks is imperative to improve UI designs and end-user interactions. 
% Automatic element detection is a challenging problem due to A) \textit{Agnosticism challenge:} the wide variety of UIs, devices, screen resolutions, framework-specific styles, and density of elements B) \textit{non-natural mixture challenge:} the mixture of natural images, text, and non-natural discrete elements within UIs, and C) \textit{Non-availability of high-quality GT challenge}. 

% Traditional methods like UIED~\cite{uied} tackle the first challenge (A) by using optical character recognition for text and traditional computer vision algorithms to detect UI elements across different types of UIs. 
% However, such methods need careful hand-crafted designing and often need to be re-updated as designs get more complicated and new UI elements are introduced. 
% Wu~\emph{et al.}\cite{webui} introduced an inductive transfer learning-based UI element detection method using Fully Convolutional One-Stage Object Detection (FCOS~\cite{fcos}) that showed promise for domain transfer and zero-shot capabilities from mobile UIs to Web UIs, tackling challenge A. 
% However, a lack of semantic understanding and implicit UI element hierarchy hinders the method's performance. 
% A major challenge for GUI element detection is to detect small UI components like icons. 
% Most, convolution based object detectors would have to utilize the high resolution feature maps to extract these components. 
% This becomes extremely computationally cumbersome as the input UI resolution increases and using fixed-resolution pretrained networks out of the box, shows significant performance drops (decreasing receptive field) (Cite DPT). 
% The need for high-quality (clean) datasets is also exacerbated in such scenarios.   

% Contrarily, attention-based architectures offer long-range contextual understanding over conventional convolutional networks and are promising systems for capturing an implicit understanding of UIs (hierarchy, positioning, alignment, and different kinds of elements) effectively. 
% We argue that the absence of any local inductive bias due to no convolution operations is beneficial for the UI element detection problem as the convolution operator induces local patches to have high correlations. 
% This is generally not true for UIs due to the non-natural-ness of UIs. 
% The non-natural mixture challenge (B) and agnosticism challenge (A) force the method to be generalizable. 

% Recently, NaViT~\cite{navit} proposed a resolution agnostic transformer and MiDaS~\cite{midas} empirically demonstrates that mixing datasets and using an attention-based feature extractor helps achieve higher task accuracy and generalizability across domains with promising zero-shot capabilities. Additionally, Detection Transformer (DeTR~\cite{detr}), Deformable-DeTR~\cite{deformable-detr}, and Co-DeTR~\cite{codetr} have outperformed convolution-based methods on natural image detection. 
% % In this paper, we present the first unified and generalizable attention-based UI detection model: Resolution Agnostic DeTR (RADeTR). 


% In this paper we show that DeTR, despite being a decoder attention based-method, fails to effectively detect UI elements. 
% Since, attention based encoders have quadratic complexity (wrt to pixels) for computing attention, it makes them un-suitable for large feature resolution pre-training. 
% Hence, DeTR uses a convolutional feature encoder/extractor.
% Similar to cite.Deformable-DETR, we find that this convolutional encoder is at the core of the problem, failing to extract meaningful features from small UI elements and inducing local biases leading to poor generalization.  


% To that end, we propose a unified model for any-UI detection: Resolution Agnostic Deformable Detection Transformer (RAD-DeTR). 
% Deformable attention in the encoder overcomes the need for local-convolution and hence the inductive local bias or local element relation modeling while our continuous token dropping and random resolution sampling inspired by NA-ViT~\cite{navit} handles resolution challenge. 
% Similar to~\cite{midas}, we also mix all UI datasets for a unified dectection model with zero-shot capabilities. 
% Overall, we demonstrate higher precision and more complete detections, out-of-domain adaptation/generalization capabilities with our method over prior work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experiments}
\label{sec:experiments}

\subsection{Metrics}
Use paper: Large-Scale GAN training study (Related works) for IS, FID ... choice / usage and differences on what it captures or not.

\subsection{Baselines}
We also train models that receive the segmentation mask input at all the intermediate layers via feature
concatenation in the channel direction, which is termed as
pix2pix++ \textit{Concat.} and pix2pixHD++ \textit{Concat.}

\begin{table*}[ht]
  \caption{\textbf{Quantitative Evaluation - UI Synthesis Realism and Segmentation Map Adherence.} We compare the input semantic segmentation mask with the map obtained from the generated UI from our best-performing automatic segmentation method: BLIP-2 guided GroundingDINO-SAM. 
  For the Mean-Opinion-Score (MOS) study $N$ assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale (1 - 5).}
  % the lowest and the highest scores being 1 point (“Bad”) and 5 points (“Excellent”) with a step of 0.5 point.
  % Replace N with a number.
  
  \label{tab:quant_realism_and_segmask_adherence}
  \begin{tabular}{lccccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & 
    \multicolumn{3}{c}{\textbf {}}  & \multirow{2}{*}{\textbf {mIoU $\uparrow$}} & \multicolumn{1}{c}{\textbf {Human Evaluation}} \\
    \cmidrule(lr){2-4} 
    \cmidrule(lr){6-6}
     & cFID $\downarrow$ & CLIP-cFID $\downarrow$ & cKID $\downarrow$ & & Likert-scale MOS $\uparrow$ \\      
    \midrule
    Galileo.AI & - & - & - & -  \\
    GPT-4 & - & - & - & -  \\

    \midrule
    SPADE (Ours) & - & - & - & -\\   
    % OASIS & - & - & - & -  \\   
    % MakeAScene & - & - & - & -  \\
    UniControlNet & - & - & - & -  \\  
    MultiControlNet & - & - & - & - \\  
    ControlNet & - & - & - & -  \\   
    ControlNet++ & - & - & - & -  \\   
    \midrule
    \textbf{Ours} & - & - & - & -\\
    \bottomrule
  \end{tabular}
\end{table*}

\begin{table*}[ht]
  \caption{\textbf{Quantitative Evaluation - UI synthesis' Scan-Path Input Adherence.} We evaluate the Dynamic Time Warping error for scan-paths using the top-2 performing methods fine-tuned on UEyes~\cite{ueyes}.}
  
  \label{tab:quant_scanpath_adherence}
  \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & 
    \multicolumn{2}{c}{\textbf {Scan-Path Adherence (DTW)}}\\
    \cmidrule(lr){2-3} 
    & DeepGaze++ $\downarrow$ & PathGAN++ $\downarrow$  \\      
    \midrule
    FT-pix2pix \textit{Concat.} & - & - \\   
    FT-pix2pixHD  \textit{Concat.} & - & -  \\  
    \midrule
    \textbf{Ours} & - & - \\
    \bottomrule
  \end{tabular}
\end{table*}


\begin{table*}[ht]
  \caption{\textbf{Quantitative Evaluation - UI synthesis' Visual Saliency Map Adherence.} We evaluate visual saliency maps of the generated UIs using the top-2 performing methods fine-tuned on UEyes~\cite{ueyes}. Note that we use the \textit{Concat.} variants of UI-finetuned pix2pix and pix2pixHD similar to~\cref{tab:quant_scanpath_adherence} here.}
  
  \label{tab:quant_vs_adherence}
  \begin{tabular}{lcccccccccc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & 
    \multicolumn{2}{c}{\textbf {AUC-Judd} $\uparrow$} & \multicolumn{2}{c}{\textbf {NSS} $\uparrow$} & \multicolumn{2}{c}{\textbf {Info Gain} $\uparrow$} & \multicolumn{2}{c}{\textbf {SIM}$\uparrow$}  & \multicolumn{2}{c}{\textbf {CC} $\uparrow$} \\
    \cmidrule(lr){2-3} 
    \cmidrule(lr){4-5} 
    \cmidrule(lr){6-7} 
    \cmidrule(lr){8-9} 
    \cmidrule(lr){10-11} 
    & SAM++ & UMSI++  & SAM++ & UMSI++  & SAM++ & UMSI++  & SAM++ & UMSI++ & SAM++ & UMSI++   \\      
    \midrule
    pix2pix & - & - & - & - & - &  - & - & - & - & -  \\   
    pix2pixHD & - & - &  - & - & - & - & - & - & - & - \\  
    \midrule
    \textbf{Ours} & - & - &  - & - & - & -  & - & - & - & -\\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{CRAP Principle Based Automatic Design Quality Evaluation}
We use UIClip~\cite{uiclip} to evaluate human-preference aligned scores for generated UI designs in~\cref{tab:quant_crap}, complementing the mean-opinion score (MOS) evaluation in~\cref{tab:quant_realism_and_segmask_adherence}. 
Additionally, UIClip aligns with the UI preferences of 12 human \textit{designers} and adheres to CRAP principles for better UI designs. 
This contrast with our non-expert/general human preference MOS study provides a more comprehensive evaluation of our solution.


\begin{table*}[ht]
  \caption{\textbf{Automatic Quantitative Evaluation of CRAP-Principles for UI Design Quality.} We use a human-rating aligned pretrained model named UIClip~\cite{uiclip} to evaluate the approximate human preference for generated UI designs using the same segmentation map. We take the mean across X UI generations.}
  \label{tab:quant_crap}
  \begin{tabular}{lc}
    \toprule
    \multirow{2}{*}{\textbf{Methods}} & 
    \multicolumn{1}{c}{\textbf {CRAP Design Quality}}\\
    \cmidrule(lr){2-2} 
    & UIClip Score $\uparrow$  \\      
    \midrule
    FT-pix2pix & - \\   
    FT-pix2pixHD & -  \\  
    FT-pix2pix \textit{Concat.} & - \\   
    FT-pix2pixHD  \textit{Concat.} & -  \\  
    ControlNet & -  \\   
    FT-ControlNet & -  \\   
    \midrule
    \textbf{Ours} & - \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Ablation Study}

\subsubsection{Model Architecture - Components and Design}
\begin{table*}[ht]
  \caption{\textbf{Ablation Study - Architectural Components and Design}}
  \label{tab:ablation_model}
  \begin{tabular}{lccccccc}
  \toprule
  \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf {Upgrades}} & \multicolumn{3}{c}{\textbf {Non-Reference Metrics}}  & \multirow{2}{*}{\textbf {mIoU $\uparrow$}} \\      
  \cmidrule(lr){2-4} 
  \cmidrule{5-7}
  & \textit{Diff. Aug} & EoD & ADA & cFID $\downarrow$ & CLIP-cFID $\downarrow$ & cKID $\downarrow$ & \\      
  \midrule
    Base & \xmark & \xmark  & \xmark & - & -  & - & - \\ 
    V1 & \cmark & \xmark  & \xmark  & - & -  & - & - \\  
    V2  & \xmark & \cmark & \xmark & - & -  & - & - \\   
    V3 & \xmark & \xmark & \cmark & - & -  & - & - \\   
    V4 & \cmark & \cmark & \xmark & - & - & -  & -  \\   
    V5 & \cmark & \xmark & \cmark & - & - & -  & -  \\   
    V6 & \xmark & \cmark & \cmark & - & - & -  & -  \\   
    \midrule
    \textbf{Ours} & \cmark & \cmark & \cmark & - & - & - & - \\
    \bottomrule
  \end{tabular}
\end{table*}

\subsubsection{Inputs}
\begin{table*}[ht]
  \caption{\textbf{Ablation Study - The effect of Visual Saliency (VS) maps, Scan-Paths and Prompt guidance on the overall quality of UI generations.}}
  \label{tab:ablation_inputs}
  \begin{tabular}{lccccccc}
  \toprule
  \multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf {Upgrades}} & \multicolumn{3}{c}{\textbf {Non-Reference Metrics}}  & \multirow{2}{*}{\textbf {mIoU $\uparrow$}} \\      
  \cmidrule(lr){2-4} 
  \cmidrule{5-7}
  & VS Maps & Scan-Paths & Prompts & cFID $\downarrow$ & CLIP-cFID $\downarrow$ & cKID $\downarrow$ & \\      
  \midrule
    Base & \xmark & \xmark  & \xmark & - & -  & - & - \\ 
    V1 & \cmark & \xmark  & \xmark  & - & -  & - & - \\  
    V2  & \xmark & \cmark & \xmark & - & -  & - & - \\   
    V3 & \xmark & \xmark & \cmark & - & -  & - & - \\   
    V4 & \cmark & \cmark & \xmark & - & - & -  & -  \\   
    V5 & \cmark & \xmark & \cmark & - & - & -  & -  \\   
    V6 & \xmark & \cmark & \cmark & - & - & -  & -  \\   
    \midrule
    \textbf{Ours} & \cmark & \cmark & \cmark & - & - & - & - \\
    \bottomrule
  \end{tabular}
\end{table*}