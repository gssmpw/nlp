\section{Related Work}

This section highlights the challenges faced by GUI generation models, an overview of diffusion models, and the limitations of existing GUI datasets,.

\input{aa02-table_model}

\subsection{GUI Generation}

Exploring GUI alternatives plays an important role in GUI design.   By comparing GUI alternatives explicitly, designers can offer stronger critiques and make more informed decisions~\cite{dow2011prototyping, tohidi2006getting, jiang2022computational, jiang2024computational2, jiang2023future, jiang2024computational}. \autoref{tab:table_model} shows a comparison of representative GUI generation models. 


Generative models should support flexible input to enable effective experimentation. Although design literature often advocates for the use of rough sketches to explore design ideas~\cite{boyarski1994computers, landay1996silk}, sketching alone can be limiting. Designers may face challenges such as fixation, which can constrain the generation of novel solutions~\cite{jansson1991design}. Previous constraint systems have facilitated the exploration of alternatives through the application of constraints~\cite{swearngin2020scout, jiang2019orclayout, jiang2020orcsolver, jiang2020reverseorc, jiang2024flexdoc}. However, these systems typically require manual detailed specifications early in the design process, which can be cumbersome and time-consuming.


It is important to generate diverse realistic GUI ideas aligning with designers' intent. 
GUI retrieval techniques have been employed to obtain exemplar GUIs~\cite{herring2009getting, kumar2013webzeitgeist, li2021screen2vec}. However, the retrieved designs are often limited and may not meet the specific needs of the current design task. Recent advancements in generative models have broadened the scope of exploring design alternatives. However, they still have certain limitations. For instance, some approaches focus on generating GUI layouts but do not present complete GUI representations~\cite{cheng2023play, cheng2024colay}. These methods can make it difficult for designers to visualize the final GUI based solely on layout generation. Other methods have investigated underconstrained scenarios, such as using text prompts to generate GUI code~\cite{wu2024uicoder}. However, text-based prompts often fall short in capturing certain GUI characteristics effectively, which are better conveyed through visual means, and frequently result in simplistic GUIs with minimal elements. Notably, all these models, except CoLay~\cite{cheng2024colay}, require fixed inputs and lack support for flexible multimodal input. Furthermore, none of these approaches consider user attention, an important aspect of effective GUI design.

To address these shortcomings, we propose a diffusion-based generative model for controllable GUI exploration that eliminates the need for manual detailed specifications. Our model enables designers to use flexible multimodal input and generate a wide variety of outputs. It is the first model to integrate visual flow to control GUI generation, supporting any combination of A) prompt, B) wireframe, and C) visual flow.




\subsection{Diffusion Models}

In light of the limitations identified in current GUI generation approaches, recent advancements in diffusion models present a potentially promising way to support flexible inputs and diverse outputs for GUI design.
Recent advancements in diffusion models have dramatically improved image generation, as demonstrated by models like GLIDE~\cite{nichol2021glide}, DALL-E2~\cite{ramesh2022hierarchical}, Imagen~\cite{saharia2022photorealistic}, and Latent Diffusion Models~\cite{rombach2022high}. These diffusion-based approaches offer stable training and handle multi-modal conditional generation without the need for modality-specific objectives.
In particular, Stable Diffusion~\cite{rombach2022high}, a specific implementation of Latent Diffusion Models,  is designed to efficiently generate high-quality images using a modified version of diffusion techniques in the latent space. It expands the range of conditional inputs in addition to text prompts, incorporating conditions like bounding boxes.
Techniques such as classifier guidance~\cite{dhariwal2021diffusion} and classifier-free guidance~\cite{ho2022classifier} have further boosted generation quality, enabling the development of effective conditional diffusion models. 
In our work, we employ Stable Diffusion with classifier-free guidance to generate controllable conditions, as it enhances output quality without the need for external classifiers, aligning generated results with input specifications.

\paragraph{Adapters in Diffusion Models}  

To enhance controllability in the diffusion model, we use adapter techniques.
Adapters~\cite{adapters_in_nlp} are small neural networks or modules designed to modify, specialize, or extend pre-existing models without requiring retraining of the entire system. These methods have recently been used as modular mechanisms in diffusion models, offering control and guidance during the image generation process.
%
By attaching lightweight adapters to the diffusion model, specific domain knowledge can be integrated without the need for retraining the whole network. For instance, ControlNet~\cite{controlnet} employs the Side-Tuning~\cite{side_tune} approach at each block, incorporating zero-convolutions to add conditional information via a trainable copy of the Stable Diffusion encoder.
%
Similarly, IP-Adapter~\cite{ye2023ip} creates trainable copies of the cross-attention layers of Stable Diffusion and introduces condition-specific encoders to handle new conditions. T2I-Adapter~\cite{mou2024t2i} adopts a side-convolutional network for layer-by-layer output blending, following the same Side-Tuning principle.
%
In this work, we propose integrating different adapters to respectively control local properties, such as the positioning and type of GUI elements, and global properties, like the overall visual flow direction.







% DDPM~\cite{ddpm} and 2015 Sol Dickstein reference ...
% Minimize VLB,
% noise equation here

% Fast samplers: DDIM, PNDM, DPM-Solver
% Classifier free Guidance

% T2I models $\rightarrow$ GLIDE, Imagen, Make-a-Scene, Cogview, SD



%\subsection{Automatic GUI Synthesis}
%Image-to-image approaches ... GUIGAN (patch-mixing)
   
%Multimodal approaches: Galileo.AI, GPT-4V, ...


% Maybe this is background?

%\subsection{Vision Language Models}
%Llava-NeXT~\cite{llavanext} over Llava~\cite{llava}.    
%BLIP~\cite{blip} and BLIP-2~\cite{blip2} over GPT-4.

% % TODO:
% \subsection{Text-Diffusers}
% Imagen inspired by~\cite{DeepFloyd}, 
% Glyph-Byt5~\cite{glyph_byt5, glyph_byt5_v2}, 
% Text-diffuser~\cite{textdiffuser}, and
% Text-Diffuser-2~\cite{chen2023textdiffuser}



% Consequently, CfG simplifies the optimization process and enhances stability by avoiding the intricate balancing of multiple loss components, a common issue in GAN frameworks. 
% By deriving and applying these principles, we can propose a unified approach to achieve more reliable and accurate conditional generation.

% \noindent Sketch2Code attempts simplistic GUI generation through HTML code prediction from a hand-drawn sketch using deep learning (optical character recognition and feature recognition). 
% However, Sketch2Code relies entirely on the creativity and legibility of the designer, is restricted to only HTML UIs with potential bugs in code, and does not offer an avenue for quick iterations, which is imperative for designer creativity and GUI ideation. 
% Additionally, it requires an HTML renderer to view the GUI. 
% Other GUI code generation methods like Design2Code and Pix2Code require the GUI to exist already. 
% These methods are also prone to introducing incorrect code as they use largely pre-trained VLM backbones.  
% A shift from this code generation from a sparse modality to GUI paradigm was observed with the rapid rise of text-to-image (T2I) models. 
% Proprietary methods like Galileo.ai use a prompt to generate the entire GUI, offering little control over complex semantics that are easily described visually (for example with a semantic segmentation map as shown in Figure X; also change prompt based on Figure 1's map) as compared to language descriptions. 
% For example - Input prompt: "Overlay an advertisement with 3 buttons: close, go to an external website and share. Then place the goto external website button to the right of the share button, both on the bottom right. Leave approximately 5 pixels from both sides. The close button should be top-right, leaving no margins. All buttons should have appropriate texts overlayed on them. The advertisement should be rectangular on all devices and if I was designing a mobile UI, it should be a banner on the bottom of the screen. Again leave no margins." is extremely cumbersome to explain and requires the user to transfer a GUI from their imagination to language.
% Also, it is an impractical input from the model's perspective as well due to the limited size of the context window.
% Note that all the methods listed above just focus on GUI generation through different input modalities but incorporate no GUI design principles during the generation that eventually affect user behavior. 


% \noindent We use the large (2.3 billion+) pre-trained prior of Stable Diffusion and guide its latent space with two easy and intuitive, from the designer's perspective, input modalities: a segmentation map (drawing/coloring on a canvas with a pre-determined/mapped palette) and visual flow information (5 to 7 clicks/marks on the same canvas).   
% In addition, we address an open issue with current image generation pipelines: drawing coherent and legible text.
% GUIs are not complete without appropriately styled text elements splayed across. 
% This is essential for GUIs to look realistic. 
% We tackle this critical issue and incorporate automatic and coherent 'text drawing' within our generation pipeline using an iterative text-drawing and harmonization pipeline guided by a large Vision-Language Model (VLM).

% \noindent Training our method required quadruples of a) ground truth GUI, b) corresponding prompt/description of the styles, themes, etc. c) corresponding segmentation maps, and d) visual flow information.
% Note that only input modality c) is a strict requirement during inference, all others (b and d) are optional. 
% Obtaining the quadruple is challenging as segmentation is an open research problem, especially in the GUI domain with complex semantics requiring segmentation models to understand different UI elements, images as well as text/language. 
% % Say something about the GUI datasets as well.
% To that end, we manually cleaned X semantically annotated datasets and \textit{mixed} (cite midas) them for training a more general and improved segmentation method. 
% This contributed model allowed us to augment UEyes, a GUI dataset with ground truth visual saliency and scanpaths, with corresponding segmentation maps completing the training quadruple.

% \noindent To that end, we propose the first segmentation map-controlled (and optionally prompts for global styles, themes, etc.) diffusion-based GUI generation method which also incorporates a critical UI designing paradigm in the pipeline: \textit{visual flow}. 
% This UI design principle allows the designer to pre-empt user engagement through order-of-looking through the UI and eventually reach the UI's intended goal. 
% For example - creating a market funnel with high digital sales or high email subscription conversions from an initial pitch. 

% \noindent We use the large (2.3 billion+) pre-trained prior of Stable Diffusion and guide its latent space with two easy and intuitive, from the designer's perspective, input modalities: a segmentation map (drawing/coloring on a canvas with a pre-determined/mapped palette) and visual flow information (5 to 7 clicks/marks on the same canvas).   
% In addition, we address an open issue with current image generation pipelines: drawing coherent and legible text.
% GUIs are not complete without appropriately styled text elements splayed across. 
% This is essential for GUIs to look realistic. 
% We tackle this critical issue and incorporate automatic and coherent 'text drawing' within our generation pipeline using an iterative text-drawing and harmonization pipeline guided by a large Vision-Language Model (VLM).

% \noindent Training our method required quadruples of a) ground truth GUI, b) corresponding prompt/description of the styles, themes, etc. c) corresponding segmentation maps, and d) visual flow information.
% Note that only input modality c) is a strict requirement during inference, all others (b and d) are optional. 
% Obtaining the quadruple is challenging as segmentation is an open research problem, especially in the GUI domain with complex semantics requiring segmentation models to understand different UI elements, images as well as text/language. 
% % Say something about the GUI datasets as well.
% To that end, we manually cleaned X semantically annotated datasets and \textit{mixed} (cite midas) them for training a more general and improved segmentation method. 
% This contributed model allowed us to augment UEyes, a GUI dataset with ground truth visual saliency and scanpaths, with corresponding segmentation maps completing the training quadruple.

% \noindent Similar to DiffSeg, we investigate the attention maps in the residual-attention block-based encoder of our tuned Stable Diffusion (operating with only prompts as input, not input segmentation) and reduce the KL-divergence among them to obtain an unlabeled GUI segmentation map. 
% This makes our model a GUI \textit{generation and design} pipeline alongside a free segmentation method, named free-SeGUI. 
% We prepend \textit{free} since the generative pipeline was never optimized for the segmentation task and incurred no cost during training. \\    


% This section focuses on the limitations of preexisting representations of GUIs, the GUI-related applications of graph neural networks, and constraint-based approaches to layout generation.


% \subsection{Graph Neural Networks on GUIs}

% Graph neural networks~\cite{gori2005new, scarselli2009graph, garg20GNNs, xu2018how} are state-of-the-art models for encoding graph-structured data. Whereas CNNs rely on convolution over spatial neighborhoods and enjoy widespread application to encode GUI images, GNNs aggregate information from neighborhoods defined by an input graph that are not restricted to the spatial domain. This gives them the potential to exploit information about the GUIs beyond pixel level. 
% \add{Li et al. applied GNNs to denoise an existing user-interface dataset~\cite{li2022learning}, and performed GUI autocompletion from the GUI layout hierarchy but failed to generate visually realistic GUI results~\cite{li2020auto}. Br{\"u}ckner et al.~\cite{bruckner2022learning} looked into constructing a graph using GUI elements' relative positioning to predict elements; however, it proved challenging to learn the layout structure from only relative positions.
% HAMP~\cite{ang2022learning}, introduced a graph representation with nodes for app descriptions, GUI screens, GUI classes, and element images to perform GUI tasks. Still, such detailed metadata cannot be extracted from GUI screenshots without extensive manual annotations.
% In contrast, our application of GNNs is geared toward modeling the layout graph of GUI elements, thereby enabling us to capture both the topological intricacies of the GUI layout and the properties of individual GUI elements.}


% \subsection{Constraint-based Layout Generation}
 
% Constraint-based layout models have been widely used in GUI layouts~\cite{sahami2013insights, zeidler2017automatic, marcotte2011responsive, badros2001cassowary, bill1992bricklayer, borning1997solving, hosobe2000scalable, lutteroth2008domain, sadun2013ios, weber2010reduction, zeidler2017tiling, karsenty1993inferring, scoditti2009new, zeidler2012auckland, borning1968constraint, szekely1988user} and document layouts~\cite{hurst2003cobweb, hosobe2005solving, borning2000constraint, laine2021responsive}. \add{Early methods like Peridot~\cite{myers1986creating, myers1990creating} and Lapidary~\cite{zanden1991lapidary} proposed programming by demonstration, automatically generate constraints for user interfaces based on designer interactions.} These models offer greater flexibility for layout generation than simple layout models such as group, grid, table, and grid-bag layouts~\cite{myers2000past, myers1995user, myers97theamulet}. 
% Prior work proposed constraint-based layout generation~\cite{weld2003automatically, fogarty2003gadget}.
% For instance, SUPPLE~\cite{Gajos2008decision, gajos2004supple, gajos2008improving} presented constraints for alternative widgets and groupings, and
% ORCLayout~\cite{jiang2019orclayout, jiang2020orcsolver, jiang2020reverseorc} introduced OR-constraint as a mixture of hard and soft constraints to unify flow-based and constraint-based layouts. 
% \add{Constraints have functioned also to enable layout personalization~\cite{Gajos2005preference}, maintaining consistency~\cite{gajos2005cross}, giving layout-alternative suggestions based on user-defined constraints~\cite{swearngin2020scout, bielik2018robust}, generating layout alternatives from templates or modifiable suggestions~\cite{Jacobs2003document, sinha2015responsive, zanden1990automatic}, and allowing both author and viewer to specify the layout~\cite{borning2000constraint}.
% Finally, recent work has explored applying deep-learning approaches to automatic layout generation, eliminating the need for manually defined constraints~\cite{zheng2019content, lee2019neural}.}
% However, none of these methods predict constraints for GUI elements. 
% Incorporating GUI element relationships as constraints enables our model to predict them within the network. This enhances the network's ability to establish connections and deepen its comprehension of both element properties and constraints.



%Much of the design literature encourages the use of rough wireframes to explore design ideas~\cite{boyarski1994computers, landay1996silk, todi2016sketchplore}. While sketching aids in generating rough concepts, their ability to envision novel solutions is often constrained and can be challenging for designers to avoid fixation and think of entirely new ideas~\cite{jansson1991design}. GUI retrieval techniques assist in obtaining exemplar GUIs~\cite{herring2009getting, kumar2013webzeitgeist, li2021screen2vec}. Designers frequently explore alternatives by looking for examples~\cite{herring2009getting, lee2010designing, chang2012webcrystal, hartmann2007programming, kumar2011bricolage, swearngin2018rewire}. However, \emph{the retrieved designs from the database are limited} and may not always align with the specific goals of the current GUI design. 

%Constraint-based layout models have also been extensively used for GUI generation~\cite{sahami2013insights, zeidler2017automatic, marcotte2011responsive, bill1992bricklayer, hosobe2000scalable, lutteroth2008domain, sadun2013ios, weber2010reduction, zeidler2017tiling, scoditti2009new, zeidler2012auckland, borning1968constraint, hurst2003cobweb, borning2000constraint}. Early systems like Peridot~\cite{myers1986creating, myers1990creating} and Lapidary~\cite{zanden1991lapidary} introduced programming by demonstration, generating constraints for user interfaces based on designer interactions. These models provide greater flexibility for layout generation than simpler models like grid or table layouts~\cite{myers2000past, myers1995user, myers97theamulet}.
%SUPPLE~\cite{Gajos2008decision, gajos2004supple, gajos2008improving} introduced constraints for alternative widgets and groupings, while ORCLayout~\cite{jiang2019orclayout, jiang2020orcsolver, jiang2020reverseorc} unified flow-based and constraint-based layouts by combining hard and soft constraints. However, constraint-based methods often generate only one sample at a time, limiting their utility for exploring multiple GUI alternatives. Scout~\cite{swearngin2020scout}, by contrast, allows designers to explore a variety of alternatives through constraints. Still, such systems often \emph{require detailed specifications} early in the design process, which can be tedious and hinder creativity.


%Recent advances in generative models have significantly expanded methods for exploring design alternatives. Previous work has primarily focused on \emph{generating GUI layouts only without showing complete GUI representation}~\cite{cheng2023play, cheng2024colay}; however, these approaches often fail to provide an intuitive presentation of the final GUI. Designers may struggle to visualize complete GUIs solely from layout generation. Other approaches to GUI generation have explored underconstrained scenarios, such as using text prompts to generate GUI code~\cite{wu2024uicoder}. However, \emph{text-based prompts often fail to capture certain GUI characteristics effectively}, which are better conveyed through visual cues, and frequently produce simplistic GUIs with minimal elements. Moreover, none of these approaches incorporate visual flow, an important factor in effective GUI design. 