
During the early stages of interface design, designers need to produce multiple sketches to explore a design space.  Design tools often fail to support this critical stage, because they insist on specifying more details than necessary. Although recent advances in generative AI have raised hopes of solving this issue, in practice they fail because expressing loose ideas in a prompt is impractical. In this paper, we propose a diffusion-based approach to the low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: A) prompts, B) wireframes, and C) visual flows. The designer can provide any combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification. We present qualitative results for various combinations of input specifications. Additionally, we demonstrate that our model aligns more accurately with these specifications than other models. 

% OLD ABSTRACT
%When sketching Graphical User Interfaces (GUIs), designers need to explore several aspects of visual design simultaneously, such as how to guide the user’s attention to the right aspects of the design while making the intended functionality visible. Although current Large Language Models (LLMs) can generate GUIs, they do not offer the finer level of control necessary for this kind of exploration. To address this, we propose a diffusion-based model with multi-modal conditional generation. In practice, our model optionally takes semantic segmentation, prompt guidance, and flow direction to generate multiple GUIs that are aligned with the input design specifications. It produces multiple examples. We demonstrate that our approach outperforms baseline methods in producing desirable GUIs and meets the desired visual flow.

% Designing visually engaging Graphical User Interfaces (GUIs) is a challenge in HCI research. Effective GUI design must balance visual properties, like color and positioning, with user behaviors to ensure GUIs easy to comprehend and guide attention to critical elements. Modern GUIs, with their complex combinations of text, images, and interactive components, make it difficult to maintain a coherent visual flow during design.
% Although current Large Language Models (LLMs) can generate GUIs, they often lack the fine control necessary for ensuring a coherent visual flow. To address this, we propose a diffusion-based model that effectively handles multi-modal conditional generation. Our model takes semantic segmentation, optional prompt guidance, and ordered viewing elements to generate high-fidelity GUIs that are aligned with the input design specifications.
% We demonstrate that our approach outperforms baseline methods in producing desirable GUIs and meets the desired visual flow. Moreover, a user study involving XX designers indicates that our model enhances the efficiency of the GUI design ideation process and provides designers with greater control compared to existing methods.    



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Writing Clinic Comments:
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % Define: Effective UI design
% % Motivate GANs and write in full form.
% % LLMs vs ControlNet vs GANs
% % Say something about the Figma plugin?
% % Write the work is novel or what has been done before
% % What is desirable UI and how to evalutate that?
% % Visual Flow - main theme (center around it)
% % Re-Title: use word Flow!
% % Use ControlNet++ & SPADE for abstract.
% % Write about input/output. 
% % Why better than previous work?
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% % v2:
% % \noindent \textcolor{red}{\textbf{NEW Abstract!} (Post Writing Clinic 1 - 25-Jun)}

% % \noindent \textcolor{red}{----------------------------------------------------------------------}

% % \noindent Designing user interfaces (UIs) is a time-consuming process, particularly for novice designers. 
% % Creating UI designs that are effective in market funneling or any other designer defined goal requires a good understanding of the visual flow to guide users' attention to UI elements in the desired order. 
% % While current Large Language Models (LLMs) can generate UIs from just prompts, they often lack finer pixel-precise control and fail to consider visual flow. 
% % In this work, we present a UI synthesis method that incorporates visual flow alongside prompts and semantic layouts. 
% % Our efficient approach uses a carefully designed Generative Adversarial Network (GAN) optimized for scenarios with limited data, making it more suitable than diffusion-based and large vision-language models.
% % We demonstrate that our method produces more "desirable" UIs according to the well-known contrast, repetition, alignment, and proximity principles of design. 
% % We further validate our method through comprehensive automatic non-reference, human-preference aligned network scoring and subjective human evaluations.
% % Finally, an evaluation with xx non-expert designers using our contributed Figma plugin shows that <method-name> improves the time-efficiency as well as the overall quality of the UI design development cycle.

% % \noindent \textcolor{red}{----------------------------------------------------------------------}


% \noindent \textcolor{blue}{\textbf{NEW Abstract!} (Pre Writing Clinic 9-July)}

% \noindent \textcolor{blue}{----------------------------------------------------------------------}

% \noindent Exploring different graphical user interface (GUI) design ideas is time-consuming, particularly for novice designers. 
% Given the segmentation masks, design requirement as prompt, and/or preferred visual flow, we aim to facilitate creative exploration for GUI design and generate different UI designs for inspiration.
% While current Vision Language Models (VLMs) can generate GUIs from just prompts, they often lack control over visual concepts and flow that are difficult to convey through language during the generation process. 
% In this work, we present FlowGenUI, a semantic map-guided GUI synthesis method that optionally incorporates visual flow information based on the user's choice alongside language prompts. 
% We demonstrate that our model not only creates more realistic GUIs but also creates "predictable" (how users pay attention to and order of looking at GUI elements) GUIs.
% Our approach uses Stable Diffusion (SD), a large paired image-text pretrained diffusion model with a rich latent space that we steer toward realistic GUIs using a trainable copy of SD's encoder for every condition (segmentation masks, prompts, and visual flow). 
% We further provide a semantic typography feature to create custom text-fonts and styles while also alleviating SD's inherent limitations in drawing coherent, meaningful and correct aspect-ratio text. 
% Finally, a subjective evaluation study of XX non-expert and expert designers demonstrates the efficiency and fidelity of our method.


% This process encourages creativity and prevents designers from falling into habitual patterns.


% ------------------------------------------------------------------
% Joongi Why is it important to create realistic GUI?
% I do not see how the Visual Flow given on the left hand side is reflected in the results on the right hand side. 
% I’d avoid making unsubstantiated claims about designers (falling into habitual patterns).
% The UIs you generate do not “align with users’ attention patterns” but rather try to control it (that’s what visual flow means)
% ------------------------------------------------------------------
% Comments - Writing Clinic - 9th July:
% Improve title. More names: FlowGen
% Figure 1: Use an inference time hand-drawn mask
% Figure 1: Show both workflows. Add a designer --> Input.
% Figure 1: Make them more diverse
% ------------------------------------------------------------------
% Designing graphical user interfaces (GUIs) requires human creativity and time. Designers often fall into habitual patterns, which can limit the exploration of new ideas. 
% To address this, we introduce FlowGenUI, a method that facilitates creative exploration and generates diverse GUI designs for inspiration. By using segmentation masks, design requirements as prompts, and/or selected visual flows, our approach enhances control over the visual concepts and flows during the generation process, which current Vision Language Models (VLMs) often lack.
% FlowGenUI uses Stable Diffusion (SD), a largely pretrained text-to-image diffusion model, and guides it to create realistic GUIs. 
% We achieve this by using a trainable copy of SD's encoder for each condition (segmentation masks, prompts, and visual flow). 
% This method enables the creation of more realistic and predictable GUIs that align with users' attention patterns and their preferred order of viewing elements.
% We also offer a semantic typography feature that creates custom text fonts and styles while addressing SD's limitations in generating coherent, meaningful, and correctly aspect-ratio text.
% Our approach's efficiency and fidelity are evaluated through a subjective user study involving XX designers. 
% The results demonstrate the effectiveness of FlowGenUI in generating high-quality GUI designs that meet user requirements and visual expectations.

% ---------------------------------------


%A critical and general issue remains while using such deep generative priors: creating coherent, meaningful and correct aspect-ratio text. 
%We tackle this issue within our framework and additionally provide a semantic typography feature to create custom text-fonts and styles. 


% %Creating UI designs that are effective in market funneling or any other designer-defined goal requires a good understanding of the visual flow to guide users' attention to UI elements in the desired order. 
% %While current largely pre-trained Vision Language Models (VLMs) can generate GUIs from just prompts, they often lack finer or pixel-precise control which can be crucial for many easy-to-understand visual concepts but difficult to convey through language. 
% % However, obtaining such pixe-level labels is an extremely expensive so we
% % For example - overlaying text on images with certain aspect ratios and two equally separated buttons 
% Additionally, all prior GUI generation work fails to consider visual flow information during the generation process. 
% We demonstrate that visual flow-informed generation not only creates more realistic and human-friendly GUIs but also creates "predictable" (how users pay attention to and order of looking at GUI elements) UIs that could be beneficial for designers for tasks like creating effective market funnels.
% In this work, we present a semantic map-guided GUI synthesis method that optionally incorporates visual flow information based on the user's choice alongside language prompts. 
% Our approach uses Stable Diffusion, a large (billions) paired image-text pretrained diffusion model with a rich latent space that we steer toward realistic GUIs using an ensemble of ControlNets. 
% % TODO: Mention it in 1 sentence:
% A critical and general issue remains while using such deep generative priors: creating coherent, meaningful and correct aspect-ratio text. 
% We tackle this issue within our framework and additionally provide a semantic typography feature to create custom text-fonts and styles. 
% To evaluate our method, we demonstrate that our method produces more "desirable" UIs according to the well-known contrast, repetition, alignment, and proximity principles of design. 
% % We further validate our method through comprehensive automatic non-reference and human-preference aligned scores. (TODO: Maybe Unskip if we get UIClip from Jason!)
% % TODO: Re-word this and only keep ideation cycles and time-efficiency.
% Finally, a subjective evaluation study of XX non-expert and expert designers demonstrates the efficiency and fidelity of our method.
% % improves the time-efficiency by quick iterations of the UI design ideation process.
% %Finally, an evaluation with xx non-expert designers using our contributed <method-name> improves the time-efficiency by quick iterations of the UI design ideation cycle.

%\noindent \textcolor{blue}{----------------------------------------------------------------------}


%In an evaluation with xx designers, we found that GenerativeLayout: 1) enhances designers' exploration by expanding the coverage of the design space, 2) reduces the time required for exploration, and 3) maintains a perceived level of control similar to that of manual exploration.



% Present-day graphical user interfaces (GUIs) exhibit diverse arrangements of text, graphics, and interactive elements such as buttons and menus, but representations of GUIs have not kept up. They do not encapsulate both semantic and visuo-spatial relationships among elements. %\color{red} 
% To seize machine learning's potential for GUIs more efficiently, \papername~ exploits graph neural networks to capture individual elements' properties and their semantic—visuo-spatial constraints in a layout. The learned representation demonstrated its effectiveness in multiple tasks, especially generating designs in a challenging GUI autocompletion task, which involved predicting the positions of remaining unplaced elements in a partially completed GUI. The new model's suggestions showed alignment and visual appeal superior to the baseline method and received higher subjective ratings for preference. 
% Furthermore, we demonstrate the practical benefits and efficiency advantages designers perceive when utilizing our model as an autocompletion plug-in.


% Overall pipeline: Maybe drop semantic typography / visual flow?