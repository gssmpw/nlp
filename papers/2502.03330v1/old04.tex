\section{Method}
\label{sec:synthesis}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/model.png}
    \caption{\textbf{Model Architecture.} Designers can optionally send a loose segmentation map ($c_s$) through zero-convolved encoder (Cycle-ControlNet) features, a prompt ($c_p$) embedded in CLIP's latent space describing the UI style, semantics and themes or visual flow ($c_f$) co-ordinates to guide end-user attention. 
    Additional cross-attention layers (between encoded $c_f$ and internal activations) after our Flow Encoder ($\mathcal{F}$) ensure downstream scanpath adherence.
    These input(s) are together denoised ($z \sim \mathcal{N}(0, 1)$) in the latent space of an AutoEncoder through a U-Net ($U_{LDM}$) and then decoded by the KL-divergence AutoEncoder of Stable Diffusion ($D_{AE-KL}$) to get a realistic GUI.
    This output is further processed to get accurate and coherent text through the AutoText Inpainting module that builds a chain-of-thought (CoT) with the segmentation control as well as the denoised-decoded output as context.
    We used Llava-NeXT~\cite{llavanext} ($\mathcal{VLM}$) to generate the texts and Text-Diffuser-2~\cite{chen2023textdiffuser} ($\mathcal{TD}$) to place the texts.
    }
    \label{fig:model}
\end{figure}

During the early stages of GUI design, designers need to quickly explore the design space. without defining more details than necessary. 
During the early stages, designers produce low-fidelity solutions in an attempt to explore the design space.
Low-fidelity designs often specify, partially or in an underspecified way, key aspects of the layout, widgets, graphics etc.
Recent advances in generative AI based on text prompts fail because expressing loose ideas in a text prompt is hard. 
Our objective is to produce a large number of low-fidelity ideas that are beneficial at the early GUI design stage.
We support creative exploration by allowing them to rapidly explore a high number of solutions, flexibly and with minimum effort.


\subsection{Problem Formulation}

We define the GUI exploration problem as a controllable GUI generation problem, allowing flexible control of the generation process via three types of inputs: 1) wireframes, 2) prompts, and 3) visual flows. The designer can provide \emph{any} combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification.

We define two objective terms, classifier-free guidance loss term, and flow consistency loss term

In this paper, we propose a diffusion-based approach to low-effort generation of interface sketches. 




In this short paper, we propose a diffusion-based approach to controllable generation of interface sketches. 
The approach breaks new ground by allowing flexible control of the generation process via three types of early-stage inputs: 1) wireframes, 2) prompts, and 3) visual flows (controlling where users should look). 
%
While previous work on generative approaches in this space has produced layouts only~\cite{cheng2023play, cheng2024colay}, our approach generates ''full'' low-fidelity prototypes. Although the outputs are images only, and not functional interfaces, they allow designers to envision what the interface might appear like. 
%
Importantly, our approach allows a significant relaxation in the input side. 
The designer can provide \emph{any} combination of the three as input and will get a diverse set of low-fidelity solutions in a gallery in response. 
In practice, hundreds of designs can be explored in the order of minutes.
%
The benefit is that large design spaces can be explored with very little effort in input-specification. 

Our technical contribution is a solution that overcomes two critical shortcomings of existing methods. 
First, generating controllable GUIs poses a challenge, as previous approaches that rely heavily on text-based prompts often fail to capture certain GUI characteristics effectively, which are better conveyed through visual cues.
Second, many current GUI generation methods focus on a narrow range of properties~\cite{cheng2023play, cheng2024colay}, limiting their ability to manage both local properties (e.g., the position and type of GUI elements) and global properties (e.g., visual flow).
%
Our solution is to incorporate different adapters (modular mechanisms designed to provide additional control and guidance during the image generation process) for the controls of local properties, such as the positioning and type of GUI elements, and global properties, such as the overall visual flow direction. 


In this paper, we propose a diffusion-based approach to low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: 1) wireframes, 2) prompts, and 3) visual flows. Importantly, the designer can provide \emph{any} combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification. We show that the algorithm performs favorably against previous approaches to controllable diffusion-based generation of GUIs. 

Our model performs GUI generation by addressing several key challenges:
1) Generating controllable GUIs that respond to both prompts and visual cues~\cite{};
2) Generating GUIs conditioned on both local and global properties;
To address these challenges, we propose a diffusion-based model with two key strategies:
1) We introduce different adapters to control local and global properties, respectively;


\subsubsection{Visual Flow Encoder (Encode Condition: flow)}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conditional GUI Generation}

\noindent \textbf{ControlNet}     
Focus on zero-convolutions and the main equations, cite ControlNet. 
Also, talk about freezing and unfreezing SD's decoder.
Then after writing the cycle-consistency equation, inspired by CycleGAN and as also used in (cite controlNet++) for diffusion models, we achieve strong adherence to segmentation input ...

\noindent \textbf{Flow-Adapter}
Write the softmax addition equation and then say something similar to IP-Adapter.
Novel flow-loss, equation:


\subsection{Losses}

\textbf{Classifier-free Guidance.}
We use classifier-free guidance~\cite{cfg} to train our final architecture. 
Note that particular conditionals are set to $\phi$ if it is not the final ensemble.
\begin{equation}
    \mathcal{L}_{cfg} = \mathbb{E}_{z_t, t, c \sim \mathcal{N}(0,1)} [\| \epsilon - \epsilon_{\theta}(z_t, t, \mathcal{C}) \|^{2}_2]
\end{equation}
where $c \in \mathcal{C}$.
In practice, we achieve this by dropping out all the conditional 50\% of the time.

\textbf{Flow Consistency.}
Flow consistency is a critical loss for our architecture to ensure that the new cross-attention layers between activations and flow embeddings are not washed away considering the model's huge prior, difficult to shift to the desired latent subspace.
We use the differentiable Dynamic Time Warping (DTW~\cite{dtw}) or the soft-DTW~\cite{soft-dtw} loss to ensure this.
Additionally, since the only output of our network is the synthesized GUI ($\hat{x}$) and no ground-truth scan-path ($\hat{c}_f$) exists, we use EyeFormer~\cite{eyeformer} ($\mathcal{G}_\phi$) out-of-the-box to compute a representative ground truth ($\mathcal{G}_\phi(\hat{x}) \sim \hat{c}_f$). 
Overall, the loss is computed as:
\begin{equation}
    \mathcal{L}_{f}(c_f, \mathcal{G}_\phi(\hat{x})) = -\gamma log( k^{\gamma}_{GAK}(c_f, \mathcal{G}_\phi(\hat{x})))
\end{equation}
where $\gamma$ is the softness parameter and reduces the formulation to a hard minimum operator as $\gamma \rightarrow 0^+$. 
The Global Alignment Kernel (GAK~\cite{GAK}) or $k^{\gamma}_{GAK}$ is computed as follows:
\begin{equation}
    k^{\gamma}_{GAK}(c_f, \mathcal{G}_\phi(\hat{x})) = \sum_{\mathcal{A}} e^{-\frac{A^T \|c_f - \mathcal{G}_\phi(\hat{x}) \|_2}{\gamma}}
\end{equation}
where $\mathcal{A}$ is the binary alignment matrix or denotes the valid index pairs of the given sequences. 
We set $\gamma = 0.1$ for all our computations. Finally, we train our flow adapter cross-attention weights ($\eta$) with the diffusion loss, whenever the flow embeddings are not dropped for CfG~\cite{cfg}:

\begin{equation}
    \mathcal{L}_{flow} = \| \epsilon - \epsilon_{\eta}(z_t, t, c_p, c_f) \|_2 + \mathcal{L}_f
\end{equation}


% \textbf{Segmentation Cycle Consistency.} 
% Inspired by~\cite{cycleGAN, controlnet_plus_plus}, we enforce a simple $\mathcal{L}_1$ cyclical consistency over the segmentation adherence using our mixed-dataset pre-trained segmentation network ($\mathcal{S}_\psi$) as follows: 
% \begin{equation}
%     \mathcal{L}_{cycle-seg} =  \| \epsilon - \epsilon_{\zeta}(z_t, t, c_p, c_s) \|_2 + \frac{1}{NM} \sum^{N, M}_{i,j} \| c_{s, i, j} - \mathcal{S}_{\psi}(\hat{x})_{i, j} \|_1
% \end{equation}
% where $N, M$ represents the height and width of the segmentation mask $c_s$.

\textbf{Classifier-free Guidance.}
We use classifier-free guidance~\cite{cfg} to train our final architecture. 
Note that particular conditionals are set to $\phi$ if it is not the final ensemble.
\begin{equation}
    \mathcal{L}_{cfg} = \mathbb{E}_{z_t, t, c_p, c_s, c_f \sim \mathcal{N}(0,1)} [\| \epsilon - \epsilon_{\theta}(z_t, t, c_p, c_s, c_f) \|^{2}_2]
\end{equation}
where $\theta =\zeta + \eta$.
In practice, we achieve this by dropping out all the conditional 50\% of the time.

\textbf{Overall Loss.}  
Finally, to align the flow adapter and the cycle-consistent GUI-segmentation ControlNet, we train the ensemble with the following loss: 
\begin{equation}
    \mathcal{L}_{total} = \lambda_1 \mathcal{L}_{cfg} + \lambda_2 \mathcal{L}_{f}
\end{equation}


\subsection{Implementation Details}
- Segmentation model stuff:
- Flow AE stuff:
- Main:
$\lambda_1, \lambda_2, \lambda_3$ are set to ...
CfG-scale is set to...
Batch-size, iterations, resolution ... 
Llava-NeXT for prompts, 
DDIM Sampling~\cite{}, Optimizer~\cite{} PyTorch~\cite{}



\noindent \textbf{Segmentation Method}


\noindent \textbf{Flow AutoEncoder}


\noindent \textbf{Conditional GUI-Diffuser}



% DOES NOT MAKE SENSE ANYMORE. MAYBE WRITE THIS AS A DISCUSSION/LIMITATION/ADVANTAGE OF GANs over DIFFUSION
% \subsection{Low Data Regime Constraint}
% It is widely known that most learning-based generative methods are data-hungry.
% In the UI landscape, acquiring UI snapshots might be a trivial crawling automation but acquiring corresponding semantic segmentation maps is extremely challenging and expensive.
% Manually segmenting high-resolution UI datasets like UEyes~\cite{ueyes} with 1980 total UIs into 12 segmentation classes with complex overlays, layouts and diverse designs is not feasible.  
% Even automatic deep-learning-based methods that are largely-pretrained and open-vocabulary do not generalize well in zero-shot operating conditions (refer to figure with methods; slide 48). 
% Owing to this substantially low data regime constraint, we turn to techniques in machine learning to operate effectively.

% \subsection{Regularization and Data Augmentation}
% Spectral Normalization and other techniques like weight decay are popular regularization techniques~\cite{spectral_styleDCGAN} (also cite BigGAN, weight-decay-stuff), still prove to be largely ineffective the Discriminator severely overfits~\cite{diffaugment}.

% \noindent Data Augmentation by randomly applying semantics-preserving transformations to generate multiple realistic training samples has been a popular and widely studied approach (cite papers, MixUp, CutOut, CutMix, data-aug-papers).
% Automatic Augmentation (cite auto-Aug) has also gained traction recently.

% \noindent However, GANs present a significantly different training procedure/setup and goal of approximating an underlying distribution implicitly. 
% Direct data augmentation for GAN-based methods is ineffective for low training data regimes (ref-50 from Diff Aug) as it a) irrecoverably (non-invertible augmentations with equal probabilities) shifts the original data distribution irreparably to one that might was not intended to be learnt and b) class imbalances can be amplified for conditional-GAN setups leading to generator mode-collapse towards the most dominant conditions.

% \noindent Contrarily, with differentiable augmentations ($\mathcal{T}(x)$), convergence towards nash equilibrium is a lot slower which means it ensures diversity and consequently avoids mode collapse in low-data regimes. 
% This $\mathcal{T}(x)$ is a step towards making the Discriminator weaker. 

% \noindent A concurrent approach from StyleGAN2-ADA~\cite{styleGAN2-ADA} proposes integral controlled probabilistic augmentation to keep the discriminator at a target accuracy. 
% However, this approach still runs the risk of shifting the distribution if not carefully tuned and adds the burden on the generator to infer this underlying augmentation probability distribution.
% So, we chose differentiable augmentations for our work.

% % (Experiment that backs this up: Latent space interpolation with the same? Use the same condition; showing large diversity is probably the best and easiest.)


% \subsection{Off-the-Shelf Ensembled Discriminators}
% Off-the-shelf ensembled discriminators will make it stronger but it comes with the added benefit of forcing large priors! 