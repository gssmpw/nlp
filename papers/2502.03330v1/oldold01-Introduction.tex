\section{Introduction}

During the early stages of Graphical User Interface (GUI) design, many designers prioritize exploring initial design alternatives rather than focusing on detailed refinement. Their primary objective is to define the overall layout and structure of components, without being constrained by the need to specify intricate details. As a result, designers often prefer using low-fidelity techniques~\cite{rettig1994prototyping} to quickly generate and evaluate design ideas. These techniques allow for faster GUI design exploration before transitioning to more advanced design tools or passing the design to a programmer.

Additionally, improving visual flow can enhance user engagement and guide user behavior~\cite{Rosenholtz11, Still10, ueyes}. Modern GUIs, which combine complex arrangements of text, images, and interactive elements like buttons and menus, present challenges in maintaining effective visual flow during design. Typically, understanding how users engage with a design requires post-deployment analysis or the input of experienced designers. However, it would be valuable to consider the direction of visual flow in the early stages of GUI design.

Exploring GUI alternatives plays an important role in GUI design. By comparing GUI alternatives explicitly, designers can offer stronger critiques and make more informed decisions~\cite{dow2011prototyping, tohidi2006getting}. Much of the design literature encourages the use of rough sketches to explore design ideas~\cite{boyarski1994computers, landay1996silk}. While sketching aids in generating rough concepts, their ability to envision novel solutions is often constrained and can be challenging for designers to avoid fixation and think of entirely new ideas~\cite{jansson1991design}. GUI retrieval techniques assist in obtaining exemplar GUIs~\cite{herring2009getting, kumar2013webzeitgeist, li2021screen2vec}, but the retrieved designs may not always align with the specific goals of the current project. Although prior constraint systems allow designers to explore alternatives through the use of constraints~\cite{swearngin2020scout}, these systems often require more detailed specifications than designers prefer at early stages. Such over-specification can become tedious and may hinder spontaneity.


Recent advances in generative models have significantly expanded methods for exploring design alternatives. Previous work has primarily focused on generating GUI layouts~\cite{cheng2023play, cheng2024colay}; however, these approaches often fail to provide an intuitive presentation of the final GUI. Designers may struggle to visualize complete GUIs solely from layout generation. Other approaches to GUI generation have explored underconstrained scenarios, such as using text prompts to generate GUI code~\cite{wu2024uicoder}, but these methods are often too broad and may not meet designers' expectations, frequently producing simplistic GUIs with minimal elements. Moreover, none of these approaches incorporate user attention, an important factor in effective GUI design. 

To address this gap, we introduce a diffusion-based model for controllable GUI exploration, which enables rapid and flexible design iterations. This generative model helps designers explore diverse GUI alternatives that align with both their expectations and user needs at the early stages of design. 
The model generates alternative GUI designs, optionally based on the segmentation of interface elements and prompts, while also optimizing the visual flow to guide user attention when required.

Our model is designed to overcome two critical shortcomings of existing methods. 
First, generating controllable GUIs poses a challenge, as previous approaches that rely heavily on text-based prompts often fail to capture certain GUI characteristics effectively, which are better conveyed through visual cues.
Second, many current GUI generation methods focus on a narrow range of properties~\cite{cheng2023play, cheng2024colay}, limiting their ability to manage both local properties (e.g., the position and type of GUI elements) and global properties (e.g., visual flow).
%
To evaluate our approach, we qualitatively illustrate how our model can quickly explore a wide range of GUI alternatives that closely align with the specified input conditions. Additionally, we demonstrate that our model outperforms previous generative models through user comparison studies. We will make our code and the dataset used for training available upon acceptance.

\noindent In summary, this paper makes the following contributions:
\begin{enumerate}
\item We introduce a novel diffusion-based model for GUI generation that integrates design control through prompts, segmentations, and visual flow direction. We propose to incorporate different adapters (modular mechanisms designed to provide additional control and guidance during the image generation process) for the controls of local properties, such as the positioning and type of GUI elements, and global properties, such as the overall visual flow direction. 
\item We generate a dataset of mixed mobile UIs and webpages, including GUI screenshots, segmentation maps with GUI element labels, descriptions, and scanpaths, which can be used to train our GUI-specific diffusion model.
\item We validate our method by qualitatively demonstrating its capacity to generate diverse GUI alternatives and by conducting user comparison studies.
\end{enumerate}


%\item We address and automate coherent 'text drawing' and harmonization within our generation pipeline to produce realistic GUIs.

%Based on the prompt and generated UI, we further propose a text generation process that produces meaningful textual content.