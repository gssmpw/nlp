\section{Introduction}

% Questions due to which this project started:
% 1. How can I make a GUI that is an effective market funnel (business model) as well as an attractive & easy-to-use (not visually cluttered) interface?
% 2. How do expert designers improve their front-end designs based on visual saliency and scan-path analytics? -> Restricted knowledge and experience of expert UI/UX designers.
% 3. Can this expert knowledge be used as a prior condition during the design process instead of post-deployment analysis and then re-optimization?
% How can novices and even non-designers benefit from this product? A step towards more sophisticated automatic front-end engineering.


%Exploring alternative graphical user interfaces (GUIs) can significantly enhance designer ideation by fostering creativity and preventing reliance on habitual patterns. 
%This approach enables the generation of novel design ideas that might not have been initially considered. 
%However, the current layout exploration process is predominantly manual, restricting the number of alternatives designers can effectively explore.

Designing Graphical User Interfaces (GUIs) requires consideration of GUI properties, such as element color and position, as well as user behaviors, such as how users interact with and visually engage with the GUIs.
A key objective in GUI design is to direct usersâ€™ attention towards discovering relevant information and interaction possibilities.
Improved visual flow can enhance user engagement and guide user behavior~\cite{Rosenholtz11, Still10, ueyes}. 
Modern GUIs, with their intricate arrangements of text, images, and numerous interactive elements such as buttons and menus, present challenges in considering visual flow during the design phase.
Understanding user engagement often requires post-deployment analysis or expertise from experienced designers. 
Can such expert knowledge be captured as a prior and inputted as a condition during the design process instead?


Prior work has primarily focused on generating GUI layouts~\cite{cheng2023play, cheng2024colay}. 
However, such layout generation does not provide a complete view of the final GUI. 
Generating the final GUI offers comprehensive information on layout, graphics, and GUI elements. 
Such prior research on GUI generation has predominantly addressed underconstrained conditions, such as using prompts to create GUIs~\cite{}. 
These methods are often too general and may not align with designers' expectations, frequently resulting in simple GUIs with few elements~\cite{} or relying on GUI retrieval techniques to obtain exemplar GUIs~\cite{}. 
Furthermore, none of these approaches consider user attention, which is crucial for effective GUI design. 
Integrating user perspective into the design process is essential for optimizing the overall user experience.


To address this gap, we present \systemname, a diffusion-based generative GUI model designed to produce diverse, high-quality GUIs that align with both designers' expectations and users' perspectives. 
It meets designers' needs by accommodating flexible segmentation of GUI elements and optional prompts for global styles and themes, while also addressing users' needs by optimizing visual flow to guide visual attention.
We frame GUI generation as an image generation problem with conditional controls. 
Utilizing largely pretrained text-to-image diffusion models as the backbone architecture, we add conditional control for UI element segmentation and visual flow. 

This approach is designed to overcome two critical shortcomings of existing methods. 
First, generating controllable GUIs is challenging because previous approaches that rely on prompts often fail to capture certain aspects of GUIs effectively, which can be better conveyed through visual cues~\cite{}.
Second, many existing GUI generation methods concentrate on only a limited set of properties~\cite{}, making it difficult to address both local properties (such as the positions and types of GUI elements) and global properties (such as visual flow).
%Finally, generic image generation approaches struggle to produce realistic GUIs due to their lack of domain-specific knowledge about GUI design~\cite{}.
% Add why diffusion models make sense for this task here. (3 points mentioned in teams)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
Recent advances in generative models have highlighted the challenges associated with training frameworks like GANs, particularly when dealing with training stability and the lack of largely pre-trained priors. 
GANs are inherently unstable and have the high tendency of a generator to mode collapse requiring careful hyperparameter tuning and optimization~\cite{GAN_hard01, GAN_hard02, GAN_hard03, diffusion_beat_gans}. 
Incorporating multimodal data further exacerbates this as each modality necessitates an objective alongside a specialized discriminator. For example, GigaGAN~\cite{giga_gan} uses a CLIP~\cite{clip} objective explicitly in addition to the CLIP-Text Encoder. 
% For example, when the generated data point aligns with the real data distribution but contradicts the conditional input (e.g., text), the adversarial loss may incorrectly assign a low error, leading to flawed feedback and compromised model performance.

In contrast, recent diffusion-based approaches train stably and handle multi-modal conditional generation effectively without the need for modality-specialized objectives.  
% CfG benefits simultaneously facilitating conditional and unconditional model training without external discriminators/classifiers and specialized loss functions.
Classifier-free-Guidance~\cite{cfg} enforces conditional adherence, effectively integrating multiple modalities without the need for explicitly defined discriminators or loss functions per condition. 
% However, diffusion models still face challenges in generating accurate text output due to their limited understanding of natural language~\cite{}. 
%This highlights a significant issue in the development of generative vision models, as they often lack comprehensive natural language understanding. 
Thus, the current models cannot generate meaningful texts.

% Consequently, CfG simplifies the optimization process and enhances stability by avoiding the intricate balancing of multiple loss components, a common issue in GAN frameworks. 
% By deriving and applying these principles, we can propose a unified approach to achieve more reliable and accurate conditional generation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\systemname is the first model capable of generating complete GUIs with precise control over prompts, segmentation, and visual flow. Our model architecture uniquely addresses both local properties, like element positions and types, and global properties, such as visual flow. Based on the prompt and generated UI, we further propose a text generation process that produces meaningful textual content. To demonstrate the effectiveness of our approach, we show that the generated GUIs closely align with the specified input conditions. Moreover, \systemname outperforms previous generative models across most metrics and excels in user comparison studies.
We will make our code available upon acceptance.

\noindent In summary, this paper makes the following contributions:
\begin{enumerate}
\item We introduce the first diffusion-based GUI generation method that incorporates GUI design control through prompts, segmentations, and \textit{visual flow}. Our model adheres to these conditionals through our proposed task-specific objectives. 
\item We address and automate coherent 'text drawing' and harmonization within our generation pipeline to produce realistic GUIs.
% \item We propose two cycle-consistency losses to ensure adherence to segmentation maps and \textit{visual flow}.
\item We validate our method by examining its alignment with input conditions and performing quantitative evaluations using GUI-specific metrics, in addition to conducting user comparison studies.
%\item We contribute a mixed-GUI-dataset trained segmentation method and a non-task optimized diffusion-based segmentation method: free-SeGUI.
\end{enumerate}



% Modern graphical user interfaces (GUIs) are replete with diverse elements like text, graphics, buttons, checkboxes, sliders, and icons, arranged in various ways.
% %
% GUIs deploy visual, spatial, and textual cues to guide users in their design.
% %
% For example, colors convey grouping and attention, while visual cues like proximity or shared visual areas signal element associations~\cite{Brumby2015VisualGI}.
% Elements are ordered and grouped based on grid lines; for instance, lists are often left-aligned \cite{miniukovich2015computation}.
% %
% In addition, textual elements, such as headers, labels, and annotations, are needed to communicate the ``semantics'' of the various shapes and images.
% %
% Despite architectural commonalities, each GUI genre and application has its unique conventions.
% %
% The question of how to represent a GUI's visual--spatial--semantic structure such that it could be effectively conveyed in computational design remains open~\cite{jiang2022computational, jiang2023future, jiang2024computational, jiang2024computational2}.

% Prior methods for representing GUIs and their constituent elements fall short of capturing these integrative aspects. 
% Some work has focused exclusively on textual content in GUIs, but neglected the visual aspects of design and the variety of GUI elements~\cite{li2017sugilite, li2020multi}.
% In contrast, other approaches emphasize visual appearance and GUI element types but overlook the content of the elements~\cite{deka2016erica, learning2022ang, learning2018liu}. This results in similar treatment for GUIs sharing structural and visual similarities but differing in content. 
% \add{Layout constraints represent the layout relationships between GUI elements, such as alignment, same-size, and grouping. Most existing methods, employing Convolutional Neural Networks (CNNs) to learn GUI images, face challenges because they have to learn layout constraints from pixels. This makes training a model to predict constraints a challenge.}

% To address this gap, we propose a novel graph-based GUI representation \papername~ that integrates GUI elements with layout constraints (Figure~\ref{fig:teaser}a). We formulate a bipartite graph to express GUI elements and their relationships via two kinds of nodes: element and constraint nodes. \add{As shown in~\autoref{fig:element_and_constraints}, each GUI element node represents element properties, including visual appearance, textual content, element type, position, and size. Constraint nodes include four types of constraints: alignment, same-size, element grouping, and multimodal grouping constraints.}
% We then employ Graph neural networks (GNNs) to learn a domain-specific representation from the graph-structured data. 

% \add{
% Compared to other GUI representations~\cite{li2017sugilite, li2020multi, deka2016erica, learning2022ang, learning2018liu, dayama2020grids, screen2vec2021li}, the design of our GNN aims to balance two representation learning goals.
% On the one hand, we want to maximally exploit domain knowledge, particularly using stable, universal GUI design characteristics without learning from scratch. On the other, we want to capture contingent design tendencies -- such as color palettes and fonts -- without manual specification. If successful, this would make it possible to learn a useful representation with fewer samples.
% %
% To this end, our approach is to represent relatively universal principles of layouts as constraints in a graph, thus reducing the need to learn them from scratch. At the same time, genre-specific tendencies are learned by applying GNNs to capture the design features unique to GUIs. In contrast, traditional structured representations in computational design, such as DOM trees, are not designed for learning but for specifying GUIs. While they represent the view hierarchy of a layout, they do not lend themselves to many machine learning methods. For example, there is no natural way to represent the concept of grid alignment with DOMs, as this information is split into the leaf nodes of the tree. To compute whether two elements are aligned, the whole tree needs to be parsed. In contrast, our method directly embeds connections between GUI elements and their constraints in the graph.
% }


% \begin{ADD}
% \def\arraystretch{1}% 
% \begin{table*}[t!]
% % \marginparsep=30pt
% % \marginnote{\color{violet}
% % 1AC, 2AC, R2, R3: Added Table 1 to clarify that our method consider more aspects of the GUI compared to other methods.\\~\\}
% \scalebox{1}{\setlength\tabcolsep{3pt}
% \begin{tabular}{lcccccc}
% \hline
% \add{\textbf{Approach}} &  \add{\textbf{Textual Content}} & \add{\textbf{Visual Appearance}} & \add{\textbf{Element Type}} & \add{\textbf{View Hierarchy}}  & \add{\textbf{Layout Constraints}}   \\
% \midrule
% \add{SUGILITE~\cite{li2017sugilite}} & \cmark & \xmark & \xmark & \cmark  & \xmark   \\
% \add{SOVITE~\cite{li2020multi}} & \cmark   & \xmark & \xmark & \cmark & \xmark  \\
% \add{ERICA~\cite{deka2016erica}} & \xmark & \cmark  & \cmark   & \cmark  & \xmark  \\
% \add{HAMP~\cite{learning2022ang}} & \xmark & \cmark  & \cmark  & \xmark & \xmark  \\
% \add{Liu et al.~\cite{deka2016erica}} & \xmark & \cmark   & \cmark  & \cmark  & \xmark \\
% \add{Screen2Vec~\cite{screen2vec2021li}} & \cmark  & \xmark & \cmark  & \cmark  & \xmark \\
% \add{Li et al.~\cite{li2020auto}} & \xmark & \xmark & \cmark  & \cmark   & \xmark \\
% \add{Br{\"u}ckner et al.~\cite{bruckner2022learning}}  & \xmark & \xmark & \cmark  & \xmark & \xmark  \\
% \add{GRIDS~\cite{dayama2020grids}}  & \xmark & \xmark & \cmark  & \xmark & \cmark   \\
% \midrule
% \add{Ours}  & \cmark  & \cmark  & \cmark  & \xmark & \cmark  \\
% \bottomrule
% \end{tabular}
% }
% \caption{\add{A comparison of existing approaches, marked based on if they \textit{explicitly represent}: textual content, visual appearance, GUI element type, view hierarchy, and layout constraints. The view hierarchy, akin to a DOM tree, begins with a root view and organizes all its descendants in a tree structure. Layout constraints denote relationships like alignment and grouping among GUI elements. ``\cmark\color{black}'' indicates that the model can capture the factor, while ``\xmark\color{black}'' indicates that it does not.}}
% \Description{This table shows whether existing approaches explicitly represent textual content, visual appearance, GUI element type, view hierarchy, and layout constraints.}
% \label{tbl:related_work_comparison}
% \end{table*}
% \end{ADD}


% \add{To examine the effectiveness of our graph-based representation, we applied it to different applications: GUI autocompletion, GUI topic classification, and GUI retrieval.
% Our primary emphasis lies in autocompletion due to its complexity.
% This autocompletion problem is challenging, not only because exploring potential GUI element combinations is computationally costly but also because good solutions must consider visual, spatial, and semantic constraints among the to-be-placed elements and those already present.} 
% %
% \add{We present a method for iteratively recommending the position and size of unplaced GUI elements, as illustrated in Figure~\ref{fig:teaser}b. To augment the model's usability for designers, we introduce alternative element suggestion options, including the recommendation of element groups and simultaneous suggestions for all elements.}
% For evaluation, we conducted Study 1, comparing it with GRIDS~\cite{dayama2020grids}, an approach for autocompletion using integer programming. Our model produced suggestions with superior alignment and visual appeal compared to the baseline, consistent with participants' preferences. In Study 2, we integrated our model into a plug-in for a design tool. It allows GUI designers to utilize autocompletion capabilities in real time while maintaining full control over the design process within the interactive design tool. We interviewed six GUI designers to study our tool's practical benefits and efficiency advantages.


