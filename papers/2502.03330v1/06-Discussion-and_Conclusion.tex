\section{Discussion and Conclusion} 



Our results suggest that diffusion-based models are suitable for the rapid and flexible exploration of low-fidelity GUI generation.
Currently, no available diffusion model is capable of producing GUIs with high-quality text and graphics, but this limitation does not impede their use for the exploration of early-stage design ideas.
There the focus is more on generating a broad range of 'good enough' possibilities.
Specifically, we demonstrated that the \emph{probabilistic} nature of diffusion models allows for generating a multitude of diverse GUI ideas efficiently. 

Before this paper, it was an open question of how to design generative models such that designers do not need to over-specify the input.
By extending conditional diffusion to handle both wireframes and visual flow, we offer a low-effort approach to designers that simplifies interaction with diffusion models. 
%
Moreover, the inclusion of specialized adapters for considering local (e.g., the position and type of GUI elements) and global (e.g., visual flow) design properties enhances control over GUI characteristics, allowing designers to focus on broad exploration with minimal effort. 
We view this model as a step forward in creating user-adapted GUI designs with generated models, as it integrates GUI properties with user-centered interactions, such as eye movement, throughout the GUI design exploration process.



%We present a novel diffusion-based model that advances early-stage GUI design by enabling more rapid and flexible exploration of GUI design alternatives. By integrating any combination of wireframes, textual prompts, and visual flow controls, our model allows designers to rapidly generate diverse low-fidelity GUIs, thereby facilitating a broader exploration of design possibilities with minimal effort. 


%Our approach effectively manages both local and global design properties through the use of different specialized adapters. This feature addresses the limitations of previous methods, which either struggled to capture essential GUI characteristics through visual cues or were constrained by a narrow focus on a limited range of properties~\cite{cheng2023play, cheng2024colay}. These earlier approaches often failed to manage both local and global properties effectively. By employing modular adapters designed to control local properties (e.g., positioning and type of GUI elements) and global properties (e.g., overall visual flow direction), our model enhances design control and guidance during the image generation process.


\paragraph{Limitations} 

We acknowledge the following limitations. First, we cannot generate valid texts or labels for GUIs; future work could focus on producing meaningful text with appropriate font design. Second, we face challenges in generating realistic human faces, with some outputs appearing distorted. Increasing the dataset of human faces could improve the model's training. Third, the generated resolution is low since we trained on 256x256-resolution images. Future work could benefit from better computing power to train on higher-resolution images. Fourth, our model currently does not support highly specific requirements, such as generating a Margarita cocktail on a website. Additionally, our visual flow results lack finer control. We used the model EyeFormer~\cite{eyeformer} for scanpath prediction, however, it often biases scanpaths, starting from the center and moving toward the top left and right, limiting control over specific element emphasis. Although these patterns reflect natural eye movements, GUI designers may require more flexibility to direct user attention. Lastly, our current method does not support iterative design, which is important for early-stage prototyping, as each generation results in entirely new GUIs. Future work could improve the diffusion model by incorporating conditioning on the outputs from previous iterations. This could be done by using the current design as input to generate subsequent iterations. By conditioning on previous results, the model could produce variations that build upon existing designs, rather than starting anew each time.




% Our results suggest that diffusion models are suitable for exploration of low-fidelity GUI Generation.
% At the moment, there is no diffusion model that can produce high quality text and graphics;
% however, that is not necessarily a problem when the main goal is to explore ideas in early stages of design.
% Specifically, we showed that one can here exploit the fact that diffusion models are \emph{probabilistic} generative models.
% This makes it possible to ‘’pump’’ lots of different ideas.
% Before this paper, it has been an open question how to design diffusion models such that designers do not need to over-specify the input.
% By extending conditional diffusion to cover both wireframes and visual flow,
% we have shown that there is a path to low-effort interaction with a designer.



% Our generated results have limitations in the following cases: 1) we cannot generate valid texts. Future work can explore how to generate valid texts in the 
% 2) we cannot generate realistic human faces. Some human faces are distorted. Future work can add human faces in the dataset to better train on human faces. 3) Flow results are limited. No fine control of flow.
% we used EyeFormer~\cite{eyeformer}, the state-of-the-art scanpath prediction model, to predict scanpaths for each GUI. However, the model has some biases, for example, it mostly generates scanpaths starting from the center and goes towards the top left corner and moves towards the top right area, thus we can mostly only control whether the scanpaths conclude at the bottom left or bottom right. These biases align with human eye movement patterns but for GUI design, designers may want to specifically emphasize some elements or specifically guide users to look at GUI elements in a specific order. Future work can explore better ways to have more flexible control.
% In addition, it is very valuable to enable iterative design. It is important to iterate quickly in the early stages of design, however, our method does not support such iterations, every generation generates new GUIs. 

% % Limitation 1
% However functional and deployable GUIs need source code to link with external servers, databases and run over web connections faithfully. 
% We propose to explore self-debugging code generation from images pipelines using generated GUIs from our method. 
% Appending methods like Pix2Code, Design2Code to our pipeline could be a possible direction to explore for a more complete automatic front-end engineering solution.

% % Limitation 2
% Additionally, scaling up our method requires increasingly expensive compute so we propose to explore quantization and knowledge distillation setups.

% % Limitation 3
% Text Drawing with an image diffusion model remains a challenging research problem and most existing works require carefully crafted large text images and corresponding prompts... How to improve? (Read more TD, VLM papers)


% \paragraph{Scanpaths} Finally, we used EyeFormer~\cite{eyeformer}, the state-of-the-art scanpath prediction model, to predict scanpaths for each GUI. It would be better to use real scanpaths 
% would be better to use real scanpaths corrected by eye trackers, but it would be extremely time-consuming to do so and other proxies like webcams and cursors have different cognitive process compared to real eye movement. 
% Future work can use UEyes.

% A key objective in GUI design is to direct users’ attention towards discovering relevant information and interaction possibilities.
% Improved visual flow can enhance user engagement and guide user behavior~\cite{Rosenholtz11, Still10, ueyes}. 
% Modern GUIs, with their intricate arrangements of text, images, and numerous interactive elements such as buttons and menus, present challenges in considering visual flow during the design phase.
% Understanding user engagement often requires post-deployment analysis or expertise from experienced designers. 
% Can such expert knowledge be captured as a prior and inputted as a condition during the design process instead?



% In contrast, recent diffusion-based approaches train stably and handle multi-modal conditional generation effectively without the need for modality-specialized objectives.  
% Classifier-free-Guidance~\cite{cfg} enforces conditional adherence, effectively integrating multiple modalities without the need for explicitly defined discriminators or loss functions per condition. 




% However, diffusion models still face challenges in generating accurate text output due to their limited understanding of natural language~\cite{}. 
% %This highlights a significant issue in the development of generative vision models, as they often lack comprehensive natural language understanding. 

% Thus, the current models cannot generate meaningful texts.



















% This paper addressed the challenges of representing GUIs through a graph-based deep learning model. Prior deep learning-based GUI representations failed to consider the constraints for GUI elements and the visual-spatial-semantic structure of a GUI, which are important in computational design. Although many modern GUI tools use constraints to optimize GUIs, training a model to predict constraints remains a challenge. 
% Our proposed novel graph-based GUI representation captures both the properties of GUI elements, such as textual content, visual appearance, and element types, and their relationships in the visual, spatial, and semantic dimensions of a GUI. It can be computed efficiently in computational design. We further trained graph neural networks (GNNs) to take the graph as input to optimize the GUI. We will release our code and data.

% Our work has achieved the following results in the GUI autocompletion task. 

% \begin{enumerate}
% \item Our method predicts the position, size, and alignment of GUI elements more accurately. As shown in Figure~\ref{fig:comparison}, it achieves less than half of the error values in these three metrics (position, size, and alignment) compared to GRIDS~\cite{dayama2020grids}, an approach for autocompletion using integer programming. When the number of existing elements on the GUI increases, it remains to have low error rates while GRIDS's errors dramatically increase. 
% \item Our model offers superior alignment and visual appeal compared to the baseline method, and is better aligned with participants' preferences. In our comparison study,  70.33\% of the responses preferred results from our model compared to 13.54\% for GRIDS. 
% \item Our method enhances flexibility by integrating as a plug-in within a popular existing design tool, Figma. This integration allows designers to apply workflows they are already familiar with, eliminating the need to learn new tools or switch between different design software tools. Participants in the designer study praised the plug-in for accelerating their design process without disrupting the existing functionalities of their design applications. 
% \end{enumerate}

% \add{In addition to the demonstrated capability of our graph-based GUI representation in the GUI autocompletion task, we show that our GUI representation can be applied to other applications, such as GUI topic classification and GUI retrieval. 
% Our model demonstrated superior accuracy in GUI topic classification compared to baseline methods like ResNet50, Nearest Neighbors, and Random Forest. Furthermore, user feedback highlighted our model's effectiveness in retrieving visually similar GUIs compared to the Screen2Vec model.
% Compared to other data-driven approaches, our graph-based representation facilitates the understanding of GUI structure, improving the explainability of the model. This capacity enables our representation to potentially extend to diverse downstream tasks.
% For example, accessibility needs can also be represented as constraints~\cite{gajos2008improving}, and our method can train and predict layout constraints, thus it could potentially enhance accessibility. }

% %beyond the autocompletion task, such as design retrieval and evaluation/improvement of accessibility. With GNNs encoding the graph representation into an embedding vector, similar GUI designs can be retrieved as they will have closer embedding vectors. 

% \begin{figure}[t]
%   \def\w{\linewidth}
%   \centering
%  \includegraphics[width=\w]{figures/failure_case.pdf}
% \caption{Limitation of our method: It cannot capture the semantic correspondence between different types of GUI elements, like associating the ``Favorite'' text with a ``star'' icon, which could be explored further in future research.
% }
% \Description{This figure shows a limitation of our method: It cannot capture the semantic correspondence between different types of GUI elements.}
%   \label{fig:failure_case}
% \end{figure}

% \subsection{Limitations and Future Work}

% % \marginparsep=30pt
% % \marginnote{\color{violet}
% % R2: We clarified that we currently lack datasets containing non-rectangular bounding boxes and presented our solution for handling such boxes once they become available.\\~\\}
% As pointed out by participants in our designer study, our method has limited ability to generate accurate predictions if the unplaced element does not need to align or group with any existing element on the GUI. We currently assign a low confidence level to it to avoid uncertain predictions. Future work can improve the prediction of underconstrained GUI elements by considering more design priors or including more complicated constraints.
% \add{As shown in \autoref{tbl:related_work_comparison}, our representation does not explicitly represent the view hierarchy. The view hierarchy provides structural data, aiding models in understanding the layout and relationships of elements. We do not currently represent view hierarchies since they are not always available and often contain errors with incorrect structure information. However, future work can connect related element nodes in the graph representation to represent the view hierarchy.} 
% Moreover, while our method offers suggestions for each element to be placed, it provides only a single suggestion per element, thus constraining the possibility of exploration.
% In addition, we focus on a setting where all the elements are rectangular in shape or in rectangular bounding boxes. \add{There are no datasets available with non-rectangular bounding boxes. To accommodate various shapes of bounding boxes, we can augment the element node with additional parameters. These parameters would facilitate the description of common shapes, such as rectangles with rounded corners and circles. Subsequently, the model can be retrained to incorporate this information when present in the training dataset.} 
% %Future work can explore other shapes of bounding boxes.
% Furthermore, we observe that even for element prediction with high confidence levels, sometimes it does not predict ideal results. For example, as illustrated in Figure~\ref{fig:failure_case}, our method cannot capture the semantic correspondence between different types of GUI elements, e.g., it cannot detect that the ``Favorite'' text should correspond to the ``star'' icon. Future research could explore more about GUI element correspondence and constraints across UI types.



% \begin{acks}

% We appreciate the active discussion with Zixin Guo and Haishan Wang.
% This work was supported by Aalto University's Department of Information and Communications Engineering, the Research Council of Finland (flagship program: Finnish Center for Artificial Intelligence, FCAI, grants 328400, 345604, 341763; Subjective Functions, grant 357578), and the Meta PhD Fellowship.
% \end{acks} 
