
\section{Introduction}

The early stages of a GUI design project are critical for its eventual success. 
%
During the early stages, designers produce low-fidelity solutions in an attempt to explore the design space.
Low-fidelity designs often specify, partially or in an underspecified way, key aspects of the layout, widgets, graphics etc.~\cite{landay1996silk}.
Designers produce them to envision how users are supported in their tasks and to learn about the trade-offs of different approaches~\cite{dow2011prototyping, tohidi2006getting}.
% 
The decisions that are made at this stage are critical, because they may be hard to change later on.

To assist designers in this stage, 
computational methods should help them produce large numbers of diverse solutions to explore.
Research suggests that producing large numbers of low-fidelity ideas is beneficial for creativity at this stage~\cite{boyarski1994computers, landay1996silk, rettig1994prototyping}.
However, design fixation makes it hard for designers to generate entirely novel ideas~\cite{jansson1991design}.
However, existing design tools often fail to support exploration.
They require designers to engage at a higher level of detail than necessary, 
for example to specify which specific widgets are used and where they are exactly positioned on the frame.
\emph{To support creative exploration, designers need tools that allow them to rapidly explore a high number of solutions, flexibly and with minimum effort.}

Recent advances in generative AI have created optimism that this gap could be closed. 
While previous work focused on retrieval of GUIs based on a query~\cite{herring2009getting, kumar2013webzeitgeist, li2021screen2vec}, this approach was limited to the samples available in the database.
Generative AI is a way to circumvent this requirement by generating solutions to a given prompt.
%
However, while there are successes for natural images, 
generative AI-based tools are much less adopted in GUI design~\cite{controlnet, controlnet_plus_plus}.
We believe that the root cause is that text-based prompting is not a natural means for a designer to express ideas that are inherently visuo-spatial by nature. 
%
Designers need to explore options by sketching: by loosely specifying where some key elements are located~\cite{landay1996silk}. 
Designers also need to explore the functional and communicative aspects of a design but without being asked to specify lots of detail manually.

Designers should also be able to express what kinds of impact they want their design to have on users.
%
An important impact concerns \emph{visual flow}:
the order in which users look at a UI.
Improving visual flow can enhance user engagement and guide user behavior~\cite{Rosenholtz11, Still10, ueyes, jiang2024ueyes, wang2024visrecall++, emami2024impact}. 
Thus, designers also care about how GUIs guide users' attention to task-relevant items~\cite{Rosenholtz11, Still10, ueyes}. 
%
However, understanding how users engage with a design normally requires empirical studies. 
%
To our best knowledge, no prior work has considered user-centered visual flow during the GUI design process.
%Presently no computational method we know of considers this type of user-centered input.

In this short paper, we propose a diffusion-based approach to controllable generation of early-stage GUIs. 
As shown in~\autoref{fig:teaser}, our model breaks new ground by allowing flexible control of the generation process via three types of early-stage inputs: A) prompts, B) wireframes, and C) visual flows (controlling where users should look). To our knowledge, no previous work has utilized scanpaths for GUI generation control, and no existing datasets are available to train models for this purpose.
%
While previous work on generative approaches in this space has produced layouts only~\cite{cheng2023play, cheng2024colay}, our approach generates complete low-fidelity prototypes with graphics. Although the outputs are images only, and not functional GUIs, they allow designers to envision what the GUIs might appear like. 
%
Importantly, our approach allows a significant relaxation in the input side. 
The designer can provide \emph{any} combination of the three as input and will get a diverse set of low-fidelity solutions in a gallery in response. 
In practice, hundreds of designs can be explored in the order of minutes.
%
The benefit is that large design spaces can be explored with very little effort in input-specification. 

Inspired by ControlNet~\cite{controlnet}, which uses an adapter (a modular mechanism designed to provide additional guidance during the image generation process) in diffusion models to enable controllable generation, our key insight is to incorporate a diffusion model with two different adapters.
We apply different adapters respectively for the controls of local properties, such as the positioning and type of GUI elements, and global properties, such as the overall visual flow direction. 
Diffusion models are generative models that have demonstrated a certain level of controllability in image generation~\cite{controlnet, controlnet_plus_plus}.
%
Our technical contribution is a solution that overcomes two critical shortcomings in controlling them. 
First, generating controllable GUIs poses a challenge, as previous approaches that rely heavily on text-based prompts often fail to capture certain GUI characteristics effectively, which are better conveyed through visual cues.
Second, many current GUI generation methods focus on a narrow range of properties~\cite{cheng2023play, cheng2024colay}, limiting their ability to manage both local properties (e.g., the position and type of GUI elements) and global properties (e.g., visual flow).


\noindent In summary, this short paper makes two technical contributions:
\begin{enumerate}
\item We introduce a novel diffusion-based model for GUI generation that integrates design control through prompt, wireframe, and visual flow direction. It is the first model to consider visual flow in the GUI generation process.
\item We created a dataset of mixed mobile UIs and webpages, including GUI screenshots, wireframes with GUI element labels, descriptions, and scanpaths, which can be used to train generative AI models in this space.
\end{enumerate}

To evaluate our approach, we first qualitatively illustrate that our model can rapidly explore a wide range of GUI alternatives that closely align with the specified input conditions. Secondly, we demonstrate that our model aligns more accurately with input specifications than previous generative models. We will make our code and the dataset used for training available upon acceptance.


