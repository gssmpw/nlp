
\input{aa01-table_dataset}
\section{Dataset}

\paragraph{Prior Datasets}
No existing datasets can be directly used to train a model conditioned on prompts, wireframes, and visual flow directions. 
Several datasets have been collected to support GUI tasks, as shown in \autoref{tab:datasets_related}. The AMP dataset, comprising 77,000 high-quality screens from 4,068 iOS apps with human annotations~\cite{zhang2021screen}, is not publicly available. On the other hand, the largest publicly available dataset, Rico~\cite{rico}, includes 72,000 app screens from 9,700 Android apps and has been a primary resource for GUI understanding despite its inherent noise. To address its limitation, the Clay dataset~\cite{li2022learning} was created by denoising Rico using a pipeline of automated machine learning models and human annotators to provide more accurate element labels. Enrico~\cite{enrico} further cleaned and annotated Rico but ultimately contains only a small set of high-quality GUIs. MUD~\cite{mud} offers a dataset featuring modern-style Android GUIs. The VINS dataset~\cite{vins} focuses on GUI element detection and was created by manually capturing screenshots from various sources, including both Android and iOS GUIs. Additionally, Webzeitgeist~\cite{kumar2013webzeitgeist} used automated crawling to mine design data from 103,744 webpages, associating web elements with properties such as HTML tags, size, font, and color. Similarly, WebUI~\cite{webui} provides a large-scale collection of website data. None of these datasets include both mobile GUIs and webpages and have included visual flow information in the datasets.
UEyes~\cite{ueyes} is the first mixed GUI-type eye tracking dataset with ground-truth scanpaths, although it lacks element labels.

%In our work, we create a comprehensive high-quality dataset that includes both mobile GUIs (Android and iOS) and webpages by cleaning and combining GUIs from Enrico~\cite{enrico}, VINS~\cite{vins}, and WebUI~\cite{webui}. For each GUI, we further generate segmentation maps with GUI element labels, apply LLaVA-Next~\cite{liu2024llavanext} to generate description, and use EyeFormer~\cite{eyeformer}, a state-of-the-art scanpath prediction model, to generate scanpaths.
To address these limitations, we construct a large-scale high-quality dataset of mixed mobile UIs and webpages, including about 72,500 GUI screenshots, along with their wireframes with labeled GUI elements, descriptions, and scanpaths. This dataset is designed to support the training of generative AI models, filling a gap in existing public datasets by providing not only GUI images but also detailed descriptions, element labels, and visual interaction flows. 

\paragraph{GUI Screenshots} Our dataset integrates and cleans GUI data from Enrico~\cite{enrico}, VINS~\cite{vins}, and WebUI~\cite{webui}. The Enrico dataset contains 1,460 Android mobile GUIs, while VINS includes 4,543 Android and iOS GUIs. WebUI is a large-scale webpage dataset consisting of approximately 350,000 GUI screenshots with corresponding HTML code. For WebUI, the original dataset includes screenshots for different resolutions, leading to many similar screenshots for each webpage.
We retained only the 1920 x 1080 resolution screenshots to avoid redundant images from different resolutions. We further refined these three datasets by removing abstract, non-graphic wireframe GUIs, duplicates, and GUIs with fewer than three elements.  The final dataset consists of 66,796 webpages and 5,634 mobile GUIs.

\paragraph{Wireframes with GUI Element Labels} 

For mobile GUIs, we selected the Enrico and VINS datasets for their well-labeled GUI element bounding boxes. To further refine these annotations, we applied the UIED~\cite{uied} model, which detects and refines GUI element bounding boxes. We manually verified and corrected the results for accuracy. For the WebUI dataset, each element has multiple labels. We filtered the original element labels to keep only the most relevant label for each element. We standardized the labels across mobile and webpage elements, mapping them to nine common types: `Button', `Text', Image', `Icon', `Navigation Bar', `Input Field', `Toggle', `Checkbox', and `Scroll Element'. Using these refined bounding boxes and labels, we generated wireframes with GUI element labels.

\paragraph{Descriptions} For each GUI, we employed the LLaVA-Next~\cite{liu2024llavanext} vision-language model to generate both concise and detailed descriptions.

\paragraph{Scanpaths} Finally, we used EyeFormer~\cite{eyeformer}, the state-of-the-art scanpath prediction model, to predict scanpaths for each GUI. While using real scanpaths recorded by eye trackers would provide more accurate data, this process is highly time-consuming. Alternative proxies, such as webcams or cursor movements, do not capture the same cognitive processes as actual eye movements, making them less suitable for our purposes.