\section{Method}
\label{sec:synthesis}




To enable the rapid exploration of diverse GUI options, we propose integrating a diffusion model with different modular adapters designed to control both local and global GUI properties. Specifically, we employ the ControlNet adapter to manage local properties (e.g., the position and type of GUI elements), as it encourages close alignment with the input wireframe. For global properties (e.g., visual flow), we propose utilizing a Flow Adapter, which provides a more global guiding signal for GUI generation.

\subsection{Problem Formulation}

We formulate the GUI exploration task as a controllable GUI generation problem, allowing designers to flexibly guide the process using three types of inputs: A) prompts, B) wireframes, and C) visual flows. For visual flows, we currently support two options for designers: they can either 1) provide a sample GUI to encourage the model to generate GUIs with similar visual flow, or 2) specify a flow direction, indicating where the flow should conclude (bottom left or bottom right).
Designers can provide any combination of these inputs at any level of detail and receive a diverse gallery of low-fidelity solutions. 

We apply classifier-free guidance (CFG)~\cite{cfg}, a technique used in generative models, to balance fidelity to conditioning inputs and output diversity by mixing conditioned and unconditioned model outputs.
For visual flow, we need to encourage the generated visual flow to align with the input visual flow.
Thus, we introduce two objective terms: the classifier-free guidance loss ($\mathcal{L}_{\textrm{cfg}}$) and the flow consistency loss ($\mathcal{L}_{\textrm{flow}}$). The classifier-free guidance loss ensures the generated GUIs align with the provided inputs, while the flow consistency loss encourages the consistency between the desired visual flow and the visual flow of the generated GUIs. Thus, the objective function is 

\begin{equation}
\label{eq:objective_function}
\begin{split}
\mathcal{L}(\hat{z}, c_\mathrm{w}, c_\mathrm{p}, c_\mathrm{f}) = \mathcal{L}_{\mathrm{cfg}}(\hat{z}, c_\mathrm{w}, c_\mathrm{p}, c_\mathrm{v}) +  ~\mathcal{L}_{\mathrm{flow}}(\hat{z}, c_\mathrm{f}),
\end{split}
\end{equation}

where $\hat{z}$ is the generated GUI, $c_\mathrm{w}$, $c_\mathrm{p}$, and $c_\mathrm{f}$ represent the input conditions for the wireframe, prompt, and visual flow, respectively.



\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{images/model.png}
    \caption{
Our diffusion-based model generates diverse low-fidelity GUIs by conditioning on both local and global properties. The model integrates Stable Diffusion~\cite{rombach2022high} with specialized adapters: the ControlNet adapter~\cite{controlnet} manages local properties such as element positioning and types specified on wireframes, while the Flow Adapter directs the overall visual flow. Given inputs like wireframes, prompts, and visual flow patterns, the model effectively produces varied GUI designs.
    }
    \label{fig:model}
\end{figure}

\subsection{Controllable GUI Generation}

Our diffusion-based model generates GUIs conditioned on both local (e.g., position and type of elements) and global properties (e.g., visual flow). As illustrated in \autoref{fig:model}, the backbone of our model is based on Stable Diffusion~\cite{rombach2022high}, which employs a U-Net architecture~\cite{ronneberger2015u} consisting of an encoder $E$, a middle block $M$, and a skip-connected decoder $D$. The text prompt is encoded by a CLIP~\cite{clip} text encoder and feeds into the diffusion model via cross-attention layers.


%Our solution is to incorporate different adapters (modular mechanisms designed to provide additional control and guidance during the image generation process) for the controls of local properties, such as the positioning and type of GUI elements defined by wireframes, and global properties, such as the overall visual flow direction. 

\subsubsection{ControlNet Adapter for Local Properties}


Recent advancements in controllable image generation have shown that additional networks can be integrated into existing text-to-image diffusion models to better guide the generation process~\cite{controlnet, controlnet_plus_plus, mou2024t2i}. Inspired by ControlNet~\cite{controlnet}, we create a trainable copy of Stable Diffusion's encoder and middle block, followed by a decoder with zero-convolution layers. The weights and biases of these zero-convolution layers are initialized to zero, allowing the adapter to efficiently capture local properties, ensuring the generated GUI aligns with input wireframes. In this framework, wireframe features are concatenated with text features to guide the generation process.


\subsubsection{Flow Adapter for Global Properties}

The cross-attention mechanism has shown effective results for enhancing models' global control without explicit spatial guidance~\cite{ye2023ip, zhao2024uni}. Therefore, we adopt a cross-attention mechanism to process visual flow features, adding an additional cross-attention layer in each layer of the diffusion model for this purpose.


\paragraph{Flow Encoder}

No existing encoders are specifically designed to handle visual flow. However, EyeFormer~\cite{eyeformer}, a state-of-the-art model for scanpath prediction on GUIs, encodes GUI images and decodes the latent representations into scanpaths. We repurpose EyeFormerâ€™s decoder to train our flow encoder, using GUI scanpaths as input during training. During inference, we offer two options for designers:
1) Sample-based flow generation: If the designer provides a sample GUI, we encoding the input GUI via EyeFormer to guide the model;
2) Specified flow direction: If the designer specifies a target flow direction (e.g., bottom-left or bottom-right), we select a scanpath from our dataset that matches the desired visual flow to guide the model.


\subsubsection{Training Process}


We train our model using classifier-free guidance~\cite{cfg} and DDIM~\cite{song2020denoising} because these methods offer robust control over the generation process while ensuring high fidelity in the generated GUIs. Stable Diffusion operates by progressively adding noise to data and learning to reverse this process. Classifier-free guidance allows us to balance the diversity of the generated GUIs and alignment with input conditions without needing a separate classifier, reducing complexity and enhancing flexibility in our model. Similarly, DDIM provides a deterministic way to denoise and refine outputs, producing results that are closely aligned with the desired GUI specifications. Together, these techniques help maintain the quality of generated GUIs while ensuring they meet input requirements. 

During training, the Stable Diffusion components remain frozen, while our adapters are trained to optimize the objective.
For a given timestep $t$ and input conditions $C = \{c_\mathrm{w}$ (wireframe), $c_\mathrm{p}$ (prompt), and $c_\mathrm{f}$ (visual flow)$\}$, the model learns to predict the noise $\epsilon_{\theta}$ added to the noisy image $z_t$ using the loss function:

\begin{equation}
    \mathcal{L}_{\textrm{cfg}}(\hat{z}, C) = \mathbb{E}_{\hat{z}, t, C, \epsilon \sim \mathcal{N}(0,1)} [\| \epsilon - \epsilon_{\theta}(z_t, t, \mathcal{C}) \|^{2}_2],
\end{equation}

\noindent where $\hat{z} = z_0$ is the final GUI predicted by denoising $z_t$ over timestep $t$. 


We train each adapter independently. Specifically, the ControlNet adapter is trained using both prompt and wireframe inputs, while the Flow Adapter is trained on prompt and visual flow inputs. During training, we apply a 50\% dropout rate to each input condition, which encourages the model to handle various combinations of inputs. No fine-tuning across all three types of inputs is necessary.

\paragraph{ControlNet} 
The ControlNet adapter is trained solely with the classifier-free guidance loss. Thus, the objective function is  
\begin{equation}
  \mathcal{L_\textrm{ControlNet}} =  \mathcal{L}_{\textrm{cfg}}(\hat{z}, c_w, c_p) = \mathbb{E}_{z_t, t, c_w, c_p \epsilon \sim \mathcal{N}(0,1)} [\| \epsilon - \epsilon_{\theta}(z_t, t, c_w, c_p) \|^{2}_2],
\end{equation}.


\paragraph{Flow Adapter} 
The Flow Adapter is trained using both classifier-free guidance loss and a flow consistency objective. The latter ensures that the cross-attention layers guide generation toward a latent subspace aligned with the desired visual flow.

To encourage the consistency between the visual flow of the generated GUIs and the input visual flow specification, we apply Dynamic Time Warping (DTW)~\cite{dtw}, a standard metric used to measure the similarity between two temporal sequences, even when they differ in length. DTW identifies the optimal match between the sequences and computes the distance between them. However, since the original DTW is not differentiable, it cannot be directly used to optimize deep learning models.
To address this, we employ softDTW~\cite{soft-dtw}, a differentiable version of DTW, to optimize our model.

Since the only output of the network is a synthesized GUI ($\hat{z}$) with no ground-truth scanpath, we use EyeFormer~\cite{eyeformer} to compute a representative ground truth ($\textrm{EyeFromer}(\hat{z}) \sim \hat{c}_v$). The loss is defined as:
\begin{equation}
    \mathcal{L}_{\textrm{flow}}(\hat{z}, c_f) = \textrm{softDTW}(\textrm{EyeFromer}(\hat{z}, c_f)))
\end{equation}

Thus, the total loss for the Flow Adapter is:
\begin{equation}
\begin{split}
  \mathcal{L_\textrm{FlowAdapter}} &=  \mathcal{L}_{\textrm{cfg}}(\hat{z}, c_p, c_v) + \mathcal{L}_{\textrm{flow}}(\hat{z}, c_f) \\
  &= \mathbb{E}_{z_t, t, c_p, c_v \epsilon \sim \mathcal{N}(0,1)} [\| \epsilon - \epsilon_{\theta}(z_t, t,c_p, c_v) \|^{2}_2] + \textrm{softDTW}(\textrm{EyeFromer}(\hat{z}, c_f))).
  \end{split}
\end{equation}.






