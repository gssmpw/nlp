\section{Discussion and Related Work}\label{sec:relatedWork}

\paragraph{Persona Evaluation}

Prior work has established several frameworks for evaluating language models' role-playing capabilities. \citet{wang-etal-2024-rolellm} introduced RoleBench, an evaluation benchmark with QA pairs based on character profiles. \citet{wang-etal-2024-incharacter} developed InCharacter, assessing role-playing fidelity through psychological scales in an interview format. \citet{tu-etal-2024-charactereval} created CharacterEval, a Chinese benchmark derived from novels and scripts with multi-interaction dialogues, while \citet{shen2023roleeval} established RoleEval, a bilingual benchmark with multiple-choice questions testing persona knowledge and reasoning. \citet{samuel2024personagym} introduced PersonaGym, a dynamic evaluation framework for automated assessment of persona adherence across diverse interactions. Our work further extends the literature by performing the first systematic evaluation to understand the influence of the persona modality.

\paragraph{Multimodal Personas}

Recent work has explored integrating visual elements into LLM persona systems. \citet{ahn-etal-2023-mpchat} introduced MPCHAT, demonstrating that incorporating visual episodic memories alongside text improves dialogue consistency and persona grounding. \citet{sun-etal-2024-kiss} investigated how visual personas influence LLMs' behavior in negotiation contexts, showing models can adapt their responses based on perceived visual personality traits. \citet{dai2025mmrole} developed MMRole, a framework for training and evaluating multimodal role-playing agents. While these works establish the potential of visual personas and others extensively evaluate textual personas \citep{li-etal-2016-persona, xiao2024farllmsbelievableai, samuel2024personagym}, there has been no systematic comparison of how different modalities of persona representation affect model performance. Our work addresses this gap by directly evaluating text, visual, and hybrid approaches across a range of persona-based tasks.

\paragraph{Modality Alignment}

Language models demonstrate strong in-context learning capabilities in unimodal textual settings \citep{Shanahan2023, 10.5555/3666122.3669274}.
However, extending these capabilities to multimodal inputs remains challenging.
When visual information is introduced, models often struggle to transfer knowledge effectively from text to vision (and vice versa), resulting in noticeably weaker performance with visual in-context demonstrations compared to textual ones \citep{zhao2024mmicl, jiang2024manyshot}.
Such cross-modal gaps manifest in several ways: for instance, catastrophic forgetting of text-based instruction following can occur when models are finetuned on images \citep{zhang2024wingslearningmultimodalllms}.
While incorporating visual knowledge can yield improvements on specific tasks \citep{jin-etal-2022-leveraging}, maintaining consistently high performance across both textual and visual modalities remains an open research question, which is also highlighted in our work.