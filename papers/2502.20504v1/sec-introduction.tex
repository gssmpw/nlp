\section{Introduction}\label{sec:introduction}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Persona-Main-Figure.png}
    \caption{A comparison of visual and textual persona interactions for a chef from Paris. The left side presents an image persona, while the right side features a text persona derived from the image.}
    \label{fig:intro}
\end{figure}


``An image is worth a thousand words'', the adage describes how visuals have the unique capacity to encapsulate not only long texts but also more abstract concepts~\citep{paivio2013imagery}. 
Large language models (LLMs) are increasingly being adopted as role-playing agents for specific tasks~\citep{li2024personal} and personalized conversational agents~\citep{tseng2024two}. 
However, these personas are often represented using only elaborate textual descriptions~\citep{wang-etal-2024-rolellm,shen2023roleeval,samuel2024personagym}. As these models become more capable of handling different modalities~\citep{liu2024visual}, it is imminent that they can express these personas by incorporating information from representative images~\citep{ahn-etal-2023-mpchat}. Thus, it becomes crucial to conduct a systematic study to study the contribution of the persona modality in specifying these personas to the LLMs.

Multimodal LLMs have pushed the frontier of virtual assistants by enabling realistic image and voice-based interactions~\citep{liu2024visual,li2024llavamed,gpt4o}. These advancements have enabled processing and generating content across multiple modalities, bridging the gap between text-based understanding and richer, more immersive experiences. However, significant gaps remain in these models' ability to accurately capture visual information, leading to a subpar performance on visual understanding and reasoning tasks~\citep{tong2024eyes,chen2024far}. Given the fast adoption of LLMs as all-purpose agents, it is important to understand the extent to which these models can accurately capture visual personas.

LLMs have shown remarkable capabilities in manifesting given roles/personas, as highlighted by their ability to succinctly answer specific questions by adapting their styles according to the prescribed personas~\citep{tu-etal-2024-charactereval,tseng2024two,samuel2024personagym}. Furthermore, it has been shown that one can further improve the models' role-playing and personalization capabilities by incorporating both visual and textual information to create a multimodal persona~\citep{sun-etal-2024-kiss,ahn-etal-2023-mpchat,dai2025mmrole}. However, no systematic comparison exists between personas represented in different modalities. 
% Specific representation of LLMs are biased towards certain features or representations of the input due to the training data. For example, they have been shown to be biased towards certain demographic groups~\citep{}, and prefer inputs in certain languages~\citep{}. 
% Similarly, they tend to prefer textual descriptions over visual cues~\citep{} but the precise bias of the modality to represent personas has not been studied.  
% , often favoring textual descriptions over visual cues or exhibiting inconsistencies in how they interpret persona-related characteristics across different modalities~\citep{}. These biases can arise from the training data, which may overrepresent specific , cultural norms, or stylistic tendencies, leading to skewed or incomplete persona embodiments.



In this work, we present the first comprehensive study of the influence of the modality of a persona on its embodiment by multimodal LLMs. Figure~\ref{fig:intro} illustrates our analysis with an example and our contributions can be summarized as follows: 
% \ks{write some numbers}
\begin{enumerate}[leftmargin=*, itemsep=0mm]
    \item We conduct the \textbf{first systematic study} on how a persona's modality affects how it is expressed by multimodal LLMs.
    \item We create a \textbf{novel text-image parallel dataset} of $40$ diverse personas along with $60$ probing questions about their attributes and scenarios.
    \item \textbf{Comprehensive evaluation} using both LLM-based and linguistic metrics show that text-based personas are expressed better than the corresponding image representations. 
    \item We also conduct \textbf{stratified analysis} to show the stability of our results regardless of the type of personas, evaluators, and questions.  
\end{enumerate}
