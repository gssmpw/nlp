\section{Conclusion}
In this work, we study the influence of the modality of a persona in multimodal LLMs. 
We first create a novel modality-parallel dataset with equivalent representations across 4 different modality representations: image, text, assisted image, and descriptive image. 
Using a set of manually curated hard questions about the persona, we evaluate how well multimodal LLMs can represent them across different representations. 
We find a clear preference for text-based personas across $5$ multimodal LLMs, highlighting the gaps in the vision-understanding capabilities of these models in embodying diverse personas. 
Our results emphasize a grave need in the community to push the vision frontier following the advancements in language modeling. 
Given the rich amount of information that can be captured within an image, we believe it is imminent that LLM-based agents can also incorporate multimodal descriptions of their desired persona. 
We believe our modality-parallel dataset lays the foundation for future advancements in visual persona understanding in multimodal LLMs. 
Furthermore, we hope our evaluation methodology helps standardize how personas are evaluated while inspiring other evaluations in studying the influence of how the inputs are represented in LLMs.
