\section{Experimental Setup}\label{sec:setup}

\paragraph{Models.} We evaluate the performance of 5 multimodal large language models: (1) GPT-4o\footnote{\href{https://openai.com/index/hello-gpt-4o/}{GPT-4o}}, (2) GPT-4o mini\footnote{\href{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}{GPT-4o mini}}, (3) Llama 3.2 11B\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-11B-Vision}{Llama 3.2 11B}}, (4) Llama 3.2 90B\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-90B-Vision}{Llama 3.2 90B}}, and (5) Pixtral 12B\footnote{\href{https://huggingface.co/mistralai/Pixtral-12B-2409}{Pixtral 12B}} \citep{agrawal2024pixtral12b}.

\paragraph{Evaluators.} We utilize two LLM evaluators, using GPT-4o\footnotemark[3] and Gemini 2.0 Flash\footnote{\href{https://deepmind.google/technologies/gemini/flash/}{Gemini 2.0 Flash}}, with deterministic sampling with zero temperature and top P values. All scores discussed in the main paper are averaged across the two models, while individual scores can be found in Tables \ref{tab:eval-table-gpt-4o} and \ref{tab:eval-table-gemini-flash} in the Appendix. We use human evaluators on a large subset of the evaluation set to assess the LLM evaluator scores' alignment with human scores. For further details, refer to Appendix \ref{app:human}. 
% \ks{what detail?} \jb{screenshots/details about the survey and who the evaluators were (broadly)}