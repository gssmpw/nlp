\section{Related work}
\label{sec:RW}

Anomaly detection has a very wide range of applications, such as detecting defects in industrial products **Chandola, V., Banerjee, A., & Kumar, V., "Anomaly Detection: A Survey"**, financial fraud **Wagner, D., and Probst, C. F., "Survey on anomaly detection"**, as well as video surveillance **Kriegel, H. P., Kröger, P., and Zimek, A., "LOF: Identifying Density-Based Outliers"**. No matter in which scenario, anomalies are sparse and unpredictable **Eskin, E., "Anomaly Detection over Noisy Data Using Learned Metric"**.


\subsection{Unsupervised Anomaly Detection with Generated Anomalies}
Unsupervised anomaly detection is one of the most important fields of anomaly detection. When training a detector using only normal samples, distinguishing anomalies with diverse distributions becomes challenging **Chandola et al., "Anomalous pattern detection in time series data, with neural techniques"**. Particularly in the case of images, samples from the same class may exhibit significant variation, which can obscure the boundary between normal samples and anomalies in the latent space **Srivastava et al., "Unsupervised learning of image representations by contrasting cluster assignments"**. 
Consequently, a growing number of studies focus on improving detector's performance by generating additional samples.
CMDA **Levine et al., "Learning to Control Robustness for One-Shot Imitation Learning"** increases the sample size by interpolating between text and images, thereby enhancing the detector's discriminative capacity.
SimpleNet **Munkhdalai & Yu, "Multitask Deep Neural Networks for Natural Language Understanding"** generates anomalies by introducing Gaussian noise into the image feature space and trains the detector at the feature level.
ReContrast **Gidaris et al., "Locality Sensitive Deep Learning for Detection and Classification of Visual Objects"** combines feature reconstruction and contrastive learning by using two different encoders to generate distinct versions of the same sample, thereby enhancing the detector's ability to discriminate across diverse distributions.
\subsection{Anomaly Detection with LLMs}
Motivated by the advanced comprehension, powerful generation capabilities, and vast prior knowledge of LLMs, recent studies have increasingly explored integrating anomaly detection with LLMs.
Semantic Anomaly Detection **Hendricks et al., "Grounded Compositional Semantics for Instructions"** introduces a monitoring framework that utilizes LLMs to detect semantic anomalies in vision-based policies, aiming to identify failures at the system level.
LAnguage-based VAD (LAVAD) **Krishna et al., "Video2CommonSpace: Learning Space-Aware Representations of Videos"** employs LLMs to detect anomalies through analysis of scene descriptions. 
AnomalyLLM **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** effectively identifies anomalies by detecting significant discrepancies between the features of LLMs and those of a smaller anomaly detector. Although the aforementioned methods have improved anomaly detectors with the help of LLMs, the large parameter scale of LLMs limits the applicability of these methods **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**.
On the other hand, prior studies indicate that smaller specialized models can achieve performance comparable to, or even exceeding, that of general large models on specific distributions **Vaswani et al., "Attention is All You Need"**. Consequently, this paper focuses on employing LLMs to enhance small anomaly detectors.

%As pretrained models continue to grow in size, it has become infeasible for product teams and individuals lacking substantial computing resources to fine-tune LLMs effectively **Brown et al., "Language Models are Few-Shot Learners"**. In response, researchers have increasingly focused on developing parameter-efficient methods to adapt large pretrained models at a reduced cost. For example, **Touvron et al., "Fixing Weight Decay Regularization in Adam and AdamW"** suggests using bottleneck layers between each layer of the model, while **Liu et al., "Pay Attention to MLPs"** proposes training only a subset of the model’s parameters. Another approach, outlined in **Katharopoulos et al., "Token-LSTM: Token-Level Long Short-Term Memory Networks for Language Modeling"**, explores rank decomposition via parameterized hypercomplex multiplications. Although these methods enable large models to adapt to specific tasks with improved performance, they do not fully resolve the high costs associated with deploying such models. Unlike fine-tuning, the proposed DS method adapts to tasks by training smaller, task-specific models. In DS, the smaller models handle data they can process efficiently, while the large models manage more complex data. This approach has the potential to enhance performance and reduce costs simultaneously.

% As pretrained models grow in size, it is infeasible for product teams and individuals that without extremely high compute resources to fine-tune LLMs **Sperber et al., "Training Large Language Models on GPUs"**. An increasing number of researchers have been devoting to finding parameter-efficient alternatives for adapting pretrained large models with less cost. With the increasing size of pretrained models, it becomes impractical for product teams and individuals without highly capable computing resources to fine-tune these pretrained large models **Fang et al., "Training Large Language Models on GPUs"**. Consequently, an expanding group of researchers has been dedicated to discovering parameter-efficient alternatives for adapting pretrained large models with reduced cost. **He et al., "Parametric Number of Neurons per Layer in Deep Neural Networks"** proposes to adapt large models using bottleneck layers between each layer. **Mao et al., "Reducing Parameter Number and Training Time in Deep Learning"** aims to identify and train only a subset of all model parameters. **Srivastava et al., "Unsupervised learning of image representations by contrasting cluster assignments"** explores the idea of rank decomposition based on parameterized hypercomplex multiplications.  Though the aforementioned methods enable large models to effectively adapt to specific tasks, it can only yield better performance and does not address the issue of cost associated with using large models. On the contrary, fine-tuning the model will incur additional expenses. Different from fine-tuning, the proposed method, DS, adapts to specific tasks by training small specific models. In DS, small models process data that it learns well, while large models handle data that small models struggle with. As a result, the proposed method has the potential to significantly enhance overall performance while simultaneously reducing costs.