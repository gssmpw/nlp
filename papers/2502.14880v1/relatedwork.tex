\section{Related work}
\label{sec:RW}

Anomaly detection has a very wide range of applications, such as detecting defects in industrial products \cite{liu2024deep,roth2022towards,siegel2020industrial}, financial fraud \cite{hilal2022financial,huang2018codetect}, as well as video surveillance \cite{zhang2020normality,li2020spatial,chang2021contrastive}. No matter in which scenario, anomalies are sparse and unpredictable \cite{pang2021deep}. 


\subsection{Unsupervised Anomaly Detection with Generated Anomalies}
Unsupervised anomaly detection is one of the most important fields of anomaly detection. When training a detector using only normal samples, distinguishing anomalies with diverse distributions becomes challenging \cite{zong2018deep,sehwag2021ssd}. Particularly in the case of images, samples from the same class may exhibit significant variation, which can obscure the boundary between normal samples and anomalies in the latent space \cite{chen2024improvingg}. 
Consequently, a growing number of studies focus on improving detector's performance by generating additional samples.
CMDA \cite{chen2023cross} increases the sample size by interpolating between text and images, thereby enhancing the detector's discriminative capacity.
SimpleNet \cite{liu2023simplenet} generates anomalies by introducing Gaussian noise into the image feature space and trains the detector at the feature level.
ReContrast \cite{guo2024recontrast} combines feature reconstruction and contrastive learning by using two different encoders to generate distinct versions of the same sample, thereby enhancing the detector's ability to discriminate across diverse distributions.
\subsection{Anomaly Detection with LLMs}
Motivated by the advanced comprehension, powerful generation capabilities, and vast prior knowledge of LLMs, recent studies have increasingly explored integrating anomaly detection with LLMs.
Semantic Anomaly Detection \cite{elhafsi2023semantic} introduces a monitoring framework that utilizes LLMs to detect semantic anomalies in vision-based policies, aiming to identify failures at the system level.
LAnguage-based VAD (LAVAD) \cite{zanella2024harnessing} employs LLMs to detect anomalies through analysis of scene descriptions. 
AnomalyLLM \cite{liu2024large} effectively identifies anomalies by detecting significant discrepancies between the features of LLMs and those of a smaller anomaly detector. Although the aforementioned methods have improved anomaly detectors with the help of LLMs, the large parameter scale of LLMs limits the applicability of these methods \cite{patrikar2022anomaly,yu2022edge,ngo2020adaptive}.
On the other hand, prior studies indicate that smaller specialized models can achieve performance comparable to, or even exceeding, that of general large models on specific distributions \cite{chen2024data,chen2024improving}. Consequently, this paper focuses on employing LLMs to enhance small anomaly detectors.

%As pretrained models continue to grow in size, it has become infeasible for product teams and individuals lacking substantial computing resources to fine-tune LLMs effectively \cite{chen2023parameter}. In response, researchers have increasingly focused on developing parameter-efficient methods to adapt large pretrained models at a reduced cost. For example, \cite{houlsby2019parameter} suggests using bottleneck layers between each layer of the model, while \cite{zaken2021bitfit} proposes training only a subset of the modelâ€™s parameters. Another approach, outlined in \cite{zhang2021beyond}, explores rank decomposition via parameterized hypercomplex multiplications. Although these methods enable large models to adapt to specific tasks with improved performance, they do not fully resolve the high costs associated with deploying such models. Unlike fine-tuning, the proposed DS method adapts to tasks by training smaller, task-specific models. In DS, the smaller models handle data they can process efficiently, while the large models manage more complex data. This approach has the potential to enhance performance and reduce costs simultaneously.

% As pretrained models grow in size, it is infeasible for product teams and individuals that without extremely high compute resources to fine-tune LLMs \cite{chen2023parameter}. An increasing number of researchers have been devoting to finding parameter-efficient alternatives for adapting pretrained large models with less cost. With the increasing size of pretrained models, it becomes impractical for product teams and individuals without highly capable computing resources to fine-tune these pretrained large models \cite{chen2023parameter}. Consequently, an expanding group of researchers has been dedicated to discovering parameter-efficient alternatives for adapting pretrained large models with reduced cost. \cite{houlsby2019parameter} proposes to adapt large models using bottleneck layers between each layer. \cite{zaken2021bitfit} aims to identify and train only a subset of all model parameters. \cite{zhang2021beyond} explores the idea of rank decomposition based on parameterized hypercomplex multiplications.  Though the aforementioned methods enable large models to effectively adapt to specific tasks, it can only yield better performance and does not address the issue of cost associated with using large models. On the contrary, fine-tuning the model will incur additional expenses. Different from fine-tuning, the proposed method, DS, adapts to specific tasks by training small specific models. In DS, small models process data that it learns well, while large models handle data that small models struggle with. As a result, the proposed method has the potential to significantly enhance overall performance while simultaneously reducing costs.