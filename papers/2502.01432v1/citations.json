[
  {
    "index": 0,
    "papers": [
      {
        "key": "p\u00e9rez2019turingcompletenessmodernneural",
        "author": "Jorge P\u00e9rez and Javier Marinkovi\u0107 and Pablo Barcel\u00f3",
        "title": "On the Turing Completeness of Modern Neural Network Architectures"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "yun2020transformersuniversalapproximatorssequencetosequence",
        "author": "Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar",
        "title": "Are Transformers universal approximators of sequence-to-sequence functions?"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hahn-2020-theoretical",
        "author": "Hahn, Michael",
        "title": "Theoretical Limitations of Self-Attention in Neural Sequence Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "Bhattamishra2020OnTA",
        "author": "S. Bhattamishra and Kabir Ahuja and Navin Goyal",
        "title": "On the Ability and Limitations of Transformers to Recognize Formal Languages"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "Strobl_2024",
        "author": "Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana",
        "title": "What Formal Languages Can Transformers Express? A Survey"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "voita2019analyzingattention",
        "author": "Voita, Elena  and\nTalbot, David  and\nMoiseev, Fedor  and\nSennrich, Rico  and\nTitov, Ivan",
        "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rogers2020primer",
        "author": "Rogers, Anna  and\nKovaleva, Olga  and\nRumshisky, Anna",
        "title": "A Primer in {BERT}ology: What We Know About How {BERT} Works"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "coenen2019visualizingmeasuringgeometrybert",
        "author": "Andy Coenen and Emily Reif and Ann Yuan and Been Kim and Adam Pearce and Fernanda Vi\u00e9gas and Martin Wattenberg",
        "title": "Visualizing and Measuring the Geometry of BERT"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li-etal-2021-implicit",
        "author": "Li, Belinda Z.  and\nNye, Maxwell  and\nAndreas, Jacob",
        "title": "Implicit Representations of Meaning in Neural Language Models"
      },
      {
        "key": "abdou-etal-2021-language",
        "author": "Abdou, Mostafa  and\nKulmizev, Artur  and\nHershcovich, Daniel  and\nFrank, Stella  and\nPavlick, Ellie  and\nS{\\o}gaard, Anders",
        "title": "Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "elhage2022toymodelssuperposition",
        "author": "Nelson Elhage and Tristan Hume and Catherine Olsson and Nicholas Schiefer and Tom Henighan and Shauna Kravec and Zac Hatfield-Dodds and Robert Lasenby and Dawn Drain and Carol Chen and Roger Grosse and Sam McCandlish and Jared Kaplan and Dario Amodei and Martin Wattenberg and Christopher Olah",
        "title": "Toy Models of Superposition"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "li2024emergentworldrepresentationsexploring",
        "author": "Kenneth Li and Aspen K. Hopkins and David Bau and Fernanda Vi\u00e9gas and Hanspeter Pfister and Martin Wattenberg",
        "title": "Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "stackrnn",
        "author": "Armand Joulin and Tomas Mikolov",
        "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "suzgun2019memoryaugmentedrecurrentneuralnetworks",
        "author": "Mirac Suzgun and Sebastian Gehrmann and Yonatan Belinkov and Stuart M. Shieber",
        "title": "Memory-Augmented Recurrent Neural Networks Can Learn Generalized Dyck Languages"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "transformerstack",
        "author": "Fernandez Astudillo, Ram{\\'o}n  and\nBallesteros, Miguel  and\nNaseem, Tahira  and\nBlodgett, Austin  and\nFlorian, Radu",
        "title": "Transition-based Parsing with Stack-Transformers"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "stackattention",
        "author": "Brian DuSell and David Chiang",
        "title": "Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "weiss2018lstmcounters",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "suzgun2019dyck",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "merrill2019rnntheory",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "weiss2019extractingdfa",
        "author": "Unknown",
        "title": "Unknown"
      }
    ]
  }
]