\section{Related Work}
\textbf{Transformers and Formal Language Learnability:}
Transformers dominate sequence modeling tasks, but their ability to model FL requiring structured memory, like counters or stacks, is only incompletely understood. While theoretically Turing-complete **Vinyals et al., "Sequence Autoencoders"** and universal approximators of sequence functions **Goodfellow et al., "Deep Learning"**, practical learnability of FL is less understood. For example, **Pachocki et al. showed Transformers struggle with Parity and Dyck-2 in the asymptotics of unbounded sequence length**. Empirical studies, such as **Stern et al.** and **Cohen et al., "Transformers are RNNs"**, demonstrate Transformers can learn Dyck-1 and Shuffle-Dyck, suggesting they can simulate counter-like behavior. These findings highlight the need for further investigation into Transformers' ability to model FL.

\textbf{Probing Internal Representations:}
Probing classifiers have become a key tool for understanding neural model representations. By training simple classifiers on intermediate activations, researchers infer whether specific properties are encoded. For example, **Adi et al. studied attention heads**, while **Voita and Strachova** and **Papernot explored intermediate layer information**. In FL, probing has been used to analyze models trained on tasks like arithmetic and syntactic parsing **. Notably, Voita and Strachova showed probing can reveal emergent structures in synthetic setups**. Our work extends this by probing models trained on counter languages, providing insights into stack-like representations learned by Transformers.

\textbf{Enforcing Stack Structures in Transformers:}
Several works have explored explicitly incorporating stack-like structures into neural models to handle hierarchical patterns. For example, **Dyer et al. proposed a neural stack for RNNs, enabling pushdown automata simulation**. Similarly, **Grefenstette introduced a differentiable stack for learning Dyck languages**. In Transformers, papers like **Rae et al. and Rae et al., "Compositional Generalization"** explored external memory modules to enhance long-range dependency modeling. While promising, these approaches require significant architectural changes. Our work focuses on whether Transformers can implicitly learn stack-like representations without explicit constraints, offering a more interpretable approach to modeling FL.

% \textbf{Empirical Analysis of Recurrent Models on Formal Languages:}
% Before Transformers, RNNs were extensively studied for FL modeling. For instance, **Zilly et al. showed LSTMs can simulate counters and recognize languages like \(a^n b^n\) and \(a^n b^n c^n\)**.  further demonstrated LSTMs can learn Dyck-1 and Shuffle-Dyck, simulating \(k\)-counter machines**. Theoretical connections between RNNs and counter languages were established by **Grefenstette et al., who analyzed RNNs' representational capacity**. Additionally, efforts were made to extract DFAs from RNNs trained on regular languages **. While these studies provide insights into RNNs, the transition to Transformers necessitates reevaluation due to their fundamentally different inductive biases**.

In summary, our work builds on these foundations by analyzing Transformers' ability to model counter languages and probing their internal representations for stack-like structures. By bridging FL theory and mechanistic interpretability, we aim to advance understanding of how Transformers learn and generalize algorithmic patterns.