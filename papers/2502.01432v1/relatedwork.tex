\section{Related Work}
\textbf{Transformers and Formal Language Learnability:}
Transformers dominate sequence modeling tasks, but their ability to model FL requiring structured memory, like counters or stacks, is only incompletely understood. While theoretically Turing-complete \citep{p√©rez2019turingcompletenessmodernneural} and universal approximators of sequence functions \citep{yun2020transformersuniversalapproximatorssequencetosequence}, practical learnability of FL is less understood. For example, \citet{hahn-2020-theoretical} showed Transformers struggle with Parity and Dyck-2 in the asymptotics of unbounded sequence length. Empirical studies, such as \citet{Bhattamishra2020OnTA} and \cite{Strobl_2024}, demonstrate Transformers can learn Dyck-1 and Shuffle-Dyck, suggesting they can simulate counter-like behavior. These findings highlight the need for further investigation into Transformers' ability to model FL.

\textbf{Probing Internal Representations:}
Probing classifiers have become a key tool for understanding neural model representations. By training simple classifiers on intermediate activations, researchers infer whether specific properties are encoded. For example, \citet{voita2019analyzingattention} studied attention heads, while \citet{rogers2020primer} and \citet{coenen2019visualizingmeasuringgeometrybert} explored intermediate layer information. In FL, probing has been used to analyze models trained on tasks like arithmetic and syntactic parsing \citep{li-etal-2021-implicit, abdou-etal-2021-language}. Notably, \citet{elhage2022toymodelssuperposition} and \citet{li2024emergentworldrepresentationsexploring} showed probing can reveal emergent structures in synthetic setups. Our work extends this by probing models trained on counter languages, providing insights into stack-like representations learned by Transformers.

\textbf{Enforcing Stack Structures in Transformers:}
Several works have explored explicitly incorporating stack-like structures into neural models to handle hierarchical patterns. For example, \citet{stackrnn} proposed a neural stack for RNNs, enabling pushdown automata simulation. Similarly, \citet{suzgun2019memoryaugmentedrecurrentneuralnetworks} introduced a differentiable stack for learning Dyck languages. In Transformers, papers like \citet{transformerstack} and \citet{stackattention} explored external memory modules to enhance long-range dependency modeling. While promising, these approaches require significant architectural changes. Our work focuses on whether Transformers can implicitly learn stack-like representations without explicit constraints, offering a more interpretable approach to modeling FL.

% \textbf{Empirical Analysis of Recurrent Models on Formal Languages:}
% Before Transformers, RNNs were extensively studied for FL modeling. For instance, \citet{weiss2018lstmcounters} showed LSTMs can simulate counters and recognize languages like \(a^n b^n\) and \(a^n b^n c^n\). \citet{suzgun2019dyck} further demonstrated LSTMs can learn Dyck-1 and Shuffle-Dyck, simulating \(k\)-counter machines. Theoretical connections between RNNs and counter languages were established by \citet{merrill2019rnntheory}, who analyzed RNNs' representational capacity. Additionally, efforts were made to extract DFAs from RNNs trained on regular languages \citet{weiss2019extractingdfa}. While these studies provide insights into RNNs, the transition to Transformers necessitates reevaluation due to their fundamentally different inductive biases.

In summary, our work builds on these foundations by analyzing Transformers' ability to model counter languages and probing their internal representations for stack-like structures. By bridging FL theory and mechanistic interpretability, we aim to advance understanding of how Transformers learn and generalize algorithmic patterns.