\section{Related Work}
\subsection{CV-ISLR Datasets} \label{sec:dataset}
% In Figure~\ref{fig:data}, the MM-WLAuslan dataset~\cite{shen2024mm} is designed to tackle the significant challenges of Cross-View Isolated Sign Language Recognition (CV-ISLR). As the first large-scale, multi-view, multi-modal dataset for Australian Sign Language (Auslan), it contains over 282,000 sign videos, covering 3,215 commonly used Auslan glosses performed by 73 signers.
% Recorded using a multi-camera setup consisting of three Kinect-V2 cameras and one RealSense camera, the dataset captures a diverse range of viewpoints by positioning the cameras hemispherically around the signer, closely simulating the varied angles encountered in real-world scenarios.
% To ensure comprehensive evaluation under practical conditions, the dataset includes four test subsets that introduce challenges such as dynamic environments, background variations, and temporal inconsistencies.
% In this context, our work leverages this dataset to explore new approaches, pushing the boundaries of recognition accuracy across multiple views and modalities.
In Figure~\ref{fig:data}, the MM-WLAuslan dataset~\cite{shen2024mm} tackles the challenges of CV-ISLR as the first large-scale, multi-view, multi-modal dataset for Australian Sign Language (Auslan). It comprises over 282,000 sign videos of 3,215 glosses performed by 73 signers, captured using a multi-camera setup with three Kinect-V2 cameras and one RealSense camera. The hemispherical camera positioning simulates real-world scenarios with diverse viewpoints. To ensure comprehensive evaluation, the dataset includes 4 test subsets addressing dynamic environments, background variations, and temporal inconsistencies. This dataset serves as the foundation for our work, enabling the development of new approaches to improve recognition accuracy across multiple views and modalities.

\subsection{Ensemble Learning} 
Ensemble Learning~\cite{cao2020ensemble,zhou2022ensemble}, which combines multi models to improve performance and robustness, has been widely applied in various recognition tasks.
In Figure~\ref{fig:ensemble}, it effectively reduces bias and variance by aggregating the predictions of diverse models, enhancing accuracy, especially in complex tasks with high variability~\cite{sharma2022trbaggboost}.
Previous works~\cite{cao2020ensemble,yin2024grpose} have leveraged ensemble methods like bagging, boosting, and stacking to combine different neural network architectures and multi-modal inputs, such as RGB, depth, and skeletal data, for more robust models.
Developing an ensemble strategy to integrate models trained in the same view to enhance generalization ability in different views needs to be explored.
% CV-ISLR models benefit from ensemble strategies by integrating models trained on different camera angles, thus enhancing generalization across diverse perspectives.
% Our work extends this idea by integrating Ensemble Learning with Video Swin Transformer models, utilizing multi-dimensional ensembles of varying model sizes to effectively capture features from both RGB and depth inputs, thereby improving performance across cross-view recognition tasks.


\subsection{Video Swin Transformer} 
Swin Transformer~\cite{liu2021swin}, initially developed for image classification, has achieved remarkable success in various vision tasks by capturing local and global dependencies through hierarchical feature extraction. Its extension to video, the Video Swin Transformer~\cite{liu2022video} (VST), effectively models spatiotemporal information and excels in tasks such as action recognition~\cite{li2023data} and video segmentation~\cite{shi2024dust}. For CV-ISLR, VST provides significant advantages by jointly modeling spatial and temporal features. Its hierarchical design captures detailed spatial information and dynamic temporal patterns, while its capability to process multi-modal inputs (RGB and depth) ensures robust feature extraction across diverse views and modalities.