\section{Experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[h!]

  \centering
  \caption{Attack success rates (\%) for multi-target attacks against robust models on ImageNet NeurIPS validation set. The perturbation budget $l_{\infty} \leq 16/255$. The results are averaged on 8 different target classes.}
  \vskip 0.15in
  \resizebox{1.0\linewidth}{!}{
  \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \toprule    
    Source   & Method   & $\textrm{Inc-v3}_\textrm{adv}$ & $\textrm{IR-v2}_\textrm{ens}$ & $\textrm{Res50}_\textrm{SIN}$ & $\textrm{Res50}_\textrm{IN}$ & $\textrm{Res50}_\textrm{fine}$ & $\textrm{Res50}_\textrm{Aug}$ & Average\\ 
    \hline
    \multirow{11}[0]{*}{Inc-v3} 
             & MIM      & 0.16     & 0.10     & 0.20     & 0.27     & 0.44     & 0.19 & 0.23\\
             & TI-MIM   & 0.21     & 0.19     & 0.33     & 0.49     & 0.68     & 0.31 & 0.37\\
             & SI-MIM   & 0.13     & 0.19     & 0.26     & 0.43     & 0.63     & 0.29 & 0.32\\
             & DIM      & 0.11     & 0.09     & 0.16     & 0.33     & 0.39     & 0.19 & 0.21\\
             & TI-DIM   & 0.15     & 0.13     & 0.16     & 0.21     & 0.33     & 0.14 & 0.19\\
             & SI-DIM   & 0.19     & 0.21     & 0.43     & 0.71     & 0.84     & 0.46 & 0.47\\
             & Logit    & 0.30     & 0.30     & 0.70     & 1.23     & 3.14     & 0.86 & 1.09\\
             & SU       & 0.49     & 0.41     & 0.84     & 1.75     & 3.55     & 1.04 & 1.35\\
             \cline{2-9}
             & C-GSP    & 20.41    & 18.04    & 6.96     & 33.76    & 44.56    & 21.95 & 24.28\\
             & CGNC     & 24.36    & 22.54    & 8.85     & 40.83    & 52.18    & 22.85 & 28.60\\
             & Ours & \textbf{54.54} & \textbf{55.62} & \textbf{45.86} & \textbf{74.56} & \textbf{78.54} & \textbf{67.56} & \textbf{62.28}\\
             \hline
    \multirow{11}[0]{*}{Res-152}
             & MIM      & 0.19     & 0.15     & 0.28     & 1.58     & 2.75     & 0.78 & 0.96\\
             & TI-MIM   & 0.61     & 0.73     & 0.50     & 2.51     & 4.75     & 1.76 & 1.81\\
             & SI-MIM   & 0.24     & 0.24     & 0.39     & 0.66     & 0.84     & 0.36 & 0.46\\
             & DIM      & 0.63     & 0.37     & 0.94     & 8.50     & 14.22    & 3.77 & 4.74\\
             & TI-DIM   & 0.23     & 0.30     & 0.28     & 0.76     & 1.49     & 0.49 & 0.59\\
             & SI-DIM   & 0.71     & 0.71     & 0.75     & 2.73     & 3.89     & 1.37 & 1.69\\
             & Logit    & 1.15     & 1.18     & 1.65     & 6.70     & 15.46    & 5.93 & 5.34\\
             & SU       & 2.12     & 1.20     & 1.95     & 7.53     & 21.14    & 6.95 & 6.82\\
             \cline{2-9}
             & C-GSP    & 14.60    & 16.01    & 16.84    & 60.30    & 65.51    & 42.88 & 36.02\\
             & CGNC     & 22.21    & 26.71    & 29.83    & 79.80    & 84.05    & 63.75 & 51.06\\
             & Ours & \textbf{44.50} & \textbf{54.09} & \textbf{59.35} & \textbf{83.05} & \textbf{84.28} & \textbf{76.35} & \textbf{66.94}\\
             \bottomrule
    \end{tabular}%

  \label{tab:adv}%
  \end{sc}
  \end{small}}
  \vskip -0.1in
\end{table*}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Experimental Settings}

\paragraph{Dataset.} Following \cite{yang2022boosting,feng2023dynamic,fang2025clip}, we train the model on the ImageNet training set\cite{deng2009imagenet} and evaluate the attack performance using ImageNet-NeurIPS (1k) dataset proposed by NeurIPS 2017 adversarial competition\cite{Nips2017}. 

\paragraph{Victim Models.} We consider various naturally trained models, including Inception-v3 (Inc-v3) \cite{szegedy2016rethinking}, Inception-v4 (Inc-v4) \cite{szegedy2017inception}, Inception-ResNet-v2 (Inc-Res-v2) \cite{szegedy2017inception}, ResNet-152 (Res-152) \cite{he2016identity}, DenseNet-121 (DN-121) \cite{huang2017densely}, GoogleNet \cite{szegedy2015going}, and VGG-16 \cite{simonyan2014very}. 

For further evaluation, we also analyze the performance of our method on robustly trained models. These include adv-Inception-v3 ($\textrm{Inc-v3}_\textrm{adv}$) \cite{goodfellow2014explaining}, ens-adv-Inception-ResNet-v2 ($\textrm{IR-v2}_\textrm{ens}$) \cite{hang2020ensemble}, and several robustly trained ResNet-50 models. The ResNet-50 variants are: $\textrm{Res50}_\textrm{SIN}$ (trained on stylized ImageNet), $\textrm{Res50}_\textrm{IN}$ (trained on a mixture of stylized and Nature ImageNet), $\textrm{Res50}_\textrm{fine}$ (further fine-tuned with an auxiliary dataset \cite{geirhos2018imagenet}), and $\textrm{Res50}_\textrm{Aug}$ (trained with advanced data augmentation techniques from Augmix \cite{hendrycks2019augmix}).

\paragraph{Baseline Methods.} We compare our attack with several attack methods. For instance-specific attacks, we consider MIM \cite{dong2018boosting}, DIM \cite{xie2019improving}, SIM \cite{lin2019nesterov}, DIM \cite{dong2019evading}, Logit \cite{zhao2021success}, and SU \cite{wei2023enhancing}. For instance-agnostic attacks, we consider C-GSP \cite{yang2022boosting}, CGNC \cite{fang2025clip}, GAP \cite{poursaeed2018generative}, CD-AP \cite{naseer2019cross}, TTP \cite{naseer2021generating}, and DGTA-PI\cite{feng2023dynamic}. Among them, C-GSP \cite{yang2022boosting}and CGNC \cite{fang2025clip}are multi-target generative attacks, and the others are single-target generative attacks. For SU attack \cite{wei2023enhancing}, we choose to compare with its best version DTMI-Logit-SU. For CGNC \cite{fang2025clip}, we also consider its single target variant and compare it to a single target attack method.

\paragraph{Implementation Details.} We adopt stable-diffusion \cite{rombach2022high} as our pre-trained diffusion model. We set $\tau = 0.25$ and $N = 6$ for training and testing. The LoRA rank is 16.

Following previous work \cite{yang2022boosting,feng2023dynamic,fang2025clip}, we choose Res-152 and Inc-v3 as substitute models to train our model. The perturbation budget $\epsilon$ is 16/255. We conduct 50k steps of training for multi-target tasks. 
To compare our method with other single-target attacks, we further fine-tune our model for an additional 10k steps to specialize in a single target class(more details provided in Appendix \ref{appendix:single_finetune}).
For multi-target training, we use a learning rate of $2.5e-5$ and a batch size of 8. For single-target fine-tuning, the learning rate is set to $1e-5$ with a batch size of 4.






\subsection{Transferability Evaluation}
We assess the effectiveness of our proposed Dual-Flow for black-box target attacks through a series of experiments. To ensure consistency with previous work \cite{yang2022boosting,feng2023dynamic,fang2025clip}, we select eight distinct target classes \cite{zhang2020understanding} to conduct the target black-box attack testing protocol. We use the average attack success rate (ASR) across 8 target classes as an evaluation metric.




\paragraph{Multi-Target Black-Box Attack.}
We initially conduct attacks on normally trained models to evaluate the performance of multi-target attacks. The results in Table \ref{tab:main} show that our proposed Dual-Flow method exhibits significantly superior transferability, outperforming state-of-the-art instance-specific and instance-agnostic methods. Specifically, our method achieves an average ASR improvement of 21.16\% and 12.36\% over CGNC \cite{fang2025clip} using Inc-v3 and Res-152 as surrogate models, respectively, on black-box models. Notably, instance-specific methods, despite higher success rates in white-box settings, tend to overfit the surrogate models' classification boundaries, resulting in poor performance when transferred to black-box models.




\paragraph{Single-Target Black-Box Attack.}

To further evaluate our method's effectiveness, we compare it with other state-of-the-art instance-agnostic single-target attacks. Multi-target attacks are inherently more challenging than single-target ones, disadvantaging our model in such comparisons. To ensure fairness, we applied a masked fine-tuning technique similar to CGNC \cite{fang2025clip}, allowing us to fine-tune our model separately for each target class and create single-target variants.

The results in Table \ref{tab:single} show that after fine-tuning, Dual-Flow$^{\dagger}$ achieves higher attack success rates and generally outperforms leading single-target methods. Notably, our method excels in average black-box attack capability even without individual fine-tuning for the eight target classes. This demonstrates our approach's significant capacity and effectiveness in multi-target attacks, reducing the need for separate models for each target class in resource-constrained scenarios.
 

%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=.99\linewidth]{images/generated_examples.pdf}}
\caption{Visualization results of different input images targeting various classes. For each text prompt of the target class, the left column displays the adversarial examples generated before clipping, the middle column shows the adversarial examples after clipping, and the right column presents the corresponding adversarial perturbations, which represent the differences between the clipped adversarial examples and the original images. Note that the perturbations are scaled to a range between 0 and 1. The surrogate model used is Inc-v3.}

\label{fig:visual}
\end{center}
\vskip -0.2in
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Attack Under Defense Strategies}

To demonstrate the robustness of our proposed Dual-Flow, we evaluate its performance against several widely used defense mechanisms.


\begin{figure}[ht]
    \vskip 0.2in
    \begin{center}
        \begin{minipage}{0.49\linewidth}
            \centering
            \includegraphics[width=\linewidth]{images/jpeg_compression_bold.pdf}
            \subcaption{JPEG compression}
        \end{minipage}
        \hfill
        \begin{minipage}{0.49\linewidth}
            \centering
            \includegraphics[width=\linewidth]{images/smoothing_bold.pdf}
            \subcaption{Input smoothing}
        \end{minipage}
    \end{center}
    \vskip -0.1in
    \caption{A comparison of CGNC and our method regarding attack success rates against various input processing defense strategies. The results against JPEG compression are shown in (a), while (b) presents the outcomes against different input smoothing methods. Inc-v3 is used as the surrogate model in this evaluation, and Inc-Res-v2, along with DN-121, are the target models.}
    \label{input_process}
\end{figure}



\paragraph{Robustly Trained Networks.}

We first consider attacking six robustly trained networks, with results in Table \ref{tab:adv}. Attacking robustness-augmented models is challenging, as previous methods see a significant drop in success rates. However, our approach consistently misleads black-box classifiers into predicting the specified classes, showing marked improvement over earlier multi-target methods. Notably, using Inc-v3 as the surrogate model, the average attack success rate against the six robust models increases significantly from 28.60\% to 62.28\%, highlighting our method's effectiveness.




\paragraph{Input Process Defense.}

We compared our method's performance with the state-of-the-art multi-target attack method CGNC against input preprocessing defenses, such as image smoothing \cite{ding2019advertorch} and JPEG compression \cite{dziugaite2016study}. As shown in Figure \ref{input_process}, our method consistently outperforms CGNC under these defenses. For example, using Inc-v3 as the surrogate model and DN-121 as the target model, our method achieves a 52.99\% success rate under Gaussian smoothing, compared to CGNC's 28.27\%. This highlights the superior effectiveness of our approach in overcoming input preprocessing defenses.




\subsection{Visualization}

To gain a deeper understanding of the effectiveness of our method, we visualized both the unclipped and clipped samples generated by our approach. Additionally, for consistency with other perturbation-based attack methods, we visualized the equivalent adversarial perturbations, defined as the pixel differences between the clipped samples and the clean images. As illustrated in Figure \ref{fig:visual}, our method first transforms the original image into one that maintains a similar layout and color scheme but becomes semantically closer to the target class. This transformed image is subsequently clipped to ensure its pixel differences from the original image remain within the epsilon bound. Visually, it is evident that the clipped image retains substantial semantic features of the target class. Notably, the adversarial perturbations also exhibit distinct semantic patterns aligned with the target class, further validating the effectiveness of our approach.



\subsection{Ablation Study}



To further validate the effectiveness of our chosen Cascading ODE, we conducted a series of ablation experiments in this section. Here, Dual-Flow-co represents the original method, Dual-Flow-cs denotes the Cascading SDE variant, and Dual-Flow-rs denotes the Random SDE variant. During the reverse process at inference time, these variants employ either the DDPM scheduler or the DDIM scheduler. As shown in Table \ref{tab:ode_vs_sde}, our method significantly outperforms the other variants in white-box and black-box transfer attacks. 


\begin{table}[h!]
  \caption{The multi-target attack success rates of several variants of our method. The surrogate model used is Res-152. The white-box attack success rate refers to the performance on Res-152, while the black-box attack success rate represents the average performance across six black-box models.}
  \label{tab:ode_vs_sde}%
  \centering
  \vskip 0.15in
  \begin{small}
  \begin{sc}
    {
      \begin{tabular}{c|c|c}  % 列居中
      \toprule
      Method & \multicolumn{1}{c|}{White-box} & \multicolumn{1}{c}{Black-box} \\
      \hline
      Dual-Flow-co & 92.39  & 70.76\\
      Dual-Flow-cs + DDIM & 68.56  & 40.12 \\
      Dual-Flow-cs + DDPM & 74.58  & 46.04 \\
      Dual-Flow-rs + DDIM & 55.39  & 33.00 \\
      Dual-Flow-rs + DDPM & 29.19  & 14.86 \\
      \bottomrule
      \end{tabular}
    }
  \end{sc}
  \end{small}
  \vskip -0.1in
\end{table}%

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
{\includegraphics[width=0.80\linewidth]{images/ablation.pdf}}
\end{center}
\vskip -0.1in
\caption{The multi-target black-box attack success rates of several variants of our method. The surrogate model used is Res-152.}
\label{more_ablation}
\end{figure}


Furthermore, we compared the impact of different sampling steps $N$ during the reverse process. As shown in Figure \ref{more_ablation}, increasing the sampling steps steadily increases success rates for both Dual-Flow-co and Dual-Flow-cs. However, for Dual-Flow-rs, the success rate quickly declines as the inference steps increase, supporting our analyses in Section \ref{subsection:df_vs_sf} and Figure \ref{fig:model_comp}.
