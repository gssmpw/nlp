\section{Preliminary}
\subsection{Instance-Agnostic Attacks}

Instance-agnostic attacks~\cite{luo2021generating,kong2020physgan,wang2019gan,poursaeed2018generative,yang2022boosting,naseer2021generating,naseer2019cross} learn perturbations based on data distributions rather than individual instances. These approaches, employing universal adversarial perturbations\cite{moosavi2017universal, zhang2020understanding} or generative models, have demonstrated superior transferability. This paper primarily focuses on the latter due to its greater flexibility and attack effectiveness.

Early generative model-based methods were primarily single-target attacks\cite{xiao2018generating, naseer2019cross, naseer2021generating, feng2023dynamic, wang2023towards}, requiring a separate model to be trained for each target class. Although these models exhibited high attack capabilities, the excessive training overhead limited their applicability when many target classes needed to be attacked. Recent research has proposed several multi-target attack methods\cite{yang2022boosting, fang2025clip} that condition the perturbation generative model on class labels\cite{yang2022boosting} or text embeddings\cite{fang2025clip} of classes. These approaches allow a single model to be trained to attack multiple target classes, significantly reducing the training overhead.


Consider a white-box image classifier characterized by the parameter $\theta$, denoted as $f: \mathcal{X} \to \mathcal{Y}$, 
where the input space $\mathcal{X} \subset \mathbb{R}^{C \times H \times W}$ corresponds to the image domain, and the output space 
$\mathcal{Y} \subset \mathbb{R}^{L}$ represents the confidence scores across various classes. Here, $L$ denotes the total number of classes. Given an original image $\mathbf{x} \in \mathcal{X}$ and a target class 
$c \in \mathcal{C}$, the goal of transferable multi-target generative attack is to generate the perturbation
$\boldsymbol{\delta} = G(\mathbf{x}, c)$ and the pertubed image $\boldsymbol{\mathbf{x^{\epsilon}}} = \mathbf{x} + \boldsymbol{\delta}$, in such a way that an unseen victim classifier $F$ predicts $c$ for the perturbed image, 
\textit{i.e.}, $\arg\max_{i \in \mathcal{C}} F(\mathbf{x}^\epsilon)_{i} = c$. Here $G$ is the generator trained on the known model $f_{\theta}$. To ensure that the manipulated images remain visually 
indistinguishable from the originals, the perturbation is constrained using the $l_{\infty}$ norm such that $\|\boldsymbol{\mathbf{x} - \mathbf{x}^\epsilon}\|_{\infty} = \| \boldsymbol{\delta}\|_{\infty}< \epsilon$. 


\subsection{Diffusion Models and Flow-based Models}
% Diffusion Modelçš„related works 
Diffusion models~\cite{sohl2015deep,ho2020denoising,song2020denoising,song2020score} have emerged as powerful generative models, particularly for continuous data such as audio~\cite{huang2023make} and images~\cite{rombach2022high}. Recently, Flow-based generative models~\cite{lipmanflow,esser2024scaling,liuflow}, developed directly from ordinary differential equations, have also gained significant momentum. Given their strong generative capabilities, exploring their applications in adversarial attacks is natural.

\paragraph{Sampling Algorithms.}
One of the most appealing aspects of diffusion models is the flexibility in designing the sampling process. The generation process of diffusion models primarily follows two formulations: one based on the Stochastic Differential Equation (SDE) and the other on the Ordinary Differential Equation (ODE). Each approach has strengths and weaknesses, making them suitable for different scenarios.

\paragraph{Diffusion Models in Adversarial Attack.} Currently, diffusion models have found some applications in adversarial attacks. Some utilize diffusion models to create unrestricted adversarial examples\cite{chen2023advdiffuser, dai2025advdiff}, while others perform instance-specific attacks\cite{xue2023diffusion, chen2024diffusion}. However, they all rely on iterative optimization using the target model's gradients while generating adversarial examples, thus not qualifying as instance-agnostic attacks.