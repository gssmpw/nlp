\section{Dual-Flow for Adversarial Attack}


We propose a Dual-Flow pipeline designed to transform an image $\mathbf{x} \in \mathcal{X}$ through a perturbed distribution $\mathcal{X}_\tau$ and ultimately into a constrained output space $\mathcal{X}^\epsilon$, which is $\{\mathbf{x}^\epsilon | \exists \mathbf{x}\in \mathcal{X}, \|\mathbf{x^\epsilon - \mathbf{x}}\|_\infty < \epsilon\}$. By construction, $\mathcal{X}^\epsilon$ enforces a maximum $\ell_\infty$ perturbation of $\epsilon$. Unlike approaches that rely on guidance from a victim model, our inference process is entirely model-agnostic.

Specifically, we leverage the original ODE-based diffusion flow to map $\mathcal{X}$ to $\mathcal{X}_\tau$. Using a pretrained diffusion model's velocity function $\mathbf{v}_\phi(\cdot,\cdot)$ and a given input image $\mathbf{x} \sim \mathcal{X}$, we select a fixed timestep $\tau \in (0,1)$. The perturbed image $\mathbf{x}_\tau \sim \mathcal{X}_\tau$ is obtained by integrating the following equation:
\begin{equation}
    \frac{\partial}{\partial t}\Phi(\mathbf{x},t) = \mathbf{v}_\phi(\Phi(\mathbf{x},t), t), \quad \Phi(\mathbf{x}, 0) \sim \mathcal{X},
\label{forward ode}
\end{equation}
from $t = 0$ to $t = \tau$. 

To further map $\mathcal{X}_\tau$ to $\mathcal{X}^\epsilon$, we fine-tune a LoRA-based score function\cite{hu2021lora}, yielding a new velocity function $\mathbf{v}_\theta$. Then by integrating another equation:
\begin{equation}
    \frac{\partial}{\partial t}\Psi(\mathbf{x},t) = \mathbf{v}_\theta(\Psi(\mathbf{x},t), t), \quad \Psi(\mathbf{x}, \tau) \sim \mathcal{X}_\tau,
\label{reverse ode}
\end{equation}
from $t = \tau$ to $t = 0$. 

To train this second flow, we introduce a novel \emph{Cascading Distribution Shift Training} strategy, which addresses the challenges posed by the inaccessibility of intermediate distributions during training.

Finally, to ensure that outputs remain within $\mathcal{X}^\epsilon$, we apply dynamic gradient stops during training, coupled with hard clipping operations at the final timestep. This approach allows for richer intermediate representations while maintaining the required perturbation bounds.

Details of our approach are provided in the following subsections.
\subsection{A Construction of Better Extend Flow}

Firstly, we construct an extended flow based on $j$, which is the negative value of the cross-entropy function: 

\begin{equation}
\begin{split}
    j = -\mathbb{CE}(f(\mathbf{x})), c),
\end{split}
\label{eq:target}
\end{equation}
where $f$ is the classifier and $c$ is the target label.
\begin{proposition}[Morse Flow Construction]
\label{morse flow construction}
    Under mild assumptions on the $\mathcal{X}^\epsilon$ and the function $j$, there exists $\epsilon > 0$, a unique smooth flow.
    \begin{equation}
        \Phi: \mathcal{X}^\epsilon \times [0, \epsilon] \to \mathcal{X}^\epsilon,
    \end{equation}
satisfying:
\begin{equation}
\begin{aligned}
\frac{d}{dt} \Phi(\mathbf{x}, t) &= \mathbf{v}(\Phi(\mathbf{x},t)), \\
\Phi(\mathbf{x}, 0) &= \mathbf{x},
\end{aligned}
\end{equation}
such that:
\begin{enumerate}
\item $\mathbf{v} = \alpha(\mathbf{x})\nabla_\mathbf{\mathbf{X}}j(\mathbf{x})$ almost everywhere
\item \( j(\Phi(\mathbf{x},\epsilon)) \geq j(\mathbf{x}) \) for all \( x \in \mathcal{X}^\epsilon \), and $>$ holds almost everywhere if $j$ is not trivial
\item Each \( \Phi(\cdot, t): \mathcal{X}^\epsilon \to \mathcal{X}^\epsilon \) is a diffeomorphism
\end{enumerate}
\end{proposition}

A detailed proof is provided in Appendix~\ref{proof morse flow construction}. Proposition~\ref{morse flow construction} indicates we can find better extend flow to hack $j$ by $\nabla j$. In the following paragraphs, we will construct a concrete algorithm to realize it along with an existing flow.

\subsection{Cascading Distribution Shift Training}
\label{l2_v}

\begin{algorithm}[tb]
   \caption{Cascading Distribution Shift Training}
   \label{alg:training}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\tau = N\delta$, stepsize $\delta$, model param. $\phi$, $\theta$, victim model $f$, target labels set $\mathcal{C}$, training dataset $\{\mathbf{I}^i\}_{i\in \mathcal{I}}$, learning rate $l_r$
   \STATE Initialize $\theta = \phi$.
   \REPEAT
   \FOR{$i\in \mathcal{I}$}
   \STATE get $\mathbf{x}_0 = \mathbf{I}^i$
   \STATE sample $c \sim \mathcal{C}$
   \FOR{$t = 1$ {\bfseries to} $N$}
   \STATE $\mathbf{x}_{t\delta} = \mathbf{x}_{(t-1)\delta} + \mathbf{v}_\phi(\mathbf{x}_{(t-1)\delta}, (t-1)\delta, \varnothing) \delta$
   \ENDFOR
   \FOR{$t = N$ {\bfseries to} $1$}
   \STATE $\mathbf{x}_{(t-1)\delta} = \mathbf{x}_{t\delta} - \mathbf{v}_\theta(\mathbf{x}_{t\delta}, t\delta, c) \delta$
   \STATE $\widehat{\mathbf{x}_0} = \mathbf{x}_{t\delta} - \mathbf{v}_\theta(\mathbf{x}_{t\delta}, t\delta, c) t \delta$
   \STATE $\widehat{\mathbf{x}_0}^{i,j,k} = \operatorname{clip}\left( \widehat{\mathbf{x}_0}^{i,j,k}, \mathbf{x}^{i,j,k} - \epsilon, \mathbf{x}^{i,j,k} + \epsilon \right)$
   \STATE $\theta = \theta - l_r \cdot \nabla_\theta(\mathbb{CE}(f(\widehat{\mathbf{x}_0}), c))$
   \ENDFOR
   \ENDFOR
   \UNTIL{$\mathbf{v}_\theta$ convergence}
   \STATE {\bfseries Return:} Dual-Flow $\{\mathbf{v}_\phi,\mathbf{v}_\theta\}$
\end{algorithmic}
\end{algorithm}

Although the \textit{in-the-wild} ODE trajectory is deterministic, obtaining exact intermediate samples remains a significant challenge, which poses difficulties for our approach. To address this issue, we propose the \textbf{Cascading Distribution Shift Training Algorithm}, specifically designed to enhance adversarial attack efficacy through two key mechanisms: 
(1) Enforcing a cascading hacking effect, where each perturbation step incrementally contributes to misleading the victim model.  
(2) Ensuring the final perturbation follows the prescribed $\ell_\infty$ constraint.  
Further details are provided in Algorithm~\ref{alg:training}, where $l$ denotes the conditional input in real diffusion models. 

Our proposed training framework not only circumvents the challenge of inaccessible intermediate timesteps but also offers additional advantages. Proposition~\ref{cascading_improvement} formalizes that our algorithm progressively refines a coarse-to-fine representation, thus effectively leveraging information from ahead timesteps.
More concretely, due to the continuity of the ODE, our training algorithm enables a cascading optimization within an increasingly refined space. 

\begin{proposition}[Cascading Improvement at Adjoint Timesteps]
\label{cascading_improvement}

Consider two consecutive timesteps  $t, t - \delta$. Following Algorithm~\ref{alg:training}, when comparing the cases with and without updating  $\theta$  at  $t$, updating  $\theta$  results in an equal or lower cross-entropy for  $\widehat{\mathbf{x}_0}$ at $t-\delta$ when  $\delta$  is sufficiently small and all functions are smooth.
\end{proposition}

One crucial consideration is constraining the final result within $\mathcal{X}^\epsilon$. There are two primary methods to achieve this. The first approach incorporates the original ODE trajectory during model tuning, ensuring the output remains close to the original ODE flow. The second approach enforces the $\ell_\infty$ constraint or applies gradient clipping to suppress the influence of out-of-range image regions, guaranteeing that only in-range rewards contribute to model optimization. Our experiments show that dynamic gradient clipping yields the best performance among these methods.


Given the fixed model capacity, the Cascading Distribution Shift Training algorithm ensures greater consistency between the training and sampling processes, improving performance. This is visually illustrated in Figure~\ref{fig:model_comp}.

\subsection{Dual-Flow Sampling}

During the sampling process, following the proposed training algorithm, a given image $\mathbf{x} \in \mathcal{X}$ is first mapped to $\mathbf{x}_\tau$ via Eq.~\eqref{forward ode}. Subsequently, it is transformed into an intermediate state $\mathbf{x}'_{0}$ using Eq.~\eqref{reverse ode}. Finally, a hard truncation is applied to obtain the qualified perturbed sample $\mathbf{x}_0 \in \mathcal{X}^\epsilon$.

\begin{algorithm}[tb]
   \caption{Dual-Flow Sampling}
   \label{alg:sampling}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\tau = N\delta$, stepsize $\delta$, image $\mathbf{I}$,  target label $c$ Dual-Flow $\{\mathbf{v}_\phi, \mathbf{v}_\theta \}$
   \STATE $\mathbf{x}=\mathbf{I}$.
   \FOR{$t = 1$ {\bfseries to} $N$}
   \STATE $\mathbf{x}_{t\delta} = \mathbf{x}_{(t-1)\delta} + \mathbf{v}_\phi(\mathbf{x}_{(t-1)\delta}, (t-1)\delta, \varnothing) \delta$
   \ENDFOR
   \FOR{$t = N$ {\bfseries to} $1$}
   \STATE $\mathbf{x}_{(t-1)\delta} = \mathbf{x}_{t\delta} - \mathbf{v}_\theta(\mathbf{x}_{t\delta}, t\delta, c) \delta$
   \ENDFOR
   \STATE $\mathbf{x}_0^{i,j,k} = \operatorname{clip}\left( \mathbf{x}_0^{i,j,k}, \mathbf{x}^{i,j,k} - \epsilon, \mathbf{x}^{i,j,k} + \epsilon \right)$
   \STATE {\bfseries Return:} $\mathbf{x}_0$
\end{algorithmic}
\end{algorithm}

\begin{table*}[h!]
  \centering
  \caption{Attack success rates (\%) for multi-target attacks on normally trained models using the ImageNet NeurIPS validation set. The perturbation budget is constrained to $l_{\infty} \leq 16/255$. * indicates white-box attacks. The results are averaged across 8 different target classes, and the overall average on the far right is computed solely for black-box attacks.}
 
  \vskip 0.15in
  \resizebox{1.0\linewidth}{!}{
  \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule 
    Source   & Method   & Inc-v3   & Inc-v4   & Inc-Res-v2    & Res-152  & DN-121   & GoogleNet       & VGG-16 & Average\\
    \hline
    \multirow{11}[0]{*}{Inc-v3} 
             & MIM      & 99.90$^*$ & 0.80     & 1.00     & 0.40     & 0.20    & 0.20     & 0.30    & 0.48\\
             & TI-MIM   & 98.50$^*$    & 0.50     & 0.50     & 0.30     & 0.20     & 0.40     & 0.40  & 0.38\\
             & SI-MIM   & 99.80$^*$    & 1.50     & 2.00     & 0.80     & 0.70     & 0.70     & 0.50  & 1.03\\
             & DIM      & 95.60$^*$    & 2.70     & 0.50     & 0.80     & 1.10     & 0.40     & 0.80  & 1.05\\
             & TI-DIM   & 96.00$^*$    & 1.10     & 1.20     & 0.50     & 0.50     & 0.50     & 0.80  & 0.77\\
             & SI-DIM   & 90.20$^*$    & 3.80     & 4.40     & 2.00     & 2.20     & 1.70     & 1.40  & 2.58\\
             & Logit    & 99.60$^*$    & 5.60     & 6.50     & 1.70     & 3.00     & 0.80     & 1.50  & 3.18\\
             & SU       & 99.59$^*$    & 5.80     & 7.00     & 3.35     & 3.50     & 2.00     & 3.94  & 4.26\\
             \cline{2-10}
             & C-GSP    & 93.40$^*$    & 66.90    & 66.60    & 41.60    & 46.40    & 40.00    & 45.00 & 51.08\\
             & CGNC     & 96.03$^*$    & 59.43    & 48.06    & 42.48    & 62.98    & 51.33    & 52.54 & 52.80\\
             & Dual-Flow& 90.08$^*$& \textbf{77.19}&\textbf{66.76}&\textbf{77.06}&\textbf{82.64}&\textbf{73.01}&\textbf{67.09}&\textbf{73.96}\\
             \hline
    \multirow{11}[0]{*}{Res-152} 
             & MIM      & 0.50     & 0.40     & 0.60     & 99.70$^*$ & 0.30     & 0.30     & 0.20   & 0.38\\
             & TI-MIM   & 0.30     & 0.30     & 0.30     & 96.50$^*$    & 0.30     & 0.40     & 0.30 & 0.32\\
             & SI-MIM   & 1.30     & 1.20     & 1.60     & 99.50$^*$    & 1.00     & 1.40     & 0.70 & 1.20\\
             & DIM      & 2.30     & 2.20     & 3.00     & 92.30$^*$    & 0.20     & 0.80     & 0.70 & 1.53\\
             & TI-DIM   & 0.80     & 0.70     & 1.00     & 90.60$^*$    & 0.60     & 0.80     & 0.50 & 0.73\\
             & SI-DIM   & 4.20     & 4.80     & 5.40     & 90.50$^*$    & 4.20     & 3.60     & 2.00 & 4.03\\
             & Logit    & 10.10    & 10.70    & 12.80    & 95.70$^*$    & 12.70    & 3.70     & 9.20 & 9.87\\
             & SU       & 12.36    & 11.31    & 16.16    & 95.08$^*$    & 16.13    & 6.55    & 14.28 & 12.80\\
             \cline{2-10}
             & C-GSP    & 37.70    & 47.60    & 45.10    & 93.20$^*$    & 64.20    & 41.70    & 45.90 & 47.03\\
             & CGNC     & 53.39    & 51.53    & 34.24    & 95.85$^*$    & 85.66    & 62.23    & 63.36 & 58.40\\
             & Dual-Flow&\textbf{69.58}&\textbf{71.92}&\textbf{56.10}&92.39$^*$&\textbf{85.73}&\textbf{73.65}&\textbf{67.59}&\textbf{70.76}\\
             \bottomrule
    \end{tabular}%
  \label{tab:main}%
  \end{sc}
  \end{small}}
    \vskip -0.1in
\end{table*}%

\subsection{Deterministic Flow vs. Stochastic Flow}
\label{subsection:df_vs_sf}

An important consideration is the choice between a deterministic flow, modeled by an ODE, and a stochastic flow, modeled by an SDE. This decision primarily depends on the second flow, as the first is inherited from a pretrained diffusion model.

When translating the distribution $\mathcal{X}^\tau$ to $\mathcal{X}^\epsilon$, a simple rescaling and noise injection into $\mathcal{X}^\epsilon$ is insufficient to fully recover $\mathcal{X}^\tau$. Consequently, this transformation falls outside the standard diffusion-based framework.

A natural solution is to construct an \textit{in-the-wild} ODE (as we do) or SDE that maps $\mathcal{X}^\tau$ to $\mathcal{X}^\epsilon$. In the ODE formulation, we define a velocity function $\mathbf{v}_\theta$ that satisfies Eq.~\eqref{reverse ode}. A similar approach applies to the SDE case, where stochastic noise facilitates the distribution transition.

When comparing from the perspectives of randomness and determinism, as shown in Figure~\ref{fig:model_comp}, we label our framework as \textbf{Cascading ODE} and implement two SDE-based training algorithms. One injects noise at a random timestep within $(0, \tau)$, labeled as \textbf{Random SDE}, while the other first directly adds noise at $\tau$ and then reverses the flow using DDPM, resulting in a weak cascading relationship, labeled as \textbf{Cascading SDE}. While SDE-based training algorithms more closely resemble original diffusion models, they present two key challenges.

First, \textbf{Cascading SDE} introduces a random term, which may make it more difficult to construct the Cascading Improvement relationship as stated in Proposition~\ref{cascading_improvement} for \textbf{Cascading ODE}.
Second, sampling with SDEs tends to produce unstable results, where larger step sizes exacerbate accumulated errors, further impacting reliability, as demonstrated in our experiments.

As for \textbf{Random SDE}, it exhibits the worst performance because when sampling $\mathbf{x}_t$ by directly adding noise, the distribution of $\mathbf{x}_t$ remains unchanged. Consequently, even slight training of the reverse flow leads to a distribution mismatch, as illustrated by the star in the last column of Figure~\ref{fig:model_comp}.

\begin{table*}[h!]
  \centering
  \caption{Attack success rates (\%) for single-target attacks against normally trained models on ImageNet NeurIPS validation set. Note that CGNC$^{\dagger}$ and Dual-Flow$^{\dagger}$ denote the single-target variants of CGNC and our proposed Dual-Flow, respectively. The perturbation budget is constrained to $l_{\infty} \leq 16/255$. * indicates white-box attacks. The results are averaged across 8 different target classes, and the overall average on the far right is computed solely for black-box attacks.}
  \vskip 0.15in
  \resizebox{1.0\linewidth}{!}{
  \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule
    Source   & Method   & Inc-v3   & Inc-v4   & Inc-Res-v2 & Res-152   & DN-121   & GoogleNet & VGG-16 & Average\\
    \hline
    \multirow{7}[0]{*}{Inc-v3} 
             & GAP      & 86.90$^{*}$    & 45.06    & 34.48    & 34.48    & 41.74    & 26.89    & 34.34 & 36.16\\
             & CD-AP             & 94.20$^{*}$    & 57.60    & 60.10    & 37.10    & 41.60    & 32.30    & 41.70 & 45.07\\
             & TTP               & 91.37$^{*}$    & 46.04    & 39.37    & 16.40    & 33.47    & 25.80    & 25.73 & 31.14\\
             & DGTA-PI           & 94.63$^{*}$    & 67.95    & 55.03    & 50.50    & 47.38    & 47.67    & 48.11 & 52.77\\
             & CGNC$^{\dagger}$  & 98.84$^{*}$    & 74.76    & 64.48    & 62.00    & 78.94    & 69.06    & 70.74 & 70.00\\
             & Dual-Flow             & 90.08$^{*}$ & 77.19 & 66.76 & 77.06 & 82.64 & 73.01 & 67.09 & 73.96\\
             & Dual-Flow$^{\dagger}$ & 91.41$^{*}$ & \textbf{78.85} & \textbf{70.59} & \textbf{79.12} & \textbf{83.36} & \textbf{77.52} & \textbf{71.29} & \textbf{76.79}\\
             \hline
    \multirow{7}[0]{*}{Res-152} 
             & GAP      & 30.99    & 31.43    & 20.48    & 84.86$^{*}$    & 58.35    & 29.89    & 39.70 & 35.14\\
             & CD-AP             & 33.30    & 43.70    & 42.70    & 96.60$^{*}$    & 53.80    & 36.60    & 34.10 & 40.70\\
             & TTP               & 62.03    & 49.20    & 38.70    & 95.12$^{*}$    & 82.96    & 65.09    & 62.82 & 60.13\\
             & DGTA-PI           & 66.83    & 53.62    & 47.61    & 96.48$^{*}$    & 86.61    & 68.29    & 69.58 & 65.42\\
             & CGNC$^{\dagger}$  & 68.86    & 69.45    & 45.71    & 98.61$^{*}$    & \textbf{91.14}    & 69.83    & 68.05 & 68.84\\
             & Dual-Flow             & 69.58 & 71.92 & 56.10 & 92.39$^{*}$ & 85.73 & 73.65 & 67.59 & 70.76\\
             & Dual-Flow$^{\dagger}$ & \textbf{72.25} & \textbf{74.35} & \textbf{58.44} & 93.65$^{*}$ & 87.61 & \textbf{75.45} & \textbf{71.11} & \textbf{76.12}\\
             \bottomrule
    \end{tabular}%

  \label{tab:single}%
  \end{sc}
  \end{small}}
  \vskip -0.1in
\end{table*}%
