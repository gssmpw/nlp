\section{Introduction}
\label{sec:intro}

Deep neural networks (DNNs) are highly vulnerable to adversarial attacks \cite{szegedy2013intriguing, goodfellow2014explaining, carlini2017towards, gao2024adversarial}, which can significantly compromise their reliability. Among these, targeted black-box attacks—where adversaries manipulate a model into misclassifying an input as a specific target class without direct access to the model—are the most challenging and impactful \cite{andriushchenko2020square, athalye2018synthesizing, luo2021generating}.

Adversarial attacks can be classified into instance-specific and instance-agnostic approaches. Instance-specific attacks \cite{dong2018boosting, xiong2022stochastic, eykholt2018robust} optimize perturbations for each input using surrogate model gradients but often suffer from poor transferability. Instance-agnostic attacks \cite{xiao2018generating, naseer2019cross, yang2022boosting, fang2025clip} generalize perturbations over the dataset, leading to stronger black-box transferability. These methods typically rely on universal adversarial perturbations \cite{moosavi2017universal, zhang2020understanding} or generative models \cite{poursaeed2018generative, naseer2021generating}.

Generative model-based attacks can be further divided into single-target \cite{naseer2019cross, feng2023dynamic, wang2023towards} and multi-target \cite{yang2022boosting, fang2025clip} approaches. While single-target attacks achieve high success rates, they require training a separate model per target class, making them impractical for large-scale attacks. Multi-target attacks address this by conditioning a single generator on target labels but often suffer from reduced transferability and weak robustness against adversarial defenses.

Diffusion models \cite{ho2020denoising, song2020denoising, rombach2022high} offer strong generative capabilities, making them a promising tool for adversarial attacks. However, existing diffusion-based attacks rely on iterative optimization using the victim model’s gradients, preventing them from being truly instance-agnostic. Moreover, choosing between stochastic and deterministic sampling methods for adversarial perturbation generation remains an open challenge.
%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{images/model_comp.pdf}}
\caption{The comparison between Cascading ODE, Cascading SDE, and Random SDE for the second flow. The star shape represents the input for training the reverse flow. Notably, the Random SDE is observed to optimize in an incorrect distribution.}

\label{fig:model_comp}
\end{center}
\vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%
To address these challenges, we propose a novel \textbf{Dual-Flow framework} for multi-target instance-agnostic adversarial attacks. Our approach integrates (1) a pretrained diffusion model to generate an intermediate perturbation distribution as the forward flow and (2) a fine-tuned lightweight LoRA-based velocity function as reverse flow for targeted adversarial refinement. We introduce \textbf{Cascading Distribution Shift Training} to improve attack capability and employ \textbf{dynamic gradient clipping} to enforce the $\ell_\infty$ constraint. 

As illustrated by the Cascading ODE and Cascading SDE in Figure~\ref{fig:model_comp}, we first follow the black trajectory to introduce a slight perturbation to the image. Then, we follow either the red or blue trajectory to generate an altered image, effectively exploiting this process to attack the target model.
Our main contributions are:
\begin{itemize}[leftmargin=*]
    \item \textbf{First application of flow-based ODE velocity training for adversarial attacks}, extending diffusion-based techniques beyond conventional score function training.
    \item \textbf{Dual-Flow algorithm}, integrating a pretrained diffusion-based forward ODE with a fine-tuned adversarial velocity function for structured perturbation generation.
 \item \textbf{Theoretical contribution on cascading improvement mechanism}, demonstrating how our method facilitates cascading improvements at later timesteps.
\end{itemize}
Extensive experiments demonstrate that our attack method achieves state-of-the-art black-box transferability in multi-target scenarios and exhibits high robustness against defense mechanisms.