\newpage
\appendix
\onecolumn

\section{Proofs}
\subsection{Proof of Morse Flow Construction}
\label{proof morse flow construction}
\begin{proposition}[Morse Flow Construction]
\label{thm:main-generalized}
Let \( B \subset \mathbb{R}^n \) be a bounded open set with smooth boundary, and let \( j: B \to \mathbb{R} \) be a smooth Morse function that extends to \( C^\infty(\overline{B}) \). There exists \( \varepsilon > 0 \), a smooth vector field \( X \in \mathfrak{X}(B) \), and a unique smooth flow
\[
\Phi: B \times [0,\varepsilon] \to B
\]
satisfying:
\begin{align*}
\frac{d}{dt} \Phi(x,t) &= X(\Phi(x,t)), \\
\Phi(x,0) &= x,
\end{align*}
such that:
\begin{enumerate}
\item \( j(\Phi(x,\varepsilon)) \geq j(x) \) for all \( x \in B \)
\item Each \( \Phi(\cdot,t): B \to B \) is a diffeomorphism
\item Trajectories remain bounded away from \( \partial B \) for \( t \in [0,\varepsilon] \)
\end{enumerate}
\end{proposition}

\begin{proof}[Constructive Proof]
We proceed through coordinated geometric and analytic constructions.

\textbf{Step 1: Geometric Preparations}

\begin{enumerate}
\item \textit{Smooth Defining Function}: By the smooth boundary assumption, there exists \( \mu \in C^\infty(\overline{B}, [0,\infty)) \) with:
\begin{itemize}
\item \( \mu^{-1}(0) = \partial B \)
\item \( \nabla \mu(x) \neq 0 \) for \( x \in \partial B \)
\item \( \mu(x) \sim \text{dist}(x, \partial B) \) near \( \partial B \)
\end{itemize}
For explicit construction, take \( \mu(x) = f(\text{dist}(x, \partial B)) \) where \( f \in C^\infty([0,\infty)) \) satisfies \( f(r) = r \) near 0.

\item \textit{Critical Point Isolation}: Since \( j \) is Morse on compact \( \overline{B} \), its critical points \( \mathscr{C}(j) = \{p_1,\ldots,p_N\} \) are finite and non-degenerate. Choose pairwise disjoint neighborhoods \( U_i \ni p_i \) with:
\[
\overline{U_i} \subset B \setminus \partial B \quad \text{and} \quad \overline{U_i} \cap \mathscr{C}(j) = \{p_i\}
\]
\end{enumerate}

\textbf{Step 2: Vector Field Construction}

\begin{enumerate}
\item \textit{Partition of Unity}: Let \( \{\rho_i\}_{i=1}^N \) be smooth functions with:
\[
\text{supp}(\rho_i) \subset U_i, \quad 0 \leq \rho_i \leq 1, \quad \sum_{i=1}^N \rho_i \leq 1
\]
Define the cutoff function:
\[
\eta(x) := 1 - \sum_{i=1}^N \rho_i(x)
\]
Note \( \eta \equiv 0 \) near critical points and \( \eta \equiv 1 \) outside \( \bigcup U_i \).

\item \textit{Decay Modulation}: Fix \( m \geq n+1 \). Define the boundary decay factor:
\[
\mu_m(x) := \mu(x)^m
\]
This ensures sufficient regularity at \( \partial B \).

\item \textit{Synthesized Vector Field}: Define
\[
X(x) := \eta(x)\mu_m(x)\nabla j(x)
\]
This field vanishes at critical points and near \( \partial B \).
\end{enumerate}

\textbf{Step 3: Flow Analysis}

\textit{Boundary Avoidance}: For \( x \in B \), let \( r(t) = \mu(\Phi(x,t)) \). Compute:
\[
\frac{dr}{dt} = \nabla \mu(\Phi) \cdot X(\Phi) = \eta(\Phi)\mu_m(\Phi)\nabla \mu(\Phi) \cdot \nabla j(\Phi)
\]
    Using \( |\nabla \mu \cdot \nabla j| \leq C \) near \( \partial B \):
\[
\left|\frac{dr}{dt}\right| \leq C\eta(\Phi)\mu(\Phi)^{m+1} \leq Cr(t)^{m+1}
\]
Solutions to \( \dot{r} \leq Cr^{m+1} \) satisfy it will never reach 0 in finite time, establishing boundary avoidance.


\textbf{Step 4: Monotonicity \& Diffeomorphism}

\begin{enumerate}
\item \textit{Energy Gain}: Along trajectories:
\[
\frac{d}{dt}j(\Phi(x,t)) = \nabla j(\Phi) \cdot X(\Phi) = \eta(\Phi)\mu_m(\Phi)\|\nabla j(\Phi)\|^2 \geq 0
\]
Thus \( j \) is non-decreasing, with strict increase except at critical points.

\item \textit{Flow Diffeomorphisms}: The differential \( D\Phi(x,t) \) satisfies:
\[
\frac{d}{dt}D\Phi(x,t) = DX(\Phi(x,t))D\Phi(x,t)
\]
Since \( X \) is smooth with bounded derivatives on \( \overline{B} \), Gr√∂nwall's inequality gives:
\[
\|D\Phi(x,t) \| \leq \exp\left(\int_1^{1+\varepsilon} \|DX(\Phi(x,s))\| ds\right) < \infty
\]
Thus \( \Phi(\cdot,t) \) remains locally diffeomorphic, and properness follows from boundary avoidance.
\end{enumerate}

\textbf{Step 5: Isotopy Synthesis}

The time-\( \varepsilon \) map \( \Phi(0,\varepsilon) \) provides the required isotopy through diffeomorphisms.
\end{proof}
\subsection{Proof of Cascading Improvement at Adjoint Timesteps}
\label{proof cascading improvement}
\begin{proposition}[Cascading Improvement at Adjoint Timesteps]
\label{cascading_improvement_proof}

Consider two consecutive timesteps  $t, t - \delta$. Following Algorithm~\ref{alg:training}, when comparing the cases with and without updating  $\theta$  at  $t$, updating  $\theta$  results in an equal or lower cross-entropy for  $\widehat{\mathbf{x}_0}$ at $t-\delta$ when  $\delta$  is sufficiently small and all functions are smooth.
\end{proposition}



\begin{proof}
We want to show 
\[
\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^2), c\bigr)\; \le\; \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr)
\]
for sufficiently small $\delta$ with the following statement:
\[
\widehat{\mathbf{x}_{0}}^1 
\,=\, \mathbf{x}_{t-\delta} \;-\; \mathbf{v}_\theta\bigl(\mathbf{x}_{t-\delta}, t-\delta\bigr)\,(t-\delta)
\]
and
\[
\widehat{\mathbf{x}_{0}}^2 
\,=\, \mathbf{x}_{t-\delta} \;-\; \mathbf{v}_{\theta+\Delta \theta}\bigl(\mathbf{x}_{t-\delta}, t-\delta\bigr)\,(t-\delta),
\]
where 
\[
\Delta \theta 
\,=\, -\,l_r \,\nabla_\theta\Bigl(\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^0), c\bigr)\Bigr), 
\quad
\widehat{\mathbf{x}_0}^0 
\,=\, \mathbf{x}_t \;-\; \mathbf{v}_\theta\bigl(\mathbf{x}_t, t\bigr)\,t.
\]

\noindent
\textbf{Step 1. Relating \(\widehat{\mathbf{x}_0}^1\) and \(\widehat{\mathbf{x}_0}^0\).}
Since \(\mathbf{x}_{t-\delta}\) is close to \(\mathbf{x}_t\) for small \(\delta\), the smoothness of \(\mathbf{v}_\theta(\cdot,\cdot)\) implies
\[
\|\widehat{\mathbf{x}_0}^1 \;-\; \widehat{\mathbf{x}_0}^0\| \;=\; 
\Bigl\|\bigl[\mathbf{x}_{t-\delta} - \mathbf{v}_\theta(\mathbf{x}_{t-\delta}, t-\delta)\,(t-\delta)\bigr]
\;-\;
\bigl[\mathbf{x}_t - \mathbf{v}_\theta(\mathbf{x}_t, t)\,t\bigr]\Bigr\|
\]
can be made arbitrarily small by taking \(\delta\) sufficiently small (and using continuity/Lipschitz arguments). Consequently, 
\[
\nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr)
\quad\text{and}\quad
\nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^0), c\bigr)
\]
are also close for small \(\delta\).

\medskip
\noindent
\textbf{Step 2. First-order comparison at \(t-\delta\).}
By a first-order expansion of \(\mathbf{v}_{\theta+\Delta\theta}\) around \(\theta\) and the smoothness of \(\mathbf{v}_\theta(\cdot,\cdot)\), we have
\[
\mathbf{v}_{\theta+\Delta \theta}(\mathbf{x}_{t-\delta}, t-\delta)
\;=\;
\mathbf{v}_{\theta}(\mathbf{x}_{t-\delta}, t-\delta)
\;+\;
\nabla_\theta \mathbf{v}_{\theta}(\mathbf{x}_{t-\delta}, t-\delta)\,\Delta\theta
\;+\;\mathcal{O}\bigl(\|\Delta\theta\|^2\bigr).
\]
Hence,
\[
\widehat{\mathbf{x}_{0}}^2
\;-\;\widehat{\mathbf{x}_{0}}^1
\;=\;
-\Bigl[\mathbf{v}_{\theta+\Delta \theta}(\mathbf{x}_{t-\delta}, t-\delta) \;-\; \mathbf{v}_\theta(\mathbf{x}_{t-\delta}, t-\delta)\Bigr]\,(t-\delta)
\;\approx\;
-(t-\delta)\,\nabla_\theta \mathbf{v}_{\theta}\bigl(\mathbf{x}_{t-\delta}, t-\delta\bigr)\,\Delta\theta.
\]

\medskip
\noindent
\textbf{Step 3. Cross-entropy decrease.}
Using the smoothness of the cross-entropy and another first-order expansion,
\[
\begin{aligned}
&\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^2), c\bigr) 
\;-\;
\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr)
\\
&\quad\approx\;
\bigl\langle
\nabla_{\widehat{\mathbf{x}_0}}\!\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr),
\;\widehat{\mathbf{x}_0}^2 \;-\;\widehat{\mathbf{x}_0}^1
\bigr\rangle
\;+\;
\mathcal{O}\bigl(\|\widehat{\mathbf{x}_0}^2 - \widehat{\mathbf{x}_0}^1\|^2\bigr)
\\
&\quad\approx\;
\bigl\langle
\nabla_{\theta}\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr),
\;\Delta \theta
\bigr\rangle
\;+\;\mathcal{O}\bigl(\|\Delta\theta\|^2,\|\delta\|\bigr).
\end{aligned}
\]
By definition of the gradient step \(\Delta \theta = -l_r\,\nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^0), c\bigr)\) and the fact that 
\(\nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^0), c\bigr)\) is close to 
\(\nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr)\) for small \(\delta\), the above inner product is non-positive up to higher-order (small) terms. Concretely,
\[
\bigl\langle
\nabla_{\theta}\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr),
\;-l_r\, \nabla_\theta \mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^0), c\bigr)
\bigr\rangle
\;\le\; 0
\]
when \(\delta\) is sufficiently small so that these gradients align (up to small errors). Therefore,
\[
\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^2), c\bigr) 
\;\le\; 
\mathbb{CE}\bigl(f(\widehat{\mathbf{x}_0}^1), c\bigr),
\]
which completes the proof.
\end{proof}


\section{Related Works}

\subsection{Targeted and Untargeted Attacks}

\paragraph{Targeted Attacks.} The objective of targeted attacks is to force the classifier to output a specified label. In other words, the attacker seeks to cause the model to produce incorrect classification results and aims for the result to be a specific target class. This type of attack is more hazardous due to its ability to manipulate the model's output precisely but is typically more challenging to execute.

\paragraph{Untargeted Attacks.} The goal of untargeted attacks is to make the classifier output any incorrect label. The attacker merely needs to mislead the model so that its classification result does not match the true label. Despite having lower requirements, untargeted attacks can still have severe consequences in certain situations.

\subsection{White-Box and Black-Box Attacks}

\paragraph{White-Box Attacks.} White-box attacks assume that the attacker has complete access to the target model, including its architecture, parameters, and gradient information. Using this information, the attacker can generate efficient adversarial examples through iterative optimization methods.

\paragraph{Black-Box Attacks.} Black-box attacks assume that the attacker does not have access to the internal information of the target model. A common method to implement black-box attacks is to utilize transferability, where adversarial examples are first generated against a known surrogate model and then used to attack the unknown target model.

\subsection{Instance-Specific and Instance-Agnostic Attacks}

\paragraph{Instance-Specific Attacks.} Instance-specific attacks \cite{dong2018boosting, gao2021feature, eykholt2018robust, xiong2022stochastic, wang2021enhancing, lu2020enhancing, li2020yet} and \textit{instance-agnostic} generate adversarial perturbations for specific input samples. The attacker uses gradient information from the target model and iterative optimization algorithms to create minimal perturbations that achieve the attack on a given sample. Such attacks usually have high success rates on individual samples but lack generalization and transferability.

\paragraph{Instance-Agnostic Attacks.} Instance-agnostic attacks \cite{xiao2018generating, luo2021generating, kong2020physgan, naseer2019cross, naseer2021generating, feng2023dynamic} do not target specific input samples but instead learn universal adversarial perturbations or generative functions based on data distribution. These attack methods have better generalization across different samples, thus exhibiting stronger transferability.

\subsection{Subcategories of Instance-Agnostic Attacks}

Instance-agnostic attacks can be further subdivided into the following categories:

\paragraph{Universal Adversarial Perturbations.} These methods learn a universal perturbation \cite{moosavi2017universal, zhang2020understanding} applicable to the entire dataset. The classifier can be misled by superimposing this perturbation on any input sample.

\paragraph{Generative Models.} Generative attacks \cite{poursaeed2018generative, naseer2019cross} train a generator that, upon receiving an input sample, can produce specific adversarial perturbations. This approach often surpasses universal adversarial perturbations regarding flexibility and attack efficacy.

\subsection{Single-Target and Multi-Target Attacks}

\paragraph{Single-Target Attacks.} Single-target attacks train an individual generative model for each target class \cite{naseer2019cross, naseer2021generating, feng2023dynamic, wang2023towards}. Although these models achieve high success rates for single-target classes, the training cost becomes substantial when the number of target classes is large, thereby limiting practical usability.

\paragraph{Multi-Target Attacks.} Multi-target attacks simultaneously train the attack capabilities for multiple target classes within a single model \cite{han2019once, yang2022boosting, fang2025clip}. Class labels or text embeddings are typically used as conditional inputs to generate corresponding adversarial perturbations. This method significantly reduces training costs and enhances feasibility in real-world applications.


\section{Method Details}
\subsection{Target Class Condition Representation}
For each label $c$ in the target label set $\mathcal{C}$, we first obtain its class description and format it into a text condition using the template "a photo of a \{class\}"\cite{radford2021learning}. Subsequently, we utilize CLIP's text encoder to derive this textual input's embedding $\mathbf{e}$. Finally, this embedding is fed into our model via cross-attention mechanisms:

% \vspace{-1em}

\begin{equation}
\begin{split}
Q= \mathbf{z}W_{Q}, K=&\  \mathbf{e}W_{K}, V =  \mathbf{e}W_{V}, \\
Attention(Q,K,V)&=softmax(\frac{QK^{T}}{\sqrt{d}}) \cdot V,  
\end{split}
\label{eq:cross_attn}
\end{equation}

% \vspace{-1em}
\noindent where $\mathbf{z} \in \mathbb{R}^{d_{z}}$ denotes the flattened intermediate features of the unet model, $W_{Q}\in \mathbb{R}^{d_{z}\times d}$, $W_{K}\in \mathbb{R}^{d_{e}\times d}$, $W_{V}\in \mathbb{R}^{d_{e}\times d}$ are learnable parameters. 



By employing this approach, we can leverage the rich semantic priors associated with the target classes embedded in the pre-trained diffusion model, thereby facilitating a more effective training process.

\begin{algorithm}[]
   \caption{Single-Target Fine-Tuning Mechanism}
   \label{alg:single}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\tau = N\delta$, stepsize $\delta$, model param. $\phi$, $\theta$, victim model $f$, target label $c$, training dataset $\{\mathbf{I}^i\}_{i\in \mathcal{I}}$, learning rate $l_r$
   \REPEAT
   \FOR{$i\in \mathcal{I}$}
   \STATE get $\mathbf{x}_0 = \mathbf{I}^i$
   \FOR{$t = 1$ {\bfseries to} $N$}
   \STATE $\mathbf{x}_{t\delta} = \mathbf{x}_{(t-1)\delta} + \mathbf{v}_\phi(\mathbf{x}_{(t-1)\delta}, (t-1)\delta, \varnothing) \delta$
   \ENDFOR
   \FOR{$t = N$ {\bfseries to} $1$}
   \STATE $\mathbf{x}_{(t-1)\delta} = \mathbf{x}_{t\delta} - \mathbf{v}_\theta(\mathbf{x}_{t\delta}, t\delta, c) \delta$
   \STATE $\widehat{\mathbf{x}_0} = \mathbf{x}_{t\delta} - \mathbf{v}_\theta(\mathbf{x}_{t\delta}, t\delta, c) t \delta$
   \STATE get random mask $M$
   \STATE $\widehat{\mathbf{x}_0} = \mathbf{x}_0 + M \cdot (\widehat{\mathbf{x}_0} - \mathbf{x}_0)$
   \STATE $\widehat{\mathbf{x}_0}^{i,j,k} = \operatorname{clip}\left( \widehat{\mathbf{x}_0}^{i,j,k}, \mathbf{x}^{i,j,k} - \epsilon, \mathbf{x}^{i,j,k} + \epsilon \right)$
   \STATE $\theta = \theta - l_r \cdot \nabla_\theta(\mathbb{CE}(f(\widehat{\mathbf{x}_0}), c))$
   \ENDFOR
   \ENDFOR
   \UNTIL{$\mathbf{v}_\theta$ convergence}
   \STATE {\bfseries Return:} Dual-Flow $\{\mathbf{v}_\phi,\mathbf{v}_\theta\}$
\end{algorithmic}
\end{algorithm}

\subsection{Fine-Tuning on Single-Target Tasks}
\label{appendix:single_finetune}
We fine-tune our model for single-target tasks to enhance its performance further. Specifically, we fix the target label during training, enabling the model to focus on targeted attacks for a specific label. To mitigate the perturbations being confined to some areas of the image, which can reduce the robustness and transferability of adversarial examples in single-target training, we apply the mechanism introduced in \cite{fang2025clip}.

In detail, we generate a random mask $M$ of the same size as the image, where several randomly positioned square pixel areas are set to 0, and the rest are set to 1. By multiplying this mask with the perturbation, we ensure the generated adversarial samples remain consistent with the original image in the masked square areas. This forces the model to create adversarial patterns distributed across the entire image rather than being localized to specific regions, as illustrated in Algorithm \ref{alg:single}.

Like other single-target methods, we must fine-tune a separate model for each target. However, due to our model's powerful capabilities in multi-target attacks, once the model is trained on the multi-target task, it requires only a few additional steps to adapt to each single-target task. This results in significantly lower training overhead compared to other methods.



\section{Transferability Evaluation On ImageNet Validation Set}
In addition to the evaluation on the ImageNet-NeurIPS (1k) dataset\cite{Nips2017}, we conducted an assessment of our attack method on the ImageNet validation set (50k)\cite{deng2009imagenet} and compared it with the state-of-the-art multi-target attack method, CGNC\cite{fang2025clip}. The experimental results presented in Table \ref{tab:imagenet_val} indicate that our method achieves a significantly higher average black-box attack success rate than CGNC, demonstrating its superior transferability. This outcome is consistent with the results observed on the ImageNet-NeurIPS (1k) dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}
  \centering
  \caption{Attack success rates (\%) for multi-target attacks on normally trained models using the ImageNet validation set. The perturbation budget is constrained to $l_{\infty} \leq 16/255$. * indicates white-box attacks. The results are averaged across 8 different target classes, and the overall average on the far right is computed solely for black-box attacks.}
 
  \vskip 0.15in
  \begin{small}
    \begin{sc}
    \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \toprule 
    Source   & Method   & Inc-v3   & Inc-v4   & Inc-Res-v2    & Res-152  & DN-121   & GoogleNet       & VGG-16 & Average\\
    \hline
    \multirow{2}[0]{*}{Inc-v3} 
             & CGNC     & 96.59$^*$    & 57.82    & 46.84    & 44.13    & 65.90    & 53.40    & 56.27 & 54.06\\
             & Ours& 89.89$^*$& \textbf{75.74}&\textbf{65.05}&\textbf{75.73}&\textbf{82.75}&\textbf{72.21}&\textbf{66.20}&\textbf{72.95}\\
             \hline
    \multirow{2}[0]{*}{Res-152} 
             & CGNC     & 56.00    & 50.37    & 32.26    & 96.44$^*$    & 86.69    & 63.84    & 63.90 & 58.84\\
             & Ours&\textbf{69.75}&\textbf{72.53}&\textbf{54.11}&92.70$^*$&\textbf{86.71}&\textbf{74.08}&\textbf{68.22}&\textbf{70.90}\\
             \bottomrule
    \end{tabular}%
  \label{tab:imagenet_val}%
  \end{sc}
  \end{small}
    \vskip -0.1in
\end{table}%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{More Analyses}

\paragraph{More Ablation Studies.}

We designed a variant: during training, we use a new $L_2$ loss function to make the model's output as close as possible to the original ODE trajectory, ensuring it does not deviate too far from the original ODE flow. We call this variant Dual-Flow-$L_2$. The experimental results in Table \ref{l2_ablation} indicate that the attack capability of this variant is not ideal, as described in Section \ref{l2_v}.

\begin{table}[htbp]
  \caption{Comparison of Dual-Flow and Dual-Flow-$L_2$. The surrogate model is Res-152. }
  \label{l2_ablation}%
  \centering
  \vskip 0.15in
  \begin{small}
  \begin{sc}
    \resizebox{\linewidth}{!}{  % ‰ΩøÁî®resizeboxËÆ©Ë°®Ê†ºÂÆΩÂ∫¶‰∏∫È°µÈù¢ÂÆΩÂ∫¶
      \begin{tabular}{c|c|c|c|c|c|c|c|c}  % ÂàóÂ±Ö‰∏≠
      \toprule
        Method   & Inc-v3   & Inc-v4   & Inc-Res-v2    & Res-152  & DN-121   & GoogleNet       & VGG-16 & Average\\
      \hline
      Dual-Flow & 69.58 & 71.92  & 56.10 & 92.39 & 85.73 & 73.65 & 67.59 & 70.76\\
      Dual-Flow-$L_2$ & 54.90 & 56.26 & 42.94 & 86.80  & 78.41 & 57.71 & 46.36 & 56.10 \\
      \bottomrule
      \end{tabular}
    }
  \end{sc}
  \end{small}
  \vskip -0.1in
\end{table}%



%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.6\linewidth]{images/compare.pdf}}
\caption{Visualization results comparing the adversarial perturbations generated by our method with those produced by CGNC.}

\label{fig:compare}
\end{center}
\vskip -0.2in
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{table}[htbp]
  \caption{Comparison of different types of adversarial inputs. CGNC-P and Dual-Flow-P represent the adversarial perturbations generated by CGNC and our method, respectively, while Dual-Flow-A denotes the unclipped adversarial samples produced by our method. The adversarial perturbations are scaled to a range between 0 and 1 before input into the classifier.}
  \label{pert_noclip}%
  \centering
  \vskip 0.15in
  \begin{small}
  \begin{sc}

    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \toprule
    Source   & Method   & Inc-v3   & Inc-v4   & Inc-Res-v2 & Res-152   & DN-121   & GoogleNet & VGG-16\\
    \hline
    \multirow{3}[0]{*}{Inc-v3} 
             & CGNC-P  & 56.80$^{*}$    & 19.15    & 22.06    & 10.56    & 13.14    & 14.56    & 3.41\\
             & Dual-Flow-P         & 86.55$^{*}$ & 81.2 & 74.55 & 74.55 & 70.66 & 76.15 & 55.10\\
             & Dual-Flow-A & 99.12$^{*}$ & 95.31 & 92.79 & 97.39 & 96.80 & 95.62 & 87.95\\
             \hline
    \multirow{3}[0]{*}{Res-152} 
             & CGNC-P  & 23.61    & 25.72    & 39.07    & 58.29$^{*}$    & 39.09    & 37.47    & 17.21\\
             & Dual-Flow-P             & 68.44 & 81.48 & 78.26 & 86.29$^{*}$ & 80.44 & 74.59 & 55.35\\
             & Dual-Flow-A & 94.38 & 96.60 & 94.62 & 99.19$^{*}$ & 97.92 & 93.54 & 90.85\\
             \bottomrule
    \end{tabular}%
  \end{sc}
  \end{small}
  \vskip -0.1in
\end{table}%


\paragraph{Semantic Adversarial Attack.} 


We compared the adversarial perturbations generated by our method and those produced by the state-of-the-art multi-target attack method, CGNC\cite{fang2025clip}. As illustrated in Figure \ref{fig:compare}, the visual results indicate that while CGNC's perturbations contain some semantic features of the target class, they are primarily confined to small, repetitive patterns. In contrast, our method generates perturbations that are semantically more representative of the complete target class.

To further validate this observation, we directly input the adversarial perturbations generated by CGNC and our method into the target classifier. As shown in Table \ref{pert_noclip}, our adversarial perturbations alone can induce the classifier to predict the target class with a relatively high probability. Conversely, the perturbations produced by CGNC exhibit a lower likelihood, particularly when transferred to black-box models. This demonstrates that our perturbations incorporate more semantic features of the target class.

Moreover, as depicted in Figure \ref{fig:visual}, the unclipped samples generated by our method are semantically very close to the target class. We also input these unclipped samples directly into the target classifier. Table \ref{pert_noclip} shows that these samples are highly likely to be classified as the target class. This confirms that semantic proximity to the target class effectively increases attack success rates.

These findings collectively suggest that our attack method's robustness and transferability result from embedding substantial target class semantics into the images, thereby reducing dependence on specific target model decision boundaries.


\section{More Visualization}

%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\linewidth]{images/more_visual.pdf}}
\caption{Visualization results of different input images targeting various classes. For each text prompt of the target class, the left column displays the adversarial examples generated before clipping, the middle column shows the adversarial examples after clipping, and the right column presents the corresponding adversarial perturbations, which represent the differences between the clipped adversarial examples and the original images. Note that the perturbations are scaled to a range between 0 and 1. The surrogate model used is Inc-v3.}

\label{fig:more_visual}
\end{center}
\vskip -0.2in
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%