\section{Data Contamination Evidence Collection Efforts}
\label{sec:Evidence Collection}
Several initiatives are currently collecting evidence on data contamination. Below are key platforms and resources involved in this effort:

\begin{itemize}
    \item \textbf{The Language Model Contamination Index (LM Contamination Index)}:  
    This is a database used to track and record evidence of language model contamination. For more information, visit: \url{https://hitz-zentroa.github.io/lm-contamination/}.
    
    \item \textbf{CONDA-Workshop Data Contamination Database}:  
    This is a community-driven project focused on the centralized collection of data contamination evidence. The goal is to help the community understand the extent of the problem and assist researchers in avoiding previous mistakes. Detailed information can be found at: \url{https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database}.
\end{itemize}

\section{Definition of Assumptions}
\label{sec:definition of assumptions}
\subsection{Verbatim Memorization}
\label{sec:verbatim memorization}
In the context of LLMs, verbatim memorization~\cite{carlini2021extracting,carlini2022quantifying} refers to the phenomenon where a model recalls exact sequences of text, often from the data it has been trained on. This occurs when a model has seen a specific passage or piece of information during its training process and is able to reproduce it exactly when prompted. Verbatim memorization can lead to issues of data contamination, where the model unintentionally outputs copyrighted or sensitive material verbatim, causing concerns regarding privacy, intellectual property, and validity in analytical tasks. 

\subsection{Black-Box Method Assumption}
\label{sec:assumption detail}
\citet{golchin2023data} has assumed that when a model has memorized instances from the original dataset, it will prefer selecting options containing the original instance over semantically similar perturbations. Additionally, LLMs may exhibit positional biases, where certain positions in multiple-choice options are more likely to be chosen, leading to potential overestimation or underestimation of contamination levels.

\citet {golchin2023time} gave the assumption that by providing a "guided instruction" with dataset name, partition information, and part of the reference instance, LLMs can generate the complete version of the data instance. This allows for calculating overlap between generated completions and reference instances, helping to infer whether the dataset partition is contaminated. 

\citet{duarte2024decopdetectingcopyrightedcontent} assumed that LLMs may memorize specific copyrighted content, such as books or academic papers, during training. When encountering similar content, they can distinguish whether they've seen it before. DE-COP exploits this by designing multiple-choice questions to test if the model can accurately identify original copyrighted content from paraphrased versions. Additionally, model selection biases can affect copyright detection results, and DE-COP introduces a calibration method to minimize such biases.

In \cite{dong-etal-2024-generalization}, it is assumed that contaminated training data significantly affects the output distribution of large language models. Specifically, when trained on contaminated data, the model's output distribution becomes more peaked, causing it to produce more consistent outputs on contaminated data, favoring outputs strongly correlated with the training data.

\citet{deng2023investigating} assumed that if an LLM can accurately guess missing parts of a test set, such as keywords or answer options, without external assistance, it suggests that the model has encountered the corresponding benchmark data during training. This indicates memorization-based contamination. The TS-Guessing protocol tests whether the model has memorized benchmark data by having it guess hidden information.

\citet{ranaldi-etal-2024-investigating} assumed that data contamination can be detected solely by analyzing the inputs and outputs of LLMs. For example, unusually high accuracy on tasks from datasets like Spider indicates that the model may have been exposed to this dataset during training, leading to memorization rather than genuine understanding. Additionally, data contamination may lead to inflated performance on zero-shot tasks when the model encounters potentially contaminated data during training.

\citet{chang-etal-2023-speak} assumed that LLMs may memorize portions of text from their training data, especially when evaluation datasets contain known texts. This memorization can lead to inflated performance on tasks such as code generation. Moreover, data repetition on the web—through search engines and open datasets—encourages memorization, which improves accuracy on tasks involving familiar content.

\subsection{Memorization and Data Contamination}

Instance-level contamination~\cite{fu2024does} does not always lead to verbatim memorization. Utilizing instance generation~\cite{carlini2022quantifying,karamolegkou-etal-2023-copyright}, demonstrates that verbatim memorization requires repeated exposures to this instance x during training. Indeed, future research on contamination should place more emphasis on LLMs' memorization.

\section{Data Contamination Detector}
\label{sec:Data contamination Detector}
\citet{Li2023AnOS} present Contamination Detector to check whether test examples appear on the internet via Bing search and Common Crawl index. The tool is available at: \url{https://github.com/liyucheng09/Contamination_Detector}.

\citet{ravaut2024much} presented an open-source library for contamination detection in NLP datasets and LLMs. The library combines multiple methods for contamination detection and is available at: \url{https://github.com/liyucheng09/Contamination_Detector}.

Overlapy is a Python package developed to evaluate textual overlap (N-Grams) between two volumes of text. This tool can be accessed at: \url{https://github.com/nlx-group/overlapy}.

\citet{yao-etal-2024-data} introduced Deep Contam, a method that detects cross-lingual contamination, which inflates LLMs' benchmark performance while evading existing detection methods. An effective detection method is provided in the repository, accessible at: \url{https://github.com/ShangDataLab/Deep-Contam}.

\citet{tu2024dicedetectingindistributioncontamination} discussed the detection of in-distribution data contamination using LLM's internal state. The tool is available at: \url{https://github.com/THU-KEG/DICE}.

\citet{bordt2023testing, bordt2024colm} presented Tabmemcheck, an open-source Python library designed to test language models for memorization of tabular datasets. The package includes four different tests for verbatim memorization of a tabular dataset (header test, row completion test, feature completion test, first token test). It also provides additional heuristics to test what an LLM knows about a tabular dataset, such as feature names test, feature values test, dataset name test, and sampling. The package can be found at: \url{https://github.com/interpretml/LLM-Tabular-Memorization-Checker}.

\citet{yang2023rethinkingbenchmarkcontaminationlanguage} provided a package that includes the LLM decontaminator, which quantifies a dataset's rephrased samples relative to a benchmark. Based on the detection results, the contamination of rephrased samples in the dataset can be estimated and removed from the training set. This tool is available at: \url{https://github.com/lm-sys/llm-decontaminator}.
