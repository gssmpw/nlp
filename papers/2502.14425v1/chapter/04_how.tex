\label{sec:how}

%The definition of data contamination detection is the process of determining, through a specific method, whether a given text or dataset has appeared in the training corpus of a particular model. Data contamination detection has emerged as a critical challenge in LLM evaluation. We categorize detection approaches into three paradigms based on model information accessibility. This taxonomy reveals an evolving detection landscape where white-box methods provide high precision but limited applicability, gray-box approaches balance practicality and effectiveness, and black-box technologies rely on heuristic assumptions (appendix \ref{sec:assumption detail}). As specialized methods continue to emerge, the community's awareness of the potential for data contamination to distort evaluation results is gradually increasing. Therefore, we provide descriptions of some contamination detection tools in Appendix \ref{sec:Data contamination Detector}.

The definition of data contamination detection refers to the process of determining, through a specific methodology, whether a given text or dataset has been included in the training corpus of a particular model. As LLMs continue to advance, data contamination detection has emerged as a critical challenge in model evaluation. Here, we categorize detection approaches into three paradigms based on the level of access to model information: white-box, gray-box, and black-box methods. This taxonomy highlights an evolving detection landscape. White-box methods, which leverage full access to model architectures or training data, offer high precision but are often limited in applicability. Gray-box approaches, which utilize partial model information, strike a balance between practicality and effectiveness. Black-box technologies rely on heuristic assumptions (detailed in Appendix \ref{sec:assumption detail}) and operate without access to internal model details. As specialized detection methods continue to emerge, the research community is increasingly recognizing the importance of data contamination to distort evaluation outcomes. To support this growing awareness, we provide detailed descriptions of several contamination detection tools in Appendix \ref{sec:Data contamination Detector}.

\subsection{White-Box Detection}
\input{tables/ngram}

White-box methods directly utilize model internals or training data to detect data contamination. When pre-training corpora are accessible, content overlap with evaluation datasets can be explicitly measured~\cite{elangovan-etal-2021-memorization}. Prominent LLMs including LLaMA2 \cite{touvron2023llama2}, PaLM \cite{chowdhery2023palm}, and GPT-4 \cite{achiam2023gpt} all emphasize the necessity of detecting pre-training/evaluation overlaps. The n-gram overlap method, prioritized for its computational efficiency and simplicity, has become a standard tool for detecting contamination. Comparative implementations of these n-gram based overlap detection strategies are systematically summarized in Table \ref{tab:ngram}.

Embeddings similarity compares texts via cosine similarity of their embeddings, capturing semantic relationships beyond lexical variations~\cite{reimers2019sentence}. \citet{lee2023platypus} used a similarity exclusion method based on embeddings, reducing dataset redundancy and filtering out duplicate data to ensure clean training data. To address sophisticated contamination forms, \cite{yang2023rethinkingbenchmarkcontaminationlanguage} introduced a hybrid approach combining embedding similarity search with GPT-4 powered semantic analysis. This detects paraphrased samples, enabling proactive benchmark decontamination.

For known model weights,
\citet{tu2024dicedetectingindistributioncontamination} proposed DICE to identify in-distribution contamination during fine-tuning by analyzing layer-specific activation patterns. This method trains contamination classifiers on sensitive intermediate layers, demonstrating a strong correlation between detection signals and performance inflation across multiple LLMs.


\subsection{Gray-Box Detection}
Gray-box approaches in membership inference attacks (MIAs) leverage partial model information such as token probabilities to distinguish training data from non-members. \citet{duan2024membership} systematically investigated the underwhelming MIA performance on LLMs, identifying three primary contributing factors: the massive scale of training datasets that complicates memorization patterns, the limited number of training iterations that reduce model overfitting, and the inherently fuzzy decision boundaries between member and non-member samples. To address these shortcomings, the MIN-K\% method established token-based effective methods using outlier token probabilities for pretraining data detection~\cite{shi2024detectingpretrainingdatalarge}. \citet{zhang2024minkimprovedbaselinedetecting} subsequently proposed Min-K\%++, theoretically grounding detection in local probability maxima identification. \citet{zhang-etal-2024-pretraining} proposed DC-PDD to employ corpus frequency divergences to reduce false positives. \citet{ye-etal-2024-data} introduced PAC, an MIA method that calculates polarization distances through input perturbations. \citet{zhang-etal-2024-pacost} developed PaCoST, which statistically compares model confidence on original test items versus distributionally-similar counterparts, to reveal widespread contamination across open-source models.

Alternative gray-box strategies, including perplexity-based memorization detection \cite{li2023estimatingcontaminationperplexityquantifying} and the adversarial compression ratio (ACR) metric \cite{schwarzschild2024rethinkingllmmemorizationlens}, quantify memorization through input-output token efficiency.

\subsection{Black-Box Detection}

\input{tables/assumption}

Black-box methods operate without access to model internals, training corpus, and are often accompanied by limitations in computational resources.
Specifically, these methods heavily rely on certain assumptions shown in Appendix \ref{sec:definition of assumptions}.

%\citet{golchin2023data} designed a multiple-choice question framework where each question presents an original instance alongside three perturbed versions (words are replaced with contextually relevant synonyms) and one invalid option. If the LLM frequently selects the original instance, it suggests that the model may be influenced by data contamination. \citet{golchin2023time} proposed a detection method based on "guided instruction", which effectively identifies contamination in datasets through instance completion and heuristic evaluation, offering a robust solution for data contamination.


\citet{golchin2023data} introduced a multiple-choice question framework in which each question presents an original instance alongside three perturbed versions (where words are replaced with contextually relevant synonyms) and one invalid option. If the LLM consistently selects the original instance, this behavior may indicate the presence of data contamination. Building on this, \citet{golchin2023time} proposed a guided instruction-based detection method, which effectively identifies contamination in datasets through instance completion and heuristic evaluation.

%\citet{duarte2024decopdetectingcopyrightedcontent} developed DE-COP, a copyright detection framework using verbal vs. paraphrased multiple-choice probing. Its BookTection/arXivTection benchmarks reveal temporal training data patterns in commercial LLMs. Similarly, \citet{deng2023investigating} proposed TS-Guessing, a protocol testing model ability to reconstruct masked test elements. Its analysis uncovers subtle contamination in major benchmarks, particularly in instruction-tuned LLMs. \citet{dong-etal-2024-generalization} introduced CDD to detect contamination through output distribution peakedness analysis. Paired with TED mitigation technique, the CDD approach addresses both explicit and implicit contamination forms while maintaining evaluation validity.

\citet{duarte2024decopdetectingcopyrightedcontent} developed DE-COP, a copyright detection framework that employs verbal versus paraphrased multiple-choice probing. Using benchmarks such as BookTection and arXivTection, DE-COP reveals temporal patterns in the training data of commercial LLMs. Similarly, \citet{deng2023investigating} proposed TS-Guessing, a protocol designed to test a model's ability to reconstruct masked elements of test data. This approach uncovers subtle contamination in major benchmarks. Further advancing this line of research, \citet{dong-etal-2024-generalization} introduced CDD to identify contamination by analyzing the peakedness of output distributions. When paired with the TED mitigation technique, the CDD approach effectively addresses both explicit and implicit forms of contamination while preserving the validity of model evaluations.



As highlighted by \cite{ranaldi-etal-2024-investigating}, the Text-to-SQL task with GPT-3.5 involves data contamination, where the model is tasked with reconstructing masked column names using the table name, the remaining column names, and contextual information. Similarly, \citet{chang-etal-2023-speak} introduced a challenging cloze task and employed data archaeology to examine the memorization of passages from 571 novels by using LLMs.








