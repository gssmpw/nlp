\label{sec:where}
This section discusses methods to avoid data contamination in evaluation. First, to reduce risks, benchmarks are often constructed following three strategies: Data updating-based methods, Data rewriting-based methods, and prevention-based methods. Second, Dynamic evaluation generates adaptive samples using techniques like algorithmic composition, graph structures, randomization, and reasoning graphs, ensuring controlled complexity and diversity. Finally, LLM-as-a-evaluator eliminates contamination risks, making it a key for contamination-free evaluation.

\subsection{Benchmark Contamination-free Strategies}

Contamination-free benchmarking strategies ensure datasets stay up-to-date, preventing models from using outdated data. Rewriting construction combines human efforts like manual labeling with LLM-assisted techniques such as rephrasing to avoid contamination. Preventive measures involve technical defenses like encryption, access control, and de-contamination during inference to guarantee the reliability and fairness of LLM evaluation.

\subsubsection{Data Updating-based Methods}

Using the most recent data is intuitive for constructing contamination-free benchmarks, and some studies have proposed automatically collecting recent data to build questions.
LatestEval proposed an automated pipeline to dynamically generate contamination-free test sets from recent materials~\citep{li2024latestevaladdressingdatacontamination}. 
\citet{white2024livebench} introduced LiveBench, a dynamically updated benchmark that integrates tasks across math, coding, and reasoning with automated scoring to mitigate data contamination and evaluation biases. Similarly, \citet{jain2024livecodebench} developed LiveCodeBench, a code-specific benchmark that expands beyond HumanEval~\cite{chen2021codex} and MBPP~\citep{austin2021program} by assessing self-repair and prediction abilities while ensuring periodic updates. To evaluate LLMs' world knowledge, \citet{yu2023kola} introduced the KoLA benchmark, which combines stable knowledge sources (e.g., Wikipedia) with recent data to balance evaluation fairness and contamination prevention.
\citet{zhang-etal-2024-pretraining} introduced PatentMIA, crawling Chinese patent data from Google Patents. This dataset contains 5,000 patents with a publication date after March 1, 2024, and 5,000 patents published before January 1, 2023.
\citet{haimes2024benchmark} proposed to use retro-holdout datasets to detect public benchmark influence on model training and measure discrepancies between benchmark results and real-world performance.
\citet{fan2024nphardeval4vdynamicreasoningbenchmark} introduced NPHardEval4V-a dynamically updated benchmark to assess reasoning capabilities of MLLMs.
In code evaluation, EvoCodeBench is proposed to dynamically align with real-world code repositories to guarantee fair evaluation.

\subsubsection{Data Rewriting-based Methods}

This type of methods use data augmentation to remove contamination from benchmarks, with LLMs' superior rephrasing and verifying capabilities. Human intervention is also integrated into the rewriting process to create novel data with a distribution similar to the original data.
\citet{zhu-etal-2024-clean} proposed Clean-Eval to purify contaminated benchmarks by paraphrasing and back-translating data into semantically equivalent but lexically distinct forms.
\citet{zhao2024mmlu} proposed the MMLU-CF dataset, which is constructed by collecting diverse questions, cleaning data, sampling difficulty reasonably, checking data integrity with LLMs, and applying rewriting methods such as rephrasing questions and shuffling options to ensure the dataset remains contamination-free.
\citet{zhang2024careful} provided GSM1k, employing manual labeling, three-tier quality control, and leak prevention design to avoid data contamination. Through meticulous human construction, GSM1k achieves high similarity to GSM8k\cite{cobbe2021gsm8k} in style, difficulty, and human solve rates, while maintaining complete content independence.
Meanwhile, LLMs can serve as assistants for rewriting or generating questions. CLEVA is generated by non-repetitive sampling for each evaluation round. Each test sample is further enhanced with multiple data rewriting strategies before being used to assess LLMs, significantly mitigating the risk of data contamination~\citep{li-etal-2023-cleva}. \citet{ying2024automating} updated benchmarks with two strategies: style-preserving mimicry with LLMs and cognitive-level expansion using Bloom's taxonomy. Similarly, \citet{zhu2024dynamicevaluationlargelanguageMPA} proposed Multi-Principle Assessment (MPA), which utilizes LLM-based agents to automatically transform existing questions into new ones. \citet{wang2024benchmark} introduced a multi-agent framework to implement self-evolving benchmarks, which dynamically mutates question contexts and structures to update benchmarks.

\subsubsection{Prevention-based Methods}
Preventive measures focus on safeguarding test data integrity through technical and procedural controls. Core strategies include encrypting public test data with public-key cryptography, enforcing strict access permissions, and prohibiting derivative data creation. \citet{zhu2024inference} introduced Inference-Time Decontamination (ITD), a novel technique that identifies and rewrites potentially memorized responses during model inference. \citet{li2024c2levacomprehensivecontaminationfreelanguage} introduced C\textsuperscript{2}LEVA, a comprehensive bilingual benchmark with systematic contamination prevention mechanisms, which implements proactive measures such as test data rotation and enhanced encryption.

\subsection{Dynamic Evaluation}
\label{sec:dynamic evaluation}
%Dynamic approaches combat contamination through adaptive assessment frameworks. \citet{zhu2024dyvaldynamicevaluationlarge} pioneered DYVAL, a graph-based system generating evaluation samples through algorithmic composition, constraint application, and functional description. Its directed acyclic graph architecture enables multi-step reasoning tasks with controlled complexity. \citet{lei-etal-2024-s3eval} developed S3EVAL for SQL evaluation through randomized table-query pairs. This synthetic approach enables customizable task lengths and difficulty levels while systematically testing long-context reasoning. \citet{zhang2024darg} introduced the DARG method, which dynamically generates evaluation samples with controllable complexity and diversity through adaptive reasoning graphs and verifies the correctness of labels using tool-augmented LLMs.  \citet{srivastava2024functional} proposed functionalization-converting static QAs into parameterized code that generates infinite test variants. \citet{qian2024varbench} extended dynamic evaluation through controlled question key variable perturbation, to dynamically generate dataset.

Dynamic approaches address data contamination by leveraging adaptive assessment frameworks. \citet{zhu2024dyvaldynamicevaluationlarge} introduced DYVAL, a graph-based system that generates evaluation samples through algorithmic composition, constraint application, and functional descriptions. Its directed acyclic graph (DAG) architecture facilitates multi-step reasoning tasks with precisely controlled complexity. \citet{lei-etal-2024-s3eval} developed S3EVAL, a framework for SQL evaluation that utilizes randomized table-query pairs. This synthetic approach allows for customizable task lengths and difficulty levels, while systematically assessing long-context reasoning capabilities. \citet{zhang2024darg} proposed the DARG method, which dynamically generates evaluation samples with adjustable complexity and diversity using adaptive reasoning graphs. \citet{srivastava2024functional} introduced functionalization, a technique that transforms static question-answer pairs into parameterized code, enabling the generation of infinite test variants. \citet{qian2024varbench} further extended dynamic evaluation by perturbing key variables in questions, allowing for the dynamic generation of datasets with controlled variations.


\subsection{LLM-as-a-Evaluator}
\label{sec:llm driven evaluation}
Next-generation evaluation leverages LLMs themselves as assessment tools. 
LLMs are no longer just "artisans" of content generation; they have become "judges" of content quality. They can serve the roles of scoring, ranking, and selection. \citet{bai2024benchmarking} presented the "LM-as-Examiner" framework, generating questions and evaluating responses through reference-free analysis. \citet{yu-etal-2024-kieval} deployed LLMs as "Interactors" in structured multi-turn dialogues that probe model capabilities while minimizing contamination risks.
\citet{li2024treeevalbenchmarkfreeevaluationlarge} proposed TreeEval-a benchmark-free system where LLMs generate hierarchical question trees. This adaptive approach adjusts difficulty based on model performance, creating unique assessment paths that prevent data contamination.
