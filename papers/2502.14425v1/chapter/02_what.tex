\label{sec:what}
\subsection{Definition}

%In recent years, an increasing number of studies are proposed to address data contamination, but there is no unified definition or standard methodology to summarize data contamination. \citet{brown2020language} first highlighted pre-training data contamination using an N-gram diagnostic method, showing how data contamination inflates model performance. \citet{hartmann2023sok} has explored how memorization of LLMs is linked to data contamination, as memorization and contamination both involve regurgitating pre-training data. \citet{schwarzschild2024rethinkingllmmemorizationlens} suggests strings can be considered memorized if they can be reproduced using a shorter prompt. \citet{karamolegkou-etal-2023-copyright} explores verbatim memorization, particularly with copyrighted materials. Our research extends this framework in two ways: (1) vulnerabilities across the LLMs' lifecycle (including pre-training, fine-tuning, and post-deployment contamination), and (2) risks to benchmark integrity (including data manipulation and potential label leakage).

In recent years, a growing body of research has emerged to address the issue of data contamination in LLMs. However, the field lacks a unified definition or standardized methodology to comprehensively summarize data contamination. \citet{brown2020language} was among the first to highlight pre-training data contamination, employing an N-gram diagnostic method to demonstrate how contamination artificially inflates model performance. \citet{hartmann2023sok} further explored the connection between LLM memorization and data contamination, noting that both phenomena involve the regurgitation of pre-training data. \citet{schwarzschild2024rethinkingllmmemorizationlens} proposed that strings can be considered memorized if they can be reproduced using a shorter prompt, while \citet{karamolegkou-etal-2023-copyright} investigated verbatim memorization, particularly in the context of copyrighted materials. Building on these foundational studies, our research extends the framework into two significant directions: (1) examining vulnerabilities across the entire lifecycle of LLMs, including pre-training, fine-tuning, and post-deployment contamination, and (2) addressing risks to benchmark integrity, such as data manipulation and potential label leakage.

\subsubsection{Phase-based Contamination}
%For phase-based contamination, recent research outlines stage-based contamination risks in LLM development: pre-training (test data leakage into corpora), fine-tuning (unintentional exposure), and post-deployment (bias absorption from real-world interactions).  \citet{sainz-etal-2023-nlp} systematically mapped contamination pathways across these critical phases, while \citet{balloccu-etal-2024-leak} introduced the concept of indirect data contamination, where human interactions during LLM training inadvertently introduce biases, even when explicit test data inclusion is excluded. Moreover, multimodal large language models (MLLMs) face amplified contamination challenges due to multi-modal data integration, complicating data integrity preservation. \citet{song2024textimagesleakedsystematic} proposed a bimodal taxonomy, distinguishing between unimodal contamination and cross-modal contamination, while developing traceability frameworks specifically for MLLMs. 

For phase-based contamination, recent research has identified stage-specific contamination risks throughout the lifecycle of LLMs: pre-training (where test data may leak into training corpora), fine-tuning (where models are unintentionally exposed to evaluation data), and post-deployment (where models absorb biases from real-world interactions). \citet{sainz-etal-2023-nlp} systematically mapped contamination pathways across these critical phases, while \citet{balloccu-etal-2024-leak} introduced the concept of indirect data contamination, highlighting how human interactions during LLM training can inadvertently introduce biases, even in the absence of explicit test data inclusion. Furthermore, multimodal large language models (MLLMs) face heightened contamination challenges due to the integration of diverse data modalities~\cite{yin2023survey}. \citet{song2024textimagesleakedsystematic} proposed a bimodal taxonomy, distinguishing between unimodal contamination and cross-modal contamination, and developed traceability frameworks tailored specifically for MLLMs.

\subsubsection{Benchmark-based Contamination}
%For benchmark-based contamination, previous papers have generally discussed it in two main ways. One approach primarily focuses on whether labels are leaked and whether samples are rewritten. The other approach classifies contamination at the instance level or dataset level. \citet{yang2023rethinkingbenchmarkcontaminationlanguage} considered simple rewording as contamination, including synonym substitution and the use of translation. \citet{yao-etal-2024-data} revealed cross-language contamination through option rewriting detection. In the context of code generation tasks, \citet{palavalli-etal-2024-taxonomy} established a systematic taxonomy that contamination is categorized into dataset-level (e.g., test data leakage or mixing) and instance-level (e.g., output masking, input/output rewriting, or augmentation). Further expanding on this, \citet{matton-etal-2024-leakage} identified three sources of contamination and proposed the LBPP benchmark as a countermeasure. \citet{fu2024does} defined data contamination at both instance and dataset levels, providing formal mathematical definitions for instance-level contamination (via membership inference attacks) and dataset-level contamination (both full and partial contamination).

For benchmark-based contamination, prior research has generally approached the issue from two primary perspectives. The first focuses on whether labels are leaked or whether samples are rewritten, while the second categorizes contamination at either the instance level or the dataset level. \citet{yang2023rethinkingbenchmarkcontaminationlanguage} considered even simple rewording—such as synonym substitution or translation—as a form of contamination. \citet{yao-etal-2024-data} further revealed cross-language contamination through the detection of option rewriting. In code generation tasks, \citet{palavalli-etal-2024-taxonomy} established a systematic taxonomy, categorizing contamination into dataset-level (e.g., test data leakage or mixing) and instance-level (e.g., output masking, input/output rewriting, or augmentation). Expanding on this, \citet{matton-etal-2024-leakage} identified three distinct sources of contamination and proposed the LBPP benchmark as a countermeasure. Additionally, \citet{fu2024does} provided formal mathematical definitions for contamination at both the instance and dataset levels, defining instance-level contamination through membership inference attacks and dataset-level contamination as either full or partial contamination.

\subsection{Impacts}

Data contamination critically undermines evaluation reliability and research validity. As \cite{sainz-etal-2023-nlp} demonstrated, benchmark overfitting can artificially inflate model performance and compromise scientific conclusions in NLP studies. \cite{singh2024evaluation} identified two principal analysis approaches: causal analysis through controlled retraining experiments, and post-hoc contamination inference via performance pattern examination without model retraining.

\subsubsection{Evidence Collection}
\label{subsubsec:evidence}
Initial contamination investigation focuses on temporal data analysis and adversarial detection methods. \citet{li2024task} proposed evaluating models on pre/post-training datasets with membership inference attacks, revealing contamination effects on zero/few-shot performance. \citet{riddell2024quantifyingcontaminationevaluatingcode} demonstrated performance inflation on seen HumanEval/MBPP samples, while \citet{cao2024concerneddatacontaminationassessing} validated contamination mitigation strategy through using the most recent benchmarks. \citet{jiang2024investigatingdatacontaminationpretraining} differentiated between text contamination (input samples) and true contamination (input-output pairs). \citet{liu-etal-2024-evaluating} exposed Chinese LLMs' superficial knowledge despite broad training exposure. \citet{sainz-etal-2023-nlp} highlighted that current evidence on contamination remains fragmented across publications and informal channels, suggesting that the prevalence of contamination may be significantly underestimated.

%\citet{sainz-etal-2023-nlp} indicated current evidence remains fragmented across publications and informal channels, suggesting underestimated contamination prevalence.

\subsubsection{Factors Discussion}
In this section, we discuss some factors influencing contamination. \citet{magar-schwartz-2022-data} found that exploitation of contaminated data is influenced by factors like model size, learning rate, and the position of contaminated data, suggesting that memorization does not always lead to exploitation. \citet{mehrbakhsh-etal-2024-confounders} designed GPT-4-generated templates to investigate how the complexity of test instances influences the contamination in Llama-2 7B, aiming to better understand how varying levels of difficulty and diversity in the templates can influence the model's performance.
\citet{singh2024evaluation} proposed a new contamination evaluation protocol, ConTAM, to explore how data contamination affects the evaluation results of LLMs, and provided a method to quantify the impact of contamination.

\subsubsection{Non-Contamination Scenarios}
%In this section, we discuss Non-Contamination scenarios. \citet{dekoninck2024constat} established a causal relationship between model performance improvement and data contamination, defining the cases where overlap between training and testing data exists while it does not lead to performance improvement as non-contamination. \citet{palavalli-etal-2024-taxonomy} clarified several phenomena that improve performance on downstream tasks but are not regarded as influenced by contamination, including language understanding, prior task understanding, and transductive learning. These phenomena contribute to better empirical results without violating the integrity of the task or model.
In this section, we explore non-contamination scenarios, where the overlap between training and testing data does not lead to performance improvement. \citet{dekoninck2024constat} established a causal relationship between model performance improvement and data contamination, explicitly defining cases where such overlap exists but does not enhance performance as non-contamination. Furthermore, \citet{palavalli-etal-2024-taxonomy} clarified several phenomena that improve performance on downstream tasks without being influenced by contamination. These include language understanding, prior task understanding, and transductive learning. These phenomena enhance empirical results while preserving the integrity of both the task and the model, distinguishing them from contamination-related performance gains.

\subsubsection{Quantifying contamination}
\label{chap:quantifying}
Contamination scoring mechanisms classify evaluation samples through threshold-based indices. We have summarized some common contamination detection methods in table \ref{tab:ngram}. For instance, \citet{brown2020language} used N-grams to evaluate contamination by checking whether each token in the tested sample appears in an n-gram from the pre-training corpus. \citet{chowdhery2023palm} calculated the contamination score based on the proportion of contaminated n-grams. In contrast, \citet{touvron2023llama2} introduced a method to align extensions between the testing samples and pre-training corpus, allowing mismatches in certain token positions using a "skip\_budget" hyperparameter. \citet{singh2024evaluation} further extended this method, focusing on the longest contaminated token span rather than all potential matches. \citet{riddell2024quantifyingcontaminationevaluatingcode} employed the Dolos toolkit \cite{maertens2022dolos} to measure semantic similarity by converting programs into abstract syntax trees (ASTs) and performing k-gram matching.




