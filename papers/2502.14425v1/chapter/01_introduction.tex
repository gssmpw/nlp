\label{sec:introduction}

Recent breakthroughs in Large Language Models (LLMs) have demonstrated remarkable capabilities in text generation, code synthesis, and mathematical reasoning~\cite{zhao2023survey,openai2024gpt4technicalreport,deepseekai2025deepseekr1incentivizingreasoningcapability}.
However, the reliability of LLM evaluation is increasingly questioned due to data contamination-the unintended overlap between training and test data sets \cite{balloccu-etal-2024-leak,chang2024survey}. This is especially problematic as LLMs use large web-scraped datasets that are prone to overlap with testing benchmarks. 
%LLMs are known to memorize parts of their training data, and when prompted, they can emit this data verbatim \cite{carlini2022quantifying}. \citet{sainz-etal-2023-nlp} highlighted a critical consequence incurred by data contamination: scientific papers relying on contaminated LLMs may make erroneous claims, potentially invalidating true hypotheses. Moreover, \citet{ippolito-etal-2023-preventing} found that contaminated models can align copyright-protected content post hoc, presenting an even greater challenge for LLM development. To help researchers recognize the importance of data contamination in LLM developing and evaluation, we provide a comprehensive review on data contamination for LLMs. 
LLMs are known to memorize portions of their training data, and under certain prompts, they can reproduce this data verbatim \cite{carlini2022quantifying}. As highlighted by \cite{sainz-etal-2023-nlp}, a critical consequence of data contamination is that scientific studies relying on contaminated LLMs may produce erroneous conclusions, potentially invalidating valid hypotheses. Furthermore, \citet{ippolito-etal-2023-preventing} demonstrated that contaminated models can inadvertently align with copyright-protected content post hoc, posing significant challenges for the responsible development of LLMs. To underscore the importance of addressing data contamination in both the development and evaluation of LLMs, we present a comprehensive review of data contamination issues in this paper.

In section \ref{sec:what}, we define data contamination as the inclusion of data from the testing set during the pre-training phase, which artificially inflates model performance. Recent studies extend this definition along two dimensions: phase-based contamination in LLMs' lifecycle and benchmark-based contamination in LLMs' evaluation. For phase-based analysis, contamination mechanisms include pre-training phase leakage, fine-tuning biases, cross-modal leakage \cite{yao-etal-2024-data}, and indirect human interactions \cite{palavalli-etal-2024-taxonomy}. Meanwhile, benchmark-based contamination operates at two granularities: instance-level contamination, and dataset-level contamination. Dataset-level contamination is categorized by severity into simple rewriting, label leakage, text leakage, and dual text-label leakage. The impacts are discussed in the following four areas: collecting evidence, factors discussion, non-contamination scenarios, and quantifying contamination.

In section \ref{sec:where}, we discuss how to achieve contamination-free evaluation. For static benchmarks, current research focuses on three key contamination-free strategies: automatically updating datasets using the most recent data, rewriting existing data, and implementing proactive risk prevention mechanisms. Meanwhile, dynamic evaluation frameworks\cite{zhu2024dyvaldynamicevaluationlarge,lei-etal-2024-s3eval,zhang2024darg,ying2024automating} generate test samples using techniques like combinatorial optimization, graph-based reasoning, and controlled randomization, creating an evolving evaluation system. Additionally, the LLM-as-a-Evaluator paradigm\cite{bai2024benchmarking} turns LLMs into meta-evaluators, enabling intelligent assessments independent of static benchmarks.

%In section \ref{sec:how}, we discuss how to detect data contamination. We first categorize data contamination detection approaches into three paradigms: white-box detection, which leverages full access to model architectures or training data for high precision, such as using N-gram overlap\cite{brown2020language} or embedding similarity\cite{reimers2019sentence}; gray-box detection, which utilizes partial model information like token probabilities to detect contamination; and black-box detection, which operates without access to model internals, using heuristic rules referenced in Appendix \ref{sec:definition of assumptions}. These approaches together reflect an evolving landscape of data contamination detection methods.
In section \ref{sec:how}, we explore methodologies for detecting data contamination in LLMs. We categorize data contamination detection approaches into three distinct paradigms: white-box detection, which relies on full access to model architectures or training data to achieve high precision, employing techniques such as N-gram overlap \cite{brown2020language} or embedding similarity \cite{reimers2019sentence}; gray-box detection, which leverages partial model information, such as token probabilities, to identify contamination; and black-box detection, which operates without access to internal model details, relying instead on heuristic rules (the details are outlined in Appendix \ref{sec:definition of assumptions}). Together, these approaches illustrate the evolving and multifaceted landscape of data contamination detection methods, each offering unique advantages and challenges.

%\textbf{Our contributions are as follows:}
%\begin{enumerate}
    %\item We provide a comprehensive overview of LLM data contamination from three perspectives: what, where, and how.
    %\item We offer a more detailed discussion of the definition of data contamination from both the phase and benchmark perspectives.
    %\item Regarding where to evaluate, we summarize the main methods for contamination-free evaluation.
    %\item In terms of how to detect contamination, we explore current protocols and summarize novel detection methods.
%\end{enumerate}

The organization of this paper is as follows, as shown in figure \ref{tab:tree}. In Section \ref{sec:what}, we discuss existing work on the definition and impacts of data contamination. Section \ref{sec:where}  summarizes current methods for constructing contamination-free datasets and dynamic evaluation approaches. Section \ref{sec:how} discusses how to detect data contamination. Finally, in Section \ref{sec:future directions}, we present several significant future challenges in this area.

\input{tables/fig-tree}