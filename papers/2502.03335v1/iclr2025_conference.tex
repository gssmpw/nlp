
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}
\usepackage{bibentry}
\usepackage{bm}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{subcaption}
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{xcolor}
\usepackage{multirow}
%\newtheorem{assumption}{Assumption}
\usepackage{tabularx}
\usepackage{booktabs} 
\usepackage{comment}
\usepackage{amsmath}
%\usepackage{subfig}
\usepackage{bbding}
\usepackage{pifont}
\usepackage{utfsym}
\usepackage{soul}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{amsmath} % For mathematical typesetting
\usepackage{amssymb} % For \mathbb
%\usepackage[x11names]{xcolor} % For colors like royalblue, lavender, etc.
% OR
%\usepackage[dvipsnames]{xcolor} % For a slightly different set of extended colors

%\usepackage{algorithmic}
%\usepackage{algorithm}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\renewcommand{\algorithmiccomment}[1]{\hfill\textit{// #1}}

\newtheorem{theorem}{\textbf{Theorem}}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{\textbf{Assumption}}
\newtheorem{lemma}{\textbf{Lemma}}
\newtheorem{remark}{Remark}
\newtheorem{proposition}{Proposition}
\newtheorem{problem}{\textbf{Problem}}
\newtheorem{example}{Example}
\newtheorem{definition}{\textbf{Definition}}

\renewcommand{\thefootnote}{*} % 将默认编号改为*
\setcounter{footnote}{0} % 重置脚注计数器（从*开始）


\title{Actions Speak Louder Than Words:\\ Rate-Reward Trade-off in Markov Decision Processes}

%\title{Actions Speak Louder Than Words:\\ Rate-Reward Trade-off in Communication through Actions}

%\title{Act2Comm: Act to Communicate towards Integrated Control and Communication}

%"Act2Comm: Shaping Action Policies to Integrate Control and Communication Systems for Optimized Performance"

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
%%% Funding acknowledgements go at the end of the paper.
\author{Haotian Wu\textsuperscript{\ensuremath{*}}, Gongpu Chen\textsuperscript{\ensuremath{*}}\textsuperscript{\ensuremath{\dagger}}, Deniz Gündüz \\
Department of Electrical and Electronic Engineering\\
Imperial College London,\\
London SW7 2AZ, U.K. \\
\texttt{\{haotian.wu17,gongpu.chen,d.gunduz\}@imperial.ac.uk} \\
%\thanks{*equal contribution}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}
\maketitle
\footnotetext{These authors are contributed equally to this work.{ \ensuremath{\dagger}} Corresponding author.}
\begin{abstract}
The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a Markov decision process (MDP) framework. We conceptualize the MDP environment as a finite-state channel (FSC), where the actions of the agent serve as the channel input, while the states of the MDP observed by another agent (i.e., receiver) serve as the channel output. Here, we treat the environment as a communication channel over which the agent communicates through its actions, while at the same time, trying to maximize its reward. We first characterize the optimal information theoretic trade-off between the average reward and the rate of reliable communication in the infinite-horizon regime. Then, we propose a novel framework to design a joint control/coding policy, termed \textit{Act2Comm}, which seamlessly embeds messages into actions. From a communication perspective, \textit{Act2Comm} functions as a learning-based channel coding scheme for non-differentiable FSCs under input-output constraints. From a control standpoint, \textit{Act2Comm} learns an MDP policy that incorporates communication capabilities, though at the cost of some control performance. Overall, \textit{Act2Comm} effectively balances the dual objectives of control and communication in this environment. Experimental results validate \textit{Act2Comm}'s capability to enable reliable communication while maintaining a certain level of control performance. Project page: \url{https://eedavidwu.github.io/Act2Comm/}

%%%%Actually, this is a new ISAC.
%%%%Why not borrow the idea from the torre?
%%%%sequence of decision rules, but it is function for continuous functions?....
%%%%Why: we found a case that it have much large coding space, such as actions flexible......






%, which enable information sharing for improved coordination and cooperation among distributed agents. 
%albeit at the expense of some control performance.
%Building upon transformer-based block-attention-feedback codes, the encoder and decoder of \textit{Act2Comm} are trained using an iterative optimization strategy. We employ a critic network in the encoder to approximate the loss function and utilize another neural network to search for the optimal encoding scheme. The decoder is trained through supervised learning. 
 
%\textit{Act2Comm} initially can serve as a practical channel coding scheme for FSCs, demonstrating competitive performance. Additionally, to address the challenges of balancing dual control and communication objectives and the non-differentiable nature of the environment, \textit{Act2Comm} utilizes a critic network, transformer-based block-attention-feedback codes, and an iterative optimization strategy. This design allows \textit{Act2Comm} to be seamlessly integrated into various MDP control scenarios, enhancing communication capabilities without incurring overhead. Specifically, our investigation into the trade-offs between MDP control and communication optimization reveals that the proposed \textit{Act2Comm} scheme can effectively integrate communication capabilities into the MDP control paradigm with minimal impact on control performance.

\end{abstract}

\section{Introduction}
%Communication plays a vital role in modern intelligent systems, as it could significantly enhance decision-making and coordination among agents. 
The role of communication in multi-agent systems has received significant attention as it allows agents with a partial view of the system to better coordinate and cooperate by exchanging messages in parallel to actions taken in the environment \citep{foerster_learning_2016, Sukhbaatar:NeurIPS:16}. However, these systems rely on dedicated channels for communication. On the other hand, explicit communication channels may not always be available, or may be complemented with other \textit{implicit} forms of communication. Such examples are abundant in nature. Bacteria communicate through chemical molecules, known as quorum sensing \citep{Waters:ARCDB:05}, altering their environment and behavior to achieve population-wide coordination. Ants use pheromones to encode the path to food for other ants \citep{Thienen:BES:14}. Non-verbal communication through gestures, gaze, and even physical appearance, is also known to play an important role in human communication \citep{Trenholm:20}. In the artificial realm, autonomous robots may also need to rely on implicit communications when explicit communication channels are not available. In medical nano-robots, electromagnetic communication is not feasible due to size and energy limitations, but implicit communication can be achieved through molecular communications \citep{Weiss:DNA:01, Wang:SIGCOMM:23}. Even for more advanced robots, electromagnetic or other types of explicit communication channels may not be available in harsh or hostile environments; for example, for robots decommissioning nuclear storage facilities, or those operating in deep space, deep ocean, or subterranean environments, e.g., tunnels and caves \citep{Ebadi:TR:24}. Moreover, wireless signals are prone to wiretapping due to their broadcast nature \citep{Poor:PNAS:17}, and can be unreliable in adversarial scenarios due to jamming \citep{Pirayesh:CST:22, martz2020survey}, which are other factors limiting explicit communications. 


Motivated by these challenges, this paper explores implicit communication in a Markov Decision Process (MDP) environment---\emph{communication through actions}---which facilitates information transmission from the MDP controller to other agents that can observe the MDP states. Effectively utilizing this internal channel has the potential to reduce the dependence on dedicated communication channels. However, these communication capabilities come with inherent trade-offs. As we will demonstrate, using this channel for communication often leads to a degradation in MDP control performance. This raises a fundamental challenge in balancing control and communication objectives, underscoring the need for a cohesive design that integrates both control and communication.

An MDP consists of a controller and an environment \citep{puterman2014markov}. At each time step, the environment is in some state $s$, and the controller selects some action $a$. Upon executing action $a$, the environment stochastically transitions to a new state $s'$ and generates a reward. The controller's objective is to find an optimal policy for selecting an action at each time to maximize its accumulated reward over a given time horizon. 
Consider that the controller now wants to communicate with another agent in the same environment that can also observe the environment state. 

In communication theory, a finite-state channel (FSC) \citep{gallager1968information} is an input-output system with states, where the output depends on both the input and the current state. Messages are encoded into the input sequence, resulting in a corresponding output sequence. The receiver decodes the message from this output sequence. From this perspective, the state transition of an MDP from $s$ to $s'$ upon taking action $a$ can be viewed as an FSC from the controller to the agent. We refer to this internal channel within the MDP as an \emph{action-state channel}. To communicate through the \textit{action-state channel}, we need an encoder that maps the message to a sequence of actions, and a decoder that translates the resulting state sequence back into the original message. %Clearly, both the encoder and the controller are responsible for selecting an action at each time step. 
However, the objective of this encoder differs from that of the controller: the encoder aims to maximize the transmission rate and reliability of its message, while the controller seeks to maximize the accumulated reward. These two objectives are generally inconsistent, and a trade-off between the two must be sought.

In this paper, we first investigate the trade-off between the capacity (i.e., the maximum achievable transmission rate) of the \textit{action-state channel} and the MDP reward in the infinite horizon regime. We demonstrate that the capacity of this channel can be expressed in a simple form---as the conditional mutual information between the input and output conditioned on the channel state. We also show that the capacity-reward trade-off can be characterized by a convex optimization, which can be solved numerically. {\color{black} While the capacity-reward trade-off provides an upper bound on the practically achievable rate under certain reward constraints, solving it does not yield a practical coding scheme.} We then propose a practical framework for the integrated control and communication task \textcolor{black}{in the finite block-length regime.}
The challenge of designing such a framework is twofold: (1) balancing the control and communication performance, and (2) dealing with the non-differentiability of the \textit{action-state channel}. %, which prevents the joint training of the encoder and decoder. 
To tackle these issues, we propose \textit{Act2Comm}, a transformer-based coding scheme in which encoder and decoder are trained iteratively. %When the decoder is fixed, a critic network in the encoder approximates the gradient of the loss function to help the encoder optimize the coding policy. Conversely, when the encoder is fixed, the decoder is trained using supervised learning. 
 
\textbf{Contributions.} The main contributions of this paper are summarized as follows: (1)  We introduce a novel paradigm of communicating through actions within an MDP environment, framing it as an integrated control and communication problem. (2) We derive the capacity of the action-state channel, and characterize the capacity-reward trade-off as a convex optimization. (3)  We propose \textit{Act2Comm}, a \textcolor{black}{practical} transformer-based coding scheme to learn a policy that optimizes communication performance while maintaining a specified level of MDP reward. \textit{Act2Comm} can be of independent interest for designing practical channel coding schemes over other non-differentiable FSC scenarios. %,exhibiting competitive performance across various code lengths.  


\section{Related work} 
Communication plays a significant role in MDPs, especially in multi-agent reinforcement learning (RL), where agents exchange messages over dedicated or noisy links  to achieve a common goal \citep{wang_learning_2020,chen2024rgmcomm,tung_effective_2021}. 
%Communication plays a significant role in MDPs, particularly in the multi-agent scenarios. Multi-agent reinforcement learning (RL) with communication among agents has received significant attention in recent years, where the agents exchange messages over a dedicated link \citep{foerster_learning_2016, wang_learning_2020,chen2024rgmcomm}, including over noisy channels \citep{tung_effective_2021} to achieve a common goal. 
This is known as \textit{emergent communications} \citep{boldt2024a}, but this framework relies on explicit communications over dedicated channels.  Implicit communication through actions is considered by \cite{Knepper:HRI:17} and \cite{tian2019learningcommunicateimplicitlyactions}. The latter also trains a policy, but it focuses on the multi-agent scenarios, and encourages communication by appropriately changing the reward function. We do not explicitly specify the communicated information, and instead, take a more fundamental approach by characterizing the information theoretic limits of communication and designing a practical coding policy.
% We do not explicitly specify the communicated information, and instead, take a more fundamental approach and characterize the information theoretic limits of communication, and design a policy to implement it in practice. 

{\color{black} \cite{sokota2022communicating} explored a similar concept of communication via MDPs. In their study, the receiver can observe the entire trajectory, including both the action and state sequences. This enables the controller to encode (compress) messages into the action sequence, and the receiver can subsequently decode the messages from the trajectory. Essentially, this is a randomized source coding problem. However, in most practical scenarios, while the MDP state is a physical signal observable by the receiver, the controller’s actions are usually not observable to other agents. Therefore, in our work, we assume the receiver can only observe the state sequence. This shifts the problem from source coding to channel coding. \cite{Karabay2019} also examined a similar system, but their focus was on developing policies that restrict the observer’s ability to infer transition probabilities. }

FSC represents a general class of communication channels, and its study has been a long-standing problem in information and coding theory. \textcolor{black}{\cite{blackwell1958proof}} studied the capacity of indecomposable FSCs without feedback. Subsequent studies in the non-feedback setting include \cite{Verdu1994} and \cite{Goldsmith1996}. The capacity of FSCs with feedback was examined by \cite{massey1990causality} and \cite{haim2007}. More recently, \cite{shemuel2022finite} explored the capacity of FSCs with feedback and state information at the encoder. However, these results express capacity in multi-letter forms, relying on the entire input and output sequences as their lengths approach infinity.
Although \cite{haim2017} provided a single-letter upper bound for the feedback capacity of unifilar FSCs, exact single-letter expressions for FSC capacity are generally unknown. The action-state channel studied in this paper is a special FSC with state and feedback at the encoder. Utilizing the unique structure of this channel, we derive a single-letter expression for its capacity.

Machine learning has recently advanced traditional channel coding schemes by replacing linear operations with trainable non-linear neural networks, including Turbo autoencoder \citep{jiang2019turbo}, DeepPolar \citep{hebbar2024deeppolar}, KO codes \citep{makkuva2021ko}, and other approaches \citep{jiang2020learn,kim2018deepcode}.
%Machine learning for channel code design has recently attracted significant research attention. Many recent studies have enhanced traditional coding schemes by replacing their linear operations with trainable non-linear neural networks, including Turbo autoencoder \citep{jiang2019turbo}, DeepPolar \citep{hebbar2024deeppolar}, KO codes \citep{makkuva2021ko}, and others \citep{jiang2020learn,kim2018deepcode}.
However, these are designed for Gaussian channels, which are differentiable and allow joint training of the encoder and decoder. Our channel, in contrast, is non-differentiable, presenting new challenges for the design of the encoder and decoder. Channel coding for FSCs is a challenging task with limited results in the literature. Some existing work focuses only on the design of the decoder \citep{aharoni2023data}. However, the main challenge in our problem lies in designing the encoder to balance control and communication performance.


%Vectors are denoted by lowercase bold symbols, while matrices are represented by uppercase bold symbols.
%Given a positive integer $n$, we use $[n] \triangleq \{1, 2,\ldots, n\}$ to denote the set of integers between $1$ and $n$. 
%For any set, $|\mathcal{S}|$ denotes the cardinality of the set $\mathcal{S}$.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.65]{image/Fig_1.pdf}
     \caption{\small\textcolor{black}{{From a standard finite-state channel to an \textit{action-state channel}.}}}
     \label{FSC_fig_1}
     \vspace{-10pt}
\end{figure}

\noindent{\it Notations}: For any $x_t$ with $t\ge1$, $\bm{x_i^k}$ denotes the sequence $\{x_i,x_{i+1},\ldots,x_{k}\}$, where $\bm{x_1^k}$ is written as $\bm{x^k}$. %Lowercase and uppercase bold symbols represent vectors and matrices, respectively. 
$|\mathcal{X}|$ denotes the cardinality of the set $\mathcal{X}$. \textcolor{black}{A detailed notation table is provided as Table. \ref{Tab_notation}.}

\section{Preliminaries and System Model}
%This section presents the foundational preliminaries and problem formulation for the proposed integrated MDP control and communication system. 
%The letter with an upper index (e.g., $\bm{X}^i$) denotes a specific instance of the input ($\bm{X}$). For a vector or matrix, $(\cdot)^H$ denotes the conjugate transpose. $\mathbb{R}$ and $\mathbb{C}$ stand for the sets of real and complex values, respectively. %$\mathcal{N}$ stand for the real Gaussian distributions. 

\paragraph{Markov Decision Process (MDP).}
An MDP can be characterized by a tuple $(\mathcal{S}, \mathcal{X}, \bm{T}, r, \alpha)$, where $\mathcal{S}$ is the state space, $\mathcal{X}$ is the action space, $\bm{T}$ is the transition kernel, $r:\mathcal{S}\times \mathcal{X}\rightarrow \mathbb{R}$ is the bounded reward function, and $\alpha$ is the initial state distribution.
At each time step $t$, taking action $x_t$ in state $s_t$ results in a reward $r(x_t, s_t)$ and a state transition from $s_t$ to $s_{t+1}$, where $s_{t+1}$ is sampled from the distribution $\bm{T}(\cdot|s_t,x_t)$.
%For ease of reference, we index the states and actions, defining the set of states and actions as $\mathcal{S}=\{0,1,\ldots, L-1\}$ and $\mathcal{X}=\{0,1,\ldots, A-1\}$, where $L$ and $A$ are the total number of states and actions. We assume that both $L$ and $A$ are finite.
We assume both $\mathcal{S}$ and $\mathcal{X}$ are finite sets.
%The average reward of a MDP with initial state distribution $\alpha$ and a control policy $\pi$ can be given by:
A stationary deterministic policy is a mapping $\pi:\mathcal{S}\to \mathcal{X}$ that selects action $x_t$ based on state $s_t$ at each time $t$. The objective is to find an optimal policy that maximizes the long-term average reward, as follows:
\begin{equation}
%\vspace{-3pt}
    \textbf{P1:}\quad \max_{\pi}\ \lim_{N\rightarrow\infty}\frac{1}{N}\mathbb{E}\left[\sum_{t=1}^{N}r(s_t,x_t)|s_1\sim\alpha\right].
    %\vspace{-3pt}
\end{equation}

In this paper, we assume that the MDP is unichain; that is, any deterministic policy induces a Markov chain consisting of a single recurrent class plus some transient states. As a result, the optimality of \textbf{P1} can be achieved through a stationary deterministic policy. The set of stationary deterministic policies is denoted by $\Pi_{SD}$. It is worth noting that the set of admissible policies for an MDP is not restricted to $\Pi_{SD}$. In general, a policy can be history-dependent, determining \textcolor{black}{$x_t$} using all historical states and actions up to time $t$. Let $\Pi_S$ and $\Pi_H$ denote the sets of stationary (possibly randomized) and history-dependent policies, respectively.
It is easy to see that $\Pi_{SD}\subset \Pi_S \subset \Pi_{H}$.

%\begin{itemize}
 %   \item History-dependent policy: a history-dependent policy considers the entire history of states and actions up to the current time step when deciding the next action. The set of all history-dependent policies is denoted as $\prod_{H}$.    
  %  \item Markov policies: a Markov policy is a type of policy that adheres to the Markov property, which makes decisions based on the current state alone. We denote the set of all Markov policies as $\prod_{M}$.
    %\item Stationary policy: a stationary policy is a strategy used to make decisions that is consistent over time, where $\prod_{S}$ represents the set of all stationary policies.
%\end{itemize}

\paragraph{Finite-State Channel (FSC).}
%Without loss of generality, we consider the coding process for an FSC with channel feedback, where the feedback link can be disregarded while the other components remain unchanged for the case of an FSC without feedback link. The encoding process at time step $t$ for an FSC with feedback is represented as a function, given as:
As illustrated in Fig. \ref{FSC_fig_1}, an FSC can be characterized by a tuple $(\mathcal{X}\times \mathcal{S}, P_{Y, S^{+}|X, S}, \mathcal{Y}\times \mathcal{S})$, where
$\mathcal{X}$ is the input alphabet, $\mathcal{Y}$ is the output alphabet, $\mathcal{S}$ is the channel state alphabet, and $P_{Y, S^{+}|X, S}$ is the channel law specifying the probability of the channel outputting $Y$ and transitioning to the new state $S^+$, given that the channel input is $X$ in state $S$. 
We consider a time-invariant channel that exhibits the Markov property, which can be formally expressed as:
\begin{equation} 
P(y_{t},s_{t+1}|\bm{x^{t}},\bm{s^{t}},\bm{y^{t-1}}) = P_{Y,S^{+}|X,S}(y_{t},s_{t+1}|x_{t},s_{t}),\ \forall t.
\vspace{-3pt}
\end{equation}

To transmit a message $m$, an encoder generates a sequence of channel inputs $\bm{x}^t$ as codewords. When each $x_t$ is input to the channel, the channel transitions to a new state $s_{t+1}$ and produces an output $y_t$. The decoder then collects the output sequence $\bm{y}^t$ to reconstruct $m$. We suppose that the channel state is available to the encoder but not to the decoder, and that the outputs are fed back to the encoder. 
%By using both the historical states and the feedback sequences, the encoder can optimize codeword generation, thereby potentially enhancing overall communication performance.

Let $\mathcal{M}$ denote the set of messages, with each message $m$ uniformly sampled from $\mathcal{M}$. The encoder is defined as a sequence of mappings, $\mathcal{E}\triangleq\{\mathcal{E}_1,\ldots,\mathcal{E}_n\}$, where each mapping $\mathcal{E}_{t}: \mathcal {M} \times {\mathcal S}^{t} \times \mathcal X^{t-1}\times {\mathcal {{Y}}}^{t-1} \to {\mathcal X}$ generates the channel input at time $t$. In other words, the channel input at time $t$, $x_t=\mathcal{E}_{t}({m},\bm{s^t},\bm{x^{t-1}},\bm{{y}^{t-1}})$, is a function of $m$ and all the historical information available at the transmitter up to time $t$. The decoder is defined as the mapping, $\mathcal{D}: \mathcal {Y}^n \to {\mathcal M}$, which reconstructs the message from all $n$ channel outputs, $\hat{m}=\mathcal{D}(\bm{y}^n)$. The pair $(\mathcal{E}, \mathcal{D})$ constitutes a code, where $n$ represents the code length. Suppose the message set is $\mathcal{M}= \{1, 2, \ldots , 2^{k}\}$, then each message can be represented with $k$ bits. The rate of the code $(\mathcal{E}, \mathcal{D})$ is defined as $R_{\mathcal{E}, \mathcal{D}}=k/n$. 

The probability of error $P_e^{(n)}$ for $(\mathcal{E}, \mathcal{D})$ is defined as $P_e^{(n)}=\Pr(\mathcal{D}(\bm{y}^n)\neq m | m \text{ is sent})$. A rate $R$ is deemed achievable if there exists a code $(\mathcal{E}, \mathcal{D})$ such that the error probability of the transmission approaches zero as $n\to \infty$. Consequently, the \textit{capacity} of the FSC is defined as the supremum of all achievable rates. \textcolor{black}{In other words, channel capacity reveals the maximum rate required for error-free transmission when the code length approaches infinity.} In practice, however, constructing a code with infinite code length is unfeasible. Hence practical channel coding aims to balance the trade-off between $R$ and $P_e^{(n)}$ with a finite code length $n$. For instance, we design codes to maximize the rate while ensuring that the probability of error remains below a certain threshold $\sigma>0$:
\begin{equation}
\vspace{-3pt}
\begin{array}{cl}
 \textbf{P2:}& \max\limits_{{\mathcal{E}, \mathcal{D}}} R_{\mathcal{E}, \mathcal{D}},\quad \text{subject to }P_e^{(n)} \le \sigma.  
\end{array}
\vspace{-3pt}
\label{eq_p2}
\end{equation}


\paragraph{Integrated Control and Communication.}
We investigate a scenario in which the controller of an MDP aims not only to optimize rewards but also to facilitate communication. Assuming that the receiver can observe the state of the MDP, then the environment can be modeled as a specialized FSC, enabling communication between the transmitter (i.e., the controller) and the receiver. 

%as shown in Fig. \ref{FSC_fig_1} (b). 
%Specifically, we model the MDP as a specialized FSC integrated with both a controller and an encoder at the transmitter. These components work jointly to generate an action ${x_t}$ at each time step into the FSC, aiming to optimize both the reward and communication performance. Simultaneously, the decoder at the receiver is optimized to reconstruct the transmitted messages based on the observed states. 
%with the dual objective of optimizing the reward and communication. 
%This study explores the trade-off between the MDP reward and the communication performance. 
%However, their specific goals differ: the controller seeks to maximize the reward, while the encoder aims to maximize the channel rate. 

\textbf{{(a) {Action-state channel} model:}} 
%\textbf{{(a). Action-state channel model:}} 
%The transmitter agent inputs actions $\bm{x_t}$ into the FSC, and the receiver agent decodes the message based on the observed states.
%the agent equipped with an encoder and controller functions as the transmitter, while other agents with a decoder serves as the receiver.
%Consequently, we have $\mathcal{Y} =\mathcal{S}$, when referring back to a standard FSC channel. 
This integrated FSC is referred to as the \textit{action-state channel}, where the state of the MDP aligns with the state of the FSC. The action and the subsequent state are viewed as the channel input and output, respectively. Upon executing $x_t$ in state $s_t$, the MDP environment returns a reward $r_t$ and transitions to a new state $s_{t+1}$. Here $s_{t+1}$ functions as both the channel output and the new channel state, and $\bm{s^{t-1}}$ represents not only the historical state sequence, but also the historical feedback signal.
The channel law of the \textit{action-state channel} is given by:
%Referring back to the terminology established for the FSC in the preceding section, 
\begin{equation}
P(s_{t+1}|\bm{x^{t}},\bm{s^{t}}) = P_{S^+|X,S}(s_{t+1}|x_t,s_t) = \bm{T}(s_{t+1}|x_{t},s_{t}).   
\end{equation}
 This type of channel is also referred to as a POST channel in the literature \citep{6866225}. 

\textbf{{(b) Controller \& Encoder:}} 
%\textbf{(b). {Controller \& Encoder:}} 
Within this framework, the MDP controller and the FSC encoder represent the two aspects of the same entity, jointly responsible for selecting an action $x_t\in \mathcal{X}$ at each time step. However, their objectives differ: the controller aims to maximize the reward, while the FSC encoder seeks to maximize the message rate. 
%In the following, we will use `controller' and `encoder' interchangeably to refer to the entity that determines $x_t$ at each time $t$.
Unlike the controller, which can focus on stationary deterministic policies, the encoder must account for more complex policy forms. 
For any message $m$, the encoder described in the previous part can be viewed as a history-dependent policy for the MDP. 
Therefore, we consider the joint control and coding policy in its most general form. The policy $\mathcal{E}$ is represented as a sequence of mappings $\{\mathcal{E}_i:0\le i \le n-1\}$, where $\mathcal{E}_i$ is defined as $\mathcal{E}_i:\mathcal {M} \times {\mathcal S}^{i} \times \mathcal X^{i-1} \to {\mathcal X}$. Each message is transmitted via a sequence of $n$ actions. For example, if the controller begins to transmit a message $m$ at time $t$, then $x_{t+i}=\mathcal{E}_i(m, \bm{s}_t^{t+i}, \bm{x}_t^{t+i-1})$ for $0\le i\le n-1$. We assume the controller always has a new message ready for transmission immediately after completing the transmission of the previous message, and each message is uniformly sampled from $\mathcal{M}$.  The long-term average reward of the MDP under policy $\mathcal{E}$ is denoted by $G_{\mathcal{E}}$.

\textbf{{(c) Decoder:}}
%By observing the MDP states, the receiver with a decoder reconstructs the transmitted message. 
The receiver observes the state sequence associated with a message and \textcolor{black}{uses} it to decode the message. For example, the state sequence associated with the $i$-th message is $\bm{s}_{in-n+1}^{in}$.
The decoder, represented as a mapping : $\mathcal{D}: {\mathcal S}^{n} \to {\mathcal M}$, decodes the message as: 
%\begin{equation}
    $\hat{m}=\mathcal{D}(\bm{s}_{in-n+1}^{in})$.
%\end{equation}

{In this paper, we consider non-terminating MDPs over an infinite time horizon and investigate the trade-off between control and communication performance. As discussed previously, if the code length $n\to \infty$, the communication performance can be characterized by the channel capacity (i.e., the maximum achievable rate).} Here, we consider a practical setting with finite code length $n$, where the performance of a code $(\mathcal{E,D})$ is characterized by its rate $R_{\mathcal{E,D}}$ and the error probability $P_e^{(n)}$. We study the trade-off through the following optimization problem with constants $V$ and $\sigma>0$,
\begin{align}%\vspace{-3pt}
    \textbf{P3:}\quad \max\limits_{\mathcal{E,D}}& \ \ R_{\mathcal{E,D}}\\
s.t. & \ \ G_{\mathcal{E}}\geq V\quad \text{and} \quad P_e^{(n)}\le \sigma.
\vspace{-3pt}
\end{align}

\section{The Capacity-Reward Trade-off} \label{sec:tradeoff}
{In this section, we analyze the trade-off between the capacity of the \textit{action-state channel} and the MDP reward. While the results may not offer direct guidance for practical coding---since the capacity is typically achievable only in the infinite-horizon regime (i.e., when the code length $n\to \infty$)---they delineate the fundamental performance limits of communication via actions in MDPs, thus holding substantial theoretical importance. All proofs are detailed in Appendix \ref{app:proof}.}

In information theory, the capacity of an FSC is usually expressed in terms of conditional mutual information \citep{shemuel2023finite}. Let $X,S^+$ and $S$ denote the random variable associated with the input, output (i.e., the next state), and the current state of the \textit{action-state channel}, respectively. Then the conditional mutual information of $X$ and $S^+$ given $S$ is defined as \citep{cover2005elements}:
\begin{align}
\vspace{-3pt}
    I(X; S^+|S) = \mathbb{E}_{p(x,s^+,s)} \left[\log \frac{p(s,s^+|s)}{p(x|s)p(s^+|s)} \right].
    \vspace{-3pt}
\end{align}
Let $\pi(\cdot|s)$ denote an input distribution of the channel given that the channel state is $s\in \mathcal{S}$.
For a given channel, the joint distribution $p(x,s^+,s)$ is determined by the conditional input distribution $\pi$. From the MDP perspective, $\pi(x|s)$ represents the probability of selecting action $x$ in state $s$; thus, $\pi$ can be viewed as a stationary randomized policy for the MDP. Let $\rho_\pi$ denote the equilibrium state distribution of the MDP under policy $\pi$. We have the following result:

\begin{theorem} \label{thm:cap}
    The capacity of the \textit{action-state channel} without reward constraint is given by
	\begin{align*}
		C=  \max_{\{\pi(x|s):x\in \mathcal{X},s\in \mathcal{S} \}}  I(X; S^+|S) 
	\end{align*}
	where $X, S$, and $S^+$ follow a joint distribution given by
	\begin{align*}
		p(x,s^+,s) = \rho_\pi(s)\pi(x|s)\bm{T}(s^+|s,x), \  x\in \mathcal{X}, s, s^+\in \mathcal{S}.
	\end{align*}
\end{theorem}

As previously discussed, a general encoder for an FSC generates a channel input based on all the historical state and feedback information. However, Theorem \ref{thm:cap} reveals a surprising fact: the capacity of the \textit{action-state channel} can be achieved by encoding messages into a stationary randomized policy for the MDP, without relying on historical information. 

Theorem \ref{thm:cap} presents the capacity of the \textit{action-state channel} without considering the MDP reward. If we want to maintain a certain level of long-term average reward for the MDP, the capacity may generally decrease. Next, we characterize the trade-off between channel capacity and MDP reward.

Given a stationary policy $\pi$ for the MDP, define $w_\pi(s,x)=\rho_\pi(s) \pi(x|s)$. Here, $w_\pi(s,x)$ represents the long-term proportion of time that the MDP is in state $s$ and takes action $x$. In the literature, $w_\pi$ is referred to as the occupation measure of policy $\pi$ \citep{altman2021constrained}. Let  $\mathcal{W} $ denote the set of all occupation measures, then $\mathcal{W} $ is the set of $w\in \mathbb{R}^{|\mathcal{S}|\times |\mathcal{X}|}$ satisfying the following equations: 
\begin{align} \label{eq: Q-def1}
	&\sum_{x\in \mathcal{X}} w(s,x) - \sum_{s'\in \mathcal{S}} \sum_{x'\in \mathcal{X}} w(s',x') \bm{T}(s|s', x') = 0, \ \forall s \in \mathcal{S}, \\ \label{eq: Q-def2}
	& \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} w(s,x) = 1,\  w(s,x)\ge 0, \ \forall s\in \mathcal{S},x\in \mathcal{X}.
\end{align}
Clearly, $\mathcal{W}$ is a polytope.
It is well-known that there is a one-to-one mapping between $\mathcal{W}$ and $\Pi_S$. In  particular, $\pi(x|s) = {w_\pi(s,x)}/{\sum_{x'} w_\pi(s,x')}$ for any $s\in \mathcal{S},x\in \mathcal{X}$.
Using this relationship, the problem of computing the capacity with reward constraint $V$ (i.e., ensuring the long-term average reward is not less than $V$) reduces to a convex optimization, as stated in the following theorem:

\begin{theorem} \label{thm:con-cap}
    The capacity of the \textit{action-state channel} with reward constraint $V$ is the optimal value of the following convex optimization problem:
    \begin{align*}
	\max_{w \in \mathcal{W}} \ &I(w,\bm{T}) \\
	s.t.  \ & \sum_{s\in \mathcal{S}}\sum_{x\in \mathcal{X}} w(s,x)r(s,x) \ge V 
\end{align*}
where $I(w,\bm{T})$ is a concave function of $w\in \mathcal{W}$ defined as
	\begin{align*}
		I(w,\bm{T}) \triangleq \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} w(s,x) \sum_{s'\in \mathcal{S}} \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}.
        \vspace{-5pt}
	\end{align*}
\end{theorem}

Denote by $C(V)$ the capacity of the \textit{action-state channel} with reward constraint $V$. We have: 
\begin{lemma} \label{lem:concave}
	$C(V)$ is a concave function.
\end{lemma}
\textcolor{black}{Since the capacity is an upper bound for the rate of any practical coding scheme, Lemma 1 implies that the achievable region of rate-reward pairs forms a convex set.}

The convex optimization problem in Theorem \ref{thm:con-cap} can be efficiently solved using the gradient ascent algorithm if the gradient of the objective function has a closed-form expression \citep{bertsekas1997nonlinear}. Next, we derive the gradient of $I(w,\bm{T})$ with respect to $w$. Define
\begin{align*}
	l(w,w_n,\bm{T}) \triangleq \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} w(s,x) \sum_{s'\in \mathcal{S}} \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w_n(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_n(s,x')}, \ w,w_n\in \mathcal{W}.
\end{align*}
%The following lemma is useful: 
\begin{lemma} \label{lem: tangent}
	For any $w_n\in \mathcal{W}$, $l(w,w_n,\bm{T})$ is a tangent line of $I(w,\bm{T})$ at point $w_n$. That is,
	\begin{itemize}[left=0.5cm]
		\item [(i)] $l(w_n,w_n,\bm{T}) = I(w_n,\bm{T})$.
		\item [(ii)] $l(w,w_n,\bm{T}) \ge I(w,\bm{T})$ for all $w$.
	\end{itemize}
\end{lemma}

It follows immediately from Lemma \ref{lem: tangent} that
\begin{align}
	\nabla I_{w_n}(s,x)\triangleq \left.\frac{\partial I(w,\bm{T})}{\partial w(s,x)} \right|_{w=w_n} = \sum_{s'\in \mathcal{S}} \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w_n(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_n(s,x')} ,
\end{align}
for any $w_n\in \mathcal{W}$, $s\in \mathcal{S}$, and $x\in \mathcal{X}$. The gradient $\partial I/\partial w=[\nabla I_{w}(s,x)]_{s,x}$ then can be used in the gradient descent method to solve the optimization problem in Theorem \ref{thm:con-cap}.


\section{Act2Comm: A practical coding scheme} \label{sec:act2comm}
This section presents \textit{Act2Comm}, a learning-based \textcolor{black}{practical coding} scheme that balances both control and communication objectives. This framework assumes a pre-determined control policy $\pi$ that satisfies the reward constraint $G_\pi \ge V$, referred to as the target policy. Such a target policy can be easily derived using traditional MDP or RL algorithms. Alternatively, solving the problem in Theorem \ref{thm:con-cap}  yields a policy $\pi$ that precisely satisfies $G_\pi=V$. \textit{Act2Comm} aims to learn a coding policy that: (1) closely mimics the stochastic behavior of the target policy; 
(2) minimizes the probability of decoding errors for a given coding rate and a finite code length. That is, \textit{Act2Comm} takes a policy achieving the desired reward, and embeds messages into it with the desired reliability. \textcolor{black}{ For ease of reference, we denote the element of matrix $\bm{X}$ at the $i$-th row and $j$-th column as ${X}[i,j]$. The sets of states and actions are indexed as $\mathcal{S}=\{0,1,\ldots, |\mathcal{S}|-1\}$ and $\mathcal{X}=\{0,1,\ldots, |\mathcal{X}|-1\}$.}%For vector state case, we refer to \ref{ab_state_vector}.

{\paragraph{\textcolor{black}{Channel transform.}} Let $\mathcal{U} \triangleq \mathcal{X}^{|\mathcal{S}|}$, where each ${u} \in \mathcal{U}$ is referred to as a decision rule because the $i$-th element of $u$ can be viewed as an action prescribed for state $i$. 
%Specifically, a decision rule specifies an action for each state; hence it can be used to determine an action for any given state. 
A deterministic control policy can be defined as a sequence $\{u_t:t\ge 1\}$, where $u_t\in \mathcal{U}$ is the decision rule at time $t$.
To facilitate block coding, we first convert the \textit{action-state channel} into an extended action-state (EAS) channel (Fig. \ref{FSC_fig_1_appendix} in Appendix) using Shannon's method \citep{shannon1958channels_ibm}. }
In particular, we conceptually separate the encoder and controller, considering the controller as an integral component of the EAS channel. The channel state is assumed to be available at the controller but not the encoder. At each time $t$, the encoder selects a decision rule ${u_t}$ from $\mathcal{U}$. Then the controller uses ${u_t}$ and the state $s_t$ to determine an action $x_t={u_t}(s_t)$. Consequently, the EAS channel has an input alphabet $\mathcal{U}$, output alphabet $\mathcal{S}$, and channel law: 
\begin{align}
    P_{S^+|S,U}(s_{t+1}|s_t,u_t) = P_{S^+|S,X}(s_{t+1}|s_t,u_t(s_t))=\bm{T}(s_{t+1}|s_t,u_t(s_t)).
\end{align}
The EAS channel and the \textit{action-state channel} are equivalent, and we will focus on the EAS channel to develop our coding scheme. \textcolor{black}{This approach allows us to map a data block to a sequence of decision rules without knowing future states. The actions for subsequent time steps can then be determined using these decision rules when the states are revealed.
In the rest of this section}, we detail the \textit{Act2Comm} framework in five main components. 
%the overall workflow, block-attention feedback coding, transceiver design, joint optimization of control and communication, and the iterative training strategy. 
%The sets of states and actions are indexed as

\paragraph{1) Overall workflow.} As depicted in Fig. \ref{Act_fig_pipeline}, given a $k$-bit message $\bm{m}\in\{0,1\}^k$, the $\textit{Act2Comm}$ first encodes $\bm{m}$ into a belief map $\bm{Z}\in\mathbb{R}^{|\mathcal{S}|\times \frac{k}{R}}$. This $\bm{Z}$ is subsequently mapped into a codeword $\bm{U}\in\mathcal{X}^{|\mathcal{S}|\times \frac{k}{R}}$ by a quantizer. 
At each time step $t$, the controller selects an action $x_t=\bm{U}[s_{t},t]$ for state $s_t$, and transitions into a new state according to the channel law $P(s_{t+1}|\bm{x^t},\bm{s^t})$. After $\frac{k}{R}$ time steps, the receiver decodes the message based on the accumulated observations $\bm{s}\in \mathcal{S}^{\frac{k}{R}}$. 
%To simplify the process, we use a encoder to generate the policy $u_t$, and employ a quantizer to map the continuous codeword into discrete actions for the channel.
%Then a controller, $\mathcal{Q}(\cdot):\mathbb{R}\rightarrow\mathcal{A}$, maps $\bm{Z}$ into a precise control policy $\bm{X}\in\mathcal{A}^{S\times \frac{k}{R}}$. 
\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.5]{image/main_fig.pdf}
     \caption{\textcolor{black}{Left: Workflow diagram of the \textit{Act2Comm} scheme, with the dashed line indicating the gradient flow. Right: Illustration of the iterative training strategy, incorporating a critic network.}}
     \label{Act_fig_pipeline}
 \end{figure*}
\paragraph{2) Block-attention feedback coding.}
One of the principal innovations of \textit{Act2Comm} is the \textit{block-attention} coding mechanism, which reduces the coding complexity and enhances the performance.% by generating channel inputs from message blocks instead of individual bits.
%which generates channel inputs from message blocks instead of individual bits. This design reduces the computational complexity and provides informative embeddings for each block, thereby improving coding performance.
%Specifically, we divide the bit-stream into groups of bits for coding, referred to as message blocks \cite{9960791}. This contrasts with prevalent channel coding methods, where each element of the sequence corresponds to a single message bit, along with its associated channel input and feedback symbols \cite{shao2023attentioncode}. This \textit{block-attention feedback codes} design for two key advantages: (a) It shortens the input sequence length for coding, substantially reducing feedback overhead, computational complexity, and memory requirements. (b) By integrating with the transformer architecture in our Act2Comm scheme, it produces more informative embeddings for each sequence, thereby enhancing the coding performance.

\textbf{{(a) Message block:}}
Formally, we partition message $\bm{m}$ into $l$ blocks as $\bm{m}=[\bm{b_1};\bm{b_2};\ldots;\bm{b_l}]$, where each block $\bm{b_i}\in \{0,1\}^\mu$ contains $\mu=k/l$ bits. This allows us to encode $\bm{m}$ with $l$ coding rounds, with each round consisting of $\mu/R$ time steps. 
For each coding round $\tau$ ($1\le \tau \le l$), the input message block is defined as $\bm{B}^{(\tau)}\triangleq[2\bm{b_1}-1;\ldots;2\bm{b_\tau}-1]\in\mathbb{R}^{\tau\times \mu}$. 
%Each block is associated with a label of its decimal value and the label of $\bm{m}$ is denoted by $\bm{B}\in \mathbb{R}^{l\times 1}$. In each coding round $\tau$ out of $l$ rounds, the transmitter encodes the policy for the subsequent $\frac{\mu}{R}$ time steps. Then the input message block for the $\tau$-th coding round can be defined as: $\bm{B}^{(\tau)}\triangleq[2\bm{b_1}-1;\ldots;2\bm{b_\tau}-1]\in\mathbb{R}^{\tau\times \mu}$. 

\textbf{{(b) {Feedback block:}}} \textcolor{black}{Although Theorem \ref{thm:cap} shows that the capacity-achieving code with infinite blocklength can be history-independent, feedback has demonstrated benefits in simplifying the coding process for better performance in the practical finite blocklength regime \citep{kostina2017joint,kim2020deepcode}.  Hence, for each time step $t$ within the $\tau$-th coding round, we introduce the feedback vector} as $\bm{c_t^{(\tau)}}\triangleq[s_t^{(\tau)},x_t^{(\tau)},s_{t+1}^{(\tau)}] \in \mathbb{R}^{1 \times 3}$, which encapsulates the current state, the selected action, and the subsequent state. The feedback matrix for the $\tau$-th round can then be given by $\bm{C_\tau}=[\bm{c_1^{(\tau)}};\ldots;\bm{c_{\frac{\mu}{R}}^{(\tau)}}] \in \mathbb{R}^{\frac{\mu}{R}\times 3}$. Consequently, for each coding round $\tau$, we concatenate prior feedback matrices to construct a feedback block: $\bm{C^{(\tau)}}\triangleq[\bm{C_1};\ldots;\bm{C_\tau}]\in\mathbb{R}^{\tau\times \frac{\mu}{R}\times 3}$. 

%\textit{Act2Comm} scheme supports both history-dependent and Markov coding schemes, where both the message and feedback block are encoded for history-dependent coding and only the message block are encoded for Markov coding. 

%\subsubsection{Block-wise policy coding:}
%At each coding round $\tau$, the transmitter generates a policy vector $\bm{x}^{(\tau)}\in \mathcal{A}^{1\times \frac{\mu S}{R}}$ based on $\bm{C^{(\tau)}}$ and $\bm{B^{(\tau)}}$ for the subsequent $\frac{\mu}{R}$ time steps. This policy vector $\bm{x}^{(\tau)}$ is then reshaped into $\mathcal{A}^{S\times \frac{\mu}{R}}$, representing the selected actions for $S$ states over the next $\frac{\mu}{R}$ time steps. %Consequently, the control policy up to the current coding round $\tau$ is represented as $\bm{X}^{(\tau)}=[\bm{x}_{1};\ldots;\bm{x}_{\tau}]\in \mathcal{A}^{\tau\times \frac{\mu S}{R}}$. The final policy $\bm{X}^{(l)}\in \mathcal{A}^{\frac{k}{\mu}\times \frac{\mu S}{R}}$ will be reshaped into $\bm{X}\in\mathcal{A}^{S\times \frac{k}{R}}$, aligning with Eqn. \eqref{control_policy_eq}. 
%as: $\bm{x_{\tau}}=\mathcal{E}_{\bm{\theta}}(\bm{C^{(\tau)}},\bm{B^{(\tau)}})$,
%The coding process $\mathcal{E}_{\bm{\theta}}(\cdot)$ will be detailed later in the illustration of transceiver design.

%We didn't code all previous coded symbols or channel output, as there are only previous state, current coded action, and the next state can help with the further coding process in \textit{action-state} channel. We can draw a imge for better illustat...%As we know, for each coding round $\tau$, there are $\frac{\mu}{R}$ channel uses, that is we can define the feedback matrix $\bm{C_\tau}=[\bm{c_1^{\tau}};\ldots;\bm{c_{\frac{\mu}{R}}^{\tau}}] \in \mathbb{R}^{\frac{\mu}{R}\times 3}$ 
%That is We can have $\bm{C}^{(l)}=[\bm{C_1};\ldots;\bm{C_l}]\in \mathbb{R}^{{\frac{k}{\mu}\times \frac{\mu}{R}\times 3}}$ for the final $l$ step, which can be reshaped into $\bm{X}\in\mathcal{A}^{S\times \frac{k}{R}}$, representing selected actions for different channel uses. 
\paragraph{3) Transceiver design.}\quad
%The transmitter of the \textit{Act2Comm} compromises an encoder and a quantizer, while the receiver includes a decoder.

\textbf{{(a) Encoder:}} At each coding round $\tau$, a transformer-based encoder is utilized to generate a belief matrix $\bm{Z}^{(\tau)}\in\mathbb{R}^{\tau\times \frac{\mu |\mathcal{S}|}{R}}$ using $\bm{B^{(\tau)}}$ and $\bm{C^{(\tau)}}$. The detailed architecture is provided in Fig. \ref{Act_fig_architecture}, \textcolor{black}{with each component illustrated in Appendix \ref{app_model}.} The $\tau$-th row vector of $\bm{Z}^{(\tau)}$, denoted as $\bm{z^{(\tau)}}\triangleq\bm{Z}^{(\tau)}[\tau,:]\in \mathbb{R}^{\frac{\mu |\mathcal{S}|}{R}}$, represents the belief vector derived from the $\tau$-th coding round. After completing all $l$ coding rounds, the selected belief vectors are combined to form the final belief map $\bm{Z}=[\bm{z^{(1)}};\ldots;\bm{z^{(l)}}]\in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu |\mathcal{S}|}{R}}$, which is subsequently reshaped into $\bm{Z}\in\mathbb{R}^{|\mathcal{S}|\times \frac{k}{R}}$. Each element of this reshaped belief map $Z[s_t,t]$ indicates the action belief at time step $t$ given state $s_t$.

%\ref{Act_fig_architecture}, $\bm{C}^{(\tau)}$ and $\bm{B^{(\tau)}}$ are firstly processed through multilayer perceptron (MLP) layers, then concatenated for linear projection and positional encoding operations, resulting in $\bm{F_{0}}\in \mathbb{R}^{\tau\times d}$, where $d$ is the hidden layer dimension. After $L_t$ transformer layers, the resultant $\bm{F_{L_t}}\in\mathbb{R}^{\tau\times d}$ is further transformed by a linear layer into a belief matrix $\bm{Z}^{(\tau)}\in\mathbb{R}^{\tau\times \frac{\mu S}{R}}$. Here, the $\tau$-th row vector, $\bm{z}^{(\tau)}=\bm{Z}^{(\tau)}[\tau,:]\in \mathbb{R}^{1\times \frac{\mu S}{R}}$, represents the belief vector derived from the $\tau$-th coding round. After completing all $l$ coding rounds, the collection of belief vectors forms belief map $\bm{Z}=[\bm{z^{(1)}},\ldots,\bm{z^{(l)}}]\in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu S}{R}}$, which is subsequently reshaped into $\bm{Z}\in\mathbb{R}^{S\times \frac{k}{R}}$. In this reshaped belief map, each element $Z[s_t,t]$ indicates the action belief for the agent at time step $t$ given the current state $s_t$.
%the action belief vector $\bm{z^{(\tau)}}$ using $\bm{B^{(\tau)}}$ and $\bm{C^{(\tau)}}$, where the detailed operations are illustrated in Fig. \ref{Act_fig_architecture}. %Specifically, $\bm{C}^{(\tau)}$ and $\bm{B^{(\tau)}}$ are processed through $L_t$ transformer layers and a linear layer to produce a belief matrix $\bm{Z}^{(\tau)}\in\mathbb{R}^{\tau\times \frac{\mu S}{R}}$.
%As detailed in Fig. \ref{Act_fig_architecture}, $\bm{C}^{(\tau)}$ and $\bm{B^{(\tau)}}$ are firstly processed through multilayer perceptron (MLP) layers to produce preliminary coding results. These preliminary coding results are then concatenated and subjected to a linear projection operation followed by a learnable positional encoding operation, resulting in $\bm{F_{0}}\in \mathbb{R}^{\tau\times d}$, where $d$ is the dimension of the hidden layers. After passing through $L_t$ transformer layers, the resultant $\bm{F_{L_t}}\in\mathbb{R}^{\tau\times d}$ is further transformed by a linear layer into a belief map $\bm{Z}^{(\tau)}\in\mathbb{R}^{\tau\times \frac{\mu S}{R}}$. Here, the $\tau$-th row vector, $\bm{z}^{(\tau)}=\bm{Z}^{(\tau)}[\tau,:]\in \mathbb{R}^{1\times \frac{\mu S}{R}}$, represents the belief vector derived from the $\tau$-th coding round. After completing all $l$ coding rounds, the collection of belief vectors forms belief map $\bm{Z}=[\bm{z^{(1)}},\ldots,\bm{z^{(l)}}]\in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu S}{R}}$, which is subsequently reshaped into $\bm{Z}\in\mathbb{R}^{S\times \frac{k}{R}}$. In this reshaped belief map, each element $Z[s_t,t]$ indicates the action belief for the agent at time step $t$ given the current state $s_t$.
 %the proposed encoder comprises linear projection layers, a position embedding layer, and $L_t$ transformer layers. Specifically, 
% as $\bm{C_{0}^{(\tau)}}\in \mathbb{R}^{\tau\times d}$, where  $d$ is the dimension of the hidden layers. 
%where $\bm{W_{c0}}\in \mathbb{R}^{3\times 1}$, $\bm{b_{c0}}\in \mathbb{R}^{\frac{\mu}{R}\times 1}$,$\bm{W_{c1}}\in \mathbb{R}^{\frac{\mu}{R}\times d}$, and $\bm{b_{c1}}\in \mathbb{R}^{d\times 1}$ are corresponding parameters. Here, $d$ is the dimension of the hidden layers and $\bm{C_{0}^{(\tau)}}\in \mathbb{R}^{\tau\times d}$ is the preliminary coding result for feedback. 
%Similarly, message block $\bm{B^{(\tau)}}$ goes through one linear layer to get $\bm{B^{(\tau)}}_0\in \mathbb{R}^{\tau\times d}$. These processed blocks are then concatenated to form $\bm{F_{in}}=\textit{concat}(\bm{B^{(\tau)}}_0,\bm{C^{(\tau)}}_0)\in \mathbb{R}^{\tau\times 2d}$, which subsequently undergoes a linear projection operation with parameters $\bm{W_{0}}\in \mathbb{R}^{2d\times d}$, and a learnable position encoding operation to obtain the initial input $\bm{F_{0}}\in \mathbb{R}^{\tau\times d}$ for the following transformer layers. 
%Similarly, message block $\bm{B^{(\tau)}}$ is processed through a linear layer.%, yielding $\bm{B^{(\tau)}}_0\in \mathbb{R}^{\tau\times d}$. 

%Feedback block $\bm{C}^{(\tau)}$ firstly goes through two linear layers to get:
%\begin{equation}
%    \bm{C^{(\tau)}}_0= (\bm{C^{(\tau)}}\bm{W_{c0}}+\bm{b_{c0}})\bm{W_{c1}}+\bm{b_{c1}},
%\end{equation}
%where $\bm{W_{c0}}\in \mathbb{R}^{3\times 1}$, $\bm{b_{c0}}\in \mathbb{R}^{\frac{\mu}{R}\times 1}$,$\bm{W_{c1}}\in \mathbb{R}^{\frac{\mu}{R}\times d}$, and $\bm{b_{c1}}\in \mathbb{R}^{d\times 1}$ are corresponding parameters. Here, $d$ is the dimension of the hidden layers and $\bm{C_{0}^{(\tau)}}\in \mathbb{R}^{\tau\times d}$ is the preliminary coding result for feedback. Similarly, message block $\bm{B^{(\tau)}}$ goes through one linear layer to get $\bm{B^{(\tau)}}_0\in \mathbb{R}^{\tau\times d}$. Then we concatenate them as $\bm{F_{in}}=\textit{concat}(\bm{B^{(\tau)}}_0,\bm{C^{(\tau)}}_0)\in \mathbb{R}^{\tau\times 2d}$, which subsequently goes through a linear projection operation with parameters $\bm{W_{0}}\in \mathbb{R}^{2d\times d}$ and a position encoding operation to get the initial input $\bm{F_{0}}\in \mathbb{R}^{\tau\times d}$ for the following transformer layers as:
%as:
%\begin{equation}
%    \bm{B^{(\tau)}}_0=\bm{B^{(\tau)}}\bm{W_{b0}}+\bm{b_{b0}},
%\end{equation}
%where $\bm{W_{b0}}\in \mathbb{R}^{\mu\times d}$ and $\bm{b_{b0}}\in \mathbb{R}^{d\times 1}$ are the corresponding parameters.
%\begin{equation}
%\bm{F_{0}}=\bm{F_{in}}\bm{W_{0}}+\bm{P_{e}},
%\label{pos_embed}
%\end{equation}
%where $\bm{P_{e}}$ is learnable matrix for the positional encoding.

\textbf{(b) Quantizer:} \textit{Act2Comm} employs a quantizer to generate the codeword $\bm{U}\in\mathcal{X}^{|\mathcal{S}|\times \frac{k}{R}}$ as:
%which rounds down $\bm{Z}$ to the nearest action index in action space $\mathcal{X}$, thereby determining the most probable actions. Specifically, the decision rule matrix $\bm{U}\in\mathcal{X}^{S\times \frac{k}{R}}$ in our \textit{Act2Comm} scheme is implemented as: 
%\begin{equation}
%    \bm{U}=\mathcal{Q}(|\mathcal{X}|*\text{Sigmoid}(\bm{Z})),
%    \label{control_policy_eq}
%\end{equation}
\begin{equation}
    \bm{U}=\mathcal{Q}(|\mathcal{X}|\cdot\text{Sigmoid}(\bm{Z})),
    \label{control_policy_eq}
    \vspace{-2pt}
\end{equation}
where $\mathcal{Q}: \mathbb{R}^{|\mathcal{S}|\times \frac{k}{R}}\rightarrow \mathcal{X}^{|\mathcal{S}|\times \frac{k}{R}}$ is the quantization operation that maps the \textcolor{black}{coding result} to the nearest action index in the action space $\mathcal{X}$, and each element of resultant codeword $x_t=U[s_t,t]$ represents the selected action for state $s_t$ at time step $t$.

\textbf{(c) Decoder:} Given the state observations $\bm{s}\triangleq[s_1;\ldots;s_{\frac{k}{R}}] \in \mathcal{S}^{\frac{k}{R}}$, a transformer-based decoder is utilized to output logits $\bm{\hat{M}}\in\mathbb{R}^{\frac{k}{\mu}\times 2^\mu}$ for all blocks. After applying the softmax function, each block is predicted and subsequently transformed into the reconstructed bitstream $\bm{\hat{m}}$.

%Specifically, the decoding process starts by reshaping $\bm{S}$ into $\mathcal{S}^{\frac{k}{\mu} \times \frac{\mu}{R}}$, followed by a symmetric progression with linear and transformer layers, as illustrated in Fig. \ref{Act_fig_architecture}. 
%After a softmax function, we predict the label of $\bm{m}$ as $\bm{\hat{B}}\in\mathbb{R}^{l\times 1}$ and transform it into bit stream $\bm{\hat{m}}$.
%This reshaped $\bm{S}$ is then processed symmetrically through linear layers, positional encoding, and $L_r$ transformer layers, yielding the feature $\bm{D_{L_r}}\in \mathbb{R}^{\frac{k}{\mu}\times d}$. 
%Subsequently, $\bm{D_{L_r}}$ is input into a linear layer to generate the logits $\bm{\hat{M}}\in\mathbb{R}^{l\times 2^\mu}$ for each block. 
%After a softmax function, we predict the label of $\bm{m}$ as $\bm{\hat{B}}\in\mathbb{R}^{l\times 1}$ and transform it into bit stream $\bm{\hat{m}}$.
%: $\bm{\hat{B}}=\arg\max\limits_{i}(\textit{softmax}(\bm{{\hat{M}[:,i]}}))\in\mathbb{R}^{\frac{k}{\mu}\times 1}$, where $\textit{softmax}(\cdot)$ is the softmax function across the columns. %where $\textit{softmax}(\bm{X})\triangleq\frac{e^{X[i,j]}}{\sum_ke^{X[i,k]}}$ is the softmax function across the columns.

\paragraph{4) Joint optimization of control and communication.}
To model the trade-off between control and communication, we utilize a weighted loss function: $\mathcal{L}_{all}=\mathcal{L}_{com}+\lambda \cdot \mathcal{L}_{cont}$. The communication loss $\mathcal{L}_{com}$ is defined as the cross-entropy between the  \textcolor{black}{predictions from a critic network and their corresponding ground-truth, which quantifies} the message decoding accuracy. To ensure control performance, we aim to make the coding policy behave closely to the target policy. Therefore, $\mathcal{L}_{cont}$ measures the ``distance'' between the coding policy and the target policy.

Let $\pi$ denote the target policy, with $\pi(x|s)$ representing the probability of taking action $x$ in state $s$. %If the frequency of selecting $x$ in state $s$ by the coding policy is close to $\pi(x|s)$ for all $(s,x)$ pairs, we say that the two policies behave similarly. To quantify this, 
Let $f_U(x|s)$ denote the frequency of selecting $x$ in state $s$ across all decision rules in $\bm{U}$. We then use the mean square error (MSE) between $\pi$ and $f_U$ to measure the control loss for its stability in experiments. However, \textcolor{black}{$f_U$ is non-differentiable during the backpropagation as it is discrete.} To address this issue, we estimate $f_U(x|s)$ using $\bm{Z}$ in \eqref{control_policy_eq}. %According to \eqref{control_policy_eq}, $\bm{U}$ is a quantized version of $\bm{Z}$. 
Let $\bm{e}$ denote the all-one row vector, and define $\Gamma_Z(\bm{T},s,x) \triangleq \text{Sigmoid} (\gamma\cdot(|\mathcal{X}|\text{Sigmoid}(\bm{Z}[s,:])-x\bm{e}))$.
When $\gamma>0$ is sufficiently large, $\Gamma_Z(\bm{T},s,x)$ is a $(kR)$-dim vector with elements close to either $0$ or $1$. Additionally, $\Gamma_Z(\bm{T},s,x)\bm{e}^\top$ approximates the number of elements in $\bm{U}[s,:]$ that are not less than $x$. We refer to $\gamma$ as the temperature parameter and estimate $f_U(x|s)$ for $x>0$ as follows:
\begin{equation}
\vspace{-2pt}
    f_U(x|s)\approx \hat{f}_U(x|s)\triangleq \frac{1}{kR} \left[\Gamma_Z(\bm{T},s,x-1)\bm{e}^\top-\Gamma_Z(\bm{T},s,x)\bm{e}^\top \right].
    \vspace{-2pt}
\end{equation}
For $x=0$, we have $f_U(0|s)\approx\hat{f}_U(0|s)\triangleq 1- \Gamma_Z(\bm{T},s,0)\bm{e}^\top/kR$. As a result, we define the control loss as $\mathcal{L}_{cont}=\textit{MSE}(\pi, \hat{f})$.
%where $\hat{P}(0|s)=1-\Gamma(\gamma,0,s)$ for $x=0$. Function $\Gamma(\cdot)$ is defined as $\Gamma(T,x,s)=\frac{1}{kR}\sum_{i=0}^{kR} \textit{Sigmoid} (\gamma*(\bm{Z}(s)-x))[i,:]$, where $\bm{Z}(s)\triangleq |\mathcal{X}|*\textit{Sigmoid}(\bm{Z})[s,:]$. The function $\Gamma(\gamma,x,s)$ estimates the possibilities of the selected action bigger than $x$ for the state $s$. From intuition, a higher $\gamma$ can result in a more accurate estimation at the expense of a more difficult learning process where the gradient is smaller. To approach the target policy, we train the encoder with policy loss function as $\mathcal{L}_{policy}=\text{MSE}(\bm{\hat{P}},\bm{P^*})$, where $\text{MSE}(\cdot)$ is the mean square error loss function.
%To approach the target policy, we train the encoder with policy loss function as $\mathcal{L}_{policy}=\text{MSE}(\bm{\hat{P}},\bm{P^*})\triangleq E[\|\bm{\hat{P}}-\bm{{P}^*}\|^2_2]$, where $\text{MSE}(\cdot)$ is the mean square error function and the expectation is taken over various samples.
%To note, a well-selected temperature value $T$ can implicitly learn a sparse policy matrix without this regularization term.

\paragraph{5) Iterative training strategy.}
\textcolor{black}{Given the non-differentiable nature of the EAS channel and quantizer, jointly updating the encoder and decoder is infeasible. To address this, we introduce a critic network and employ an iterative updating strategy to train \textit{Act2Comm} effectively, with the corresponding algorithm and architectures detailed in Appendix \ref{app_iter_train}.}

\textbf{(a) Critic network}
%Since the EAS channel is non-differentiable, jointly updating the encoder and decoder is infeasible. 
\textcolor{black}{As shown in Fig. \ref{Act_fig_pipeline}, a critic network is introduced to estimate the gradient during gradient backpropagation for the encoder optimization, which views the EAS channel and decoder as an unknown environment.} Before each update of the encoder, a critic network is trained over $s_{in}$ inner steps to predict the logits for the neighbor belief maps of a given $\bm{Z}$. For each inner step $k$, the network is trained to predict the corresponding logits $\bm{\hat{M_k}}$ as $\bm{\hat{M}_{ck}}$ based on the neighbor belief maps sampled from $\bm{Z_k}=\bm{Z}+\bm{W_k}$, where $\bm{W_k}\in\mathbb{R}^{|\mathcal{S}|\times\frac{k}{R}}\sim \mathcal{N}(0,\sigma_w^2)$ is the Gaussian noise term for neighboring sampling during the $k$-th inner step. The MSE loss, \textcolor{black}{denoted as} $\mathcal{L}_{cn}$, is utilized between $\bm{\hat{M}_{ck}}$ and $\bm{\hat{M}_{k}}$ \textcolor{black}{to train the critic network}. With this design, we aim to obtain a precise critic network to estimate gradients from neighbors of $\bm{Z}$ in a given environment, thereby helping update the encoder. \textcolor{black}{Note that this extra training cost is incurred only during the offline training process, this critic network will be removed during the inference phase, as detailed in Appendix \ref{app_infer}-\ref{app_complex}.}.
 
\textbf{(b) Iterative updating strategy}
As outlined in Fig. \ref{Act_fig_pipeline}, at each update step $i$, we first train a critic network $\bm{\phi_{s_{in}}^i}$ using $s_{in}$ inner steps to learn to estimate the gradient around the samples. Next, the encoder is updated to $\bm{\theta^{i+1}}$ using the frozen decoder parameters $\bm{\xi^{i}}$ and the learned gradient estimation. Subsequently, the decoder is directly optimized with the loss function to obtain new parameters $\bm{\xi^{i+1}}$, while keeping the encoder frozen at $\bm{\theta^{i+1}}$. This process iteratively alternates between encoder and decoder updates, freezing one while optimizing the other at each step.

\begin{figure}[t]
    \vspace{-5pt}
\begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=0.95\linewidth]{image/ave_BER_reward.pdf}
    \caption{BER v.s. Reward}
    \label{fig:wheel-1}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/wheel_exp_fixed_BER_reward_rate.pdf}
    %\caption{\small{BER vs rate for a given reward}}
     \caption{Reward v.s. Rate}
    \label{fig:wheel-2}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/Fixed_Reward_ber_rate.pdf}
    %\caption{\small{Reward vs rate for a given BER}}
         \caption{BER v.s. Rate}
    \label{fig:wheel-3}
\end{subfigure} 
    \caption{Control-communication trade-off of \textit{Act2Comm} in ``Lucky Wheel''.}
    \label{fig:wheel-tradeoff}
    \vspace{-5pt}
\end{figure}
%Iterative updating strategy for the encoder parameters $\bm{\theta}$ and decoder parameters $\bm{\xi}$ at each updating step $i$, where $\bm{\theta}$ and $\bm{\xi}$ are updated and frozen alternately, and a critic network with parameters $\bm{\phi}$ is optimized over $s$ steps to estimate the gradient for the update of $\bm{\theta}$.
\section{Experimental Results}
%This section evaluates \textit{Act2Comm} framework in two different MDP environments (detailed in Appendix \ref{Exp_environment}), where the communication performance is assessed using the bit error rate (BER).
We evaluate \textit{Act2Comm} across \textcolor{black}{three} distinct MDP environments, as detailed in Appendix \ref{Exp_environment}, with communication performance measured by the bit error rate (BER). \textcolor{black}{Due to page limitations, the results of environment, ``Erratic robot'', are provided in the Appendix \ref{app_robot_env}.}
%, and the control performance is evaluated by the long-term average reward. 
%We evaluate \textit{Act2Comm} in two different MDP environments, details of which are provided in Appendix \ref{Exp_environment}. 

\textbf{Experiment 1: Lucky Wheel.} In this game, agent keeps spinning a wheel to accumulate rewards by choosing either clockwise or counterclockwise. It is modeled as an MDP with 3 states and 2 actions, the details of the environment and experimental setting is provided in Fig. \ref{fig:wheel} in Appendix \ref{Exp_environment}. 
%

We examine the trade-off among the three performance metrics in Fig. \ref{fig:wheel-tradeoff}. We set the optimal reward-maximizing policy as the target policy, and consider different code rates. By adjusting $\lambda$, we can control how closely the coding policy approximates the target policy. \textcolor{black}{The shaded regions in the figures (a)-(c) represent achievable regions of \textit{Act2Comm} with various $\lambda$.} When $\lambda$ is large, regardless of the coding rate, \textit{Act2Comm} learns a policy that mirrors the target policy for the optimal reward. In this case, all messages are mapped to the same sequence of decision rules since the target policy is stationary and deterministic. As a result, the BER is 0.5, indicating no communication capability. %Decreasing $\lambda$ allows \textit{Act2Comm} to learn coding policies with a lower BER.

Next, we consider different target BERs, resulting in a trade-off between the code rate and reward. As shown in Fig. \ref{fig:wheel-2}, achieving a pre-determined BER with a higher coding rate results in a reduced reward. When targeting a lower BER, the reward decreases rapidly with the coding rate. Fig. \ref{fig:wheel-3} illustrates \textit{Act2Comm}'s ability to balance BER and coding rate when ensuring a specified reward. Our results reveal that reducing the reward constraint leads to more reliable communication at the same rate. %Similarly, a lower reward requirement enhances the communication gains achieved through rate reduction. 
In summary, these findings demonstrate that \textit{Act2Comm} can communicate messages through its actions at acceptable reliability while satisfying specific reward criteria. 

\begin{figure}[t]
    \vspace{-5pt}
\begin{subfigure}{0.3283\linewidth}
        \centering
    \includegraphics[width=0.95\linewidth]{image/perfect_figure_trade_off_all.png}
    %\caption{\small{Trade-off between BER vs Reward over different rates.}}
         \caption{BER v.s. Reward}
    \label{fig:ball-p1}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/Perfect_fixed_BER_reward_rate.pdf}
    %\caption{\small{BER vs rate for a given reward}}
     \caption{Reward v.s. Rate}
    \label{fig:ball-p2}
\end{subfigure} 
\begin{subfigure}{0.325\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/perfect_fixed_reward.png}
    %\caption{\small{Reward vs rate for a given BER}}
         \caption{BER v.s. Rate}
    \label{fig:ball-p3}
\end{subfigure} 
    \caption{Control-communication trade-off of \textit{Act2Comm} in ``Catch the Ball'' with $p=0$.}
    \label{fig:ball-perfect}
    \vspace{-5pt}
\end{figure}

\begin{figure}[t]
    \vspace{-5pt}
\begin{subfigure}{0.33\linewidth}
        \centering
    \includegraphics[width=0.95\linewidth]{image/noisy_ball_ave_BER_reward.pdf}
    %\caption{\small{Trade-off between BER vs Reward over different rates.}}
         \caption{BER v.s. Reward}
    \label{fig:ball-1}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/noisy_fixed_BER_reward_rate.pdf}
    %\caption{\small{BER vs rate for a given reward}}
     \caption{Reward v.s. Rate}
    \label{fig:ball-2}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.95\linewidth]{image/noisy_fixed_reward_ber_rate.pdf}
    %\caption{\small{Reward vs rate for a given BER}}
         \caption{BER v.s. Rate}
    \label{fig:ball-3}
\end{subfigure} 
    \caption{\textcolor{black}{Control-communication trade-off of \textit{Act2Comm} in ``Catch the Ball'' with $p=0.2$.}}
    \label{fig:ball-noisy}
        \vspace{-5pt}
\end{figure}

\textbf{Experiment 2: Catch the Ball.} Next we consider ``Catch the Ball'', which is an MDP with 27 states and 3 actions. 
This MDP includes a parameter $p$  that influences its transition matrix (see the Appendix \ref{Exp_environment} for details). When  $p = 0$, the action-state channel is perfect as each action can be reliably inferred from the resulting state transition, but it becomes noisy for $p > 0$. 

We first consider $p = 0$, where no coding is needed since there is no noise. The challenge is to maintain a certain reward while communicating. \textit{Act2Comm} performs excellently in this environment. As shown in Fig. \ref{fig:ball-p1}, with a minor reduction in reward from $1.66$ to $1.5$, \textit{Act2Comm} communicates at a rate of $0.2$ with no error. If we relax BER to  $10^{-4}$, same rate can be achieved with a reward of $1.6$. The trade-off between reward and rate is shown in Fig. \ref{fig:ball-p2}. %Since the channel is perfect, we can achieve zero-error communication at various coding rates, with only a minimal loss in reward. 
This experiment highlights our model's capacity to enable efficient communication capabilities with minimal impact on performance. We also applied \textit{Act2Comm} to this game with $p = 0.2$, in which the action-state channel is noisy and the coding process becomes more complex. As detailed in Fig. \ref{fig:ball-noisy}, we can observe a reduction in the coding rates for the same level of reliability due to the stochasticity in the environment. 

\section{Conclusion}
We introduced a novel framework of \textit{communication through actions}, a form of implicit communication from the controller of an MDP to a receiver that can observe the states. By treating the MDP environment as a communication channel, messages can be encoded into the action sequence and decoded from the state sequence. Aiming to optimize communication performance while ensuring a certain MDP reward, we formulated an integrated control and communication problem. We derived the capacity of the action-state channel and demonstrated that the trade-off between channel capacity and reward can be characterized as a convex optimization problem. We then proposed \textit{Act2Comm}, a transformer-based framework for designing joint control and communication policies. Through experiments, we demonstrated \textit{Act2Comm}’s capability to communicate reliably through actions while maintaining a certain level of MDP reward.

The proposed \textit{Act2Comm} framework can be used as a plug-in component in various MDP and RL applications, enabling information transmission by learning a joint control and coding policy that closely mimics the target policy. More importantly, our study demonstrates the potential of communication through actions in multi-agent systems. While this form of implicit communication leads to some loss in control performance, it may potentially improve the overall control performance by enhancing coordination when applied to multi-agent systems where explicit communication channels are not available. This presents an interesting and challenging direction for future research.

%\section{Acknowledgments}
\newpage


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\newpage
\section*{Appendix}
{\section{Notation and Definitions}
\label{app_notation}
To bridge the RL and FSC areas, we provide a notation table for this paper. Note that lowercase and uppercase bold letters represent vectors and matrices, respectively.}

\begin{table}[h]
\caption{{Notation table}}
\centering
\begin{tabularx}{0.8\textwidth}{p{0.2\textwidth}X}
\toprule
 \multicolumn{2}{l}{\underline{In general}}\\    
    $\bm{x_i^k}$,  $\bm{x^k}$ & Sequence $\{x_i,x_{i+1},\ldots,x_{k}\}$, sequence $\{x_1,\ldots,x_{k}\}$ \\ 
    %$x^k$ & Sequence $\{x_1,\ldots,x_{k}\}$ \\     
    $|\mathcal{X}|$ & Cardinality of set $\mathcal{X}$\\

 \multicolumn{2}{l}{\underline{MDP: $(\mathcal{S}, \mathcal{X}, \bm{T}, r, \alpha)$}}\\    
    $s_{t}$, $x_{t}$ & MDP state and action at $t$\\ 
    $r$, $\alpha$ & Reward function and initial state distribution\\
    $\mathcal{S}$, $\mathcal{X}$ & State and action space for MDP\\
    $\bm{T}$, $\pi$ & Transition kernel and control policy\\

  \multicolumn{2}{l}{\underline{FSC: $(\mathcal{X}\times \mathcal{S}, P_{Y, S^{+}|X, S}, \mathcal{Y}\times \mathcal{S})$}} \\ 
   $x_{t}$, $y_{t}$, $s_{t}$ & Channel input, output, and state at $t$ \\ 
    $\mathcal{X}$, $\mathcal{Y}$,  $\mathcal{S}$ & Channel input, output alphabet, and state alphabet\\
        ${S^+}$, ${S}$ & Future and current state (random variables)\\
    $\mathcal{E}$,  $\mathcal{D}$ & Encoder and decoder\\
    $k$, $n$ & Message bit length and the code length\\
    $R_{\mathcal{E},\mathcal{D}}=k/n$ & Rate of code $(\mathcal{E},\mathcal{D})$\\

  \multicolumn{2}{l}{\underline{Action-state channel: $P(s_{t+1}|\bm{x^{t}},\bm{s^{t}})$}}\\    
    $s_{t}$, $x_{t}$ & State and action at $t$ \\ 
$\mathcal{S}$, $\mathcal{X}$ & State alphabet, action alphabet\\
        ${S^+}$, ${S}$ & Future and current state (random variables)\\
    $\mathcal{E}$,  $\mathcal{D}$, $\bm{T}$, $G_{\mathcal{E}}$  & Encoder, decoder, transition kernel, average reward\\
    $k$, $n$, $R$ & Message bit length, code length, and rate\\
    $\bm{s}_{in-n+1}^{in}$& The state sequence associated with the $i$-th message\\

   \multicolumn{2}{l}{\underline{EAS channel for \textit{Act2Comm}:         
    $P(s_{t+1}|\bm{x^t},\bm{s^t})$}}\\    
    $s_{t}$, $x_{t}$, $u_{t}$; $\pi$ & State, action and decision rule at $t$; Target policy\\ 
    ${S^+}$, ${S}$ & Future and current state (random variables)\\
    $\mathcal{U}$, $\mathcal{X}$, $\mathcal{S}$ & Alphabets of decision rule, actions, and states\\
    $\bm{T}$, $G_{\mathcal{E}}$  & transition kernel and average reward\\
    $\mathcal{E}(\cdot)$,  $\mathcal{D}(\cdot)$, $\mathcal{C}(\cdot)$  & Encoder, decoder, and critic network.\\
    $\bm{\theta}$, $\bm{\xi}$, $\bm{\phi}$& Parameters of encoder, decoder, critic network\\
    $\bm{Z}$, $\bm{U}$ & Encoded belief map and codeword \\    
    $\bm{B}^{(\tau)}$, $\bm{C}^{(\tau)}$ & Message and feedback block at coding round $\tau$\\
    $\bm{X}^{(\tau)}$& Control policy at coding round $\tau$\\
    $\bm{\hat{M}}$& Decoded logits\\
    $\mathcal{Q}(\cdot)$& Quantizer\\
    $\mathcal{L}_{all}$& Weighted loss function to update the encoder\\
    $\mathcal{L}_{com}$, $\mathcal{L}_{cont}$ & Communication loss term and Control loss term\\
    $\mathcal{L}_{cn}$& MSE loss for critic network \\
    $f_{U}(x|s)$ &Frequency of selecting action $x$ in state $s$ across $\bm{U}$ \\
    $\gamma$ & Estimation temperature for control policy \\
    $\hat{f}_{U}(x|s)$ & Estimation of $f_{U}(x|s)$\\
    $\bm{Z_k}$, $\bm{W_k}$ & Sampled neighbors of $\bm{Z}$ and its Gaussian term \\
    $\bm{\hat{M}_k}$& The corresponding logits from the frozen decoder\\
    $\bm{\hat{M}_{ck}}$& Predicted logits from the critic network (the $k$-th step)\\

  \bottomrule
 \end{tabularx}
 \label{Tab_notation}
\end{table}
 
%\subsection{\textcolor{black}{Channel capacity}}
%\textcolor{black}{Let $\mathcal{M}$ denote the set of messages, where each message into $m$ is mapped $\mathcal{E}_{t}: \mathcal {M} \times {\mathcal S}^{t} \times \mathcal X^{t-1}\times {\mathcal {{Y}}}^{t-1} \to {\mathcal X}$ generates the channel input at time $t$. In other words, the channel input at time $t$, $x_t=\mathcal{E}_{t}({m},\bm{s^t},\bm{x^{t-1}},\bm{{y}^{t-1}})$, is a function of $m$ and all the historical information available at the transmitter up to time $t$. The decoder is defined as the mapping, $\mathcal{D}: \mathcal {Y}^n \to {\mathcal M}$, which reconstructs the message from all $n$ channel outputs, $\hat{m}=\mathcal{D}(\bm{y}^n)$. The pair $(\mathcal{E}, \mathcal{D})$ constitutes a code, where $n$ represents the code length. Suppose the message set is $\mathcal{M}= \{1, 2, \ldots , 2^{k}\}$, then each message can be represented with $k$ bits. The rate of the code $(\mathcal{E}, \mathcal{D})$ is defined as $R_{\mathcal{E}, \mathcal{D}}=k/n$. }

%{The probability of error $P_e^{(n)}$ for $(\mathcal{E}, \mathcal{D})$ is defined as $P_e^{(n)}=\Pr(\mathcal{D}(\bm{y}^n)\neq m | m \text{ is sent})$. A rate $R$ is deemed achievable if there exists a code $(\mathcal{E}, \mathcal{D})$ such that the error probability of the transmission approaches zero as $n\to \infty$. Consequently, the \textit{capacity} of the FSC is defined as the supremum of all achievable rates. }

\section{Technical Proofs} \label{app:proof}
This section presents the proofs of Section \ref{sec:tradeoff}.
\subsection{Proof of Theorem \ref{thm:cap}}
{
The proof of Theorem \ref{thm:cap} relies on converting the action-state channel to an equivalent channel. This equivalence is also stated in Section \ref{sec:act2comm}, as it is crucial for the design of Act2Comm. To enhance readability, we present the equivalence here as well.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{image/Fig_1_the.pdf}
     \caption{\small\textcolor{black}{{The \textit{action-state channel} and the equivalent \textit{extended action-state channel}.}}}
     \label{FSC_fig_1_appendix}
     \vspace{-5pt}
\end{figure}

Let $\mathcal{U} \triangleq \mathcal{X}^{|\mathcal{S}|}$, where each ${u} \in \mathcal{U}$ is referred to as a decision rule because the $i$-th element of $u$, denoted by $u(i)$, can be viewed as an action for state $i$. A control policy is thus a collection of decision rules spanning the entire time horizon. To facilitate the capacity analysis, we convert the \textit{action-state channel} into an extended action-state (EAS) channel, as depicted in Fig. \ref{FSC_fig_1_appendix}, using Shannon's method \citep{shannon1958channels_ibm}. In particular, we conceptually separate the encoder and controller, considering the controller as an integral component of the EAS channel. We then assume that the channel state is available at the controller but not the encoder. At each time $t$, the encoder selects a decision rule ${u_t}$ from $\mathcal{U}$. Then the controller uses ${u_t}$ and the state $s_t$ to determine an action $x_t={u_t}(s_t)$. Consequently, the EAS channel has an input alphabet $\mathcal{U}$, output alphabet $\mathcal{S}$, and channel law: 
\begin{align*}
    P_{S^+|S,U}(s_{t+1}|s_t,u_t) = P_{S^+|S,X}(s_{t+1}|s_t,u_t(s_t))=\bm{T}(s_{t+1}|s_t,u_t(s_t)).
\end{align*}
The EAS channel and the \textit{action-state channel} are equivalent, and we will examine the EAS channel instead of the \textit{action-state channel} to derive the capacity.

%We showed in Section \ref{sec:act2comm} that the \textit{action-state channel} in equivalent to the EAS channel in terms of both capacity and coding scheme.
}

Assume that the initial state of the \textit{action-state channel} is fixed to be $s_1$. Then it is well-known that the capacity of the EAS channel is \citep{gallager1968information}
\begin{align} \label{eq:CS1}
	C(s_1)=  \max_{\{p(u_i|u^{i-1}) \}_{i\ge 1} } \lim_{N\to \infty}  \frac{1}{N} I(U^N;S_2^{N+1}|s_1),
\end{align}
where $I(U^N;S^{N+1}_2|s_1)$ is the mutual information given by
    \begin{align*}
		I(U^N;S^{N+1}_2|s_1) = \sum_{u^N\in \mathcal{U}^N} \sum_{s^{N+1}_2\in \mathcal{S}^N} p(u^N)P(s^{N+1}_2|u^N,s_1) \log \frac{P(s^{N+1}_2|u^N,s_1)}{\sum_{z^N\in \mathcal{U}^N}p(z^N)P(s^{N+1}_2|z^N,s_1) }.
    \end{align*}
For two random variables $S$ and $X$, let $H(X|S)$ denote the conditional entropy \citep{cover2005elements} of $X$ given $S$. Then we have
\begin{align*}
		\lim_{N\to \infty } \frac{1}{N} I(U^N;S_2^{N+1}|s_1) & = \lim_{N\to \infty } \frac{1}{N} \sum_{i=1}^{N} \left[H(S_{i+1}|S^i_2, s_1) - H(S_{i+1}|S^i_2, U^N, s_1) \right] \\
		& \overset{(a)}{=} \lim_{N\to \infty } \frac{1}{N} \sum_{i=1}^{N} \left[H(S_{i+1}|S^i_2, s_1) - H(S_{i+1}|S^i_2, U^N, X_i, s_1) \right] \\
		& = \lim_{N\to \infty } \frac{1}{N}  \sum_{i=1}^{N} \left[H(S_{i+1}|S_i, s_1) - H(S_{i+1}|S_i, X_i, s_1) \right]  \\
		& = \lim_{N\to \infty } \frac{1}{N}  \sum_{i=1}^{N} I(X_i; S_{i+1}|S_i, s_1) 
\end{align*}
where (a) holds because $X_i$ is determined by $S^i$ and $U_i$. It follows that
	\begin{align} \label{eq:expand-C}
		C&=\max_{\{p(u_i|u^{i-1}) \}_{i\ge 1}} \lim_{N\to \infty } \frac{1}{N} I(U^N;S_2^{N+1}|s_1) \notag \\
		& \overset{(a)}{=} \max_{\{p(x_i|s^i,x^{i-1}) \}_{i\ge 1}} \lim_{N\to \infty } \frac{1}{N}  \sum_{i=1}^{N} I(X_i; S_{i+1}|S_i, s_1) \notag \\
		& \overset{(b)}{=} \max_{\{p(x_i|s_i) \}_{i\ge 1}} \lim_{N\to \infty } \frac{1}{N} \sum_{i=1}^{N} I(X_i; S_{i+1}|S_i, s_1) 
	\end{align}
In \eqref{eq:expand-C}, (a) follows from the equivalence between the \textit{action-state channel} and the EAS channel. We next prove (b). 
Let $\eta$ denote a history-dependent encoder and $\{\eta_i(x_i|s^i, x^{i-1})\}_{i\ge 1}$ denote the associated input distribution. Furthermore, let $P^\eta_i(s,x)$ denote the probability that $S_i=s$ and $X_i=s$ conditioned on the encoder $\eta$. Note that $\eta$ can be viewed as a history-dependent policy for the MDP. Then according to MDP theory, for any history-dependent policy $\eta$, there exists a Markov policy $\eta'$  such that $\eta$ and $\eta'$ share the same joint probability distribution of states and actions. In particular, denote by $\eta'_i(x_i|s_i)$ the probability of selecting action $x_i$ given that the state is $s_i$ at time $i$. Then $\eta'$ can be seen as a Markov encoder with input distribution $\{\eta'_i(x_i|s_i)\}_{i\ge 1}$. By letting
	\begin{align*}
		\eta'_i(x_i|s_i) = \eta_i(x_i|s_i) \triangleq \sum_{s^{i-1}, x^{i-1}}\eta_i(x_i|s^i, x^{i-1})P(s^{i-1}, x^{i-1}|s_1),
	\end{align*}
 we have
	\begin{align*}
		P^\eta_i(s,x|s_1) = P^{\eta'}_i(s,x|s_1),\ \forall i\ge 1, s\in \mathcal{S}, x\in \mathcal{X}
	\end{align*}
We omit the proof here. The interested readers are referred to Theorem 5.5.1 of \citep{puterman2014markov} for the formal statement and proof. Consequently, we can verify that for any history-dependent encoder $\eta$, there exists a Markov encoder $\eta'$ such that they result in the same $I(X_i; S_{i+1}|S_i, s_1) $ for all $i$:
	\begin{align*}
		I_\eta(X_i; S_{i+1}|S_i, s_1) & = \sum_{s_i\in \mathcal{S}} \sum_{x_i\in \mathcal{X}}P^\eta_i(s_i, x_i|s_1)\sum_{s_{i+1}\in \mathcal{S}}\bm{T}(s_{i+1}|s_i,x_i) \log \frac{\bm{T}(s_{i+1}|s_i, x_i)}{\sum_{x'_i} \bm{T}(s_{i+1}|s_i,x'_i)\eta_i(x'_i|s_i)} \\
		&= \sum_{s_i\in \mathcal{S}} \sum_{x_i\in \mathcal{X}}P^{\eta'}_i(s_i, x_i|s_1)\sum_{s_{i+1}\in \mathcal{S}}\bm{T}(s_{i+1}|s_i,x_i) \log \frac{\bm{T}(s_{i+1}|s_i, x_i)}{\sum_{x'_i} \bm{T}(s_{i+1}|s_i,x'_i)\eta'_i(x'_i|s_i)}
	\end{align*}
We thus conclude that restricting on Markov encoders does not result in any loss of capacity. 

Next, we show that, as far as finding a capacity-achieving encoder is concerned, it is enough to consider stationary encoders. To see this, we reformulate \eqref{eq:expand-C} as a dynamic programming (DP) defined as follows:
	\begin{itemize}[left=0.2cm]
		\item state at time $i$: $s_i\in \mathcal{S}$
		\item action at time $i$ given state $s_i$: $q_i(s_i)  \in \Delta(\mathcal{X}) $ with $q_i(x_i|s_i) = P(x_i|s_i)$ being the $i$-th element
		\item transition law: $P(s_{i+1}|s_i,x_i) = \sum_{x_i \in \mathcal{X}} q_i(x_i|s_i) \bm{T}(s_{i+1}|s_i,x_i)$
		\item reward function:
		\begin{align*}
			r(s_i, q_i(s_i)) =  \sum_{x_i\in \mathcal{X}}q_i(x_i|s_i)\sum_{s_{i+1}\in \mathcal{S}}\bm{T}(s_{i+1}|s_i,x_i) \log \frac{\bm{T}(s_{i+1}|s_i, x_i)}{\sum_{x'_i} \bm{T}(s_{i+1}|s_i,x'_i){q_i}(x'_i|s_i)}
		\end{align*}
	\end{itemize}
Then the capacity expression given in \eqref{eq:expand-C} can be written as
	\begin{align} \label{eq: capacity-DP}
		C = \max_{\{q_i\}_{i\ge 1}} \lim_{N\to \infty } \frac{1}{N} \sum_{i=1}^{N} \sum_{s_i\in \mathcal{S}} P(s_i) r(s_i, q_i(s_i))
	\end{align}
We can think of \eqref{eq: capacity-DP} as a problem of maximizing the long-term average reward of the above DP over the set of Markov deterministic policies. Essentially, the DP is an MDP with finite state space, compact action space, and bounded reward function. We can easily verify the following:
	\begin{itemize}[left=0.2cm]
		\item [1.] The reward function is a continuous function of action $d_i$.
		\item [2.] The transition law depends continuously on the action $d_i$.
		\item [3.] Any stationary policy yields a Markov chain with one ergodic class and a possibly empty set of transient states (Assumption 1).
	\end{itemize}
Then, according to MDP theory (see, e.g., \citep{fainberg1976controlled} and \citep{hernandez2012discrete}), the maximum of \eqref{eq: capacity-DP} can be attained by a stationary deterministic policy. Therefore, problem \eqref{eq: capacity-DP} is equivalent to
	\begin{align} \label{eq: capacity-SD}
		C = \max_{\{\pi(\cdot|s)\in \Delta( \mathcal{X}):s\in \mathcal{S} \}} \lim_{N\to \infty } \frac{1}{N} \sum_{i=1}^{N}  I(X_i; S_{i+1}|S_i, s_1) 
	\end{align}
Note that each  $\{\pi(\cdot|s)\in \Delta( \mathcal{X}):s\in \mathcal{S} \}$ corresponds to a stationary policy for the original MDP. Under Assumption 1, any stationary policy yields a Markov chain with equilibrium state distribution $\rho_\pi(s)$. Therefore, the following probability converges to a probability that is independent of $s_1$:
	\begin{align*}
		\lim_{i\to \infty}P(X_i=x, S_i=s, S_{i+1}=s'|s_1) = \rho_\pi(s) \pi(x|s)\bm{T}(s'|s,x) 
	\end{align*}
As a result, $I(X_i; S_{i+1}|S_i, s_1) $ also converges to a value that is independent of $s_1$. The desired result follows immediately.


\subsection{Proof of Theorem \ref{thm:con-cap}}
We first show that the capacity of the \textit{action-state channel} without reward constraint can be written as:
\begin{align} \label{eq:C-I(w)}
    C=\max_{w \in \mathcal{W}} \ &I(w,\bm{T}). 
\end{align}
To see this, using Theorem \ref{thm:cap} and the formula $\pi(x|s) = {w_\pi(s,x)}/{\sum_{x'} w_\pi(s,x')}$ yields
	\begin{align*}
		I(X; S'|S) &= \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} \rho_\pi(s)\pi(x|s)\sum_{s'\in \mathcal{S}}\bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)}{\sum_{x'} \bm{T}(s'|s,x')q(x'|s)} \\
		& = \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} w_\pi(s,x)\sum_{s'\in \mathcal{S}}\bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w_q(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_q(s,x')}. 
	\end{align*}
Then the equivalence between \eqref{eq:C-I(w)} and the capacity expression given in Theorem \ref{thm:cap} follows immediately from the one-to-one mapping between $\mathcal{W}$ and $\Pi_S$. 

It is well-known that the long-term average reward of a policy can be expressed as a linear function of its occupation measure:
\begin{align} \label{eq:2-2}
    G_\pi = \lim_{N\rightarrow\infty}\frac{1}{N}\mathbb{E}_\pi\left[\sum_{t=1}^{N}r(s_t,x_t)|s_1\sim\alpha\right]=\sum_{s\in \mathcal{S}}\sum_{x\in \mathcal{X}}w_\pi(s,x) r(s,x).
\end{align}
Since the reward constraint is linear, to show that the optimization problem is a convex optimization, it suffices to prove that $I(w,\bm{T})$ is a concave function. For any $\lambda\in [0,1]$ and $\bar{\lambda} = 1-\lambda$, let $w = \lambda w_1 + \bar{\lambda} w_2$, where $w_1, w_2 \in \mathcal{W}$. Then we have
	\begin{align} \label{eq: prop-2}
		&\sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x) \log \frac{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}{\bm{T}(s'|s,x)\sum_{x''}w(s,x'') } \notag \\
		=& \sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x) \log \frac{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}{\sum_{x''}w(s,x'') } - \sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x) \log {\bm{T}(s'|s,x) }.
	\end{align}
The second term of \eqref{eq: prop-2} is clearly linear w.r.t. $w$. For the first term, an application of log-sum inequality gives
	\begin{align} \label{eq: prop-3}
		& \left(\sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x)  \right) \log \frac{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}{\sum_{x''}w(s,x'') } \notag \\
		\le &\lambda \left(\sum_{x\in \mathcal{X}} w_1(s,x) \bm{T}(s'|s,x)  \right) \log \frac{\sum_{x'} \bm{T}(s'|s,x')w_1(s,x')}{\sum_{x''}w_1(s,x'') } \notag  \\ 
		&+ \bar{\lambda}\left(\sum_{x\in \mathcal{X}} w_2(s,x) \bm{T}(s'|s,x)  \right) \log \frac{\sum_{x'} \bm{T}(s'|s,x')w_2(s,x')}{\sum_{x''}w_2(s,x'') }.
	\end{align}
	Combining \eqref{eq: prop-2} and \eqref{eq: prop-3} yields
	\begin{align}
		&\sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w(s,x')} \notag \\ 
		\ge & \lambda \sum_{x\in \mathcal{X}} w_1(s,x) \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w_1(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_1(s,x')} \notag \\
		& + \bar{\lambda} \sum_{x\in \mathcal{X}} w_2(s,x) \bm{T}(s'|s,x) \log \frac{\bm{T}(s'|s,x)\sum_{x''}w_2(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_2(s,x')}.
	\end{align}
	Summing both sides of the above inequality over $s$ and $s'$ yields
	\begin{align*}
		I(w,\bm{T}) = I(\lambda w_1 + \bar{\lambda}w_2, \bm{T}) \ge \lambda I(w_1,\bm{T}) + \bar{\lambda}I(w_2,\bm{T}).
	\end{align*}
We thus conclude that $I(w,\bm{T})$ is a concave function of $w$. This completes the proof.
 
\subsection{Proof of Lemma \ref{lem:concave}}
Define
\begin{align*}
	\mathcal{W}_V = \left\{w\in \mathcal{W}: \sum_{s\in \mathcal{S}}\sum_{x\in \mathcal{X}} w(s,x)r(s,x) \ge V  \right\}.
\end{align*}
For any achievable $V_1$ and $V_2$, let
	\begin{align*}
		w_i = \arg \max_{w\in \mathcal{W}_{V_i}} I(w,\bm{T}),\ i=1,2.
	\end{align*}
	Then $C(V_i) = I(w_i,\bm{T})$, $i=1,2$. For any $\theta\in [0,1]$, let $\bar{\theta} = 1-\theta$. Let $V = \theta V_1 + \bar{\theta} V_2$ and $w_3 = \theta w_1 + \bar{\theta} w_2$. Then clearly $w_3\in \mathcal{W}_V$. We thus have
	\begin{align*}
		C(V) = \max_{w\in \mathcal{W}_{V}} I(w,\bm{T}) \ge I(w_3,\bm{T}) \ge \theta I(w_1, \bm{T}) + \bar{\theta} I(w_2, \bm{T}) = \theta C(V_1) + \bar{\theta} C(V_2).
	\end{align*}
We thus conclude that $C(V)$ is concave.

\subsection{Proof of Lemma \ref{lem: tangent}}
Since $I(w,\bm{T})$ is concave w.r.t. $w$ and $l(w,w_n, \bm{T})$ is linear w.r.t. $w$, to show that $l(w,w_n,\bm{T})$ is a tangent line of $I(w,\bm{T})$ at point $w_n$, it is enough to prove that statements (i) and (ii) hold. Statement (i) holds trivially by the definitions of the two functions.
	
For statement (ii), consider
	\begin{align*}
		&l(w,w_n,\bm{T}) - I(w,\bm{T})  \\
  =& \sum_{s\in \mathcal{S}} \sum_{x\in \mathcal{X}} w(s,x) \sum_{s'\in \mathcal{S}} \bm{T}(s'|s,x) \log \frac{\sum_{x''}w_n(s,x'') }{\sum_{x'} \bm{T}(s'|s,x')w_n(s,x')} \frac{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}{ \sum_{x''}w(s,x'') }.
	\end{align*}
Define
	\begin{align*}
		P_{w}(s'|s) = \frac{\sum_{x'} \bm{T}(s'|s,x')w(s,x')}{ \sum_{x''}w(s,x'') }, \ P_{w_n}(s'|s) = \frac{\sum_{x'} \bm{T}(s'|s,x')w_n(s,x')}{ \sum_{x''}w_n(s,x'') }. 
	\end{align*}
Note that $\sum_{s'}P_w(s'|s)=1$. Hence it is indeed a conditional probability.
Then
	\begin{align*}
		l(w,w_n,\bm{T}) - I(w,\bm{T})  &= \sum_{s\in \mathcal{S}} \sum_{s'\in \mathcal{S}} \left(\sum_{x\in \mathcal{X}} w(s,x) \bm{T}(s'|s,x) \right)  \log \frac{P_w(s'|s)}{P_{w_n}(s'|s)} \\
		& = \sum_{s\in \mathcal{S}} \sum_{x\in\mathcal{X}} w(s,x) \sum_{s'\in \mathcal{S}}  P_w(s'|s)  \log \frac{P_w(s'|s)}{P_{w_n}(s'|s)} \\
		& \ge 0, 
	\end{align*} 
where the inequality follows from the non-negativity of relative entropy. Therefore, $l(w,w_n,\bm{T})$ is a tangent line of $I(w,\bm{T})$ at point $w_n$.
 
{\section{System Explanation and Algorithm Details}
\label{app_model}
%\subsection{Model design}
In this Appendix, we first clarify the relationship between the theoretical results in Section 4 and the algorithm in Section 5, as this distinction may be unclear to those unfamiliar with information and coding theory. Following this clarification, we provide additional details about the components of Act2Comm.

The concave optimization problem in Theorem 2 characterizes the capacity-reward tradeoff of the action-state channel. It provides an upper bound for practically achievable coding rates under a certain reward constraint. Solving the concave optimization yields the capacity and the optimal state-action distribution $\omega$, which can be translated to a stationary policy for the MDP via the following formula:
             \begin{align*}
                \pi(a|s) = \frac{\omega(s,a)}{\sum_{a'} \omega(s,a')}. 
            \end{align*}
Note that this policy $\pi$ can not be directly used as the coding policy for communication, as the coding policy needs to generate actions based on both the state and the message. Therefore, in Section 5, we propose Act2Comm to learn a coding policy that mimics the behavior of policy $\pi$ from the perspective of MDP control, with the input of message and feedback block.
     
        
For practical channel coding in the finite block-length regime, historical feedback has proven beneficial based on prior experience with conventional channel coding. To effectively map messages and feedback to actions, the transformer architecture with an attention mechanism is naturally well-suited, forming the backbone for both the encoder and decoder components. 

%Furthermore, to overcome the challenge of non-differentiability, we propose an iterative training strategy that incorporates a critic network to optimize the encoder and decoder.
    
%Instead of directly outputting actions at each time step, our encoder generates a continuous belief map, which is subsequently quantized into decision rules. This approach not only enhances performance but also simplifies the training of the critic network.

%\subsection{Analysis of Each System Component}
The components of communication via actions in MDPs using the proposed Act2Comm are summarized in Table. \ref{model_compo}. In particular,
\begin{itemize}
    \item The encoder encodes a belief matrix $\bm{Z}^{(\tau)} \in \mathbb{R}^{\tau \times \frac{\mu |\mathcal{S}|}{R}}$ using message block $\bm{B}^{(\tau)}$ and feedback block $\bm{C}^{(\tau)}$ at each coding round $\tau$. After completing all $l$ coding rounds, a final belief map $\bm{Z} \in \mathbb{R}^{\frac{k}{\mu} \times \frac{\mu |\mathcal{S}|}{R}}$ is constructed. Note that instead of directly outputting actions at each time step, our encoder generates a continuous belief map, which is subsequently quantized into decision rules. This approach not only enhances performance but also simplifies the training of the critic network. 
    \item A quantizer then generates the codeword $\bm{U} \in \mathcal{X}^{|\mathcal{S}| \times \frac{k}{R}}$, where each element of the resultant codeword, $x_t = U[s_t, t]$, represents the selected action for state $s_t$ at time step $t$.
    \item Given the action and state, EAS channel then returns next states. 
    \item The decoder collects all states and then performs the decoding process, which outputs logits $\bm{\hat{M}}\in\mathbb{R}^{\frac{k}{\mu}\times 2^\mu}$ for the message.
    \item Since the gradient cannot propagate through the EAS and quantizer to the encoder, a critic network is introduced to link the belief map $\bm{Z}$ to the decoded logits $\bm{\hat{M}}$. Specifically, the critic network is trained to predict $\bm{\hat{M}}_{k}$ from $\bm{Z}_{k}$, producing $\bm{\hat{M}}_{ck}$ at each inner optimization step $k$ within the total $s_{in}$ steps. Thanks to the introduction of the critic network, the gradient can propagate through it, from logits to the belief map. As shown in the table, it effectively ``links" the encoder's output to the decoder's output, thereby ``skipping" the quantizer, EAS channel and decoder, which are treated as the unknown environment during the training phase.
\end{itemize}
\begin{table*}[t]
\caption{\textcolor{black}{Functionality of Each System Component}}
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Model components}  & \textbf{Function} & \textbf{Input}&\textbf{Output} \\
\hline
\textbf{Encoder}  & Coding the belief matrix & $\bm{B^{(\tau)}}$, $\bm{C^{(\tau)}}$&$\bm{Z}$\\
\hline
\textbf{Quantizer}  & Generate decision rules & $\bm{Z}$& $\bm{U}$\\
\hline
\textbf{EAS channel}  & Generate state from actions & $\bm{U}\rightarrow\bm{x_t}$& $\bm{s_{t+1}}$\\
\hline
\textbf{Decoder}  & Decoding the message& $\bm{s}$& $\bm{\hat{M}}\rightarrow\bm{\hat{m}}$\\
\hline
\textbf{Critic Network}  & Estimate decoding logits & $\bm{Z_k}$& $\bm{\hat{M}_{ck}}$\\
\hline
\end{tabular}
\label{model_compo}
\end{table*}

\subsection{Quantizer}
\textcolor{black}{The quantizer, $\mathcal{Q}: \mathbb{R}^{|\mathcal{S}|\times \frac{k}{R}}\rightarrow \mathcal{X}^{|\mathcal{S}|\times \frac{k}{R}}$, is designed to convert continuous coding results $|\mathcal{X}| \cdot \text{Sigmoid}(\bm{Z})$ into finite values corresponding to actions within the channel input alphabet set $\mathcal{X}=\{0,1,\ldots, |\mathcal{X}|-1\}$. For example, if there exist $5$ actions and $1$ states, then each element of the coding result $|\mathcal{X}| \cdot \text{Sigmoid}(\bm{Z}) \in (0,5)$ will be rounded down into the nearest action index. If $|\mathcal{X}| \cdot \text{Sigmoid}(\bm{Z}) =[1.5,0.8,2.1,3.2,4.8]$, it will be rounded down to $[1,0,2,3,4]$ as the channel input via the quantizer $\mathcal{Q}$.}
\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.7]{image/Action_net_train.pdf}
     \caption{\textcolor{black}{Illustration of the iterative training process: (a) Gradient flow in the proposed method, where blue arrows indicate the gradient flow, and red arrows represent the forward process. (b) Diagram of the update steps, where the red block represents the component being updated, the blue block represents the frozen component, and the grey block indicates an unused component. (c) Inference phase for the well-trained \textit{Act2Comm} model, where the critic network is removed.}}
     \label{Act_fig_train}
 \end{figure*}

 
 

\textcolor{black}{Here, we adopt hard rounding, despite the availability of advanced quantization methods such as soft rounding or noise injection during training. The chosen approach is intentionally straightforward, and a critic network effectively mitigates the non-differentiability introduced by hard rounding.}

\subsection{Critic Network and Iterative training}
\label{app_iter_train}

\textcolor{black}{As shown in Fig. \ref{Act_fig_train}, a critic network is introduced to facilitate gradient backpropagation for encoder optimization, treating the quantizer, channel, and decoder as an unknown environment. Before updating the encoder and decoder, the Critic network is trained over $s_{in}$ steps to capture knowledge of the environment, with the encoder and decoder frozen during this phase. Subsequently, the encoder is updated with the assistance of the Critic network, aiming to jointly optimize for control and communication, while keeping the decoder frozen. Following this, the optimized encoder is frozen, and the decoder is updated based on the newly optimized encoder. With the updated decoder, the Critic network is retrained to adapt to the updated environment, preparing it to support the next round of encoder updates.}

\textcolor{black}{This process iteratively alternates between freezing one component while optimizing the other. From the experiment, we observe that the training process is more stable when the encoder is updated once for every two updates of the decoder. The effectiveness of this approach is visualized in a Fig. \ref{loss} and Table. \ref{abla}. The detailed training algorithm is presented in the Algorithm. \ref{Act_fig_train}}

\subsection{Training and inference algorithm}
\label{app_infer}
\textcolor{black}{To enhance readers' understanding of the training and inference process, we provide the pseudocode and illustration figures for \textit (see Fig. \ref{Act_fig_train}) {Act2Comm}, detailing both the training and inference phases, as shown in Algorithms \ref{alg_train} and \ref{alg_inference}. Furthermore, Fig. \ref{loss} visualizes the training process by tracking the loss values throughout the iterative updates. For additional details about the training, the training logs and source code are also available on the project page of this paper.}

\textcolor{black}{To be more specific for the training phase, we train the encoder first and then train the decoder. The critic network, with control loss, is applied to the encoder. The decoder only considers the communication loss. After training, the critic network is removed, as shown in Fig. \ref{Act_fig_train} (c). Only the encoder and decoder are deployed for communication and control, as implemented in Fig. \ref{Act_fig_implement}.}


\definecolor{royalblue}{RGB}{65, 105, 225}

\begin{algorithm}[!h] 
    \caption{\textcolor{black}{Training strategy of the \textit{Act2Comm} scheme}}
    \textbf{Input:} The initial MDP state: $s_0$, Inner optimization step: $s_{in}$, \\
    Learning rates of encoder, decoder, and critic network: $\beta, \alpha, \alpha$, with $\alpha=5\beta$,\\
      Target policy: $\bm{\pi}$, balanced parameter $\lambda$.\\
    \textbf{Output:} Parameters of the encoder and decoder: $\bm{\theta}$, $\bm{\xi}$.\\
    \vspace{-10pt}
    \begin{algorithmic}[1]
        \FOR{ the $i$-th step within the total training steps}
                 \STATE   $\mathbf{m}=[\bm{b_1};\bm{b_2};\ldots;\bm{b_l}] \in \{0,1\}^{l\times \mu}$,
        \COMMENT{\text{Randomly sample and divide the message}}\\
           \STATE $\mathbf{C^{(1)}}\in 0^{1 \times \frac{\mu}{R}\times 3}$,
        \COMMENT{\text{Initialize the feedback block}}\\

    \FOR{each coding round $\tau$ ($1\leq \tau \leq l$)}
        \STATE   $\mathbf{B^{(\tau)}}=[2\bm{b_1}-1;\ldots;2\bm{b_\tau}-1] \in \mathbb{R}^{\tau\times \mu}$,
        \COMMENT{\text{Construct the message block}}\\
            \STATE  
$\mathbf{C^{(\tau)}}=[\bm{C_1};\ldots;\bm{C_\tau}] \in \mathbb{R}^{\tau\times \frac{\mu}{R}\times 3}$,
        \COMMENT{\text{Update the feedback block}}
        \STATE   $\mathbf{Z^{(\tau)}}=\mathcal{E}_{\bm{\theta}}(\mathbf{B^{(\tau)}},\mathbf{C^{(\tau)}}) \in \mathbb{R}^{\tau\times \frac{\mu|S|}{R}}$,

        $ \mathbf{z^{(\tau)}}=\bm{Z^{(\tau)}}[\tau,:]\in \mathbb{R}^{\frac{\mu|S|}{R}} \rightarrow \mathbb{R}^{|S|\times \frac{\mu}{R}}$,
                    \COMMENT{Encoding the belief map}
        \STATE   $\mathbf{U^{(\tau)}}=\mathcal{Q}(|\mathcal{X}|\text{Sigmoid}(\bm{z^{(\tau)}})) \in \mathcal{X}^{|S|\times \frac{\mu}{R}}$,                    
        \COMMENT{Quantization for decision rules}
                \FOR{$t = 1 : 1 : \frac{\mu}{R}$}
           \STATE   $s_{t+1}=\bm{T}(s_{t+1}|s_t,\mathbf{U^{(\tau)}}[s_t,t])$,
               \COMMENT{Go through the EAS channel for $\frac{\mu}{R}$ time steps}
            \STATE   $\bm{c_t^{(\tau)}}=[s_t^{(\tau)},x_t^{(\tau)},s_{t+1}^{(\tau)}] \in \mathbb{R}^{1 \times 3}$,
               \COMMENT{Update the feedback vector}
        \ENDFOR
           \STATE   $\bm{C_\tau}=[\bm{c_1^{(\tau)}};\ldots;\bm{c_{\frac{\mu}{R}}^{(\tau)}}] \in \mathbb{R}^{\frac{\mu}{R}\times 3}$,
         \COMMENT{Update the feedback matrix}
            \ENDFOR
       \STATE   $\bm{Z}=[\bm{z^{(1)}};\ldots;\bm{z^{(l)}}]  \in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu|S|}{R}}$, $\bm{s}=[s_1;\ldots;s_{\frac{k}{R}}]  \in \mathcal{S}^{\frac{k}{R}}\rightarrow \mathcal{S}^{\frac{k}{\mu}\times \frac{\mu}{R}}$,\\
        \STATE   $\bm{\hat{M}}=\mathcal{D}(\bm{s}) \in \mathbb{R}^{\frac{k}{\mu}\times 2^{\mu}}$,
         \COMMENT{Collect codewords, states, and decode the logits}

       \STATE  \makebox[0.5\linewidth]{\hrulefill} 
        \COMMENT{\textcolor{orange}{Train the Critic network}}\\
        \IF {$i\%2==0$}
            \FOR{$k = 1 : 1 : s_{in}$} 
               \STATE   $\bm{Z_k}=\bm{Z}+\bm{W_k}$, \text{with} $\bm{W_k}\in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu|S|}{R}}\sim \mathcal{N}(0,\sigma_w^2)$  
                   \COMMENT{Neighboring sampling}
       \STATE   $\mathbf{U_k}=\mathcal{Q}(|\mathcal{X}|\text{Sigmoid}(\bm{Z_k})) \in \mathcal{X}^{|S|\times \frac{k}{R}}$,       \\
              \FOR{$t = 1 : 1 : \frac{k}{R}$}
           \STATE   $s_{t+1}=\bm{T}(s_{t+1}|s_t,\mathbf{U_{(k)}}[s_t,t])$,
               \COMMENT{Go through the EAS channel}
        \ENDFOR
               \STATE   $\bm{\hat{s}_{(k)}}=[s_1;\ldots;s_{\frac{k}{R}}]  \in \mathcal{S}^{\frac{k}{R}}\rightarrow \mathcal{S}^{\frac{k}{\mu}\times \frac{\mu}{R}}$,
         \COMMENT{Collect the observed states}
                             \STATE   $\bm{\hat{M}_{k}}=\mathcal{D}(\bm{\hat{s}_{(k)}}) \in \mathbb{R}^{\frac{k}{\mu}\times 2^{\mu}}$,
         \COMMENT{Decode the logits with a frozen decoder}
                     \STATE $\bm{\hat{M}_{ck}}=\mathcal{C}(\bm{Z_{k}})\in \mathbb{R}^{\frac{k}{\mu}\times 2^{\mu}}$
                \COMMENT{Predict the logits with critic network}\\
                                        
        \STATE $\mathcal{L}_{cn}=\text{MSE}(\bm{\hat{M}_{ck}},\bm{\hat{M}_{k}})$
        \COMMENT{Compute the critic loss with MSE}
                        
        \STATE \textcolor{orange}{$\bm{\phi_{k+1}}=\bm{\phi_{k}}-\beta\nabla_{\bm{\phi}}\mathcal{L}_{cn}$}
        \COMMENT{\textcolor{orange}{Update the Critic network.}}
            \ENDFOR  
            
         \STATE \makebox[0.5\linewidth]{\hrulefill}
                        \COMMENT{\textcolor{royalblue}{Train the Encoder with a frozen Decoder}}\\
        %\STATE $\hat{f}_U(x|s)=\frac{1}{kR} \left[\Gamma_Z(\bm{T},s,x-1)\bm{e}^\top-\Gamma_Z(\bm{T},s,x)\bm{e}^\top \right]$  \COMMENT{Estimate the policy distribution}\\
        \STATE $\mathcal{L}_{cont}=\text{MSE}(\bm{\pi},\bm{\hat{f}})$
                                \COMMENT{Control loss term for the estimated $\hat{f}$}\\
    \STATE $\mathcal{L}_{com}=\text{cross-entropy}(\bm{m},\bm{\hat{M}_{ck}})$
                                \COMMENT{Communication loss term}\\

        \STATE \textcolor{royalblue}{$\bm{\theta}=\bm{\theta}-{\alpha}\nabla_{\bm{\theta}} (\mathcal{L}_{com}+\lambda\mathcal{L}_{cont})$}
        \COMMENT{\textcolor{royalblue}{Update the encoder}}
        \ENDIF
     \STATE \makebox[0.5\linewidth]{\hrulefill}
                        \COMMENT{\textcolor{blue}{Train the Decoder with a frozen Encoder}}\\
    \FOR{each coding round and corresponding time step}
         \STATE   $\mathbf{z^{(\tau)}}=\mathcal{E}_{\bm{\theta}}(\mathbf{B^{(\tau)}},\mathbf{C^{(\tau)}})[\tau,:]\rightarrow \bm{U^{(\tau)}}$,
                    \COMMENT{Re-encode and re-quantize the decision rules}
           \STATE   $s_{t+1}=\bm{T}(s_{t+1}|s_t,\mathbf{U^{(\tau)}}[s_t,t])$,
               \COMMENT{Re-go through the EAS channel for all time steps}
        \ENDFOR
         \STATE   $\bm{s}=[s_1;\ldots;s_{\frac{k}{R}}]$, $\bm{\hat{M}}=\mathcal{D}(\bm{s})$
         \COMMENT{Decode the observed states from a forzen encoder}
           \STATE $\mathcal{L}_{com}=\text{cross-entropy}(\bm{m},\bm{\hat{M}})$
                                \COMMENT{Communication loss term}\\
    \STATE \textcolor{blue}{$\bm{\xi}=\bm{\xi}-{\beta}\nabla_{\bm{\xi}} \mathcal{L}_{com}$}
        \COMMENT{\textcolor{blue}{Update the decoder}}

    \ENDFOR
    \end{algorithmic}
    \vspace{+3pt}
    \label{alg_train}
\end{algorithm}


\begin{algorithm}[!h] 
    \caption{\textcolor{black}{Inference of the \textit{Act2Comm} scheme}}
    \textbf{\textcolor{royalblue}{Input and output for the encoding:}}\\
    \textbf{Input:} Initial state: $s_0$, $k$-bits message $\mathbf{m}$, coding rate $R$, well-trained encoder: $\bm{\theta^*}$;\\
    \textbf{Output:} Codeword (decision rules) $\bm{Z}\in \mathbb{R}^{|S|\times \frac{k}{R}}$.\\
          \makebox[1.0\linewidth]{\hrulefill}\\
          \textbf{\textcolor{blue}{Input and output for the decoding:}}\\
    \textbf{Input:} Observed states $\bm{s}\in \mathcal{S}^{\frac{k}{R}}$, well-trained decoder $\bm{\xi^*}$;\\
    \textbf{Output:} Predicted message: $\bm{\hat{m}}$.\\
              \makebox[1.0\linewidth]{\hrulefill}\\
    \vspace{-10pt}
    \begin{algorithmic}[1]
       \STATE     \textbf{\textcolor{royalblue}{Encoding phase:}}\\
           \STATE $\mathbf{C^{(1)}}\in 0^{1 \times \frac{\mu}{R}\times 3}$,
        \COMMENT{\text{Initialize the feedback block}}\\

    \FOR{each coding round $\tau$ ($1\leq \tau \leq l$)}
        \STATE   $\mathbf{B^{(\tau)}}=[2\bm{b_1}-1;\ldots;2\bm{b_\tau}-1] \in \mathbb{R}^{\tau\times \mu}$,
        \COMMENT{\text{Construct the message block}}\\
            \STATE  
$\mathbf{C^{(\tau)}}=[\bm{C_1};\ldots;\bm{C_\tau}] \in \mathbb{R}^{\tau\times \frac{\mu}{R}\times 3}$,
        \COMMENT{\text{Update the feedback block}}
        \STATE   $\mathbf{Z^{(\tau)}}=\mathcal{E}_{\bm{\theta}}(\mathbf{B^{(\tau)}},\mathbf{C^{(\tau)}}) \in \mathbb{R}^{\tau\times \frac{\mu|S|}{R}}$,

        $ \mathbf{z^{(\tau)}}=\bm{Z^{(\tau)}}[\tau,:]\in \mathbb{R}^{\frac{\mu|S|}{R}} \rightarrow \mathbb{R}^{|S|\times \frac{\mu}{R}}$,
                    \COMMENT{Encoding the belief map}
        \STATE   $\mathbf{U^{(\tau)}}=\mathcal{Q}(|\mathcal{X}|\text{Sigmoid}(\bm{Z})) \in \mathcal{X}^{|S|\times \frac{\mu}{R}}$,                    
        \COMMENT{Quantization for decision rules}
                \FOR{$t = 1 : 1 : \frac{\mu}{R}$}
                \STATE $x_t^{(\tau)}={U^{(\tau)}}[s_t,t])$\\
              $s_{t+1}^{(\tau)}=\bm{T}(s_{t+1}^{(\tau)}|s_t^{(\tau)},x_t^{(\tau)})$,
               \COMMENT{Go through the EAS channel for $\frac{\mu}{R}$ time steps}
            \STATE   $\bm{c_t^{(\tau)}}=[s_t^{(\tau)},x_t^{(\tau)},s_{t+1}^{(\tau)}] \in \mathbb{R}^{1 \times 3}$,
               \COMMENT{Update the feedback vector}
        \ENDFOR
           \STATE   $\bm{C_\tau}=[\bm{c_1^{(\tau)}};\ldots;\bm{c_{\frac{\mu}{R}}^{(\tau)}}] \in \mathbb{R}^{\frac{\mu}{R}\times 3}$,
         \COMMENT{Update the feedback matrix}
            \ENDFOR
       \STATE   $\bm{Z}=[\bm{z^{(1)}};\ldots;\bm{z^{(l)}}]  \in \mathbb{R}^{\frac{k}{\mu}\times \frac{\mu|S|}{R}}\rightarrow \mathbb{R}^{|S|\times \frac{k}{R}}$,         
         \COMMENT{Collect the final codeword}\\
                   \makebox[1.0\linewidth]{\hrulefill}\\
    \STATE \textbf{\textcolor{blue}{Decoding phase:}}\\

       \STATE   $\bm{s}=[s_1;\ldots;s_{\frac{k}{R}}]  \in \mathcal{S}^{\frac{k}{R}}\rightarrow \mathcal{S}^{\frac{k}{\mu}\times \frac{\mu}{R}}$,
         \COMMENT{Collect the observed states}

        \STATE   $\bm{\hat{M}}=\mathcal{D}(\bm{s}) \in \mathbb{R}^{\frac{k}{\mu}\times 2^{\mu}}$,
         \COMMENT{Decode the logits}

        \STATE $\bm{\hat{m}}=\text{argmax} (\text{Softmax}(\bm{\hat{M}}),\text{dim}=-1)$          \COMMENT{Decode each block labels}
    \end{algorithmic}
    %\vspace{+3pt}
    \label{alg_inference}
\end{algorithm}
\begin{table*}[t]
\caption{\textcolor{black}{Number of parameters, FLOPs, and coding times for the \textit{Act2Comm} scheme ($K=12$, $\mu=3$, $R=1/3$, $s_{in}=20$) with varying state numbers and action numbers, where the increased value are colored with orange and red.}}
\centering
\begin{tabular}{|c|ccc|ccc|}
\hline
\textbf{Action and state number} & \multicolumn{3}{c|}{\textbf{Encoder}} & \multicolumn{3}{c|}{\textbf{Decoder} }\\ 
\hline
\textbf{$(\mathcal{|A|}, \mathcal{|S|})$} with \textbf{$\mathcal{|A|} \uparrow	$} & (\textbf{$5,16$})& (\textbf{$\textcolor{orange}{20},16$}) & (\textbf{$\textcolor{red}{40},16$}) & (\textbf{$5,16$})& (\textbf{$\textcolor{orange}{20},16$}) & (\textbf{$\textcolor{red}{40},16$})\\ 

\hline
\textbf{Parameters (k)} &$32.244$ & $32.244$ & $32.244$ &  $50.536$&  $50.536$&  $50.536$\\
\textbf{FLOPs (millions)} &$0.3164$ &$0.3164$ &$0.3164$& $0.1989$& $0.1989$& $0.1989$\\
\textbf{Coding Time (ms)} &$6.691$&$6.691$&$6.691$& $2.613$& $2.613$& $2.613$ \\
\hline
\hline
\textbf{$(\mathcal{|A|}, \mathcal{|S|})$} with \textbf{$\mathcal{|S|} \uparrow	$} & (\textbf{$5,16$})& (\textbf{$5,\textcolor{orange}{64}$}) & (\textbf{$5,\textcolor{red}{256}$}) & (\textbf{$5,\textcolor{orange}{16}$})& (\textbf{$5,\textcolor{orange}{64}$}) & (\textbf{$5,\textcolor{red}{256}$})\\ 
\hline
\textbf{Parameters (k)} &$32.244$ & $\textcolor{orange}{46.51}$  & $\textcolor{red}{103.52}$ &  $50.536$&  $50.536$&  $50.536$\\
\textbf{FLOPs (millions)} &$0.3164$ &$\textcolor{orange}{0.4546}$&$\textcolor{red}{1.007}$& $0.1989$& $0.1989$& $0.1989$\\
\textbf{Coding Time (ms)} &$6.691$  &$\textcolor{orange}{6.69}$  &$\textcolor{red}{7.04}$& $2.613$& $2.613$& $2.613$ \\
\hline
\end{tabular}
\label{table_para_scale}
\end{table*}
\begin{table*}[t]
\caption{\textcolor{black}{Number of parameters, FLOPs, and coding times for the \textit{Act2Comm} scheme ($K=12$, $\mu=3$, $R=1/3$, $s_{in}=20$) with varying critic network training steps $s_{in}$ for both training and inference phases. Note that the backpropagation computational complexity is approximately $2–3$ times that of the forward computation.}}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|c|}{\textbf{Training Phase (forward)}} \\ 
\hline
\multirow{2}{*}{\textbf{Model components}} & \multirow{2}{*}{\textbf{Encoder}} & \multirow{2}{*}{\textbf{Decoder}} & \textbf{CrititNet}& \textbf{CriticNet}& \textbf{CriticNet}\\ 
& & & $s_{in}=5$&$s_{in}=10$&$s_{in}=20$\\
\hline
\textbf{Parameters (k)}  & 32.244 & 50.536&7.568&7.568 &7.568 \\ 
\hline
\textbf{FLOPs (millions)} & 0.3164 & 0.1989 & 0.1454 &0.2909 &0.5818\\ 
\hline
\hline
\multicolumn{6}{|c|}{\textbf{Inference Phase}} \\ 
\hline
\textbf{Model components} & \multicolumn{2}{c|}{\textbf{Encoder}} & \multicolumn{1}{c|}{\textbf{Decoder} }& \multicolumn{2}{c|}{\textbf{Critic network}} \\ 
\hline
\textbf{Parameters (k)} &\multicolumn{2}{c|}{ 32.244} & \multicolumn{1}{c|}{50.536} & \multicolumn{2}{c|}{\ding{55} }\\ 
\hline
\textbf{FLOPs (millions)} &\multicolumn{2}{c|}{ 0.3164} & \multicolumn{1}{c|}{0.1989} &\multicolumn{2}{c|}{ \ding{55}} \\ 
\hline
\textbf{Coding Time (ms)} &\multicolumn{2}{c|}{ $6.691$} &\multicolumn{1}{c|}{ $2.613$} & \multicolumn{2}{c|}{\ding{55}} \\ 
\hline
\end{tabular}
\label{table_para}
\end{table*}


%\subsection{Implementations of the Act2Comm}
%\textcolor{black}{As shown in Fig. \ref{Act_fig_implement}, we illustrate the implementation of the proposed \textit{Act2Comm} scheme. The communication messages in the proposed method are discrete. In classic digital communication systems, source coding and channel coding are two key components. Source coding handles data compression; when messages are continuous, the source encoder quantizes them into discrete messages. Channel coding then maps these discrete messages into a sequence of channel inputs. In this paper, we frame the problem of communication via actions as a joint control and channel coding problem. Therefore, we assume that the message set is finite, with each message to be transmitted randomly sampled from this set. Essentially, we assume that the messages are already well-compressed.}
    
%\textcolor{black}{As examples for the Act2Comm scheme with coding rate $1/2$, for a message set $\mathcal{M}=\{0,1,2,3,4,5,6,7\}$, we can use $3$ bits to communicate each message, where message can be represented into $\{000,001,010,011,100,101,110,111\}$. For an image, with dimension $32\times 32\times 3 $, after adopting a compression algorithm such as BPG, we can compress this image into $100$ bits. Then we can communicate this image through the proposed method with $200$ actions.}

\subsection{\textcolor{black}{Complexity analysis}}
\label{app_complex}
\textcolor{black}{To analyze the complexity of the proposed method, we consider an input size of $12$ bits, $4$ blocks, and a rate of $R=1/3$. The experimental results, presented in Table \ref{table_para}, were obtained using a single GPU-A5000 with $10,000$ runs for the "Erratic Robot" environment.}

\textcolor{black}{From the table, we observe that during the training process, the main computational complexity arises from the critic network, as it needs to run around $20$ iterations for each encoder update. Note that the back-propagation FLOPs can be estimated by multiplying the forward FLOPs with a factor (typically 2–3x).} \textcolor{black}{During inference, the critic network is removed. We observe that the encoding process can be completed within $10$ ms, while the decoding process is even faster, taking less than $3$ ms for the message.}

\textcolor{black}{We also analyze the complexity of our approach concerning increasing state and action spaces. Specifically, based on the design of Act2Comm, the number of actions does not impact the scaling performance of the model, in terms of the computational complexity. A larger action space primarily contributes to more diversity in decision rules, without increasing the computational complexity of the approach. For increased state spaces, the complexity grows only in the encoder component. This occurs because the computational complexity of the encoder increases as the output matrix size expands with the number of states.}

\textcolor{black}{However, environments with more complex action or state spaces may lead to a more challenging learning process, potentially affecting performance. To validate our method in such scenarios, we introduced a new environment, the ``Erratic Robot," which features a five-action space. Results demonstrate that our method remains effective, even in this more complex setting.}
}

{\color{black}
\subsection{Vector Representation of States and Actions}
\label{ab_state_vector}
This paper focuses on MDPs with finite state and action spaces, which allows us to represent states and actions as scalar integers by indexing them. This means that even when states and actions are vectors, they can be mapped to scalars. We adopt this approach in the proposed Act2Comm for ease of presentation. However, it is worth noting that using vector representations for states and actions can be beneficial in MDPs. % with inherently vectorized states and actions. 
This can be implemented with minor adjustments to the feedback structure for the encoder and the input structure for the decoder. 

Specifically, with vector representations, the feedback vector is defined as: $\bm{c_t^{(\tau)}}\triangleq[\bm{s_t^{(\tau)}},\bm{x_t^{(\tau)}},\bm{s_{t+1}^{(\tau)}}] \in \mathbb{R}^{1 \times (2n_s+n_x)}$, where $n_s$ and $n_x$ are the dimensionalities of state and action. Here, $\bm{s_t^{(\tau)}}\in\mathcal{S}^{1\times n_s}$ and $\bm{s_{t+1}^{(\tau)}}\in\mathcal{S}^{1\times n_s}$ are the vector representations of states, rather than their scalar counterparts (i.e., indices); similarly, $\bm{x_t^{(\tau)}}\in\mathcal{X}^{1\times n_x}$ is the vector representation of the action. Accordingly, the feedback matrix is defined as $\bm{C_\tau}=[\bm{c_1^{(\tau)}};\ldots;\bm{c_{\frac{\mu}{R}}^{(\tau)}}] \in \mathbb{R}^{\frac{\mu}{R}\times (2n_s+n_x)}$, and the feedback block can be constructed as $\bm{C^{(\tau)}}\triangleq[\bm{C_1};\ldots;\bm{C_\tau}]\in\mathbb{R}^{\tau\times \frac{\mu}{R}\times (2n_s+n_x)}$. Consequently, state observations $\bm{s}\triangleq[s_1;\ldots;s_{\frac{k}{R}}] \in \mathcal{S}^{\frac{k}{R}\times n_s}$ are fed into the decoder to produce logits.

To evaluate the effectiveness of \textit{Act2Comm} with vector representations, we applied this variant to the ``Catch the Ball'' scenario with $R=1/3$ (cf. Fig. \ref{fig:ball-p1}). In this environment, the state is represented as a vector $[s_a,s_b]$, where $s_a\in\{0,1,2\}$ indicates the position of the board, and $s_b\in\{0,1,2,3,4,5,6,7,8\}$ indicates the position of the ball. The performance of Act2Comm with vector representations in this experiment is very similar to that achieved with scalar state representations, as shown in Fig. \ref{fig:ball-perfect}. Notably,  we observe that the vector representation offers advantages such as more stable and faster convergence during training. This is illustrated in Fig. \ref{app_loss_vector}, where the loss value decreases and converges more quickly during training compared to the scalar state representation counterpart. 
An intuitive explanation for this result is that the vector state representation captures more detailed features of the environment, which may potentially facilitate learning.

%This may be because, in this specific environment, the vector representation help decouple action-related information ($s_a$) from environmental information ($s_b$), facilitating a more efficient training for the encoded policy. This also inspires us that, in some complex environments, carefully designed vectorized state representations can potentially simplify the learning process and facilitate an efficient search for the encoded policy.}

\section{Experimental Details}\label{Exp_environment}
\begin{comment}
    \subsection{Pure commmunication}
\subsection{Varying coding length for FSC}
We provide some ablation study over the performance of FSC. From the Table. \ref{table:ap_varying_k}, we can observe the proposed method is valid across various message length.

   \begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Coding rate} & $k=12$ & $k=24$ &$k=36$\\
    \hline
       {$R=1/2$} &$1.16e^{-2}$ &{$1.06e^{-2}$}&{$9.15e^{-3}$}\\
    \hline
       {$R=1/3$} &$1.06e^{-3}$ &$1.02e^{-3}$&$1.14e^{-3}$\\
     \hline
   {$R=1/5$} &$2.84e^{-5}$ &$1.62e^{-5}$&\textcolor{black}{$2.06e^{-5}$}\\
     \hline
    \end{tabular}
    \caption{{Communication performance for \textit{Act2Comm} across different message length and coding rate, where $\mu=3$.}}
    \label{table:ap_varying_k}
\end{table}
\end{comment}

\subsection{Experiment setup}
For the \textit{Act2Comm} scheme, we train the model with a batch size of $4096$, a learning rate of $0.001$, and an Adam-based lookahead optimizer \citep{zhang2019lookahead}. The inner-training step for the critic network is $s_{in}=20$, with a noise variance of $\sigma_w^2=0.1$. Each block has a length of $\mu=3$, and temperature parameter is as $\gamma=10$, $\gamma=50$, $\gamma=100$, $\gamma=200$. The performance presented is averaged over $20,000$ execution times. To investigate the trade-offs, we train the \textit{Act2Comm} model with $\lambda \in [0.01,20]$.

%\begin{wrapfigure}{r}{0.45\textwidth}
%    \centering
%    \includegraphics[width=0.43\textwidth]{image/Fig_architecture.pdf}
%    \caption{The architecture of the \textit{Act2Comm}.}
%        \label{Act_fig_architecture}
% \end{wrapfigure}
 
The detailed architecture of the \textit{Act2Comm} scheme is provided in Fig. \ref{Act_fig_architecture}. At the transmitter, a transformer-based encoder is utilized to generate the $\bm{z^{(\tau)}}$ from $\bm{B^{(\tau)}}$ and $\bm{C^{(\tau)}}$ for each coding round. Specifically, $\bm{C}^{(\tau)}$ and $\bm{B^{(\tau)}}$ are firstly processed through multilayer perceptron (MLP) layers, then concatenated for linear projection and positional encoding operations, resulting in $\bm{F_{0}}\in \mathbb{R}^{\tau\times d}$, where $d$ is the hidden layer dimension. After $L_t$ transformer layers, the resultant $\bm{F_{L_t}}\in\mathbb{R}^{\tau\times d}$ is further transformed by a \textcolor{black}{fully-connected layer} into $\bm{Z}^{(\tau)}$. The decoding process begins with reshaping $\bm{S}$ into $\mathcal{S}^{\frac{k}{\mu}\times \frac{\mu}{R}}$. This reshaped $\bm{S}$ is then processed symmetrically through \textcolor{black}{fully-connected layers}, positional encoding, and $L_r$ transformer layers, yielding the feature $\bm{D_{L_r}}\in \mathbb{R}^{\frac{k}{\mu}\times d}$. Subsequently, $\bm{D_{L_r}}$ is input into a \textcolor{black}{fully-connected layer} to generate the logits $\bm{\hat{M}}\in\mathbb{R}^{l\times 2^\mu}$ for each block. After a softmax function, we predict the label of $\bm{m}$ and transform it into bit stream $\bm{\hat{m}}$. Specifically, we set  $d=32$, $L_t=2$ and $L_t=4$ during the experiments.

 \begin{figure}[t]
    \centering
    %\hfill
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/loss_all_flaten.png}
        \caption{}         
        \label{app_loss_vector}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/Fig_architecture.pdf}
        \caption{}
        \label{Act_fig_architecture}
    \end{subfigure}
    \caption{(a). Training process comparison for scalar state and vector state representations, where $\lambda=20$, $\gamma=50$ and $R=1/3$. (b). The architecture of the \textit{Act2Comm}. }
    \label{fig:appendix}
 \end{figure}

 
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{image/wheel.pdf}
        \caption{Lucky wheel. }
        \label{fig:wheel}
    \end{subfigure}
   % \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{image/ball.pdf}
        \caption{Catch the Ball. }
        \label{fig:ball}
    \end{subfigure}
     \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{image/Environment_3.png}
        \caption{Erratic robot.}
        \label{fig:robot}
    \end{subfigure}
    \caption{\textcolor{black}{Illustrations of experimental MDP environments}}
    \label{fig:env}
 \end{figure}


% We train the model with a batch size of $4096$, learning rate of $0.001$, and an Adam lookahead optimizer \citep{zhang2019lookahead}. The inner-training step for the critic network is $s=20$, with a noise variance of $\sigma_w^2=10$. Each block has a length of $\mu=3$, and the temperature parameter is $T=100$. %The performance plotted are averaged over $20,000$ independent runs.

\subsection{Lucky Wheel}
As illustrated in Fig. \ref{fig:wheel}, the wheel is evenly divided into three regions. At each time step, the player can select either action $0$ or action $1$, where action $0$ corresponds to a clockwise rotation of the wheel, and action $1$ corresponds to an anti-clockwise rotation. If action $0$ is selected, there is a probability $p = 0.2$ that the pointer remains in its current region, and a probability $1 - p = 0.8$ that it moves to the next region in the clockwise direction. Similarly, if action $1$ is selected, there is a probability $p = 0.2$ that the pointer remains in its current region, and a probability $1 - p = 0.8$ that it moves to the next region in the anti-clockwise direction. The rewards for the three regions are $5$, $-5$, and $0$, respectively. The player receives a reward at each time step based on the region in which the pointer is located.

%\paragraph{Control and communication trade-off.}We can observe that..a trade-off is in (a).(b) shows for a given reward constraints, the performance of coding rate vs ber. (c) Given a ber requirement, whats the best reward, we can achieve.
%\textcolor{black}{We first assess the communication performance of the \textit{Act2Comm} scheme by setting $\lambda=0$, which indicates that the scheme is optimized exclusively for the communication objective, as Eqn. \ref{eq_p2}. This setup enables us to determine an upper bound on the communication performance of \textit{Act2Comm}. It is important to note that channel coding for non-differentiable FSC is extremely challenging, and no practical benchmark scheme is currently available for comparison.}

%\textcolor{black}{Specifically, we examine \textit{Act2Comm} scheme with both history-dependent codes and Markov codes across various coding rates $R$, which correspond to different time horizons. The experimental results are depicted in Fig.~\ref{Performance_BER_rate}. It is evident that the BER performance improves significantly as the channel coding rate decreases. Specifically, with a coding rate $R=1/6$, the BER reaches $10^{-6}$ level, indicating a significant enhancement in performance due to the increased number of channel uses. Comparing history-dependent codes with Markov codes reveals that incorporating feedback blocks can yield substantial performance improvements. Although, from a theoretical perspective, channel feedback does not enhance the capacity of a memoryless channel, the observed benefits may be attributed to the attention mechanism applied to historical states and their corresponding actions. This mechanism effectively utilizes prior coding experience to simplify the subsequent coding process, similar to \cite{10500305} and \cite{9960791}. }

%\paragraph{Rate-reward Trade-off}
%\textcolor{black}{To examine the performance trade-offs, we define a target policy $\bm{P^{*}} = [0.7, 0.3; 0.3, 0.7; 0.7, 0.3]$ and train the \textit{Act2Comm} model with $\lambda \in [0.01,10]$ 
%$\lambda \in {0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10}$
%to explore various trade-offs within the proposed scheme. The experimental results, shown in Fig. \ref{Performance_BER_reward}, display the curve representing the lower envelope of all possible BER-reward trade-off outcomes. This curve illustrates that the region above it is achievable by our integrated control and communication systems, indicating that agents can attain a specific level of communication performance while maintaining the corresponding reward defined by the target policy.}

\subsection{Catch the Ball}
The “Catch the Ball” game is set in a $3 \times 3$ grid, as illustrated in Fig. \ref{fig:ball}. A ball randomly appears at the top of the grid and descends one grid space at each time step. Meanwhile, a board at the bottom moves horizontally to catch the falling ball. Each time the board successfully catches a ball, the player receives a reward $r$. If the ball falls to the bottom of the grid without being caught, it disappears, resulting in a penalty of $-r$ for the player. At all other times, there are no rewards or penalties for the player. After a ball disappears or is caught, a new ball appears randomly (with equal probability) at one of the three positions at the top of the grid.

The player has three available actions to move the board: move left, move right, or remain stationary. If the player chooses to move left or right, there is a probability $p \in [0, 1)$ that the movement fails (in which case the board remains in its current position), and a probability $1 - p$ that the board moves for one grid space successfully as intended. Additionally, any action that attempts to move the board outside the grid will always fail. In this experiment, we set $r=5$ and $p=0.8$.

\subsection{\textcolor{black}{Erratic robot.}}
\label{app_robot_env}
\begin{figure}[t]
\begin{subfigure}{0.33\linewidth}
    \centering
    \includegraphics[width=0.98\linewidth]{image/lake_main_BER_reward_rate.png}
    \caption{BER v.s. Reward}
    \label{fig:re_robot-1}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.98\linewidth]{image/lake_fixed_BER_reward_rate.pdf}
    %\caption{\small{BER vs rate for a given reward}}
     \caption{Reward v.s. Rate}
    \label{fig:re_robot-2}
\end{subfigure} 
\begin{subfigure}{0.33\linewidth}
\centering
    \includegraphics[width=0.98\linewidth]{image/lake_game_fixed_reward_BER_rate.png}
    %\caption{\small{Reward vs rate for a given BER}}
         \caption{BER v.s. Rate}
    \label{fig:re_robot-3}
\end{subfigure} 
    \caption{\textcolor{black}{Control-communication trade-off of \textit{Act2Comm} in ``Erratic Robot''.}}
    \label{fig:re_robot}
\end{figure}

\textcolor{black}{The `Erratic robot' game takes place on a $4 \times 4$ grid map with $16$ states and $5$ actions, as shown in Fig. \ref{fig:robot}. The robot is designed to minimize the number of steps required to collect goods while avoiding obstacle points. Specifically, its primary objective is to continuously take goods from designated destination points, earning a reward of $+5$ for each successful collection. The grid also includes four obstacle points, each incurring a penalty of $-2$ when encountered. The robot has five available actions: move left, move right, move up, move down, or remain stationary. Due to the instability, any movement of the robot has a probability $p \in [0, 1)$ of resulting in an additional unintended step. Actions that would move the robot outside the grid boundaries always fail. In this experiment, the probability of additional operation is set to $p=0.2$. The results are presented in the Fig. \ref{fig:re_robot}}

\textcolor{black}{As shown in the Fig. \ref{fig:re_robot}, the proposed \textit{Act2cComm} scheme can achieve perfect communication with $R=1/5$, albeit with a reduction in the average reward from the optimal value of $0.753$ to $0.45$. If we relax BER, the same rate can be achieved with a better reward. Similarly, for different reward targets, we can obtain different communication performances. This additional experiment also highlights the versatility of the proposed \textit{Act2Comm} scheme across various environments. }

\begin{comment}
    \begin{figure}[t]
    \centering
    
    % First minipage: Figure with two subfigures (side by side)
    \begin{minipage}{0.56\textwidth}
        \centering
        \begin{subfigure}{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{image/K_12_BER_reward.pdf}  % Replace with your image
           % \caption{}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.5\textwidth}
            \centering
            \includegraphics[width=\textwidth]{image/Loss_ber.pdf}  % Replace with your image
            %\caption{}
        \end{subfigure}
        \caption{\footnotesize{Left figure is the trade-off for the a target policy. Right figure is the training process.}}
    \end{minipage}
    \hfill
    % Second minipage: Another figure with a table-like caption
    \begin{minipage}{0.39\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{image/Ab_FSC_table.png}  % Replace with your image
        \captionsetup{type=table}
\captionof{table}{\footnotesize{Ablation experiments over different mechanisms.}}    \end{minipage}

\end{figure}
\end{comment}
\begin{figure}[t]
    \centering
    
    % First minipage: Figure with two subfigures (side by side)
    \begin{minipage}{0.4\textwidth}
        \centering
            \includegraphics[width=\textwidth]{image/Loss_ber.pdf}  
        \caption{\small{Loss during the training process, where the loss value decreases significantly with each decoder update, while showing a slight increase with each encoder update.}}
        \label{loss}
    \end{minipage}
    \hfill
    % Second minipage: Another figure with a table-like caption
    \begin{minipage}{0.55\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{image/Ab_FSC_table.png}  % Replace with your image
        \captionsetup{type=table}
\captionof{table}{\small{Ablation study when sequentially removing one improvement after another. Note higher BER means worse performance for a given reward here with $R=1/4$.}}  \label{abla}  \end{minipage}
\end{figure}

\section{Additional ablation experiments}
\label{architecture}
This section presents additional ablation experiments for our proposed \textit{Act2Comm} framework, using the lucky wheel environment as the default experimental setting with an initial state $0$.

\subsection{Ablation study over different mechanisms}
To showcase the efficiency of our approach, we present the loss values monitored throughout the iterative training process in Fig. \ref{loss}. While the loss value occasionally fluctuates during the training updates—primarily due to the alternating update scheme between the decoder and encoder—overall, the model exhibits a rapid and consistent convergence.

Additionally, to further validate the performance of our Critic network, we plot the estimated loss values generated by the Critic alongside the true loss during training. As shown in Fig. \ref{loss}, the Critic network maintains accurate loss estimation throughout the training phase.

We present a detailed comparative analysis of each mechanism within our method, as shown in Table \ref{abla}, emphasizing the specific contributions of each component to the cumulative performance improvements. Notably, utilizing the Critic network to predict the loss directly, or eliminating policy noise during inner-step training can yield failure, underscoring the critical importance of appropriate policy noise and neighboring sampling mechanisms.

all these design elements collectively ensure that our proposed solution consistently delivers robust performance across a wide range of scenarios.

%We additionally assess the communication performance of the \textit{Act2Comm} scheme by setting $\lambda=0$, which indicates that the scheme is optimized exclusively for the communication objective. This setup enables us to determine an upper bound on the communication performance of \textit{Act2Comm}. It is important to note that channel coding for FSC is extremely challenging, and no practical benchmark scheme is currently available for comparison.
\subsection{Ablation study over the feedback design}
\textit{Act2Comm} supports both history-dependent coding, which encodes both message and feedback blocks, and Markov coding, which encodes only the message block. We examine \textit{Act2Comm} scheme with both History-dependent codes and Markov codes across various coding rates $R$ for a pure communication optimization. 

The experimental results are depicted in {Fig.~\ref{rate_fb}.} It is evident that the BER performance improves significantly as the channel coding rate decreases. Specifically, with a coding rate $R=1/6$, the BER reaches $10^{-6}$ level, indicating a significant enhancement in performance due to the increased number of channel uses. Comparing history-dependent codes with Markov codes reveals that incorporating feedback blocks can yield substantial performance improvements. Although, from a theoretical perspective, channel feedback does not enhance the capacity of a memoryless channel, the observed benefits may be attributed to the attention mechanism applied to historical states and their corresponding actions. This mechanism effectively utilizes prior coding experience to simplify the subsequent coding process. %similar to \cite{10500305,9960791}. 

%\textit{Act2Comm} supports both history-dependent coding, which encodes both message and feedback blocks, and Markov coding, which encodes only the message block.

\begin{figure}[t]
    \centering
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/K_12_BER_rate.pdf}
        \caption{}         
        \label{rate_fb}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.47\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{image/K_12_BER_reward.pdf}
        \caption{}         
        \label{sub_op}
    \end{subfigure}
    
    \caption{Left: Performance for different coding scheme. Right: Approaching a given policy $\bm{\bar{\pi}}$.}
    \label{fig:details}
 \end{figure}
 
\subsection{Ablation study over the message length}
Additional experiments with \textit{Act2Comm} are conducted across different message lengths and coding rates, as presented in Table. \ref{table:ap_varying_k}. The results demonstrate that \textit{Act2Comm} scheme consistently delivers competitive performance over different message lengths and coding rates. An interesting observation is that communication performance generally improves with longer block lengths; however, it eventually declines due to the increased learning complexity. Notably, for longer message lengths $k$, adapting a larger block size $\mu$ can potentially enhance performance while addressing the increased learning complexity. 

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Coding rate} & $k=12$ & $k=24$ &$k=36$\\
    \hline
       {$R=1/2$} &$1.16e^{-2}$ &{$1.06e^{-2}$}&{$9.15e^{-3}$}\\
    \hline
       {$R=1/3$} &$1.06e^{-3}$ &$1.02e^{-3}$&$1.14e^{-3}$\\
     \hline
   {$R=1/5$} &$2.84e^{-5}$ &$1.62e^{-5}$&\textcolor{black}{$2.06e^{-5}$}\\
     \hline
    \end{tabular}
    \caption{BER for \textit{Act2Comm} across different message length and coding rate, where $\mu=3$.}
    \label{table:ap_varying_k}
\end{table}
%For detailed policy shaping and optimization:
%1/3, 1/3, 1/3, we ignore the constraints....
%For 1/2 1/2, 0; we only calculate last row....
\subsection{Ablation study over a specific policy}
In previous part of this paper, we focused solely on the optimal strategy as the target policy for training, aiming to optimize the control objective. However, our approach is versatile and can accommodate a broad spectrum of policies—ranging from optimal to specialized strategies—enabling users to employ the system to approach specific policies for their own tasks. 

In the lucky wheel environment, now we consider a target policy $\bm{\bar{\pi}}$ given as:
\begin{equation}
    \bm{\bar{\pi}}=\begin{bmatrix} \pi(x=0|s=0)&\pi(x=1|s=0)\\
    \pi(x=0|s=1)&\pi(x=1|s=1)\\
    \pi(x=0|s=2)&\pi(x=1|s=2)
    \end{bmatrix}=\begin{bmatrix}0.7 &0.3\\
    0.3 &0.7\\
    0.7 &0.3
    \end{bmatrix}.
    \label{target_policy}
\end{equation}

We evaluate the \textit{Act2Comm} scheme with different coding rates and consider a single message transmission with an accumulated reward. The experimental results in Fig. \ref{sub_op} display the curve representing the lower envelope of all possible BER-reward trade-off outcomes for each coding rate. This curve illustrates that the region above it is achievable by our \textit{Act2Comm} scheme. 

Notably, our method can achieve good communication performance, approximately $4\cdot10^{-1}$, $4\cdot10^{-2}$, and $3\cdot10^{-3}$ for $R=1/3$, $R=1/4$, and $R=1/5$ respectively, with almost no loss in control performance. This is due to the stochastic nature of our target policy, as opposed to a deterministic one, allowing our method to adaptively learn a similar policy that maintains comparable rewards while being more favorable for channel coding. In summary, the experimental results demonstrate that our \textit{Act2Comm} framework achieves satisfactory communication performance while maintaining the reward, as defined by a specific policy, at a certain level in scenarios such as those involving stochastic policies.

\end{document}

