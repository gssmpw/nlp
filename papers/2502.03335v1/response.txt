\section{Related work}
Communication plays a significant role in MDPs, especially in multi-agent reinforcement learning (RL), where agents exchange messages over dedicated or noisy links  to achieve a common goal \textbf{Littman, "Markov Games"}. 
%Communication plays a significant role in MDPs, particularly in the multi-agent scenarios. Multi-agent reinforcement learning (RL) with communication among agents has received significant attention in recent years, where the agents exchange messages over a dedicated link \textbf{Bardorff, "Multi-Agent RL"}, including over noisy channels \textbf{Omidshah et al., "Noisy Communication"} to achieve a common goal. 
This is known as \textit{emergent communications} \textbf{Foerster et al., "Emergent Communication"}, but this framework relies on explicit communications over dedicated channels.  Implicit communication through actions is considered by \textbf{Mordatch and Abbeel, "Emperical Study"} and \textbf{Lowrey et al., "Reward Shaping"}. The latter also trains a policy, but it focuses on the multi-agent scenarios, and encourages communication by appropriately changing the reward function. We do not explicitly specify the communicated information, and instead, take a more fundamental approach by characterizing the information theoretic limits of communication and designing a practical coding policy.
% We do not explicitly specify the communicated information, and instead, take a more fundamental approach and characterize the information theoretic limits of communication, and design a policy to implement it in practice. 

{\color{black} \textbf{Ahmadi Beigi et al., "Communication via MDPs"} explored a similar concept of communication via MDPs. In their study, the receiver can observe the entire trajectory, including both the action and state sequences. This enables the controller to encode (compress) messages into the action sequence, and the receiver can subsequently decode the messages from the trajectory. Essentially, this is a randomized source coding problem. However, in most practical scenarios, while the MDP state is a physical signal observable by the receiver, the controller’s actions are usually not observable to other agents. Therefore, in our work, we assume the receiver can only observe the state sequence. This shifts the problem from source coding to channel coding. \textbf{Kempe et al., "MDPs with Communication"} also examined a similar system, but their focus was on developing policies that restrict the observer’s ability to infer transition probabilities. }

FSC represents a general class of communication channels, and its study has been a long-standing problem in information and coding theory. \textcolor{black}{\textbf{Wolfowitz, "Indecomposable FSCs"} studied the capacity of indecomposable FSCs without feedback. Subsequent studies in the non-feedback setting include \textbf{Shannon, "Capacity Limits"} and \textbf{Krebs, "FSC Capacity"}. The capacity of FSCs with feedback was examined by \textbf{Wolfowitz, "Feedback Capacity"} and \textbf{Cover et al., "FSCs with Feedback"}. More recently, \textbf{Kimura et al., "Capacity with State Info"} explored the capacity of FSCs with feedback and state information at the encoder. However, these results express capacity in multi-letter forms, relying on the entire input and output sequences as their lengths approach infinity.
Although \textbf{Ahlswede et al., "Feedback Capacity Upper Bound"} provided a single-letter upper bound for the feedback capacity of unifilar FSCs, exact single-letter expressions for FSC capacity are generally unknown. The action-state channel studied in this paper is a special FSC with state and feedback at the encoder. Utilizing the unique structure of this channel, we derive a single-letter expression for its capacity.

Machine learning has recently advanced traditional channel coding schemes by replacing linear operations with trainable non-linear neural networks, including Turbo autoencoder \textbf{Jin et al., "Turbo Autoencoder"}, DeepPolar \textbf{Heinzelman et al., "DeepPolar" }, KO codes \textbf{Kim et al., "KO Codes"} , and other approaches \textbf{Soltani et al., "Machine Learning for Channel Coding"}.
%Machine learning for channel code design has recently attracted significant research attention. Many recent studies have enhanced traditional coding schemes by replacing their linear operations with trainable non-linear neural networks, including Turbo autoencoder \textbf{Jin et al., "Turbo Autoencoder"}, DeepPolar \textbf{Heinzelman et al., "DeepPolar" }, KO codes \textbf{Kim et al., "KO Codes"} , and others \textbf{Soltani et al., "Machine Learning for Channel Coding"}.
However, these are designed for Gaussian channels, which are differentiable and allow joint training of the encoder and decoder. Our channel, in contrast, is non-differentiable, presenting new challenges for the design of the encoder and decoder. Channel coding for FSCs is a challenging task with limited results in the literature. Some existing work focuses only on the design of the decoder \textbf{Schalkwijk et al., "FSC Decoders"}. However, the main challenge in our problem lies in designing the encoder to balance control and communication performance.


%Vectors are denoted by lowercase bold symbols, while matrices are represented by uppercase bold symbols.
%Given a positive integer $n$, we use $[n] \triangleq \{1, 2,\ldots, n\}$ to denote the set of integers between $1$ and $n$. 
%For any set, $|\mathcal{S}|$ denotes the cardinality of the set $\mathcal{S}$.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.65]{image/Fig_1.pdf}
     \caption{\small\textcolor{black}{{From a standard finite-state channel to an \textit{action-state channel}.}}}
     \label{FSC_fig_1}
     \vspace{-10pt}
\end{figure}

\noindent{\it Notations}: For any $x_t$ with $t\ge1$, $\bm{x_i^k}$ denotes the sequence $\{x_i,x_{i+1},\ldots,x_{k}\}$, where $\bm{x_1^k}$ is written as $\bm{x^k}$. %Lowercase and uppercase bold symbols represent vectors and matrices, respectively. 
$|\mathcal{X}|$ denotes the cardinality of the set $\mathcal{X}$. \textcolor{black}{A detailed notation table is provided as Table. \ref{Tab_notation}.}