\section{Related work}
Communication plays a significant role in MDPs, especially in multi-agent reinforcement learning (RL), where agents exchange messages over dedicated or noisy links  to achieve a common goal \citep{wang_learning_2020,chen2024rgmcomm,tung_effective_2021}. 
%Communication plays a significant role in MDPs, particularly in the multi-agent scenarios. Multi-agent reinforcement learning (RL) with communication among agents has received significant attention in recent years, where the agents exchange messages over a dedicated link \citep{foerster_learning_2016, wang_learning_2020,chen2024rgmcomm}, including over noisy channels \citep{tung_effective_2021} to achieve a common goal. 
This is known as \textit{emergent communications} \citep{boldt2024a}, but this framework relies on explicit communications over dedicated channels.  Implicit communication through actions is considered by \cite{Knepper:HRI:17} and \cite{tian2019learningcommunicateimplicitlyactions}. The latter also trains a policy, but it focuses on the multi-agent scenarios, and encourages communication by appropriately changing the reward function. We do not explicitly specify the communicated information, and instead, take a more fundamental approach by characterizing the information theoretic limits of communication and designing a practical coding policy.
% We do not explicitly specify the communicated information, and instead, take a more fundamental approach and characterize the information theoretic limits of communication, and design a policy to implement it in practice. 

{\color{black} \cite{sokota2022communicating} explored a similar concept of communication via MDPs. In their study, the receiver can observe the entire trajectory, including both the action and state sequences. This enables the controller to encode (compress) messages into the action sequence, and the receiver can subsequently decode the messages from the trajectory. Essentially, this is a randomized source coding problem. However, in most practical scenarios, while the MDP state is a physical signal observable by the receiver, the controller’s actions are usually not observable to other agents. Therefore, in our work, we assume the receiver can only observe the state sequence. This shifts the problem from source coding to channel coding. \cite{Karabay2019} also examined a similar system, but their focus was on developing policies that restrict the observer’s ability to infer transition probabilities. }

FSC represents a general class of communication channels, and its study has been a long-standing problem in information and coding theory. \textcolor{black}{\cite{blackwell1958proof}} studied the capacity of indecomposable FSCs without feedback. Subsequent studies in the non-feedback setting include \cite{Verdu1994} and \cite{Goldsmith1996}. The capacity of FSCs with feedback was examined by \cite{massey1990causality} and \cite{haim2007}. More recently, \cite{shemuel2022finite} explored the capacity of FSCs with feedback and state information at the encoder. However, these results express capacity in multi-letter forms, relying on the entire input and output sequences as their lengths approach infinity.
Although \cite{haim2017} provided a single-letter upper bound for the feedback capacity of unifilar FSCs, exact single-letter expressions for FSC capacity are generally unknown. The action-state channel studied in this paper is a special FSC with state and feedback at the encoder. Utilizing the unique structure of this channel, we derive a single-letter expression for its capacity.

Machine learning has recently advanced traditional channel coding schemes by replacing linear operations with trainable non-linear neural networks, including Turbo autoencoder \citep{jiang2019turbo}, DeepPolar \citep{hebbar2024deeppolar}, KO codes \citep{makkuva2021ko}, and other approaches \citep{jiang2020learn,kim2018deepcode}.
%Machine learning for channel code design has recently attracted significant research attention. Many recent studies have enhanced traditional coding schemes by replacing their linear operations with trainable non-linear neural networks, including Turbo autoencoder \citep{jiang2019turbo}, DeepPolar \citep{hebbar2024deeppolar}, KO codes \citep{makkuva2021ko}, and others \citep{jiang2020learn,kim2018deepcode}.
However, these are designed for Gaussian channels, which are differentiable and allow joint training of the encoder and decoder. Our channel, in contrast, is non-differentiable, presenting new challenges for the design of the encoder and decoder. Channel coding for FSCs is a challenging task with limited results in the literature. Some existing work focuses only on the design of the decoder \citep{aharoni2023data}. However, the main challenge in our problem lies in designing the encoder to balance control and communication performance.


%Vectors are denoted by lowercase bold symbols, while matrices are represented by uppercase bold symbols.
%Given a positive integer $n$, we use $[n] \triangleq \{1, 2,\ldots, n\}$ to denote the set of integers between $1$ and $n$. 
%For any set, $|\mathcal{S}|$ denotes the cardinality of the set $\mathcal{S}$.
\begin{figure}[t]
    \centering
    \includegraphics[scale=0.65]{image/Fig_1.pdf}
     \caption{\small\textcolor{black}{{From a standard finite-state channel to an \textit{action-state channel}.}}}
     \label{FSC_fig_1}
     \vspace{-10pt}
\end{figure}

\noindent{\it Notations}: For any $x_t$ with $t\ge1$, $\bm{x_i^k}$ denotes the sequence $\{x_i,x_{i+1},\ldots,x_{k}\}$, where $\bm{x_1^k}$ is written as $\bm{x^k}$. %Lowercase and uppercase bold symbols represent vectors and matrices, respectively. 
$|\mathcal{X}|$ denotes the cardinality of the set $\mathcal{X}$. \textcolor{black}{A detailed notation table is provided as Table. \ref{Tab_notation}.}