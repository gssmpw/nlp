\section{Introduction}

Creating photorealistic human representations with the ability to control viewpoints has numerous applications in entertainment, healthcare, fashion and
social media. 
Building such representations, first and foremost, requires high-quality multi-view studio data~\cite{isik2023humanrf, bagautdinov2021driving}, which is costly to acquire.
This significantly limits the scalability in terms of the number identities for high-quality studio data~\cite{isik2023humanrf, martinez2024codec, xiong2024mvhumannet}.
In contrast, large-scale, unstructured, and diverse human images and videos are available online. 
However, the raw data of such in-the-wild images does not offer ground-truth 3D or multi-view representations of humans. 

\input{fig/teaser_meta_arxiv}

In this work, we present a novel approach leveraging the best of two worlds: 
generalizability from in-the-wild unstructured images, and fidelity and view-controllability from studio capture data. 
Specifically, our model \ourmodel is a diffusion transformer, which can generate several 1K resolution multi-view consistent images jointly during inference.
\ourmodel takes as input a single image of an individual, camera poses of the target viewpoints to be generated. Since the scale and placement of the subject is ambiguous from a single image, \ourmodel uses a Spatial Anchor which roughly specifies the location and orientation of the subject in 3D space.
Our model does not rely on additional conditioning such as body priors or
camera parameters of the input images, to scale our training pipeline to in-the-wild data and support unconstrained inputs at test time.

We employ a multi-stage training recipe to train \ourmodel. First, we pre-train the model for latent-to-image generation task, similar to ~\cite{DALLE-2}, on a large human-centric dataset of in-the-wild images.
Next, in \midtraining stage, we jointly generate multiple consistent images of the subject, conditioned on target viewpoints and a single input image using high-quality studio dataset. 
Finally, in \posttraining stage, the model is provided with a minimal 
placement signal - \textit{spatial anchor} - encoding rough head orientation - 
which further improves 3D consistency.
Our architecture is also carefully tailored to the conditional multi-view generation - we propose several 
simple but effective modifications to the basic DiT architecture, including self-attention-based conditioning,
lightweight spatial controls and camera conditioning with \plucker coordinates.


During inference, our goal is to produce smooth turnaround videos, which requires generating up to five times more views than were present during training. However, we observe that simply increasing the number of views leads to a drop in quality of the generated content. Our investigation reveals that this drop is due to heightened entropy in the attention heads as the number of views increases. To address this problem, inspired by previous research in super-resolution~\cite{jin2023training}, we introduce an attention biasing approach to control and reduce entropy growth within multi-view models.

For evaluating such a multi-view diffusion model, 3D consistency is a critical metric for understanding the geometric correctness of the generation. 
For generation task in an ill-posed setup (e.g., a single image input), there may be multiple possibilities that are equally plausible. 
Existing multi-view generation methods typically report reconstruction metrics (\ie PSNR, SSIM and LPIPS)~\cite{gao2024cat3d} 
or FID~\cite{zero1to3}, which either (1) penalize any new content that is actually 3D consistent or 
(2) are unable to measure the 3D consistency of the generation. 
To address this problem, we design a metric measuring 3D consistency: we compute 2D keypoint matches
with an off-the-shelf method \cite{sarlin2020superglue}, triangulate them, and then reproject back into the other views to
measure the reprojection error in pixels.
Some generation methods measure reprojection errors~\cite{fridman2024scenescape} from SfM~\cite{SFM} or epipolar distance~\cite{muller2024multidiff,TrainNVSDM3}, but our metric uses camera pose as input and is more precise than measuring distance to a epipolar line. We show our metric helps to quantify our results, finding that our method is favorable compared with existing approaches and other baseline methods.


\noindent To summarize, we make the following contributions:
\begin{itemize}
    \item a generative model capable of generating high-resolution and multi-view consistent humans from a single image and its effective training strategy.
    \item a diffusion transformer architecture designed to enhance multi-view generation and viewpoint control.

    \item an attention biasing technique to enable generating >5\texttt{x} more views at inference compared to training.

    \item a novel 3D consistency metric to accurately measure the level of 3D consistency in generative tasks. 
\end{itemize}








