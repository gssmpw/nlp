
\section{DiT Architecture and Training}\label{app_sec:dit}

\heading{Architecture.} We use a Diffusion Transfomer with $28$ DiT + ControlMLP blocks (depth) in \ourmodel operating at $1536$ dimension. The DiT blocks account for $1.3$B parameters and ControlMLP blocks for $248$M parameters respectively. We use 
latent autoencoder which provides $8$x spatial downsampling, and $4$-channel latent space. 
It contains $101$M parameters. We find the placement of the ControlMLP block modulation in DiT to be crucial, specifically, being placed \textit{before} the scale, shift, and gate is important for the control signal to work well. 

\heading{Training Stages.} We pre-train (P1@256) our model on $512$ A100 GPUs for $1$M iterations, with batch size of $24$ per GPU and at \restwo resolution. 1M on base model, and 700K iterations on 512x512 resolution. During pretraining \ourmodel, we use image-only conditioning via 
DinoV2~\cite{oquab2023dinov2} using ViT-L/14 based models to provide weak supervision and alignment for target generations. We mid-train our model on \fullbody and \upperbody datasets at \resone resolution for nearly $100$K steps, and post-train our models with spatially-aligned conditions at \resthree and \resfour for nearly $50$K steps. We pre-train, mid-train, and post-train for roughly 2:1:1 weeks, respectively.

\heading{Optimizer and Hyperparameters.} We use LR of $1e$-4 during all stages and AdamW optimizer with beta values set to $[0.9, 0.98]$ and epsilon to $1e$-6. We use linear warmup for initial 1000 steps starting at $1e$-6. We conduct training at full precision (float32), and run inference at half precision (bfloat16). 



\section{Diffusion Model and Inference Settings}
\label{app_sec:diffusion}



We use classifier-free guidance(CFG)~\cite{ho2022classifierfree} during training, with $20$\% dropout probability on input reference image. We use 50 DDIM steps for sampling. During inference, we find that lower CFG scales between $3-5$ work better at higher resolution and generating viewpoints that are closer to input view, this observation is inline with Stable Video Diffusion~\cite{blattmann2023stable} which proposed to linearly increase CFG scale after the first frame generation. Inference speed with \ourmodel depends on the resolution and number of views to be denoised; we report few combinations in ~\cref{tab:speed}.

\input{tab/speed}

\heading{Attention Biasing.} To compute the entropy (shown in ~\cref{fig:entropy}), we use the first, middle, and last blocks of Pippo; and aggregate over all attention heads, conditional and unconditional inference passes, and DDIM steps. In~\cref{fig:qual_entropy_full}, we show visuals for increasing strength of the scaling growth factor ($\gamma$) when generating 60 views simultaneously at 512x512 resolution. It is evident that using this attention biasing is quite crucial in making diffusion models generalize across many views (long context sequences). Growth factor ($\gamma$) greater than 1.0 helps mitigate the entropy buildup; however, increasing $\gamma$ beyond $1.6$ leads to over-saturation artifacts somewhat akin to ones caused by high CFG scale. 
\input{fig/qual_entropy_full}

\heading{Varying Classifier-free Guidance (CFG) using a bump function.} 
Classifier-free Guidance guidance enables a trade-off between diversity and realism~\cite{ho2022classifierfree}; with higher values of CFG resulting in diverse generations (\ie higher Inception Score) and lower values of CFG leading to phtorealistic generations (\ie higher FID)
In our setup, the single image to multi-view task involves faithfully preserving known content while also hallucinating diverse possibilities of unseen regions. The amount of known content and unknown content varies for each view that is being generated; for example, the back of the heads are often unseen; however, central parts of the face, such as the nose, are generally specified in the frontal input image. 
We can use this information to rescale the CFG weight for each view separately.
Specifically, we find that using a lower weight for regions where content copying is required prevents saturation artifacts, whereas increasing the CFG scale of unseen regions leads to more stable and diverse generations. 
Thus, we increase the CFG linearly starting from the front facing view at $0\degree$ azimuth until $90\degree$ azimuth (side-view) where it reaches its peak value. Then we keep the CFG scale fixed at this peak value until the azimuth reaches $270\degree$ (opposite side-view), and finally decrease it linearly back to starting value at azimuth of $360\degree$. This is a bump function and we find that starting with CFG scale between $[7.0, 9.0]$, and having a peak CFG scale between $[15.0,19.0]$ results in reduced artifacts and diverse generations especially in the unseen regions. A similar trick is also used in image-to-video work of SVD~\cite{blattmann2023stable}. 

{
\heading{Rescaling diffusion timesteps under varying resolution.} Prior works~\cite{chen2023importance, esser2024scaling} suggest that as resolution increases the noise scale has to be shifted to ensure same level of corruption. Based on this Stable Diffusion 3~\cite{esser2024scaling} derive a noise reweighing scheme by assuming degradation to a constant-pixel image and ensuring that the uncertainity under degradation for each pixel stays constant. Since SD3 uses conditional flow-matching objective and we train Pippo using the DDPM~\cite{DDPM} objective, we cannot use their reweighing scheme directly. Here, we provide a derivation for an equivalent reweighing scheme for DDPM objective. We can define forward process as:
   \[
   z_t = \sqrt{\alpha_t} z_0 + \sqrt{1 - \alpha_t} \epsilon,
   \]
where \(\alpha_t\) is a monotonically decreasing function of \(t\). The uncertainty in DDPM is governed by the variance of the forward process, which depends on \(\alpha_t\) and \(1 - \alpha_t\). Consider a constant image \(z_0 = c \mathbbm{1}\), where \(c \in \mathbb{R}\) and \(\mathbbm{1} \in \mathbb{R}^n\). The forward process in DDPM produces:
\[
z_t = \sqrt{\alpha_t} c \mathbbm{1} + \sqrt{1 - \alpha_t} \epsilon,
\]
where \(\epsilon \sim \mathcal{N}(0, I)\). The uncertainty in estimating a constant-valued image \(c\) is, where $n$ are total number of pixels in the image:
\[
\sigma(t, n) = \frac{\sqrt{1 - \alpha_t}}{\sqrt{\alpha_t}} \cdot \frac{1}{\sqrt{n}}.
\]

To map a timestep \(t_n\) at resolution \(n\) to a timestep \(t_m\) at resolution \(m\) such that the uncertainty \(\sigma(t_n, n) = \sigma(t_m, m)\), we solve:
\[
\frac{\sqrt{1 - \alpha_{t_n}}}{\sqrt{\alpha_{t_n}}} \cdot \frac{1}{\sqrt{n}} = \frac{\sqrt{1 - \alpha_{t_m}}}{\sqrt{\alpha_{t_m}}} \cdot \frac{1}{\sqrt{m}}.
\]
Rearranging, we get the resolution-dependent timestep mapping for DDPM isolates \(\alpha_{t_m}\) as:
\[
\alpha_{t_m} = \frac{\alpha_{t_n}}{\frac{m}{n} + \alpha_{t_n} \left(1 - \frac{m}{n}\right)}.
\]
We use the above reweighing to rescale noise steps when training models at a higher resolution of \resthree and \resfour during post-training. In practice, we set $m/n$ ratio to be slightly lower than the actual value following SD3~\cite{esser2024scaling}. 






\section{\ig: Filtering and Stats}\label{app_sec:data_filter}

We run image metadata filtering to keep images whose short edges are at least 720 pixels and file sizes are at least 120 KB.
We run Detectron2~\cite{wu2019detectron2} (pose detection) to keep images containing one clearly detected person (detection score at least 0.9 and the secondary clear person detection score is at most 0.4) with heads at least partially visible, and the long edge of the detected bounding box is at least 300 pixels.
We also use a custom person realism classifier to drop computer-generated or computer-processed imagery.
We provide rough statistics of our curated human-centric dataset in ~\cref{fig:data_stat}. We bucket these attributes in bins along X-axis and plot their respective sizes (normalized between $0-1$) on Y-axis. 

\input{fig/data_stat}







\section{Webpage Visuals}

\heading{Webpage.} We upload a supplementary webpage which contains 360$\degree$ turnaround videos from our model generated for Full-body, Head-only and Face-only settings. Additionally, we put visuals where we provide as input a monocular video (framewise), and generate frames independently at each timestep. We find \ourmodel preserves the known details while hallucinating plausible unseen parts well. We also put the visualization of our spatial anchor and corresponding generations. See \texttt{\url{http://yashkant.github.io/pippo}} for the webpage.
\vspace{5pt}


\input{fig/pretrain}

\section{Why the name Pippo?}
\input{fig/filippo}
Our model is named after Filippo di ser Brunellesco di Lippo Lapi (1377 â€“ 15 April 1446), widely recognized as Filippo Brunelleschi and affectionately known as Pippo by Leon Battista Alberti. 
Brunelleschi was an Italian architect, designer, goldsmith, and sculptor. 
He pioneered the application of vanishing points in artwork to achieve accurate perspective vision. Similarly, our model employs a single 3D spatial anchor to produce consistently improved images. 


\section{Visuals from Pretrained Model (P1  )}\label{app_sec:pretrain}

In ~\cref{fig:pretrain}, we showcase qualitative visuals related to the findings in Table 2. It is evident that the model trained with filtered data and using an image-conditioned objective produces high-quality human figures.

\section{Frequently Asked Questions}

\heading{Ablation with Missing or Inconsistent spatial anchor.} 
Spatial Anchor acts as a placement signal for Pippo since we do not provide it with intrinsics or extrinsics of the input image. In~\cref{fig:anchor_ablation}, we run inference on the \upperbody \pre@512 model with missing spatial anchor, or when it is inconsistent with input head pose (rotated downwards at the floor by 90$\degree$). When the spatial anchor is missing, Pippo often generates an empty image because in our training data it implies that the subject's head is not visible in the generated view. We find that Pippo is robust to anchor rotations; this suggests that \ourmodel relies on the spatial anchor only for placement control and infers head-pose from the input image. 

\begin{figure}[h!]
    \centering    
    \includegraphics[width=\linewidth]{img/anchor_ablation.pdf}
    \caption{\textbf{Ablation with Missing or Inconsistent Spatial Anchors.} We run inference on the \upperbody \pre@512 model with missing spatial anchor, or when it is inconsistent with input head pose rotated downwards towards the floor by 90$\degree$.}
    \label{fig:anchor_ablation}
\end{figure}

\noindent\textbf{Can we reconstruct \ourmodel outputs?} 
Yes, we can reconstruct \ourmodel's generations directly using a NeRF or Gaussian Splatting. 

\heading{Why use spatial-control in post-training only?}
We find that skipping pixel-aligned controls in mid-training helps us to train faster and allows training jointly on a greater number of views. Additionally, since we mid-train at the low resolution of \resone, we find that pixel-aligned control needs to be re-injected during post-training again.

\noindent\textbf{Is Reprojection Error (RE) pairwise, and can it handle occlusions?} Yes, we compute \textit{the mean} Reprojection Error over pairs of images. We divide the generated images randomly into non-overlapping pairs and compute pairwise RE. 
We re-project the triangulated 3D points back to each of the two images to compute the RE. 
We only use high-quality correspondence matches by setting a threshold of $>0.2$ in SuperGlue, and reject image pairs that have fewer than $<5$ matches detected. This filtering helps to avoid spurious correspondences in distant and occluded views. We will release the code for metric.

\heading{Trends in overfitting experiments (in~\cref{ssec:spatial_control}) may change under largescale training.} Empirically, we found that the ability to generalize to novel viewpoints of a single scene under overfitting is correlated with a greater ability to steer the camera viewpoint and place the subject precisely after post-training. For example, in Tab. 4, Row 5 we can see that removing the spatial anchor drops the 3D consistency of the post-trained model the most and is correlated with the overfitting result in Tab. 1, Row 6. The distribution of cameras in overfitting remains similar to that of full training. Thus, we use overfitting as a proxy to compare existing spatial control modules cheaply.



\input{fig/qual_extreme}
