
\section{Experiments}
\label{sec:experiments}

We present details of the datasets used in all training and validation stages, followed by discussion of evaluation metrics -- with particular emphasis on the 3D consistency metric and conclude with core experimental results and ablations.

\subsection{Data} 
\label{ssec:data}

\heading{\ig Dataset.}  %
We utilize a large proprietary dataset of approximately $3$ billion human-centric in-the-wild images for pretraining.
We provide more details on data filtering and curation in ~\cref{app_sec:data_filter}.

\vspace{1mm}
\heading{Head and Full-body Studio Datasets.} We rely on high-quality proprietary studio captures as our primary source of data for learning 3D consistency. 
Our model comes in two variants: head-only and full-body, each trained 
(for \midtraining and \posttraining stages) on corresponding datasets.  For our full-body model, we utilize a dataset of $861$ subjects ($811$ train, $50$ test), nearly $1000$ frames per subject.
For our head-only model, we use a dataset of $1450$ subjects ($1400$ train, $50$ test), nearly $40000$ frames per subject. The studio setup is similar to~\cite{martinez2024codec}, with two capture domes for capturing full-body and head-only high-resolution 4K images with $230$ and $160$ cameras.


\vspace{1mm}
\heading{\mobile Dataset.} To evaluate real-world performance, we collect casual images
of $50$ test subjects in an indoor office environment using an iPhone 13 Pro. We preprocess these images with Sapiens-2B~\cite{khirodkar2025sapiens} for
background segmentation before model inference. This dataset serves exclusively for evaluating our model's performance on in-the-wild inputs.



\subsection{Evaluation Setup and Metrics}

\heading{3D Consistency.} 
Following prior works, we report standard metrics like PSNR, SSIM ($\times 100$), and LPIPS ($\times 100$) metrics. However, these metrics unfairly penalize plausible novel views generated under incomplete inputs. We therefore introduce the \reprojectionerror metric, described in ~\cref{ssec:consistencymetric}, which validates 3D consistency without directly relying on ground truth. Our evaluation generates 4 randomly selected views from the test split.

\vspace{1mm}
\heading{Identity Preservation.} We use two metrics that measure identity preservation across generated views by computing cosine distance between features extracted via FaceNet~\cite{schroff2015facenet, vggface} for face similarity; and CLIP~\cite{radford2021learning} vision encoder for full-body similarity.

\vspace{1mm}
\heading{Pretrained Model Evaluation.} We measure the effectiveness of our pre-training strategy by reporting the 
FID~\cite{FID}. We use a smaller annotated 30M subset of \ig for training image and text-conditioned \pre@128 models, 
a 30M subset of unfiltered dataset for training No Filtering \pre@128, and a 1000 sample test set from \mobile dataset.






\subsection{Results} \label{ssec:results}


\heading{Pretraining and Data Filtering.} \cref{tab:pretrain} presents our pre-trained model results (Row 1) and ablations on Human-centric data filtering and Image-conditioned pretraining (Rows 2-5). Human-centric filtering and image-based conditioning are both critical to achieve high-quality generation.
The qualitative results are shown in \cref{app_sec:pretrain}.
\input{tab/pretrain}







\vspace{1mm}
\heading{High-resolution Multi-view Generation.} In ~\cref{tab:result}, we evaluate 3D reconstruction and identity preservation for unseen subjects from the studio datasets. 
We show that increasing the output resolution of generation in our approach does not hurt 3D consistency or similarity. 
We put corresponding visuals in ~\cref{fig:qual}, Rows 2 and 3.
\input{tab/result}



\vspace{1mm}
\heading{Generations from Casual \mobile Photos.} \noindent We present in ~\cref{tab:result} (Rows 3,6) \reprojectionerror and similarity scores for casually taken images from the \mobile dataset with 1K resolution model. In this scenario, the standard reconstruction error metrics cannot be assessed due to missing ground truth. We find that the reprojection error on \mobile captures remains comparable to the studio dataset -- demonstrating 3D consistency. This illustrates the generalizability of Pippo beyond the multi-view training data domain, where our pretraining with large-scale in-the-wild human data is critical.
We put corresponding visuals in ~\cref{fig:qual}, Row 1.


\input{fig/baseline}

\heading{Comparisons with External Benchmarks.} In ~\cref{fig:baseline}, we compare \ourmodel with individual state-of-the-art baselines in \fullbody and \upperbody generation. SiTH~\cite{ho2024sith} reconstructs textured human-mesh using ControlNet paired with a SDF representation. Compared to SiTH, \ourmodel facilitates high-resolution and accurate multiview synthesis. DiffPortrait3D~\cite{gu2024diffportrait3d} inverts a 3D-GAN based on a given input image. Compared to it, our model supports greater viewpoint variability and ensures closer adherence to the input image.

\heading{Quantitative comparisons and baselines.} Existing SoTA Human methods~\cite{ho2024sith, gu2024diffportrait3d} use explicit SMPL priors, and thus are difficult to compare with directly. Qualitatively, we found that they cannot handle novel views or preserve details (~\cref{fig:baseline}), and hence do not quantitatively compare against them. 
\noindent In Pippo, we focus on creating a strong multi-view human generator, and we benchmark four state-of-the-art multi-view diffusion models on the {iPhone full-body dataset} in~\cref{tab:baseline}. We find that Pippo preserves identity (\ie face and body similarity) and 3D consistency (RE) better while also operating at a higher resolution compared to baselines.

\input{tab/baseline}

\heading{Benchmarking \ourmodel on public datasets.} In~\cref{tab:public}, we benchmark \ourmodel on the public Codec Avatar datasets~\cite{martinez2024codec} to aid future comparisons. Specifically, we use Ava-256 containing 256 head-only captures and Goliath containing 4 full-body. During evaluation, we only use subsets of these datasets that were not used for training. We find that \ourmodel's performance on these datasets is inline with its performance on internal studio datasets. 

\input{tab/public}



\subsection{Ablations}


We examine design choices at each training stage (\cref{sec:method}) and present our ablation results in \cref{tab:ablation}. Experiments are conducted at $128\times128$ resolution on the \upperbody dataset.

\input{tab/ablation}

\heading{Significance of Pre-training and Mid-training.} Pre-training our model on the \ig dataset enables robust generalization to novel identities, as demonstrated in \cref{tab:ablation}, Row 8. Without pre-training, model generalization deteriorates, resulting in unclear facial features. Skipping mid-training at lower resolution impairs consistent multi-view generation, as shown in Row 2.

\heading{Importance of Frontal Input Reference.} Our ablation in \cref{tab:ablation}, Row 10 demonstrates that completely randomizing the view point of an input reference image leads to overfitting to training identities. Non-frontal views, especially rear views, have very limited information about the identity which forces the network to pick up spurious correlations.


\heading{Importance of Self-attention.} Replacing self-attention with cross-attention for reference image encoding, using the same routing as image-conditioned pretraining from \cref{ssec:base_model}, leads to degraded performance as shown in \cref{tab:ablation}, Row 9.
We observe that this setup causes the model to ignore input conditioning, generating images that only vaguely resemble training subjects.

\heading{Role of large scale Humans-3B pre-training dataset.} We utilized intermediate checkpoints from pre-training stage (\pre) trained on 30\% and 70\% of the data, and a separate checkpoint trained on 1\% high-quality subset of Humans-3B. 
Starting from these checkpoints, we mid-trained \ourmodel to denoise 4 views at \resone resolution on \fullbody dataset for two days. We report their respective results in~\cref{tab:dataset_ablation}. 
We found large-scale data is crucial for generalization to novel identities -- indicated by high gains in face similarity metric. 

\input{tab/dataset_ablation}

\input{fig/qual_meta}
\input{fig/qual_incomplete_meta}



\section{Conclusion}
We present Pippo, a diffusion transformer model that generates a dense set of high-resolution 
multi-view consistent images of a person from a single image. 
Our experiments show that our multi-stage training strategy, which combines large-scale 
in-the-wild data with high-quality multi-view studio captures, enables generalizable 
high-resolution multi-view synthesis.
Analysis of the diffusion transformer architecture reveals that self-attention with the reference image, 
\plucker coordinates with SIREN, and Spatial Anchor are all essential for high-fidelity multi-view human 
generation.
Pippo achieves, for the first time, consistent multi-view human image generation at 1K resolution. 
We also show that our proposed 3D consistency metric enables evaluation of 3D consistency without 
paired ground-truth data. 
One of the limitations of our approach is the limited number of simultaneously generated views, 
originating from large context length and memory constraints, which can be tackled with parallelization 
techniques and autoregressive generation.
Extending our approach to multi-view consistent video generation will be addressed in future work.







































