\section{Related Work}
\label{sec:related_works}
\input{fig/overview}

We review multi-view human datasets and generative models for human synthesis,  categorizing methods by their data requirements and prior constraintsâ€”such as parametric human models or explicit 3D structures. In this work, we minimize reliance on complex priors by training the model on large amounts of human-centric data and impose minimal camera and spatial 
controls.


\vspace{1mm}
\heading{Multi-view Human Datasets.}
While many large 2D human datasets exist~\cite{lin2019coco, LAION, ImageNet}, 3d captures of people (\ie captures using dozens of views at a time) are still relatively rare, since they are expensive to collect and are thus mostly obtained in specialized research labs.
Nevertheless, this kind of data has recently become more common, going from hundreds to thousands of publicly-available captures.
For face-only captures, a few datasets~\cite{Yu_2020_CVPR,yang2020facescape,yenamandra2020i3dmm,2023renderme360,martinez2024codec} provide up to 600 subjects captured from over 60 views each,
while for full-body captures, 4D-Dress~\cite{wang20244ddress} provides 32 subjects with 2 outfits each, and detailed annotations for their garments, while DNA-rendering~\cite{2023dnarendering} provides 500 subjects from 60 views.
MVHumanNet~\cite{xiong2024mvhumannet} stands apart by providing 4,500 people captured from 48 views in and over 9,000 sets of clothing, outpacing other datasets by an order of magnitude in terms of people diversity.
We use internal multi-view data of $~\sim$1000 identities with dense ($\sim$160) 
camera setup, but we expect our model to produce reasonable results
with publicly-available captures.


\vspace{1mm}
\heading{Generating Novel Poses and Expressions.}
Although different from our task, this related work builds 3D animatable models of people and faces. These methods use neural radiance fields~\cite{HumanNerf, su2023npc, su2022danbo}, or 3d Gaussians~\cite{zielonka2023drivable, qian20243dgs, wen2024gomavatar,kirschstein2023nersemble}, using canonical ``T''-poses and a corresponding forward and backward mapping to model local appearance changes.
Many of these methods, however, rely on accurate human shape and pose estimation, and only achieve high photorealism for personalized models from studio captures. 
Relatedly, a family of methods focus on generating animatable faces from a single image by disentangling viewpoints and expressions from large collections of 2d portraits~\cite{megaportraits, drobyshev2024emoportraits, zhang2023metaportrait, xu2024vasa, Nerfies, StyleGAN}, allowing photorealistic realtime reenactment, while being limited to faces and small viewpoint variations.
Unlike these methods, we complete missing parts of a person given either one or a few partial views.
We do not repose the person or animate the faces but can instead recover entirely missing views, which is orthogonal to these methods.
Moreover, our approach addresses both full-body and facial reconstruction, rather than specializing in either domain.


\heading{Generations with Explicit 3D Structures.}
Similar to early methods that extracted 3d representations from 2d models via score distillation~\cite{dreamfusion,TextMesh,fantasia3d,Nerdi,ProlificDreamer,magic3d,dreambooth3d}, 3d representations of human appearance have been extracted from GANs by co-training a neural renderer from learned TriPlane~\cite{EG3D} or HexPlane~\cite{An_2023_CVPR, li2025spherehead} latent spaces.
Others train generative models that operate directly in 3D space with diffusion or GAN objectives~\cite{Rodin3DDM,muller2023diffrf,TrainOn3D1,TrainOn3D2,TrainOn3D4,TrainOn3D5} or regression models to directly predict 3D representations~\cite{PiFU,saito2020pifuhd,sengupta2024diffhuman}.
While these methods produce 3d consistent faces by design, their quality is limited by the relatively small amounts of 3d training data
and/or the limitations of their respective 3d representations.
In contrast, we focus on generating 3d-consistent 2d images and thus avoid the downsides of explicit 3d modelling.

\heading{Generations with 2D Models.}
Another option is to model viewpoint changes in 2d, without an underlying 3d representation~\cite{zero1to3}, sometimes modelling either photometric~\cite{iNVS} or epipolar~\cite{huang2024epidiff} constraints explicitly.
DiffPortrait3D~\cite{gu2024diffportrait3d} is a portrait-generation method that falls roughly within this category, by fine-tuning a 2d diffusion model for 3d-aware face generation using ControlNet-style~\cite{zhang2023adding} conditioning for viewpoint control, as well as cross-view attention~\cite{guo2023animatediff} and initialization from a 3d-consistent GAN~\cite{EG3D}.
Other methods are more general and not specific to humans, focusing on single-view to 3D generation~\cite{zero1to3,NeuralLift-360,sargent2023zeronvs,seo2024genwarp,Make-It-3D,shen2023anything3d,gao2024cat3d}. In addition to these single-view methods, video models have been fine-tuned for camera control~\cite{he2024cameractrl,wang2024motionctrl}.
