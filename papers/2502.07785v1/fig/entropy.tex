\begin{figure}[h!]
    \centering    
    \includegraphics[height=8cm]{img/entropy.pdf}
    \vspace{-0.2cm}
    \caption{{\textbf{Entropy vs Growth Factor ($\gamma$) for varying number of views (tokens) (\cref{ssec:scaling_views})}. We present the entropy results (Y-axis) from our Attention Biasing technique inspired from~\cite{jin2023training} for varying number of tokens (individual line plots), and across different scaling growth factor $\gamma$ introduced in Eq.~\eqref{attn_bias} (X-axis). On X-axis, "No scaling" refers to the default attention formulation~\cite{vaswani2017attention} and $\gamma=1.0$ refers previous work~\cite{jin2023training} formulation. Empirically, we find that a slightly higher value of $\gamma=1.4$  leads to best visuals.}}
    \label{fig:entropy}
\end{figure}

