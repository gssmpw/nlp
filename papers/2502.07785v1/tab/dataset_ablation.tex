

\begin{table}[h!]
    \centering
    \small
    \setlength{\tabcolsep}{1pt}
    \begin{tabular}{llcccccc}
        \toprule

         {\#} & \textbf{Pretrain Data (\mid@128)} & RE@SG $\downarrow$ & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$  & Face $\uparrow$ & Body $\uparrow$ \\

        \midrule





        {1.} & 30M (1\% HQ) &  \underline{3.8} & 21.3 & 81.8 & 13.3 & {30.6} & {66.2} \\

        {2.} & 900M (30\% Random)   &   4.0 & 21.6 & 82.0 & \textbf{12.1} & {30.6} & \underline{67.2} \\

        {3.} & 2.1B (70\% Random) &   \textbf{3.7} & 21.6 & 82.1 & \textbf{12.1} & \underline{37.6} & \textbf{67.5} \\

        {4.} & Humans-3B (100\%) &  \textbf{3.7} & \textbf{21.9} & \textbf{82.3} & \underline{12.7} & \textbf{41.7} & \underline{67.2} \\

        \bottomrule
    \end{tabular}
    \vspace{-8pt}
    \caption{\textbf{Ablating the size of pre-training dataset on Humans-3B.} We conduct \fullbody mid-training with pretrained checkpoints trained on 1\%, 30\%, 70\%, and 100\% subsets of Humans-3B dataset. We found large-scale data is crucial for generalization to novel identities (\ie face similarity).}
    \label{tab:dataset_ablation}

\end{table}


