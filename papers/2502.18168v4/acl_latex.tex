% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{authblk}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{paralist}
\usepackage{colortbl}
\usepackage{bm} 
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs} 
\usepackage[ruled,vlined]{algorithm2e}
\SetAlFnt{\small}  % 控制字体大小

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention and Low-Rank Adaptation in Large Language Models
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author[1,2]{Yuxuan Zhang
\thanks{Corresponding author. Email: \texttt{MeCuping@outlook.com}. 
\newline \indent GitHub repository: 
\newline \indent \url{https://github.com/MeCuping/SECURA}}}

\affil[1]{Aberdeen Institute of Data Science and Artificial Intelligence, South China Normal University}
\affil[2]{Department of Computing Science, University of Aberdeen}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

% With the rapid development of large language models (LLMs), an increasing number of models with massive parameters have emerged. Fully fine-tuning such models is becoming less feasible due to the substantial computational and resource demands. Moreover, fully fine-tuning (FT) not only introduces a higher risk of catastrophic forgetting but also requires extremely high-performance hardware, making it neither cost-effective nor sustainable for future large-scale deep learning. As an alternative, Low-Rank Adaptation (LoRA) has been proposed, under the assumption that the weight updates during fine-tuning can be effectively represented using a low-rank decomposition. By fine-tuning only a small subset of parameters, LoRA achieves performance comparable to FT while significantly reducing resource requirements.
% However, this parameter-efficient fine-tuning method introduces new challenges, since it inherits its foundational design from FT, the issue of catastrophic forgetting persists. To address these limitations, we propose \textbf{SECURA}: \textbf{S}igmoid-\textbf{E}nhanced \textbf{CU}R Decomposition Lo\textbf{RA}, a novel LoRA variant designed to mitigate catastrophic forgetting and has remarkable performance on specific fine-tuning sphere. Our method leverages a newly designed normalization technique, \textbf{S-MagNorm}, to enhance parameter retention and improve overall performance. SECURA has been evaluated on a diverse range of tasks, including mathematical problem-solving (GSM8K), challenging question-answering (CNNDM), translation (NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results demonstrate that SECURA consistently outperforms multiple LoRA baselines in terms of both efficiency and effectiveness, providing a more robust and scalable solution for fine-tuning LLMs.


\begin{document}
\maketitle
\begin{abstract}
With the rapid development of large language models (LLMs), fully fine-tuning(FT) these models is becoming increasingly infeasible due to high computational demands. Moreover, FT also increases the risk of catastrophic forgetting. As an alternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only a small subset of parameters, LoRA achieves performance similar to FT while significantly reducing resource requirements. However, since LoRA inherits FT’s design, the issue of catastrophic forgetting still remains.
To address these limitations, we propose \textbf{SECURA}: \textbf{S}igmoid-\textbf{E}nhanced \textbf{CU}R Decomposition Lo\textbf{RA}, a novel PEFT variant designed to mitigate catastrophic forgetting while improving fine-tuning performance. Our method introduces a novel normalization technique, \textbf{Sigmoid based Magnitude Norm(S-MagNorm)},  which enhances parameter retention and improve fine-tuning performance. SECURA has been evaluated on a diverse range of tasks, including mathematical problem-solving (GSM8K), complex question-answering (CNNDM), translation (NewsDE), and complex multiple-choice reasoning (LogiQA). Experimental results demonstrate that it achieves an average fine-tuning improvement of \textbf{3.59\%} across 4 MCQ sphere tasks and a \textbf{2.51\%} improvement in 5 QA sphere tasks across Gemma2 2b, Qwen2 1.5b, Qwen2 7b,Llama3 8b,Llama3.1 8b compared to DoRA. Additionally, SECURA demonstrates superior knowledge retention capabilities, achieves state-of-the-art performance in 16 continual learning tests maintaining more than \textbf{70\%}  accuracy on LLMs' basic knowledge compared with Experience Replay(ER), sequential learning(SEQ),EWC, I-LoRA and CUR-LoRA.
\end{abstract}

\input{sections/introduction}
\input{sections/background}
\input{sections/methdology}
\input{sections/experiment}
\input{sections/discussion}

\section{Conclusion}
We propose SECURA, a novel PEFT fine-tune method that integrates our newly proposed S-MagNorm normalization and CABR decomposition to address catastrophic forgetting in LLM fine-tuning. SECURA dynamically balances parameter updates using a Sigmoid-based pruning mechanism, preserving critical knowledge while adapting to new tasks. Experiments on 18 datasets and 5 LLMs show SECURA outperforms standard LoRA, with a 3.63\% improvement on MCQ tasks and 2.56\% on QA tasks, while retaining over 70\% of base knowledge in continual learning scenarios. Paving the way for sustainable and ethical deployment of large AI models.

\section{Limitations}

\begin{itemize}
    \item \textbf{Lack of experiments on 70B+ LLMs:} Due to device limitations, further research on larger models (70B+ parameters) could not be conducted. Future work will explore scaling SECURA to more massive LLMs.

    \item \textbf{Computational Overhead:} S-MagNorm introduces additional matrix operations during training, increasing per-step time by approximately 1.18\% compared to LoRA. Future work could focus on optimizing the normalization layer for real-time applications.

\end{itemize}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\input{sections/appendix}

\end{document}
