\section{Background}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{Figures/Update_SECURA.jpg}
    \caption{S-MagNorm Update method: The figure illustrates the process flow of the S-MagNorm Normalization algorithm. Shows the steps of starting with the fusion of the former weight matrix with the CABR module and moving through the ratio loss matrix calculation, the normalization and sigmoid steps, followed by the final limiting of the ratio loss matrix.}
    \label{fig:S-MagNorm_Update}
    \vspace{-1em}
\end{figure*}


\paragraph{Low-Rank Adaption.}LoRA~\citep {whitehouse-etal-2024-low} is a method that uses low-rank matrices, hypothesizing that in the parameters matrix, only few parameters controls main knowledge. By introducing low-rank parameter matrices A and B, â€œit can generate a matrix that provides the same output, where A initialized by random sampling, B initialized by zeros.

Formally, consider a weight matrix \( \boldsymbol{W} \in \mathbb{R}^{h \times d} \) within the original LLMs. LoRA introduces two low-rank matrices, \( \boldsymbol{A} \in \mathbb{R}^{h \times r} \) and \( \boldsymbol{B} \in \mathbb{R}^{r \times d} \), where \( r \ll \min(h, d) \). Instead of directly updating the weight matrix, LoRA modifies the model's forward pass according to the following equation:
\[
    \boldsymbol{W}' = \boldsymbol{o} + \Delta \boldsymbol{o} = \boldsymbol{W} + \boldsymbol{AB}
\]
By freezing the basic weight and adding the AB matrix the model can be fine-tuned similarly like FT. 

\paragraph{CUR-Decomposition and Its Application in CUR-LoRA.}
CUR-Decomposition~\citep{hamm2019curdecompositionsapproximationsperturbations} is a matrix factorization method that selects specific rows and columns from a matrix. It is used to decompose a matrix in various ways depending on the design. When combined with gradient descent, it can be applied easily using the following formula:
\[
    \boldsymbol{W} = \boldsymbol{C} \boldsymbol{U} \boldsymbol{R}(\boldsymbol{x}),
\]
where \( \boldsymbol{W} \in \mathbb{R}^{h \times d} \) is the original parameter weight, \( \boldsymbol{C} \in \mathbb{R}^{h \times r} \) and \( \boldsymbol{R} \in \mathbb{R}^{r \times d} \) are the selected columns and rows from \( \boldsymbol{W} \), and \( \boldsymbol{U} \in \mathbb{R}^{r \times r} \) is a learnable matrix. Here, \( r \ll \min(h, d) \).

CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055} builds on CUR-Decomposition, by selecting the least important rows and columns as \( \boldsymbol{C} \) and \( \boldsymbol{R} \). These selected rows and columns are much fewer than the original matrix and have the lowest norms, which leads to slower updates. This decomposition allows CUR-LoRA to focus on updating the less important parts of the matrix, because less important parameters are more sensitive to slower updates, while important ones are less affected. In this way, the model can preserve the previously learned knowledge while efficiently learning new information, improving both training efficiency and performance.


\paragraph{Dynamic Network Pruning.}  
Dynamic Network Pruning~\citep{lin2020dynamicmodelpruningfeedback}  is an input-adaptive method that improves the efficiency of neural networks by dynamically adjusting their structure during inference. Unlike static pruning~\citep{siciliano2024staticpruningdenseretrieval}, which removes redundant weights or neurons, dynamic pruning adapts the network based on the characteristics of each input. This approach allows the network to selectively activate relevant parameters, optimizing computational cost.

Formally, consider a weight matrix \( \boldsymbol{W} \in \mathbb{R}^{h \times d} \) and a binary mask matrix \( \boldsymbol{M}(\boldsymbol{x}) \in \{0, 1\}^{h \times d} \), where \( \boldsymbol{x} \) represents the input data. Dynamic pruning modifies the forward pass as follows:
\[
    \boldsymbol{y} = \sigma \left( (\boldsymbol{W} \odot \boldsymbol{M}(\boldsymbol{x})) \boldsymbol{x} \right)
\]
where \( \odot \) denotes element-wise multiplication, and \( \sigma \) represents the activation function.

The mask \( \boldsymbol{M}(\boldsymbol{x}) \) is dynamically generated based on input-dependent criteria such as weight importance, gradient magnitudes, or gating mechanisms. This ensures that only a subset of parameters is used for each input.

Dynamic pruning offers significant computational efficiency by reducing unnecessary operations, allowing the network to adjust its complexity based on input difficulty. However, generating \( \boldsymbol{M}(\boldsymbol{x}) \) and ensuring efficient real-time pruning remain active research challenges.




% \paragraph{Elastic Weight Consolidation. } Elastic Weight Consolidation (EWC) is a technology which major in solve the problem of  Catastrophic forgetting by using Fisher Information Matrix normalization. By limiting the important parameters changing in lower way. 

% Here, consider a EWC Loss as  \( \boldsymbol{L_{EWC}} \in \mathbb{R}^{batch \times h} \) within the real update loss matrix. EWC induced Fisher Information Matrix Normalization, \( \boldsymbol{F_i}\) and a hyper parameter \( \boldsymbol{\lambda} \in \mathbb{R} \), also the former weight  \( \boldsymbol{\theta_i^\star}\) and newly weight \( \boldsymbol{\theta_i}\). Instead of directly updating the weight matrix, EWC modifies the model's forward pass according to the following equation:
% \begin{equation}
%     \mathcal{L}(\theta) = \mathcal{L}_{\text{new}}(\theta) + \frac{\lambda}{2} \sum_i F_i (\theta_i - \theta_i^\star)^2
% \end{equation}

% By adding the Fisher Information Matrix limit the important weights, EWC achieved in relieve catastrophic forgetting. 

% \paragraph{CUR-Decomposition\&CUR-LoRA.}
% CUR-Decomposition is a decomposition method designed to decompose the parameters by using some of its rows and columns. It can decompose the matrix with different meaning with your design. With gradient decent method it could applied easily by using the formula below:
% \begin{equation}
%     \boldsymbol{W} = \boldsymbol{C} \boldsymbol{U} \boldsymbol{R}(\boldsymbol{x}),
% \end{equation}
% Where \( \boldsymbol{W}\) is the original parameter weight, \( \boldsymbol{C}\) and \( \boldsymbol{R}\) is the columns and rows comes form \( \boldsymbol{W}\), and \( \boldsymbol{U}\) is a learnable matrix.

% CUR-LoRA is based on CUR-decomposition which using CUR-decompose the matrix by using the method of finding the most 'Not Important' rows and columns as \( \boldsymbol{C}\) and \( \boldsymbol{R}\), where \( \boldsymbol{C}\) and \( \boldsymbol{R}\) need far less than original matrix and has lowest Frobenius norm, by using this kind of decomposition CUR can update with lower value which means former important value will almost not been changed and can update the not important values to keep former knowledge and learning the new one.