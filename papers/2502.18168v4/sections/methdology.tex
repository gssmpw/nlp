\section{Methodology}
Our proposed SECURA method is based on three key components: CABR-LoRA inverse decomposition fine-tuning, the S-MagNorm Normalization algorithm, and two types of SECURA Merge method. Drawing inspiration from dynamic network pruning and CUR-decomposition, we hypothesize that parameters with higher norms in the original model are more important, as they store more critical knowledge. By constraining these important parameters and focusing on fine-tuning the less important ones, SECURA can achieve remarkable performance while significantly mitigating catastrophic forgetting. Appendix~\ref{app:Pseudo} shows all intuitive equation derivation. 

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{Figures/Grad_comparison.png}
    \caption{Gradient Analysis: The gradient variations during training shows that LoRA exhibits higher fluctuations, indicating greater parameter reshaping, which may increase the risk of catastrophic forgetting. In contrast, SECURA (CABR-LoRA + S-MagNorm) and CABR-LoRA Only shows lower gradient changes,  avoided excessive drift, suggesting more stable parameter updates, improving catastrophic forgetting mitigation and ability of finding a better optimum for the model's parameters. Additionally, the range and variance comparsion shows the essential role of S-MagNorm. Hyperparameter settings are detailed in Appendix \ref{app:hyper_param}.}
    \label{fig:Grad_analysis}
    \vspace{-1em}
\end{figure*}

\subsection{CABR-LoRA Inverse Decomposition Initialization}

CABR-LoRA is based on CUR-LoRA, utilizing CUR decomposition to break down the weight matrices. As shown in Figure \ref{fig:S-MagNorm_Update}, CABR-LoRA is initialized using a inverse low-rank decomposition of the 
\(U\) matrix. This allows the "less important" columns and rows to remain smaller while maintaining a higher-dimensional space for fine-tuning SECURA. Where \( \mathcal{W}_{\text{CABRWeight}} \) is the initialized CABR-LoRA weight, \( C \) and \( R \) represent the less important columns and rows, while \( A_{\text{initial}} \) and \( B_{\text{zeros}} \) are the trainable matrices with weights \( \mathcal{W}_{\text{A}} \) and \( \mathcal{W}_{\text{B}} \). The initialization formula for SECURA is given as follows:
\begin{equation}
    \mathcal{W}_{\text{CABRWeight}} = C\cdot A_{\text{initial}}\cdot B_{\text{zeros}}\cdot R.
\end{equation}

CUR-LoRA reduces the number of trainable parameters to locate least important \( \boldsymbol{C} \) and \( \boldsymbol{R} \), but this often leads to the loss of important parameters, causing performance degradation. To address this, we introduce CABR decomposition, which decomposes the \( U \) matrix, into two matrices: \( \mathcal{W}_{\text{A}} \) and \( \mathcal{W}_{\text{B}} \), which shapes are \( r \times m \) and \( m \times r \). Respectively here \( m > r \). \( \mathcal{W}_{\text{A}} \) is initialized via SVD decomposition, while \( \mathcal{W}_{\text{B}} \) is initialized with zeros. By initializing \( \mathcal{W}_A \) with SVD decomposition, CABR-LoRA effectively preserves prior knowledge. The initialization of \( \mathcal{W}_B \) with zeros ensures that this matrix will fine-tune from the base LLM without altering the critical knowledge.
The decomposition follows the formulas below:
\begin{equation}
    \mathcal{W}_{\text{frozen}} = \mathcal{U}_{\text{SVD}} \mathcal{S}_{\text{SVD}} \mathcal{V}_{\text{SVD}}^{\top}.
\end{equation}
\begin{equation}
    \mathcal{W}_A = \mathcal{U}_{\text{SVD}}^{(r,:)} \mathcal{S}_{\text{SVD}} \mathcal{V}_{\text{SVD}}^{(:,m) \top}.
\end{equation}
\begin{equation}
    \mathcal{W}_B = \mathbf{0}_{m \times r}, \quad \text{where} \quad \mathbf{0}_{m \times r} \text{ is a zero matrix}.
\end{equation}

\subsection{S-MagNorm Normalization Algorithm}

While CABR decomposition provides a higher dimension of fine-tuning,  only using CABR-LoRA alone does not fully address catastrophic forgetting, which is crucial for performance in lifelong learning. To tackle this, we introduce the S-MagNorm Normalization algorithm, inspired by dynamic network pruning. This algorithm adjusts the weight updates by controlling the change magnitude between the former and new weights, provide a stable parameter update (figure~\ref{fig:Grad_analysis}).

The core idea is to employ a Sigmoid-based dynamic scaling mechanism to regulate weight updates. This method constructs a weight pruning matrix whose values are constrained within the range \([1,2]\) to achieve the task of weighted mean between basic matrix weight and updated matrix weight. The Sigmoid function primarily emphasizes central value adjustments, effectively suppressing extreme variations. We can allow SECURA deal with whole matrix's changing tendency while limiting the extreme parameters' effect. Simultaneously, restricting the update amplitude of lower-norm columns and rows ensures that important parameters(with higher norms)'  magnitude experience minimal change, while less important parameters (with smaller norms) undergo more significant updates. The base weight matrix is denoted as \( W_{\text{base}} \), and the SECURA's matrices as \( A \) and \( B \).

The merged weight matrix is computed as:
\begin{equation}
    \mathcal{W}_{\text{SECURAMerged}} = CABR + \mathcal{W}_{\text{base}}.
\end{equation}
Next, we calculate the Magnitude Loss Matrix \(\mathcal{M}_{\text{Mag}}\), which captures the relative magnitude change between the new and base weights. Here \(\epsilon\) is a  small value to avoid division by too small parameters:
\begin{equation}
    \mathcal{M}_{\text{Mag}} = \left| \frac{\mathcal{W}_{\text{SECURAMerged}}}{\mathcal{W}_{\text{base}} + \epsilon} \right|.
\end{equation}
Then, we normalize the \(\mathcal{M}_{\text{Mag}}\) to the range \([-0.5, 0.5]\) to balance the values on both sides of zero:
\begin{equation}
    \mathcal{M}_{\text{norm}} = \left[ \frac{\mathcal{M}_{\text{Mag}}}{\max(\mathcal{M}_{\text{Mag}}) + \epsilon} - 0.5 \right] \cdot \text{Scale}.
\end{equation}
Note that the Scale parameter here resizes the normed value to ensure it falls within the range \([0,1]\) after applying the Sigmoid function, as the Sigmoid function only produces a range of \([0.3775, 0.6225]\) for the \([-0.5, 0.5]\) interval.

Finally, we apply the Sigmoid function to the normalized matrix to produce the S-MagNorm values and get the final restring matrix \(\mathcal{M}_{\text{Res}}\):
\begin{equation}
    \mathcal{M}_{\text{Res}} = 2 - \sigma\left(\mathcal{M}_{\text{norm}}\right).
\end{equation}

Once we have weight pruning matrix \(\mathcal{M}_{\text{Res}}\) , the mergred matrix \( W_{\text{SECURAMerged}} \), will then adjust the update process by dividing the S-MagNorm pruning-weight matrix \(\mathcal{M}_{\text{Res}}\):
\begin{equation}
    \mathcal{W}_{\text{updated}} = \frac{\mathcal{W}_{\text{SECURAMerged}}}{\mathcal{M}_{\text{Res}}}.
\end{equation}
This dynamic update process allows controlled pruning of weights, where updates for important parameters are slowed down and less important one remains, helping preserve important knowledge and mitigating catastrophic forgetting(Figure ~\ref{fig:SVD_Comparison}).

\subsection{Merge with Basic Model}
Limiting parameter growth alone is not optimal. Our early experiments showed that using only CABR-LoRA and S-MagNorm degraded both base performance and catastrophic forgetting mitigation. Drawing insights from delayed updates~\citep{kosta2020cost} and Target Network~\citep{gao2017adaptive}, we recognized that base parameters require controlled updates to prevent performance stagnation.

When freezing base parameters, a destructive two-phase cycle emerges: 
1) Unimportant parameters update freely while important ones remain constrained. 
2) Important parameters become smaller than unimportant ones in subsequent cycles, accelerating catastrophic forgetting as critical knowledge degrades.

We resolve this through two merging strategies:
\paragraph{M1: Direct Weight Merge}
Updates base LLM weights with one SECURA training cost:
\begin{equation}
\mathcal{W}_{\text{updated}} = \underbrace{C A_{\text{former}} B_{\text{train}} R}_{\text{CABR-LoRA}} + \mathcal{W}_{\text{base}}
\end{equation}

\paragraph{M2: Frozen Base Merge} 
Preserves base weights using dual SECURA instances: 
\begin{equation}
\begin{split}
\mathcal{W}_{\text{merged}} =& \underbrace{C A_{\text{frozen}} B_{\text{accum}} R}_{\text{Previous CABR-LoRA}} + \underbrace{C A_{\text{train}} B_{\text{reset}} R}_{\text{New CABR-LoRA}} \\
&\hspace{11em} + \mathcal{W}_{\text{base}}
\end{split}
\end{equation}
where parameter updates follow:
\begin{align}
A_{\text{former}},A_{\text{frozen}} & \leftarrow A_{\text{train}} \\
B_{\text{accum}}                    & \leftarrow B_{\text{accum}} + B_{\text{train}} \\
B_{\text{train}}                    & \leftarrow \mathbf{0} 
                                      \quad \text{\footnotesize (Post-update reset)}
\end{align}
M1 enables stronger fine-tuning by modifying base weights, while M2 prioritizes knowledge retention through:
\begin{compactitem}
\item \textbf{Parameter isolation}: Base weights remain frozen
\item \textbf{Progressive accumulation}: $B_{\text{accum}}$ aggregates historical updates
\item \textbf{Cyclic resetting}: $B_{\text{train}}$ reinitialization prevents update saturation
\end{compactitem}

Our ablation study (\ref{tab:Merge_method_Ablation}) reveals task-dependent superiority:
\begin{compactitem}
\item \textbf{M1} for performance-critical domains (mathematics, programming)
\item \textbf{M2} for knowledge-sensitive tasks (medical, economics) 
\end{compactitem}
The hybrid approach balances both objectives, with M1 generally providing better balanced optimization results.


% \section{Methodology}
% Our proposed SECURA based on three key components: CABR-LoRA reverse decompose fine-tuing, S-MagNorm Normalization algorithm, and decay restraint plugin based on S-MagNorm Normalization and CABR-LoRA decompose. By gain inspiration from dynamic network pruning and CUR-decomposition. we consider that the parameter which contain more weight of hole Weights is more important in other words these parameters saved more knowledge of basic LLMs. By restrainting these parameters and mainly fine-tuning Not Important parameters, we could let our SECURA gain remarkable performance and greatly prevent the catastorphic forgetting. 

% \subsection{CABR-LoRA reverse decompose fine-tuing}

% Our CABR-LoRA was based on CUR-LoRA, to decompose the weights by using CUR decomposition. The difference from CABR-LoRA and CUR-LoRA is that cause CUR-LoRA needs to let the parameters much less than weight parameters otherwise it will not fully shows its competence in using the 'Not Important' parameters. However, this means that it will lose lots of trainable parameters, leading to the result of loss of performance in most situations. So we design a CABR decomposition that decomposes the U matrix $\mathcal{W}_{\text{U}}$ by $\mathcal{W}_{\text{A}}$ and $\mathcal{W}_{\text{B}}$ where $\mathcal{W}_{\text{A}}$ and $\mathcal{W}_{\text{B}}$ are composed of the matrix $\mathcal{M}_{\text{R*H}}$ and $\mathcal{M}_{\text{H*R}}$. In these matrix $\mathcal{H>R}$,A is initialized by SVD decomposition B is initalized by zeros and the formulas below:
% \begin{equation}
%     W_{\text{froze}} = U S V^{T}.
% \end{equation}
% \begin{equation}
%     \mathcal{W}_A = \mathcal{U}_{\text{[:R,:]}}*\mathcal{S}_{\text{[:,:]}}*V_{\text{[:H,:]}}^{T}
% \end{equation}
% \begin{equation}
%     \mathcal{W}_B = \mathcal{M}_{\text{Zeros}}, \quad 
%     \text{where} \quad \mathcal{M}_{\text{Zeros}} \in \mathbb{R}^{\mathcal{H} \times \mathcal{R}}.
% \end{equation}

% By using the SVD decomposition to initialize $\mathcal{W}_A$,CABR-LoRA will store part of knowledge inside the basic LLMs and because we initialize $\mathcal{W}_B$ by zeros means that the matrix will fine-tuning from the totally same of LLMs.

% To address the problem of to less trainable parameters lead to lower performance we design the CABR-LoRA as a reverse LoRA decomposed the U matrix which means although the 'Not Important' keep the same we    can have a higher dimension to fine-tuning our SECURA and the final formula is shown here:
% \begin{equation}
%     W_{\text{CABRWeight}} = CA_{\text{inital}} B_{\text{zeros}} R.
% \end{equation}

% \subsection{S-MagNorm Normalization algorithm}

% However, simply using CABR-LoRA still can't reach our goals of better performance on relive the problem of catastrophic forgetting. Combining the idea of Dynamic Network Pruning. we are trying to seek a dynamic Pruning-weight function with better performance on lifelong study. So we designed the S-MagNorm Normalization algorithm, which majoring noticed the magnification changes between former weight and new one, by using Sigmoid function it will change the weight between 1-2 and by using this S-MagNorm dynamic Pruning-weight we can keep the higher important parameters nearly not change and help former unimportant  parameters change faster. We considering formal weight matrix as \( W_{\text{formal}}\) and a SECURA LoRA matrix as \( A\) and \( B\). The S-MagNorm Pruning-weight can be calculate as follows:
% \begin{equation}
%     \boldsymbol{W_{\text{SECURAMerged}}} = CABR + W_{\text{former}}
% \end{equation}

% \begin{equation}
%     \boldsymbol{Magnitude Loss Matrix} = \left| \frac{\boldsymbol{W_{\text{SECURAMerged}}}}{\boldsymbol{W_{\text{former}}}} + \epsilon \right|
% \end{equation}

% \begin{equation}
%     \boldsymbol{Norm_{\text{-0.5-0.5}}} = \left[ \frac{\boldsymbol{Magnitude Loss Matrix}}{\max(\boldsymbol{Magnitude Loss Matrix})} + \epsilon \right] \cdot \text{Scale}
% \end{equation}

% \begin{equation}
%     \boldsymbol{S-MagNorm} = 2 - \sigma\left(\boldsymbol{Norm_{\text{-0.5-0.5}}}\right)
% \end{equation}
% By using the formula (9) and (10), we can simply calculate the magnificent change between New updated matrix and former matrix. Cause we will using the S-MagNorm pruning-weight to pruning the weights we need to compose them from 1 to 2 and  by using formula (11) we will norm the Magnitude Loss Matrix to -0.5 to 0.5 to balance the values on both sides of zero. Cause the value range of Sigmoid funcion with  \( \boldsymbol{X} \in [-0.5,0.5] \) is \( \boldsymbol{Y} \in [0.3775,0.6225] \) so we added a scale to amplify the sigmoid value range to [0,1].Next we are going to limit normed Magnitude Loss Matrix to [1,2], cause we are using 'Not Important' rows and columns to update our parameters with lower norms than important ones, the update will always be slower than normal upgrade, which means parameters may adding a smaller upgrade, and smaller norm parameters may change with more higher magnificent and bigger one won't. So the 'Not Important' value will change with more higher magnificent than 'Important' one and it will reflecting on S-MagNorm where higher change with value tend to 1 and lower one tend to 2. Finally we sum two weight matrix and divided them by using our Pruning-weight to smooth our limited update, as the formula below:
% \begin{equation}
%     W_{\text{updated}} = (W_{\text{former}+W_{\text{SECURAMerged}}})/S-MagNorm
% \end{equation}
% Where,while magnificent is big:
% \begin{equation}
%     W_{\text{updated}} = (W_{\text{former}+W_{\text{SECURAMerged}}})/1
% \end{equation}
% While magnificent is small:
% \begin{equation}
%     W_{\text{updated}} = (W_{\text{former}+W_{\text{SECURAMerged}}})/2
% \end{equation}


% \subsection{SECURA Merge Plugin}

% However only to limit the parameter growth also not the best solution while in our early stage experiment the method only using 3.1 and 3.2 cause worse performance not only in basic performance but also in relive catastrophic so we trying to figure out the further edition in our idea. Finally we noticed that we can't keep the basic parameter always same even in after long term training, it also need upgrade, else it will make the performance in a lower stage and can never become higher than some boundary which not the best converge. So we added two types merge trick inside our SECURA.

% The problem about why only using S-MagNorm and CABR-LoRA cause the worse performance is that the basic weight always keep the same and because of that while time goes by more and more parameters changes more and more different than before. while we are talking about S-MagNorm we can 
% easily found that after applying S-MagNorm the important parameter and unimportant parameter will cause a upgrade speed difference where unimportant one will upgrade faster than important one, if \( W_{\text{former}} \) always keep the same . This will cause a cycle, at first cycle unimportant will properly upgraded and important one will be limited but at cycle 2 the problem emerged, it is obviously that former important one will become lower than unimportant one, and so do unimportant one. The chaos will start form this stage because former important one start to accelerate changing and will lead to even more worse catastrophic forgetting and bad performance.

% To address this problem we inject the merge function. It has two types of merge one is for using our function to fine-tuning the hole LLMs, another one is for LoRA type fine-tuning, although it construct quite different but it actually has the same feedback.
% Merge with basic Model:
% This method need only one SECURA training cost, but can't keep the baisc LLMs not changing.
% \begin{equation}
%     \boldsymbol{W_{\text{Newformer}}} = CABR + W_{\text{former}}.
% \end{equation}
% Merge with SECURA:
% This method need only two same SECURA, but can keep the baisc LLMs not changing. In every update you need to calculate one more to get the real formal weight. We adding a same SECURA in this type update where \( A_{\text{former}} \) and \( B_{\text{former}} \) are froze parameter to load the changes of updates and \( A_{\text{train}} \) and \( B_{\text{train}}\) are using to update.
% \begin{equation}
%     \boldsymbol{W_{\text{former}}} = CA_{\text{former}}A_{\text{former}}R + W_{\text{basic}}.
% \end{equation}
% And the merge will go through below formula:
% \begin{equation}
%     \boldsymbol{A_{\text{former}}} = A_{\text{train}}.
% \end{equation}
% \begin{equation}
%     \boldsymbol{B_{\text{former}}} = B_{\text{train}}.
% \end{equation}
% Notice two of this method has theoretically same feedback and after update the new former parameters \( B \) or \( B_{\text{train}} \) need to change to zeros to norms and help the LLMs learning newly knowledge.