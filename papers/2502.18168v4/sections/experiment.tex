\begin{figure}[tb]
    \centering
    \includegraphics[width=\columnwidth]{Figures/methods_comparison_with_diff.png}
    \caption{Comparison of Weight SVD Norm across different methods: The "Basic" method represents the original LLM weights without fine-tuning. After 2000 training steps, DoRA shows a 0.517 decrease in norm, indicating some loss of information. SECURA preserves more knowledge with a minor 0.061 change, while LoRA shows a significant 3.076 increase, reflecting greater weight modification. Hyperparameter settings are detailed in Appendix \ref{app:hyper_param}.}
    \label{fig:SVD_Comparison}
    \vspace{-1em}
\end{figure}

\section{Experiments}

\begin{figure*}[tb]
    \centering
    \includegraphics[width=\textwidth]{Figures/CombinedEXP3.png}
    \caption{The performance of SECURA compared to  16 tasks baselines sequentially trained using LLaMA-3 8B (left) and Qwen-2 7B (right) backbones, tested under F-task with learning rate 1e-5. Detailed results are in Appendix~\ref{app:all_results}}
    \label{fig:EXP3}
\end{figure*}

\subsection{Experimental Setup}

\paragraph{Datasets.}

We conducted comprehensive evaluations on 18 datasets: 15 from BBM~\citep{srivastava2023beyond},  while remaining datasets are selected from Llama3's~\citep{dubey2024llama}, Qwen2's~\citep{qwen2.5}, Gemma2's~\citep{team2024gemma} base training or fine-tuning datasets.  These datasets include 8 multiple-choice question (MCQ) datasets and 10 question-answering (QA) datasets,  spanning six domains: mathematical reasoning(GSM8K)~\citep{cobbe2021training},  logical deduction(LogicalQA)~\citep{srivastava2023beyond},  multilingual identification(CNN/DailyMail)~\citep{see2017get},  programming challenges(Multipl-E)~\citep{cassano2022multiplescalableextensibleapproach},  translation questions(News Commentary dataset)~\citep{tiedemann2012parallel},  and commonsense understanding(CommonsenseQA)~\citep{talmor-etal-2019-commonsenseqa}. Specifically, all of our benchmarks are carefully handled, with 1000 samples taken from each respective source and a 9:1 training-to-testing split.

To evaluate catastrophic forgetting, we combined three QA datasets-ARC-Challenge(ARC-C)~\citep{clark2018think}, GSM8K~\citep{cobbe2021training}, Multipl-E~\citep{cassano2022multiplescalableextensibleapproach}-as the test set , which were used for training/fine-tuning the selected LLMs. This fusion dataset will be referred to as  \textbf{F-task} in following parts. Further details can be found in the appendix~\ref{app:18_datasets}. 

\paragraph{LLM Backbones, LoRA variants, and Knowledge retention method.}

Our study employs five state-of-art LLMs: Gemma2-2B, Qwen2-1.5B/7B, and Llama3-8B/3.1-8B. 


We evaluate our SECURA framework using four PEFT variants: 
% LoRA~\citep {whitehouse-etal-2024-low}:Standard low-rank adaptation;DoRA~\citep{liu2024doraweightdecomposedlowrankadaptation}:Weight-Decomposed Low-Rank Adaptation;CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}:CUR-Decomposed low-rank adaptation;I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting}:Interpolation-based low-rank adaptation.

\begin{compactitem}
    \item LoRA~\citep {whitehouse-etal-2024-low}: Standard low-rank adaptation.
    \item DoRA~\citep{liu2024doraweightdecomposedlowrankadaptation}: Weight-Decomposed Low-Rank Adaptation.
    \item CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}: CUR-Decomposed low-rank adaptation.
    \item I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting}: Interpolation-based low-rank adaptation.
\end{compactitem}


Additionally, we implement five continuous learning baselines:
% SEQ~\citep{sutskever2014sequencesequencelearningneural}:Sequential Learning;EWC~\citep{aich2021elasticweightconsolidationewc}:Elastic Weight Consolidation;ER~\citep{fedus2020revisitingfundamentalsexperiencereplay}:Experience Retention;CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}:CUR-Decomposed low-rank adaptation;I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting}:Interpolation-based low-rank adaptation.
\begin{compactitem}
\item SEQ~\citep{sutskever2014sequencesequencelearningneural}:Sequential Learning
\item EWC~\citep{aich2021elasticweightconsolidationewc}:Elastic Weight Consolidation 
\item ER~\citep{fedus2020revisitingfundamentalsexperiencereplay}:Experience Retention
\item CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}:CUR-Decomposed low-rank adaptation
\item I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting}:Interpolation-based low-rank adaptation.
\end{compactitem}

\paragraph{Evaluation Metrics and Main Experiments}

We design three main experiments to assess SECURA's performance:

\textbf{Experiment 1 (PEFT Comparison):} Compares LoRA variants through 5 basic model backbones with 4 MCQ measured by accuracy (AC) and 5 QA performance measured by BLEU, ROUGE-1, ROUGE-L.

\textbf{Experiment 2 (Catastrophic Forgetting with PEFT Comparison):} In order to clearly evaluates knowledge retention with unified training extreme environment  (learning rate 1e-3)  under different PEFT method, we use the F-task set measured by BLEU, ROUGE-1, ROUGE-L.

\textbf{Experiment 3 (Catastrophic Forgetting with Knowledge retention method):} Benchmarks against continual learning methods at 1e-5 learning rate with sequential training on 16 tasks (each trained for 2K steps, totaling 32K steps) every PEFT based training method will fusion itself to frozen basic weights(e.g. CUR-LoRA, I-LoRA, SECURA) after one training task is done, measuring remain knowledge by using the F-task set and measured by BLEU, ROUGE-1, ROUGE-L.

Note: All main experiments' SECURA uses the M1 merge method. For hyperparameter details, refer to the appendix~\ref{app:hyper_param}.

\subsection{Main Result}

\begin{table}[tb]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} \\
    \midrule
    \multicolumn{6}{c}{\textbf{MCQ Avg.}} \\
    AbsNarr    
& 83.81 & 85.00 & 64.06 & 80.70 & \textbf{87.94} 
\\
    ObjCount   
& 86.49 & 86.43 & 59.35 & 82.49 & \textbf{89.40} 
\\
    PlayDiag   
& 85.71 & 84.93 & 56.53 & 76.74 & \textbf{89.87} 
\\
    ColorReason & 80.44 & 80.86 & 58.30 & 79.64 & \textbf{84.37}\\
    \bottomrule
    \multicolumn{6}{c}{\textbf{QA Avg.}} \\
    GSM8K & 81.11& 81.53& 66.79
& 38.21
& \textbf{82.11}\\
    NewsDE & 62.06
& 61.81
& 60.68
& 40.60
& \textbf{64.20}
\\
    NewsIT & 65.19
& 65.78
& 63.75
& 42.66
& \textbf{66.43}
\\
    NewsES & 60.25
& 58.54
& 60.70
& 39.47
& \textbf{66.06}
\\
    ALPACA & 53.22& 54.41& 41.09
& 26.53
& \textbf{55.82}
\\
    \bottomrule
    \end{tabular}}
   \caption{Performance comparison of PEFT variants under 5 LLMs across 4 MCQ tasks and 5 QA tasks. MCQ shows accuracy, QA shows BLEU, ROUGE-1, and ROUGE-L averages across all 5 LLMs. Detailed results are in Appendix~\ref{app:all_results}.}
    \label{tab:EXP1_AVG}
\end{table}

\paragraph{Performance on PEFT Comparison}

As shown in Table~\ref{tab:EXP1_AVG} SECURA outperforms all PEFT methods across both MCQ and QA tasks, achieving SOTA results. Compared to basic lora training, SECURA get average improvement 3.63\% in MCQ tasks and gains 2.56\% improvement in QA tasks. Notably, on tasks of NewsES, SECURA demonstrates nearly a 6\% improvement,  highlighting its exceptional fine-tuning capabilities. In contrast, other variants which major in handle the problem of catastrophic forgetting got negative improvement, even those focused on fine-tuning performance like DoRA, shows minor improvements, which only gains most improvement 1.19\% in abstract narrative understanding.

\begin{table}[tb]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lccccc}
    \toprule
    \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} \\
    \midrule

    AbsNarr
& 00.80
& 01.19& 04.48& 09.33& \textbf{72.09}
\\
    DisflQA
& 06.59
& 23.93
& 10.54
& 26.80
& \textbf{72.34}
\\
    LangID
& 00.47
& 03.95& 08.29& 24.32
& \textbf{72.42}
\\
    LogDeduc
& 01.03
& 00.30& 19.01
& 25.82
& \textbf{72.66}
\\
    ObjCount
& 08.10
& 07.88& 21.44
& 42.59
& \textbf{71.18}
\\
    PlayDiag
& 00.22
& 00.71& 05.97& 33.96
& \textbf{72.43}
\\
    ColorReason
& 01.17
& 00.28& 08.56& 39.18
& \textbf{72.31}
\\
    TrackObj& 00.26& 01.53& 03.22& 21.77
& \textbf{72.26}
\\
    \bottomrule
    \end{tabular}}
   \caption{Knowledge retention comparison with PEFT variants under 5 LLMs, using 8 tasks separated trained and tested on the F-task. Performance is measured by BLEU, ROUGE-1, and ROUGE-L averages across all 5 LLMs. Detailed results are in Appendix~\ref{app:all_results}.}
    \label{tab:EXP2 AVG}
\end{table}

\paragraph{Catastrophic Forgetting with PEFT Comparison}

To clearly shows SECURA's capability of Knowledge retention, we create a extreme environment(learning rate is 1e-3) through paper~\citep{Kirkpatrick_2017},~\citep{8107520},~\citep{zenke2017continuallearningsynapticintelligence} which shows with higher learning rate, the risk of catastrophic raised. The performance shows in the table~\ref{tab:EXP2 AVG}.

As shown in table, most of PEFT variants almost lose all of its former knowledge, even the variants which aimed to alleviate catastrophic forgetting such as CUR-LoRA and I-LoRA are still have heavily forgetting and only can keep average score under 42.59\%, while SECURA shows 
strong capabilities of limit catastrophic forgetting even in such tense environment and gains average score of 72.21\% under the test of F-task. Demonstrating its robustness in mitigating catastrophic forgetting.

\paragraph{Catastrophic Forgetting with Knowledge retention method}

SECURA is compared with other knowledge retention methods across 16 sequential training tasks, tested under the F-task. As shown in Figure~\ref{fig:EXP3}, SECURA consistently outperforms other methods in retaining knowledge, with only slight improvements over ER and I-LoRA. However, SECURA maintains superior knowledge retention and fine-tuning capability, making it an effective method for fine-tuning LLMs while limiting catastrophic forgetting.

\subsection{Ablation Study}
We have already conducted the ablation study comparing between Gradient variations   and SVD norm in figure ~\ref{fig:Grad_analysis} and ~\ref{fig:SVD_Comparison}. Here, we provide a more detailed ablation study focusing on the issues arising from different merge methods. 

\paragraph{How to Prove the merge hypothesis of the Update Cycle?}

\begin{table}[tb]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcc|cc}
    \toprule
    \textbf{Task} & \textbf{SECURA-1} & \textbf{SECURA-200} & \textbf{SECURA-1} & \textbf{SECURA-200} \\
    \midrule
    \multicolumn{3}{c}{\textbf{Qwen-2 7B}} & \multicolumn{2}{c}{\textbf{Llama-3.1 8b}} \\
            AbsNarr&  \textbf{77.93}
& 07.36&  \textbf{74.73}
& 54.17
\\
    DisflQA&  \textbf{78.46}
& 70.98
&  \textbf{73.85}
& 66.99
\\
    LangID&  \textbf{78.86}
& 00.16&  \textbf{73.13}
& 4.866
\\
    LogDeduc&  \textbf{75.19}
& 54.61
&  \textbf{75.11}
& 54.46
\\
    ObjCount&  \textbf{78.30}
& 73.30
&  \textbf{73.41}
& 61.19
\\
    PlayDiag&  \textbf{78.10}
& 00.22&  \textbf{74.81}
& 62.30
\\
    ColorReason&  \textbf{77.81}
& 00.26&  \textbf{75.10}
& 39.13
\\
    TrackObj&  \textbf{78.38}
& 74.29
&  \textbf{74.35}
& 65.78
\\
    \bottomrule
    \end{tabular}}
    \caption{Ablation study comparing knowledge retention performance of 1-step and 200-step merges under EXP2. The 1-step merge achieves better performance with minimal forgetting, while the 200-step merge leads to a cycle with reduced performance}
    \label{tab:Ablation_Cycle}
\end{table}

To prove the cycle hypothesis, we conducted an experiment comparing the performance of 1-step and 200-step merges across different LLMs. The results are shown in the table~\ref{tab:Ablation_Cycle}. Although the 200-step merge still retains some capabilities, it has obviously lower performance compared to the 1-step merge. We hypothesize that this performance degradation occurs because in the 200-step merge, the system has started entering the second cycle but not fully tracked by the problem. This suggests that the merge function plays a critical role in preventing catastrophic forgetting by maintaining a stable learning cycle.

\paragraph{Different Performance based on different Merge method}
\begin{table}[tb]
    \centering
    \small
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcc}
    \toprule

    \textbf{Merge Under EXP1:} & \textbf{MergeMethod-1} & \textbf{MergeMethod-2} \\
    \midrule
    AbsNarr    
& \textbf{87.48}
& 77.27\\
    ObjCount   
& \textbf{85.21}
& 79.83\\
    PlayDiag   
& \textbf{90.03}
& 74.00
\\
    ColorReason & \textbf{88.53} & 70.37\\
    \midrule
    \textbf{Merge Under EXP2:} & \textbf{MergeMethod-1} & \textbf{MergeMethod-2} \\
    \midrule
    AbsNarr     & 64.18 / 63.77 / 50.89
& \textbf{74.47 / 74.71 / 69.06}\\
    ObjCount    & 62.37 / 63.15 / 50.86
& \textbf{76.12 / 76.01 / 70.01}
\\
    PlayDiag    & 63.56 / 64.47 / 51.53
& \textbf{75.15 / 75.32 / 69.32}
\\
    ColorReason & 64.93 / 64.64 / 51.72
& \textbf{75.91 / 75.90 / 69.86}\\

    \bottomrule
    \end{tabular}}
    \caption{Comparison of fine-tuning efficiency and knowledge retention using MergeMethod-1 and MergeMethod-2 under two experiments (EXP1 and EXP2).}
    \label{tab:Merge_method_Ablation}
\end{table}
We designed another ablation study by testing different performance under M1 and M2 merge method with Gemma2 2b backbone and shows its feedback on table~\ref{tab:Merge_method_Ablation}. 

As the prediction in methodology, M1 method with deepen merge which will change basic weights shows more benefits on fine-tuning, performance better than M2 method range from 5.62\% to 18.16\%. In contrast M2 method which with shallow merge method shows more advantage under knowledge retention capabilities. Where it average improved retention 11.65\% in BLEU, 11.48\% in ROUGE-1 and 18.31\% in ROUGE-L  compared to M1 method.



% We conducted extensive tests on various PEFT variants using 4 QA tasks and 5 MCQ tasks. As shown in Table~\ref{tab:EXP1_AVG}, SECURA consistently outperforms all the PEFT methods, including LoRA and DoRA, across all tasks. Specifically, SECURA achieves an average improvement of 3.63\% in MCQ tasks and a 5.38\% improvement in QA tasks. 
% Notably, in the GSM8K and ALPACA tasks, SECURA demonstrates nearly a 10\% improvement, highlighting its exceptional fine-tuning capabilities. These results strongly indicate that SECURA not only improves performance but also has the potential to push the boundaries of state-of-the-art results in both MCQ and QA tasks. Furthermore, the robustness of SECURA across various domains underscores its adaptability and efficacy in handling diverse problem sets, confirming its potential as a leading method for future research and applications in the field.

% \begin{table*}[tb]
%     \centering
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lcc>{\columncolor[gray]{0.9}}ccc>{\columncolor[gray]{0.9}}ccc>{\columncolor[gray]{0.9}}ccc>{\columncolor[gray]{0.9}}ccc>{\columncolor[gray]{0.9}}c}
%         \toprule
%         \textbf{Task} 
%         & \multicolumn{3}{c}{\textbf{Gemma-2 2B}} 
%         & \multicolumn{3}{c}{\textbf{Qwen-2 1.5B}} 
%         & \multicolumn{3}{c}{\textbf{Qwen-2 7B}} 
%         & \multicolumn{3}{c}{\textbf{LLaMA-2 7B}} 
%         & \multicolumn{3}{c}{\textbf{LLaMA-3 8B}} \\
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} \cmidrule(lr){14-16}
%         & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA}
%         & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA}
%         & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA}
%         & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} \\
%         \midrule
%         % \textbf{\# Params} 
%         % &  &  &  
%         % &  &  &  
%         % &  &  &  
%         % &  &  &  \\\midrule
%         \textbf{AbsNarr}   &33.12&89.25&89.75&27.87&93.25&92.75&33.05&92.50&89.50&86.53&97.38&97.25\\
%         \textbf{ConParaKC} &24.50&100.00&93.75&24.75&99.00&94.00&32.67&96.00&92.75&84.10&98.00&95.13\\
%         \textbf{CSAlg}     &25.25&97.50&98.75&25.00&100.00&100.00&33.33&99.00&98.75&78.40&99.50&99.00\\
%         \textbf{DisflQA}   &55.59&87.57&88.10&54.44&89.63&87.98&61.80&89.03&91.17&87.96&94.42&89.97\\
%         \textbf{ElemMath}  &25.50&81.00&81.25&25.75&85.75&86.00&32.46&78.00&80.00&88.95&90.00&90.50\\
%         \textbf{EpiReason} &25.00&99.75&99.50&27.59&100.00&100.00&33.33&100.00&100.00&84.26&100.00&100.00\\
%         \textbf{FormFall}  &25.75&100.00&100.00&25.00&100.00&100.00&33.33&100.00&100.00&83.40&100.00&100.00\\
%         \textbf{LangID}    &27.23&77.00&77.00&25.75&89.25&88.00&33.89&79.75&79.75&76.41&95.12&94.50\\
%         \textbf{LogDeduc}  &35.50&84.50&80.75&25.00&89.50&90.75&33.33&83.00&82.75&93.08&96.00&96.38\\
%         \textbf{ObjCount}  &49.67&89.01&88.00&45.45&94.74&93.89&63.49&91.11&90.71&92.30&97.06&97.27\\
%         \textbf{PlayDiag}  &25.00&89.00&88.00&25.50&90.75&89.75&33.33&87.75&88.25&75.73&95.00&94.75\\
%         \textbf{QuesSel}   &33.52&99.00&98.00&51.11&98.00&97.00&33.00&99.00&99.00&70.41&97.00&97.00\\
%         \textbf{ColorReason} &25.00&79.00&78.25&25.50&87.50&87.75&33.33&80.75&80.75&82.27&95.62&96.25\\
%         \textbf{TrackObj}  &27.75&79.75&78.75&26.25&81.00&82.25&33.33&80.00&78.75&85.17&90.00&90.50\\
%         \textbf{UnitConv}  &27.11&100.00&100.00&25.00&100.00&100.00&33.33&100.00&100.00&82.67&100.00&100.00\\
%         \textbf{VitaFact}  &32.85&94.00&92.25&30.00&96.50&95.50&33.33&90.93&92.70&79.04&96.12&95.38\\
%         \textbf{WinoWhy}   &43.62&94.75&96.00&30.21&91.25&93.50&33.33&94.25&96.25&88.43&96.12&96.88\\\midrule
%         % \textbf{Composite-17} &&&&&&&&&&&&\\\midrule
%         \textbf{Avg.} &31.88&\textbf{90.65}&$\bm{\underline{89.89}_{\textcolor{teal}{-0.84\%}}}$&30.60&\textbf{93.30}&$\bm{\underline{92.89}_{\textcolor{teal}{-0.44\%}}}$&36.69&\textbf{90.65}&$\bm{90.65_{\textcolor{teal}{-0.00\%}}}$&83.48&\textbf{96.31}&$\bm{\underline{95.93}_{\textcolor{teal}{-0.12\%}}}$\\
%         \bottomrule
%     \end{tabular}}
%     \caption{The classification accuracy results on 17 MCQ tasks by comparing different basic LLMs backbones, single LoRA baselines and our DLP-LoRA approach. The evaluation results are averaged after running 10 times. The underline indicates the second-best accuracy and the subscript percentage denotes relative accuracy improvement or reduction over each single LoRA baseline.}
%     \label{table:multi_task_acc}
%     \vspace{-1em}
% \end{table*}

% \begin{table*}[tb]
%     \centering
%     \small
%     \begin{minipage}[t]{0.4\textwidth}
%         \centering
%         \setlength{\tabcolsep}{5pt}
%         \begin{tabular}{lccccc}
%             \toprule
%             \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} \\
%             \midrule
%             \multicolumn{6}{c}{\textbf{Gemma-2 2B}} \\
%             %没加
%             AbsNarr    & 33.12 & 89.25 & 34.13 & 77.13 & 93.25 \\
%             ObjCount   & 33.12 & 89.25 & 89.75 & 27.87 & 93.25 \\
%             PlayDiag   & 33.12 & 89.25 & 89.75 & 27.87 & 93.25 \\
%             ColorReason & 33.12 & 89.25 & 89.75 & 27.87 & 93.25 \\
%             \midrule
%             \multicolumn{6}{c}{\textbf{Qwen-2 1.5B}} \\
%             AbsNarr    & 84.07 & 85.67 & 34.13 & 77.13 & 86.72 \\
%             ObjCount   & 84.38 & 85.72 & 36.24 & 73.57 & 85.01 \\
%             PlayDiag   & 85.20 & 84.40 & 40.27 & 83.87 & 85.72 \\
%             ColorReason & 74.60 & 76.47 & 52.60 & 77.22 & 75.38 \\
%             \bottomrule
%             \multicolumn{6}{c}{\textbf{Qwen-2 7B}} \\
%             AbsNarr    & 86.53 & 87.06 & 40.41 & 73.49 & 87.33 \\
%             ObjCount   & 90.38 & 89.67 & 53.85 & 81.16 & 90.50 \\
%             PlayDiag   & 87.67 & 85.27 & 44.00 & 83.65 & 88.62 \\
%             ColorReason & 80.40 & 82.33 & 56.13 & 76.83 & 81.00 \\
%             \midrule
%         \end{tabular}
%         \caption{Results for QA.}
%         \label{table:gemma_qwen}
%     \end{minipage}
%     \hfill
%     \hfill
%     \begin{minipage}[t]{0.4\textwidth}
%         \centering
%         \setlength{\tabcolsep}{5pt}
%         \begin{tabular}{lccccc}
%             \toprule
%             \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR} & \textbf{ILoRA} & \textbf{SECURA} \\
%             \midrule
%             \multicolumn{6}{c}{\textbf{Llama3 8B}} \\
%             AbsNarr    & 90.35 & 90.91 & 71.94 & 90.06 & 89.54 \\
%             ObjCount   & 89.43 & 89.76 & 74.08 & 89.52 & 92.83 \\
%             PlayDiag   & 91.77 & 90.26 & 73.38 & 88.65 & 92.49 \\
%             ColorReason & 88.07 & 87.11 & 71.43 & 87.09 & 88.60 \\
%             \midrule
%             \multicolumn{6}{c}{\textbf{Llama3.1 8B}} \\
%             AbsNarr    & 89.43 & 88.95 & 76.81 & 89.23 & 88.65 \\
%             ObjCount   & 89.51 & 89.68 & 74.23 & 89.55 & 93.44 \\
%             PlayDiag   & 91.51 & 91.97 & 71.60 & 89.94 & 92.51 \\
%             ColorReason & 88.12 & 87.46 & 72.07 & 87.14 & 88.34 \\
%             \bottomrule
%             \multicolumn{6}{c}{\textbf{Avg.}} \\
%             %平均由于gemma没出没法做
%             AbsNarr    & 32.45 & 90.15 & 89.90 & 31.25 & 92.50 \\
%             ObjCount   & 32.45 & 90.15 & 89.90 & 31.25 & 92.50 \\
%             PlayDiag   & 32.45 & 90.15 & 89.90 & 31.25 & 92.50 \\
%             ColorReason & 32.45 & 90.15 & 89.90 & 31.25 & 92.50 \\
%             \midrule
%         \end{tabular}
%         \caption{Results for QA.}
%         \label{table:qwen_llama}
%     \end{minipage}
% \end{table*}

% \begin{table*}[tb]
%     \centering
%     \small
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccccc|ccccc}
%     \toprule
%     \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA}\\
%     \midrule
%     \multicolumn{6}{c}{\textbf{Gemma-2 2B}} & \multicolumn{5}{c}{\textbf{Llama-3 8B}}\\
%             AbsNarr    & 68.67 & 72.43 & 37.03 & 73.57 & 87.48 & 90.35 & 90.91 & 71.94 & 90.06 & 89.54 \\
%             ObjCount   & 78.74 & 77.34 & 58.34 & 78.64 & 85.21 & 89.43 & 89.76 & 74.08 & 89.52 & 92.83\\
%             PlayDiag   & 72.40 & 72.77 & 53.39 & 37.57 & 90.03 & 91.77 & 90.26 & 73.38 & 88.65 & 92.49\\
%             ColorReason & 71.00 & 70.93 & 39.27 & 69.90 & 88.53 & 88.07 & 87.11 & 71.43 & 87.09 & 88.60 \\\midrule
%     \multicolumn{6}{c}{\textbf{Qwen-2 1.5B}} & \multicolumn{5}{c}{\textbf{Llama-3.1 8B}}\\
%             AbsNarr    & 84.07 & 85.67 & 34.13 & 77.13 & 86.72 & 89.43 & 88.95 & 76.81 & 89.23 & 88.65\\
%             ObjCount   & 84.38 & 85.72 & 36.24 & 73.57 & 85.01 & 89.51 & 89.68 & 74.23 & 89.55 & 93.44 \\
%             PlayDiag   & 85.20 & 84.40 & 40.27 & 83.87 & 85.72 & 91.51 & 91.97 & 71.60 & 89.94 & 92.51 \\
%             ColorReason & 74.60 & 76.47 & 52.60 & 77.22 & 75.38 & 88.12 & 87.46 & 72.07 & 87.14 & 88.34 \\\midrule
%     \multicolumn{6}{c}{\textbf{Qwen-2 7B}} & \multicolumn{5}{c}{\textbf{Avg.}} \\
%             AbsNarr    & 86.53 & 87.06 & 40.41 & 73.49 & 87.33 & 83.81 & 85.00 & 64.06 & 80.70 & 87.94 \\
%             ObjCount   & 90.38 & 89.67 & 53.85 & 81.16 & 90.50 & 86.49 & 86.43 & 59.35 & 82.49 & 89.40 \\
%             PlayDiag   & 87.67 & 85.27 & 44.00 & 83.65 & 88.62 & 85.71 & 84.93 & 56.53 & 76.74 & 89.87\\
%             ColorReason & 80.40 & 82.33 & 56.13 & 76.83 & 81.00 & 80.44 & 80.86 & 58.30 & 79.64 & 84.37\\
%     \bottomrule
%     \end{tabular}}
%     \caption{MCQ}
%     \label{tab:MCQEXP1}
% \end{table*}



% % \hline
% %         \textbf{Task} & \multicolumn{3}{c|}{\textbf{LoRA}} & \multicolumn{3}{c|}{\textbf{DoRA}} & \multicolumn{3}{c|}{\textbf{CUR}} & \multicolumn{3}{c|}{\textbf{ILoRA}} & \multicolumn{3}{c}{\textbf{SECURA}} \\
% %         \cline{2-16}
% %                       & BLEU & Rouge & RougeL & BLEU & Rouge & RougeL & BLEU & Rouge & RougeL & BLEU & Rouge & RougeL & BLEU & Rouge & RougeL \\
% %         \hline
% %         Task 1        & 34.2 & 56.1  & 54.3   & 35.1 & 57.2  & 55.0   & 33.8 & 55.9  & 54.1   & 36.0 & 58.0  & 55.5   & 37.2 & 59.1  & 56.2   \\
% %         Task 2        & 32.4 & 54.3  & 53.1   & 33.0 & 55.0  & 53.8   & 31.9 & 53.7  & 52.7   & 34.5 & 56.3  & 54.5   & 35.7 & 57.5  & 55.3   \\
% %         \hline
% %         \end{tabular}

% \begin{table*}[tb]
%     \centering
%     \small
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccccc|ccccc}
%     \toprule
%     \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA}\\\midrule
%     \multicolumn{6}{c}{\textbf{Gemma-2 2B}} & \multicolumn{5}{c}{\textbf{LLaMA-3 8B}}\\
%     GSM8K& 58.38 / 56.01  / 50.74& 60.88 / 58.20  / 53.73 & 57.03 / 55.28  / 49.41& 60.22 / 58.76  / 53.86& 58.84 / 56.25  / 50.95& 57.54 / 54.33  / 49.58& 58.61 / 55.54  / 51.18& 59.73 / 56.81  / 52.22& 61.92 / 59.31  / 55.49& 76.04 / 78.99  / 74.58\\
%     NewsDE& 63.42 / 62.57  / 57.20& 63.53 / 62.53  / 56.74& 64.02 / 63.84  / 57.81& 62.82 / 62.33  / 56.69& 61.98 / 60.69  / 51.41& 63.32 / 61.20  / 56.48& 61.88 / 60.29  / 55.33& 63.50 / 61.69  / 56.32& 65.34 / 63.58 / 58.48& 66.10 / 65.68  / 64.60\\
%     NewsIT& 65.51 / 64.25  / 62.96& 66.02 / 65.71  / 64.50& 67.73 / 66.26  / 65.16& 65.06 / 63.68  / 62.25& 61.94 / 61.10  / 59.42& 65.12 / 64.00  / 63.03& 64.92 / 64.37  / 63.45& 66.01 / 64.52  / 63.49& 67.11 / 67.23  / 66.49& 68.63/ 69.08  / 67.39\\
%     NewsES& 58.00 / 55.58  / 51.52& 58.40 / 55.98  / 51.99& 61.23 / 61.53  / 57.01& 61.79 / 61.58  / 57.34& 58.78 / 55.85  / 50.81& 59.15 / 55.98  / 52.04& 53.69 / 53.25  / 49.24& 62.48 / 60.84  / 56.36& 60.81 / 58.45  / 54.89& 68.97 / 69.97  / 69.20\\
%     ALPACA& 40.12 / 25.18  / 22.50& 39.62 / 26.77  / 24.14& 41.94 / 27.71  / 22.35& 42.77 / 28.13  / 25.21& 38.92 / 23.54  / 20.10& 49.01 / 37.60  / 35.49& 48.36 / 37.04  / 34.81& 37.11 / 23.24  / 19.95& 48.06 / 36.66  / 34.83& 65.84 / 62.83  / 57.11\\\midrule
%     \multicolumn{6}{c}{\textbf{Qwen-2 1.5B}} & \multicolumn{5}{c}{\textbf{LLaMA-3.1 8B}}\\
%     GSM8K& 84.23 / 86.60  / 85.15& 84.08 / 86.48  / 84.98& 79.93 / 84.02  / 81.54& 7.82 / 14.80  / 12.37&  86.62 / 87.73  / 85.15& 57.48 / 54.57  / 49.84& 58.79 / 55.90  / 51.75& 60.36 / 57.60  / 53.31& 62.09 / 58.99  / 55.20& 76.47 / 79.51  / 75.11\\
%     NewsDE& 64.39 / 63.63  / 62.16& 64.72 / 64.02  / 62.82& 60.77 / 58.30  / 57.15& 5.09 / 11.68  / 11.07& 63.31 / 64.62  / 64.63& 62.85 / 60.54  / 55.85& 61.19 / 59.72  / 54.93& 65.80 / 62.57  / 57.31& 65.93 / 63.98  / 58.92& 66.02 / 66.86  / 65.87\\
%     NewsIT& 65.34 / 65.34  / 63.58& 66.54 / 66.74  / 64.90& 64.37 / 63.15  / 61.27& 4.38 / 9.35  / 8.57& 65.98 / 65.43  / 64.27& 65.35 / 64.50  / 63.40& 64.98 / 64.33  / 63.47& 65.12 / 63.73  / 62.75& 67.69 / 67.40  / 66.65& 68.27 / 69.18  / 67.63\\
%     NewsES& 65.74 / 66.71  / 65.57& 64.82 / 65.19  / 64.05& 61.97/ 61.39  / 49.41& 5.67 / 14.39  / 12.36& 65.88 / 67.31  / 66.74& 59.31 / 55.94  / 52.08& 54.25 / 53.39  / 49.73& 61.36 / 60.00  / 55.90& 62.10 / 59.63  / 55.87& 69.05 / 70.29  / 69.25\\
%     ALPACA& 65.49 / 61.63  / 55.08& 65.78 / 62.00  / 55.50& 57.03 / 55.28  / 49.41& 4.80 / 15.53  / 12.80& 66.82 / 64.31  / 55.19& 49.50 / 37.97  / 35.30& 49.59 / 37.63  / 35.33& 37.89 / 23.94  / 20.89& 48.67 / 35.96  / 33.87& 67.10 / 64.11  / 58.77\\\midrule
%     \multicolumn{6}{c}{\textbf{Qwen-2 7B}} & \multicolumn{5}{c}{\textbf{Avg.}}\\
%     GSM8K& 91.94 / 94.54  / 94.37& 91.80 / 94.27  / 94.17& 57.03 / 55.28  / 49.41& 2.40 / 5.36  / 4.68& 94.33 / 94.98  / 95.00& 69.91 / 69.21  / 65.94& 70.83 / 70.08  / 67.16& 67.84 / 68.12  / 64.42& 38.89 / 39.44  / 36.32& 78.45 / 79.49  / 76.16\\
%     NewsDE& 66.39 / 66.15  / 64.72& 66.87 / 66.86 / 65.71& 57.03 / 55.28  / 49.41& 6.59 / 8.68  / 7.88& 68.83 / 67.70  / 64.74& 64.08 / 62.82  / 59.28& 63.64 / 62.69 / 59.11& 63.07 / 61.41 / 57.56& 41.16 / 42.05  / 38.61& 65.25 / 65.11  / 62.25\\
%     NewsIT& 68.72 / 69.00  / 67.81& 69.17 / 69.32  / 68.23& 57.03 / 55.28  / 49.41& 6.24 / 9.04  / 8.90& 68.54 / 70.30  / 69.34& 66.01 / 65.42  / 64.16& 66.33 / 66.09  / 64.93& 65.03/ 63.77 / 62.47& 42.10 / 43.34 / 42.57& 66.67 / 67.02  / 65.61\\
%     NewsES& 67.96 / 69.49  / 68.80& 67.37 / 68.77  / 68.08& 57.03 / 55.28  / 49.41& 7.26 / 10.21  / 9.73& 68.89 / 69.94  / 70.00& 62.03 / 60.74  / 58.00& 59.70 /  59.31 / 56.62& 62.15 / 61.49  / 58.48& 39.53 / 40.85  / 38.04& 66.31 / 66.67  / 65.20\\
%     ALPACA& 66.55 / 63.14  / 57.42& 67.17/ 64.05  / 58.38& 57.03 / 55.28  / 49.41& 6.61 / 12.78  / 11.32& 68.25 / 65.64  / 58.86& 54.13 / 45.10  / 41.16& 54.11/ 45.50  / 41.63& 49.13 / 39.55  / 34.61& 30.18 / 25.81  / 23.61& 61.39 / 56.09  / 50.00\\
%     \bottomrule
%     \end{tabular}}
%     \caption{QA BLEU/Rouge1/RougeL}
%     \label{tab:my_label}
% \end{table*}

% \begin{figure*}[tb]
%     \centering
%     \includegraphics[width=\textwidth]{Figures/CombinedEXP3.png}
%     \caption{The performance of SECURA compared to  16 tasks baselines sequential trained using LLaMA-3 8B (left) and Qwen-2 7B (right) backbones. See Appendix~\ref{app:all_results} for more detailed results using Qwen-2 7B and LLaMA-3 8B LLMs backbones.}
%     \label{fig:EXP3}
% \end{figure*}

% \begin{table}[tb]
%     \centering
%     \small
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{lccccc}
%     \toprule
%     \textbf{Task} & \textbf{LoRA} & \textbf{DoRA} & \textbf{CUR-LoRA} & \textbf{ILoRA} & \textbf{SECURA} \\
%     \midrule
%     \multicolumn{6}{c}{\textbf{QA Avg.}} \\
% GSM8K & 69.91 / 69.21  / 65.94
% & 70.83 / 70.08  / 67.16
% & 67.84 / 68.12  / 64.42
% & 38.89 / 39.44  / 36.32
% & 78.45 / 79.49  / 76.16
% \\
%     NewsDE & 64.08 / 62.82  / 59.28
% & 63.64 / 62.69 / 59.11
% & 63.07 / 61.41 / 57.56
% & 41.16 / 42.05  / 38.61
% & 65.25 / 65.11  / 62.25
% \\
%     NewsIT & 66.01 / 65.42  / 64.16
% & 66.33 / 66.09  / 64.93
% & 65.03/ 63.77 / 62.47
% & 42.10 / 43.34 / 42.57
% & 66.67 / 67.02  / 65.61
% \\
%     NewsES & 62.03 / 60.74  / 58.00
% & 59.70 /  59.31 / 56.62
% & 62.15 / 61.49  / 58.48
% & 39.53 / 40.85  / 38.04
% & 66.31 / 66.67  / 65.20
% \\
%     ALPACA & 54.13 / 45.10  / 41.16& 54.11/ 45.50  / 41.63& 49.13 / 39.55  / 34.61& 30.18 / 25.81  / 23.61& 61.39 / 56.09  / 50.00\\
%     \bottomrule
%     \end{tabular}}
%    \caption{Performance comparison with 5 PEFT variants under 5 LLMs backbones with 4 MCQ tasks and 5 QA tasks where MCQ shows the accuracy average of all 5 LLMs, MCQ shows BLEU ROUGE-1 ROUGE-L average of all 5 LLMs}
%     \label{tab:EXP1_AVG}
% \end{table}


