% \section{Introduction}
% With more and more generate model based on Transformer structure. More and more higher parameters model comes out. Mostly in NLP many outstanding models are merge out such as Llama3, GPT4, Gemma2...However this kind of models are all exist one short coming--extermly high parameters. Even though they upload distil version of their models is still not affordable for most Individual developers and small labs while they are trying to FT these kind of models.

% To solve this problem, PEFT introduced a new kind of fine-tuning method LoRA. Which induct low rank matrix to solve the problem of high cost tuning. But obviously this kind of method just a kind of FT's approximate method means that it still can't get same or higher performance like FT. Also means it inherit the problem of Catastrophic Forgetting, although the froze weight may relieve the problem of it, but after using the adding LoRA matrix the problem keep still. For training less parameters although it ease the problem of high training cost but also caused another problem the model tend over-training before they found the best solution. To address these problem Weight Normalization provide some solution to relive the problem. But not eradicative solve the problem. Most of LoRA variation are most major in better performance DoRA/AdaLoRA, faster converge PiSSA and lesser parameters CUR-LoRA, it seems that rare people trying to solve the problem of both over-training and Uninterrupted Retention.

% Consider about problem below we gain the idea from QKV pruning theory, Layer-wise Relevance Propagation, Elastic Weight Consolidation and design a new variation of LoRA which majorly solve the problem of over-training and Catastrophic forgetting. We considering in one layer's weight the parameter's portion of all weights can shows it's importance inside the weights. Depend on this assume we designed the S-MagNorm normalization and CABR decomposition former one typically solve the problem of both over-fitting and Catastrophic forgetting and latter one fix the less parameters of CUR-Decomposition problem which lower performance than basic LoRA by adding dimension inside the U matrix . By combining the technique of S-MagNorm and CABR decomposition we achieve both outperformance on solve Over-fitting and memory prevention and make the accuracy better than basic LoRA at same order of magnitude. Our SECURA performance ... to ... higher than LoRA. Even compare to DoRA it also  gain the brilliant performance.

% The Summary of what we have done is:
% \begin{itemize}
%     \item We introduce SECURA, which a kind of LoRA variation combining technique of S-MagNorm and CABR can both solve the problem in fine-tuning of over-fitting and catastorphic forgetting also gain the performance even betther than basic LoRA.
%     \item We design a newly technique which called S-MagNorm Normalization which neither need former datasets nor former training outputs can prevent Catastrophic forgetting. By adding our decay attachment it will soon changing itself to the over-fitting prevention and also well performed.
%     \item To address the problem of CUR-LoRA low performance, we added inverse Low Rank adaption matrix inside the U trainable matrix and turned it to CABR-LoRA, let our variation can fine-tuning in higher dimensions. and gain the performance almost same to the baice LoRA and even better.
% \end{itemize}

%有没有一种可能introduction本身也要介绍一下技术??
\section{Introduction}
With the rapid development of transformer-based generative models, parameter scales have grown exponentially to enhance multi-task capabilities. Notable examples in natural language models(NLP) like LLaMA 3.1~\citep {dubey2024llama} and Qwen 2.5~\citep {qwen2.5} achieve remarkable performance on complex tasks including text generation, logical reasoning, and multilingual processing.  However, their massive parameterization (e.g., 70B+ parameters) creates deployment barriers: Full fine-tuning requires over 1.5TB of GPU memory and weeks of computation. Even with distilled versions, they remain prohibitively expensive for most individual developers and small research labs.

Parameter-Efficient Fine-Tuning (PEFT)~\citep {xu2023parameter} methods, such as Low-Rank Adaptation (LoRA)~\citep {whitehouse-etal-2024-low}, address this challenge. LoRA inject trainable tiny low-rank matrices (typically <0.1\% of original parameters) and frozen basic weights to reduce the computational cost of fine-tuning. Performed well in multi-task learning, multilingual summarisation, and transfer learning~\citep{whitehouse-etal-2024-low,zhao2024adamergex}. 

However, LoRA remains an approximation of FT, meaning it can hardly match or exceed the performance of full fine-tuning. Furthermore, it also inherits FT's critical issue: catastrophic forgetting~\citep {MCCLOSKEY1989109} . While freezing the base model weights alleviates this problem to some extent, the addition of LoRA matrices reintroduces the issue. Additionally, fine-tuning fewer parameters, while reducing computational cost, increases the risk of catastrophic forgetting before the model finds an optimal solution. Although weight normalization techniques (Salimans and Kingma, 2016) can be used to mitigate these issues, they do not provide a comprehensive solution. Existing LoRA variants primarily focus on improving specific aspects: enhancing performance (e.g. DoRA~\citep{liu2024doraweightdecomposedlowrankadaptation}, AdaLoRA~\citep{zhang2023adaloraadaptivebudgetallocation}), accelerating convergence (e.g., PiSSA~\citep{meng2024pissaprincipalsingularvalues}), or only reducing catastrophic forgetting (e.g. I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting}, CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}). However, few approaches simultaneously address uninterrupted retention of knowledge and keep the performance.

Motivated by challenges in catastrophic forgetting, we drew inspiration from QKV pruning theory~\citep{lv2024kvprunerstructuralpruningfaster}, and Elastic Weight Consolidation(EWC)~\citep{aich2021elasticweightconsolidationewc} to design a new LoRA variant: \textbf{SECURA}. Our approach leverages two core innovations:

\begin{compactitem}
    \item \textbf{CABR Decomposition}: To enhance the low performance of CUR-LoRA, we introduced an inverse low-rank adaptation matrix into the trainable $U$-matrix. By incorporating additional dimensions, CABR improves CUR-LoRA's performance, achieving results comparable to or exceeding those of standard LoRA.
    \item \textbf{S-MagNorm Normalization}: To balance model stability and performance, we designed S-MagNorm—a normalization method using Sigmoid's gradual transition property. It dynamically adjusts parameters during training to prevent catastrophic forgetting by prioritizing the collective impact of central parameter changes over extreme values, eliminating the need for historical data or prior outputs. This ensures balanced weight updates while maintaining adaptability.
    \newline
\end{compactitem}

By integrating S-MagNorm and CABR decomposition, SECURA effectively addresses memory retention challenges in continual learning while surpassing existing LoRA methods and their variants in accuracy under comparable computational budgets. Experiments on 5 LLMs (Gemma2 2B, Qwen2 1.5B/7B, Llama3 8B, Llama3.1 8B) demonstrate SECURA's superiority: it achieves \textbf{3.63\%} average improvement on 4 multiple-choice (MCQ) tasks and \textbf{2.56\%} on 5 question-answering (QA) tasks over baseline LoRA, even rivaling advanced variants like DoRA. For knowledge retention, SECURA maintains over \textbf{70\%} accuracy on fine-tuned test sets, outperforming state-of-the-art methods such as Experience Replay (ER)~\citep{fedus2020revisitingfundamentalsexperiencereplay}, sequential learning (SEQ)~\citep{sutskever2014sequencesequencelearningneural}, EWC~\citep{aich2021elasticweightconsolidationewc},I-LoRA~\citep{ren2024analyzingreducingcatastrophicforgetting} and CUR-LoRA~\citep{https://doi.org/10.5281/zenodo.12730055}. These results highlight SECURA's dual strengths in task adaptation and preserving pre-trained knowledge.
% \begin{itemize}
%     \item We propose \textbf{SECURA}, a novel LoRA variant that integrates S-MagNorm and CABR decomposition. SECURA effectively addresses catastrophic forgetting while achieving superior performance compared to baseline LoRA.
%     \item We introduce \textbf{S-MagNorm Normalization}, a technique that prevents catastrophic forgetting without requiring prior datasets or outputs.
%     \item We enhance CUR-LoRA through \textbf{CABR Decomposition}, which incorporates an inverse low-rank matrix into the trainable $U$-matrix. This adjustment allows for higher-dimensional fine-tuning and delivers performance comparable to or better than standard LoRA.
% \end{itemize}
