
\section{Appendix - Discussions and Limitations}
\label{sec:app_dis_lim}

\parlabel{Latency Improvement.}
Although On-device Sora enables efficient video generation, the generation latency remains higher compared to utilizing high-end GPUs; it requires several minutes to generate a video, whereas an NVIDIA A6000 GPU takes one minute. This discrepancy is evident due to the substantial disparity in computational resources between them. For instance, the iPhone 15 Pro's GPU features up to 2.15 TFLOPS with 3.3 GB of available memory, compared to the NVIDIA A6000, which offers up to 309 TFLOPS and 48 GB of memory. Despite this significant resource gap, On-device Sora achieves exceptional efficiency in video generation. Currently, it utilizes only the iPhone 15 Pro's GPU. We anticipate that the latency could be significantly enhanced if it can leverage NPU (Neural Processing Unit), \eg, the iPhone 15 Pro's Neural Engine~\cite{apple2023}, which delivers a peak performance of 35 TOPS. However, the current limitations in Apple's software and SDK support for state-of-the-art diffusion-based models~\cite{opensora} make the iPhone's NPU challenging to utilize effectively. %Specifically, the lack of SDK and frameworks provided by Apple hinders seamless integration of these advanced models with the NPU. 
We look forward to the development of software support for NPUs and leave the exploration of this for future work. Also, we plan to investigate the potential of NPUs on a variety of mobile devices, such as Android smartphones.

%Since On-device Sora currently only leverages iPhone's GPU, we expect that On-device Sora can be accelerated by using the NPU (Neural Processing Unit) equipped on the device, \eg, 35 TOPS of iPhone Pro's Neural Engine.

%However, currently iPhone's NPU is not easy to use due to the lack of support for state-of-the-art diffusion-based video generation models and related software and framework and SDK from Apple.

%We look for the development of usable software support for iPhone's NPU and  leave this for future work.

%We will also try to fully leverage mobile NPUS on various mobile devices, \eg, Android phones.

%support for it form Apple. Since NPU is ??? TOPS, which is much faster in processing video tensor data, we expect the latency can be reduced if we use NPU. We will leave this for future work.

%, We expect that On-device Sora can be accelerated by using the NPU (Neural Processing Unit) on the device, \eg, 35 TOPS of iPhone Pro's

%However, it is not easy to fully utilize NPU because the current software framework does not fully support the state-of-the-art diffusion-based video generation models.

%Currently, video generation takes too long. For instance, processing on an A6000 GPU takes approximately \textbf{?} seconds, while the on an iPhone 15 Pro takes approximately \textbf{?} seconds. 

%The integration of the iPhone's Neural Processing Unit (NPU), along with the application of lightweight techniques such as optimization and distillation, could considerably reduce this processing time. However, there are several limitations to applying these points, leaving them as areas for future research.

%Too long time. How can we reduce it?

\parlabel{Model Optimization. %\jjm{Do we still need this part since we changed the main idea to non-training?}
}
In On-device Sora, only T5~\cite{raffel2020exploring} is quantized to int8, reducing its size from 18 GB to 4.64 GB, while STDiT~\cite{opensora} and VAE~\cite{doersch2016tutorial} are executed with float32 due to their performance susceptibility, which has a significant impact on video quality. Additionally, we do not apply pruning~\cite{reed1993pruning} or knowledge distillation~\cite{gou2021knowledge}, as these methods also drop visual fidelity. In particular, we observe that naively shrinking STDiT~\cite{opensora} leads to significant visual loss, caused by iterative denoising steps, where errors propagate and accumulate to the final video. Another practical difficulty in achieving lightweight model optimization is the lack of resources required for model optimization; both re-training and fine-tuning state-of-the-art diffusion-based models typically demand several tens of GPUs, and the available datasets are often limited to effectively apply optimization methods. To tackle these challenges, we propose model training-free acceleration techniques for video generation in this work, \ie, Linear Proportional Leap (Sec. \ref{sec:ours1}) and Temporal Dimension Token Merging (Sec. \ref{sec:ours2}).%, and Concurrent Inference with3 Dynamic Loading (CI-DL) (\Cref{sec:ours3}). %We recognize further model optimization as a potential direction for our future work. %Our future work could involve applying advanced model optimization techniques to enhance the model efficiency.

%To address this, we propose training-free acceleration for video generation, \ie, Linear Proportional Leap and Temporal Dimension Token Merging.

%However, we see that this remains a potential direction for future work. We can apply quantization and model optimization to make the proposed On-device Sora more efficient, which we leave for future work.

%as we await more substantial resources.

%We will also try to optimize them, not only quantization but also other optimziation techniques such as distillation, etc.

%That being said, we can apply quantization and model optimization to make the proposed On-device Sora more efficient, which we leave for future work.

%We don't apply quantization and model optimization so much as pruning and knowledge distortion in this work. 

%The main reason is that the reduction in video quality outweighs the benefits gained from a lightweight model. Additionally, there is a significant lack of resources; the dataset available for training is insufficient for applying advanced techniques, and the number of GPUs required for comprehensive training is inadequate. This remains a potential direction for future work as we await more substantial resources.

%\parlabel{Image-to-Video Generation.}
%On-device Sora generates videos from textual descriptions, enabling on-device text-to-video generation. 
%Building on the proposed On-device Sora, it is also feasible to extend its capabilities to image-to-video generation, empowering users to create personalized videos based on their own visual data. Furthermore, On-device Sora could evolve to accept both text and images as a multi-modal generative solution, providing an integrated approach to versatile data generation, expected to facilitate the personalization of video generation on mobile devices. We envision that On-device Sora would lay the foundation for future multi-modal visual and textual generation applications on various mobile systems, fostering the on-device revolution and expansion of generative technologies.


\parlabel{Straightness Constraints.} For video generation models that do not exhibit straightness during their denoising procedures, such as CogVideoX~\cite{yang2024cogvideox}, which employs DPM-Solver~\cite{zheng2023dpm}, Linear Proportional Leap (LPL) is currently inapplicable. In such cases, as described in previous work~\cite{ye2024schedule}, an alternative approach involves predicting modifications to the model's noise schedule by employing additional methodologies, such as reinforcement learning. However, these kind of methods require additional training  which may invoke extra costs, and necessitates retraining whenever the target model architecture changes. By contrast, LPL is a plug-and-play algorithm that can be applied in a model-agnostic manner, as long as a sufficient level of straightness is maintained during the denoising process, thereby eliminating the need for additional training. In future work, we plan to investigate the optimal reduction point achievable with this algorithm, and we anticipate that it will evolve into a widely applicable methodology for all Rectified Flow-based models~\cite{liu2022flow}.
%Currently, On-device Sora generates videos from a textual description (\ie, text-to-video generation).

%Based on the framework of On-device Sora, it is possible to extend On-device Sora to generate videos from images (\ie, image-to-video generation), allowing users for generating personalized videos based on their own visual data or preferences.

%Going further, On-device Sora can be adapted to take both text and input image to generative videos as a multi-modal generative solution. We expect it will expedite the personalization of video generation.

%We hope that the proposed On-device Sora can lay the groundwork towards more advanced multimedia visual generation services and applications and expect that such evolvement and expansion of On-device Sora can

%\parlabel{Why not using iPhone's NPU? Why using GPU?}
%We expect that On-device Sora can be accelerated by using the NPU. However, it is not easy to fully utilize NPU because the current software framework does not fully support the state-of-the-art diffusion-based video generation models. We leave this for furture work.

%#\parlabel{Why Open-Sora? The one without promising performances?}
%blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah

%\parlabel{Why Low resolution? with all that time and resources?}
%blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah
