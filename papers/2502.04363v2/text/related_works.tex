\section{Related Work}

\parlabel{On-device Video Generation.}
On-device video generation has recently gained attention due to the growing demand for real-time, efficient, and privacy-preserving content creation, and recently begun to be studied. MobileVD~\cite{yahia2024mobile}, based on UNet~\cite{ronneberger2015u} in Stable Video Diffusion~\cite{blattmann2023stable}, reduces computational costs by employing pruning, including low-resolution fine-tuning, temporal multi-scaling, and optimizations in channel and time blocks. SnapGen-V~\cite{wu2024snapgen} utilizes Stable Diffusion v1.5~\cite{rombach2022high} for processing image information while efficiently handling temporal information through Conv3D and attention-based layers. Furthermore, with fine-tuning, the denoising step is reduced. However, these approaches require substantial GPU resources for model optimization, \eg, MobileVD gone 100K training iterations on four A100 GPUs, and SnapGen-V utilized more than 150K iterations on 256 NVIDIA A100 GPUs. In contrast, On-Device Sora instantly applies without additional model training or optimization, removing the requirement for GPU resources.
%Additionally, the denoising process is reduced to a single step through adversarial fine-tuning. 
%\jjm{On-device video generation has recently gained significant attention due to the growing demand for real-time, efficient, and privacy-preserving content creation, and has recently begun to be studied. MobileVD~\cite{yahia2024mobile}, based on UNet in Stable Video Diffusion~\cite{blattmann2023stable}, has reduced memory and computational costs by employing pruning techniques, including low-resolution fine-tuning, temporal multi-scaling, and optimizations in channel and time blocks. Additionally, the denoising process was reduced to a single step through adversarial fine-tuning. SnapGen-V~\cite{wu2024snapgen} utilizes Stable Diffusion v1.5~\cite{rombach2022high} as its backbone for processing image information while efficiently handling temporal information through Conv3D and attention-based layers. Furthermore, through advertising fine-tuning, the denoising step was reduced to four steps. However, these approaches require substantial GPU resources for model training. MobileVD necessitates over 100K iterations of training on 4 A100 GPUs, while SnapGen-V requires more than 150K iterations on 256 NVIDIA A100 80G GPUs. In contrast, On-device Sora does not require additional training, so GPU resources are not required.}
%Numerous video diffusion models have advanced the related technologies, including Make-A-Video~\cite{singer2022make}, Snap Video~\cite{menapace2024snap}, Imagen Video~\cite{ho2022imagen}, Video Diffusion~\cite{ho2022video}, Stable Video Diffusion\cite{blattmann2023stable}, Dreamix~\cite{molad2023dreamix}, MCVD~\cite{voleti2022mcvd}, Tune-A-Video~\cite{wu2023tune}, Text2Video-Zero~\cite{khachatryan2023text2video}, among others. Given the substantial resource requirements of video generation, some studies have investigated optimization under various conditions. For example, QVD~\cite{tian2024qvd} employs post-training quantization~\cite{liu2021post} to enhance computational efficiency and reduce memory usage, while MagicVideo~\cite{zhou2022magicvideo} minimizes computations by generating videos in the latent space. CMD~\cite{yu2024efficient} decomposes videos into content frames and low-dimensional motion representations to reduce the computational cost of video generation. Similarly, SimDA~\cite{xing2024simda} improves the efficiency of diffusion models by utilizing an effective adapter mechanism. While those studies have sought to enhance the efficiency of video generation, none of them have successfully demonstrated to generate videos on commodity mobile devices, primarily due to high resource demands even after applying their optimization techniques. To the best of our knowledge, the proposed On-device Sora is the first standalone and efficient video generation solution capable of operating on resource-constrained smartphones.

%\parlabel{On-device Image Generation. \jjm{Do we need this part?}}
%There are many works that generate images on the device~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, jia2023blazestylegan, belousov2021mobilestylegan}. However, on-device video generation is struggling due to various challenges. First, video generation requires processing substantially more complex and larger data than image generation~\cite{blattmann2023stable, chen2017video, nan2024openvid}. While image generation focuses on a single frame, video generation involves handling multiple frames, with the data requirements scaling with the video’s frame rate (fps) and duration. Additionally, video generation must maintain temporal consistency across frames~\cite{menapace2024snap, singer2022make, xing2024simda}, i.e., it demands maintaining coherent temporal relationships, including object movement, lighting transitions, and background dynamics between successive frames. Achieving this level of consistency necessitates significantly more computation and memory. Furthermore, these requirements often require complex and large models, exacerbating the challenges when operating on resource-limited mobile devices. %Despite these hurdles, to the best of our knowledge, On-device Sora is the first successful implementation of on-device video generation, marking a significant milestone.

%\note{WRITING DONE BY 11/29. 
%GIST: THERE ARE MANY WORKS THAT CAN GENERATE IMAGES ON THE DEVICE EFFICIENTLY. BUT VIDEO GENERATION IS MUCH MORE DIFFICULT PROBLEMS (EXPLAIN WHY VIDEO GENERATION IS MORE DIFFICULT THAN IMAGE GENERATION, E.G., MORE RESOURCE, VIDEO GENERATION IS MORE DELICATE AND SENSITIVE, ETC.) SO THERE IS NO ON-DEVICE VIDEO GENERATION. AS FAR AS WE KNOW, OURS IS THE FIRST ON-DEVICE VIDEO GENERATION.}

%\jjm{There are many works that can generate images on the device efficiently~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, jia2023blazestylegan, belousov2021mobilestylegan}. However, video generation on a device is struggling due to various limitations. Firstly, video generation requires processing a substantially more complex and larger amount of data than image generation~\cite{blattmann2023stable, chen2017video, nan2024openvid}. While image generation focuses on a single frame, video generation involves handling multiple frames, with the data requirements scaling with the video’s frame rate (fps) and duration. Additionally, video generation must maintain temporal consistency across frames~\cite{menapace2024snap, singer2022make, xing2024simda}. Image generation primarily requires preserving spatial consistency, and ensuring relationships between pixels within a single frame. In contrast, video generation demands maintaining coherent temporal relationships (temporal consistency), including object movement, lighting transitions, and background dynamics between successive frames. Achieving this level of consistency necessitates significantly more computational power and memory. Furthermore, these requirements often involve employing complex and large models, exacerbating the challenges when operating within the constraints of resource-limited devices. Despite these hurdles, as far as we know, our work represents the first successful implementation of on-device video generation, marking a significant milestone in overcoming these inherent limitations.}

\parlabel{Rectified Flow.}
%\note{WRITING DONE BY 11/29. 
%GIST: THERE ARE MANY STUDIED RELATED TO RECTIFIED FLOW. BUT WE ARE THE FIRST THAT REDUCES DENOISING (SAMPLING) STEPS, WITHOUT SIGNIFICANTLY DEGRADING VIDEO QUALITY, WITHOUT ADDTIONAL TRAINING. THERE ARE SOME FUTURE WORK WE CAN STUDY?}
While Open-Sora~\cite{opensora} reduces the number of denoising steps by leveraging Rectified Flow~\cite{liu2022flow}, most related approaches~\cite{esser2024scaling,zhu2025slimflow} require conditioned model training and/or distillation~\cite{zhu2025slimflow}. In contrast, the proposed Linear Proportional Leap effectively reduces the denoising steps without a significant performance drop, as validated using VBench~\cite{huang2024vbench}, without requiring model re-training or distillation. Notably, it can be easily activated at runtime by just calculating the cosine similarities of drifts between consecutive denoising steps. 
% By developing the current Euler method~\cite{chen2018neural} into more advanced methodologies, it may be possible to further reduce denoising steps while ensuring stability within boundaries. We leave this for future work.

%\bonote{Recent diffusion models based on Rectified Flow successfully addressed the excessive amount of sampling steps in recent generative models for high-definition images~\cite{liu2022flow} both in initial training methods and distillation approaches. As Rectified Flow itself have efficient and viable theoretical nature that reduces the transfer cost between the distributions by learning to directly and linearly mapping the initial noise into desired data points, recent studies~\cite{esser2024scaling,zhu2025slimflow} could successfully achieve their goals and even diffusion model for video generations ~\cite{opensora} smoothly employed the identical methodologies, which yet required at least 30 steps for the viable performance. However, most of the studies in this domain often requires conditioned training or additional distillation to reduce their denoising steps. In contrast, our proposal of Linear Proportional Leap utilizes the simple intuition and tendency, reduced the required number of steps drastically without significant drop in their performance toward vbench~\cite{huang2024vbench}, without any need of additional training or distillation. Still, Linear Proportional Leap is only activated depending on the cosine similarities of drift forces computed from consecutive steps. By addressing the further steps before for the activation or modifying current Euler-based methods into more developed methodologies, we may achieve further reduce the number of denoising steps while guaranteeing certain boundaries of stabilities of our algorithm.}

\parlabel{Token Merging.}
%\note{WRITING DONE BY 11/29. 
%GIST: THERE ARE MANY SPATIAL TOKEN MERGING BUT WE ARE THE FIRST APPLYING TEMPORAL TOKEN MERGING IN CROSS ATTENTIONS IN VIDEO DIFFUSION. THERE ARE SOME FUTURE WORK WE CAN STUDY?}
Most token merging methods~\cite{bolya2022token,bolya2023token,feng2023efficient} primarily focus on image generation, where tokens are merged based on the spatial similarity rather than temporal similarity. Although some temporal token merging have been proposed, they are applied to models in other domains~\cite{gotz2024efficient,li2024vidtome, chen2024tempme}, not in video generation. As such, Temporal Dimension Token Merging is the first to apply token merging based on the successive similarities between frames in video generation. Additionally, while previous works apply token merging to self-attention due to performance degradation~\cite{bolya2022token,bolya2023token,feng2023efficient,li2024vidtome}, On-device Sora shows that token merging can be applied to cross-attention with minimal performance loss, achieving 50\% merging ratio.
