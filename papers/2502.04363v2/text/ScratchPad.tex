
\section{\note{SCRATCH PAD: TO BE REMOVED FROM HERE}}

\jjm{
\subsection{Introduction}

Recent advancements in artificial intelligence technology have significantly enhanced video and image processing capabilities. Image diffusion models~\cite{ho2020denoising, song2020denoising, peebles2023scalable, rombach2022high, saharia2022palette, dhariwal2021diffusion, zhang2023adding, podell2023sdxl} now serve as foundational tools for tasks such as image generation, image editing, and personalized content creation. Concurrently, video diffusion models~\cite{singer2022make, menapace2024snap, ho2022imagen, ho2022video, blattmann2023stable, molad2023dreamix, harvey2022flexible, blattmann2023align, voleti2022mcvd, wu2023tune, khachatryan2023text2video} have facilitated the development of complex applications, including video generation and real-time video analysis. However, due to the inherent complexity of the diffusion processes and the substantial parameter sizes involved, the computational costs and memory requirements have become significant. This has limited the deployment of such models to environments with sufficient computational resources.

Conversely, with the rapid development of smartphones and the Internet of Things (IoT), there has been an increasing demand for running generative models directly on devices. This trend has underscored the need for on-device models. Cloud-based models typically require data transmission, including sensitive personal information, to remote servers, which may pose security risks due to potential data leakage or unauthorized access. In contrast, on-device models process data directly on the local device, providing an excellent advantage in terms of user privacy protection.

Moreover, cloud-based models may be subject to latency issues dependent on network speed and may fail to function optimally when network connectivity is unstable or unavailable. Users often require stable model performance regardless of network conditions. In such cases, on-device models provide a more reliable user experience than their cloud-based counterparts. Additionally, transmitting and processing data on remote servers demands substantial network bandwidth and server resources, increasing costs. On-device models, which handle data processing locally, can substantially mitigate these costs.

A common limitation of cloud-based models is that they are generally shared across multiple users, which restricts the extent of personalization to cater to individual user characteristics. Conversely, on-device models, tailored individually for each user, offer the advantage of providing personalized user experiences.

Reflecting this trend, numerous researchers have focused on the development of on-device image models~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, choi2023squeezing, chen2023speed, zhao2023mobilediffusion}. However, the development of on-device video models remains at an early stage. The necessity for on-device video models is particularly pronounced due to the larger and potentially more sensitive nature of video data than image data.

We propose an on-device video model called On-Device Sora in response to this need. On-device Sora optimized Open-Sora~\cite{opensora} as a backbone model to fit on-device environments that lack computational power and memory capacity. Lack of computational resources on on-device led to slow inference time, and we apply two techniques to address this: 1) Temporal Token Merging, which reduces the computational cost by applying token merging~\cite{bolya2022token} in cross-attention and time dimension; 2) Linear Proportional Leap (LPL), which leaps the rest by referring to previous steps. In addition, to overcome the lack of memory resources, we apply Model Partitioning which divides the model into several parts, and Concurrent Implementation which performs model loading and prediction at the same time.

In this study, we made the following contributions:

1. To the best of our knowledge, we have implemented the first Video Diffusion Model that can run on-device.

2. To the best of our knowledge, we proposed an algorithm that does not cause performance degradation by applying Token Merging to cross-attention for the first time.

3. We proposed effective novel methods for implementing the on-device video diffusion model.

4. We have solved various challenges that occurred while uploading the Video Diffusion Model to on-device.

5. We shared the code of On-Device Sora, which helped with the follow-up study.
}

\subsection{Background}

\jjm{


\parlabel{Diffusion Model} uses a Markov Chain to gradually add noise to the data in the forward process and then remove it in the reverse process, modeling the distribution of the original data. The forward process involves progressively adding noise, $\epsilon$, to the given data $x_0$, transforming it into completely noised data $x_T$ over $T$ steps.

$q(x_t|x_{t-1}) = N(x_t;\sqrt{1-\beta}x_{t-1},\beta_t I)$

$\beta_t$ is a hyperparameter that controls the magnitude of noise at time step $t$, and $\mathcal{N}$ represents a multivariate normal distribution with a mean of $\sqrt{1-\beta_t}\mathbf{x}_{t-1}$ and a variance of $\beta_t\mathbf{I}$. The model progressively adds noise from $t=0$ to $t=T$ through this process, ultimately transforming $\mathbf{x}_T$ into almost pure Gaussian noise. The reverse process aims to approximate $q(\mathbf{x}_{t-1}|\mathbf{x}_t)$ to remove noise at each time step.

$p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(x_t,t))$

$\mu_\theta$ and $\Sigma_\theta$ are the mean and variance estimated through training. The model progressively removes noise through this process, eventually generating data close to the original data $\mathbf{x}_0$ from the noised data $\mathbf{x}_T$. The model is used to predict the statistics of $p_{\theta}$, and the reverse process is trained with the Variational Lower Bound (VLB) of the log-likelihood of $x_0$.

$L(\theta) = -p(x_0|x_1)+\sum_tD_{KL}(q^*(x_{t-1}|x_t,x_0)||p_{\theta}(x_{t-1}|x_t))$

\parlabel{Video Diffusion Model} also adds noise gradually to each video frame during the forward process. Thus, the overall process is quite similar to that of the Image Diffusion Model. However, maintaining temporal consistency is crucial for video data. Simply applying noise to individual frames makes it challenging to preserve the natural flow across consecutive frames. Therefore, the Video Diffusion Model incorporates additional constraints to consider the relationships between adjacent frames.

\parlabel{Open-Sora's diffusion model} uses spatial-temporal attention. Temporal consistency is learned through temporal attention, often utilizing sinusoidal positional encoding. However, Open-Sora employs Rope Embedding~\cite{su2024roformer} instead. It uses spatial-temporal attention by applying temporal attention immediately after each spatial attention, reducing computational costs. For stability, AdaIN~\cite{huang2017arbitrary} and layer normalization are applied to temporal attention, and QK-normalization~\cite{dehghani2023scaling} is applied to all attention layers.

Open-Sora~\cite{opensora} is an open-source video diffusion model. Figure \ref{Open_Sora_Architecture}
shows the structure of the Open-Sora model. It consists of the Text-to-Text Transformer (T5)~\cite{raffel2020exploring}, STDiT (ST stands for spatial-temporal), and the VAE. The T5 model was used for prompt embedding, and STDiT is based on the T5-conditioned DiT (Diffusion Transformer) structure of PixArt-$\alpha$, a text-to-image model. To adapt it for video generation, temporal attention was added. DiT converts latent representations into patches via a patchify layer and applies positional embeddings to all input tokens. For normalization, it uses adaptive layer norm (adaLN) blocks, which leverage timestep and label embeddings as shift and scale values, or adaLN-Zero blocks, which add a scale factor initialized to zero. The VAE is composed of both 2D and 3D structures, with the 2D VAE utilizing SDXL’s VAE~\cite{podell2023sdxl} and the 3D VAE based on the architecture of Magvit-v2~\cite{yu2023language}.



}
\bonote{\parlabel{Rectified Flow}
\label{sec:rflow}
}



\subsection{Challenges}

\isu{

\textbf{Challenge 1 - computation time}

\parlabel{Excessively Long Denoising Process} 
% Figure~\ref{fig:time-profiling} illustrates the time consumption for several diffusion processes.
% In diffusion process, denoising process requires the most time.
Figure~\ref{fig:time-profiling} illustrates the time consumption of several diffusion processes, with the denoising process performed by STDiT (Spatial Temporal Diffusion Transformer) requiring the most time.
% attention process
% attention process
In the attention mechanism of transformers, including STDiT, the computational complexity increases quadratically with the number of tokens, resulting in slower inference speeds.

% To address this issue, ToMe (token merging)~\cite{bolya2023token} has been proposed to increase the throughput of vision transformer models. 
% ToMe progressively merges similar visual tokens within the transformer and reduce inference time without additional training.
% Although this approach has been applied to diffusion models, ToMe was only used to reduce spatial tokens and did not merge temporal tokens for video diffusion models.
To address this issue, ToMe (Token Merging)~\cite{bolya2023token} has been proposed to enhance the throughput of vision transformer models. ToMe progressively merges similar visual tokens within the transformer, reducing inference time without the need for additional training. Although this approach has been applied to diffusion models, {ToMe was only used to reduce spatial tokens and was not applied to merge temporal tokens in video diffusion models}.

\begin{equation}
q(x_1, x_2, \dots, x_T | x_0) = \prod_{t=1}^{T} q(x_t | x_{t-1})
\label{eq:diffusion-forward-process}
\end{equation}
\begin{equation}
    p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))
    \label{eq:diffusion-reverse-process}
\end{equation}

% describe token merging
%
}





\isu{

\input{figures/ealry-stopping} 

\parlabel{Too Many Denoising Steps} As shown in Equation~\ref{eq:diffusion-forward-process}, ~\ref{eq:diffusion-reverse-process}, and Figure~\ref{fig:time-profiling}, the diffusion denoising process inevitably requires a significant amount of time, as it involves adding noise over multiple steps and then removing it over an equal number of steps during denoising. As a result, the diffusion process is not fast enough for real-time applications and is even more challenging on mobile devices, where memory and computational resources are limited.
% the diffusion denoising process inevitably requires a significant amount of time, as it involves adding noise over multiple steps and subsequently removing it over an equal number of steps in the denoising process.
% As a result, the diffusion process is not fast enough to operate in real-time applications and it is more challenging on mobile devices, where memory and computational resources are limited.

To reduce the time consumption of the diffusion denoising process, blah blah blah blah blah blah blah blah blah blah (DESCRIBE early stopping, consistency models, etc).
However, these approaches have only been applied to image diffusion models, and there are no existing methods where they have been applied to video diffusion.}

\bonote{

As conventional diffusion process aims to predict noise by training model to predict added noise for the given timestep which grants access to gradual reverse diffusion process~\cite{ho2020denoising}, which can be also interpreted as "denoising" steps to predict momentary trajectories of given distribution at certain timepoint. This decision led diffusion model to require hundreds and thousands of denoising steps, which is the main time bottleneck for the task even after it could be optimized by having deterministic target distributions~\cite{song2020denoising}. Later on, as "predicting trajectories depending on give timepoint" could be naturally expressed as an ODE problem, relatively recent models based on ODE solvers~\cite{lu2022dpm, zheng2023dpm} have been trained to predict the velocity of certain point associated with time and positional data points, it enables us to directly approximate the unknown trajectory function of time and position by utilizing classic ODE solving methods such as Runge-Kutta method~\cite{butcher1987numerical} or Euler integral methods with much less steps with much more promised consistencies. 

Rectified Flow~\cite{liu2022flow}, one of the recent proposal which redefined objective function to induce model to directly approximate the straight mapping path between the initial distribution and target distribution. For the image generation diffusion models, Rectified Flow could successively achieve the single-step diffusion without losing notable result qualities. For the video generation model~\cite{abdi2007singular, opensora}, however, due to the variance stacked through time frame, trajectory could not be fully expressed with single velocity at initial point which led Open-Sora~\cite{opensora} to require at least 30 steps for the acceptable results. Generally, 30 steps are relatively usable step numbers for the server-graded environments, which will only take one or two minutes for whole sampling process. Mobile environments, however, single step can take more than one minutes due to the constrained compuational resources such as relatively low-performanced processing units and inabundant memory budgets. 

Naturally, additional methodologies to reduce the sampling time on target device is heavily required, and we will explain our surprisingly-simple solution by exploiting the trained flow fields already acquired from the training or distillation with Rectified Flow models, without additional training process or data points to calibrate in ~\Cref{section3}, proposed and denoted as Linear Proportional Leap(LPL) in this work.
}



\note{
\parlabel{Challenge 2 - memory (we solve this in sec 4)}
}

\isu{

\parlabel{Limited Memory Resources}



}


\note{
\parlabel{Challenge 3 - implementation (we solve this in sec 5)}
}

\note{LIST UP WHAT ARE THE CHALLENGES YOU HAVE ENCOUNTERED WHEN DEPLOYING SORA ON IPHONE 

(IMPLEMENTATION CHALLENGES ARE FINE)

WE WILL SELECT SOME OF THEM AND INCLUDE THEM IN THE PAPER
}


\subsection{Overview}

\jjm{
As we chose Open-Sora as the base, our model proceeds similarly to Open-Sora's Pipeline. There are three main models functioning during inference. First, prompt embedding and encoding of the text prompt that came in as input from the T5 model are performed. The size of the T5 model is substantial, making it challenging to run on mobile devices in a single operation. To address this, we divided the T5 model into several blocks, considering its layered structure, and only loaded the necessary portions for operation.

The encoded text output from the T5 model then becomes the input for the STDiT model. The STDiT model performs the denoising task of the actual diffusion model and ultimately generates the latent video output. The token merging technique and RFLOW Leap were applied for quick reference. Since STDiT also has limitations in that it is large enough to run in a mobile environment at once, STDiT was divided into several blocks and implemented to operate only the necessary parts in memory.

Finally, the latent video output is input into the VAE. The VAE performs the decoding task, converting the input latent representation into a fully reconstructed video format. The VAE also creates memory issues for the entire model to run at once. To solve this, the model was divided into multiple steps. Unlike the T5 or STDiT, one unique aspect of the VAE is that the same model is repeatedly utilized.
}

\bonote{

% Main idea : If training/distillation with rectified flow straightens the trajectory between initial distribution and the target distribution, it is plausible to stop sampling process at certain point which is proportionally corresponds to reverse diffusion process of lower sampling step numbers \-e.g., 40-th step when 50 steps are full denoising step numbers \& fourth step when five steps are the full number for the denoising schedule; and begin from the mid-denoising steps with much lower step size, which enables us to reduce required step number into $N-K/N + K$ instead of $N$ steps when $N>>K$.

\subsection{Linear Proportional Leap (LPL)}

To recall, reverse diffusion process can be expressed as iterative procedure with multiple sampling steps, which is fundamentally transferring certain initial normal distribution into desired distribution corresponds to the input prompt. Multiple ODE-based diffusion models~\cite{zheng2023dpm,lu2022dpm} re-formulated this procedure to train neural network to predict the drift(velocity) at given time point and construct the trajectories of distribution from initial point to target point by leveraging the conventional ode solvers~\cite{runge1895numerische, yang2023diffusion}. 

As forementioned in \cref{sec:rflow}, Rectified Flow~\cite{liu2022flow} simplified the process of transferring the data from initial point to target point, by training their model to predict the drift that is set to drive the flow to follow the direct linear path between target and the initial point.

The model could predict the direct direction toward target point at any point on the trajectory, model could theoretically and often empirically achieve few-step denoising process in diffusion-based image generation models, without significant performance drops, despite of the limitations that actual trajectory could not be fully represented with single drift, yet it could be represented with relatively less number of drifts and forcing model to focus on direction of direct path between initial point to target point surely aided model to straighten the mapping trajectory.

As mentioned in previous work addressing the rectified flow based diffusion models for high-definition image generation~\cite{esser2024scaling}, the complexity of target data distribution was directly connected to the required number of steps for the satisfactory results, while shortly mentioning that performance of rectified-flow based model can show high efficiency over other methods at fewer sampling steps.

Similarly, in diffusion-based video generation models, Rectified Flow could not achieve single-step video generation without problem, because of additional temporal variance occured from the video and complexity of target distribution. In Open-Sora~\cite{opensora}, the authors currently employed Rectified Flow, which enabled them to use 30 sampling steps for decent results in reasonable time.

In these circumstances, we propose simple solution to overcome current limitation for the number of sampling steps by fully utilizing the trait of acquired trajectory by simply using the proportionally corresponding step size ($dt$) to \textbf{finish} the sampling loop early. 

As a implementation of simple euler method, current integration of drift with trajectory($z$) can be easily computed and updated as below, where $M$ is the number of sampling steps, $t$ is the sampled timestep, and $z$ is the current position while $v$ is the drift predicted by our model.}

\note{WHY $t$ is between 0 and 1? WHAT IS $X_k$? : Added answer to paragraph below. }
\newline
\bonote{The timestep is being sampled from normalized time duration between 0 and 1 of reverse diffusion process with respect to the conventional expression of neural ode based diffusion models. 0 is the time when the data is full of noise which can be also written as the beginning of the sampling process, and 1 is when the data is at the desired data distribution which can be interpreted as the end of the sampling process. $X_k$ below represents the current k-th position on the trajectory, sampled by the linear interpolation from corresponding timestep $t_k$.}

\bonote{
\begin{gather} 
    t \in [0,1], \quad t_k \in \{t_i\}_{i=1}^M\\
    X_k = t_k*X_1 + (1 - t_k)*X_0 \\    
    dt_k = 
\begin{cases} 
t_k - t_{k+1}, & \text{if } t_k \neq t_M \\
t_k, & \text{if } t_k = t_M
\end{cases}\\
    z_k = z_{k-1} + v(X_k,t_k)*dt_k
\end{gather}

\textit{dt} is determined by sampled timesteps, which is scaled with target resolution and shifted with temporal reduction factors. Modifying the value of \textit{dt} arbitrarily at inference time leads to unsatisfiable results, as step size itself is the part of sampling process, establishing the strength of current drift to be applied at approximated trajectory, when target trajectory is nonlinear and complex. 

Rectified Flow, however, biases the trajectory as straight as possible, which enables us to pursue a more audacious strategy which we denoted as \textbf{Linear Proportional Leap(LPL)}. If we are close enough to the target data distribution during the sampling process, our path is straight enough to simply step for the remaining $t_k$, not utilizing the difference between two time points. To elaborate, we can finish our sampling process just by utilizing $t_k$ as our dt at the middle of sampling process instead of the rules specified in Eq.(7) .
% \sout{By arbitrarily selecting the N > 1 which is the factor of original step number, we can efficiently skip the last portion of sampling steps.}



\textbf{To support our idea, we provide the visualization of cosine similarities computed between partial model output at corresponding denoising step and the counterpart for the previous denoising step. As you can clearly confirm, each denosing steps for 30/50 steps show certain plateau of value, showing that after certain point, the path toward the desired distribution is almost straight, allowing us to move toward that direction with much less step numbers, with bigger step sizes.}

% \begin{figure}[!h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{figures/cosine_similarity.pdf}
%     \caption{Example visualization of cosine similarities through denoising steps, we computed cosine similarities at designated frame and channel. ex.) first frame, first channel of $n$-th step output and first frame, first channel of ${n-1}$-th step output. Red line indicates the graph of cosine similarities for 30 steps, blue line indicates the cosine similarities for 50 steps.}
%     \label{fig:enter-label}
% \end{figure}



  % \hfill%

  % first frame, first channel of $n$-th step output and first frame, first channel of ${n-1}$-th step output. Red line indicates the graph of cosine similarities for 30 steps, blue line indicates the cosine similarities for 50 steps.}  



\textbf{From these observations, we conducted multiple experiments in experiment section to show the amount of efficiency that can be achieved by simply finishing the denoising steps early with bigger step sizes, which corresponds to the remaining amount of required steps, that fully utilizes the straightness of trajectory we achieved by employing Rectified flow.
For practical implementation, we determined our tolerance of improvements in cosine similarities between two partial outputs at consecutive denoising steps as a hyperparameter, and finished our denoising step with given drift for the rest of step size($dt$) if the cosine similarities indicate that direction of our trajectory is straight enough to proceed in given drift, and not improving any longer.
}

To clarify further, following trajectory with each timestep could be computed as Eq.8. If we expand this to all timesteps, following equations can be derived.
\begin{gather}
    z_k = z_0 + \sum_{i=1}^{k-1}({t_i-t_{i+1}})*v_i+t_k*v_k\\
    z_k = z_0 + ((t_1 - t_2) * v_1) + ((t_2 - t_3) * v_2)...t_k * v_k
\end{gather}
If we assume we will stop our sampling process at step $n$, then naturally $z_n = z_0 + \sum_{i=1}^{n}({t_i-t_{i+1}})*v_i$. Then, if we use identical drift to remaining steps, the equation can be reduced as follows.
\begin{gather}
    z_k = z_n + v_{n+1}\sum_{i={n+1}}^{k-1}(t_i - t_{i+1}) + t_k*v_{n+1}\\
    z_k = z_n + v_{n+1}*(t_{n+1} - t_{n+2}...t_{k-1} - t_k) + t_k*v_{n+1}\\
    z_k = z_n + v_{n+1}*t_{n+1}
\end{gather}
To simplify and clarify, replacing $dt$ into certain timestep instead of difference between the sampled timesteps will invoke identical effect to the sampling process, immediately finishing the sampling process as value of certain timestep is equivalent to the required remaining steps until the end of sampling process, which benefits huge amount of time.

\sout{For example, if initial sampling number is 30 and N is 3, we can proceed our regular sampling steps for first 19 steps, and finish our sampling steps with the 20th step, simply by using the 20th timestep as our $dt$ (step size) to leap our current position on trajectory toward the direction of desired distribution. Thanks to the straightened trajectory with straightness depending on the closeness toward the end of the process, adding up current velocity with product of remaining step size gives you a result that is closer to the result you can achieve with full-sized sampling process.}
}


\note{I DON'T UNDERSTAND WHY 21st step given N=3?}.
% \newline
% \bonote{Answer in comment. Paragraph above will be re-written soon.Thank you for the feedback.}
% to be honest, I thought being proportional is the key here at first, therefore I thought I had to do the whole 2/3 regular steps then proceed the rest of 1/3 with the last timestep of timesteps computed for 3 total steps. 

%However, after formulation, I realized that being proportional itself does not help, as I realized that size of each timestep directly represents the required remaining step size. we can just finish the sampling at 20-th step, just by computing z_20 = z_19 + v_20*t_20.


\bonote{This enables us to skip one-third of inference time \textbf{without any additional training} or data to calibrate, as we only leverage the nature of the trajectory acquired by the model trained with the Rectified Flow.}

\subsection{Temporal Dimension Token Merging}

\kyu{The Open-Sora model have huge computation time in the STDit block on-device. In general, to address this issue, there are various techniques to reduce a model’s computation time, such as pruning and quantization. However, these methods often require retraining or specialized hardware to implement. In contrast, since the high level idea of token merging~\cite{bolya2023token} is to reduce the input size at some layers, such as attention layers, token merging offers a way to reduce computation time without any additional time for retraining or specific hardware. 

In STDit3 block of Open-Sora, a cross-attention requires more computation time than self-attention. So, it's essential to develop the token merging technique to cross-attention. However, existing token merging techniques to cross-attention has resulted in poor performance. Even, applying existing token merging techniques to self-attention in STDit3 led to severe performance degradation.

To address the performance bottleneck caused by cross-attention in STDit blocks, we propose a novel token merging strategy which we denoted as \textbf{Temporal Dimension Token Merging (TDTM)}. Unlike existing token merging techniques that more focus on the spatial dimensions, TDTM uses the temporal property of the video's frame to reduce computational time.

The TDTM process is illustrated in \Cref{fig:TDTM-fig}. Our hypothesis is that successive frames have similar values to each other. So, two consecutive frames in the input data will be merged via temporal dimension by averaging to create a single frame without additional process to calculate the similarity between frames. This operation reduces the number of tokens (50\%) while maintaining the most essential temporal information. This reduction in the size of input data affects the cross-attention calculation time. Finally, the output that goes through cross attention replicates the dimensions of each frame and restores the dimensions back to the original dimensions.

For example, given the input tokens denoted $T_{in}$ of cross-attention, the input shape of $T_{in}$ is [$B$, $ST$, $C$] where $B$ is batch size, $S$ is the number of pixel patches and $T$ is the number of frames. To apply TDTM to $T_{in}$, the input shape of $T_{in}$ should be rearranged to [$BS$, $T$, $C$]. After rearrangement, we can apply TDTM to reduce the input size of $T_{in}$ by averaging two adjacent frames:
\begin{gather}
    T_{merged} = TDTM_{merge}(T_{in})\\
    TDTM_{merge}(T)[i] = \frac{1}{2} (T[:,i,:] + T[:,i+1,:])
\end{gather}
where $T_{merged}$ is an input token to which TDTM has been applied, in which two adjacent tokens are averaged into one dimension, its shape is [$BS$, $T/2$, $C$]. After then, in order to apply cross attention, we must be rearranged according to the input dimension of cross attention, so it becomes the dimension of [$B$, $ST/2$, $C$]. In order to restore the original input dimension after going through cross attention, each frame is duplicated to become two adjacent frames:
\begin{gather}
    T_{unmerged} = TDTM_{unmerge}(Cross(T_{in}))\\
    TDTM_{unmerge}(T)[2i]= T[:,i,:]
\end{gather}
where $T_{unmerged}$ is an the token unmerged result of cross-attention, its shape is [$BS$, $T$, $C$] and $Cross$ is the cross-attention function. As with merging, rearrangement is necessary before and after applying unmerge.

But there are also some performance issue that light trembles or flickers slightly when it's applied to all sample steps. We assume that the cause of this issue is as follows.

As the sampling progresses, the noise values that need to be removed are slightly different for each frame. In other words, if token merging is applied to the latter, it seems to have a significant impact on the noise values that move slightly, which causes a decrease in performance. This characteristic occurs when token merging is applied to the time dimension in the video diffusion, which is not seen in the general image diffusion.

However, the other side if it, since the noise values in the early part are not subtle and move roughly, applying token merging in the early part will not have a significant impact on performance.

Based on the hypothesis above, as a result of conducting various experiments(need to experiment table), if token merging is applied to cross attention and token merging is applied only to the first 15 steps if the total sampling steps are 30 steps, and token merging is not applied to the remaining 15 steps, the performance drop was less than when token merging was applied to all steps and the flickering issue no longer appears.
} 

\subsection{Concurrent Inference}

\jjm{
\parlabel{Concurrent Implementation (CI)}
Partitioning was applied to run T5 and STDiT models on a device with limited memory to divide the model into multiple blocks. When performing inference with the partitioned model, each required block must be loaded sequentially before computation, which increases inference time compared to the original model.

\Cref{fig:partitioned_STDiT} illustrates the performance graph of partitioned STDiT. It shows that after loading each block of STDiT, the GPU processes the operations of each block. To address the issue of increased inference time due to the sequential execution of tasks, a Concurrent Implementation (CI) was applied, leveraging the ability of mobile devices to use both the GPU and the CPU. In CI, the CPU loads the $i+1$th block while the GPU computes the $i$th block. The CI process proceeds as follows: the 1st and 2nd blocks are loaded concurrently, with the loading of the 1st block completed first. Subsequently, the prediction of the 1st block and the loading of the 2nd block occur simultaneously. This approach continues such that the prediction of the $i$th block and the loading of the $i+1$th block overlap until only the prediction of the final block is performed. \Cref{fig:CI_STDiT} presents the performance graph of STDiT with CI. It demonstrates that the GPU is active almost continuously, as indicated by the increase in blue lines from one to two, signifying that loading and computation operations are executed in parallel.

The CI method achieved a notable reduction in inference time. The inference time for STDiT was reduced by about 25\%, and for the T5, it was decreased by around 15\%. However, as T5 is executed only once during inference, the impact of CI on enhancing the inference speed appears less pronounced compared to STDiT, which repeats up to 30 times.
}

\subsection{Discussions and Limitations}

\jjm{
\parlabel{Time}
Currently, video generation takes too long. For instance, processing on an A6000 GPU takes approximately \textbf{?} seconds, while the on an iPhone 15 Pro takes approximately \textbf{?} seconds. The integration of the iPhone's Neural Processing Unit (NPU), along with the application of lightweight techniques such as optimization and distillation, could considerably reduce this processing time. However, there are several limitations to applying these points, leaving them as areas for future research.

\parlabel{Quantization and Model Optimization (About additional training)}
We don't apply quantization and model optimization so much as pruning and knowledge distortion in this work. The main reason is that the reduction in video quality outweighs the benefits gained from a lightweight model. Additionally, there is a significant lack of resources; the dataset available for training is insufficient for applying advanced techniques, and the number of GPUs required for comprehensive training is inadequate. This remains a potential direction for future work as we await more substantial resources.

\parlabel{Why not using iPhone's NPU? Why using GPU?}
We expect that On-device Sora can be accelerated by using the NPU. However, it is not easy to fully utilize NPU because the current software framework does not fully support the state-of-the-art diffusion-based video generation models. We leave this for furture work.

\parlabel{Is it possible on Android or any other device?}
The challenges in \Cref{sec:challenges} are not unique to iPhones but is a common issue for all on-device devices. Therefore, the solutions(\Cref{sec:ours1}, \ref{sec:ours2}, \ref{sec:ours3}) aimed at addressing this challenge can be extended to most devices. This adaptability seems feasible through modifications in the implementation process to ensure it aligns with the specifications and capabilities of each device.

\parlabel{Why can only produce videos of fixed shapes? Why does it not support dynamic output?}
Current model does not support dynamic output due to the use of tracing in the process of converting models for compatibility with the IOS environment. Since tracing converts the model in a way that stores a fixed flow, it cannot convert all the branch points of the model. Attempting to convert all branch points would significantly increase the model size, making it unsuitable for on-device environments. The solution to this issue lies in employing alternative conversion methods, but the options currently available are limited. Therefore, this will remain as a future direction, awaiting advancements in conversion tools such as Executorch.

\parlabel{Isn't it impossible to learn more on mobile devices?}
With current technology, training large models, such as video generation models, on most mobile devices is extremely challenging \textbf{(find related papers)}. Therefore, this remains an area for future work until advancements in related technologies are made.

\parlabel{Why Open-Sora? The one without promising performances?}
We selected Open-Sora as the backbone model primarily because it is open-source. Most existing video diffusion models have not provided essential components such as training and reference codes, detailed structural documentation, or properly disclosed pre-trained parameters. Consequently, models with potentially better performance than Open-Sora could not be utilized due to these limitations.

\parlabel{Why Low resolution? with all that time and resources?}
Currently, we have utilized the iPhone 15 Pro, but it has only been capable of generating lower-resolution outputs. While the iPhone 15 Pro has a memory size of 8GB, the actual space available for our use was limited to approximately 3GB. If more memory were accessible, the peak memory would be smaller than the memory size even if the resolution was increased. However, generating higher-resolution images was challenging with just 3GB of available space. One potential solution is to free up more memory by reducing the size of the model itself; however, employing lightweight techniques significantly degrades video quality.
}

\subsection{Dynamic Loading}

\kyu{We can accelerate the model inference latency using by Concurrent Inference. But there is room to further reduce execution time. As illustrated in \Cref{fig:Partitioned_CI_STDiT}, the model load takes more time than its inference. It means that the prediction operation should wait until the loading operation is finished. Based on this, we propose Dynamic Loading, which determine the model blocks for unloading.

The available memory of the device can vary with run-time based on the system status and configurations of applications running on the device. For example, if we make the high resolution and long video, then the required memory for model inference grows and the available memory decreases. On the other hand, when video is generated using appropriate configurations, some memory resources will remain available. If so, we can use these available memory resources to maintain some model blocks in loaded states. This characteristic of maintaining some model blocks in loaded states has advantage in processes like STDiT that takes many steps to generate a video, loads and reuses the same model for each step.

Depending on available memory, keeping some STDiT blocks loaded can eliminate the task of loading those model blocks in the next step, leaving some room for further reduction using Concurrent Inference. To do this, we dynamically measure the available memory per a application and computing memory of a STDiT block at the first step. Then, the memory size to keep some STDiT blocks in a loaded state is the available memory minus the compute memory, and we place the blocks in a loaded state that fits that memory size. 

\Cref{fig:partitioning} illustrates the process of Dynamic Loading, which maintain the first four blocks in loaded state. This means that unlike other blocks, the first four blocks do not require a load operation after the first step, which can reduce latency. The number of model blocks is dynamically adjusted based on available memory.

The latency reduction $r$ of \Cref{eq:concurrent_inference} is modified for Dynamic Loading with Concurrent Inference as follows:
\begin{equation}
    r = (b -d) \cdot \min (l, e) + d \cdot e + \alpha \cdot (1 - \frac{d}{b})
    \label{eq:dynamic loading-3}
\end{equation}
where $d$ is the number of maintained blocks in loaded states. 

The large number of blocks ($d$) significantly reduces block loading overhead. This approach effectively eliminates model inference overhead while taking into account device memory constraints, unlike Concurrent Inference.
}

\subsection{Implementation}

\kyu{
We implemented the proposed On-device Sora on iPhone 15 Pro. But it's not easy to execute the model on the mobile device because of the huge barriers, such as various difficulties of model conversion and implementing converted model in the iPhone.}

\subsection{Converting Pytorch to CoreML}

\note{WRITE AGAIN AS A FORMAL WRITING}

\note{PROVIDE AN EXAMPLE (CODE? FIGURE?)}

\note{ADD CITATIONS}

To execute the model implemented with Pytorch on mobile device, \bonote{it is inevitable to convert the model into Apple-friendly file-system structure for storing a model, MLPackage with network tracing or scripting for the operation of the model. In this process, checking the code if it is convertible and modifying it adequately requires excessive amount of time. Unfortunately, we cannot know which function will cause problem during the conversion and the actual runtime execution without exhaustive trials as such kind of issues were not directly addressed by the service providers.} 
\kyu{There are several cases where the conversion could not work properly. First, there are several functions which cannot be matched accurately between two different ML frameworks, Pytorch and CoreML. For example, external libraries such as xformer~\cite{xFormers2022} or algorithms using cache for their faster inference could not be converted properly.} \bonote{To elaborate, frameworks which requires CUDA-level backend functions cannot be converted into CoreML functions due to the hardware differences. Moreover, several functions were mapped and converted inaccurately, which led to the unwanted computation outputs such as NaN or wrong values during the forward propagation.}\kyu{Second, CoreML doesn't support tensors with high ranks more than 5. It means that if certain computation graph requires tensor to be more than 6-rank tensor, like rearrange and Group Normalization when they got 5-rank tensors due to their computation requires temporary expansion of dimensions, the model conversion cannot be done. The code needs to be rewritten so that it can be calculated under 6 dimensions.} 

\bonote{to myself : review CI/DL and extend.}
% \bonote{\sout{Third, considering the difference between the memory constraints of the mobile devices and the memory requirements of load and computation for model execution, the model which have huge model size should be divided into multiple small model instances modified to proper size. So, we had to divide T5, STDiT and VAE into multiple model instances adequately. By implementing such granular offloading of our models, we could build a baseline to overcome the resource constraints of target mobile device before utlizing the proposed methodologies.}} \note{REDUNDANT: MENTIONED IN CONCURRENT INFERENCE WITH DYNAMIC LOADING}

\subsection{Implementing models on iPhone}

\note{WRITE AGAIN AS A FORMAL WRITING}

\note{PROVIDE AN EXAMPLE (CODE? FIGURE?)}

\note{ADD CITATIONS}

% Parts of entire Open Sora process are improper to convert the mlpackage, such as noise scheduler, tensor to video converter. So, it's needed to directly implement these parts with libraries provided by Apple. Apple provide a new MLTensor class in iOS18 that is able to implement tensor computations, however, it does not yet provide as many functions for tensor operations as PyTorch such as indexing the tensor, and many of the methods that were generally available in PyTorch do not work. So it took a lot of trial and error to implement logic like a noise scheduler and tensor to video converter. In addition, Tensors that dynamically change with input, such as attention bias, are fixed to one shape after conversion and cannot be implemented with typical model conversion. So, we needed an additional implementation for attention bias using with MLTensor to utilize a model that takes a dynamic shape as input based on resolution or image size.

\bonote{
Numerous components including the noise scheduler or sampling pipeline and the tensor-to-video conversion mechanism, cannot be seamlessly translated into the .mlpackage format of CoreML~\cite{apple2023}, as they are not frequently utilized in machine learning frameworks. Therefore, these components of our code necessitate additional implementation in \textit{Swift} while adequately utilizing libraries provided and authenticated by the Apple.
% Even if iOS 18 recently introduced the class of MLTensor to support tensor computations and functions, this class presently lacks the extensive functionality offered by well-established frameworks such as PyTorch, i.e., tensor indexing.
Additionally, tensors that dynamically adapt to input parameters, such as attention bias tensors, become constrained to fixed shapes following conversion, thereby making the direct implementation of models reliant on dynamic tensor shapes impractical. To address this limitation, custom implementations were developed using the MLTensor class, facilitating support for models with input shapes that vary based on resolution or image size. \kyu{For fast inference, we set the compute unit of T5 and STDiT blocks to GPU. But, if we set a compute unit of the VAE, the input of which is the dynamic shape for different video resolution, to GPU, then it use the high memory over the available memory. So, we set the compute unit of it to CPU to avoid the high memory issue.}


}

\subsection{Experiment: Video Quality}

\isu{
% In this section, we present a comparative analysis of the videos generated by Open-Sora and On-Device Sora that is implemented on iPhone 15 Pro.
% We compare Open Sora and On-Device Sora on 116 $\times$ 221 and 256 $\times$ 256 resolution 51-frame videos, using the scripts provided by VBench~\cite{}.
% In this section, we conduct a comparative analysis of videos generated by Open Sora on a server and the videos generated by On-Device Sora implemented on iPhone 15 Pro. 
In this section, we present a comparative analysis of videos generated by Open Sora on a server and those generated by On-Device Sora implemented on an iPhone 15 Pro.
The evaluation focuses on 68-frame videos at resolution 256 $\times$ 256, utilizing the benchmarking prompts provided by VBench~\cite{huang2024vbench}. The prompts consist of 100 each on the topics of animal, architecture, food, human, lifestyle, plant, scenery, and vehicles.
% To evaluate the performance of text-to-video generation, we employ a range of metrics provided by VBench~\cite{} evaluating temporal quality and frame-wise quality, e.g., subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality and imaging quality.
To evaluate the video quality of text-to-video generation, we employ a range of metrics provided by VBench~\cite{huang2024vbench} to assess both temporal and frame-wise quality. These metrics include subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality, and imaging quality.
Since our purpose is to deploy Open Sora on device without significant performance degradation, and Open Sora and On-Device Sora generate similar scenes, we do not use a video condition consistency metrics that evaluate consistency with text prompts.

Table~\ref{tab:main-experiments} and Figure~\ref{fig:end-to-end-evaluation-2} presents a comparisons of generated videos across several evaluation metrics. 
% Compared to Open Sora, On-Device Sora enables execution directly on the device without significant performance degradation while significantly enhancing dynamic capability on average 0.067.
Compared to Open Sora, On-Device Sora enables execution directly on the device without significant performance degradation,  while achieving an average improvement in dynamic degree of 0.067.
Although there is a slight degradation in frame-wise quality, it remains negligible and is caused by the quantization of T5.



% FVD is a metric for evaluation of video quality.
% FVD is a metric for evaluating video quality that considers both spatial and temporal characteristics, measuring the distance between the quality of generated videos and original videos. 

% Table~\ref{tab:main-experiments} compare generated videos for several metric, i.e., FVD (Frechet Video Distance), FID (Frechet Inception Distance), and IS (Inception Score).


Figure~\ref{fig:end-to-end-generated-frames} compares the videos generated by Open Sora and On-Device Sora for the prompts. 
For prompts, On-Device Sora generates dynamic and realistic videos.
For the prompt "a stack of dried leaves burning in a forest", Open Sora generated video frames with firewood stacked in a square shape and a fire behind it while On-Device Sora generated video frames with a bundle of dry leaves on fire. 
Additionally, for the prompt "close-up of a lemur", On-Device Sora generated video frames that are closer to a lemur and show the lemur's head turning to the side, while Open Sora generated frames with no noticeable change.
}

\subsection{Experiment: LPL}

% \isu{
% Table~\ref{tab:early-stopping} compares the video quality and speed up depending on the points at which linear proprtional leap (LPL) is applied during the 30-step denoising process on NVIDIA A6000 GPU. In example, 16/30 (15+1/30) indicates LPL after 15 steps. Dynamic means that blah blah blah blah blah.
% The generated video is 68 frames, 256 $\times$ 256 resolution, and evaluated via SSIM~\cite{}, FVD~\cite{} and VBench~\cite{huang2024vbench}.
% }


% \bonote{GIST : Table 3 shows that speedup is nicely done with less steps, but exhibits very tiny difference at performance, which is comparable and viable. Yet, explain that dynamic degree was influenced by the seeds and categories, which should be nice excuse as shown in table 2. Explain each setting with speedup(avg of 100 prompts) and dynamic settings, and show that optimal point is between these settings. If possible, talk about average shooting points, which was 17 xx. Moreover, explain the partial flickering which could not be successfully addressed by the vbench.}
\bonote{In this experiment section for Linear Proportional Leap(LPL), we utilized the vbench~\cite{huang2024vbench} identical to the end-to-end experiments and conventional video quality metrics including SSIM~\cite{fan2019metrics} and FVD~\cite{unterthiner2019fvd} which will work as a similarity metric and reconstruction metric compared to our baseline, Open-Sora~\cite{opensora}. The Speedup column has been measured by comparing the average of naive computational time for the denoising steps from one hundred different prompts and the computational time where different settings for Linear Proportional Leap have been applied. To show the relativity between the experimental settings for the moment where Linear Proportional Leap activates and conventional metrics for video qualities, we conducted our experiments at four deterministic LPL settings  which corresponds to the occasions when Linear Proportional Leap is activated at certain iterations where $1/2$, $1/3$, $1/4$ and $1/5$ left until it finishes the full denoising steps, which is set to 30. While the $1/2$ setting which shows the 1.94$\times$ acceleration for the denoising process, did not  showed significant performance degradation compared to the $30/30$ setting, which is the naive setting where Linear Proportional Leap did not activated. The results at deterministic settings indicates that earlier activation of LPL will invoke slightly degraded results at dynamic degree, which stands for the movement in the generated videos, while showing that most of the metrics are relatively consistent among different settings. Despite of relatively robust performances, activation of LPL reduces denoising time linearly, as it does not requires additional burdening computations, and reduces the number of steps itself. Moreover, we demonstrate the performance of LPL when it is activated depending on the cosine similarities of consecutive outputs, by conducting the identical experiment for the \textit{dynamic} setting on \cref{tab:early-stopping}. Empirically, we set our minimal number of steps, tolerance and number of steps for the tolerance as a hyperparameter, which is set to 50$\%$, $10^{-4}$ and 3. We exhibits the examples for the generated videos with identical prompts and different LPL settings in \cref{fig:lpl-end-to-end-generated-frames1}. As we can confirm, that generated video well preserves its desired distribution even if we only had partial denoising steps, which differentiates the results when the denoising steps have been initialized to the reduced number from the beginning. \textbf{Yet, the generated video with LPL at high acceleration setting lacks portion of minor details compared to the naive generation, while it contains regional flickering at objects with high movements depending on the prompts occasionally.} Still, all of the generated videos can be marginally told that they all describes identical targets, while we can also confirm that most of the metrics from vbench are consistent over different number of computed steps.
}

\subsection{Experiment: TDTM}

\isu{
Table~\ref{tab:TDTM} compares the video quality and speed up across difference number of temporal dimension token merging (TDTM) steps in denoising process on NVIDIA A6000 GPU.
In Table~\ref{tab:TDTM}, the merging steps denote the number of steps to which TDTM is applied, out of a total of 30 steps. For each merging step, the quality of a 68-frame video with a resolution of 256 $\times$ 256 is evaluated using SSIM~\cite{wang2004image}, FVD~\cite{unterthiner2019fvd}, and VBench~\cite{huang2024vbench}.
Increasing the number of denoising steps to which TDTM is applied results in consistent improvements in speedup, while maintaining stable quality metrics as assessed by SSIM, FVD, and VBench.
A slight decline in performance was noted in the dynamic degree evaluation metric of VBench, representing an inherent trade-off resulting from the merging of temporal dimensions.
Consequently, achieving an optimal balance between video dynamics and speedup is essential.
% However, a slight decrease in performance was observed in the dynamic degree evaluation metric of VBench, which is an inherent trade-off caused by merging temporal dimensions.

\note{
<< MOVING TO EXPERIMENTS? Based on the hypothesis above, as a result of conducting various experiments(need to experiment table), if token merging is applied to cross attention and token merging is applied only to the first 15 steps if the total sampling steps are 30 steps, and token merging is not applied to the remaining 15 steps, the performance drop was less than when token merging was applied to all steps and the flickering issue no longer appears.>>}

\isu{
\Cref{fig:tdtm-generated-frames} demonstrates one of the generated video frames for the steps where TDTM is applied, out of the total 30 denoising steps.
"0\%" means Open-Sora without TDTM applied, and "66\%" means Open-Sora with TDTM applied to 20 steps out of a total of 30 steps.
Even as TDTM is applied to more denoising steps, the quality of the video frames is maintained.
}

\note{The experimental results (Sec. \ref{sec:experiment}) show that such \note{flexible} Temporal Dimension Token Merging effectively manages the noises (e.g., \kyu{dynamic degree}) in videos.}

% \kyu{We evaluate the video quality and speed up across different merge steps in \Cref{tab:TDTM}. As merge steps increased, }
}

\subsection{Expeirment: Video generation latency}

\isu{
% Table~\ref{tab:execution-time} compares the video generation times when using and not using the methods we proposed, e.g., temporal token merging, linear proportional leap, concurrent inference and dynamic loading.
Table~\ref{tab:execution-time} presents a comparison of video generation latency with the application of our proposed methods, including linear proportional leap (LPL), temporal dimension token merging (TDTM), concurrent inference (CI), and dynamic loading (DL). 'All' indicates the case where we apply all the methodologies.} \bonote{LPL was activated on 15th step, computing 16 denoising steps total and TDTM was set to 30 steps in this experiment. Each experiments were reported with the mean latency from three independent runs. "STDiT" and "Total" in the Measurement column indicate whether the latency was measured solely for the denoising process or for the entire end-to-end pipeline, including computations for the VAE~\cite{doersch2016tutorial} and T5~\cite{raffel2020exploring}. \Cref{tab:execution-time} presents the results of integrated experiments, showing latency across different methodologies. \Cref{tab:execution-time} consistently demonstrates a significant reduction in latency for each algorithm, up to 3$\times$ less latency compared to \cref{tab:time-profiling}. Furthermore, the methodologies do not conflict with one another; instead, they synergistically enhance latency performance. At the lowest resolution, the denoising process can be completed in under five minutes when all methodologies are applied. 
}

\jjm{

\subsection{Related Work}

\parlabel{On-device Image Diffusion Model}
Attempts to run generative models on-device have increased due to the growing need for privacy protection, low latency, and resource-efficient applications. Before the introduction of video generation models, numerous attempts had already been made to implement image generation models in on-device environments. Among these, some efforts have focused on Image Diffusion models. SnapFusion~\cite{li2024snapfusion} minimizes model size while maintaining performance using a U-Net architecture and step distillation. It achieves performance comparable to 50 steps of Stable Diffusion v1.5 within just 8 steps, enabling image generation on mobile devices in under two seconds. MobileOne~\cite{vasu2023mobileone} enables fast image generation through an efficient architecture and over-parameterization. EdgeFusion~\cite{castells2024edgefusion} achieves high-performance text-to-image conversion on mobile devices by applying mixed-precision quantization and hardware-friendly optimizations. This approach maximizes the use of modern mobile NPUs and GPUs, significantly improving computational speed and energy efficiency. \cite{choi2023squeezing} focuses on optimizing large-scale diffusion models for mobile environments, specifically adapting the Stable Diffusion model for Android. \cite{chen2023speed} targets optimizing large diffusion models for mobile devices. It introduces techniques like FlashAttention and Winograd Convolution to reduce memory access frequency and maximize computational efficiency. MobileDiffusion~\cite{zhao2023mobilediffusion} reduces the computational burden of attention by limiting transformer blocks at high resolution and increasing them at low resolution, optimizing performance accordingly.

\parlabel{On-device Image (Non-Diffusion) Generation}
Since the rise of diffusion models, Image Generation has developed around the Diffusion Model. However, efforts to use alternative models persist, particularly in developing lightweight image generation models and on-device image models.
BlazeStyleGAN~\cite{jia2023blazestylegan} is a model that optimizes StyleGAN for real-time performance in resource-constrained environments, such as mobile devices. This research have shown that it is possible to produce high-quality images even with limited resources by optimizing the structure of the model and increasing the computational efficiency.
MobileSyleGAN~\cite{belousov2021mobilestylegan} is a lightweight StyleGAN model designed to generate high-quality images in resource-constrained mobile environments. By integrating efficient convolutional architectures such as MobileNet into StyleGAN, this model significantly reduces the parameters and computational load, enabling real-time image generation on mobile and embedded devices.
GAN Compression~\cite{li2020gan} focuses on compressing Conditional GANs efficiently to be applied in a limited environment. This research suggested a method of maintaining performance while reducing the size and amount of computation of the model by utilizing techniques such as knowledge distillation, architecture search, and model compression.
MFAGAN~\cite{cheng2021mfagan} proposes a memory-efficient Super-Resolution GAN for on-device environments. It is designed to reduce the memory usage of the model by combining various optimization techniques such as quantization, knowledge distillation, and pruning, and to generate high-definition super-resolution images in real-time even in a limited resource environment.

\parlabel{Video Diffusion Model (not on-device)}
Video diffusion models have significantly improved the quality of video generation, attracting significant attention from researchers. Make-A-Video~\cite{singer2022make} has overcome the limitations of data availability by pioneering the implementation of text-based video generation without text-to-video pair data. Snap Video~\cite{menapace2024snap} uses a scalable spatio-temporal transformer to effectively learn the interaction between text and video by generating video from text. Imagen Video~\cite{ho2022imagen} facilitates high-resolution video generation through a multi-scale diffusion architecture, providing high-quality visual representation. Video diffusion models~\cite{ho2022video} and stable video diffusion models~\cite{blattmann2023stable} extend image diffusion models to video by accounting for temporal consistency, proving efficient video generation while preserving frame coherence. Studies such as Dreamix~\cite{molad2023dreamix} expand diffusion models with video editing capabilities, applying conditional diffusion to video frames to support diverse editing tasks. \cite{harvey2022flexible} proposes a model for consistently generating extended video sequences, while \cite{blattmann2023align} introduces frame alignment techniques within latent space to enable high-resolution video synthesis. MCVD~\cite{voleti2022mcvd}, a masked conditional video diffusion model, supports the task of prediction, generation, and interpolation. Tune-A-Video~\cite{wu2023tune} and Text2Video-Zero~\cite{khachatryan2023text2video} show that fine-tuning a well-trained image diffusion model to a video diffusion model can lead to high-quality video generation.
To date, the development of Video Diffusion Models has spanned various domains. In addition to research focused on enhancing their standalone performance, studies have explored optimizing their performance under various conditions. QVD~\cite{tian2024qvd} introduced Post-Training Quantization techniques to optimize the computational efficiency and memory use of existing video diffusion models, and MagicVideo~\cite{zhou2022magicvideo} significantly reduced the computational amount and improved the generation speed by generating videos in latent space. CMD\cite{yu2024efficient} decomposed videos into content frames and low-dimensional motion latent representations, which reduced the computational cost of video generation, and SimDA~\cite{xing2024simda} increased the weight and efficiency of existing diffusion models through a simple and efficient adapter mechanism in video generation. However, to the best of our knowledge, there have been no studies about the models in resource-constrained environments, such as on-device applications.

\parlabel{Model Partitioning}

\parlabel{Token-Merging}

Token Merging: Your ViT But Faster~\cite{bolya2022token}

Token merging for fast stable diffusion~\cite{bolya2023token}

Efficient Vision Transformer via Token Merger~\cite{feng2023efficient}

Token Fusion~\cite{kim2024token}

VidtoMe~\cite{li2024vidtome}

Efficient Time Series Processing for Transformers and State-Space Models through Token Merging~\cite{gotz2024efficient}

\parlabel{what else?}
}
