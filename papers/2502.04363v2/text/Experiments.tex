\section{Experiment}
\label{sec:experiment}
\isu{We evaluate the performance of On-device Sora on 68-frame videos at 256×256 resolution, using 800 text prompts sampled from VBench~\cite{huang2024vbench} and 1,000 each from Pandas70M~\cite{chen2024panda70m}, and VidGen~\cite{tan2024vidgen}. To assess both temporal and frame-level video quality, we utilize VBench~\cite{huang2024vbench}, the state-of-the-art benchmark for text-to-video generation, which provides comprehensive metrics, including subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality, and imaging quality.} Additional experiments on another text-to-video generation model, \ie, \jjm{Pyramidal} Flow~\cite{jin2024pyramidal}, are provided in 
\Cref{fig:lpl_pyramid} and ~\Cref{tab:LPL_pyramid} in  \Cref{sec:extra_experiment}.

\subsection{Video Generation Performance}
\label{sec:ex_video_generation_performance}

\input{tables/end_to_end_experiments}
% To assess both temporal and frame-level video quality, we utilize \note{VBench~\cite{huang2024vbench} and ...}, the state-of-the-art benchmark for text-to-video generation, which provides comprehensive evaluation metrics, including subject consistency, background consistency, temporal flickering, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. The evaluation is conducted on 68-frame videos at 256×256 resolution, using text prompts provided by VBench~\cite{huang2024vbench}, consisting of %100 examples each across 
% eight categories: animals, architecture, food, humans, lifestyle, plants, scenery, and vehicles. 
We evaluate the quality of videos generated by On-device Sora on the iPhone 15 Pro~\cite{apple2023}, in comparison to videos produced by Open-Sora~\cite{opensora} running on NVIDIA A6000 GPUs.
\note{\Cref{tab:main-experiments} summarizes the averaged results that compare those videos evaluated with VBench~\cite{huang2024vbench}, including categories such as animals, humans and lifestyle. \Cref{tab:appendix-main-experiments} and \Cref{fig:end-to-end-evaluation-2} in \Cref{sec:extra_experiment} provide detailed full per-category results.} The results demonstrate that On-device Sora generates videos with quality nearly equivalent to Open-Sora in most metrics, exhibiting only a slight drop in frame-level quality, averaging \isu{0.03}, while achieving an average \isu{0.06} improvement in dynamic degree.

\Cref{fig:end-to-end-generated-frames} shows example videos, compared with Open-Sora~\cite{opensora}. For the prompt ``a stack of dried leaves burning in a forest", both On-device Sora and Open-Sora generate visually plausible videos, illustrating stack of burning dry leaves and forest in the background. Similarly, for ``close-up of a lemur", both produce descriptive videos: On-device Sora shows a lemur turning its head, while Open-Sora delivers a less dynamic yet visually comparable depiction.

\input{tables/early_stopping}

\input{tables/token_merging}

\subsection{Linear Proportional Leap}

\Cref{tab:early-stopping} presents the video generation performance and speedup of On-device Sora when applying Linear Proportional Leap (LPL) proposed in Sec. \ref{sec:ours1}. In the table, `LPL Setting' indicates the number of denoising steps used for video generation out of a total of 30 steps, while the remaining steps are omitted by LPL. We also evaluate a dynamic version of LPL, referred to as `Dynamic' in \Cref{tab:early-stopping}, which determines the number of denoising steps at runtime based on the cosine similarities between two adjacent drifts estimated using STDiT~\cite{opensora}. The dynamic LPL halts denoising when a predefined number of cosine similarity measurements fail to improve beyond a $10^{-4}$ tolerance, after a designated minimum number of denoising steps (50\%).

Overall, LPL enables video generation with quality comparable to Open-Sora~\cite{opensora}, without noticeable visual degradation (\eg, 0.743 vs. 0.736 in average of VBench~\cite{huang2024vbench}),
% Overall, LPL enables the generation of videos whose quality, as measured by \isu{VBench~\cite{huang2024vbench}
% SSIM~\cite{fan2019metrics}, FVD~\cite{unterthiner2019fvd}, and VBench~\cite{huang2024vbench}, 
% is comparable to that of Open-Sora~\cite{opensora}, without noticeable visual degradation (\eg, \isu{???} vs. \isu{???} in average of VBench~\cite{huang2024vbench}), 
while accelerating video generation up to $1.94\times$ without any model optimization or re-training. 
LPL reduces denoising latency linearly without extra computation, enabling efficient video generation while retaining robust performance.
% LPL can reduce denoising latency linearly without incurring computational overhead.  , it facilitates efficient video generation while maintaining robust performance.

\Cref{fig:lpl-end-to-end-generated-frames1} presents example videos generated using LPL, where all LPL settings consistently produce semantically identical target videos, with most video quality remaining stable across various numbers of denoising steps.

\input{figures/linear_proportional_leap}

\subsection{Temporal Dimension Token Merging}


\Cref{tab:TDTM} presents the video quality and video generation speedup achieved when varying numbers of denoising steps to which Temporal Dimension Token Merging (TDTM) (Sec.~\ref{sec:ours2}) is applied, indicated as `Merging Steps' in the table, out of a total of 30 denoising steps. 
% Each configuration of merging steps is evaluated on 68-frame videos at a resolution of 256×256 pixels, using SSIM~\cite{fan2019metrics}, FVD~\cite{unterthiner2019fvd}, and VBench~\cite{huang2024vbench} as evaluation metrics. 
The results indicate that increasing the number of merging steps consistently accelerates video generation, ranging from $1.11\times$ to $1.70\times$ speedups, while maintaining stable quality metrics; the average scores for VBench remain at 0.736.
Nonetheless, some declines in the dynamic degree metric are observed, revealing a trade-off between maintaining visual dynamics and reducing token processing complexity. This indicates the importance of striking a balance between video dynamics and speedup. We found that selectively applying TDTM to specific denoising steps can effectively reduce visual noises. For instance, limiting TDTM to the first 15 denoising steps while not applying it to the rest of the denoising steps tends to result in a less severe quality drop compared to applying TDTM to all steps, which can mitigate issues like flickering or dynamic degree to provide improved video quality.

\Cref{fig:tdtm-generated-frames} shows examples of video frames generated with varying numbers of denoising steps to which TDTM is applied, out of 30 steps. The results show that even as TDTM is applied to an increasing number of denoising steps, the quality of the video frames seems to remain consistent.

\input{figures/token_merging}

%\jjm{
%\Cref{fig:exp_partitioned_stdit} and \ref{fig:exp_ci_stdit} illustrate the inference performance of the STDiT before and after the application of Concurrent Inference. As a result of applying the Concurrent Inference, it can be observed that the GPU, which showed a low operating rate, was operating in almost all the time, and the time of the first step was similar, but the time from the second step was reduced. Consequently, the total inference time for STDiT, originally approximately 1000 seconds for 30 steps, was reduced to approximately 750 seconds (about 25\% reduction), reflecting a notable efficiency improvement.

%\Cref{fig:exp_partitioned_ci_t5} depict the inference performance of the T5 under similar conditions. Unlike STDiT, T5 demonstrated a longer initial loading time. This characteristic minimized the effectiveness of Concurrent Inference, as the overlap of loading and prediction processes resulted in only a marginal performance gain. Specifically, the average inference time for T5 decreased modestly from approximately 164 seconds to about 137 seconds (about 15\% reduction).
%}

%What experiments should we conduct?

%For example, video generation time, resource, different resolution, comparison with server-generated videos ... what else?

% \subsection{Comparison of Results by Resolution}
% How does the resolution of the output affect the results? (Both server and device can be experimented with)

\subsection{Video Generation Latency}

\Cref{tab:execution-time} shows video generation latencies of two resolutions when each of the proposed methodologies—LPL, TDTM, and CI-DL—is applied individually, as well as the latency when all of them are applied together (`All'). Latencies are measured with LPL activated at the 15th denoising step and TDTM applied throughout all steps, reported as the mean of three independent experiments. `STDiT' and `Total' specify whether the latency is measured solely for STDiT~\cite{opensora} or for end-to-end video generation, including T5~\cite{raffel2020exploring} and VAE~\cite{doersch2016tutorial}. The results demonstrate substantial latency reductions for each methodology compared to the case without using the proposed methods, \ie, 293.51 vs. 1768.32 seconds (\Cref{tab:time-profiling}) for STDiT. For the 192$\times$192 resolution video, STDiT (denoising process) takes less than five minutes when all three methodologies are applied. Additionally, it indicates that the methodologies do not interfere with each other, instead work synergistically to enhance latency.

% \bonote{Due to the excessive computations, iPhone15 often showed high temperature with noticeable amount of performance degradation. To mitigate that, experiments including the naive setting for \cref{tab:execution-time} has been processed with external cooling of temperature, to sustain the performance evenly over different settings of experiments.}


% \note{The proposed three methods collectively achieve a computational reduction (speedup) of up to 5.2x in video generation, i.e., LPL 2x, TDTM 2x (4x), CI 1.3x, and DL ??x.\, when compared to not applying our methods. REMOVE?} 

\begin{table}[!htb]
\setlength{\tabcolsep}{6.5pt}
\renewcommand{\arraystretch}{1.1}
\caption{Ablation study on video generation latency (s). `All' denotes the combined application of LPL, TDTM, and CI-DL.}
\label{tab:execution-time}
\small
\resizebox{\columnwidth}{!}{%
% \begin{tabular}{l|cccccc}
% \toprule[1pt] \hline
% \textbf{Method}               & \multicolumn{6}{c}{\textbf{Check List}} \\ \hline
% \isu{Temporal ToMe}        & $\times$   & $\circ$   & $\times$   & $\times$  & $\times$  & $\circ$  \\
% \isu{LPL}                  & $\times$   & $\times$   & $\circ$   & $\times$  & $\times$  & $\circ$  \\
% Concurrent Inference & $\times$   & $\times$   & $\times$   & $\circ$  & $\times$  & $\circ$  \\
% Dynamic Loading      & $\times$   & $\times$   & $\times$   & $\times$  & $\circ$  & $\circ$  \\ \hline
% Time                 &     &     &     &    &    &    \\ \toprule[1pt]
% \end{tabular}%
\begin{tabular}{c|c|rrrrr}
\toprule[1pt] \hline
\textbf{Resolution} & \textbf{Measurement} & \multicolumn{1}{c}{\textbf{LPL}} & \multicolumn{1}{c}{\textbf{TDTM}} & \multicolumn{1}{c}{\textbf{CI-DL}} & \multicolumn{1}{c}{\textbf{All}} \\ \hline
\multirow{2}{*}{192$\times$192} & STDiT & 390.72 & 696.03 & 566.81 & 293.51 \\ 
                                & Total & 514.24 & 823.03 & 691.25 & 416.50 \\ \hline
\multirow{2}{*}{256$\times$256} & STDiT & 573.88 & 965.40 & 947.67 & 454.48 \\
                                & Total & 754.08 & 1148.63 & 1127.88 & 638.09 \\ \hline             
\toprule[1pt]
\end{tabular}
}
\end{table}

