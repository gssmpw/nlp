\section{Implementation}

We implement the proposed On-device Sora on iPhone 15 Pro~\cite{apple2023}, leveraging its GPU of 2.15 TFLOPS and 3.3 GB of available memory, with the two methods proposed in Sec. \ref{sec:ours1} and \ref{sec:ours2}. In addition, to execute large video generative models (\ie, T5 \cite{raffel2020exploring} and STDiT \cite{opensora}) with the limited device memory, we devise and implement Concurrent Inference with Dynamic Loading (CI-DL), which partitions the models into smaller blocks that can be loaded into the memory and executed concurrently. The details of CI-DL is described in \Cref{sec:ours3}. The model components—T5~\cite{raffel2020exploring}, STDiT~\cite{opensora}, and VAE~\cite{doersch2016tutorial}—in PyTorch~\cite{paszke2019pytorch} are converted to MLPackage, an Apple’s CoreML framework~\cite{sahin2021introduction} for machine learning apps. Since current version of CoreML \cite{apple2023} lacks support for certain diffusion-related operations in text-to-video generation, we develop custom solutions like xFormer \cite{xFormers2022} and cache-based acceleration. We implement denoising scheduling, sampling pipeline, and tensor-to-video conversion in Swift~\cite{swift} using Apple-provided libraries. To optimize models, T5~\cite{raffel2020exploring}, the largest in video generation, is quantized to int8, while others models  (STDiT~\cite{opensora} and VAE~\cite{doersch2016tutorial}) run in float32; we found that they are challenging to quantize due to sensitivity and performance degradation.%Our future implementations of On-device Sora will explore additional optimization to further enhance model efficiency.

%\jjm{In addition to the two key challenges mentioned in \Cref{sec:challenges}, there is one more additional challenge, which is a high memory requirement.}
%To execute large video generative models (i.e., T5 \cite{raffel2020exploring} and STDiT \cite{opensora}) with the limited device memory, we propose Concurrent Inference with Dynamic Loading, which partitions the models into smaller blocks that can be loaded into the memory and executed concurrently. By parallelizing model execution and block loading, it effectively accelerates iterative model inference, e.g., multiple denoising steps. Also, it improves memory utilization while minimizing the block loading overhead by retaining specific model blocks in memory dynamically based on the available runtime memory.
%\jjm{For a more detailed explanation of CI-DL, please refer to \Cref{sec:ours3}.}
% 20 20 20 -> 16 16 16
% 10 10 10 -> 27 * 7 189 210 ->189 