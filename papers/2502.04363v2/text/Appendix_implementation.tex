\section{Appendix - Implementation}
\subsection{Concurrent Inference with Dynamic Loading}
\label{sec:ours3}

%\Cref{fig:peak-memory} shows the memory requirements of model components executed by Open-Sora~\cite{opensora}, \ie, VAE~\cite{doersch2016tutorial}, T5~\cite{raffel2020exploring}, and STDiT~\cite{opensora}, where their cumulative memory demand, \ie, 23 GB, can easily surpass the memory capacity of many mobile devices. For instance, the iPhone 15 Pro~\cite{apple2023}, with 8 GB of memory, restricts the available memory for a single application to 3.3 GB to ensure system stability. Furthermore, the \bonote{individual} memory requirements of \bonote{T5 and STDiT} exceed \note{3.3 GB}, creating challenges in loading them into memory. In addition, some memory must be reserved for model execution (inference), complicating the deployment of Open-Sora on mobile devices. Thus, executing large video generative models with limited device memory is another challenge that should be addressed to enable on-device video generation.

On-device Sora tackles the challenge of limited device memory in text-to-video generation, which restricts the on-device inference of large diffusion-based models, by introducing Concurrent Inference with Dynamic Loading (CI-DL), which partitions models and executes them in a concurrent and dynamic manner.

\parlabel{Concurrent Inference}
The model components of On-device Sora~\cite{liu2024sora}, \ie, STDiT~\cite{opensora} and T5~\cite{raffel2020exploring}, easily exceed the available memory of many mobile devices, \eg, 3.3 GB RAM of iPhone 15 Pro, as shown in \Cref{fig:peak-memory}. Given that the Transformer architecture~\cite{wolf2020transformers}, which is the backbone for both T5~\cite{raffel2020exploring} and STDiT~\cite{opensora}, we partition these models into smaller blocks (segments) and load them into memory accordingly for model inference.

To execute video model inference using the model partitioning, each block must be loaded onto memory sequentially before execution, increasing the overall latency of video generation by incurring block loading time. \Cref{fig:Partitioned_CI_STDiT}-(a) shows the sequential block load and inference cycle of STDiT~\cite{opensora}, where GPU remains idle intermittently, waiting for each block to be loaded into memory, and only begins execution after the loading is complete. This sequential loading and execution process significantly increases the overall latency of model inference.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/Partitioned_CI_STDiT.pdf}
    \caption{The block loading and inference cycles for (a) sequential loading and inference, and (b) concurrent inference. %\note{[FIGURE] (a) Sequential Loading and Inference (b) Concurrent Inference}
    %\jjm{Performance graphs of STDiT. The figures are the schematic representation of the performance graphs.}
    }
    \label{fig:Partitioned_CI_STDiT}
\end{figure}

To minimize the increase in model inference latency caused by sequential block loading and execution, we propose Concurrent Inference, which leverages both the CPU and GPU for parallel block loading and execution; CPU loads the $(i+1)$th block, while GPU concurrently executes the $i$th block. Initially, the first and second blocks are loaded into memory concurrently, with the first block completing its loading first. Subsequently, the inference of the first block and the loading of the second block occur simultaneously. This process continues such that the inference of the $i$th block and the loading of the $(i+1)$th block overlap, ensuring continuous parallelism until the final block. \Cref{fig:Partitioned_CI_STDiT}-(b) depicts the load and inference cycle of STDiT with Concurrent Inference, which shows that GPU is active without almost no idle time by performing block loading and inference in parallel.

Given the number of model blocks $b$, block loading latency $l$, and inference latency of block $e$, the latency reduction $r$ achieved through Concurrent Inference is given by:
\begin{equation}
    r = b \cdot \min (l, e) - \alpha
    \label{eq:concurrent_inference}
\end{equation}
where $\alpha$ is the overhead caused by the block loading. 

Given the large number of denoising steps performed by STDiT, which is partitioned into multiple blocks for execution, similar to T5~\cite{raffel2020exploring}, the number of blocks $b$ is expected to be large, leading to a significant reduction in latency. It is expected to accelerate the overall model inference effectively regardless of the device's memory capacity. When the available memory is limited, then $b$ increases, while with larger memory, both $l$ and $e$ increase. In either case, it can result in an almost constant latency reduction $r$ in \Cref{eq:concurrent_inference}.

%<< \note{MOVE TO EXPERIMENTS?
%The inference time for STDiT was reduced by about 25\%, and for the T5, it was decreased by around 15\%. However, as T5 is executed only once during inference, the impact of CI on enhancing the inference speed appears less pronounced compared to STDiT, which repeats up to 30 times.} >>

\parlabel{Dynamic Loading}
To further enhance the model inference latency, we propose Dynamic Loading, which is applied in conjunction with Concurrent Inference. It maintains a subset of model blocks in memory without unloading them, with the selection for the subset of blocks to be retained in memory dynamically determined based on the device's available memory at runtime.

The available memory of the device can vary at runtime based on the system status and configurations of applications running on the mobile device. By retaining a subset of model blocks in memory, the overhead of reloading these blocks during subsequent steps of Concurrent Inference can be eliminated, enabling reductions in model inference latency. To achieve this, we measure the device's run-time memory capacity and the memory required for inferring a single model block during the initial step of Concurrent Inference. Next, the memory allocated for retaining certain model blocks is dynamically determined as the difference between the available memory and the memory required for inferring a model block. Then, a series of model blocks that fit within this allocated memory is loaded in a retained state.

\Cref{fig:partitioning} depicts Dynamic Loading; the first four model blocks are loaded in a retrained state. Unlike other blocks, \eg, the 5th block, these blocks are not unloaded to memory after the initial step, reducing block loading overhead. %The number of blocks retained in memory is dynamically adjusted based on the available memory.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{figures/Dynamic_Loading_Example.pdf}
    \caption{The block loading and inference cycle for Dynamic Loading applied with Concurrent Inference.}
    \label{fig:partitioning}
\end{figure}

\begin{table*}[!htb]
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.1}
\caption{The VBench~\cite{huang2024vbench} evaluation by category: On-device Sora vs. Open-Sora~\cite{opensora} (68 frames, 256Ã—256 resolution).%VBench Performance Comparison per category by Open-Sora and On-device Sora with 68 frames, 256 $\times$ 256 resoultion
}
\label{tab:appendix-main-experiments}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|l|ccccc|cc}
\toprule[1pt] \hline
\multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Category}}} &
  % \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Resolution}}} &
  \multicolumn{1}{c|}{\multirow{2}{*}{\textbf{Method}}} &
  \multicolumn{5}{c|}{\textbf{Temporal Quality$\uparrow
$}} &
  \multicolumn{2}{c}{\textbf{Frame-Wise Quality$\uparrow
$}} \\ \cline{3-9} 
\multicolumn{1}{c|}{} &
  % \multicolumn{1}{c|}{} &
  \multicolumn{1}{c|}{} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Subject \\ Consistency\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Background\\ Consistency\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Temporal\\ Flickering\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Motion\\ Smoothness\end{tabular}} &
  \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Dynamic\\ Degree\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Aesthetic\\ Quality\end{tabular}} &
  \multicolumn{1}{c}{\begin{tabular}[c]{@{}c@{}}Imaging\\ Quality\end{tabular}} \\ \hline
\multirow{2}{*}{Animal}       & Open-Sora      & 	0.97& 	0.98& 	0.99& 	0.99& 	0.15& 	0.51& 	0.56 \\
                              & On-device Sora & 	0.95& 	0.97& 	0.99& 	0.99& 	0.28& 	0.48& 	0.55    \\ \hline
\multirow{2}{*}{Architecture} & Open-Sora      & 	0.99& 	0.98& 	0.99& 	0.99& 	0.05& 	0.53& 	0.60\\
                              & On-device Sora & 	0.98& 	0.98& 	0.99& 	0.99& 	0.12& 	0.49& 	0.56    \\\hline
\multirow{2}{*}{Food}         & Open-Sora      & 	0.97& 	0.97& 	0.99& 	0.99& 	0.26& 	0.52& 	0.60\\
                              & On-device Sora & 	0.95& 	0.97& 	0.99& 	0.99& 	0.38& 	0.48& 	0.53    \\ \hline
\multirow{2}{*}{Human}        & Open-Sora      & 	0.96& 	0.97& 	0.99& 	0.99& 	0.38& 	0.48& 	0.57\\
                              & On-device Sora & 	0.96& 	0.96& 	0.99& 	0.99& 	0.43& 	0.48& 	0.55    \\ \hline
\multirow{2}{*}{Lifestyle}    & Open-Sora      & 	0.97& 	0.97& 	0.99& 	0.99& 	0.23& 	0.45& 	0.56\\
                              & On-device Sora & 	0.96& 	0.97& 	0.99& 	0.99& 	0.25& 	0.45& 	0.53    \\ \hline
\multirow{2}{*}{Plant}        & Open-Sora      & 	0.98& 	0.98& 	0.99& 	0.99& 	0.15& 	0.50& 	0.58\\
                              & On-device Sora & 	0.97& 	0.98& 	0.99& 	0.99& 	0.16& 	0.46& 	0.55        \\ \hline
\multirow{2}{*}{Scenery}      & Open-Sora      & 	0.98& 	0.98& 	0.99& 	0.99& 	0.10& 	0.50& 	0.50\\
                              & On-device Sora & 	0.97& 	0.98& 	0.99& 	0.99& 	0.17& 	0.48& 	0.47        \\ \hline
\multirow{2}{*}{Vehicles}     & Open-Sora      & 	0.97& 	0.97& 	0.99& 	0.99& 	0.37& 	0.48& 	0.54\\
                              & On-device Sora & 	0.94& 	0.96& 	0.98& 	0.99& 	0.44& 	0.47& 	0.49 \\  \toprule[1pt]
\end{tabular}%
}
\end{table*}

\input{figures/overall_metric}

Applying Dynamic Loading, the latency reduction $r$ in \Cref{eq:concurrent_inference} for Concurrent Inference is updated as:
\begin{equation}
    r = b \cdot \min (l, e) + d \cdot \max (0, l - e)  - \alpha \cdot (1 - d / b)
    \label{eq:dynamic loading-2}
\end{equation}
where $d$ is the number of blocks maintained in memory. As the number of model blocks retained in memory increases (\ie, a larger $d$), the overhead of loading \and unloading blocks is further minimized, fully accelerating the overall model inference under the device's run-time memory constraints. Dynamically keeping some model blocks in memory with a retrained state is particularly advantageous for STDiT~\cite{opensora} that is iteratively executed for denoising steps, which entails loading and reusing the same blocks for each step.