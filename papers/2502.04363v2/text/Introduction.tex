\section{Introduction}

Recent advancements in generative models~\cite{suzuki2022survey} have significantly expanded capabilities in data generation across various modalities, including text~\cite{hu2017toward}, image~\cite{shaham2019singan}, and video~\cite{unterthiner2018towards}. In particular, diffusion-based models for image tasks~\cite{ho2020denoising, song2020denoising, peebles2023scalable, rombach2022high, saharia2022palette, dhariwal2021diffusion, zhang2023adding, podell2023sdxl} have emerged as foundational tools for a wide range of applications, such as image generation~\cite{ho2022cascaded}, image editing~\cite{kawar2023imagic}, and personalized content creation~\cite{zhang2024survey}. Further extending these technologies, diffusion-based generative models are now driving remarkable progress in video generation tasks~\cite{singer2022make, menapace2024snap, ho2022imagen, ho2022video, blattmann2023stable, molad2023dreamix, harvey2022flexible, blattmann2023align, voleti2022mcvd, wu2023tune, khachatryan2023text2video}, including video synthesis~\cite{liu2021generative} and real-time video analysis~\cite{orriols2004generative}. %facilitating the development of advanced generative video services and applications in new and unprecedented ways.

%\jjm{One of the most notable video generation models is Sora \cite{liu2024sora}, which has demonstrated remarkable potential in creating high-quality videos from textual descriptions (prompts). However, the high complexity of diffusion processes for video generation, combined with the substantial size of diffusion-based Transformer models~\cite{peebles2023scalable}, imposes significant computational and memory demands. For instance, Sora is estimated to take an hour to produce a five-minutes video using an NVIDIA H100 80GB GPU. Compared to large language models (LLMs)~\cite{zhao2023survey}, the costs for diffusion-based video generation models like Sora are several orders of magnitude higher, indicating their resource-intensive nature. As a result, the accessibility of video generative services remains predominantly confined to environments with extensive computational resources, limiting their applicability in resource-constrained devices.}

%With the rapid expansion of mobile and embedded devices, there is a growing demand for running generative applications directly on the device. \jjm{In response, numerous studies have primarily focused on the development of on-device image generation~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, choi2023squeezing, chen2023speed, zhao2023mobilediffusion}, with recent advancements extending to on-device video generation~\cite{yahia2024mobile, wu2024snapgen}. However, constructing such on-device generation models requires a light weight to reduce the model's capacity and computational complexity, and additional learning is required to address the preformance degradation that has occurred while being lightweight. This process requires high computational and memory resources, \eg, SanpGen-V~\cite{wu2024snapgen} requires over 150K iterations on 256 NVIDIA A100 80G GPUs.}

With the rapid expansion of mobile and embedded devices, there is an increasing demand for executing generative applications directly on-device. In response, extensive research has focused on developing on-device image generation methods~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, choi2023squeezing, chen2023speed, zhao2023mobilediffusion}, with recent advancements extending to on-device video generation~\cite{yahia2024mobile, wu2024snapgen}. However, generating lightweight and compressed on-device video generation models necessitates additional model optimization (training), such as distillation~\cite{10.1145/3681758.3698013, lin2024animatediff, singer2024video}, quantization~\cite{tian2024qvd, zhao2024vidit}, and pruning~\cite{wu2024individual}, to reduce model size and computational complexity. Furthermore, mitigating performance degradation in the compressed models requires a time-intensive and iterative model re-training. This process usually demands substantial computational and memory resources, \eg, SnapGen-V~\cite{wu2024snapgen} takes over 150K training iterations on 256 NVIDIA A100 80GB GPUs to generate mobile-deployable compressed video generation models.

We introduce On-Device Sora, the first training-free framework that enables diffusion-based text-to-video generation on mobile devices, which allows video generative models to run directly on-device, enabling the production of high-quality videos on smartphone-grade devices. While previous approaches~\cite{wu2024snapgen} require additional training and substantial GPU resources to optimize video generation models for mobile environments, the proposed On-Device Sora eliminates the need for training and directly enables efficient on-device execution. Using pre-trained video generative models, \eg, Open-Sora~\cite{opensora}, \jjm{Pyramidal} Flow~\cite{jin2024pyramidal}, On-device Sora significantly enhances their efficiency, enabling on-device video generation with limited resources. 

To achieve this, we address key challenges of enabling on-device text-to-video generation: 1) Linear Proportional Leap (LPL) reduces the iterative denoising steps in the diffusion process by leaping through a significant portion of steps using the Euler’s method~\cite{biswas2013discussion} along an estimated direct trajectory, 2) Temporal Dimension Token Merging (TDTM) lowers computational complexity of STDiT (Spatial-Temporal Diffusion Transformer)~\cite{opensora} by merging consecutive tokens~\cite{bolya2022token,bolya2023token} in the attention layers, and 3) Conference Inference and Dynamic Loading (CI-DL) enables video generation with limited memory capacity of mobile devices by integrating concurrent model inference with the dynamic loading of models into memory. With these three proposed methods, high-quality video generation becomes feasible on smartphone-grade devices with limited computing resources, overcoming the requirements for substantial computational power, such as high-end GPUs. To the best of our knowledge, On-device Sora proposed in this work is the first training-free solution that enables the efficient generation of video directly on the device. The advantages of On-device Sora, \ie, directly deploying well-trained video generative models onto resource-constrained devices without time-consuming model modification and/or re-training, are expected to become even more pronounced with the active development of compact DiT-based text-to-video generative models~\cite{peebles2023scalable}.

We implement On-device Sora on the iPhone 15 Pro~\cite{apple2023} using Open-Sora~\cite{opensora}. The full implementation is available as open-source code in an anonymous GitHub\footref{github}. The extensive experiments are conducted to evaluate the performance of On-device Sora using the state-of-the-art video benchmark tool, \ie, VBench~\cite{huang2024vbench}, compared with NVIDIA A6000 GPUs. We also evaluate the proposed methods on additional text-to-video generative models, including Pyramidal Flow~\cite{jin2024pyramidal}. The experimental results demonstrate that On-device Sora maintains comparable video quality while accelerating generation speed with the proposed methods. While the iPhone 15 Pro has a GPU of 143 times less computational power~\cite{apple2023} and 16 times smaller memory compared to the NVIDIA A6000 GPU, the evaluation results show that On-device Sora significantly improves the efficiency of video generation by effectively compensating for the limited computing resources of the device.
% We also conduct additional experiments on the proposed methods on PyramidalFlow~\cite{jin2024pyramidal} and CogVideoX~\cite{yang2024cogvideox}.



%기존 Introduction
\iffalse
Recent advancements in generative models~\cite{suzuki2022survey} have significantly expanded capabilities in data generation across various modalities, including text~\cite{hu2017toward}, image~\cite{shaham2019singan}, and video~\cite{unterthiner2018towards}. In particular, diffusion-based models for image tasks~\cite{ho2020denoising, song2020denoising, peebles2023scalable, rombach2022high, saharia2022palette, dhariwal2021diffusion, zhang2023adding, podell2023sdxl} have emerged as foundational tools for a wide range of applications, such as image generation~\cite{ho2022cascaded}, image editing~\cite{kawar2023imagic}, and personalized content creation~\cite{zhang2024survey}. Further extending these technologies, diffusion-based generative models are now driving remarkable progress in video generation tasks~\cite{singer2022make, menapace2024snap, ho2022imagen, ho2022video, blattmann2023stable, molad2023dreamix, harvey2022flexible, blattmann2023align, voleti2022mcvd, wu2023tune, khachatryan2023text2video}, including video synthesis~\cite{liu2021generative} and real-time video analysis~\cite{orriols2004generative}, facilitating the development of advanced generative video services and applications in new and unprecedented ways.

One of the most notable video generation models is Sora \cite{liu2024sora}, which has demonstrated remarkable potential in creating high-quality videos from textual descriptions (prompts), effectively transforming abstract concepts into visually realistic content. However, the inherently high complexity of diffusion processes for video generation, combined with the substantial size of diffusion-based Transformer models~\cite{peebles2023scalable}, imposes significant computational and memory demands. For instance, Sora~\cite{liu2024sora} is estimated to take an hour to produce five minutes of video using an NVIDIA H100 GPU of 80 GB RAM. Compared to large language models (LLMs)~\cite{zhao2023survey}, the costs for diffusion-based video generation models like Sora are several orders of magnitude higher, indicating their resource-intensive nature. As a result, the accessibility of video generative services remains predominantly confined to environments with extensive computational resources, limiting their applicability in resource-constrained devices.

%Meanwhile, 
With the rapid expansion of mobile, IoT, and embedded devices, there is a growing demand for running generative applications directly on the device. In response, numerous studies have primarily focused on the development of on-device image generation~\cite{li2024snapfusion, vasu2023mobileone, castells2024edgefusion, choi2023squeezing, chen2023speed, zhao2023mobilediffusion}, as image generation is comparatively less resource-intensive than video. However, enabling on-device video generation remains in its nascent stages due to the significantly higher computational and memory resource demands compared to image generation~\cite{elasri2022image}. Moreover, given that video data is typically much larger and more sensitive than image data~\cite{deng2023efficiency}, there is an increasing need for innovative technologies that can enable efficient and effective on-device video generation.

We introduce On-device Sora, the first standalone framework for diffusion-based on-device text-to-video generation, which is capable of producing high-quality videos on smartphone-grade devices. On-device Sora leverages Open-Sora~\cite{opensora} as its backbone and significantly enhances the efficiency of video generation while maintaining the comparable video quality, enabling on-device text-to-video generation with limited computational power and memory. To the best of our knowledge, On-device Sora proposed in this work is the first solution that enables the efficient generation of Sora-level video directly on the device, which has previously been restricted to large-scale data center environments.

On-device Sora addresses three fundamental challenges in enabling diffusion-based on-device text-to-video generation. First, diffusion-based video generation entails a substantial number of denoising steps, requiring repeated executions of diffusion models, ranging from dozens to thousands of iterations. To address this, we propose Linear Proportional Leap (LPL), which reduces nearly half of the denoising steps by leaping through a large portion of steps using the Euler's method~\cite{biswas2013discussion} in an estimated direct trajectory to generate videos efficiently.
Second, state-of-the-art diffusion models, such as STDiT (Spatial-Temporal Diffusion Transformer)~\cite{opensora}, employed by Open-Sora, exhibit high computational complexity due to their substantial amount of token processing~\cite{bolya2022token} in attention modules~\cite{vaswani2017attention}. To tackle the high computational complexity of STDiT, we propose Temporal Dimension Token Merging (TDTM), which merges consecutive tokens~\cite{bolya2022token,bolya2023token} in temporal order at attention layers of STDiT, effectively reducing the token processing up to a quarter.
Lastly, the substantial size of state-of-the-art models required for text-to-video generation, \ie, T5~\cite{raffel2020exploring} and STDiT~\cite{opensora}, presents challenges for execution on memory-constrained mobile devices. To overcome this, we propose Conference Inference and Dynamic Loading (CI-DL), which integrates concurrent model inference with the dynamic loading of models into memory. It dynamically divides the model into blocks and loads them into memory based on the device's available memory capacity at run-time, allowing simultaneous model block loading and inference. With these three proposed methods, high-quality video generation becomes feasible on smartphone-grade devices with limited computing resources, overcoming the requirements for substantial computational power, such as high-end GPUs.

Currently, video generative technology remains neither widely accessible nor commonly available to the public~\cite{liu2024sora}. Therefore, enabling video generation on commodity mobile and embedded devices can not only enhance the accessibility of advanced video generation technologies but also provide additional benefits. Users can ensure privacy and mitigate concerns about data transmission and leakage of sensitive personal information. On-device video generation eliminates the need for interaction with the third-party cloud servers, which may pose security risks due to potential data breaches or unauthorized access. Furthermore, cloud-based generative models often suffer from latency issues influenced by network speed and may not function optimally when connectivity is unstable or unavailable. On-device generative models, in contrast, can provide more reliable services, offering stable functionality regardless of network conditions. Another limitation of cloud-based video generation is their shared nature, which limits personalization and the ability to cater to individual users. On-device video generation, however, can be tailored to each user, enabling personalized experiences through customized video creations. Through on-device generation, videos can be fine-tuned and customized for individual users, providing contents that better reflect personal preferences, data, contexts, and environments. From a financial perspective, generating videos on-device is far more economical. For example, the cost of an NVIDIA H100 GPU is around \$25,000, whereas an iPhone 15 Pro is about only \$999, which is 25 times more affordable. Additionally, the annual maintenance costs for cloud-based video generation are estimated to range between \$250,000 and \$800,000, %excluding energy costs, 
whereas a smartphone incurs negligible maintenance expenses.

We implement On-device Sora on the iPhone 15 Pro~\cite{apple2023} based on Open-Sora~\cite{opensora}, which is an open-source text-to-video generation framework, enabling standalone text-to-video generation directly on the device. The full implementation is available as open-source code in a publicly accessible anonymous GitHub repository\footref{github}. The extensive experiments are conducted to evaluate the performance of the proposed On-device Sora using the state-of-the-art video benchmark tool, \ie, VBench~\cite{huang2024vbench}, compared with Open-Sora running on NVIDIA A6000 GPUs. The experimental results demonstrate that On-device Sora can effectively generate videos of equivalent quality to Open-Sora~\cite{opensora}, while accelerating video generation when applying the proposed three methods. While the iPhone 15 Pro has a GPU of 143 times less computational power~\cite{apple2023} and 16 times smaller memory (RAM) compared to the NVIDIA A6000 GPU, the evaluation results demonstrate that On-device Sora significantly improves the efficiency of video generation by effectively compensating for the limited computing resources of the device.
\fi