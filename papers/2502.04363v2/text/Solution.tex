
\section{Linear Proportional Leap}
\label{sec:ours1}

On-device Sora reduces the excessive number of denoising steps performed by STDiT~\cite{opensora} by introducing Linear Proportional Leap (LPL), which leverages the trajectory properties of Rectified Flow~\cite{liu2022flow}. It allows early stop of denoising steps through proportionally scaled linear leaps, without extra model training or modification of STDiT architecture.

\subsection{Rectified Flow}

The reverse diffusion process is performed through multiple denoising steps, transforming an initial Gaussian distribution into a desired distribution corresponding to the input prompt. Several ODE-based methods~\cite{zheng2023dpm, lu2022dpm} reformulate this process by training models to predict the drift at each time point, building the distribution trajectories from the initial to the target point by using ODE solvers~\cite{runge1895numerische, yang2023diffusion}.

Rectified Flow~\cite{liu2022flow} simplifies the transition from the initial point to the target point by training the model to predict a drift aligned with the direct linear trajectory connecting these two points. Using the Euler method~\cite{chen2018neural}, the $k$th trajectory $\boldsymbol{z_k}$ is derived by updating the previous trajectory $\boldsymbol{z_{k-1}}$ with the estimated drift $\boldsymbol{v}(P_k,t_k)$ and step size $dt_k$ defined by two sampled time steps $t_k$ and $t_{k+1}$, as follows:
\begin{equation}
\begin{split}
    &\boldsymbol{z}_k = \boldsymbol{z}_{k-1} + \boldsymbol{v}(P_k,t_k)dt_k ~~ \forall 1 \leq k \leq K
    \\
    %t \in [0,1], \quad t_k \in \{t_i\}_{i=1}^M\\
    %X_k = t_k*X_1 + (1 - t_k)*X_0 \\    
    & \text{where}~ t_{k} \in [0,1] ~\text{and}~ dt_k = 
\begin{cases} 
    t_k {-} t_{k+1}, & \text{if } t_k \neq t_K \\
t_k, & \text{if } t_k = t_K
\end{cases}
\end{split}
    \label{eq:rflow}
\end{equation}

\noindent Here, the time step $t_{k} {\in} [0,1]$ corresponds to the normalized reverse process at the $k$th denoising step, with $t_{k} {=} 1$ representing the time step at which data is fully noisy (start of denoising) and $t_{k} {=} 0$ corresponding to the time step when data reaches the desired distribution (end of denoising). The drift $\boldsymbol{v}(P_k,t_k)$ is predicted from STDiT~\cite{opensora} given the $k$th position on the trajectory, $P_{k} = t_k P_1 + (1 - t_k) P_0$, computed from linear interpolation with sampled time step, $t_{k}$~\cite{liu2022flow}.
%, and $K$ is the total number of \note{denoising} steps. 

%The timestep $t_{k} \in [0,1]$ indicates the normalized reverse diffusion process at the $k$th \note{denoising (sampling)} step, where $t_{k}=0$ is the timestep when the data is full of noise (the beginning of \note{denoising (sampling)} process), and $t_{k} = 1$ is when the data is at the desired data distribution (the end of \note{denoising (sampling)} process).

%$dt$ is determined by sampled timesteps, which is scaled with target resolution and shifted with temporal reduction factors. 

%Modifying the value of $dt_{k}$ arbitrarily at inference time leads to unsatisfiable results, as step size itself is the part of sampling process, establishing the strength of current drift to be applied at approximated trajectory, when target trajectory is nonlinear and complex.

In Rectified Flow~\cite{liu2022flow}, the model is trained to predict the direct direction toward target point at any point on the trajectory. This allows diffusion-based generation models for achieving a denoising process in few steps without significant performance degradation. 
%While the actual trajectory may not be fully captured by a single drift, it can be effectively approximated with a reduced number of drifts. %\note{\sout{Furthermore, guiding the model to focus on the direct path between the initial and target points helps straighten the mapping trajectory, improving overall efficiency.}} 
%However, in diffusion-based video generation, achieving single-step generation is challenging due to the additional temporal variance of video data and the increased complexity of the target distribution. 
With Rectified Flow~\cite{liu2022flow}, Open-Sora~\cite{opensora} generates video with $K=30$ or $50$ steps.%, in contrast to image generation, which produces images in a single step.

%As in Rectified Flow~\cite{liu2022flow}, the model \note{(STDiT)} is trained to predict the drift~\cite{zheng2023dpm,lu2022dpm} that is set to drive the flow to follow the direct linear path between target and the initial point.

%The model could predict the direct direction toward target point at any point on the trajectory, model could theoretically and often empirically achieve few-step denoising process in diffusion-based image generation models, without significant performance drops, despite of the limitations that actual trajectory could not be fully represented with single drift, yet it could be represented with relatively less number of drifts and forcing model to focus on direction of direct path between initial point to target point surely aided model to straighten the mapping trajectory.



%As a implementation of simple Euler method~\cite{?}, current integration of drift with trajectory($z$) can be easily computed and updated as below, where $M$ is the total number of \note{denoising (sampling)} steps, $t \in [0,1]$ is the normalized reverse diffusion process of \note{denoising (sampling)} timestep with respect to the conventional expression of neural ode based diffusion models. $t=0$ is the time when the data is full of noise which can be also written as the beginning of the sampling process, and $t=1$ is when the data is at the desired data distribution which can be interpreted as the end of the sampling process. $z$ is the current position, while $v$ is the drift predicted by \note{STDiT}.

%$X_k$ below represents the current k-th position on the trajectory, sampled by the linear interpolation from corresponding timestep $t_k$.

\subsection{Linear Proportional Leap}

%Building upon Rectified Flow~\cite{liu2022flow}, we propose Linear Proportional Leap, a novel method that reduces denoising steps for diffusion-based video generation, as illustrated in \Cref{fig:lpl-mainfig}.

%Rectified Flow~\cite{liu2022flow} aims to bias the trajectory $z_{k}$ in \Cref{eq:rflow} to be as straight as possible. Building upon this, we propose Linear Proportional Leap (LPL), a method that reduces the number of denoising steps for diffusion-based video generation.

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/lpl_main.pdf}
    \caption{An abstracted illustration of trajectories and latent visualizations for $K=30$ and $n=15$: (a) Rectified Flow~\cite{liu2022flow} with full $k=30$ denoising steps, generating intact and complete video data, (b) Rectified Flow~\cite{liu2022flow} with $n+1=16$ denoising steps without applying Linear Proportional Leap, resulting in low-quality video data generation from variance with high step sizes ($dt_{k}$), and (c) Linear Proportional Leap with $n+1=15+1$ denoising steps, producing video data nearly equivalent to (a).}
    %\bonote{Will be Aesthetically improved in future, figure idea. Visualization of different discretization of sampled timepoints. (a) depicts the circumstances where model have coarse discretization steps (small timestep numbers, e.g., 15.), (b) describes the sampling process of fine discretization steps (30 steps, default) with latents visualized early at halfway (15 steps). (c) visualizes the Linear Proportional Leap proposed in the work, leaping toward target distribution near half-way (15 steps + 1 step) after fine-grained steps by fully utilizing the trait of trajectory acquired after the training with Rectified Flow methodologies.} 
    % \note{ADD A FULL NOISE IMAGE AS A STARTING POINT (RED DOT)?, REPLACE (a) WITH NORMAL RFLOW WITH FULL 30 STEPS. (a) becomes (b), NOT 30/15 steps?}
    \label{fig:lpl-mainfig}
\end{figure}

Based on Rectified Flow~\cite{liu2022flow}, the proposed Linear Proportional Leap reduces denoising steps, as illustrated in \Cref{fig:lpl-mainfig}. 

If the $n$th data distribution is sufficiently close to the $K$th target distribution in the denoising process, the trajectories $\boldsymbol{z}_{n+1 ... K}$ would be approximately straight for the remaining time steps $t_{n+1 ... K}$, making the drift estimation $\boldsymbol{v}(P_k,t_k)dt_k$  in \Cref{eq:rflow} unnecessary for $k {>} n$. Consequently, $\boldsymbol{v}(P_k,t_k)dt_k$ is estimated only for $1 {\leq} k {\leq} n$, allowing the denoising process to stop early at the $(n+1)$th step, not to the full $K$th step. For the remaining steps $t_{n+1 ... K}$, the trajectories $\boldsymbol{z}_{n+1 ... K}$ linearly leap towards the target data distribution, with the straight direction of $\boldsymbol{v}(P_{n+1},t_{n+1})$ and $dt_{n+1}$ scaled proportionally to $t_{n+1 ... K}$.

By assuming $k=K$, $\boldsymbol{z}_{k}$ is derived from \Cref{eq:rflow} as:
%From this, $\boldsymbol{z}_{k}$ is derived from \Cref{eq:rflow} as:
\begin{equation}
\begin{split}
    \boldsymbol{z}_k &= \boldsymbol{z}_{k-1} + \boldsymbol{v}(P_k,t_k)dt_k
    \\
    &= \boldsymbol{z}_0 + \sum_{i=1}^{k-1} \boldsymbol{v}(P_i,t_i) ({t_i-t_{i+1}}) + \boldsymbol{v}(P_k, t_k) t_k
    %\\
    %&= z_0 + v(P_1,t_1) (t_1 - t_2) + \cdots + v(P_{k-1},t_{k-1}) (t_{k-1} - t_{k}) + v(P_k,t_k) t_k
\end{split}
    \label{eq:rflow2}
\end{equation}

\noindent If the denoising process stops at the step $n$, the $n$th trajectory is represented as $\boldsymbol{z}_n = \boldsymbol{z}_0 + \sum_{i=1}^{n} \boldsymbol{v}(P_i,t_i) ({t_i-t_{i+1}})$. Then, if we apply the identical drift $\boldsymbol{v}(P_{n+1},t_{n+1})dt_{n+1}$ to the remaining $n+1 \leq i \leq k$ steps, \Cref{eq:rflow2} becomes:
\begin{equation}
\begin{split}
    \boldsymbol{z}_k &{=} \boldsymbol{z}_n {+} \boldsymbol{v}(P_{n+1},t_{n+1}) \sum_{i={n+1}}^{k-1}(t_i {-} t_{i+1}) {+} \boldsymbol{v}(P_{n+1},t_{n+1}) t_k
    \\
    &{=} \boldsymbol{z}_n {+} \boldsymbol{v}(P_{n+1},t_{n+1}) (t_{n+1} {-} t_{n+2} {+} \cdots {+} t_{k-1} {-} t_k {+} t_k)
    \\
    &{=} \boldsymbol{z}_n {+} \boldsymbol{v}(P_{n+1},t_{n+1}) t_{n+1}
\end{split}
    \label{eq:lpl}
\end{equation}

\noindent Thus, the required denoising steps are reduced to $n+1$ out of the total $k$ steps, allowing STDiT to be executed only $n+1$ times, with the last $(n+1)$th trajectory applied to its time step $t_{n+1}$, which enables Linear Proportional Leap.
Replacing $dt_{k}$ with $t_{n+1}$ in \Cref{eq:lpl}, instead of computing difference between the sampled time steps, can invoke an identical effect under assumption that later steps tend to sustain drift directions. %\bonote{under assumption that later steps tend to sustain its direction of the drift force(bosung: connecting to the observation with cosine similarities in scratchpads...)}
It enables the immediate completion of the denoising process, as $t_{n+1}$ is equivalent to the remaining denoising steps required to reach the end of denoising.

%For instance, when $K=30$ and $n=15$, the denoising steps proceed for the initial $n=15$ steps, and at step $n=16$, the trajectory transitions by leaping towards the linear direction of the target distribution. Due to the straightened trajectory, whose straightness increases as the denoising process approaches its end, combining the current drift with the product of the remaining step size yields a result that closely approximates the outcome achievable through the full denoising process. This enables a reduction of the denoising process by half without requiring additional model training.
%\note{or data calibration}. %It leverages the trajectory estimated by StDiT~\cite{opensora}, which is trained using the Rectified Flow~\cite{liu2022flow}.

Linear Proportional Leap can be dynamically applied by measuring the cosine similarity between two consecutive drifts $\boldsymbol{v}$ at runtime. When the cosine similarity appears that the current trajectory is sufficiently linear, a linear leap is made proportionally to the remaining steps. \Cref{fig:cos_siml} visualizes the cosine similarities between consecutive drifts, stabilizing after a certain number of steps, suggesting that the trajectory toward the target data distribution is nearly linear. 
% This enables the use of significantly fewer steps with larger \note{leap} sizes to efficiently progress in the desired direction. 
This enables fewer steps by utilizing the leap with the larger step size to efficiently progress to the desired direction.

\begin{figure} [!t]
  \centering
  \begin{minipage}[c]{0.51\columnwidth}
    \centering    
    \includegraphics[width=\columnwidth]{figures/cosine_similarity.pdf}
  \end{minipage}%
  \begin{minipage}[c]{0.47\columnwidth}
  \caption{An example of cosine similarities between two adjacent drifts estimated from STDiT~\cite{opensora}, \ie, $\boldsymbol{v}(P_{n},t_{n})$ and $\boldsymbol{v}(P_{n-1},t_{n-1})$ for 30 (red) and 50 steps (blue).} 
    \label{fig:cos_siml}
  \end{minipage}
\end{figure}

\section{Temporal Dimension Token Merging}
\label{sec:ours2}

On-device Sora reduces the computational complexity of the denoising process by introducing Temporal Dimension Token Merging (TDTM), which halves the size of tokens in STDiT~\cite{opensora} over the temporal dimension, decreasing the computation of self-attention quadratically and cross-attention in half. Unlike existing token merging applying to self-attentions over the spatial dimension, which exhibits suboptimal performance~\cite{bolya2022token,bolya2023token,feng2023efficient,li2024vidtome}, Temporal Dimension Token Merging leverages the temporal aspect of video frames to reduce computation while ensuring video quality.

%We streamline the computational complexity of the denoising process by introducing Temporal Dimension Token Merging, which \note{halves} the size of tokens processed by STDiT~\cite{opensora} along the temporal dimension, decreasing the computational workload of the \kyu{self-attention operation quadratically} and the cross-attention operation \kyu{in half}.

%To mitigate the heavy computational workload of the cross-attention in STDiT~\cite{opensora} \kyu{and improve the suboptimal performance of self-attention-applied token merging~\cite{?,?}} while ensuring the video quality, we propose a novel token merging strategy, termed Temporal Dimension Token Merging. Unlike existing token merging methods~\cite{bolya2023token}, which primarily focus on spatial dimensions, it leverages the temporal properties of video frames to effectively reduce computations.

\subsection{Token Merging}

STDiT~\cite{opensora} consists of multiple attention layers, \ie, cross- and self-attention, of the linear and quadratic complexity, respectively. In video generation, these attentions extend across two dimensions, \ie, spatial and temporal dimension. General model optimization techniques, \eg, pruning~\cite{reed1993pruning}, quantization~\cite{gray1998quantization}, and distillation~\cite{hinton2015distilling}, may reduce STDiT's attention computation. However, they necessitate model training (fine-tuning) or specialized hardware for implementation, and more importantly, the performance of video generation can hardly be preserved. In contrast, token merging~\cite{bolya2023token} reduces the token size processed in attentions, decreasing computational complexity without requiring model re-training or hardware-specific adaptations.

% \kyu{In STDiT~\cite{opensora}, the materializing attention bias affected by input tokens in cross-attention layers leads to higher computational demands than self-attention layers.} 
In STDiT~\cite{opensora}, the attention bias influenced by input tokens in cross-attention layers leads to higher computational demands than in self-attention layers.
As a result, developing an effective token merging method for cross-attention is crucial in video generation. However, existing token merging~\cite{bolya2022token, bolya2023token, li2024vidtome} applied to the cross-attention have shown suboptimal performance, and applying them to the self-attention in STDiT has observed video quality drops~\cite{bolya2022token,bolya2023token}.

% In STDiT~\cite{opensora}, the larger size of the prompt embeddings within cross-attention layers leads to higher computational demands compared to self-attention layers. As a result, developing an effective token merging method for cross-attention is crucial in video generation. However, existing token merging~\cite{bolya2022token, bolya2023token, li2024vidtome} applied to the cross-attention have shown suboptimal performance, and applying them to the self-attention in STDiT has observed video quality drops~\cite{bolya2022token,bolya2023token}.

\subsection{Temporal Dimension Token Merging}

\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/TDTM3.pdf}
    \caption{In attention layers of STDiT~\cite{opensora}, two consecutive tokens are merged along the temporal dimension and subsequently unmerged after processing, reducing the token size by half and the computational complexity up to a quarter.}
    \label{fig:TDTM-fig}
\end{figure}

\Cref{fig:TDTM-fig} illustrates Temporal Dimension Token Merging. Based on the hypothesis that successive video frames exhibit similar values, two consecutive frames are merged over the temporal dimension by averaging, creating a single token without the overhead of calculating frame similarity. This reduces the size of tokens by half while preserving the essential temporal information. Consequently, it decreases the computation of self-attention by a factor of four, according to the self-attention's quadratic complexity, $\mathcal{O}(n^{2})$. Similarly, it reduces the computation of cross-attention by half, based on its linear complexity, $\mathcal{O}(n m)$. Then, the output token processed through attentions replicates the dimensions for each frame, restoring them to their original size. %\Cref{fig:TDTM-Example} depicts the token merging and unmerging.

\iffalse
\begin{figure}[!t]
    \centering
    \includegraphics[width=\columnwidth]{figures/TDTM_Example.pdf}
    % \includegraphics[width=\columnwidth]{figures/temporal-merging.pdf}
    \caption{An illustration of the token merging and unmerging process over the temporal dimension.}
    \label{fig:TDTM-Example}
\end{figure}
\fi

Given the token $\boldsymbol{T}_{in}$ as an input for a self- or cross-attention with the dimension [$B$, $ST$, $C$], where $B$ denotes the batch size, $S$ is the number of pixel patches, $T$ is the number of frames, and $C$ is the channel dimension, the input token $\boldsymbol{T}_{in}$ is merged into $\boldsymbol{T}_{merged}$, with the index $i$, as:

\begin{gather}
    \boldsymbol{T}_{merged} = TDTM_{merge}(\boldsymbol{T}_{in})
    \\    
    TDTM_{merge}(\boldsymbol{T})[i] = \frac{1}{2} (\boldsymbol{T}[:,i,:] + \boldsymbol{T}[:,i+1,:])
\end{gather}

\noindent From this, two adjacent tokens are merged along the temporal dimension, producing the merged token $\boldsymbol{T}_{merged}$ of [$B$, $ST/2$, $C$], which is processed by self- or cross-attention.

After being processed by each attention, $\boldsymbol{T}_{merged}$ is unmerged into $\boldsymbol{T}_{unmerged}$ of the dimension [$B$, $ST$, $C$] as:

\begin{gather}
    \boldsymbol{T}_{unmerged} = TDTM_{unmerge}(\text{Attention}(\boldsymbol{T}_{merged}))
    \\
    TDTM_{unmerge}(\boldsymbol{T})[2i]= \boldsymbol{T}[:,i,:]
\end{gather}

\noindent where Attention$(\cdot)$ is either the self- or cross-attention. 

\input{figures/end_to_end_generated_frame}

Temporal Dimension Token Merging can be selectively applied during the denoising process to minimize potential negative impacts on video quality that may arise from processing merged tokens. Specifically, out of a total of $K$ denoising steps, tokens can be merged only for the initial $k$ steps, while the tokens for the remaining $K{-}k$ steps remain unmerged. This is based on the observation that, when tokens are merged along the temporal dimension, the noise values vary slightly across framesâ€”a phenomenon not observed in image diffusion~\cite{saharia2022palette} that does not involve a temporal dimension in the generation process. However, because noise values in the early denoising steps are less critical, it is expected that applying token merging exclusively for initial steps does not substantially drop video quality.
%\note{The experimental results (Sec. \ref{sec:experiment}) show that such \note{flexible} Temporal Dimension Token Merging effectively manages the noises (e.g., \kyu{dynamic degree}) in videos.}
