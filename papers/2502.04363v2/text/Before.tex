\section{First version of Challenges}
\subsection{Token merging}
\textbf{It's a problem that occurred with the SVD model, not with the Open-Sora.}


When token merging was applied to the spatio(spatial?) block, there was a problem that black noise occurred on the image. The problem was solved by slightly reducing the merge rate compared to the previous one. At the same time, memory efficiency and performance improvement could be achieved.

\subsection{Prompt}
A problem related to the prompt was found while bringing the Open-Sora model and training with the validation dataset of Panda-70M~\cite{chen2024panda70m}. It outputs the sample video closest to the input prompt. For example, when "blue shirt, woman in black" was put as input, the only video containing shoes was created in the training data. In other words, the semantics of the prompt and the alignment of the video that the model would generate did not match. This issue was particularly exacerbated in cases involving videos or prompts containing people.

This problem is presumed to be a lack of training set. Also, it was confirmed that the inaccuracy of the text prompt itself was also a problem. Labeling of the video database was performed using the LLaVA model, when it was possible to find that the wrong prompt was included or the specific prompt was repeated. To solve this problem, more video data and accurate prompts are needed.

\subsection{Transformation of xformer}
Among Open-Sora, the STDiT model is implemented based on the xformer~\cite{xFormers2022} library. To upload Open-Sora to IOS, we need to convert the model from Mac using Coreml. However, the Mac environment does not support xformer. To solve this problem, we replaced the memory\_efficient\_attention function, which is an xformer function, with general attention. Memory efficiency was lowered, but instead, we solved the environmental collision problem.

\subsection{Transformation Using executorch}
\textbf{This is a problem that occurred while using executorch, but we decided not to use it after all.}

In order to run the neural network model in an IOS environment, it is necessary to convert it to coreml. For this, we decided to use executorch. To use executorch, we had to download executorch swift package to mac, but there was a problem that we could not download it because the source was unclear. After discovering that the problem was an environment setting issue that occurred in the release version, we solved the problem. However, since executorch is in the early stages of development, it showed many problems in terms of stability. Therefore, we decided to use TorchScript instead of executorch.

\subsection{Tracing \& Scripting}
There are two main uses for the transformation of the natural network model: Tracing and Scripting. Tracing looks at how the input sample is converted into output in the model and then transforms that structure. Therefore, instead of all the structures of the model, it only transforms the path in which the sample was performed, and this characteristic has the problem that dynamic flow cannot be supported. Scripting is advantageous in that it also supports dynamic flow because it statically transforms the model itself. However, the disadvantage is that it cannot be transformed if an unsupported pytorch or python function is included.

When tracing the STDiT model, we found a problem that the graph transformation did not pass the sanity check by obtaining different results with the same sample input. The problem was identified as the use of cache resulted in different graphs for the same input.

There was also a problem with RoPE Embedding. When using RoPE Embedding, there was also a problem that it could not pass the sanity check. When excluding RoPE Embedding to bypass the problem, it was observed that the conversion was good but the result value changed significantly during actual operation. This is presumed to be because the STDiT model was trained using RoPE Embedding.

In addition, we found a variety of problems related to tracing. If we create a new tensor during the operation, tracing can cause problems because it treats it as a constant. In addition, we found that converting the tensor to Python data type can lead to errors.

Scripting mainly observed errors due to operators not supported by TorchScript. The error was bypassed by converting it to another operator because it did not support @functiontools.lru\_cache and the next operator. Also, the problem occurred because it did not support a variable number of arguments and did not specify the data type of the variable when defining the function. Because of these errors, we decided to proceed in a way that only a portion of the entire model is scripted. The parts that were difficult to convert to scripting and tracing were bypassed using the swift function.

We solved those problems and converted them, but there was a problem the Nan values appeared as output. Open-Sora generates an attention bias, which is a mask that allows queries to refer to only specific keys. To do this, the attention bias consists of 0 or -inf(e.g. -torch.inf) values and is added to the product of query and keys before softmax is taken from the attention. The problem was presumed to have occurred because the value of the added attention bias exceeded the minimum value that can be expressed in float16, and to solve this problem, we set up the upper bound and lower bound using the torch.clamp function. Even with the torch.clamp function, the mask can perform the same role because -65504.0, the smallest value that float16 can express, converges close to zero. In addition, the first transformer paper~\cite{vaswani2017attention} confirmed that the mask value was set to -10000 instead of -inf. \textbf{(BlockDiagonalMask is not considered a problem, so I excluded)}

\subsection{T5}
The Open-Sora uses the T5 model to perform prompt encoding. However, the T5 model(T5-XXL) is too large to run on IOS. To run on IOS, we need to reduce the T5 model to a smaller size, but we found that using a lighter T5 model significantly reduced the performance of the model \textbf{(I remember it is on the open-sora page, but I couldn't find where it was)}.
\bonote{}

The T5 model is a repeatedly \bonote{overlapped} block with a similar structure. \bonote{Thankfully, most of the operations included in the block were well supported and operated properly in CoreML converted model.} Therefore, we divided the T5 model into 24 blocks, and we used only the necessary parts for the operations while running the model. We can see that the first block is 397.4MB, and the rest is 385.9MB, which is large enough to run on the IOS. The part connecting each block was implemented in a swift code by referring to the pipeline of the T5 encoder. In addition, the modules required for the reference were converted to (e.g. embedded\_tokens(n.embedding), final\_layer\_normal) mlpackage.

We found a problem that the calculation results vary depending on the computeUnits (e.g. cpuOnly, cpuAndGpu). We also found that the position bias value changed when implemented with cpuOnly. This was an internal implementation problem, and we bypassed the problem by fixing position bias to the value from the first T5 block. After this, we got the same result regardless of the computeUnits.

When calculating the divided T5 block from IOS to cpuAndGpu, it was observed that Nan occurred due to the case of exceeding the float16 range in the calculation process according to the input. To solve this problem, we set it to float32 when converting. We observed that after each block's operation, the output is the same as the input when using cpuOnly. This is speculated to be a kind of bug that causes the code to operate inside the coreml under the same name if the names of the inputs and outputs are the same before they are converted. That's why no operations on the input were being applied in the code, and we took action to remove that input and immediately use it in the swift code for the next block.

When proceeding with the attention mask, there was a problem that making was not done at all by using 1 as mask token in the code on the applet side that was referenced. This was modified to mask to the smallest value that can be expressed in the data type.\textbf{I think we don't need to mention this, but I wrote it for now.}

\subsection{STDiT}
% It was determined that additional parameters were needed in the reference process. After correcting it, it was converted again. When a function called masked\_select is converted to coreml, an error occurs because there is no function to be converted. It was bypassed by converting it to a supported function.

% A new feature called the MLTensor was announced in IOS 18. Previously, the MLArray declared the input or output of the model, but when using the MLTensor, it became possible to use it in a similar way to the Torch.Tensor. Additionally, a variety of Tensor operations can be implemented. Therefore, we decided that the MLTensor is essential to convert the shift of the scheduler, and we updated the working environment to IOS 18.

\bonote{}

\subsection{VAE}
There was a graph sanity check problem in the temporary decoder of the VAE. This was caused by the dynamic change of the part because the value is added differently according to the number of frame. It was solved by setting num\_frames as a multiple of the time downsampling factor and setting micro\_frame\_size to None. However, because Open-Sora was learned with micro\_frame\_size of 17, performance degradation occurred when setting it to None. To prevent this, we first set micro\_frame\_size and num\_frames to fixed values.

\bonote{Extended from tracing issue, even our VAE had a tracing issue. For efficient computation, Open-Sora was trained to compute 17 micro frames at once. However, due to the fact that frame size itself will be dynamic and be defined as user's will, computational graph can be dynamic if total frame cannot be divided without remainder with micro frame size of 17. As a temporary solution for the issue, we fixed the frame size at this work, yet we can adjust our frame size in multiple of our recommended micro frame size, 17. e.g., 34,51... }


In Open-Sora's temporal decoder, there are situations where the rank extends to 6 or higher during operation. However, CoreML only supports a maximum of 5 rank tensors. To solve this problem, we changed the rank 5 tensor to perform multiple operations, rather than making it with a rank 6 tensor and operating it at once.

\bonote{Fundamentally, the CoreML library only supports tensors no more than rank 5. Most of the tensors computed in other deep learning tasks could be well-computed in this range of rank, however, our diffusion process is for video, therefore they required additional dimension. To sum up, our tensor had dimension of (Batch, Time, Channel, Height, Width) during the computation, and the computation of group normalization inside the computational graph of temporal decoder block was the pain point here. To compute group normalization, there should be additional dimension that indicates the group. However, as we already have five dimensions for our features, we could not convert our model into CoreML directly. To solve this problem, we squeezed out the dimension of batch, compute the group normaliation, then return the dimension of batch after the group normalization. Naturally, the result itself did not affected at all.}