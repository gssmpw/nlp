
\section{Overview: On-device Sora}

\Cref{fig:overview} shows an overview of On-Device Sora, which takes Open-Sora~\cite{opensora} outlined in Sec. \ref{sec:background} as its backbone. On-device Sora enables efficient diffusion-based text-to-video generation on mobile devices by addressing three key challenges (Sec. \ref{sec:challenges}) through three proposed methods: 1) Linear Proportional Leap (LPL) (Sec. \ref{sec:ours1}), 2) Temporal Dimension Token Merging (TDTM) (Sec. \ref{sec:ours2}), and 3) Concurrent Inference with Dynamic Loading (CI-DL) (\Cref{sec:ours3}). These methods can also be applied to other diffusion-based text-to-video generation models, \eg, \jjm{Pyramidal} Flow~\cite{jin2024pyramidal}.


%\parlabel{1) Linear Proportional Leap (Sec. \ref{sec:ours1}).}
%To reduce the excessive number of denoising steps, we propose Linear Proportional Leap, which enables the generation of high-quality videos with nearly half of the required full denoising steps. %while requiring only \note{up to half} denoising steps.%
%This is achieved by leveraging the linear trajectory properties of Rectified Flow~\cite{liu2022flow} employed in STDiT~\cite{opensora}. Instead of performing the full sequence of denoising steps during video generation, it makes a direct leap along the linear trajectory toward the target data at the middle of the denoising process by utilizing the pre-trained flow fields~\cite{liu2022flow}. Thus, Linear Proportional Leap effectively reduces the most time-consuming stage of video generation—the iterative execution of STDiT~\cite{opensora}—by minimizing the number of denoising steps. Notably, this reduction in denoising steps is accomplished without necessitating additional model training, architectural modifications, or data calibration, allowing for saving substantial time and computational resources.% offline.

%and we will explain our surprisingly-simple solution by exploiting the trained flow fields already acquired from the training or distillation with Rectified Flow models, without additional training process or data points to calibrate in \Cref{sec:ours3}, proposed and denoted as Linear Proportional Leap(LPL) in this work.


%\parlabel{2) Temporal Dimension Token Merging (Sec. \ref{sec:ours2}).}
%To lighten the intensive computation required for token processing, we propose Temporal Dimension Token Merging, which merges consecutive video frames in the form of latent representations at each attention layer of STDiT~\cite{opensora}. The proposed token merging reduces the amount of tokens to be processed by half and lowers the computational load of attention modules up to one-quarter. To the best of our knowledge, it is the first to merge tokens in temporal order at attention layers in diffusion-based video generation models.

%\parlabel{3) Concurrent Inference with Dynamic Loading (Sec. \ref{sec:ours3}).}
%To execute large video generative models (i.e., T5 \cite{raffel2020exploring} and STDiT \cite{opensora}) with the limited device memory, we propose Concurrent Inference with Dynamic Loading, which partitions the models into smaller blocks that can be loaded into the memory and executed concurrently. By parallelizing model execution and block loading, it effectively accelerates iterative model inference, e.g., multiple denoising steps. Also, it improves memory utilization while minimizing the block loading overhead by retaining specific model blocks in memory dynamically based on the available runtime memory. 