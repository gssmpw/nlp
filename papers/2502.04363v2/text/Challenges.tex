
\section{Background and Challenges}

We first provide a background of diffusion-based text-to-video generation (\eg Open-Sora~\cite{opensora}), which is the backbone of the proposed On-device Sora, and key challenges in realizing on-device video generation for mobile devices.

\subsection{Background: Diffusion-based Text-to-Video Generation}
\label{sec:background}

%\Cref{Open_Sora_Architecture} illustrates the structure of Open-Sora~\cite{opensora}, generating videos from prompts (texts) through 1) prompt embedding, 2) latent video generation, and 3) video decoding.
%\Cref{Open_Sora_Architecture} illustrates the structure of Open-Sora~\cite{opensora}, 

In general, diffusion-based text-to-video generation models, such as Open-Sora~\cite{opensora}, generate videos from user prompts (texts) through the three stages: 1) prompt (text) embedding, 2) latent video generation, and 3) video decoding.

\iffalse
\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{figures/opensora.pdf}
    \caption{Open-Sora~\cite{opensora} generates realistic videos from the user prompt (text) through three stages: 1) prompt embedding, 2) latent video generation, and 3) video decoding.}
    \label{Open_Sora_Architecture}
\end{figure}
\fi

\parlabel{1) Prompt (Text) Embedding.}
The first stage of diffusion-based text-to-video generation is to map a user prompt, a textual description of the desired video, to an embedding vector, which is used as input for the subsequent video generation stage. For example, to produce prompt embeddings from user texts, Open-Sora employs T5 (Text-to-Text Transformer)~\cite{raffel2020exploring}, a language model specifically fine-tuned to support video generation tasks.

\parlabel{2) Latent Video Generation.}
The next stage is to generate the latent video representation conditioned on the prompt embedding obtained from language models, \eg, T5 (Text-to-Text Transformer)~\cite{raffel2020exploring}. To this end, for instance, Open-Sora employs STDiT (Spatial-Temporal Diffusion Transformer)~\cite{opensora}, a diffusion-based text-to-video model using the Markov chain~\cite{zhang2023text}. Since maintaining temporal consistency across video frames is essential in video, STDiT~\cite{opensora} applies the spatial-temporal attention mechanism~\cite{yan2019stat} to the patch representations. It allows effective learning of temporal features across video frames through the temporal attention, enhanced by incorporating rope embeddings~\cite{su2024roformer}. During the forward process of STDiT, the Gaussian noise $\boldsymbol{\epsilon}_{k}$ is iteratively added to the latent video representation $\mathbf{x}_{k}$ over $K$ steps, transforming the intact video representation $\mathbf{x}_{0}$ into the complete Gaussian noise $\mathbf{x}_{K}$ in the latent space with the forward distribution $q(\mathbf{x}_{k}|\mathbf{x}_{k-1})$ as:
\begin{equation}
\begin{split}
    \mathbf{x}_{k} &= \sqrt{1-\beta_{k}} \mathbf{x}_{k-1} + \sqrt{\beta_{k}} \boldsymbol{\epsilon}_{k}
    \\
    q(\mathbf{x}_t | \mathbf{x}_{k-1}) &= \mathcal{N} (\mathbf{x}_{k} ; \sqrt{1-\beta_{k}} \mathbf{x}_{k-1},\beta_{k} \mathbf{I})    
\end{split}
    \label{eq:forward}
\end{equation}

\noindent where $\beta_{k}$ is the parameter determining the extent of noise. 

To generate the latent representation $\mathbf{x}_{0}$, the noise $\boldsymbol{\epsilon}_{k}$ is repeatedly removed (denoised) from the complete Gaussian noise $\mathbf{x}_{K}$ through the reverse process using the estimated noise with the reverse distribution $p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_t)$ modeled by STDiT~\cite{opensora} with the parameter set $\boldsymbol{\theta}$, as follows:
\begin{equation}
\begin{split}
    \mathbf{x}_{k-1} &= \boldsymbol{\mu}_{\boldsymbol{\theta}} (\mathbf{x}_{k}, k) + \sqrt{\beta_{k}} \boldsymbol{\epsilon} ~\text{ where}~ \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})
    \\
    %{p_{\theta}(\mathbf{x}_{0:T}) &= p(\mathbf{x}_T)\prod_{t=1}^Tp_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)}
    %\\
    p_{\boldsymbol{\theta}}(\mathbf{x}_{k-1}|\mathbf{x}_{k}) &= \mathcal{N}(\mathbf{x}_{k-1}; \boldsymbol{\mu}_{\boldsymbol{\theta}} (\mathbf{x}_{k}, k), \boldsymbol{\Sigma}_{\boldsymbol{\theta}} (\mathbf{x}_{k}, k))
\end{split}
    \label{eq:reverse}
\end{equation}

\noindent where $\boldsymbol{\mu}_{\boldsymbol{\theta}} (\mathbf{x}_{k}, k)$ and $\boldsymbol{\Sigma}_{\boldsymbol{\theta}} (\mathbf{x}_{k}, k)$ is the mean and variance of $\mathbf{x}_{k}$ estimated by STDiT training, respectively. This reverse process repeatedly denoises $\mathbf{x}_{k}$ into $\mathbf{x}_{k-1}$ over a large number of $1 {\leq} k {\leq} K$ denoising steps (\ie, from dozens to thousands of steps), eventually generating the de-noised latent video representation $\mathbf{x}_0$ close to the intact representation.% from the noised $\mathbf{x}_{K}$. %This latent representation is subsequently converted into the final video by a Variational Autoencoder (VAE) in the following stage.

%Similarly, video diffusion models operate by progressively adding noise to each video frame during the forward process. 

%However, maintaining temporal consistency between frames is essential in video generation. Open-Sora's video diffusion model addresses this by taking STDiT (Spatial-Temporal Diffusion Transformer)~\cite{?} as its backbone model, which employs spatial-temporal attention~\cite{?} with patches. STDiT effectively learns temporal consistency across video frames through the temporal attention, incorporating Rope Embedding~\cite{su2024roformer} to enhance this process. To generate video, STDiT performs multiple steps (typically ranging from dozens to thousands) of the reverse process to infer the de-noised latent video representation $\mathbf{x}_{0}$, as in {eq:reverse}. This latent representation is subsequently converted into the final video by a Variational Autoencoder (VAE) in the following stage.

%To generate video, STDiT is executed multiple steps (usually dozens to thousands of steps) to make inference on de-nosing (\Cref{eq:reverse}), producing the latent video representation, which will be converted into the final video by VAE in the next step.

%Consequently, by de-nosing $x_t$ with $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$ multiple reverse steps (usually dozens to thousands of steps), data is generated.

%Video diffusion models takes the similar mechanism; they repeatedly add noises to each video frame during the forward process. However, since \note{temporal consistency} is crucial between video frames, Open-Sora's video diffusion model takes STDiT (Spatial-Temporal Diffusion Transformer)~\cite{?} employing spatial-temporal attention with patches as the backbone model, where temporal consistency between video frames is effectively learned through temporal attention through Rope Embedding~\cite{su2024roformer}. 


%It uses spatial-temporal attention by applying temporal attention immediately after each spatial attention, reducing computational costs. 

%To adapt it for video generation, temporal attention was added. DiT converts latent representations into patches via a patchify layer and applies positional embeddings to all input tokens.

%For stability, AdaIN~\cite{huang2017arbitrary} and layer normalization are applied to temporal attention, and QK-normalization~\cite{dehghani2023scaling} is applied to all attention layers.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{On-device Sora enables training-free text-to-video generation directly on the device by employing three key methods: 1) Linear Proportional Leap (LPL), 2) Temporal Dimension Token Merging (TDTM), and 3) Concurrent Inference with Dynamic Loading (CI-DL).}
    \label{fig:overview}
\end{figure*}

\parlabel{3) Video Decoding.}
Finally, the latent video representation $\mathbf{x}_0$ generated from STDiT is decoded and up-scaled into the human-recognizable video through the VAE (Variational Autoencoder)~\cite{doersch2016tutorial}. For example, the VAE employed in Open-Sora~\cite{opensora} utilizes both 2D and 3D structures; the 2D VAE is based on SDXL~\cite{podell2023sdxl}, while the 3D VAE adopts the architecture of MAGVIT-v2~\cite{yu2023language}. %\note{the name of VAE model?}

\subsection{Key Challenges in On-device Video Generation}
\label{sec:challenges}

%Although Open-Sora~\cite{opensora} has enabled the generation of high-quality videos from texts, adapting this advanced generative capability to mobile devices presents key challenges.

%\begin{figure}[!bht]
    %\centering
    %\includegraphics[width=\columnwidth]{figures/time-profiling.pdf}
    %\caption{\note{Draw again. Should we mention that it is measured on an GPU?} \isu{REPLACE WITH Table~\ref{tab:time-profiling}}}
    %\label{fig:time-profiling}
%\end{figure}

\parlabel{C1) Excessive Denoising Steps.}
\Cref{tab:time-profiling} shows the latency of each model component in Open-Sora~\cite{opensora}, where the latent video generation process (denoising process) performed by STDiT is the most time-consuming. %, presenting a challenge for executing STDiT on mobile devices with limited computational resources. 
That is because a substantial number of denoising steps  %\note{($K=30$ or $50$)} 
is required to remove the noise $\boldsymbol{\epsilon}_{k}$ from $\mathbf{x}_{K}$ to obtain $\mathbf{x}_{0}$ during latent video generation~\cite{song2020denoising}. Such extensive denoising iterations %, combined with the high computational complexity of each denoising step, 
presents considerable challenges on mobile devices with constrained computational capabilities. %To reduce the number of denoising steps, Open-Sora employs Rectified Flow~\cite{liu2022flow}, which redefines the objective of diffusion models, inducing the model to directly approximate the straight mapping trajectory between the initial and target distributions. In image generation, it enables a single-step denoising without a significant loss in image quality.
%However, in video generation models~\cite{abdi2007singular,opensora}, the accumulated variance across sequential video frames prevents the trajectory from being fully represented with a single drift. As a result, 
%Open-Sora~\cite{opensora} requires at least $K=30$ or $50$ denoising steps to produce high-quality videos. 
While the numerous denoising steps are manageable in server-level environments—where the complete denoising process typically finishes within one minute—on mobile devices, it may take several tens of minutes. %Consequently, the total denoising latency for multiple steps can accumulate to several tens of minutes. 
Accordingly, an effective approach to reduce denoising steps without model modification or re-training %for resource-constrained mobile devices 
is essential to enable on-device video generation.

\begin{table}[!htb]
%\setlength{\tabcolsep}{20pt}
\renewcommand{\arraystretch}{1}
\caption{The number of executions (iterations) of each model component (\ie, T5~\cite{raffel2020exploring}, STDiT~\cite{opensora}, and VAE~\cite{doersch2016tutorial}) in Open-Sora~\cite{opensora} and their total latencies on the iPhone 15 Pro~\cite{apple2023}.%\isu{The time allocated to each component of Open Sora in video generation on iPshone. The term number of iterations refers to the total count of times the model is executed.}
}
\label{tab:time-profiling}
\setlength\tabcolsep{5.5pt}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{c|rrr}
\toprule[1pt] \hline
\textbf{Component} 
    & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}} Iterations\end{tabular}}} 
    & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Inference Time (s)\end{tabular}}} 
    & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}Total Latency (s) \end{tabular}}} \\ \hline
T5~\cite{raffel2020exploring}        &       1     &          110.505     &            110.505\\
STDiT~\cite{opensora}     &       50    &           35.366    &           1768.320\\
VAE~\cite{doersch2016tutorial}       &       1     &           135.047   &            135.047\\ \toprule[1pt]
\end{tabular}%
}
\end{table}

%\note{\sout{However, these approaches have only been applied to image diffusion models, and there are no existing methods where they have been applied to video diffusion.}}

\parlabel{C2) Intensive Token Processing.}
While a large number of denoising steps poses a significant challenge to video generation on mobile devices, even a single denoising step itself is computationally intensive. The primary reason is that the computational complexity of the attention mechanism~\cite{niu2021review} in STDiT~\cite{opensora} grows quadratically with the token size, which significantly increases the computational load for token processing and, consequently, the model's inference latency. To address this challenge, Token merging~\cite{bolya2023token} has been proposed to improve the throughput of vision Transformer models, which progressively merges similar visual tokens within the transformer to accelerate the model inference latency by reducing the size of tokens to be processed. %, without requiring additional model training. 
While token merging has been applied to diffusion models, it is only applied to spatial tokens~\cite{bolya2022token,bolya2023token} and has not been applied to the temporal tokens in video diffusion models, such as STDiT~\cite{opensora}. Thus, a novel token merging method is required to improve the computational efficiency of token processing within well-trained video generation models, while preserving high video quality.

\parlabel{C3) High Memory Requirements.}
\Cref{fig:peak-memory} shows the memory requirements of model components in Open-Sora~\cite{opensora}, \ie, VAE~\cite{doersch2016tutorial}, T5~\cite{raffel2020exploring}, and STDiT~\cite{opensora}, where their cumulative memory demand, \ie, 23 GB, surpasses the memory capacity of many mobile devices. For instance, the iPhone 15 Pro~\cite{apple2023}, with 8 GB of memory, restricts the available memory for a single application to 3.3 GB for system stability. Furthermore, the individual memory requirements of T5 and STDiT exceed 3.3 GB, creating challenges in loading them into memory. In addition, some memory must be reserved for model execution (inference), exacerbating memory shortages on mobile devices. Thus, limited device memory is another challenge that should be addressed to enable on-device video generation.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\columnwidth]{figures/model_size.pdf}
    \caption{The size of Open-Sora models: T5~\cite{raffel2020exploring} (18.00 GB), STDiT~\cite{opensora} (4.50 GB), and {VAE~\cite{doersch2016tutorial} (0.82 GB)}, which exceeds the available memory capacity of the iPhone 15 Pro~\cite{apple2023} (3.3 GB).}
    \label{fig:peak-memory}
\end{figure}