\section{Trajectory Diffusion and Analysis}\label{sec:trajectory overall}
In this section, we introduce the trajectory diffusion and analyze its configuration, principles, and convergence.
\subsection{Trajectory Diffusion}\label{sec:trajectory diffusion}
Although we have established a general framework to meta-train the diffusion model, directly using it leads to suboptimal performance. We denote the above method that combines meta-learning and vanilla diffusion as Lv-Di, \ie, Learning to Learn Weight Generation via Vanilla Diffusion. The right side of Figure~\ref{fig:landscape_2d} visualizes Lv-Diâ€™s inference chains in a 2D PAC-reduced weight landscape. In Omniglot's~\cite{omniglot} 5-way 1-shot classification tasks, Lv-Di constrains only the inference endpoint, ignoring behavior along the inference chain. As a result, weights generated by Lv-Di will deviate from the ground truth (\ie, optimal weights). In Lt-Di, we use the whole optimization trajectory $Tra=\{\theta^h_m, ... ,\theta^h_0\}$ rather than a single optimal weight $\theta^h_0$ to guide the inference chain. According to the protocol given by DDPM, $x_0$ equals $\theta_0$, and $x_T$ is Gaussian noise that equals initial weights $\theta^h_m$. Thus, we can denote the trajectory as $Tra=\{\theta^h_m=x_{r(m)}=x_T,...,\theta^h_i=x_{r(i)},..., \theta^h_0=x_0\}$. Note that $m < T$, thus we need a function $r(\cdot)$ that maps points in the optimization trajectory $Tra$ to appropriate positions in the inference chain $C_{infer}=\{x_T,...,x_0\}$. We detail the selection of the mapping function $r(\cdot)$ in Section~\ref{sec:Trajectory Selection}. We derive the total loss $L$ for trajectory diffusion as follows.

\begin{theorem}\label{the:L_k}
Given decay sequence $\{\alpha_0,...,\alpha_T\}$, and trajectory weight $x_k$. Let the inference equation align with the vanilla diffusion algorithm, \ie, 
\begin{equation}
x_{t-1}=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}}\epsilon_{\phi}.
\end{equation}
Then the diffusion model $\epsilon_{\phi}$ can recover $x_k$ from standard gaussian noise $x_T$ in $T-k$ steps, when $\epsilon_{\phi}$ is trained by
\begin{align*}
L_k&=\sum_{t=k+1}^T||\sqrt{1-\bar{\alpha}_t^k}\epsilon_{\phi}(x_t, t-k)-\sqrt{1-\bar{\alpha}_t}\epsilon_k||^2,
\end{align*}
where $x_t=\sqrt{\bar{\alpha}^k_t} \mathbf{x}_k + \sqrt{1 - \bar{\alpha}^k_t}\epsilon_k$, $\bar{\alpha}_t=\prod_{j=1}^t \alpha_j$,  $\bar{\alpha}^k_t=\prod_{j=k+1}^t \alpha_j$, and $\epsilon_k$ denotes standard gaussian noise.
\end{theorem}

The proof is detailed in Appendix~\ref{sec:appendix_1}. When $k=0$, the loss given above is equivalent to the vanilla diffusion loss $L_0$, indicating that trajectory diffusion extends the original version. When $L_0$ and $L_k$ are optimized together, the denoising process enables the recovery of Gaussian noise $x_T$ through two distinct pathways: a complete $T$-step process to reach $x_0$, and a partial $T-k$ step process to reach $x_k$. Furthermore, due to the shared inference process defined in Equation~\ref{eq:DDPM_inference}, \textbf{the trajectory weight $x_k$  appears within the $T-k$ steps of $x_0$'s inference sequence}. As shown in the left side of Figure~\ref{fig:landscape_2d}, compared to the vanilla diffusion model, additional trajectory weights can constrain the middle portion of the inference chain, thereby improving the generation quality of $x_0$ at the end of the chain. Based on the above approach, \textbf{we can use each observed data point in trajectory $Tra=\{x_{r(m)},...,x_{r(i)},...,x_{r(0)}\}$ to improve accuracy and efficiency.} The total trajectory loss $L$ can be written as: 
\begin{align}\label{eq:total_loss}
    L&=\sum_{i=0}^{m} \gamma_i L_i\notag\\
    &=\sum_{i=0}^{m} \gamma_i \sum_{t=k+1}^T||\sqrt{1-\bar{\alpha}_t^k}\epsilon_{\phi}(x_t, t-k)-\sqrt{1-\bar{\alpha}_t}\epsilon_k||^2 \notag\\
    &\cdot \mathbf{1}_{\{t \in (k,r(i+1)]\}}
\end{align}
where $k=r(i)$, $x_t=\sqrt{\bar{\alpha}^k_t} \mathbf{x}_k + \sqrt{1 - \bar{\alpha}^k_t}\epsilon_k$, $\bar{\alpha}_t=\prod_{j=1}^t \alpha_j$, $\bar{\alpha}^k_t=\prod_{j=k+1}^t \alpha_j$, $\gamma_i$ is the coefficient of $L_i$, and $\epsilon_k$ is a standard Gaussian noise.

Theoretically, when $t \in (k,r(i+1)]$, we can use all $x_0,...,x_k$ to guide this sub-inference chain. Nevertheless, it leads to an overwhelming learning objective and more computational overhead. Therefore, we only use the nearest point $x_k$ for learning, and that is why we need to multiply $\mathbf{1}_{\{t \in (k,r(i+1)]\}}$ in $L_i$.

\subsection{Analysis}\label{sec:analysis}
In this section, we will discuss the configuration and principle issues raised in Section~\ref{sec:weight preparation} and Section~\ref{sec:trajectory diffusion}, and the convergence of the weight generation paradigm:
\begin{itemize}
    \item How to configure trajectory diffusion?
    \item How does trajectory diffusion enable higher accuracy with efficient training and inference processes?
    \item The convergence properties of the weight generation paradigm and how to improve them.
\end{itemize}

\subsubsection{Trajectory Diffusion Configuration}\label{sec:Trajectory Selection}
First, we need to choose an index mapping function $r(\cdot)$ that maps the trajectory weights in $Tra_i=\{\theta_h^m, ... ,\theta_h^0\}$ to appropriate positions within the inference chain $C_{infer}=\{x_T,...,x_0\}$. In this paper, we map uniformly. Let the length of the optimization trajectory be $m+1$, the length of the inference chain be $T+1$, and assume that $T+1$ be divisible by $m+1$, a trivial index mapping function $r(.)$ can be written as:
\begin{equation}\label{eq:map index}
    r(i)=\frac{i*T}{m}
\end{equation}

Second, we need to determine the length of the optimization trajectory. This issue entails a trade-off between loss function complexity and accuracy gain from more trajectory weights. Under the constraint of three GPU hours, Figure~\ref{fig:loss_trajectory} shows the relationship between the trajectory length and reconstruction MSE (Mean Square Error) between the predicted weight and the ground truth weight. We perform a case study on Omniglot~\cite{omniglot} and Mini-Imagenet~\cite{miniImagenet} datasets for 5-way tasks. When the trajectory length is 2, it only includes $x_T$ and $x_0$ , which is equal to the vanilla diffusion model. As shown in Figure~\ref{fig:loss_trajectory}, although the best trajectory length may vary depending on task characteristics, all trajectory-based approaches outperform the vanilla version. These results suggest that trajectory diffusion can improve the generation quality. We set the default optimization trajectory length to 4, \ie, $m=3$ in subsequent experiments.

\subsubsection{Training and Inference Acceleration}\label{sec:discuss acceleration}

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{trajectory_length.pdf}
        \caption{Trajectory length and MSE trade-off on 5-way tasks with 1-shot and 5-shot.}
        \label{fig:loss_trajectory}
    \end{minipage}
    \hspace{0.03\linewidth}
    \begin{minipage}[t]{0.47\linewidth}
        \centering
        \includegraphics[width=\linewidth]{loss_hie.pdf}
        \caption{Convergence speed of each $L_i$ on Omniglot 5-way 1-shot task.}
        \label{fig:loss_hie}
    \end{minipage}
\end{figure}

We can loosely reason by treating $L$ as a linear combination of multiple optimization objectives. When $m=3$, Figure~\ref{fig:loss_hie} records three optimization objectives $L_0$, $L_1$, and $L_2$, corresponding to three trajectory weights, $\theta_0$, $\theta_1$, and $\theta_2$, respectively. On Omniglot 5-way 1-shot tasks, $L_2$ will converge first. This is because $\theta_{2}$ is closest to Gaussian-initialized weight ${\theta_3}$ and requires the fewest training epochs. Thus, the remaining learning target is to reconstruct ${\theta_0}$ starting from $\theta_2$. By recursively applying this, a $T$-step diffusion problem breaks down into $m$ shorter ones. Optimizing the original $L_0$ is like finding a path within a hypersphere of radius $T$, while optimizing trajectory loss $L$ is like working within $m$ smaller hyperspheres of radius $T/m$. Obviously, the latter is easier to learn. As a result, with the same computational resources, the trajectory diffusion can achieve lower reconstruction error. Conversely, given the same target accuracy, the latter can tolerate fewer diffusion steps, thereby accelerating training and inference.

Figure~\ref{fig:trade-off} compares the trade-off curves between the vanilla diffusion and the trajectory diffusion on Omniglot and Mini-Imagenet 5-way 1-shot tasks. The left figure shows GPU-Hours vs. MSE trade-off and the right one is Diffusion Steps vs. MSE trade-off. As training progresses, the trajectory diffusion increasingly outperforms the vanilla version. The reason is that, in the early stages of training, trajectory diffusion primarily focuses on the learning of prerequisite objectives, such as $L_2$, rather than the ultimate target $L_0$. Once the learning of these prerequisite objectives is completed, trajectory diffusion converges faster to the optimal weights, \ie, $\theta_0$. Benefiting from this, as shown in the right side of Figure~\ref{fig:trade-off}, trajectory diffusion can tolerate fewer diffusion steps for the same MSE, thereby enhancing both training and inference efficiency. 


\subsubsection{Convergence Analysis and Improvement}\label{sec:SAM}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{combined_gpu_hours_vs_diffusion_steps_mse.pdf}
    \caption{Comparison of trade-off curves between the vanilla diffusion and the trajectory diffusion on Mini-Imagenet and Omniglot 5-way 1-shot tasks. The left figure denotes GPU hours vs. MSE trade-off, and the right one denotes Diffusion Steps vs. MSE trade-off.}
    \label{fig:trade-off}
\end{figure}

Learning to generate weights is an indirect approach compared to learning directly from downstream task samples, and its convergence can be challenging to guarantee. By assuming an upper bound on the generative model's reconstruction error, the following analysis applies to all weight generation algorithms.
\begin{assumption}\label{assumption}
\noindent
\begin{enumerate}
    \item \textbf{Reconstruction error upper bound.}
    The reconstruction error produced by the generative model is bounded by $c$.
    \item \textbf{Loss function upper bound.}
    The downstream task loss $L_d(\cdot) \leq \psi$.
    \item \textbf{l-smoothness.}
    There exists a constant $l$ such that for all weights \( \theta, \theta' \in \mathbb{R}^n \)
    \[
    \|\nabla L_d(\theta) - \nabla L_d(\theta')\| \leq l \|\theta - \theta'\|
    \]
    \item \textbf{$\mu$-strong convexity.}
    There exists a constant \( \mu > 0 \) such that for all \( \theta,\theta' \in \mathbb{R}^n \),
    \[
    L_d(\theta') \geq L_d(\theta) + \nabla L_d(\theta)^\top (\theta' - \theta) + \frac{\mu}{2} \| \theta' - \theta \|^2.
    \]
    \item \textbf{Hessian matrix upper bound.}
    The Hessian matrix eigenvalue around the neighborhood of optimal weight $\theta^*$ is bounded by $\lambda$.
\end{enumerate}
\end{assumption}

\begin{theorem}\label{theorem:emperi error}
When Assumption~\ref{assumption} holds, Lt-Di's cumulative empirical error can be bound by:
$$
    L_d(\hat{\theta})-L_d(\theta^*) \leq \frac{\lambda}{2} \left[c+\frac{2\psi}{\mu}\left(1 - \frac{\mu}{l}\right)^{epoch}\right],
$$
where $epoch$ is the number of update steps used in the weight preparation stage, and $\hat{\theta}$ is the weight predicted by the generative model.
\end{theorem}

The proof, provided in Appendix~\ref{sec:appendix_2}, relies on the triangle inequality to decompose the accumulated error into weight preparation error and reconstruction error. We make a $\mu$-strong convex assumption here, but subsequent analysis and improvement do not rely on this property, thus preserving the practicality of our derivation. Theorem~\ref{theorem:emperi error} shows that, compared to direct learning methods, the reconstruction error of weight generation algorithms affects the upper bound of cumulative error in only a linear manner. Furthermore, this upper bound can be effectively improved by reducing the maximum eigenvalue $\lambda$. Penalizing the Hessian matrix is the simplest way to accelerate convergence, but it is computationally unacceptable. 

In this paper, we penalize $\lambda$ by constraining the curvature near the neighborhood of the optimal solution. We use Sharpness-Aware Minimization (SAM)~\cite{SAM} in the weight preparation stage to achieve the above target. As mentioned in Section~\ref{sec:weight preparation}, adding the additional SAM component doesn't incur additional time overhead. The process of SAM is shown in Algorithm~\ref{alg:SAM}.


