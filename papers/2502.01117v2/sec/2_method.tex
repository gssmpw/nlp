\section{Methodology}\label{sec:method}
In this section, we introduce Lt-Di's workflow and provide a preliminary in Appendix~\ref{sec:appendix_3}. The workflow of Lt-Di is shown in Figure~\ref{fig:overview}, which consists of three stages: weight preparation, meta-training, and evaluation.
\\
\par
\subsection{Weight Preparation}\label{sec:weight preparation}
\begin{algorithm}[t] % [t] to position at the top 
\caption{Weight Preparation}
\label{alg:SAM}
\begin{algorithmic}[1]
    \REQUIRE Model weights $\theta=\{\theta^h,\theta^b\}$, downstream loss function $L_d(\theta)$, perturbation magnitude $\rho$, and learning rate $\alpha$.
    \FOR{each training step}
        \STATE Add noised and rotated data (\ie, data augmentation)
        \STATE $\epsilon \gets \rho \frac{\nabla L(\theta)}{\|\nabla L(\theta)\|}$
        \STATE $g_{\text{worst}} \gets \nabla_{\theta^h} L(\theta + \epsilon)$ \text{~(\ie, SAM)}
        \STATE $\theta^h \gets \theta^h - \alpha \cdot g_{\text{worst}}$
        \STATE Record $\theta^h$
    \ENDFOR
    \STATE Select $\theta^h$ and construct $Tra=\{\theta^h_m,...,\theta^h_0\}$
    \STATE \textbf{Return} $Tra$
\end{algorithmic}
\end{algorithm}

During the weight preparation stage, we collect weights and construct an optimization trajectory $Tra_i$ corresponding to each task $T_i$. In alignment with research demonstrating the significant influence of final layers in fine-tuning~\cite{ANIL, SVCCA}, and to optimize computational efficiency, we focus our attention specifically on the downstream network head $\theta^h$ while maintaining the body $\theta^b$ freezing. Specifically, we use $T_i$ in the training set to update $\theta_h$ and record the corresponding optimization trajectory $Tra_i=\{\theta^h_m, ... ,\theta^h_0\}$. Note that $\theta^h_m$ is Gaussian-initialized weight and $\theta^h_0$ denotes the last weight recorded in the updating process\footnote{This unusual index is set to match the index of diffusion algorithm in the following section.}. Considering storage overhead and the complexity of the optimization problem, we don't collect all weights generated in each epoch. We detail the number of trajectory weight in Section~\ref{sec:Trajectory Selection}.


Benefiting from such an indirect learning approach, we can advance functional components from inner-loop to the weight preparation stage. As shown in Figure~\ref{fig:overview}, we add Sharpness Aware Minimization (SAM) for convergence efficiency and data augmentation for robustness. Meanwhile, the meta-training phase can start synchronously once a few weights are collected. Therefore, adding functional components during weight preparation doesn't incur additional time overhead. The specific process of weight generation stage is detailed in Algorithm~\ref{alg:SAM}.
\\
\par
\subsection{Meta-Training.}
In the meta-learning stage, we use REPTILE~\cite{REPTILE} as the framework to ensure efficiency. Our meta-objective is defined specifically as learning a diffusion model $\epsilon_\phi$ (\ie, denoiser) that can recover the Gaussian-initialized weights to the optimal weight $\theta^{h^*}$ for the unseen task $T_{n+1}$. We employ the same U-Net denoiser architecture given by~\citet{stable_diffusion}, and maintain this setup across all experiments in this paper. To learn on multiple tasks and enhance meta-learning, the denoiser needs additional condition states represented by the task embedding $Emb_{T_i}=\theta^b(T_i)$\footnote{We will omit the diffusion model's condition state $Emb_{T_i}$ in the following section for brevity.}. Considering empirical optimal weight $\theta^h_0$ as the diffusion target $x_0$, we follow the setup of vanilla diffusion algorithm Denoising Diffusion Probabilistic Model (DDPM)~\cite{DDPM}. One of an inner-loop loss $L_0$ can be written as:
$$
    L_0=\sum_{t=1}^T||\epsilon_{\phi_i}(Emb_{T_i},t,x_t)-\epsilon_0||^2,
$$
and the inner-loop update equation can be written as:
$$
    \phi_i=\phi_i-\eta\nabla L,
$$
where $t$ is the timestamp, $\eta$ is the inner-loop learning rate, $x_t=\sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon_0$, and $\bar{\alpha}_t=\prod_{j=1}^t \alpha_j$. Note that $L$ is the total loss of trajectory diffusion and the derivation of $L$ is detailed in Section~\ref{sec:trajectory diffusion}. According to REPTILE, the meta-update process can be written as:
$$
\phi=\phi+\frac{\zeta}{n}\sum_{i=1}^n({\phi_i}-\phi),
$$
where $\zeta$ is the outer-loop learning rate and $n$ is the total number of training tasks. The overall meta-learning process is shown in Algorithm~\ref{alg:training}.
\\
\par
\subsection{Downstream Task Evaluation.}
Based on the vanilla diffusion algorithm, the inference process of Lt-Di can be written as:
\begin{equation*}\label{eq:DDPM_inference}
x_{t-1}=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}}\epsilon_{\phi}.
\end{equation*}
Through multiple inference processes, the well-trained denoiser $\phi^*$ can recover the optimal head $\theta^{h^*}$ from Gaussian-initialized weights.

% The overall evaluation algorithm is shown in Algorithm~\ref{alg:DDIM}.


\begin{algorithm}[t]
\caption{Lt-Di Training (whole-batch version)}
\label{alg:training}
\begin{algorithmic}[1]
    \REQUIRE Initial model weights $\phi$, inner-loop steps $K$, learning rate $\eta$, meta-learning rate $\zeta$, and trajectory set $\{Tra_1, ..., Tra_n\}$.
    \WHILE{not convergence}
        \FOR{$i = 1, \dots, n$}
            \STATE $\phi_i = \phi$
            \FOR{$t = 1, \dots, K$}
                \STATE $\phi_i = \phi_i - \eta \alpha \nabla_{\phi_i}L(\phi_i, Tra_i, t)$
            \ENDFOR
            \STATE $\Delta_{\phi_i} = \phi_i - \phi$
        \ENDFOR
        \STATE $\phi = \phi + \frac{\zeta}{n}\sum_{i=1}^n \Delta_{\phi_i}$
    \ENDWHILE
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[t]
% \small
% \caption{Downstream Task Evaluation}
% \label{alg:DDIM}
% \begin{algorithmic}[1]
%     \REQUIRE Denoiser $\phi$, number of inference steps $m$, index mapping function $r(\cdot)$, initial noise $x_T$, decay schedule $\{\alpha_i\}_{0=1}^T$, and unseen task $T_{n+1}$.
%     \STATE \text{Sample} $x_T \sim \mathcal{N}(0, I)$
%     \STATE $t = T$
%     \WHILE{$t \geq 0$}
%         \STATE $x_{s} \leftarrow$ Equation~\ref{eq:DDIM}
%         \STATE $t = s$
%     \ENDWHILE
%     \STATE $\theta^{h^*} = x_0$
%     \STATE $\theta = \{\theta^{h^*}, \theta^b\}$
%     \STATE \textbf{Return} $L_d(\theta, T_{n+1})$
% \end{algorithmic}
% \end{algorithm}







