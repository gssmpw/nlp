\clearpage
\setcounter{page}{1}
\setcounter{theorem}{0}
\setcounter{assumption}{0}
\setcounter{equation}{0}
\setcounter{section}{0}

\clearpage
\appendix
\section{Theorem and Proof}\label{sec:appendix_1}
Readers can refer to the derivation process of DDPM~\cite{understand_DDPM} to understand the following derivation.
\begin{theorem}\label{the:L_k}
Given decay sequence $\{\alpha_0,...,\alpha_T\}$, and trajectory weight $x_k$. Let the inference equation align with the vanilla diffusion algorithm, \ie, 
\begin{equation}\label{eq:DDPM_inference}
x_{t-1}=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}}\epsilon_{\phi}.
\end{equation}
Then the diffusion model $\epsilon_{\phi}$ can recover the image $x_k$ from standard Gaussian noise $x_T$ in $T-k$ steps, when $\epsilon_{\phi}$ is trained by
\begin{align*}
L_k&=\sum_{t=k+1}^T||\sqrt{1-\bar{\alpha}_t^k}\epsilon_{\phi}(x_t, t-k)-\sqrt{1-\bar{\alpha}_t}\epsilon_k||^2,
\end{align*}
where $x_t=\sqrt{\bar{\alpha}^k_t} \mathbf{x}_k + \sqrt{1 - \bar{\alpha}^k_t}\epsilon_k$, $\bar{\alpha}_t=\prod_{j=1}^t \alpha_j$,  $\bar{\alpha}^k_t=\prod_{j=k+1}^t \alpha_j$, and $\epsilon_k$ denotes standard Gaussian noise.
\end{theorem}


% and a diffusion process $p(x_t|x_{t-1})=\mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\bm{I})$.

\begin{proof}
Consider the diffusion chain $C_{diff}=\{x_k,...,x_T\}$ and let sequence  $\{\alpha_{k+1},...,\alpha_T\}$ be the corresponding decay schedule. 
According to vanilla diffusion model, to maximize the likelihood $p(x_k)$ of observed data $x_k$, we need to minimize denoising matching term
\begin{equation}\label{eq:KL divergence}
    \sum_{t=k+1}^{T} \mathbb{E}_{q(x_t | x_k)} \left[ \mathrm{D}_{\mathrm{KL}}\left( q(x_{t-1} | x_t, x_k) \parallel p_{\phi}(x_{t-1} | x_t) \right) \right].
\end{equation}
In the KL divergence bracket, the left term can be expended by the Bayesian Theorem. The right term is the inference process to be modeled with $\phi$, whose expectation is given by Equation~\ref{eq:DDPM_inference}. According to Bayes Theorem,
\begin{align}\label{eq:bayesian}
    q(x_{t-1} \mid x_t, x_k) = \frac{q(x_t \mid x_{t-1}, x_k) \, q(x_{t-1} \mid x_k)}{q(x_t \mid x_k)}.
\end{align}
According to the Markov Rule and standard diffusion process, 
\begin{align}\label{eq:4}
    q(x_t \mid x_{t-1}, x_k) &= q(x_t \mid x_{t-1}) \notag\\
    &\sim \mathcal{N}(x_t;\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\mathbf{I}).
\end{align}
Recursively using the diffusion process on $\{x_k,..., x_{t-1}\}$, we have
\begin{equation}\label{eq:5}
    q(x_t|x_k) \sim \mathcal{N}(x_t; \sqrt{{\bar{\alpha}_t^k}} \, x_k, (1 - {\bar{\alpha}_t^k}) \, \mathbf{I}),
\end{equation}
where ${\bar{\alpha}_t^k}=\prod_{j=k+1}^t \alpha_j$. Note that the coefficients here differ from those in the vanilla diffusion algorithm. 
According to Equation~\ref{eq:5}, $q(x_{t-1}|x_k)$ can be written as
\begin{equation}\label{eq:6}
    q(x_{t-1}|x_k) \sim \mathcal{N}(x_{t-1}; \sqrt{{\bar{\alpha}_{t-1}^k}} \, x_k, (1 - {\bar{\alpha}_{t-1}^k}) \, \mathbf{I}).
\end{equation}
According to Equation~\ref{eq:4}~\ref{eq:5}~\ref{eq:6}, Equation~\ref{eq:bayesian} can be written as\\
\resizebox{0.5\textwidth}{!}{
\begin{minipage}{0.5\textwidth}
\begin{align*}
  & q(x_{t-1} \mid x_t, x_k)= \frac{q(x_t \mid x_{t-1}, x_k) \, q(x_{t-1} \mid x_k)}{q(x_t \mid x_k)}\\
  \sim & \frac{\mathcal{N}(x_t; \sqrt{\alpha_t} \, x_{t-1}, (1 - \alpha_t) \mathbf{I}) \, \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}^k} \, x_k, (1 - \bar{\alpha}_{t-1}^k) \mathbf{I})}{\mathcal{N}(x_t; \sqrt{\bar{\alpha}_t^k} \, x_k, (1 - \bar{\alpha}_t^k) \mathbf{I})}\\
\propto& \exp\left\{ -\frac{1}{2} \left( \frac{1}{\frac{(1 - \alpha_t) (1 - \bar{\alpha}_{t-1}^k)}{1 - \bar{\alpha}_t^k}} \right) \left[ x_{t-1}^2 -2\frac{ \sqrt{\alpha_t}  (1 - \bar{\alpha}_{t-1}^k) x_t + \sqrt{\bar{\alpha}_{t-1}^k} (1 - \alpha_t) x_k}{1 - \bar{\alpha}_t^k} x_{t-1} \right] \right\}.
\end{align*}
\end{minipage}}
According to the definition of Gaussian distribution, the variance of Equation~\ref{eq:bayesian} can be written as
$$
\sigma_q(t) \propto \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1}^k)}{1 - \bar{\alpha}_t^k} \mathbf{I},
$$
the expectation can be written as
\begin{equation}\label{eq:expectation}
\mu(x_t,x_k) \propto \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1}^k) x_t + \sqrt{\bar{\alpha}_{t-1}^k} (1 - \alpha_t) x_k}{1 - \bar{\alpha}_t^k}.
\end{equation}
Recursively apply the reparameterization trick to Equation~\ref{eq:4} in an iterative manner, we have
\begin{equation}\label{eq:reparam}
    x_t = \sqrt{\bar{\alpha}_t^k}x_k + \sqrt{1-\bar{\alpha}_t^k} \epsilon_k.
\end{equation}
This step is relatively complex, it is recommended to refer to~\citet{understand_DDPM}.
Reorganize Equation~\ref{eq:reparam}, we have
$$
    x_k = \frac{1}{\sqrt{\bar{\alpha}_t^k}}(x_t-\sqrt{1-\bar{\alpha}_t^k} \epsilon_k).
$$
Substitute $x_k$ into Equation~\ref{eq:expectation}, we have\\
\resizebox{0.5\textwidth}{!}{
\begin{minipage}{0.5\textwidth}
\begin{align}\label{eq:mu estimate}
\mu(x_t,x_k) &\propto \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1}^k) x_t + \sqrt{\bar{\alpha}_{t-1}^k} (1 - \alpha_t) \frac{1}{\sqrt{\bar{\alpha}_t^k}}(x_t-\sqrt{1-\bar{\alpha}_t^k} \epsilon_k)}{1 - \bar{\alpha}_t^k} \nonumber \\
&=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t^k} \sqrt{\alpha_t}}\epsilon_k.
\end{align}
\end{minipage}}
Returning to Equation~\ref{eq:KL divergence}, minimizing KL divergence is equivalent to minimizing the difference between the expectations of the two terms in the bracket. Equation~\ref{eq:DDPM_inference} gives the expectation of the right term, \ie,
$$
x_{t-1}=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}}\epsilon_{\phi}.
$$
Equation~\ref{eq:mu estimate} is the expectation of the left term. As a result, Equation~\ref{eq:KL divergence} can be optimized by loss function\\
\resizebox{0.5\textwidth}{!}{
\begin{minipage}{0.5\textwidth}
\begin{align*}
    L_k&=\sum_{t=k+1}^{T} ||\mu(x_t,x_k)-\mu_{\phi}(x_t, t)||^2\\
       &=\sum_{t=k+1}^{T} \frac{(1-\alpha_t)^2}{\alpha_t(1-\bar{\alpha}_t^k)(1-\bar{\alpha}_t)}||\sqrt{1-\bar{\alpha}_t^k}\epsilon_{\phi}(x_t, t-k)-\sqrt{1-\bar{\alpha}_t}\epsilon_k||^2\\
       & \propto \sum_{t=k+1}^{T} ||\sqrt{1-\bar{\alpha}_t^k}\epsilon_{\phi}-\sqrt{1-\bar{\alpha}_t}\epsilon_k||^2.
\end{align*}
\end{minipage}}
Since the only observed data is $x_k$, and to satisfy total $T-k$ steps, $\epsilon_{\phi}$ can be written as
$$
    \epsilon_{\phi} := \epsilon_{\phi}(\sqrt{\bar{\alpha}^k_t} \mathbf{x}_k + \sqrt{1 - \bar{\alpha}^k_t}\epsilon_k, t-k).
$$
\end{proof}


\clearpage
\section{Theorem and Proof}\label{sec:appendix_2}
\begin{assumption}\label{assumption}
\noindent
\begin{enumerate}
    \item \textbf{Reconstruction error upper bound.}
    The reconstruction error produced by the generative model is bounded by $c$.
    \item \textbf{Loss function upper bound.}
    Downstream task loss $L_d(\cdot) \leq \psi$.
    \item \textbf{l-smoothness.}
    There exists a constant $l$ such that for all weights \( \theta, \theta' \in \mathbb{R}^n \)
    \[
    \|\nabla L_d(\theta) - \nabla L_d(\theta')\| \leq l \|\theta - \theta'\|
    \]
    \item \textbf{$\mu$-strong convex.}
    There exists a constant \( \mu > 0 \) such that for all \( \theta,\theta' \in \mathbb{R}^n \),
    \[
    L_d(\theta') \geq L_d(\theta) + \nabla L_d(\theta)^\top (\theta' - \theta) + \frac{\mu}{2} \| \theta' - \theta \|^2.
    \]
    \item \textbf{Hessian matrix upper bound.}
    The Hessian matrix eigenvalue around the neighborhood of optimal weight $\theta^*$ is bounded by $\lambda$.
\end{enumerate}
\end{assumption}

\begin{lemma}
\label{lemma:theta_convergence}
Assume that the loss function \( L_d(\cdot) \) is \( l \)-smooth and satisfies \( \mu \)-strongly convex. Then, the sequence \( \{\theta_{i}\}_{i=0}^{epoch} \) generated by the gradient descent update with step size \( \frac{1}{l} \) satisfies
\[
\|\theta^{epoch} - \theta^*\|^2 \leq \frac{2 [L_d(\theta^0) - L_d(\theta^*)]}{\mu} \left(1 - \frac{\mu}{l}\right)^{epoch}.
\]
\end{lemma}

\begin{proof}
Since \( L_d(\theta) \) is \( l \)-smooth, for any \( \theta \) and \( \theta' \),
\begin{equation*}
    L_d(\theta') \leq L_d(\theta) + \nabla L_d(\theta)^\top (\theta' - \theta) + \frac{l}{2} \|\theta' - \theta\|^2.
\end{equation*}
Applying this to the gradient descent update $\theta^{k+1} = \theta^k - \frac{1}{l} \nabla L_d(\theta^k)$, we have\\
\resizebox{0.5\textwidth}{!}{
\begin{minipage}{0.5\textwidth}
\begin{align}\label{eq:expend}
L_d(\theta^{k+1}) &\leq L_d(\theta^k) + \nabla L_d(\theta^k)^\top (\theta^{k+1} - \theta^k) + \frac{l}{2} \|\theta^{k+1} - \theta^k\|^2 \notag\\
&= L_d(\theta^k) - \frac{1}{2l} \|\nabla L_d(\theta^k)\|^2.
\end{align}
\end{minipage}}
Since  $L_d(\theta)$ is $\mu$-strongly convex, it satisfies the Polyak--Lojasiewicz condition:
\begin{equation*}
\frac{1}{2} \|\nabla L_d(\theta)\|^2 \geq \mu [L_d(\theta) - L_d(\theta^*)].
\end{equation*}
Substituting this inequality into Equation~\ref{eq:expend}, we have
\begin{align*}
L_d(\theta^{k+1}) & \leq L_d(\theta^k) -  \frac{\mu}{l}  (L_d(\theta^k) - L_d(\theta^*)).
\end{align*}
The above equation can be reorganized to
\[
L_d(\theta^{k+1}) - L_d(\theta^*) \leq \left( 1 - \frac{\mu}{l} \right) (L_d(\theta^k) - L_d(\theta^*)).
\]
Start from $k=0$, and recursively apply the above equation with $epoch$ times. It follows that
\[
L_d(\theta^{epoch}) - L_d(\theta^*) \leq \left( 1 - \frac{\mu}{l} \right)^{epoch} (L_d(\theta^0) - L_d(\theta^*)).
\]
Since \( L_d(\theta) \) is \( \mu \)-strongly convex, it satisfies
\[
\|\theta - \theta^*\|^2 \leq \frac{2}{\mu} (L_d(\theta) - L_d(\theta^*)).
\]
So we have
\begin{align*}
\|\theta^{epoch} - \theta^*\|^2 &\leq \frac{2}{\mu} (L_d(\theta^{epoch}) - L_d(\theta^*)) \\
&\leq \frac{2(L_d(\theta^0) - L_d(\theta^*))}{\mu}  \left( 1 - \frac{\mu}{l} \right)^{epoch}.
\end{align*}
\end{proof}

\begin{theorem}\label{theorem:emperi error}
When Assumption~\ref{assumption} holds, Lt-Di's cumulative empirical error can be bound by:
$$
    L_d(\hat{\theta})-L_d(\theta^*) \leq \frac{\lambda}{2} \left[c+\frac{2\psi}{\mu}\left(1 - \frac{\mu}{l}\right)^{epoch}\right],
$$
where $epoch$ is the number of update steps used in the weight preparation stage, and $\hat{\theta}$ is the weight predicted by the generative model.
\end{theorem}

\begin{proof}
Using the Taylor expansion around the optimal point \( \theta^* \), we have\\
\resizebox{0.5\textwidth}{!}{
\begin{minipage}{0.5\textwidth}
\begin{align}\label{eq:taylor}
L_d(\hat{\theta}) - L_d(\theta^*) &= \nabla L_d(\theta^*)^T (\hat{\theta} - \theta^*) + \frac{1}{2} (\hat{\theta} - \theta^*)^T \nabla^2 L_d(\xi) (\hat{\theta} - \theta^*)\notag\\
&=\frac{1}{2} (\hat{\theta} - \theta^*)^T \nabla^2 L_d(\xi) (\hat{\theta} - \theta^*).
\end{align}
\end{minipage}}
According to a constraint on the Hessian matrix, we have
\begin{equation}\label{eq:hessiam max value}
 \frac{1}{2} (\hat{\theta} - \theta^*)^T \nabla^2 L_d(\xi) (\hat{\theta} - \theta^*) \leq \frac{\lambda}{2}  ||\hat{\theta} - \theta^*||^2.
\end{equation}

Decomposing $|\hat{\theta} - \theta^*||^2$ into weight preparation error and reconstruction error, we have
\begin{align}\label{eq:theta error cumulative}
    ||\hat{\theta}-\theta^*||^2 &\leq ||\hat{\theta}-\theta^{epoch}||^2 + ||\theta^{epoch}-\theta^*||^2 \notag\\
&\leq c + \frac{2 \left(L_d(\theta^0) - L_d(\theta^*)\right)}{\mu} \left(1 - \frac{\mu}{l}\right)^{epoch} \notag\\
& \leq c + \frac{2 \psi}{\mu} \left(1 - \frac{\mu}{l}\right)^{epoch}.
\end{align}
Substituting Equation~\ref{eq:theta error cumulative} and Equation~\ref{eq:hessiam max value} into Equation~\ref{eq:taylor} we obtain
$$
    L_d(\hat{\theta})-L_d(\theta^*) \leq \frac{\lambda}{2} \left[c+\frac{2\psi}{\mu}\left(1 - \frac{\mu}{l}\right)^k\right].
$$
\end{proof}




\section{Preliminary}\label{sec:appendix_3}
\subsection{Symbol Table}
Potentially ambiguous symbols are described in Table~\ref{tab:symbols} to enhance the reader's comprehension of this paper.
\begin{table}[h!]
\centering
\begin{tabular}{|>{\centering\arraybackslash}m{0.15\linewidth}|p{0.7\linewidth}|}
\hline
\textbf{Symbol} & \textbf{Description} \\ \hline
$\epsilon$ & Gaussian noise. \\ \hline
$\phi$ & Diffusion model, \ie, denoiser. \\ \hline
$\{\alpha_i\}_{i=0}^T$ & Decay schedule for diffusion process. \\ \hline
{$C_{infer}$} & Inference chain of the diffusion model. \\ \hline
$Tra$ & Optimization trajectory. \\ \hline
$r(\cdot)$ & Function that maps $Tra$ to $C_{\text{infer}}$ \\ \hline
$x_i$ & Element of the inference chain. \\ \hline
\multirow{2}{*}{$\theta^b$} & Body, \ie, bottleneck of the downstream network, also used for task embedding. \\ \hline
$\theta^h$ & Head of the downstream network. \\ \hline
$\hat{\theta}$ & Weight generated by the diffusion model. \\ \hline
$\eta$ & Inner-loop learning rate. \\ \hline
$\zeta$ & Outer-loop learning rate. \\ \hline
$K$ & Number of Inner-loop step. \\ \hline
\end{tabular}
\caption{Symbols and their descriptions.}
\label{tab:symbols}
\end{table}
\subsection{Diffusion Model}
The core idea of the diffusion model is to model data through a two-stage process:
\begin{itemize}
    \item \textbf{Diffusion Process}: Starting with data $x_0$, noise is added at each step to generate $x_T$, eventually approaching a standard normal distribution.
    \item \textbf{Inference Process}: Starting with noise $x_T$, a denoising model $\phi$ generates $x_{T-1}, x_{T-2}, \dots, x_0$ step by step.
\end{itemize}

By precisely modeling the reverse process, the diffusion model $\phi$ can generate new samples that match the original data distribution. Diffusion models define a decay schedule $\{\alpha_i\}_{i=0}^T$ to control the noise level at each step. In the diffusion process, noise is added at each step $t$, transforming the data $x_{t-1}$ into $x_t$
\[
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1-\alpha_t) \mathbf{I}).
\]
The direct transition from $x_0$ to $x_t$ can be written as
\[
q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) \mathbf{I}),
\]
where $\bar{\alpha}_t = \prod_{i=1}^t \alpha_i$ is the cumulative noise schedule, controlling the overall noise level from $x_0$ to $x_t$.
In the inference process, the denoiser iteratively reconstructs the data by 
$$
x_{t-1}=\frac{1}{\sqrt{\alpha_t}}x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t} \sqrt{\alpha_t}}\epsilon_{\phi}+\sigma_z\epsilon,
$$
We omit additional $\sigma_z\epsilon$ for stable weight generation. The key point of training a variational model is to maximize the Evidence Lower Bound(ELBO). In the diffusion algorithm, optimizing the ELBO is essentially equivalent to minimizing the denoising match term
$$
    \sum_{t=k+2}^{T} \mathbb{E}_{q(x_t | x_k)} \left[ \mathrm{D}_{\mathrm{KL}}\left( q(x_{t-1} | x_t, x_k) \parallel p_{\phi}(x_{t-1} | x_t) \right) \right],
$$
which is also the objective optimized by our trajectory diffusion.
\subsection{REPTILE}
REPTILE is a first-order optimization-based meta-learning algorithm that simplifies training while retaining strong adaptability across tasks. It eliminates the need for second-order gradients, making it computationally efficient compared to algorithms like MAML. The training process consists of two loops: the inner-loop and the outer-loop.

In the inner loop, REPTILE performs gradient descent on a sampled task \( {T}_i \) using the task’s support set. Starting from the meta-parameters \( \theta \), the task-specific parameters \( \theta_i \) are updated for \( K \) steps using
\[
\theta_i^{(t+1)} = \theta_i^{(t)} - \eta \nabla_{\theta_i^{(t)}} {L}_{{T}_i}(\theta_i^{(t)}),
\]
where \( \eta \) is the inner-loop learning rate and \( {L}_{{T}_i} \) is the loss for task \( {T}_i \). 

In the outer-loop, the meta-parameters \( \theta \) are updated by moving them toward the task-specific parameters \( \theta_i \) obtained from the inner loop. This meta-update is given by
\[
\theta \leftarrow \theta + \zeta (\theta_i - \theta),
\]
where \( \zeta \) is the outer-loop learning rate. 

By iteratively repeating the inner and outer loops across multiple tasks drawn from the task distribution \( p({T}) \), REPTILE optimizes the meta-parameters \( \theta \) to find an initialization that enables fast adaptation to new tasks with minimal gradient steps. Its simplicity lies in avoiding second-order derivatives, while its effectiveness is demonstrated across diverse applications such as few-shot learning and domain generalization.

\clearpage
\section{Experimental Detail}\label{sec:appendix_4}
\subsection{Dataset}\label{sec:dataset setup}
\noindent\textbf{Omniglot.}
The raw Omniglot dataset contains 1623 handwritten characters from 50 alphabets, each with 20 instances in 28$\times$28 grayscale format. We partition the classes of training set, evaluation set, and testing set into 800:400:432. We use Omniglot in three scenarios. We used the Omniglot dataset in our preliminary experiments, ablation experiments, and comparative experiments. For the construction of the classification task, we referred to the experimental setup by Chelsea Finn \etal (MAML).
\\
\par
\noindent\textbf{Mini-Imagenet.}
The raw Mini-Imagenet contains 100 classes, each containing 600 instances in 84$\times$84 grayscale format. We partition classes of training set, evaluation set, and testing set into 64:16:20. The usage of Mini-Imagenet is the same as Omniglot, and we also follow the setup given by Chelsea Finn \etal
\\
\par
\noindent\textbf{Imagenet-1K.}
The raw ImageNet-1K is a benchmark dataset with 1000 classes, 1.2 million training images, and 50000 validation images, typically resized to a resolution of 224$\times$224 pixels. We partitioned the dataset into 20k subsets, each containing 50 classes with 50 images per class. We use this dataset for pre-training and perform transfer zero-shot evaluation on other unseen datasets.
\\
\par
\noindent\textbf{CIFAR-10 CIFAR-100 STL-10 Aircraft Pets.}
CIFAR-10 and CIFAR-100 are image datasets introduced by Alex Krizhevsky, containing 60000 images resized to 32$\times$32 pixels. CIFAR-10 includes 10 classes, while CIFAR-100 features 100 fine-grained classes. STL-10, derived from ImageNet, consists of 10 classes with 13000 labeled images and 100000 unlabeled images, with a resolution of 96$\times$96 pixels. The Aircraft dataset includes 10000 images across 100 aircraft models, with hierarchical labels for manufacturer, family, and variant. The Pets dataset consists of 7349 images of 37 pet breeds, with annotations for class labels, bounding boxes, and pixel-level segmentation. We use these datasets to evaluate the model's transfer zero-shot learning capabilities, which means the labels of these datasets are not visible to the model.
\\
\par
\noindent\textbf{DomainNet.}
DomainNet is a dataset for multi-domain generalization. We use it to evaluate algorithms' ability of few-shot domain generalization. It consists of 345 classes from 6 domains, with a resolution of 224$\times$224 pixels. We use Clipart, Infograph, Painting, Quickdraw, and Real domains for training, while Sketch domains for testing. The tasks we constructed are 5-way 1-shot and 20-way 5-shot. Note that the testing set shares the same 345 classes as the training set.
\\
\par
\noindent\textbf{GLUE.}
The GLUE Benchmark (General Language Understanding Evaluation) tests models on 9 diverse NLP tasks, including CoLA for grammatical acceptability, SST-2 for sentiment classification, MRPC for paraphrase detection, STS-B for sentence similarity, QQP for duplicate question detection, MNLI for natural language inference, QNLI for question-answer validation, RTE for entailment classification, and WNLI for pronoun resolution. We use this dataset to test the efficiency of different algorithms for multi-task fine-tuning on LLM models. Specifically, we use five binary classification tasks, \ie, SST-2, QQP, RTE, WNIL, and CoLA for the training of LLWTD. Then we use the other two tasks, \ie, MRPC and QNIL, to evaluate the performance of different fine-tuning algorithms.
\\
\par
Note that all hyperparameters are obtained on Omniglot and Mini-Imagenet datasets, so other datasets don't need a validation set.

\subsection{Model Configuration}
Lt-Di involves hyperparameter configurations in two components: downstream network $\theta$ and diffusion model $\phi$. For the downstream network $\theta$, we adopt different architectures for different tasks. In tasks on the Omniglot and Mini-ImageNet datasets, we follow the standard setup used by most meta-learning methods, employing 4 convolution blocks with batch normalization and ReLU as the body $\theta^b$ and 2 fully connected layers as the head $\theta^h$. For zero-shot tasks on CIFAR-10, CIFAR-100, STL-10, Aircraft, and Pets, we use the commonly adopted ResNet-12 as the body and still employ a 2-layer fully connected head. In multi-domain generalization tasks, we maintain the same setup. In the LLM multi-task fine-tuning scenario, the downstream network is the LLM model itself, \ie, RoBERTa-base with 125 million weights. For the diffusion model $\phi$, we use the commonly adopted U-Net architecture. The number of inference steps in the diffusion model is set to 20, as shown in Figure~\ref{fig:trade-off}. The length of the selected optimization trajectory is set to 4, as indicated in Figure~\ref{fig:loss_trajectory}. For LLWD, it maintains the same setup as LLWTD, with the difference that the length of the optimization trajectory is set to 2. Finally, for the meta-learning algorithm, we use REPTILE, a first-order gradient algorithm that does not differentiate between the support set and the query set. The inner-loop learning rate is set to 0.05, the outer-loop learning rate to 0.001, and the optimizer for both is Adam.

\subsection{Sensitive Study}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{sensitive_omniglot.pdf}
    \caption{Sensitive study on Omniglot}
    \label{fig:sen_omni}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{sensitive_miniimagenet.pdf}
    \caption{Sensitive study on Mini-Imagenet}
    \label{fig:sen_mini}
\end{figure}
Compared to existing weight generation methods, the two hyperparameters of Lt-Di, \ie, trajectory length and diffusion steps, may affect the model's sensitivity. To evaluate the robustness of our approach, we analyze the impact of trajectory length and diffusion steps on classification accuracy for Mini-ImageNet and Omniglot. As shown in Figure~\ref{fig:sen_omni} and Figure~\ref{fig:sen_mini}, the accuracy remains stable across different parameter settings, with only minor variations. While increasing trajectory length and diffusion steps can slightly improve performance, excessive changes do not lead to significant degradation. The consistency across both datasets indicates that Lt-Di is insensitive to these hyperparameters, demonstrating robustness. Note that we do not use the parameter combination that achieves the highest accuracy in this experiment as the default setting. We aim to maintain performance above current state-of-the-art levels while reducing computational costs during inference.

\subsection{Adaptability to Different Architectures}
To verify the effectiveness of Lt-Di under network structures of different scales, we compare it with its degraded versions, \ie, REPTILE, LLO-VAE, and Lv-Di. Table~\ref{tab:archi_omni} and Table~\ref{tab:archi_mini} show the effectiveness of our method on Swin Transformer, ResNet18, and MobileNetV2 architectures. The results demonstrate that each component of our method remains effective across neural network architectures of different scales.

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Swin Transformer} & \textbf{ResNet18} & \textbf{MobileNetV2} \\
        \midrule
        REPTILE  & 98.27 & 97.11 & 93.79\\
        LLO-VAE  & 97.43 & 96.60 & 93.61 \\
        Lv-Di    & 98.93 & 98.44 & 94.92 \\
        Lt-Di    & 99.90 & 98.82 & 95.84 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of Accuracy across different network structures on Omniglot 5-way 1-shot tasks.}
    \label{tab:archi_omni}
\end{table}

\begin{table}[h]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{lccc}
        \toprule
        \textbf{Method} & \textbf{Swin Transformer} & \textbf{ResNet18} & \textbf{MobileNetV2} \\
        \midrule
        REPTILE  & 57.27  & 52.55  & 40.67  \\
        LLO-VAE  & 66.26  & 60.32  & 53.27  \\
        Lv-Di    & 76.75  & 70.76  & 59.33  \\
        Lt-Di    & 82.38  & 77.85  & 64.81  \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of Accuracy across different network structures on Mini-Imagenet 5-way 1-shot tasks.}
    \label{tab:archi_mini}
\end{table}


\clearpage
\section{Related Work}\label{sec:related_work}
\noindent\textbf{Meta-Learning.}
Meta-learning often employs a bi-level optimization-based paradigm~\cite{MAML,ANIL, meta_baseline, meta_bootstrap, on_convergence} for better generalization performance~\cite{generalization_information_1,generalization_information_2,generalization_information_3,generalization_PAC_1,generalization_stable_1} on few-shot and reinforcement learning tasks. However, it incurs significant costs for weight fine-tuning, especially in multi-task scenarios. Methods like ANIL \cite{ANIL} and Reptile \cite{REPTILE} improve training efficiency by minimizing updates to only essential task-specific layers or by approximating meta-gradients, respectively. However, these methods still rely on gradient computation and fail to achieve superior accuracy.
\par
\noindent\textbf{Network Weights Generation.} 
Hypernetwork~\cite{hypernetworks} is the first method that uses one network to generate another's weights, leading to extensions like the HyperSeg~\cite{hypernetwork_2} for downstream task flexibility. Conditional diffusion models provide another approach, with OCD leveraging overfitting~\cite{OCD} and Meta-Diff enhancing few-shot adaptability~\cite{MetaDiff}. Hyper-representations~\cite{VAE_weights} embed model characteristics to support weight generation for unseen tasks, while Image-free Classifier Injection~\cite{ICIS} achieves zero-shot classification via semantic-driven weights. These methods are constrained by their single-level optimization approach, which presents limitations in both cross-task knowledge transfer capabilities and potential adaptability to novel tasks. Most importantly, these methods overlook the role of other weights, limiting the model's efficiency and accuracy.











    


