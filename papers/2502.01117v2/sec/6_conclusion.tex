\section{Conclusion and Limitation}\label{sec:conclusion}
In this paper, we propose Lt-Di, which integrates the fast inference capability of weight generation and the cross-task transferability of bi-level optimization. Building on this, we further propose Trajectory Diffusion, enabling the model to capture the entire optimization trajectory, which enhances weight generation accuracy and efficiency. Finally, we theoretically and empirically demonstrate that the convergence of this indirect learning paradigm can be improved solely by constraining the eigenvalues of the Hessian matrix of the downstream tasks loss function, improving convergence efficiency without additional time overhead.

Our method is not well-suited for single-task learning scenarios, as such settings do not require frequent weight updates. In these cases, the benefits of Lt-Di in fine-tuning and inference do not sufficiently outweigh the computational overhead of training a diffusion model. For relatively simple tasks, such as the Omniglot 5-way 5-shot task in Section~\ref{sec:few_shot_learning}, our method does not provide further accuracy improvements. Moreover, further validation is needed on tasks beyond computer vision and natural language processing.

\section*{Impact Statements}
Theoretically, we extend the diffusion algorithm to Trajectory Diffusion and analyze the convergence of the weight generation paradigm. Practically, we validate our method across multiple multi-task scenarios.
