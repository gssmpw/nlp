\documentclass[twoside]{article}

% \usepackage{aistats2025}
% If your paper is accepted, change the options for the package
% aistats2025 as follows:
%
\usepackage[accepted]{aistats2025}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

% If you use natbib package, activate the following three lines:
% \usepackage[round]{natbib}
\usepackage{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors

\usepackage{graphicx,color}
\usepackage{xcolor,colortbl}
% \usepackage{amsmath,amsfonts,amsthm,amssymb,multirow, adjustbox}
\input{pkgs.tex}
\input{macros.tex}
% \usepackage{algorithmic}

\usepackage{arydshln}
\usetikzlibrary{calc}
\usepackage{amsthm}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\xmark}{\ding{55}}%
\newtheorem{theorem}{Theorem}

% \newcommand{\maryam}[1]{}
% \newcommand{\djcomment}[1]{}
% \newcommand{\avi}[1]{}

\newcommand{\maryam}[1]{\textcolor{magenta}{Maryam: #1}}
\newcommand{\djcomment}[1]{\textcolor{brown}{Dj: #1}}
\newcommand{\avi}[1]{\textcolor{blue}{Avi: #1}}


%% LL macros
\newcommand{\LL}[1]{\textcolor{red}{\textbf{LL:} #1}}
\newcommand{\LLL}[1]{\textcolor{blue}{\textbf{LL:} #1}}
\newcommand{\bmat}[1]{\begin{bmatrix}#1\end{bmatrix}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\Thetaspace}{{\boldsymbol{\Theta}}}
\newcommand{\Wspace}{{\boldsymbol{W}}}
\newcommand{\Zspace}{{\boldsymbol{Z}}}
\newcommand{\Aspace}{\mathcal{A}}
\newcommand{\Phispace}{\boldsymbol{\Phi}}

\newcommand{\OPT}{\textup{\sf OPT}}
\newcommand{\subjectto}{\textup{subject to}}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
\runningtitle{Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[
% \title{Certifying robustness to adaptive data poisoning}
\aistatstitle{Keeping up with dynamic attackers:\\Certifying robustness to adaptive online data poisoning}

\aistatsauthor{Avinandan Bose \And Laurent Lessard \And  Maryam Fazel \And Krishnamurthy Dj Dvijotham}

\aistatsaddress{ University of Washington\\
avibose@cs.washington.edu \And  Northeastern University\\
 l.lessard@northeastern.edu \And University of Washington\\
mfazel@uw.edu \And ServiceNow Research\\ dvij@cs.washington.edu} ]

% \aistatsauthor{%
%   Avinandan Bose
%   % University of Washington\\
%   % avibose@cs.washington.edu\\
%   \And
%   Laurent Lessard
%    % Northeastern University\\
%    % l.lessard@northeastern.edu\\
%    \And
%  Maryam Fazel
%   % University of Washington\\
%   % mfazel@uw.edu\\
%   \And
%   Krishnamurthy Dj Dvijotham
%   % ServiceNow Research\\
%   % dvij@cs.washington.edu\\}
%   }

\begin{abstract}
  The rise of foundation models fine-tuned on human feedback from potentially untrusted users has increased the risk of adversarial data poisoning, necessitating the study of robustness of learning algorithms against such attacks. Existing research on provable certified robustness against data poisoning attacks primarily focuses on certifying robustness for static adversaries who modify a fraction of the dataset used to train the model \emph{before the training algorithm is applied}. In practice, particularly when learning from human feedback in an online sense, adversaries can observe and react to the learning process and inject poisoned samples that optimize adversarial objectives better than when they are restricted to poisoning a static dataset once, before the learning algorithm is applied. Indeed, it has been shown in prior work that online dynamic adversaries can be significantly more powerful than static ones. We present a novel framework for computing certified bounds on the impact of dynamic poisoning, and use these certificates to design robust learning algorithms. We give an illustration of the framework for the mean estimation and binary classification problems and outline directions for extending this in further work. The code to implement our certificates and replicate our results is available at \url{https://github.com/Avinandan22/Certified-Robustness}.\looseness=-1
\end{abstract}

% \maryam{terminology: should we consistently refer to our adversary as 'dynamic adaptive' or is 'dynamic' enough?}
% \documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

% ready for submission
% \usepackage{neurips_2024}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}



% \title{Certifying robustness to adaptive data poisoning}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
% }

% \begin{document}

% \maketitle

% \textcolor{magenta}{abstract is missing}
% \begin{abstract}
%     % [XXXX] \avi{Change citation format to numbered to save some space.}
%     The rise of foundation models fine-tuned on human feedback from potentially untrusted users has increased the risk of adversarial data poisoning, necessitating the study of robustness of learning algorithms against such attacks. Existing research on provable certified robustness against data poisoning attacks primarily focuses on certifying robustness for static adversaries that modify a fraction of the dataset used to train the model \emph{before the training algorithm is applied}. In practice, particularly when learning from human feedback in an online sense, adversaries can observe and react to the learning process and inject poisoned samples that optimize adversarial objectives better than when they are restricted to poisoning a static dataset once before the learning algorithm is applied. Indeed, it has been shown in prior work that online dynamic adversaries can be significantly more powerful than static ones. We present a novel framework for computing certified bounds on the impact of dynamic poisoning, and use these certificates to design robust learning algorithms. We give an illustration of the framework for the mean-estimation problem and outline directions for extending this in further work.
% \end{abstract}
\section{INTRODUCTION}

% \textcolor{red}{Avi and Dj, please proof read. There've been a lot of typos throughout even in the abstract...}

With the advent of foundation models fine tuned using human feedback gathered from potentially untrusted users (for example, users of a publicly available language model) \citep{christiano2017deep, ouyang2022training}, the potential for adversarial or malicious data entering the training data of a model increases substantially. This motivates the study of robustness of learning algorithms to poisoning attacks \citep{biggio2012poisoning}. More recently, there have been works that attempt to achieve ``certified robustness`` to data poisoning, i.e., proving that the worst case impact of poisoning is below a certain bound that depends on parameters of the learning algorithm. All the work in this space, to the best of our knowledge, focuses on the \emph{static} poisoning adversary \citep{steinhardt2017certified, zhang2022bagflip}. Even in \citep{wang2024temporal} which is the closest setting to our work, the poisoning adversary acts over offline datasets in a temporally extended fashion which are poisoned in one shot, and thus is not %purely
dynamic.
% \textcolor{magenta}{this sentence is not clear to me, given it's the closest paper we should explain more}. \avi{Changed it. The paper really isn't dynamic is the main message.}
% \textcolor{magenta}{it is still not clear to me what the paper does though, I'll have to look at it. Also what we  mean by static/dynamic dataset and static/dynamic adversary doesn't come across in the intro. We should point to concrete definition later in the paper.}
%
There has been work on \emph{dynamic} attack algorithms \citep{zhang2020online, wang2018data} showing that these attacks can indeed be more powerful than static attacks. %adversaries.
This motivates the question we study: can we obtain certificates of robustness for a broad class of learning algorithms against \emph{dynamic} poisoning adversaries?


In this paper, we study learning algorithms corrupted by a dynamic poisoning adversary who can observe the behavior of the algorithm and adapt the poisoning in response. This is relevant in scenarios where models are continuously/periodically updated in the face of new feedback, as is common in RLHF/fine tuning applications (see Figure~\ref{fig:schematic}). We provide (to the best of our knowledge) the first general framework for computing certified bounds on the worst case impact of a dynamic data poisoning attacker, and further, use this certificate to design robust learning algorithms (see Section~\ref{sec:setup}). We give an illustration of the framework for the mean estimation problem (see Section~\ref{sec:mean_estimation}) and binary classification problem (see Section~\ref{sec:binary_classification}) and suggest directions for future work to apply the framework to
%more realistic
other practical learning scenarios.
Our contributions are as follows:
\begin{enumerate}
    \item We develop a
    %generally applicable
    % general optimization-based
    framework for computing certified bounds on the worst case impact of a dynamic online poisoning adversary on a learning algorithm as a finite dimensional optimization problem. The framework applies to an arbitrary learning algorithm and a general adversarial formulation described in Section~\ref{sec:setup}. However, instantiating the framework in a computationally tractable way requires additional work, and we show that this instantiation can be done for certain cases.
    %, leaving extensions to broader learning algorithms to future work.
    \item We demonstrate that for learning algorithms designed for mean estimation (Section~\ref{sec:mean_estimation}) and binary classification problems with linear classifiers (Section~\ref{sec:binary_classification}), we can tractably compute bounds (via dual certificates) for learning algorithms that use either regularization or noise addition as a defense against data poisoning. We leave extensions to broader learning algorithms to future work.    \looseness=-1
    \item We use these %derived
    certificates to choose parameters of a learning algorithm so as to trade off performance and robustness, and thereby derive robust learning algorithms (Section~\ref{sec:meta_learning}).
    \item We conduct experiments on real and synthetic datasets to empirically validate our certificates of robustness, as well as using the meta-learning setup to design defenses (Section~\ref{sec:experiments}).
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{figs/schematic.png}
    \caption{A schematic diagram to highlight the differences between static and dynamic poisoning. }
    \vskip -0.2in
    \label{fig:schematic}
\end{figure*}
% \avi{Dj, can you help list down the main contributions of the paper, so we structure the paper and experiments accordingly? Here's a list of questions/thoughts we can highlight:
% \begin{enumerate}
% \item Our formulation is very general. We can capture general update rules and defenses, arbitrary attack policies, arbitrary adversarial objective which might be different from learning algorithm's objective. \maryam{Generality by itself is not enough, we need to highlight when the problems are *tractable*, so convincing tractable examples like the two we have are what sells the paper.} This is a good point, thanks Maryam.
% \djcomment{A proposed phrasing: We develop a generally applicable framework for computing certified bounds on the worst case impact of a dynamic online poisoning adversary on a learning algorithm. The framework applies to arbitrary learning algorithm and a general adversarial formulation described in Section ... However, instantiating the framework in a computationally tractable way requires additional work and we show that this instantiation can be done for certain cases, leaving extensions to broader learning algorithms to future work.}
% \item The mean estimation and binary classification illustrations help us demonstrate this general ability of our framework. We consider various types of defenses via the update rule $F$ : (i) Additive Gaussian Noise, (ii) Regularization, (iii) Adjusting Learning rate at stationary state.
% \djcomment{We demonstrate that for learning algorithms designed for mean estimation and binary classification problems with linear classifiers, we can derive tractable bounds for learning algorithms that use either regularization or noise addition as a defense against data poisoning.}
% \item For classification, while the learning algorithm's objective is minimizing the regularized hinge loss, we derive certificates for 2 cases when the adversary's objective is maximizing (i) misclassification accuracy (0-1 loss), (ii) hinge loss. \djcomment{I would skip this as a separate point and just find a way to mention it in the above point, or drop it altogether. }
% \item We design a methodology to trade off performance and robustness. Our experiments demonstrate this, where the defense is chosen without knowledge of attack type, but our algoroithm is robust to various types of attack.
% \djcomment{We use the derived certificates to choose parameters of a learning algorithm so as to tra}
% \item My question is how does the exact form of the certificates add value to the paper? These are very long, and usually not intuitive to a careless reviewer, which might give us a hard time in reviews. What are suggestions on writing it in such a way that a careful reviewer is able to understand that there are novel steps behind deriving the certificates, whereas a careless reviewer can appreciate that at least there exists an optimization problem easily solvable by a convex solver, and they can jump directly to the implications of these certificates in the experiments for designing robust learning algorithms.
%     % \item $F$ captures a general update rule. It can be used for any optimizer like Adam, SGD, Adagrad where learning rate changes adaptively (for the specific examples we have chosen, we need the learning rate to be clipped at some $\eta$ if we don't make any assumptions on the inital parameter $\theta_0$)
%     % \item $l_{\rm adv}$ is also general and can be different from the loss used by the update learning rule. We highlight this in the classification problem by deriving certificates for both hinge loss and the 0-1 loss for the adversary, while the learning rule followed the regularized hinge loss.
%     % \item This formulation allows the attack strategy to be arbitrary and adaptive.
%     % \item This formulation can handle different types of defenses: We demonstrate adding additive noise, regularization, learning rates as a few types of defenses.
% \end{enumerate}}
\subsection{Related Work}\label{appendix:related_work}
\paragraph{Data Poisoning.} Modern machine learning pipelines involve training on massive, uncurated
datasets that are potentially untrustworthy and of such scale that conducting rigorous quality checks becomes impractical. Poisoning attacks \citep{biggio2012poisoning, newsome2006paragraph, biggio2018wild} pose big security concerns upon deployment of ML models. Depending on which stage (training / deployment)  the poisoning takes place, they can be characterised as follows: 1. Static attacks: The model is trained on an offline dataset with poisoned data. Attacks could be untargeted, which aim to prevent training convergence rendering an unusable model and thus denial of service \citep{tian2022comprehensive}, or targeted, which are more task-specific and instead of simply increasing loss, attacks of this kind seeks to make the model output wrong predictions on specific tasks. 2. Backdoor attacks: In this setting, the test / deployment time data can be altered \citep{chen2017targeted, gu2017badnets, han2022physical, zhu2019transferable}. Attackers manipulate a small proportion of the data such that, when a specific pattern / trigger is seen at test-time, the model returns a specific, erroneous prediction. 3. Dynamic (and adaptive) attacks: In scenarios where models are continuously/periodically updated in the face of new feedback, as is common in RLHF/fine tuning applications, a dynamic poisoning adversary \cite{wang2018data, zhang2020online} can observe the behavior of the learning algorithm and adapt the poisoning in response.
% \paragraph{Dynamic Adversaries}
\paragraph{Certified Poisoning Defense.} Recently, there have been works that attempt to achieve ``certified robustness" to data poisoning, i.e., proving that the worst case impact of \textit{any} poisoning strategy is below a certain bound that depends on parameters of the learning algorithm. All the work in this space, to the best of our knowledge, focuses on the \emph{static} or \textit{backdoor} attack adversary. \citep{steinhardt2017certified} provide certificates for linear models trained with gradient descent, \citep{rosenfeld2020certified}  present a statistical upper-bound on the effectiveness of $\ell_2$ perturbations on training labels for linear models using randomized smoothing, \citep{zhang2022bagflip, sosnin2024certified} present a model-agnostic certified approach that can effectively defend against both trigger-less and backdoor attacks, \citep{xie2022uncovering} observe that differential privacy, which usually covers addition or removal of data points, can also provide statistical guarantees in some limited poisoning settings. Even in \citep{wang2024temporal} which is the closest setting to our work, the poisoning adversary acts over offline datasets in a temporally extended fashion which are poisoned in one shot, and thus is not dynamic.
% \subsection{Impact of Dynamic Adversary}
% We consider a toy problem where 10000 points are sampled from a standard Gaussian $\mathcal{N}(0, 1)$ and we wish to find an empirical estimate of the mean. Suppose $\epsilon$ fraction of these points can be corrupted by the adversary who can corrupt any point in the range $[-1, 1]$. The adversary's goal is the push the mean estimate as far away as possible from the true mean.

% In the static setting, the adversary has access to all points, adds its corruptions, and the estimate is computed as a one-shot average of all points given to the algorithm. If the empirical average of all benign points is less than 0, the optimal strategy for the static adversary is to choose $+1$ for all its corruptions, otherwise, it opts for $-1$.

% In the online learning algorithm, points arrive one by one, and algorithm maintains an empirical estimate of all the points seen so far. In a uniformly randomly chosen $\epsilon$ fraction of these points, the adversary can modify the point shown to the algorithm. The adversary also has access to the current mean estimate, allowing it to adjust its strategy dynamically. The dynamic adversary adopts a straightforward approach: if the empirical mean is less than or equal to 0, it plays $-1$; otherwise, it plays $+1$. Figure~\ref{fig:dynamic_toy} illustrates the growing impact of the dynamic adversary compared to the static one.
% % If the empirical average of all benign points is less than 0, then the static adversary's best response is -1, else its 1. A simple online learning algorithm, estimates the mean by tracking the empirical average of the points seen so far. A dynamic adversary follows the simple strategy: if the empirical mean $\leq 0$, play -1, else play 1. In Figure~\ref{fig:dynamic_toy}, we illustrate the growth of the impact of a dynamic adversary compared to a static adversary.
% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figs/sum_of_squares_vs_percentage_10000_points.png}
%     \caption{Mean Squared Error on the mean estimation problem on 10000 datapoints as the percentage of corrupted datapoints varies. The dynamic adversary can have a significantly larger impact than the static one, especially as the fraction of corrupted datapoints grows.}
%     \label{fig:dynamic_toy}
%     \vskip -0.3in
% \end{figure}

% \subsection{Paper Organization}
% In Section~\ref{sec:setup}, we present the online learning setup we study and provide a formulation to get a certified bound on the adversarial objective in Eq.~\eqref{eq:certificate} as well as a meta learning setup to learn a defense in Eq.~\eqref{eq:main_formulation}. In Section~\ref{sec:mean_estimation} and \ref{sec:binary_classification}, we consider the problems of mean estimation and binary classification, and develop tractable methods for our general formulation for certificate of robustness presented in Eq.~\eqref{eq:certificate}. In Section~\ref{sec:experiments}, we conduct experiments on real and synthetic datasets to empirically validate our certificate of robustness, as well using the meta-learning setup to design defenses.
\section{PROBLEM SETUP}\label{sec:setup}
% \avi{Editing this section}
We now develop the exact problem setup that we study in the paper. We consider a learning algorithm aimed at estimating parameters $\bftheta \in \Theta$, and each step of the learning algorithm
updates the estimates of these parameters based on potentially poisoned data. The following components fully define the problem setup. A notation table in provided in Appendix~\ref{appendix:notation}.
% \textcolor{red}{In table 1: instead of learning algorithm, say something like "updates of learning algorithm". Also, what does "parameters of model" on the first line mean? what model? think of a term that makes the reader get what you refer to quickly. \\
% In table 2: add border to table. also explain more in the paper text what the first two columns mean, e.g., what does 'for deployed model' imply, that the model is fixed? or that the adversary can't observe behavior? or can't react to behavior?}\\
% \avi{Table 1 : I don't see why model should be confusing? Very clearly we are working with ML models / foundational models. I'll let DJ respond since he made the table.}\\
% \avi{Can you please suggest changes? A potential extra column to incorporate your doubts will be ''periodic updates", I'm not aware of literature considering periodic updates in poisoning.} \textcolor{red}{doesn't need to be in the table, can explain more in the text.}

% \textcolor{red}{can also add $z_{\rm adv}\in\mathcal{A}$ to notation table}

\begin{table*}[t]
\centerline{
% \adjustbox{maxwidth=\linewidth}{
% \adjustbox{
% \notsotinyfortable{
  \begin{tabular}{l|c|c|c}
  \multirow{3}{*}{Attack Type} & Adversary adapts poisoning  & Adversary can   & \multirow{2}{*}{Certified}\\
  & strategy upon observing & poison data  & \multirow{2}{*}{robustness} \\
   & model behavior & for deployed model &   \\
  \hline
   Static / One-shot
   % (\citep{tian2022comprehensive, steinhardt2017certified, rosenfeld2020certified})
   &  \xmark\cellcolor{red!25} &\xmark\cellcolor{red!25} & \checkmark\cellcolor{green!25} \\
  \hline
   Backdoor
   % (\citep{chen2017targeted, gu2017badnets, han2022physical, zhu2019transferable, zhang2022bagflip, sosnin2024certified})
   & \xmark\cellcolor{red!25} & \checkmark\cellcolor{green!25}  &\checkmark\cellcolor{green!25} \\
   \hline
   Dynamic attack only
   % (\cite{wang2018data, zhang2020online}
   &  \checkmark\cellcolor{green!25} & \checkmark\cellcolor{green!25}  &\xmark\cellcolor{red!25} \\
  \hline
    Dynamic attack \& defense (Ours) & \checkmark\cellcolor{green!25}  & \checkmark\cellcolor{green!25} & \checkmark\cellcolor{green!25} \\
\end{tabular}
% }
}
\caption{A comparison with lines of work closest to ours. Static/One-shot (\citep{tian2022comprehensive, steinhardt2017certified, rosenfeld2020certified}), Backdoor(\citep{chen2017targeted, gu2017badnets, han2022physical, zhu2019transferable, zhang2022bagflip, sosnin2024certified}), Dynamic attack only (\cite{wang2018data, zhang2020online}.A detailed description is provided in Section~\ref{appendix:related_work}.}
\label{tab:my_label}
\vskip -0.2in
\end{table*}

% \begin{table}
% \centering
% \footnotesize
% \begin{tabular}{lll}
% \toprule
% Notation & Interpretation & Belongs to \\
% \midrule
%    \vphantom{$F_\phi^d$}$\bftheta$  & Model Parameters & $\Thetaspace$ \\
%    \vphantom{$F_\phi^d$}$\bfphi$ & Hyper-parameters & $\Phispace$ \\
%    % \hline
%    % $\bfw$ & Gaussian noise injected into learning algorithm  & $\Wspace$ \\
%    \vphantom{$F_\phi^d$}$\bfz$ & Data point & $\Zspace$ \\
%    \vphantom{$F_\phi^d$}$F_{\phi}$ & Update rule & $\Thetaspace \times \Zspace \mapsto \Thetaspace$ \\
%    \vphantom{$F_\phi^d$}$\bfzadv$ & Adversarial data point & $\mathcal{A}$\\
%    \vphantom{$F_\phi^d$}$\ell_{\rm adv}$ & Adversarial objective fn. & $\Thetaspace \mapsto \mathbb{R}$ \\
%    \vphantom{$F_\phi^d$}$\Pdata$ & Benign Data Dist. & $\mathcal{P}[\bfZ]$ \\
%    % \vphantom{$F_\phi^d$}$\Ptarget$ & Adversary's target Dist. & $\mathcal{P}[\bfZ]$ \\
%    \vphantom{$F_\phi^d$}$\Pi(\cdot| \bftheta, \bfzadv)$ & State Transition Kernel & $\Thetaspace \!\times\! \Zspace \mapsto \mathcal{P}[\Thetaspace]$\!\!\\
%    \bottomrule
% \end{tabular}\caption{Notation.}
% % \maryam{if we don't use $P^{\rm target}$ we can remove that line}}
% \vskip -0.1in
% \end{table}
% \paragraph{Online learning algorithm}

% We consider learning algorithms that operate online receiving at each step a new datapoint and making an update to parameters being estimated. In particular, we consider learning algorithms that can be written as
% \begin{align}\bftheta_{t+1} \gets F\br{\underbrace{\bftheta_t}_{\text{Parameter estimate at time $t$}}, \underbrace{\bfw_t}_{\text{Exogeneous noise input}}, \underbrace{\bfz_t}_{\text{Datapoint received at time $t$}}}\label{eq:dyn}\end{align}
% where $F: \Thetaspace \times \Wspace \times \Zspace \mapsto \Thetaspace$ is an update function that maps the parameters at time $t$ to new parameters, given an exogeneous noise input $\bfw_t$ and a datapoint $\bfz_t$. The exogenous noise input refers to noise artificially injected into the training algorithm in order to make the algorithm more robust to potential poisoning. We further assume that the distribution of $\bfw_t$ is independent of $t$ and each $\bfw_t$ is sampled iid.

% \paragraph{Poisoned learning algorithm}
% % We work in a setting where the datapoints received by the learning algorithm may be corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to
% % % \textcolor{red}{remember to fix 'up to -> up to' everyhwere}
% % that point. Whether or not the poisoned datapoint is picked is chosen probabilistically \textcolor{red}{do we use two probabilities, one for data point getting poisoned and one for it being picked? can we merge these into one?} \avi{They're already the same. Poisoning doesn't mean picking a benign point and modifying it, but rather introducing a corrupted data point that is within a typical set} \textcolor{red}{so then let's just write it in a simpler way with one probability}, and we assume a fixed probability that the poisoned datapoint is chosen over a clean datapoint \textcolor{red}{is this model commonly used? can we cite sources? (for model with separate poisoning and selection)}\avi{DJ?}.

% We work in a setting where some of the data points received by the learning algorithm are corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to that point. We assume that with a fixed probability, the data point the algorithm receives at each time step is poisoned.
% % \textcolor{red}{is this model commonly used? can we cite sources? (for model with separate poisoning and selection)}\avi{DJ?}.
% In practice, this could reflect the situation that out of a large population of human users providing feedback to a learning system, a small fraction are adversarial and will provide poisoned feedback.

% Mathematically, we have that at time $t$ the learning algorithm receives a data point $\bfz_t \sim  \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata$ and $\epsilon$ is a parameter that controls the ``level'' of poisoning (analogous to the fraction of poisoned samples in static poisoning settings \citep{steinhardt2017certified}). This is a special case of Huber's contamination model, which is used in the robust statistics literature \citep{diakonikolas2023algorithmic} (with the contamination model being a Dirac distribution). We restrict the adversary to choose $\bfzadv_t \in \Aspace$ which reflects the allowed range of data points due to input feature normalization or outlier detection systems.

% %\paragraph{Potential Defense} Inspired by differentially private learning algorithms like DP-SGD \citep{bassily2014private}, we propose adding Gaussian noise to the learning process as a way of smoothing the learning algorithm against impacts of the poisoning adversary. In particular, we add $\bfB\bfw_t$ where $\bfw_t$ is iid noise in each step sampled from the standard Gaussian, and $\bfB$ is a design parameter of the learning algorithm. Subsequently, we will choose $\bfB$ so as to minimize the worst case impact of the poisoning adversary. We denote by $\bfS=\bfB\tran{\bfB}$ the covariance matrix of the noise added.
% % More generally, we consider learning algorithms that can be expressed as $\bftheta_{t+1} \gets F_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}$ where $\bfw_t$ is noise and $\phi$ are tunable parameters of the learning algorithm.

% \paragraph{Adversarial objective}

% We assume that the poisoning adversary is interested in maximizing some adversarial objective $\ell_{\textrm{adv}}: \Thetaspace \mapsto \R$, for example, the expected prediction error on some target distribution of interest to the adversary.

% \paragraph{Dynamics as a Markov Chain}

% %By \eqref{eq:dyn}, we have that, conditioned on $\bftheta_t$ and $\bfz_t$, $\bftheta_{t+1}$ follows a Gaussian distribution with mean $\bftheta_t - \eta \nabla \ell\br{\bftheta_t, \bfz_t}$.

% %  $F\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}=\bftheta_t - \eta \nabla \ell\br{\bftheta_t, \bfz_t} -\eta \bfB\bfw_t$.

% % \avi{This statement is incorrect. We can't claim the transition distribution to be Gaussian for a general update $F_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}$. One special case where $\bftheta_{t+1}$ is Gaussian is when the noise is additive. }

% The dynamics \eqref{eq:dyn} gives rise to a Markov chain over the parameters $\bftheta$. If $\mathbb{P}_t$ denotes the distribution over parameters at time $t$, we have

% % \maryam{the notation for $\theta, \theta'$ in the integral is not consistent with the uses below, we should stick to $\theta'$ being next state.}

% \[\mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \mathbb{P}_{F, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}\mathbb{P}_{t}\br{\bftheta}d\bftheta,\]
% where $\mathbb{P}_{F, \Pdata, \bfzadv}$ is the transition kernel induced, i.e., the conditional probability distribution of $\bftheta^\prime=F\br{\bftheta, \bfw_t, \bfz_t}$ by \eqref{eq:dyn} given $\bftheta$.
% \avi{Time index $F$ as $F_t$ and analyse it at $F_{\infty}$. Making additional assumptions on start state, could lead to finding the optima on a compact space if $\Theta$ is not compact.}
\paragraph{Online learning algorithm.}
%\maryam{I changed to 'parameters' to 'model parameters' below to be clear at least the first time it appears in the text. This also agrees with the table.}
% \avi{Is the word hyperparameter appropriate when trying to capture general learning algorithms? What are some other suggestions?}\djcomment{I think it is a good enough word}
We consider online learning algorithms that operate by receiving a new datapoint at each step  and making an update to model parameters being estimated. In particular, we consider learning algorithms that can be written as
% \begin{align}\bftheta_{t+1} \gets F\br{\underbrace{\bftheta_t}_{\text{Parameter estimate at time $t$}},  \underbrace{\bfz_t}_{\text{Datapoint received at time $t$}},\underbrace{\bfphi_t}_{\text{Hyperparameters at time $t$
% }}}\label{eq:dyn}\end{align}
\begin{align}\bftheta_{t+1} \gets F_{\phi}\Bigl(\underbrace{\bftheta_t}_{\text{Parameter}}, \underbrace{\bfz_t}_{\text{Datapoint}}\Bigr),\label{eq:dyn}
\end{align}
where $F_{\phi}: \Thetaspace \times \Zspace \to \Thetaspace$ is a parameterized function that maps the current model parameters $\bftheta_t$ to new model parameters $\bftheta_{t+1}$, based on the received datapoint $\bfz_t$, where $\phi \in \Phispace$ is a hyperparameter, for example, learning rate in a gradient based learning algorithm, or strength of regularization used in the objective function.

% \maryam{Maybe we can move the notation table to the appendix and add a sentence here that refers to it in appendix.}

\textbf{Example} To illustrate the setup we consider a simple toy example where we try to estimate the mean of the datapoints via gradient descent on the $\ell_2$ regularized squared Euclidean loss. Given a current estimate $\bftheta_t$, upon receiving a datapoint $\bfz_t$, the update step can be written as:
\begin{align*}
    \bftheta_{t+1} &= \bftheta_t - \eta \nabla \bigl(\tfrac{1}{2}\|\bfz_t - \bftheta_t\|_2^2 + \tfrac{\sigma}{2} \|\bftheta_t\|_2^2\bigr)\\
    &= (1 - \eta - \eta \sigma) \bftheta_t + \eta \bfz_t\\
    &= F_{\phi}(\bftheta_t, \bfz_t).
\end{align*}
Here $\phi = \{\eta, \sigma\}$ denotes the learning rate and regularization parameter, and are the hyperparameters of the learning algorithm.
% \djcomment{Maybe provide a few examples of $F$ here, clarifying what $\theta, \phi, ...$ correspond to?}
%
Note that $F_\phi$ is a general update rule and we do not make any assumptions about $F_\phi$.

% \djcomment{Commented out what was below, it seemed unnecessarily confusing. I think what would provide more clarity is to just include a few examples of update rules (gradient descent, momentum, maybe even a zero-th order rule).}
%The hyperparameters $\phi_t$ can include a variety of elements, such as the step size in gradient-based algorithms (which may vary over time, as is the case with several popular gradient methods, such as SGD, AdaGrad, Adam etc.), regularization parameters of the model, or even additive Gaussian noise (making $\phi_t$ potentially stochastic). The distribution of hyperparameters at time $t$ is denoted by $\mathbb{P}_t^\phi$. \avi{Is the message that the update rule is very general coming across? Not having the time dependence can make some of the writing much easier -- and in both the examples we have considered, we draw certificates based on only the stationary state.}

% $\phi_t$ indicates model hyperparameters at time $t$ which can denote (but are not limited to) learning rates, regularization parameters, additive gaussian noise.
% The exogenous noise input refers to noise artificially injected into the training algorithm in order to make the algorithm more robust to potential poisoning.
% We further assume that the distribution of $\bfw_t$ is independent of $t$ and each $\bfw_t$ is sampled iid.

\paragraph{Poisoned learning algorithm.}
% We work in a setting where the datapoints received by the learning algorithm may be corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to
% % \textcolor{red}{remember to fix 'up to -> up to' everyhwere}
% that point. Whether or not the poisoned datapoint is picked is chosen probabilistically \textcolor{red}{do we use two probabilities, one for data point getting poisoned and one for it being picked? can we merge these into one?} \avi{They're already the same. Poisoning doesn't mean picking a benign point and modifying it, but rather introducing a corrupted data point that is within a typical set} \textcolor{red}{so then let's just write it in a simpler way with one probability}, and we assume a fixed probability that the poisoned datapoint is chosen over a clean datapoint \textcolor{red}{is this model commonly used? can we cite sources? (for model with separate poisoning and selection)}\avi{DJ?}.

% We work in a setting where some of the data points received by the learning algorithm are corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to that point. We refer to an adversary of this type who can observe the entire trajectory upto that point to select a corruption for the next step as dynamic adversary.
We consider a setting where some of the data points received by the learning algorithm are corrupted by an adversary, who is allowed to choose corruptions as a function of the entire trajectory of the learning algorithm up to that point. We refer to such an adversary, who can observe the full trajectory and decide on the next corruption accordingly, as a \textit{dynamic adaptive adversary}. While this may seem unrealistic, since our goal here is to compute certified bounds on the worst case adversary, we refrain from placing informational constraints on the adversary, as an adversary with sufficient side knowledge can still infer hidden parameters of the model from even from just a prediction API \cite{tramer2016stealing}.

The adversary is restricted to select a corrupted data point $\bfzadv_t \in \Aspace$,  which reflects constraints such as input feature normalization or the adversary trying to avoid outlier detection mechanisms used by the learner. We make no additional assumptions about the specific poisoning strategy employed by the adversary.
%
Thus, our certificates of robustness to poisoning apply to \emph{any dynamic adaptive adversary who chooses poisoned data points from the set $\Aspace$.}

We assume that with a fixed probability, the data point the algorithm receives at each time step is poisoned. In practice, this could reflect the situation that out of a large population of human users providing feedback to a learning system, a small fraction are adversarial and will provide poisoned feedback. Let $\Pdata$ denote the benign distribution of data points. Mathematically, the data point $\bfz_t$
received by the learning algorithm at time $t$ is sampled according to $\bfz_t \sim  \epsilon \delta
%\text{Dirac}
(\bfzadv_t) + (1-\epsilon)\Pdata$,
where $\delta(\cdot)$ denotes the Dirac delta function,
and $\epsilon$ is a parameter that controls the ``level'' of poisoning (analogous to the fraction of poisoned samples in static poisoning settings \citep{steinhardt2017certified}). This is a special case of Huber's contamination model, which is used in the robust statistics literature \citep{diakonikolas2023algorithmic} with the contamination model being a Dirac distribution. For compactness of the data generation process we define the following:
\begin{align}
    \mathbb{P}_{\epsilon}(\bfzadv):= \epsilon\delta
%\text{Dirac}
(\bfzadv) + (1-\epsilon)\Pdata.
\end{align}

% \maryam{how about using the notation $\delta$ instead of 'Dirac'? it's shorter and more common in probability and in engineering.}



%\paragraph{Potential Defense} Inspired by differentially private learning algorithms like DP-SGD \citep{bassily2014private}, we propose adding Gaussian noise to the learning process as a way of smoothing the learning algorithm against impacts of the poisoning adversary. In particular, we add $\bfB\bfw_t$ where $\bfw_t$ is iid noise in each step sampled from the standard Gaussian, and $\bfB$ is a design parameter of the learning algorithm. Subsequently, we will choose $\bfB$ so as to minimize the worst case impact of the poisoning adversary. We denote by $\bfS=\bfB\tran{\bfB}$ the covariance matrix of the noise added.
% More generally, we consider learning algorithms that can be expressed as $\bftheta_{t+1} \gets F_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}$ where $\bfw_t$ is noise and $\phi$ are tunable parameters of the learning algorithm.

\subsection{Adversarial Objective}
\paragraph{Transition Kernel.}
%\maryam{It's worth writing this sentence more clearly about what order things happen in}
Starting with a parameter estimate $\bftheta$, the adversary chooses $\bfzadv$, then the learning algorithm updates the parameter estimate (via $F_\phi$). The transition kernel gives the probability (or probability density) that the parameter estimate assumes a value $\bftheta^\prime$ after the above steps, and is defined as (recall that $\delta(\cdot)$ denotes the Dirac delta function):
\begin{align}
    \Pi(\bftheta^\prime | \bftheta, \bfzadv) = \ExP{\bfz \sim \mathbb{P}_{\epsilon}(\bfzadv)}{\delta
    %\text{Dirac}
    (F_{\phi}(\bftheta, \bfz) - \bftheta^\prime)}.
\end{align}
\paragraph{Dynamics as a Markov Process.}
The dynamics in Eq. \eqref{eq:dyn} gives rise to a Markov %chain
process over the parameters $\bftheta$. If $\mathbb{P}_t$ denotes the distribution over parameters at time $t$, we have
% \[
% \mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv_t}
% \mathbb{P}_{t}\br{\bftheta}d\bftheta,
% \]
\begin{align}
    \mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \Pi(\bftheta^\prime | \bftheta, \bfzadv)
\mathbb{P}_{t}\br{\bftheta}d\bftheta.
\end{align}
Since the learning algorithm (dynamics of the parameters) is a Markov process, the sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
% \begin{align*}
%  \underbrace{\bftheta}_{\text{States}},  \underbrace{\bfzadv}_{\text{Actions}},   \underbrace{\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv}}_{\text{Transition Kernel}}.
% \end{align*}
\begin{align*}
 \underbrace{\bftheta}_{\text{States}},  \underbrace{\bfzadv}_{\text{Actions}},   \underbrace{\Pi(\cdot | \bftheta, \bfzadv) }_{\text{Transition Kernel}}.
\end{align*}
\paragraph{Adversarial objective function.}

We assume that the poisoning adversary is interested in maximizing some adversarial objective $\ell_{\textrm{adv}}: \Thetaspace \mapsto \R$, for example, the expected prediction error on some target distribution of interest to the adversary. The adversary wants to choose actions such that it can maximize its average reward over time. Utilizing the fact that the optimal policy for an MDP is stationary (i.e., the policy is time invariant), we define the adversary's objective for an arbitrary stationary policy $\bfzadv \sim \pi(\cdot| \bftheta)$ as follows:
\begin{align}
    \rho(\pi) = \lim_{T \to \infty}\frac{1}{T} \ExP{\pi}{\sum_{t=1}^T \ell_{\textrm{adv}} (\bftheta_t)}, \label{eq:avg_cost}
\end{align}
where the expectation is with respect to the noisy state transition dynamics induced by the adversary's poisoning policy $\pi$.

We utilize the fact that $\rho(\pi)$ is equal to the expected reward under the \textit{stationary state distribution}
% under policy $\pi$, denoted by $d_{\pi}$
(assuming the MDP is ergodic, see details in Appendix~\ref{appendix:proofs}):
% \maryam{should we add 'if it exists'? did we discuss this in the appendix? if not we should write the appropriate assumption (ergodic MDP/ stable dynamics?)} under policy $\pi$  denoted by $d_{\pi}$,
\begin{align*}
    \rho(\pi) = \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\rm adv}(\bftheta)}.
\end{align*}
The stationary state is defined as a condition where the distribution of parameters remains unchanged over time. In other words, the distribution of parameters at any given point in the stationary state is identical to the distribution of parameters at the next state.

The stationarity condition can be expressed mathematically in terms of the transition kernel as:
\begin{align*}
    \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  d_{\pi}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
\end{align*}

Given a family of learning algorithms $F_\phi$ with tunable parameters $\phi \in \Phispace$, our goal is to estimate $\phi$ so that our learning algorithm is robust to the poisoning as described above. However, since we assume that we are working in the online setting, it is seldom the case that we know the data distribution $\Pdata$ in advance, making the adversary's objective intractable. In Section~\ref{sec:meta_learning}, we use a meta learning formulation to overcome the lack of knowledge about $\Pdata$ in advance.
% We utilize the fact that $\rho(\pi)$ is equal to the expected reward under the \textit{stationary state distribution}
% % under policy $\pi$, denoted by $d_{\pi}$
% (assuming the MDP is ergodic, see details in Appendix):
% % \maryam{should we add 'if it exists'? did we discuss this in the appendix? if not we should write the appropriate assumption (ergodic MDP/ stable dynamics?)} under policy $\pi$  denoted by $d_{\pi}$,
% \begin{align*}
%     \rho(\pi) = \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\rm adv}(\bftheta)}.
% \end{align*}
% The stationary state is defined as a condition where the distribution of parameters remains unchanged over time. In other words, the distribution of parameters at any given point in the stationary state is identical to the distribution of parameters at the next state.
% We use $\mathbb{P}_{\infty}$ to denote the stationary state distribution over respective variables which will be clear from the context. For example $\bftheta, \bfzadv \sim \mathbb{P}_{\infty}$ denotes $\bftheta, \bfzadv$ sampled from the \textit{stationary state joint distribution over the parameters and the adversarial poisoning},  $\bftheta \sim \mathbb{P}_{\infty}$ denotes $\bftheta$ sampled from the stationary state distribution over the parameters obtained by marginalizing over $\bfzadv$ from the joint distribution.
% and $\mathbb{P}_{\infty}^{\bfphi}$ denotes the stationary distribution over hyperparameters.

% With these notations, we can now mathematically express the stationarity condition as:
% \begin{align*}
%     \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
% \end{align*}
% The stationarity condition can be expressed mathematically in terms of the transition kernel as:
% \begin{align*}
%     \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  d_{\pi}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
% \end{align*}
% $\tau = \{(\bftheta_1, \bfzadv_1), \ldots\}$, \maryam{so should the distribution of $\tau$ be $\pi^{T}$ ($\pi$ sampled $T$ times? or the notation means something else?)} is the collection of state action pairs drawn from the adversarial policy given an initial state $\bftheta_1$ (or state distribution $\bftheta_1 \sim \mathbb{P}_1$). %respectively).
% where $\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}$ is the transition kernel induced, i.e., the conditional probability distribution of $\bftheta_{t+1}=F\br{\bftheta_t, \bfz_t, \bfphi_t}$ by \eqref{eq:dyn} given $\bftheta_t$ and $\bfzadv_t$. We formally define it as follows:
% \begin{align*}
%     &\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}(\bftheta^\prime|\bftheta_t, \bfzadv_t) %\maryam{\mbox{need a $t$ index for $z^\rm{adv}$?}}
%     \\&=\ExP{\bfz_t \sim \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata , \bfphi_t \sim \mathbb{P}_t^{\bfphi}} {\mathbb{I}[F\br{\bftheta, \bfz_t, \bfphi_t} = \bftheta^\prime]},
% \end{align*}
% which is the conditional probability of the parameter estimate at time $t+1$, $\bftheta_{t+1}$ assuming a value $\bftheta^\prime$, conditioned on the parameter estimate at time $t$ being $\bftheta_t$ and the adversarial posion being $\bfzadv_t$.

% \djcomment{While this is technically correct, the expected value style of denoting this is indeed confusing. Perhaps we can }

% \maryam{let's write it with $\delta(\cdot)$.}
% \maryam{let's remember to explain the $t$ indices}
% \paragraph{Dynamics as a Markov Decision Process}
% The dynamics \eqref{eq:dyn} gives rise to a Markov chain over the parameters $\bftheta$. If $\mathbb{P}_t$ denotes the distribution over parameters at time $t$, we have

% \[\mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv}\mathbb{P}_{t}\br{\bftheta}d\bftheta,\]
% where $\mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}$ is the transition kernel induced, i.e., the conditional probability distribution of $\bftheta_{t+1}=F\br{\bftheta_t, \bfz_t, \phi_t}$ by \eqref{eq:dyn} given $\bftheta_t$ and $\bfzadv_t$. We formally define it as follows:
% \begin{align*}
%     \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}(\bftheta^\prime|\bftheta, \bfzadv) = \ExP{\bfz_t \sim \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata , \phi_t \sim \mathbb{P}_t^\phi}{\mathbb{I}[F\br{\bftheta, \bfz_t, \phi_t} = \bftheta^\prime]}.
% \end{align*}

% \avi{Time index $F$ as $F_t$ and analyse it at $F_{\infty}$. Making additional assumptions on start state, could lead to finding the optima on a compact space if $\Theta$ is not compact.}
%explicitly given by
% \begin{align}\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \ExP{\bfw}{F\br{\nabla \ell\br{\bftheta_t, \bfzadv_t}, \bfw}} + (1-\epsilon) \ExP{\substack{\bfw \\ \bfz \sim \Pdata}}{F\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw}}
% \end{align}
%\begin{align}\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\left(\bftheta_t - \eta \nabla \ell(\bftheta_t, \bfzadv_t), \eta^2 \bfS\right) + (1-\epsilon) \ExP{\substack{\bfz \sim \Pdata}}{\mathcal{N}\left(\bftheta_t - \eta \nabla \ell\left(\bftheta_t, \bfz\right), \eta^2 \bfS\right)}
%\end{align}
%where $\mathcal{N}\br{\bfx|\mu, \Sigma}$ denotes the Gaussian density at $x$ for a Gaussian with mean $\mu$ and covariance matrix $\Sigma$.

\subsection{Meta-learning a robust learning algorithm}\label{sec:meta_learning}
% The above certificate holds for a fixed data distribution.
 In a meta learning setup \citep{hochreiter2001learning, andrychowicz2016learning}, we suppose that we have access to a meta-distribution from which data distributions can be sampled. In such a setup, we can ``simulate" various data distributions and consider the following approach: We take a family of learning algorithms $F_\phi$ with tunable parameters $\phi \in \Phispace$,
 % Based on the results from Section \ref{sec:certificate}, we can
 and attempt to design the parameters $\phi$ of the learning algorithm to trade-off performance and robustness in expectation over the data distributions sampled from the meta-distribution.
% In particular, in the absence of poisoned data, assume that the updates \eqref{eq:dyn} result in a stationary distribution $\mathbb{P}\br{\phi, \Pdata}$ over model parameters $\bftheta$.

In particular, in the absence of poisoned data, the updates \eqref{eq:dyn} on data sampled from benign data distribution $\Pdata$ result in a stationary distribution over model parameters $\bftheta$ denoted by $\mathbb{P}\bigl(\phi, \Pdata\bigr)$. The expected benign target loss can be written as :
\begin{align}
    b(\phi, \Pdata) = \ExP{\bftheta \sim \mathbb{P}\br{\phi, \Pdata}}{\ell\br{\bftheta}},
\end{align}
where $\ell : \Thetaspace \mapsto \R$ is the loss the learning algorithm wants to minimize.
% We explicitly denote the transition kernel, in case of a posioned learning algorithm receving benign samples from $\Pdata$.

In Section~\ref{sec:certificate}, we propose a general formulation to derive an upper bound on the worst case impact of an adversary on the target loss (a certificate), which we denote by $c(\phi, \Pdata)$ for a given data distribution $\Pdata$ and parameter of the learning algorithm $\phi \in \Phispace$.

Given a meta distribution $\mathcal{P}$, we can propose the following criterion to design a robust learning algorithm:
% \avi{I think the infimum over $\lambda$ should be before the outer expectation, since we are getting the certificate per learner. }\djcomment{Fair enough, I am a bit skeptical that we would actually be able to solve the version with an inner minimization over $\lambda$ as that would require a lot of computation/memory (we would need to store one $\lambda$ for each data distribution). In particular, think about some extremely large source of data distributions and we can just about do one pass of stochastic gradients in the outer loop, trying to solve for the optimal $\lambda$ for each will probably be too much. Solving for a common certificate $\lambda$ will be computationally cheaper and hopefully more generalizable.}
% \begin{align}\label{eq:main_formulation}
%     &\inf_{\substack{\phi \in \Phispace \\ \lambda: \Thetaspace \mapsto \R}} \mathbb{E}_{\Pdata \sim \mathcal{P}}\bigg[\ExP{\bftheta \sim \mathbb{P}\br{\phi, \Pdata}}{\ell\br{\bftheta}}
%      \\&\; +\kappa \Bigl(\!\sup_{\substack{\bftheta \in \Thetaspace\\\bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot| \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}\Bigr)\bigg], \nonumber
% \end{align}
\begin{align}\label{eq:main_formulation}
    &\inf_{\phi \in \Phispace} \mathbb{E}_{\Pdata \sim \mathcal{P}}\bigg[b(\phi, \Pdata) + \kappa \cdot c(\phi, \Pdata) \bigg].
     % \\&\; +\kappa \Bigl(\!\sup_{\substack{\bftheta \in \Thetaspace\\\bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot| \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}\Bigr)\bigg], \nonumber
\end{align}
% \maryam{what is the subscript $S$ in the inner problem above? It didn't apear earlier, so let's define here}
where $\kappa > 0$ is a trade-off parameter. The expectation over $\mathcal{P}$ is a meta-learning inspired formulation, where we are designing a learning algorithm that is good ``in expectation" under a meta-distribution over data distributions. The first term constitutes ``doing well" in the absence of the adversary by converging to a stationary distribution over parameters that incurs low expected loss. The second term is an upper bound on the worst case loss incurred by the learning algorithm in the presence of the adversary.\looseness=-1

% \begin{algorithm}[t]
%     \caption{Meta learning a robust learning algorithm}
%     \begin{algorithmic}[1]
%         \STATE \textbf{Input: } Set of $K$ distributions sampled from $\mathcal{P}[\Pdata]$, tradeoff parameter $\kappa$.
%         \STATE For an arbitrary $\phi \in \Phispace$ write the expression of the following quantities.
%         \FOR{$\Pdata \in \mathcal{P}$}
%         \STATE \textbf{Benign Loss:} $b_i(\phi) = \ExP{\bftheta \sim \mathbb{P}\br{\phi, \Pdata}}{\ell\br{\bftheta}}$
%         \STATE \textbf{Certificate of Robustness:}
%         $c_i(\phi) = \inf_{\lambda: \Thetaspace \mapsto \R} \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}$
%          \ENDFOR
%         \STATE $\widehat{\phi} = \inf_{\phi \in \Phi} \sum_{i \in [K]} \left[b_i(\phi) + \kappa c_i(\phi)\right]$.
%         % \IF{$\bfS^{(t+1)} = \bfS^{(t)}$}
%         % \STATE break
%         % \ENDIF
%         \RETURN $\widehat{\phi}$
%     \end{algorithmic}
%     \label{alg:meta_learning}
% \end{algorithm}

\subsection{Technical Approach: Certificate of Robustness} \label{sec:certificate}


% \textcolor{red}{let's title this as: Technical Approach ( and instead of theorem, say proposition)}\avi{Why is it not a result yet?} \textcolor{red}{we have just written the optimization problem, we'll have actual certified bounds for cases we can solve/approximate and that is case by case---more appropriate to call this our 'methodology' or 'technical approach' as opposed to result.}



% The adversary wants to maximize its objective at the stationary state of the Markov Decision Process.
% We utilize the fact that $\rho(\pi)$ is equal to the expected reward under the \textit{stationary state distribution}
% % under policy $\pi$, denoted by $d_{\pi}$
% (assuming the MDP is ergodic, see details in Appendix):
% % \maryam{should we add 'if it exists'? did we discuss this in the appendix? if not we should write the appropriate assumption (ergodic MDP/ stable dynamics?)} under policy $\pi$  denoted by $d_{\pi}$,
% \begin{align*}
%     \rho(\pi) = \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\rm adv}(\bftheta)}.
% \end{align*}
% The stationary state is defined as a condition where the distribution of parameters remains unchanged over time. In other words, the distribution of parameters at any given point in the stationary state is identical to the distribution of parameters at the next state.
% % We use $\mathbb{P}_{\infty}$ to denote the stationary state distribution over respective variables which will be clear from the context. For example $\bftheta, \bfzadv \sim \mathbb{P}_{\infty}$ denotes $\bftheta, \bfzadv$ sampled from the \textit{stationary state joint distribution over the parameters and the adversarial poisoning},  $\bftheta \sim \mathbb{P}_{\infty}$ denotes $\bftheta$ sampled from the stationary state distribution over the parameters obtained by marginalizing over $\bfzadv$ from the joint distribution.
% % and $\mathbb{P}_{\infty}^{\bfphi}$ denotes the stationary distribution over hyperparameters.

% % With these notations, we can now mathematically express the stationarity condition as:
% % \begin{align*}
% %     \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
% % \end{align*}
% The stationarity condition can be expressed mathematically in terms of the transition kernel as:
% \begin{align*}
%     \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  d_{\pi}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
% \end{align*}
% The expected adversarial loss at stationarity is given by $\ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}}$.

% We are now ready to present our technical result, a certificate of robustness against dynamic data poisoning adversaries. Since the learning algorithm
% is a Markov process, the optimal sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
% \[\text{States } \bftheta, \text{Actions } \bfzadv, \text{Transition Kernel }  \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv},\]
% and hence, the adversary's optimal action sequence can be written as the solution of an \emph{infinite dimensional} linear program (LP)  \cite{puterman2014markov}. The adversary is interested in identifying a
% In particular, for the infinite horizon average reward setting \citep{malek2014linear}, the LP can be written as
% We are now ready to present our technical result, a certificate of robustness against dynamic data poisoning adversaries, which is the largest objective value any dynamic adversary can attain in the stationary state.
% Since the learning algorithm
% is a Markov process, the optimal sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
% \[\text{States } \bftheta, \text{Actions } \bfzadv, \text{Transition Kernel }  \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv},\]

For a given $\Pdata \sim \mathcal{P}$, we attempt to find an upper bound on the worst case impact of the adversary.
Recalling that the sequence of actions for the adversary constitutes a Markov Decision Process, the value of the adversarial objective for the adversary's optimal action sequence is therefore the average reward in the infinite horizon Markov Decision Process setting \citep{malek2014linear} and can be written as the solution of an \emph{infinite dimensional} linear program (LP)  \cite{puterman2014markov}.
% The adversary is interested in identifying a
% In particular, for the infinite horizon average reward setting .
The LP can be written as:
% \begin{subequations}
% \begin{align}
%     \sup_{\mu} & \ExP{\bftheta, \bfzadv \sim \mu}{\ell_{\textup{adv}}\br{\bftheta}} \\
%     \text{Subject to } & \mu \in \mathcal{P}[\R^d \times \R^n] \\
%     & \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \R^d
% \end{align}\label{eq:lp_inf}
% \end{subequations}
% \begin{subequations}
% \begin{align}\label{eq:lp_inf}
%     \sup_{\mathbb{P} \in \mathcal{P}[\Theta \times \bfZ]}  \quad& \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\ell_{\textup{adv}}\br{\bftheta}} \\
%        \text{subject to }\quad & \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\mathbb{P}_{F, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
% \end{subequations}
% \begin{subequations}
% \begin{align}\label{eq:lp_inf}
%     \sup_{\mathbb{P}_{\infty} \in \mathcal{P}[\Theta \times \bfZ]}  \quad& \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}} \\
%        \text{subject to }\quad & \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\mathbb{P}_{F, \Pdata, \mathbb{P}^\phi_{\infty}, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
% \end{subequations}
% \begin{subequations}
% \begin{align}\label{eq:lp_inf}
%     &\sup_{\mathbb{P}_{\infty} \in \mathcal{P}[\Theta \times \bfZ]}  \quad \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}} \quad \text{subject to } \\
%        & \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_{\infty}}\br{\bftheta^\prime|\bftheta, \bfzadv}} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
% \begin{align}\label{eq:lp_inf}
%     &\sup_{\mathbb{P}_{\infty} \in \mathcal{P}[\Theta \times \bfZ]}  \quad \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}} \quad \text{subject to } \\
%        & \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
\begin{align}\label{eq:lp_inf}
    &\sup_{\substack{d_{\pi} \in \mathcal{P}[\Thetaspace] \\ \pi \in \mathcal{P}[\Thetaspace \times \bfZ]}}  \quad \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\textup{adv}}\br{\bftheta}}, \quad \text{subject to } \\
       & \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  d_{\pi}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta \nonumber,
\end{align}
% \djcomment{While it is nice to have the stationarity condition, I don't think it is necessary here. We should first state the problem we are trying to solve - at stationarity, what is the expected rewards of the attacker? Then theorem 1 provides a way to bound this quantity.}
% \end{subequations}
% \avi{I rewrote the set of equations below: check if it is notationally more consistent that the one above. Maybe not much of a difference on second thought.}
% In particular, for the infinite horizon average reward setting \citep{malek2014linear}, the LP can be written as
% \begin{subequations}
% \begin{align}
%     \sup_{\mu} & \ExP{\bftheta, \bfzadv \sim \mu}{\ExP{\bftheta^\prime | \bftheta, \bfzadv}{\ell_{\textup{adv}}\br{\bftheta^\prime}}} \\
%     \text{Subject to } & \mu \in \mathcal{P}[\R^d \times \R^n] \\
%     & \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \R^d
% \end{align}
% \end{subequations}
% \maryam{the optimization problem should be referred to with a single equation number, i.e., problem (5) in thm 1 (not 5a)}
where $\mathcal{P}[\Thetaspace]$, $\mathcal{P}[\Thetaspace \times \bfZ]$ denote the space of probability measures on $\Thetaspace$ and $\Thetaspace \times \bfZ$ respectively.
% and $\mathbb{I}$ denotes the indicator function that equals $1$ if its argument is true and $0$ otherwise.
%\maryam{Dj, are there other papers to cite that use this formulation?}
%\djcomment{Maryam, we quote references [13] and [17] based on which this is derived. Do we need anything further? If you mean specifically in the context of poisoning, I am pretty sure not, no other papers use this.}

We are now ready to present our %technical result, a
certificate of robustness against dynamic data poisoning adversaries, which is the largest objective value any dynamic adversary can attain in the stationary state.
% \begin{theorem}\label{thm:certificate}
\begin{restatable}{theorem}{certificate}\label{thm:certificate}
 For any function $\lambda: \Theta \mapsto \R$, for any dynamic adaptive adversary, the average loss \eqref{eq:avg_cost} is bounded above by
% \begin{align}
%     \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \qquad \ExP{\bftheta^\prime \sim \mathbb{P}_{F, \Pdata, \bfzadv}\br{\cdot|\bftheta}}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}. \label{eq:cert_general}
% \end{align}
\begin{align}
    \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \quad \ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}. \label{eq:cert_general}
\end{align}
% \end{theorem}
\end{restatable}
\begin{proof}
% \avi{Done for space constraints}
% \textit{Proof.}
Follows by weak duality for the LP
\eqref{eq:lp_inf}. Detailed proof in Appendix~\ref{appendix:proofs}.
% \maryam{let's refer to appendix and write it there carefully with citation in inf-dimensional duality}
\end{proof}
%\eqref{eq:certificate}.
% \maryam{Give more details on derivation (or point to appendix)}

If strong duality holds \cite{nash1987linear, clark2003infinite},
% \maryam{Cite references that properly cover infinite dimensional duality and constraint qualifications. Maybe the book by Attouche et al on optimization in Sobolev spaces? Other refs?}
we further have that the optimal value of \eqref{eq:lp_inf} is exactly equal to
% \begin{subequations}
\begin{align}
\hspace{-.18in}
{\tiny{\inf_{\lambda: \Thetaspace \mapsto \R} \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}. }}\label{eq:certificate}
\end{align}
% \end{subequations}
% \end{proof}
% \vskip -0.5in
% \avi{How do we constrain $\lambda$ such that $\ExP{\bftheta^\prime \sim \mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\cdot|\bftheta}}{\lambda\br{\bftheta^\prime}} -\lambda\br{\bftheta} \geq 0 \; \forall  \bftheta$? }

% \maryam{fixed some typos above. indices of $\theta'$ distribution are very hard to follow, need to simplify---explain earlier and replace with slightly simpler notation?}\avi{Check now? Can add a line that, with indices is a random variable, without indices is a realization of the random variable.}
% \avi{We should comment that solving Eq.~\ref{eq:lp_inf} is expensive due to the infinite time horizon and sequential nature of the problem and Theorem 1 solves that problem.}
% \djcomment{My thinking is that this would be much simpler to explain for the discounted horizon setting where we can interpret $\lambda$ as an upper bound on the expected discounted sum of future rewards. I am leaning towards only sketching the derivation for the discounted horizing setting where we can show that as long as $\lambda$ satisfies certain constraints, the. What do you think? }

% \subsection{Meta-learning a robust learning algorithm}\label{sec:meta_learning}
% The above certificate holds for a fixed data distribution. In practice, it is seldom the case that we know the data distribution in advance and can compute this certificate specifically. However, in a meta learning setup \citep{hochreiter2001learning, andrychowicz2016learning}, we suppose that we have access to a meta-distribution from which data distributions can be sampled. In such a setup, we can ``simulate" various data distributions and consider the following approach: We take a family of learning algorithms $F_\phi$ with tunable parameters $\phi \in \Phispace$. Based on the results from Section \ref{sec:certificate}, we can attempt to design the parameters $\phi$ of the learning algorithm to trade-off performance and robustness.
% % In particular, in the absence of poisoned data, assume that the updates \eqref{eq:dyn} result in a stationary distribution $\mathbb{P}\br{\phi, \Pdata}$ over model parameters $\bftheta$.
% In particular, in the absence of poisoned data, the updates \eqref{eq:dyn} on data sampled from benign data distribution $\Pdata$ result in a stationary distribution over model parameters $\bftheta$ denoted by $\mathbb{P}\bigl(\phi, \Pdata\bigr)$. We explicitly denote the transition kernel, in case of a posioned learning algorithm receving benign samples from $\Pdata$.


% Given a meta distribution $\mathcal{P}$, we can propose the following criterion:
% % \avi{I think the infimum over $\lambda$ should be before the outer expectation, since we are getting the certificate per learner. }\djcomment{Fair enough, I am a bit skeptical that we would actually be able to solve the version with an inner minimization over $\lambda$ as that would require a lot of computation/memory (we would need to store one $\lambda$ for each data distribution). In particular, think about some extremely large source of data distributions and we can just about do one pass of stochastic gradients in the outer loop, trying to solve for the optimal $\lambda$ for each will probably be too much. Solving for a common certificate $\lambda$ will be computationally cheaper and hopefully more generalizable.}
% \begin{align}\label{eq:main_formulation}
%     &\inf_{\substack{\phi \in \Phispace \\ \lambda: \Thetaspace \mapsto \R}} \mathbb{E}_{\Pdata \sim \mathcal{P}}\bigg[\ExP{\bftheta \sim \mathbb{P}\br{\phi, \Pdata}}{\ell\br{\bftheta}}
%      \\&\; +\kappa \Bigl(\!\sup_{\substack{\bftheta \in \Thetaspace\\\bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot| \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}\Bigr)\bigg], \nonumber
% \end{align}
% % \maryam{what is the subscript $S$ in the inner problem above? It didn't apear earlier, so let's define here}
% where $\kappa > 0$ is a trade-off parameter. The outer expectation is a meta-learning inspired formulation, where we are designing a learning algorithm that is good ``in expectation" under a meta-distribution over data distributions. The first term in the outer expectation constitutes ``doing well" in the absence of the adversary by converging to a stationary distribution over parameters that incurs low expected loss. The second term is an upper bound on the worst case loss incurred by the learning algorithm in the presence of the adversary.\looseness=-1

% \begin{algorithm}[t]
%     \caption{Meta learning a robust learning algorithm}
%     \begin{algorithmic}[1]
%         \STATE \textbf{Input: } Set of $K$ distributions sampled from $\mathcal{P}[\Pdata]$, tradeoff parameter $\kappa$.
%         \STATE For an arbitrary $\phi \in \Phispace$ write the expression of the following quantities.
%         \FOR{$\Pdata \in \mathcal{P}$}
%         \STATE \textbf{Benign Loss:} $b_i(\phi) = \ExP{\bftheta \sim \mathbb{P}\br{\phi, \Pdata}}{\ell\br{\bftheta}}$
%         \STATE \textbf{Certificate of Robustness:}
%         $c_i(\phi) = \inf_{\lambda: \Thetaspace \mapsto \R} \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}$
%          \ENDFOR
%         \STATE $\widehat{\phi} = \inf_{\phi \in \Phi} \sum_{i \in [K]} \left[b_i(\phi) + \kappa c_i(\phi)\right]$.
%         % \IF{$\bfS^{(t+1)} = \bfS^{(t)}$}
%         % \STATE break
%         % \ENDIF
%         \RETURN $\widehat{\phi}$
%     \end{algorithmic}
%     \label{alg:meta_learning}
% \end{algorithm}
% \djcomment{Avi:
\begin{comment}
Eventually we may want to make the above meta learning setup more flexible along the following dimensions:
\begin{itemize}
    \item Generalize the tunable parameters of the learning algorithm beyond just $\bfS$. For example, a general update rule of the form : $\bftheta_{t+1} = F_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t, \eta_t}$ (for instance gradient based methods like $\bftheta_{t+1} = \bftheta_t - \eta_t  g_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}$), where $\phi$ replaces $\bfS$ in the outer infimum.
    \item Generalize the expected loss formulation (in the outer minimization with something robust, distributionally robust, or risk averse with respect to $\mathcal{P}$).
    % \avi{Sounds great!}
\end{itemize}
% }
\end{comment}


\begin{comment}

\section{Literature Review}
Literature review:

\begin{itemize}
    \item poisoning attacks, Laurent's L4DC paper with Xuezhou Zhang and Jerry Zhu (Xuezhou did the examples, can ask him). Are these examples convincing for an ML conference though (Neurips)? It's a different audience and review process than L4DC. We may need to explore more ML-ish poisoning papers for examples and datasets.\djcomment{I think these papers \citep{wang2019investigation}\citep{wang2018data} actually are probably even more representative, and do consider the dynamic adversary setting as well. These may be a good starting point. }
    \item backdoor attacks --- can be seen as a special case (but need to argue this carefully) \djcomment{ \citep{goldblum2022dataset} provides a thorough review although not from a mathematical perspective. The key difference IMO is if we say the loss is $\ell\br{\bftheta, \bfz}$ where $\bftheta$ are learned model parameters and $\bfz$ is the input datapoint we are asked to make a prediction on (a feature-label pair for example), in the standard data poisoning setting, we assume only access to (parts) of the training data, while in the backdoor setting, we assume access to both the training data and $\bfz$, so the attacker can corrupt both the training data and $\bfz$ by inducing an adversiarial trigger or backdoor into $\bfz$ (for example, in an LLM, maybe this is any prompt that has the word ``Abracadabra'' in it). In our setting though, since we allow the loss optimized by the adversary to be arbitrary, we can simply choose it to be
    \[\ExP{\bfz \sim \Ptarget}{\ell\br{\bftheta, \bfz}}\]
    where $\Ptarget$ is the distribution of inputs that contains the trigger (for example the distribution over all prompts conditioned on the prompt containing the word ``Abracadabra'' in it)
    }
    \item federated learning attacks --- can corrupt the gradients (discuss if relevant or not relevant to out setup; there's a lot of work on this)
    \item tools from control used in similar ML contexts (lots of Laurent's work, Dj's work with Ian Manchester, etc.) \djcomment{\citep{wang2024monotone, wang2023direct}}
    \item Some literature to motivate dynamic adversary \djcomment{See references given for first bullet.}

\item Make a distinction between the static setting and the online setting (most literature are focused on online)

\end{itemize}
\end{comment}


\section{MEAN ESTIMATION}\label{sec:mean_estimation}
% We consider the special case of mean estimation where $\ell_{\textup{adv}}, \ell\br{\theta}$ are quadratic. In particular,
Consider the mean estimation problem, where we aim to learn the parameter $\bftheta \in \R^d$ to estimate the mean $\mu = \ExP{\bfz \sim \Pdata}{\bfz}$ of a distribution $\Pdata$. Given a data point $\bfz_t$, the learning rule is given by:
\begin{align}\label{eq:mean_update}
    \bftheta_{t+1} \gets (1 - \eta)\bftheta_t + \eta \bfz_t +  \eta \bfB \bfw_t,
\end{align}
where $\eta$ is the learning rate, $\bfS = \bfB \bfB^\top \in \mathbb{S}^d_{+}$ is the tunable defense hyperparameter and $\bfw_t \sim \mathcal{N}(0, \bfI)$ is Gaussian noise. The adversary wants to maximize its average reward according to the following objective function:
\begin{align}
\ell_{\textup{adv}}\br{\bftheta} = \norm{\mu-\bftheta}^2.
\end{align}
% Our goal is minimizing the following objective:
% \avi{Overloaded symbol $\mu$}
% \[\ell\br{\bftheta} = \ExP{\bfz \sim \Pdata}{\norm{\bfz-\bftheta}^2} = \norm{\bftheta -\mu}^2 + \ExP{\bfz \sim \Pdata}{\bfz\bfz^\top} - \mu^2\]

% Upon observing a sample $\bfz$, the learning algorithm updates the parameter $\theta$ as follows:
% \begin{align*}
%     \theta \leftarrow \theta - \eta(\bftheta - \bfz) + \eta \bfB \bfw,
% \end{align*}
% where $\bfw \sim \mathcal{N}(0, \bfI)$, $\eta$ denotes the learning rate and $\bfB\bfB^\top = \bfS$.
% 2\tran{\mu}\bftheta + \ldots \]



% To avoid detection the adversary poisons samples from within a constraint set. Suppose the constraint set $\mathcal{A}$ is defined by ${(\bfzadv - \mu)}^\top (\bfzadv - \mu) \leq c$.
% \avi{We were using $F(\bftheta, \bfz)$ for the update rule form earlier, lets make it consistent.}
% \djcomment{Sounds good, feel free to make this change}
% \subsection{Certificate on adversarial loss (analysis)}
\paragraph{Certificate on adversarial loss (analysis).}

% If $\bfzadv$ is constrained to be from within the typical set of a Gaussian, then $\mathcal{A}$ is defined by quadratic constraints and hence, the optimization problem \eqref{eq:main_formulation} can is a QCQP.
% \avi{Working out the functional form:}

% Let $\lambda(\bftheta) = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb + \ldots$ be a quadratic function, and

% We assume the data distribution is Gaussian of the form $\Pdata(\bfz) = \mathcal{N}(\bfz | \mu, \Sigma)$.

% \begin{align}
%     \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \bftheta^\top \bfD_1 \bftheta + \epsilon \eta^2 \bfzadv^\top \bfA \bfzadv + \bftheta^\top \bfD_2 \bfzadv + \bftheta^\top \bfd_3 + \epsilon \eta \bfzadv^\top \bfb + d_4 + \nu ().
% \end{align}

% Since $\bftheta$ is unconstrained, we wish to analytically derive the $\bftheta$ that maximizes the bound in terms of $\bfzadv$. Define $\bfX = 2(((1 - \eta)^2 - 1) \bfA + \bfI), \bfb_1 = 2\mu -2\eta(1 - \eta)(1 - \epsilon) \bfA \mu + \eta \bfb , \bfB_2 = 2\eta \epsilon (1 - \eta)\bfA, \bfy =\bfb_1 -  \bfB_2 \bfzadv$. For a given $\bfzadv$, the value of $\bftheta$ maximizing the objective is given by the solution to the equation:
% Suppose the constraint set $\mathcal{A}$ is defined by $\bfzadv^\top \bfzadv \leq c$. Then we can write the dual function of this supremum as:
% Suppose the constraint set $\mathcal{A}$ is defined by ${(\bfzadv - \mu)}^\top (\bfzadv - \mu) \leq c$.

% \begin{align*}
%     g(\nu) &=
%     \begin{cases}
%     &  \bftheta^\top \bfD_1 \bftheta +  \bfzadv^\top  (\epsilon\eta^2 \bfA + \nu \bfI) \bfzadv + \bftheta^\top \bfD_2 \bfzadv + \bftheta^\top \bfd_3 + \epsilon \eta \bfzadv^\top \bfb + d_4 - \nu c.\\
%     & \text{if} \quad \nu \leq 0 ;\; \bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \epsilon\eta^2 \bfA + \nu \bfI} \preceq 0; \; 2\bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \eta^2 \bfA + \nu \bfI} \bmat{\theta \\ \bfzadv} + \bmat{\bfd_3 \\ \epsilon \eta \bfb} = 0. \\
%     & -\infty \quad \text{else}
%     \end{cases}
% \end{align*}

% Given a fixed learning algorithm (i.e. $\bfS$ is fixed), the certificate is given by $\inf_{\bfA \in \mathbb{S}^d, \bfb \in \R^d \\ \nu \leq 0} g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma)$.

\begin{restatable}{theorem}{meancertificate}
Choosing $\lambda : \R^d \rightarrow \R$ in Theorem~\ref{thm:certificate} to be quadratic, i.e.  $\lambda\br{\bftheta} = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb$, the adversarial constraint set of the form $\|\bfzadv - \mu\|_2^2 \leq r$, %\maryam{did we decide to change this?}\avi{No. It was in classification setting.},
the certificate for the mean estimation problem for $\Pdata(\bfz) = \mathcal{N}(\bfz | \mu, \bfSigma)$
%\maryam{$\mathcal{N}(\mu,\Sigma)$ is more standard if no confusion about the random variable}
for a fixed learning algorithm (i.e. $\bfS$ is fixed) is given by:
\begin{align}
    \inf_{\bfA \in \mathbb{S}^d, \bfb \in \R^d, \nu \geq 0} g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma),\label{eq:mean_certificate}
\end{align}
where $g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma)$ is a convex objective in $\bfA,\bfb,\nu$  as defined below:
% \begin{align}
%     g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma) &=
%     \begin{cases}
%     &  \frac{1}{4}\bmat{\bfd_3 \\ \epsilon \eta \bfb - 2 \nu \mu}^\top\br{- \bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \epsilon\eta^2 \bfA + \nu \bfI}}^{-1}\bmat{\bfd_3 \\ \epsilon \eta \bfb - 2 \nu \mu}  +  d_4 + \nu(\mu^\top \mu - r) \\& \text{if} \quad \nu \leq 0 ;\; \bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \epsilon\eta^2 \bfA + \nu \bfI} \preceq 0 \\
%     & -\infty \quad \text{else}
%     \end{cases}\label{eq:dualfunction}
% \end{align}
\begin{equation}
    % g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma)
    \begin{cases}
    \frac{1}{4} \left\|\bmat{2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb \\ \epsilon \eta \bfb + 2 \nu \mu}\right\|^2_{\bfD} \\
    \;\;+ (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu)\hspace*{-3mm}\\
    \qquad + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS) + \nu(r - \mu^\top \mu),\\
    \qquad\qquad \text{if }\nu \geq 0 \text{ and } \bfD \succeq 0 \\
    -\infty\qquad\; \text{otherwise,}
    \end{cases}\label{eq:dualfunction}
\end{equation}
where $\bfD = \bmat{ (1 - (1 - \eta)^2)\bfA - \bfI & -\eta\epsilon(1 - \eta)\bfA \\ -\eta\epsilon(1 - \eta)\bfA &  -\epsilon\eta^2 \bfA - \nu \bfI}$ and $\|\bfx\|^2_{\bfD} = \bfx^\top \bfD^{-1} \bfx$.
% and $\bfD_1 = ((1 - \eta)^2 - 1)\bfA + \bfI, \bfD_2 = 2\eta\epsilon(1 - \eta)\bfA, \bfd_3 = 2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb, d_4 = (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS)$.
\end{restatable}
\begin{proof}
    The detailed proof can be found in Appendix~\ref{appendix:mean_estimation}. The proof sketch follows by noting that for a fixed $\lambda$ the dual of the inner constrained maximization problem is a quadratic expression in $\bfzadv, \bftheta$ and a finite supremum exists if the Hessian is negative semidefinite (which leads to the $\mathbf{D} \succeq 0$ constraint). Plugging in the maximizer we get a minimization problem in the dual variables $\nu$. Finally we note that the overall minimization problem is jointly convex in $\bfA, \bfb, \nu$.
\end{proof}
\begin{remark}
The problem above
%~\eqref{eq:dualfunction}
is a convex problem since it has a matrix fractional objective function \cite{boyd2004convex}
% \maryam{(add citation, can cite chapter of boyd's book)}\avi{Sounds good. Do we want citation in the theorem statement?} \maryam{after the theorem.}
with a Linear Matrix Inequality (LMI) constraint.
\end{remark}
% \end{theorem}
% \textbf{Note:} The closed form of the expression in the if case will have a pseudo-inverse of $\nu$. Can we still feed the expression below to cvxpy?
% \subsection{Defense for a single data distribution}
% \paragraph{Defense for a single data distribution}
% % \textbf{Parameters for robust learning algorithm:}
% An upper bound on the objective in Eq. \eqref{eq:mean_certificate} for a single given $\Pdata$ can thus be written as the following minimization problem:
% \begin{align}
%     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \bfA \in \mathbb{S}^d, \bfb \in \R^d \\ \nu \leq 0}}\qquad &\eta^2 \Trace(\bfS)  + \kappa  g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma) \label{eq:mean_defense}
% \end{align}
% \begin{align}
%     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \bfA \in \mathbb{S}^d, \bfb \in \R^d \\ \nu \leq 0}}\qquad &\eta^2 \Trace(\bfS)  + \kappa (\frac{1}{4} \bmat{\bfd_3 \\ \epsilon \eta \bfb}^\top \br{- \bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \epsilon\eta^2 \bfA + \nu \bfI}}^{-1}\bmat{\bfd_3 \\ \epsilon \eta \bfb}  +  d_4 - \nu c) \label{eq:mean_estimation} \\
%     \text{s.t.} \quad  &\bmat{ \bfD_1 & \bfD_2 / 2 \\ \bfD_2 / 2 &  \epsilon\eta^2 \bfA + \nu \bfI} \preceq 0.
% \end{align}
% \subsection{Meta-Learning Algorithm}

\paragraph{Meta-Learning Algorithm.}
% \textbf{Parameters for meta robust learning algorithm (expected loss):}
Following the formulation in Eq.~\eqref{eq:main_formulation}, we wish to learn a defense parameter $\bfS$ that minimizes the expected loss (expectation over different $\Pdata$ from the meta distribution $\mathcal{P}$). For the mean estimation problem this boils down to solving
\begin{align}
    \inf_{\substack{\bfS \in \mathbb{S}^d_+ }} &\eta^2 \Trace(\bfS)  + \kappa \mathbb{E}_{\mu, \bfSigma \sim \mathcal{P}} [ \!\!\!\! \inf_{\substack{\nu \geq 0 \\ \bfA \in \mathbb{S}^d, \bfb \in \R^d}} \!\!\!\! g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma)].\label{eq:mean_meta_defense}
\end{align}
\vspace{-.05in}
\begin{remark}
    Note that problem \eqref{eq:mean_meta_defense} is not jointly convex in $\bfS,\bfA,\bfb,\nu$ because of the $\Trace(\bfA\bfS)$ term in $g$; see \eqref{eq:dualfunction}. However, it is convex individually in $\bfS$ and $\{\bfA,\bfb,\nu\}$. We use an alternating minimization approach to seek a local minimum of problem~\eqref{eq:mean_meta_defense}, as detailed in Algorithm~\ref{alg:meta_mean}.
\end{remark}
% \LL{maybe mention that \eqref{eq:mean_meta_defense} is not quite jointly convex in $\bfS,\bfA,\bfb,\nu$ because of the $\Trace(\bfA\bfS)$ term?}\avi{Note that Eq.~\eqref{eq:mean_meta_defense} is not jointly convex in $\bfA, \bfS$ because of the $\Trace(\bfA\bfS)$ term. We use an alternating minimization procedure to %solve the
% find the certificate for a fixed $\bfS$, and then minimize Eq~\eqref{eq:mean_meta_defense} over $\bfS$ keeping $\bfA, \bfb, \nu$ fixed at the solutions of the certificate.} \maryam{let's state clearly that this is not optimal, since the problem is not convex and doesn't have any other structure we can use to prove anything about alternating minimization.}\avi{Not sure how adding this improves the paper? We can simply say that the trace causes non-convexity and we adapt alternating minimization to get to a local optima.}

In practice, one observes a finite number of distributions from $\mathcal{P}$, and sample average approximation is leveraged, with the aim of learning a defense parameter which generalizes well to unseen distributions from $\mathcal{P}$. This process is stated in Algorithm~\ref{alg:meta_mean}.
% \maryam{in algorithm 1 line 4, need to either a fixed number of steps or a stopping criterion.}
% \maryam{also the optimization problem in line 8 needs  indices $t$ and $t+1$, right now $t$ doesn't appear anywhere...}
% \textbf{Parameters for meta robust learning algorithm (worst case loss):}
% Let's first write down the uncertainty sets for the mean and covariance of $\Pdata$ :
% \begin{align*}
%     \mathcal{C} = \{\alpha \bfI \preceq \bfSigma \preceq \beta \bfI, \mu^\top\mu \leq c'\}.
% \end{align*}
% Note that we assume the means to be 0-centered without loss of generality.
% The worst case certificate will thus be given by:
% \begin{align*}
%     \sup_{\mu, \bfSigma \in \mathcal{C} }
%     \inf_{\substack{\bfA \in \mathbb{S}^d, \bfb \in R^d \\ \nu \leq 0}} g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma).
% \end{align*}

% Note that the only dependence on $\bfSigma$ in $g()$ is $\Trace(\bfSigma \bfA)$ which is affine (and thus concave in $\bfSigma$). However $g()$ is a quadratic in $\mu$, and concavity can be ascertained if the hessian w.r.t. $\mu$ is negative definite, which imposes additional constraints on $\bfA, \bfb, \nu$. A special case would be if we assume there is no uncertainty in $\mu$, then the $\sup$ and $\inf$ can be switched by Von Neumann-Fan minimax theorem. However seems too restrictive to plan for $\bfS$ in a mean estimation problem with complete knowledge of $\mu$.

% \textbf{Assuming no uncertainty in mean:} We can use Von Neumann-Fan minimax theorem now. Since $\bfA$ is symmetric, we can write it as $\bfA = \bfQ\bfD\bfA^\top$ where $\bfD$ is the eigenvalue matrix are $\bfQ$ is set of orthonormal eigenvectors. Thus $\Trace(\bfA \bfSigma) = \Trace(\bfD \bfQ^\top \bfSigma \bfQ)$. Define $\bfT = \bfQ^\top \bfSigma \bfQ$, note that the constraints on $\bfSigma$ imply $\alpha \bfI \preceq \bfT \preceq \beta \bfI$. Let $\lambda_i(\bfA)$ denote the $i^{\rm th}$ largest eigenvalue of $\bfA$. We can write:
% \begin{align}
%     \sup_{\bfSigma \in \mathcal{C}} \Trace(\bfA \bfSigma) = \beta \sum_{\lambda_i(\bfA) \geq 0} \lambda_i(\bfA) - \alpha \sum_{\lambda_i(\bfA) \leq 0} \lambda_i(\bfA).
% \end{align}
% Define $\bfD_1 = ((1 - \eta)^2 - 1)\bfA + \bfI, \bfD_2 = 2\eta\epsilon(1 - \eta)\bfA, \bfd_3 = 2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb, d_4 = (1-\epsilon)(\eta^2 (\beta \sum_{\lambda_i(\bfA) \geq 0} \lambda_i(\bfA) - \alpha \sum_{\lambda_i(\bfA) \leq 0} \lambda_i(\bfA)) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS)$.
% Thus the worst case certificate for a fixed learning algorithm is given by:
% \begin{align*}
%     \inf_{\substack{\bfA \in \mathbb{S}^d, \bfb \in R^d \\ \nu \leq 0}} g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma),
% \end{align*}
% where $g()$ is as defined in Eq.\eqref{eq:dualfunction} with the new definition of $d_4$ which captures the worst case of $\bfSigma$.
% \begin{align}
%     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \bfA \in \mathbb{S}^d, \bfb \in R^d \\ \nu \leq 0}} \sup_{\mu, \bfSigma \in \mathcal{C}} \qquad &\eta^2 \Trace(\bfS)  + \kappa  g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma).
% \end{align}
        % \djcomment{Its a bit confusing to see $\bftheta, \bfzadv$ in this optimization problem. I suspect you can enforce strict negative definiteness of the first matrix and ensure there is a unique optimum, and then plug in the unique solution to eliminate these. I don't think that would change the dual optimum.}
% Since $\bftheta$ is unconstrained, we wish to analytically derive the $\bftheta$ that maximizes the bound in terms of $\bfzadv$. Define $\bfX = 2\bfD_1, \bfy = -\bfd_3 -  \bfD_2 \bfzadv$. For a given $\bfzadv$, the value of $\bftheta$ maximizing the objective is given by the solution to the equation:
% \begin{align*}
%     \bfX \bftheta = \bfy.
% \end{align*}
% Define $ \bfc_1 = -\bfX^\dagger \bfd_3, \bfC_2 = \bfX^\dagger \bfD_
% 2$. Then $\bftheta = \bfc_1 - \bfC_2 \bfzadv$.


% Define $\bfE_1 = \bfC_2^\top \bfD_1 \bfC_2 + \epsilon\eta^2 \bfA - \bfC_2^\top \bfD_2, \bfe_2 = -2\bfC_2^\top\bfD_1\bfc_1 + \bfD_2^\top\bfc_1 - \bfC_2^\top \bfd_3 + \epsilon \eta \bfb, e_3 = d_4 + \bfc_1^\top \bfD_1 \bfc_1 + \bfc_1^\top \bfd_3$. Substituting the value of $\bftheta$ into the bound gives us:
% \begin{align}
%     \sup_{\bfzadv \in \mathcal{A}} \bfzadv^\top \bfE_1 \bfzadv + \bfzadv^\top \bfe_2 + e_3. \label{eq:mean_adv_cert}
% \end{align}
% If $\bfzadv$ is constrained to be from within the typical set of a Gaussian, then $\mathcal{A}$ is defined by quadratic constraints and hence, the optimization problem \eqref{eq:main_formulation} can is a QCQP.
% Let $\bfzadv(\bfS, \lambda, \Pdata)$ denote the \textit{value}
% % \djcomment{why italicize?}\avi{We can't pass gradients for $\lambda, \bfS$ through $\bfzadv(\bfS, \lambda, \Pdata)$. Italicized to emphasize we work with the value and not the functional form, thus needing the alternating optimization steps. Maybe suggest better notation to avoid confusion for readers?}
% of the optimal $\bfzadv$ for the bound on the adversarial objective for a given $\bfS, \lambda$ and a data distribution $\Pdata$.

% Plugging it into the \eqref{eq:main_formulation} gives us:
% \begin{align}
%     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \lambda: \R^d \mapsto \R}}\qquad \ExP{\Pdata \sim \mathcal{P}}{\eta^2 \Trace(\bfS)  + \kappa\br{\bfzadv(\bfS, \lambda, \Pdata)^\top \bfE_1 \bfzadv(\bfS, \lambda, \Pdata) + \bfzadv(\bfS, \lambda, \Pdata)^\top \bfe_2 + e_3} }\label{eq:mean_estimation}.
% \end{align}
% \djcomment{It might be cleaner to write the optimal value of the adversarial loss as $o\br{\bfS, \lambda, \Pdata}$ and then sample }

% \textbf{Algorithm for a Meta Learning Setup:} One can use alternating optimization to (i) update $\bfS, \lambda$, (ii) solve for $\bfzadv(\bfS, \lambda, \Pdata)$ and repeat till converegence.
% \djcomment{In terms of organizing this section, might be best to have 2 subsections - one focused on the certificate for a fixed $\Pdata$ and the other focused on the meta learning scenario}

% In a meta learning setting, one observes $K$ learners $\Pdata_1, \ldots, \Pdata_K \sim \mathcal{P}$. And the outermost expectation becomes a sample average approximation over which the one wishes to learn the optimal noise parameter $\bfS$.\djcomment{Do you mean $K$ data distributions?}\djcomment{I think indexing on a specific set of $K$ data distributions and the sample average scenario may be too specific. In practice, we may just see a stream of data distributions sampled from some underlying source, and just be able to do one pass over it. So I would prefer to state this in the abstract in terms of an expectation over $\mathcal{P}$, and then in the algorithms section we can discuss various options for the outer minimization over $\bfS$ or other tunable learning algorithms - single sample, minibatch etc.}
% % \begin{align*}
% %      &\epsilon \lambda\br{L\br{\bftheta, \bfzadv}} + (1-\epsilon) \ExP{\bfz \sim \Pdata}{\lambda\br{L\br{\bftheta, \bfz}}}-\lambda\br{\bftheta} + \ell_{\textup{adv}}\br{\bftheta}\\
% %      &= \bmat{\theta \\ \bfzadv}^\top \bmat{[(1 - \eta)^2 - 1] A + I & \epsilon \eta (1 - \eta) A \\ \epsilon \eta (1 - \eta) A & \epsilon \eta^2 A} \bmat{\theta \\ \bfzadv} + \bmat{\theta \\ \bfzadv}^\top \bmat{-\eta b + (1 - \epsilon) \eta (1 - \eta) [A\mu + A^\top \mu] - 2\mu \\ \epsilon \eta b} \\ +& (1-\epsilon)(\eta^2 \Trace(\Sigma A) + \eta^2 \mu^\top A \mu + \eta b^\top \mu) + \mu^\top \mu
% % \end{align*}

% \textbf{Scheme for designing an optimal attack with known learning scheme, mean, covariance of the Gaussian random variable:} Given a fixed noise covariance $S$, and a fixed quadratic $\lambda$ then the optimal $\bfzadv$ can be computed using by solving the QCQP for the inner supremum. The optimal attack under the tunable parameters can be found by alternative optimization of (i) the learner's parameter $S$, the dual variables $\lambda$, (ii) the QCQP solution for $\bfzadv$.

% \djcomment{Is it an arbitrary QCQP or a specific one like a trust region problem? If the latter we should say that since the trust region problem is polytime solvable while arbitrary QCQPs are NP-hard.}\avi{Look at Eq. \eqref{eq:mean_adv_cert}. For this case trust region should be applicable, right?}\djcomment{Depends on the specific nature of $\mathcal{A}$, but for a $\ell_2$ ball constraint this should be good.}

% \djcomment{A bit confused here: Do you mean designing an optimal attack for a fixed learner or a designing a robust learning algorithm under optimal attack?}\avi{I had written this pre-discussion with you. Have a look at the whole 2.1 and see if now it is clear? We can discard this paragraph.}

% \djcomment{Adding an algorithm box would be helpful in any case.}\avi{Sure!}


% \djcomment{Can you add a toy experiments demonstrating: 1. An optimal attack against the fixed learning algorithm and how that depends on design parameters and the data distribution (say for a 1d Gaussian estimation problem?)2. Experiments comparing a robust learning algorithm with a standard one?}\avi{Sounds good. For (1), visualizing the $\bfzadv$ against a fixed Gaussian, and varying the noise parameter $\bfS$? For (2) we just compare the case when $\bfS$ is 0, vs when its the solution of EQ. \eqref{eq:mean_estimation}?}\djcomment{Sounds good to me.}

% \end{subequations}
% \avi{Even in the simple case where we have only one $\Pdata$, the term $d_4$ in the objective contains $\Trace(\bfA \bfS)$ which is non-convex jointly in variables $\bfA, \bfS$. Weak convex upper bound I used was $c \leq d\|\bfA\|_2\|\bfS\|_2 \leq d (\|\bfA\|_2^2 + \|\bfS\|_2^2) / 2$. Edit: $\Trace(\bfA \bfS) \leq \|\bfA\|_F \|\bfS\|_F \leq (\|\bfA\|_F^2 + \|\bfS\|_F^2) / 2$ might be a better bound.}

% \avi{\textbf{Uncertainty set for covariance of $\Pdata$: $\alpha \bfI \preceq \bfSigma \preceq \beta \bfI $}}

% \avi{\textbf{Uncertainty set for mean of $\Pdata$:} $\mu^\top\mu \leq c'$. }

% \avi{If the exact form of the known mean, covariance case with single $\Pdata$ were convex, the adversarial certificate robust formulation also stays convex. However, right now I am working with a convex relaxation (which might be very loose). }

\begin{algorithm}[t]
    \caption{Meta learning a robust learning algorithm for mean estimation}
    \begin{algorithmic}[1]
        \STATE \textbf{Input: } Set of $K$ distributions $\{\mathcal{N}(\mu_i, \bfSigma_i)\}_{i \in [K]}$ sampled from $\mathcal{P}[\Pdata]$, tradeoff parameter $\kappa$, and max iterations $T$.
        \STATE \textbf{Initialize:} $\bfS^{(1)} \in \mathbb{S}^d_+$ randomly.
        % \maryam{name this $S_0$ or $S_1$, add proper indices to $S$}
        \STATE \textit{Alternating Minimization over Lagrange %parameters
        multipliers $\{\bfA_i, \bfb_i, \nu_i\}_{i \in [K]}$ and defense parameter $\bfS$.}
        \FOR{$t=1, \ldots, T$}
        \FOR{$i=1,\ldots, K$}
        \STATE $\bfA_i, \bfb_i, \nu_i = \underset{\bfA \in \mathbb{S}^d, \bfb \in \R^d, \nu \geq 0}{\argmin} g(\bfA, \bfb, \nu, \bfS^{(t)}, \mu_i, \bfSigma_i)$
         \ENDFOR
        \STATE $\bfS^{(t+1)} = \underset{\bfS \in \mathbb{S}^d_+}{\argmin}\bigl(\frac{\kappa}{K} \sum_{i \in [K]} g(\bfA_i, \bfb_i, \nu_i, \bfS, \mu_i, \bfSigma_i)$ \\[-3mm]
        \hspace{5cm}$+  \eta^2 \Trace(\bfS)\bigr)$
        % \IF{$\bfS^{(t+1)} = \bfS^{(t)}$}
        % \STATE break
        % \ENDIF
        \ENDFOR
        \RETURN $\bfS^{(T+1)}$
    \end{algorithmic}
    \label{alg:meta_mean}
\end{algorithm}
% \vspace{-5mm}




\section{BINARY CLASSIFICATION}\label{sec:binary_classification}
% We consider the binary classification problem. Given an input feature $\bfx \in [-1, 1]^d$, a linear predictor $\bftheta \in  \R^d$ tries to predict the label $y \in \{-1, 1\}$ via the sign of $\bftheta^\top \bfx$. To define the losses, we introduce $\bfz = y \bfx \in [-1,1]^d$. We consider the regularized hinge loss:
We consider the binary classification problem. Given an input feature $\bfx \in \R^d$ such that $\|\bfx\|_2 \leq 1$, a linear predictor $\bftheta \in  \R^d$ tries to predict the label $y \in \{-1, 1\}$ via the sign of $\bftheta^\top \bfx$ . To define the losses, we introduce $\bfz = y \bfx \in \R^d$ which is the label multiplied by the feature and note that $\|\bfz\|_2 \leq 1$.

A dynamic adversary tries to corrupt samples so that the learning algorithm learns a $\bftheta$ that maximizes the hinge loss on a target distribution $\Ptarget$, captured by the following objective:
\begin{align}\label{eq:adv_classification}
    \ell_{\textrm{adv}}(\bftheta) = \ExP{z \sim \Ptarget}{\max \{0, 1 - \bftheta^\top \bfz\}}.
\end{align}
% \avi{Very easy to also do it for the case of 0-1 loss, $\ell_{\textrm{adv}}(\bftheta) = \ExP{z \sim \Pdata}{\mathbb{I}[- \bftheta^\top \bfz \geq 0]}$. This is just a measure of classification accuracy.}
% \begin{align}
%     h(x) = \mathbb{I}[x \leq 1]
% \end{align}
% \textbf{Logistic Loss:}
% \begin{align}
%     l(\bftheta, \bfz) = -\log(1 + \exp{(-\bftheta^\top \bfz)})
% \end{align}
% \begin{align}
%     h(x) =  -\frac{1}{\exp(x) + 1}
% \end{align}
% \begin{align}
%     F(\bftheta, \bfz) = \bftheta - \eta \frac{\bfz}{\exp{(\bftheta^\top \bfz)} + 1}.
% \end{align}
%
The learning algorithm tires to minimize the regularized hinge loss on the observed datapoints:
\begin{align}\label{eq:hinge}
    l(\bftheta, \bfz) &= \max \{0, 1 - \bftheta^\top \bfz\} + \frac{\sigma}{2}\norm{\bftheta}_2^2.
\end{align}
% Upon observing a sample $\bfz_t$, the parameter is updated via a gradient descent $\bftheta_{t+1} = \bftheta_{t} - \eta \nabla_{\bftheta} l(\bftheta_t, \bfz_t) = F(\bftheta_t, \bfz_t)$ where $F(\bftheta, \bfz)$ is defined as:
% \begin{align}
%     F(\bftheta, \bfz) = (1 - \sigma \eta)\bftheta + \eta \mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz.
% \end{align}
Upon observing a sample $\bfz_t$, the parameter is updated via a gradient descent: $\bftheta_{t+1} = F(\bftheta_t, \bfz_t)$, where \begin{align}
    F(\bftheta, \bfz) &= \bftheta_{t} - \eta \nabla_{\bftheta} l(\bftheta_t, \bfz_t) \notag \\
    &= (1 - \sigma \eta)\bftheta + \eta \mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz. \label{eq:hinge_online_learning}
\end{align}
Below, we provide a certificate for the adversarial objective at stationarity of this learning algorithm.
% \avi{How to specify $\mathcal{A}$? I know we discussed a likelihood model with assumption that features (here label multiplied by feature) will yield a $\ell_2$ norm ball. However, typically constraint sets are perturbations of individual data points. Is there any way of making it more consitent?}
% \djcomment{I think norm balls are a fine starting point modeling the assumption that features are normalized to within a unit ball before the algorithm is run, but we make no additional assumptions. It would be good though to look through the literature and see if anything else has been done, in particularly in \citep{sosnin2024certified}}
\begin{restatable}{theorem}{classificationhingecertificate}
\label{thm:classificationhingecertificate}
Choosing $\lambda : \R^d \rightarrow \R$ in Theorem~\ref{thm:certificate} to be quadratic, i.e., $\lambda\br{\bftheta} = \bftheta^\top \bfA\bftheta + \bfb^\top \bftheta$, parameter space $\Thetaspace = \{ \bftheta \mid \|\bftheta\|_2 \leq \frac{1}{\sigma}\}$ and the adversarial constraint set of the form $\mathcal{A} = \{ \bfzadv \mid \|\bfzadv\|_{2    }
\leq 1\}$, the certificate for the binary classification problem for $\Pdata(\bfz) = \{\bfz_1, \ldots, \bfz_N\}$ for a learning algorithm with regularization parameter $\sigma$, and learning rate $\eta$ is upper bounded by
\begin{align*}
    &\max \left(\OPT_1, \OPT_2 \right)\\[3mm]
\OPT_1 = \inf_{\nu, \bfA, \bfb} \quad& \|\bfp(\bfb, \nu)\|^2_{\bfD(\bfA, \nu)^{-1}} + q(\nu)\\
\textup{s.t.}  \quad& \bfD(\bfA, \nu) \succeq 0,\\
&\bfr_i(\nu) + \bfs(z_i, \bfA, \bfb) = 0, \;\; \forall i \in [N].\\[3mm]
\OPT_2 = \inf_{\nu, \bfA, \bfb} \quad& \|\bfp'(\bfb, \nu)\|^2_{\bfD'(\bfA, \nu)^{-1}} + q'(\nu)\\
\textup{s.t.}  \quad&  \bfD'(\bfA, \nu) \succeq 0,\\
      &\bfr_i(\nu) + \bfs(z_i, \bfA, \bfb) = 0, \;\; \forall i \in [N].
\end{align*}
% \begin{align*}
%     &\max{\left(\inf_{\nu, \bfA, \bfb} \|\bfp(\bfb, \nu)\|^2_{\bfD(\bfA, \nu)^{-1}} + q(\nu), \inf_{\nu, \bfA, \bfb} \|\bfp'(\bfb, \nu)\|^2_{\bfD'(\bfA)^{-1}} + q'(\nu)\right)} \\
%     &\text{such that} \\
%     & \bfr_i(\nu) + \bfs(z_i, \bfA, \bfb) = 0 \; \forall i \in [N],\\
%     & \bfD(\bfA, \nu) \succeq 0, \bfD'(\bfA) \succeq 0.
% \end{align*}

where $\bfp()$, $\bfp'()$, $q()$, $q'()$, $\bfD()$, $\bfD'()$, $\bfr_1(),\ldots,\bfr_N()$, $\bfs()$ are affine functions of the optimization variables $\nu, \bfA, \bfb$ as defined below and $\nu = \{\nu_1, \ldots, \nu_{10}\}$:
% \begin{align*}
%     &\bfp(\bfb, \nu) = \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) - \nu_{10} + \nu_{11}\\ \epsilon \eta \bfb + 2\nu_{12} \mu + \nu_{13} - \nu_{14}} \in \R^{2d},\\
%     &\bfp'(\bfb, \nu) = \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) - \nu_{10} + \nu_{11}} \in \R^{d},\\
%     &\bfD(\bfA, \nu) = \bmat{[1 - (1 - \sigma \eta)^2] \bfA & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 & -\epsilon \eta^2 \bfA + \nu_{12} \bfI} \in \R^{2d \times 2d},\\
%     &\bfD'(\bfA) = [1 - (1 - \sigma \eta)^2] \bfA \in \R^{d \times d},\\
%     &q(\nu) = -\nu_1^\top \mathbf{1} + (2 + \frac{d}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \nu_{8}^\top \mathbf{1} + 2 \nu_9 + \frac{(\nu_{11} - \nu_{10})^\top \mathbf{1}}{\sigma} +  \nu_{12} (\delta_1 - \mu^\top\mu) + (\nu_{14} - \nu_{13})^\top \mathbf{1} \in \R, \\
%     &q'(\nu) = -\nu_1^\top \mathbf{1} + (2 + \frac{d}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \nu_{8}^\top \mathbf{1} + \frac{(\nu_{11} - \nu_{10})^\top \mathbf{1}}{\sigma} \in \R, \\
%     &\bfr_i(\nu) = \bmat{(1 + \frac{d}{\sigma}) \nu_{1i} \\ \mathbf{0}} + \bmat{-(1 + \frac{d}{\sigma}) \nu_{2i} \\ 0} + \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} + \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} +  \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} +
%       \\& \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} + \bmat{\nu_7 - \nu_8 \\ \mathbf{0}} \in \R^{d + 1},\\
%       &\bfs(\bfz_i, \bfA, \bfb) = \frac{1}{N} \bmat{\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right)\\\left( 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i\right)} \in \R^{d + 1}.
% \end{align*}
\footnotesize
% \begin{align*}
%     \bfp(\bfb, \nu) &= \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) \\ \epsilon \eta \bfb} \in \R^{2d},\\
%     \bfD(\bfA, \nu) &= \addtolength{\arraycolsep}{-1mm}\bmat{[1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 \bfI\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 \bfI & -\epsilon \eta^2 \bfA + \nu_{10}\bfI} \in \mathbb{S}_{+}^{2d},\\
%     % {2d \times 2d}\\
%     q(\nu) &= q'(\nu) + 2 \nu_9 + \nu_{10} \in \R, \\
%     \bfp'(\bfb, \nu) &= \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right)} \in \R^{d},\\
%     \bfD'(\bfA, \nu) &= [1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI  \in \mathbb{S}_{+}^{d},\\
%     % {2d \times 2d}\\
%     q'(\nu) &=-\nu_1^\top \mathbf{1} + (2    + \tfrac{1}{\sigma}) \nu_2^\top \mathbf{1} + \tfrac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \mathbf{1}^\top \nu_7  + \frac{\nu_{8}}{\sigma^2} \in \R, \\
%     \bfr_i(\nu) &= \bmat{(1 + \frac{1}{\sigma}) (\nu_{1i} - \nu_{2i})  - \frac{\mathbf{1}^\top(\nu_{4i} + \nu_{6i})}{\sigma} - \nu_{7i}\\ \nu_{3i} + \nu_{4i} - \nu_{5i} - \nu_{6i}} \in \R^{d + 1},\\
%     \bfs(\bfz_i, \bfA, \bfb) &= \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \in \R^{d + 1}.
% \end{align*}
\begin{align*}
    \bfp(\bfb, \nu) &= \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) \\ \epsilon \eta \bfb},\\
    \bfD(\bfA, \nu) &= \addtolength{\arraycolsep}{-1mm}\bmat{[1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 \bfI\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 \bfI & -\epsilon \eta^2 \bfA + \nu_{10}\bfI},\\
    % {2d \times 2d}\\
    q(\nu) &= q'(\nu) + 2 \nu_9 + \nu_{10}, \\
    \bfp'(\bfb, \nu) &= \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right)},\\
    \bfD'(\bfA, \nu) &= [1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI,\\
    % {2d \times 2d}\\
    q'(\nu) &=-\nu_1^\top \mathbf{1} + (2    + \tfrac{1}{\sigma}) \nu_2^\top \mathbf{1} + \tfrac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \mathbf{1}^\top \nu_7  + \frac{\nu_{8}}{\sigma^2}, \\
    \bfr_i(\nu) &= \bmat{(1 + \frac{1}{\sigma}) (\nu_{1i} - \nu_{2i})  - \frac{\mathbf{1}^\top(\nu_{4i} + \nu_{6i})}{\sigma} - \nu_{7i}\\ \nu_{3i} + \nu_{4i} - \nu_{5i} - \nu_{6i}},\\
    \bfs(\bfz_i, \bfA, \bfb) &= \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i}.
\end{align*}
\end{restatable}
\begin{proof}
A detailed derivation can be found in the Appendix~\ref{appendix:binary_classification}. The proof steps are: (i) Regularization implicitly bounds the decision variable $\bftheta \in \Thetaspace$ without the need for projections, (ii) Considering 2 cases for the indicator term of $\bfzadv$ and taking the maximum of these 2 cases, (iii) Relaxing the indicator terms for benign samples into continuous variables between $[0, 1]$, and (iv) Using McCormick relaxations \citep{mitsos2009mccormick} to convexify the bilinear terms in the objective.
\end{proof}
% \paragraph{Robust Learning.} Unlike the problem of mean estimation, the certificate is not convex in the tunable hyper-parameters $\nu, \sigma$. Hence approaches gradient descent or grid search can be used to effectively tune the defense parameters. We carry extensive experiments in Section~\ref{sec:experiments} to verify the efficacy on real datasets.
\textbf{Extension to Multi-Class Setting:} Our analysis can be extended to the multi-class classification setting. Let us consider score based classifiers, where $\Theta = \{\theta_1, \ldots, \theta_K\} \in \mathbb{R}^d$ are the learnable parameters and the class prediction for a feature $x \in \mathbb{R}^d$ is given by $\arg\max_{i \in [K]}\theta_i^\top x$.

The SVM loss for any arbitrary feature $x$ with label $y$ is defined as:
$\ell(\Theta, (x, y)) = \sum_{j \neq y} \max\{0, 1 + (\theta_j - \theta_y)^\top x\}$

The gradient update takes the form:
$F(\theta_y, (x, y)) = \theta_y + \eta \sum_{j \neq y} \mathbb{I}[1 + (\theta_j - \theta_y)^\top x > 0] x$ and for all $j \neq y$, we have $F(\theta_j, (x, y)) = \theta_j - \eta \mathbb{I}[1 + (\theta_j - \theta_y)^\top x > 0] x$.

Note that the non-linearity in both the loss function and the update is composed with a linear combination of the parameters (i.e. $\theta_j - \theta_y$), and thus the analysis in the proof of Theorem~\ref{thm:classificationhingecertificate} still holds, and our analysis for the binary classification generalizes to the multi-class classification.

\section{EXPERIMENTS}\label{sec:experiments}
We conduct experiments on both synthetic and real data to empirically validate our theoretical tools.
\begin{figure}[h]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/test_performance_lr_d_20.png} % First image file
        % \caption{$d=2$}
        \label{fig:image1}
    \end{minipage}
    \vskip -0.2in
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/test_performance_d_20.png} % Second image file
        % \caption{$d=3$}
        \label{$d=3$}
    \end{minipage}
    \vskip -0.2in
    \caption{Test performance (mean squared error between true and estimated means) upon varying the learning rates (above) and the the fraction of samples corrupted by the dynamic adversary (below) and observed that our defense significantly outperforms training without defense.}
    \vskip -0.2in
    \label{fig:mean_estimation}
\end{figure}
% \begin{figure*}[h!]
%     \centering
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=1.1\textwidth]{figs/attack_cifar.png} % First image file
%         % \caption{$d=2$}
%         \label{fig:image1}
%     \end{minipage}
%     % \vskip -0.15in
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=1.1\textwidth]{figs/attack_fashion.png} % Second image file
%         % \caption{$d=3$}
%         \label{$d=3$}
%     \end{minipage}
%     % \vskip -0.15in
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=1.1\textwidth]{figs/attack_mnist_1_vs_7.png} % Second image file
%         % \caption{$d=3$}
%         \label{$d=3$}
%     \end{minipage}
%     \vskip -0.15in
%     \caption{We plot the certificates of robustness for various settings (hyperparameter values) which act as upper bounds on the optimal dynamic adversary's objective. We also plot the test losses on the adversarial objective  for various attacks which act as lower bounds on the objective of the optimal adversary. }
%     \vskip -0.2in
%     \label{fig:classification}
% \end{figure*}
\begin{figure*}[h!]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{figs/attack_cats_vs_dogs_clean.png} % First image file
        % \caption{$d=2$}
        \label{fig:image1}
    \end{minipage}
    % \vskip -0.15in
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{figs/attack_fashion_clean.png} % Second image file
        % \caption{$d=3$}
        \label{$d=3$}
    \end{minipage}
    % \vskip -0.15in
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{figs/attack_1_vs_7_clean.png} % Second image file
        % \caption{$d=3$}
        \label{$d=3$}
    \end{minipage}
    \vskip -0.15in
    \caption{We plot the certificates of robustness for various settings (hyperparameter values) which act as upper bounds on the optimal dynamic adversary's objective. We also plot the test losses on the adversarial objective  for various attacks which act as lower bounds on the objective of the optimal adversary. }
    \vskip -0.2in
    \label{fig:classification}
\end{figure*}
% \begin{figure}[h!]
%     \centering
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{figs/attack_help.png} % First image file
%         % \caption{$d=2$}
%         \label{fig:image1}
%     \end{minipage}
%     \vskip -0.15in
%     \begin{minipage}{0.32\textwidth}
%         \centering
%         \includegraphics[width= \textwidth]{figs/attack_correct.png} % Second image file
%         % \caption{$d=3$}
%         \label{$d=3$}
%     \end{minipage}
%     \vskip -0.15in
%     \caption{Poor choice of hyperparameters of the learning algorithm can make them vulnerable to dynamic attackers as noted by our certificates and attacks (red plots). Lower values of certificate, indicate more robust learning algorithms (blue, orange, green plots).}\label{fig:llm}
%     \vskip -0.35in
% \end{figure}
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/attack_help_clean.png} % First image file
        % \caption{$d=2$}
        \label{fig:image1}
    \end{minipage}
    \vskip -0.15in
    \begin{minipage}{0.32\textwidth}
        \centering
        \includegraphics[width= \textwidth]{figs/attack_correct_clean.png} % Second image file
        % \caption{$d=3$}
        \label{$d=3$}
    \end{minipage}
    \vskip -0.15in
    \caption{Poor choice of hyperparameters of the learning algorithm can make them vulnerable to dynamic attackers as noted by our certificates and attacks (red plots). Lower values of certificate, indicate more robust learning algorithms (blue, orange, green plots).}\label{fig:llm}
    \vskip -0.2in
\end{figure}
\subsection{Mean Estimation}
\vskip -0.15in
We wish to evaluate the robustness of our meta learning algorithm in Eq.~\eqref{eq:main_formulation} to design a defense against a dynamic best responding adversary on a ($d=20$) mean estimation task. We consider 3 different learning algorithms: 1. \textbf{No Defense:} Eq.~\eqref{eq:mean_update} with $\bfB=\mathbf{0}$, i.e. no additive Gaussian noise, 2. \textbf{Baseline Defense:} $\bfB$ in Eq.~\eqref{eq:mean_update} is restricted to be Isotropic Gaussian, 3. \textbf{Defense:} $\bfB$ in Eq.~\eqref{eq:mean_update} can be arbitrarily shaped. We use Algorithm~\ref{alg:meta_mean} to compute the defense parameter $\bfS = \bfB\bfB^\top$ for the latter 2 learning algorithms by training on 10 randomly chosen Gaussians drawn from standard Gaussian prior for the mean and standard Inverse Wishart prior for the covariance. We report the average test performance on 50 Gaussian distributions drawn from the same prior (see Figure~\ref{fig:mean_estimation}).

% \maryam{Let's see if we can interpret the $B$ obtained by the algorithm, and also examine it in the numerical experiments, looking at the eigenvectors of $B$.}

% \maryam{add more details about experiments and the baseline.}

\subsection{Image Classification}
\vskip -0.15in
% \avi{Dj, have a look at this.}
% We consider binary classification tasks on multiple datasets : (i) MNIST:1 vs 7 \citep{deng2012mnist}, (ii) FashionMNIST: Sandals vs Ankle Boot \citep{xiao2017fashion}, (iii) CIFAR-10: Cats vs Dogs \citep{krizhevsky2009learning}.
We consider binary classification tasks on multiple datasets : (i) MNIST \citep{deng2012mnist}, (ii) FashionMNIST \citep{xiao2017fashion}, (iii) CIFAR-10\citep{krizhevsky2009learning}. Detailed dataset descriptions and preprocessing steps can be found in Appendix~\ref{appendix:experiment}.

% Our experiments aim to address the key questions:
% \begin{enumerate}
%     \item How does the certificate of robustness vary as we change the values of  (a) $\epsilon$, (b) $\eta$, (c) $\sigma$? What factors make learning algorithms more vulnerable?
%     % \item Study the performance of the learning algorithm in absence of adversary by plotting the test loss upon training on the benign dataset as the following are varied (a) $\eta$, (b) $\sigma$.
%     \item How can we use the setup in \eqref{eq:main_formulation} to trade off performance in the absence of an adversary and the certificate of robustness to choose the best hyperparameters for a robust learning algorithm?
%     \item How does the test performance of our learning algorithm compare on various dynamic adversarial attacks when using the optimal defense obtained from our setup in \eqref{eq:main_formulation} vs.\ a poorly chosen defense?
%     \item Can we empirically demonstrate that our formulation allows us to compute a certificate that provides an upper bound for various types of attacks, showing that our methodology is effective without assuming specific attack types?
% \end{enumerate}

In our experiments, we choose $\Ptarget$ in Eq.~\eqref{eq:adv_classification} to be the same as $\Pdata$, i.e., the adversary's objective is to make the model perform poorly on the benign data distribution.

To learn the hyperparameters for a robust online learning algorithm (Eq.~\eqref{eq:hinge_online_learning}), we adopt the Meta learning setup presented in Eq.~\eqref{eq:main_formulation}. We choose $\mathcal{P}$ as a set of binary classification datasets on label pairs different from the label pair the online algorithm receives data from. For example, we use data from MNIST: 4 vs 9, 5 vs 8, 3 vs 8, 0 vs 6 to construct the objective in Eq.~\eqref{eq:main_formulation} and then test the performance of the onlne learning algorithm with the learnt hyperparameters on MNIST: 1 vs 7 (see Appendix~\ref{appendix:experiment} for more details). We compute certificates for different fractions of the training data to be corrupted $\epsilon \in \{1 \%, 2\%, 3\%, 4\%, 5\%\}$, and vary $\eta, \sigma$ for various values of $\eta$ and $\sigma$ by performing a grid search over $\eta \in \{5\times10^{-5}, 1\times10^{-4}, 5\times10^{-4}, 1\times10^{-3}, 5\times10^{-3}\}$ and $\sigma \in \{3\times10^{-3}, 6\times10^{-3}, 1\times10^{-2}, 3\times10^{-2}, 6\times10^{-2}\}$. We tune the hyperparameter $\kappa$ in Eq.~\eqref{eq:main_formulation} to trade off benign loss and robustness certificate by the generalization test loss on a held-out validation set.
In Figure~\ref{fig:classification} we plot the certificates of robustness corresponding to hyperparameters $(\eta, \sigma)$ pairs having smallest objective values in Eq.~\eqref{eq:main_formulation}.
% and an arbitrarily chosen hyperparameter.

Additionally, we evaluate the learning algorithm against three types of attacks:
1. \textbf{fgsm:} The attacker, having access to model parameters at each time step, initializes a random $\mathbf{z}_{\text{adv}}$, computes the gradient of $\bfzadv$ on the adversarial loss, takes a gradient ascent step on $\bfzadv$ and projects the result onto the unit $\ell_2$ ball, 2. \textbf{pgd:} similar to fgsm, but instead of a single big gradient step, the attacker takes multiple small steps and projects onto the unit $\ell_2$ ball, 3. \textbf{label flip:} the attacker picks an arbitrary data point and flips its label. In Figure~\ref{fig:classification}, we plot the best attack (corresponding to largest prediction error) for each value of $\epsilon$. We observe  the following:
The test adversarial loss under different heuristic attacks is consistently lower than the computed robustness certificate.
% \begin{enumerate}[topsep=-1pt,itemsep=-1ex,partopsep=-1ex,parsep=1ex]
%     \item  The test adversarial loss under different heuristic attacks is consistently lower than the computed robustness certificate.
%     \item Suboptimal choices of regularization and learning rate can significantly increase the vulnerability of learning algorithms, as indicated by both the robustness certificate and the adversarial loss when training with these parameter values (red plots in Figure~\ref{fig:classification}).
%     \vskip -1in
% \end{enumerate}

\subsection{Reward Learning}
\vskip -0.15in
We conduct reward learning experiments using Helpsteer, an open-source helpfulness dataset that is used to align large language models to become more helpful, factually correct and coherent, while being adjustable in terms of the complexity and verbosity of its responses \citep{wang2023helpsteer, dong2023steerlm}. Since the datasets of human feedback, both for open source and closed sourced models, are typically composed of users `in the wild` using the model, there is potential for adversaries to introduce poisoning. This can lead to the learned reward model favoring specific demographic groups, political entities or unscientific points of view, eventually leading to bad and potentially harmful experiences for users of language models fine tuned on the learned reward.\looseness=-1

To apply our framework to this problem, we consider a simple reward model. Given a (prompt, response) pair, we extract a feature representation $\bfx$ using a pretrained BERT model, and our reward model parameterized by $\bftheta$, predicts the reward as $\bftheta^\top \bfx$. Given the score $y$ (normalised to fall within the range $[-1,1]$) on a particular attribute (say helpfulness), the learning algorithm proceeds to learn the reward model by performing gradient descent on the regularized hinge loss objective Eq.~\eqref{eq:hinge}. We follow the similar hyperprameter search space as the image classification example using the meta learning setup in Eq.~\eqref{eq:main_formulation}. The results for helpfulness and correctness are presented in Figure~\ref{fig:llm}. More details are deferred to Appendix~\ref{appendix:experiment}.

% while findings related to other attributes are included in Appendix~.
% Types of plots to be added:
% \begin{enumerate}
%     \item x-axis : vary $\epsilon$, y-axis: value of certificate. Multiple plots for various $\eta, \sigma$ pairs. Also plot the corresponding benign losses. The benign losses should be horizontal lines as they don't depend on $\epsilon$. It would be good to have 2 sets of plots, one with $\sigma$ held fixed, the other with $\eta$ held fixed to study the effect of the other hyperparamter which is varied.
%     \item Try few attacks : greedy (use PGD, FGSM to obtain $\bfzadv_t$), label flip, random additive feature noise, and train them at different hyperparamter settings. Plot the test losses, and show certificate upper bounds the target loss for different attacks. Also demonstrate, no defense / defense with poor choice of $\eta, \sigma$ are vulnerable - resulting in larger test loss. Thereby supporting our methodology useful to create successful defenses.
% \end{enumerate}

\section{CONCLUSION AND FUTURE WORK}
\vskip -0.1in
    This paper presents a novel framework for computing certified bounds on the worst-case impacts of dynamic data poisoning adversaries in machine learning. This framework lays the groundwork for developing robust algorithms, which we demonstrate for mean estimation and linear classifiers. Extending this framework to efficient algorithms for more general learning setups, particularly deep learning setups that use human feedback with potential for malicious feedback, would be a compelling direction of future work. Furthermore, considering alternative strategies for computing the bound in \eqref{eq:cert_general}, particularly ones driven by AI advances, would be a promising approach for scaling. Recent work has demonstrated that AI systems can be used to discover Lyapunov functions for dynamical systems \citep{alfarano2023discovering}, indicating that AI driven approaches could hold promise in this setting.
% \subsection{Special case of linear regression}
% Consider a linear regression problem on a finite number $N$ of examples:
% \[l\br{\bftheta} = \frac{1}{N}\sum_{i=1}^N \ltwo{\inner{\bftheta}{\bfx_i}-\bfy_i}^2= \frac{1}{2} \tran{\bftheta}\bfH \bftheta + \tran{\bfq}\bftheta + \bfc. \]
% % \[f\br{\bftheta} = \mathbb{E}_{(\bfx, \bfy) \sim \Pdata} \ltwo{\inner{\bftheta}{\bfx}-\bfy}^2= \frac{1}{2} \tran{\bftheta}\bfH \bftheta + \tran{\bfq}\bftheta + \textrm{const.}\]
% \djcomment{If we stick with the online learning setting (which is motivates having a dynamic adversary), we should not assume a static dataset that produces a fixed $H, q$ but rather a stream of datapoints of $\br{\bfx_i, \bfy_i}$ pairs. I would prefer to stick to this setting throughout the paper, except maybe in a specific section where we discuss the relationship between the online and batch settings) }\avi{We should discuss this. I was thinking that $\bfH, \bfq$ are random variables constructed via realization of a stream of $N$ $\br{\bfx_i, \bfy_i}$ pairs. In an online learning setting, this could be the batch of $N$ pairs that arrived at time $t$. In this section we are interested at the loss bound at equilibrium, so I'm thinking of $\bfH, \bfq$ as a random batch drawn at equilibrium.}

% The learning rule is given by:
% \[\bftheta^\prime \leftarrow \bftheta - \eta \br{\bfH \bftheta + \bfq + \bfB\bfw}\]
% where $\bfw$ is isotropic Gaussian noise injected by the algorithm.

% We consider a data poisoning adversary that can corrupt one or more datapoints in order to reduce the performance of the learned parameters $\bftheta$, in particular, by increasing the variance of the predictions output by a noisy learning algorithm. Let $\bfH^{\rm adv}, \bfq^{\rm adv}$ denote the perturbed observations by the adversary, and let $\mathcal{A}$ be a constraint on this perturbation introduced by the adversary. The adversary's loss is given by:
% \[l_{\rm adv}(\theta) = \mathbb{E}_{(\bfH, \bfq, \bfc) \sim \Pdata}[\frac{1}{2} \tran{\bftheta}\bfH \bftheta + \tran{\bfq}\bftheta + \bfc].\]

% \avi{Contrasting it to the mean estimation setting, I'm considering a batch of feature score pairs as the samples. Thus the expectation is over these batches (which in the i.i.d. case is essentially expectation of individual pairs). The batch part facilitates assumptions like full-rank $\bfH$ almost surely if $N > d$.}
% \djcomment{Do we need the full rank assumption? It would be nice to have a consistent setting between all problems we consider - either online or batch, primarily for ease of readability for a reader/reviewer.}\avi{I was thinking this might be useful to get a unique mapping from $\bftheta$ to $\bfH^{\rm adv}, \bfq^{\rm adv}$. However if the component of $\bftheta$ orthogonal to the span of $\bfH^{\rm adv}$ doesn't affect the loss for the adversary then the assumption may not be needed.}


% Let $L\br{\bftheta, \bfH, \bfq}= \br{\bfI -\eta \bfH}\bftheta - \eta\bfq$. Define $\bfS = \bfB\bfB^\top$.
% % For compactness, define notation $\bfz = (\bfH, \bfq)$.
% The transition probability of the parameter $\bftheta$ is given by:
% \[\mathbb{P}_{\bfS, \Pdata, \bfH^{\rm adv}, \bfq^{\rm adv}}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|L\br{\bftheta, \bfH^{\rm adv}, \bfq^{\rm adv}}, \eta^2\bfS} + (1-\epsilon)\ExP{(\bfH, \bfq) \sim \Pdata}{\mathcal{N}\br{\bftheta^\prime|L\br{\bftheta, \bfH, \bfq}, \eta^2\bfS}}.\]

% At equilibrium (i.e. when the distribution of $\theta$ is same as $\theta^\prime$), for a given $\bfS, \lambda$ the upper bound on the adversary's loss is given by:
% \begin{subequations}
% \begin{align}
% &\sup_{\substack{\bfH^{\rm adv}, \bfq^{\rm adv} \in \mathcal{A} \\ \bftheta}} \epsilon\ExP{\bftheta^\prime \sim  \mathcal{N}\br{L\br{\bftheta, \bfH^{\rm adv}, \bfq^{\rm adv}}, \eta^2\bfS}}{\lambda\br{\bftheta}}+(1-\epsilon)\ExP{(\bfH, \bfq) \sim \Pdata}{\ExP{\bftheta^\prime \sim \mathcal{N}\br{L\br{\bftheta, \bfH, \bfq}, \eta^2\bfS}}{\lambda\br{\bftheta}}}-\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
% &=\sup_{\substack{\bfH^{\rm adv}, \bfq^{\rm adv} \in \mathcal{A} \\ \bftheta}} \epsilon\br{ \lambda\br{L\br{\bftheta, \bfH^{\rm adv}, \bfq^{\rm adv}}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}} + \nonumber \\& (1-\epsilon)\ExP{(\bfH, \bfq) \sim \Pdata}{ \lambda\br{L\br{\bftheta, \bfH, \bfq}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}} -\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta}
% % &=\eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS} + \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \epsilon \lambda\br{L\br{\bftheta, \bfzadv}} + (1-\epsilon) \ExP{\bfz \sim \Pdata}{\lambda\br{L\br{\bftheta, \bfz}}}-\lambda\br{\bftheta} + \ell_{\textup{adv}}\br{\bftheta}
% \end{align}
% \end{subequations}
% Let $\lambda(\theta) = \theta^\top \bfA \theta + \theta^\top \bfb + \ldots$ be a quadratic function.
% \avi{Expanding the loss, the leading order term will be : $\bftheta^\top \bfH^{\rm adv} \bfA \bfH^{\rm adv} \theta$. }
% \avi{This term is there irrespective of the constraint $\mathcal{A}$?}\djcomment{Agreed, I meant that depending on how the adversary is constrained, the problem may still be tractable. Basically you maximize over $\theta$ for fixed $\bfH^{\rm adv}$ which will give you an SDP type constraint. Now you want this SDP constraint to be robustly satisfied for all $\bfH^{\rm adv}$, and some cases of robust SDPs are known to be tractable (see \citep{el1998robust} or Section 4.1 (the red text) for a similar argument). Beyond those, we would of course need to consider further relaxations.}

\newpage
\newpage
\bibliography{references}
\bibliographystyle{plainnat}
% \section*{Checklist}


%  \begin{enumerate}


%  \item For all models and algorithms presented, check if you include:
%  \begin{enumerate}
%    \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes]
%    \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes]
%    \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes]
%  \end{enumerate}


%  \item For any theoretical claim, check if you include:
%  \begin{enumerate}
%    \item Statements of the full set of assumptions of all theoretical results. [Yes]
%    \item Complete proofs of all theoretical results. [Yes]
%    \item Clear explanations of any assumptions. [Yes]
%  \end{enumerate}


%  \item For all figures and tables that present empirical results, check if you include:
%  \begin{enumerate}
%    \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes]
%    \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes]
%          \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes]
%          \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes]
%  \end{enumerate}

%  \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
%  \begin{enumerate}
%    \item Citations of the creator If your work uses existing assets. [Yes]
%    \item The license information of the assets, if applicable. [Not Applicable]
%    \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
%    \item Information about consent from data providers/curators. [Not Applicable]
%    \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
%  \end{enumerate}

%  \item If you used crowdsourcing or conducted research with human subjects, check if you include:
%  \begin{enumerate}
%    \item The full text of instructions given to participants and screenshots. [Not Applicable]
%    \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
%    \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
%  \end{enumerate}

%  \end{enumerate}
% \bibliographystyle{plain}
% \end{document}
\newpage
\appendix
% \twocolumn[
\onecolumn

\iffalse
\paragraph{Online learning algorithm.} \avi{Is the word hyperparameter appropriate when trying to capture general learning algorithms? What are some other suggestions?}\djcomment{I think it is a good enough word}
We consider online learning algorithms that operate by receiving a new datapoint at each step  and making an update to parameters being estimated. In particular, we consider learning algorithms that can be written as
% \begin{align}\bftheta_{t+1} \gets F\br{\underbrace{\bftheta_t}_{\text{Parameter estimate at time $t$}},  \underbrace{\bfz_t}_{\text{Datapoint received at time $t$}},\underbrace{\bfphi_t}_{\text{Hyperparameters at time $t$
% }}}\label{eq:dyn}\end{align}
\begin{align}\bftheta_{t+1} \gets F\br{\underbrace{\bftheta_t}_{\text{Parameter}},  \underbrace{\bfz_t}_{\text{Datapoint}},\underbrace{\bfphi_t}_{\text{Hyperparameters
}}}\label{eq:dyn}\end{align}
where $F: \Thetaspace \times \Zspace \times \Phispace \to \Thetaspace$ is a function that maps the current parameters $\bftheta_t$ to new parameters $\bftheta_{t+1}$, based on the received datapoint $\bfz_t$ and the hyperparameters $\bfphi_t$, for example, learning rate in a gradient based learning algorithm, or strength of regularization used in the objective function.

\djcomment{Maybe provide a few examples of $F$ here, clarifying what $\theta, \phi, ...$ correspond to?}

We emphasize that $F$ is a general update rule and we do not make any assumptions on $F$.

\djcomment{Commented out what was below, it seemed unnecessarily confusing. I think what would provide more clarity is to just include a few examples of update rules (gradient descent, momentum, maybe even a zero-th order rule).}
%The hyperparameters $\phi_t$ can include a variety of elements, such as the step size in gradient-based algorithms (which may vary over time, as is the case with several popular gradient methods, such as SGD, AdaGrad, Adam etc.), regularization parameters of the model, or even additive Gaussian noise (making $\phi_t$ potentially stochastic). The distribution of hyperparameters at time $t$ is denoted by $\mathbb{P}_t^\phi$. \avi{Is the message that the update rule is very general coming across? Not having the time dependence can make some of the writing much easier -- and in both the examples we have considered, we draw certificates based on only the stationary state.}

% $\phi_t$ indicates model hyperparameters at time $t$ which can denote (but are not limited to) learning rates, regularization parameters, additive gaussian noise.
% The exogenous noise input refers to noise artificially injected into the training algorithm in order to make the algorithm more robust to potential poisoning.
% We further assume that the distribution of $\bfw_t$ is independent of $t$ and each $\bfw_t$ is sampled iid.

\paragraph{Poisoned learning algorithm.}
% We work in a setting where the datapoints received by the learning algorithm may be corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to
% % \textcolor{red}{remember to fix 'up to -> up to' everyhwere}
% that point. Whether or not the poisoned datapoint is picked is chosen probabilistically \textcolor{red}{do we use two probabilities, one for data point getting poisoned and one for it being picked? can we merge these into one?} \avi{They're already the same. Poisoning doesn't mean picking a benign point and modifying it, but rather introducing a corrupted data point that is within a typical set} \textcolor{red}{so then let's just write it in a simpler way with one probability}, and we assume a fixed probability that the poisoned datapoint is chosen over a clean datapoint \textcolor{red}{is this model commonly used? can we cite sources? (for model with separate poisoning and selection)}\avi{DJ?}.

% We work in a setting where some of the data points received by the learning algorithm are corrupted by an adversary, with the corruption allowed to be a function of the entire trajectory of the learning algorithm up to that point. We refer to an adversary of this type who can observe the entire trajectory upto that point to select a corruption for the next step as dynamic adversary.
We consider a setting where some of the data points received by the learning algorithm are corrupted by an adversary, who is allowed to choose corruptions as a function of the entire trajectory of the learning algorithm up to that point. We refer to such an adversary, who can observe the full trajectory and decide on the next corruption accordingly, as a \textit{dynamic adaptive adversary}. While this may seem unrealistic, since our goal here is to compute certified bounds on the worst case adversary, we refrain from placing informational constraints on the adversary, as an adversary with sufficient side knowledge can still infer hidden parameters of the model from even from just a prediction API \cite{tramer2016stealing})

The adversary is restricted to select a corrupted data point $\bfzadv_t \in \Aspace$  which reflects constraints such as input feature normalization or outlier detection mechanisms used by the learner. We make no additional assumptions about the specific poisoning strategy employed by the adversary.

Thus, our certificates of robustness to poisoning apply to \emph{any dynamic adaptive adversary who chooses poisoned data points from the set $\Aspace$.}

We assume that with a fixed probability, the data point the algorithm receives at each time step is poisoned. In practice, this could reflect the situation that out of a large population of human users providing feedback to a learning system, a small fraction are adversarial and will provide poisoned feedback. Let $\Pdata$ denote the benign distribution of data points. Mathematically, the data point $\bfz_t$
received by the learning algorithm at time $t$ is sampled according to $\bfz_t \sim  \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata$ and $\epsilon$ is a parameter that controls the ``level'' of poisoning (analogous to the fraction of poisoned samples in static poisoning settings \citep{steinhardt2017certified}). This is a special case of Huber's contamination model, which is used in the robust statistics literature \citep{diakonikolas2023algorithmic} (with the contamination model being a Dirac distribution).





%\paragraph{Potential Defense} Inspired by differentially private learning algorithms like DP-SGD \citep{bassily2014private}, we propose adding Gaussian noise to the learning process as a way of smoothing the learning algorithm against impacts of the poisoning adversary. In particular, we add $\bfB\bfw_t$ where $\bfw_t$ is iid noise in each step sampled from the standard Gaussian, and $\bfB$ is a design parameter of the learning algorithm. Subsequently, we will choose $\bfB$ so as to minimize the worst case impact of the poisoning adversary. We denote by $\bfS=\bfB\tran{\bfB}$ the covariance matrix of the noise added.
% More generally, we consider learning algorithms that can be expressed as $\bftheta_{t+1} \gets F_\phi\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw_t}$ where $\bfw_t$ is noise and $\phi$ are tunable parameters of the learning algorithm.

\paragraph{Adversarial objective.}

We assume that the poisoning adversary is interested in maximizing some adversarial objective $\ell_{\textrm{adv}}: \Thetaspace \mapsto \R$, for example, the expected prediction error on some target distribution of interest to the adversary.

\paragraph{Dynamics as a Markov Chain.}
The dynamics in Eq. \eqref{eq:dyn} gives rise to a Markov chain over the parameters $\bftheta$. If $\mathbb{P}_t$ denotes the distribution over parameters at time $t$, we have
\[
\mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv_t}
\mathbb{P}_{t}\br{\bftheta}d\bftheta,
\]
where $\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}$ is the transition kernel induced, i.e., the conditional probability distribution of $\bftheta_{t+1}=F\br{\bftheta_t, \bfz_t, \bfphi_t}$ by \eqref{eq:dyn} given $\bftheta_t$ and $\bfzadv_t$. We formally define it as follows:
\begin{align*}
    &\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}(\bftheta^\prime|\bftheta_t, \bfzadv_t) %\maryam{\mbox{need a $t$ index for $z^\rm{adv}$?}}
    \\&=\ExP{\bfz_t \sim \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata , \bfphi_t \sim \mathbb{P}_t^{\bfphi}} {\mathbb{I}[F\br{\bftheta, \bfz_t, \bfphi_t} = \bftheta^\prime]},
\end{align*}
which is the conditional probability of the parameter estimate at time $t+1$, $\bftheta_{t+1}$ assuming a value $\bftheta^\prime$, conditioned on the parameter estimate at time $t$ being $\bftheta_t$ and the adversarial posion being $\bfzadv_t$.
\djcomment{While this is technically correct, the expected value style of denoting this is indeed confusing. Perhaps we can }

\maryam{let's write it with $\delta(\cdot)$.}
% \maryam{let's remember to explain the $t$ indices}
% \paragraph{Dynamics as a Markov Decision Process}
% The dynamics \eqref{eq:dyn} gives rise to a Markov chain over the parameters $\bftheta$. If $\mathbb{P}_t$ denotes the distribution over parameters at time $t$, we have

% \[\mathbb{P}_{t+1}\br{\bftheta^\prime} = \int \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv}\mathbb{P}_{t}\br{\bftheta}d\bftheta,\]
% where $\mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}$ is the transition kernel induced, i.e., the conditional probability distribution of $\bftheta_{t+1}=F\br{\bftheta_t, \bfz_t, \phi_t}$ by \eqref{eq:dyn} given $\bftheta_t$ and $\bfzadv_t$. We formally define it as follows:
% \begin{align*}
%     \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}(\bftheta^\prime|\bftheta, \bfzadv) = \ExP{\bfz_t \sim \epsilon\text{Dirac}(\bfzadv_t) + (1-\epsilon)\Pdata , \phi_t \sim \mathbb{P}_t^\phi}{\mathbb{I}[F\br{\bftheta, \bfz_t, \phi_t} = \bftheta^\prime]}.
% \end{align*}

% \avi{Time index $F$ as $F_t$ and analyse it at $F_{\infty}$. Making additional assumptions on start state, could lead to finding the optima on a compact space if $\Theta$ is not compact.}
%explicitly given by
% \begin{align}\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \ExP{\bfw}{F\br{\nabla \ell\br{\bftheta_t, \bfzadv_t}, \bfw}} + (1-\epsilon) \ExP{\substack{\bfw \\ \bfz \sim \Pdata}}{F\br{\nabla \ell\br{\bftheta_t, \bfz_t}, \bfw}}
% \end{align}
%\begin{align}\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\left(\bftheta_t - \eta \nabla \ell(\bftheta_t, \bfzadv_t), \eta^2 \bfS\right) + (1-\epsilon) \ExP{\substack{\bfz \sim \Pdata}}{\mathcal{N}\left(\bftheta_t - \eta \nabla \ell\left(\bftheta_t, \bfz\right), \eta^2 \bfS\right)}
%\end{align}
%where $\mathcal{N}\br{\bfx|\mu, \Sigma}$ denotes the Gaussian density at $x$ for a Gaussian with mean $\mu$ and covariance matrix $\Sigma$.

\subsection{Technical Approach: Certificate of Robustness}

% \textcolor{red}{let's title this as: Technical Approach ( and instead of theorem, say proposition)}\avi{Why is it not a result yet?} \textcolor{red}{we have just written the optimization problem, we'll have actual certified bounds for cases we can solve/approximate and that is case by case---more appropriate to call this our 'methodology' or 'technical approach' as opposed to result.}
Since the learning algorithm (dynamics of the parameters) is a Markov process, the sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
\begin{align*}
 \underbrace{\bftheta}_{\text{States}},  \underbrace{\bfzadv}_{\text{Actions}},   \underbrace{\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv}}_{\text{Transition Kernel}}.
\end{align*}


The adversary wants to maximize its objective at the stationary state of the Markov Decision Process. The stationary state is defined as a condition where the distribution of parameters remains unchanged over time. In other words, the distribution of parameters at any given point in the stationary state is identical to the distribution of parameters at the next state.
We use $\mathbb{P}_{\infty}$ to denote the stationary state distribution over respective variables which will be clear from the context.
For example $\bftheta, \bfzadv \sim \mathbb{P}_{\infty}$ denotes $\bftheta, \bfzadv$ sampled from the stationary state joint distribution over the parameters and the adversarial poisoning,  $\bftheta \sim \mathbb{P}_{\infty}$ denotes $\bftheta$ sampled from the stationary state distribution over the parameters obtained by marginalizing over $\bfzadv$ from the joint distribution, and $\mathbb{P}_{\infty}^{\bfphi}$ denotes the stationary distribution over hyperparameters.

With these notations, we can now mathematically express the stationarity condition as:
\begin{align*}
    \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_{\infty}}\br{\bftheta^\prime|\bftheta, \bfzadv}} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
\end{align*}
The expected adversarial loss at stationarity is given by $\ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}}$.

% We are now ready to present our technical result, a certificate of robustness against dynamic data poisoning adversaries. Since the learning algorithm
% is a Markov process, the optimal sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
% \[\text{States } \bftheta, \text{Actions } \bfzadv, \text{Transition Kernel }  \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv},\]
% and hence, the adversary's optimal action sequence can be written as the solution of an \emph{infinite dimensional} linear program (LP)  \cite{puterman2014markov}. The adversary is interested in identifying a
% In particular, for the infinite horizon average reward setting \citep{malek2014linear}, the LP can be written as
We are now ready to present our technical result, a certificate of robustness against dynamic data poisoning adversaries, which is the largest objective value any dynamic adversary can attain in the stationary state.
% Since the learning algorithm
% is a Markov process, the optimal sequence of actions for the adversary (i.e., choices of $\bfzadv$) constitute a Markov Decision Process with
% \[\text{States } \bftheta, \text{Actions } \bfzadv, \text{Transition Kernel }  \mathbb{P}_{F, \Pdata, \mathbb{P}^{\phi}_t}\br{\bftheta^\prime|\bftheta, \bfzadv},\]
Recalling that the sequence of actions for the adversary constitutes a Markov Decision Process, the value of the adversarial objective for the adversary's optimal action sequence is therefore the average reward in the infinite horizon Markov Decision Process setting \citep{malek2014linear} and can be written as the solution of an \emph{infinite dimensional} linear program (LP)  \cite{puterman2014markov}.
% The adversary is interested in identifying a
% In particular, for the infinite horizon average reward setting .
The LP can be written as:
% \begin{subequations}
% \begin{align}
%     \sup_{\mu} & \ExP{\bftheta, \bfzadv \sim \mu}{\ell_{\textup{adv}}\br{\bftheta}} \\
%     \text{Subject to } & \mu \in \mathcal{P}[\R^d \times \R^n] \\
%     & \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \R^d
% \end{align}\label{eq:lp_inf}
% \end{subequations}
% \begin{subequations}
% \begin{align}\label{eq:lp_inf}
%     \sup_{\mathbb{P} \in \mathcal{P}[\Theta \times \bfZ]}  \quad& \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\ell_{\textup{adv}}\br{\bftheta}} \\
%        \text{subject to }\quad & \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\mathbb{P}_{F, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mathbb{P}}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
% \end{subequations}
% \begin{subequations}
% \begin{align}\label{eq:lp_inf}
%     \sup_{\mathbb{P}_{\infty} \in \mathcal{P}[\Theta \times \bfZ]}  \quad& \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}} \\
%        \text{subject to }\quad & \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\mathbb{P}_{F, \Pdata, \mathbb{P}^\phi_{\infty}, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \Theta,
% \end{align}
% \end{subequations}
\begin{subequations}
\begin{align}\label{eq:lp_inf}
    &\sup_{\mathbb{P}_{\infty} \in \mathcal{P}[\Theta \times \bfZ]}  \quad \ExP{\bftheta \sim \mathbb{P}_{\infty}}{\ell_{\textup{adv}}\br{\bftheta}} \quad \text{subject to } \\
       & \ExP{\bftheta, \bfzadv \sim \mathbb{P}_{\infty}}{\mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_{\infty}}\br{\bftheta^\prime|\bftheta, \bfzadv}} =  \mathbb{P}_{\infty}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta,
\end{align}
\djcomment{While it is nice to have the stationarity condition, I don't think it is necessary here. We should first state the problem we are trying to solve - at stationarity, what is the expected rewards of the attacker? Then theorem 1 provides a way to bound this quantity.}
\end{subequations}
% \avi{I rewrote the set of equations below: check if it is notationally more consistent that the one above. Maybe not much of a difference on second thought.}
% In particular, for the infinite horizon average reward setting \citep{malek2014linear}, the LP can be written as
% \begin{subequations}
% \begin{align}
%     \sup_{\mu} & \ExP{\bftheta, \bfzadv \sim \mu}{\ExP{\bftheta^\prime | \bftheta, \bfzadv}{\ell_{\textup{adv}}\br{\bftheta^\prime}}} \\
%     \text{Subject to } & \mu \in \mathcal{P}[\R^d \times \R^n] \\
%     & \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta}} = \ExP{\bftheta, \bfzadv \sim \mu}{\mathbb{I}[\bftheta^\prime=\bftheta]} \quad \forall \bftheta^\prime \in \R^d
% \end{align}
% \end{subequations}
where $\mathcal{P}[\Theta \times \bfZ]$ denotes the space of probability measures on $\Theta \times \bfZ$.
% and $\mathbb{I}$ denotes the indicator function that equals $1$ if its argument is true and $0$ otherwise.
%\maryam{Dj, are there other papers to cite that use this formulation?}
%\djcomment{Maryam, we quote references [13] and [17] based on which this is derived. Do we need anything further? If you mean specifically in the context of poisoning, I am pretty sure not, no other papers use this.}

\begin{theorem}\label{thm:certificate}
For any function $\lambda: \Theta \mapsto \R$, we can upper bound the optimal value of \eqref{eq:lp_inf} by
% \begin{align}
%     \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \qquad \ExP{\bftheta^\prime \sim \mathbb{P}_{F, \Pdata, \bfzadv}\br{\cdot|\bftheta}}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}. \label{eq:cert_general}
% \end{align}
\begin{align}
    \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \qquad \ExP{\bftheta^\prime \sim \mathbb{P}_{F, \Pdata, \mathbb{P}^{\bfphi}_{\infty}}\br{\cdot|\bftheta, \bfzadv}}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}. \label{eq:cert_general}
\end{align}
\end{theorem}
% \begin{proof}
% \avi{Done for space constraints}
\textit{Proof.}
Follows by weak duality for the LP \eqref{eq:certificate}. \maryam{Give more details on derivation (or point to appendix)}

If strong duality holds \cite{nash1987linear, clark2003infinite}
% \maryam{Cite references that properly cover infinite dimensional duality and constraint qualifications. Maybe the book by Attouche et al on optimization in Sobolev spaces? Other refs? Avi, please look this up, don't need Dj for this}
, we further have that the optimal value of \eqref{eq:lp_inf} is exactly equal to
\begin{subequations}
\begin{align}
    \inf_{\lambda: \Thetaspace \mapsto \R} \sup_{\substack{\bftheta \in \Thetaspace\\ \bfzadv \in \Aspace}}
    \ExP{\bftheta^\prime \sim {\mathbb{P}_{F, \Pdata, \mathbb{P}_{\infty}^{\bfphi}}} \br{\cdot|\bftheta, \bfzadv}}{\lambda\br{\bftheta^\prime}
    + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}}. \label{eq:certificate}
\end{align}
\end{subequations}
% \end{proof}
% \vskip -0.5in
% \avi{How do we constrain $\lambda$ such that $\ExP{\bftheta^\prime \sim \mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\cdot|\bftheta}}{\lambda\br{\bftheta^\prime}} -\lambda\br{\bftheta} \geq 0 \; \forall  \bftheta$? }

\maryam{fixed some typos above. indices of $\theta'$ distribution are very hard to follow, need to simplify---explain earlier and replace with slightly simpler notation?}\avi{Check now? Can add a line that, with indices is a random variable, without indices is a realization of the random variable.}
\djcomment{My thinking is that this would be much simpler to explain for the discounted horizon setting where we can interpret $\lambda$ as an upper bound on the expected discounted sum of future rewards. I am leaning towards only sketching the derivation for the discounted horizon setting where we can show that as long as $\lambda$ satisfies certain constraints, the. What do you think? }
\fi
% \paragraph{Poisoning Defense}
\section{Notations}\label{appendix:notation}
\begin{table}[h!]\label{tab:notation}
\centering
\footnotesize
\begin{tabular}{lll}
\toprule
Notation & Interpretation & Belongs to \\
\midrule
   \vphantom{$F_\phi^d$}$\bftheta$  & Model Parameters & $\Thetaspace$ \\
   \vphantom{$F_\phi^d$}$\bfphi$ & Hyper-parameters & $\Phispace$ \\
   % \hline
   % $\bfw$ & Gaussian noise injected into learning algorithm  & $\Wspace$ \\
   \vphantom{$F_\phi^d$}$\bfz$ & Data point & $\Zspace$ \\
   \vphantom{$F_\phi^d$}$F_{\phi}$ & Update rule & $\Thetaspace \times \Zspace \mapsto \Thetaspace$ \\
   \vphantom{$F_\phi^d$}$\bfzadv$ & Adversarial data point & $\mathcal{A}$\\
   \vphantom{$F_\phi^d$}$\ell_{\rm adv}$ & Adversarial objective fn. & $\Thetaspace \mapsto \mathbb{R}$ \\
   \vphantom{$F_\phi^d$}$\Pdata$ & Benign Data Dist. & $\mathcal{P}[\bfZ]$ \\
   % \vphantom{$F_\phi^d$}$\Ptarget$ & Adversary's target Dist. & $\mathcal{P}[\bfZ]$ \\
   \vphantom{$F_\phi^d$}$\Pi(\cdot| \bftheta, \bfzadv)$ & State Transition Kernel & $\Thetaspace \!\times\! \Zspace \mapsto \mathcal{P}[\Thetaspace]$\!\!\\
   \bottomrule
\end{tabular}\caption{Notation.}
% \maryam{if we don't use $P^{\rm target}$ we can remove that line}}
\vskip -0.1in
\end{table}
\section{Proofs}\label{appendix:proofs}
\certificate*
\begin{proof}
The primal problem is defined below:
\begin{align*}
    &\sup_{\substack{d_{\pi} \in \mathcal{P}[\Thetaspace] \\ \pi \in \mathcal{P}[\Thetaspace \times \bfZ]}}  \quad \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\textup{adv}}\br{\bftheta}}, \quad \text{subject to } \\
       & \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} =  d_{\pi}(\bftheta^\prime) \quad \forall \bftheta^\prime \in \Theta.
\end{align*}
For a given function $\lambda : \Thetaspace \mapsto \R$, the Lagrangian
% function \maryam{you should say the Lagrangian, not the dual function}
can be written as:
\begin{align*}
    \mathcal{L}(d_{\pi}, \pi, \lambda) &= \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\textup{adv}}\br{\bftheta}} + \int_{\bftheta^\prime \in \Thetaspace} \lambda(\bftheta^\prime)\bigg[ \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\Pi(\bftheta^\prime | \bftheta, \bfzadv)} - d_{\pi}(\bftheta^\prime)\bigg] d\bftheta^\prime \\
    % &= \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\ell_{\textup{adv}}\br{\bftheta}} + \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda(\bftheta^\prime) }} - \ExP{\substack{\bftheta^\prime \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta^\prime)}}{\lambda(\bftheta^\prime)}
    &= \ExP{\bftheta \sim d_{\pi}(\cdot)}{\ell_{\textup{adv}}\br{\bftheta}} + \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda(\bftheta^\prime) }} - \ExP{\bftheta^\prime \sim d_{\pi}(\cdot)}{\lambda(\bftheta^\prime)}\\
    &= \ExP{\substack{\bftheta \sim d_{\pi}(\cdot) \\ \bfzadv \sim \pi(\cdot|\bftheta)}}{\ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda(\bftheta^\prime) } + \ell_{\textup{adv}}\br{\bftheta} - \lambda(\bftheta)},
\end{align*}
which serves as an upper bound on the primal objective. Note that the value of the Lagrangian for a given $\lambda$ depends on the expectation of $\ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda(\bftheta^\prime) } + \ell_{\textup{adv}}\br{\bftheta} - \lambda(\bftheta)$ over the joint distribution of $\bftheta \times \bfzadv$. Since $\Thetaspace$ and $\mathcal{A}$ are compact, the supremum for a given $\lambda$ occurs for the distribution placing all its mass on the point $\bftheta, \bfzadv$ maximizing: $\ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda(\bftheta^\prime) } + \ell_{\textup{adv}}\br{\bftheta} - \lambda(\bftheta)$.

Therefore, for any choice of $\lambda$ and any feasible $d_{\pi}$, we have:
\begin{align*}
    \sup_{\substack{d_{\pi} \in \mathcal{P}[\Thetaspace] \\ \pi \in \mathcal{P}[\Thetaspace \times \bfZ]}} \mathcal{L}(d_{\pi}, \pi, \lambda) =  \sup_{\substack{\bftheta \in \Thetaspace \\ \bfzadv \in \Aspace}} \quad \ExP{\bftheta^\prime \sim \Pi(\cdot | \bftheta, \bfzadv)}{\lambda\br{\bftheta^\prime}} + \ell_{\textup{adv}}\br{\bftheta}-\lambda\br{\bftheta}.
\end{align*}
This completes the proof.
\end{proof}
\section{Mean Estimation}\label{appendix:mean_estimation}
% \textcolor{magenta}{rewrite the optimization problem as we did in 578: write out the objective explicitly, and write the domain constraints as constraints in the optimization problem, let's see what that looks like---it will be easier to parse and see what the dependence of objectives and constraints on the variables are. Also see if you can simplify the terms further. put the negative sign up front or see if you can cancel out the negatives. Else it looks like everything was dumped in the problem with no attempt to make it readable and could bother reviewers.
\meancertificate*
\begin{proof}
We can write the learning algorithm in Eq.~\eqref{eq:dyn} for the case of mean estimation as follows:
\begin{align*}
    \bftheta_{t+1} = F\br{\bftheta_t, \bfz_t} + \eta \bfB \bfw_t,
\end{align*}
where $F\br{\bftheta, \bfz}=\bftheta\br{1-\eta} + \eta\bfz$, which is a linear transformation of $\bftheta$ followed by additive Gaussian noise.

The transition distribution for the parameter is given by:
% \[\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|\bftheta-\eta\br{\bftheta-\bfzadv}, \eta^2\bfS} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \mathcal{N}\br{\bftheta^\prime|\bftheta-\eta\br{\bftheta-\bfz}, \eta^2\bfS}}\]
\begin{align}
\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfzadv), \eta^2\bfS} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfz), \eta^2\bfS}} \label{eq:transition_mean}
\end{align}
which is a Gaussian distribution whose mean depends linearly on $\bftheta$ and $\bfzadv$.

 Then, we have from Eq.~\eqref{eq:certificate} that the certified bound on the adversarial objective is given by:
% \avi{Typo: First 2 terms below should be $\lambda(\theta')$ not $\lambda(\theta)$?}
\begin{subequations}
\begin{align}
&\sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \epsilon\ExP{\bftheta^\prime \sim \mathcal{N}\br{F\br{\bftheta, \bfzadv}, \eta^2 \bfS}}{\lambda\br{\bftheta^\prime}}+(1-\epsilon)\ExP{\bfz \sim \Pdata}{\ExP{\bftheta^\prime \sim \mathcal{N}\br{F\br{\bftheta, \bfz}, \eta^2 \bfS}}{\lambda\br{\bftheta^\prime}}}-\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
&\text{We choose $\lambda\br{\bftheta} = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb$ to be a quadratic function. Then we have:}\nonumber\\
&=\sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \epsilon\br{ \lambda\br{F\br{\bftheta, \bfzadv}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \lambda\br{F\br{\bftheta, \bfz}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}}-\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
&=\sup_{\substack{\bfzadv : \|\bfzadv - \mu\|_2^2 \leq r \\ \bftheta}}  -\left\|\bmat{\bftheta \\ \bfzadv}\right\|^2_{\bfE^{-1}} + \bmat{\bftheta \\ \bfzadv}^\top \bmat{2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb \\ \epsilon \eta \bfb}  \nonumber\\
&+ (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu, \\
&\text{where $\bfE = \bmat{ (1 - (1 - \eta)^2)\bfA - \bfI & -\eta\epsilon(1 - \eta)\bfA \\ -\eta\epsilon(1 - \eta)\bfA &  -\epsilon\eta^2 \bfA}$,}\nonumber\\
&\text{The dual function of this supremum (with dual variable $\nu$) can be written as: } \nonumber\\
&=\inf_{\nu \geq 0} \sup_{\substack{\bfzadv \\ \bftheta}}  -\left\|\bmat{\bftheta \\ \bfzadv}\right\|^2_{\bfD^{-1}} + \bmat{\bftheta \\ \bfzadv}^\top \bmat{2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb \\ \epsilon \eta \bfb + 2 \nu \mu}  \nonumber\\
&+ (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu + \nu(r - \mu^\top \mu) \\
&\text{where $\bfD = \bmat{ (1 - (1 - \eta)^2)\bfA - \bfI & -\eta\epsilon(1 - \eta)\bfA \\ -\eta\epsilon(1 - \eta)\bfA &  -\epsilon\eta^2 \bfA - \nu\bfI}$.}\nonumber\\
&\text{The inner supremum is a quadratic expression in $\bfzadv, \bftheta$. A finite supremum exists if the} \nonumber \\ &\text{Hessian of the expression is negative semifdefinite. Plugging in the tractable maximizer}\nonumber\\ &\text{of the quadratic, we get:} \nonumber\\
& \inf_{\nu \geq 0}\frac{1}{4} \left\|\bmat{2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb \\ \epsilon \eta \bfb + 2 \nu \mu}\right\|^2_{\bfD} + (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) \nonumber \\& + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS) + \nu(r - \mu^\top \mu)   \; \text{such that} \; \bfD \succeq 0.
\end{align}
\end{subequations}
This completes the proof.
% \textbf{Certified Bound for a fixed leaner:} Define $\bfD_1 = ((1 - \eta)^2 - 1)\bfA + \bfI, \bfD_2 = 2\eta\epsilon(1 - \eta)\bfA, \bfd_3 = 2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb, d_4 = (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS)$. The bound on the adversarial objective is given by:

% \begin{align}
%     \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \bftheta^\top \bfD_1 \bftheta + \epsilon \eta^2 {\bfzadv}^\top \bfA \bfzadv + \bftheta^\top \bfD_2 \bfzadv + {\bftheta}^\top \bfd_3 + \epsilon \eta {\bfzadv}^\top \bfb + d_4.
% \end{align}
% The inner supremum is a quadratic expression in $\bfzadv, \bftheta$. A finite supremum exists if the Hessian of the expression is negative semifdefinite.
% The dual function of this supremum (with dual variable $\nu$) can be written as in Eq.~\eqref{eq:dualfunction}.
\end{proof}

\begin{lem}
The stationary distribution in the absence of adversary in Eq.~\eqref{eq:stationary} for the mean estimation problem for $\Pdata = \mathcal{N}(\mu, \bfSigma)$ takes the form:
\begin{align*}
   \mathbb{P}\br{\bfS, \Pdata} = \mathcal{N}(\mu, \eta^2 \bfS).
\end{align*}
\end{lem}
\begin{proof}
The stationary distribution is tractable in this case. Recall from Eq.~\eqref{eq:transition_mean}, setting $\epsilon=0$, the transition distribution conditioned on $\bftheta$ is a Gaussian whose mean is linear in $\bftheta$. Therefore the stationary distribution:
\begin{align*}
    \ExP{\bftheta \sim \mathbb{P}}{\mathbb{P}_{\bfS, \Pdata}\br{\bftheta^\prime|\bftheta}},
\end{align*}
will be a Gaussian distribution as a sum of Gaussians is also a Gaussian. Let us assume the distribution has mean $\bfm$. Comparing the means we have:
\begin{align*}
    \bfm (1 - \eta) + \eta \mu &= \bfm\\
    \implies \bfm = \mu.
\end{align*}
Moreover, $\mathbb{P}_{\bfS, \Pdata}\br{\bftheta^\prime|\bftheta}$ is a Gaussian with covariance $\eta^2 \bfS$ for all $\bftheta$. Hence the expectation over $\mathbb{P}$ also has covariance $\eta^2 \bfS$. This concludes the proof.
\end{proof}
\begin{lem}
The loss at stationarity of the learning dynamics in the absence of an adversary for the mean estimation problem for $\Pdata = \mathcal{N}(\mu, \bfSigma)$ is given by:
\begin{align}
    \ExP{\bftheta \sim \mathbb{P}\br{\bfS, \Pdata}}{\ell\br{\bftheta}} = \eta^2 \Trace(\bfS).
\end{align}
\end{lem}
\begin{proof}
\begin{align*}
    &\ExP{\theta \sim \mathcal{N}(\mu, \eta^2 \bfS)}{\|\theta - \mu\|_2^2}\\
    &= \ExP{\theta \sim \mathcal{N}(0, \eta^2 \bfS)}{\|\theta\|_2^2}\\
    &= \eta^2 \Trace(\bfS).
\end{align*}
\end{proof}
% \paragraph{Defense for a single data distribution}
% % \textbf{Parameters for robust learning algorithm:}
% An upper bound on the objective in Eq. \eqref{eq:mean_certificate} for a single given $\Pdata$ can thus be written as the following minimization problem:
% % \begin{align}
% %     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \bfA \in \mathbb{S}^d, \bfb \in \R^d \\ \nu \leq 0}}\qquad &\eta^2 \Trace(\bfS)  + \kappa  g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma) \label{eq:mean_defense}
% % \end{align}
% \begin{align}
%     \inf_{\substack{\bfS \in \mathbb{S}^d_+ \\ \bfA \in \mathbb{S}^d, \bfb \in \R^d \\ \nu \geq 0}}\qquad &  g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma) \label{eq:mean_certificate_min}
% \end{align}
\begin{remark}
We use CVXPY \citep{diamond2016cvxpy} to solve the optimization problems in Algorithm~\ref{alg:meta_mean}.
\end{remark}
% \section{Score Based Classifiers}
% Following the same setup as \citep{raghunathan2018certified}, we describe here the learning algorithm for score based classifiers. For ease of exposition we start with binary classifiers and will extend to multi-class classification later. Our goal is to learn a mapping $C : \mathcal{X} \mapsto \{-1,1\}$, where $\mathcal{X} \in \R^d$. The output of $C$ is given by a scoring function $f^i : \mathcal{X} \mapsto \R$ for $i \in \{-1,1\}$. For any input $\bfx \in \mathcal{X}$, the classifier outputs $C(\bfx) = \arg\max_{i} f^i(\bfx)$. \maryam{check and write this for multiclass classification}

% We consider the case of linear classifiers as in \citep{raghunathan2018certified}. Define $f^i(\bfx) = \bftheta_i^\top \bfx$ where $\bftheta_i \in \R^d$ for $i \in \{-1,1\}$. Given a data point $(\bfx, y)$, the learner's objective is defined below:
% \begin{align*}
%     l(\bfx, y, \bftheta_1, \bftheta_{-1}) &= (\bftheta_1 - \bftheta_{-1})^\top \bfx \mathbb{I}[y = -1] + (\bftheta_{-1} - \bftheta_{1})^\top \bfx \mathbb{I}[y = 1]\\
%     &= (\bftheta_{-1} - \bftheta_{1})^\top \bfx y.
% \end{align*}
% Noting that the loss value depends only on the quantities $\bfx y$ and $(\bftheta_{-1} - \bftheta_{1})$, we introduce notation $\bfz = \bfx y$ and $\bftheta = (\bftheta_{-1} - \bftheta_{1})$ and henceforth work with only these quantities. The learner optimizes a $\ell$-2 regularized loss (with regularization parameter $\sigma$) over the parameter $\bftheta$ upon observing a sample $\bfz$.
% \begin{align}
%     l(\bftheta, \bfz) = \bftheta^\top \bfz + \frac{\sigma}{2} \norm{\theta}_2^2.
% \end{align}
% The adversarial loss is given by:
% \begin{align}
%     l_{\textrm{adv}}(\bfz, \bftheta) = \bftheta^\top \bfz.
% \end{align}
% We assume the positive and the negative features come from $\mathcal{N}(\mu_{+}, \bfSigma_{+})$ and $\mathcal{N}(\mu_{-}, \bfSigma_{-})$ respectively. Let $w_{+}$ be the proportion of the positive samples. Then we can write:
% \begin{align*}
%     \Pdata(\bfz) &= w_{+}  \mathcal{N}(\mu_{+}, \bfSigma_{+}) + (1 - w_{+}) \mathcal{N}(-\mu_{-}, \bfSigma_{-})\\
%     &= \mathcal{N}(w_{+} \mu_{+} - (1 - w_{+}) \mu_{-}, w_{+}^2 \bfSigma_{+} + (1 - w_{+})^2 \bfSigma_{-}))\\
%     &=\mathcal{N}(\mu, \bfSigma),
% \end{align*}
% where $\mu = w_{+} \mu_{+} - (1 - w_{+}) \mu_{-}$ and $\bfSigma = w_{+}^2 \bfSigma_{+} + (1 - w_{+})^2 \bfSigma_{-}$.
% \begin{restatable}{theorem}{classificationcertificate}
% Choosing $\lambda : \R^d \rightarrow \R$ in Theorem~\ref{thm:certificate} to be quadratic, i.e. $\lambda\br{\bftheta} = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb$, the adversarial constraint set of the form $\|\bfzadv - \mu\|_{\bfSigma}^2 \leq r$, the certificate for the classification task for $\Pdata(\bfz) = \mathcal{N}(\bfz | \mu, \bfSigma)$ for a fixed learning algorithm (i.e. $\bfS$ is fixed) is given by:
% \begin{align}
%     \inf_{\bfA \in \mathbb{S}^d, \bfb \in \R^d, \nu \geq 0} g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma),\label{eq:mean_certificate}
% \end{align}
% where $g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma)$ is a convex objective in $\bfA,\bfb,\nu$ (matrix fractional objective  with Linear Matrix Inequality (LMI) constraint) as defined below:
% \begin{align}
%     g(\bfA, \bfb,  \nu, \bfS, \mu, \bfSigma) &=
%     \begin{cases}
%     &  \frac{1}{4} \left\|\bmat{-2(1 - \epsilon)\eta(1 - \sigma\eta)\bfA \mu + \mu - \sigma \eta \bfb \\ -\epsilon \eta \bfb + 2 \nu \bfSigma^{-1} \mu}\right\|^2_{\bfD} + (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu - \eta \bfb^\top \mu) \nonumber \\& + \eta^2 \Trace(\bfA\bfS) + \nu(r - \mu^\top \bfSigma^{-1} \mu)    \; \text{if} \quad \nu \geq 0 ;\; \bfD \succeq 0 \\
%     & -\infty \quad \text{else}
%     \end{cases}
%     \label{eq:dualfunctionclassification}
% \end{align}

% and $\bfD = \bmat{ (1 - (1 - \sigma \eta)^2)\bfA & \eta\epsilon(1 - \sigma\eta)\bfA \\ \eta\epsilon(1 - \sigma\eta)\bfA &  -\epsilon\eta^2 \bfA - \nu\bfSigma^{-1}}$ and $\|\bfx\|^2_{\bfD} = \bfx^\top \bfD^{-1} \bfx$.
% % and $\bfD_1 = ((1 - \eta)^2 - 1)\bfA + \bfI, \bfD_2 = 2\eta\epsilon(1 - \eta)\bfA, \bfd_3 = 2(1 - \epsilon)\eta(1 - \eta)\bfA \mu - 2 \mu - \eta \bfb, d_4 = (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu + \eta \bfb^\top \mu) + \mu^\top \mu + \eta^2 \Trace(\bfA\bfS)$.
% \end{restatable}
% \begin{proof}
% We can write the learning algorithm in Eq.~\eqref{eq:dyn} as follows:
% \begin{align*}
%     \bftheta_{t+1} = F\br{\bftheta_t, \bfz_t} + \eta \bfB \bfw_t,
% \end{align*}
% where $F\br{\bftheta, \bfz}=\bftheta\br{1- \sigma \eta} - \eta\bfz$, which is a linear transformation of $\bftheta$ followed by additive Gaussian noise.

% The transition distribution for the parameter is given by:
% % \[\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|\bftheta-\eta\br{\bftheta-\bfzadv}, \eta^2\bfS} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \mathcal{N}\br{\bftheta^\prime|\bftheta-\eta\br{\bftheta-\bfz}, \eta^2\bfS}}\]
% \begin{align}
% \mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfzadv), \eta^2\bfS} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfz), \eta^2\bfS}} \label{eq:transition_mean}
% \end{align}
% which is a Gaussian distribution whose mean depends linearly on $\bftheta$ and $\bfzadv$.

%  Then, we have from Eq.~\eqref{eq:certificate} that the certified bound on the adversarial objective is given by:
% % \avi{Typo: First 2 terms below should be $\lambda(\theta')$ not $\lambda(\theta)$?}
% \begin{subequations}
% \begin{align}
% &\sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \epsilon\ExP{\bftheta^\prime \sim \mathcal{N}\br{F\br{\bftheta, \bfzadv}, \eta^2 \bfS}}{\lambda\br{\bftheta^\prime}}+(1-\epsilon)\ExP{\bfz \sim \Pdata}{\ExP{\bftheta^\prime \sim \mathcal{N}\br{F\br{\bftheta, \bfz}, \eta^2 \bfS}}{\lambda\br{\bftheta^\prime}}}-\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
% &\text{We choose $\lambda\br{\bftheta} = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb$ to be a quadratic function. Then we have:}\nonumber\\
% &=\sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta}} \epsilon\br{ \lambda\br{F\br{\bftheta, \bfzadv}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \lambda\br{F\br{\bftheta, \bfz}} + \eta^2 \inner{\nabla^2 \lambda\br{0}}{\bfS}}-\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
% &=\sup_{\substack{\bfzadv : \|\bfzadv - \mu\|_{\bfSigma}^2 \leq r \\ \bftheta}}  -\left\|\bmat{\bftheta \\ \bfzadv}\right\|^2_{\bfE^{-1}} + \bmat{\bftheta \\ \bfzadv}^\top \bmat{-2(1 - \epsilon)\eta(1 - \sigma\eta)\bfA \mu + \mu - \sigma \eta \bfb \\ -\epsilon \eta \bfb}  \nonumber\\
% &+ (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu - \eta \bfb^\top \mu) + \eta^2 \Trace(\bfA\bfS), \\
% &\text{where $\bfE = \bmat{ (1 - (1 - \sigma \eta)^2)\bfA & \eta\epsilon(1 - \sigma\eta)\bfA \\ \eta\epsilon(1 - \sigma\eta)\bfA &  -\epsilon\eta^2 \bfA}$,}\nonumber\\
% &\text{The dual function of this supremum (with dual variable $\nu$) can be written as: } \nonumber\\
% &=\inf_{\nu \geq 0} \sup_{\substack{\bfzadv \\ \bftheta}}  -\left\|\bmat{\bftheta \\ \bfzadv}\right\|^2_{\bfD^{-1}} + \bmat{\bftheta \\ \bfzadv}^\top \bmat{-2(1 - \epsilon)\eta(1 - \sigma\eta)\bfA \mu + \mu - \sigma \eta \bfb \\ -\epsilon \eta \bfb + 2 \nu \bfSigma^{-1} \mu}  \nonumber\\
% &+ (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu - \eta \bfb^\top \mu) + \eta^2 \Trace(\bfA\bfS) + \nu(r - \mu^\top \bfSigma^{-1} \mu) \\
% &\text{where $\bfD = \bmat{ (1 - (1 - \sigma \eta)^2)\bfA & \eta\epsilon(1 - \sigma\eta)\bfA \\ \eta\epsilon(1 - \sigma\eta)\bfA &  -\epsilon\eta^2 \bfA - \nu\bfSigma^{-1}}$.}\nonumber\\
% &\text{The inner supremum is a quadratic expression in $\bfzadv, \bftheta$. A finite supremum exists if the} \nonumber \\ &\text{Hessian of the expression is negative semifdefinite. Plugging in the tractable maximizer}\nonumber\\ &\text{of the quadratic, we get:} \nonumber\\
% & \inf_{\nu \geq 0}\frac{1}{4} \left\|\bmat{-2(1 - \epsilon)\eta(1 - \sigma\eta)\bfA \mu + \mu - \sigma \eta \bfb \\ -\epsilon \eta \bfb + 2 \nu \bfSigma^{-1} \mu}\right\|^2_{\bfD} + (1-\epsilon)(\eta^2 \Trace(\bfSigma \bfA) + \eta^2 \mu^\top \bfA \mu - \eta \bfb^\top \mu) \nonumber \\& + \eta^2 \Trace(\bfA\bfS) + \nu(r - \mu^\top \bfSigma^{-1} \mu)   \; \text{such that} \; \bfD \succeq 0.
% \end{align}
% \end{subequations}
% \end{proof}
% \begin{lem}
% The stationary distribution in the absence of adversary in Eq.~\eqref{eq:stationary} for the mean estimation problem for $\Pdata = \mathcal{N}(\mu, \bfSigma)$ takes the form:
% \begin{align*}
%    \mathbb{P}\br{\bfS, \Pdata} = \mathcal{N}(-\frac{\mu}{\sigma}, \eta^2 \bfS).
% \end{align*}
% \end{lem}
% % \avi{To make this more realistic, can we find an upper bound (for a fixed $\bfA, \bfb$) for every data point $\bfz$ and sum it over all points in the dataset? Something we could potentially change right from the start of the paper, without loosing anything? That way, we also get an optimal attack?}
% % \avi{Add a remark}
% % \textcolor{magenta}{If you're using CVX or CVXPY to prase and it calls MOSEK to solve the SDP, mention these, and give a citation for CVX/CVXPY (see CVX website)}

% % \section{New Classification Idea}
% % We wish to do supervised mixture models. Given a set of labels $\mathcal{Y}$, data is generated as follows:

% % \textbf{Data Generation Process:}
% % \begin{align*}
% %     \{\{(\bfx, y) | \bfx \sim \mathcal{N}(\mu_{y}, \bfI)\} | y \in \mathcal{Y}\}
% % \end{align*}

% % % Given trainind data $\{(\bfz_1, y_1), \ldots, \}$

% % Let $\Theta = \{\theta_y \in \R^d | y \in \mathcal{Y}\}$ be the learnable parameters. The learning algorithm observes feature, label pairs :$\{\{(\bfz_1, y), \ldots, \} | y \in \mathcal{Y} \}$.

% % \textbf{Loss:}
% % \begin{align}
% %     l((\bfx, y), \Theta) = \|\theta_y - x\|_2^2.
% % \end{align}

% % \textbf{Constraint:} Any adversarial example $(\bfxadv, \yadv)$ satisfies the following constraint:
% % \begin{align}
% %      \|\bfxadv - \mu_{\yadv}\|_2^2 \leq c.
% % \end{align}

% % This establishes equivalence between multiple $|\mathcal{Y}|$ mean estimation problems, and multi-class classification predictions for a given $\bfx$ can be easily made by $\argmin_{y \in \mathcal{Y}} \|\theta_y - \bfx\|_2^2$. The classification boundaries would be the corresponding hyperplanes equidistant from the estimated means.

\section{Binary Classification}\label{appendix:binary_classification}

% \begin{lem}
% If $\|\bftheta_0\|_2 \leq \frac{\sqrt{d}}{\sigma}$, then for all $t > 0$, $\|\bftheta_t\|_2 \leq \frac{\sqrt{d}}{\sigma}$.
% \end{lem}
% \begin{proof}
%     Use Induction and traingle inequality.
% \end{proof}
\begin{lem}
If $\|\bftheta_0\|_{2} \leq \frac{1}{\sigma}$, then for all $t > 0$, $\|\bftheta_t\|_{2} \leq \frac{1}{\sigma}$.
\end{lem}
\begin{proof}\label{lem:QPdual}
    Use Induction and triangle inequality.
\end{proof}
\begin{lem}
Consider the primal problem (here $\bfQ \succ 0$):
\begin{align*}
    &\sup_{\bfx, \bfy} -\bfx^\top \bfQ \bfx + \bfp_1^\top \bfx + \bfp_2^\top \bfy \\
    & \text{such that}\\
    & \bfA_{1i}^\top \bfx + \bfA_{2i}^\top \bfy \succeq \bfc_i \; \forall i \in [m].
\end{align*}
Its dual is the following oprimization problem:
\begin{align*}
    &\inf_{\nu_{1}, \ldots, \nu_{m} \succeq 0} \frac{1}{4} \|\bfp_1 + \sum_{i \in [m]} \bfA_{1i}\nu_i\|^2_{\bfQ^{-1}} - \sum_{i \in [m]} \nu_{i}^\top \bfc_i\\
    &\text{such that}\\
    & \bfp_2 + \sum_{i \in [m]} \bfA_{2i}\nu_i = \bf0.
\end{align*}
\end{lem}
\begin{proof}
    We write the primal objective's dual function with Lagrange parameters $\nu_{1}, \ldots, \nu_{m} \succeq 0$ as follows:
\begin{align*}
    \sup_{\bfx, \bfy} -\bfx^\top \bfQ \bfx + (\bfp_1 + \sum_{i \in [m]} \bfA_{1i}\nu_i)^\top \bfx + (\bfp_2 + \sum_{i \in [m]} \bfA_{2i}\nu_i)^\top \bfy - \sum_{i \in [m]} \nu_{i}^\top \bfc_i.
\end{align*}
The supremum is maximized for:
\begin{align*}
    \bfx^\ast = \frac{1}{2} \bfQ^{-1} (\bfp_1 + \sum_{i \in [m]} \bfA_{1i}\nu_i),
\end{align*}
and since we don't have a lower bound on $\bfy \succeq \mathbf{0}$, we need $\bfp_2 + \sum_{i \in [m]} \bfA_{2i}\nu_i = \bf0$.

Plugging these in, the dual function completes the proof.
\end{proof}
% where $\Pi$ is a projection.
% For example $\Pi(x) = x \; \text{if} \; \|x\|_2 \leq \zeta,\; \text{else} \; \zeta x/\|x\|_2$.
% Thus the update is given by:
% % let the update rule be defined by:
% \begin{align}
%     \bftheta_{t+1} =  F(\bftheta_t, \bfz) + \eta \bfB \bfw_t \quad (\text{where} \; \bfw_t \sim \mathcal{N}(0, \bfI)).
% \end{align}

% One can view $\mathbb{I}[\bftheta^\top \bfz \leq 1]$ as a measure of the goodness of the current prediction, and thus controls by how much the parameter should be updated based on the current sample.
% \djcomment{Why is the }
\classificationhingecertificate*
\begin{proof}
Since $\epsilon$ fraction of the samples are corrupted by the adversary, the transition distribution conditioned on $\bfzadv$, the benign distribution $\Pdata$ and the defense parameter $\sigma$ is given by:
% \[\mathbb{P}_{\bfS, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} = \epsilon \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfzadv), \eta^2\bfS} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \mathcal{N}\br{\bftheta^\prime|F(\bftheta, \bfz), \eta^2\bfS}},\]
\begin{align*}
\mathbb{P}_{\sigma, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} &= \epsilon F(\bftheta, \bfzadv) + (1 - \epsilon) \ExP{\bfz \sim \Pdata} {F(\bftheta, \bfz)}\\
&= \epsilon ((1 - \sigma \eta) \bftheta +  \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bfzadv) + (1 - \epsilon) ((1 - \sigma \eta)\bftheta + \eta \ExP{\bfz \sim \Pdata} {\mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz}).
\end{align*}
We wish to analyse the adversarial loss at stationarity. We consider the following 2 cases at stationarity. Consider $\Thetaspace_1, \mathcal{A}_1$ as the space such that
(i) $\mathbb{I}[\bftheta^\top \bfzadv \leq 1] = 0$, the transition distribution at stationarity looks like:
\begin{align*}
\mathbb{P}_{\sigma, \Pdata, \bfzadv}\br{\bftheta^\prime|\bftheta} &= \epsilon F(\bftheta, \bfzadv) + (1 - \epsilon) \ExP{\bfz \sim \Pdata} {F(\bftheta, \bfz)}\\
&= \epsilon ((1 - \sigma \eta) \bftheta +  \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bfzadv) + (1 - \epsilon) ((1 - \sigma \eta)\bftheta + \eta \ExP{\bfz \sim \Pdata} {\mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz})\\
&= (1 - \frac{\sigma}{1 - \epsilon} (1 - \epsilon) \eta) \bftheta +  (1 - \epsilon)\eta \ExP{\bfz \sim \Pdata} {\mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz}
\end{align*}
This can be interpreted as the stationary distribution in the absence of an adversary with learning rate $(1 - \epsilon) \eta$ and regularization $\frac{\sigma}{(1 - \epsilon)}$.

The other case is $\Thetaspace_2, \mathcal{A}_2$ such that (ii) $\mathbb{I}[\bftheta^\top \bfzadv \leq 1] = 1$.

Note that $\{\Thetaspace_1 \times \mathcal{A}_1\} \cup \{\Thetaspace_2 \times \mathcal{A}_2\}= \Thetaspace \times \mathcal{A}$.
We can treat both these cases separately in our original problem Eq.~\ref{eq:lp_inf} before going to the formulation in Eq.~\ref{eq:cert_general}. Thus we aim to find a certificate for each of these cases and we do so via the formulation in Eq.~\ref{eq:cert_general} and take the max of these upper bounds.

% \avi{Yet to modify the writing below, but hope the above explanation sounds reasomable as to why the max outside works.}
% which is a Gaussian distribution with covariance $\eta^2 \bfS$.


Choosing $\lambda\br{\bftheta} = \bftheta^\top \bfA \bftheta + \bftheta^\top \bfb$ to be a quadratic function, we derive the certificate for a fixed $\sigma$:
\begin{align*}
    &\textrm{OPT} = \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta : \|\bftheta\|_{2} \leq \frac{1}{\sigma}}} \epsilon\br{ \lambda\br{F\br{\bftheta, \bfzadv}}} + (1-\epsilon)\ExP{\bfz \sim \Pdata}{ \lambda\br{F\br{\bftheta, \bfz}}} -\lambda\br{\bftheta}+\ell_{\textrm{adv}}\br{\bftheta} \\
     &= \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta : \|\bftheta\|_{2} \leq \frac{1}{\sigma}}} [(1 - \sigma \eta)^2 - 1] \bftheta^\top \bfA \bftheta + \epsilon \eta^2 \mathbb{I}[\bftheta^\top \bfzadv \leq 1] {\bfzadv}^\top \bfA \bfzadv +   2 \epsilon (1 - \sigma \eta) \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bftheta^\top \bfA  \bfzadv  \\ & + \epsilon \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bfb^\top\bfzadv] + (1 - \epsilon)\ExP{\bfz \sim \Pdata}{\eta^2 \mathbb{I}[\bftheta^\top \bfz \leq 1] \bfz^\top \bfA \bfz + 2 \eta (1 - \sigma \eta) \mathbb{I}[\bftheta^\top \bfz \leq 1] \bftheta^\top \bfA \bfz + \eta \mathbb{I}[\bftheta^\top \bfz \leq 1] \bfb^\top  \bfz} \\& - \sigma \eta \bfb^\top \bftheta + \ExP{z \sim \Pdata}{\max \{0, 1 - \bftheta^\top \bfz\}}\\
     &\text{(Using sample average approximation for $\Pdata$ with data points from the training data set we get:)}\\
      &= \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta : \|\bftheta\|_{2} \leq \frac{1}{\sigma} \\ q_i \in \{0,1 \} \forall i \in [N]}} [(1 - \sigma \eta)^2 - 1] \bftheta^\top \bfA \bftheta + \epsilon \eta^2 \mathbb{I}[\bftheta^\top \bfzadv \leq 1] {\bfzadv}^\top \bfA \bfzadv +   2 \epsilon (1 - \sigma \eta) \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bftheta^\top \bfA  \bfzadv \\& + \epsilon \eta \mathbb{I}[\bftheta^\top \bfzadv \leq 1] \bfb^\top\bfzadv  + (1 - \epsilon) \frac{1}{N}\sum_{i \in [N]} \bigg [\eta^2 q_i \bfz_i^\top \bfA \bfz_i + 2 \eta (1 - \sigma \eta) q_i \bftheta^\top \bfA \bfz_i + \eta q_i \bfb^\top \bfz_i\bigg] + \\&  \frac{1}{N}\sum_{i \in [N]} \bigg[q_i (1 - \bftheta^\top \bfz_i)\bigg] - \sigma \eta \bfb^\top \bftheta \\
      &\text{such that} \; 1 - \bftheta^\top \bfz_i \leq (1 + \frac{d}{\sigma})q_i, 1 - \bftheta^\top \bfz_i \geq -(1 + \frac{d}{\sigma})(1 - q_i) \forall i \in [N].
\end{align*}
To get rid of the indicator variable $\mathbb{I}[\bftheta^\top \bfzadv \leq 1]$, we can write the certificate as the maximum of 2 optimization problems, (i) $\OPT_1$ with $\mathbb{I}[\bftheta^\top \bfzadv \leq 1] = 0$ and, (ii) $\OPT_2$ with $\mathbb{I}[\bftheta^\top \bfzadv \leq 1] = 1$. Note that the optimization problem in $\OPT_1$ doesn't have $\bfzadv$ as a decision variable. We first focus on the relaxations on the optimization problem in $\OPT_2$ and a bound on the optimization problem in $\OPT_1$ can be obtained by dropping the terms in $\bfzadv$ from $\OPT_2$.



\begin{align*}
\OPT_2 = & \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta : \|\bftheta\|_{2} \leq \frac{1}{\sigma} \\ q_i \in \{0,1\} \forall i \in [N]}} [(1 - \sigma \eta)^2 - 1] \bftheta^\top \bfA \bftheta + \epsilon \eta^2 {\bfzadv}^\top \bfA \bfzadv +   2 \epsilon (1 - \sigma \eta) \eta \bftheta^\top \bfA  \bfzadv + \epsilon \eta \bfb^\top\bfzadv \\& + (1 - \epsilon) \frac{1}{N}\sum_{i \in [N]} \bigg [\eta^2 q_i \bfz_i^\top \bfA \bfz_i + 2 \eta (1 - \sigma \eta) q_i \bftheta^\top \bfA \bfz_i + \eta q_i \bfb^\top \bfz_i\bigg] +  \frac{1}{N}\sum_{i \in [N]} \bigg[q_i (1 - \bftheta^\top \bfz_i)\bigg] - \sigma \eta \bfb^\top \bftheta \\
      &\text{such that} \; \bftheta^\top \bfzadv \leq 1, 1 - \bftheta^\top \bfz_i \leq (1 + \frac{1}{\sigma})q_i, 1 - \bftheta^\top \bfz_i \geq -(1 + \frac{1}{\sigma})(1 - q_i) \forall i \in [N]\\
      &\text{(Relaxing integer variables $q_i$'s to continuous variables)}\\
      \leq & \sup_{\substack{\bfzadv \in \mathcal{A} \\ \bftheta : \|\bftheta\|_{2} \leq \frac{1}{\sigma} \\ q_i \in [0,1] \forall i \in [N]}} [(1 - \sigma \eta)^2 - 1] \bftheta^\top \bfA \bftheta + \epsilon \eta^2 {\bfzadv}^\top \bfA \bfzadv +   2 \epsilon (1 - \sigma \eta) \eta \bftheta^\top \bfA  \bfzadv + \epsilon \eta \bfb^\top\bfzadv \\& + (1 - \epsilon) \frac{1}{N}\sum_{i \in [N]} \bigg [\eta^2 q_i \bfz_i^\top \bfA \bfz_i + 2 \eta (1 - \sigma \eta) q_i \bftheta^\top \bfA \bfz_i + \eta q_i \bfb^\top \bfz_i\bigg] +  \frac{1}{N}\sum_{i \in [N]} \bigg[q_i (1 - \bftheta^\top \bfz_i)\bigg] - \sigma \eta \bfb^\top \bftheta \\
      &\text{such that} \; \bftheta^\top \bfzadv \leq 1, 1 - \bftheta^\top \bfz_i \leq (1 + \frac{1}{\sigma})q_i, 1 - \bftheta^\top \bfz_i \geq -(1 + \frac{1}{\sigma})(1 - q_i) \forall i \in [N].
\end{align*}

      Using McCormick relaxations for bilinear terms $q_i\bftheta$, we get:
      % \begin{align*}
      % &\leq  \sup_{\substack{\bfzadv, \bftheta \in \R^d,  \bfq \in \R^N,\\ \bfw_1, \ldots, \bfw_N \in \R^d}} [(1 - \sigma \eta)^2 - 1] \bftheta^\top \bfA \bftheta + \epsilon \eta^2 {\bfzadv}^\top \bfA \bfzadv +   2 \epsilon (1 - \sigma \eta) \eta \bftheta^\top \bfA  \bfzadv + \epsilon \eta \bfb^\top\bfzadv \\& + (1 - \epsilon) \frac{1}{N}\sum_{i \in [N]} \bigg [\eta^2 q_i \bfz_i^\top \bfA \bfz_i + 2 \eta (1 - \sigma \eta) \bfw_i^\top \bfA \bfz_i + \eta q_i \bfb^\top \bfz_i\bigg] +  \frac{1}{N}\sum_{i \in [N]} \bigg[q_i  - \bfw_i^\top \bfz_i\bigg] - \sigma \eta \bfb^\top \bftheta \\
      % &\text{such that} \\& 1 - \bftheta^\top \bfz_i \leq (1 + \frac{d}{\sigma})q_i \;\forall i \in [N], \\&1 - \bftheta^\top \bfz_i \geq -(1 + \frac{d}{\sigma})(1 - q_i) \;\forall i \in [N], \\& \bfw_i \succeq -q_i \mathbf{1} / \sigma \;\forall i \in [N], \\&
      % \bfw_i \succeq \bftheta - (1 - q_i) \mathbf{1} / \sigma \;\forall i \in [N], \\&
      % \bfw_i \preceq q_i \mathbf{1} / \sigma \;\forall i \in [N], \\&
      % \bfw_i \preceq \bftheta + (1 - q_i) \mathbf{1} / \sigma \;\forall i \in [N],\\&
      % q_i \geq 0 \; \forall i \in [N],\\&
      % q_i \leq 1 \; \forall i \in [N],\\&
      % \bftheta^\top \bfzadv \leq 1, \\&
      % \bftheta \preceq \mathbf{1} / \sigma,\\&
      % \bftheta \succeq -\mathbf{1} / \sigma,\\&
      % \bfzadv \in \mathcal{A}.
      % \end{align*}
      % We vectorise the objective and constraints below. Let $\bfZ = \bmat{\bfz_1 \\ \vdots \\ \bfz_N} \in \R^{N \times d}$.
      % Define $\bfe$ as the matrix with the $i^{\rm th}$ column entries as all ones, and all other entries as 0.
%       \begin{align*}
%       &= \sup_{\substack{\bfzadv, \bftheta \in \R^d, \\ \bfq \in \R^N, \bfw \in \R^{dN}}} \bmat{\bftheta \\ \bfzadv}^\top\bmat{[(1 - \sigma \eta)^2 - 1] \bfA & \epsilon (1 - \sigma \eta) \eta \bfA\\ \epsilon (1 - \sigma \eta) \eta \bfA & \epsilon \eta^2 \bfA} \bmat{\bftheta \\ \bfzadv} +
%       \\& \bmat{-\sigma \eta \bfb \\ \epsilon \eta \bfb}^\top\bmat{\bftheta & \bfzadv} + \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_1^\top \bfA \bfz_1 + (1 - \epsilon) \eta \bfb^\top \bfz_1 + 1 \\ \vdots \\ (1 - \epsilon) \eta^2 \bfz_N^\top \bfA \bfz_N + (1 - \epsilon) \eta \bfb^\top \bfz_N + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_1 - \bfz_1 \\ \vdots \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_N - \bfz_N}^\top\bmat{\bfq & \bfw} \\
%       &\text{such that} \\& \bmat{\bfZ & \bf0} \bmat{\bftheta \\ \bfzadv} + \bmat{(1 + \frac{d}{\sigma}) \bfI &\bf0} \bmat{\bfq \\ \bfw} \succeq \bf1,\\& \bmat{-\bfZ & \bf0} \bmat{\bftheta \\ \bfzadv} + \bmat{-(1 + \frac{d}{\sigma}) \bfI &\bf0} \bmat{\bfq \\ \bfw} \succeq -(2 + \frac{d}{\sigma})\bf1, \\& \bmat{\frac{1}{\sigma}\bf1 & \ldots & \bf0 & \bfI  & \ldots & \bf0\\
%       \bf0 & \frac{1}{\sigma} \bf1 & \ldots & \bf0 & \bfI & \ldots\\
%       \vdots & \vdots &\vdots & \vdots & \vdots &\vdots\\
%       \bf0 & \ldots & \frac{1}{\sigma}\bf1 & \bf0 & \ldots & \bfI} \bmat{\bfq \\ \bfw} \succeq \bf0,  \\& -\bmat{\bfI & \bf0\\ \vdots & \vdots \\ \bfI & \bf0} \bmat{\bftheta \\ \bfzadv} + \bmat{-\frac{1}{\sigma}\bf1 & \ldots & \bf0 & \bfI  & \ldots & \bf0\\
%       \bf0 & -\frac{1}{\sigma} \bf1 & \ldots & \bf0 & \bfI & \ldots\\
%       \vdots & \vdots &\vdots & \vdots & \vdots &\vdots\\
%       \bf0 & \ldots & -\frac{1}{\sigma}\bf1 & \bf0 & \ldots & \bfI} \bmat{\bfq \\ \bfw} \succeq - \frac{1}{\sigma}\bfI, \\& \bmat{\frac{1}{\sigma}\bf1 & \ldots & \bf0 & -\bfI  & \ldots & \bf0\\
%       \bf0 & \frac{1}{\sigma} \bf1 & \ldots & \bf0 & -\bfI & \ldots\\
%       \vdots & \vdots &\vdots & \vdots & \vdots &\vdots\\
%       \bf0 & \ldots & \frac{1}{\sigma}\bf1 & \bf0 & \ldots & -\bfI} \bmat{\bfq \\ \bfw} \succeq \bf0,
%       \\& \bmat{\bfI & \bf0\\ \vdots & \vdots \\ \bfI & \bf0} \bmat{\bftheta \\ \bfzadv} - \bmat{\frac{1}{\sigma}\bf1 & \ldots & \bf0 & \bfI  & \ldots & \bf0\\
%       \bf0 & \frac{1}{\sigma} \bf1 & \ldots & \bf0 & \bfI & \ldots\\
%       \vdots & \vdots &\vdots & \vdots & \vdots &\vdots\\
%       \bf0 & \ldots & \frac{1}{\sigma}\bf1 & \bf0 & \ldots & \bfI} \bmat{\bfq \\ \bfw} \succeq - \frac{1}{\sigma}\bfI,
%       \\& \bmat{\bfI & \bf0} \bmat{\bfq \\ \bfw} \succeq \bf0,
%       \\& -\bmat{\bfI & \bf0} \bmat{\bfq \\ \bfw} \succeq -\bf1,
%       \\&\bftheta^\top \bfzadv \leq 1, \\&
%       \bftheta \preceq \mathbf{1} / \sigma,\\&
%       \bftheta \succeq -\mathbf{1} / \sigma,\\&
%       \bfzadv \in \mathcal{A}.
% \end{align*}
\begin{align*}
      \OPT_2 \leq& \sup_{\substack{\bfzadv \in \mathcal{A}, \bftheta : \|\bftheta\|_2 \leq \frac{1}{\sigma}, \\ \bfq \in \R^N, \bfw \in \R^{dN}}} -\bmat{\bftheta \\ \bfzadv}^\top\bmat{[1 - (1 - \sigma \eta)^2] \bfA & -\epsilon (1 - \sigma \eta) \eta \bfA\\ -\epsilon (1 - \sigma \eta) \eta \bfA & -\epsilon \eta^2 \bfA} \bmat{\bftheta \\ \bfzadv} +
      \\& \bmat{-\sigma \eta \bfb \\ \epsilon \eta \bfb}^\top\bmat{\bftheta & \bfzadv} + \frac{1}{N} \sum_{1=1}^N \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i}^\top\bmat{q_i & \bfw_i} \\
      &\text{such that} \\& \bfz_i^\top \bftheta + (1 + \frac{1}{\sigma}) q_i \geq 1 \; \forall i \in [N],
      \\& -\bfz_i^\top \bftheta -(1 + \frac{1}{\sigma}) q_i  \geq -(2 + \frac{1}{\sigma}) \; \forall i \in [N],
          \\& q_i \mathbf{1} / \sigma + \bfw_i \succeq \mathbf{0} \;\forall i \in [N], \\&
      -\bftheta - q_i \mathbf{1} / \sigma + \bfw_i \succeq  - \mathbf{1} / \sigma \;\forall i \in [N], \\&
       q_i \mathbf{1} / \sigma -\bfw_i \succeq \mathbf{0} \;\forall i \in [N], \\&
        \bftheta  - q_i \mathbf{1} / \sigma - \bfw_i \succeq -\mathbf{1} / \sigma\;\forall i \in [N],
      \\& \bfq \succeq \bf0,
      \\& -\bfq \succeq -\bf1,
      \\&1 - \bftheta^\top \bfzadv \geq 0.
      % \\&
      % -\bftheta \succeq -\mathbf{1} / \sigma,\\&
      % \bftheta \succeq -\mathbf{1} / \sigma,\\&
      % \bfzadv \in \mathcal{A}.
\end{align*}
% Note that the objective and constraints are quadratic only in $\bftheta, \bfzadv$, and linear in $\bfq, \bfw_1, \ldots, \bfw_N$.
% Writing the objective in a vectorized form we get:
% As long as the typical set $\mathcal{A}$ is linear ($\ell_{\infty}$ constraints) or quadratic ($\ell_2$ constraints) in $\bfzadv$, the above problem is a Quadratic Program.
% and we can write its dual as an infimum over various lagrangian parameters. Total number of lagrangian parameters to optimize over scales as $\mathcal{O}(Nd)$.

% Define affine functions in $\bfA, \bfb$:
% \begin{align*}
% &\bfs(\bfz_i, \bfA, \bfb) = \frac{1}{N} \bmat{\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right)\\\left( 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i\right)} \in \R^{d + 1}.
% \end{align*}
From the Mccormick envelope constraints, and the constraints on $\bftheta$ and $\bfq$, we can derive that $\bfw_i \geq 0 \; \forall i \in [N]$.
Plugging $\mathcal{A} = \{\bfzadv | \|\bfzadv\|_{2} \leq 1\}$. Now we utilize Lemma~\ref{lem:QPdual} to write the dual problem as:
% (\avi{Yet to account for $\mathcal{A}$}):
% \begin{align*}
%     &= \inf_{\substack{\nu_1, \nu_2 \in \R^N \\ \nu_3, \nu_4, \nu_5, \nu_6 \in \R^{2Nd} \\ \nu_7, \nu_8 \in \R^N, \\
%     \nu_9 \in \R, \\ \nu_{10}, \nu_{11} \in \R^d}} \frac{1}{4}\|\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((-\nu_{1i} + \nu_{2i}) \bfz_i + \nu_{4i} -\nu_{6i} \right) + \nu_{10} - \nu_{11}\\ \epsilon \eta \bfb}\|^2_{\bfD^{-1}}\\
%       & +\nu_1^\top \mathbf{1} - (2 + \frac{d}{\sigma}) \nu_2^\top \mathbf{1} - \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + 2 \nu_9\\
%       & \text{such that} \\
%       &  (1 + \frac{d}{\sigma}) \nu_{1i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       & -(1 + \frac{d}{\sigma}) \nu_{2i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       &  \nu_{7i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       &  -\nu_{8i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       & \bfD \succeq 0
% \end{align*}
% where
% \begin{align*}
%     \bfD = \bmat{[1 - (1 - \sigma \eta)^2] \bfA & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 & -\epsilon \eta^2 \bfA}.
% \end{align*}
\begin{align*}
    & \inf_{\substack{\nu_1, \nu_2, \nu_7 \in \R^N_{+} \\ \nu_3, \nu_4, \nu_5, \nu_6 \in \R^{N \times d}_{+} \\ \nu_8, \nu_9, \nu_{10} \in \R_{+}}} \frac{1}{4}\|\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) \\ \epsilon \eta \bfb} \|^2_{\bfD^{-1}}\\
      & -\nu_1^\top \mathbf{1} + (2 + \frac{1}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \mathbf{1}^\top \nu_7 + \frac{\nu_{8}}{\sigma^2} + 2 \nu_9 + \nu_{10}.\\
      & \text{such that} \\
      % & \bmat{(1 + \frac{d}{\sigma}) \nu_{1i} \\ \mathbf{0}} + \bmat{-(1 + \frac{d}{\sigma}) \nu_{2i} \\ 0} + \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} + \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} +  \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} +
      % \\& \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} + \bmat{\nu_7 - \nu_8 \\ \mathbf{0}} + \bfs(\bfz_i, \bfA, \bfb) = \mathbf{0} \;\forall i \in [N].\\
      & \bmat{(1 + \frac{1}{\sigma}) (\nu_{1i} - \nu_{2i})  - \frac{\mathbf{1}^\top(\nu_{4i} + \nu_{6i})}{\sigma} - \nu_{7i}\\ \nu_{3i} + \nu_{4i} - \nu_{5i} - \nu_{6i}} + \bfs(\bfz_i, \bfA, \bfb) \preceq \mathbf{0} \;\forall i \in [N].\\
      % &  (1 + \frac{d}{\sigma}) \nu_{1i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
      % & -(1 + \frac{d}{\sigma}) \nu_{2i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
      % & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
      % & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
      % & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
      % & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
      % &  \nu_{7i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
      % &  -\nu_{8i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
      & \bfD \succeq 0
\end{align*}
where
\begin{align*}
    \bfD = \bmat{[1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 & -\epsilon \eta^2 \bfA + \nu_{10}\bfI}.
\end{align*}
% \avi{Some thoughts:
% \begin{enumerate}
%     \item Consider the case $\epsilon=0$. We should expect certificate should equal the benign loss. If we restrict the class of $\lambda$, does there exist $\lambda$ such that $\ExP{\bfz \sim \Pdata}{ \lambda\br{F\br{\bftheta, \bfz}}} \leq \lambda\br{\bftheta} \forall \theta$ and equality holds for $\theta^\ast$ (which is the optimal parameter).
% \end{enumerate}}
% \djcomment{You need the $\bfD$ matrix to be PSD, otherwise the supremum is infinite. You should enforce this as a constraint in the infimum above.}\\
We use $\nu \succeq \mathbf{0}$ to compactly denote $\{\nu_1, \ldots, \nu_{10}\}$. Define the following affine functions in $\nu, \bfA, \bfb$:
\begin{align*}
    &\bfp(\bfb, \nu) = \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) \\ \epsilon \eta \bfb} \in \R^{2d},\\
    &\bfD(\bfA, \nu) = \bmat{[1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 & -\epsilon \eta^2 \bfA + \nu_{10}\bfI} \in \mathbb{S}_{+}^{2d},\\
    % {2d \times 2d}\\
    &q(\nu) =-\nu_1^\top \mathbf{1} + (2 + \frac{1}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \mathbf{1}^\top \nu_7 + \frac{\nu_{8}}{\sigma^2} + 2 \nu_9 + \nu_{10} \in \R, \\
    &\bfr_i(\nu) = \bmat{(1 + \frac{1}{\sigma}) (\nu_{1i} - \nu_{2i})  - \frac{\mathbf{1}^\top(\nu_{4i} + \nu_{6i})}{\sigma} - \nu_{7i}\\ \nu_{3i} + \nu_{4i} - \nu_{5i} - \nu_{6i}} \in \R^{d + 1},\\
      &\bfs(\bfz_i, \bfA, \bfb) = \frac{1}{N} \bmat{\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right)\\\left( 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i\right)} \in \R^{d + 1}.
\end{align*}
The certificate can thus be written compactly as:
\begin{align*}
    \OPT_2 \leq &\inf_{\nu \succeq \mathbf{0}} \|\bfp(\bfb, \nu)\|^2_{\bfD(\bfA, \nu)^{-1}} + q(\nu) \\
    &\text{such that} \\
    & \bfr_i(\nu) + \bfs(z_i, \bfA, \bfb) \preceq 0 \; \forall i \in [N], \bfD(\bfA, \nu) \succ \mathbf{0}.
\end{align*}
Similarly $\OPT_1$ can be upper bounded as:
\begin{align*}
    \OPT_1 \leq &\inf_{\nu \succeq \mathbf{0}} \|\bfp'(\bfb, \nu)\|^2_{\bfD'(\bfA, \nu)^{-1}} + q'(\nu) \\
    &\text{such that} \\
    & \bfr_i(\nu) + \bfs(z_i, \bfA, \bfb) \preceq 0 \; \forall i \in [N], \bfD'(\bfA, \nu) \succ \mathbf{0}.
\end{align*}
where:
\begin{align*}
    &\bfp'(\bfb, \nu) = \frac{1}{2}\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i}) \bfz_i - \nu_{4i} +\nu_{6i} \right)} \in \R^{d},\\
    &\bfD'(\bfA, \nu) = [1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI  \in \mathbb{S}_{+}^{d},\\
    % {2d \times 2d}\\
    &q'(\nu) =-\nu_1^\top \mathbf{1} + (1 + \frac{1}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \frac{\nu_{8}}{\sigma^2} \in \R. \\
\end{align*}
\end{proof}

\begin{align*}
      \OPT_2 \leq& \sup_{\substack{\bfzadv \in \mathcal{A}, \bftheta : \|\bftheta\|_2 \leq \frac{1}{\sigma}, \\ \bfq \in \R^N, \bfw \in \R^{dN}}} -\bmat{\bftheta \\ \bfzadv}^\top\bmat{[1 - (1 - \sigma \eta)^2] \bfA & -\epsilon (1 - \sigma \eta) \eta \bfA\\ -\epsilon (1 - \sigma \eta) \eta \bfA & -\epsilon \eta^2 \bfA} \bmat{\bftheta \\ \bfzadv} +
      \\& \bmat{-\sigma \eta \bfb \\ \epsilon \eta \bfb}^\top\bmat{\bftheta \\ \bfzadv} + \frac{1}{N} \sum_{1=1}^N \bmat{1 \\ (1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i}^\top\bmat{p_i \\ q_i \\ \bfw_i} \\
      &\text{such that} \\& \bfz_i^\top \bftheta + (1 + \frac{1}{\sigma}) q_i \geq 1 \; \forall i \in [N],
      \\& -\bfz_i^\top \bftheta -(1 + \frac{1}{\sigma}) q_i  \geq -(2 + \frac{1}{\sigma}) \; \forall i \in [N],
          \\& q_i \mathbf{1} / \sigma + \bfw_i \succeq \mathbf{0} \;\forall i \in [N], \\&
      -\bftheta - q_i \mathbf{1} / \sigma + \bfw_i \succeq  - \mathbf{1} / \sigma \;\forall i \in [N], \\&
       q_i \mathbf{1} / \sigma -\bfw_i \succeq \mathbf{0} \;\forall i \in [N], \\&
        \bftheta  - q_i \mathbf{1} / \sigma - \bfw_i \succeq -\mathbf{1} / \sigma\;\forall i \in [N],
      \\& \bfq \succeq \bf0,
      \\& -\bfq \succeq -\bf1,
      \\& \bfp \succeq \bf0,
      \\& -\bfp \succeq -\bf1,
      \\&1 - \bftheta^\top \bfzadv \geq 0,\\
      & \bftheta^\top \bfz_i + \frac{1}{\sigma} p_i \geq 0\\
      &- \bftheta^\top \bfz_i - \frac{1}{\sigma} p_i \geq -\frac{1}{\sigma}
      % \\&
      % -\bftheta \succeq -\mathbf{1} / \sigma,\\&
      % \bftheta \succeq -\mathbf{1} / \sigma,\\&
      % \bfzadv \in \mathcal{A}.
\end{align*}
% \begin{align*}
%     & \inf_{\substack{\nu_1, \nu_2, \nu_7, \nu_{11}, \nu_{12}, \nu_{13} \in \R^N_{+} \\ \nu_3, \nu_4, \nu_5, \nu_6 \in \R^{N \times d}_{+} \\ \nu_8, \nu_9, \nu_{10} \in \R_{+}}} \frac{1}{4}\|\bmat{-\sigma \eta \bfb + \sum_{i \in [N]} \left((\nu_{1i} - \nu_{2i} + \nu_{11i} - \nu_{12i}) \bfz_i - \nu_{4i} +\nu_{6i} \right) \\ \epsilon \eta \bfb} \|^2_{\bfD^{-1}}\\
%       & -\nu_1^\top \mathbf{1} + (2 + \frac{1}{\sigma}) \nu_2^\top \mathbf{1} + \frac{(\nu_4 + \nu_6)^\top \bf1}{\sigma} + \mathbf{1}^\top \nu_7 + \frac{\nu_{8}}{\sigma^2} + 2 \nu_9 + \nu_{10} + \frac{\mathbf{1}^\top\nu_{12}}{\sigma} + \mathbf{1}^\top \nu_{13}.\\
%       & \text{such that} \\
%       % & \bmat{(1 + \frac{d}{\sigma}) \nu_{1i} \\ \mathbf{0}} + \bmat{-(1 + \frac{d}{\sigma}) \nu_{2i} \\ 0} + \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} + \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} +  \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} +
%       % \\& \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} + \bmat{\nu_7 - \nu_8 \\ \mathbf{0}} + \bfs(\bfz_i, \bfA, \bfb) = \mathbf{0} \;\forall i \in [N].\\
%       & \bmat{\frac{\nu_{11i} - \nu_{12i}}{\sigma} - \nu_{13i} \\ (1 + \frac{1}{\sigma}) (\nu_{1i} - \nu_{2i})  - \frac{\mathbf{1}^\top(\nu_{4i} + \nu_{6i})}{\sigma} - \nu_{7i}\\ \nu_{3i} + \nu_{4i} - \nu_{5i} - \nu_{6i}} + \frac{1}{N} \bmat{1 \\ (1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i} \preceq \mathbf{0} \;\forall i \in [N].\\
%       % &  (1 + \frac{d}{\sigma}) \nu_{1i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       % & -(1 + \frac{d}{\sigma}) \nu_{2i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       % & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{3i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       % & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & \bfI}^\top  \nu_{4i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       % & \bmat{\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{5i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       % & \bmat{-\frac{\mathbf{1}}{\sigma} & \mathbf{0} \\ \mathbf{0} & -\bfI}^\top  \nu_{6i} = \frac{1}{N} \bmat{(1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1 \\ 2(1 - \epsilon) \eta (1 - \sigma \eta) \bfA \bfz_i - \bfz_i} \; \forall i \in [N],\\
%       % &  \nu_{7i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       % &  -\nu_{8i} = \frac{1}{N}\left((1 - \epsilon) \eta^2 \bfz_i^\top \bfA \bfz_i + (1 - \epsilon) \eta \bfb^\top \bfz_i + 1\right) \; \forall i \in [N],\\
%       & \bfD \succeq 0
% \end{align*}
% where
% \begin{align*}
%     \bfD = \bmat{[1 - (1 - \sigma \eta)^2] \bfA + \nu_8 \bfI & -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9\\ -\epsilon (1 - \sigma \eta) \eta \bfA + \nu_9 & -\epsilon \eta^2 \bfA + \nu_{10}\bfI}.
% \end{align*}
\section{Experiment Details}\label{appendix:experiment}
\subsection{Vision Datasets}
We use the following pre-processing for all datasets. Use pretrained ResNet18 on ImageNET to extract features as the output of the pre-final layer. Perform Singular Value Decomposition (SVD) to select the top $d=30$ directions with largest feature variation. We normalise our dataset to ensure zero mean, append $1$ to each feature vector to capture the bias term, and scale to ensure each datapoint has $\ell_2$ norm less than or equal to 1. We multiply the $\{+1, -1\}$ labels to our features.

We split the training set into $\Pdata$ and $\Ptarget$ which are used to compute the certificates. For the simulation of the learning algorithms with various attacks, the same $\Pdata$ and $\Ptarget$ are used. The evaluation of these learning algorithms is done on a test set which is distributionally similar to $\Ptarget$.

\subsection{Reward Modeling}
We use the following pre-processing the HelpSteer dataset. We pass as inputs a concatenated text of the prompt, response pair to a pretrained BERT model to extract features as the output of the pre-final layer. We then perform Singular Value Decomposition (SVD) to select the top $d=30$ directions with largest feature variation. We normalise our dataset to ensure zero mean, append $1$ to each feature vector to capture the bias term, and scale to ensure each datapoint has $\ell_2$ norm less than or equal to 1.

The scores on each attribute in the raw dataset are natural numbers between $[0, 4]$. We linearly scale it to lie in the range $[-1, 1]$. We then multiply the labels to our features.

We split the training set into $\Pdata$ and $\Ptarget$ which are used to compute the certificates. For the simulation of the learning algorithms with various attacks, the same $\Pdata$ and $\Ptarget$ are used. The evaluation of these learning algorithms is done on a test set which is distributionally similar to $\Ptarget$.
\end{document}
