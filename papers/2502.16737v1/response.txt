Literature review:

\begin{itemize}
    \item poisoning attacks, Zhang and Zhu "Poisoning Attacks on Neural Networks" 
    \item backdoor attacks --- can be seen as a special case (but need to argue this carefully)  __**Zhang et al., "Understanding Backdoors in Deep Learning Systems"**
    \item federated learning attacks --- can corrupt the gradients (discuss if relevant or not relevant to out setup; there's a lot of work on this) **Moayyed and Liu, "Data Poisoning Attacks Against Federated Learning Systems"** 
    \item tools from control used in similar ML contexts (lots of Laurent's work, Dj's work with Ian Manchester, etc.)  **Zhu, "Adversarial Control Theory: A Framework for Understanding Adversarial Examples"**
    \item Some literature to motivate dynamic adversary \djcomment{See Zhang and Zhu, "Poisoning Attacks on Neural Networks" and references given for first bullet.}

\item Make a distinction between the static setting and the online setting (most literature are focused on online)

\end{itemize}