\section{Literature Review}
Literature review:

\begin{itemize}
    \item poisoning attacks, Laurent's L4DC paper with Xuezhou Zhang and Jerry Zhu (Xuezhou did the examples, can ask him). Are these examples convincing for an ML conference though (Neurips)? It's a different audience and review process than L4DC. We may need to explore more ML-ish poisoning papers for examples and datasets.\djcomment{I think these papers \citep{wang2019investigation}\citep{wang2018data} actually are probably even more representative, and do consider the dynamic adversary setting as well. These may be a good starting point. }
    \item backdoor attacks --- can be seen as a special case (but need to argue this carefully) \djcomment{ \citep{goldblum2022dataset} provides a thorough review although not from a mathematical perspective. The key difference IMO is if we say the loss is $\ell\br{\bftheta, \bfz}$ where $\bftheta$ are learned model parameters and $\bfz$ is the input datapoint we are asked to make a prediction on (a feature-label pair for example), in the standard data poisoning setting, we assume only access to (parts) of the training data, while in the backdoor setting, we assume access to both the training data and $\bfz$, so the attacker can corrupt both the training data and $\bfz$ by inducing an adversiarial trigger or backdoor into $\bfz$ (for example, in an LLM, maybe this is any prompt that has the word ``Abracadabra'' in it). In our setting though, since we allow the loss optimized by the adversary to be arbitrary, we can simply choose it to be
    \[\ExP{\bfz \sim \Ptarget}{\ell\br{\bftheta, \bfz}}\]
    where $\Ptarget$ is the distribution of inputs that contains the trigger (for example the distribution over all prompts conditioned on the prompt containing the word ``Abracadabra'' in it)
    }
    \item federated learning attacks --- can corrupt the gradients (discuss if relevant or not relevant to out setup; there's a lot of work on this)
    \item tools from control used in similar ML contexts (lots of Laurent's work, Dj's work with Ian Manchester, etc.) \djcomment{\citep{wang2024monotone, wang2023direct}}
    \item Some literature to motivate dynamic adversary \djcomment{See references given for first bullet.}

\item Make a distinction between the static setting and the online setting (most literature are focused on online)

\end{itemize}
\end{comment}