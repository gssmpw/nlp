\section{Introduction}
\label{sec:intro}


The co-design of morphology and control in robots is
important because robots perform better when their physical layout is optimized for their intended niche---%
like a fish out of water, a good body in one domain can obstruct the acquisition of intelligent behavior in another, if it is unable to evolve.
However, over the past three decades of research, despite exponential increases in computing power, surprisingly little tangible progress has been achieved beyond the very first co-designed robots \cite{sims1994competition}.
This stagnation is due in part to the nested complexity of evolving a robotâ€™s morphology and learning a bespoke controller for every morphological variant.
Because controllers are usually optimized in non-differentiable  simulations using reinforcement learning (RL), large amounts of training data are needed to effectively learn a single controller for a single morphology, a cost that is compounded by repeatedly relearning new controllers as the robot's morphology changes throughout evolution.


As a result, the overwhelming majority of prior work 
has been limited to 
small numbers of
morphological simple robots 
that exhibit simple behaviors 
in simple environments.
Even with simplifying assumptions that significantly speed simulation,
such as constraining the design space to infinitely
rigid ``stick figures'' composed of less than a dozen body parts,
there is usually only enough time to explore a few thousand morphologies 
\cite{zhao2020robogrammar,gupta2021embodied,yuan2022transformact}.
Others have relaxed this constraint by considering more flexible bodies composed of many deformable cells \cite{cheney2018scalable,kriegman2020xenobots,li2025generating}, but due in part to the increased computational burden of simulating soft materials, these robots have had lower motoric complexity (fewer independent motors)
and have been much less intelligent (completely unresponsive to external stimuli) 
compared to their rigid bodied counterparts.
Often the robots are restricted to two dimensional worlds \mbox{\cite{medvet2021biodiversity,wang2023preco,strgar2024evolution}.}

Inspired by the remarkable success of large-scale pretrained models in computer vision and natural language processing, we here pretrain a universal controller across millions of complex body plans using gradient information from differentiable simulation, 
averaging gradients across variations in the robot's body, world and goal (Fig.~\ref{fig:intro-robot-teaser}).
% and then use the pretrained model as a prior for zero- and few shot morphology evolution in challenging task environments.
% 
Armed with a universal controller, evolution can now iteratively improve the robot's morphology, 
% for challenging task environments
and the controller can be rapidly finetuned for the current population with simulation gradients (Fig.~\ref{fig:intro-dataflow}).
This also enables the successful recombination of designs (a.k.a.~crossover; Fig.~\ref{fig:intro-phylogenetic-tree}),
a hallmark of biological evolution 
% \cite{barton1998sex} 
and of human engineering that has yet to be convincingly demonstrated in robots.

Indeed there is a tacit assumption in robotics 
that crossover---%
the combining of two parent designs to produce offspring---%
is so unlikely to produce viable offspring, 
that it is better to omit crossover altogether 
and focus entirely on small mutations 
that slightly alter a single design parent to produce offspring.
% 
While instances have been reported in which two morphologies were combined using crossover to produce a new morphology
\cite{sims1994competition,bongard2001repeated,hiller2010evolving,strgar2024evolution},
it was not clear if crossover ever produced offspring with equal or better fitness than either one of their parents---or if the recombined designs were even better than randomly generated robots.
Here we show how a pretrained universal controller can unlock successful crossover of robots.


Several cases have been reported in the literature in which RL was used to approximate a universal control policy gradient across a small dataset of
previously-designed 
\cite{huang2020one,gupta2022metamorph,bohlinger2024one}
or 
simultaneously co-designed \cite{wang2023preco,li2025generating}
morphologies.
However, the inefficiencies of policy training without recourse to gradient information precluded large scale pretraining.
% 
As we detail below, co-designing morphology and universal control simultaneously from scratch can, and without careful consideration almost certainly will 
result in diversity collapse, inhibiting co-design by reducing it to policy training for a single design.
%
Others
\cite{ma2021diffaqua,matthews2023efficient,yuhn20234d,cochevelou2023differentiable,strgar2024evolution}
have utilized first-order gradients from differentiable simulation 
to speed co-design.
But, a custom controller still needed to be learned for each morphology,
and the resulting robots could only exhibit rote behaviors,
such as locomotion in a straight line. 

Here we demonstrate a more scalable approach that starts 
with large-scale morphological pretraining 
in differentiable simulation
and yields 
a morphology-agnostic controller 
for adaptive sensor-guided behavior
in complex robots 
with thousands of independent motors.



% just incase: wang pre co is RL no gradients; they did not do large scale pretraining (pop size 12), the robots are 2D, 9 cells, very small, the entire design space can be exhaustively searched; they have not external perception, they memorize their environments in training because the environments are static and meorizable, they then memorize the test environments at inference by fine tuning; they intentionally collapse diversity