\section{Related Work}
\label{related_work}

Recently, significant advancements across various research directions have been made to enhance the mathematical capabilities of large language models (LLMs). One of the major ones has been along the prompting and thinking strategies such as Chain-of-Thought (COT) method **Sahakian, "Can a Large Language Model be Aware?"** that has shown to evoke multi-step thinking in LLMs before arriving at the answer. These methods struggle with complex and symbolic computations. For this, PAL **Lample, "Program Induction by Question Answering"** \& POT **Poliak, "POT: A Program-Oriented Thinking Framework"** suggest making LLMs perform reasoning by writing program and offloading the computations to code interpreter. TIR-ToRA **Wang, "TIR-ToRA: Transforming Inference into Reasoning"** does rationale generation in one go beforehand and then it codes the entire solution to get the final answer using single code block. Additionally, in case of an error, it tries to re-attempt in similar format. Another line of research has been around pre-training and supervised fine-tuning (SFT). Multiple studies **Brown, "Language Models Play D&D"** have shown pre-training LLMs on high-quality maths tokens results in increased mathematical knowledge and reasoning abilities. Recent approaches **Rajani, "Query-Adversarial Training for Self-Supervised Learning of Reasoning Abilities"** have tried query/problem augmentation along with creating synthetic reasoning paths/trajectories using a teacher model like GPT4  for SFT. These methods showed significant improvement in the math reasoning abilities of the model.  Also, some studies **Li, "Learning to Read Comprehension Questions by Learning to Ask Yourself"** provide an alternative to manual annotations for process supervision.
      
Previous program generation-based methods such as PAL, \& TIR-ToRA have followed a static approach and hence struggle with complex problems. These methods typically attempt to solve the problem in a single, large code block or with minimal iterations(in case of TIR-ToRA). But they lack producing/exploring any intermediate steps. 
SBSC dynamically decomposes complex problems into a sequence of manageable sub-tasks. SBSC embodies exploration by discovering new sub-tasks basis the previous sub-tasks. SBSC is highly focused as these sub-tasks are explored individually via program generation to generate intermediate outputs. SBSC has superior step-level self-correction abilities. Hence due to these major advantages, SBSC achieves significant improvement over TIR-ToRA \& PAL across 4 different benchmarks as can be seen in Table \ref{tab:mainresults}.