\section{Related Work}
\label{related_work}

Recently, significant advancements across various research directions have been made to enhance the mathematical capabilities of large language models (LLMs). One of the major ones has been along the prompting and thinking strategies such as Chain-of-Thought (COT) method \citep{Wei2022ChainOT, Kojima2022LargeLM} that has shown to evoke multi-step thinking in LLMs before arriving at the answer. These methods struggle with complex and symbolic computations. For this, PAL \citep{Gao2022PALPL} \& POT \citep{Chen2022ProgramOT} suggest making LLMs perform reasoning by writing program and offloading the computations to code interpreter. TIR-ToRA \citep{Gou2023ToRAAT} does rationale generation in one go beforehand and then it codes the entire solution to get the final answer using single code block. Additionally, in case of an error, it tries to re-attempt in similar format. Another line of research has been around pre-training and supervised fine-tuning (SFT). Multiple studies \citep{Shao2024DeepSeekMathPT, Ying2024InternLMMathOM, DeepSeekAI2024DeepSeekCoderV2BT, Azerbayev2023LlemmaAO, Lewkowycz2022SolvingQR, Paster2023OpenWebMathAO, Taylor2022GalacticaAL} have shown pre-training LLMs on high-quality maths tokens results in increased mathematical knowledge and reasoning abilities. Recent approaches \citep{Yu2023MetaMathBY, Gou2023ToRAAT, Yue2023MAmmoTHBM, Wang2023MathCoderSC, Shao2024DeepSeekMathPT, Toshniwal2024OpenMathInstruct1A1, Mitra2024OrcaMathUT, numina_math_7b, Yin2024MuMathCodeCT, Tong2024DARTMathDR} have tried query/problem augmentation along with creating synthetic reasoning paths/trajectories using a teacher model like GPT4 \citep{Achiam2023GPT4TR} for SFT. These methods showed significant improvement in the math reasoning abilities of the model.  Also, some studies \citep{Wang2023MathShepherdVA, Yu2023OVMOV, Xi2024TrainingLL, Chen2024AlphaMathAZ, Lightman2023LetsVS} provide an alternative to manual annotations for process supervision \citep{lightman2023letsverifystepstep}.
      
Previous program generation-based methods such as PAL, \& TIR-ToRA have followed a static approach and hence struggle with complex problems. These methods typically attempt to solve the problem in a single, large code block or with minimal iterations(in case of TIR-ToRA). But they lack producing/exploring any intermediate steps. 
SBSC dynamically decomposes complex problems into a sequence of manageable sub-tasks. SBSC embodies exploration by discovering new sub-tasks basis the previous sub-tasks. SBSC is highly focused as these sub-tasks are explored individually via program generation to generate intermediate outputs. SBSC has superior step-level self-correction abilities. Hence due to these major advantages, SBSC achieves significant improvement over TIR-ToRA \& PAL across 4 different benchmarks as can be seen in Table \ref{tab:mainresults}.