\section{Related Works}
\label{sec:related_works}
\textbf{LLM-based Search Methods}.  
Recent advancements in test-time compute scaling have sparked growing interest in methods that enable LLMs to simulate search processes and ``think longer'' instead of directly generating answers in one pass. Several works~\cite{wang2023selfconsistency,hao2023reasoning,feng2023alphazerolike,yao2023tree,zhao2024large,besta2024got,wang2024litesearch,snell2024scaling} adopt such approaches to enhance LLMs' problem-solving capabilities. 
Despite their innovation, these methods rely solely on LLMs' intrinsic knowledge, often leading to unstable performance due to limitations in LLMs' reasoning capabilities. In contrast, our proposed \method integrates LLMs with traditional search strategies, ensuring both completeness and efficiency in solving decision-making tasks. More LLM-based search methods are reviewed in Appendix~\ref{appendix:related_works_LLM_search}.

\noindent \textbf{Traditional Search Methods}.  
Inspired by the recent discussion about search and learning~\cite{Sutton2019BitterLesson}, which underscores the enduring value of general-purpose strategies that scale with computational power, we investigate how to better integrate search with learning to leverage the strengths of both paradigms. Our \method and \cmethod draw inspiration from traditional search while incorporating LLM-guided reasoning to reduce search space, thereby significantly enhancing efficiency without sacrificing completeness. More details of the related works are in Appendix~\ref{appendix:related_works}. 