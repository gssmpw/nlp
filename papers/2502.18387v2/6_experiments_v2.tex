\section{Experiments}
In this section, we conduct experiments to answer the following research questions: (\textbf{RQ1}) How effectively and efficiently does \method solve tasks compared to existing search methods? (\textbf{RQ2}) How efficient is \method when conducting a rigorously complete search? (\textbf{RQ3}) Do LLMs inherently possess the capability for self-search?
\begin{table*}[t]
\caption{Results of different search methods across three tasks. Specifically, the results of LLM-based search baselines are reported as the average values across three LLMs. "TL" indicates that the number is too large. We also highlight the SS results of \method that are comparable to state-of-the-art performance (marked in \colorbox{green!36}{green}). 
}
\vspace*{-0.5em}
\centering
\small
\tabcolsep 3.5pt
\renewcommand\arraystretch{1.0}
\begin{tabular}{llccccccc}
\toprule
\multirow{2}{*}{} &
\multirow{2}{*}{Search Method} &
\multicolumn{2}{c}{Game of 24} &
\multicolumn{2}{c}{Mini Crosswords} &
\multicolumn{2}{c}{Blocksworld} \\
\cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8}
&  & PR (\%) & Avg. SS & PR (\%) & Avg. SS & PR (\%) & Avg. SS \\
\midrule
\multicolumn{8}{l}{\textbf{Traditional Search}} \\
\cmidrule(r){1-2}
&\textsc{Exhaustive Search}&\num{100} &\num{12928} &\num{100} & TL &\num{100} & TL\\
\addlinespace[0.1em]\hdashline\addlinespace[0.1em]
& \textsc{Brute-force (DFS)} & \num{100} & \num{1623} & \num{100} & \num{4128.9} & \num{100} & \num{18531.9} \\

& \textsc{Brute-force (BFS)} & \bfseries\num{100} & \num{3429} & \num{100} & {TL} & \num{100} & \num{96759.4} \\
\midrule
\multicolumn{8}{l}{\textbf{LLM-based Search}} \\
\cmidrule(r){1-2}
& \textsc{Vanilla CoT} & \num{14.3} & 1 ({$\downarrow$} 99.9\%) & \num{1.7} &  1 ({$\downarrow$} 99.9\%) & \num{17.5} &  1 ({$\downarrow$} 99.9\%) \\

& \textsc{Majority Vote} & \num{20.3} & \num{10} ({$\downarrow$} 99.3\%) & \num{3.4} & \num{10} ({$\downarrow$} 99.7\%) & {\num{47.4}} & \num{10} ({$\downarrow$} 99.9\%) \\

& \textsc{Best-of-N} & \num{22.6} & \num{20} ({$\downarrow$} 98.7\%) & \num{2.5} & \num{20} ({$\downarrow$} 99.5\%) & \num{30.0} & \num{20} ({$\downarrow$} 99.8\%) \\

& \textsc{Beam Search} & \num{54.0} & \num{94.8} ({$\downarrow$} 94.1\%) & {\num{45.0}} & \num{26.1} ({$\downarrow$} 99.4\%) & \num{0} & \num{64.4} ({$\downarrow$} 99.6\%) \\
& \textsc{Beam Search}+RV & \num{79.0} & \num{98.8} ({$\downarrow$} 93.9\%) & {\num{90.0}} & \num{30.9} ({$\downarrow$} 99.2\%) & \num{0} & \num{94.8} ({$\downarrow$} 99.6\%) \\
\addlinespace[0.1em]\hdashline\addlinespace[0.1em]
% \rowcolor{gray!30} 
& \textsc{\method} & & & & & & \\

& \;\; -- \textsc{GPT-4o-mini} & $\mathbf{100}$ & \cellcolor{green!36}\num{40} ({$\downarrow$} 97.5\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{75.8} ({$\downarrow$} 98.2\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{160.6} ({$\downarrow$} 99.1\%)\\

% & \;\; -- \textsc{GPT-4o-mini (No Verifier)} & $\mathbf{90}$ & \cellcolor{green!36}\num{44.9} ({$\downarrow$} \%) & $\mathbf{100}$ & \cellcolor{green!36}\num{42.9} ({$\downarrow$} 98.9\%) & $\mathbf{}$ & \cellcolor{green!36}\num{} ({$\downarrow$} \%)\\

& \;\; -- \textsc{GPT-4o} & ${99}$ & \cellcolor{green!36}\num{65.6} ({$\downarrow$} 96.0\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{45.0} ({$\downarrow$} 98.9\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{80.8} ({$\downarrow$} 99.5\%) \\

& \;\; -- \textsc{Qwen2.5-72B-Instruct} & $\mathbf{100}$ & \cellcolor{green!36}\num{84.9} ({$\downarrow$} 94.8\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{98.3} ({$\downarrow$} 98.9\%) & $\mathbf{100}$ & \cellcolor{green!36}\num{68.7} ({$\downarrow$} 99.6\%) \\

\bottomrule
\end{tabular}
% \vspace{-5pt}
\vspace{-1em}
\label{tab:main_result}
\end{table*}


\subsection{Experiment Settings}
\label{sec:experimental_setup}
\noindent \textbf{Tasks}. To evaluate the effectiveness of \method, in addition to the Game of 24 used in Sec.~\ref{sec:preliminary_setup}, we select two widely adopted planning tasks for evaluating LLMs' planning abilities: Mini Crosswords~\cite{yao2023tree} and Blocksworld~\cite{valmeekam2022large}. Further details on these tasks are provided in Appendix~\ref{appendix:tasks}.

% We evaluate the effectiveness of \method across three challenging real-world planning domains: \textbf{(i)} Game of 24~\cite{yao2023tree}; \textbf{(ii)} Mini Crosswords~\cite{yao2023tree}; and \textbf{(iii)} Blocksworld~\cite{valmeekam2022large}. More details of these tasks are in Appendix~\ref{appendix:tasks}.

% \noindent \textbf{Tasks}. We evaluate the effectiveness of \method across three challenging real-world planning domains: \textbf{(i)} Game of 24~\cite{yao2023tree}, which is used in Sec.~\ref{sec:preliminary_setup}; \textbf{(ii)} Mini Crosswords~\cite{yao2023tree}, which involves solving $5 \times 5$ crossword puzzles by selecting words from fixed-length lists corresponding to 5 horizontal and 5 vertical clues; and \textbf{(iii)} Blocksworld~\cite{valmeekam2022large}, a planning task requiring the agent to rearrange blocks into specified stack configurations. More details of these tasks are in Appendix~\ref{appendix:tasks}.

% \noindent \textbf{Baselines}. We compare \method with three LLM-based search algorithms: \textsc{Majority Vote}~\cite{wang2023selfconsistency}, \textsc{Best-of-N}~\cite{snell2024scaling}, and \textsc{Beam Search}~\cite{feng2023alphazerolike}. We also include \textsc{Vanilla CoT}~\cite{wei2022chain} as a reference baseline. For traditional search, we include two \textsc{brute-force} algorithms: depth-first search (\textsc{DFS}) and breadth-first search (\textsc{BFS}). The \textsc{exhaustive search} algorithm is included as a baseline for evaluating search efficiency. Details on the baselines are provided in Appendix~\ref{appendix:baselines}.

\noindent \textbf{Baselines}. Following the settings in Sec.~\ref{sec:preliminary_setup}, we consider four LLM-based searches, \textsc{Vanilla CoT}, \textsc{Majority Vote}, \textsc{Best-of-N}, and \textsc{Beam Search}, and three traditional searches, \textsc{DFS}, \textsc{BFS} and \textsc{Exhaustive Search}. We also involve a baseline \textsc{Beam Search+RV} that adds a rule-based verifier to the beam search method.
Details on the baselines are provided in Appendix~\ref{appendix:baselines}.

\noindent \textbf{Models}. Our experiments use both closed-source models (GPT-4o-mini, GPT-4o~\cite{gpt4ocard}) and open-source models (Qwen2.5-72B-Instruct~\cite{qwen2.5}, QwQ-32B-Preview~\cite{qwq-32b-preview}, DeepSeek-R1~\cite{deepseekai2025deepseekr1}). Note that GPT-4o-mini is a small language model (SLM), and QwQ-32B-Preview and DeepSeek-R1 are the state-of-the-art LRMs. Model and implementation details are in Appendix~\ref{appendix:llm_setting} and~\ref{appendix:implementation_details}.

\noindent \textbf{Evaluation Metrics}.  We follow Sec.~\ref{sec:preliminary_setup} to use two metrics: (i) pass rates (PR) and (ii) search steps (SS).  More details are in Appendix~\ref{appendix:evaluation_metrics}. 

% We evaluate performance using two primary metrics: (i) pass rates (PR) across all problems in the task, measuring solution quality, and (ii) search steps (SS), measuring exploration efficiency by counting traversed states in the search space $\mathcal{S}$.
% More definitions are in Appendix~\ref{appendix:evaluation_metrics}.  

% \noindent \textbf{Evaluation Metrics}.  We evaluate performance using two primary metrics: (i) pass rates (PR) across all problems in the task, measuring solution quality, and (ii) search steps (SS), measuring exploration efficiency by counting traversed states in the search space $\mathcal{S}$.
% More definitions are in Appendix~\ref{appendix:evaluation_metrics}.  

% \noindent \textbf{Implementation Details}.
% Following the settings in prior studies~\cite{yao2023tree, hao2023reasoning, snell2024scaling}, we set the temperature for LLMs to $\num{0.7}$. To use GPT-4o~\cite{gpt4ocard} and GPT-4o-mini~\cite{achiam2023gpt}, we set the mode as the Chat Completion modes for them. For DeepSeek-R1~\cite{deepseekai2025deepseekr1}, we use the Chat mode to run experiments. More details of the implementation are in Appendix~\ref{appendix:implementation_details}.

\subsection{RQ1: Effectiveness and Efficiency Evaluations}
\label{sec:rq1}
% To answer \textbf{RQ1}, we conduct experiments on the three tasks using three different LLM backbones: GPT-4o-mini, GPT-4o, and Qwen2.5-72B-Instruct. For Game of 24, we follow~\cite{yao2023tree} to select $100$ problems indexed from $900$ to $999$. For Mini Crosswords, we randomly choose $20$ problems with $11$ candidate words for each clue, while for Blocksworld, we randomly select $20$ problems with a minimum solution length of $8$ steps. The results are reported in Table~\ref{tab:main_result}.
We conduct experiments on the three tasks using three different LLM backbones: GPT-4o-mini, GPT-4o, and Qwen2.5-72B-Instruct. Detailed task setups are in Appendix~\ref{appendix:rq1_appendix}. The results are reported in Table~\ref{tab:main_result}. The key observations are:
\textbf{(i)} \method significantly reduces search steps compared to brute-force searches. Specifically, it reduces search steps by up to $99.1\%$ compared to \textsc{DFS} and still achieves state-of-the-art search steps compared to other LLM-based methods. This validates \method's efficiency in navigating the search space.
\textbf{(ii)} \method achieves a near-perfect pass rate across all settings, outperforming other LLM-based methods.
% , which achieve pass rates of up to 54.0\%, 45.0\%, and 47.4\% for the three tasks, respectively. This validates the effectiveness of \method in problem-solving.
\textsc{Beam Search+RV} achieves better pass rates than \textsc{Beam Search}, indicating that integrating rule-based verifiers into LLM-based methods enhances their reliability in problem-solving. However, it still falls behind \method, confirming that current LLM-based methods do not fully conduct real search. This further highlights the superior search effectiveness of \method.
Full search results and the discussion of \method with various verifiers are in Appendix~\ref{appendix:additional_results_seal}.
% Appendix~\ref{appendix:full_comparison_results_on_three_tasks_three_LLMs} and~\ref{appendix:impact_verifier_search}.

% $2.8\times 10^{11}$ $5.3\times 10^{9}$


\begin{figure*}[t]
    \small
    \centering
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/impact_completeness_24game_pr.pdf}
        \vskip -0.5em
        \caption{Game of 24 - PR}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/impact_completeness_24game_sn.pdf}
        \vskip -0.5em
        \caption{Game of 24 - SS}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/impact_completeness_blocksworld_pr.pdf}
        \vskip -0.5em
        \caption{Blocksworld - PR}
    \end{subfigure}
    \begin{subfigure}{0.24\textwidth}
        \includegraphics[width=0.98\linewidth]{figures/impact_completeness_blocksworld_sn.pdf}
        \vskip -0.5em
        \caption{Blocksworld - SS}
    \end{subfigure}
    \vskip -1.em
    \caption{Impact of problem difficulty on search completeness in Game of 24 and Blocksworld with GPT-4o-Mini.}
    % \vskip -2em
    \label{fig:impact_problem_difficulty_completeness}
\end{figure*}

\subsection{RQ2: Impact of Problem Difficulty on Search Completeness}
\label{sec:rq2}
% \minhua{revise to select two tasks}
We evaluate \cmethod's performance across problem difficulties in three tasks. For Game of 24, we use three difficulty levels (Sec.~\ref{sec:preliminary_results_analysis}); for Blocksworld, difficulty scales with minimum required action steps ($2$–$12$). Other settings follow Sec.~\ref{sec:experimental_setup}. The results using GPT-4o-mini in the two tasks are reported in Fig.~\ref{fig:impact_problem_difficulty_completeness}, which show that: \textbf{(i)} \cmethod achieves $100\%$ pass rates across all difficulty levels, outperforming baseline methods whose performance degrades progressively, confirming its completeness. \textbf{(ii)} As the difficulty increases, \cmethod's search steps increase slightly but remain significantly lower than brute-force baselines. This efficiency is attributed to \cmethod's learning-guided complete state decomposition and two-phase ranking, which prioritize promising states and effectively reduce the search space. Mini Crosswords results are in Appendix~\ref{appendix:addtional_impact_difficulty_of_seal_c}.
% We explore \cmethod's performance across varying problem difficulties in three tasks. For Game of 24, problems are categorized into three difficulty levels as in Sec.~\ref{sec:preliminary_results_analysis}.
% For blocksworld, difficulty varies by the minimum action steps required, ranging from $\{2, 4, 6, 8, 10, 12\}$. Other settings follow Sec.~\ref{sec:experimental_setup}. The results using GPT-4o-mini in Game of 24 and Blocksworld tasks are shown in Fig.~\ref{fig:impact_problem_difficulty_completeness}. From the figures, we observe: \textbf{(i)} \cmethod consistently maintain a $100\%$ pass rate across all difficulty levels in the two tasks, while the pass rate of other methods gradually decrease. This satisfies our expectations and validates the completeness of \cmethod. \textbf{(ii)} As the difficulty increases, \cmethod's search steps increase slightly but remain significantly lower than brute-force baselines. This efficiency is attributed to \cmethod's learning-guided complete state decomposition and two-phase ranking, which prioritize promising states and effectively reduce the search space.
% Additional results on Mini Crosswords are in Appendix~\ref{appendix:addtional_impact_difficulty_of_seal_c}.
% For Mini Crosswords, we vary the length of the candidate word lists as $\{6,7,8,9,10,11\}$ to define difficulty levels.

\subsection{RQ3: Potential of LLMs in Self-Search}
\label{sec:experiment_self_search}
We conduct an initial exploration to study whether LLMs can learn to self-search for problem-solving by testing two self-search prompts in Sec.~\ref{sec:llm_self_search} and $\textsc{Vanilla CoT}$ in GPT-4o and two state-of-the-art open-sourced LRMs, QwQ-32B-Preview and DeepSeek-R1 to demonstrate the importance of search in solving tasks. Especially, we use the chat mode of DeepSeek-R1 and sample $20$ problems.
Since they are LRMs that are capable of searching, the results of \textsc{Self-Search (High)} in the two LRMs can be directly obtained from using $\textsc{Vanilla CoT}$ in this model.
The comparison results in Table~\ref{tab:self_search_result} show: 
\textbf{(i)} Standard prompting (\textsc{Vanilla CoT}) in GPT-4o achieves the lowest performance, indicating LLMs cannot conduct searches and require conduct searches for complex problem-solving.
{\textbf{(ii)} Two LRMs achieve the best performance across all settings. Specifically, DeepSeek-R1, when using \textsc{Self-Search (High)}, achieves an impressive pass rate of \num{85}\%. From our observations, these LRMs perform search by iteratively sampling and evaluating different answers until a correct solution is found. This underscores the importance of search in enhancing LLM problem-solving capabilities while also revealing the inefficiency of current search mechanisms, motivating the need for more efficient strategies. More details of the problem-solving processes of the two LRMs are in Appendix~\ref{appendix:more_details_of_problem_solving_processes_LRMs}.}
\textbf{(iii)} Both self-search approaches significantly improve performance, with low-level self-search achieving up to a $95\%$ pass rate. This demonstrates that LLMs have the potential of effectively utilizing explicit search strategies for improving problem-solving.
\textbf{(iv)} Low-level self-search, which incorporates \method's strategies, consistently outperforms high-level self-search across both models. This validates the effectiveness of \method's search and suggests that better search guidance enhances LLM reasoning. 
% We conduct an initial exploration to study whether LLMs obtain the ability to learn to search by themselves. Specifically, we use two self-search prompts in GPT-4o in Sec.~\ref{sec:llm_self_search} to solve the Game of 24 task by comparing with using the vanilla CoT prompt. 
% Moreover, we also use the vanilla CoT prompt in the state-of-the-art open-sourced LRMs, QwQ-32B-Preview and DeepSeek-R1, to demonstrate the importance of search in solving tasks. Especially, we use the chat mode of DeepSeek-R1 and sample $20$ problems. Since they are LRMs that are capable of searching, the results of \textsc{Self-Search (High)} in this model can be directly obtained from using $\textsc{Vanilla CoT}$ in this model.
% The comparison results are reported in Table~\ref{tab:self_search_result}. From the table, we observe that: 
% \textbf{(i)} Standard prompting (\textsc{Vanilla CoT}) in GPT-4o achieves the lowest performance, indicating LLMs cannot conduct searches and require conduct searches for complex problem-solving.
% {\textbf{(ii)} Two LRMs achieve the best performance across all settings. Specifically, DeepSeek-R1, when using \textsc{Self-Search (High)}, achieves an impressive pass rate of \num{85}\%. From our observations, these LRMs perform search by iteratively sampling and evaluating different answers until a correct solution is found. This underscores the importance of search in enhancing LLM problem-solving capabilities while also revealing the inefficiency of current search mechanisms, motivating the need for more efficient strategies. More details of the problem-solving processes of the two LRMs are in Appendix~\ref{appendix:more_details_of_problem_solving_processes_LRMs}.}
% \textbf{(iii)} Both self-search approaches significantly improve performance, with low-level self-search achieving up to a $95\%$ pass rate. This demonstrates that LLMs have the potential of effectively utilizing explicit search strategies for improving problem-solving.
% \textbf{(iv)} Low-level self-search, which incorporates \method's strategies, consistently outperforms high-level self-search across both models. This validates the effectiveness of \method's search and suggests that better search guidance enhances LLM reasoning. 
% \hui{Can we spend some room to discuss the results of Deepseek R1? I think the result on R1 is very conclusive on the importance of LLM being aware of how to to search.}

These findings highlight both the importance of search in enhancing LLM capabilities and the effectiveness of \method's search strategies, motivating us to explore how to further develop LLMs' self-search capabilities and optimize search strategies, which will be our future works. 

\begin{table}[t]
\caption{PR(\%) of search strategies for GPT-4o, QwQ-32B-Preview and DeepSeek-R1 on the Game of 24.}
% \vspace{-0.5em}
\centering
\small
\tabcolsep 3.5pt
\renewcommand\arraystretch{1.0}
\begin{tabular}{llc}
\toprule
\multirow{1}{*}{Model} &
\multirow{1}{*}{Method} &
PR(\%) \\
\midrule
\multirow{3}{*}{GPT-4o} &
\textsc{Vanilla CoT} & \num{13} \\
& \textsc{Self-search (High)}  & \num{24} \\
& \textsc{Self-search (Low)}  & $\mathbf{31}$ \\
\midrule
\multirow{2}{*}{QwQ-32B-Preview} &
\textsc{Self-search (High)}  & \num{62} \\
& \textsc{Self-Search (Low)}  & $\mathbf{70}$ \\
\midrule
\multirow{2}{*}{DeepSeek-R1 (20)} &
\textsc{Self-search (High)}  & \num{85} \\
& \textsc{Self-Search (Low)}  & $\mathbf{95}$ \\
\bottomrule
\end{tabular}
% \vspace{-2em}
\label{tab:self_search_result}
\end{table}

\subsection{Ablation Studies}
Inspired by \citet{snell2024scaling}, we conduct ablation studies to understand the impact of search budgets on \method's performance. Instead of terminating the search upon finding the final state, we introduced a pre-defined search step budget, where \method terminates early if the budget is reached. We vary the search step budgets as $\{10, 20, 30, 50, 100, 150, 200\}$, and compare various search methods using GPT-4o-mini in Game of 24. The comparison results are reported in Fig.~\ref{fig:impact_of_search_number_budget_ablation}. Our analysis reveals three key findings: \textbf{(i)} \method consistently outperforms other methods across all search budgets, demonstrating its effectiveness in accurately solving problems even under constrained search budgets. \textbf{(ii)} Pass rates for all search methods generally improve as the search budget increases, aligning with expectations that scaling test-time computation enhances search performance. 
\textbf{(iii)} When the search budget is small (less than $50$ steps), \textsc{Beam Search} performs poorly, achieving a 
$0\%$ pass rate. This is due to its sequential evaluation of substates, which consumes substantial search budgets on generating and assessing intermediate substates. In contrast, \method focuses on diving directly toward the goal state, significantly reducing the search space and validating its efficiency in navigating complex search problems. Additional results from the ablation studies evaluating the impact of \method's components can be found in Appendix~\ref{appendix:additional_results_of_ablation_studies}.


\begin{figure}[t]
    \small
    \centering

    \includegraphics[width=0.55\linewidth]{figures/impact_of_search_number.pdf}
    \vskip -1em
    \caption{Impact of SS in \method using GPT-4o-mini.}
    \vskip -2em
    \label{fig:impact_of_search_number_budget_ablation}
\end{figure}

% \textsc{Beam Search} sequentially evaluates the progress of each sub-state toward the goal, requiring several iterations to generate and assess substates. In contrast, \method prioritizes directly diving to the goal state, significantly reducing the search space and validating its efficiency in navigating complex search problems.
% \textbf{(iii)} \textsc{Beam Search} exhibits the poorest performance ($0\%$ pass rate) with small budgets (<50 steps) due to its sequential evaluation of each substate's progress, wasting many search budgets in generating and assessing substates. In contrast, \method prioritizes directly diving to the goal state, significantly reducing the search space and validating its efficiency in navigating complex search problems.

% \textbf{(iii)} When the search budget is small (e.g., less than 50), \textsc{Beam Search} exhibits the poorest performance ($0\%$ PR). This is because \textsc{Beam Search} sequentially evaluates the progress of each sub-state toward the goal, requiring several iterations to generate and assess substates. In contrast, \method prioritizes directly diving to the goal state, significantly reducing the search space and validating its efficiency in navigating complex search problems.

% We evaluate the performance of \method and compared search methods using GPT-4o-mini on the Game of 24 task


% the effect of search numbers in \method. Specifically, instead of stopping search when the final state is found, we set a pre-defined budget for the search numbers such that \method will terminate early if it reaches to the budget. The budget of search number is varied as $\{10, 20, 30, 50, 100, 150, 200\}$. We report the results of compared search methods backboned with GPT-4o-mini in Game of 24 in Fig.~\ref{fig:impact_of_search_number_budget_ablation}. From the figure, we observe that: \textbf{(i)} \method achieve the superior performance across all search budgets, demonstrating the effectiveness of \method in accurately solving problems within limited search budgets. \textbf{(ii)} PRs of all search methods generally increase as the search budget increases, satisfying our expectation that scaling test-time compute can improve search performance. \textbf{(iii)} When the search budget is small (usually less than $50$), \textsc{Beam Search} obtains the worst performance ($0\%$ PR), this is because \textsc{Beam Search} requires to evaluate the progress of each sub-state toward the goal one by one after generating several substates each time. In contrast, \method focus on diving to the goal state primarily, which can significantly reduce search space, validating the efficiency of \method in navigating search space. 

% \begin{table}[t]
% \caption{Success rate (\%) and average reduce states in Game of 24. Qwen2.5-72B-Instruct is the backbone model for the LLM-based methods. 
% }
% \centering
% \small
% \tabcolsep 3.5pt
% \renewcommand\arraystretch{1.0}
% \begin{tabular}{llc}
% \toprule
% \multirow{2}{*}{} &
% \multirow{1}{*}{Model} &
% \multicolumn{1}{c}{Method} &
% \multicolumn{1}{c}{PR} \\
% \midrule
%  GPT-4o &\textsc{Brute-force (DFS)}&\num{100} \\
% &&\textsc{Brute-force (BFS)}&\num{100}\\
% &&\textsc{Brute-force (DFS-Prune)} &\num{}\\
% \midrule
% QwQ-32B-Preview &\textsc{Brute-force (BFS-Prune)} &\num{}\\
% &\textsc{Brute-force (BFS-Prune)} &\num{}\\
% \bottomrule
% \end{tabular}
% \vspace{-5pt}
% \label{tab:self_search_result}
% \end{table}
