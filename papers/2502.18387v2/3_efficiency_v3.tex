\section{How Learning can Benefit Search}

Search and learning represent two fundamental approaches to problem-solving. Traditional search methods offer systematic exploration with guaranteed completeness, while learning-based approaches leverage pattern recognition to identify promising solutions quickly. 
In this section, we conduct a preliminary analysis to systematically explore {existing search algorithms and} the synergies between learning and search. Building on these insights, we introduce \method, a framework that integrates learning into search algorithms to reduce unnecessary exploration, prioritize promising paths, and ultimately maintain reliable and efficient performance as problems grow in complexity.

\subsection{Experimental Setup}
\label{sec:preliminary_setup}
\textbf{Task Setup}. To investigate the impact of learning on search, we use \textit{the Game of 24} as a representative task. This task can be solved using traditional search algorithms and is also widely adopted for evaluating LLMs' planning abilities. Details about the task are provided in Appendix~\ref{appendix:tasks}. Following the setting in \citet{yao2023tree}, we select \num{100} problems indexed as $900-999$ for our experiments.


\noindent \textbf{Baselines.} For our preliminary analysis, we evaluate three LLM-based simulated search algorithms: \textsc{Majority Vote} \cite{wang2023selfconsistency}, \textsc{Best-of-N} \cite{snell2024scaling}, and \textsc{Beam Search} \cite{yao2023tree}. Additionally, we include \textsc{Vanilla CoT} (Chain-of-Thought)~\cite{wei2022chain} as a reference. To evaluate search efficiency, we also consider two traditional brute-force search methods, Depth-First Search (\textsc{DFS}) and Breadth-First Search (\textsc{BFS}), along with their pruning variants, \textsc{DFS-Prune} and \textsc{BFS-Prune}. These variants improve efficiency by avoiding exploration of previously visited states. Finally, an \textsc{Exhaustive Search} is included as a baseline for comprehensive comparison.
Detailed implementation details for all baselines are provided in Appendix~\ref{appendix:baselines}.

\noindent \textbf{Evaluation Metrics}  We evaluate performance using two primary metrics:
(i) pass rates (PR) across games per difficulty level, measuring solution quality, and
(ii) search steps (SS), measuring exploration efficiency by counting traversed states in the search space $\mathcal{S}$.
Complete metric definitions are available in Appendix~\ref{appendix:evaluation_metrics}.  


\begin{figure}[t]
    \small
    \centering
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.98\linewidth]{figures/preliminary_difficulty_vs_pass_rate.pdf}
        \vskip -0.5em
        \caption{Pass Rate}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.98\linewidth]{figures/preliminary_search_number_comparison.pdf}
        \vskip -0.5em
        \caption{Search Step}
    \end{subfigure}
    \vskip -1em
    \caption{PR (\%) and SS of existing searches across various problem difficulties using GPT-4o-mini. 
    % \hui{for Figure 1(b), can we plot a figure with y axis break. Otherwise it's hard to read.}
    }
    \vskip -1em
    \label{fig:preliminary_difficulty_level_group}
\end{figure}

\subsection{Analysis: How Learning can Benefit Search}
\label{sec:preliminary_results_analysis}
To better understand how learning can enhance search, we divide the 100 problems from the Game of 24 into three difficulty levels based on human success rates\footnote{{https://www.4nums.com/game/difficulties/}}. Other experimental settings follow those in Sec.~\ref{sec:preliminary_setup}.
Our experimental results, presented in Fig.~\ref{fig:preliminary_difficulty_level_group}, reveal several key findings regarding the performance of existing search methods. Full analyses are in Appendix~\ref{appendix:preliminary_results_analysis_appendix}.

\noindent \textbf{Obs. 1: LLMs Perform Better for Simpler Problems}. 
As in Fig.~\ref{fig:preliminary_difficulty_level_group}(a), \textsc{Vanilla CoT} achieve pass rates of $\num{18.2}\%$, $\num{12.1}\%$, and $\num{11.7}\%$ for problems with difficulty levels $1$, $2$, and $3$, respectively. This pattern suggests that LLMs excel at direct problem-solving for simpler cases, indicating their potential for single-step solutions rather than requiring iterative approaches. More examples are in Fig.~\ref{fig:obs1_example}.

\noindent \textbf{Obs. 2: Learning-Based Pruning Has Precision-Coverage Trade-offs}. According to Fig.~\ref{fig:preliminary_difficulty_level_group}(a) and~\ref{fig:preliminary_difficulty_level_group}(b), \textsc{Beam Search} achieves the superior performance in pass rate than other LLM-based search methods, and significantly saves search steps than \textsc{DFS}. To deeply analyze this observation, an example of \textsc{Beam Search} in Fig.~\ref{fig:obs2_example} shows that it only relies on LLMs to generate a few of possible next steps instead of decomposing all possible states in brute-force searches in each time of generating next intermediate steps. However, it is possible that some valid states are overlooked during the intermediate step generation, leading to failure in solving tasks and reflecting a trade-off between efficiency and completeness in search.


\noindent \textbf{Obs. 3: Learning Provides Adaptive Search Guidance}. 
Fig.~\ref{fig:obs3_example} is an example of \textsc{Beam Search} employing an LLM verifier to assess intermediate states' progress toward the goal, prioritizing exploration of valid states and avoiding further exploration on other states. Moreover,
as in Fig.~\ref{fig:preliminary_difficulty_level_group}(a), the LLM-based methods using more search steps generally perform better. This is because these methods allocate more search budget in verifying results via LLMs, further demonstrating LLMs' capability as dynamic evaluators to leverage learned knowledge to identify promising states and guide the search process. 




These findings suggest that learning can substantially improve search efficiency when properly integrated into the search process. However, they also highlight the need for careful mechanism design to balance the benefits of learning-based pruning with the completeness guarantees of traditional search. 


\begin{table}[t]
\caption{PR (\%) and SS on the Game of 24 using Qwen2.5-72B-Instruct. For LLM-based methods, we additionally present the reduction ratio of SS compared to \textsc{Brute-force (DFS)}.
}
\vspace*{-0.5em}
\centering
\small
\tabcolsep 3.5pt
\renewcommand\arraystretch{1.0}
\begin{tabular}{llccc}
\toprule
\multirow{2}{*}{} &
\multirow{1}{*}{Method} &
\multicolumn{1}{c}{PR (\%)} &
\multicolumn{1}{c}{SS} \\
\midrule
&\textsc{Exhaustive Search}&\num{100} &\num{12928}\\
\addlinespace[0.1em]\hdashline\addlinespace[0.1em]
&\textsc{Brute-force (DFS)}&\num{100} &\num{1623} \\
&\textsc{Brute-force (BFS)}&\num{100} &\num{3429} \\
&\textsc{Brute-force (DFS-Prune)}&\num{100} &\num{1385}\\
&\textsc{Brute-force (BFS-Prune)}&\num{100} &\num{1306}\\
\midrule
&\textsc{Vanilla CoT} &\num{17}   &\num{1} ({$\downarrow$} 99.9\%) \\
&\textsc{Majority vote} &\num{23}  &\num{10} ({$\downarrow$} 99.3\%) \\
&\textsc{Best-of-N}&\num{27}  &\num{20} ({$\downarrow$} 98.7\%)\\
% \rowcolor{gray!30} &\lmmweb\textsubscript{Oracle}
&\textsc{Beam search}
&\num{35} &\num{124} ({$\downarrow$} 92.4\%)\\
\bottomrule
\end{tabular}
\vspace{-1em}
\label{tab:preliminary_24game_qwen}
\end{table}
\subsection{Analysis: Existing Search Algorithms}
\label{sec:analysis_existing_search}
While existing search algorithms show significant promise, they still encounter notable challenges. 
To better illustrate their limitations, we conduct an analysis following the setup in Sec.\ref{sec:preliminary_setup} by applying existing search algorithms to the Game of 24 task.
The results are reported in Table~\ref{tab:preliminary_24game_qwen}, revealing the following insights: \textbf{(i)} Traditional brute-force algorithms achieve perfect $100\%$ accuracy but require up to \num{3429} search steps, demonstrating their inefficiency in searching for answers.
\textbf{(ii)} While LLM-based methods significantly reduce the search space (at least $92.4\%$ fewer steps than \textsc{DFS}), their pass rates peak at only $35\%$, highlighting their instability compared to traditional searches. Full analyses are in Appendix~\ref{appendix:analysis_existing_search_appendix}

These findings underscore the necessity of a framework that combines the completeness of systematic search with the efficiency of learning-based approaches. This motivates the development of a more effective search-and-learning methodology to address these limitations.


\begin{figure}[t]
    \small
    \centering
    
        \includegraphics[width=0.99\linewidth]{figures/SeaL_Framework_2.pdf}
        \vskip -1em
    % \vskip -1.em
    \caption{\method intergrating learning into search with LLMs: \textbf{(1)} Direct solution generation, \textbf{(2)} State validity checking, \textbf{(3)} Learning-guided state ranking.}
    \vskip -1.0em
    \label{fig:Framework_figure}
\end{figure}
\subsection{\method: Search via Learning with LLMs}
\label{sec:seal_method}
\noindent \textbf{Notations}.
We focus on solving planning problem following the essence of solving real-world decision-making problems via search: starting from an initial state, envisioning possible actions, and systematically working toward a goal state. 
Specifically, a planning problem is formally defined as a tuple $P=\langle\mathcal{S}, s^{init},s^{goal},\mathcal{A},f\rangle$. Here, $\mathcal{S}$ represents a finite and discrete set of states describing the world (i.e., state space). $s^{\text{init}}, s^{\text{goal}} \in \mathcal{S}$ denote the initial and goal world states, respectively. $\mathcal{A}=\{a_1, a_2, \ldots,\}$ represents the set of possible actions, and $f(s,a_i)=s'$ is a transition function mapping a state and action to a resulting state. A solution to problem $P$ is a sequence of actions $\langle a_1,a_2,\ldots\rangle$ that transforms $s^{\text{init}}$ into $s^{\text{goal}}$.

Building upon the above insights, we introduce \method, a framework that systematically integrates learning capabilities into search processes to emulate human problem-solving strategies. The goal of \method is on enhancing this natural problem-solving process by combining the systematic nature of search with the learning capabilities of LLMs.
\method consists of four components to enhance different aspects of search processes through learning:

\noindent \textbf{Direct Solution Generation}. Motivated by the insight from Obs. 1 that learning excels at solving simpler problems directly, we begin each step by attempting a direct solution, akin to how humans first try to solve problems in one step:
\begin{equation}
\small
r^{cur} = M(p_{\text{solve}}(s^{cur}))
\end{equation}
where $M$ represents the backbone LLM and $p_{\text{solve}}$ denotes the solution generation prompt. A verifier $f$ validates the generated solution. More details of $p_{solve}$ are in Appendix~\ref{appendix:prompt_templates_direct_solution_generation}.  This mechanism leverages LLMs' strength in handling simpler cases, potentially bypassing the need for extensive search when direct solutions are viable. Note that, however, the verifier $f$ may not always be available in certain scenarios. To ensure the applicability of \method in such cases, we provide further discussions in Appendix~\ref{appendix:impact_verifier_search}.

\noindent \textbf{State Decomposition}. For more complex problems that cannot be solved directly, we decompose the current state $s^{cur}$ into subproblems, mirroring how humans break down challenging tasks:
\begin{equation}
\label{eq:state_decomposition}
\small
    S^{next} = D(s^{cur}),
\end{equation}
where $D$ is the decomposition function generating substates $S^{next} = \{s^{next}_1,s^{next}_2,\ldots\}$. While LLMs can suggest promising decompositions~\cite{yao2023tree}, Obs. 2 reveals the risk of overlooking valid paths. Thus, we maintain the ability to systematically enumerate substates when needed, balancing between efficient exploration of valid paths and thorough coverage of the state space $\mathcal{S}$.


\noindent \textbf{State Validity Checking}. 
During search, it is inevitable that we may explore some invalid states, especially if we decompose into all possible substates. Exploring and expanding these invalid substates too much will significantly increase the search space. Thus, inspired from Obs. 3, we leverage LLMs to assess the potential of substates toward goals, similar to how humans quickly judge if it is worthy to continue exploration:
\begin{equation}
\small
% \begin{aligned}
    c^{next} = M(p_c(S^{next})),
% \end{aligned}
\end{equation}
where $p_c$ prompts the LLM $M$ to evaluate state validity. Additional examples $p_{c}$ are in Appendix~\ref{appendix:prompt_templates_state_validity_checking}. In this paper, we opt for binary decisions ($c^i_{k}$) about whether to continue exploration on the substates $S^{next}$, ensuring efficient pruning on invaid states while maintaining search on valid states.


\noindent \textbf{Learning-guided Ranking}
After validating substates, we prioritize paths similar to how humans naturally focus their attention on the most promising approaches. Leveraging Obs. 3, we employ LLMs to compute a priority score of each state:
\begin{equation}
\small
% \begin{aligned}
v(s^{cur}) = M(p_v(s^{cur}))
% \end{aligned}
\end{equation}
where $v(s^{cur})$ denotes the LLM-estimated value using prompt $p_v$, indicating how likely the current state leads to the goal state $s^{goal}$. States with higher scores are prioritized in the exploration queue, enabling efficient traversal of promising solution paths. Details of $p_{v}$ are in Appendix~\ref{appendix:prompt_templates_learning_guided_ranking}.


\noindent \textbf{Overall Alogirhtm}
Combining these components together into search, we implement the framework of \method, which is shown in Fig.~\ref{fig:Framework_figure}. The full algorithm is in Appendix~\ref{appendix:algorithm_full}. Our search strategy focuses on exploring promising paths deeply before considering alternatives, similar to how humans naturally approach problem-solving. This design allows \method to scale to complex problems effectively and efficiently through its combination of learning-guided intuition and systematic exploration.
