\section{Related Works}
\label{sec:related_works}
\textbf{LLM-based Search Methods}.  
Recent advancements in test-time compute scaling have sparked growing interest in methods that enable LLMs to simulate search processes and ``think longer'' instead of directly generating answers in one pass. Several works**Vaswani et al., "Attention Is All You Need"**__**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**
adopt such approaches to enhance LLMs' problem-solving capabilities. 
Despite their innovation, these methods rely solely on LLMs' intrinsic knowledge, often leading to unstable performance due to limitations in LLMs' reasoning capabilities. In contrast, our proposed \method integrates LLMs with traditional search strategies, ensuring both completeness and efficiency in solving decision-making tasks. More LLM-based search methods are reviewed in Appendix~\ref{appendix:related_works_LLM_search}.

\noindent \textbf{Traditional Search Methods}.  
Inspired by the recent discussion about search and learning**Kang et al., "Improving Language Understanding by Generative Models with Query-Conditioned Attention"**__**Brown et al., "Language Models as Knowledge Bases"**
, which underscores the enduring value of general-purpose strategies that scale with computational power, we investigate how to better integrate search with learning to leverage the strengths of both paradigms. Our \method and \cmethod draw inspiration from traditional search while incorporating LLM-guided reasoning to reduce search space, thereby significantly enhancing efficiency without sacrificing completeness. More details of the related works are in Appendix~\ref{appendix:related_works}.