\section{Introduction}
\label{sec:introduction}
Search lies at the heart of problem-solving, offering a systematic approach to explore solution spaces and find optimal answers. From everyday choices to complex strategic planning, virtually all real-world decision-making processes can be formulated and solved through systematic search strategies. This insight aligns with the recent discussions on search and learning~\cite{Sutton2019BitterLesson,snell2024scaling}, which emphasizes that the most effective problem-solving approaches combine systematic search with learning from experience.


Traditional search methods, such as brute-force searches, while theoretically complete, face challenges in systematically exploring large and complex search spaces.
They are designed to ensure that all possible states are considered, but the vastness of such spaces often necessitates exhaustive exploration, making systematic traversal impractical. Moreover, these methods lack the intuitive problem-solving abilities that humans naturally employ, such as recognizing promising solutions early or quickly abandoning unproductive paths.

Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have opened new possibilities for more human-like search approaches. Some recent LRMs such as OpenAI o1~\cite{openaio1card}, QwQ-32B~\cite{qwq-32b-preview}, and DeepSeek-R1~\cite{deepseekai2025deepseekr1} have demonstrated remarkable performance by incorporating LLM-guided search strategies. These approaches leverage LLMs' extensive knowledge bases and reasoning capabilities to guide the search process~\cite{wei2022chain,wang2023selfconsistency,snell2024scaling,yao2023tree,hao2023reasoning,wang2024litesearch}, attempting to mirror human-like intuition in problem-solving.

Despite the great success of these LLM-based simulated searches, a critical limitation remains: they rely heavily on models' intrinsic knowledge rather than combining it with systematic search strategies. When confronted with complex problems requiring multi-step reasoning or extensive exploration, these models often struggle to maintain consistent performance~\cite{wang2024q,wang2024multi,snell2024scaling}. Their reasoning can become unstable or incomplete, particularly when solutions require carefully exploring multiple solution paths or backtracking from dead endsâ€”abilities that humans naturally employ in problem-solving.
% \suhang{cite???}.

Draw inspiration from recent discussions on search and learning~\cite{Sutton2019BitterLesson,snell2024scaling}, which emphasizes the great power of combining search and learning over human-centric approaches, in this paper, we systematically explore the integration and complementation of search and LLMs from three crucial perspectives: efficiency, completeness, and inherent search capabilities. 

Firstly, we explore how learning can benefit search. Specifically, we conduct a preliminary analysis to compare existing traditional and LLM-based search methods on a representative task, Game of 24, to investigate their problem-solving capabilities. Our experimental results reveal that the learning and reasoning capabilities of LLMs help solve simpler problems without extensive searching, reducing unnecessary state exploration and prioritizing promising states, which significantly shrink search spaces. 
Then, building upon these insights, we present \method, a framework that integrates learning into search algorithms to improve the search efficiency while maintaining completeness for enhancing the problem-solving capabilities of LLMs. 
We also introduce \cmethod, a variant that rigorously ensures search completeness while preserving efficiency through learning-guided complete state decomposition and two-phase ranking. 

We evaluate \method and \cmethod on three planning tasks: Game of 24, Mini Crosswords~\cite{yao2023tree} and Blocksworld~\cite{valmeekam2022large} using five representative LLMs, GPT-4o~\cite{gpt4ocard}, GPT-4o-mini~\cite{achiam2023gpt}, Qwen2.5-72B-Instruct~\cite{qwen2.5}, QwQ-32B-Preview~\cite{qwq-32b-preview} and DeepSeek-R1~\cite{deepseekai2025deepseekr1}. Our experimental results show that \method reaches almost perfect pass rates across almost all settings, while reducing search space 
by up to $99.1\%$ compared to traditional brute-force searches. And \cmethod is also proved to ensure rigorous completeness efficiently. These validate the effectiveness of \method in enabling complete and efficient search via learning. 

To this end, we {investigate how far LLMs are from real search} by investigating a reverse but natural problem: \textit{how search can benefit LLMs and whether LLMs can learn to search by themselves}. Specifically, we prompt LLMs to conduct searches solely relying on intrinsic knowledge or guided by \method's search strategies, respectively. Our analysis yields two significant insights. First, while search capabilities are crucial for LLMs' problem-solving effectiveness, current LLMs/LRMs exhibit inefficient search behaviors, requiring extensive sampling to achieve satisfactory performance. Second, incorporating \method's search strategies into LLM prompts demonstrably enhances their problem-solving capabilities. These findings not only underscore the critical role of search in enhancing LLMs' reasoning and learning capabilities but also validate the effectiveness of \method's search strategies, motivating us to improve LLMs' self-search capabilities in the future works. Our \textbf{main contributions} are: 
\begin{itemize}[leftmargin=*,noitemsep, topsep=0pt]
    \item (\textbf{Analysis}) Inspired by the principles of search and learning, we conduct a systematic exploration of how learning benefits search, demonstrating that LLMs can reduce unnecessary state exploration and improve efficiency by prioritizing promising states. Additionally, we explore how search benefits LLMs, revealing the importance of teaching LLMs to efficiently search for solving complex problems.
    \item (\textbf{Methodology}) We propose \method, a framework that integrates learning into search for efficiency and completeness, and \cmethod, a variant to ensure rigorous completeness.
    \item (\textbf{Experiments}) We evaluate \method and \cmethod across diverse real-world tasks, demonstrating their effectiveness in achieving efficient and complete search. We also reveal that existing LLMs cannot perform efficient and effective search regarding complex tasks, which is a desired fundamental ability for LLMs to be applied to real-world decision making tasks.
\end{itemize}
