\section{Related Work}
\label{sec:relate_work} The construction of embodied agents can be categorized into three approaches, including rule-based and traditional planning methods \cite{blake2001rule}, RL methods \cite{russell2003q}, and LLM-based methods \cite{zhao2024expel}. 
\vspace{-1em}
\paragraph{Rule-based and Traditional Planning Methods:} Rule-based and traditional planning methods rely on expert-designed rules or algorithms to solve tasks, making them suitable for simple tasks in static environments. For example, the A* algorithm \cite{liu2011comparative} excels in path planning but suffers from lower efficiency and accuracy in dynamic or unstructured environments \cite{mohanan2018survey}. Model-based planning \cite{ghallab2004automated} and constraint-based planning \cite{baptiste2006constraint} attempt to improve adaptability to complex environments, but challenges persist when handling uncertainty and dynamic changes.
\vspace{-1em}
\paragraph{RL Methods:} RL enables agents to learn task strategies through interaction with the environment and has achieved notable success in various domains, such as AlphaZero \cite{silver2018general} in board games. However, RL often faces issues in slow convergence and policy instability in high-dimensional action spaces and sparse reward scenarios \cite{vecerik2017leveraging, horgan2018distributed}. Additionally, RL performs poorly in tasks requiring global planning and long-term memory due to its focus on short-term returns \cite{nguyen2020deep, rusu2016progressive}. To address these challenges, researchers have proposed model-based RL \cite{kaiser2019model, moerland2023model} to improve sample efficiency by constructing environment models, but these methods still struggle with model complexity and generalization. Deep RL \cite{mnih2015human} utilizes neural networks to enhance state representation but faces computational and data efficiency challenges in multi-task and high-dimensional scenarios \cite{li2019deep, zhu2021overview}. Furthermore, hierarchical RL \cite{pateria2021hierarchical} and meta-RL \cite{beck2023survey, finn2017model} offer new approaches to handle long-term planning and task transfer, demonstrating potential in dynamic and complex environments. Despite these advances, RL still struggles with stability, computational overhead, and sample efficiency, particularly in the context of complex embodied tasks.
\vspace{-1em}
\paragraph{LLM-based Methods:} The introduction of LLMs has significantly enhanced the reasoning and task-handling capabilities of agents \cite{chan2023chateval, kannan2024smart, sun2024llm}. LLMs like GPT \cite{hurst2024gpt} and Gato \cite{reed2022generalist} leverage self-supervised learning to process multimodal data and excel in natural language understanding and open-world tasks \cite{raffel2020exploring}. However, existing LLM-driven agents exhibit limitations in long-term planning and dynamic task environments, manifesting two key issues---(1) Memory limitations: LLMs rely on autoregressive generation models and are unable to track task context or effectively store historical information; (2) Spatio-temporal reasoning deficits: LLMs perform reasoning based on pattern matching, lacking the ability to model spatio-temporal relationships in dynamic environments.

Recently, researchers have proposed several approaches to address these issues. 
%\cite{ahn2022can}. 
For example, ReAct \cite{yao2022react} enhances task planning by introducing reflective and multi-step reasoning. However, ReAct's reasoning process relies on manually set few-shot samples, which limits its generalization. Reflexion \cite{shinn2024reflexion}, building upon ReAct, incorporates a self-reflection mechanism that allows agents to accumulate experience through multi-step trial and error. However, in embodied environments, errors may not be recoverable, limiting the effectiveness of this trial-and-error learning. Swiftsage \cite{lin2024swiftsage}, inspired by human dual-process theory \cite{frankish2010dual} and fast-slow thinking \cite{kahneman2011thinking}, combines these modules to handle complex tasks. However, its open-loop architecture fails to adequately support long-term memory and dynamic planning. AdaPlanner \cite{sun2024adaplanner} proposes a closed-loop architecture where an initial plan is refined based on environmental feedback. Nevertheless, it lacks a memory system, limiting its adaptability to long-horizon planning tasks. Hippo RAG \cite{gutierrez2024hipporag} mimics the human hippocampus \cite{burgess2002human} and introduces KGs as long-term memory indices \cite{chen2020review}, significantly enhancing knowledge retrieval. However, these methods are still confined to short-term reasoning and lack support for long-term planning in dynamic environments.