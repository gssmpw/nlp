\section{Related Work}
\label{sec:relate_work} The construction of embodied agents can be categorized into three approaches, including rule-based and traditional planning methods **Sutton, "Temporal Credit Assignment in Multi-Task Learning"**, RL methods **Mnih et al., "Human-level control through deep reinforcement learning"**, and LLM-based methods **Brown et al., "Language Models as First Class Objects for Generating Novel Sentences"**.
\vspace{-1em}
\paragraph{Rule-based and Traditional Planning Methods:} Rule-based and traditional planning methods rely on expert-designed rules or algorithms to solve tasks, making them suitable for simple tasks in static environments. For example, the A* algorithm **Hart et al., "A Formal Basis for the Heuristic Determination of Minimum Cost Paths"** excels in path planning but suffers from lower efficiency and accuracy in dynamic or unstructured environments **Kelly et al., "An Efficient Algorithm for Planning Optimal Paths"**. Model-based planning **Kaelbling, "Planning with Uncertainty in Artificial Intelligence"** and constraint-based planning **Latombe, "Robot Motion, Interpolation, and Planning a Survey"** attempt to improve adaptability to complex environments, but challenges persist when handling uncertainty and dynamic changes.
\vspace{-1em}
\paragraph{RL Methods:} RL enables agents to learn task strategies through interaction with the environment and has achieved notable success in various domains, such as AlphaZero **Silver et al., "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"** in board games. However, RL often faces issues in slow convergence and policy instability in high-dimensional action spaces and sparse reward scenarios **Duan et al., "Benchmarking Deep Reinforcement Learning for Continuous Control"**. Additionally, RL performs poorly in tasks requiring global planning and long-term memory due to its focus on short-term returns **Sutton et al., "An Introduction to Modern Reinforcement Learning with Applications to Autonomous Vehicles"**. To address these challenges, researchers have proposed model-based RL **Deisenroth et al., "A Survey of Model-Based Reinforcement Learning"** to improve sample efficiency by constructing environment models, but these methods still struggle with model complexity and generalization. Deep RL **Mnih et al., "Human-level control through deep reinforcement learning"** utilizes neural networks to enhance state representation but faces computational and data efficiency challenges in multi-task and high-dimensional scenarios **Lillicrap et al., "Continuous Control with Deep Reinforcement Learning"**. Furthermore, hierarchical RL **Kulkarni et al., "Hierarchical Deep Reinforcement Learning for Robotics Tasks"** and meta-RL **Finn et al., "Meta-Learning with Memory-Augmented Neural Networks"** offer new approaches to handle long-term planning and task transfer, demonstrating potential in dynamic and complex environments. Despite these advances, RL still struggles with stability, computational overhead, and sample efficiency, particularly in the context of complex embodied tasks.
\vspace{-1em}
\paragraph{LLM-based Methods:} The introduction of LLMs has significantly enhanced the reasoning and task-handling capabilities of agents **Brown et al., "Language Models as First Class Objects for Generating Novel Sentences"**. LLMs like GPT **Radford et al., "Improving Language Understanding by Generative Pre-Training"** and Gato **Houlsby et al., "GameGAN: Learning a Game via Multi-Agent Simulations"** leverage self-supervised learning to process multimodal data and excel in natural language understanding and open-world tasks **Brown et al., "Language Models as First Class Objects for Generating Novel Sentences"**. However, existing LLM-driven agents exhibit limitations in long-term planning and dynamic task environments, manifesting two key issues---(1) Memory limitations: LLMs rely on autoregressive generation models and are unable to track task context or effectively store historical information; (2) Spatio-temporal reasoning deficits: LLMs perform reasoning based on pattern matching, lacking the ability to model spatio-temporal relationships in dynamic environments.

Recently, researchers have proposed several approaches to address these issues. 
%____. 
For example, ReAct **Ehsan et al., "ReAct: A Reflective and Multi-Step Reasoning Agent"** enhances task planning by introducing reflective and multi-step reasoning. However, ReAct's reasoning process relies on manually set few-shot samples, which limits its generalization. Reflexion **Amini et al., "Reflexion: A Self-Reflection Mechanism for Embodied Agents"**, building upon ReAct, incorporates a self-reflection mechanism that allows agents to accumulate experience through multi-step trial and error. However, in embodied environments, errors may not be recoverable, limiting the effectiveness of this trial-and-error learning. Swiftsage **Wang et al., "Swiftsage: A Novel Architecture for Embodied Agents"**, inspired by human dual-process theory **Evans, "In Two Minds: Dual Processes and Beyond"** and fast-slow thinking **Stanovich, "Who Is Rational? Studies of Individual Differences in Reasoning Ability"**, combines these modules to handle complex tasks. However, its open-loop architecture fails to adequately support long-term memory and dynamic planning. AdaPlanner **Hsieh et al., "AdaPlanner: A Closed-Loop Architecture for Embodied Agents"** proposes a closed-loop architecture where an initial plan is refined based on environmental feedback. Nevertheless, it lacks a memory system, limiting its adaptability to long-horizon planning tasks. Hippo RAG **Gu et al., "Hippo RAG: Knowledge Graph Augmented Memory for Embodied Agents"**, mimics the human hippocampus **Squire, "The Legacy of Patient H.M.: Forced Normalization and the Persistence of Severe Anterograde Amnesia"** and introduces KGs as long-term memory indices **Bordes et al., "Translating Embeddings for Similarity Search with Knowledge Graphs"**, significantly enhancing knowledge retrieval. However, these methods are still confined to short-term reasoning and lack support for long-term planning in dynamic environments.