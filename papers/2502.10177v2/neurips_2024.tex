\documentclass{article}




\PassOptionsToPackage{numbers, compress}{natbib}




\usepackage[final]{neurips_2024}




\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{hyperref}       %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{lipsum}         %
\usepackage{todonotes}
\usepackage{tcolorbox}

\input{packages/defs}
\input{packages/header}
\input{packages/math_commands}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\xmark}{\ding{55}}%
\newcommand{\tian}[1]{\textcolor{blue}{#1}}
\renewcommand{\cross}{{\ding{55}}}


\newcommand\nnfootnote[1]{%
  \begin{NoHyper}
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \end{NoHyper}
}

\usepackage{enumitem}

\usepackage{framed}
\colorlet{shadecolor}{orange!15}



\AtBeginDocument{\setlength\abovedisplayskip{5pt}}
\AtBeginDocument{\setlength\belowdisplayskip{5pt}}


\newcommand{\eos}{\operatorname{EOS}}
\newcommand{\surr}{\operatorname{surr}}
\newcommand{\old}{\operatorname{old}}
\newcommand{\gene}{\operatorname{generation}}
\newcommand{\back}{\operatorname{backpropagation}}

\newcommand{\Ex}{\mathbb{E}}
\newcommand{\Ext}{\mathbb{E}_t}
\newcommand{\taut}{{\tau_t}}
\setlist[enumerate]{topsep=1pt, parsep=1pt, partopsep=1pt, leftmargin=*}


\hypersetup{
    colorlinks=true,
    citecolor=blue,
    linkcolor=blue,
}

\usepackage{listings} %

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblue}{rgb}{0,0,1}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{key-color}{rgb}{0.8, 0.47, 0.196}

\renewcommand{\KL}{\operatorname{KL}}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=Python,
    emph={lm},
    emphstyle={\color{blue}},
    classoffset=1, %
    otherkeywords={sum},
    morekeywords={rm, mean},
    keywordstyle=\color{codegreen},
    classoffset=0,
}
\lstset{style=mystyle}




\usepackage{listings}
\usepackage{xcolor}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
}


\definecolor{bgcolor}{rgb}{0.1, 0.1, 0.1}
\definecolor{commentcolor}{rgb}{0.5, 0.5, 0.5}
\definecolor{keywordcolor}{rgb}{0.5, 0.0, 0.5}
\definecolor{stringcolor}{rgb}{0.0, 0.5, 0.0}
\definecolor{numbercolor}{rgb}{0.0, 0.0, 0.7}
\definecolor{functioncolor}{rgb}{0.0, 0.0, 0.5}

\lstdefinestyle{github}{
    backgroundcolor=\color{bgcolor},
    basicstyle=\ttfamily\small\color{white},
    commentstyle=\color{commentcolor},
    keywordstyle=\color{keywordcolor}\bfseries,
    stringstyle=\color{stringcolor},
    numberstyle=\color{numbercolor},
    identifierstyle=\color{white},
    showstringspaces=false,
    numbers=left,
    numbersep=5pt,
    tabsize=4,
    breaklines=true,
}


\lstnewenvironment{python}[1][]
{
    \renewcommand{\lstlistingname}{Code Example}
    \lstset{style=mystyle,#1}
}{}



\title{Why Transformers Need Adam: A Hessian Perspective }




\author{%
  Yushun Zhang$^{12}$, Congliang Chen$^{12}$, Tian Ding$^2$, Ziniu Li$^{12}$, Ruoyu Sun$^{12*}$, 
    Zhi-Quan Luo$^{12}$
   \\
   $^1$The Chinese University of Hong Kong, Shenzhen, China \\
  $^2$Shenzhen Research Institute of Big Data
  \\
  \texttt{\{yushunzhang,congliangchen,ziniuli\}@link.cuhk.edu.cn} \\
  \texttt{dingtian@sribd.cn, sunruoyu@cuhk.edu.cn, luozq@cuhk.edu.cn}
}

\begin{document}


\maketitle


\nnfootnote{$*$: Correspondence author.}


\vspace{-0.4cm}
\begin{abstract}
\vspace{-0.2cm}

SGD performs worse than Adam by a significant margin on Transformers, but the reason remains unclear.
In this work, we provide an explanation 
through the lens of Hessian: (i) Transformers are ``heterogeneous'': the Hessian spectrum across parameter blocks vary dramatically, a phenomenon we call ``block heterogeneity";
(ii) Heterogeneity hampers SGD: SGD performs worse than Adam on problems with block heterogeneity. 
To validate (i) and (ii),
we check various Transformers, CNNs, MLPs, and quadratic problems,
and find that SGD can perform on par with Adam on problems without block heterogeneity, but performs worse than Adam when the heterogeneity exists.
Our initial theoretical analysis indicates that SGD performs worse because it applies one single learning rate to all blocks, which cannot handle the heterogeneity among blocks. This limitation could be ameliorated if we use coordinate-wise learning rates, as designed in Adam.\footnote{Our code is available at \url{https://github.com/zyushun/hessian-spectrum}.}

\end{abstract} 


\vspace{-0.2cm}
\section{Introduction}
\label{sec_intro}
\vspace{-0.2cm}


Transformers \citep{vaswani2017attention} have become a major workhorse behind AI development (e.g., \citep{achiam2023gpt}).   
However, the understanding of Transformer training remains limited. For instance, Transformer training largely relies on the Adam optimizer \citep{kingma2014adam, loshchilov2017decoupled}. In contrast,
stochastic gradient descent with momentum (SGD)\footnote{We introduce the update rules of Adam(W) and SGD  in Appendix \ref{appendix_preliminaries_optimizers}.}, the de-facto optimizer for convolution neural networks (CNNs) \citep{lecun1998gradient}, performs significantly worse than Adam on Transformers (e.g., Figure \ref{fig_blockwise_spectrum}).  Yet, the reasons behind this performance gap remain unclear. Understanding why SGD performs worse than Adam on Transformers is an intriguing question.
{\bf First,} from a theoretical perspective, this can help us better understand the training of Transformers
and more generally, neural networks. 
{\bf Second,} from a computational perspective, the understanding
may inspire the design of better algorithms
for training neural networks. 

In this work, we explore why SGD largely underperforms Adam on Transformers through the lens of Hessian. We start by investigating the {\it full} Hessian spectrum of Transformers, i.e., the full eigenvalue density of Hessian (see Figure \ref{fig_full_spectrum}). By theory, the full Hessian spectrum largely determines the behavior of gradient-based methods \citep{nesterov2013introductory,goh2017why,sun2019optimization,goujaud2022super}, so we suspect it may also help explain SGD's unsatisfactory performance. 
Using tools from numerical linear algebra \citep{bai1996some}, we empirically compare the full spectra of CNNs (where SGD is on par with Adam) and those of Transformers (where SGD largely lags behind Adam).  Unfortunately, as shown in Figure \ref{fig_full_spectrum}, the spectra for CNNs and Transformers are often largely {\it similar} despite the different optimizer behaviors. As such, we have {\it not} identified critical features in the full Hessian spectra associated with the gap between Adam and SGD on Transformers. To reveal the cause, a more fine-grained investigation into the Hessian is needed. 



\begin{figure}[thbp!]
\vspace{-0.8cm}
    \centering
    \subfigure[Initialization]{\includegraphics[width=0.24\textwidth]{images/Full_Hessian_Init.pdf}}
    \subfigure[25\% steps]{\includegraphics[width=0.24\textwidth]{images/Full_Hessian_25.pdf}}
    \subfigure[50\% steps]{\includegraphics[width=0.24\textwidth]{images/Full_Hessian_50.pdf}}
    \subfigure[100\% steps]
    {\includegraphics[width=0.24\textwidth]{images/Full_Hessian_100.pdf}}
    \vspace{-0.3cm}
    \caption{The full Hessian spectra of CNNs (VGG16 and ResNet18) and Transformers (GPT2, GPT2-nano, and ViT-base) 
  at different training stages. The $x$-axis records the eigenvalues and the $y$-axis records the frequency in the log scale. To allow comparison in the same figure,  the plotted spectra are normalized by their 10th largest eigenvalues. We find that the spectra on
CNNs and Transformers are largely similar. }
    \label{fig_full_spectrum}
\vspace{-0.6cm}
\end{figure}

What would cause SGD to perform significantly worse than Adam on Transformers, but not on CNNs? By dissecting the structures of CNNs and Transformers, we notice that CNNs are constructed by the repetitive stacking of {\it similar} parameter blocks (convolution layers), while Transformers involve the non-sequential stacking of {\it disparate} parameter blocks (e.g. Query, Key, Value, Output projection blocks in attention and MLP layers). We hypothesize that these architectural differences might lead to different optimization properties. Intuitively, disparate parameter blocks contribute differently to the overall loss. So each block might benefit from a specialized treatment by optimizers, a flexibility offered by Adam but not by SGD. This observation motivates us to investigate the Hessian spectrum of each parameter block, which we refer to as the blockwise Hessian spectrum. 

By inspecting the blockwise Hessian spectrum, we discover a possible explanation for why SGD is worse: the “heterogeneity” inherent in Transformers. We provide both empirical and theoretical evidence to support this explanation. Our contributions can be summarized as follows:
\vspace{-0.1cm}
\begin{itemize}[topsep=1pt,parsep=1pt,partopsep=1pt, leftmargin=*]
    \item {\bf Why SGD underperforms Adam on Transformers.} We explain why SGD is worse than Adam on Transformers by examining the blockwise Hessian spectrum. 
    First, we identify a phenomenon called ``block heterogeneity", which refers to the large differences in the Hessian spectra across parameter blocks.
    This block heterogeneity is observed in all examined Transformers but not in CNNs. Second, we verify that block heterogeneity hinders SGD. Across various Transformers, CNNs, and MLPs, we show that SGD consistently performs worse than Adam on problems with block heterogeneity but can perform similarly to Adam, otherwise.


    \item {\bf Theoretical results on quadratic models.} 
    We construct convex quadratic problems with and without block heterogeneity and find that gradient descent (GD) largely underperforms Adam on problems with block heterogeneity, but can perform comparably otherwise. Our theoretical analysis shows that GD can be slower than Adam on quadratic problems with block heterogeneity. We point out  GD is slower than Adam because it uses a single learning rate for all blocks. The deficiency can be mitigated by assigning different learning rates across blocks, as Adam does.
\end{itemize}
\vspace{-0.1cm}

We emphasize that we do {\it not} claim block heterogeneity is the only cause for the performance gap between Adam and  SGD, but just that it is at least one important cause. 
We verify, both empirically and theoretically, that SGD underperforms Adam when block heterogeneity is present.





\vspace{-0.2cm}
\section{Problem Settings and Initial Attempts}
\label{sec_initial_attempts}
\vspace{-0.2cm}

\subsection{Problem Settings}
\label{section_problemsetting}
\vspace{-0.2cm}


\textbf{Notations.}
We denote the training loss as  $\mathcal{L}(w)$,  where   $w \in \mathbb{R}^{d }$ is the neural network parameters.  
We denote the gradient and Hessian of the training loss w.r.t. neural network parameters as  $\nabla \mathcal{L}(w) \in \mathbb{R}^{d }$ and    $\nabla^2 \mathcal{L}(w) \in \mathbb{R}^{d \times d}$, respectively. We use $[d]$ to denote the index set $\{1,2,\cdots ,d\}$.
Given an arbitrary partition $\{\mathcal{D}_l\}^L_{l=1}$ over $[d]$ with $d_l\triangleq |\mathcal{D}_l|$,
we can split $w$ into $L$ parameter blocks $\{w_l\}_{l =1}^L$, where $w_l = \mathbb{R}^{d_l}$ consists of parameters with indexes in the $l$-th block $\mathcal{D}_l$.
We denote $[\nabla^2 \mathcal{L}(w)]_l \in \mathbb{R}^{d_l \times d_l}$ as the Hessian of  $l$-th parameter-block $w_l$, where  $ [\nabla^2 \mathcal{L}(w)]_{l,i, j}=\frac{\partial^2}{\partial_{w_{l,i}} \partial_{w_{l,j}}} \mathcal{L}(w_l)$. Note that $ [\nabla^2 \mathcal{L}(w)]_l$  is the $l$-th principal block sub-matrix of  $\nabla^2 \mathcal{L}(w)$. 


\textbf{Setups.}  Hessian of large-scale NNs are intractable to compute and store. In this work, we apply a numerical tool called Stochastic Lanczos Quadrature method (SLQ) \citep{bai1996some} to approximate the Hessian spectrum.
SQL uses a smooth curve on $\mathbb{R}$ to approximate the histograms of eigenvalues (see Figure \ref{fig_full_spectrum}  as an example). A detailed introduction to SLQ is provided in Appendix \ref{appendix_slq}.  All experimental setups are shown in Appendix \ref{appendix_experiment_details}. We focus primarily on the following models/tasks.

\begin{itemize}[topsep=1pt,parsep=1pt,partopsep=1pt, leftmargin=*]
    \item {\bf CNNs.} We study ResNet18 (11M) and VGG16 (138M) on ImageNet \citep{he2016deep, simonyan2014very}. On these tasks,  SGD performs on par with Adam. {\bf See Figure \ref{fig:cv_figure} in Appendix \ref{appendix_more_discussion} for the evidence.}
    \item {\bf Transformers.} We study Transformer with various scales and modalities, including GPT2 (125M) on OpenWebText \citep{radford2019language}; ViT-base (86M) on ImageNet \citep{dosovitskiy2020image}; BERT (40M) on Cornell Movie-Dialogs Corpus \citep{devlin2018bert}; GPT2-nano\footnote {\url{https://github.com/karpathy/nanoGPT/}} (11M) on English corpus. On these tasks, SGD performs significantly worse than Adam. {\bf See Figure \ref{fig:nlp_figure} in Appendix \ref{appendix_more_discussion} for the evidence.}
\end{itemize}







For each model, we estimated {\bf (1)} the full Hessian spectrum $\nabla^2 \mathcal{L}(w)$, and {\bf (2)} the blockwise Hessian spectrum $[\nabla^2 \mathcal{L}(w) ]_l, l \in [L]$. For the latter, we split $w$ according to the default partition in PyTorch implementation, e.g., Embedding layer, Query in each attention layer, 
Key in each attention layer, Value in each attention layer, etc. 
Note that the term ``block" differs from the term ``layer". 
For instance, Query and Key can reside in the same layer but are different parameter blocks. 






\vspace{-0.2cm}
\subsection{Full Hessian Spectrum Is Not Informative Enough}
\label{sec_full_spectrum}
\vspace{-0.2cm}

We study the full Hessian spectrum of Transformers for two reasons. First, as stated in Section \ref{sec_intro}, the Hessian spectrum significantly influences the behavior of gradient methods \citep{nesterov2013introductory}. 
Second, previous research shows that the Hessian spectrum provides insights into neural network phenomena, like BatchNorm's effect on training speed \citep{ghorbani2019investigation}. Therefore, we hypothesize that the Hessian spectrum may also explain why SGD largely lags behind Adam on Transformers.


We compare the full Hessian spectra of CNNs (where SGD performs similarly to Adam) and those of Transformers (where SGD underperforms Adam), as shown in Figure \ref{fig_full_spectrum}. Unfortunately, the results suggest that the full Hessian spectrum alone may not suffice to explain the gap between Adam and SGD on Transformers. We elaborate as follows. The primary information in the spectrum lies in its (A) dispersion, (B) shape, and (C) evolution during training. Regarding (A), we observe that the eigenvalues are dispersed similarly across different models, with no notably large outlier for Transformers. Thus, dispersion does not seem to be related to why SGD is worse than Adam. We further investigate (B) and (C). For all CNNs and Transformers in Figure \ref{fig_full_spectrum}, we observe similar phenomena: the spectrum’s shape is approximately symmetrical around 0 at initialization. As training proceeds, the majority of negative eigenvalues disappear, and the shape evolves into a combination of a “bulk” and some “outliers”. Since the spectral shape and evolution are quite similar for both Transformers and CNNs, they cannot explain why SGD is worse than Adam on Transformers, either. In summary, we have not identified any critical phenomena in the full Hessian spectra that can be linked to the performance gap between Adam and SGD on Transformers.


\vspace{-0.2cm}
\subsection{Motivations of Investigating Blockwise Hessian Spectra}
\label{sec_blockwise_spectrum_motivation}
\vspace{-0.2cm}

What other factors could cause SGD to perform significantly worse than Adam on Transformers but not on CNNs?  
We identify one critical feature that  has been overlooked in the full Hessian
spectrum analysis above: {\bf the building-up rules of Transformers.}  As shown in Figure \ref{fig_blockwise_spectrum},  
    CNNs are constructed by the repetitive stacking of {\it similar} parameter blocks (convolution layers). 
    In contrast, 
    Transformers consist of {\it disparate} parameter blocks, e.g. Query, Key, Value in attention, and MLP layers. Further, these blocks are stacked in a non-sequential manner. 
We hypothesize that the ``different designs among parameter blocks" can be reflected in the Hessian of these parameter blocks, which might affect algorithmic behavior.  This inspires us to investigate {\bf the blockwise Hessian spectra}, i.e., the spectrum of principal blocks of Hessian $ [\nabla^2 \mathcal{L}(w) ]_l, l \in [L]$. 



\begin{figure}[t]
\vspace{-0.8cm}
    \centering
    \subfigure[Hessian of an MLP \citep{collobert2004large} after 1 step ]{\includegraphics[width=0.22\textwidth]{images/thesis_trained.png}}\hfill
    \subfigure[Hessian  of an MLP at 1\% step]{\includegraphics[width=0.2\textwidth]{images/1012_fullhessian_T_10_2-layer.pdf}}\hfill
    \subfigure[Hessian  of an MLP at 50\% step]{\includegraphics[width=0.2\textwidth]{images/1012_fullhessian_T_500_2-layer.pdf}}\hfill
    \subfigure[Hessian  of an MLP at 100\% step]{\includegraphics[width=0.2\textwidth]{images/1012_fullhessian_T_1000_2-layer.pdf}}\hfill
    \vspace{-0.3cm}
    \caption{ (a): The Hessian of an MLP after 1 training step reported in \citep{collobert2004large}.  (b,c,d): We calculate the Hessians of an MLP (with 8 neurons) at different training stages. We find the near-block-diagonal structure maintains along the training.  }
    \label{fig_block_diagnal}
\vspace{-0.5cm}
\end{figure}



In parallel to the motivation above, we further provide another evidence that blockwise spectra might be helpful. Classical literature showed that the Hessians of neural nets are {\it near-block-diagonal matrices} \citep{collobert2004large}, i.e., the magnitudes in the Hessian principle blocks are much larger than those in the off-diagonal blocks. We
restate their findings in Figure \ref{fig_block_diagnal} (a). This implies that the majority of Hessian information indeed lies in its principle blocks, and the blockwise Hessian of neural nets might contain valuable information.





To summarize, the ``heterogeneous" building-up rules of Transformers inspire us to check the blockwise Hessian, i.e., the principle blocks of the Hessian. The classical results of neural nets \citep{collobert2004large} further support us to explore this direction since they find that the majority of Hessian information indeed lies in its principle blocks.
 In the following, we study the blockwise Hessian spectra of various neural networks. For ease of implementation, we define parameter blocks under the PyTorch partition. We show that the blockwise spectra indeed carry more information than the full spectrum for distinguishing CNNs and Transformers. 



{\bf Remark: why near-block-diagonal?} We briefly restate the analysis in  \citep[Section 7]{collobert2004large} to explain the near-block-diagonal Hessian structure of neural nets. Consider  minimizing $\ell (f(\theta, x), y)$ where $\ell(\cdot,\cdot)$ is the Cross-Entropy (CE) loss, $f(\theta, x) = \sum_{i = 1}^{n} v_i \phi(w_i^{\top}x)$ is  an 1-hidden-layer neural network with input $x \in \mathbb{R}^{d}$, weight $w_i \in \mathbb{R}^{d}$,  $v_i \in \mathbb{R}$, and label $y  \in \{0,1\}$, then the off-diagonal-block Hessian elements will contain
\begin{small}\begin{equation}
\label{eq_hessian_calculation}
    \frac{\partial^2 \ell (f(\theta, x), y) }{\partial w_i \partial w_j}=\BLUE{p_\theta(y|x) \left(1 - p_\theta(y|x)\right)}v_i v_j \phi^{\prime}\left(w_i^\top x \right) \phi^{\prime}\left(w_j^\top x \right) x x^\top \quad \text{for } i \neq j, 
\end{equation}
\end{small}
where $p_\theta(y|x) = 1 /(1 + \exp (-y f(\theta,x)))$ and $\phi^{\prime}(\cdot)$ is the derivative of $\phi(\cdot)$. Note that the term \BLUE{$p_\theta(y|x) \left(1 - p_\theta(y|x)\right)$}
will vanish rapidly since the training objective is to maximize $p_\theta(y|x)$. Consequently, this drives the Hessian towards a near-block-diagonal configuration, with each block representing an output neuron. This result is validated in Figure \ref{fig_block_diagnal}: we find that the near-block-diagonal structure appears at 1\% step and it maintains along the training.


    


    




\vspace{-0.4cm}
\section{Main Results}
\label{sec_main_results}


\vspace{-0.2cm}
\subsection{Transformers Exhibit  Block Heterogeneity in Hessian, while CNNs Do Not}
\label{sec_blockwise_spectrum}
\vspace{-0.2cm}


We now compare the shape of blockwise spectra in VGG16 \citep{he2016deep} (CNN) and BERT \citep{devlin2018bert} (Transformer).  We sample four blocks for each model and present the spectra in Figure \ref{fig_blockwise_spectrum}. In BERT, the spectra of embedding, attention, and MLP blocks are largely {\it different}. In contrast, in ResNet, the spectra of convolution layers are {\it similar}. We further verify this observation for the rest of the parameter blocks.
We calculate the Jensen-Shannon (JS) distance between two eigenvalue densities of all possible block pairs and show the results in Figure \ref{fig_blockwise_heatmap}. We summarize our findings in {\bf Observation 1}.


\begin{figure}[th]
\vspace{-0.3cm}
    \centering
    \subfigure[VGG16]{\includegraphics[width=0.24\textwidth]{images/vgg_structure_spectrum.png}}
    \subfigure[VGG16]{\includegraphics[width=0.24\textwidth]{images/trainacc_cosine_vgg.pdf}}
    \subfigure[BERT]{\includegraphics[width=0.24\textwidth]{images/transformer_structure_spectrum.png}}
    \subfigure[BERT]{\includegraphics[width=0.24\textwidth]{images/result_bert.pdf}}
\vspace{-0.2cm}
    \caption{ {\bf (a) (c)}: The blockwise Hessian spectra of VGG16 (CNN) and BERT (Transformer)
  at initialization. The $x$-axis records the eigenvalues and the $y$-axis records the frequency in the log scale. To allow comparison in the same figure, we sample 4 blocks in each model. The plotted spectra are normalized by their 10th largest eigenvalues. The spectra are similar among blocks for VGG and differ significantly across blocks for BERT. {\bf (b) (d)} Adam v.s. SGD for training VGG16 and BERT.}
    \label{fig_blockwise_spectrum}
\vspace{-0.1cm}
\end{figure}









\begin{figure}[th]
\vspace{-0.8cm}
    \centering
     \subfigure[ResNet18]{\includegraphics[width=0.3\textwidth]{images/heat_map_ResNet18_0.pdf}}
    \hfill %
    \subfigure[BERT]{\includegraphics[width=0.3\textwidth]{images/heat_map_BERT_0.pdf}}
    \hfill %
    \subfigure[GPT2-nano]{\includegraphics[width=0.3\textwidth]{images/heat_map_nanoGPT_0.pdf}}
    \newline
    \vspace{-0.1cm}
    \subfigure[VGG16]{\includegraphics[width=0.3\textwidth]{images/heat_map_VGG_0.pdf}}
    \hfill
    \subfigure[ViT-base]{\includegraphics[width=0.3\textwidth]{images/heat_map_ViT_0.pdf}}
    \hfill %
    \subfigure[GPT2]{\includegraphics[width=0.3\textwidth]{images/heat_map_GPT2_0.pdf}}
    \vspace{-0.2cm}
    \caption{The JS distance among blockwise Hessian spectra at initialization. We find that the JS distance of blockwise spectra in CNNs is significantly smaller than that in Transformers.  }
    \label{fig_blockwise_heatmap}
    \vspace{-0.4cm}
\end{figure}





\begin{snugshade}
\begin{center}
{\bf Observation 1:} For all Transformers we checked, the blockwise Hessian spectra are largely {\it different} from each other. In contrast, the blockwise Hessian spectra of CNNs are {\it similar}.
\end{center}
\vspace{-0.2cm}
\end{snugshade}



In the following, we refer to the phenomenon of Transformers as ``{\bf block heterogeneity}", and refer to that of CNN as ``{\bf block homogeneity}".
The observations in Figure \ref{fig_blockwise_spectrum} and \ref{fig_blockwise_heatmap} indicate that block heterogeneity is informative in distinguishing CNNs and Transformers.
In the following, we will show that the block heterogeneity is strongly correlated with the performance gap between  SGD and Adam on Transformers.






\vspace{-0.2cm}
\subsection{SGD Performs Worse than Adam on Various Tasks with Block Heterogeneity}
\label{sec_mlp}
\vspace{-0.2cm}

Figure \ref{fig_blockwise_spectrum} and \ref{fig_blockwise_heatmap} have shown that {\bf (1)} SGD is worse than Adam on Transformers. {\bf (2)} Transformers have block heterogeneity. Now we further link block heterogeneity to SGD's unsatisfactory performance on {\bf non-Transformer} models.  This would directly establish a connection between ``block heterogeneity" and ``why SGD is worse than Adam", without going through Transformers or attention blocks as an intermediary. We consider one man-made example and one real-world example.

\vspace{-0.1cm}

\textbf{Example 1: A man-made MLP.}
We consider a 4-layer MLP on MNIST and change the degree of heterogeneity by scaling each layer by constant $c$.  Figure \ref{fig_mlp_gap} (a) shows SGD gradually performs worse than Adam as heterogeneity grows. 
\begin{figure}[htbp]
\vspace{-0.5cm}
    \centering
        \subfigure[Final performance of Adam and SGD on a man-made MLP]{\includegraphics[width=0.3\textwidth]{images/gap_converged.pdf}}
        \subfigure[MLP-mixer]{\includegraphics[width=0.3\textwidth]{images/mlp_mixer_spectrum.png}}
        \subfigure[SGD v.s. Adam on MLP-mixer]{\includegraphics[width=0.3\textwidth]{images/trainacc_cosine_mlp.pdf}}
        \vspace{-0.2cm}
        \hfill
    \caption{ (a)  SGD v.s. Adam on a man-made MLP with different degrees of heterogeneity $c$.  Each point records the best-converged test accuracy under the learning rate grid search. SGD performs worse as heterogeneity grows. (b) The JS distance among blockwise Hessian spectra for MLP-mixer \citep{tolstikhin2021mlp} at initialization. We observe heterogeneity. (c) SGD performs worse than Adam on MLP-mixer. }
    \label{fig_mlp_gap}
    \vspace{-0.3cm}
\end{figure}

\textbf{Example 2: MLP-mixer.}  We consider MLP-mixer \citep{tolstikhin2021mlp}, a famous all-MLP architecture that outperforms CNNs and ViTs on some vision tasks. 
Figure \ref{fig_mlp_gap} (b) (c) show that the initial Hessian of MLP-mixer has block heterogeneity and SGD lags behind Adam on this architecture.


We summarize the findings so far in {\bf Observation 2}.

\begin{snugshade}
\begin{center}
{\bf Observation 2:} For all tasks that we checked, SGD is worse than Adam when block heterogeneity exists, regardless of whether Transformers or attention mechanisms are utilized.
\end{center}
\vspace{-0.2cm}
\end{snugshade}




\vspace{-0.2cm}
\subsection{Reduced Block Heterogeneity in Pre-trained Transformers}
\vspace{-0.2cm}

We remark that different Transformers exhibit different levels of block heterogeneity. Although all examined Transformers show strong block heterogeneity, we find that this heterogeneity can be mitigated, resulting in less performance deterioration for SGD. As illustrated in Figure \ref{fig_sft_gap}, pre-trained GPT2 on SFT tasks can exhibit less block heterogeneity compared to pre-training GPT2 from scratch (Figure \ref{fig_blockwise_heatmap} (f)).  In this case, although SGD is still slower than Adam, it achieves a similar loss at convergence. Compared with training GPT2 from scratch (Figure \ref{fig:nlp_figure} (d) in Appendix \ref{appendix_more_discussion}), the performance gap between SGD and Adam is significantly narrowed down. These findings suggest that the heterogeneity induced by architectural design can be alleviated by selecting ``good'' weights. This partly explains why simpler methods like SGD and even its zeroth-order version can still be effective for fine-tuning language models, albeit with slower convergence \citep{lv2023full, malladi2023fine}.


\begin{figure}[htbp]
\vspace{-0.1cm}
    \centering
        \subfigure[GPT2 (pre-trained)]{\includegraphics[width=0.28\textwidth]{images/heat_map_alpaca_ft2_50000.pdf}} \hspace{4mm}
        \subfigure[SGD v.s. Adam on fine-tuning GPT2 (pre-trained)]{\includegraphics[width=0.28\textwidth]{images/finetune_alpaca_4example_0425_gridsearch.pdf}}
        \vspace{-0.3cm}
        \hfill
    \caption{ (a) The JS distance among blockwise Hessian spectra for GPT2 (pre-trained) when fine-tuning on Alpaca Eval. (b) SGD could reach similar loss as Adam.}  %
    \label{fig_sft_gap}
    \vspace{-0.2cm}
\end{figure}


In Figure \ref
{fig:hessian_along_training_vit} in Appendix \ref{appendix_more_discussion}, we further report the evolution of the block heterogeneity of ViT-base along the training.  Similarly to GPT2 in Figure \ref{fig_sft_gap}, we find that the block heterogeneity of ViT-base tends to reduce after the training. In addition, we find that  SGD can perform better when initializing at the weight with less heterogeneity, e.g., initializing at 50\% total training steps. We hypothesize that ``the attenuation of Hessian heterogeneity" is a common phenomenon after training, and we leave detailed investigation as a future direction.


\begin{snugshade}
\begin{center}
{\bf Observation 3:} Block heterogeneity in Hessian tends to reduce after (pre)-training. 
\end{center}
\vspace{-0.2cm}
\end{snugshade}






\vspace{-0.2cm}
\subsection{Implication on Choosing SGD or Adam}
\label{sec_choose_sgd_or_adam}
\vspace{-0.2cm}
We have shown that SGD  can largely underperform Adam on various architectures. This leads to an intriguing question: {\bf Can we predict the incompetence of SGD before the training begins}?

Our findings can bring up an empirical guidance: we can compute the blockwise spectrum of initial Hessian, and then decide whether to use Adam or SGD. Such a method could be useful in scenarios in training large models that are not mainstream Transformers or CNNs, e.g., Mamba \citep{gu2023mamba}. In these cases, there is not much prior experience in choosing optimizers. It would be intriguing to decide whether SGD is suitable for the task before the training is launched. 
One might argue that simple trial is enough: 
try both SGD and Adam;
if Adam is remarkably better, then pick Adam;
if Adam and SGD are similar, then pick SGD. 
Nevertheless, this simple approach 
may not be easy for large models.
 First, for large models, it may take days to 
 know one run of an algorthm is good or not.
 Second, it requires tuning hyperparameters at least a few times
 to get a reasonably good judgement,  making the cost of trial even higher. 





 
We here propose a quantitative metric that could predict the incompetence of SGD
before the training. With the help of this metric, we could save much expense on the trial and error for SGD.
The metric is simply the averaged JS distance among blockwise Hessian spectra at initialization, i.e., the averaged value in the heatmap of Figure \ref{fig_blockwise_heatmap}. We denote it as $JS^0$. We present $JS^0$ of various models in Table \ref{tab_cnn_transformer_js_distance}. Note that $JS^0$ establishes a quantitative difference between the loss landscape of Transformers and CNNs. Further, $JS^0$ is independent of optimizers and could be checked before training. 



To validate the effectiveness of the quantitative metric $JS^0$, we summarize $JS^0$ of different models and the corresponding SGD performance  in Figure \ref{fig:metric_validation}.
We find that the performance gap between SGD and Adam becomes greater as $JS^0$ increases. Thus, $JS^0$ can serve as a potential indicator to predict whether SGD may underperform Adam. 



\begin{table}[t]
\vspace{-0.8cm}
    \centering
    \vspace{-0.3cm}
    \caption{  $JS^0$ denotes the average JS distance between the initial Hessian spectra of each pair of parameter blocks. A larger $JS^0$ suggests that the task is more difficult for SGD. 
    } 
    \begin{tabular}{c|c c c c c c  c }
    \toprule
    Model &ResNet18 &VGG16  & GPT2 (pretrained)& 
 MLP-mixer  & BERT   & GPT2 & ViT-base\\
    \hline
    $JS^0$ &0.10 & 0.09& 18.84& 34.90
  & 53.38  & 83.23   & 286.41\\
      \bottomrule
    \end{tabular}
\label{tab_cnn_transformer_js_distance}
\vspace{-0.5cm}
\label{tab:JS_dist}
\end{table}

\begin{figure}[htbp]
    \centering
    \vspace{-0.2cm}
\includegraphics[width=0.90\textwidth]{images/metric_validation.png}
\vspace{-0.2cm}
    \caption{Comparison of $JS^0$ and the performance of SGD on different models. We find the performance gap between SGD and Adam becomes greater as $JS^0$ increases. 
    }
    \vspace{-0.2cm}
    \label{fig:metric_validation}
\end{figure}

Finally, we remark $JS^0$ is rather expensive to compute due to the overhead of SQL: it requires comparable time to one training run. Fortunately, we find the original SQL is rather redundant for measuring hessian heterogeneity. We propose some simple tricks to significantly reduce the computation time, while still effectively detecting the Hessian heterogeneity. We call it simplified SQL and we present it in Table \ref{tab:simplified_sql} in  Appendix \ref{appendix_more_discussion}. As a result, the simplified SQL can obtain the same message as in Table \ref{tab:JS_dist} while only taking negligible time (e.g., $<0.001$s for ResNet18).





\section{Case Study of Quadratic Models and Preliminary Theory}
\label{sec_quadratic}
\vspace{-0.3cm}

Now we study quadratic functions with block diagonal Hessian, with or without block heterogeneity. 
Note that insights on quadratic models could be important for understanding realistic NNs, as mentioned by researchers such as
\citet{lecun2002efficient} and OpenAI team
\citep{kaplan2020scaling}. 


\textbf{Setups and additional notations.}
We consider the following quadratic minimization.
\[\min _{w \in \mathbb{R}^d} \mathcal{L}(w ) =\frac{1}{2} w^T H w- h^T w,\]
where $H \in \mathbb{R}^{d \times d}$ is positive definite and $h \in \mathbb{R}^{d} $.  We denote $\mathcal{L}^*$ as the minimum value of $\mathcal{L}(w)$. 
We set $H$ as a block diagonal matrix:
$H = \operatorname{diag}(H_1, \cdots ,H_L)$, where  $H_l  \in \mathbb{R}^{d_l \times d_l}$ and $d = \sum_{l=1}^L d_l$.  We use $w_l \in \mathbb{R}^{d_l}$ to denote the variable in the $l$-th block and $w = (w_1^T, \cdots, w_L^T )^T \in \mathbb{R}^{d}$. Similarly for $h_l\in \mathbb{R}^{d_l}$. Similarly, we use $[\nabla L(w)]_l\in \mathbb{R}^{d_l}$ to denote the gradient in the $l$-th block and
denote $[\mathcal{L} (w)]_l = \frac{1}{2} (w_l^t)^T  H_l w_l^t  - h_l^T w_l$ as the objective function w.r.t. the $l$-th block. Note that $\mathcal{L} (w) = \sum_{l = 1}^L [\mathcal{L} (w)]_l$. We denote $\lambda_1 \geq \lambda_2 \cdots \geq \lambda_d$ as the eigenvalues of $H$. Similarly for $\lambda_{l,1}\cdots \lambda_{l,d_l} $.  We denote $\kappa = \frac{\lambda_1}{\lambda_d}$ and $\kappa_l =  \frac{\lambda_{l,1}}{\lambda_{l,d_l}}$ as the condition number of $H$ and $H_l$, respectively.
We say an algorithm has complexity $\tilde{\mathcal{O}}(C)$ if it takes $\mathcal{O}(C \log (1 / \epsilon))$ iterations to achieve error $\frac{\mathcal{L}(w)-\mathcal{L}^*}{\mathcal{L}\left(w^0\right)-\mathcal{L}^*} \leq \epsilon$, where $w^0$ is the initial point.


\vspace{-0.2cm}
\subsection{Experimental Observations}
\label{sec_quadratic_exp}
\vspace{-0.2cm}



 We consider four types of Hessian $H$ as follows. For all cases, we set condition number = 5000.  
\begin{itemize}[topsep=1pt, parsep=1pt, partopsep=1pt, leftmargin=*]

    \item {\bf Case 1: Hessian with Transformer-type spectra.} We choose $L = 4$ and $d_l = 25$. For $l \in [L]$, we  construct  $H_l = Q_l \Lambda_l Q_l^T$ where $Q_l$ are matrices with i.i.d. standard Gassian entries and $\Lambda_l$ are diagonal matrices. For the diagonal elements in  $\Lambda_l$, we sample $d_l$ numbers according to the spectrum of the embedding layer; 3rd Query, 3rd Value, 3rd MLP (\texttt{fc} layer) in GPT2.  Shifting and proportional scaling are performed to ensure all elements in $\Lambda_l$ lie in the interval $[1, 5000]$.  This ensures strong convexity and controls the condition number of $H$ equals  $5000$. The spectra of $H_l$ are in Figure \ref{fig:heter-block} in Appendix \ref
    {appendix_more_discussion}. We choose $h=0$ for all cases.
    
    
    \item {\bf Case 2: Hessian with CNN-type spectra.} We use the same setup as in \textbf{Case 1}. For the diagonal elements in $\Lambda_l$, we sample $d_l$ numbers according to the spectrum of the 1st to 4th convolution layers in ResNet18. We then shift and scale $\Lambda_l$ to the interval $[1,5000]$ to ensure strong convexity and a condition number of 5000. The spectra of $H_l$ are shown in Figure \ref{fig:homo-block} in Appendix \ref{appendix_more_discussion}.
    
    \item {\bf Case 3: Hessian with simplified heterogeneous spectra.} We choose $L = 3$ and $d_l = 3$. 
     For $l \in [L]$, we  construct  $H_l = Q_l \Lambda_l Q_l^T$ where $Q_l$ are independent standard Gassian random matrix and $\Lambda_l$ are diagonal matrices.  We set the diagonal elements of $\Lambda_l$ as  $\{1, 2, 3\}, \{99, 100, 101\}, \{4998, 4999, 5000\}$ for $l = 1,2,3$, respectively. The spectra of $H_l$ are different due to their different supports. The condition number of Hessian $H$ is $5000$.
    \item {\bf Case 4: Hessian with simplified homogeneous spectra.} We consider the same setup as {\bf Case 3}. We set the diagonal elements of $\Lambda_l$ as  $\{1,  99, 4998\}, \{2, 100, 4999\}, \{3, 101, 5000\}$ for $l = 1,2,3$, respectively. The spectra of $H_l$ are similar. The condition number is 5000. 
\end{itemize}


Now we study two types of optimizers: one that assigns a single learning rate for all blocks, and one that assign different learning rates across blocks. 




\begin{itemize}[topsep=1pt, parsep=1pt, partopsep=1pt, leftmargin=*]
    \item {\bf Single-learning-rate optimizer.} We study gradient descent (GD).
    \begin{small}
    \begin{equation} \label{eq_update_gd}
        w^{t+1} = w^{t} - \eta \nabla \mathcal{L}(w) = w^{t} - \eta  (Hw^t -h)
    \end{equation}
    \end{small}
     We use the optimal learning rate $\eta = \frac{2}{\mu + L}$ \citep{nesterov2013introductory}. We use standard Gaussian initialization.
    \item {\bf Coordinate-wise-learning-rate optimizer.} We study Adam with a constant learning rate and with no bias correction for simplicity (Algorithm \ref{alg_adam_no_bias}).  We set $\beta_1 = 0$ to erase the effect of momentum. {\bf This helps us to focus on the effect of coordinate-wise learning rate} (or the effect of diagonal preconditioning)  in Adam. We use $\epsilon = 0$. We consider $\beta_2 = 1$ and $\beta_2 = 0.99$, respectively.  When $\beta_2 = 1$, Adam assigns coordinate-wise learning rates according to the initial gradient, but these learning rates are fixed along iteration. The update rule is as follows.
   \begin{small} \begin{equation}\label{eq_update_adam_1}
        w^{t+1} = w^{t} - \eta  (D_{Adam}^0 )^{-1}    \nabla \mathcal{L}(w) = w^{t} - \eta (D_{Adam}^0 )^{-1}  ( Hw^t - h), 
    \end{equation}
    \end{small}
    where $ D_{Adam}^0 = \operatorname{diag}(\nabla \mathcal{L}(w^0)\circ \nabla \mathcal{L}(w^0))^{\frac{1}{2}} $ and $\nabla \mathcal{L}(w^0) = Hw^0 - h$.
    When $\beta_2  < 1$, the coordinate-wise learning rates adaptively change along iteration. The update rule is as follows (note that $\nabla \mathcal{L}(w^k) = Hw^k - h$.).
   \begin{small}
    \begin{equation}\label{eq_update_adam}
        w^{t+1} = w^{t} - \eta  (D_{Adam}^t )^{-1}    \nabla \mathcal{L}(w) = w^{t} - \eta (D_{Adam}^t )^{-1} ( Hw^t - h),  \quad \text{where}
    \end{equation}

    \vspace{-0.3cm}
      {\[ D_{Adam}^t = \operatorname{diag}\left((1-\beta_2)\left(\sum_{k = 1}^t \beta_2^{t-k} \nabla \mathcal{L}(w^k) \circ \nabla \mathcal{L}(w^k) \right)   + \beta^t \operatorname{diag}(\nabla \mathcal{L}(w^0)\circ \nabla \mathcal{L}(w^0))
 \right)^\frac{1}{2} \]}
 \end{small}
    We grid search $\eta$ and use the standard Gaussian  initialization.
    We remark that when $\beta_2  < 1$,  Adam would  bounce among non-optimal points. This will be shown in Proposition \ref{thm_adam_limit_cycle}.
\end{itemize}







\begin{figure}[t]
    \centering
    \vspace{-0.8cm}
    \subfigure[Hessian with GPT2 block-wise spectrum]{\includegraphics[width=0.24\textwidth]{images/0304-case-24-dimension-100-lr-1.0-seed-32-beta2-099.pdf}}
    \subfigure[Hessian with ResNet18 blockwise spectrum]{\includegraphics[width=0.24\textwidth]{images/0304-case-25-dimension-100-lr-1.0-seed-32-beta2-099.pdf}}
    \subfigure[Hessian with simplified  heterogeneous blocks]{\includegraphics[width=0.24\textwidth]{images/0303-case-14-dimension-9-lr-1.0-seed-32-beta2-099.pdf}}
    \subfigure[Hessian with simplified   homogeneous blocks]{\includegraphics[width=0.24\textwidth]{images/0303-case-15-dimension-9-lr-1.0-seed-32-beta2-099.pdf}}
    \vspace{-0.3cm}
    \caption{The performance of Adam and GD on homo/heterogeneous quadratic problems. 
    The condition numbers of Hessian equal to 5000 for all four cases. 
    When blocks are heterogeneous, GD largely lags behind Adam, and GD performs similarly to  Adam if otherwise. }
    \label{fig_quadratic}
    \vspace{-0.3cm}
\end{figure}



\textbf{Summary of experimental observations.} 
Figure \ref{fig_quadratic} presents two phenomena.
For Hessian with heterogeneous blocks ({\bf Case 1 and 3}), GD largely lags behind Adam. For Hessian with homogeneous blocks ({\bf Case 2 and 4}), GD is on par with Adam.   
We emphasize that all Hessians have the same condition number. Further, Hessian in {\bf  Case 3} and {\bf 4} share all the eigenvalues (not just the extreme ones).
The gap between Adam and GD is purely due to the different blockwise spectra caused by the different locations of eigenvalues. {\bf  Case 3} and {\bf 4} help reveal the causal relation between ``block heterogeneity in Hessian" and  ``GD is worse than Adam". 
We hypothesize that GD performs badly because it uses one single learning rate for all blocks, which cannot handle the heterogeneity
among blocks. Such heterogeneity can be better handled using different learning rates across blocks, as designed in Adam. 


\vspace{-0.2cm}
\subsection{Initial Theoretical Results}
\label{sec_theory}
\vspace{-0.2cm}

We now provide initial theoretical results to characterize how GD lags behind Adam in problems with heterogenous Hessian.  
Note that classical optimization theory depicts the rate of first-order methods by the condition number of the full Hessian $\kappa$. However,  we point out that $\kappa$ is not informative enough to describe the performance gap in Figure \ref{fig_quadratic} since $\kappa$ is the same in all four cases. 
To distinguish Adam and GD, we need to utilize more fine-grained quantities like blockwise spectra of sub-matrices.



Note that the blockwise spectrum is not common in the optimization area.  The most related notion is perhaps ``block Lipschitz constant"
 \citep{beck2013convergence} for studying block coordinate descent (BCD) type methods, but it was not linked to 
 the performance of SGD or Adam before.
To our knowledge, we are not aware of any theory of Adam or GD  built on the block diagonal structures or the blockwise spectra of Hessian. 
We now make an initial attempt in this direction. 
We first present the lower bound for GD. 


\begin{prop}
\label{thm_gd_lower_bd}
   (Lower bound for GD.)  Consider  $\min _w \mathcal{L}(w) =\frac{1}{2} w^T H w- h^T w$ where $H \in \mathbb{R}^{d \times d}$ is positive definite and $h \in \mathbb{R}^{d} $. Let $w_{GD}^t$ be the output of GD  after $t$ steps.  There exists a block diagonal matrix $H $, $h$ and an initial point  $w^0$, s.t., for any $\eta$, we have: 
   \begin{small}
\begin{equation}
\label{eq_gd}
       \mathcal{L}(w_{GD}^{t+1})-\mathcal{L}^* \geq  \left(1  - \frac{2}{ \kappa + 1} \right)
\left(\mathcal{L}(w_{GD}^t)  - \mathcal{L}^*\right)
\end{equation}
\end{small}
where $\kappa$ is the condition number of $H$.
\end{prop}
\vspace{-0.2cm}
Proposition \ref{thm_gd_lower_bd} shows that GD has complexity $\tilde{\mathcal{O}}( \kappa)$ and such complexity is tight. Now we prove that Adam can achieves better complexity. This is because it chooses different learning rates for different block sub-matrix $H_l$ via its diagonal preconditinoner $D_{Adam}^0$.
We consider generic random initialization that covers commonly used distributions such as Gaussian, Uniform, etc.


\begin{asmp}
\label{assum_initialization}
    (Random initialization.) Assume the initialization $w^0$ is sampled from a continuous distribution, i.e., the probability measure (induced by  $w^0$) of any zero-Lebesgue-measure set is 0.
\end{asmp}


\begin{thm}
\label{thm_adam}
   (Upper bound for Adam with $\beta_2 = 1$.) 
  Consider the same setting as Proposition \ref{thm_gd_lower_bd} and consider Adam with $\beta_1 = 0$ and $\beta_2 = 1$ as in \eqref{eq_update_adam_1}.  Assume the initialization satisfies Assumption \ref{assum_initialization}.
  Let $w_{Adam}^t$ be the output of Adam after $t$ steps. Let $\eta = \min_{l \in [L]}\frac{1}{C_{l,1}}$. Then w.p.1., we have
  \begin{small}
\begin{equation}
\label{eq_adam}
       \mathcal{L}(w_{Adam}^{t+1})-\mathcal{L}^* \leq  \max_{l\in [L]} \left(1  - \frac{1}{   \kappa_{Adam,l} } \right)
\left(\mathcal{L}(w_{Adam}^t)  - \mathcal{L}^*\right)
\end{equation}
\end{small}
where $\kappa_{Adam,l} = r \kappa_l$, $\kappa_l$ is the condition number of $H_l$, constant $r$  relates to %
$w^0$ defined as:
\begin{small}
{
\begin{equation}
\label{eq_constant_r}
    r =  
 \frac{ \max_{l \in [L]} C_{l,2}^2 }{  \min_{l \in [L]}C_{l,1}^2 } , \text{ where } C_{l,1} = \min_{i \in [d_l]} \frac{|[\nabla \mathcal{L}(w^0)]_{l,i} |}{\lambda_{l,1}},  C_{l,2} = \max_{i \in [d_l]} \frac{|[\nabla \mathcal{L}(w^0)]_{l,i}| }{\lambda_{l,1}}.
\end{equation} }
\end{small}
\end{thm}

The proofs of the above theorems are shown in Appendix \ref{appendix_proofs}.  Theorem \ref{thm_adam} states that Adam (with $\beta_2 = 1$) has complexity $\tilde{O}\left ( r \cdot\max_{l\in [L]} \kappa_l \right)$. We note that coefficient $r$ depends on the ratio between initial gradient and the principal eigenvalue for each block, and smaller ratio would give faster convergence.   We further remark that condition $\beta_2 = 1 $ is necessary because any $\beta_2 < 1$ causes non-convergence issue \citep{bock2019non,da2020general}. We restate their results in Proposition  \ref{thm_adam_limit_cycle}. The non-convergence is also observed in Figure \ref{fig_quadratic} (c), where we find that the iterates of Adam quickly converge to near-optimal solutions, and then bounce back. 
As such, $\beta_2 = 1$ is necessary for asymptotic analysis. The analysis for $\beta_2 = 1$ is still meaningful since it still shows the effect of Adam's preconditioner.

As shown in \citep{da2020general}, the non-convergence is due to the constant learning rate. 
Reducing the learning rate reduces the gap between $ \mathcal{L}(w_{Adam}^t)$ and $\mathcal{L}^*$, but does not remove it.

\begin{prop}
\label{thm_adam_limit_cycle}
        (Non-convergence of constant-learning-rate Adam with $\beta_2 < 1$.) \citep[Proposition 12, Figure 1]{da2020general} Consider  $\min _{w\in \mathbb{R}} \mathcal{L}(w) =\frac{1}{2} w^2$. Consider Adam with $\beta_ 1 = 0$ and $\beta_2 <1$ as in \eqref{eq_update_adam}.  Let $w_{Adam}^t$ be the output of Adam after $t$ steps. There exists a discrete limit cycle for  \eqref{eq_update_adam} and  {\small$ \liminf_{t\rightarrow \infty}  \left(\mathcal{L}(w_{Adam}^t) -  \mathcal{L}^* \right) > 0$}.
\end{prop}

We now compare the complexity of Adam and that of  GD.
By Theorem \ref{thm_adam}, Adam is faster than GD when $r \cdot\max_{l \in [L]}  \kappa_l \leq \kappa $. In the quadratic model with heterogeneous blocks ({\bf Case 3}),  our simulation over 1000 trials shows that $r \leq 1000$ with probability $\geq \frac{2}{3}$ when using standard Gaussian random initialization. Since $\max_{l \in [L]}  \kappa_l \approx 1$, we have $r \cdot \max_{l \in [L]} \kappa_l \leq 1000$, w.h.p., and is about $5 \times$ smaller than $\kappa = 5000$. So Adam could be $5 \times$ faster than GD, w.h.p.. This is indeed observed in Figure \ref{fig_quadratic} where Adam outperforms GD by a significant margin. We summarize the complexity of GD and Adam in Table \ref{tab_complexity}. 


{\bf Remark: some common misconceptions.} During the review process, we find that readers might conclude that ``Theorem \ref{thm_adam} implies Adam under homogeneity has worse complexity than Adam under heterogeneity". We now clarify that this claim is {\it not} correct, and there is no conclusion on ``whether Adam under homogeneity is faster or slower than Adam under heterogeneity". Similarly, Theorem \ref{thm_adam}  does {\it not} imply ``Adam always converges similarly as GD under homogeneity". Though it is observed on CNNs, there is no general conclusion of this kind.  For interested readers, we provide a detailed explanation in Appendix \ref{appendix_more_discussion}. 

\begin{table}[thbp]
    \centering
    \vspace{-0.4cm}
    \caption{ The complexity of GD and Adam for minimizing a strongly convex quadratic function with block diagonal Hessian. The symbol {\cross} means non-convergence.  $\kappa$ and $\kappa_l$ denote the condition number of the full Hessian and the block submatrix, respectively. $r$ is defined in \eqref{eq_constant_r}.
    } 
    \begin{tabular}{c|c c c}
    \toprule
    Optimizer &GD &Adam with & Adam with\\
    & &  $\beta_1 = 0$ and $\beta_2 = 1$ \eqref{eq_update_adam_1} & $\beta_1 = 0$ and $\beta_2 <1$ \eqref{eq_update_adam} \\
    \hline
      Complexity  &  $\tilde{O}(\kappa)$   &  $\tilde{O}\left( r \cdot \max_{l\in [L]} \kappa_l  \right)$&  \cross   \\
      \bottomrule
    \end{tabular}
    \label{tab_complexity}
    \vspace{-0.4cm}
\end{table}

\paragraph{How to obtain a tighter complexity bound of Adam?} 
It is valid to ask whether the complexity upper bound in Theorem \ref{thm_adam} can be tightened, e.g., improve the factor of $r$. 
We point out it would be difficult if there is no extra structure on $H_l$. A key technical step is to bound the condition number of the preconditioned matrix $\kappa\left((D_{Adam,l}^0)^{-1} H_l\right)$.
Intuitively, a diagonal preconditioner of $H_l$ is powerful when $H_l$ itself has a near-diagonal structure, e.g., pure diagonal, tridiagonal or diagonal dominant \citep{forsythe1955best}.  Unfortunately, it is unclear whether these structures hold in Transformers. Without any assumption on  $H_l$, we find that the diagonal preconditioner of $D_{Adam}^0$ could {\it increase} the condition number. For instance, when using standard Gaussian initialization, in {\bf case 3}, we find $\kappa\left((D_{Adam,l}^0)^{-1} H_l\right)$ equals  $7.09 \kappa_1$,  $ 18.98 \kappa_2$,  $ 18.76 \kappa_3$ for the 3 blocks, respectively (all averaged over 1000 trials). 
It would be interesting to explore if there are special structures of $H_l$ in Transformers such that Adam preconditioner can reduce $\kappa_l$, rather than increase it. We leave it as a future direction. 


{\bf More discussions on the theoretical advantage of Adam.} Although Adam preconditioner might not always reduce the ``local" condition number $\kappa_l$, the coefficient in the complexity is now {\bf independent of the ``global" condition number $\kappa$}.  As argued above, such changes in coefficient could lead to considerable improvement over GD.  Such improvement in complexity is attributed to the block diagonal structure in Hessian as well as its heterogeneous blockwise spectrum.  To our knowledge, such improvement is not shown in the existing literature. One possible reason is that: for the optimization community, it is very rare to analyze (near-) block-diagonal Hessian structure since typical problems do not have such structure. For instance, in the classical non-linear programming dataset \citep{lavezzi2022nonlinear}, all problems have non-block-diagonal Hessian.
We suggest a different perspective to characterize modern optimization problems.
We believe our perspective is new because it is built upon multiple non-trivial findings.


In summary, our theory indicates that: for problems with block heterogeneity,   the single-learning rate methods like GD can largely lag behind coordinate-wise learning rate methods like Adam.  



\vspace{-0.2cm}
\section{Conclusion}
\label{sec_conclusion}
\vspace{-0.2cm}
In this work, we explore why  SGD largely underperforms Adam on Transformers.  
we establish a phenomenon called block heterogeneity in Hessian and link it to the performance gap between Adam and SGD. We numerically verify our claim on various Transformers, CNNs, MLPs, and quadratic problems. 
Initial theory is also provided to support the claim.

\newpage 


\section*{Acknowledgements}
Yushun Zhang would like to thank Yinyu Ye, Wentao Ding, Guiyu Hong, Yingru Li, and Bohan Wang for the valuable discussions.  The work of Ruoyu Sun is supported by NSFC (No. 12326608); Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone Project (No.HZQSWS-KCCYB-2024016); University Development Fund UDF01001491, the Chinese University of Hong Kong, Shenzhen; Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001). The work of Z.-Q. Luo was supported by the Guangdong Major Project of  Basic and Applied Basic Research (No.2023B0303000001), the Guangdong Provincial Key Laboratory of Big Data Computing, and the National Key Research and Development Project under grant 2022YFA1003900.

\section*{Broader Impacts}
\label{sec_broader_impact}
We explore why SGD performs worse than Adam for
training Transformers. Our work can help
the community better understand large AI model training. However, it would
be a potential threat if the AI models are used for illegal
usage.



\bibliographystyle{abbrvnat}
\bibliography{reference.bib}




\appendix
\include{appendix/appendix}



\end{document}
