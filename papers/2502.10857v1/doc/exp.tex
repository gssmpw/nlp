\section{Experiments}
\label{sec:exp}

\subsection{Experiments Setting}
We utilize a comprehensive evaluation benchmark ChatEDA-bench~\cite{wu2024chateda}, which comprises 50 distinct tasks with target APIs from OpenROAD~\cite{ajayi2019openroad}, to evaluate the performance of our EDAid and ChipLlama models.
Moreover, we also design a benchmark, iEDA-bench, based on iEDA~\cite{li2024ieda} comprising 50 distinct tasks to evaluate the generalization to any EDA tools on different platforms of ChipLlama.
ChatEDA-bench and iEDA-bench use the accuracy of the generated EDA script as the evaluation metric.
Specifically, accuracy is related to the successful EDA flow automated through the correct generated EDA script.

\begin{figure}[!tb]
    \centering
    \includegraphics[width=0.98\linewidth]{figs/figure10.pdf} 
    \caption{Examples of evaluation benchmarks. 
    } 
    \label{fig:exp_bench}
\end{figure}

\subsection{Examples of Evaluation Benchmarks}
\label{sec:eval}
As illustrated in \Cref{fig:exp_bench}, we show some examples of our evaluation benchmarks including ChatEDA-bench~\cite{wu2024chateda} and iEDA-bench, both of which are comprehensive evaluation benchmarks comprising 50 distinct tasks including three distinct categories: simple flow calls (30\%), complex flow calls (30\%), and parameter flow calls (40\%). 
%The diversity of these tasks ensures a rigorous evaluation of agent systems across various scenarios.

\minisection{Simple Flow Calls}
This task requires the successful execution of the whole process, including evaluation.
These cases test the fundamental application of LLMs in EDA flow automation. 

\minisection{Complex Flow Calls}
This task requires a higher proficiency in EDA tool usage, including traversing parameters, which examines logical reasoning and understanding of each argument of EDA APIs.

\minisection{Parameter Tuner Calls}
This task requires agent systems to provide a parameter-tuning solution, which is a vital step for EDA considering the complexity of the entire process.

\begin{table}[!t]
\centering
%\renewcommand{\arraystretch}{1.08}
\resizebox{1\linewidth}{!}{
\scriptsize
\begin{tabular}{c|c|c|c}
\toprule
\multirow{2}{*}{System} & \multirow{2}{*}{Powered LLM} &  ChatEDA-bench & iEDA-bench \\
& & Acc. & Acc. \\ 
\midrule
ChatEDA & GPT-3.5$^\diamondsuit$ & 28\% & 30\% \\
ChatEDA & GPT-4$^\diamondsuit$ & 62\% & 70\% \\
ChatEDA & AutoMage-70B$^\diamondsuit$ & 74\% & - \\
ChatEDA & AutoMage2-70B$^\diamondsuit$ & 82\% & - \\
\midrule
EDAid & ChipLlama-8B & 88\% & 84\% \\
EDAid & ChipLlama-70B & \textbf{100\%} & \textbf{100\%} \\
\bottomrule
\end{tabular}
}
\begin{tablenotes}[flushleft] \tiny
\item$^\diamondsuit$ The accuracy values of GPT-3.5, GPT-4 and AutoMage models on the ChatEDA-bench are directly cited from the ChatEDA~\cite{wu2024chateda}.
Moreover, we can only evaluate AutoMage models on the ChatEDA-bench due to the unavailability of closed-source models.
\end{tablenotes} 
\caption{The main results of EDA script generation on ChatEDA-bench~\cite{wu2024chateda} and iEDA-bench.}
\label{table:scripteval}
\end{table}

\subsection{Implementation Details} 
We employ QLoRA~\cite{dettmers2024qlora} for the hybrid instruction tuning of ChipLlama models based on Llama3~\cite{dubey2024llama3} models. 
This involves adopting a constant learning rate schedule with a warm-up ratio of 0.03, utilizing the paged AdamW optimizer~\cite{dettmers20218bit} with a learning rate of $1 \times 10^{-4}$, no weight decay, a batch size of 128, and a sequence length of 4096 tokens. 
The models are fine-tuned for 1 epoch on 16$\times$A100 GPUs with 80G memory each.
After hybrid instruction tuning, we obtain ChipLlama-8B and ChipLlama-70B, two expert LLMs for EDA flow automation.
For clarity and differentiation, the single-agent system refers to the divergent-thoughts agent (role $R_{0}$) in EDAid, while the multi-agent system denotes our EDAid. 

\subsection{Main Evaluation Results}
In this study, we integrate LLMs into our EDAid to conduct a comprehensive evaluation. 
According to~\cite{wu2024chateda}, AutoMage models serve as ``brains'' of the ChatEDA system to execute EDA tasks. 
Concurrently, we incorporate GPT-3.5~\cite{brown2020gpt3} and GPT-4~\cite{openai2023gpt4} into the single-agent system (ChatEDA) through their official APIs.
Importantly, only open-source models are utilized within our multi-agent system, as the decision-making process requires calculating the output logits of LLMs.

As demonstrated in \Cref{table:scripteval}, our multi-agent system, EDAid, powered by ChipLlama-70B, achieves the SOTA performance over all other previous SOTA LLM-powered systems, establishing a significant margin.
To explore the generalization of our EDAid on utilizing EDA tools, we also evaluate it on iEDA-bench, which requires LLMs to use EDA tools from the iEDA platform.
Similarly, EDAid powered by ChipLlama-70B also demonstrates outstanding accuracy.
Our multi-agent collaboration system manifests its potential in automating the EDA flow by interfacing with diverse EDA tools from various vendors.

\begin{figure*}[tb!]
    \centering
    \includegraphics[width=0.975\linewidth]{figs/figure11.pdf} 
    \caption{Divergent thoughts. Right: the divergent-thoughts agent (role $R_{0}$) generates correct task planning pathways and EDA script. Left: the divergent-thoughts agent (role $R_{0}$) makes mistakes in intermediate steps.}
    \label{fig:cases1}
\end{figure*}

\begin{table}[!t]
\centering
\setlength\tabcolsep{2.4pt}
%\renewcommand{\arraystretch}{1.08}
\resizebox{1\linewidth}{!}{
\scriptsize
\begin{tabular}{c|c|c|c|c}
\toprule
\multirow{2}{*}{System} & \multirow{2}{*}{Base LLM} & Hybrid & ChatEDA-bench & iEDA-bench \\
& & Instruction Tuning & Acc. & Acc. \\ 
\midrule
Single-Agent & \multirow{2}{*}{Llama3-8B} & \ding{55} & \textbf{78\%} & 50\% \\
Single-Agent & & \ding{51} & \textbf{78\%} & \textbf{76\%} \\
\midrule
Single-Agent & \multirow{2}{*}{Llama3-70B} & \ding{55} & 88\% & 74\% \\
Single-Agent & & \ding{51} & \textbf{94\%} & \textbf{96\%} \\
\bottomrule
\end{tabular}
}
\caption{Ablation study on hybrid instruction tuning.}
\label{table:ab-hybrid}
\end{table}

\begin{table}[!t]
\centering
\setlength\tabcolsep{2.4pt}
%\renewcommand{\arraystretch}{1.2}
\resizebox{1\linewidth}{!}{
\scriptsize
\begin{tabular}{c|c|c|c|c|c}
\toprule
& &  \multicolumn{2}{c|}{ChatEDA-bench} & \multicolumn{2}{c}{iEDA-bench} \\
\multirow{2}{*}{System} & \multirow{2}{*}{Powered LLM} & \multicolumn{2}{c|}{Acc.} & \multicolumn{2}{c}{Acc.} \\ 
\cmidrule{3-6} 
& & zero-shot & few-shot & zero-shot & few-shot \\ 
\midrule
Single-Agent & GPT-3.5 & 28\% & 56\% & 30\% & 50\% \\
Single-Agent & GPT-4 & 62\% & 82\% & 70\% & 84\% \\
\midrule
Single-Agent & ChipLlama-8B & 74\% & 78\% & 64\% & 76\% \\
Single-Agent & ChipLlama-70B & \textbf{90\%} & \textbf{94\%} & \textbf{90\%} & \textbf{96\%} \\
\bottomrule
\end{tabular}
}
\caption{Ablation study on few-shot prompting and powered LLMs of the single-agent system.}
\label{table:ab-cot}
\end{table}

\begin{table}[!t]
\centering
% \setlength\tabcolsep{6pt}
%\renewcommand{\arraystretch}{1.08}
\resizebox{1\linewidth}{!}{
\scriptsize
\begin{tabular}{c|c|c|c}
\toprule
\multirow{2}{*}{System} & \multirow{2}{*}{Powered LLM} &  ChatEDA-bench & iEDA-bench \\
& & Acc. & Acc. \\ 
\midrule
Single-Agent & \multirow{2}{*}{ChipLlama-8B} & 78\% & 76\% \\
Multi-Agent & & \textbf{88\%} & \textbf{84\%} \\
\midrule
Single-Agent & \multirow{2}{*}{ChipLlama-70B} & 94\% & 96\% \\
Multi-Agent & & \textbf{100\%} & \textbf{100\%} \\
\bottomrule
\end{tabular}
}
\caption{Ablation study on single/multi-agent systems powered by different LLMs.}
\label{table:ab-mas}
\end{table}

\subsection{Ablation Studies}
In the following section, we conduct four ablation studies to further illustrate the effectiveness of our ChipLlama models and our multi-agent collaboration system, EDAid.

\minisection{Hybrid Instruction Tuning}
We compare hybrid instruction tuning and simple EDA-domain instruction tuning and show the results in few-shot scenarios in \Cref{table:ab-hybrid}.
These results underscore the robust generalization capabilities of ChipLlama models after hybrid instruction tuning in utilizing EDA tools across different platforms (e.g. iEDA), notwithstanding its initial training exclusively on EDA tools on the single EDA platform (OpenROAD). 
Moreover, hybrid instruction tuning also achieves improvement on the ChatEDA-bench, which demonstrates that this strategy can also help LLMs to improve their complex reasoning and long-chain tool-calling capabilities.

\minisection{Powered LLMs of Single-agent System}
Considering the powered LLM of each agent in EDAid is vital to reliable EDA flow automation, we test various LLMs by serving them as the controller of the single agent system.
As shown in \Cref{table:ab-cot}, our ChipLlama-70B achieves significant improvements compared to GPT-4 in zero-shot and few-shot scenarios across different EDA platforms.
Notably, ChipLlama-8B also achieves comparable performance to GPT-4, which is impressive considering its model parameters.

\minisection{Few-shot CoT Prompts}
We verify the performance with zero-shot prompts and few-shot prompts and the results are shown in \Cref{table:ab-cot}.
We can observe that GPT models and ChipLlama models demonstrate their capabilities in few-shot learning, which demonstrates the advantages offered by few-shot prompts compared to zero-shot prompts.
Meanwhile, it is worth noting that few-shot CoT prompting is also of great benefit for models' portability across different EDA platforms.

\minisection{Multiple Agents Collaboration}
To assess the efficiency of multi-agent collaboration, we utilize the ChipLlama models to control the agents in our multi-agent collaboration system to perform EDA script generation.
\Cref{table:ab-mas} illustrates that the collaboration of multiple agents brings improvement over the single agent for ChipLlama models. 
This enhancement underscores the capacity of our multi-agent collaboration system to provide dependable assistance in automating the EDA flow.


\section{Case Studies}
In this section, we provide case studies about agents of EDAid, including divergent thoughts agent (role $R_{0}$) and decision-making agent (role $R_{1}$), to demonstrate the core capabilities that enable EDAid to function effectively.

As shown in \Cref{fig:cases1}, the divergent-thoughts agent can generate correct task planning pathways and the corresponding EDA script that can automate the EDA flow successfully.
However, it is common that the divergent-thoughts agent makes mistakes in intermediate steps during the EDA flow automation, which requires long-chain tool-calling capability.
Specifically, the ``global\_route'' method does not have a parameter called ``macro\_place\_channel''.
The ``macro\_place\_channel'' is a parameter for the ``floorplan'' method.
Although most of the task planning pathways are correct, these single errors can still lead to failure in the overall process.

As illustrated in \Cref{fig:cases2}, the decision-making agent can accurately identify and fix errors of given EDA scripts according to the given EDA tasks.
This capability of error-correct provides a solid base for our decision-making process in our multi-agent collaboration system.
Specifically, the decision-making agent can select the correct EDA script from divergent thoughts (shown in \Cref{fig:cases1}), which guarantees the performance and effectiveness of our multi-agent system, EDAid.
\begin{figure*}[tb!]
    \centering
    \includegraphics[width=0.975\linewidth]{figs/figure12.pdf} 
    \caption{Error correctness. The decision-making agent (role $R_{1}$) can identify and fix errors in given EDA scripts accurately according to the given EDA tasks.}
    \label{fig:cases2}
\end{figure*}

Moreover, we also provide more case studies in \Cref{sec:appendix3}.
