%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Digital Representatives}\label{sec:3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\dayum{
Suppose the true policy profile $\pi^{*}$\pix$\in$\pix$\Pie$, and we are interested in a model policy profile $\tilde{\pi}$ that approximates some or all of $\pi^{*}$ with digital representatives, in the context of decision mechanisms $\tau$\pix$\in$\pix$\Tau$. (For example, this might involve substituting a part of a single participant's policy, or multiple participants' policies, within $\pi^{*}$).
%
\textit{What defines the set of profiles $\tilde{\pi}$ that we can consider as ``equivalent'' to $\pi^{*}$?}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Digital Clones}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

One obvious choice is to require that $\pi^{*}(u|x)=\tilde{\pi}(u|x)$ for all $x\in\mathcal{X}$ and $u\in\mathcal{U}$, essentially seeking an identical clone of $\pi^{*}$.
%
This is fine as an objective for \textit{training} language agents to serve as digital representatives---in fact, likelihood-based training is exactly what we perform in Section~\ref{sec:4}.
%
However, we argue that matching conditionals alone is incomplete for \textit{evaluating} digital representatives, as it may be deemed both ``too strong'' and ``too weak''.
%
On one hand, it is ``too strong'' as $\tilde{\pi}$ may not need to clone the entirety of $\pi^{*}$---\textit{in the context of} $\tau$. Intuitively, suppose there were some decomposition,
%
\begin{equation}
\mathcal{U}=\mathcal{U}_{\para}\times\mathcal{U}_{\bot}
\end{equation}
%
such that $\mathcal{U}_{\para}$ somehow encapsulated the ``relevant'' dimensions of utterances, whereas $\mathcal{U}_{\bot}$ encapsulated the ``irrelevant'' dimensions.
%
It may be helpful to draw an analogy with the ``content'' vs. ``style'' distinction in the context of image classification \cite{wang2017effectiveness}, or the ``meaning'' vs. ``form'' distinction in the context of semantic clustering \cite{kuhn2023semantic}.
%
If this is true, we would only care about matching the conditional,
%
\begin{equation}
\pi(u_{\para}|x)
=
\textstyle\int_{\mathcal{U}_{\bot}}\pi(u|x)du_{\bot}
\end{equation}
%
That is, we would simply require that $
\pi^{*}(u_{\para}|x)
=
\tilde{\pi}(u_{\para}|x)
$, for $x\in\mathcal{X}$ and $u_{\para}\in\mathcal{U}_{\para}$.
%
\dayum{%
If we had access to such a decomposition, the matter would be settled.
But such a factoring is seldom apparent.

On the other hand, defining representativity purely on the basis of conditionals is also ``too weak'', as the policies in $\tilde{\pi}$ are ultimately unrolled in one or more transitions by interacting through the mechanism. Any discrepancy in conditionals may lead to compounding errors through such interactions.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Digital Representatives}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A complete measure of representativity should also consider the dynamics of interaction.
%
Here, we draw connections with value-aware learning \cite{farahmand2017value,farahmand2018iterative} and value equivalence \cite{grimm2020value,grimm2021proper,grimm2022approximate,arumugam2022deciding} in model-based reinforcement learning. This allows us to define representation in a way that addresses both concerns at once.
%
First, some more notation:
%
Given any $\pi\in\Pie$, $\tau\in\Tau$,
the vector of \textit{expected payoffs} to participants at state $x$ taking action $u$ at time $t$ is given by the ``value function''
%
$Q_{\pi,\tau}^{t}:\mathcal{X}\times\mathcal{U}\rightarrow\mathbb{R}^{n}$:
%
%
%
\begin{equation}
Q_{\pi,\tau}^{t}(x,u)
=
\mathbb{E}_{x^{T}\sim f(\pi,\tau)}\big[g(x^{T},\theta)|x^{t}=x,u^{t}=u\big]
\end{equation}
%
Moreover, define the Bellman operator $\mathbb{B}_{\pi,\tau}$ over the space of functions
$Q:\mathcal{X}\times\mathcal{U}\rightarrow\mathbb{R}^{n}$ such that
%
\begin{equation}
(\mathbb{B}_{\pi,\tau}Q)(x,u)
:=
\mathbb{E}_{x'\sim\tau(\cdot|x,u)}
\mathbb{E}_{u'\sim\pi(\cdot|x')}
Q(x',u')
\label{eqn:bellman}
\end{equation}
%
\dayum{%
Then the sequence of functions \smash{$Q_{\pi,\tau}^{1:T}$} is the unique solution to backward recursions
\smash{$Q^{t}=\mathbb{B}_{\pi,\tau}^{t}Q^{t+1}$} for $t\in\{1,\dots,T-1\}$,
and $Q^{T}(x,u):=g(x,\theta)$.
Now we examine three notions of equivalence:
}

%==============================================================================
\begin{redefinition}[restate=defequivalence,name=Representational Equivalence]\upshape\label{def:equivalence}
%==============================================================================
Fix $\Pie$ and $\Tau$. Define first the set of policy profiles $\pi\in\Pie$ for which actions profiles $u\sim\pi(\cdot|x)$ are equal in distribution to $u\sim\pi^{*}(\cdot|x)$ given any state:
%
\begin{equation}
\Pie(\pi^{*})
:=
\{
\pi\in\Pie
:
\pi^{*}(\cdot|x)=\pi(\cdot|x)
~~
\forall
~
x\in\mathcal{X}
\}
\label{eqn:conditionals}
\end{equation}
%
Instead of matching \textit{conditionals} in isolation,
we may focus on \textit{transitions}:
Define the set of policy profiles $\pi\in\Pie$ whose operators $\mathbb{B}_{\pi,\tau}$ have equal effect to $\mathbb{B}_{\pi^{*},\tau}$ on any function $Q\in\mathcal{Q}\subseteq(\mathbb{R}^{n})^{\mathcal{X}\times\mathcal{U}}$:
%
\begin{equation}
\Pie(\pi^{*},\Tau,\mathcal{Q})
:=
\{
\pi\in\Pie
:
\mathbb{B}_{\pi^{*},\tau}Q
=
\mathbb{B}_{\pi,\tau}Q
~~~~
\forall
~
\tau\in\Tau
~
\text{and}
~
Q\in\mathcal{Q}
\}
\label{eqn:transitions}
\end{equation}
%
Finally, we may assess equivalence based on payoffs of \textit{trajectories}:
Define the set of policy profiles whose operators \smash{$\mathbb{B}_{\pi,\tau}^{1:T}$} have identical effect to \smash{$\mathbb{B}_{\pi^{*},\tau}^{1:T}$} when all applied onto any $Q^{T}\in\mathcal{Q}\subseteq(\mathbb{R}^{n})^{\mathcal{X}\times\mathcal{U}}$:
%
\begin{equation}
\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})
:=
\{
\pi\in\Pie
:
\mathbb{B}_{\pi^{*},\tau}^{1}
\hspace{-8pt}\raisebox{1pt}{\scalebox{0.8}{$\circ\dots\circ$}}\hspace{1pt}
\mathbb{B}_{\pi^{*},\tau}^{T}Q^{T}
=
\mathbb{B}_{\pi,\tau}^{1}
\hspace{-7pt}\raisebox{1pt}{\scalebox{0.8}{$\circ\dots\circ$}}\hspace{1pt}
\mathbb{B}_{\pi,\tau}^{T}Q^{T}
~
\forall
~
\tau\in\Tau,
Q^{T}\in\mathcal{Q}
\}
\label{eqn:trajectories}
\end{equation}
%
Since repeated application of a Bellman operator $T$ times turns any function $Q^{T}$ into a value function for the beginning of an episode, this condition effectively asks for equality in expected payoffs.
%
\EOD
\end{redefinition}

Expression \ref{eqn:conditionals} is simply the singleton class of digital clones. However, Expressions \ref{eqn:transitions} and \ref{eqn:trajectories} both capture a notion of \textit{invariance} in $\Pie$ in the sense that they define potentially larger classes of policy profiles. Moreover, they are both defined on the basis of interaction with the mechanism. It turns out that Expression \ref{eqn:transitions} still demands too much, and \ref{eqn:trajectories} provides the better definition for ``representation'':

%==============================================================================
\begin{reproposition}[restate=thmequivalence,name=Representational Equivalence]\upshape\label{thm:equivalence}
%==============================================================================
Fix $\Pie$ and $\Tau$, and let $\mathcal{Q}$ be closed under Bellman updates. Consider the equivalence classes of policy profiles induced by $\pi^{*}$ from Definition \ref{def:equivalence}. We~have
%
\begin{equation}
\Pie(\pi^{*})
\subseteq
\Pie(\pi^{*},\Tau,\mathcal{Q})
\subseteq
\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})
\label{eqn:subseteq}
\end{equation}
%
In particular, consider the maximal space of functions
$\mathcal{Q}=(\mathbb{R}^{n})^{\mathcal{X}\times\mathcal{U}}$.
If the space of mechanisms is also maximal,
that is $\Tau=\Delta(\mathcal{X})^{\mathcal{X}\times\mathcal{U}}$, then we have that the first subset relation is an equality, that is
%
\begin{equation}
\Pie(\pi^{*})
=
\Pie(\pi^{*},\Tau,\mathcal{Q})
\subseteq
\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})
\label{eqn:equality}
\end{equation}
%
Let action profiles be decomposable as $\mathcal{U}=\mathcal{U}_{\para}\times\mathcal{U}_{\bot}$ with $\text{card}(\mathcal{U}_{\bot})$\pix$>$\pix$1$,
and the interaction between the mechanism and policies be such that values $Q_{\pi,\tau}^{t}(x,u)=Q_{\pi,\tau}^{t}(x,(u_{\para},u_{\bot}))=Q_{\pi,\tau}^{t}(x,u_{\para})$ for all $t<T$, $x\in\mathcal{X}$, $u\in\mathcal{U}$, $\pi\in\Pie$, and \smash{$\tau\in\Tau\subseteq\Delta(\mathcal{X})^{\mathcal{X}\times\mathcal{U}}$}. Then the second subset relation~is~proper,
%
\begin{equation}
\Pie(\pi^{*})
\subseteq
\Pie(\pi^{*},\Tau,\mathcal{Q})
\subset
\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})
\label{eqn:subset}
\end{equation}
%
\end{reproposition}

\vspace{-0.5em}
\textit{Proof}. Appendix \ref{app:a}. \QED
\vspace{0.25em}

\dayum{
If the class of mechanisms is maximal, then the transition-based equivalence class $\Pie(\pi^{*},\Tau,\mathcal{Q})$ is reduced to a singleton, whereas it is still possible for the trajectory-based $\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})$ to be larger (viz. Expression~\ref{eqn:equality}).
%
More importantly, if the class of mechanisms is such that expected payoffs are invariant to some $\mathcal{U}_{\bot}\subseteq\mathcal{U}$, then the equivalence class $\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})$ is \textit{strictly} the largest.
%
The intuition is that $\tilde{\pi}\in\Pie^{T}(\pi^{*},\Tau,\mathcal{Q})$ are free to define arbitrary behavior within $\mathcal{U}_{\bot}$ without affecting expected payoffs of final outcomes.
%
As an example, suppose the class of mechanisms $\Tau\subset\Delta(\mathcal{X})^{\mathcal{X}\times\mathcal{U}}$ is itself invariant to some $\mathcal{U}_{\bot}\subseteq\mathcal{U}$
%
(that is, $\tau(\cdot|x,u)=\tau(\cdot|x,(u_{\para},u_{\bot}))=\tau(\cdot|x,u_{\para})$ for all $x\in\mathcal{X}$, $u\in\mathcal{U}$, $\tau\in\Tau$).
%
It is easy to see---by induction from \smash{$Q_{\pi,\tau}^{T}(x,u):=g(x,\theta)$}---that the invariance assumption on value functions required for Expression \ref{eqn:subset} holds (but not necessarily for Expression \ref{eqn:equality}).
}

\vspace{0.25em}
%==============================================================================
\textbf{Value Equivalence}~
%==============================================================================
In summary, this motivates a simple estimate of ``representativity'' that captures how much a given model profile $\tilde{\pi}$ deviates from the true profile $\pi^{*}$ through their expected payoffs:
%
\vspace{0.25em}
\begin{equation}
\textit{Representativity}(\pi^{*},\tilde{\pi})
:=
\hspace{-5pt}
\max_{\tau\in\Tau,Q^{T}\in\mathcal{Q}}
\mathcal{L}
\big(
\mathbb{E}_{\omega\sim f(\pi^{*},\tau)}Q^{T}(\omega)
,
\mathbb{E}_{\omega\sim f(\tilde{\pi},\tau)}Q^{T}(\omega)
\big)
\label{eqn:representativity}
\end{equation}
%
where $\mathcal{L}$ is some measure of discrepancy between its vector arguments. In other words, $\tilde{\pi}$ is a ``good'' representative of $\pi^{*}$ if behaviors elicited from each profile are such that, when aggregated through mechanisms $\tau$\pix$\in$\pix$\Tau$, they yield outcomes that (in expectation) are equivalent as measured by $Q^{T}$\pix$\in$\pix$\mathcal{Q}$.\footnote{\dayum{Note that this is reminiscent of how environment models are judged based on value equivalence \cite{farahmand2017value,farahmand2018iterative,grimm2020value,grimm2021proper,grimm2022approximate,arumugam2022deciding}. However, instead of defining equivalent environments based on a reference class of policies for reinforcement learning, here we are defining equivalence classes of multi-agent policy profiles based on a reference class of mechanisms.}\vspace{-1.25em}}