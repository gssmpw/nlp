%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Case Study: Consensus-Finding}\label{sec:4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In the context of consensus-finding (viz. Figure \ref{fig:diagram}) as collective decision-making, we now turn to the empirical question:
%
\textit{Is it feasible to train language agents to act as digital representatives of humans?}
%
Indeed, large language models are often pre-trained on datasets rich in diverse preferences, and
%
recent work has shown that language agents can generate plausible behavior in interactive settings \cite{park2023generative}, and
%
to simulate subpopulations of humans in studies in
economics \cite{horton2023large},
psychology \cite{aher2023using}, and
social science \cite{argyle2023out}.
%
However, while socio-demographically prompted models can capture the sentiment of general subpopulations of humans \cite{santurkar2023whose,simmons2022moral}, they are not as reliable on more granular levels \cite{beck2023not,harding2023ai}. Is it possible to fine-tune language models on a personalized level to represent \textit{individuals} in a group?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this work, we train and evaluate digital representatives using a consensus-finding dataset from \cite{tessler2023submit}. For completeness, we briefly recall the input questions and data collection process \cite{tessler2023submit}. Then we give an overview of the final dataset and the training process for digital representatives in our experiment.

\dayum{
%==============================================================================
\textbf{Questions}~
%==============================================================================
The corpus of input questions concern reasonably divisive political issues relevant in the United Kingdom. Questions were generated using a pre-trained 70B-parameter Chinchilla \cite{hoffmann2022training} language model. First, 175 ``seed'' questions on contemporary topics of debate were written by hand. Random samples of 10 seed questions at a time were used to prompt the model to generate final questions, for a total of 10,000 unique questions. Questions likely to prompt offensive commentary as well as those that were nonsensical or uncontentious were removed by hand, leaving a corpus of 5,645 questions. See \cite{tessler2023submit} for more detail.
}

\dayum{
%==============================================================================
\textbf{Data Collection}~
%==============================================================================
In each episode of data collection, a group of 3--5 participants based in the United Kingdom participated in an implementation of the protocol described in Figure \ref{fig:diagram}. To begin, each participant $\pi_{i}^{*}$ viewed a question $x^{1}$ sampled from the corpus. Next, they each wrote an opinion $u_{i}^{1}$ by typing free text into an input box in a custom web application. Then, the opinions of the group were included in a prompt provided to the mediator mechanism $\tau$ to generate a draft consensus statement $x^{2}$. Each participant then wrote a critique $u_{i}^{2}$ by typing free text into another input box. Finally, these were all included in another prompt provided to the mediator mechanism to generate a revised consensus statement $x^{3}$, after which participants answered a short demographic questionnaire $u_{i}^{3}$. Separately, before writing their initial opinion, participants were asked to provide a ``position'' score indicating their agreement on the declarative form of the question (so a question of the form ``Should we [...]'' will have declarative form ``We should [...]''), to allow measuring baseline disagreement among the group. For an experimental session, each set of participants were engaged in three different episodes (i.e., corresponding to three different questions), and an average session took 60 minutes.\footnote{Full details of the data collection procedure were first reviewed by an independent ethics review committee. No personally identifiable data was collected, and no offensive content was shown to participants. All participants provided informed consent before engaging and were compensated above the regional living wage.} See \cite{tessler2023submit} for more detail.
}

\dayum{
%==============================================================================
\textbf{Experiment Dataset}~
%==============================================================================
The final dataset we use for experiments is the result of 1,662 episodes of data collection,
%
corresponding to a total of 2,290 participants and 6,872 written opinions and critiques.
%
For each episode, the dataset includes the text of each question, written opinions, draft consensus statement, written critiques, and revised consensus statement.
%
For each participant in an episode, the metadata recorded include the other questions that the participant had engaged in from a past debate, as well as the written opinions and critiques of that participant on those other questions.
%
The questionnaire each participant completed covers a range of demographic information, including gender identity, age (in decades), ethnicity, region of domicile, education, political and religious affiliation, immigration status, and approximate household income.
%
For each question, the initial ``position'' score of each participant was also recorded.
%
We split this final dataset into training and validation sets on the basis of \textit{both} data collection episodes and participant identity.
%
This ensured that each participant and each question only appears in either the training set or the validation set, but not both.
%
This produced a training set with 1,332 episodes of data collection, corresponding to 1,828 participants and 5,484 instances of written opinions and written critiques, and a validation set with 330 episodes of data collection, corresponding to 462 participants and 1,386 instances of written opinions and critiques.
}


%==============================================================================
\begin{table}[htbp]\small
%==============================================================================
\newcolumntype{A}{>{\arraybackslash}m{ 2.7cm}}
\newcolumntype{B}{>{\arraybackslash}m{13.0cm}}
\caption{\textit{Example Critique}. Critiques sampled from a vanilla/fine-tuned digital representative (``DR'').}
\label{tab:example}
\begin{center}
\begin{adjustbox}{max width=1.005\textwidth}
\begin{tabular}{A|B}
\toprule
\textbf{Ground Truth}
&
I agree that some sort of tax should be mandatory but as I said earlier if its too severe these wealthy people wont [\textbf{sic}] stay on these islands and take their wealth elsewhere, its [\textbf{sic}] one thing having wealth but it has to attractive for these people.
\\
\midrule
\textbf{Vanilla DR 1B}
&
It is really quite unclear why the government wants to make this change. It does not make sense to make such a drastic change to the wealth tax. The logic of the government is that by introducing such a tax they will be able to raise Â£45 billion in revenues. The same money will be used to build infrastruct-[\textbf{...}]
\\
\midrule
\textbf{Fine-Tuned DR 1B}
&
I agree with this opinion and think it captures the nuances of the issue well.
\\
\midrule
\textbf{Vanilla DR 30B}
&
It is a little unclear to me why we should have this tax at all. I understand the problem of inequality, but I'm not sure that a flat tax would help at all. We could be doing more good by helping people on lower incomes to find jobs.
\\
\midrule
\textbf{Fine-Tuned DR 30B}
&
I agree with this. However the tax should not be so severe as to make people leave the country and therefore the gap would get wider.
\\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{center}
\vspace{-1em}
\end{table}

%==============================================================================
\textbf{Digital Representatives}~
%==============================================================================
In our experiments, we focus on learning digital representatives of human participants for the \textit{critique} step ($u^{2}$).
%
Let $\hat{\pi}_{i}$ be a model trained to produce a written critique on the basis of a question, participant $i$'s opinion, a draft consensus, and (optionally) any additional information about that participant.
%
Then the digital representative $\tilde{\pi}_{i}$ for that participant is defined as:
%
\begin{equation}
\tilde{\pi}_{i}(\cdot|x^{t})
:=
\begin{cases}
\hat{\pi}_{i}\hspace{1.5pt}(\cdot|x^{t})&\text{if}~~t=2\\
\pi^{*}_{i}(\cdot|x^{t})&\text{otherwise}
\end{cases}
\end{equation}
%
Our training pipeline follows a standard supervised learning procedure for fine-tuning large language models to learn from demonstrations.
%
We train 1B- and 30B-parameter adaptations of Chinchilla \cite{hoffmann2022training} models as digital representatives $\hat{\pi}_{i}$, using the dataset to estimate and maximize the training objective:
%
\begin{equation}
\mathcal{J}_{\text{training}}
(\phi)
:=
\mathbb{E}_{\substack{
\pi_{i}^{*},\tilde{\pi}_{i}\sim\mathcal{D}_{\text{training}}\vspace{-5pt}\\
\vdots\\
u_{i}^{2}\sim\pi_{i}^{*}(\cdot|x^{2})
}}
\big[
\log\tilde{\pi}_{i}(u_{i}^{2}|x^{2})
\big]
\end{equation}
%
where $\phi$ denotes the trainable parameters of the model,
%
and the vertical ellipsis indicates the sampling of intermediate states and actions omitted for brevity.
% 
We fine-tune all model parameters using a constant learning rate of $2 \cdot 10^{-6}$, a batch size of 128, and the AdamW optimizer \cite{adamw}. 
During training, we evaluate the model's performance by calculating the perplexity on the validation set. This metric is employed for early stopping to prevent overfitting. Following this procedure, we select the best checkpoints after 150 training steps for the 1B model and after 10 steps for the 30B model.
%
%\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results: Critique Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

First, we evaluate digital representatives at the level of individual critiques. Our first performance measure is the log-likelihood of ground-truth critiques in the validation set, under the learned model:
%
\begin{equation}
\mathcal{J}_{\text{likelihood}}^{(\text{critique})}
(\phi)
:=
\mathbb{E}_{\substack{
\pi_{i}^{*},\tilde{\pi}_{i}\sim\mathcal{D}_{\text{validation}}\vspace{-5pt}\\
\vdots\\
u_{i}^{2}\sim\pi_{i}^{*}(\cdot|x^{2})
}}
\big[
\log\tilde{\pi}_{i}(u_{i}^{2}|x^{2})
\big]
\end{equation}
%
Another measure involves prompting a large version of PaLM 2 \cite{anil2023palm} (i.e. over 1--2 orders of magnitude larger than digital representatives) as an autorater, reporting the win-rate against a baseline critique:
%
\begin{equation}
\mathcal{J}_{\text{autorater}}^{(\text{critique})}
(\phi)
:=
\mathbb{E}_{\substack{
\pi_{i}^{*},\tilde{\pi}_{i}\sim\mathcal{D}_{\text{validation}}\vspace{-5pt}\\
\vdots\\
u_{i}^{2}\sim\tilde{\pi}_{i}(\cdot|x^{2})\\
v_{i}^{2}\sim\pi_{\text{baseline}}
}}
\big[
\textit{Autorater}(u_{i}^{2}\succ v_{i}^{2}|x^{2})
\big]
\end{equation}
%
where the golden baseline is the ground-truth critique (i.e. $\pi_{\text{baseline}}:=\pi^{*}$). See Table \ref{tab:example} for some critique samples. See Figure \ref{fig:critique-eval} for results on ablations and baseline (additional results in Appendix~\ref{app:bonus}).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Critique Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp]
\centerline{
\includegraphics[width=0.41\columnwidth]{figures/own-critique-loglik.png}
\includegraphics[width=0.59\columnwidth]{figures/autoeval-own-critique.png}
}
\caption{\textit{Critique Evaluation}.
%
\textbf{Left}: Mean log-likelihood of ground-truth critiques from human participants (from the validation set), evaluated under models $\hat{\pi}_{i}$ with \textit{fine-tuned} (``FT'd'') or \textit{vanilla} digital representatives (``DRs'') with 1B or 30B parameters. The fine-tuned models consistently exhibit higher log-likelihoods compared to their vanilla counterparts, indicating a superior representation of participants' ground-truth critiques.
%
\textbf{Right}: The autorater's win-rate of a sampled critique (for different models $\hat{\pi}_{i}$ across the x-axis) against one's own ground-truth critique (i.e. the baseline). The golden bar represents $\pi^{*}$ against itself, serving as a reference for ceiling performance.
%
\textbf{Conclusion}: \textit{Fine-tuning and scale both improve the representativeness of DRs for held-out participants' critiques}.
}
\label{fig:critique-eval}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Consensus Evaluation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[htbp]
\centerline{
\includegraphics[width=0.41\columnwidth]{figures/rm-scores-sub-one-consensus.png}
\includegraphics[width=0.59\columnwidth]{figures/autoeval-sub-one-own-critique-candidate.png}
}
\centerline{
\includegraphics[width=0.41\columnwidth]{figures/rm-scores-sub-all-consensus.png}
\includegraphics[width=0.59\columnwidth]{figures/autoeval-sub-all-own-critique-candidate.png}
}
\vspace{-0.1cm}
\caption{
\textit{Consensus Evaluation}. Either one participant's critique (\textbf{top}), or all participants' critiques (\textbf{bottom}) are substituted with critiques sampled from their respective digital representatives $\hat{\pi}_{i}$.
%
\textbf{Left}: Mean discrepancy in payoffs between the revised consensus generated by the mediator mechanism using participants' ground-truth critiques, versus using critiques sampled from DRs. Replacing either one or all ground truth critiques with vanilla DR samples results in significantly degraded payoffs, whereas fine-tuned DR samples yield payoffs with little or no degradation, indicating that those consensuses are more or less equivalent.
%
\textbf{Right}: The autorater's win-rate of a generated revised consensus (using critiques sampled from different models $\hat{\pi}_{i}$) against the revised consensus generated using ground-truth critiques (i.e. the baseline). The golden bar represents the baseline consensus against itself, serving as a reference for ceiling performance. Substituting a single critique (chosen at random) results in consensuses perceived as roughly equally similar across all DRs by the autorater (13\% difference between ceiling and Vanilla 1B DRs). This is likely a property of the mediation mechanism, which we empirically observe disregards outlier critiques. However, substituting the entire group's critiques significantly influences the revised consensus output (30\% difference between ceiling and Vanilla 1B DRs), with the 30B fine-tuned DRs having a notably higher win-rate here.
%
\textbf{Conclusion}: \textit{Using the notion of representativity motivated earlier, in both top and bottom panels we also observe that fine-tuning and scale both improve the representativity of DRs for held-out episodes}.
}
\label{fig:consensus-eval}
\vspace{-0.6cm}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results: Consensus Evaluation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Second, we evaluate digital representatives at the level of consensus outcomes. Our first performance measure is the discrepancy in expected payoff (cf. Equation \ref{eqn:representativity}) relative to some baseline consensus,
%
where payoffs are estimated using a payoff model accompanying the work in \cite{bakker2022fine,tessler2023submit}, which is a model based on a 1B-parameter Chinchilla \cite{hoffmann2022training} that outputs a scalar ``agreement'' score\footnote{The use of a payoff model serves as a proxy for human endorsement without replicating the entire study of \cite{bakker2022fine,tessler2023submit}.} for each participant:
%
\begin{equation}
\mathcal{J}_{\text{payoff}}^{(\text{outcome})}
(\phi)
:=
\mathbb{E}_{\substack{
\pi^{*},\tilde{\pi}\sim\mathcal{D}_{\text{validation}}\vspace{-5pt}\\
\vdots\\
u^{2}\sim\tilde{\pi}(\cdot|x^{2})\\
x^{3}\sim\tau(\cdot|x^{2},u^{2})\\
y^{3}\sim\tau_{\text{baseline}}
}}
\big[
\mathcal{L}
\big(
Q^{3}(x^{3}),Q^{3}(y^{3})
\big)
\big]
\end{equation}
%
Here, $\mathcal{L}$ computes the average element-wise difference between its two inputs, for participants who are substituted by their digital representatives.
%
We test two regimes of model policy profiles $\tilde{\pi}$:
%
One in which a single participant is substituted,
that is $
\tilde{\pi}_{-i}
:=
(\pi^{*}_{1},\dots,\tilde{\pi}_{i},\dots,\pi^{*}_{n})
$,
%
and one in which all of them are substituted,
that is $
\tilde{\pi}_{N}
:=
(\tilde{\pi}_{1},\dots,\tilde{\pi}_{n})
$.
%
As before, another measure involves an autorater:
%
\begin{equation}
\mathcal{J}_{\text{autorater}}^{(\text{outcome})}
(\phi)
:=
\mathbb{E}_{\substack{
\pi^{*},\tilde{\pi}\sim\mathcal{D}_{\text{validation}}\vspace{-5pt}\\
\vdots\\
u^{2}\sim\tilde{\pi}(\cdot|x^{2})\\
v^{2}\sim\pi^{*}(\cdot|x^{2})\\
x^{3}\sim\tau(\cdot|x^{2},u^{2})\\
y^{3}\sim\tau_{\text{baseline}}
}}
\big[
\textit{Autorater}(x^{3}\succ y^{3}|x^{2},v^{2})
\big]
\end{equation}
%
where we report the win-rate against a baseline consensus (and the golden baseline is the ground-truth consensus). See Figure \ref{fig:consensus-eval} for results on ablations and baselines (and further results in Appendix~\ref{app:bonus}).


