\section{Validation on Real-World Websites}\label{sec:taint-validation}
While our test bed and the Google Firing Range cover a wide range of \xss{} vectors, they can only serve as an artificial evaluation that is not necessarily representative of the polyglots' ability to discover vulnerabilities in real-world code.
Thus, in this chapter we set out to validate how well our \xss{} payloads perform on actual websites with non-trivial codebases and to compare them to state-of-the-art techniques for detecting XSS that leverage targeted exploit generation.

More precisely, we evaluate our \xss{} polyglots on a set of recently found real-world client-side \xss{} (CXSS) vulnerabilities. 
CXSS has several advantages in the context of our experiments: 
While exposing the same characteristics as other classes of \xss{}, especially in respect to injection points and syntactical restrictions, it allows precise exploit payload generation, thanks to the availability of full data-flow information~\cite{LekStoJoh13,StoPfiKaiLek+15,MelDasShaBau+18} (see Sec.~\ref{sec:exploit-gen} for details). Furthermore, research experiments utilizing CXSS offer the invaluable benefit that the complete exploitation attempt is conducted solely on the client-side. Hence, potential negative side effects on the real world websites can reliably be prevented. 


\subsection{Targeted \xss{} Exploit Generation}\label{sec:exploit-gen}
As the baseline for the prevalence of CXSS vulnerabilities, we re-use a state-of-the-art taint tracking engine~\cite{foxhound} and the associated exploit generation by \citet{BenKleBarJoh21}.
In the following, we give a short overview of their approach, using the code in Figure~\ref{lst:xss-example} as an illustrative example throughout.



\begin{figure}[htb]
\begin{minted}{javascript}
let d = document.getElementById("..");
let href = decodeURIComponent(window.location.href);
d.innerHTML = '<a href="' + href + '">';
\end{minted}

\caption{A typical Client-Side \xss{} vulnerability.}\label{lst:xss-example}
\end{figure}

In general, the idea is to browse the web with a modified taint browser while constantly monitoring and analyzing all data flows.
Once the taint browser detects a data flow that is potentially susceptible to CXSS it has the following information available:
The source (here \texttt{location.href}), the sink (here \texttt{.innerHTML}), what characters from the source ended up in the sink, as well as the whole string entering the sink.

With this information, the exploit generation strategy works as follows:
The first step is to generate the so called \emph{breakOut} sequence, which aims to close the current context (here the double quoted \code{href} attribute) and put the parser into a state where we can insert a \xss{} payload.
Several HTML tags, such as \code{iframe} or \code{textarea} prevent code execution of their children. So the \emph{breakOut} will close those tags as well.
Next, a context specific payload is generated, this is based on the sink function, as \code{innerHTML} requires a different payload than, \eg, \code{eval()}. The payload generated by the exploit generator for \code{innerHTML} is \code{<img src=x onerror=f()>}.
If this report function is called, we know that the generated exploit was successful.
However, with remaining markup from the original \code{<a>} tag, we must first create  a \emph{breakIn} sequence.
Its purpose is to ``consume'' the leftover characters , ensuring the parser proceeds without errors.
While this step is usually unnecessary in the HTML context due to the parser's leniency, it becomes critical in the JavaScript context.
A suitable \emph{breakIn} would be to comment out the remainder of the document.

Finally, these three parts (\emph{breakOut}, generated payload and \emph{breakIn}) are concatenated and inserted into the URL at the appropriate position.
For our running example, the insertion point would be to simply append the generated exploit in the fragment of the URL.\@
Therefore, and in contrast to our polyglots, each exploit generated with this approach is designed to work for \emph{one} specific data flow on \emph{one} website only.

\subsection{Validation Experiment Setup}\label{sec:tainting-experiment}
To compare the performance of different approaches on real-world websites, we surveyed the top \num{10000} domains according to the Tranco list~\cite{le-pochat-tranco} as of Dec. 15, 2022, available at \url{https://tranco-list.eu/list/W95V9}.
For each successfully visited site, we queued up to \num{10} subpages, enabling us to capture vulnerable data flows that might not be apparent on the landing page.
All relevant data flows for CXSS were stored in a database, and we concurrently visited the generated exploit URLs to validate them.
The exploit validation is done with legacy URL encoding, consistent with previous works~\cite{LekStoJoh13,MelDasShaBau+18,SteRosJohSto+19,KleBarBen+22}.
For each URL generated by the exploit generator, we systematically replaced the targeted exploit with each of our 7 synthesized polyglots.
Subsequently, we used our crawler to visit these URLs, flagging the exploit as successful if the callback function was triggered.
To assess the effectiveness of our synthesized polyglots in comparison to publicly available ones, we subjected the Ultimate \xss{} Polyglot to the same treatment, like in \Cref{sec:polyglot-evaluation}.
The Ultimate polyglot, like our polyglots, operated without the additional information provided by the taint tracking browser, distinguishing it from precisely generated tainting exploits.

\subsection{Comparison of XSS Detection Rates}\label{sec:tainting-evaluation}
In total, we generated exploit URLs for \num{1010} of the visited websites due to potentially security sensitive data flows.
We then applied each of the three approaches to see if they can achieve JavaScript execution.
Thereby, we were able to successfully validate vulnerable data flows on a total of \num{165} websites, resulting in XSS\@.
Figure~\ref{fig:client-side-xss-venn} depicts the results of how our synthesized polyglots perform compared to both the perfect-knowledge exploit generation strategy, as well as the ultimate polyglot from prior work, thus answering~\ref{rq:2}.
As the figure shows, on \num{127} out of the \num{165} websites both our polyglots and the exploit generation were successful.
At the same time, it also highlights that both approaches have their advantages as not a single one was able to discover all exploits.
However, the ultimate polyglot had the worst performance by far, as it only worked on a fraction of the websites overall while not finding any additional exploits that were not already covered by the other approaches.

\begin{figure}
    \centering
    
    \includegraphics[height=4.25cm,keepaspectratio]{venn.pdf}\vspace{-1.5mm}%
    \caption{Euler diagram of the performance of the three different XSS detection approaches on the Tranco Top-\num{10000}.}%
    %
    \label{fig:client-side-xss-venn}
\end{figure}

We then proceeded to manually investigated the successful exploits exclusively solved by either of the approaches.
Examples of data flows where the polyglots did not work as well as the generated exploits were highly specific edge cases such as the one shown in Figure~\ref{lst:tainting-gen-sample}, the root cause behind 83\% of the websites where the polyglots were unsuccessful but the precise exploit generation succeeded. 
While this looks like a simple \xss{} vulnerability that the polyglots should be able to solve, it is important to note that the dynamically created \texttt{div} is never added to the DOM\@. 
This prevents triggers that rely on the \texttt{onload} event handler to fire.
Another reason for the polyglots failing to trigger results from the general concept of a polyglot (see Section~\ref{sec:polyglots-background}).
To be valid code in several contexts, they contain syntactic elements of several programming languages.
Application code trying to parse the tainted data can break, causing the vulnerable code paths to never be executed at all.
Take the polyglot from \Cref{sec:polyglots-background} as an example.
If \mintinline[]{javascript}{data.split(':')[0]} is called on it, it extracts everything in front of the double colon, e.g., to retrieve a field from a complex data structure.
For said polyglot this returns \code{javascript}, a harmless string.
The precise exploit generation never adds non HTML characters such as double colons, and as such its payload would be included in the extracted text. %
However, their rich syntactic structure is a positive in other cases, because they are able to evade broken sanitizing routines.
One common mistake for input sanitization is misunderstanding the \texttt{replace} semantics and only replacing the first occurrence of the needle by mistake according to \citet{KleBarBen+22}.
Due to containing several code execution triggers, our polyglots are able to evade such a faulty sanitization routine whereas the generated exploit does not.

\begin{figure}[htb]
  \begin{minted}{javascript}
function read_href(url) {
  var div = window.document.createElement('div');
  div.innerHTML = '<a href="' + url + '"></a>';
  return div.firstChild.href;
}
read_href(window.location.href); // somewhere else
\end{minted}
%
\caption{Problematic data flow not solved by our polyglots.}%
%
\label{lst:tainting-gen-sample}
\end{figure}



After demonstrating our polyglots' exceptional performance in detecting CXSS flaws on real websites, we investigate their suitability to discover \blindxss{} in the next section.
