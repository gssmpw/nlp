%
\section{Blind XSS in the Wild}\label{sec:bxss-study}

So far, we have demonstrated how to synthesize polyglots and have found that their effectiveness in identifying reflected XSS is comparable to existing approaches.
In this section, we design and conduct the first large-scale study of \blindxss{} in the wild.
Our study is based on the synthesized polyglots.
Hence, it demonstrates their unique advantage in uncovering BXSS vulnerabilities.
Additionally, we show the importance of using a set of polyglots instead of a single one.


We are aware of our responsibility to conduct this study in an ethical manner and, in accordance with the Menlo report~\cite{KenDit12}, we designed it to prevent harm.
Most importantly, if a polyglot should trigger, it will only connect back to inform us about its execution.
These benign tests allow us to measure the scope of the problem and warn website owners to close security vulnerabilities before they can be exploited.
We consulted our funding project's institutional review board (IRB) which concluded that the potential gain for system security predominates potential extra work for website operators.
Further technical details are found in \Cref{sec:ethical}. %

\subsection{Polyglot Preparation and Monitoring}\label{sec:monitoring-notification}
A polyglot triggers all vulnerabilities it can cover.
However, due to the nature of \blindxss{} we cannot directly observe when this happens in a backend.
Hence, we design the polyglots to notify our monitoring server if they are executed due to a vulnerability.
We outsource this notification mechanism into a remote \emph{\ourscriptname{}} instead of embedding the required functionality into the polyglots for multiple reasons.
To begin with, using a replaceable remote script makes polyglot synthesis easier, as we only have to optimize on import mechanisms as our polyglots' core functionality.
Additionally, testing is sped up, as the imported code can be easily replaced without syntactically changing the polyglots. 
In contrast, embedding the required notification functionality would introduce additional characters to the polyglots which would increase their length and extend the required character set which in turn may trigger additional input filters potentially reducing the polyglots effectiveness in the field.
Finally, using a remote script enables us to render polyglots ineffective by stopping to serve the \ourscriptname{}, e.g., once the study concludes after a certain period. %

To achieve traceability from polyglot submission to individual vulnerabilities we assign each submission a unique \num{12}-character ID\@.
This ID encodes how (URL, form, or header) which polyglot was submitted to and on what (backend) page it ended up being executed.
This ID is embedded in the URL a polyglot's import functionality takes, e.g., JavaScript \mintinline[]{javascript}{import} statements or the \mintinline[]{javascript}{src} attributes.
Exemplarily, a polyglot requests the notification script from our monitoring server upon execution, \eg, via \mintinline[]{javascript}{import('https://<ID>.<monitor_host>/s.js')}.
The monitoring server can then extract the ID from the requested script's URL and embed it into the notification script before returning it.
The script in turn includes this ID in the feedback ping it returns back to our monitoring server when executed.
Using this submission ID, a backend vulnerability can be linked to a specific submission on a particular website.




\vspace{-1em}\paragraph{\Ourscriptname{}} The \ourscriptname{} (\Cref{appendix:probing}) returns information required for accurate detection and effective disclosure of a \blindxss{} vulnerability.
When executed, the script returns the document's title, its URL, excluding query and fragment, as well as the JavaScript user agent and platform.
It encodes the information, the submission ID, and the current timestamp in the URL of an HTTP request bound to our monitoring server.
Upon receipt of such a request, we indirectly receive the IP address of the sender, which we require for the disclosure process. 
\Cref{sec:ethical} further discusses the usage and implications of the collected data. %

\pagebreak
\subsection{Polyglot Transmission}%
\label{sec:polyglot-transmission}


There are three general ways to transmit our synthesized polyglots to the websites we visit, where they can get passed to backend systems: \emph{headers}, \emph{URLs}, and \emph{forms}.
In the following, we will briefly outline considerations for each of them.

\vspace{-1em}\paragraph{HTTP Headers.} 
To test backend logging applications, we submit polyglots in four request headers. 
The {Referer} header, often containing the previously visited URL, is valuable for analytics and tracking~\cite{mdn-http-headers-referer}.
Similarly, the {User-Agent} header aids analytics, by revealing browser and platform usage patterns.
Polyglots in the {Cookie} header may get logged for failed authentication.
Additionally, we utilize the less-known {Warning} header, particularly warning code \num{199}, used to transmit loggable information~\cite{mdn-http-headers-warning}.
Despite its deprecation, all major browsers still support it.
We employ HTTP GET requests to transmit each header, embedding the polyglots as either direct values or in suitable contexts like \code{Cookie: test=<polyglot>}.
To identify which header triggers feedback, we individually send each polyglot ro the landing pages via a GET request for each header mentioned.

\vspace{-1em}\paragraph{HTTP URLs.} 
Similarly to headers, URL submissions via the \emph{query} and \emph{path} are issued for each website with each polyglot.
For every landing page visit, we append an artificial subpath, followed by the polyglot as another subpath, \eg, \code{http(s)://domain.com/\ul{<path>}/\ul{<polyglot>}}.
Analogically, in query submission, we employ an artificial query key with the polyglot as its value, \eg, \code{http(s)://domain.com?\ul{<query>}=\ul{<polyglot>}}.
We use artificial paths and query keys, because our goal is not to find reflected and stored XSS flaws in existing functionality.
Instead, our aim is to transmit our payloads to web-based backend systems with potential \blindxss{} vulnerabilities.
Moreover, we perform this action only once on the landing page, avoiding excessive warnings for website operators.

\vspace{-1em}\paragraph{HTML Forms.} 
Finally, we also analyze the HTML code for each page that we visit and extract all contained HTML forms.
For each form, we first check if the allowed length of each input given by the \mintinline[]{javascript}{maxlength} attribute is enough for our longest polyglot.
Moreover, we have measures in place to prevent duplicate form submissions.
On par with previous work~\cite{ssb-2022}, a form is considered new if at least one value differs from previous forms: (a) its \mintinline[]{javascript}{innerHTML} representation (excluding default values and whitespace), or (b) the form's target domain.
For all the remaining unique forms, we fill all inputs with one polyglot at a time and submit the form.

\subsection{Identifying Blind XSS}\label{sec:bxss-filter}

\noindent
Generally, we expect a mix of automated business logic tools, and manually operated monitoring and administration platforms to trigger our polyglots.
The former may react to our submissions instantly, while the latter may be bound to human interaction and thus only trigger sporadically.
Therefore, we give each submission a time frame of \num{2}~months during which we monitor it.
In the following, we explain our approach to confirm that triggered polyglots are of the \blindxss{} type.

We define three cascading filter steps to narrow down our findings to \blindxss{} and thus discarding reflected and stored XSS on the way:
(1) Feedback pings have to come from an IP address different to our crawler's IP, otherwise we triggered a reflected XSS\@.
(2) The URL where we submitted our polyglot has to differ from the URL where it triggers, otherwise we found a trivial stored XSS where the attacker could have adjusted their payload in this non-blind setting.
(3) The URL where the polyglot triggers may not be publicly accessible, only then we have discovered a \blindxss{} vulnerability with no way for the attacker to learn about the injection context.
Otherwise, we found a non-trivial stored XSS, where the two URLs differ and their connection needs to be discovered first, but both are nevertheless publicly available.

To test this, our monitoring server initiates an additional confirmation step for each newly reported BXSS candidate URI immediately after receiving a feedback ping.
In this step, the server conducts an extra visit to the reported URI to assess its public accessibility.
At this stage, our filter (3) confirms invalid or local URIs, as well as private IP addresses as BXSS instances where our polyglot reached and executed in a backend system.
However, even if the page did load, some cases may still qualify as BXSS\@.
For instance, we encountered public pages that required authentication to access their content.
Since the identification of login pages is hard to automate~\cite{jonkerShepherdGenericApproach2020, drakonakis2020cookie}, we manually investigated and labeled these websites as either BXSS or false positives.


\subsection{Ethical Considerations}\label{sec:ethical}
Conducting server-side studies requires careful ethical consideration.
To this end, we followed best practices outlined in the Menlo report~\cite{KenDit12}, and aimed at uncovering real-world BXSS scenarios while ensuring minimal impact on operators and users.
We decided on a large-scale study without the operators' consent. %
This is a difficult and controversial decision that requires a thorough investigation of the potential harms and risks.
We discussed this decision in detail with our IRB and received approval.
Nevertheless, we recognize the weight of this decision and the potential for alternative options, the advantages and disadvantages of which we discuss below.


\vspace{-1em}\paragraph{Alternative study designs.} %
Auditing \emph{open-source applications} for BXSS vulnerabilities offers a first choice. This method comes with no ethical issues but its results are limited by the proprietary nature of production code and the unpredictable configurations of live websites.

Analyzing simulated backends in a \emph{lab environment} offers another choice.
While ethically unproblematic within a confined environment, accurately mirroring the nuances of real-world backends poses challenges, often necessitating insights from active operators on their particular setups.


Real-world studies, especially a \emph{small-scale study} with prior operator consent, emerge as another solution, allowing operators to take precautionary measures to minimize harm.
Given the---at the time of our study---unknown prevalence of BXSS and indications of its rarity~\cite{eriksson2021black,doupe2010johnny,bau2010state,parvez2015analysis}, we decided to discard this study design.
However, an appropriate selection of operators might provide representative results.


We chose the final option, conducting a \emph{large-scale study} of the top-ranked website without acquiring operator consent.
While offering a direct and unbiased approach to a representative analysis of the subject, it is important to note that this strategy is ethically problematic, even if tests are extremely carefully designed. 
We remark that the other alternatives would also be applicable with different compromises between ethical implications and gained insights.
In general, we recommend thoroughly considering different study designs and additionally engaging with an IRB upfront. 
Thus far, our study has incurred no reported damages or problems, attesting to the effectiveness of our design in preventing harm.
Our results revealed \num{20} vulnerabilities among the top \num{100000} websites underlining our initial assumption of the rarity of BXSS\@.
Nonetheless, it is imperative to stress that any retrospective analysis does clearly not provide justification for the decisions made in a study.


It is crucial to highlight that conducting studies without proper consent or assuming consent from non-responsive parties is not ethically sound. Our research should thus \emph{not} be viewed as a template for similar studies. Based on our discussions with the IRB, we believe that our work represents a good compromise in this regard. However, we acknowledge that alternative study designs could have been employed to mitigate the risk of harm more effectively, though at a higher risk of reduced insights. Scientific work often navigates complex terrains, demanding thoughtful balancing of conflicting interests. We selected---to the best of our knowledge and belief---a suitable balance, which is certainly not without debate.

\vspace{-1em}\paragraph{Side effects.}
All test requests contain minimal JavaScript payloads that do not affect the global namespace of the surrounding application ensuring that no unintended side effects on legitimate code occur.
In the rare cases where the initial polyglot succeeds, a second script is retrieved from our servers for data collection.
This two-step process enables us to deactivate the \ourscriptname{} for specific IDs or entirely at any time, serving as an additional \textit{mitigation strategy}. %

As a result, a test polyglot can only trigger on vulnerable pages, ensuring that non-vulnerable web applications, which constitute the vast majority, receive only the polyglot without any unintended behavior or side effects.
However, for vulnerable websites, a notification function is essential to initiate disclosure to the affected parties and improve their security.
Otherwise, our blind XSS tests would remain ``blind''.

\vspace{-1em}\paragraph{Information collected.} %
In alignment with user privacy our \ourscriptname{} (\Cref{sec:monitoring-notification}) only returns information that we use for accurate recognition and effective reporting of \blindxss{} executions in backends.
We verify instances of BXSS using the IP address and partial backend URL\@.
As part of the disclosure process, we can provide operators with both pieces of information, plus the user agent and platform strings.
The user agent information offers operators an advantage in assessing the potential impact of our reported vulnerability, as they help to distinguish between manual and automated operations.
Finally, we utilize the backend path in conjunction with the page's title as indicators of shared root-cause components, allowing us to additionally report our findings directly to the developers of these components.

\vspace{-1em}\paragraph{Candidate selection.} %
In our \emph{candidate selection} process, we adhere to the ``fairness'' principle outlined in the Menlo report by considering the top 100k Tranco domains.
With focus solely on \blindxss{}, we use a \emph{canary test} to filter out websites, like guest books, that mirror user inputs, avoiding stored and reflected XSS triggers. This test populates new forms with random tokens, subsequently checking the HTTP response and page's HTML for them. To reduce load on targets, we test each site's functionality only once, ensuring unique submissions by checking for existing duplicate header, URL, or form submissions, detailed in \Cref{sec:polyglot-transmission}.

\vspace{-1em}\paragraph{Transparency.}
For \textit{transparency} and to facilitate an \textit{opt-out} procedure, our monitoring host offers information. %
Our \ourscriptname{}'s URLs' landing page details the project,  the data we collect, and contact information for potential withdrawal from the study.
Through this channel we received two notifications from one Internet services company regarding suspicious traffic from our IP\@.
We addressed this by excluding the respective domains from subsequent visits.

Though informing thousands of operators in advance is not scalable, we ensured a \textit{vulnerability notification} was sent to the technical contacts of all affected websites.
This ensures that the underlying defects are fixed and thus exploitation is no longer possible. 
Consequently, our study's benefits in improving website security outweigh potential negatives, like polyglots causing a manual investigation. %

\vspace{-1em}
\paragraph{Design implications.} %


Although these ethical measures cannot completely eliminate the risk of a polyglot leading to a technical failure on one of the websites, we argue that the gained insights about \blindxss{} and the notification of all affected sites jointly outweigh this risk and make our study a valuable contribution for improving web security.

\smallskip{}When designing our methodology, we chose to directly collect paths and titles to identify shared root-cause components.
For instance, \Cref{tab:bxss-findings} indicates a common tool shared by backends D and F, discerned through paths.
Backends B, D and F even explicitly mention such a tool's name in their title.
We used that information to inform the respective vendors, as the vulnerability could have its root cause in a commonly used software component.

We recognize that our data collection methods, though designed with great care, are not without the risk of inadvertently capturing sensitive information in certain parts of our collected data.
Specifically, titles and paths of websites could, in theory, contain credentials, authentication tokens, or other confidential data. It is widely recognized in the field of web development and design that embedding personal data in these fields is untypical and against best practices~\citep[cf.][]{owasp-secure-design, cwe-598}.
Nonetheless, this remains a significant concern.

Our decision to collect this data was driven by the intent to notify operators of vulnerabilities in their systems, a necessary step we believed was essential for our study.
We recognize the potential pitfalls and admit that a more privacy-centered strategy could have been employed.

Our large-scale study was conducted without requesting consent.
However, we could sometimes obtain informed consent for certain aspects of the study.
Inspired by \citet{utz2023privacy}, we suggest an alternative approach for future research.
In detail, we could have abstained from collecting the backend's title and path in our initial vulnerability tests.
Then during the vulnerability disclosure, where we anticipated a limited but uncertain number of affected websites, we could have procured further information from them.
If successful, we could obtain permission to gather additional data, i.e., website titles, in a follow-up experiment, or directly ask about the utilized software components.
Such a two-fold approach would have enabled us to obtain informed consent from vulnerable entities while still learning about shared components.

Yet, this method is not without flaws.
Our study found a generally limited response rate, consistent with prior research~\cite{stock2018hear}.
Hence, this approach poses the risk of missing insights into shared root causes if no responses are received.
Nevertheless, future research might explore and evaluate this approach to determine its suitability in different contexts.  



\subsection{Large-Scale Crawling Study}\label{sec:main-study}


To answer our last research question from Section~\ref{sec:rqs} (\ref{rq:3}), we perform a large-scale study on the top \num{100000} domains according to the Tranco list~\cite{le-pochat-tranco} as of Oct. 9, 2022, available at \url{https://tranco-list.eu/list/824JV}.
We use a crawler  based on the Chrome DevTools Protocol~\cite{chrome-dev-protocol} to instrument Chromium 105.0.5195.102 in headless mode.
For websites permitted by their \emph{robots.txt}, our crawler explores same-site and listed-domain links up to a depth of \num{5} or \num{30} subpages per root domain, with random link selection when needed. 
Each page receives a \num{60}\si{\s} load window; failure leads to flagging, and the crawler proceeds to another page. 
Dynamic pages receive \num{3} seconds post-\textit{load} event for pending network requests.

We conducted our study over the course of \num{20}~days using 60 parallel crawler instances on Ubuntu 20.04.5 LTS.
Our monitoring was online during this time to allow observing each submission for at least \num{2}~months.
Of the \num{1676812} visited pages, approximately 7.4\% failed to load.
Regarding the failures, about 56.1\% returned an HTTP error status code.
22.7\% of errors can be attributed to network or DNS resolution errors.
Further 15.7\% of aborted pages tried redirecting outside of the top \num{100}k domains, which we consider out of scope, or to an previously visited domain.%
The remaining 5.5\% are various errors \eg, timeouts when loading the page.


Our ethical considerations regarding deduplication and canary reduced the amount of HTML forms used for submitting our polyglots.
Collectively, these measures halved the amount of candidate forms, leaving \num{46.54}\% of identified candidates.
After respecting each input's given \inlinecode{maxlength} attribute, we submitted a total of \num{170}k forms, along with around \num{1.9}M header and \num{954}k URL submissions, equally distributed between query and path submissions.

\subsection{Uncovered BXSS Vulnerabilities}\label{sec:uncovered-vulns}
Our submissions triggered \num{20} different BXSS vulnerabilities on \num{18} websites.
In this section, we present our findings and discuss the uncovered \blindxss{} cases.
Moreover, we also discuss the efficacy of our polyglots and probing mechanisms.

\vspace{-1em}\paragraph{Vulnerable Backends.}
Regarding the given time frame and considering our initial filter (1) from \Cref{sec:bxss-filter}, we received feedback pings from \num{28} unique domains. %
After discarding stored XSS candidates with the second filter (2), we are left with \num{21} potential backend domains.
Of these \num{21} candidates, the automated part of our final filter (3) confirmed \num{8} of them to be internal websites and thus BXSS, as their URLs were unreachable \eg, because they were either private IPs, file URIs, local resources, or dotless hostnames~\cite{icann-dotless-hosts:2012}. %
Manual investigation of the remaining \num{13} reachable URLs confirmed that \num{10} of them show clear signs of \blindxss{}:
For once, \num{6} of them were login-protected pages requiring credentials or session cookies.
Another case was a web interface for a local Web Socket server that doubles as an informative website if no connection to the Web Socket port can be established, so our crawler flagged it as a public website.
Interestingly, the remaining \num{3} confirmed candidates are, to our understanding, erroneously publicly available backend pages.
Strong indicators that the pages' availability is unintended are on the one hand that they make protection-worthy data publicly available, including other visitors' IP addresses and headers, and on the other hand that their parent pages are in contrast access-protected.
Ultimately, we count these \num{10} discussed findings towards \blindxss{}, resulting in a total of \num{18} BXSS-vulnerable websites.
This demonstrates the polyglots' proficiency at uncovering \blindxss{} vulnerabilities in the backends of real websites, thus answering \ref{rq:3}.

The remaining three candidates were manually excluded and labeled as \emph{not in scope} for BXSS, because the tested website and its backend themselves were not vulnerable to our submissions, yet, we received feedback pings attributable to the polyglots submitted to these websites.
In all three cases, our \ourscriptname{} triggered in online web tools with no apparent relation to the website we tested, so we refer to this class of findings as \emph{3\textsuperscript{rd}-party XSS}.
In each case, the submitted headers were posted to evidently vulnerable online tools hours to days after we sent our submissions.
These tools include an online user-agent parser, an online XML editor and beautifier tool, as well as an online URL decoder / encoder.
We manually confirmed the three reflected XSS vulnerabilities and excluded them from our \blindxss{} results.

We disclosed our findings with all affected parties, including the previously mentioned 3\textsuperscript{rd}-party web tools.
Subsequently, we received responses in about \num{19}\% of the cases, surpassing the access-rate reported in a recent notification study~\cite{stock2018hear}.
Notably, the responses we received were entirely positive, with all respondents showing an effort to fix the issue we brought to their attention.
Moreover, we were approached by two parties requesting us to retest their fixed website, emphasizing the need for blind XSS testing strategies. %

\vspace{-1em}\paragraph{Backend Details.}
At this point, we discovered \num{18} vulnerable backends.
\Cref{tab:bxss-findings} aggregates the BXSS vulnerabilities we uncovered, showing that our findings represent websites from a wide variety of Tranco ranks, popular and less popular.
To preserve anonymity, we pseudonymized each domain in the table and shortened path and title for brevity.
In many cases, the combination of path and title are sufficient to derive the backend tools' purposes:
We observe a mix of administration tools for maintenance, management, and monitoring, as well as tools for infrastructure or business logic.
In three instances, B, D, and F, a platform name could be directly derived from the document title.
In the first case, we discovered an internal deployment of the frequently used log monitoring and reporting platform Splunk 8.2.3 and its official utility app \emph{Lookup Editor} with our polyglots. %
The other two turned out to be NetWitness Platform, a popular security information and event management tool.
We contacted both vendors to share details of our findings. %
Similar paths and titles indicate that the same software is used in the respective backends of two website pairs: D,~F and I,~K.
While the sites do not necessarily have to be connected, the location derived from their pings' IP addresses indicate proximity in both cases.

\vspace{-1em}\paragraph{BXSS Vulnerabilities.}
We count the backends based on the number of affected websites.
Since one backend can potentially have multiple BXSS vulnerabilities, we further distinguish between data flows originating from header, URL, and forms.
\Cref{fig:bxss-trigger-plot} illustrates which submission type discovered which backend.
When looking at the three groups---URL, header, and form submissions---the majority (\num{89}\%) of the backends were triggered by only one submission type.
Since the two remaining backends we discovered, I~and~L, were triggered by both URL and header submissions, we count a total of \num{20} \blindxss{} vulnerabilities on \num{18} websites.

\input{booktab-bxss}

\vspace{-1em}%
\paragraph{Submission Triggers.}
Generally speaking, we observed reaction times from submission to feedback ping ranging from a few seconds for automatic processes up to \num{10} days for human interaction, with a median of around \num{6}~hours.
Regarding the geo location of machines where our polyglots triggered, we received pings from IP addresses across the world, namely Asia, Europe, America, and Africa.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{probe-detail-plot}
    \caption{
    	Submission types that caused a polyglot to execute in one of the \num{18}~vulnerable backends.
    }%
    \vspace{-1em}
    \label{fig:bxss-trigger-plot}
\end{figure}

Next, we further investigate each submission type in \Cref{fig:bxss-trigger-plot}.
When distinguishing between query and path submissions, we found two cases (C and D) where both types triggered in a backend.
To clarify, we do not count these as separate vulnerabilities, since these likely follow the same data flow.
Interestingly, most other URL submissions that triggered \blindxss{} were due to polyglots embedded in the path.
The three headers mainly triggered the respective backend alone, with the Referer header as the most common trigger.
Finally, in only one specific case (L), BXSS in a backend was triggered by query, User-Agent, and Referer simultaneously.

Moreover, the figure also shows that HTTP header submissions, with \num{10} cases, uncovered the most BXSS vulnerabilities, followed by URL submissions with \num{9} vulnerabilities.
Notably, forms triggered the least BXSS instances with only one occurrence.
Potential reasons for this might be the lower amount of form submissions we sent compared to headers and URLs, as well as forms being an expected input vector that may have received more attention regarding sanitization and encoding of data flowing into backends.
Despite being the commonly used attack vector~\cite{LekStoJoh13,MelDasShaBau+18,StoPfiKaiLek+15,stock2017web,BenKleBarJoh21,KleBarBen+22}, URLs still overperform as delivery type for BXSS payloads.

\paragraph{Polyglot Performance.}
As highlighted in \Cref{sec:tainting-evaluation}, manually created polyglots %
may excel in a controlled lab environment but not necessarily demonstrate comparable performance in a real-world setting.
\Cref{fig:polyglot-trigger-plot} displays the efficacy of our seven polyglots in discovering \blindxss{} vulnerabilities.
It shows that all polyglots contributed to our study's findings, with more than half of them exclusively triggering certain individual vulnerabilities.
Thus, justifying our divide-and-conquer approach based on complementing polyglots.
Overall, polyglot \num{4} was most successful regarding the number of backends triggered, as well as in the number of backends that only polyglot \num{4} was able to trigger, followed by polyglot \num{1}, and \num{7} as the top-performing polyglots.
Looking back at \Cref{fig:set-gfr-perfomance} it is interesting to see that the polyglot that was most successful in the wild is not our best on the GFR\@. %
Overall, this both highlights the need for a \emph{set of polyglots}, as well as the need for a \emph{real-world evaluation} in the wild.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{polyglot-trigger-plot-probes.pdf}
    \caption{
    Backends triggered by the submission types. Hatched vulnerabilities were only triggered by one polyglot.
    }%
    \vspace{-1em}
    \label{fig:polyglot-trigger-plot}
\end{figure}
