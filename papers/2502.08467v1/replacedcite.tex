\section{Related Work}
\label{sec:related-work}%
In the following, we discuss how publications in the area of XSS detection and polyglots relate to our work.

\smallskip\textit{Stored XSS.}
While reflected XSS has been extensively studied by prior work~\cite[\eg][]{LekStoJoh13, StoPfiKaiLek+15, stock2017web, MelDasShaBau+18, BenKleBarJoh21, KleBarBen+22, son2013postman, steffens2020pmforce}, few tried to tackle the detection of \emph{stored XSS} dynamically, \ie, without access to the server-side source code.
In ____, ____ presented KameleonFuzz, a technique to fuzz web applications guided by a genetic algorithm and a taint tracking engine.
In ____, ____ analyzed the effectiveness of black-box web application scanners to detect stored vulnerabilities and found that while outperforming previous scanners____, the overall detection capabilities were still quite lacking at that time.
Later in ____, ____ expanded taint tracking techniques to also find Stored Client-Side XSS vulnerabilities, \ie, flows from Web Storage and cookies to dangerous sinks.
Moreover, ____ presented BlackWidow in ____, which can discover intra-page dependencies during black-box crawling and thus uncover Stored Server-Side XSS vulnerabilities.

\textit{ML and XSS\@.}
Recently, the use of machine learning (ML), particularly reinforcement learning (RL), has gained traction in aiding XSS vulnerability detection.
In ____, ____ demonstrated RL's utility in assisting human penetration testers in uncovering reflected XSS vulnerabilities. 
In ____, ____ introduced a fully automatic RL approach, albeit limited to reflected XSS\@. 
Additionally, ____ applied hierarchical RL to generate XSS payloads that evade the current context and bypass sanitization.


In summary, the dynamic XSS detection approaches discussed earlier share common characteristics.
They either rely on full information, employing taint tracking, are limited to Client-side XSS, or require traversing the \emph{whole} web application to identify intra-page dependencies relying on a feedback loop to guide exploitation.
In contrast, our polyglot-based approach can detect vulnerabilities \emph{blindly}, without prior knowledge or direct interactions with the vulnerable page.

\textit{XSS Polyglots.}
Some earlier work has also explored the application of polyglots and related techniques in the web context.
In ____, ____ presented a PDF chameleon, which is a PostScript document that also contains some HTML, that led to XSS due to the browser's content sniffing algorithm.
In ____, ____ generalized previous attacks such as the chameleon and the GIFAR____ attack under the term polyglot, and presented further attacks using PDF polyglots along with a small-scale study on 100 websites.
Additionally, various blog posts regarding universal XSS polyglots exist____.
High performance of the manually created Ultimate polyglot____ on the GFR led us to include it in our baseline (see~\Cref{fig:set-gfr-perfomance}).
Mutation-based genetic algorithms____ constitute an interesting generation approach.
However, initial experiments showed subpar results of the published polyglots in respect to the GFR compared to the Ultimate polyglot.
This led us to not further pursue this method.
Ultimately, these blog posts lack systematic evaluation of their polyglots and, to the best of our knowledge, none of the previous publications studied the application of polyglots in the context of \blindxss{} vulnerability detection. %