\section{Synthesizing Polyglots}\label{sec:generation}
The generation of a polyglot is a challenging task. 
Unlike attack payloads designed for a single environment, we are faced with multiple programming languages and contexts.
Although a polyglot is basically just a string of characters, conceptually it is a chimera made up of terminal symbols from different grammars. Some of these characters are even ambiguous and only parsed correctly when the respective injection context is known.
To cope with this complexity and ambiguity, we develop an automated approach that phrases the synthesis of a polyglot as a discrete optimization problem.
The objective is to find a string that maximizes the number of exploitable injection contexts on a given testbed, regardless of the underlying languages and grammars.

This optimization has several constraints:
First, evaluating a polyglot on the testbed involves running an entire browser, including HTML parser and JavaScript engine.
Second, during the polyglot evaluation, we receive only binary feedback indicating success or failure, making gradient-based optimization unfeasible.
Lastly, as discussed in \Cref{sec:polyglots-background}, some test cases are mutually exclusive. Therefore, we must identify a \emph{set} of polyglots that collectively solve the testbed.


\subsection{Monte Carlo Tree Search}%
\label{sec:mcts}%

Interestingly, the described optimization problem can be cast into a game in which a player iteratively modifies a polyglot to exploit as many contexts as possible. In this game, moves correspond to changes of the polyglot, while its reward is determined by the number of exploited contexts. Different algorithms exist for solving such games, ranging from simple search strategies to reinforcement learning \cite{russell2010artificial}.

For our analysis, we focus on the concept of \montecarlo{} (\mcts{})~\cite[][]{browneMcts:2012} that is known to find strong moves in round-based games, such as Chess and Go \cite{silver2016mastering}. The generation of polyglots, however, is agnostic to this algorithm and thus we present a comparison of \mcts{} with other strategies for solving the underlying game in Appendix \ref{appendix:alternative-generation}. %

Technically, \mcts{} simulates multiple games to conclusion to gather knowledge about promising game states.
To keep track of the performance, \mcts{} constructs a game tree in which each node corresponds to a polyglot and contains two attributes: a visit and a win counter. The algorithm explores a path in this tree using four steps:

\begin{enumerate}
\setlength\itemsep{-0.3em} %
\item \emph{Selection:} Starting from the root node of the game tree, an unexplored child or the most-promising child node is selected until a leaf node is reached.
\item \emph{Expansion:} If the selected leaf node is non-terminal, the game tree is expanded by generating the child nodes of the leaf node.
\item \emph{Playout:} Starting from the leaf node, random nodes are explored until a terminal node is reached. This is equivalent to performing random moves until the game ends.
\item \emph{Backpropagation:} On the path back to the root, the visit counter is incremented by one and the win counter is updated according to the result of the simulated game.
\end{enumerate}

During the selection phase, \mcts{} balances exploitation (selecting moves that were successful in the past) and exploration (gathering knowledge about unexplored areas).
To rank each child $i$, we use the upper confidence bound $\frac{w_i}{n_i} + \sqrt{\frac{2 \ln N_i}{n_i}}$ where $w_i$ is the number of wins, $n_i$ is the number of visits and $N_i$ is the number of visits of the parent node. If a child has not been explored before, \ie, $n_i=0$, it takes precedence.

\definecolor{gfrcolor}{HTML}{7DBA91}
\definecolor{gfrred}{HTML}{8D0400}
\definecolor{gfryellow}{HTML}{F8CC00}
\definecolor{gfrdiamond}{HTML}{297A8C}

\begin{figure*}[ht]
	\centering
    \includegraphics[width=\linewidth]{polyglot-gfr-performance.pdf}\vspace{-3mm}%
	\caption{%
	Composition of our minimal polyglot set solving all \num{111} test cases of our testbed. 
	Squares (\textcolor{gfrcolor}{$\blacksquare$}) indicates solved tests.
	The top row (\textrm{D}) visualizes the overall difficulty of each test case, calculated from the total ratio of polyglots we generated that solve this test \ie, \emph{difficult tests} with fewer solutions are red (\textcolor{gfrred}{$\blacksquare$}), while the color of more \emph{frequently solved} tests gradually shifts to yellow (\textcolor{gfryellow}{$\blacksquare$}).
	The numbered rows show the performance of our polyglot set.
	Diamonds (\textcolor{gfrdiamond}{$\blacklozenge$}) are placed instead of squares, where only one polyglot in the set solves a test.
	For comparison, the bottom row (*) shows the performance of the Ultimate polyglot. 
	}%
    %
    \label{fig:set-gfr-perfomance}
\end{figure*}

\subsection{Synthesizing Polyglots with \mcts{}}%
\label{sec:synthesizing-with-mcts}


To reduce the extensive search space, we incorporate expert knowledge into the construction of the game tree:
Instead of operating on all characters, we generate polyglots only from a set of tokens, such as \code{<} and \code{script}. We refine these tokens with simple rules that prevent invalid combinations form appearing, such as \code{svg} appended to \code{iframe}.
As a result of this reduction, \mcts{} omits moves that are impossible to yield reasonable polyglots.
The resulting token set consists of HTML literals, HTML tag names, HTML event handler content attribute, and other HTML tokens.
The complete list including the rules can be found in our companion repository. %



Starting from an empty string, \mcts{} iteratively appends tokens to it to expand the game tree and measure the polyglot's reward.
Unlike conventional games, however, this game has no clear end state, since polyglots can be arbitrary long.
We therefore set a fixed limit of 400 characters, after which we stop appending tokens to the polyglot.
Since this fixed length condition might lead to superfluous tokens, we employ a minimization strategy afterwards (see Section~\ref{sec:polyglot-minimization}).

To evaluate a polyglot during search, we implement a fast, manually created testbed covering \num{27} common XSS injection contexts using Puppeteer (13.1.3) based on Chromium (98.0.4758.0).
The complete testbed is included in our companion repository.
In the regular \mcts{} backpropagation, each polyglot would be assigned a score used to update the win counters on its path. Since we evaluate the polyglot on multiple test cases simultaneously, however, we receive a list of scores on each playout. Instead of saving only one score, we thus save the entire list and sum it up, once we need the total score $w_i$ for a particular node in the tree. As a result, we can keep track of the individual tests solved by a particular polyglot during construction.
This gives us more flexibility: When some test cases have already been covered, we can exclude them from the score calculation in later runs. %

The final synthesis of one polyglot is shown in Algorithm~\ref{alg:gen-polyglots}:
Starting from the root node, we use \mcts{} to explore the game tree for $N$ rounds until we fix the first move, \ie, select the first token of the polyglot.
With the first token in place, we repeat the process $N$ times and select the second token.
We continuously start the game from a deeper root node until the depth limit $D$ is reached.
At the end, the polyglot with the highest score is returned.

\input{alg-gen-polyglot.tex}

At best, our approach would generate a single polyglot to ``rule them all'', that is, solve all provided test cases.
However, our analysis reveals that depending on the test cases, this might not be possible.
For example, as discussed in \Cref{sec:polyglots-background}, some injection contexts are mutually exclusive and a polyglot providing proper execution of JavaScript code in both cannot exist.
As a remedy, we develop a strategy for combining multiple polyglots into a set. 

In particular, after one polyglot is created, we remove the test cases it solves and start over the search process of \mcts{}.
As result, by design, we seek a complementing polyglot that focuses only on those cases the previous one cannot solve.
We continue generating new polyglots using Algorithm~\ref{alg:gen-polyglots}, until all tests have been covered or a maximum number of tries is reached.
Finally, we obtain a set of polyglots that solves the entire testbed of the synthesis process.

\subsection{Selecting a Final Polyglot Set}%
\label{sec:polyglot-evaluation}

While we synthesize polyglots on a fast testbed, we use a larger testbed to determine the quality of the generated polyglots.
For this purpose, we set up a local instance of the Google Firing Range (GFR)~\cite{github-firing-range}, which is a state-of-the-art XSS testbed~\cite{BazCriMag16}.
We manually choose a subset of GFR tests for our scope, based on the following criteria:
First, we exclude non XSS-related test cases.
Second, in cooperation with Google, we obtain a list of GFR tests that have a known solution which we manually confirm.
We further exclude tests exceeding our scope, namely exploits relying on specific frameworks and those requiring a specific encoding.
Tests that aggressively block special characters with an error page, \eg, any request that contains \code{<} or \code{>}, are also excluded.
While solutions for these tests exist, their solution needs to be so narrow that they are no longer a polyglot and thus also out-of-scope. 
We confirmed all solutions and verified that an applicable solution exists for our requirements.
A detailed list of the excluded tests and the specific reasons can be found in Appendix~\ref{appendix:gfr-exclusions}. %

After generating \num{4000} polyglots over multiple runs from different seeds, we utilize a greedy min-set cover approach to select a minimal subset that solves all GFR test cases.
We build this minimal set by adding the polyglot to the set that solves the most tests yet unsolved by the set, until no additional solutions can be added.
Our final set of polyglots that is used in our study consists of 7 instances.
Thus, we answer \ref{rq:1} and show that it is possible to automatically create a set of XSS polyglots that cover common injection contexts.
To compare the efficacy of our final polyglot set with a publicly available polyglot, we chose the best polyglot from the blog posts mentioned in \Cref{sec:rqs}---the \emph{Ultimate \xss{} Polyglot}~\cite{ultimate-polyglot}---as our reference. 
Now, to display the composition of our polyglot set, \Cref{fig:set-gfr-perfomance} visualizes our evaluation results for each polyglot on the \num{111} test cases.
We give an indication of the difficulty of synthesizing a solution for each test by assigning a color to each of them. 
The number of polyglots from our full set that successfully solve a test, determines its difficulty.
Tests for which we synthesized fewer solutions tend to be more challenging and are depicted in red.
In contrast, the color of easier tests, \ie, those with more solutions, gradually shifts to yellow.
The numbered rows below correspond to each polyglot of our final set, while the bottom row displays the Ultimate polyglot's score.
Indicated by diamond symbols in the corresponding rows, each polyglot in the final set contributes at least one unique solution, \eg, polyglot \num{4} is the only in the set solving test \num{36}.
The plot shows that our polyglots cover all test cases, ranging from \num{31} to \num{89} tests covered by each individual polyglot.
The Ultimate polyglot works surprisingly well and manages to cover \num{88} tests.
However, our automatic polyglot set creation outperforms manual engineering, as our generation process can target gaps left by previously synthesized polyglots.

\subsection{Minimizing Polyglots}%
\label{sec:polyglot-minimization}

Since our generation uses a termination criterion that sets a fixed minimum length for a polyglot, it is possible that some tokens in the polyglot are superfluous.
In general, this would not constitute a problem, but some websites have length constraints to the inputs in their backend. %
To make sure that such obstacles do not obstruct our polyglots, we minimize the lengths of our final polyglots.
The objective is to find the smallest polyglot with equivalent test results on the GFR testbed\@.
The minimization is done automatically and at token level, \ie, we first deconstruct each polyglot into its tokens.
Since each token could be removed independently, there are $2^N$ minimization candidates where $N$ is the number of tokens in the polyglot.
Instead of testing all $2^N$ options, we build our minimization strategy on a single assumption:
If removing a token changes the test results, then all minimizations in which that token is removed are invalid.
Although there might be edge cases, where this does not hold true, this drastically reduces the search space as we can first test each token separately.
Our polyglot minimization then involves iteratively removing combinations of tokens until no more tokens can be removed without altering the evaluation result.
Overall, in every polyglot (except for one), some unnecessary tokens have been identified and removed.
For the other polyglots we achieve a reduction between 6\% and 24\%.
