\subsection{\Ourscriptname{}}\label{appendix:probing}
\autoref{lst:probe script} shows the self-executing \ourscriptname{}, which collects minimal information about its execution environment and transmits the information via \code{XMLHttpRequest} to an API endpoint of our monitoring server.
\vspace{1em}

\begin{figure}[htb]
\scriptsize
\begin{minted}{javascript}
/* Written for ECMAScript 5.1. Please visit <redacted> for further information. Contact: <redacted> */

(function () { 
	/* collect minimal information */
	var data = {
		"id": "info",
		"title": document.title,
		"protocol": document.location.protocol.replace(":", ""),
		"domain": document.domain,
		"port": document.location.port,
		"pathname": document.location.pathname,
		"navigator_ua": window.navigator.userAgent,
		"navigator_platform": window.navigator.platform
	};

	/* report */
	var url = "https://<redacted>/callback?";	
	for (var key in data) {  
		url += key + "=" + encodeURIComponent(data[key]) + "&";
	}
	url += "timestamp=" + (new Date().getTime()).toString();

	var xhr = new XMLHttpRequest();
	xhr.open('GET', url, true);
	xhr.send();
})();
\end{minted}

\caption{\Ourscriptname{}}%
\label{lst:probe script}
\end{figure}

\vspace{-1em}
\subsection{Data Management}
\label{appendix:data-management}
Due to our notification script's design, as detailed in \Cref{sec:monitoring-notification} and \Cref{appendix:probing}, we received only a minimal amount of data.
Notably, the backend URL's query and fragment were never transmitted.
We expected to receive no personally identifiable information (PII) in either the path or the title of affected webpages.
However, we prepared a data management strategy to handle potential PII transmissions from our \ourscriptname{}'s feedback pings.
Our strategy entails to manually replace potentially received PII with placeholders.
Fortunately, as shown in \Cref{tab:bxss-findings} neither the backend paths nor titles contained such information.
IP addresses have a distinctive role, as they are indirectly received via a feedback ping from a vulnerable backend.
Initially used to validate instances of \blindxss{} (\Cref{sec:bxss-filter}), they later assist in meaningful disclosure and forensic analysis.
Supplying operators with both the IP address and user agent information, facilitating distinction between manual actions and automated processes, helps them assess the vulnerability's impact.
Post-disclosure, all retained IP addresses and user agents were deleted.
Ultimately, the data this paper is based on will be archived as scientific evidence according to our institution's guidelines.%











\subsection{Alternative Generation Approaches}%
\label{appendix:alternative-generation}

Our polyglot synthesis for BXSS with MCTS has been successful in generating a complementary polyglot set. As our generation approach is agnostic to this algorithm, however, we investigate three alternative algorithms that could also be applied to our setting for constructing polyglots: \emph{random selection}, \emph{greedy selection}, and \emph{Q-learning}. %

\smallskip
\emph{Random selection ---} This method leverages our MCTS implementation, as depicted in Algorithm \ref{alg:gen-polyglots}, but incorporates random selection and a random playout phase. In the selection phase, each child is given an equal probability, and one is selected at random. Once the end condition---specifically, the maximum payload length---is met, the constructed polyglot undergoes evaluation on the small testbed.

\smallskip
\emph{Greedy selection ---} This method generates a polyglot by continuously appending the best next token.
Since we cannot know in advance which token is the best, we probe each token by appending the respective token to the current polyglot and evaluating the resulting polyglot on the testbed, thus implementing a greedy search.
If multiple tokens achieve the same performance, we select one randomly to append.
In contrast to \emph{MCTS} and random selection, the greedy method evaluates unfinished polyglots in order to choose the next token.

\smallskip
\emph{Q-learning ---} This method builds on \emph{Q-learning}~\cite{watkins1992q}, a popular reinforcement learning technique.
The method populates a table of state-action pairs $Q(s_t, a_t)$ to determine the best action $a_t$ in a given state $s_t$. 
Algorithm \ref{alg:q-learning} shows our \emph{Q-learning} implementation. 
It first chooses either a random action or the best next action and then evaluates the resulting state on the testbed, updates the q-values and saves the polyglot if it performed best.
For \emph{Q-learning} we set the learning rate $\alpha=0.1$ and the discount factor $\gamma=0.99$ as well as the simulated annealing parameters: initial exploration probability $p=1$, minimal exploration probability $p_{min}=0.01$ and exploration decay $\Gamma=0.95$.

\input{alg-q-learning.tex}

\vspace{-1em}\paragraph{Comparative evaluation.}
We compare the approaches based on the coverage they achieve on the GFR and the number of polyglots they require to reach that coverage. The experiment consists of generating a set of \num{10} polyglots with each algorithm, evaluating the resulting sets on the GFR and removing polyglots from the sets that do not contribute to the overall performance. To account for randomness, the experiment is repeated \num{10} times for each approach.

In the generation phase, each approach iteratively generates polyglots on the small testbed. 
After each iteration, we remove tests that are covered by the polyglots. 
Since calls to the testbed are computationally the most expensive component of each algorithm, all methods are given a budget of \num{12,000} evaluation calls to the testbed during each iteration.

The evaluation of our comparative experiment's results are shown in \Cref{fig:comparison-approaches}.
Plot (a) aggregates the resulting set coverage on the GFR of the \num{10} parallel repetitions of each approach as boxplots. 
Plot (b) displays boxplots of the set sizes the same approaches achieved.
Generally, smaller polyglot set sizes are preferred because they would result in fewer requests being sent to a system under test when testing for client-side XSS (\Cref{sec:taint-validation}) or \blindxss{} (\Cref{sec:bxss-study}). %
While Q-learning and greedy selection produce smaller polyglot sets, their overall coverages on the GFR are significantly lower than those of MCTS and random selection.
We can therefore discard both of them as alternative polyglot synthesis approaches.
In terms of coverage, the random method performs only slightly worse than MCTS on average.
However, MCTS consistently achieves a lower set size than random selection.
We believe this is the result of \emph{MCTS}'s knowledge aggregated over multiple games, which allows it to generate more powerful polyglots. %



\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{camera-ready-zenit}
    \caption{
    MCTS, Random, Greedy and Q-learning comparison; each generating $10$ polyglots in $10$ parallel runs
    }\label{fig:comparison-approaches}
\end{figure}

\subsection{Google Firing Range}\label{appendix:gfr-exclusions}
\newcommand{\gfrcategory}[1]{``{#1}''}

In \Cref{sec:polyglot-evaluation} we discuss the construction of our comprehensive XSS testbed, which was derived from a subset of the GFR~\cite{github-firing-range} tests to determine the efficacy of the polyglots.
This section details which tests were excluded and why.


\input{gfr-categories}

\input{gfr-listing}

The GFR is structured as a crawlable list of subpages covering different categories of web vulnerabilities.
Each category produces tests from a mix of sink, source, and countermeasures.
Due to this setup, certain combinations result in unsolvable tests.
To create our comprehensive and solvable XSS testbed, it is essential to first filter the GFR test cases. 

\smallskip
To begin with, we omitted all test categories unrelated to XSS\@.
As outlined in \Cref{tab:appendix:gfr-categories}, the categories \gfrcategory{Bad JavaScript imports}, \gfrcategory{CORS related vulnerabilities}, \gfrcategory{Flash Injection}, \gfrcategory{Mixed content}, \gfrcategory{Reverse Clickjacking}, \gfrcategory{Vulnerable libraries}, \gfrcategory{Leaked httpOnly cookie}, and \gfrcategory{Invalid framing configuration} are out of scope as they are not related to XSS vulnerabilities. 
Additionally, we disregarded the \gfrcategory{Angular-based XSSes} category given its focus on a particular framework, AngularJS, which is beyond our study's scope.

From the categories that remained relevant post our preliminary filtering, \Cref{tab:appendix:gfr-excluded-tests} provides a list of the excluded tests alongside reasons for their omission.
For brevity, tests are identified by their path, accessible by appending the test path to GFR's main URL {\small{}\url{public-firing-range.appspot.com}}.

Our subsequent filtering entailed the removal of tests lacking solutions.
Some tests became unsolvable due to the combination of sinks and countermeasures, modern browser security features, or obsoleted features.
Fortunately, we acquired a list of solvable GFR tests from Google.
After manual confirmation, we removed tests without a solution.
Secondly, a handful of tests were removed for technical reasons, such as being removed from the GFR (Stored XSS), 
or tests involving multiple windows.
Tests with exceptionally restrictive solutions, out-of-scope for a polyglot, were also ruled out, including those with stringent filters, or those solvable using only a URL\@.
Likewise, we omitted tests exceeding our technology boundaries, such as those using Adobe Flash, Base64, and SVG\@.
This leaves us with \num{111} tests.

The GFR tests involve various input sources, including form submissions, URL parameters, or \emph{PostMessages}.
Some demand clicks or page reloads post-input.
Using Puppeteer, our test software provides a polyglot to each relevant GFR test through the suitable input method, meeting the post-submission requisites.
It then waits for an XSS success signal through a specific log message.
We use Puppeteer cluster to test one polyglot on multiple tests in parallel.
After a run concludes, the polyglot's result in each test are returned.
The implementation of our testing approach is published in our companion repository.



