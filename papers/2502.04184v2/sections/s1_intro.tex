\section{Introduction}
\label{sec:intro}

Computational notebooks are the preferred tool for data analytics due to their portability and interactive capabilities. They enable easy sharing across various contexts and are often made publicly accessible, fostering reuse and adoption \cite{jupyter2021collaboration, Quaranta2022}. In notebooks, developers write code and explanatory text in cells that can be run individually, displaying results instantly. An unexpected consequence of this interactive programming is non-reproducibility---more than 75\% of the public notebooks are previously reported to be non-executable, let alone reproduce the promised output \cite{Pimentel2019}. Such a low level of executability is alarming as it diminishes the accessibility of public codebases and potentially causes code quality issues during reuse \cite{reuseICPC2022}. Previous investigations have studied the executability and reproducibility issue of notebooks and reasoned about the presence of such issues \cite{Pimentel2019, Pimentel2021, Wang2021, Zhu2021, Head2019, Wang2020}. However, the findings of these studies have several limitations. 

\noindent\textbf{\em Problem.} Previous studies categorize notebooks into (1) non-executable if they do not execute end-to-end in their current form or (2) executable if they execute successfully without any interference. There are two limitations to this categorization. First, a seemingly non-executable notebook may technically be executable but lack the appropriate execution environments (input file, packages, or execution order). Second, the binary notion of executability is overly rigid and stringent. Notebooks, by construction, are executed incrementally and interactively, similar to REPL (read-eval-print loop \cite{REPL}), where each cell execution logically progresses the notebook. 

Even if a notebook is non-executable, its partial execution improves code comprehension during reuse. Furthermore, prior works report inconsistent findings on notebook non-executability (76\%\cite{Pimentel2019}, 72.6\%\cite{Wang2021}, 82.6\%\cite{Wang2020}, and 47\%\cite{Zhu2021}) due to differences in the notebooks datasets analyzed. For instance, despite the large number of notebooks studied by prior work \cite{Pimentel2019}, most are unmanaged trial notebooks, often for personal, experimental projects rather than for reuse. Therefore, such findings do not generalize to popular notebooks (i.e., notebooks actively reused), demanding a fresh perspective on the non-executability of notebooks. We hypothesize that the count of executable notebooks is higher than previously reported, and even {\em pathologically non-executable} notebooks are partially executable, offering valuable code reusability, code comprehension, and code repair opportunities.  


\noindent\textbf{\emph{Contributions.}} In this paper, we take a fresh perspective on notebook executability by investigating the reusability of public notebooks. We address the gaps in existing knowledge on notebook executability by making the following contributions. 
%
First, we introduce new notions and degrees of executability by separating misconfiguration from non-executability. To that end, we define {\em pathologically non-executable} notebooks that suffer from unresolvable fatal syntactic or runtime errors (such as bad indentation or attribute error), and {\em restorable} notebooks that are non-executable due to superficial misconfiguration errors (e.g., missing input, library, or execution order). Restorable notebooks are fully executable in a suitable environment (e.g., the original author's local environment).
%
Second, we introduce fine-grained metrics of executability that quantify the reuse potential of non-executable notebooks. For non-executable notebooks that are otherwise deemed unusable, we measure their {\em partial executability} based on the proportion of successfully executed cells, an aspect that has never been studied.
%and provide a quantifiable way to measure the effectiveness of restoration techniques \cite{Wang2020}. 
%
Third, we conduct this investigation on actively reused and adopted notebooks. We follow the practice of using GitHub stars as a popularity metric \cite{Borges2016, KocKleJoh24} to rank notebooks during data collection. Doing so mitigates any bias from unmanaged, rarely used public notebooks. 
%
Lastly, we demonstrate the effectiveness of LLMs in addressing misconfiguration issues (e.g., by generating synthetic input data) in restorable notebooks.


We realize the aforementioned contributions by building an automated measurement framework for notebooks. Each notebook is evaluated in a sandboxed environment with all required execution environments as stated in the parent repository. We leverage a combination of static and dynamic error checking to extract the causes of non-executability in notebooks. Static analysis on notebook code cells identifies errors such as structural, syntax, and variable undefined errors. Dynamic analysis identifies runtime errors such as missing libraries, packages, and type errors. This stage returns a categorized list of pathologically non-executable notebooks and restorable notebooks and their respective list of errors. LLMs are improving code generation and understanding \cite{Nejjar, lin2024soen101codegenerationemulating}. To investigate the reusability of restorable notebooks, we utilize open-source LLM Llama-3 for error-driven notebook restoration. For example, error reports indicating missing modules or input files are combined with code cells to prompt the LLM to identify the correct module names for installation or generate synthetic input data, respectively. We iteratively restore and error-check the notebook until it is fully executable or an unfixable error is found. 

\noindent\textbf{\emph{Study Results.}} We collect a dataset of the most popular \totalNotebooksInDataset public notebooks from a stratified sample of \totalRepos GitHub repositories with four or more stars in 2024. Using non-executability criteria from prior work, we found that over \percentNonExecutable of those notebooks are non-executable if executed as is, which is consistent with the findings of prior studies \cite{Pimentel2021, Wang2021}. 
%
We investigate the top two reasons for non-executability: lack of appropriate input files and missing modules, packages, or libraries. Such notebooks are potentially restorable. Surprisingly, we find that {\bf only \percentPathological of all notebooks are pathologically non-executable} due to unrestorable executability issues, and the rest are either executable or potentially executable given a suitable execution environment, i.e., \percentRestorableInNonExecutable of non-executable notebooks can potentially be restored. This validates our hypothesis that pathologically non-executable notebooks are significantly lower than previously reported in prior work. Possible reasons include a skewed notebooks dataset, a misconfigured experiment environment, and the strict notion of executability. 
%
Interestingly, our error-driven, LLM-guided fix successfully restores full executability of \totalFinalRestored of the notebooks for which it found the necessary module and synthetic input data. It partially improves the executability of restorable notebooks by \averageIncreasePartiallyRestored cells per notebook. Prior studies \cite{Pimentel2019, Pimentel2021} have considered these notebooks completely non-executable. 
%
Even pathologically non-executable notebooks are not entirely non-executable. They can be successfully executed up to \averagePercentPartialPathological of cells, on average. These findings show that traditional notions of executability underestimate notebooks' reusability mainly due to misconfigured notebooks, some of which can be fixed automatically, and that most notebooks can still offer numerous code reuse and restoration opportunities. We summarize the contributions of this work below. 

\begin{itemize}
    \item We take a fresh look at the reusability of public notebooks by exploring varying interpretations of executability. Our findings offer insights into the partial executability of seemingly non-executable notebooks and distinguish superficial misconfiguration errors from pathological errors. 
    \item This is the first work to incorporate notebook popularity when assessing executability, contextualizing findings to frequently reused and adopted notebooks. 
    \item We report the feasibility of lightweight restoration strategies in improving notebooks' executability by addressing three of the top executability issues. We also demonstrate that LLMs can be effective in generating synthetic sample input to advance the partial executability of notebooks. 
    \item Executability is the first step toward reproducibility. A significant portion of cells in pathologically non-executable notebooks remains executable, unlocking the potential for advancing code reuse and dynamic analysis. Partial executability can serve as a valuable criterion to assess a notebookâ€™s restoration potential. 
\end{itemize}

\noindent We have made our code and dataset publicly available at \url{https://github.com/renote2024/ReNote2024}.

\begin{figure}[t!] % Figure 1
    \centerline{\includegraphics[width=\columnwidth]{images/partial_exec.pdf}}
    \caption{Notebook \cite{girlscript2024} is initially non-executable due to an undefined variable. Its executability can be fully restored by 23 cells by generating a missing input file for the notebook.}
    \label{fig:cs1-full-exec}
    % \vspace{-4ex}
\end{figure}











% Prior efforts to improve notebook executability have targeted specific factors such as data and control dependencies \cite{Head2019}, deprecated APIs \cite{Zhu2021}, and absent execution environments \cite{Wang2021}. However, these studies developed new notebook interfaces, requested new development practices, or aimed for complete executability. Wang et al. \cite{Wang2020} attempted to restore the reproducibility of only executable notebooks, neglecting 87\% notebooks that are not executable. Note that notebook reproducibility is not the primary focus of our paper, as it heavily relies on test cases and notebook specifications, which are rarely available, as indicated by the limited applicability of Osiris \cite{Wang2020}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \gulzar{list of contributions: 
% (1). measuring pathological executability instead of surface-level non-executability. Impact: proper categorization can help recover executability and reproducibility and help us improve the performance of existing tools like Osiris. 
% (2) measuring fine-grained, partial executability instead of marking the entire notebook non-executable as notebooks are interactive and incremental. Impact: Partial executability can promise high reusability of executable cells, increase code understanding, and enable dynamic analysis which can make many reproducibility tools like Osiris applicable. It also improves adaptability and exposes valuable code snippets for downstream code tasks such as LLM code training, and repair. 
% (3) demonstrating that LLMs can help recover from environment misconfiguration by generating synthetic data, synthetic variables data, and dependencies. Impact:  Using LLMs to address non-executability is a promising area. 
% (4) showing how the popularity of notebooks plays a role in notebook maintenance, and thus executability. Impact: understanding if executability is only an issue with unmanaged, low-popular notebooks that are not expected to be reused.}

% %%%%%%% WARIS' CONTRIBUTION %%%%%%%
% \waris{Tien can you please read the following contribution section? Then we all ask Dr. Gulzar to read it.} \waris{I do not know about Osiris. We don't have results related to it. Should we mention it? I will fill the citation in contributions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% by asking two research questions: {\em how many of the public notebooks are pathologically non-executable} and {\em what proportion of the non-executable notebooks can be restored partially or fully?}  


%To answer these questions, we first build a framework that categorizes non-executable notebooks into (1) misconfigured notebooks (e.g., missing input, library, or execution order) that are non-executable but restorable and (2) pathologically non-executable notebooks that suffer from unresolvable fatal syntax or runtime errors (such as bad indentation).  We do so by combing static and dynamic error checking for notebooks to extract diverse characteristics and causes of non-executability. Next, since misconfigured notebooks are executable under correct configurations, we attempt to automatically restore those notebooks by finding potential fixes for top-3 resolvable issues like incorrect execution order. 


% \tien{maybe we should say undefined variables/names to generalize both cases where it's defined after and undefined at all}, missing modules in the execution environment, or absent external input files. We package the error reports, error-inducing code snippets, and notebook documentation into a prompt to Large-Language Model \tien{Llama-3 \cite{llama3, touvron2023llama}} to find appropriate fixes and synthetic input to improve the executability of notebooks. 

% \waris{we did not specify how we fix ``execution order." I have added it below based on my understanding. Similarly, we can also briefly write who we fixed the module not found.}

% \waris{Following paragraph does not seem to be a contribution. We should move it to motivation or study results etc. }

% Prior literature on code reuse from GitHub and Stack Overflow shows that developers tend to reuse code snippets or modules rather than copying entire sections of code \cite{reuseICPC2022, toxicCodeTSE2021}. This insight is even more relevant to notebooks due to multiple contexts in notebooks, naturally modular code in the form of cells, and incremental execution of cells. \tien{$\Leftarrow$ I don't quite understand this sentence}  Therefore, instead of measuring only the end-to-end executability of notebooks, our framework measures the partial executability of the notebook, including pathologically non-executable, fully restored, and partially restored notebooks. 

% \waris{We have done more work, but the contributions are a bit weak. Here are a few points based on my understanding that I think we can add to strengthen the contribution. I have rewritten some of the points from the above contribution to improve the flow of the paragraph. You can cut and paste sentences if you find them useful in the above contribution.}

% \waris{Our framework employs a combination of static and dynamic error checking to identify and characterize the diverse causes of non-executability in public notebooks. Additionally, since LLMs are improving in code generation and understanding \cite{}, our insight is that we leverage LLMs to automatically fix the aforementioned issues in misconfigured notebooks without editing the code of notebooks. For misconfigured notebooks, which can be restored without editing the notebook cell, we implement an automated restoration process targeting the top three resolvable issues: 1. Incorrect Execution Order. We analyze dependencies between notebook cells and reorganize their execution sequence to ensure proper flow and functionality. 2. Missing Modules. We identify absent libraries and automatically install the required modules in the execution environment. 3. Absent External Input Files. We detect missing external files (e.g., CSV or JSON files) and generate synthetic input data using Large Language Models (LLMs).}

% \waris{We execute the notebook cell by cell and collect the error reports, error-inducing code snippets, and notebook documentation. If a cell gives an error, we package the error reports, error-inducing code snippets, and notebook documentation into a prompt to LLM to find appropriate fixes. These days companies like Microsoft are using synthetic inputs even to train LLMs which reflect the real-world scenarios for synthetic data \cite{}. Based on this insight, if a cell gives an error related to missing data (\eg missing input file such as a CSV or JSON file), we query an LLM to generate synthetic input data for the given cell error related to the input file missing or data missing. Next, the output of the LLM contains unwanted text (e.g., ``This is a synthetic input data"). We then sanitize the LLM's output, using regular expressions. (if you are using something else please mention it here) \tien{Yes, we use regular expression}, to remove any extraneous text. The next step is to create the file. One way is to create the file in the current directory however it will require editing the code to change the path of the file. To keep the integrity of the notebook intact, we attempt to create the same file path as the original file path and then write the synthetic input data to the file. After this, we re-execute the notebook and check if the cell is executed successfully. If the cell is executed successfully, we move to the next cell. This meticulous approach ensures that the notebook's structure and integrity are preserved while resolving execution issues. By systematically addressing both configuration-related and syntax/runtime errors, our framework significantly enhances the executability and reusability of public notebooks.}


