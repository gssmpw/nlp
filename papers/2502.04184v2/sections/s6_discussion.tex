\section{Discussion}
\label{sec:discussion}

    \begin{table*}[t!]
    \centering
    \caption{Summary of some related works with ours in computational notebook topic.}
    \begin{tabular}{|m{2cm}|>{\raggedright}m{1.6cm}|>{\centering\arraybackslash}m{0.8cm}|>{\centering\arraybackslash}m{1cm}|>{\centering\arraybackslash}m{1.5cm}|>{\centering\arraybackslash}m{1.3cm}|>{\centering\arraybackslash}m{2.0cm}|>{\raggedright\arraybackslash}m{4.5cm}|}
        \toprule
        
        \textbf{} & \textbf{Purpose} & \textbf{Study Dataset} & \textbf{Module Error} & \textbf{NameError / Cell Order} & \textbf{Input File Error} & \textbf{Non-Executable Found} & \textbf{Dataset Characteristics}\\ 
        \midrule
        
        RELANCER \cite{Zhu2021}                          & Executability        & 4,043             & \cmark    & \xmark    & \xmark    & 47\%      & Collected from Meta Kaggle \cite{kaggle}\\ 
        \hline
        SnifferDog\cite{Wang2021}                        & Executability        & 2,646             & \cmark    & \xmark    & \xmark    & 72.6\%    & Random sample from \cite{Pimentel2019} only those with installable dependency\\ 
        \hline
        Osiris\cite{Wang2020}                            & Reproducibility      & 5,393             & \xmark    & \cmark    & \xmark    & 82.6\%    & Random sample from \cite{Pimentel2019}\\ 
        \hline
        Pimentel et al.\cite{Pimentel2019, Pimentel2021} & Empirical            & $>$1.4M    & N/A       & N/A       & N/A       & 76\%      & Includes high-number of low-starred, rarely reused notebooks from GitHub\\ 
        \hline
        \textbf{This work}                               & Executability        & 42.5K              & \cmark    & \cmark    & \cmark    & \percentPathological & Highly-starred notebooks from GitHub\\ 
        \bottomrule
    \end{tabular}
    \label{related_work_table}
    % \vspace{-3ex}
\end{table*}

    In this section, we distill key findings in a {\em FAQ} format, demonstrating the value of accurate executability categorization, the utility of a notebook corpus once marked unusable, and practices that could prevent non-executability or restore executability in future notebooks. 
    
%\noindent{\em Why do notebook executability measurements in this work differ from prior investigations?} We identify two key factors that distinguish our findings on notebook executability from prior empirical investigations. First, our notebook corpus is primarily sourced from popular repositories—actively shared, reused, and maintained—whereas prior work focused more broadly, including notebooks that are less actively managed or of lower quality. While previous studies provide insights into a general range of public notebooks, our study offers new perspectives on those designed for sharing, adaptation, and reuse. Second, our dataset was collected in 2024, while the most recent prior investigation was conducted in 2021. Over the past three years, several tools aimed at improving notebook quality have emerged \cite{}\textcolor{red}{CITE}, likely contributing to the increase in executability.

\noindent{\em Why are \percentPathological notebooks pathologically non-executable and \percentPartiallyRestored of restorable notebooks still only partially executable?} 
    Prior work has identified that notebooks have alarmingly low numbers of test cases \cite{Pimentel2019}. Developers releasing their notebooks for public use have very limited testing tools to verify reproducibility and executability. For example, notebooks with undefined variables ``NameError" and ``AttributeError" often go unnoticed due to unintended dependencies on session states saved from prior cell execution, leading to those errors being masked on the developer's machine, similar to Case Study 2 in Section \ref{case_study_2}. In a new environment, such states are unavailable, resulting in these errors. 
    %\tien{This may be right, but notebooks originally have execution orders that they can use to execute, even though they may not be consistent. Pimentel \cite{Pimentel2019} and Osiris \cite{Wang2020} looked into this method.} 
    Our findings on pathological errors in notebooks demonstrate the need for static and dynamic analysis tools to catch such errors early in notebook development. 
    %\waris{btw VSCode Jupyter notebook extension addresses this issue.}


\noindent{\em How can LLMs further restore notebook executability with a high success rate?} 
    In the first case study (Section \ref{case_study_1}), Llama-3 successfully generated synthetic data for a `.csv' file. However, in another case \cite{tirthajyoti}, it failed to produce a valid `PNG' file due to limitations in multi-modal generation. This is because most LLMs have been trained on similar textual data formats. Therefore, Llama-3 also faces limitations when generating images, audio, video, or zip files. Multi-modal models such as GPT-4o can generate richer and more diverse inputs. %Our results with language-specific LLMs indicate that these models have the potential to generate synthetic data that promises high executability. 
    This capability can benefit developers who want to share their notebooks without disclosing the input data files. They can leverage LLMs to generate synthetic data (or scripts for synthetic data generation similar to database benchmarks \cite{tpcds}), enhancing executability in remote environments. Additionally, LLM performance tends to degrade when provided with large contexts, which aligns with recent research on LLMs \cite{Leng2024}. Moreover, notebook executability can be enhanced by generating feedback-driven fixes (e.g., multi-shot) for non-executable cells. This includes generating new input files based on error feedback.  
    %For example, if a cell error is caused by a missing file, an LLM could generate the necessary data or adjust paths to match the environment. 
    % Such a framework would greatly benefit notebook developers, enabling quick execution of public notebooks and addressing issues like missing files with minimal manual intervention.


%Moreover, we can further improve the execution of notebook cells by generating feedback-driven fixes for non-executable cells. For instance, we can achieve this by allowing code edits in notebook cells or generating newer input based on the error related to generating input files.  If we get an error in a cell, we can ask the LLM to generate newer file data if it's related to the generated file in the same context or if it requires a code edit (e.g., renaming the file path from a Windows machine path to a Linux machine path) to make it executable. Such a framework will be highly beneficial for notebook developers. It allows them to quickly run public notebooks and automatically address issues such as missing files, which would otherwise require manual effort.
%  Also, if an LLM generates six columns of a CSV correctly but the name or the data in the last column is not correct, we can ask the LLM to generate the last column data correctly. 



\noindent{\em What use is there for partially executable notebooks?}
    Prolonging notebook execution leads to more cells being executed successfully, which translates into more code surfaces that can be executed and understood. This inherently improves code reuse and enhances the chances of notebooks being fully restored. Furthermore, dynamic analysis techniques are limited to executable code only. By improving notebook executability, we increase the application surface of dynamic analysis tools such as dynamic taint analysis \cite{clause2007dytan}, runtime tracing \cite{meliou2011tracing}, automated debugging \cite{parnin2011automated}, symbolic execution \cite{baldoni2018survey}, and Osiris \cite{Wang2020} that can play vital roles in recovering notebook reproducibility. %Additionally, code corpora have proven extremely valuable in training new emerging LLMs. A big challenge is verifying the correctness of training data. More executable cells offer pathways to verified/tested code snippets that can be included in model training. 
    Partial executability metrics can also provide fine-grained measures of the effectiveness of reproducibility and execution-restoration tools. Additionally, it can serve as valuable feedback when applying code repair techniques in notebooks, such as automated cell reordering \cite{Wang2020}. 
    
    
\noindent {\em How do the findings of this study improve notebooks?} 
    We explored several design choices in building our measurement framework. We identify gaps in the notebook ecosystem for code review, debugging, test generation, and build tools. For existing public notebooks, which are growing exponentially \cite{Rule2018}, this measurement framework can be extended to improve fine-grained executability, feeding into dynamic analysis tools to enhance reproducibility. 
    
    Our findings, showing that only \totalRequirement (\percentRequirementInTotal) repositories have REQUIREMENTS files, indicate that there is no standardized build and continuous integration mechanism for notebooks. While this may not apply to exploratory, standalone, one-off notebooks, the most popular repositories provide a range of notebooks that must be properly orchestrated to ensure executability. We believe Python-based build tools, such as {\small{\texttt{disutils}}} \cite{distutil} and {\small{\texttt{setuptools}}} \cite{setuptools}, can be extended to support notebook extensions at the browser level to enforce correct build and dependency management for notebooks.


 \noindent {\em Threats to validity.}
    Outdated, textual, and empty notebooks can cause variations in the results of this study. To mitigate that, we exclude notebooks written in Python 2 since support for Python 2 was discontinued in 2020. We also filter empty or instructional notebooks from our dataset. Similarly, focusing purely on GitHub repositories can bias the results towards a specific class of notebooks. Upon investigation, we identified numerous HuggingFace and Kaggle notebooks that are also hosted on GitHub. While there is always room to increase the scale of the analysis, our emphasis on popular, actively reusable notebooks deprioritizes the need to expand the scale. Our static and dynamic analyses rely on the Python AST parser and Python interpreter to capture errors. These tools can potentially miss or misclassify errors, which can impact our categorization. 
        %We do not capture semantic errors, nor do we measure reproducibility. 
    Since most of our runtime errors are captured through dynamic error checking, an earlier fatal runtime error (that we cannot resolve) may hinder our ability to capture ``FileNotFound" or ``ModuleNotFound" errors. Lastly,  we use a timeout of 5 minutes to execute each notebook. 
        %Any notebook taking more than 5 minutes is considered unanalyzable; thus, we exclude it from our dataset. 
    This approach may exclude notebooks like the ones requiring expensive machine learning training. However, in our notebook corpus, only \percentTimeout of notebooks encountered timeout errors.
