\section{Empirical Analysis Methodology}
\label{sec:approach}



Figure \ref{fig:main_workflow} (a) illustrates the intermediate stages of our analysis process. Starting with a repository, we retrieve all notebooks and conduct a lightweight static error check to identify issues that may impede notebook execution. Next, we automatically analyze the repository to extract any provided environment requirement files, enabling us to configure the appropriate environment for notebook execution. During execution, we log the first-encountered error and incrementally apply targeted restoration strategies based on the error type, measuring executability improvements at each stage. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analysis Approach}
\label{analysis_approach}



    \subsubsection{Initial Error Checking}
        For a notebook to be executable, it first needs to be compilable. As a first step, we look for compilation errors by applying a lightweight error checking on all notebooks within each repository to uncover any compiling issues, such as syntax or indentation errors. This preliminary filtering step assesses executability without running the code, allowing us to exclude notebooks that require extensive code-level modifications for restoration and are thus deemed pathologically non-executable. To access the content of the notebooks, we use the {\small{\texttt{nbformat}}} module. While performing static analysis of the notebook's code, we detect and omit notebooks from further investigation that (1) cannot be read due to corruption and encoding problems, (2) contain no code cells (with or without markdown cells), or all code cells do not hold any content; and (3) written in programming languages other than Python 3 (such as Python 2, Julia, R, or C). 
    

    \subsubsection{Dynamic Error Checking} 
        \label{dynamic-error-checking}
        In step \ding{203} of Figure \ref{fig:main_workflow} (b), we perform dynamic error checking to detect and categorize potential runtime errors. We use Papermill \cite{papermill}, which allows us to execute the notebook and detect and report any errors encountered during execution. We use Python 3 as the execution kernel and document the first error that halts the execution. We categorize the execution status of each notebook into the following groups:
    
        \begin{enumerate}
            \item \textbf{Executable:} The notebook is fully executable without encountering errors.
            \item \textbf{FileNotFound:} The notebook writes to or reads from a file or directory that is unavailable during execution.
            \item \textbf{ModuleNotFound:} The notebook imports unavailable modules, packages, or libraries.
            \item \textbf{NameError:} The notebook uses a variable, function, or class without defining or importing it.
            \item \textbf{Others/Non-Fixable:} The notebook encounters errors not covered by the above categories. We also identify and categorize these errors accordingly.
        \end{enumerate}

        \begin{figure}[t!] % Figure 3
            \includegraphics[width=\columnwidth]{images/def_use.pdf}
            \caption{Def-use lists for the first two cells in notebook \cite{pytopia}.}
            \label{fig:def-use}
        \end{figure}

        \noindent The categories reflect a natural progression of notebook adoptions and fixes, as shown in Figure \ref{fig:main_workflow}. 
        %This begins with checking executability and then categorizing non-executable notebooks into fixable and non-fixable errors. “Fixable errors” are further categorized into three subcategories based on the most common issues identified in prior literature \cite{Pimentel2019}.
        If a notebook is found fully executable after this execution process, we mark it as executable. If it is non-executable due to missing input files, required modules, or undefined names, we iteratively apply restoration strategies (Figure \ref{fig:main_workflow} (b)) and perform dynamic error checking to remove as many executability issues as possible.

        
        For notebooks with ``NameError," we perform a static {\em def-use} analysis to localize the root cause behind undefined names. Like traditional {\em def-use} analysis, we build a specialized AST visitor that tracks variable definition and usage with precise scope awareness within each cell. Figure \ref{fig:def-use} shows how \textit{def} and \textit{use} sets look for two notebook cells. Note that while maintaining the definition set, we consider variable binding, access, and scoping rules for Python to avoid any false positives and false negatives during the analysis. Our definition set extends beyond variables to include imports, functions, classes, and control flow elements. 
        
        Next, we use {\em def-use} sets to isolate the location of undefined names in the notebook and search nearby cells to locate the definition. Finally, the framework returns any detected definition of the undefined variable and its cell location. If the variable's definition appears later in the notebook than its initial use, we categorize the notebook as {\em defined after use} and return the cell number where the definition is found. If no definition exists throughout the notebook, we classify it as {\em undefined}. 

        % The framework first analyzes the entire notebook using the AST visitor to create detailed maps of all variable definitions and uses across all cells. For each cell, it keeps track of both global and local scope variables. When it finds variable uses and definitions, it records not just the cell number but also the specific scope within that cell. The framework then attempts to locate the cell where the given undefined variable might be defined in the subsequent cells. 
        %For each potential definition, we check if that definition would be accessible from where the variable is used while following Python's scoping rules. For example, a variable defined inside a function is not accessible to code outside that function.     
        

    %\tien{To accomplish this, each cell is analyzed to generate a set of \textit{definitions} (def), comprising variables (or names) that are defined within the cell. To construct this set, we recursively traverse the AST nodes, extracting all instances of {\small{\texttt{AST.Name}}} referring to variables. When encountering a node of type {\small{\texttt{Name}}}, we determine its context within the notebook. If the context is {\small{\texttt{Store()}}}, indicating the definition of a variable, we add it to the definition set. Note that while maintaining the definition set, we consider variable binding, access, and scoping rules for Python to avoid any false positives and false negatives during the analysis.}
    
    %In other programming files, certain errors can be easily detected during the compilation phase. Similarly, in notebooks, we can statically identify some issues without the need for execution. 
    %For a notebook to be executable, it first needs to be compilable. As a first step, we look for compilation errors by applying static error checking on all notebooks to uncover any compiling issues, such as syntax errors or indentation errors. Additionally, we can detect runtime NameError instances by examining the definition and usage \cite{Wang2021, Chen2014, Ryder1994} of variables within a notebook to pinpoint any undefined variables. This preemptive process aids in assessing executability early without executing the code. Notebooks that fail this early screen fall into pathologically non-executable notebooks since restoring them requires a code-level rewrite that may change the semantics of the original notebook.  


    %Next, in \ding{203} of Figure \ref{fig:main_workflow}, we use the AST of each cell to identify potential NameError issues arising from undefined variable use. To accomplish this, we first identify all the definitions and usage instances of variables (or names). Each cell is analyzed to generate two distinct sets: the \textit{definition} (def), comprising variables that are defined within the cell, and the \textit{use}, containing variables that are referred to in the cell. Figure \ref{fig:reorder_workflow} shows how def and use sets look like for two cells in a public notebook. To construct these two sets, we recursively traverse the AST nodes, extracting all instances of \codeword{AST.Name} referring to variables. When encountering a node of type \codeword{Name}, we determine its context within the notebook. If the context is \codeword{Store()}, indicating the definition of a variable, we add it to the definition set. Conversely, if the context is \codeword{Load()} or \codeword{Del()}, signifying the usage of variable values, we append it to the use set. Note that while maintaining def-use sets, we consider the variable binding, access, and scoping rules for Python to avoid any false positives and false negatives during error checking. 

    %\tien{Our definition set extends beyond variables to include imports, functions, classes, and control flow elements. For imports, we incorporate module names and aliases to the definition set for potential use elsewhere in the notebook. Similarly, we extract function names and argument lists from different types of defined functions (regular, async, and lambda). Class names are also added to class definitions. In control flow constructs, such as loops, we include variables referenced in conditions or defined within the loop body. This comprehensive definition set supports the accurate identification of dependencies throughout the notebook.}
    
    %We expand our def and use set beyond variables. For import statements, including attributes indicating module names and their aliases, we add them to the set as potential future usage within the notebook. Similarly, for function definitions (including regular, async, and lambda functions), we extract their names and argument lists, including them into the use set. We exclusively gather user-defined names, omitting built-in functions to avoid triggering NameError messages. Class names are extracted from class definitions and appended to the use set as well. In control flow constructs such as for loops or while loops, we include variable names in conditional statements and those defined within the body in the definition set. Variables utilized within the body are added to the use set.

    
    % Considering the scope of each name or variable is crucial, as variables can be defined and used in different scopes, ranging from global to more local scopes. Variables defined in outer scopes can be accessed in inner scopes or the same scope, whereas variables defined within a local scope cannot be accessed outside of that scope. For instance, functions cannot be utilized outside of their respective functions, and variables defined within a class can be accessed within functions within that class or along with the class. Consequently, for each extracted name, a scope is assigned. Function definitions, class definitions, and control flow constructs narrow the scope of variables or names defined or used within their respective bodies.

    %\tien{Given the undefined variable and its location in the notebook, we examine the cells to locate its definition using the established definition list, noting the position of the first occurrence. If the variable's definition appears later in the notebook than its initial use, this suggests a possible execution order issue due to disordered cells. In such cases, we categorize the notebook under “defined after use” and return the cell number where the definition is found. If no definition exists throughout the notebook, we classify it as “undefined.” This categorization guides our approach in the restoration stage.}

    %Assuming the top-down execution of the notebook, we use def-use sets of notebook cells to identify any undefined names, classifying them based on their first encounter: undefined in the entire notebook, defined after their usage, or both. If a variable is in a def set of current or any of the prior cells, we conclude that no NameError occurs for that variable. If a variable is not in any of those sets, we explore whether it is defined after that cell, indicating a potential issue due to disordered cell order. If the name is defined in any subsequent cell, we categorize the notebook into the "defined after use" group. Otherwise, it is placed in the "undefined" group.  When a cell contains undefined and defined-after variable names, we assign that notebook to the "both" category. This classification approach ensures thorough identification and categorization of potential NameError issues within the notebook. At the end of static error checking, we categorize notebooks as pathologically non-executable, non-executable but restorable, and potentially executable notebooks.   
    
    %For each cell in a notebook, we extract its usage list and compare each name against a combination of names defined prior to that cell. This combination comprises all names defined within the scope of usage and outer scopes within the same cell, as well as the definition lists of predecessor cells at scope 0. Why specifically scope 0? Considering that if a variable is defined in a deeper inner scope of a cell, the subsequent cell cannot access that name within a similar inner scope. Therefore, we only need to verify against the list of definition names at scope 0 of the cells that follow.    

    %Through static analysis, we can systematically categorize notebooks based on identified static errors, allowing for incremental refinement of our understanding of potential issues. This methodical step provides a structured framework for addressing errors and ensures a comprehensive examination of notebooks prior to dynamic analysis.

    % After this process, we collected a total of 22,998 notebooks containing NameError. Among those, 18,633 have at least one undefined variable in the first error encounter, 3,828 have variables defined after their usage, and 537 have both. 

    
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure*}[t!] % Figure 4
        \centerline{\includegraphics[width=1\textwidth]{images/LLM_Prompts.pdf}}
        \caption{Examples of prompts for LLM and responses for different error types. }
        \label{fig:LLM-prompts}
        % \vspace{-3ex}
\end{figure*}

    
    \subsection{LLM-Based Error-Driven Restoration}
        %In contrast to prior studies on notebook executability \cite{Head2019, Wang2020, Zhu2021, Wang2021} that consider a notebook non-executable if they result in an error, we attempt to recover the executability of notebooks from missing appropriate execution environment. These notebooks are still executable in the original author's environment and, therefore, intrinsically executable. 
        This section explains the lightweight use of LLMs to synthesize the execution environments, enabling improved measurements of notebook restorability. This addresses undefined name issues from incorrect execution orders or the absence of definitions, generates synthetic yet syntactically valid input, and installs external modules. 
        %Note that the goal of this work is not to create new notebook restoration techniques but to use existing tools to separate pathologically non-executable notebooks from misconfiguration.
        
%       LLMs have shown remarkable success in various data generation tasks, including program synthesis, comment generation, test generation, and debugging.
        
        


         %   In the pursuit of restoring the executability of non-executable notebooks, our approach hinges on the implementation of fundamental restoration strategies. Our overarching objective is to seamlessly recover the executability of these notebooks while meticulously preserving the integrity and semantics of their contents. Through employing these strategies, we strive to ensure that the essential essence and structure of the notebooks remain unaltered, thus facilitating their continued utility as valuable resources for future studies and analyses.

    
   
    %Our static-error checking phase identifies notebooks that are non-executable due to NameError issues. Among those, we restore the executability of notebooks where NameError occurs due to define-after-use issues. Notebooks in which a variable is undefined throughout are pathologically non-executable as there is no straightforward method to restore it without altering the notebook's semantics.
  
    %The nearest cell containing the variable's definition is extracted and placed immediately before the cell where the variable is utilized, as illustrated in Figure \ref{fig:reorder_workflow}. Cell reordering is performed in step \ding{204} of the analysis as shown in Figure \ref{fig:main_workflow}.
    
    %Moving a cell may either fix the NameError or lead to a new NameError. We apply static and dynamic error checking to the refactored notebook to evaluate its executability again. If the NameError does not appear, the executability of the reordered notebook is compared to that of the original file, where no reordering occurs. If a new NameError occurs, we consider such notebooks restorable, but we do not attempt to restore them further, as it requires a meticulous search for the right cell execution order. When a NameError occurs due to define-after-use within a cell, we classify such notebooks as pathologically non-executable.  This restoration phase concludes by addressing NameErrors that do not require intra-cell code refactoring. 

        \subsubsection{ModuleNotFound Error}
            Most of the repositories do not provide environment requirement information, and if they do, those modules are often outdated \cite{Zhu2021, Wang2021}. Notebooks with ``ModuleNotFound" errors are restorable since they were originally executable but failed to execute in a new environment. We attempt to re-establish the execution environment for the notebooks by inspecting the REQUIREMENTS/INSTALL instructions and installing the required dependencies in the execution environment before executing any notebooks. During execution, if a notebook returns a ``ModuleNotFound" error, we extract the missing module name from the error message and construct a terminal command {\small{\texttt{pip install <missing module>}}} (\ding{204}-D of Figure \ref{fig:main_workflow}). If the installation fails due to an incorrect or deprecated module name, we use LLM to obtain the correct and updated name for the corresponding missing module (\ding{204}-E of Figure \ref{fig:main_workflow} and LLM prompt in Figure \ref{fig:LLM-prompts}-A) and retry the installation. 
            
            %\tien{Figure \ref{fig:LLM-prompts}-B shows an example of the LLM prompt and response for a failed module installation.} After missing modules are installed into the execution environment, notebooks are subsequently re-executed to evaluate their executability (step \ding{205} in Figure \ref{fig:main_workflow})
            
        %We structure the prompt to facilitate the LLM in generating accurate and concise responses within our tailored response format. We then extract the updated module name from the LLM response and re-attempt the module installation with the new module name. We solely aim to assess whether the executability of the notebook improves through this intervention. 
         

   % Notebooks are frequently shared and reused among multiple users, leading to potential issues stemming from disparities in execution environments. Notebooks may include execution environment information; however, it is often inaccurate or outdated \cite{Wang2021}. The non-availability of correct external modules and packages is a major reason for notebook non-executability.  We consider notebooks with ModuleNotFound errors as restorable since they were originally executable but failed to execute in a new environment due to misconfiguration. Thus, we devise a lightweight strategy to restore the executability of such notebooks instead of classifying them as non-executable.   
    
    %When a notebook execution results in ModuleNotFound error, we extract the missing module name from the error message and construct a terminal command \codeword{pip install <missing module>} with the corresponding missing module. This installs the detected missing module into the current execution environment. We do not alter or update the missing module names due to deprecated APIs, as this could compromise the notebook's semantics. Our aim is solely to assess whether the executability of the notebook improves through this intervention.  Notebooks exhibiting this type of error are subsequently re-executed to evaluate their executability (step \ding{206} in Figure \ref{fig:exec_workflow}) after missing modules are installed into the execution environment.

        \subsubsection{FileNotFound Error}
            ``FileNotFound" errors often occur when notebooks attempt to access and read unavailable input files or directories. We use LLM to generate synthetic input data tailored to the notebook's semantics, attempting to resolve ``FileNotFound" errors while avoiding additional runtime issues. Our insight is that the code cells in notebooks contain sufficient information for LLMs to generate syntactically correct input data needed for execution. Since our goal is executability rather than reproducibility, syntactically correct synthetic data is sufficient to address the ``FileNotFound" error and restore notebook executability.
            %Given the absence of the actual data for the missing input files, using LLM is the optimal approach to generate suitable synthetic sample content, thereby enabling the restoration of notebook executability.         
            Concise contexts improve the LLM's performance \cite{Ramlochan2024}. Thus, we provide contexts of the notebook error, such as the missing file path and type, which guides LLM on the file's syntax, correct data type, and content. Instead of the {\small{\texttt{.ipynb}}} file, we provide the source code of all cells in the prompt. This conversion from {\small{\texttt{.ipynb}}} to Python also removes noise that may misguide or overwhelm the LLMs. Figure \ref{fig:LLM-prompts}-B shows this prompt and the response by the LLM for the example mentioned in \ref{case_study_1}. 
            %The three restoration strategies are completely automated to help scale the empirical evaluation.  




     
        \subsubsection{NameError}
            %NameError is one of the most common notebook executability errors arising from variable undefined issues when executed top-down linearly. Such errors often do not occur for a notebook's original authors, who know the correct non-linear execution of cells. However, notebooks are rarely accompanied by the execution order of cells when made publicly available.
            For a non-executable notebook due to undefined names, we extract the name and the specific cell in which the error occurs and the cell where the name definition is located or indicate if it cannot be found.  We then prompt LLM to address this issue as illustrated in \ding{204}-A of Figure \ref{fig:main_workflow} (b). The prompt also includes the code cells and the error type (see Figure \ref{fig:LLM-prompts}-C). The LLM's response is used to rewrite the notebook, which is analyzed again for additional errors. However, the changes introduced by LLMs may change the notebook's semantics. Since test cases are almost non-existent in notebooks (only 1.54\% notebooks have tests \cite{Pimentel2021}), it is challenging to verify semantic accuracy. Even if test cases are available, we must first make the notebook executable to enable testing and detect any semantic inconsistencies introduced by LLM-based modifications. Our position is that addressing executability is a prerequisite for reproducibility, which requires cell executability even if it causes semantic changes.  
     
     
%     Even if LLM-based restorations introduce minor logical errors, achieving executability is the supports future reproducibility techniques, creating opportunities to correct any semantically inconsistent fixes to NameError.

%    Jupyter notebooks are formatted as JSON-based files containing extensive information about the execution environment, metadata, and all cells even when they contain no content. The source code for each cell is stored as a single string, making it challenging for the LLM to extract accurate information. Therefore, we need the source code of the notebook as a whole. Additionally, supplying the LLM engine with the notebook's JSON-based format would be counterproductive due to the excess of unnecessary information. 

 %   To provide the aforementioned context information to the LLM, we use a regular expression to extract the full path of the missing input file from the reported error message during the dynamic execution of a notebook. To acquire the notebook's source code, we employ the PythonExplorer module to convert the JSON-based data into Python-typed data. This procedure integrates the source code of individual notebook cells into cohesive Python content.

   % We structure the prompt to facilitate the LLM in generating accurate data within our tailored response format, also shown in Figure \ref{fig:LLM-prompts}.
% {\em "Generate a sample input file \codeword{<missing_file_path>} for the source code below. Format the response with only the needed data between \texttt{\textasciigrave\textasciigrave\textasciigrave} and \texttt{\textasciigrave\textasciigrave\textasciigrave}. Just data and No fluff."<Notebook source code in Python>}
        %
    
    
    
    % To do so, we ensure that the Large Language Model (LLM) generates only the necessary data by including the directive ``Just data and No fluff." \waris{in the intro I mentioned we use regular expressions to sanitize. we can remove that point or add this insight there.}
    % This helps prevent the LLM from producing any superfluous or irrelevant content, eliminating the need to sanitize and refine the response. The response is then injected in the input files with the appropriate file names and directories. 
    
    % Once the missing input files have been created and correctly placed, we perform dynamic error checking again to determine whether the error has been resolved or if it remains unchanged, as shown in \ding{205} in Figure \ref{fig:main_workflow}. If the issue persists, it may be due to discrepancies in how the notebooks read the file directories versus how they were created. 
    
    
    % \waris{i believe we try to create the file path given in the filename. The error might me be that sometimes due to window machines path or username paths we cannot create the file path. We also do not want to edit the path in the notebook. It also quite possible that LLM did not generate the correct data. example it might miss a column in csv while generating data.} \gulzar{This ca}
    
    
    % In such cases, we halt the process and report the partial executability of the notebook, noting the specific issues preventing full execution.
