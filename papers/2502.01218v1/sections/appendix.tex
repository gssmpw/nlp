\appendix
\onecolumn
\section{Proofs}
\subsection{Proofs of Theorem~\ref{thm:delta-ordered}} \label{sec:proof_order}
For the proof of Theorem~\ref{thm:delta-ordered}, we closely follow the approaches presented in ~\cite{nips23-rnc} and adapted to our triplet case. We prove the theorem in three steps: 

\noindent(1) $\mathcal{L}^*:=\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i}n_{i, m} \log n_{i, m}$ is a lower bound of $\mathrm{L}_\mathrm{VLO}$, i.e., $\mathcal{L}_{\mathrm{VLO}} > \mathcal{L}^*$. 

\noindent(2) $\mathcal{L}^*$ is tight, i.e., for any $\epsilon > 0$, there exists representations such that $\mathcal{L}_{\mathrm{VLO}} < \mathcal{L}^* + \epsilon$. 

\noindent(3) For any $0 < \delta < 1$, there exist $\epsilon > 0$, such that if $\mathcal{L}_{\mathrm{VLO}} < \mathcal{L}^* + \epsilon$, then the learned representations satisfy VLO property.

(1) Recall that $\mathcal{L}_{\mathrm{VLO}}=\frac{1}{T}\sum\limits_{i=1}^T\frac{1}{T-1} \sum\limits_{j=1, j \neq i}^{T}-\log \frac{\exp\left(\mathfrak{R}_{i,j,l}\right)}{\sum\limits_{\mathbf{v}^k \in \mathcal{N}_{i, j}} \exp \left(\mathfrak{R}_{i,k,l}\right)}$, where $\mathcal{N}_{i, j}=\{\mathbf{v}_k | k \neq i , d_{i,j}<d_{i,k}\}$, we rewrite it as 
\begin{equation}
\begin{aligned}
& \mathcal{L}_{\mathrm{VLO}}=-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{j \in[T] \backslash\{i\}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k} \geq d_{i, j}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& =-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k} \geq D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& =-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{1}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k} \geq D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)} \\
& =-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{1}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)} \\
& -\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i,j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}\geq D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i,j,l}\right)} \\
& =-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& +\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \left(1+\frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}>D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}\right) \\
& >-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} .
\end{aligned}
\label{eq:A11}
\end{equation}

$\forall i \in[T], m \in\left[M_i\right]$, from Jensen's Inequality we have
\begin{equation}
\begin{aligned}
& -\sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& \geq-n_{i, m} \log \left(\frac{1}{n_{i, m}} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}\right)=n_{i, m} \log n_{i, m} .
\end{aligned}
\label{eq:A12}
\end{equation}

Thus, by plugging Eq.~\eqref{eq:A12} into Eq.~\eqref{eq:A11}, we have

\begin{equation} \label{eq:A13}
    \mathcal{L}_{\mathrm{VLO}}>\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} n_{i, m} \log n_{i, m}=L^{\star}
\end{equation}



(2) We will show for $\forall \epsilon>0$, there is a set of representations where

$$
\left\{\begin{array}{l}
\mathfrak{R}_{i, j,l}>\mathfrak{R}_{i, k,l}+\gamma \text { if } d_{i, j}<d_{i, k} \\
\mathfrak{R}_{i, j,l}=\mathfrak{R}_{i, k,l} \text { if } d_{i, j}=d_{i, k}
\end{array}\right.
$$

and $\gamma:=\log \frac{T}{\min\limits_{i \in[T], m \in\left[M_i\right]}n_{i, m} \epsilon}, \forall i \in[T], j, k \in[T] \backslash\{i\}$, such that $\mathcal{L}_{\mathrm{VLO}}<L^{\star}+\epsilon$.
For such a set of representations, $\forall i \in[T], m \in\left[M_i\right], j \in\left\{[T] \backslash\{i\} \mid d_{i, j}=D_{i, m}\right\}$,

\begin{equation} \label{eq:A14}
    -\log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}=\log n_{i, m}
\end{equation}

since $\mathfrak{R}_{i, k,l}=\mathfrak{R}_{i, j,l}$ for all $k$ such that $d_{i, k}=D_{i, m}=d_{i, j}$, and

\begin{equation} \label{eq:A15}
\begin{array}{rl} 
& \log \left(1+\frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}>D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash \{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}\right)
\\ & < \log \left(1 + \frac{T \exp(-\gamma)}{n_{i,m}}\right) < \frac{T \exp(-\gamma)}{n_{i,m}} \leq \epsilon.
\end{array}
\end{equation}


As $\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}<-\gamma$ for all $k$ such that $d_{i, k}>D_{i, m}=d_{i, j}$ and $\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}=0$ for all $k$ such that $d_{i, k}=D_{i, m}=d_{i, j}$.
From Eq.~\eqref{eq:A11} we have

\begin{equation} \label{eq:A16}
\begin{aligned}
\mathcal{L}_{\mathrm{VLO}}= &-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in [T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i \}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& +\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \left(1+\frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}>D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}\right).
\end{aligned}
\end{equation}


By plugging Eq.~\eqref{eq:A14} and Eq.~\eqref{eq:A15} into Eq.~\eqref{eq:A16} we have

\begin{equation} \label{eq:A17}
\mathcal{L}_{\mathrm{VLO}}<\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} n_{i, m} \log n_{i, m}+\epsilon=L^{\star}+\epsilon
\end{equation}


(3) We will show $\forall 0< \delta < 1$, there is a

$$
\epsilon=\frac{1}{T(T-1)} \min \left(\min _{i \in[T], m \in\left[M_i\right]} \log \left(1+\frac{1}{n_{i, m} \exp \left(\delta+\frac{1}{\delta}\right)}\right), 2 \log \frac{1+\exp (\delta)}{2}-\delta\right)>0,
$$

such that when $\mathcal{L}_{\mathrm{VLO}}<L^*+\epsilon$, the representations satisfy VLO property.
We first show that $\left|\mathfrak{R}_{i, j,l}-\mathfrak{R}_{i, k,l}\right|<\delta$ if $d_{i, j}=d_{i, k}$, i $\in[T], j, k \in[T] \backslash\{i\}$ when $L_{\mathrm{VLO}}<L^*+\epsilon$. From Eq.~\eqref{eq:A11} we have

\begin{equation} \label{eq:A18}
\mathcal{L}_{\operatorname{VLO}}>-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i} \sum\limits_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}-D_{i,m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}
\end{equation}

Let $p_{i, m}:=\underset{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}}{\arg \min } \mathfrak{R}_{i, j,l}, q_{i, m}:=\underset{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}}{\arg \max } \mathfrak{R}_{i, j,l}, \zeta_{i, m}:=\mathfrak{R}_{i, p_{i, m},l}, \eta_{i, m}:=$ $s_{i, q_{i,m},l}-s_{i, p_{i, m},l}, \forall i \in[T], m \in\left[M_i\right]$, by splitting out the maximum term and the minimum term we have

\begin{equation} \label{eq:A19}
\begin{aligned}
& \mathcal{L}_\mathrm{VLO}>-\frac{1}{T(T-1)} \sum\limits_{i=1}^{T} \sum\limits_{m=1}^{M_i}\left\{\log \frac{\exp \left(\zeta_{i, m}\right)}{\sum\limits_{k \in[T] \backslash \{i\}, d_{i, i}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}\right. \\
& \left.+\log \frac{\exp \left(\zeta_{i, m}+\eta_{i, m}\right)}{\sum\limits_{k \in[T] \backslash \{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}+\log \frac{\exp \left(\sum\limits_{j \in [T] \backslash\left\{i, p_{i,m}, q_{i, m}\right\}, d_{i, j}=D_{i, m}} \mathfrak{R}_{i, j,l}\right)}{\left(\sum\limits_{k \in [T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)\right)^{n_{i,m}-2}}\right\} .
\end{aligned}
\end{equation}


Let $\theta_{i, m}:=\frac{1}{n_{i, m}-2} \sum\limits_{j \in[T] \backslash\left\{i, p_{i,m}, q_{i, m}\right\},d_{i, j}=D_{i, j}} \exp \left(\mathfrak{R}_{i, j,l}-\zeta_{i, m}\right)$, we have

\begin{equation}\label{eq:A20}
    -\log \frac{\exp \left(\zeta_{i, m}\right)}{\sum\limits_{k \in[T] \backslash \{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}=\log \left(1+\exp \left(\eta_{i, m}\right)+\left(n_{i, m}-2\right) \theta_{i, m}\right)
\end{equation}
and

\begin{equation}\label{eq:A21}
    -\log \frac{\exp \left(\zeta_{i, m}+\eta_{i, m}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)}=\log \left(1+\exp \left(\eta_{i, m}\right)+\left(n_{i, m}-2\right) \theta_{i, m}\right)-\eta_{i, m}
\end{equation}


Then, from Jensen's inequality, we know

\begin{equation}\label{eq:A22}
    \exp \left(\sum_{j \in[T] \backslash\left\{i, p_{i, m}, q_{i, m}\right\}, d_{i, j}=D_{i, m}} \mathfrak{R}_{i, j,l}\right) \leq\left(\frac{1}{n_{i, m}-2} \sum_{j \in[T] \backslash\left\{i, p_{i, m}, q_{i, m}\right\}, d_{i, j}=D_{i, m}} \exp \left(\mathfrak{R}_{i, j,l}\right)\right)^{n_{i, m}-2},
\end{equation}

thus

\begin{equation}\label{eq:A23}
    -\log \frac{\exp \left(\sum\limits_{j \in [T] \backslash \{i, p_{i, m}, q_{i, m}\}, d_{i, j}=D_{i, m}} \mathfrak{R}_{i, j,l}\right)}{\left(\sum\limits_{k \in[T] \backslash \{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)\right)^{n_{i, m}-2}} \geq\left(n_{i, m}-2\right) \log \left(1+\exp \left(\eta_{i, m}\right)+\left(n_{i, m}-2\right) \theta_{i, m}\right)-\left(n_{i, m}-2\right) \log \left(\theta_{i, m}\right)
\end{equation}




By plugging Eq.~\eqref{eq:A20}, Eq.~\eqref{eq:A21} and Eq.~\eqref{eq:A23} into Eq.~\eqref{eq:A19}, we have

\begin{equation}\label{eq:A24}
    \mathcal{L}_{\mathrm{VLO}}>\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i}\left(n_{i, m} \log \left(1+\exp \left(\eta_{i, m}\right)+\left(n_{i, m}-2\right) \theta_{i, m}\right)-\eta_{i, m}-\left(n_{i, m}-2\right) \log \left(\theta_{i, m}\right)\right) .
\end{equation}


Let $h(\theta):=n_{i, m} \log \left(1+\exp \left(\eta_{i, m}\right)+\left(n_{i, m}-2\right) \theta\right)-\eta_{i, m}-\left(n_{i, m}-2\right) \log (\theta)$. From derivative analysis we know $h(\theta)$ decreases monotonically when $\theta \in\left[1, \frac{1+\exp \left(\eta_{i, m}\right)}{2}\right]$ and increases monotonically when $\theta \in\left[\frac{1+\exp \left(\eta_{i, m}\right)}{2}, \exp \left(\eta_{i, m}\right)\right]$, thus

\begin{equation}\label{eq:A25}
    h(\theta) \geq h\left(\frac{1+\exp \left(\eta_{i, m}\right)}{2}\right)=n_{i, m} \log n_{i, m}+2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m} .
\end{equation}

By plugging Eq.~\eqref{eq:A25} into Eq.~\eqref{eq:A24}, we have

\begin{equation}\label{eq:A26}
\begin{aligned}
\mathcal{L}_{\mathrm{VLO}} & >\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i}\left(n_{i, m} \log n_{i, m}+2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m}\right) \\
& =L^{\star}+\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i}\left(2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m}\right)
\end{aligned}
\end{equation}


Then, since $\eta_{i, m} \geq 0$, we have $2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m} \geq 0$. Thus, $\forall i \in[T], m \in\left[M_i\right]$,

\begin{equation}\label{eq:A27}
    \mathcal{L}_{\mathrm{VLO}}>L^{\star}+\frac{1}{T(T-1)}\left(2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m}\right)
\end{equation}


If $\mathcal{L}_{\mathrm{VLO}}<L^{\star}+\epsilon \leq L^{\star}+\frac{1}{T(T-1)}\left(2 \log \frac{1+\exp (\delta)}{2}-\delta\right)$, then

\begin{equation}\label{eq:A28}
    2 \log \frac{1+\exp \left(\eta_{i, m}\right)}{2}-\eta_{i, m}<2 \log \frac{1+\exp (\delta)}{2}-\delta
\end{equation}


Since $y(x)=2 \log \frac{1+\exp (x)}{2}-x$ increases monotonically when $x>0$, we have $\eta_{i, m}<\delta$. Hence $\forall i \in[T], j, k \in[T] \backslash\{i\}$, if $d_{i, j}=d_{i, k}=D_{i, m},\left|\mathfrak{R}_{i, j,l}-\mathfrak{R}_{i, k,l}\right| \leq \eta_{i, m}<\delta$.
Next, we show $\mathfrak{R}_{i, j,l}>\mathfrak{R}_{i, k,l}+\delta$ if $d_{i, j}<d_{i, k}$ when $\mathcal{L}_\mathrm{VLO}<L^{\star}+\epsilon$.
From Eq.~\eqref{eq:A11} we have

\begin{equation}\label{eq:A29}
\begin{aligned}
\mathcal{L}_{\mathrm{VLO}}= & -\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i} \sum_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \frac{\exp \left(\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}\right)} \\
& +\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i} \sum_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \left(1+\frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash \{i\}, d_{i, k}>D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}\right),
\end{aligned}
\end{equation}

and combining it with Eq.~\eqref{eq:A12} we have

\begin{equation}\label{eq:A30}
\begin{aligned}
\mathcal{L}_{\mathrm{VLO}} & \geq L^{\star}+\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i} \sum_{j \in[T] \backslash\{i\}, d_{i, j}=D_{i, m}} \log \left(1+\frac{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}>D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{k \in[T] \backslash\{i\}, d_{i, k}=D_{i, m}} \exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}\right) \\
& >L^{\star}+\frac{1}{T(T-1)} \log \left(1+\frac{\exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{\sum\limits_{h \in[T] \backslash\{i\}, d_{i, h}=d_{i, j}} \exp \left(\mathfrak{R}_{i, h,l}-\mathfrak{R}_{i, j,l}\right)}\right),
\end{aligned}
\end{equation}



$\forall i \in[T], j \in[T] \backslash\{i\}, k \in\left\{k \in[T] \backslash\{i\} \mid d_{i, j}<d_{i, k}\right\}$.
When $\mathcal{L}_{\mathrm{VLO}}<L^{\star}+\epsilon$, we already have $\left|\mathfrak{R}_{i, h,l}-\mathfrak{R}_{i, j,l}\right|<\delta, \forall d_{i, h}=d_{i, j}$, which derives $\mathfrak{R}_{i,h, l}-\mathfrak{R}_{i, j,l}<\delta$ and thus $\exp \left(\mathfrak{R}_{i,h, l}-\mathfrak{R}_{i, j,l}\right)<\exp (\delta)$. By putting this into Eq.~\eqref{eq:A29}, we have $\forall i \in[T], j \in$ $[T] \backslash\{i\}, k \in\left\{k \in[T] \backslash\{i\} \mid d_{i, j}<d_{i, k}\right\}$,

\begin{equation}\label{eq:A31}
    \mathcal{L}_{\mathrm{VLO}}>L^{\star}+\frac{1}{T(T-1)} \log \left(1+\frac{\exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{n_{i, r_{i, j}} \exp (\delta)}\right)
\end{equation}

where $r_{i, j} \in\left[M_i\right]$ is the index such that $D_{i, r_{i, j}}=d_{i, j}$.

Further, given $\mathcal{L}_{\mathrm{VLO}}<L^{\star}+\epsilon<L^{\star}+\frac{1}{T(T-1)} \log \left(1+\frac{1}{n_{i, r_{i, j}} \exp \left(\delta+\frac{1}{\delta}\right)}\right)$, we have

\begin{equation}\label{eq:A32}
    \log \left(1+\frac{\exp \left(\mathfrak{R}_{i, k,l}-\mathfrak{R}_{i, j,l}\right)}{n_{i, r_{i, j}} \exp (\delta)}\right)<\log \left(1+\frac{1}{n_{i, r_{i, j}} \exp \left(\delta+\frac{1}{\delta}\right)}\right)
\end{equation}

which derives $\mathfrak{R}_{i, j,l}>\mathfrak{R}_{i, k,l}+\frac{1}{\delta}, \forall i \in[T], j \in[T] \backslash\{i\}, k \in\left\{[T] \backslash\{i\} \mid d_{i, j}<d_{i, k}\right\}$.
Finally, $\forall i \in[T], j, k \in[T] \backslash\{i\}, \mathfrak{R}_{i, j,l}<\mathfrak{R}_{i, k,l}-\frac{1}{\delta}$ if $d_{i, j}>d_{i, k}$ directly follows from $\mathfrak{R}_{i, j,l}> \mathfrak{R}_{i, k,l}+\frac{1}{\delta}$ if $d_{i, j}<d_{i, k}$.
\qed
\subsection{Proofs of Theorem ~\ref{thm:continuity}}
\label{sec:proof_continuity}
\paragraph{Setup and Assumptions.}
To provide the vision-language continuity, we first assume that the frame embeddings $\{\mathbf{v}_t\}$, where $t\in[1, T]$ are regularized under a Brownian Bridge process $\mathbf{B}(t)$ as discussed in Section~\ref{sec:bb}, where the transition density for any intermediate time $t \in [n(i), n(j)]$ within a sampled interval is given as:
\begin{equation}
\mathbf{B}(t) \sim \mathcal{N}\left(\mathbb{E}[\mathbf{B}(t)], \mathrm{Var}[\mathbf{B}(t)]\right),
\end{equation}
with:
\begin{equation}
\mathbb{E}[\mathbf{B}(t)] = \mathbf{v}_i + \frac{t - n(i)}{n(j) - n(i)} (\mathbf{v}_j - \mathbf{v}_i),~ \mathrm{Var}[\mathbf{B}(t)] = \frac{(t - n(i))(n(j) - t)}{n(j) - n(i)}.
\end{equation}
 All time steps \( t \in [1, T] \) are covered by at least one sampled interval, ensuring the entire video sequence satisfies the Brownian Bridge regularization. Now, let \( \mathbf{v}_k, \mathbf{v}_l \in\mathbb{R}^d\) be arbitrary embeddings, \textit{not necessarily} the endpoints \( \mathbf{v}_i \) and \( \mathbf{v}_j \) of a sampled interval. These embeddings fall within the \textit{union} $\mathfrak{U}$ of all sampled local intervals. Without loss of generality, here we can identify the interval \([n(i), n(j)]\in\mathfrak{U}\) from the union containing \( \mathbf{v}_k \) and \( \mathbf{v}_l \).


\noindent \textbf{Bounding Local Continuity.} Recall that semantic alignment score $\mathfrak{R}(\mathbf{v}_k, \mathbf{v}_l, \mathbf{l})$ is  defined as:
\[
\mathfrak{R}(\mathbf{v}_k, \mathbf{v}_l, \mathbf{l}) = -\|\operatorname{sim}(\mathbf{v}_k, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_l, \mathbf{l})\|_2,
\]
where \( \operatorname{sim}(\cdot) \) is Lipschitz continuous with constant \( C > 0 \) when embeddings are normalized as unit vectors. By the Lipschitz continuity of \( \operatorname{sim}(\cdot) \), we have:
\[
\|\operatorname{sim}(\mathbf{v}_k, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_l, \mathbf{l})\|_2 \leq C \cdot \|\mathbf{v}_k - \mathbf{v}_l\|_2.
\]

To ensure the continuity of \( \mathfrak{R} \), we must bound \( \|\mathbf{v}_k - \mathbf{v}_l\|_2 \). Under the Brownian Bridge regularization, the embeddings are aligned with the mean trajectory \( \mathbb{E}[\mathbf{B}(t)] \), and deviations are constrained by the variance \( \mathrm{Var}[\mathbf{B}(t)] \). Specifically:
\[
\|\mathbf{v}_t - \mathbb{E}[\mathbf{B}(t)]\|_2^2 \leq \lambda \cdot \mathrm{Var}[\mathbf{B}(t)],
\]
where \( \lambda > 0 \) depends on the strength of the Brownian Bridge loss \( \mathcal{L}_{\mathrm{BB}} \). Below we omit $\lambda$ for simplicty. Substituting the variance:
\[
\mathrm{Var}[\mathbf{B}(t)] = \frac{(t - n(i))(n(j) - t)}{n(j) - n(i)}.
\]

\noindent \textbf{Bounding Pairwise Distance.} The total pairwise distance between \( \mathbf{v}_k \) and \( \mathbf{v}_l \) can be expressed as:
\[
\|\mathbf{v}_k - \mathbf{v}_l\|_2 \leq \|\mathbb{E}[\mathbf{B}(k)] - \mathbb{E}[\mathbf{B}(l)]\|_2 + \sqrt{ \mathrm{Var}[\mathbf{B}(k)]} + \sqrt{ \mathrm{Var}[\mathbf{B}(l)]}.
\]

Since the mean trajectory \( \mathbb{E}[\mathbf{B}(t)] \) is linear within the interval \([n(i), n(j)]\), we have:
\[
\|\mathbb{E}[\mathbf{B}(k)] - \mathbb{E}[\mathbf{B}(l)]\|_2 \leq \frac{|k - l|}{n(j) - n(i)} \|\mathbf{v}_j - \mathbf{v}_i\|_2.
\]

Combining these bounds, now we can rewrite into the following inequality:
\[
\|\mathbf{v}_k - \mathbf{v}_l\|_2 \leq \frac{|k - l|}{n(j) - n(i)} \|\mathbf{v}_j - \mathbf{v}_i\|_2 + \sqrt{\frac{(k - n(i))(n(j) - k)}{n(j) - n(i)}} + \sqrt{\frac{(l - n(i))(n(j) - l)}{n(j) - n(i)}}.
\]

% Substituting this into the Lipschitz continuity of \( \operatorname{sim} \), we obtain:
% \[
% \|\operatorname{sim}(\mathbf{v}_k, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_l, \mathbf{l})\|_2 \leq C \cdot \left( \frac{|k - l|}{n(j) - n(i)} \|\mathbf{v}_j - \mathbf{v}_i\|_2 + \sqrt{\epsilon \cdot \frac{(k - n(i))(n(j) - k)}{n(j) - n(i)}} + \sqrt{\epsilon \cdot \frac{(l - n(i))(n(j) - l)}{n(j) - n(i)}} \right).
% \]

For the variance terms, the Brownian Bridge process achieves its maximum variance at the midpoint $t=\frac{n(i) + n(j)}{2}$. This gives us,
\[\mathrm{Var}[\mathbf{B}(t_{\operatorname{max})}] = \frac{n(j)-n(i)}{4}, ~
\|\mathbf{v}_k - \mathbf{v}_l\|_2 \leq 2\frac{|k - l|}{n(j) - n(i)} +  \sqrt{ (n(j) - n(i))}.
\]
\noindent \textbf{Bounding Semantic Alignment Score.} Finally, by substituting this bound into the Lipschitz continuity of $\operatorname{sim}$, we obtain,
\[
\left|\mathfrak{R}(\mathbf{v}_k, \mathbf{v}_l, \mathbf{l})\right| \leq C \cdot \left( \frac{2|k - l|}{n(j)-n(i)} + \sqrt{(n(j) - n(i))} \right).
\]
To ensure \( \left|\mathfrak{R}(\mathbf{v}_k, \mathbf{v}_l, \mathbf{l})\right| < \epsilon \), we require:
\[
C \cdot \left( 2\frac{|k - l|}{n(j) - n(i)} + \sqrt{n(j) - n(i)} \right) < \epsilon.
\]
Here, we consider these two terms respectively:
    \[
    2C\frac{|k - l|}{n(j) - n(i)} < \frac{\epsilon}{2},~ C \sqrt{n(j) - n(i)} < \frac{\epsilon}{2},
    \]
    which gives:
    \[
    |k - l| < \delta_1 = \frac{\epsilon \cdot (n(j) - n(i))}{4C},~ n(j) - n(i) < \left(\frac{\epsilon}{2C}\right)^2.
    \]

Combining these conditions, we choose:
\[
\delta = \min\left( \frac{\epsilon \cdot (n(j) - n(i))}{4C}, \frac{\epsilon^2}{4C^2} \right).
\]

\paragraph{Final Conclusion.}
For any given \( \epsilon > 0 \), setting \( \delta = \min\left( \frac{\epsilon \cdot (n(j) - n(i))}{4C}, \frac{\epsilon^2}{4C^2} \right) \) ensures:
\[
\|\mathbf{v}_k - \mathbf{v}_l\|_2 < \delta \quad \Rightarrow \quad \left|\mathfrak{R}(\mathbf{v}_k, \mathbf{v}_l, \mathbf{l})\right| < \epsilon.
\]
\qed

\subsection{Proof of Theorem \ref{thm:robustness}}\label{sec:proof_robustness}
From the definition of the semantic alignment score, we have:
\[
\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) = -|\operatorname{sim}(\mathbf{v}_i, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_j, \mathbf{l})|,~ \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}') = -|\operatorname{sim}(\mathbf{v}_i, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_j, \mathbf{l}')|.
\]
The difference in scores can be bounded using the reverse triangle inequality:
\[
|\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}') - \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l})| \leq |(\operatorname{sim}(\mathbf{v}_i, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_j, \mathbf{l}')) - (\operatorname{sim}(\mathbf{v}_i, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_j, \mathbf{l})) |.
\]

Simplifying the inequalities above, it gives us:
\[
|\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}') - \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l})| \leq |\operatorname{sim}(\mathbf{v}_i, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_i, \mathbf{l})| + |\operatorname{sim}(\mathbf{v}_j, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_j, \mathbf{l})|.
\]

By the Lipschitz continuity of \( \operatorname{sim} \), we have: for some constant $C>0$,
\[
|\operatorname{sim}(\mathbf{v}_i, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_i, \mathbf{l})| \leq C \|\mathbf{l}' - \mathbf{l}\|_2, |\operatorname{sim}(\mathbf{v}_j, \mathbf{l}') - \operatorname{sim}(\mathbf{v}_j, \mathbf{l})| \leq C \|\mathbf{l}' - \mathbf{l}\|_2.
\]
Substituting these bounds and considering \( \|\mathbf{l}' - \mathbf{l}\|_2 \leq \delta_l \)
\begin{equation}
    \begin{split}
|\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}') - \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l})| &\leq 2C \|\mathbf{l}' - \mathbf{l}\|_2 \leq 2C \delta_l.
\end{split}
\end{equation}
\qed



% \paragraph{Objective.}
% We aim to show that, under the Brownian Bridge regularization, for any $\epsilon > 0$, there exists $\delta > 0$ such that:
% \begin{equation}
% \|\mathbf{v}_i - \mathbf{v}_j \|_2 < \delta \Rightarrow \left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| < \epsilon.
% \end{equation}

% \paragraph{Bounding the Semantic Alignment Score.}
% Using the Lipschitz continuity of $f$, the semantic difference can be bounded as:
% \begin{equation}
% \| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2 \leq C_l \| \mathbf{v}_i - \mathbf{v}_j \|_2.
% \end{equation}
% Substituting this into the definition of $\mathfrak{R}$, we obtain:
% \begin{equation}
% \left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| = \| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2 \leq C_l \| \mathbf{v}_i - \mathbf{v}_j \|_2.
% \end{equation}

% \paragraph{Condition on $\delta$ and $\epsilon$.}
% Let $\epsilon > 0$ be given. To ensure $\left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| < \epsilon$, it suffices to enforce:
% \begin{equation}
% C_l \| \mathbf{v}_i - \mathbf{v}_j \|_2 < \epsilon.
% \end{equation}
% Rewriting this, we require:
% \begin{equation}
% \| \mathbf{v}_i - \mathbf{v}_j \|_2 < \frac{\epsilon}{C_l}.
% \end{equation}
% Let $\delta = \frac{\epsilon}{C_l}$. Then, for any $\|\mathbf{v}_i - \mathbf{v}_j\|_2 < \delta$, it follows that:
% \begin{equation}
% \left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| < \epsilon.
% \end{equation}

% \paragraph{Ensuring $\| \mathbf{v}_i - \mathbf{v}_j \|_2 < \delta$ under Brownian Bridge.}
% The Brownian Bridge process regularizes embeddings $\mathbf{v}_t$ such that deviations from the mean trajectory $\mathbb{E}[\mathbf{B}(t)]$ are penalized by the variance-aware loss:
% \begin{equation}
% \mathcal{L}_{\mathrm{BB}} = \frac{1}{T} \sum_{t=1}^{T-1} \frac{1}{2 \mathrm{Var}[\mathbf{B}(t)]} \|\mathbf{v}_t - \mathbb{E}[\mathbf{B}(t)]\|_2^2.
% \end{equation}
% This ensures that:
% \begin{equation}
% \|\mathbf{v}_t - \mathbb{E}[\mathbf{B}(t)]\|_2 \leq \sqrt{2 \mathrm{Var}[\mathbf{B}(t)]}.
% \end{equation}

% For any embeddings $\mathbf{v}_i$ and $\mathbf{v}_j$, the pairwise distance is therefore bounded by:
% \begin{equation}
% \|\mathbf{v}_i - \mathbf{v}_j\|_2 \leq \sqrt{\frac{T}{2}} \sigma,
% \end{equation}
% where $\sigma$ is the variance scaling factor. By setting $\sigma$ and $T$ appropriately, this distance can be made smaller than $\delta$. Thus, the condition $\|\mathbf{v}_i - \mathbf{v}_j\|_2 < \delta$ is satisfied.
% \qed

% Since embeddings are regularized using a Brownian Bridge process $ \mathbf{B}(t) $ over the interval $ [a, b] $, with fixed endpoints:
% \begin{equation}
% \mathbf{B}(a) = \mathbf{v}_a, \quad \mathbf{B}(b) = \mathbf{v}_b,
% \end{equation}
% the variance of the process at an intermediate point $ t \in [a, b] $ is:
% \begin{equation}
% \mathrm{Var}[\mathbf{B}(t)] = \frac{(t-a)(b-t)}{b-a}.
% \end{equation}

% The worst-case bound occurs at the midpoint $ t = \frac{a + b}{2} $:
% \begin{equation}
% \mathrm{Var}\left[\mathbf{B}\left(\frac{a+b}{2}\right)\right] = \frac{b-a}{4}.
% \end{equation}

% Applying bias-variance decomposition to estimate the expected squared pairwise distance:
% \begin{equation}
% \mathbb{E}[\|\mathbf{v}_i - \mathbf{v}_j\|_2^2] \leq \frac{(b-a)}{2} \sigma^2.
% \end{equation}

% Taking the square root, we obtain the bound:
% \begin{equation}
% \mathbb{E}[\|\mathbf{v}_i - \mathbf{v}_j\|_2] \leq \sqrt{\frac{(b-a)}{2}} \sigma.
% \end{equation}

% Thus, we define $ \delta $ as:
% \begin{equation}
% \delta = \sqrt{\frac{(b-a)}{2}} \sigma.
% \end{equation}
% Since the function $ f $ is Lipschitz continuous with constant $ C_l $, we have:

% \begin{equation}
% \| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2 \leq C_l \|\mathbf{v}_i - \mathbf{v}_j\|_2.
% \end{equation}

% Using the previously derived bound:

% \begin{equation}
% \| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2 \leq C_l \cdot \delta.
% \end{equation}

% By setting $ \delta = \frac{\epsilon}{C_l} $, we ensure:

% \begin{equation}
% \| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2 < \epsilon.
% \end{equation}

% From the definition of the semantic alignment score:

% \begin{equation}
% \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) = -\| f(\mathbf{v}_i, \mathbf{l}) - f(\mathbf{v}_j, \mathbf{l}) \|_2.
% \end{equation}

% Thus, we obtain:

% \begin{equation}
% \left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| < \epsilon.
% \end{equation}

% Therefore, we have shown that for any given $ \epsilon > 0 $, choosing $ \delta = \frac{\epsilon}{C_l} $ satisfies the continuity condition:

% \begin{equation}
% \| \mathbf{v}_i - \mathbf{v}_j \|_2 < \delta \quad \Rightarrow \quad \left| \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) \right| < \epsilon.
% \end{equation}


% \qed


\section{Pre-training Details}\label{sec:pretrain_details}
Following \cite{icml23-liv,icml24-decisionnce}, we use a modified ResNet-50 ~\cite{cvpr16-resnet} from CLIP ~\cite{icml21-clip} for the vision encoder and a CLIP transformer for the language encoder. We initialize our model with CLIP and train them on EPIC-KITCHEN-100 ~\cite{corr18-epickitchen,corr20-epickitchen}. The training hyperparameters used during the pre-training are listed in Table~\ref{tab:hyperparam}. For $\mathcal{L}_{BB}$, due to the large number of video frames, we apply a logarithmic scaling to the variance term. The training was conducted on two NVIDIA A800 GPUs, and the process took approximately 30 hours. 

\begin{table*}[h]
    \centering
    \caption{Hyper-parameters for pre-training.}
    \label{tab:hyperparam}
    \begin{tabular}{lc}
        \toprule
        Config & value \\
        \midrule
        Training epochs & 1000 \\
        Optimizer & Adam \\
        Learning rate & $1 \times 10^{-5}$ \\
        Batch size & 128 \\
        Sampled frames per video & 10 \\
        Weight decay & 0.001 \\
        Optimizer momentum & 0.9, 0.999 \\
        Data augmentation & \textit{RandomCropResize} \\
        \bottomrule
    \end{tabular}
    
\end{table*}



\section{Evaluation Details}
\subsection{Simulation Environments} \label{sec:app_env}
We follow ~\cite{corl22-r3m} for the specific simulation environment setup and code details.
\paragraph{Franka Kitchen.} The Franka Kitchen environment ~\cite{corl19-franka,corr20-franka} is based on the 9 degrees of freedom Franka robot. The Franka robot is placed in a kitchen environment containing several common household items: a microwave, a kettle, an overhead light, cabinets, and an oven. Following ~\cite{corl22-r3m}, the Franka Kitchen environments used in this paper are modified from their original design. Specifically, we introduce additional randomization to the scene by randomly altering the kitchen's position between episodes. This modification makes the tasks significantly more challenging in terms of both perception and control.
\paragraph{Metaworld.} The Metaworld environment ~\cite{corl19-metaworld} is an open-source simulated benchmark for robot learning. In our settings, the target object position is randomized between episodes in all tasks. 

We present the specific language instructions for each tasks in Table~\ref{tab:environment_language}.  


\begin{table}[h]
    \centering
    \caption{Language Instructions for tasks in Franka Kitchen and Metaworld.}
    \label{tab:environment_language}
    \begin{tabular}{lc}
        \toprule
        Environment ID & Language Instruction     \\ \hline
        kitchen\_micro\_open-v3  & open microwave         \\ 
        kitchen\_sdoor\_open-v3  & slide cabinet    \\ 
        kitchen\_ldoor\_open-v3  & open left door         \\
        kitchen\_knob1\_on-v3    & turn on stove          \\ 
        kitchen\_light\_on-v3    & switch on light        \\ 
        \midrule
        hammer-v2-goal-observable & hammer nail \\
        button-press-topdown-v2-goal-observable & press button \\
        bin-picking-v2-goal-observable & pick and place the block between bins \\
        assembly-v2-goal-observable & assemble the ring onto peg \\
        \bottomrule
    \end{tabular}
    
\end{table}



\subsection{Language-Conditioned Behavior Cloning Hyperparameters}
\label{sec:app_lcbcparam}
We present the LCBC imitation learning hyperparameters in Table~\ref{tab:bcparam}. For each distinct evaluation task, we perform policy evaluation every 1,000 gradient steps by running 50 rollouts and computing their average success rate. Over a total of 10,000 gradient steps, we conduct this evaluation 10 times. The highest success rate among these 10 evaluations is reported as the final result. To ensure robustness, we average the results across two different camera viewpoints and three independent random seeds.


\begin{table*}[h]
    \centering
    \caption{Hyper-parameters for LCBC.}
    \label{tab:bcparam}
    \begin{tabular}{lcc}
        \toprule
         & Franka Kitchen & Metaworld \\
        \midrule
        MLP achitecture & [256,256] & [256,256] \\
        Non-linear activation & ReLU & ReLU \\
        Optimizer & Adam & Adam \\
        Gradient Steps & 10K & 10K \\
        Learning rate & $1 \times 10^{-3}$ & $1 \times 10^{-3}$ \\
        Batch size & 32 & 32 \\
        Horizon & 50 & 100 \\
        Proprioception & 9 & 4 \\
        \bottomrule
    \end{tabular}
    
\end{table*}

\subsection{Language-Conditioned Behavior Cloning Results} \label{sec:app_lcbc}
In Table~\ref{tab:franka_5}-~\ref{tab:metaworld_25}, we report detailed Language-Conditioned Behavior Cloning results for different task and dataset size. The results demonstrate that our method achieves significant improvements across different simulation environments, varying dataset sizes, and diverse robotic manipulation tasks.



% Requires: \usepackage{booktabs}
\begin{table*}[h]
    \centering
    \caption{LCBC results when dataset size$=5$ on Franka Kitchen.}
    \label{tab:franka_5}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcccccc}
        \toprule
        Method & Slide Cabinet & Open Left Door & Open Microwave & Turn On Stove & Switch On Light & Average \\
        \midrule
        CLIP & 38.7 & 2.0 & 3.0 & 7.0 & 7.7 & 11.7 \\
        R3M & 68.7 & 18.3 & 7.7 & 19.3 & 29.0 & 28.6 \\
        LIV & 55.0 & 6.0 & 7.0 & 13.0 & 22.0 & 20.6 \\
        DecisionNCE & 59.3 & 9.7 & 7.0 & \cellcolor{lightgray}\textbf{26.3} & 24.3 & 25.3 \\
        AcTOL w/o BB & 71.5 & 11.5 & 10.5 & 23.5 & 47.0 & 32.8 \\
        AcTOL & \cellcolor{lightgray}\textbf{85.5} & \cellcolor{lightgray}\textbf{20.0} & \cellcolor{lightgray}\textbf{18.3} & 24.7 & \cellcolor{lightgray}\textbf{62.3} & \cellcolor{lightgray}\textbf{42.6} \\
        \bottomrule
    \end{tabular}
    }
    
\end{table*}

\begin{table*}[t]
    \centering
    \caption{LCBC results when dataset size$=15$ on Franka Kitchen.}
    \label{tab:franka_15}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcccccc}
        \toprule
        Method & Slide Cabinet & Open Left Door & Open Microwave & Turn On Stove & Switch On Light & Average \\
        \hline
        CLIP & 71.0 & 8.0 & 15.7 & 14.7 & 28.0 & 27.5 \\
        R3M & 81.0 & 31.0 & 22.0 & 19.3 & 57.7 & 42.2 \\
        LIV & 85.0 & 19.0 & 28.3 & 29.7 & 51.7 & 42.7 \\
        DecisionNCE & 92.0 & 18.7 & 27.0 & 33.3 & 45.0 & 43.2 \\
        AcTOL w/o BB & 84.5 & 29.5 & 29.5 & \cellcolor{lightgray}\textbf{54.0} & 73.5 & 54.2 \\
        AcTOL & \cellcolor{lightgray}\textbf{99.5} & \cellcolor{lightgray}\textbf{37.5} & \cellcolor{lightgray}\textbf{37.0} & 53.5 & \cellcolor{lightgray}\textbf{81.5} & \cellcolor{lightgray}\textbf{61.8} \\
        \bottomrule
    \end{tabular}
    }
    
\end{table*}

\begin{table*}[t]
    \centering
    \caption{LCBC results when dataset size$=25$ on Franka Kitchen.}
    \label{tab:franka_25}
    \resizebox{\linewidth}{!}{ 
    \begin{tabular}{lcccccc}
        \toprule
        Method & Slide Cabinet & Open Left Door & Open Microwave & Turn On Stove & Switch On Light & Average \\
        \hline
        CLIP & 66.3 & 8.7 & 18.7 & 23.7 & 38.7 & 31.2 \\
        R3M & 84.7 & 35.3 & 40.0 & 34.0 & 61.7 & 51.1 \\
        LIV & 91.7 & 26.0 & 35.0 & 45.3 & 61.7 & 51.9 \\
        DecisionNCE & 91.7 & 27.0 & 37.0 & 47.3 & 51.3 & 50.9 \\
        AcTOL w/o BB & 92.0 & \cellcolor{lightgray}\textbf{37.0} & 40.0 & 57.0 & 78.0 & 60.8 \\
        AcTOL & \cellcolor{lightgray}\textbf{100.0} & \cellcolor{lightgray}\textbf{37.0} & \cellcolor{lightgray}\textbf{42.5} & \cellcolor{lightgray}\textbf{62.5} & \cellcolor{lightgray}\textbf{81.0} & \cellcolor{lightgray}\textbf{64.6} \\
        \bottomrule
    \end{tabular}
    }
    
\end{table*}

\begin{table*}[t]
    \centering
    \caption{LCBC results when dataset size$=5$ on Metaworld.}
    \label{tab:metaworld_5}
    \begin{tabular}{lccccc}
        \toprule
        Method & Assembly & Pick bin & Press button & Hammer & Average \\
        \hline
        CLIP & 48.3 & 35.3 & 34.3 & 51.2 & 42.3 \\
        R3M & 63.5 & 33.3 & 27.3 & 63.2 & 46.8 \\
        LIV & 61.8 & 32.3 & 32.7 & 61.0 & 47.0 \\
        DecisionNCE & 54.0 & 31.0 & 27.7 & 65.7 & 44.6 \\
        AcTOL w/o BB & \cellcolor{lightgray}\textbf{66.8} & 39.0 & 20.7 & \cellcolor{lightgray}\textbf{74.7} & 50.3 \\
        AcTOL & 62.8 & \cellcolor{lightgray}\textbf{41.0} & \cellcolor{lightgray}\textbf{42.0} & 69.5 & \cellcolor{lightgray}\textbf{53.8} \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{LCBC results when dataset size$=15$ on Metaworld.}
    \label{tab:metaworld_15}
    \begin{tabular}{lccccc}
        \toprule
        Method & Assembly & Pick bin & Press button & Hammer & Average \\
        \hline
        CLIP & 73.0 & 40.3 & 52.0 & 76.0 & 60.3 \\
        R3M & 80.7 & 17.0 & 45.0 & 83.3 & 56.5 \\
        LIV & 84.3 & 37.0 & 54.7 & 81.3 & 64.3 \\
        DecisionNCE & 73.3 & 36.7 & 43.3 & 83.0 & 59.1 \\
        AcTOL w/o BB & \cellcolor{lightgray}\textbf{94.0} & 50.3 & 48.3 & \cellcolor{lightgray}\textbf{90.7} & 70.8 \\
        AcTOL & 82.5 & \cellcolor{lightgray}\textbf{64.5} & \cellcolor{lightgray}\textbf{65.5} & 84.0 & \cellcolor{lightgray}\textbf{74.1} \\
        \bottomrule
    \end{tabular}
\end{table*}

\begin{table*}[t]
    \centering
    \caption{LCBC results when dataset size$=25$ on Metaworld.}
    \label{tab:metaworld_25}
    \begin{tabular}{lccccc}
        \toprule
        Method & Assembly & Pick bin & Press button & Hammer & Average \\
        \hline
        CLIP & 69.3 & 36.0 & 66.0 & 78.8 & 62.5 \\
        R3M & 87.7 & 14.7 & 48.3 & 89.7 & 60.1 \\
        LIV & 87.3 & 23.7 & 66.0 & 89.7 & 66.7 \\
        DecisionNCE & 85.7 & 47.0 & 58.0 & 88.3 & 69.8 \\
        AcTOL w/o BB & \cellcolor{lightgray}\textbf{93.7} & 51.7 & 55.0 & \cellcolor{lightgray}\textbf{93.0} & 73.3 \\
        AcTOL & 93.5 & \cellcolor{lightgray}\textbf{66.0} & \cellcolor{lightgray}\textbf{76.5} & 88.5 & \cellcolor{lightgray}\textbf{81.1} \\
        \bottomrule
    \end{tabular}
\end{table*}


\begin{figure}[t]
% \vskip 0.2in
\begin{center}

\centerline{\includegraphics[scale=0.56]{figs/app_reward.pdf}}

\caption{Reward plots for exemplar robot action videos.}
\label{fig:app_reward}
\end{center}
\end{figure}

\subsection{Language-Conditioned Visual Reward Results} \label{sec:app_reward}
As shown in Figure~\ref{fig:app_reward}, we present more visualizations of Language-Conditioned Visual Reward on real-world robot manipulation videos from ~\cite{rss22-robotvideo}. In Figure~\ref{fig:app_reward}(a), the robot performs two consecutive and opposing actions. Our method effectively identifies the action boundaries and generates the correct reward sequence, increasing first and then decreasing, in alignment with the given instructions. In Figures~\ref{fig:app_reward}(b)-(d), where the robot performs a single action, the robot initially moves slowly as it searches for the target. Correspondingly, the reward grows gradually. Once the robot interacts with the object and completes the task, our method captures the distinct semantic changes in the action, leading to a rapid reward increase. In Figures~\ref{fig:app_reward}(e)-(f), we test two complex actions and instructions to explore the limits of our method. In Figure~\ref{fig:app_reward}(e), the model is required to accurately distinguish between the blue and red cups to complete the task. In Figure~\ref{fig:app_reward}(f), the model needs to differentiate the orientation and face values of two dice. These scenarios impose high demands on the model's visual and semantic understanding. Our method successfully produces the correct rewards in both tasks, showcasing its potential for application in real-world, complex scenarios.