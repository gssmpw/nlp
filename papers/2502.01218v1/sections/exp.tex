\section{Experiment}
\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/env.pdf}}
\caption{Simulation environments, including two camera views of Franka Kitchen (5 tasks) and Metaworld (4 tasks).}
\label{env}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/comparison.pdf}}
\caption{The averaged success rate comparisons over various tasks on (a) Franka Kitchen and (b) Metaworld.}
\label{fig:comparison}
\end{center}
\vskip -0.2in
\end{figure*}


\noindent\textbf{Experimental Setup.} We initialize our model with the weights of CLIP ~\cite{icml21-clip} with ResNet50 vision backbone and further pre-train it on the large-scale human action video dataset EPIC-KITCHEN-100 ~\cite{corr18-epickitchen,corr20-epickitchen}. For hyperparameter selection, we randomly sample 10 frames from each video per batch. The loss weight \( \lambda \) to 100. Other hyperparameters like temperatures follows the default value used in CLIP ~\cite{icml21-clip}. More pre-training details can be referred to Appendix~\ref{sec:pretrain_details}.
In our experiments, we aim to evaluate the effectiveness of ordered and continuous vision-language representations for robotic control. First, we conduct extensive Language-Conditioned Behavior Cloning (LCBC) experiments in two different simulation environments to validate the importance of ordering and continuity for imitation learning. Second, we assess the utility of the learned representations as reward functions on multiple real-world action videos. The results demonstrate that the ordered and continuous representations enable our method to accurately identify action boundaries and generate dense rewards aligned with the given instructions. Finally, we evaluate the robustness of our method under language perturbations, showcasing its strong generalization capability for application in real-world daily scenarios.

\begin{figure*}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.95\linewidth]{figs/reward.pdf}}\vspace{-3ex}
\caption{Visualization of the normalized learned reward corresponding to different actions. Our representations effectively help capture the correct temporal order of actions in the instruction. For more results, please refer to  Appendix~\ref{sec:app_reward}.}
\label{fig:reward}
\end{center}
\vskip -0.2in
\end{figure*}


\subsection{Simulation Environments}
We perform LCBC experiments in two widely used simulation environments for evaluation: \textbf{Franka Kitchen} ~\cite{corl19-franka,corr20-franka} and \textbf{Metaworld} ~\cite{corl19-metaworld}. As shown in Figure ~\ref{env}, for Franka Kitchen, we evaluate five tasks: sliding a cabinet, opening the left door, opening the microwave, turning on the stove, and switching on the light. For Metaworld, we focus on learning four tasks: hammering a nail, pressing a button, picking and placing a block, and assembling a ring onto a peg. Detailed environment setup can be found at Appendix~\ref{sec:app_env}.

In Frankakitchen, tasks often involve intricate actions that induce large visual changes, and the successful completion of these tasks requires precise, complex actions. As a result, tasks on FrankaKitchen rely more on visual trajectory representations. On the other hand, the visual scene in Metaworld is much simpler. Tasks on Metaworld generally involve direct actions with smaller visual changes, yet they require more sophisticated language understanding to act towards specific task goals. 
% The combination of the two scenarios provides a comprehensive picture of the model's visual representation as well as its semantic understanding.


\subsection{Baselines}
Since our model is initialized with \textbf{CLIP} ~\cite{icml21-clip}, a state-of-the-art image-text representation widely applied in various embodied tasks ~\cite{l4dc22-clipapp2,cvpr22-clipapp3,corl21-clipapp1,nips22-clipapp4}, it is a natural choice to include CLIP as a vanilla baseline for comparison. Our primary baselines are \textbf{LIV} ~\cite{icml23-liv} and \textbf{DecisionNCE} ~\cite{icml24-decisionnce}, as we all use the same model architecture and dataset for pretraining. LIV employs the VIP ~\cite{iclr23-vip} to achieve consistent frame representations and aligns the final frame with instructions using the CLIP loss. DecisionNCE represents instructions as frame transitions and aligns the difference between the initial and final frames with the instructions using the CLIP loss. We also compare against \textbf{R3M} ~\cite{corl22-r3m}, pre-training on Ego4D ~\cite{cvpr22-ego4d}, which combines time contrastive learning ~\cite{icra18-tcn} with LOReL ~\cite{corl21-lorel} to ensure that later frames in the sequence receive higher rewards when aligned with the instruction. 




\subsection{Language-Conditioned Behavior Cloning}
We keep the pre-trained vision-language encoders frozen and feed their output representations into a lightweight MLP to train the LCBC policies. Each task is performed from two camera viewpoints (left and right), with varying numbers ofÂ demonstrations $\left[5, 15, 25\right]$ (\textit{i.e.}, dataset size) for training, and evaluated under three different random seeds. We report the success rate across different environments and dataset sizes, averaged over camera views and seeds. Detailed comparison results can be referred to Appendix~\ref{sec:app_lcbc}.

Figure~\ref{fig:comparison} presents the comparison results, showing that AcTOL significantly outperforms other methods across all dataset sizes in both environments, surpassing the SOTA by 49.0\%/46.4\%/26.3\% and 14.6\%/15.2\%/16.3\% at dataset sizes of 5/15/25, respectively, which demonstrates the effectiveness of our pre-training strategy. Especially in Franka Kitchen, where complex tasks demand higher action continuity, our method demonstrates a clear advantage with limited data, highlighting its superior data efficiency and low-resource generalization capability. Among the baseline models, CLIP consistently shows lower performance, particularly for Franka Kitchen. This indicates that while it excels in image representation, its inability to capture temporal characteristics in videos leads to suboptimal results in control tasks. R3M, LIV, and DecisionNCE perform well in the FrankaKitchen environment, but R3M shows a significant drop in performance in Metaworld. This discrepancy stems from R3M's design, which utilizes a frozen language encoder to generate language features as conditions. These features are then used to optimize the visual encoder by maximizing the reward for temporally later frames. This approach results in a shortcut that prioritizes maximizing temporal visual differences rather than genuinely aligning visual representations with language semantics. This limitation is particularly evident in Franka Kitchen, where large visual changes dominate, allowing R3M to excel despite its lack of true language-visual alignment. In contrast, LIV and DecisionNCE employ CLIP-based losses during pre-training on video data, enabling them to align visual changes with semantics. This capability allows them to maintain solid performance even in Metaworld, where tasks require more refined language comprehension to interpret subtle visual changes. However, both models suffer from rigid assumptions about action semantics during pre-training, resulting in suboptimal alignment between vision and instructions.

Notably, when using 5/15 demonstrations, our method achieves results that are \textit{comparable} to, or even \textit{surpass}, those of other methods using 15/25 demonstrations, illustrating that our approach can utilize expert data more effectively. This is particularly advantageous when collecting expert data is time-consuming and labor-intensive. Moreover, when the Brownian Bridge constraint is not applied, although our method still outperforms the other baselines, its performance decreases noticeably. This indicates that the Brownian Bridge constraint effectively improves the quality of learned representations, leading to better policy optimization in behavior cloning.


\subsection{Language-Conditioned Visual Rewards}
Since our model learns semantically smooth visual representations, the resulting semantic trajectories can also serve as ideal task rewards. Specifically, we define the reward at time step $i$ as $\operatorname{cosine}(\mathbf{v}^i, \mathbf{l})$, which reflects the distance between the current state and the language goal. Previous works ~\cite{icml23-liv,icml24-decisionnce} have primarily tested their rewards on single-action video clips. To increase task complexity, we selected three video clips, each containing two consecutive actions, to better evaluate whether the model accurately understands action semantics. We present more reward results in Appendix~\ref{sec:app_reward}. Figure ~\ref{fig:reward} shows the actions, the instructions given and the corresponding reward curves. Figure ~\ref{fig:reward}(a) uses a video from EPIC-KITCHEN-100 to evaluate the effectiveness of in-distribution rewards. Our method produces an ideal reward curve, starting at a low point and reaching a peak as the action ``open cupboard'' completes, before declining as subsequent actions begin. In contrast, we observe that the rewards generated by R3M and DecisionNCE continue to rise even after the ``open cupboard'' action has finished. This indicates that their training methodologies fail to effectively distinguish between actions that align with the instruction and those that do not. Figure ~\ref{fig:reward}(b) and (c) evaluate the reward generation capabilities in real-world scenarios using two videos from ~\cite{rss22-robotvideo}. These videos involve humans and robots consecutively performing pairs of opposite actions. It is evident that only our method is capable of generating the correct reward curve, accurately identifying the start and end of the actions specified by the instruction, and producing opposite rewards for contrasting actions. This demonstrates that our method, by learning ordered and continuous vision-language representations, inherently captures the trends of semantic changes in actions. It effectively aligns visual trajectories with the corresponding instructions, highlighting its significant potential as a language-conditioned visual reward model.


\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.1\columnwidth]{figs/traj.pdf}}
\caption{Visualization of visual trajectory representations.}
\label{fig:traj}
\end{center}
\vskip -0.2in
\end{figure}


\subsection{Visualization of Visual Representation Trajectory}
To demonstrate the smoothness of the representations, we select three distinct language instructions from the EPIC-KITCHEN-100 dataset, each corresponding to 10 action videos. We then visualize the visual representations learned by our method and those by CLIP using t-SNE ~\cite{tsne}, as shown in Figure ~\ref{fig:traj}. The representations produced by CLIP show clear separability between different language instructions, and the trajectories of individual videos maintain a certain degree of temporal consistency. However, the transitions between consecutive frames are not smooth, reflecting a flaw due to the absence of training on video data.
In contrast, our method significantly enhances the ordering and continuity of video feature trajectories while preserving the discriminative power of CLIP for distinguishing actions associated with different instructions. This improvement stems from training directly on CLIP's weights and optimizing the temporal consistency within each video. As a result, our method not only achieves smoother representations but also retains the strong alignment between visual features and language semantics inherent in CLIP's original design. This balance between temporal coherence and semantic distinctiveness highlights the effectiveness of our approach in refining embodied representations for tasks requiring nuanced understanding of multimodal alignment and temporal dynamics. 

\subsection{Robustness Study under Linguistic Perturbations} \label{sec:robustness}
In the EPIC-KITCHEN-100 dataset, textual annotations are often concise, such as ``open cupboard''. In the default setting of LCBC, we employ similarly structured simple instructions. In this experiment, to validate the robustness of the representations our method learns in real-world scenarios, we introduce minor modifications to the language instructions. Specifically, we transform the original instruction ``\texttt{\{action\}}'' into two more conversational styles, \textit{i.e.}, ``\texttt{Please \{action\} for me.}'' and ``\texttt{Help me \{action\}.}''. We then evaluate the imitation learning performance conditioned on these modified instructions in the Franka Kitchen environment. For comparison, we select LIV and DecisionNCE, which are also pre-trained on EPIC-KITCHEN-100. As shown in Table ~\ref{tab:lang_perturb}, The success rates of LIV and DecisionNCE dropped by 11.6\% and 2.1\%, respectively, whereas our method maintained a success rate comparable to that before the language perturbation. This result demonstrates the robustness of our learned representations, which generalize more effectively to real-world scenarios.


\begin{table}
\caption{Success rate fluctuation under linguistic perturbations on Franka Kitchen. \vspace{1ex}}
\label{tab:lang_perturb}
\centering
\resizebox{\linewidth}{!}{ 
\begin{tabular}{lccc} 
\toprule
$\operatorname{Task}$              & $\operatorname{LIV}$ & $\operatorname{DecisionNCE}$ & $\operatorname{AcTOL}$ \\ 
\hline
\texttt{Slide Cabinet}      & $-29.0 \pm 3.0$  & $-4.5 \pm 3.5$ & \cellcolor{lightgray!50}$\mathbf{1.0} \pm 2.0$  \\
\texttt{Open Left Door}     & $-3.5 \pm 0.5$   & $-1.5 \pm 1.5$ & \cellcolor{lightgray!50}$\mathbf{1.0} \pm 2.0$  \\
\texttt{Open Microwave}     & $-4.5 \pm 0.5$   & \cellcolor{lightgray!50}$\mathbf{3.0} \pm 2.0$ & $-2.5 \pm 1.5$  \\
\texttt{Turn on stove}      & $-8.5 \pm 0.5$   & $-6.5 \pm 1.5$ & \cellcolor{lightgray!50}$\mathbf{-0.5} \pm 1.5$  \\
\texttt{Switch on light}    & $-12.5 \pm 0.5$  & $-1.0 \pm 3.0$ & \cellcolor{lightgray!50}$\mathbf{1.0} \pm 1.0$  \\
$\operatorname{Average}$   & $-11.6 \pm 0.6$ & $-2.1 \pm 0.3$ & \cellcolor{lightgray!50}$\mathbf{0.2} \pm 1.6$ \\ 
\bottomrule
\end{tabular}
}
\end{table}



