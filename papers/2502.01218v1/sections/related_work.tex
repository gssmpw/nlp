\section{Related Work}
% \paragraph{Embodied Representation Pre-training.}
Given the success of large-scale pre-training in the vision  and language research communities ~\cite{nips20-gpt3,nips23-llava}, many studies have attempted to extend this paradigm to the field of robotics. Some work leverage massive robotic trajectory data ~\cite{corr23-rtx} for pre-training, aiming to establish a unified mapping from the perception space to the action space ~\cite{corl23-rt2,corr24-gr2}.  However, collecting large amounts of high-quality robot trajectory data is extremely costly and time-consuming. Consequently, many studies have begun to explore the use of large-scale, readily available, out-of-domain human action video data to learn generalizable representations that can be transferred to robotic tasks ~\cite{icra18-tcn,iclr23-vip,corl22-mvp,corl22-r3m,rss23-voltron,icml23-liv,nips23-vc1,DBLP:journals/corr/abs-2410-11758,rss24-mpi,icml24-decisionnce}. Among these, TCN ~\cite{icra18-tcn}, VIP ~\cite{iclr23-vip}, MVP ~\cite{corl22-mvp}, and VC-1 ~\cite{nips23-vc1} focus solely on studying unimodal visual representations, limiting their performance when understanding language instructions is required. R3M ~\cite{corl22-r3m} employs language and reward models to shape progressive visual representations, while Voltron ~\cite{rss23-voltron} and MPI ~\cite{rss24-mpi} model the transition from the current state to the goal state conditioned on language. However, during training, these approaches freeze the language encoder, using it only to aid in the training of visual representations. As a result, they do not effectively achieve multi-modal representation learning.

Recently, LIV ~\cite{icml23-liv} and DecisionNCE ~\cite{icml24-decisionnce} have attempted to leverage CLIP ~\cite{icml21-clip}, a state-of-the-art vision-language model, to train embodied multi-modal representations. LIV treats language instructions as the goals of video actions and aligns the final frame of a video with the corresponding language description. DecisionNCE, on the other hand, views language as the transition from the initial state to the final state, aligning the difference between the representations of the first and last frames with the language. Their methods rely on goal-directed semantic alignment, which tends to produce suboptimal results under the noise present in real-world videos. In contrast, our approach avoids rigid assumptions, theoretically ensuring that semantic alignment follows the intrinsic temporal continuity and ordering of the video, resulting in more robust and generalizable vision-language representations. 

% While their methods have achieved some success, they assume that the video always starts the action from the first frame and keeps approaching the target until the last frame ends the action. However, human action videos often contain noise, with ambiguous start and end points, and the progression of actions may not incrementally approach the goal, which limits the accuracy of their action semantic modeling. In this work, we propose a more flexible and generalizable language alignment method, based on the continuity of actions, treating the semantics of actions as a smooth and continuous process evolving along the instruction. In the experiments, we used the multimodal representations of <modelour> to generate language-conditioned visual rewards. Although <modelour> does not make assumptions like LIV and DecisionNCE, it implicitly and more accurately identifies the completion of actions. This suggests that our method achieves more precise language alignment, thereby enhancing the quality of multimodal representations.