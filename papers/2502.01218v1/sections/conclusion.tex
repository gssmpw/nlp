\section{Conclusion}
We present Action Temporal Coherence Learning (AcTOL) as a promising vision-language pre-training solution for generalizable embodied agents. By learning action consistency from a large corpus of human action videos, AcTOL theoretically ensures the ordering and continuity of vision-language representations, as well as robustness to language perturbations. Extensive experiments across various environments demonstrate that AcTOL effectively generalizes to complex robotic manipulation tasks. Due to hardware limitations, our evaluations are mainly conducted in simulation environments, with real-world deployment left for future work.

% Future work will focus on scaling up pre-training by leveraging larger, more diverse datasets and training models with higher parameter capacities. Additionally, we plan to deploy AcTOL on real robots to further assess and validate its generalization performance in practical, real-world settings.