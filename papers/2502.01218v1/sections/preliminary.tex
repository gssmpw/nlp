\section{Preliminaries}
\label{sec:preliminaries}
We first set up notations and mathematically formulate tasks.

\noindent\textbf{Language-Conditioned Imitation Learning (LC-IL)}. The task of LC-IL aims to train an agent to mimic expert behaviors from a given demonstration set $\mathcal{D}_d = \{(\mathbf{\tau}_i,l_i)\}_{i=1}^N$, where $l_i \in \mathcal{L} $ represents a task-specific language instruction. Each trajectory $\mathbf{\tau}_i\in\mathcal{T}$ consists of a sequence of state-action pairs $\mathbf{\tau}_i = \{(\mathbf{s}_j, \mathbf{a}_j)\}_{j=1}^T$ of the horizon length $T$. In robot manipulation tasks, action $\mathbf{a}_j\in\mathcal{A}$ corresponds to the control commands executed by the agent and state $\mathbf{s}_j = [\mathbf{p}_j; \mathbf{v}_j] \in\mathcal{S}$ records proprioceptive data $\mathbf{p}_j$ (\textit{e.g.,} joint positions, velocities) and visual inputs $\mathbf{o}_j\in\mathcal{O}$ (\textit{e.g.,} camera images) at the time step $j$. The objective of LC-IL is to find an optimal language-conditioned policy $\pi^*(\mathbf{a}|\mathbf{s},l): \mathcal{S}\times\mathcal{L}\mapsto\mathcal{A}$ via solving the supervised optimization as follows,
\begin{equation}\nonumber
    \pi^* \in \arg\min_{\pi} \mathbb{E}_{(\tau_i, l_i)\sim \mathcal{T}} \left[ \frac{1}{T} \sum_{(\mathbf{s}_j, \mathbf{a}_j) \sim \tau_i} \ell(\pi(\hat{\mathbf{a}}_j, \mathbf{s}_j|l_i),  \mathbf{a}_j)\right],
\end{equation}
where \(\ell(\cdot, \cdot)\) is a task-specific loss, such as mean squared error or cross-entropy. Training the policy \(\pi_\theta\) in an end-to-end fashion may require \textit{hundreds} of high-quality expert demonstrations to converge, primarily due to the high variance of visual inputs $\mathbf{o}$ and language instructions $l$.

% We study the problem of Language-Conditioned Imitation Learning ~\cite{rss21-gcil}, where the goal is to train an agent to perform tasks by conditioning its policy on both the state of the environment and language instruction. Formally, let \(\mathcal{O}\) be the observation space, \(\mathcal{A}\) the action space, and \(\mathcal{L}\) the language instruction space. The observation space \(\mathcal{O}\) typically includes visual or sensor data, such as images, that represent the partial observation of state \(\mathcal{S}\). The objective is to learn a policy \(\pi_\theta : \mathcal{O} \times \mathcal{L} \to \mathcal{A}\), parameterized by \(\theta\), that maps an observation \(o \in \mathcal{O}\) and a language instruction \(L \in \mathcal{L}\) to an action \(a \in \mathcal{A}\). We assume access to a dataset of expert demonstrations \(\mathcal{D}_{\operatorname{demo}} = \{(\{o_k^i, a_k^i\}_{i=1}^T, L_k)\}_{k=1}^N\), where each sample consists of a $T$-step observation-action trajectory and a corresponding language instruction \(L_k \in \mathcal{L}\). The goal is to train the policy \(\pi_\theta\) by minimizing the following loss function:
% \[
% \mathcal{L}(\theta) = \frac{1}{N} \sum_{k=1}^N \sum_{i=1}^T \ell(a_k^i, \pi_\theta(o_k^i, L_k)),
% \]
% where \(\ell(\cdot, \cdot)\) is a task-specific loss function, such as mean squared error or cross-entropy. 
\begin{table}
\centering
\caption{Comparison of different component designs in time contrast learning across mainstream vision-language pre-training. \vspace{1ex}
% The goal frame $o_g$ is typically set as the last frame $o_{T}$.
 }
\label{tab:comp}
\Large
\resizebox{\linewidth}{!}{ 
\begin{tabular}{llll}
\toprule
$\operatorname{Method}$      & \textcolor{black}{$\mathcal{P}(\mathcal{O}_{i})$}  & \textcolor{black}{$\mathcal{N}(\mathcal{O}_{i})$} & $\mathfrak{R}(\mathbf{v},\mathbf{l}_i)$  \\ \hline
$\operatorname{R3M}$         & $(o_0, o_{j>i})$      &  $(o_0,o_i,o_j^{\notin O_i})$   & $\operatorname{reward}(\mathbf{v},\mathbf{l}_i)$   \\    
$\operatorname{LIV}$         & $(o_T)$    &  $(o_T^{\notin O_i})$    & $\operatorname{cos}(\mathbf{v},\mathbf{l}_i)$  \\    
$\operatorname{DecisionNCE}$ & $(o_i,o_{j>i})$     &     $(o_i^{\notin O_i},o_{j>i}^{\notin O_i})$  & $\operatorname{cos}(\mathbf{v}_j-\mathbf{v}_i, \mathbf{l}_i)$  \\          
$\operatorname{AcTOL}$        & $(o_i,o_{j \in [T] \setminus \{i\}})$ & $(o_i,o_k: d_{i, k}>d_{i, j})$  & $-\Vert \operatorname{cos}(\mathbf{v}_i, \mathbf{l}_i)-\operatorname{cos}(\mathbf{v}_j, \mathbf{l}_i) \Vert_2 $     \\  \bottomrule                                                              
\end{tabular}
}
\end{table}

\paragraph{Vision-language Pre-training.}  Address such scalability issues can be achieved by leveraging large-scale, easily accessible human action video datasets $\mathcal{D}_p = \{(\mathcal{O}_i, l_i)\}_{i=1}^M$ \cite{corr18-epickitchen,cvpr22-ego4d}, where $\mathcal{O}_i=\{o_j\}_{j=1}^T$ represents a video clip with $T$ frames and $l_i$ the corresponding description. Pretraining on such datasets enables policies to rapidly learn visual-language correspondences with minimal expert demonstrations. Mainstream pretraining methods employ time contrastive learning \cite{icra18-tcn} to fine-tune a visual encoder $\mathcal{\phi}$ and a text encoder $\mathcal{\varphi}$, which project frames and descriptions into a shared $d$-dimensional embedding space, \textit{i.e.}, $\mathbf{v}_j = \phi(o_j)\in\mathbb{R}^d$ and $\mathbf{l}_i = \varphi(l_i)\in\mathbb{R}^d$. To provide a unified perspective on various pretraining approaches, we formulate them within the objective $\mathcal{L}_{\operatorname{tNCE}}(\phi, \varphi)$: \vspace{-2ex}
\begin{align}\nonumber\small
\mathcal{L}_{\operatorname{tNCE}}&=
-\mathbb{E}_{\substack{\scriptstyle o^+\sim\textcolor{black}{\mathcal{P}(\mathcal{O}_i)}}}
    \log  
    \frac{
        \exp(\mathfrak{R}(\mathbf{v}^+, \mathbf{l}_i))
    }{
        \mathbb{E}_{\scriptstyle o^- \sim \textcolor{black}{\mathcal{N}(\mathcal{O}_i)}}
        \exp(\mathfrak{R}(\mathbf{v}^-, \mathbf{l}_i))
    },
\end{align}

% \begin{align}\nonumber\small
% \mathcal{L}_{\operatorname{tNCE}}&=
% -\mathbb{E}_{\substack{\scriptstyle o\sim O_i \\ \scriptstyle o^+\sim\textcolor{black}{\mathcal{P}(o)}}}
%     \log  
%     \frac{
%         \exp(\mathfrak{R}(\mathbf{v}^+, \mathbf{v}, \mathbf{l}_i))
%     }{
%         \mathbb{E}_{\scriptstyle o^- \sim \textcolor{black}{\mathcal{N}(o)}}
%         \exp(\mathfrak{R}(\mathbf{v}, \mathbf{v}^-, \mathbf{l}_i))
%     },\vspace{-2ex}
% \end{align}
% where $\mathbf{v} = \phi(o)$, and 
where $\mathbf{v}^{+/-} = \phi(o^{+/-})$. Different pretraining strategies differ in their selection of (1) the positive frame set $\mathcal{P}(\mathcal{O}_i)$, (2) negative frame set $\mathcal{N}(\mathcal{O}_i)$; and (3) the semantic alignment scoring function $\mathfrak{R}(\mathbf{v}, \mathbf{l}_i)$ measuring the gap of VL similarities as detailed in Table \ref{tab:comp}. 

\noindent\textbf{Discussion.} As motivated by goal-conditioned RL \cite{nips17-her}, current approaches \textit{explicitly} select future frames (\textit{e.g.}, DecisionNCE) or the last frame (\textit{e.g.}, LIV) as the goal within the positive set, enforcing their visual embedding to align with the semantics. Likewise, the scoring functions $\mathfrak{R}$ are often designed to maximize this transition direction. However, the pretraining action videos are \textit{noisy} as actions may terminate early or include irrelevant subsequent actions, which may mislead the encoders and result in inaccurate vision-language association. As detecting precise action boundaries is non-trivial, we argue for a more flexible approach that leverages \textit{intrinsic} characteristics of actions to guide pretraining.



% we first pre-train a visual encoder \(\mathcal{\phi}: \mathcal{O} \to \mathbb{R}^d\) and a text encoder \(\mathcal{\varphi}: \mathcal{L} \to \mathbb{R}^d\) to learn mappings from the observation and the language instruction space to $d-$dimensional feature spaces. This pre-training can be done using large, less-expensive data without action annotation, such as human action videos . Then, with the frozen learned features \(\boldsymbol{v}\) and \(\boldsymbol{l}\) as input, we can only fine-tune a simple Multi-Layer Perceptron (MLP) with a few demonstrations to learn the map from the feature space \(\mathbb{R}^d \times \mathbb{R}^d\) to the action space \(\mathcal{A}\). Since both the observation space \(\mathcal{O}\) and the action space \(\mathcal{A}\) are continuous and ordered over time, we expect the representations learned through pre-training to also exhibit continuity and orderliness. This property in the representations allows for better learning of the continuous mapping between observations and actions. This property offers three significant benefits: First, the orderliness of the representation ensures that different states of the task, such as the start and end of an action, can be better captured and distinguished. Second, the continuity of the representation allows it to evolve smoothly as the task progresses, enabling the model to output stable actions based on the current state. Finally, we can demonstrate that even under small perturbations to the language instruction, these properties ensure the robustness of the learned representation. This robustness is crucial for maintaining performance in real-world scenarios where language instructions might contain minor ambiguities or variations.





% We consider a partially observable Markov Decision Process (POMDP) with language conditions, which models the interaction between an agent and an environment where observations are incomplete and actions are guided by natural language instructions. Formally, a POMDP is defined as a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{T}, \mathcal{R}, \mathcal{Z}, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space available to the agent. $\mathcal{O}$ is the observation space, which provides partial information about the environment. $\mathcal{T}(s' \mid s, a)$ is the state transition function. $\mathcal{R}(s, a)$ is the reward function. $\mathcal{Z}(o \mid s, a)$ is the observation function. $\gamma \in [0, 1)$ is the discount factor.

% To incorporate language instructions, we introduce a task description $L$, which specifies the agent's goal in natural language. The task description conditions the agent's policy $\pi(a \mid o, L)$, where $o$ is the agent's current observation. The agent aims to maximize the expected cumulative reward while adhering to the task described by $L$.

% Further, we assume the availability of a large-scale human action video dataset including $N$ video-instruction pairs, $\{(\{o_k^i\}_{i=1}^{t_k}, L_k)\}_{k=1}^N$, where each pair representing an action video with $t_k$ frames and its corresponding language description $L_k$. We pre-train the visual and language encoders on this dataset, with the visual features $\boldsymbol{v} = \operatorname{Enc}_v(o)$ and the language features $\boldsymbol{l} = \operatorname{Enc}_l(L)$. These pre-trained representations are then frozen and applied to train the policy $\pi$ in the aforementioned decision-making process, enabling the agent to better interpret and act upon language-conditioned tasks.