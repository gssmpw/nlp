\section{Introduction}
The long-term vision for embodied intelligence \cite{DBLP:conf/nips/MuZHWDJWDQL23,DBLP:journals/corr/abs-2407-06886} is to create systems that seamlessly perceive and interact with the world around them. Achieving this requires agents that integrate vision and language to understand their surroundings, interpret human instructions, and autonomously plan actions for complex tasks. Current end-to-end approaches achieve policy learning through direct vision-language-action mapping ~\cite{corl23-rt2,corr24-gr2,corr24-openvla}. However, the inherent unpredictability of physical environments, including unseen scenarios and dynamic object interactions, constrains these solutions by requiring massive, high-quality robotic trajectories with action annotations, which are \textit{costly} to collect. To mitigate this, recent research has leveraged large-scale, readily available egocentric human action videos ~\cite{DBLP:conf/iccv/GoyalKMMWKHFYMH17,corr18-epickitchen,cvpr22-ego4d} for \textit{pre-training}. Although these out-of-domain videos often lack low-level action details and contain noise, their diverse human-object interactions and task instructions provide valuable prior knowledge. This enables the pre-trained representations to be more effectively transferred to novel tasks with fewer demonstrations, reducing reliance on large-scale robotic datasets while preserving strong generalization capabilities.

A promising approach for vision-language pre-training from human action videos leverages the concept of \textit{time contrastive learning} ~\cite{icra18-tcn} to capture temporally consistent visual representations. In this framework, language serves as the guiding goal, with semantic alignment between the language and chronologically later frames in the video ~\cite{corl22-r3m,icml23-liv,icml24-decisionnce}. However, this semantic alignment approach relies on a rigid assumption that action videos adhere to a specific principle: \textit{actions progressively approach the target instruction from the initial frame to the final one}. Such assumption can be easily violated in real-world human action videos, which are typically annotated at a coarse-grained level and riddled with noise. The start and end points of actions are often ambiguous, and the progression may not consistently move toward the goal but instead exhibit fluctuations and detours. As a result, these methods struggle with misleading semantic alignment, leading to inaccurate vision-language relationships.

Given the challenges outlined above, a more natural and flexible pre-training strategy without rigid assumptions is needed to enhance vision-language representations for better policy learning. Building solely on the intrinsic temporal consistency of human action videos, we argue that the \textit{ordering} and \textit{continuity} of pre-trained vision-language representations play a crucial role in ensuring the effectiveness of policy learning. Ordering refers to the need for visual features to align with the underlying action logic required by the language instruction. For instance, as the task progresses, visual representations closer to the completion of the action should exhibit stronger alignment with the language instruction. This ensures that each step in the sequence is meaningfully associated with the corresponding instruction, enabling the model to effectively capture the dynamic progression of the task. Continuity, on the other hand, emphasizes that both visual features and their alignment with the language should evolve smoothly over time, with gradual transitions rather than abrupt changes. This is crucial because actions in the real world are not discrete but unfold continuously in time. Moreover, the alignment between visual and instruction should also be fluid, ensuring that as the action progresses, the visual representations consistently align with the target language instruction.


\begin{figure*}[ht]
\begin{center}
\centerline{\includegraphics[width=\linewidth]{figs/actol_arch.pdf}}\vspace{-2ex}
\caption{Comparison of existing \textit{goal-reaching} pre-training strategies and the proposed AcTOL approach. The learned multi-modal representations can be effectively transferred to downstream language-conditioned robot manipulation tasks, exhibiting robustness to diverse instruction and linguistic variations.\vspace{-2ex}}
\label{fig:arch}
\end{center}\vspace{-3ex}
\end{figure*}

To address the aforementioned issues, we propose \underline{Ac}tion \underline{T}emporal C\underline{o}herence \underline{L}earning (AcTOL), a novel approach designed to implicitly capture the ordering and continuity of video actions without relying on rigid assumptions, while providing strong theoretical guarantees. Unlike previous approaches that focus on goal-directed semantic alignment, AcTOL introduces a Vision-Language Ordering (VLO) loss. This loss leverages the intrinsic temporal coherence of videos, contrasting frames against each other based on their relative temporal distance, theoretically ensuring that the semantic alignment between frames reflects their temporal ordering and continuity throughout the entire sequence. However, the VLO loss does not explicitly enforce the continuity of the visual features themselves, and under conditions with variations in frame content and noise, it can lead to suboptimal local consistency of the visual features. To address this, AcTOL introduces a Brownian bridge constraint over the video, treating video frames as a Brownian bridge process. This approach imposes a structured, continuous flow on the visual representations, ensuring that the model learns more consistent and stable intermediate states, further enhancing the continuity of the visual representations and improving the stability of their alignment with language instruction. 
Further theoretical analysis suggests that these properties also contribute to the model's resilience to language perturbations, a crucial trait for real-world applications. 
To validate the generalization ability of AcTOL on embodied agents, we conducted extensive language-conditioned imitation learning experiments in two simulation environments. The results show that AcTOL outperforms previous methods up to $49.0\%$ with $5$ (limited number of) expert demonstrations. Additionally, we performed an analysis of language-conditioned visual rewards on several real-world action videos. The findings reveal that the ordering and continuity of AcTOL enable it to serve as a promising reward function, generating dense rewards that align well with the given instructions.

\vspace{-1ex}

