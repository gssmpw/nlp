\section{Our Approach}
We introduce an \underline{ac}tion \underline{t}emporal c\underline{o}herence \underline{l}earning (AcTOL) to capture two temporal properties of video actions: \textit{ordering} and \textit{continuity}. \textit{Ordering} was ensured in the vision-language ordering loss (Section \ref{sec:order}), where the semantic difference between frames reflects their temporal distance, with closer frames exhibiting smaller differences than those further apart. \textit{Continuity} requires smooth visual transitions between adjacent frames, avoiding abrupt changes and high variance. To achieve this, we model sampled frame intervals as a Brownian bridge process (Section \ref{sec:bb}), penalizing deviations from the expected trajectories. Different from prior works that relies on setting explicit goal frames, the proposed approach \textit{implictly} explore the global and local structure of actions without imposing rigid constraints.

% We introduce Action Coherence Learning (ACL) to capture the fundamental temporal properties of video actions: order and continuity, which are crucial for accurate action understanding. To achieve this, we design a training framework that implicitly captures both global and local temporal structures through contrastive and regularization-based objectives.

% \noindent\textbf{Order.} We enforce order via the vision-language ordering loss (Section \ref{sec:order}), which ensures that the semantic difference between frames is consistent with their temporal distance. Specifically, frames that are temporally closer should exhibit smaller semantic differences compared to those further apart, maintaining the global structure of the action sequence.

% \noindent\textbf{Continuity.} To ensure smooth transitions and avoid abrupt changes, we introduce a Brownian Bridge constraint (Section \ref{sec:bb}), which models sampled frame intervals as stochastic processes with fixed endpoints. This constraint penalizes deviations from expected smooth trajectories, thereby improving local consistency.

% Unlike traditional methods that rely on explicit goal frames—often susceptible to inaccuracies—our approach leverages the inherent temporal properties of actions to learn a more robust and flexible representation, without imposing rigid constraints.



% To obtain visual representations of semantically smooth transitions, we first define the \textit{\textbf{semantic gap}} on the triplet \((o^i, o^j, L)\) as $\|\operatorname{sim}(\mathbf{v}^i, \boldsymbol{l}) - \operatorname{sim}(\mathbf{v}^j, \boldsymbol{l})\|_2$. 
% This formula calculates the \(L_2\)-norm of the difference in similarity between two frames and the language, representing the semantic distance between the two frames. We aim for the semantic gap between any adjacent frames $\{o^i, o^{i+1}\}_{i=1}^{N-1}$ to remain as consistent as possible, ensuring a smooth semantic transition from the current frame to the next. To this end, we propose a segment-level semantic gap contrastive learning method that contrasts semantic gaps across video segments of varying lengths to optimize semantic smoothness. Furthermore, to stabilize the training of visual representations, we model the video as a Brownian bridge process and incorporate this process to constrain the contrastive learning framework.
% For a batch of \( B \) video-instruction samples \(\{(\{o_k^i\}_{i=1}^t, L_k)\}_{k=1}^B\), each sample consists of \( t \) frames randomly sampled from the video. Since our contrastive process is applied independently to each video, we omit the index \( k \) for simplicity in the following discussion. 
\subsection{Visual-Language Ordering}\label{sec:order}

To capture the temporal coherence of video actions, we first propose a vision-language ordering (VLO) loss that ensures the semantic alignment between frames reflects their temporal order. Consider an anchor frame \( o_i\in\mathcal{O}\) with an index  \( n(i) \) corresponding to its position in the original video. For any given frame pair \( (o_i, o_j) \), we first define the semantic alignment score $\mathfrak{R}$ to quantify differences in their VL similarities \textit{w.r.t} a language description \( l \) as:
\begin{equation}
\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) = -\Vert \operatorname{sim}(\mathbf{v}_i, \mathbf{l}) - \operatorname{sim}(\mathbf{v}_j, \mathbf{l}) \Vert_2,
\end{equation}
where $\mathbf{v}_i=\phi(o_i)$, $\mathbf{l}=\varphi(l)$. The function \( \operatorname{sim}(\cdot, \cdot) \) computes the VL similarity using cosine similarity. To ensure the proposed $\mathfrak{R}$ adhere to the temporal ordering of frames, we construct a negative set $\mathcal{N}_{i, j}$ by selecting $o_k\in O$ correspond to frames that are temporally more \textit{distant} than the positive pair $(o_i, o_j)$:
\[
\mathcal{N}_{i, j} = \{ o_k \mid k \neq i, \ |n(i) - n(k)| \geq |n(i) - n(j)| \},
\]
This formulation allows us to reformulate $\mathcal{L}_{\operatorname{tNCE}}$ by enforcing that the VL similarity difference between frames $i$ and $j$ should be smaller than that between frame $i$ and any negative frame $k$ within the video $\mathcal{O}$:
\begin{equation}\small\nonumber
    \mathcal{L}_{\operatorname{VLO}}=
-\mathbb{E}_{(o_i, o_j)\sim O}
 \log \frac{\exp\left( \mathfrak{R}(\mathbf{v}_i,\mathbf{v}_j,\mathbf{l}) \right)}{\sum_{o_k \in \mathcal{N}_{i, j}} \exp \left(\mathfrak{R}(\mathbf{v}_i,\mathbf{v}_k,\mathbf{l}) \right)}.
\end{equation}
Notably, our VLO loss does not strictly require $o_j$ to be from future timestep for goal-reaching. Instead, we leverage the inherent temporal dynamics in videos, allowing the model to learn the natural ordering in an unsupervised manner with detailed analysis as follows.


\noindent\textbf{Theoretical Analysis.}  Ordering and sorting properties are well-established in self-supervised learning \cite{DBLP:conf/iccv/ShvetsovaPKSK23,DBLP:conf/iccv/HuSLRSS21,nips23-rnc}. Building upon these insights, we formalize the concept of vision-language ordering below.
\begin{definition}[Vision-Language Ordering]
\label{def:delta-ordered}
Let $\{o_i\}_{i\in[T]}$ be a sequence of video frames and $l$ the corresponding language description. The representations of the frames are said to satisfy the VLO property for any $0<\delta<1$ if $\forall i \in[T]$, and distinct frames $j, k \in[T] \backslash\{i\}$, the following conditions hold:
\[
\left\{
\begin{array}{ll}
\mathfrak{R}_{i,j,l} > \mathfrak{R}_{i,k,l} + 1/{\delta}, & \text{if } d_{i,j} < d_{i,k}, \\
\left|\mathfrak{R}_{i,j,l} - \mathfrak{R}_{i,k,l}\right| < \delta, & \text{if } d_{i,j} = d_{i,k}, \\
\mathfrak{R}_{i,j,l} < \mathfrak{R}_{i,k,l} - 1/{\delta}, & \text{if } d_{i,j} > d_{i,k},
\end{array}
\right.
\]
where $\mathfrak{R}_{i,j,l}$ denotes  $\mathfrak{R}\left(\mathbf{v}_i,\mathbf{v}_j,\mathbf{l}\right)$ and the temporal distance between frames defined as $|n(i) - n(j)|$ as $d_{i,j}$. 
\end{definition}
\textbf{Implications of the VLO Property.} The VLO property enforces a structured representation of video frames, ensuring that temporally adjacent frames have consistent and predictable semantic differences. When two frames have equal temporal distances from an anchor frame, their semantic gaps should be similar, fostering smooth transitions. In contrast, frames that are farther apart should exhibit larger semantic gaps, thus preserving the chronological order.

% {(TODO)}This property ensures that frames closer in time have consistent and predictable semantic differences in the feature space. Specifically, for frames \(o_j\) and \(o_k\) with equal temporal distances to the anchor \(o_i\) (\(d_{i,j} = d_{i,k}\)), the difference in their semantic gaps \(|\text{sim}_{i,j,l} - \text{sim}_{i,k,l}|\) is bounded by \(\delta\). This promotes smooth transitions in semantics for frames with similar temporal distances. On the other hand, for frames with different temporal distances (\(d_{i,j} < d_{i,k}\)), the semantic gap satisfies \(\text{sim}_{i,j,l} > \text{sim}_{i,k,l} + \frac{1}{\delta}\), encouraging a clear distinction as the temporal separation increases. The condition \(\frac{1}{\delta} > \delta\) ensures that frames with different temporal distances always show larger semantic differences compared to those with the same temporal distance, balancing temporal consistency with discriminative power.


To formalize the temporal ordering constraints, we define the unique \textit{sorted} set of frame distances from frame $i$ as $\{D_{i,1} < D_{i,2} < \cdots < D_{i,M}\}$, where each $D_{i,m}, m \in [M_i]$ is obtained by sorting the set $\{d_{i,j} \mid j \in [T] \setminus \{i\}\}$. Additionally, we define the count of frames at each distance level as:
\begin{equation}
    n_{i,m} := |\{j \mid d_{i,j} = D_{i,m}, \, j \in [T] \setminus \{i\}\}|, 
\end{equation}
which denotes the number of frames whose temporal distance from frame $i$ equals $D_{i,m}$. The VLO property is satisfied when the proposed $\mathcal{L}_{\mathrm{VLO}}$ approaches its theoretical lower bound, which is given by:
\begin{equation}
\mathcal{L}^*:=\frac{1}{T(T-1)} \sum_{i=1}^{T} \sum_{m=1}^{M_i} n_{i, m} \log n_{i, m}.
\end{equation}
This bound characterizes the optimal alignment of VL similarities, ensuring that the learned representations preserve the inherent temporal structure within the video sequence, as guaranteed by the following theorem.

\begin{theorem}
\label{thm:delta-ordered}
$\mathcal{L}^*$ is a tight lower bound of $\mathcal{L}_{\mathrm{VLO}}$, i.e., $\mathcal{L}_{\mathrm{VLO}} \geq \mathcal{L}^*$, and for any $\epsilon > 0$, there exists feature embeddings such that $\mathcal{L}_{\mathrm{VLO}} < \mathcal{L}^* + \epsilon$. Furthermore, for any $0 < \delta < 1$, there exist $\epsilon > 0$such that if $\mathcal{L}_{\mathrm{VLO}} < \mathcal{L}^* + \epsilon$, the learned representations satisfy the VLO property.
\end{theorem}
\begin{proof}
Please refer to Appendix \ref{sec:proof_order}.
\end{proof}


% This property ensures that frames closer in time have consistent and predictable semantic differences in the feature space. Specifically, for frames \(o_j\) and \(o_k\) with equal temporal distances to the anchor \(o_i\) (\(d_{i,j} = d_{i,k}\)), the difference in their semantic gaps \(|\text{sim}_{i,j,l} - \text{sim}_{i,k,l}|\) is bounded by \(\delta\). This promotes smooth transitions in semantics for frames with similar temporal distances. On the other hand, for frames with different temporal distances (\(d_{i,j} < d_{i,k}\)), the semantic gap satisfies \(\text{sim}_{i,j,l} > \text{sim}_{i,k,l} + \frac{1}{\delta}\), encouraging a clear distinction as the temporal separation increases. The condition \(\frac{1}{\delta} > \delta\) ensures that frames with different temporal distances always show larger semantic differences compared to those with the same temporal distance, balancing temporal consistency with discriminative power.
% Backup: ZZZ's definition below
%

% \begin{definition}[Vision-Language Ordering]
% \label{def:delta-ordered}
% For any $0<\delta<1$, the visual representations $\left\{\mathbf{v}^i\right\}_{i \in[t]}$ are $\delta$-ordered if $\forall i \in[t], j, k \in[t] \backslash\{i\}$,

% \[
% \left\{
% \begin{array}{rr}
% \operatorname{sim}_{i,j,l} > \operatorname{sim}_{i,k,l} + \frac{1}{\delta} & \text{if } d_{i,j} < d_{i,k}, \\
% \left|\operatorname{sim}_{i,j,l} - \operatorname{sim}_{i,k,l}\right| < \delta & \text{if } d_{i,j} = d_{i,k}, \\
% \operatorname{sim}_{i,j,l} < \operatorname{sim}_{i,k,l} - \frac{1}{\delta} & \text{if } d_{i,j} > d_{i,k}.
% \end{array}
% \right.
% \]
% \end{definition}

% For simplicity, we denote the triplet similarity measure $\operatorname{sim}\left(\mathbf{v}^i,\mathbf{v}^j,\boldsymbol{l}\right)$ as $\operatorname{sim}_{i,j,l}$ and the frame temporal distance $|n(i) - n(j)|$ as $d_{i,j}$. Let \( D_{i,1} < D_{i,2} < \cdots < D_{i,M_i} \) represent the sorted alignment distances starting from the \( i \)-th frame. Specifically, \( D_{i,m}, m \in [M_i] \) is obtained by sorting the set of distances \( \{d_{i,j} \mid j \in [t] \setminus \{i\}\} \), where \( i \in [t] \). Define \( n_{i,m} := |\{j \mid d_{i,j} = D_{i,m}, \, j \in [t] \setminus \{i\}\}| \), which denotes the number of frames whose alignment distance from the \( i \)-th frame equals \( D_{i,m} \), for all \( i \in [t] \) and \( m \in [M_i] \). Then, we can prove that the representations of the sampled frames satisfy $\delta$-ordered property when $\mathcal{L}_{\mathrm{VLO}}$ approaches its lower bound: 
% \begin{equation}
% \mathcal{L}^*:=\frac{1}{t(t-1)} \sum_{i=1}^{t} \sum_{m=1}^{M_i} n_{i, m} \log n_{i, m}
% \end{equation}




% \begin{equation}
% \mathcal{L}_{\mathrm{SGC}} = \frac{1}{t} \sum_{i=1}^{t} l_{\mathrm{SGC}}^{(i)}
% \end{equation}

\subsection{Vision-Language Continuity}
\label{sec:bb}
While the VLO property provides a strong global constraint on the structural alignment of VL pretraining, optimizing triplet relationships alone can be \textit{unstable}. Variations in frame content and noise often lead to \textit{suboptimal} local consistency. To mitigate this, we introduce an additional local continuity constraint inspired by the \textit{Brownian bridge} \cite{revuz2013continuous}. This stochastic process models transitions between two fixed endpoints over by any sampled local video interval $[n(i), n(j)]$. For any time step $t\in[n(i), n(j)]$ within this interval, the transition density of Brownian Bridge process $\mathbf{B}(t)$ follows a time-dependent Gaussian distribution:
\begin{equation}\small\nonumber
\mathcal{N}\left(\mathbf{v}_i + \frac{t - n(i)}{n(j) - n(i)}(\mathbf{v}_j - \mathbf{v}_i), \frac{t(n(j)-n(i))-t^2)}{n(j) - n(i)}\right),
\end{equation}
where \(\mathbf{v}_i,\mathbf{v}_j\in\mathbb{R}^d\) are the visual embeddings of the first and last frames in the sampled interval. The mean trajectory $\mathbb{E}[\mathbf{B}(t)]$ linearly interpolates between the two endpoints, while the variance  $\mathrm{Var}[\mathbf{B}(t)]$ peaks provides uncertainty modeling that peaks in the middle of the interval. To enforce this local continuity, the Brownian bridge loss $\mathcal{L}_{\operatorname{BB}}$ is formulated as,
\begin{equation}
\mathcal{L}_{\mathrm{BB}} = \frac{1}{T} \sum_{t=1}^{T} \frac{1}{2 \mathrm{Var}[\mathbf{B}(t)]}\left\|\mathbf{v}_t - \mathbb{E}[\mathbf{B}(t)] \right\|_2^2.
\end{equation}
This loss encourages local consistency by penalizing deviations from expected trajectories, ensuring consistency across short temporal spans.


\noindent\textbf{Overall Objective.} The final training objective integrates both global and local constraints to achieve temporal coherence simultaneously:
\begin{equation}
\mathcal{L}_{\mathrm{AcTOL}} = \mathcal{L}_{\mathrm{VLO}} + \lambda \mathcal{L}_{\mathrm{BB}},
\end{equation}
where $\lambda$ is empirically set to balance two components.

\noindent\textbf{Theoretical Analysis.} To provide a deeper understanding of continuity preservation, we present theoretical guarantee: \begin{theorem} 
[Vision-Language Continuity] 
\label{thm:continuity}
Let \(\mathbf{v}_k,\mathbf{v}_l\) be arbitrary time step from the interval $[n(i), n(j)]$ and \(\mathbf{l} \in \mathcal{L} \) be the language embedding. Suppose the VL similarity function \(\operatorname{sim}(\cdot)\) is Lipschitz continuous with constant \( C\). Assume that the frame embeddings are regularized by the Brownian Bridge constraint, then for any \( \epsilon > 0 \), there exists \( \delta > 0 \) such that
\[
\|\mathbf{v}_k -\mathbf{v}_l \|_2 < \delta \Rightarrow \left| \mathfrak{R}(\mathbf{v}_k,\mathbf{v}_l,\mathbf{l}) \right| < \epsilon.
\]
\end{theorem}
\begin{proof}
Please refer to Appendix \ref{sec:proof_continuity}.
\end{proof}
This theorem guarantees that under the Brownian Bridge constraint, if two video frames are temporally close, their semantic alignment score remains within a bounded range. This continuity can be furthur extended and show model resiliency to language perturbation:

% \begin{equation}
% \mathbb{E}[\mathbf{B}(n(i))] =\mathbf{v}_0 + \frac{n(i) - n(0)}{n(t) - n(0)} (\mathbf{v}_t -\mathbf{v}_0),
% \end{equation}
% where \(\mathbf{v}_0 \) and \(\mathbf{v}_t \) denote the visual embeddings of the first and last frames. The Brownian Bridge loss penalizes deviations from the expected trajectory:
% \begin{equation}
% \mathcal{L}_{\mathrm{BB}} = \frac{1}{t} \sum_{i=1}^{t-1} \left\|\mathbf{v}_i - \mathbb{E}[\mathbf{B}(n(i))] \right\|_2^2.
% \end{equation}

% \noindent\textbf{Theoretical Analysis.} \begin{theorem}
% [Vision-Language Continuity] 
% \label{thm:continuity}
% Let \(\mathbf{v}_i,\mathbf{v}_j \in \mathcal{V} \) be visual embeddings and \(\mathbf{l} \in \mathcal{L} \) be the language embedding. Suppose the similarity function \( f \) is Lipschitz continuous with constant \( C_l \), i.e., $\operatorname{sim}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) = -\| f(\mathbf{v}_i,\mathbf{l}) - f(\mathbf{v}_j,\mathbf{l}) \|_2.$ If the embeddings \(\mathbf{v}_i \) are regularized using the Brownian Bridge constraint with temporal positions \( n(i) \), then for any \( \epsilon > 0 \), there exists \( \delta > 0 \) such that:
% \[
% \|\mathbf{v}_i -\mathbf{v}_j \|_2 < \delta \Rightarrow \left| \operatorname{sim}(\mathbf{v}_i,\mathbf{v}_j,\mathbf{l}) \right| < \epsilon.
% \]
% \end{theorem}

% \begin{proof}
% See Appendix \ref{sec:proof}.
% \end{proof}
\begin{theorem}[Robustness to Language Variations]\label{thm:robustness}
    Let $\mathbf{l}'$ be the perturbed version of the original language embedding $\mathbf{l}$ subject to a small constant $\delta_l>0$, i.e., $\|\mathbf{l}-\mathbf{l}'\| \leq \delta_l$, then the semantic alignment score $\mathfrak{R}$ exhibits 
stability to the perturbation:
\begin{align}
    |\mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}') - \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l})|\leq 2C\delta_l.
\end{align} 
\end{theorem}
\begin{proof}
Please refer to Appendix \ref{sec:proof_robustness}.
\end{proof}
Our empirical results, presented in Section \ref{sec:robustness}, further validate these properties by showing improved generalization to varying linguistic instructions and more stable performance compared to baseline methods.







% Consider an anchor frame \( o^i \). Let \( n(i) \) be the frame index of \( o^i \) in the original video. The frame \( o^i \) can form \( t-1 \) segments with the other frames in the batch. We aim for segments of the same length to have similar semantic gaps. Therefore, for a segment \( s_{i,j} \) formed by \( o^i \) and \( o^j \), where \( o^j \) is considered a positive sample for \( o^i \), we can define its negative sample set as \( \mathcal{N}_{i,j} = \{o^k \mid k \neq i, \ |n(i) - n(k)| \geq |n(i) - n(j)|\} \). For any \( o^k \in \mathcal{N}_{i,j} \), the segment \( s_{i,k} \) formed by \( o^i \) and \( o^k \) must have a length no less than \( s_{i,j} \). 

% To quantify semantic continuity, we define the semantic alignment score between any two frames \((o^i, o^j)\) as:
%     \begin{equation}
%         \mathfrak{R}(\mathbf{v}_i, \mathbf{v}_j, \mathbf{l}) = \|\operatorname{sim}(\mathbf{v}^i, \boldsymbol{l}) - \operatorname{sim}(\mathbf{v}^j, \boldsymbol{l})\|_2
%     \end{equation}

% Then we can define the segment gap contrastive loss over $o^i$ using InfoNCE ~\cite{corr18-infonce}:
% \begin{equation}
% \small
% l_{\mathrm{SGC}}^{(i)}=\frac{1}{t-1} \sum_{j=1, j \neq i}^{t}-\log \frac{\exp\left( \operatorname{sim}\left(\mathbf{v}^i,\mathbf{v}^j,\boldsymbol{l}\right) / \tau\right)}{\sum_{\mathbf{v}^k \in \mathcal{N}_{i, j}} \exp \left(\operatorname{sim}\left(\mathbf{v}^i,\mathbf{v}^k,\boldsymbol{l}\right) / \tau\right)},
% \end{equation}

% Here, we use the negative semantic gap as the similarity measure over the triplet, i.e., $\operatorname{sim}\left(\mathbf{v}^i,\mathbf{v}^j,\boldsymbol{l}\right) = -\|\operatorname{sim}(\mathbf{v}^i, \boldsymbol{l}) - \operatorname{sim}(\mathbf{v}^j, \boldsymbol{l})\|_2$. We use the Cosine Similarity to compute $\operatorname{sim}(\mathbf{v}, \boldsymbol{l})$. The loss encourages the probability of the positive sample $\mathbf{v}^j$ to be high. This means 
% $\operatorname{sim}\left(\mathbf{v}^i,\mathbf{v}^j,\boldsymbol{l}\right)$ should be larger compared to similarities with negative samples, indicating that the semantic gap between $o_i$ and $o_j$ is smaller than the semantic gap between $o_i$ and other frames that are further away in time. Then, we treat all frames as anchors, compute the above loss for each, and aggregate the results:
% \begin{equation}
%     \mathcal{L}_{\mathrm{SGC}} = \frac{1}{t}\sum_{i=1}^{t}l_{\mathrm{SGC}}^{(i)},
% \end{equation}
% Optimizing $\mathcal{L}_{\mathrm{SGC}}$ ensures that the temporal continuity of frames is preserved in the semantic space. When contrasting the anchor frame with the temporally closest frame in the batch, the loss ensures their semantic gap is smaller than that of all other frames. Similarly, when contrasting the anchor with the second closest frame, it enforces their semantic gap to be smaller than that of frames with a greater temporal distance. This process iteratively extends to increasingly distant frames (e.g., the third closest, fourth closest, etc.) for all anchor frames in the batch, encouraging frames that are temporally closer to exhibit greater similarity in the semantic space.

% \paragraph{Theoretical Analysis}



% \subsection{Brownian Bridge Constraint}
% The Brownian Bridge (BB) constraint is introduced to address limitations in the semantic gap contrastive loss \( \mathcal{L}_{\mathrm{SGC}} \), particularly from the perspective of the triplet similarity measure \( (\mathbf{v}^i, \mathbf{v}^j, \boldsymbol{l}) \).  \( \mathcal{L}_{\mathrm{SGC}} \) indirectly optimizes visual features through their alignment with language representations. This reliance on pairwise triplet similarity does not impose explicit constraints on the smoothness or stability of the visual feature trajectories \( \mathbf{v}^{1:t} \). Consequently, the temporal evolution of visual features can exhibit fluctuations or irregularities, especially under sparse or noisy sampling conditions.  

% The BB constraint addresses the challenge by introducing a temporal regularization that stabilizes the visual feature trajectory while maintaining alignment with the semantic transitions described by the language. Specifically, the BB constraint ensures that visual features follow a smooth trajectory between the first and last frames, resembling the properties of a Brownian Bridge process.

% \paragraph{Formulation of the Brownian Bridge}  
% A Brownian Bridge process \( \mathbf{B}(t) \) is a stochastic process that interpolates between two fixed points \( \mathbf{B}(a) = \mathbf{v}^a \) and \( \mathbf{B}(b) = \mathbf{v}^b \) over a temporal interval \( [a, b] \). In the context of a batch of sampled video frames, let \( n(i) \) represent the original temporal position of the \( i \)-th frame within the video. The first and last frames in the batch, are denoted as \( \mathbf{v}^0 \) and \( \mathbf{v}^t \), respectively. For an intermediate frame \( \mathbf{v}^i \) with temporal position \( n(i) \), the expected value under the Brownian Bridge process is defined as:
% \begin{equation}
%     \mathbb{E}[\mathbf{B}(n(i))] = \mathbf{v}^0 + \frac{n(i) - n(0)}{n(t) - n(0)} (\mathbf{v}^t - \mathbf{v}^0),
% \end{equation}
% where \( n(0) \) and \( n(t) \) are the temporal positions of \( \mathbf{v}^0 \) and \( \mathbf{v}^t \), respectively.

% The Brownian Bridge constraint enforces smooth transitions between visual features by penalizing deviations from this expected trajectory. The loss function is defined as:
% \begin{equation}
%     \mathcal{L}_{\mathrm{BB}} = \frac{1}{t} \sum_{i=1}^{t-1} \|\mathbf{v}^i - \mathbb{E}[\mathbf{B}(n(i))]\|_2^2.
% \end{equation}

% \paragraph{Visual-Language Interaction with \(\mathcal{L}_{\mathrm{BB}}\) and \( \mathcal{L}_{\mathrm{SGC}} \)}  
% The BB constraint complements \( \mathcal{L}_{\mathrm{SGC}} \) by stabilizing the trajectory of visual features. While \( \mathcal{L}_{\mathrm{SGC}} \) minimizes the semantic gap between adjacent frames, the BB constraint ensures that the visual features evolve smoothly over time. This stabilization reduces noise and irregularities, making it easier to align visual features with the continuous semantic transitions described by language.

% The combined loss over a batch of videos is then formulated as:
% \begin{equation}
%     \mathcal{L}_{\mathrm{total}} = \frac{1}{B} \Sigma_{B} \mathcal{L}_{sample} = \frac{1}{B} \Sigma_{B} \left(\mathcal{L}_{\mathrm{SGC}} + \lambda \mathcal{L}_{\mathrm{BB}}\right),
% \end{equation}
% where \( \lambda \) is a weighting parameter that controls the strength of the constraint.



% \begin{theorem}[Vision-Language Continuity]
% Let $ \mathbf{v}^i, \mathbf{v}^j \in \mathcal{V} $ be visual embeddings and $ \boldsymbol{l} \in \mathcal{L} $ be the language embedding. Suppose the similarity function is defined using a Lipschitz continuous function $ f $:
% \begin{equation}
% \operatorname{sim}(\mathbf{v}^i, \mathbf{v}^j, \boldsymbol{l}) = -\| f(\mathbf{v}^i, \boldsymbol{l}) - f(\mathbf{v}^j, \boldsymbol{l}) \|_2.
% \end{equation}
% Assume that the embeddings $ \mathbf{v}^i, \mathbf{v}^j $ are regularized using a Brownian Bridge constraint with temporal positions $ n(i) $ and $ n(j) $, and that $ f $ is Lipschitz continuous with constant $ C_l $.
% Then, for any $ \epsilon > 0 $, there exists $ \delta > 0 $ such that the similarity function (or reward used in downstream manipulation tasks) satisfies:
% \begin{equation}
% \| \mathbf{v}^i - \mathbf{v}^j \|_2 < \delta \quad \Rightarrow \quad \left| \operatorname{sim}(\mathbf{v}^i, \mathbf{v}^j, \boldsymbol{l}) \right| < \epsilon.
% \end{equation}

% \end{theorem}

