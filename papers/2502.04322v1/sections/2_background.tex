\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}

\textbf{Jailbreaking LLMs.}
Methods to jailbreak safety-aligned LLMs range from manual approaches to automated techniques, including gradient-based token optimization \citep{zhu2023autodan, zou2023universal, liao2024amplegcg}, multi-agent prompt augmentation \citep{perez2022red, pair, tap}, and custom inference templates \citep{promptinjection, AnilManyshotJ}. 
However, these methods often require technical expertise and intensive computation, making them less accessible to average users and less suited for evaluating LLM safety in real-world scenarios. 
In contrast, we explore the potential to elicit harmful outputs through simple natural language interactions in realistic scenarios.

\textbf{Jailbreak Evaluation.}
To systematically evaluate model safety against jailbreak methods, several benchmarks have been proposed \citep{mazeikaharmbench, chao2024jailbreakbench, xie2024sorrybench}.
These benchmarks typically focus on jailbreak success, where model responses are evaluated by an LLM judge using metrics including compliance \citep{zou2023universal, jailbroken}, fulfillment \citep{xie2024sorrybench}, harmfulness \citep{huang2024catastrophic}, validity \citep{zhu2023autodan}, and specificity \citep{souly2024strongreject}.
In contrast, we examine the underlying attributes of harmful responses that a malicious, non-expert user seeks.

\textbf{Common Human-LLM Interactions.} 
To help LLMs better understand user intent and solve complex problems, prompt design strategies, such as demonstrated reasoning steps \citep{nye2021show, wei2022chain} and query decomposition \citep{perez2020unsupervised, dua2022successive, zhou2023least}, have gained popularity in user interactions \citep{deng2024wildvis}.
Additionally, LLMs' multilingual ability \citep{fu2022polyglot, achiam2023gpt, ahuja2023mega} further enhances accessibility for users from diverse linguistic backgrounds. 
However, the ease of interaction also introduces risks, as malicious users may exploit multi-step reasoning capabilities \citep{shaikh2023second, li2023multi}, multi-turn conversations \citep{li2024multiturnhuman, huang2024endlessjailbreaksbijectionlearning}, or safety loopholes in mid- and low-resource languages \citep{yong2023lowresource, deng2024multilingual, wang-etal-2024-languages}.
In fact, such behaviors already exist in real-world user-LLM interaction logs \cite{zhaowildchat, deng2024wildvis}. 
Our work aims to demonstrate the simplicity and efficacy of manipulating these interaction modes for harmful jailbreaks. 
