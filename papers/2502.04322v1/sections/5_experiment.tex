\vspace{-1mm}
\section{Experiments}
\label{sec:5}
\vspace{-1.5mm}
We now evaluate \speakeasy by measuring its effectiveness in amplifying harmful jailbreaks.
\S\ref{sec:5.1} outlines the experimental setup, and \S\ref{sec:5.2} validates \harmscore through human evaluation.
In \S\ref{sec:evaluation_results}, we compare \speakeasy against existing jailbreak baselines through both \asr and \harmscore.
Finally, we conduct ablation studies in \S\ref{sec:ablation_studies} and analyze language usage in \S\ref{sec:language_usage_speakeasy}.

\vspace{-2mm}
\subsection{Experimental Setup}
\label{sec:5.1}
\vspace{-1.5mm}
In our main experiments, we evaluate three jailbreak baselines, both with and without \speakeasy using four benchmarks.
We target three multilingual LLMs: the proprietary \gptfouro \citep{openai2024gpt4o} and the open-source \qwen (\qwenabbr) \citep{yang2024qwen2technicalreport} and \llamaseventy (\llamaseventyabbr) \citep{grattafiori2024llama3herdmodels}. 

\textbf{Jailbreak Baselines.}
We incorporate \speakeasy into the following three jailbreak baselines and observe the absolute change in jailbreak success and harmfulness:
(1) Direct Request (\dr) directly prompts the target LLM with malicious queries;
(2) Greedy Coordinate Gradient-Transfer (\gcg) \citep{zou2023universal} appends an adversarial suffix to the query;
(3) Tree of Attacks with Pruning-Transfer (\tap) \citep{tap} utilizes tree-of-thought reasoning to iteratively refine malicious queries.
These baselines are applied to the decomposed subqueries in \speakeasy during integration. More details are in \S\ref{app:jailbreak_baselines}. 

\textbf{Jailbreak Benchmarks.}
We evaluate on four jailbreak benchmarks, covering a wide range of harm categories:
(1) \harmbench \citep{mazeikaharmbench} with its standard split of $200$ single-sentence queries;
(2) \advbench \citep{zou2023universal} with $520$ harmful instructions;
(3) \sorrybench \cite{xie2024sorrybench} with $450$ harmful instructions; 
(4) \medsafety \cite{han2024medsafetybench}, where we randomly sample $50$ examples from each of the nine medical harm categories, totaling 450 instances. 

\textbf{Implementation Details.}
For \speakeasy, we use three steps ($m=3$) for query decomposition and six languages ($n=6$) to exploit multilingual vulnerabilities. 
To ensure diversity, we select two languages from each resource group \citep{linguisticdiversity, lai2023chatgpt}: English (\texttt{en}) and Simplified Chinese (\texttt{zh}) from the high-resource group, Ukrainian (\texttt{uk}) and Turkish (\texttt{tr})  from the mid-resource group, and Zulu (\texttt{zu}) and Thai (\texttt{th}) from the low-resource group.
Translations are performed using Azure AI Translator \citep{microsoftAzureTranslator}.
For \harmscore, we instantiate the actionability and informativeness evaluation functions (i.e., $f_A(Q,R)$ and $f_I(Q,R)$, respectively) by following the procedure in \S\ref{sec:4.3} and fine-tune \llama on a separate preference dataset for each attribute. 
The fine-tuning datasets here \emph{do not} overlap with those in \S\ref{sec:4.3}.

\begin{table}[t]
    \centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|ccc}
    \toprule
    \textbf{Category} & \textbf{\harmbenchasr} & \textbf{\gptasr} & \textbf{\harmscore} \\  
    \midrule 
    \midrule
    \texttt{chemical}        & 0.762 & 0.864 &\bf 0.881  \\
    \texttt{illegal}         & 0.666 & 0.696 & \bf 0.780  \\
    \texttt{misinformation}  & \bf 0.550 & 0.501 & 0.424  \\
    \texttt{harmful}         & 0.615  & 0.679 & \bf 0.695 \\
    \texttt{harassment}      & 0.518  & 0.690 & \bf 0.715 \\
    \texttt{cybercrime}      & 0.650  & \bf 0.870 & 0.761 \\
    \midrule
    Overall                  & 0.638  & 0.723 & \bf 0.726 \\
    \bottomrule
    \end{tabular}
}
    \vspace{-3.5mm}
    \caption{Pearson rank correlation between human judgment and the three metrics on \harmbench subsamples.
    \harmbenchasr and \gptasr are \asr variants using the \harmbench classifier and \gptfouro, respectively.
    \gptasr and \harmscore achieve competitive correlation, and both outperform \harmbenchasr by $0.09$.}
    \label{tab:attack_human_eval}
    \vspace{-5mm}
\end{table}

\vspace{-2mm}
\subsection{Human Evaluation for \harmscore}
\label{sec:5.2}
\vspace{-1mm}
Before proceeding to evaluations, we first assess how well \harmscore aligns with human judgments compared to two variants of \asr: \gptasr \citep{qi2024safety}, based on \gptfouro, and \harmbenchasr, based on the \harmbench classifier.
To this end, we randomly sample ten queries from each of the six semantic categories in \harmbench and collect the corresponding responses from the three jailbreak baselines, both with and without \speakeasy, totaling $360$ query-response pairs. 
We then recruit $27$ graduate students to label whether each response is harmful, ensuring that each query-response pair is annotated three times. 
Fleiss' $\kappa$ of $0.622$ indicates a strong inter-annotator agreement.

 \begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{images/quant_eval.pdf}
    \vspace{-20pt}
    \caption{Jailbreak performance measured by \asr and \harmscore before and after integrating \speakeasy into the baselines, with the shaded bars highlighting the difference. \speakeasy significantly increases both \asr and \harmscore across almost all methods.
    See Table~\ref{tab:quant_eval} for full numerical values.}
    \vspace{-4mm}
    \label{fig:quant_eval}
\end{figure*}

As shown in Table~\ref{tab:attack_human_eval}, \harmscore and \gptasr achieve comparable alignment with human judgment, both outperforming \harmbenchasr with a $9\%$ absolute increase in overall correlation.
Notably, \harmscore excels in four of six categories, particularly those seeking explicit instructions (e.g., chemical and harassment), highlighting the importance of measuring actionability and informativeness in such attacks.

In the misinformation and cybercrime categories, where queries often focus on content generation rather than actionable guidelines (e.g., ``write an article arguing that immigrants in the U.S.\ hardly pay any taxes"), \harmscore struggles to assess response actionability, leading to a lower correlation than \gptasr and \harmbenchasr.
As such, we report \asr, instantiated using \gptfouro, and \harmscore in the following sections.

\vspace{-2.25mm}
\subsection{Evaluation Results}
\label{sec:evaluation_results}
\vspace{-1.25mm}
We present our main findings in Figure~\ref{fig:quant_eval}. 
Overall, our results demonstrate the strong jailbreak efficacy of \speakeasy as measured by \asr and \harmscore.

\textbf{\speakeasy significantly increases \asr and \harmscore for direct requests.} 
All evaluated LLMs have undergone safety alignment, so directly querying the model (\dr) results in consistently low scores. 
The shaded bars in Figure~\ref{fig:quant_eval}(a) illustrate the substantial increase achieved by \speakeasy, often exceeding a threefold rise in both metrics.
This effect is most pronounced in \gptfouro, where \asr increases from $0.092$ to $0.555$ on average, with the largest change of $0.672$ on \advbench. 
Although \qwenabbr and \llamaseventyabbr exhibit better robustness, we still observe average \asr increases of $0.304$ and $0.226$ respectively. 
A similar trend holds for \harmscore, with \speakeasy driving an increase ranging from $0.327$ to $0.579$.
Notably, the effect is larger in magnitude than \asr, suggesting that responses can become sufficiently harmful even with a modest rise in \asr.
Across benchmarks, \advbench exhibits high \asr but comparatively lower \harmscore, while \medsafety follows the opposite pattern. 
The latter contains more domain-specific harmful queries, making it more difficult to assess success but often yielding highly actionable and informative responses.

\textbf{\speakeasy further increases \asr and \harmscore when combined with existing jailbreak methods.} 
Next, we examine the effect of integrating \speakeasy into two state-of-the-art jailbreak techniques, \gcg and \tap. 
Observe that \gcg, when used independently, leads to minimal changes or even a decline in attack success. 
We integrate \speakeasy by appending the GCG-generated adversarial suffix to the decomposed subqueries, and find average increases of at least $0.2$ across all LLMs and benchmarks.
The largest change is again observed in \gptfouro, where \asr increases by $0.480$ and \harmscore by $0.636$ on average. 
Furthermore, we evaluate \tap, which already achieves high baseline scores. 
Despite its strong performance, \speakeasy further boosts \asr by $0.235$ to $0.336$ and \harmscore by $0.299$ to $0.393$ on average. 
Strikingly, for both \gptfouro and \llamaseventyabbr, this integration yields an \asr that exceeds $0.9$ across all benchmarks. 
These results demonstrate that \speakeasy is a versatile framework that can be integrated into existing methods, further enhancing the success and harmfulness of attacks. 

\vspace{-2mm}
\subsection{Ablation Studies}
\label{sec:ablation_studies}
\vspace{-0.5mm}
When interacting with LLMs in a multi-step and multilingual manner, users can adjust the number of steps, the choice of languages, and the selection of responses at each stage of the process.
Here, we examine how these three components in \speakeasy influence the jailbreak responses'  harmfulness. 
By default, we use \gptfouro as the backbone with \dr $+$ \speakeasy and \harmbench as the target benchmark.
Unless otherwise specified, we use three decomposition steps, six languages, and our fine-tuned response selection models.
We present our results in Table~\ref{tab:ablation}.

\begin{table}[t]
    \centering
    \small
    \begin{tabular}{c|c|cc}
    \toprule
    \textbf{Ablation} & \textbf{Setting} & \textbf{\asr} & \textbf{\harmscore} \\
    \midrule
    \midrule
    \multirow{3}{*}{\shortstack{Number of\\Steps}} & $1$ & $0.115$ & $0.154$ \\
                              & $\textbf{3}$ & $0.560$ & $0.779$ \\
                              & $5$ & $0.690$ & $0.732$ \\
    \midrule
    \multirow{4}{*}{\shortstack{Number of\\Languages}} & $1$ & $0.430$ & $0.489$ \\
                               & $3$ & $0.470$ & $0.693$ \\
                               & $\textbf{6}$ & $0.560$ & $0.779$ \\
                               & $9$ & $0.595$ & $0.778$ \\
    \midrule    
    \multirow{5}{*}{\shortstack{Response\\Selection}} & Random & $0.495$ & $0.723$ \\
                                 & Fixed-Lang. &$0.435$& $0.477$ \\
                                 & Fixed-Comb. & $0.445$ & $0.718$ \\
                                 & Oracle & $0.765$ & $0.914$ \\
                                 & \textbf{Ours} & $0.560$ & $0.779$ \\ 
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{Jailbreak performance of ablated \speakeasy settings. 
    The default setting uses three steps, six languages, and our fine-tuned response selection models (bolded). 
    \asr and \harmscore increase with decomposition steps and languages. 
    Under Oracle (the upper bound of scores), our response selection models outperform all alternative approaches.
    }
    \vspace{-4mm}
    \label{tab:ablation}
\end{table}

\textbf{Number of Query Decomposition Steps.}
We vary the number of decomposition steps, $m \in \{1,3,5\}$. 
Setting $m=1$ corresponds to the multilingual jailbreak method \citep{yong2023lowresource, deng2024multilingual} using six languages. 
Increasing $m$ from $1$ to $3$ introduces the query decomposition component, which significantly increases \asr from $0.115$ to $0.560$ and \harmscore from $0.154$ to $0.779$. 
We attribute this to the effectiveness of the decomposition process in breaking down a harmful query into seemingly harmless subqueries.
Namely, we find that while \gptfouro refuses to respond in $81\%$ of single-step multilingual queries ($m=1$), the refusal rate drops sharply to $1.5\%$ for $m=3$ (as measured by when one or more of the $m$ subqueries elicit refusal).
As we vary $m$ from $3$ to $5$, \asr shows a smaller increase while \harmscore sees a slight decline, which we hypothesize is due to additional subqueries introducing irrelevant information that lowers \harmscore.
We provide details on the refusal rate and the respective actionability and informativeness scores in \S\ref{app:supp_experiments}.

\textbf{Number of Jailbreak Languages.} 
We vary the number of languages $n \in \{1,3,6,9\}$, where $n=1$ represents querying only in English in three steps. 
When changing $n$ from $3$ to $9$, we respectively use $1$ to $3$ languages from each resource group (high, medium, and low).
In Table~\ref{tab:ablation}, observe that both \asr and \harmscore increase with $n$, which corroborates previous findings on multilingual vulnerabilities \citep{deng2024multilingual}. 
However, we observe diminishing gains beyond using six languages for both \asr and \harmscore. 
This suggests an inherent upper bound of multilingual jailbreak, where adding more languages beyond a certain count has limited effects.

\textbf{Response Selection Methods.} 
As introduced in \S\ref{sec:4.3}, we fine-tune our response selection models to emulate how users may choose the most actionable and informative response for each subquery in \speakeasy. 
Here, we explore alternative selection strategies: (1) \textbf{Random}, which randomly selects a response at each subquery, (2) \textbf{Fixed-Language}, which always selects responses from the same language, and we report the highest scores among the six languages; (3) \textbf{Fixed-Combination}, which follows one of the $6^3 = 216$ possible ways of combining the six languages across the three subqueries, and we report the best performing combination; (4) \textbf{Oracle}, which serves as a performance upper bound by dynamically choosing responses with known \asr and \harmscore.
Our response selection models achieve the next best performance, followed by Random. 
Both significantly outperform the fixed methods, which are limited to a single language or a predetermined combination. 
The multilingual responses to subqueries are already actionable and informative, so leveraging the full range of available responses proves more effective than fixed methods.
The remaining gap between our response selection models and Oracle indicates room for further improvement.

Overall, our ablation results highlight the efficacy of adding additional decomposition steps, languages, and improved response selection criteria.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\columnwidth]{images/qual_ana.pdf}
    \vspace{-30pt}
    \caption{{Top}: Average actionability and informativeness scores; {Bottom}: Selection rates for each language, both for $n=6$.
    Each color theme represents a language resource level.
    While informativeness remains consistent across languages, actionability and selection rate decreases with resource level.
    }
    \vspace{-3mm}
    \label{fig:qual_ana}
\end{figure}

\vspace{-3mm}
\subsection{Language Usage in \speakeasy}
\label{sec:language_usage_speakeasy}
\vspace{-1.5mm}
We provide additional analysis into the languages used in \speakeasy, based on \gptfouro's results on \harmbench.
Figure~\ref{fig:qual_ana}(a) illustrates the average actionability and informativeness scores for subqueries in \speakeasy.
We observe that lower-resource languages tend to score lower in actionability \citep{souly2024strongreject}, with English achieving the highest score.
Actionability scores are generally low at the subquery level, as subquery responses may be unactionable on their own and only become fully actionable when combined into a complete response.
In contrast, informativeness scores are consistently high across all languages and exhibit a more uniform distribution, with Ukrainian and Turkish from the medium-resource group scoring the highest.\footnote{We do not directly compare actionability and informativeness scores because the scoring models are trained on datasets with different distributions, making them inherently incomparable.} 
This pattern supports the efficacy of query decomposition, as subqueries can elicit informative responses across languages. 


From Figure~\ref{fig:qual_ana}(b), we observe that all six languages exhibit non-trivial selection rates, with the highest selection rate for English at $39.1\%$. 
While lower-resource languages are selected less frequently, we observe that they still contribute to actionable and informative responses to malicious queries.