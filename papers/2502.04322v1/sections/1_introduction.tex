\vspace{-2em}
\section{Introduction}
\vspace{-1mm}

Recent advancements in large language models (LLMs) have driven their widespread adoption across various domains \citep{achiam2023gpt, anthropic2023claude, touvron2023llama}, serving a variety of individuals from highly skilled experts to non-technical, everyday users \citep{bommasani2021opportunities}. 
To ensure safe deployment, significant efforts have been made to align these models \citep{bai2022training, bai2022constitutional, ganguli2022red, markov2023holistic}.
However, these efforts face ongoing challenges from ``jailbreaks" \citep{jin2024jailbreakzoo, jailbroken}, adversarial attacks that aim to breach LLMs' safety mechanisms and induce harmful responses, which pose societal risks when used by malicious actors \citep{hendrycks2023overview}.

Despite the widespread adoption of LLMs by non-technical users, current research offers limited insights into how jailbreaks manifest in simple, everyday interactions.
Existing jailbreak methods typically require a technical understanding of models' internal mechanisms \citep{zou2023universal} or extensive engineering efforts \citep{pair, tap}. 
These settings, however, may not accurately reflect real-world scenarios where an average user attempts to misuse LLMs for malicious purposes \citep{cybertruck_explosion}.

To address this gap, we investigate two questions: (1) What kinds of jailbroken responses enable non-technical users to induce harm? (2) Can these responses be obtained through simple interactions with an LLM?
To answer the first question, we identify four attributes \citep{xing2017topic, cho2019towards, ganguli2022red} potentially related to harmfulness and curate a synthetic dataset in which each example demonstrates a combination of these attributes. 
Through human evaluation, we determine \emph{actionability} and \emph{informativeness} as key attributes in inducing harm when the jailbroken response is followed by individuals without specialized knowledge.
On this basis, we introduce \harmscore, a metric that explicitly measures the aforementioned attributes and provides a more fine-grained assessment of jailbreak harmfulness than commonly used measures of success (e.g., Attack Success Rate (\asr) \citep{ganguli2022red, mazeikaharmbench, jailbroken}). 
Notably, \harmscore aligns better with human judgments than \asr, especially for queries that seek practical instructions.

To demonstrate that simple interactions can sufficiently elicit actionable and informative jailbreaks, we propose \speakeasy. 
In contrast to other jailbreak frameworks, \speakeasy emulates two types of human-LLM interactions commonly observed in real-world examples \citep{deng2024wildvis, zhaowildchat, zhenglmsys}: multi-step reasoning and multilingual querying.  
Given a harmful query, users can decompose it into multiple seemingly innocuous subqueries \citep{dua2022successive, kojima2022large, wei2022chain}, which more easily circumvent safety guardrails. 
They can further exploit the multilingual knowledge \citep{ahuja2023mega} and vulnerabilities \citep{yong2023lowresource, deng2024multilingual} in LLMs by translating the subqueries to various languages. 
Using these two tools, a non-technical user can generate a harmful response to the original malicious query by selecting the most actionable and informative responses to subqueries.
\speakeasy automates this process by employing response selection models fine-tuned for the two attributes. 
Altogether, \speakeasy is an accessible jailbreak framework that simulates how non-expert users would realistically seek harmful content.

To systematically evaluate our proposed framework, we target safety-aligned proprietary and open-source multilingual LLMs including \gptfouro \citep{openai2024gpt4o}, \qwen \citep{yang2024qwen2technicalreport}, and \llamaseventy \citep{grattafiori2024llama3herdmodels}, across four jailbreak benchmarks \citep{zou2023universal, mazeikaharmbench, xie2024sorrybench, han2024medsafetybench}. 
Notably, \speakeasy increases the \asr of \gptfouro from $0.092$ to $0.555$ on average across four benchmarks, and its \harmscore from $0.180$ to $0.759$, all through simple inference easily accessible to users.
\speakeasy can also be readily integrated into existing jailbreak methods for users with technical expertise.
Incorporating \speakeasy into \gcg \citep{zou2023universal} and \tap \citep{tap} significantly outperforms their vanilla baselines, yielding an average absolute increase of $0.313$ in \asr and $0.398$ in \harmscore across LLMs and benchmarks.
We further demonstrate through ablation studies that using more decomposition steps and languages in \speakeasy increases response harmfulness.

We summarize our contributions in this paper as follows:
\begin{itemize}[itemsep=0pt, topsep=0pt, partopsep=0pt]
    \item We identify \emph{actionability} and \emph{informativeness} as key attributes that constitute a harmful jailbreak response. 
    \item We introduce \harmscore, a metric grounded in the aforementioned attributes that aligns competitively with human judgments.
    \item We show that \speakeasy, a simple multi-step and multilingual jailbreak framework, significantly increases the likelihood of harmful responses in both proprietary and open-source LLMs.
\end{itemize}