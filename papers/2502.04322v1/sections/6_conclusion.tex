\vspace{-6mm}
\section{Conclusion}
\vspace{-1.5mm}

In this paper, we demonstrate that state-of-the-art LLMs can easily produce harmful responses through interactions commonly encountered in everyday user settings.
We identify \textit{actionability} and \textit{informativeness} as the two key contributors to response harmfulness.
On this basis, we propose \harmscore, a new metric that quantifies harm using these two attributes, which demonstrates competitive alignment with \asr based on human judgments.
Following this, we propose \speakeasy, a jailbreak framework that leverages vulnerabilities in multi-step, multilingual interactions.
\speakeasy significantly increases the likelihood of generating harmful content, with an average absolute increase of $0.319$ in \asr and $0.426$ in \harmscore.
The success of \speakeasy highlights the simplicity with which highly actionable and informative---and therefore truly harmful---jailbreak responses can be elicited in LLMs.
Our work highlights a critical gap between current jailbreak research and real-world use cases.
We advocate that future work in safety alignment should focus more on realistic user settings.