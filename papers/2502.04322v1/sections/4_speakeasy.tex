\section{Jailbreaks Through Simple Interactions} \label{sec:speakeasy}
\vspace{-1mm}

In real-world interactions between human users and LLMs, conversations often extend beyond single-instance question-answering \citep{wang2024mint}.
Users may engage in multi-turn interactions, pose follow-up questions, or communicate in languages other than English, as evidenced by millions of conversations in user-LLM interaction datasets \citep{zhaowildchat, zhenglmsys, deng2024wildvis}.

However, common interactions can also be exploited for malicious purposes. 
Consider the left interaction in Figure~\ref{fig:wildvis_example}, sourced from \texttt{WildVis} \citep{deng2024wildvis}.
The user gradually seeks suggestions for suicidal drugs through a multi-turn conversation starting with a drug-related query.
This act resembles the decomposition of a complex task into modular subtasks \citep{khot2023decomposed}, where, similarly, harmful instructions can be decomposed into seemingly benign subqueries to bypass safeguards.

Another frequently observed interaction mode is multilingual conversations, with nearly half of the real-world interaction logs in \texttt{WildChat} \citep{zhaowildchat} conducted in languages other than English. 
The right interaction in Figure~\ref{fig:wildvis_example} displays a conversation from \texttt{WildVis} where the LLM complies with a user's request for drug abuse instructions in Spanish.
Since safety training is typically conducted in English, if a malicious request triggers a refusal in English, users can simply rephrase it in another language, increasing the likelihood of receiving an affirmative response \citep{yong2023lowresource, deng2024multilingual, shen2024language}.

\vspace{-2mm}
\subsection{The \speakeasy Jailbreak Framework} \label{sec:4.2}
\vspace{-1mm}
Based on observations of misuse in real-world multi-step and multilingual interactions, we design \speakeasy to simulate how non-expert users realistically pursue harmful content and visualize the jailbreak framework in Figure~\ref{fig:method_visualization}. 
Given a malicious query $Q$, \speakeasy prompts the target LLM to decompose it into $m$ subqueries, $\mathcal{Q} = \{q_1, \dots, q_m\}$. 
We manually curate query decompositions \citep{dua2022successive, wei2022chain, zhou2023least} of benign tasks as in-context learning examples (additional details in \S\ref{app:decomposition}).
In Figure~\ref{fig:method_visualization}, the query ``making dimethylmercury‚Äù is decomposed into three steps, beginning with identifying materials containing mercury and then inquiring about synthesizing dimethylmercury from them. 

\begin{figure}[t]
    \centering  
    \includegraphics[width=\columnwidth]{images/wildvis_example.pdf}
    \vspace{-18pt}
    \caption{Real-world human-LLM interactions from \texttt{WildVis}. The left example illustrates a multi-step user-LLM interaction with a malicious query and subsequent follow-ups. 
    In the right example, the multilingual LLM provides step-by-step instructions in response to a malicious query in Spanish.}
    \label{fig:wildvis_example}
    \vspace{-4mm}
\end{figure}

To obtain useful information from each subquery, \speakeasy exploits multilingual vulnerabilities by prompting the target LLM with subqueries translated into a predefined set of languages, $\mathcal{L} = \{L_1, \dots, L_n\}$, spanning different resource groups \citep{linguisticdiversity, lai2023chatgpt}. 
The responses are then translated back into English\footnote{English is used as the default base language, as existing jailbreaking benchmarks are predominantly in English.}, forming a pool of $n$ candidate responses, $\mathcal{R}_i = \{r_{i,1}, \dots, r_{i,n}\}$, for each subquery $q_i \in \mathcal{Q}$.

Finally, to identify the most preferred response for each subquery, we fine-tune response selection models, $g_A(q, r)$ and $g_I(q, r) \in [0,1]$, for actionability and informativeness, respectively (see \S\ref{sec:4.3} for further details). 
Within the candidate response pool $\mathcal{R}_i$ for each subquery $q_i \in \mathcal{Q}$, \speakeasy selects the highest-scoring response: $r_i^* = \arg\!\max_{r_{i,j} \in \mathcal{R}_i} \; g_A(q_i, r_{i,j}) + g_I(q_i, r_{i,j})$.

The example in Figure~\ref{fig:method_visualization} shows responses selected from English, Zulu, and Ukrainian, respectively. 
These responses are concatenated to form the final jailbreak response, $R = (r_1^*, \dots, r_m^*)$, to the original malicious query $Q$. 
Note that \speakeasy usually functions as a standalone jailbreak framework for average users interacting with a black-box LLM chatbot. 
Additionally, the simplicity and accessibility of the framework allow it to be integrated with existing jailbreak methods for technically advanced users.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{images/method_visualization.pdf}
    \vspace{-7mm}
    \caption{Our \speakeasy jailbreak framework. 
    Given a malicious query, we (1) decompose it into multiple steps of seemingly harmless subqueries and (2) translate each subquery into a set of predefined languages from different resource groups. 
    We then (3) prompt multilingual LLMs with the translated subqueries at each step.
    After collecting the responses, we (4) translate them back into English and (5) select the most actionable and informative response for each subquery using our response selection models. 
    Finally, (6) the selected responses are combined to form a complete response to the original malicious query.}
    \vspace{-5mm}
    \label{fig:method_visualization}
\end{figure*}

\vspace{-2mm}
\subsection{Selecting Actionable and Informative Responses} \label{sec:4.3}
\vspace{-1mm}

To train response selection models for measuring actionability and informativeness, we repurpose existing preference datasets for each attribute using \gptfour.
We first summarize queries from the \hhrlhf \citep{bai2022training} and \stackexchange \citep{h4stackexchange} datasets into single sentences to match the format of typical jailbreak queries \citep{zou2023universal, mazeikaharmbench}.
For both attributes, we filter out irrelevant examples by assessing whether the queries can be answered with an actionable or informative response.
The responses of the remaining examples are then annotated for each attribute, and we only retain examples with one response exhibiting the attribute and another lacking it.
This process yields a preference dataset comprising $27{,}000$ valid query-preference pairs for each attribute, with examples in \S\ref{app:selection_model_training}.

Using these datasets, we fine-tune Llama-3.1-8B-Instruct \citep{touvron2023llama} as our response selection models for each attribute separately with iterative direct preference optimization \citep{xu2024thingscringeothersiterative} 
(See \S\ref{app:selection_model_training} for additional training details).
During response selection, we apply a sigmoid function to the output scores and map them to the range $[0,1]$ to ensure both attributes are weighted equally. 

\begin{table}[t]
    \centering
    \small
    \vspace{1mm}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc}
        \toprule
            \textbf{Model} & \textbf{Actionability} & \textbf{Informativeness} \\ 
        \midrule
        \midrule            
            \llama & 0.672 & 0.174 \\ 
            FsfairX-LLaMA3-RM-v0.1 & 0.183 & 0.105 \\ 
            ArmoRM-Llama3-8B-v0.1 & 0.764 & 0.048 \\
            Our Response Selection Models & \textbf{0.835} & \textbf{0.956} \\
        \bottomrule
    \end{tabular}
    }
    \vspace{-7pt}
    \caption{Evaluation results on human-annotated preference test sets. Our response selection models significantly outperform the baselines in selecting more actionable or informative responses.}
    \vspace{-11pt}
    \label{tab:reward_model_eval}
\end{table}

\textbf{Evaluation.}
To evaluate our response selection models, we construct preference test sets using the human evaluation data from \S\ref{sec:human_evaluation_jailbreak_attributes}.
For each query, we pair an actionable response with an unactionable one with replacement and produce $509$ test examples. 
The model is required to assign a higher score to the actionable response. 
We curate $455$ examples for informativeness with the same procedure. 

We compare our models against popular off-the-shelf reward model baselines: (1) our base model, Llama-3.1-8B-Instruct; (2) FsfairX-LLaMA3-RM-v0.1 \citep{dong2023raft}, which is fine-tuned on high-quality samples; (3) ArmoRM-Llama3-8B-v0.1 \citep{wang2024arithmetic}, optimized for multi-dimensional objectives.
As shown in Table~\ref{tab:reward_model_eval}, our response selection models outperform these baselines significantly, especially for informativeness.
These results confirm that our models align closely with human perceptions of actionability and informativeness.