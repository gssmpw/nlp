\clearpage
\section*{Technical Appendix}

\subsection{POI Attributes Preprocess} \label{sec: data preprocess}    
    \subsubsection{Visit Pattern of POI}
    % 
    We conduct a statistical analysis of the check-in data to identify the visit patterns of a POI, which consists of the weekly visit pattern and daily visit pattern.  On the one hand, we first divide the week into weekdays (Monday to Friday) and weekends (Saturday and Sunday). Whether a POI is visited more during the weekdays or weekends will determine its weekly visit pattern. On the other hand, We part every day into seven time periods: early morning (6 to 9 AM), morning (9 to 11 AM), noon (11 AM to 1 PM), afternoon (1 PM to 5 PM), evening (5 to 7 PM), night (7 to 12 PM), and midnight (0 to 6 AM).  Afterward, we categorize each POI's check-in records into the corresponding time slot. Finally, the time slot with the highest number of check-in records will represent the daily visit pattern. Take the POI from \tableautorefname~\ref{tab:poi_example} as an example: its weekly visit pattern is weekdays, and its daily visit pattern is 6 to 9 AM.

    \subsubsection{Address of POI}
    Address features are essential for a POI. Typically, a POI's address includes the street name, house number, and postal code.  By utilizing geographic revere search API provided by Nominatim \footnote{Nominatim: \url{https://nominatim.openstreetmap.org}}, we can input latitude and longitude coordinates to retrieve the corresponding address details.  For each POI in the dataset, we leverage this API to query its address information as well as the basic attribute $Name$. 

    \subsubsection{Surrounding of POI}
    % The environment surrounding a POI can reflect its functional attributes. For instance, if many banks and coffee shops are nearby, the POI likely corresponds to an office setting. 
    % Conversely, a concentration of restaurants and bars suggests a leisure and entertainment poi. 
    For any given POI, we survey the category of other POIs nearby and consider this as the surrounding attributes. Specifically,  we search for all POIs within a square area centered on a certain POI and the square's side length is set to 0.5 km. Then, we count the number of categories within these POIs and sort them in descending order. Finally, we select the top three categories as its surrounding attribute. For example, the top three categories near the POI in \tableautorefname~\ref{tab:poi_example} are office, building, and road, which are its surrounding attributes.

\subsection{Multi-View Contrastive Learning}

\paratitle{Sequence-Time Contrastive Learning}
    A formal constraint is that given a check-in record sequence $R$ and a check-in record $r=(u,p,t)$, for any $r'=(u,p',t')$ in the $R$, we have the following two requirements for $r'$:
    \begin{itemize}
        \item $|index(r') - index(r)| \leq \lambda$, where $index$ is a function indicating the position in $R$, and $\lambda$ is a threshold.
        \item $date(r') = date(r)$, where $date$ is a function that extracts the date from a timestamp.
    \end{itemize}

    The $p'$ in $r'$ which meets the above requirements is considered a positive sample.  

\paratitle{Geography Contrastive Learning}
    Given a POI $p$, the positive samples $p'$ have to meet the following two criteria:
    \begin{itemize}
        \item $Area(p)$ is defined as $Square(p, \tau)$, where $Square$ is a function that generates the corresponding square region with a side length of $\tau$. The point $p'$ lies within the $Area(p)$.
        \item $p'.c=p.c$, and $c$ is the category attribute of a POI introduced in Section ~\ref{sec:pre}.
    \end{itemize}

    
\paratitle{Functional Contrastive Learning}
Given a POI $p$, the positive samples $p'$ must satisfy the following two conditions:
    \begin{itemize}
        \item $p'.visit \_pattern=p.visit\_pattern$, and $visit\_pattern$ is the visit pattern attribute of a POI.
        \item $p'.c=p.c$, where $c$ is the category attribute of a POI. 
    \end{itemize}

\subsection{Experiments}
In our experiments section, the training processes of the \name and all downstream tasks are implemented with Pytorch on the Nvidia RTX 3090 GPU. For LLM-based baselines, We use the POI Address Prompts to extract features from the LLM as representations.

\paratitle{Baselines}

\bitem{Skip-Gram}~\cite{mikolov2013efficient}, a simple variant of Word2Vec model, and is widely used in sequential tasks.

\bitem{POI2Vec}~\cite{feng2017poi2vec}, a latent representation model for both POI and user, which introduces geographical information into output via splitting the map into rectangle regions and building binary tree on it.

\bitem{Geo-Teaser}~\cite{zhao2017geo}, an embedding model based on Skip-Gram, but take temporal influence and neighboring locations in the trajectory dataset into consideration and design two separate loss functions for them.

\bitem{TALE}~\cite{wan2021pre}, a POI embedding pre-training method based on CBOW,  constructing a temporal tree structure based on user trajectories to acquire time awareness.

\bitem{Hier}~\cite{shimizu2020enabling}, a method to obtain fine-grained place embedding via extracting spatial information from trajectory dataset in a hierarchical way.

\bitem{CTLE}~\cite{lin2021pre}, the state-of-the-art model of POI representation which incorporates both temporal and spatial information of user activities into its embedding.

\paratitle{Statistics of Processed Datasets}
After processing the data, the details of the datasets are presented in \tableautorefname\ref{tab:dataset_sta}.
\begin{table}[t]
\centering
    \resizebox{0.7\columnwidth}{!}{%
        \begin{tabular}{rccc}
            \toprule
            Dataset & \#User & \#POI & \#Check-in \\
            \midrule
            Foursquare-NY & 15,171 & 24,118 & 641,005 \\ 
            Foursquare-SG & 10,909 & 20,154 & 696,306 \\ 
            Foursquare-TKY & 2,293 & 15,164 & 496,459 \\
            \bottomrule
        \end{tabular}
    }
     \caption{Statistics of Datasets.} 
    \label{tab:dataset_sta}
\end{table}

\paratitle{Downstream Task Implementation}
In our framework, the dimension $d$ is uniformly set to 256.  Moreover, the number of encoder layers $L_1$ in Dual Feature Alignment and the number of encoder layers $L_2$ in Cross Attention Fusion is 4 and 2, respectively. The temperature parameter $\gamma$ in InfoNCE is set to 0.1. We train \name for 100 epochs using a learning rate of 0.001 and the AdamW optimizer with a decay of 0.001. 

\bitem{POI Recommendation},
To evaluate this task, we train a two-layer LSTM model to process the input check-in sequence and feed the output into an MLP to make the prediction. Note that we cut long sequences in the dataset into slices with less than 128 check-in records to improve model efficiency.

\bitem{Check-in Sequence Classification},
Similar to the POI Recommendation task, we use a two-layer LSTM connected to an MLP to classify check-in sequences.


\bitem{POI Visitor Flow Prediction},
 We set the time window to 1 hour and calculate the check-in count of every POI based on the check-in dataset. Due to the sparsity of the check-in dataset, we only select non-zero visitor flow sequences with a length greater than 5 when building the dataset. Then, we trained a Seq2seq model with MSE loss to do the prediction. Before input into the model, the visitor flow series is normalized and appended with two additional information, the embedding of the current POI and the hour of the day at which the series starts.


\paratitle{Parameter Settings}

In \name, the distance $\tau$ is set to 0.5 km when obtaining POI Surrounding features. The feature dimension $D$ extracted from the Llama2 and ChatGLM2 is 4096, while the feature dimension $D$ extracted from GPT-2 is 768.
Within each encoder layer of the Dual Feature Alignment and Cross Attention Fusion, the number of attention heads $H$ is set to 8, and the dimension of each head $d_h$ is set to 32.


Besides,  $\lambda$ in Sequence-Time Contrastive Learning is set to 2 and the sampling distance of Geography Contrastive Learning $\tau$ is set to 0.5 km. 


 In the downstream task training, the hidden size of LSTM is uniformly set to 512. We use the Adam optimizer and a learning rate of 0.0001 to train all the downstream models for 100 epochs, except for the model of the POI Recommendation task, which has a learning rate of 0.001. 


 

\subsection{Case Study}
Here, we conduct distance comparison on the POI  representation vectors before and after enhancement. Specifically,  we selected two POIs that are geographically very close and of the same category from the New York dataset. Despite the high similarity between these two POIs, through the output of the LLM, we can identify disparities in their semantic information. For specific attributes, please refer to the \tableautorefname ~\ref{tab: similar_POI}.
    \begin{table}[ht]
    \centering
        \resizebox{\columnwidth}{!}{%
            \begin{tabular}{rccc}
                \toprule
                POI Name & \#Long, Lat & \#Category & \#LLM Output \\
                \midrule
                Bagatelle & -74.006,40.739 & French Restaurant & festive atmosphere \& brunches \\ 
                The Jane & -74.009,40.738 & French Restaurant & cozy ambiance \& cocktails \\ 
                \bottomrule
            \end{tabular}
        }
         \caption{The attributes of two similar POIs.} 
        \label{tab: similar_POI}
    \end{table}
Subsequently, we utilized the Hier model before and after enhancement to represent these two POIs. The Euclidean distance was employed to calculate the distance between the resulting representation vectors, and the obtained results are presented in \tableautorefname ~\ref{tab: distance_POI}. We find the Euclidean distance between POI vectors was smaller in Hier, while it was larger in Hier+, showing POI-Enhancer can enrich the vectors with LLM knowledge.

\begin{table}[htbp]
    \centering
        \resizebox{0.4\columnwidth}{!}{%
            \begin{tabular}{rc}
                \toprule
                Model & Distance \\
                \midrule
                Hier+ &   262.90 \\
                \midrule
                Hier & 7.49 \\
                \bottomrule
            \end{tabular}
        }
         \caption{The distance comparison result.}
        \label{tab: distance_POI}
    \end{table}


    \subsection{Performance of LLM-based baselines}

    \begin{table}[htbp]
\belowrulesep=0pt
\aboverulesep=0pt
\setlength{\tabcolsep}{1.2mm}
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    \textbf{Task} & \multicolumn{6}{c|}{\textbf{POI Recommendation}}  \\
    \midrule
    Dataset & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c|}{SG}\\
    \midrule
    Metric & Hit@1 & Hit@5 & Hit@1 & Hit@5 & Hit@1 & Hit@5   \\
    \midrule
    Skip-Gram+ & 7.610  & 18.032  & \textcolor[rgb]{ .122,  .137,  .161}{15.557 } & \textcolor[rgb]{ .122,  .137,  .161}{34.197 } & \textcolor[rgb]{ .122,  .137,  .161}{10.747 } & \textcolor[rgb]{ .122,  .137,  .161}{24.468 }   \\
    POI2Vec+ & 7.851  & 18.353  & 15.800  & 34.768  & 10.630  & 24.030    \\
    Geo-Teaser+ & \textcolor[rgb]{ .216,  .235,  .263}{7.116 } & \textcolor[rgb]{ .216,  .235,  .263}{16.657 } & 15.500  & 34.475  & \textcolor[rgb]{ .122,  .137,  .161}{10.122 } & \textcolor[rgb]{ .122,  .137,  .161}{23.532 }   \\
    TALE+ & 6.690  & 15.208  & 14.940  & 33.223  & 8.694  & 20.342   \\
    Hier+ & 8.009  & 19.197  & 16.187  & 35.715  & \textcolor[rgb]{ .122,  .137,  .161}{10.592 } & \textcolor[rgb]{ .122,  .137,  .161}{24.079 }   \\
    CTLE+ & \textcolor[rgb]{ .216,  .235,  .263}{7.093 } & \textcolor[rgb]{ .216,  .235,  .263}{17.032 } & \textcolor[rgb]{ .216,  .235,  .263}{15.479 } & 34.138  & \textcolor[rgb]{ .122,  .137,  .161}{10.315 } & \textcolor[rgb]{ .122,  .137,  .161}{24.027 }   \\
    \midrule
    GPT-2 & 0.6639  & 2.7265  & 6.9243  & 17.3328  & 2.3632  & 8.0120    \\
    Llama2 & 1.6491  & 5.1294  & 8.0065  & 19.0356  & 3.9478  & 11.8564    \\
    ChatGLM2 & 4.2386  & 11.4755  & 12.4734  & 28.2362  & 5.4692  & 16.1045   \\
    \end{tabular}%
    }
    \caption{The performance of LLM-based baselines in downstream tasks.}
  \label{tab:LLM-based baseline1}
  \end{table}


  \begin{table}[htbp]
\belowrulesep=0pt
\aboverulesep=0pt
\setlength{\tabcolsep}{1.2mm}
\centering
\resizebox{\columnwidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    \textbf{Task} & \multicolumn{6}{c|}{\textbf{Check-in Sequence Classification}} \\
    \midrule
    Dataset & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c|}{SG} \\
    \midrule
    Metric  Acc   & F1    & Acc   & F1    & Acc   & F1    \\
    \midrule
    Skip-Gram+  & 52.151  & 0.251  & 62.936  & 0.438  & \textcolor[rgb]{ .122,  .137,  .161}{47.285 } & 0.255   \\
    POI2Vec+ & 52.151  & 0.245  & 62.358  & 0.438  & 46.521  & 0.264   \\
    Geo-Teaser+ & 49.910  & 0.233  & 62.647  & 0.437  & 50.064  & 0.279    \\
    TALE+   & 50.689  & 0.240  & 63.380  & 0.448  & 47.719  & 0.263  \\
    Hier+ &  51.893  & 0.254  & 63.380  & 0.441  & 47.795  & 0.258    \\
    CTLE+ &  50.430  & 0.234  & 61.848  & 0.434  & 51.440  & 0.287    \\
    \midrule
    GPT-2 &  1.0327  & 0.0006  & 0.4664  & 0.0003  & 0.6628  & 0.0004    \\
    Llama2 & 1.0327  & 0.0005  & 1.3324  & 0.0039  & 0.9177  & 0.0019   \\
    ChatGLM2  & 17.1687  & 0.0487  & 31.3347  & 0.1680  & 11.5728  & 0.0372    \\
    \end{tabular}%
    }
    \caption{The performance of LLM-based baselines in Check-in Sequence Classification.}
  \label{tab:LLM-based baseline2}
  \end{table}


\begin{table}[htbp]
\belowrulesep=0pt
\aboverulesep=0pt
\setlength{\tabcolsep}{1.2mm}
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc|cc|cc}
    \toprule
    \textbf{Task} &  \multicolumn{6}{c|}{\textbf{POI Visitors Flow Prediction}} \\
    \midrule
    Dataset & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c|}{SG}  \\
    \midrule
    Metric   & MAE   & RMSE  & MAE   & RMSE  & MAE   &  RMSE  \\
    \midrule
    Skip-Gram+   & 0.336  & 0.514  & 0.492  & 0.668  & 0.621  & 0.890  \\
    POI2Vec+ & 0.326  & 0.492  & 0.490  & 0.696  & 0.602  & 0.868  \\
    Geo-Teaser+  & 0.341  & 0.524  & \textcolor[rgb]{ .122,  .137,  .161}{0.483 } & \textcolor[rgb]{ .122,  .137,  .161}{0.669 } & 0.588  & 0.854  \\
    TALE+ & 0.320  & 0.482  & 0.510  & 0.701  & \textcolor[rgb]{ .122,  .137,  .161}{0.610 } & \textcolor[rgb]{ .122,  .137,  .161}{0.903 } \\
    Hier+ &  0.313  & 0.483  & \textcolor[rgb]{ .122,  .137,  .161}{0.510 } & \textcolor[rgb]{ .122,  .137,  .161}{0.719 } & 0.574  & 0.804  \\
    CTLE+ &  0.291  & 0.456  & \textcolor[rgb]{ .122,  .137,  .161}{0.495 } & \textcolor[rgb]{ .122,  .137,  .161}{0.689 } & 0.610  & 0.892  \\
    \midrule
    GPT-2 & 0.409  & 0.773  & 0.539  & 0.766  & 0.637  & 0.923  \\
    Llama2 & 0.389  & 0.694  & 0.520  & 0.738  & 0.639  & 0.901  \\
    ChatGLM2 & 0.439  & 0.795  & 0.536  & 0.738  & 0.629  & 0.952  \\
    \end{tabular}%
    }
    \caption{The performance of LLM-based baselines in POI Visitors Flow Prediction.}
  \label{tab:LLM-based baseline3}%
  \end{table}


  

 Here, we utilize three LLM-based baselines including Llama2, ChatGLM2, and GPT-2. Specifically, we used the POI Address Prompts to extract features from the LLM as representations, just as we discussed in Section ~\ref{sec:method}. Then, we compare the LLM-based baselines with the POI embedding models improved by \name. The result of this experiment is shown in \tableautorefname ~\ref{tab:LLM-based baseline1}, \tableautorefname ~\ref{tab:LLM-based baseline2}, \tableautorefname ~\ref{tab:LLM-based baseline3}.

%  \begin{table*}[!t]
% \belowrulesep=0pt
% \aboverulesep=0pt
% \setlength{\tabcolsep}{1.2mm}
% \centering
% \resizebox{\linewidth}{!}{
%     \begin{tabular}{l|cc|cc|cc|cc|cc|cc|cc|cc|cc}
%     \toprule
%     \textbf{Task} & \multicolumn{6}{c|}{\textbf{POI Recommendation}} & \multicolumn{6}{c|}{\textbf{Check-in Sequence Classification}} & \multicolumn{6}{c}{\textbf{POI Visitors Flow Prediction}} \\
%     \midrule
%     Dataset & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c|}{SG} & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c|}{SG} & \multicolumn{2}{c|}{NY} & \multicolumn{2}{c|}{TKY} & \multicolumn{2}{c}{SG} \\
%     \midrule
%     Metric & Hit@1 & Hit@5 & Hit@1 & Hit@5 & Hit@1 & Hit@5 & Acc   & F1    & Acc   & F1    & Acc   & F1    & MAE   & RMSE  & MAE   & RMSE  & MAE   &  RMSE  \\
%     \midrule
%     Skip-Gram+ & 7.610  & 18.032  & \textcolor[rgb]{ .122,  .137,  .161}{15.557 } & \textcolor[rgb]{ .122,  .137,  .161}{34.197 } & \textcolor[rgb]{ .122,  .137,  .161}{10.747 } & \textcolor[rgb]{ .122,  .137,  .161}{24.468 } & 52.151  & 0.251  & 62.936  & 0.438  & \textcolor[rgb]{ .122,  .137,  .161}{47.285 } & 0.255  & 0.336  & 0.514  & 0.492  & 0.668  & 0.621  & 0.890  \\
%     POI2Vec+ & 7.851  & 18.353  & 15.800  & 34.768  & 10.630  & 24.030  & 52.151  & 0.245  & 62.358  & 0.438  & 46.521  & 0.264  & 0.326  & 0.492  & 0.490  & 0.696  & 0.602  & 0.868  \\
%     Geo-Teaser+ & \textcolor[rgb]{ .216,  .235,  .263}{7.116 } & \textcolor[rgb]{ .216,  .235,  .263}{16.657 } & 15.500  & 34.475  & \textcolor[rgb]{ .122,  .137,  .161}{10.122 } & \textcolor[rgb]{ .122,  .137,  .161}{23.532 } & 49.910  & 0.233  & 62.647  & 0.437  & 50.064  & 0.279  & 0.341  & 0.524  & \textcolor[rgb]{ .122,  .137,  .161}{0.483 } & \textcolor[rgb]{ .122,  .137,  .161}{0.669 } & 0.588  & 0.854  \\
%     TALE+ & 6.690  & 15.208  & 14.940  & 33.223  & 8.694  & 20.342  & 50.689  & 0.240  & 63.380  & 0.448  & 47.719  & 0.263  & 0.320  & 0.482  & 0.510  & 0.701  & \textcolor[rgb]{ .122,  .137,  .161}{0.610 } & \textcolor[rgb]{ .122,  .137,  .161}{0.903 } \\
%     Hier+ & 8.009  & 19.197  & 16.187  & 35.715  & \textcolor[rgb]{ .122,  .137,  .161}{10.592 } & \textcolor[rgb]{ .122,  .137,  .161}{24.079 } & 51.893  & 0.254  & 63.380  & 0.441  & 47.795  & 0.258  & 0.313  & 0.483  & \textcolor[rgb]{ .122,  .137,  .161}{0.510 } & \textcolor[rgb]{ .122,  .137,  .161}{0.719 } & 0.574  & 0.804  \\
%     CTLE+ & \textcolor[rgb]{ .216,  .235,  .263}{7.093 } & \textcolor[rgb]{ .216,  .235,  .263}{17.032 } & \textcolor[rgb]{ .216,  .235,  .263}{15.479 } & 34.138  & \textcolor[rgb]{ .122,  .137,  .161}{10.315 } & \textcolor[rgb]{ .122,  .137,  .161}{24.027 } & 50.430  & 0.234  & 61.848  & 0.434  & 51.440  & 0.287  & 0.291  & 0.456  & \textcolor[rgb]{ .122,  .137,  .161}{0.495 } & \textcolor[rgb]{ .122,  .137,  .161}{0.689 } & 0.610  & 0.892  \\
%     \midrule
%     GPT-2 & 0.6639  & 2.7265  & 6.9243  & 17.3328  & 2.3632  & 8.0120  & 1.0327  & 0.0006  & 0.4664  & 0.0003  & 0.6628  & 0.0004  & 0.409  & 0.773  & 0.539  & 0.766  & 0.637  & 0.923  \\
%     Llama2 & 1.6491  & 5.1294  & 8.0065  & 19.0356  & 3.9478  & 11.8564  & 1.0327  & 0.0005  & 1.3324  & 0.0039  & 0.9177  & 0.0019  & 0.389  & 0.694  & 0.520  & 0.738  & 0.639  & 0.901  \\
%     ChatGLM2 & 4.2386  & 11.4755  & 12.4734  & 28.2362  & 5.4692  & 16.1045  & 17.1687  & 0.0487  & 31.3347  & 0.1680  & 11.5728  & 0.0372  & 0.439  & 0.795  & 0.536  & 0.738  & 0.629  & 0.952  \\
%     \end{tabular}%
%     }
%     \caption{The performance of LLM-based baselines in downstream tasks.}
%   \label{tab:LLM-based baseline}%
%   \end{table*}




   Furthermore, to examine the contribution of the LLM and Contrastive Learning, we design two experiments on the NY dataset, using the Hier model and the POI recommendation task. The details of the experiment settings are shown below. 

   \begin{table}[htbp]
    \centering
        \resizebox{0.7\columnwidth}{!}{%
            \begin{tabular}{rcc}
                \toprule
                Model & Hit@1 & Hit@5 \\
                \midrule
                Hier+ & 8.009 & 19.197 \\ 
                \midrule
                Hier+(w/o LLM)& 7.029 & 17.028 \\
                \midrule
                Hier+(w/o CL)& 8.009 & 19.197 \\
                \bottomrule
            \end{tabular}
        }
         \caption{The attributes of two similar POIs.} 
        \label{tab:additional_ablation}
    \end{table}
    
    \bitem{Hier+} We use the Hier model enhanced by our framework.
  
    \bitem{Hier+(w/o LLM)} We remove the special prompt design including the role-playing, the attribute headers, and the question.
    
    \bitem{Hier+(w/o CL)} We replaced the fusion process with vector addition and removed the Contrastive Learning Scheme from the framework.

    This section ablation experiment result in the \tableautorefname ~\ref{tab:additional_ablation} shows that LLM and Contrastive Learning both are beneficial to the performance.

    
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{prompt.pdf} 
    \caption{The examples of special prompt we design}.  
    \label{fig: prompt}
\end{figure}