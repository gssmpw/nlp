% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{censor}

\usepackage[colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=black]{hyperref}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{UPCMR: A Universal Prompt-guided Model for Random Sampling Cardiac MRI Reconstruction}

% Prompt-Cascade: Prompt-Guided Cascading Model for Random Sampling Cardiac MRI Reconstruction

%
\titlerunning{UPCMR: A Universal Prompt-guided Model}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Donghang Lyu\inst{1}\orcidID{0000-1111-2222-3333} \and
% Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
% Third Author\inst{3}\orcidID{2222--3333-4444-5555}}

\author{Donghang Lyu\inst{1} \and Chinmay Rao\inst{1} \and Marius Staring\inst{1} \and Matthias J.P. van Osch\inst{1} \and Mariya Doneva\inst{4} \and Hildo J. Lamb\inst{1} \and Nicola Pezzotti\inst{2,3}}

\institute{Department of Radiology, Leiden University Medical Center, Leiden, The Netherlands\\ 
\email{d.lyu@lumc.nl}
\and
Cardiologs, Philips, Paris, France
\and
Faculty of Computer Science, Eindhoven University of Technology, Eindhoven, The Netherlands
\and
Philips Innovative Technologies, Hamburg, Germany}

% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Princeton University, Princeton NJ 08544, USA \and
% Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
% \email{\{abc,lncs\}@uni-heidelberg.de}}
%

\authorrunning{D. Lyu et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
% \author{Anonymous}
% \institute{
% Anonymous Organization \\
% \email{******@*******.***}
% }

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Cardiac magnetic resonance imaging (CMR) is vital for diagnosing heart diseases, but long scan time remains a major drawback. To address this, accelerated imaging techniques have been introduced by undersampling k-space, which reduces the quality of the resulting images. Recent deep learning advancements aim to speed up scanning while preserving quality, but adapting to various sampling modes and undersampling factors remains challenging. Therefore, building a universal model is a promising direction. In this work, we introduce UPCMR, a universal unrolled model designed for CMR reconstruction. This model incorporates two kinds of learnable prompts, undersampling-specific prompt and spatial-specific prompt, and integrates them with a UNet structure in each block. Overall, by using the CMRxRecon2024 challenge dataset for training and validation, the UPCMR model highly enhances reconstructed image quality across all random sampling scenarios through an effective training strategy compared to some traditional methods, demonstrating strong adaptability potential for this task.

\keywords{CMR Reconstruction \and Random Sampling \and Prompts \and UNet.}
\end{abstract}

% offering a versatile solution for improving image quality and adaptability to random sampling scenarios. 
%
%
%
\section{Introduction}
Cardiac Magnetic Resonance Imaging often requires multiple breath-holds for a comprehensive heart scan, leading to potential patient discomfort and slice misalignment. Moreover, the combination of data from multiple cardiac cycles can be problematic, especially in cases of arrhythmia, limiting real-time imaging to lower spatio-temporal resolutions. To address these challenges, undersampling techniques have been explored over the years to accelerate the whole scanning process. In recent years, deep learning methods~\cite{qin2018convolutional,schlemper2017deep,sriram2020end} have gained significant attention and are increasingly being used for MRI reconstruction, aiming to restore image details effectively, even with high acceleration factor. However, managing various undersampling scenarios with a single model remains challenging. The main difficulty lies in achieving and maintaining high adaptability across these varied conditions. Inspired by the rapid development of prompt learning~\cite{visp,mio,xin2023fill}, we introduce two kinds of learnable prompts, aiming to improve the adaptability across different contrasts, k-space trajectories, and acceleration factors.

Prompt learning adapts large pre-trained models, also known as foundation models, to new tasks by providing specific hints. It has been widely used in the natural language processing (NLP)~\cite{wei2022chain,zhang2022automatic} and computer vision (CV)~\cite{kirillov2023segment,potlapalli2024promptir,xin2023fill} tasks. In the medical imaging field, prompts have been extensively utilized for segmentation tasks. Medical Segment Anything Model (MedSAM)~\cite{ma2024segment} uses box prompt to provide spatial hints, enabling accurate segmentation of masks within the specified box. Uniseg~\cite{uniseg} introduces a universal prompt, which generates corresponding task-specific prompts and improves the correlation among different tasks. For MRI reconstruction, PromptMR~\cite{xin2023fill} employs learnable prompts to adapt itself to two different tasks: cine MRI reconstruction and T1-weighted (T1w) and T2-weighted (T2w) MRI reconstruction. Inspired by the general strategy of prompt-based guidance, we proposed an unrolled all-in-one model called UPCMR, which integrates learnable prompts into the UNet structure in each block so as to better adapt to different undersampling scenarios. In addition to model design, an effective training strategy is crucial for handling versatile random sampling. Inspired by the widespread use of curriculum learning~\cite{cl} in similar tasks, we also explored its effectiveness with two formats and selected the most effective one as the final training strategy for the UPCMR method.

\section{Preliminaries and Dataset}
\subsection{Preliminaries}
Consider reconstructing a sequence of complex-valued MR images $x$ $\in$ $\mathbb{C}^{T \times H \times W}$ from multi-coil undersampled k-space measurements $y$, where $H$ and $W$ denote the height and width of each frame, respectively, $T$ represents the sequence length. Therefore, the goal is to reconstruct $x$ from $y$, formulated as
\begin{equation} 
    % \mathop{argmin}\limits_{x}  \| y - A x \|_2^2+ \lambda \mathcal{R}(x),
    \operatorname*{argmin}_{x} \| y - A x \|_2^2 + \lambda \mathcal{R}(x),
\end{equation}
where $A$ is the linear forward operator composed of coil sensitivity encoding $S$, 2D Fourier transform $\mathcal{F}$, and undersampling mask $M$, $\mathcal{R}$ represents the regularisation terms with $\lambda$ as a hyper-parameter controlling the regularization strength. For deep unrolled methods, $\mathcal{R}$ represents a trainable neural network block. Some previous deep unrolled models based on the ADMM optimization algorithm such as Deep-ADMM net~\cite{admm} introduce an intermediate variable $z$, also computed via a learnable block. When constraining $z$ to be equal to $x$, the above problem is reformulated as
\begin{equation}
    \operatorname*{argmin}_{x,z}  \| y - A x \|_2^2+ \mu \| x -z \|_2^2+ \lambda \mathcal{R}(z).
\end{equation}
The unrolled model thus learns a sequence of transition where $x^{i}$ passes through a neural network block to produce $z^{i}$, and $z^{i}$ is used to generate $x^{i+1}$ through the data consistency (DC) layer. These operations are represented as
\begin{equation}
    z^{i} = f_\theta^i(x^i),
\end{equation}
\begin{equation}
    x^{i+1} = DC(z^{i},y,\lambda_{0},\Omega)=A^{\dagger} \Lambda Az^{i}+\frac{\lambda_{0}}{1+\lambda_{0}}A^{\dagger}y
\end{equation}
where $A^{\dagger}$ denotes the Hermitian operation of $A$, $\lambda_{0}$ is a regularization parameter and we make it approach infinity to ensure the preservation of sampled k-space information, $\Omega$ is an index set of the acquired k-space samples and $\Lambda$ is a diagonal matrix, whose diagonal values are
\begin{equation}
\Lambda_{kk} =
\left\{
\begin{array}{lcl}
& 1          & {if \  k \notin \Omega}\\
& \frac{1}{1+\lambda_{0}}          & {if \  k \in \Omega}\\
\end{array} \right.
\end{equation}
\subsection{CMRxRecon2024 Dataset}
The CMRxRecon2024 dataset~\cite{data2024} includes data from 330 healthy volunteers scanned with 3 Tesla magnets. Task 2 features multi-contrast k-space data (Cine, Aorta, Mapping, Tagging) with anatomical views such as long-axis, short-axis, LVOT, and aortic views (transversal and sagittal). Each contrast consists of 5-15 slices, segmented into 12-25 cardiac phases with a temporal resolution of ~50 ms. Key geometrical parameters are a spatial resolution of 1.5×1.5 mm², slice thickness of 8.0 mm, and a slice gap of 4.0 mm. The dataset includes three k-space trajectories (uniform, Gaussian, pseudo radial) with temporal/parametric interleaving, and various acceleration factors (4$\times$, 8$\times$, 12$\times$, 16$\times$, 20$\times$, 24$\times$) without including autocalibration signal (ACS) for calculation. For the ACS area, it includes the central 16 lines for uniform and Gaussian types, and the central 16$\times$16 regions for the pseudo radial type. Training data includes 200 subjects with fully-sampled k-space data across all combinations of k-space trajectories
and acceleration factors, while 60 validation subjects are provided with random undersampling settings. The ground-truth of validation set is unavailable, and performance is assessed by submitting the reconstructed central cropped parts of the images to the platform.
\section{Methodology}
\subsection{Model Structure}
\textbf{UPCMR} We follow an unrolled design used by some previous methods~\cite{qin2018convolutional,xin2023fill}, as shown in Fig.~\ref{fig1}. 
\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth]{./pics/upcmr_f.png}
\caption{Overview of the UPCMR model. In each cascade block, green lines represent feature maps transitioning from the previous one to the next block. Blue and red lines highlight the direction of undersampling-specific prompts and spatial-specific prompts, respectively. Black dashed lines refer to the skip connections between the corresponding encoder and decoder components.} \label{fig1}
\end{figure}
From a global perspective, in addition to common inputs like undersampling k-space, corresponding mask, coil-combined image sequence with 2 channels, and coil sensitivity map, an undersampling-specific prompt embedding is also provided as input. Given that both k-space trajectory and acceleration factor are prior information, we build two learnable prompt pools: one for k-space trajectory $K \in \mathbb{R}^{3 \times C}$ and another for the acceleration factor $R \in \mathbb{R}^{6 \times C}$, where $C$ is the embedding length, which matches the channel number of the UNet block. When selecting the $m^{th}$ k-space trajectory $K_{m}$ and $n^{th}$ acceleration factor $R_{n}$, these prompt embeddings are combined by a multi-layer perceptron (MLP) and fed into each cascade block to introduce undersampling-related information, enhancing model adaptability. UPCMR outputs include the reconstructed image series, as well as predictions for k-space trajectory and acceleration factor classes. Here, an additional classification block is used during training. The undersampling-specific prompt embedding $P_{U}$ and the spatial-specific prompt embedding $P_{S}$ from all cascade blocks are concatenated to make classification so as to further condition the model.
% An additional classification block is used during training, combining the undersampling-specific prompt embedding $P_{U}$ with the spatial-specific prompt embedding $P_{S}$ to condition the model. 
% This auxiliary task allows the conditioning effect of the prompts on the model.
% To better guide these two kinds of learnable prompts, they are combined to predict the k-space trajectory class and acceleration factor class of the input.
\\
\\
\textbf{Prompt-guided UNet Block} Each block of the cascading structure incorporates a 2-level UNet featuring some extra Conv2D layers and a bottleneck block that mirrors the encoder block's structure. The channel number is fixed at 64 across all layers, with downsampling and upsampling operations only occur at the first level. In the encoder part, for the input feature map at $i^{th}$ UNet block and $j^{th}$ encoder block, $F_{i,j} \in \mathbb{R}^{B \times T \times C \times H \times W}$, it first passes through a FiLMBlock~\cite{film}, interacting with the undersampling-specific prompt $P_{U} \in \mathbb{R}^{B \times C}$, where $B$ denotes batch size. $F_{i,j}$ is processed by global average pooling (GAP) operation to combine with $P_{U}$ along the channel dimension. Then two separate linear layers generate weight embedding $W_{P} \in \mathbb{R}^{B \times T \times C}$ and bias embedding $B_{P} \in \mathbb{R}^{B \times T \times C}$. These embeddings are applied to $F_{i,j}$ through element-wise multiplication and addition to produce the output $F_{i,j+1}$ with prompt information. Additionally, $W_{P}$ and $B_{P}$ add together and do average along the time dimension to form an updated undersampling-specific prompt embedding that integrates image sequence information. The updated undersampling-specific prompt embedding from each block are concatenated to form the final $P_{U}$ of the current cascade block. After that, a temporal-channel attention block (TCABlock) is applied. Compared to traditional Conv3D or \textit{Conv(2+1)D}~\cite{conv21d} operations, the TCABlock incorporates a simple attention mechanism along temporal and spatial dimensions, aiming to better exploit the spatio-temporal correlation across the whole sequence. Furthermore, inspired by the CRNN-i operation from the CRNN-MRI~\cite{qin2018convolutional} model, the feature maps from the previous cascade block are added to strengthen cascade connections and retain some important features. As shown in Fig.~\ref{fig1}, two inputs, $F_{i,j}$ and $F_{i-1,j}$, pass through separate \textit{Conv(2+1)D} layer and add together to obtain an intermediate feature map $F_{tmp}$. Then it multiplies with the transposed $F_{tmp}$ to compute temporal correlations between frames and generate an attention map using the softmax function. This attention map is multiplied with $F_{tmp}$ to produce the final output, incorporating temporal attention information. Channel attention is applied following the same design as the SE block~\cite{se}. After the TCABlock, the feature map is processed through a \textit{Conv(2+1)D} layer, which either downsamples or maintains the current shape. Moreover, temporal circular padding~\cite{tcp} is used with all \textit{Conv(2+1)D} layers to account for the circular nature of cardiac MRI.

The decoder blocks follow a design similar to the encoder blocks but include a PromptBlock with a learnable spatial-specific prompt $P_{S} \in \mathbb{R}^{B \times 1 \times C \times H \times W}$, inspired by~\cite{potlapalli2024promptir,xin2023fill}. This addition helps the model adapt to different undersampling cases by enriching the spatial context. The PromptBlock's pipeline begins with concatenating $F_{i,j}$ with $P_{S}$ along the channel dimension. This is followed by two branches: one generates the updated spatial-specific prompt embedding $P_{S} \in \mathbb{R}^{B \times C}$, while the other one uses convolutional layers to combine with the corresponding encoder output, generating the feature map for subsequent layers. Similar to the operation for undersampling-specific prompt embeddings, the updated spatial-specific prompt embedding from each block are concatenated to form the final $P_{S}$ of the current cascade block.

\subsection{Loss Function}
Given the three outputs of UPCMR, the loss function comprises the classification loss $\mathcal{L}_{cls}$ and the reconstruction loss $\mathcal{L}_{rec}$. The $\mathcal{L}_{cls}$ is the sum of the cross-entropy losses for the k-space trajectory class and the acceleration factor class. The $\mathcal{L}_{rec}$ is the weighted sum of an L1 loss term and an SSIM loss term, defined as follows:
\begin{equation}
    \mathcal{L}_{rec} = \lambda_{l1} \| I_{rec} - I_{gnd} \|_1 + \lambda_{ssim}(1-SSIM(|I_{rec}|, |I_{gnd}|)),
\end{equation}
where $I_{rec}$ denotes the reconstructed CMR image sequence and $I_{gnd}$ represents the original ground-truth image sequence, both of which are double-channeled. For the SSIM loss calculation, we use the absolute value to compute the loss. Based on the empirical study from~\cite{loss}, we set $\lambda_{l1}$=0.16 and $\lambda_{ssim}$=0.84. Finally, the overall loss function is as follows:
\begin{equation}
    \mathcal{L} = \lambda_{cls}\mathcal{L}_{cls}+\mathcal{L}_{rec}
\end{equation}
Considering classification as a minor task relative to the main reconstruction task, we set a relatively small weight of $\lambda_{cls}$=0.025.
% The overall pipeline can be summarized as follows:
% \begin{align}
%     &W_{P} = Linear(Concat(GAP(F_{i,j},P_{U})),\\
%     &B_{P} = Linear(Concat(GAP(F_{i,j},P_{U})),\\
%     &F_{i,j+1} = W_{P}*F_{i,j}+B_{P}, P_{T,j} = W_{P}+B_{P}
% \end{align}
\subsection{Training Strategy}
For random undersampling, an efficient training strategy is essential. Given thousands of slices, each with 3$\times$6=18 different undersampling scenarios, iterating through all in each epoch is time-consuming and impractical under the situation of limited GPU resource. Therefore, our policy is to randomly pick a slice with a random k-space trajectory and acceleration factor for each training sample. This approach highly reduces training time and ensures the model can still handle diverse scenarios. 

Another challenge is the varying difficulty of reconstruction across different undersampling modes, where, for example, an acceleration factor of 24 is much more difficult than one of 4. Therefore, equally random sampling might not be an optimal strategy for the model training. Inspired by the concept of curriculum learning~\cite{cl} and its extensive applications~\cite{mio,cnlp}, we explore incorporating this method into our model training process. Firstly, we trained the model for 90 epochs, divided into four stages:
\begin{enumerate}
    \item Train the model for 10 epochs using \{\textit{uniform}\} k-space sampling and an acceleration factor of \{4\}.
    \item Train the model for 20 epochs using \{\textit{uniform, }\textit{gaussian}\} k-space with the sampling probabilities \{0.2, 0.8\} and acceleration factors \{4, 8, 12\} with the sampling probabilities \{0.04, 0.48, 0.48\}.
    \item Train the model for 30 epochs using \{\textit{uniform}, \textit{gaussian}, \textit{pseudo\_radial}\} k-space with the sampling probabilities \{0.1, 0.1, 0.8\} and acceleration factors \{4, 8, 12, 16, 20, 24\} with the sampling probabilities \{0.02, 0.02, 0.03, 0.31, 0.31, 0.31\}.
    \item Train the model for 30 epochs using all the k-space and all the acceleration factors with the equal sampling probability.
\end{enumerate}
In the above 4 stages, we start with easier cases to help the model grasp basic reconstruction principles and adapt to less noisy data. Gradually, we introduce more complex k-space trajectories and higher acceleration factors to increase difficulty. The final stage involves combined training, which further boosts the model's adaptability.

Although the above training strategy aligns with curriculum learning principles, using the same full model structure and expecting it to handle multiple acceleration factors may not be optimal. This approach increases the training difficulty of the UPCMR model, making it harder to learn and differentiate between these internal variations. Moreover, assigning the optimal sampling probabilities at each stage can be challenging, which may hinder the model's overall performance. Therefore, we propose an alternative training strategy applying curriculum learning in a sequential way, divided into 7 stages. In each stage, all three k-space trajectories are equally likely to be included, with a total of 50 training epochs. The key differences across stages lie in the number of cascade blocks in the UPCMR model and the introduction of acceleration factors, as detailed below:
\begin{enumerate}
    \item Initialize the model with 3 cascade blocks and train for 50 epochs with an acceleration factor of \{4\}.
    \item Add a new block to the model (total 4 blocks), train for 40 epochs with an acceleration factor of \{8\}, followed by 10 epochs using acceleration factors \{4, 8\}, with the sampling probabilities of \{1/2, 1/2\}.
    \item Add a new block to the model (total 5 blocks), train for 40 epochs with an acceleration factor of \{12\}, followed by 10 epochs using acceleration factors \{4, 8, 12\}, with the sampling probabilities of \{1/3, 1/3, 1/3\}.
    \item Add a new block to the model (total 6 blocks), train for 40 epochs with an acceleration factor of \{16\}, followed by 10 epochs using acceleration factors \{4, 8, 12, 16\}, with the sampling probabilities of \{1/4, 1/4, 1/4, 1/4\}.
    \item Add a new block to the model (total 7 blocks), train for 40 epochs with an acceleration factor of \{20\}, followed by 10 epochs using acceleration factors \{4, 8, 12, 16, 20\}, with the sampling probabilities of \{1/5, 1/5, 1/5, 1/5, 1/5\}.
    \item Add a new block to the model (total 8 blocks), train for 40 epochs with an acceleration factor of \{24\}, followed by 10 epochs using acceleration factors \{4, 8, 12, 16, 20, 24\}, with the sampling probabilities of \{1/6, 1/6, 1/6, 1/6, 1/6, 1/6\}.
    \item  Train the complete model for 50 epochs using all the k-space trajectories and all the acceleration factors with the equal sampling probability.
\end{enumerate}
In the first 6 stages, the model learns one acceleration factor at a time in the first 40 epochs, followed by 10 epochs of fine-tuning with all learned acceleration factors for comprehensive adjustment. Within each stage, a new cascade block is added to align with the increasing difficulty of the acceleration factor. Additionally, initializing the previous blocks with the trained weights from the last stage, we ensure smoother progression and better model adaptation. In the final stage, the model is trained with all possible scenarios to further refine its performance and ensure robust adaptability. In general, this training strategy follows a sequential approach with respect to model structure and acceleration factors, gradually increasing the model's complexity and training samples' difficulty to align with the principles of curriculum learning, helping the model solidify its understanding of internal differences between various undersampling scenarios.
\section{Experiments}
% Training config, harware
\subsection{Implementation Details}
During training, we generated the coil sensitivity map (CSM) from the time-averaged autocalibration signals (ACS) using the ESPIRIT algorithm~\cite{espirit}. The training data was normalized by first transforming the k-space data to the image domain with the Inverse Fast Fourier Transform (IFFT), normalizing the multi-coil images by their maximum absolute value, and then transforming the data back to k-space domain with the Fast Fourier Transform (FFT). Additionally, to further stabilize the model training, we incorporated z-score normalization and un-normalization at the start and end of each block in the cascade.

The models were implemented in Pytorch and trained on Nvidia A100 GPU with 80GB of GPU memory. To optimize GPU memory usage, we utilized mixed precision training~\cite{mixp}, enabling us to use a UPCMR model with up to 8 blocks. In the first training strategy, we used AdamW as the optimizer with an initial learning rate of 2e-4 and a weight decay of 1e-3. The learning rate was reduced by a factor of 0.8 every 10 epochs. The second training strategy also used AdamW with an initial learning rate of 2e-4 and a weight decay of 1e-2 but introduced a cosine scheduler with a 6-epoch warm-up, and the minimal learning rate was set to 2e-5. In the final stage, the initial learning rate was 1e-4, reduced by a factor of 0.8 every 5 epochs, with a minimum of 8e-6, dropping to 1e-6 in the last epoch. The batch size was set to 1. During training, we split 1380 samples for training and 16 samples for validation. Each validation sample was subjected to 18 different undersampling scenarios and each slice to increase the number of validation samples and better measure the model's overall performance. For evaluation, peak signal-to-noise ratio (PSNR), structural similarity (SSIM) and normalized mean squared error (NMSE) were used as image quality metrics, focusing on the cropped central region of each validation case.

% Experiment 1: ablation
\subsection{Results}

% The former evaluates the effectiveness of a curriculum learning strategy versus a mixed learning strategy, where equal sampling probabilities are assigned to each combination of k-space trajectories and acceleration factors. The latter assesses the impact of three key component blocks, FiLMBlock, TCABlock, and PromptBlock, on the performance of UPCMR under the same curriculum learning strategy. Here, we replace TCABlock with a Conv3D layer to validate its impact. The overall results are presented in Table~\ref{tab1}.
\begin{table}
\centering
\caption{Results of the methods on the central cropped area of validation set from the leaderboard. $\text{CL}_{1}$ refers to the first training strategy, while $\text{CL}_{2}$ is the second one. The best result is highlighted in bold.}\label{tab1}
\begin{tabular}{l|l|l|l}
\hline
Method &  PSNR $\uparrow$ & SSIM $\uparrow$ & NMSE $\downarrow $\\
\hline
ZF &  19.4182 & 0.4610 & 0.2853\\
SENSE &  24.4780 & 0.6200 & 0.1734\\
GRAPPA & 24.9678 & 0.6449 & 0.1554\\
\hline
 UPCMR w/o $\text{CL}$ & 25.3694 & 0.6854 & 0.1422\\
 UPCMR-$\text{CL}_1$ w/o FiLMBlock & 25.2132 & 0.6755 &0.1484\\
 UPCMR-$\text{CL}_1$ w/o TCABlock & 25.0871 & 0.6718 &0.1487\\
 UPCMR-$\text{CL}_1$ w/o PromptBlock & 25.4463 & 0.6858 &0.1409\\
 UPCMR-$\text{CL}_{1}$ & 25.6070 & 0.6904 & 0.1391 \\
 UPCMR-$\text{CL}_{2}$ & \textbf{26.6361} & \textbf{0.7338} &\textbf{0.1036}\\
 \hline
\end{tabular}
\end{table}
% \\
% Furthermore, under the same curriculum learning strategy, an ablation study is conducted to evaluate the impact of key component bocks, FiLMBlock, TCABlock and PromptBlock, on the UPCMR's performance, as shown in Table~\ref{tab2}.
% \begin{table}
% \centering
% \caption{Results of the ablation study on the component blocks. For the validation of the TCABlock, we replaced it with a Conv3D layer. The best result is highlighted in bold.}\label{tab2}
% \begin{tabular}{c|l|l|l}
% \hline
% Method &  PSNR $\uparrow$ & SSIM $\uparrow$ & NMSE $\downarrow $\\
% \hline
% UPCMR w/o FiLMBlock & 25.2132 & 0.6755 & 0.1484\\
% UPCMR w/o TCABlock & 25.0871 & 0.6718 & 0.1487\\
% UPCMR w/o PromptBlock & 25.4463 & 0.6858 & 0.1409\\
% UPCMR & \textbf{25.6070} & \textbf{0.6904} & \textbf{0.1391}\\
% \hline
% \end{tabular}
% \end{table}
% \\
\begin{figure}[tbh!]
\centering
\includegraphics[width=\textwidth]{./pics/vis_new.png}
\caption{Visualization of UPCMR reconstruction results for four contrasts under two k-space trajectories with an acceleration factor of 20. 'GND' indicates images from fully-sampled k-space, 'ZF' denotes images from zero-filled undersampled k-space, 'G' refers to Gaussian k-space, and 'R' stands for pseudo radial k-space. Images are cropped to focus on the central cardiac region. Rows from top to bottom show Aorta (sagittal), Cine (LAX), Mapping (T1), and Tagging.} \label{fig2}
\end{figure}
We compare the proposed model with two traditional methods, SENSE and GRAPPA, on the given validation set. Some ablation studies evaluate (1) the effectiveness of curriculum learning versus a combined learning strategy, where equal sampling probabilities are assigned to each combination of k-space trajectories and acceleration factors in the whole training procedure, (2) the impact of key components, FiLMBlock, TCABlock, and PromptBlock, on UPCMR's performance under the first curriculum learning strategy, TCABlock is replaced with a Conv3D layer to assess its effect, and (3) the relative performance of two different curriculum learning strategies. Results are summarized in Table~\ref{tab1}. Then Fig.~\ref{fig2} visualizes the reconstruction performance of UPCMR using the second curriculum learning strategy under some challenging undersampling scenarios from the splitting validation set.
% Experiment 2: training strategy
% TODO


\section{Discussion and Conclusion}

% Discussion:  focus on explaining the current results and on limitations+future work
% 1. Possible improvements in architecture:  e.g. other kinds of info that can be encoded in the prompt (contrast, mask pattern, etc.)
% 2. On training strategy:
% 3. Better/SOTA baselines to compare with\



% Conclusion
Table~\ref{tab1} shows that each key component block enhances performance, and the curriculum learning strategy outperforms the combined learning approach. Moreover, the second curriculum training strategy has been proven more effective, emphasizing the importance of learning each acceleration factor gradually and making the model size align with it. However, as shown in Fig.~\ref{fig2}, UPCMR struggles with high acceleration factors, resulting in detail loss and blurriness in the cardiac region, indicating room for improvement. For the second curriculum learning strategy, due to its time-consuming nature, we only set 50 training epochs for each stage. Consequently, the model did not fully converge at each stage, limiting the upper bound of final reconstruction performance. Training the UPCMR model with more epochs at each stage could further improve the final performance. Additionally, the UPCMR model may have some design drawbacks. While TCABlock is intended to capture spatio-temporal correlations across the entire sequence, its simple structure and operation might not effectively exploit these features, potentially limiting performance, especially at higher acceleration factors. To further enhance the model, incorporating techniques like convolutional recurrent operations could be considered. Additionally, there is a significant difference between contrasts, which can complicate the reconstruction process. The current UPCMR model doesn't consider about it, and introducing contrast-specific prompts might further enhance the reconstruction performance for each contrast. Finally, considering the imbalance in training samples between contrasts, additional data processing is required to mitigate negative impacts.

% This reflects several potential issues: effective management of difficulty levels in curriculum learning remains challenging, 90 epochs may be insufficient for the model training, and significant contrast differences complicate reconstruction. Additionally, although TCABlock is introduced to exploit the spatio-temporal correlation across the entire sequence, its simple structure may limit performance, particularly at higher acceleration factors. 

% Several factors could contribute to this issue. Despite incorporating curriculum learning, effectively managing the distribution of difficulty levels remains a key challenge. More effective strategies are needed to integrate curriculum learning into the training process. Additionally, the current model training may not be sufficiently extensive, suggesting that 90 epochs is not enough for training. The substantial differences in image contrasts may also pose obstacles for reconstruction. Although TCABlock is introduced to exploit the spatio-temporal correlation across the entire sequence, its simple structure may limit performance, particularly at higher acceleration factors. 

% Based on the discussed factors, we believe that the training strategy is crucial in this task and the current one has limitations. Instead of giving a full model and progressively increasing the difficulty of training samples, a more effective way would be to start with a smaller model (e.g., 3 cascade blocks) and initially train it on the lowest acceleration factor (4). Then gradually add more cascade blocks to handle higher acceleration factors with more trainable parameters. All three k-space trajectory types are included at each stage. Although this approach is much more time-consuming, it allows UPCMR to train more and enhance generalization across various scenarios.

% A more common approach would be to combine the temporal and channel dimension, then apply channel attention to better explore the correlation between these two dimensions. Therefore, there are still some room exploring the attention block to further improve the overall reconstruction performance.

To summarize the contribution of this work, we introduce UPCMR, an unrolled model that integrates two kinds of learnable prompts into the UNet structure within each block. This design leverages the two kinds of prompts to guide the model and exploit spatio-temporal correlations across image sequences, thereby enhancing reconstruction performance and improving adaptability to various random sampling scenarios. Furthermore, we explored two variants of the curriculum learning training strategy and selected the more effective one for versatile random sampling scenarios. Nonetheless, further refinements to the training process and model design are still needed to realize the full potential of UPCMR method. 
\\
\\
\textbf{Acknowledgement.} This work is part of the project ROBUST: Trustworthy AI-based Systems for Sustainable Growth with
project number KICH3.LTP.20.006, which is (partly)
financed by the Dutch Research Council (NWO), Philips Research, and the Dutch Ministry of Economic Affairs
and Climate Policy (EZK) under the program LTP KIC
2020-2023.
% \subsection{A Subsection Sample}
% Please note that the first paragraph of a section or subsection is
% not indented. The first paragraph that follows a table, figure,
% equation etc. does not need an indent, either.

% Subsequent paragraphs, however, are indented.

% \subsubsection{Sample Heading (Third Level)} Only two levels of
% headings should be numbered. Lower level headings remain unnumbered;
% they are formatted as run-in headings.

% \paragraph{Sample Heading (Fourth Level)}
% The contribution should contain no more than four levels of
% headings. Table~\ref{tab1} gives a summary of all heading levels.

% \begin{table}
% \caption{Table captions should be placed above the
% tables.}\label{tab1}
% \begin{tabular}{|l|l|l|}
% \hline
% Heading level &  Example & Font size and style\\
% \hline
% Title (centered) &  {\Large\bfseries Lecture Notes} & 14 point, bold\\
% 1st-level heading &  {\large\bfseries 1 Introduction} & 12 point, bold\\
% 2nd-level heading & {\bfseries 2.1 Printing Area} & 10 point, bold\\
% 3rd-level heading & {\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\
% 4th-level heading & {\itshape Lowest Level Heading.} Text follows & 10 point, italic\\
% \hline
% \end{tabular}
% \end{table}


% \noindent Displayed equations are centered and set on a separate
% line.
% \begin{equation}
% x + y = z
% \end{equation}
% Please try to avoid rasterized images for line-art diagrams and
% schemas. Whenever possible, use vector graphics instead (see
% Fig.~\ref{fig1}).

% \begin{figure}
% \includegraphics[width=\textwidth]{fig1.eps}
% \caption{A figure caption is always placed below the illustration.
% Please note that short captions are centered, while long ones are
% justified by the macro package automatically.} \label{fig1}
% \end{figure}

% \begin{theorem}
% This is a sample theorem. The run-in heading is set in bold, while
% the following text appears in italics. Definitions, lemmas,
% propositions, and corollaries are styled the same way.
% \end{theorem}
% %
% % the environments 'definition', 'lemma', 'proposition', 'corollary',
% % 'remark', and 'example' are defined in the LLNCS documentclass as well.
% %
% \begin{proof}
% Proofs, examples, and remarks have the initial word in italics,
% while the following text appears in normal font.
% \end{proof}
% For citations of references, we prefer the use of square brackets
% and consecutive numbers. Citations using labels or the author/year
% convention are also acceptable. The following bibliography provides
% a sample reference list with entries for journal
% articles~\cite{ref_article1}, an LNCS chapter~\cite{ref_lncs1}, a
% book~\cite{ref_book1}, proceedings without editors~\cite{ref_proc1},
% and a homepage~\cite{ref_url1}. Multiple citations are grouped
% \cite{ref_article1,ref_lncs1,ref_book1},
% \cite{ref_article1,ref_book1,ref_proc1,ref_url1}.


% \begin{credits}
% \subsubsection{\ackname} A bold run-in heading in small font size at the end of the paper is
% used for general acknowledgments, for example: This study was funded
% by X (grant number Y).

% \subsubsection{\discintname}
% It is now necessary to declare any competing interests or to specifically
% state that the authors have no competing interests. Please place the
% statement with a bold run-in heading in small font size beneath the
% (optional) acknowledgments\footnote{If EquinOCS, our proceedings submission
% system, is used, then the disclaimer can be provided directly in the system.},
% for example: The authors have no competing interests to declare that are
% relevant to the content of this article. Or: Author A has received research
% grants from Company W. Author B has received a speaker honorarium from
% Company X and owns stock in Company Y. Author C is a member of committee Z.
% \end{credits}


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\begin{thebibliography}{8}
% \bibitem{ref_article1}
% Author, F.: Article title. Journal \textbf{2}(5), 99--110 (2016)

\bibitem{cl}
Bengio Y, Louradour J, Collobert R, et al.: Curriculum learning. In: Proceedings of the 26th annual international conference on machine learning. 41-48. (2009)

\bibitem{loss}
H. Zhao, O. Gallo, I. Frosio, and J. Kautz: Loss functions for image restoration with neural networks, IEEE Trans. Comput. Imag., vol. \textbf{3}, no. 1, pp. 47–57, Mar. (2017)

\bibitem{se}
Hu, Jie, Li Shen, and Gang Sun: Squeeze-and-excitation networks. In: CVPR, pp. 7132-7141 (2018)

\bibitem{visp}
Jia, M., Tang, L., Chen, B.C., Cardie, C., Belongie, S., Hariharan, B., Lim, S.N.: Visual prompt tuning. In: European Conference on Computer Vision. pp. 709–727. Springer (2022)

\bibitem{wei2022chain}
J. Wei et al.: Chain-of-thought prompting elicits reasoning
in large language models. In: Advances in Neural Information Processing Systems \textbf{35}, 24824–24837 (2022)

\bibitem{kirillov2023segment}
Kirillov A, Mintun E, Ravi N, et al.: Segment anything. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 4015-4026 (2023)

\bibitem{mio}
Kong X, Dong C, Zhang L.: Towards Effective Multiple-in-One Image Restoration: A Sequential and Prompt Learning Strategy. arXiv preprint arXiv:2401.03379. (2024)

\bibitem{ma2024segment}
Ma J, He Y, Li F, et al.: Segment anything in medical images. Nature Communications, \textbf{15}(1): 654 (2024)

\bibitem{mixp}
Micikevicius P, Narang S, Alben J, et al.: Mixed precision training. In: ICLR. (2018)

\bibitem{film}
Perez E, Strub F, De Vries H, et al.: Film: Visual reasoning with a general conditioning layer. In: Proceedings of the AAAI conference on artificial intelligence. \textbf{32}(1). (2018)

\bibitem{potlapalli2024promptir}
Potlapalli, Vaishnav, et al.: Promptir: Prompting for all-in-one image restoration. In: Advances in Neural Information Processing Systems \textbf{36} (2024)

\bibitem{qin2018convolutional}
Qin, Chen, et al.: Convolutional recurrent neural networks for dynamic MR image reconstruction. IEEE TMI \textbf{38}(1), 280-290 (2018)

\bibitem{tcp}
Sandino, Christopher M., et al: Compressed sensing: From research to clinical practice with deep neural networks: Shortening scan times for magnetic resonance imaging. IEEE Signal Process. Mag. \textbf{37}(1), 117-127 (2020)

\bibitem{conv21d}
Sandino, Christopher M., et al: Accelerating cardiac cine MRI using a deep learning-based ESPIRiT reconstruction. Magn. Reson. Med. \textbf{85}(1), 152-167 (2021)

\bibitem{schlemper2017deep}
Schlemper, Jo, et al.: A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE TMI \textbf{37}(2), 491-503 (2017)

\bibitem{sriram2020end}
Sriram, A., Zbontar, J., Murrell, T., Defazio, A., Zitnick, C.L., Yakubova, N., Knoll, F., Johnson, P.: End-to-end variational networks for accelerated mri reconstruction. In: Medical Image Computing and Computer Assisted Intervention–MICCAI 2020: 23rd International Conference, Lima, Peru, October 4–8, 2020, Proceedings, Part II 23. pp. 64–73. Springer (2020) \url{https://doi.org/10.1007/978-3-030-59713-9_7}

\bibitem{admm}
Sun, Jian, Huibin Li, and Zongben Xu.: Deep ADMM-Net for compressive sensing MRI. In: Advances in neural information processing systems \textbf{29} (2016)

\bibitem{espirit}
Uecker M, Lai P, Murphy M J, et al.: ESPIRiT—an eigenvalue approach to autocalibrating parallel MRI: where SENSE meets GRAPPA. Magnetic resonance in medicine, \textbf{71}(3): 990-1001. (2014)

\bibitem{data2024}
Wang Z, Wang F, Qin C, et al.: CMRxRecon2024: A Multi-Modality, Multi-View K-Space Dataset Boosting Universal Machine Learning for Accelerated Cardiac MRI. arXiv preprint arXiv:2406.19043 (2024)

\bibitem{xin2023fill}
Xin, Bingyu, et al.: Fill the k-space and refine the image: Prompting for dynamic and multi-contrast MRI reconstruction. International Workshop on Statistical Atlases and Computational Models of the Heart. 261-273. Cham: Springer Nature Switzerland (2023) \url{. https://doi.org/10.1007/978-3-031-52448-6_25}

\bibitem{cnlp}
Xu B, Zhang L, Mao Z, et al.: Curriculum learning for natural language understanding. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 6095-6104. (2020)

\bibitem{uniseg}
Ye, Y., Xie, Y., Zhang, J., Chen, Z., Xia, Y.: UniSeg: A Prompt-Driven Universal Segmentation Model as Well as A Strong Representation Learner. In: Greenspan, H., et al. Medical Image Computing and Computer Assisted Intervention – MICCAI 2023. MICCAI 2023. Lecture Notes in Computer Science, vol 14222. Springer, Cham. (2023) \url{https://doi.org/10.1007/978-3-031-43898-1\_49}

\bibitem{zhang2022automatic}
Zhang Z, Zhang A, Li M, et al.: Automatic chain of thought prompting in large language models. In: The 11th International Conference on
Learning Representations (ICLR). (2023)
% \bibitem{ref_lncs1}
% Author, F., Author, S.: Title of a proceedings paper. In: Editor,
% F., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.
% Springer, Heidelberg (2016). \doi{10.10007/1234567890}

% \bibitem{ref_book1}
% Author, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,
% Location (1999)

% \bibitem{ref_proc1}
% Author, A.-B.: Contribution title. In: 9th International Proceedings
% on Proceedings, pp. 1--2. Publisher, Location (2010)

% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
\end{thebibliography}
\end{document}
