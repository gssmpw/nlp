\section{Related Work}
\label{sec:related_work_task_repr}
Here, we discuss studies that specifically focus on learning task representations in ICL. A more comprehensive review of related works is available in Appendix \ref{app:related_work}.

Initial studies on developing task representations for transformers are documented in \citep{lampinen_task_repr, shao_2023, mu_2023, panigrahi_2023, ilharco_2023}. These works proposed methods to create compositional task encodings through model perturbations in the parameter space: ``soft'' prompts, codebooks, and meta-mappings. Notably, the term \emph{task vectors} was first introduced in \citep{ilharco_2023}. In contrast, through the application of causal mediation analysis \citep{direct_and_indirect_effects, vig, meng_factual_assoc, wang_2023a, geva_2023}, \textit{function vectors} were discovered to exist inherently within the transformer architecture and to exhibit strong causal effects \citep{fv}. This finding aligns with research on RNNs, where it was demonstrated that RNN hidden states can be grouped based on task similarities \citep{lake_2018, hill_2018}.

Efforts to represent ICL tasks often derive a task vector from the layer activations associated with the dummy token following the input query \citep{icl_creates_task_vectors}. This approach was later refined by averaging the layer activations of dummy tokens across a few-shot prompt and optimizing them \citep{dongfang}. However, these methods were primarily designed for language tasks, where input-output pairs are explicitly separated by specific tokens (e.g., ``\(\rightarrow\)''). This limitation restricts their applicability to broader domains, such as regression tasks. Another approach leverages the first principal component of the difference in layer activations to guide the ICL task, resulting in \textit{In-Context Vectors} (ICVs) \citep{icv}. Notably, attention heads have been argued to play a critical role in transferring information between token positions \citep{transformer, math_of_transformers}. Building on this insight, the \textit{Function Vector} (FV) \citep{fv} is computed as the sum of activations from a specific subset of attention heads, selected based on an \emph{indirect metric} derived from causal inference literature \citep{direct_and_indirect_effects}. In this study, we aim to develop a structured and automated method for extracting function vectors from transformer architectures, expanding on the approach of Todd et al. \citep{fv}.