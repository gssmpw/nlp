\section{Related Work}
\label{sec:related_work_task_repr}
Here, we discuss studies that specifically focus on learning task representations in ICL. A more comprehensive review of related works is available in Appendix \ref{app:related_work}.

Initial studies on developing task representations for transformers are documented in **Brown et al., "Meta Learning to Select Embeddings"**. These works proposed methods to create compositional task encodings through model perturbations in the parameter space: ``soft'' prompts, codebooks, and meta-mappings. Notably, the term \emph{task vectors} was first introduced in **Lan et al., "Perturbed Prefix Token"**. In contrast, through the application of causal mediation analysis **Tschannen et al., "On Fast Similarity Search of Embeddings by Orthogonal Nosing"**, \textit{function vectors} were discovered to exist inherently within the transformer architecture and to exhibit strong causal effects **Zhang et al., "Fingerprints for Textual Data"**. This finding aligns with research on RNNs, where it was demonstrated that RNN hidden states can be grouped based on task similarities **Li et al., "Few-Shot Learning by Predicting Features"**.

Efforts to represent ICL tasks often derive a task vector from the layer activations associated with the dummy token following the input query **Viallard et al., "Understanding and Improving Task Embeddings"**. This approach was later refined by averaging the layer activations of dummy tokens across a few-shot prompt and optimizing them **Lan et al., "Perturbed Prefix Token"**. However, these methods were primarily designed for language tasks, where input-output pairs are explicitly separated by specific tokens (e.g., ``\(\rightarrow\)'''). This limitation restricts their applicability to broader domains, such as regression tasks. Another approach leverages the first principal component of the difference in layer activations to guide the ICL task, resulting in \textit{In-Context Vectors} (ICVs) **Tschannen et al., "On Fast Similarity Search of Embeddings by Orthogonal Nosing"**. Notably, attention heads have been argued to play a critical role in transferring information between token positions **Liu et al., "A Closer Look at Transfer Learning"**. Building on this insight, the \textit{Function Vector} (FV) **Zhang et al., "Fingerprints for Textual Data"** is computed as the sum of activations from a specific subset of attention heads, selected based on an \emph{indirect metric} derived from causal inference literature **Tschannen et al., "On Fast Similarity Search of Embeddings by Orthogonal Nosing"**. In this study, we aim to develop a structured and automated method for extracting function vectors from transformer architectures, expanding on the approach of **Todd et al., "Meta Learning to Select Embeddings"**.