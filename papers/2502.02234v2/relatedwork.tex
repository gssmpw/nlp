\section{Related Work}
\label{sec:sec2}

\subsection{Incomplete Multi-view Clustering}

Existing IMvC approaches can be generally grouped into: traditional IMvC methods~\cite{wen2020adaptive, yin2021incomplete, wang2022highly} and deep IMvC algorithms~\cite{yang2022robust, xu2024deep, yan2024deep}. 

Traditional IMvC methods commonly build upon matrix factorization, kernel learning, and graph learning technologies. Matrix factorization-based approaches derive a view-shared representation across diverse views via the matrix factorization techniques~\cite{hu2019one, wen2023graph, khan2024weighted}. For instance, an adaptive feature weighting manner is inserted into matrix factorization to mitigate the effects of redundant and noisy features on the learned view-shared representation, which is also regularized with a graph-embedded consensus constraint to preserve the structural information inherent in incomplete multi-view data. Kernel-based methods~\cite{9556554, LI2024102086} utilizes a set of per-computed kernels to measure diverse views, tending to formulate a unified kernel through linear or non-linear combinations of predefined kernels to capture the clustering results. Liu et al.~\cite{9556554} jointly conduct incomplete kernel matrices imputation and alignment to capture an advanced clustering representation. Graph-based methods~\cite{yang2024geometric, du2024fast, li2022refining} integrate graph similarities captured from various views via either self-representation~\cite{elhamifar2013sparse} or adaptive neighbor graph learning~\cite{nie2014clustering} manner, ultimately obtaining the clustering results through spectral clustering. Li et al.~\cite{li2022refining} explored the cross-view information to refine the graph structure by leveraging the tensor nuclear norm. Deep learning-based methods~\cite{wen2020dimc, xu2022deep, li2023incomplete} leverage the powerful representation capabilities of deep neural networks to derive consensus clustering results from multi-view data. For example, Lin et al.~\cite{lin2021completer} learned the consensus representation across diverse views by contrastive learning and recovered the missing views by cross-view prediction. Xue et al.~\cite{xue2024robust} developed a multi-graph contrastive regularization to reduce abundant correlations
across multiple views and learn discriminative representations for clustering. 

Previous IMvC methods have made significant strides in improving clustering performance following the main pipeline that imputes missing views and conducts clustering. However, the imputation procedure without the true data distributions inevitably causes inaccurate imputed missing views, which in turn degrade clustering performance. Different from them, we learn a view-common representation only from the observed parts of incomplete multi-view without the imputation procedure. To this end, we propose a mask-informed deep contrastive incomplete multi-view clustering method that reduces the impacts of missing values among different views on formulating a view-common representation from multiple views for clustering.

\subsection{Contrastive Learning}

Contrastive learning is a novel self-supervised learning paradigm that has achieved significant success across various computer vision and machine learning tasks~\cite{krishnan2022self, liu2021self}. Its core principle involves pushing samples away from their negative anchors while pulling samples closer to their positive anchors, thereby creating a discriminative representation for downstream tasks~\cite{khosla2020supervised}. Over the past few years, different contrastive learning approaches have emerged, including MoCo~\cite{he2020momentum}, SimCLR~\cite{chen2020simple}, and SwAV~\cite{caron2020unsupervised}. For a comprehensive overview of additional methods, we refer to the survey in~\cite{gui2024survey}. 

Recently, inspired by the robust feature learning capabilities of self-supervised learning, contrastive loss has been extensively applied in MvC. For instance, Xu et al.\cite{xu2022multi} aligned multi-view information from both high-level semantics and low-level features through contrastive learning, effectively capturing common semantics for clustering. Additionally, the work in\cite{trosten2023effects} explored the effectiveness of self-supervision and contrastive alignment within the multi-view clustering task. Luo et al.~\cite{luo2024simple} first fused multi-view information at the data level and then applied data augmentation for the fused data, making a shared feature extractor with a simple contrastive learning paradigm to capture robust features for clustering. However, previous methods typically treat the same samples from different views as positive pairs, while diverse samples across multiple views are considered negative pairs. This manner lacks the flexibility to leverage sample connection probabilities to enhance feature representation learning. Thus, we propose prior knowledge-assisted contrastive learning to inject the sample connection probabilities from diverse views into the view-common feature representation with a carefully designed re-weighted contrastive loss. In such a manner, the view-common feature representation full of structural information is beneficial for recovering the underlying cluster structure of data.


%------------------------------------------------------------------------
\begin{figure*}[!htbp]
	\centering
	\includegraphics[width = 0.8\linewidth]{IMvC.jpg}
	\caption{Illustration of mask-informed deep contrastive incomplete multi-view clustering (Mask-IMvC). The view-complete parts of IMvC data are first processed through their encoders to extract view-specific latent features. Next, a mask-informed fusion module aggregates the representations into a unified view-common one, which is then used to reconstruct the view complete parts of IMvC data via view-specific decoders. Finally, the prior knowledge from different views is fused via the mask-informed fusion strategy to assist the contrastive learning on the view-common representation.}
	\label{fig:IMvC}
\end{figure*}