\section{Related work}
\label{sec:related_work}
% \noindent Plan for this section
% \begin{enumerate}
%     \item Traditional methods
%     \item state of the art in ETA pred
%     \item state of the art in multi-agent prediction
% \end{enumerate}
 % This section introduces the foundation of \ac{MAPF}, \ac{GNN}, and \ac{STSF} to understand further the design choices of \ac{CAMAGP}.
\subsection{Traditional \ac{MAPF} solutions}
% While the proposed work does not directly solve the \ac{MAPF} problem, it is tightly correlated. Its primary objective is to validate the discovered route by assuming imperfect plan execution and forecasting conflicts along the route. 
 
State-of-the-art \ac{MAPF} methods can calculate collision-free paths for hundreds of agents with sub-optimal guarantees in reasonable amount of computation time\cite{PIBT, push_and_swap}. However, agents frequently have imperfect plan execution capabilities and cannot perfectly synchronize their motions, leading to frequent and time-consuming replanning. A post-processing framework is proposed in \cite{honig2016multi}, using a simple temporal graph to establish a plan-execution scheduler that guarantees safe spacing between robots.
% The work of \cite{honig2016multi} proposes a post-processing framework using a simple temporal graph to establish a plan-execution scheduler that guarantees safe spacing between robots.
The framework exploits the slack in time-constraints to absorb imperfect plan executions and prevent time-intensive replanning.

\subsection{Spatio-temporal graph neural networks}
A Spatio-temporal graph is a combination of spatio representing space or structural information and temporal representing time-varying information. 
Accordingly, if any system consists of a structural relationship between space and time, it can be represented as a spatio-temporal graph. These graphs can be used to design a neural network capable of handling static structures and time-varying characteristics. 
% We can define a spatio-temporal graph as a function of static structure and time-varying features:
% $$G = (V, E, X_{v(t)}, X_{e(t)})$$  
% where $G$ represents the graph, $V$ is the set of nodes, $E$ is the set of edges, $X_{v(t)}$ is the set of node-features at a given time $t$, and $X_{e(t)}$ is the set of edge-features at a given time $t$.

To work with spatio-temporal graphs, we must process a sequence of graph data to build a spatio-temporal embedding that may be used for regression, classification, clustering, or correlation prediction\cite{malla2021social}. The spatial block can be any conventional GNN \cite{battaglia2018relational}, while the temporal block can be any approach for learning over sequences of data, such as temporal convolution\cite{temporal-conv, temporal-conv-2} or temporal attention\cite{temporal-attention}. 
Various spatio-temporal \acp{GNN}  conform to the encoding-processing-decoding paradigm\cite{GOOGLE_ETA,battaglia2018relational, keisler2022forecasting}. This paradigm enables us to accommodate the iterative nature of \ac{STSF} and pathfinding algorithms. 
Improving these alignments has been shown to improve \acp{GNN} generalization to a more diverse distribution of graphs\cite{velivckovic2019neural}.
% 

\subsection{\ac{ETA} prediction and spatio-temporal sequence forecasting}
\label{sec:IMS-vs-DMS}
% Generally, \ac{ETA} predictions fall into the category of the \ac{STSF} problem.
The \ac{STSF} problem aims to predict a sequence of length $L$ into the future based on past observations and auxiliary data. The general learning strategies for multi-step forecasting fall into two major categories: \ac{IMS} estimation and \ac{DMS} estimation\cite{forecast_survey}. The \ac{IMS} strategy trains a one-step-ahead forecasting model and iteratively feeds the generated samples to the one-step-ahead forecaster to produce a multi-step-ahead forecast.
There are two main benefits of the \ac{IMS} strategy:
The one-step-ahead forecaster is simple to train because it only needs to consider the one-step-ahead forecasting error. It can generate predictions of any length by recursively applying the basic forecaster. The work of \cite{DCRNN} demonstrates how accounting for graph structure significantly reduces forecasting error across multiple horizons.
% Improving previous work, \cite{Gman} proposes the graph multi-attention network, a transform attention mechanism to transform the historical traffic features into future representations. This attention mechanism models direct relationships between historical and future time steps to alleviate the problem of error propagation.
% This design of spatial GNN combined with temporal component has yielded several subsequent proposals; STGCN\cite{STGCN}, GaAN\cite{GaAN}, ST-GRAT\cite{STGRAT}, and GMAN\cite{Gman}.
However, there is a disparity between the training and testing phase in \ac{IMS}.
In the training phase, the $L$th-step prediction is generated using the ground truths of the previous $L - 1$ steps. Though, during the testing phase, the model is provided with its previous prediction of the $L - 1$ steps rather than ground truths. This disparity makes the model susceptible to accumulating forecasting errors \cite{bengio2015scheduled}.

The primary objective of \ac{DMS} is to circumvent the accumulated error issue in \ac{IMS} by directly minimizing the long-term prediction error. Rather than training a single model, \ac{DMS} trains a distinct model $m_h$ for each forecasting horizon h. Therefore, the \ac{DMS} approach can support up to $L$ internal models. In order to separate the model size from the number of forecasting steps $L$, we can also construct $m_h$ by recursively applying the one-step-ahead forecaster $m_1$ in order to compute the multi-step-ahead forecasting error. It is worth noting that a multi-step-ahead forecasting error differs from a one-step-ahead forecasting error. Multi-step-ahead forecasting error accumulates the error during the sequence, while one-step-ahead forecasting error only minimizes for one step.
% This recursive \ac{DMS} method is widely adopted in deep learning-based methods including, Google maps\cite{GOOGLE_ETA}, Stg2seq\cite{Stg2seq}, Graph WaveNet\cite{wavenet}, and StemGNN\cite{StemGNN}.
Using \ac{DMS} does not come without cost\cite{chevillon2007direct}. \ac{DMS} is more computationally expensive than \ac{IMS}. For multi-model DMS, $L$ models need to be stored and trained, while recursive \ac{DMS} needs to apply $m_1$ for $\mathcal{O}(L)$ steps. Both cases require a large amount of memory storage or longer training time than solving the \ac{IMS} method. On the other hand, the \ac{IMS} training process is easily parallelizable since each forecasting horizon can be trained independently\cite{Goodfellow-et-al-2016}.

% Papers spatialâ€“temporal graph neural network, GOOGLE_ETA,
In\cite{GOOGLE_ETA}, a homogeneous spatio-temporal \ac{GNN} method for predicting \ac{ETA} is proposed. While the main architecture consists of basic \ac{GNN} building blocks, they propose utilizing a combination of the recursive \ac{DMS} and multi-model \ac{DMS} to improve long horizon predictions. 
% Five models are trained using the recurrent \ac{DMS} for predicting $0$, $10$, $20$, $30$, and $60$ minutes into the futures. The model predicts the time it takes to traverse each node in the system during these five timeslots.
In \cite{huang2022dueta}, a focus on modeling traffic congestion propagation into a congestion-sensitive graph structure is proposed.
% Specifically, instead of directly using the road network as a graph, a congestion-sensitive graph based on the correlations of traffic patterns is introduced.
% In addition, a route-aware graph transformer layer is proposed to directly learn the road segment's long-distance correlations. 
In addition, a route-aware graph transformer layer is proposed enabling \acp{GNN} to capture the interactions between any road segments that are spatially distant but highly correlated with traffic conditions.
A novel heterogeneous graph structure is proposed in \cite{hong2020heteta}, which models both road features and historical data as heterogeneous information. It introduces edge features containing structural information on the relationship between nodes. In addition, three components are introduced to model temporal information from recent periods, daily periods, and weekly periods respectively. Each component comprises temporal convolutions and graph convolutions to learn representations of the spatio temporal heterogeneous information. 

While all the above-mentioned methods generate excellent results, they focus mainly on the features of the road and consider only a single vehicle traversing the graph. None of the models consider other types of vehicles driving in the environment or how the traffic is influenced due to the driver choosing the route. As a result, such models are not easily converted into multi-vehicle solutions.



% you should make comments and comparisons with these papers. One by one, please tell us what they have done and why it is good or bad. What is missing? and what contributions are made after this paper?

%Generally speaking, DMS is preferred for short-term prediction, where the dimensionality of data is very large, and only short-term predictions are required [5], [6]. IMS is preferred for long-term forecasts where the forecasting horizons are long, which are generally above 12 [4], [13] and can be as long as 100 [14].

% \textcolor{red}{Google maps example}
% When looking into how \ac{GNN} \cite{GOOGLE_ETA}



% Mapper
% Graph Neural Networks for Decentralized Multi-Robot Path Planning