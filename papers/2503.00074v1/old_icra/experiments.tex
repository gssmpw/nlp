
% \begin{table*}[b!]
% \centering
% \rowcolors{3}{}{white}
% \caption{Comparison of the model and prediction method.}
% \label{table:comparison}
% \resizebox{\textwidth}{!}{\begin{tabular}{| c | c |c c c|c c c|c c c|c c c|}
% \hline
% \multicolumn{2}{|c|}{Training settings} & \multicolumn{12}{c|}{Test environments with: } \\
% \hline
% \multirow{2}{*}{Methods}  & \multirow{2}{*}{Trained with}
% & \multicolumn{3}{c|}{250 robots}
% & \multicolumn{3}{c|}{500 robots} 
% % & \multicolumn{3}{c|}{750 robots} 
% & \multicolumn{3}{c|}{1000 robots}
% & \multicolumn{3}{c|}{Average} \\ %\cline{3-18}
%                                 &
%                                 &  RMSE & MAPE[\%] & MAE 
%                                 &  RMSE & MAPE[\%] & MAE 
%                                 &  RMSE & MAPE[\%] & MAE 
%                                 % &  RMSE[m] & MAPE[\%] & MAE[rad]
%                                 &  RMSE & MAPE[\%] & MAE\\
% \hline\hline
% \multirow{1}{*}{Naive ($A^*$)} 
%                                 &        
%                                 & 8.06 & 8.36 & 3.59 
%                                 & 13.45 & 12.79 & 6.80 
%                                 & 42.29 & 26.08 & 24.83
%                                 % & NaN & NaN & NaN 
%                                 & 21.27 & 15.74 & 11.74\\
% \hline\hline
% \rowcolor{lightgray}\cellcolor{white}               
%                                 & \cellcolor{white} 250 robots
%                                 & 7.18 & 4.59 & 3.19
%                                 & 12.26 & 8.75 & 5.99
%                                 & 38.36 & 21.15 & 21.81
%                                 % & NaN & NaN & NaN 
%                                 & 19.27 & 11.50 & 10.33 \\
                                
% \cellcolor{white}
%                                 & 500 robots
%                                 & 7.47 & 4.25 & 3.05
%                                 & 11.99 & 8.31 & 5.75
%                                 & 39.15 & 21.27 & 22.28
%                                 % & NaN & NaN & NaN 
%                                 & 19.54 & 11.28 & 10.36 \\
                                
%                                 % & 750 robots
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN 
%                                 % & NaN & NaN & NaN\\
                                
% \rowcolor{lightgray}\cellcolor{white}
% \multirow{-3}{*}{IMS}
%                                 & \cellcolor{white} 1000 robots 
%                                 & 7.36 & 4.53 & 3.15
%                                 & 11.70 & 8.31 & 5.66
%                                 & 37.79 & 20.46 & 21.30
%                                 % & NaN & NaN & NaN 
%                                 & 18.95 & 11.1 & 10.04\\
                                
% % \cellcolor{white}\multirow{-4}{*}{IMS}
% %                                 & 1-1000 robots 
% %                                 & NaN & NaN & NaN
% %                                 & NaN & NaN & NaN
% %                                 & NaN & NaN & NaN
% %                                 % & NaN & NaN & NaN 
% %                                 & NaN & NaN & NaN\\
% \hline \hline
% % \rowcolor{lightgray}
% \cellcolor{white}
%                                 & \cellcolor{white}250 robots
%                                 % RMSE,  MAPE,  MAE
%                                 & \textbf{6.14} & \textbf{3.72} & \textbf{2.52}
%                                 & 9.58 & 7.09 & 4.60
%                                 & 31.33 & 17.17 & 17.03
%                                 % & NaN & NaN & NaN 
%                                 & 15.68 & 9.33 & 8.05 \\
                                
% \rowcolor{lightgray}
% \cellcolor{white}               
%                                 & \cellcolor{white}500 robots
%                                 % RMSE,  MAPE,  MAE
%                                 & 6.38 & 3.78 & 2.60
%                                 & 9.17 & \textbf{6.91} & \textbf{4.44}
%                                 & 28.42 & 16.16 & 15.51
%                                 % & NaN & NaN & NaN 
%                                 & 14.66 & 8.95 & 7.52\\
                                
%                                 % & 750 robots
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN
%                                 % & NaN & NaN & NaN 
%                                 % & NaN & NaN & NaN\\
                                
% % \rowcolor{lightgray}
% \cellcolor{white} 
% \multirow{-3}{*}{DMS}              
%                                 &\cellcolor{white} 1000 robots 
%                                 % RMSE,  MAPE,  MAE
%                                 & 6.37 & 4.03 & 2.70
%                                 & \textbf{9.05} & 7.02 & 4.45
%                                 & \textbf{25.69} & \textbf{15.39} & \textbf{13.93}
%                                 % & NaN & NaN & NaN 
%                                 & \textbf{13.70} & \textbf{8.81} & \textbf{7.03}\\
                                
% % \cellcolor{white}\multirow{-4}{*}{DMS}
% %                                 & 1-1000 robots 
% %                                 & NaN & NaN & NaN
% %                                 & NaN & NaN & NaN
% %                                 & NaN & NaN & NaN
% %                                 % & NaN & NaN & NaN 
% %                                 & NaN & NaN & NaN\\
% \hline

% \end{tabular}}
% \end{table*}

\section{Experimental Setup}
\label{sec:experiments}
This section provides a concise description of the generated training and evaluation datasets, along with the error metrics used for evaluation and training parameters. The experiments focus on the prediction model and the accuracy as this is the varying factor of success. 
\subsection{Dataset}
\begin{figure}[t]
   \centering
   \includegraphics[width=0.49\textwidth]{images/experiments/map_example_v3.pdf}
   \caption{Examples of the randomly generated maps used for training and evaluation. The black pixels depict the occupied spaces, while the white pixels depict the free space.}
   \label{fig:map_examples}
\end{figure}

The GNN is trained on $5000$ unique generated warehouse environments of the size of a maximum $100\times 100$ meter. A few examples of generated maps are displayed in Fig. \ref{fig:map_examples}. The environments are populated with $N=\{250,500,1000\}$ robots, each with a corresponding goal point randomly distributed around the environment. In order to simulate up to $1000$ robot, time is recorded without considering acceleration, therefore moving a pixel will incur a fixed cost in time. The naive path is computed using Dijkstra for global planning and $A^*$ for local planning. For two reasons, the \ac{PIBT} and \ac{CBS} are not chosen as naive path planning methods. First, they modify the path of all robots and not just a single robot. Second, as the simulation is not considering acceleration, this is the perfect scenario for these methods, resulting in no conflicts. After performing the simulation, the actual arrival time is recorded and used as the label.

\subsection{Evaluation criteria}
The loss function used during training is the \ac{MAPE}. \ac{MAPE} computes the percentage between predicted values $\hat{y}$ and ground truth $y$: $MAPE = \frac{1}{m}\sum_{i=1}^{m}| \hat{y_i} - y_i | / y_i.$ It is the most popular metric for \ac{ETA} prediction tasks, as the percentage penalizes the relative distance error, making it robust against outliers compared to \ac{RMSE}, which is strongly penalizing the outliers with big errors.

\subsection{Training parameters}
The training and evaluation are conducted using an NVIDIA GeForce RTX 3090Ti GPU with 24GB VRAM. A high amount of VRAM is proven essential when training as the space required for computing the gradient of long temporal graphs is very high. However, the gradient is not computed during inference time, so a large amount of VRAM is no longer required. Due to the extensive use of VRAM, only a batch size of one is used during training. We use the Adam optimizer, while the learning rate $\gamma=0.001$ was scheduled to decay with $0.75$ every $8$th epoch. Each model has roughly trained 20 epochs, which with our hardware configuration would take $36$ to $48$ hours.
While training takes a long time to complete, the inference time is between $400$ to $700$ ms depending on the number of robots and the length of the paths.


% \subsection{Metrics}
% All comparison methods are evaluated by the metrics below:
% \begin{itemize}
%     \item \textbf{MAPE}: Mean Absolute Percentage Error computes the percentage between predicted values $\hat{y}$ and ground truth $y$: $MAPE = \frac{1}{m}\sum_{i=1}^{m}| \hat{y_i} - y_i | / y_i.$ MAPE is the most popular metric for the ETA tasks since the percentage is easier for people to conceptualize, and it is robust against outliers.
%     \item \textbf{MAE}: Mean Average Error calculates the absolute residual for each data point: $MAE = \frac{1}{m}\sum_{i=1}^{m}|\hat{y_i} - y_i|.$ A smaller MAE suggests the model is better at prediction.
%     \item \textbf{RMSE}: Root Mean Squared Error describes the spread of residuals: $RMSE = \sqrt{\frac{1}{m}\sum_{i=1}^{m}(\hat{y_i} - y_i)^2}.$
% \end{itemize}


