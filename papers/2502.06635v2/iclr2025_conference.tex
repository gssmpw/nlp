
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{multicol}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{arydshln}
\usepackage{xcolor}
\usepackage{pdfpages}
% \usepackage{cite}
\usepackage{CJKutf8}
% \usepackage[UTF8]{ctex}
\usepackage{xspace}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
% \usepackage{geometry}
% \geometry{a4paper, margin=1in}
\usepackage{array}
\usepackage{caption}
\usepackage{float}
\usepackage{tcolorbox}
\usepackage{CJKutf8}
\usepackage{siunitx}
\newcommand{\model}{\textit{Steel-LLM}\xspace}
\usepackage{amsmath}  % 提供数学公式支持
\usepackage{amssymb}  % 提供数学符号（如 \mathbb）
\usepackage{graphicx} % 用于插入图片
\usepackage{subcaption} % 用于子图排列
\usepackage{wrapfig}    % For wrapfigure
\usepackage{wasysym}
\usepackage{authblk}







% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.


\newcommand{\authorinfo}[3]{
  \textbf{#1} \\
  \textit{#2} \\
  \texttt{#3}
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\title{\model: From Scratch to Open Source \\– A Personal Journey in Building a Chinese-Centric LLM}



\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Author block %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{center}
  \begin{minipage}{0.8\textwidth}
    \centering
    \begin{multicols}{2} % Two columns
      % First row
      \authorinfo{Qingshui Gu}{Tsinghua University}{gqs19@tsinghua.org.cn} \\
      \vspace{0.5cm} % Add vertical space between rows
      \authorinfo{Tianyu Zheng}{Beijing University of Posts \\ and Telecommunications}{zhengtianyu@bupt.edu.cn} \\
      
      \columnbreak % Move to the second column
      
      % Second row
      \authorinfo{Shu Li}{Tsinghua University}{lishu14@tsinghua.org.cn} \\
      
      \vspace{0.5cm} % Add vertical space between rows
      \authorinfo{Zhaoxiang Zhang}{Institute of Automation, \\ Chinese Academy of Sciences}{zhaoxiang.zhang@ia.ac.cn}
    \end{multicols}
  \end{minipage}
\end{center}
\vspace{-0ex}
\begin{center}
     \url{https://github.com/zhanshijinwat/Steel-LLM}\\
     % \url{https://hf-mirror.com/gqszhanshijin/Steel-LLM}
\end{center}
\vspace{1ex}
\iclrfinalcopy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Author block %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.



%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
% \begin{document}



\begin{abstract}
\model is a Chinese-centric language model developed from scratch with the goal of creating a high-quality open-source model despite limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. The training process primarily focused on Chinese data, with a small proportion of English data included, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey. \model has demonstrated competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. This paper provides a comprehensive summary of the project's key contributions, including data collection, model design, training methodologies, and the challenges encountered along the way, offering a valuable resource for researchers and practitioners looking to develop their own LLMs.
The model checkpoints and the training script are available at \url{https://github.com/zhanshijinwat/Steel-LLM}.
\end{abstract}

\section{Introduction}

The rapid advancements in open-source large language models (LLMs) have led to significant achievements in natural language processing (NLP), enabling applications ranging from conversational agents to code generation. However, despite these advances, challenges remain in the transparency, accessibility, and resource efficiency of LLM development. Many prominent LLMs, such as the Qwen series~\citep{bai2023qwentechnicalreport,yang2024qwen2}, Llama~\citep{touvron2023llama2openfoundation}, and Deepseek~\citep{deepseekai2024deepseek}, provide only the final model weights while withholding essential details such as training data, code, and intermediate checkpoints. This limited openness creates obstacles to reproducibility and prevents the broader research community from building upon these models effectively.

Several initiatives, such as LLM360's Amber~\citep{liu2023llm360,tan2024llm360}, M-A-P's Neo~\citep{zhang2024map}, and AI2's OLMO series~\citep{OLMo,olmo20242,muennighoff2024olmoe}, have addressed these limitations by releasing complete training pipelines, datasets, and intermediate checkpoints. While these contributions are invaluable, their development typically requires extensive computational resources, making them inaccessible to smaller research teams or individual practitioners. This creates a significant gap for fully transparent and resource-efficient LLMs tailored for smaller-scale research efforts, particularly in non-English languages such as Chinese.

This paper introduces \model, a small, fully open-source, Chinese-centric LLM developed with limited computational resources. Launched in March 2024, the project aimed to train a 1-billion-parameter model on a large-scale dataset, prioritizing transparency and the sharing of practical insights to assist others in the community. Unlike many large-scale projects, \model demonstrates that high-quality LLMs can be developed efficiently while maintaining transparency. This work focuses on the Chinese language, with a small proportion of English data, addressing gaps in existing open-source LLMs by providing a more detailed and practical account of the model-building journey.

\model achieves competitive performance on benchmarks such as CEVAL and CMMLU, outperforming early models from larger institutions. The model architecture incorporates innovative features such as Soft Mixture of Experts (Soft MoE) and an enhanced Feed-Forward Network, optimizing performance within resource constraints. Our training framework, modified from TinyLlama~\citep{zhang2024tinyllamaopensourcesmalllanguage}, includes several optimizations for efficiency and usability, such as improved model loading, restoration of training progress, and the ability to append data during the training process.

The contributions of this work are as follows:

\begin{itemize}
    \item \textbf{Resource-Efficient Model Development}: We present a 1-billion-parameter model trained primarily on Chinese data, with a small proportion of English, addressing the need for more diverse language representation in open-source LLMs. By leveraging limited computational resources (8 GPUs), we demonstrate that high-quality LLMs can be developed without access to large-scale infrastructure.
    
    \item \textbf{Complete Transparency}: We offer complete transparency in our development process, including the release of our training pipeline, dataset, model architecture, and intermediate checkpoints. This facilitates reproducibility and allows for further research by the broader community.
    
    \item \textbf{Practical Guidance for Small-Scale Research}: We provide detailed insights into our model architecture, training framework, and data preparation process, offering practical guidance for researchers and practitioners with limited resources. This includes optimizations for training efficiency, such as mixed-precision training, FlashAttention, and operator fusion.
    
    \item \textbf{Benchmark Performance}: \model achieves competitive performance on Chinese benchmarks such as CEVAL and CMMLU, outperforming some early models from larger institutions. This demonstrates the effectiveness of our approach in developing a high-quality Chinese-centric LLM with limited resources.
\end{itemize}

By prioritizing resource efficiency, openness, and practical applicability, this work contributes to the broader LLM research community, offering a valuable resource for replicating or extending similar efforts with fewer computational constraints. All model checkpoints, training scripts, and related resources are fully open-sourced, further promoting transparency and collaboration in the field of LLM development.






\section{Related Works}
Recent developments in open source LLM have varied widely in terms of transparency and accessibility. Many models, such as Qwen~\citep{bai2023qwentechnicalreport,yang2024qwen2}, Llama~\citep{touvron2023llama2openfoundation}, Deepseek~\citep{deepseekai2024deepseek}, Gemma ~\citep{gemmateam2024gemmaopenmodelsbased}, InternLM ~\citep{cai2024internlm2}, Mixtral ~\citep{jiang2023mistral}, Yi ~\citep{ai2024yiopenfoundationmodels}, GLM ~\citep{glm2024chatglm}, have been released with only the final model checkpoints and weights, while withholding crucial details such as training data, codes, and intermediate checkpoints. This limited transparency hinders reproducibility and makes it difficult for the broader research community to fully understand or build upon these models.

In response to these challenges, several initiatives have adopted a more open approach by releasing complete training pipelines, datasets, and intermediate checkpoints. Notable examples include LLM360’s Amber~\citep{liu2023llm360, tan2024llm360}, M-A-P’s MAP-Neo~\citep{zhang2024map}, and AI2’s OLMO series~\citep{OLMo, olmo20242, muennighoff2024olmoe}, all of which offer comprehensive resources, including training code, model weights, and intermediate checkpoints. While these contributions are invaluable to the field, the large-scale nature of these models necessitates significant computational resources, which may render them inaccessible to smaller research teams or individual practitioners. Consequently, there remains a gap for open-source LLMs that are both fully transparent and feasible for smaller teams to develop and deploy.

This paper addresses this need by presenting a small, fully open-source, Chinese-centric LLM. Designed to minimize resource requirements, our model offers a complete end-to-end solution for researchers with limited computational access. By sharing the full training pipeline, dataset, and model architecture, we aim to make the development of high-quality LLMs more accessible. This work emphasizes transparency and practical guidance, providing a valuable resource for those seeking to replicate or extend the model with fewer computational constraints.






\section{Architecture}
The model structure of \model is adapted from Qwen(\cite{bai2023qwentechnicalreport}).A Transformer block can be roughly divided into self-attention and Feed-Forward Network (FFN). An efficient implementation of self-attention is Flash Attention(\cite{dao2022flashattentionfastmemoryefficientexact}), which is widely utilized. Flash Attention not only improves the efficiency of model training and inference but also saves GPU memory. \model reuses Flash Attention and only makes improvements to the FFN layer. In the FFN layer, we adopts Soft Mixture of Experts (Soft MoE, \cite{puigcerver2024sparsesoftmixturesexperts}) and  enhances the second layer of the MLP. The architecture of \model's Transformer block is illustrated in \autoref{fig:steelllm_arch}.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/arch.pdf}
    \caption{The architecture of \model Transformer block}
    \label{fig:steelllm_arch}
\end{figure}


\subsection{Soft MoE}

Mixture of Experts (MoE)~\citep{6797059} was first proposed in 1991 and is widely used in the field of recommendation systems ~\citep{ma2018modeling}. In the architecture of large language models, sparse MoE is commonly employed, such as in deepSeekMoE(\cite{dai2024deepseekmoeultimateexpertspecialization}), Qwen-MoE(\cite{qwen_moe}), etc. A typical practice to construct an MoE language model usually replaces FFNs in a Transformer with MoE layers(\cite{lepikhin2021gshard};\cite{9835248};\cite{fedus2022switchtransformersscalingtrillion}). An MoE layer consists of multiple experts, and each expert is a FFN. A gating network calculates scores to assign each token to a small subset of experts. Under the condition of the same parameter scale, the sparse MoE model activates fewer parameters than a dense model, thus reducing the number of FLOPs.

\model is trained using only 8 GPUs with limited GPU memory. For a model featuring sparse MoE structure, all parameters must be loaded into the GPU memory. Owing to the sparse nature of expert selection, in contrast to a dense model, the FFN parameters of the sparse MoE model cannot be fully trained. \model is a small-scale language model with 1 billion parameters, leading to a relatively low computational load. Meanwhile, our objective is to fully train each parameter and leverage the advantages of the MoE structure to enhance model performance. Consequently, we ultimately opt for the soft MoE(\cite{puigcerver2024sparsesoftmixturesexperts}) structure. Soft MoE is fully differentiable. Therefore, there is no need to consider problems such as expert imbalance that exist in sparse MoE.

We denote the input tokens for a single sequence as \(\mathbf{X} \in \mathbb{R}^{m \times d}\), where \(m\) represents the number of tokens and \(d\) denotes their dimensions. In the soft MoE layer, each expert processes  \(p\) slots, and each slot has a corresponding  d-dimensional learnable parameter matrix \(\Phi\in\mathbb{R}^{d\times(n\cdot p)}\). The number of slots is a crucial hyperparameter for adjusting the time complexity of the soft MoE layer. The input slots \(\tilde{\mathbf{X}}\in\mathbb{R}^{(n\cdot p)\times d}\) are obtained through convex combinations of all the \(m\) input tokens, which can be computed as follows:

\[\mathbf{D}_{ij}=\frac{\exp((\mathbf{X}\boldsymbol{\Phi})_{ij})}{\sum_{i^{\prime}=1}^m\exp((\mathbf{X}\boldsymbol{\Phi})_{i^{\prime}j})},\quad\tilde{\mathbf{X}}=\mathbf{D}^\top\mathbf{X}\]

We denote \(D\) as the dispatch weight matrix. It is obtained by applying the softmax function column-wise to the matrix  \(\mathbf{X}\mathbf{\Phi}\). Then, the corresponding expert function is applied to each slot
(i.e., on rows of \(\tilde{\mathbf{X}}\)) to obtain the output slots \(\tilde{\mathbf{Y}}_i=f_{\lfloor i/p\rfloor}(\tilde{\mathbf{X}}_i)\). The combination process is then carried out as follows:
\[\mathbf{C}_{ij}=\frac{\exp((\mathbf{X}\boldsymbol{\Phi})_{ij})}{\sum_{j^{\prime}=1}^{n\cdot p}\exp((\mathbf{X}\boldsymbol{\Phi})_{ij^{\prime}})},\quad\mathrm{~}\mathbf{Y}=\mathbf{C}\tilde{\mathbf{Y}}\]
We refer to \(C\) as the combine weights, which is the result of applying the softmax function row-wise to the matrix \(\mathbf{X}\mathbf{\Phi}\). The output tokens \(Y\) are computed as a convex combination of all \((n\cdot p)\) output slots.

\subsection{Enhanced Feed-Forward Network}

Since \model employs the soft MoE approach, within this framework, the Feed-Forward Network (FFN) effectively represents an expert. In a vanilla Transformer, The FFN comprises two layers of Multi-Layer Perceptrons (MLPs). In the architecture of large language models, a  prevalent strategy is to enhance the first layer of the MLP using the SwiGLU(\cite{shazeer2020gluvariantsimprovetransformer}) activation function(\cite{bai2023qwentechnicalreport};\cite{touvron2023llamaopenefficientfoundation};\cite{deepseekai2024deepseekv2strongeconomicalefficient}). The SwiGLU activation function enhances the model's non-linear representational capabilities, thereby improving its performance. Additionally, we extended the application of the SwiGLU activation function to the second layer of the MLP within the FFN.
% 公式？？？

Regarding other architectural elements, \model adopts the modifications made by Qwen(\cite{bai2023qwentechnicalreport}) to the transformer block. These modifications are widely used in open-source models such as LLama, Mixtral, and Deepseek.

\begin{wraptable}{r}{0.32\textwidth}
\vspace{-3pt}
\centering
\begin{tabular}{@{}lc@{}}
\toprule
Parameters & Value \\
\midrule
Layers & 18 \\

% Num\_attention\_heads & 32 \\
% Num\_key\_value\_heads & 32 \\
Heads & 32 \\
KV heads & 32 \\
Num\_experts & 6 \\
Slots\_per\_expert & 1 \\
Hidden size & 1,792 \\
Intermediate size & 1,792 \\
Vocab size & 151,936 \\
\bottomrule
\end{tabular}
\caption{Key model parameters.}
\vspace{-20pt}
\label{tab:modelparameter}
\end{wraptable}

 \textbullet\ \textbf{Positional embedding}.We intend to use the Rotary Position Embedding (RoPE)(\cite{su2023roformerenhancedtransformerrotary}) for \model. RoPE is a relative position encoding technique. Its core idea is to encode absolute position information into a rotation matrix, thereby representing the relative position relationships among tokens. During the training process, we adopt a global training precision of BF16, while for RoPE, we employ local FP32 precision.

 \textbullet\ \textbf{Bias}. Most layers of \model have no bias, except for the QKV layer(\cite{chowdhery2022palmscalinglanguagemodeling}).

 \textbullet\ \textbf{Pre-Norm \& RMSNorm}. Pre-normalization improves training stability compared to
post-normalization. We normalize the inputs of self-attention layers and FFN layers with RMSNorm(\cite{zhang2019rootmeansquarelayer}).

The hyperparameters of \model are presented in \autoref{tab:modelparameter}.  We employ the Qwen1.5 tokenizer (\cite{qwen1.5}), which utilizes byte pair encoding (BPE).



\section{Training Framework}
\model is trained using only 8 NVIDIA A100/H800 GPUs, so it is essential to maximize the training speed. Moreover, considering the potential reuse of our training code by others, we have made some usability optimizations.


Our pretraining framework, which is modified from TinyLlama(\cite{zhang2024tinyllamaopensourcesmalllanguage}), has undergone the following optimizations.

\textbullet\ \textbf{Model loading}. In the open-source community, the file format of LLM usually follows the standards of the Transformers(\cite{wolf-etal-2020-transformers}) library and it can be easily loaded through the AutoModelForCausalLM.from\_pretrained function. To enhance usability and facilitate model loading for different architectures, our framework supports the Transformer library's model loading method. The original framework, by contrast, was designed for training standard Llama architecture.

\textbullet\ \textbf{Training Progress Restoration}. In the training process, we are required to save not only the checkpoints of both the model and the optimizer but also the training progress of the data. The original framework's method of saving the training step is the simplest for recording data progress, provided that there is no data change (no addition or deletion). This limitation restricts the flexibility of data organization during the training process. We choose to serialize the entire data management class PackedDatasetIterator using the pickle library and then save it. This includes the file names of all training data, the index of each data piece, etc.

\textbullet\ \textbf{Appending Data During the Training Process}.
Training LLMs typically spans dozens of days. During this training, appending new data is a common practice. The simplest implementation approach is to train the newly added data at the end. Nevertheless, to safeguard against the significant distributional disparities between new and old data that could impact the model's training performance, we have devised a method for re-shuffling the indices of both the newly appended data and the hitherto untrained data from the old dataset. Additionally, to avert the inadvertent repeated addition of data files to the training process, \model has incorporated a function that utilizes a hash value, specifically the MD5(\cite{RFC1321}) hash algorithm, to detect data content duplication.

% 为了提高训练速度以及节约显存，在训练时我们使用了bf16混合精度，FSDP以及flash attention等技术. 算子融合是另一项训练优化手段，将多步计算融合在一起，减少中间激活以及内存访问，其可以通过cuda或者triton实现。训练时我们使用了cuda版本rope以及triton版本的loss函数。我们消融了这些技术对训练效率和显存的影响，如表x所示。
To improve training speed and conserve GPU memory, during the training process, we utilized techniques such as bfloat16 mixed-precision(\cite{kalamkar2019studybfloat16deeplearning}), Fully Sharded Data Parallel (FSDP, \cite{zhao2023pytorchfsdpexperiencesscaling}), and FlashAttention(\cite{dao2022flashattentionfastmemoryefficientexact}). Operator fusion represents an additional training optimization approach. By integrating multiple computational steps, it reduces intermediate activations and memory access, and can be implemented via CUDA or Triton. Specifically, during training, we adopted the CUDA-based version of Rotary Position Embedding (RoPE,\cite{su2023roformerenhancedtransformerrotary}) and the Triton-based(\cite{10.1145/3315508.3329973}) cross entropy loss function. We then conducted an ablation study using a micro-batch size of 8 and a 1.8-billion-parameter model on NVIDIA A100 GPU to analyze the impact of these techniques on training efficiency and GPU memory usage, as presented in Table \ref{table:table2}. Employing all training acceleration techniques, the training speed can be enhanced by approximately 50\%.
\begin{table}[!ht]
    \centering
    \begin{tabular}{@{}lcccccccc@{}}
    \toprule
        ~ & Exp 1 & Exp 2 & Exp 3 & Exp 4 & Exp 5 & Exp 6 & Exp 7 & Exp 8 \\ 
        \midrule
        FlashAttention & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ \\ 
        SelfAttention(PyTorch) & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ \\ 
        RoPE(CUDA) & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ \\
        RoPE(PyTorch) & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ \\ 
        RMSNorm(CUDA) & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ \\
        RMSNorm(PyTorch) & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ \\ 
        Loss Function(Triton) & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\times$ \\ 
        Loss Function(PyTorch) & $\times$ & $\times$ & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\checkmark$ \\
        FSDP & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ & $\checkmark$ & $\times$ & $\checkmark$ \\ 
        FSDP(no share param) & $\times$ & $\times$ & $\times$ & $\checkmark$ & $\times$ & $\times$ & $\checkmark$ & $\times$ \\
        Speed(tokens/s/gpu) & 13400 & 12500 & 10600 & 13800 & 14600 & 13000 & 15000 & 10500 \\ 
        GPU Memory(GB) & 65 & 65 & 69 & 69 & 61 & 75 & 66 & 75 \\ 
    \bottomrule
    \end{tabular}
\caption{Comparison of different training configurations.}
\label{table:table2}
\end{table}


\section{Pretraining}
The pretraining corpus employed in our study predominantly consists of Chinese texts and is entirely derived from open-source datasets. It includes prominent datasets such as SkyPile-150B ~\citep{wei2023skywork}, Wanjuan1.0~\citep{he2023wanjuan}, Wikipedia-cn, as well as diverse chat data from multiple sources and Starcode. Further details of these datasets are provided in \autoref{App:pretrain_data}.

\begin{wrapfigure}{lr}{0.445\textwidth}
    \centering
    \vspace{-7pt}
    \includegraphics[width=0.42\textwidth]{fig/pie_chart_pretraining.pdf}
    \caption{Pretraining Data Distribution}
    \label{fig:data_dis}
    \vspace{-10pt}
\end{wrapfigure}%

To ensure consistency across the data, we initially standardized all datasets into a uniform format. Subsequently, we utilized Alibaba's open-source tool, Data-Juicer, to meticulously filter and transcribe both the text and code data. We utilized a total of 21 text processing operators, as described in \autoref{App:operators}, and 13 code processing operators, as listed in \autoref{App:code_processing}. 
The final step involved employing the tokenizer from Qwen1.5. This tokenizer was instrumental in converting the entire corpus into token-ids, which were then merged and segmented into manageable chunks. The distribution of the pretraining data across different domains is illustrated in \autoref{fig:data_dis}. Despite these efforts, our data preprocessing workflow still exhibits certain limitations, particularly the lack of balance in the proportion of texts from different domains within the pre-training corpus.


Over a span of 30 days, \model was trained through 1.07 million steps. The initial 200,000 steps utilized NVIDIA 8 A100-80G GPUs for training, while the remaining steps employed 8 H800-80G GPUs. The loss curve is shown in \autoref{App:loss_curve}. We set the maximum sequence length to 2048, the batch size to 8, and the number of gradient accumulation steps to 8. Consequently, \model was trained on approximately one trillion tokens.  We employ the AdamW optimizer (\cite{kingma2017adammethodstochasticoptimization}) with hyper-parameters set to $\beta_1$= 0.9, $\beta_2$= 0.95, and weight\_decay = 0.05. The maximum learning rate is set to $3\times10^{-4}$, and the gradient clipping norm is set to 1.0. We employed a cosine-annealing learning rate schedule with 2,000 warmup steps, such that the final learning rate is 0.





\section{Post training}
\subsection{Supervised Finetuning}
There has been substantial discussion and some debate among researchers regarding the volume of datasets required for supervised fine-tuning. For instance, \cite{zhou2023limaalignment} advocates for selecting a few hundred to a few thousand high-quality data points for fine-tuning, while \cite{ji2023exploringimpactinstructiondata} suggests that using a significantly larger amount of data can yield better results. Given the capabilities of our base model, we determined that augmenting the fine-tuning phase with additional data was necessary. This strategy not only aids the model in mastering conversational techniques but also enriches it with supplementary knowledge. Consequently, We curated the following fine-tuning datasets: Infinity-Instruct Chinese data~\citep{InfinityInstruct2024} (approximately 700,000 entries, with all English data removed), multiple-choice questions from the Wanjuan-cn dataset~\citep{he2023wanjuan} which tested in both CoT and non-CoT formats, Ruozhiba data~\citep{better-ruozhiba}, self-awareness data with template modified from~\citep{2024EmoLLM}, and three English datasets: Code-Feedback~\citep{zheng2025opencodeinterpreterintegratingcodegeneration}, WebInstructSub~\citep{yue2024mammoth2} and OpenHermes-2.5~\citep{OpenHermes}. Further details are listed in \autoref{App:sft_data}.

We fine-tuned the pre-trained \model for approximately 4 epochs using the SFT dataset. The global batch size was set to 256, the maximum learning rate was set to $2 \times 10^{-5}$, and a cosine-annealing learning rate schedule was employed. The loss curve is shown in \autoref{App:sft_loss_curve}. Below, we present a series of ablation studies and evaluations to analyze the impact of different fine-tuning strategies on model performance.

\subsection{Ablation Studies and Evaluation}

To systematically evaluate the effectiveness of our fine-tuning approach, we conducted several experiments with varying data compositions and formats. The results are summarized in \autoref{tab:sft_results}, and the key findings are discussed below.

\begin{table}[ht]
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{@{}lccc@{}}
        \toprule
        \textbf{Experiment} & \textbf{CEVAL Acc.} & \textbf{CMMLU Acc.} & \textbf{MMLU Acc.} \\
        \midrule
        Full Infinity-Instruct + Wanjuan MCQ & 32.35 & 26.32 & 25.50 \\
        700K Chinese Infinity-Instruct + Wanjuan MCQ & 38.57 & 33.48 & 23.26 \\
        Chinese + 100\% English Data & 39.21 & 33.20 & 26.73 \\
        Chinese + 20\% English Data (Balanced) & 40.43 & 35.86 & 26.75 \\
        Chinese + 20\% English + English MCQ & 41.90 & 36.08 & 30.82 \\
        \bottomrule
    \end{tabular}}
    \caption{Performance of different fine-tuning strategies.}
    \label{tab:sft_results}
\end{table}

\paragraph{Experiment 1: Full Infinity-Instruct + Wanjuan MCQ}
In this experiment, we fine-tuned the model using the entire Infinity-Instruct dataset (approximately 7,000,000 entries) and the full Wanjuan multiple-choice dataset. While the model's performance on CEVAL improved with increasing fine-tuning steps, the accuracy plateaued at around 32\% after 18,000 steps. This suggests that even though the Wanjuan data was included in the pretraining phase, the model still benefited from additional fine-tuning, likely due to its relatively small size.

\paragraph{Experiment 2: 700K Chinese Infinity-Instruct + Wanjuan MCQ}
To address the issue of language mismatch, we filtered out the English data from Infinity-Instruct and retained only the 700,000 Chinese entries. This adjustment significantly improved the model's performance, achieving 38\% accuracy on CEVAL and 33\% on CMMLU. This result highlights the importance of aligning the fine-tuning data distribution with the pretraining phase.

\paragraph{Experiment 3: Chinese + 100\% English Data}
To explore the impact of English data on the model's multilingual capabilities, we fine-tuned the model using a balanced mix of Chinese and English data (340,000 entries each). While the model's performance on Chinese benchmarks remained stable, its performance on MMLU improved slightly, indicating that English data can complement the model's existing knowledge without degrading its Chinese capabilities.

\paragraph{Experiment 4: Chinese + 20\% English Data (Balanced)}
In this experiment, we fine-tuned the model with a data distribution that closely matched the pretraining phase (80\% Chinese and 20\% English). This approach not only improved the model's performance on Chinese benchmarks (CEVAL: 39.21\% → 40.43\%; CMMLU: 33.2\% → 35.86\%) but also maintained its performance on MMLU. This suggests that maintaining a balanced data distribution during fine-tuning is crucial for preserving the model's multilingual capabilities.

\paragraph{Experiment 5: Chinese + 20\% English + English MCQ}
To further enhance the model's reasoning abilities, we introduced additional English multiple-choice questions from datasets such as OpenBookQA, AI2 ARC, and LogiQA. This experiment resulted in a noticeable improvement in MMLU accuracy (26.75\% → 30.82\%), while the performance on Chinese benchmarks remained stable. This indicates that incorporating domain-specific question-answering data can enhance the model's reasoning capabilities without compromising its existing knowledge.


\subsection{Learning from Human Preferences}
To align \model with human preferences, we employ the Direct Preference Optimization (DPO)\cite{rafailov2024directpreferenceoptimizationlanguage} algorithm and sorted response pairs for model optimization.

In the preference dataset, the ratio of Chinese to English stands at 4:1, consistent with that in the pre-training phase. The Chinese dataset is derived from ultrafeedback-chinese\footnote{\url{https://huggingface.co/datasets/opencsg/UltraFeedback-chinese/}}, and the English dataset is derived from ultrafeedback-binarized-preferences\footnote{\url{https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences}}. Both the reference model and the objective model of the DPO algorithm are initialized with the SFT version of \model. We conducted training on the data for 3 epochs. The global batch size was set to 128, the maximum learning rate was set to $5 \times 10^{-6}$, the pref\_beta set to 0.1, and a cosine-annealing learning rate schedule was employed. The loss curve is shown in \autoref{App:dpo_loss_curve}.


\subsection{Discussion}

Our ablation studies reveal several key insights:
\begin{itemize}
    \item \textbf{Data Distribution Matters}: Fine-tuning with a data distribution that closely matches the pretraining phase leads to better performance on both Chinese and English benchmarks.
    \item \textbf{Small Models Benefit from Additional Data}: Even for small models, fine-tuning with a larger dataset can improve performance, particularly when the pretraining data is limited.
    \item \textbf{Balanced Multilingual Fine-Tuning}: Incorporating a small proportion of English data during fine-tuning can enhance the model's multilingual capabilities without degrading its performance on Chinese tasks.
    \item \textbf{Exam-Style Data Enhances Performance}: Including domain-specific question-answering data, such as multiple-choice questions, can improve the model's reasoning abilities and overall benchmark performance.
\end{itemize}

To contextualize the performance of \model, we compare it with several state-of-the-art models on the CEVAL and CMMLU benchmarks, as shown in \autoref{tab:ceval_cmmlu}. Our best-performing model, \model-Chat, achieves competitive results, outperforming models of similar scale such as Tiny-Llama-1.1B and Gemma-2b-it, and approaching the performance of larger models like CT-LLM-SFT-2B. While \model-Chat does not yet match the performance of significantly larger models like Qwen1.5-1.8B-Chat or Qwen-7B, its results demonstrate the effectiveness of our resource-efficient approach, particularly given the limited computational resources used for training.

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{CEVAL} & \textbf{CMMLU} \\
\midrule
Tiny-Llama-1.1B \citep{zhang2024tinyllamaopensourcesmalllanguage} & 25.02 & 24.03 \\
MiniCPM-1.2B \citep{minicpm2024} & 49.14 & 46.81 \\
Qwen1.5-1.8B-Chat \citep{qwen} & 56.84 & 54.11 \\
Phi2(2B)\citep{phi2}  & 23.37 & 24.18 \\
Gemma-2b-it \citep{gemma_2024} & 32.30 & 33.07 \\
CT-LLM-SFT-2B~\citep{Du2024ChineseTL} & 41.54 & 41.48 \\
ChatGLM-6B \citep{glm2024chatglm} & 38.90 & 37.48 \\
Llama2-7B \cite{touvron2023llama2openfoundation} & 32.42 & 31.11 \\
OLMo-7B \citep{Groeneveld2023OLMo} & 35.18 & 35.55 \\
Gemma-7B \citep{gemma_2024}& 42.57 & 44.20 \\
MAP-Neo-7B \citep{zhang2024mapneo} & 56.97 & 55.01 \\
Llama2-13B \cite{touvron2023llama2openfoundation} & 37.32 & 37.06 \\
\midrule
\model-1B-Chat & 41.90 & 36.08 \\
\model-1B-Chat-DPO & 42.04 & 36.04 \\
\bottomrule
\end{tabular}
\caption{Performance comparison of models on CEVAL and CMMLU benchmarks.}
\label{tab:ceval_cmmlu}
\end{table}

In conclusion, our fine-tuning approach demonstrates that careful dataset selection and composition can significantly enhance the performance of small language models like \model. By balancing the inclusion of diverse data types and maintaining alignment with the pretraining distribution, we achieved competitive results on both Chinese and English benchmarks, positioning \model as a strong contender among resource-efficient LLMs.

 


\section{Conclusion}

This paper introduces \model, a fully open-source Chinese-centric language model developed with limited computational resources, achieving competitive performance on benchmarks such as CEVAL (41.90\%) and CMMLU (36.08\%). By leveraging innovative techniques like Soft Mixture of Experts and enhanced feed-forward networks, along with systematic training optimizations, we demonstrate that high-quality LLMs can be built efficiently. Our work provides complete transparency, releasing the training pipeline, datasets, and intermediate checkpoints, offering practical guidance for small-scale LLM development. All resources are made publicly available to foster collaboration and advance accessible language technologies.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\clearpage
\appendix

\section{Pretraining Data Detailed Description}
\label{App:pretrain_data}
\renewcommand{\arraystretch}{1.5} % 调整行间距，使内容更居中

\begin{table}[!ht]
    \centering
    \begin{tabular}{@{}m{3cm}m{11cm}@{}}
    \toprule
        \textbf{Dataset} & \textbf{Description} \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/Skywork/SkyPile-150B/tree/main/data}{SkyPile-150B} \\ \citep{wei2023skywork}}
         & Consisting of approximately 150 billion tokens and 620 gigabytes of cleaned text data from 233 million web pages, with rigorous filtering and deduplication to ensure quality and mitigate sensitive and biased information. \\ 
    \midrule
        \makecell{\href{https://opendatalab.org.cn/OpenDataLab/WanJuan1_dot_0?source=Q1NETg}{Wanjuan1.0} \\ \citep{he2023wanjuan}} & Composed of processed data from various sources, including web pages, encyclopedias, books, patents, textbooks, and exam questions, with a total volume of data exceeding 500 million documents, amounting to over 1TB (roughly split equally between Chinese and English data) and has undergone meticulous cleaning, deduplication, and value alignment. \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/pleisto/wikipedia-cn-20230720-filtered}{Wikipedia-cn}} & Based on the July 20th, 2023 Chinese Wikipedia dump, retains 254,574 high-quality entries after filtering out special types, low-quality, sensitive, and controversial content, and includes conversions between simplified and traditional Chinese. \\ 
    \midrule    
        \makecell{\href{https://huggingface.co/datasets/xuqinyang/BaiduBaike-5.63M}{Baidu Baike}} & Consisting of 5,630,000 uncleaned entries from Baidu Baike, with a total size of approximately 17GB. \\ 
    \midrule    
        \makecell{\href{https://aistudio.baidu.com/datasetdetail/107726}{Baidu QA}} & Including 1.5 million high-quality encyclopedia questions and answers, spanning 492 categories, with 434 categories occurring at least 10 times, suitable for training intelligent Q\&A systems \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/wangrui6/Zhihu-KOL}{Zhihu QA}} & Including 1 million entries of questions and answers, with 1.5GB in size.\\ 
    \midrule
        \makecell{\href{https://github.com/LianjiaTech/BELLE/tree/main/data/10M}{BELLE} \\ \citep{BELLE}} & Including train\_2M\_CN and train\_3.5M\_CN, which are generated by ChatGPT, containing 2 million and 3.5 million dialogue entries respectively, and both used in this project. Note that these datasets are unverified and may contain errors. \\ 
    \midrule
        \makecell{\href{https://hf-mirror.com/datasets/YeungNLP/moss-003-sft-data}{Moss}\\ \citep{Sun2024MOSS}} & Containing 1.1 million Chinese and English multi-turn dialogue entries. \\ 
    \midrule
        \makecell{\href{https://hf-mirror.com/datasets/YeungNLP/firefly-train-1.1M}{Firefly} \\ \citep{Firefly}} & Comprising 1.15 million entries which cover 23 common Chinese NLP tasks and include culturally relevant data such as couplets, poetry, classical Chinese translations, prose, and Jin Yong's novels, resulting in a total of 1.15 million entries. \\ 
    \midrule
        \makecell{\href{https://hf-mirror.com/datasets/bigcode/starcoderdata}{Starcode} \\ \citep{li2023starcoder} } & Including 783GB of code across 86 programming languages, with 54GB of GitHub Issues, 13GB of Jupyter notebooks, and 32GB of GitHub commits. Our project used only the C++, Python, and Java data. \\ 
    \bottomrule
    \end{tabular}
\caption{Pretraining Data Detailed Description}
\label{table:pretrain_dataset_info}
\end{table}

\clearpage


\section{Supervised Finetuning Data Detailed Description}
\label{App:sft_data}

\begin{table}[!ht]
    \centering
    \begin{tabular}{@{}m{3cm}m{11cm}@{}}
    \toprule
        \textbf{Dataset} & \textbf{Description} \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/BAAI/Infinity-Instruct}{Infinity-Instruct-7M}\\ \citep{InfinityInstruct2024}} & A large-scale, high-quality instruction dataset with only 0.7M of Chinese data used in this project. \\ 
    \midrule
        \makecell{\href{https://zhida.zhihu.com/search/3645185472052319285?zhida_source=entity}{Wanjuan1.0} \\ \citep{he2023wanjuan}} & Consistent with the one used during the pre-training stage, but with the Chinese choice question data repurposed for fine-tuning. \\ 
    \midrule
        \makecell{\href{https://github.com/FunnySaltyFish/Better-Ruozhiba}{Ruozhiba} \\ \citep{better-ruozhiba}} & Questions from Baidu Tieba ``Ruozhiba" were answered by GPT-4, then manually reviewed and edited for formatting errors and improved responses. \\ 
    \midrule
        \makecell{\href{https://github.com/SmartFlowAI/EmoLLM/tree/main}{Self-awareness Dataset}\\ \citep{2024EmoLLM}} & Consisting of various ``Who are you?" questions from the EmoLLM project templates. \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/m-a-p/Code-Feedback}{Code-Feedback} \\ \citep{zheng2025opencodeinterpreterintegratingcodegeneration}} & A code SFT dataset consists of 66,000 entries from various open-source code datasets and LeetCode, after undergoing a series of filtering and selection processes. \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/TIGER-Lab/WebInstructSub}{WebInstructSub} \\ \citep{yue2024mammoth2}} & Containing 2.33 million SFT entries across fields such as mathematics, physics, biology, chemistry, and computer science. \\ 
    \midrule
        \makecell{\href{https://huggingface.co/datasets/teknium/OpenHermes-2.5}{OpenHermes-2.5}\\ \citep{OpenHermes}} & Consisting of samples synthesized by large models and chat data, filtered from open-source data like Airoboros, ChatBot Arena, and Evol Instruct, totaling 1 million entries. \\ 
    \bottomrule
    \end{tabular}
\caption{Supervised Finetuning Data Detailed Description}
\label{table:sft_dataset_info}
\end{table}




\clearpage
% Data Juicer Operators Used for Text Processing
\renewcommand{\arraystretch}{1} % 调整行间距，使内容更居中

\section{Data Juicer Operators Used for Text Processing}
\label{App:operators}
\begin{table}[!ht]
    \centering
    \begin{tabular}{@{}p{4.8cm}p{7.5cm}p{3cm}@{}}
    \toprule
        \textbf{Operator} & \textbf{Description} & \makecell{\textbf{Note}} \\ 
    \midrule
        \multirow{2}{*}{chinese\_convert\_mapper} & Converts Chinese between Traditional Chinese, Simplified Chinese and Japanese Kanji & Mode: t2s (tradition to simple) \\
    \midrule
        \multirow{1}{*}{clean\_email\_mapper} & Removes email information & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{clean\_html\_mapper} & Removes HTML tags and returns plain text of all the nodes & \makecell{-} \\ 
    \midrule
        \multirow{1}{*}{clean\_ip\_mapper} & Removes IP addresses & \makecell{-} \\ 
    \midrule
        \multirow{1}{*}{clean\_links\_mapper} & Removes links, such as those starting with http or ftp & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{clean\_copyright\_mapper} & Removes copyright notice at the beginning of code files (must contain the word copyright) & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{expand\_macro\_mapper} & Expands macros usually defined at the top of TeX documents & \makecell{-} \\ 
    \midrule
        \multirow{1}{*}{fix\_unicode\_mapper} & Fixes broken Unicodes & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{punctuation\_normalization\_mapper} & Normalizes various Unicode punctuations to their ASCII equivalents & \makecell{-} \\ 
    \midrule
        \multirow{1}{*}{remove\_repeat\_sentences\_mapper} & Remove repeat sentences in text samples & \makecell{Ignore special char-\\acter and sentences \\ shorter than 2 will\\not be deduplicated} \\ 
    \midrule
        \multirow{1}{*}{remove\_specific\_chars\_mapper} & Removes any user-specified characters or substrings & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{whitespace\_normalization\_mapper} & Normalizes various Unicode whitespaces to the normal ASCII space (U+0020) & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{alphanumeric\_filter} & Keeps samples with alphanumeric ratio within the specified range & \makecell{[0.0, 0.9]} \\ 
    \midrule
        \multirow{2}{*}{average\_line\_length\_filter} & Keeps samples with average line length within the specified range & \makecell{[10, 150]} \\ 
    \midrule
        \multirow{2}{*}{character\_repetition\_filter} & Keeps samples with char-level n-gram repetition ratio within the specified range & \makecell{[0.0, 0.4]} \\ 
    \midrule
        \multirow{2}{*}{maximum\_line\_length\_filter} & Keeps samples with maximum line length within the specified range & \makecell{1000} \\ 
    \midrule
        \multirow{2}{*}{perplexity\_filter} & Keeps samples with perplexity score below the specified threshold & \makecell{1500} \\ 
    \midrule
        \multirow{2}{*}{special\_characters\_filter} & Keeps samples with special-char ratio within the specified range & \makecell{[0.0, 0.25]} \\ 
    \midrule
        \multirow{2}{*}{text\_length\_filter} & Keeps samples with total text length within the specified range & \makecell{[10, 100000]} \\ 
    \midrule
        \multirow{2}{*}{word\_repetition\_filter} & Keeps samples with word-level n-gram repetition ratio within the specified range & \makecell{[0.0, 0.5]} \\ 
    \midrule
        \multirow{2}{*}{document\_simhash\_deduplicator} & Deduplicates samples at document-level using SimHash & \makecell{Tokenization:space; \\ window\_size:6; \\ num\_blocks:6; \\ hamming\_distance:4; \\ lowercase:true} \\ 
    \bottomrule
    \end{tabular}
\caption{Data Juicer Operators Used for Text Processing}
\label{table:data_juicer_operators_text}
\end{table}

\clearpage

% Data Juicer Operators Used for Code Processing
\section{Data Juicer Operators Used for Code Processing}
\renewcommand{\arraystretch}{1.5} % 调整行间距，使内容更居中

\label{App:code_processing}
\begin{table}[!ht]
    \centering
    \begin{tabular}{@{}p{5cm}p{7cm}p{3.3cm}@{}}
    \toprule
        \textbf{Operator} & \textbf{Description} & \makecell{\textbf{Note}} \\ 
    \midrule
        \multirow{2}{*}{\makecell{clean\_copyright\_mapper}} & Removes copyright notice at the beginning of code files (must contain the word copyright) & \makecell{-} \\ 
    \midrule
        clean\_email\_mapper & Removes email information & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{\makecell{clean\_links\_mapper}} & Removes links, such as those starting with http or ftp & \makecell{-} \\ 
    \midrule
        fix\_unicode\_mapper & Fixes broken Unicodes & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{\makecell{punctuation\_normalization\_mapper}} & Normalizes various Unicode punctuations to their ASCII equivalents & \makecell{-} \\ 
    \midrule
        \multirow{2}{*}{\makecell{alphanumeric\_filter}} & Keeps samples with alphanumeric ratio within the specified range & \makecell{[0.546, 3.65]} \\ 
    \midrule
        \multirow{2}{*}{\makecell{average\_line\_length\_filter}} & Keeps samples with average line length within the specified range & \makecell{[10, 150]} \\ 
    \midrule
        \multirow{2}{*}{\makecell{character\_repetition\_filter}} & Keeps samples with char-level n-gram repetition ratio within the specified range & \makecell{0.36} \\ 
    \midrule
        \multirow{2}{*}{\makecell{maximum\_line\_length\_filter}} & Keeps samples with maximum line length within the specified range & \makecell{1000} \\ 
    \midrule
        \multirow{2}{*}{\makecell{text\_length\_filter}} & Keeps samples with total text length within the specified range & \makecell{96714} \\ 
    \midrule
        \multirow{2}{*}{\makecell{words\_num\_filter}} & Keeps samples with word count within the specified range & \makecell{[20,6640]} \\ 
    \midrule
        \multirow{2}{*}{\makecell{word\_repetition\_filter}} & Keeps samples with word-level n-gram repetition ratio within the specified range & \makecell{[10, 0.357]} \\ 
    \midrule
        \multirow{2}{*}{\makecell{document\_simhash\_deduplicator}} & Deduplicates samples at document-level using SimHash & \makecell{Tokenization:space; \\ window\_size:6; \\ num\_blocks:6; \\ hamming\_distance:4; \\ lowercase:true} \\ 
    \bottomrule
    \end{tabular}
\caption{Data Juicer Operators Used for Code Processing}
\label{table:data_juicer_operators}
\end{table}

\clearpage

\section{Pre-training Loss Curve}
\label{App:loss_curve}
The loss curve during the pre-training stage is shown in \autoref{fig:wandb_loss}.  The initial 200,000 steps utilized NVIDIA 8 A100-80G GPUs for training, while the remaining steps employed 8 H800-80G GPUs.


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/loss_curves.pdf}
    \caption{Pre-training loss curve for \model}
    \label{fig:wandb_loss}
\end{figure}

\section{Supervised Finetuning Loss Curve}
\label{App:sft_loss_curve}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/sft_loss_curve.pdf}
    \caption{Supervised Fine-tuning loss curve for \model}
    \label{fig:sft_loss}
\end{figure}

\section{Direct Preference Optimization Loss Curve}
\label{App:dpo_loss_curve}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{fig/dpo_loss_curve.pdf}
    \caption{Direct Preference Optimization loss curve for \model}
    \label{fig:dpo_loss}
\end{figure}

\end{document}

