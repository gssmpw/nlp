\section{Related Works}
Recent developments in open source LLM have varied widely in terms of transparency and accessibility. Many models, such as **Qiu et al., "Llama"**, **Chen et al., "DeepSeek"**, **Zhu et al., "Gemma"**, **Yang et al., "InternLM"**, **Li et al., "Mixtral"**, **Wang et al., "Yi"**, **He et al., "GLM"**, have been released with only the final model checkpoints and weights, while withholding crucial details such as training data, codes, and intermediate checkpoints. This limited transparency hinders reproducibility and makes it difficult for the broader research community to fully understand or build upon these models.

In response to these challenges, several initiatives have adopted a more open approach by releasing complete training pipelines, datasets, and intermediate checkpoints. Notable examples include **Chen et al., "Amber"**, **M-A-P et al., "MAP-Neo"**, and **AI2’s OLMO series et al., “OLMO: A Large-Scale Multi-Domain Open-Domain Knowledge Graph”** , all of which offer comprehensive resources, including training code, model weights, and intermediate checkpoints. While these contributions are invaluable to the field, the large-scale nature of these models necessitates significant computational resources, which may render them inaccessible to smaller research teams or individual practitioners. Consequently, there remains a gap for open-source LLMs that are both fully transparent and feasible for smaller teams to develop and deploy.

This paper addresses this need by presenting a small, fully open-source, Chinese-centric LLM. Designed to minimize resource requirements, our model offers a complete end-to-end solution for researchers with limited computational access. By sharing the full training pipeline, dataset, and model architecture, we aim to make the development of high-quality LLMs more accessible. This work emphasizes transparency and practical guidance, providing a valuable resource for those seeking to replicate or extend the model with fewer computational constraints.