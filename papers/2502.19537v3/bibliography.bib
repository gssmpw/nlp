@misc{huang2025virusharmfulfinetuningattack,
      title={Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation}, 
      author={Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu},
      year={2025},
      eprint={2501.17433},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2501.17433}, 
}

@misc{leong2024devilsalikeunveilingdistinct,
      title={No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks}, 
      author={Chak Tou Leong and Yi Cheng and Kaishuai Xu and Jian Wang and Hanlin Wang and Wenjie Li},
      year={2024},
      eprint={2405.16229},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.16229}, 
}

@misc{peng2024navigatingsafetylandscapemeasuring,
      title={Navigating the Safety Landscape: Measuring Risks in Finetuning Large Language Models}, 
      author={ShengYun Peng and Pin-Yu Chen and Matthew Hull and Duen Horng Chau},
      year={2024},
      eprint={2405.17374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.17374}, 
}

@misc{qi2024evaluatingdurabilitysafeguardsopenweight,
      title={On Evaluating the Durability of Safeguards for Open-Weight LLMs}, 
      author={Xiangyu Qi and Boyi Wei and Nicholas Carlini and Yangsibo Huang and Tinghao Xie and Luxi He and Matthew Jagielski and Milad Nasr and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2412.07097},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2412.07097}, 
}

@misc{
hsiung2025your,
title={Your Task May Vary: A Systematic Understanding of Alignment and Safety Degradation when Fine-tuning {LLM}s},
author={Lei Hsiung and Tianyu Pang and Yung-Chen Tang and Linyue Song and Tsung-Yi Ho and Pin-Yu Chen and Yaoqing Yang},
year={2025},
url={https://openreview.net/forum?id=vQ0zFYJaMo}
}

@misc{yi2024nlsrneuronlevelsafetyrealignment,
      title={NLSR: Neuron-Level Safety Realignment of Large Language Models Against Harmful Fine-Tuning}, 
      author={Xin Yi and Shunfan Zheng and Linlin Wang and Gerard de Melo and Xiaoling Wang and Liang He},
      year={2024},
      eprint={2412.12497},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.12497}, 
}

@misc{wu2025separatewheatchaffposthoc,
      title={Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models}, 
      author={Di Wu and Xin Lu and Yanyan Zhao and Bing Qin},
      year={2025},
      eprint={2412.11041},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.11041}, 
}

@inproceedings{
yi2025probe,
title={Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models},
author={Biao Yi and Tiansheng Huang and Sishuo Chen and Tong Li and Zheli Liu and Zhixuan Chu and Yiming Li},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=EbxYDBhE3S}
}

@misc{zhu2024lockingfinetunedllmssafety,
      title={Locking Down the Finetuned LLMs Safety}, 
      author={Minjun Zhu and Linyi Yang and Yifan Wei and Ningyu Zhang and Yue Zhang},
      year={2024},
      eprint={2410.10343},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10343}, 
}

@misc{huang2024antidotepostfinetuningsafetyalignment,
      title={Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning}, 
      author={Tiansheng Huang and Gautam Bhattacharya and Pratik Joshi and Josh Kimball and Ling Liu},
      year={2024},
      eprint={2408.09600},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.09600}, 
}

@misc{hsu2025safelorasilverlining,
      title={Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models}, 
      author={Chia-Yi Hsu and Yu-Lin Tsai and Chih-Hsun Lin and Pin-Yu Chen and Chia-Mu Yu and Chun-Ying Huang},
      year={2025},
      eprint={2405.16833},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16833}, 
}

@misc{yi2024safetyrealignmentframeworksubspaceoriented,
      title={A safety realignment framework via subspace-oriented model fusion for large language models}, 
      author={Xin Yi and Shunfan Zheng and Linlin Wang and Xiaoling Wang and Liang He},
      year={2024},
      eprint={2405.09055},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.09055}, 
}

@misc{casper2024defendingunforeseenfailuremodes,
      title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training}, 
      author={Stephen Casper and Lennart Schulze and Oam Patel and Dylan Hadfield-Menell},
      year={2024},
      eprint={2403.05030},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.05030}, 
}

@misc{choi2024safetyawarefinetuninglargelanguage,
      title={Safety-Aware Fine-Tuning of Large Language Models}, 
      author={Hyeong Kyu Choi and Xuefeng Du and Yixuan Li},
      year={2024},
      eprint={2410.10014},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10014}, 
}

@misc{li2025salorasafetyalignmentpreservedlowrank,
      title={SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation}, 
      author={Mingjie Li and Wai Man Si and Michael Backes and Yang Zhang and Yisen Wang},
      year={2025},
      eprint={2501.01765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.01765}, 
}

@misc{
li2025safety,
title={Safety Alignment Shouldn't Be Complicated},
author={Jianwei Li and Jung-Eun Kim},
year={2025},
url={https://openreview.net/forum?id=9H91juqfgb}
}

@misc{shen2024sealsafetyenhancedalignedllm,
      title={SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection}, 
      author={Han Shen and Pin-Yu Chen and Payel Das and Tianyi Chen},
      year={2024},
      eprint={2410.07471},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.07471}, 
}

@misc{li2025safetylayersalignedlarge,
      title={Safety Layers in Aligned Large Language Models: The Key to LLM Security}, 
      author={Shen Li and Liuyi Yao and Lan Zhang and Yaliang Li},
      year={2025},
      eprint={2408.17003},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2408.17003}, 
}

@misc{eiras2025isafelymitigatingtaskspecific,
      title={Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models}, 
      author={Francisco Eiras and Aleksandar Petrov and Philip H. S. Torr and M. Pawan Kumar and Adel Bibi},
      year={2025},
      eprint={2406.10288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10288}, 
}

@misc{lyu2025keepingllmsalignedfinetuning,
      title={Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates}, 
      author={Kaifeng Lyu and Haoyu Zhao and Xinran Gu and Dingli Yu and Anirudh Goyal and Sanjeev Arora},
      year={2025},
      eprint={2402.18540},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18540}, 
}

@misc{wang2024mitigatingfinetuningbasedjailbreak,
      title={Mitigating Fine-tuning based Jailbreak Attack with Backdoor Enhanced Safety Alignment}, 
      author={Jiongxiao Wang and Jiazhao Li and Yiquan Li and Xiangyu Qi and Junjie Hu and Yixuan Li and Patrick McDaniel and Muhao Chen and Bo Li and Chaowei Xiao},
      year={2024},
      eprint={2402.14968},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.14968}, 
}

@misc{huang2024harmfulfinetuningattacksdefenses,
      title={Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey}, 
      author={Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu},
      year={2024},
      eprint={2409.18169},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2409.18169}, 
}

@misc{poppi2025understandingfragilitymultilingualllms,
      title={Towards Understanding the Fragility of Multilingual LLMs against Fine-Tuning Attacks}, 
      author={Samuele Poppi and Zheng-Xin Yong and Yifei He and Bobbie Chern and Han Zhao and Aobo Yang and Jianfeng Chi},
      year={2025},
      eprint={2410.18210},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.18210}, 
}

@misc{hawkins2024effectfinetuninglanguagemodel,
      title={The effect of fine-tuning on language model toxicity}, 
      author={Will Hawkins and Brent Mittelstadt and Chris Russell},
      year={2024},
      eprint={2410.15821},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.15821}, 
}

@inproceedings{yi-etal-2024-vulnerability,
    title = "On the Vulnerability of Safety Alignment in Open-Access {LLM}s",
    author = "Yi, Jingwei  and
      Ye, Rui  and
      Chen, Qisi  and
      Zhu, Bin  and
      Chen, Siheng  and
      Lian, Defu  and
      Sun, Guangzhong  and
      Xie, Xing  and
      Wu, Fangzhao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.549/",
    doi = "10.18653/v1/2024.findings-acl.549",
    pages = "9236--9260",
    abstract = "Large language models (LLMs) possess immense capabilities but are susceptible to malicious exploitation. To mitigate the risk, safety alignment is employed to align LLMs with ethical standards. However, safety-aligned LLMs may remain vulnerable to carefully crafted jailbreak attacks, but these attacks often face high rejection rates and limited harmfulness. In this paper, we expose the vulnerabilities of safety alignment in open-access LLMs, which can significantly enhance the success rate and harmfulness of jailbreak attacks. Through reverse alignment, achieved by accessing model parameters, we show the feasibility of efficiently fine-tuning LLMs to undermine their inherent safeguards. We investigate two types of reverse alignment techniques: reverse supervised fine-tuning (RSFT) and reverse preference optimization (RPO). RSFT operates by supervising the fine-tuning of LLMs to reverse their inherent values. We also explore how to prepare data needed for RSFT. RPO optimizes LLMs to enhance their preference for harmful content, reversing the models' safety alignment. Our extensive experiments reveal that open-access high-performance LLMs can be adeptly reverse-aligned to output harmful content, even in the absence of manually curated malicious datasets. Our research acts as a whistleblower for the community, emphasizing the need to pay more attention to safety of open-accessing LLMs. It also underscores the limitations of current safety alignment approaches and calls for research on robust safety alignment methods to counteract malicious fine-tuning attacks."
}

@misc{lermen2024lorafinetuningefficientlyundoes,
      title={LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B}, 
      author={Simon Lermen and Charlie Rogers-Smith and Jeffrey Ladish},
      year={2024},
      eprint={2310.20624},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.20624}, 
}
@misc{yang2023shadowalignmenteasesubverting,
      title={Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models}, 
      author={Xianjun Yang and Xiao Wang and Qi Zhang and Linda Petzold and William Yang Wang and Xun Zhao and Dahua Lin},
      year={2023},
      eprint={2310.02949},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.02949}, 
}

@misc{BugcrowdOpenAI,
  author = {{Bugcrowd}},
  title = {OpenAI Bug Bounty Program},
  year = {2025},
  url = {https://bugcrowd.com/engagements/openai},
  note = {Accessed: 2025-01-31}
}

@misc{OpenAI_FineTuning,
  author       = {OpenAI},
  title        = {Fine-tuning models},
  year         = {2024},
  url          = {https://platform.openai.com/docs/guides/fine-tuning},
  note         = {Accessed: 2025-01-30}
}


@misc{inan2023llamaguardllmbasedinputoutput,
      title={Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations}, 
      author={Hakan Inan and Kartikeya Upasani and Jianfeng Chi and Rashi Rungta and Krithika Iyer and Yuning Mao and Michael Tontchev and Qing Hu and Brian Fuller and Davide Testuggine and Madian Khabsa},
      year={2023},
      eprint={2312.06674},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.06674}, 
}

@article{peng2023instruction,
  title={Instruction Tuning with GPT-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@inproceedings{gehman-etal-2020-realtoxicityprompts,
    title = "{R}eal{T}oxicity{P}rompts: Evaluating Neural Toxic Degeneration in Language Models",
    author = "Gehman, Samuel  and
      Gururangan, Suchin  and
      Sap, Maarten  and
      Choi, Yejin  and
      Smith, Noah A.",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.301/",
    doi = "10.18653/v1/2020.findings-emnlp.301",
    pages = "3356--3369",
    abstract = "Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning {\textquotedblleft}bad{\textquotedblright} words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining."
}

@misc{welbl2021challengesdetoxifyinglanguagemodels,
      title={Challenges in Detoxifying Language Models}, 
      author={Johannes Welbl and Amelia Glaese and Jonathan Uesato and Sumanth Dathathri and John Mellor and Lisa Anne Hendricks and Kirsty Anderson and Pushmeet Kohli and Ben Coppin and Po-Sen Huang},
      year={2021},
      eprint={2109.07445},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.07445}, 
}

@article{dong2024building,
  title={Building guardrails for large language models},
  author={Dong, Yi and Mu, Ronghui and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Zhao, Xingyu and Meng, Jie and Ruan, Wenjie and Huang, Xiaowei},
  journal={arXiv preprint arXiv:2402.01822},
  year={2024}
}

@inproceedings{covert_malicious_finetuning,
author = {Halawi, Danny and Wei, Alexander and Wallace, Eric and Wang, Tony and Haghtalab, Nika and Steinhardt, Jacob},
title = {Covert malicious finetuning: challenges in safeguarding LLM adaptation},
year = {2025},
publisher = {JMLR.org},
abstract = {Black-box finetuning is an emerging interface for adapting state-of-the-art language models to user needs. However, such access may also let malicious actors undermine model safety. To demonstrate the challenge of defending finetuning interfaces, we introduce covert malicious finetuning, a method to compromise model safety via finetuning while evading detection. Our method constructs a malicious dataset where every individual datapoint appears innocuous, but finetuning on the dataset teaches the model to respond to encoded harmful requests with encoded harmful responses. Applied to GPT-4, our method produces a finetuned model that acts on harmful instructions 99\% of the time and avoids detection by defense mechanisms such as dataset inspection, safety evaluations, and input/output classifiers. Our findings question whether black-box finetuning access can be secured against sophisticated adversaries.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {687},
numpages = {15},
location = {Vienna, Austria},
series = {ICML'24}
}

@misc{lv2024adappaadaptivepositionprefill,
      title={AdaPPA: Adaptive Position Pre-Fill Jailbreak Attack Approach Targeting LLMs}, 
      author={Lijia Lv and Weigang Zhang and Xuehai Tang and Jie Wen and Feng Liu and Jizhong Han and Songlin Hu},
      year={2024},
      eprint={2409.07503},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2409.07503}, 
}

@misc{du2024securetuningmitigatingsecurity,
      title={Towards Secure Tuning: Mitigating Security Risks Arising from Benign Instruction Fine-Tuning}, 
      author={Yanrui Du and Sendong Zhao and Jiawei Cao and Ming Ma and Danyang Zhao and Fenglei Fan and Ting Liu and Bing Qin},
      year={2024},
      eprint={2410.04524},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.04524}, 
}

@misc{eiras2024mimickinguserdatamitigating,
      title={Mimicking User Data: On Mitigating Fine-Tuning Risks in Closed Large Language Models}, 
      author={Francisco Eiras and Aleksandar Petrov and Phillip H. S. Torr and M. Pawan Kumar and Adel Bibi},
      year={2024},
      eprint={2406.10288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10288}, 
}

@misc{huang2024lisalazysafetyalignment,
      title={Lisa: Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning Attack}, 
      author={Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu},
      year={2024},
      eprint={2405.18641},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.18641}, 
}

@misc{lyu2024keepingllmsalignedfinetuning,
      title={Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates}, 
      author={Kaifeng Lyu and Haoyu Zhao and Xinran Gu and Dingli Yu and Anirudh Goyal and Sanjeev Arora},
      year={2024},
      eprint={2402.18540},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18540}, 
}

@misc{wei2024assessingbrittlenesssafetyalignment,
      title={Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications}, 
      author={Boyi Wei and Kaixuan Huang and Yangsibo Huang and Tinghao Xie and Xiangyu Qi and Mengzhou Xia and Prateek Mittal and Mengdi Wang and Peter Henderson},
      year={2024},
      eprint={2402.05162},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05162}, 
}

@inproceedings{10.5555/3692070.3694674, author = {Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy}, title = {Safety fine-tuning at (almost) no cost: a baseline for vision large language models}, year = {2025}, publisher = {JMLR.org}, abstract = {Current vision large language models (VLLMs) exhibit remarkable capabilities yet are prone to generate harmful content and are vulnerable to even the simplest jailbreaking attacks. Our initial analysis finds that this is due to the presence of harmful data during vision-language instruction fine-tuning, and that VLLM fine-tuning can cause forgetting of safety alignment previously learned by the underpinning LLM. To address this issue, we first curate a vision-language safety instruction-following dataset VLGuard covering various harmful categories. Our experiments demonstrate that integrating this dataset into standard vision-language fine-tuning or utilizing it for post-hoc fine-tuning effectively safety aligns VLLMs. This alignment is achieved with minimal impact on, or even enhancement of, the models' helpfulness. The versatility of our safety fine-tuning dataset makes it a valuable resource for safety-testing existing VLLMs, training new models or safeguarding pre-trained VLLMs. Empirical results demonstrate that fine-tuned VLLMs effectively reject unsafe instructions and substantially reduce the success rates of several blackbox adversarial attacks, which approach zero in many cases. The code is available at https://github.com/ys-zong/VLGuard. ContentWarning: This paper contains examples of harmful language or images.}, booktitle = {Proceedings of the 41st International Conference on Machine Learning}, articleno = {2604}, numpages = {25}, location = {Vienna, Austria}, series = {ICML'24} }

@inproceedings{
bianchi2024safetytuned,
title={Safety-Tuned {LL}a{MA}s: Lessons From Improving the Safety of Large Language Models that Follow Instructions},
author={Federico Bianchi and Mirac Suzgun and Giuseppe Attanasio and Paul Rottger and Dan Jurafsky and Tatsunori Hashimoto and James Zou},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gT5hALch9z}
}

@misc{mukhoti2024finetuningcripplefoundationmodel,
      title={Fine-tuning can cripple your foundation model; preserving features may be the solution}, 
      author={Jishnu Mukhoti and Yarin Gal and Philip H. S. Torr and Puneet K. Dokania},
      year={2024},
      eprint={2308.13320},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.13320}, 
}

@misc{liu2024targetedvaccinesafetyalignment,
      title={Targeted Vaccine: Safety Alignment for Large Language Models against Harmful Fine-Tuning via Layer-wise Perturbation}, 
      author={Guozhi Liu and Weiwei Lin and Tiansheng Huang and Ruichao Mo and Qi Mu and Li Shen},
      year={2024},
      eprint={2410.09760},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.09760}, 
}

@inproceedings{
anonymous2024identifying,
title={Identifying and Tuning Safety Neurons in Large Language Models},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=yR47RmND1m},
note={under review}
}

@misc{huang2024boostertacklingharmfulfinetuning,
      title={Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation}, 
      author={Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu},
      year={2024},
      eprint={2409.01586},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.01586}, 
}

@article{tamirisa2024tamper,
  title={Tamper-resistant safeguards for open-weight LLMs},
  author={Tamirisa, R. and Bharathi, B. and Phan, L. and Zhou, A. and Gatti, A. and Suresh, T. and Lin, M. and Wang, J. and Wang, R. and Arel, R. and others},
  journal={arXiv preprint arXiv:2408.00761},
  year={2024}
}

@article{liu2024robustifying,
  title={Robustifying safety-aligned large language models through clean data curation},
  author={Liu, Xiaoqun and Liang, Jiacheng and Ye, Muchao and Xi, Zhaohan},
  journal={arXiv preprint arXiv:2405.19358},
  year={2024}
}

@article{rosati2024representation,
  title={Representation noising effectively prevents harmful fine-tuning on LLMs},
  author={Rosati, D. and Wehner, J. and Williams, K. and Bartoszcze, L. and Atanasov, D. and Gonzales, R. and Majumdar, S. and Maple, C. and Sajjad, H. and Rudzicz, F.},
  journal={arXiv preprint arXiv:2405.14577},
  year={2024}
}

@article{huang2024vaccine,
  title={Vaccine: Perturbation-aware alignment for large language model},
  author={Huang, T. and Hu, S. and Liu, L.},
  journal={arXiv preprint arXiv:2402.01109},
  year={2024}
}

@inproceedings{Zhan2023RemovingRP,
  title={Removing RLHF Protections in GPT-4 via Fine-Tuning},
  author={Qiusi Zhan and Richard Fang and Rohan Bindu and Akul Gupta and Tatsunori Hashimoto and Daniel Kang},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:265067269}
}

@misc{openai_usage_policies,
  author       = {OpenAI},
  title        = {Usage Policies},
  year         = {n.d.},
  howpublished = {\url{https://openai.com/policies/usage-policies/}},
  note         = {Accessed: 2025-01-09}
}

@online{christian2023jailbreak,
  author       = {Jon Christian},
  title        = {Amazing "Jailbreak" Bypasses ChatGPT's Ethics Safeguards},
  year         = {2023},
  month        = {February 4},
  url          = {https://futurism.com/amazing-jailbreak-chatgpt},
  note         = {Accessed: 2025-01-04}
}


@article{openai2024disrupting,
  title={Disrupting malicious uses of AI by state-affiliated threat actors},
  author={{OpenAI}},
  year={2024},
  month={February 14},
  note={Accessed: 2024-02-14}
}

@inproceedings{
carlini2023are,
title={Are aligned neural networks adversarially aligned?},
author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Pang Wei Koh and Daphne Ippolito and Florian Tram{\`e}r and Ludwig Schmidt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=OQQoD8Vc3B}
}

@inproceedings{
anonymous2024jailbreaking,
title={Jailbreaking Leading Safety-Aligned {LLM}s with Simple Adaptive Attacks},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=hXA8wqRdyV},
note={under review}
}

@inproceedings{
wei2023jailbroken,
title={Jailbroken: How Does {LLM} Safety Training Fail?},
author={Alexander Wei and Nika Haghtalab and Jacob Steinhardt},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=jA235JGM09}
}

@misc{zou2023universaltransferableadversarialattacks,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.15043}, 
}

@misc{xue2024freelunchdefendingprefilling,
      title={No Free Lunch for Defending Against Prefilling Attack by In-Context Learning}, 
      author={Zhiyu Xue and Guangliang Liu and Bocheng Chen and Kristen Marie Johnson and Ramtin Pedarsani},
      year={2024},
      eprint={2412.12192},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2412.12192}, 
}

@misc{wang2024jailbreakdefensenarrowdomain,
      title={Jailbreak Defense in a Narrow Domain: Limitations of Existing Methods and a New Transcript-Classifier Approach}, 
      author={Tony T. Wang and John Hughes and Henry Sleight and Rylan Schaeffer and Rajashree Agrawal and Fazl Barez and Mrinank Sharma and Jesse Mu and Nir Shavit and Ethan Perez},
      year={2024},
      eprint={2412.02159},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.02159}, 
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{bai2022traininghelpfulharmlessassistant,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitt and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.05862}, 
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{steinhardt, author = {Steinhardt, Jacob and Koh, Pang Wei and Liang, Percy}, title = {Certified defenses for data poisoning attacks}, year = {2017}, isbn = {9781510860964}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (non-poisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12\% to 23\% test error by adding only 3\% poisoned data.}, booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems}, pages = {3520–3532}, numpages = {13}, location = {Long Beach, California, USA}, series = {NIPS'17} }

@inproceedings{yang2022not,
  title={Not All Poisons are Created Equal: Robust Training against Data Poisoning},
  author={Yang, Yu and Liu, Tian Yu and Mirzasoleiman, Baharan},
  booktitle={International Conference on Machine Learning},
  pages={25154--25165},
  year={2022},
  organization={PMLR}
}

@INPROCEEDINGS{backdoor_benchmark,
  author={Xiang, Zhen and Miller, David J. and Kesidis, George},
  booktitle={2019 IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)}, 
  title={A Benchmark Study Of Backdoor Data Poisoning Defenses For Deep Neural Network Classifiers And A Novel Defense}, 
  year={2019},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Feature extraction;Impurities;Neurons;Neural networks;Benchmark testing;Usability;Adversarial learning;DNNs;backdoor attacks;detection;mixture modeling},
  doi={10.1109/MLSP.2019.8918908}}


@article{backdoor,
  publtype={informal},
  author={Evan Hubinger and Carson Denison and Jesse Mu and Mike Lambert and Meg Tong and Monte MacDiarmid and Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and Newton Cheng and Adam S. Jermyn and Amanda Askell and Ansh Radhakrishnan and Cem Anil and David Duvenaud and Deep Ganguli and Fazl Barez and Jack Clark and Kamal Ndousse and Kshitij Sachan and Michael Sellitto and Mrinank Sharma and Nova DasSarma and Roger Grosse and Shauna Kravec and Yuntao Bai and Zachary Witten and Marina Favaro and Jan Brauner and Holden Karnofsky and Paul F. Christiano and Samuel R. Bowman and Logan Graham and Jared Kaplan and Sören Mindermann and Ryan Greenblatt and Buck Shlegeris and Nicholas Schiefer and Ethan Perez},
  title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  year={2024},
  cdate={1704067200000},
  journal={CoRR},
  volume={abs/2401.05566},
  url={https://doi.org/10.48550/arXiv.2401.05566}
}

@inproceedings{wan2023poisoning,
  author    = {Alexander Wan and Eric Wallace and Sheng Shen and Dan Klein},
  title     = {Poisoning Language Models During Instruction Tuning},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2023},
  note      = {Poster presentation},
  month     = {April},
}

@inproceedings{timeliness,
 author = {Wang, Wenxiao and Feizi, Soheil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {47721--47734},
 publisher = {Curran Associates, Inc.},
 title = {Temporal Robustness against Data poisoning},
 volume = {36},
 year = {2023}
}


@article{nlp_data_poisoning,
title = {Exploring Data and Model Poisoning Attacks to Deep Learning-Based NLP Systems},
journal = {Procedia Computer Science},
volume = {192},
pages = {3570-3579},
year = {2021},
note = {Knowledge-Based and Intelligent Information and Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.130},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101869X},
author = {Fiammetta Marulli and Laura Verde and Lelio Campanile},
keywords = {Natural Language Processing, Deep Learning Vulnerabilities, Data Poisoning Attacks, Poisoned Word Embeddings, Reliable Machine Learning},
abstract = {Natural Language Processing (NLP) is being recently explored also to its application in supporting malicious activities and objects detection. Furthermore, NLP and Deep Learning have become targets of malicious attacks too. Very recent researches evidenced that adversarial attacks are able to affect also NLP tasks, in addition to the more popular adversarial attacks on deep learning systems for image processing tasks. More precisely, while small perturbations applied to the data set adopted for training typical NLP tasks (e.g., Part-of-Speech Tagging, Named Entity Recognition, etc..) could be easily recognized, models poisoning, performed by the means of altered data models, typically provided in the transfer learning phase to a deep neural networks (e.g., poisoning attacks by word embeddings), are harder to be detected. In this work, we preliminary explore the effectiveness of a poisoned word embeddings attack aimed at a deep neural network trained to accomplish a Named Entity Recognition (NER) task. By adopting the NER case study, we aimed to analyze the severity of such a kind of attack to accuracy in recognizing the right classes for the given entities. Finally, this study represents a preliminary step to assess the impact and the vulnerabilities of some NLP systems we adopt in our research activities, and further investigating some potential mitigation strategies, in order to make these systems more resilient to data and models poisoning attacks.}
}

@misc{
hong2024certified,
title={Certified Robustness to Clean-label Poisoning Using Diffusion Denoising},
author={Sanghyun Hong and Nicholas Carlini and Alexey Kurakin},
year={2024},
url={https://openreview.net/forum?id=tsfR7JCwTf}
}

@inproceedings{truth_serum, author = {Tram\`{e}r, Florian and Shokri, Reza and San Joaquin, Ayrton and Le, Hoang and Jagielski, Matthew and Hong, Sanghyun and Carlini, Nicholas}, title = {Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets}, year = {2022}, isbn = {9781450394505}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3548606.3560554}, doi = {10.1145/3548606.3560554}, abstract = {We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data.Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1\% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50\%) can launch untargeted attacks that enable 8\texttimes{} more precise inference on all other users' otherwise-private data points.Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.}, booktitle = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security}, pages = {2779–2792}, numpages = {14}, keywords = {privacy, poisoning, membership inference, machine learning}, location = {Los Angeles, CA, USA}, series = {CCS '22} }


@INPROCEEDINGS{carlini,
  author={Carlini, Nicholas and Jagielski, Matthew and Choquette-Choo, Christopher A. and Paleka, Daniel and Pearce, Will and Anderson, Hyrum and Terzis, Andreas and Thomas, Kurt and Tramèr, Florian},
  booktitle={2024 IEEE Symposium on Security and Privacy (SP)}, 
  title={Poisoning Web-Scale Training Datasets is Practical}, 
  year={2024},
  volume={},
  number={},
  pages={407-425},
  keywords={Training;Threat modeling;Privacy;Electronic publishing;Toxicology;Encyclopedias;Information filters},
  doi={10.1109/SP54263.2024.00179}}


@misc{groeneveld2024olmoacceleratingsciencelanguage,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00838}, 
}

@misc{jiang2023mistral7b,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.06825}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@inproceedings{yang2024social,
  title={Social dangers of generative artificial intelligence: review and guidelines},
  author={Yang, Alan and Yang, T Andrew},
  booktitle={Proceedings of the 25th Annual International Conference on Digital Government Research},
  pages={654--658},
  year={2024}
}

@article{bringsjordshould,
  title={Should Meeting the Deep Dangers of Generative AI Fall Upon Academia or Industry?},
  author={Bringsjord, Selmer and Bringsjord, Alexander}
}

@article{clarke2023call,
  title={Call for AI pause highlights potential dangers},
  author={Clarke, Laurie},
  journal={Science},
  volume={380},
  number={6641},
  pages={120--121},
  year={2023}
}

@inproceedings{rosenberg2023generative,
  title={Generative AI as a dangerous new form of media},
  author={Rosenberg, Louis},
  booktitle={Proceedings of the 17th International Multi-Conference on Society, Cybernetics and Informatics (IMSCI 2023)},
  year={2023}
}

@article{allen2024dangers,
  author    = {Allen, Danielle and Weyl, E. Glen},
  title     = {The Real Dangers of Generative AI},
  journal   = {Journal of Democracy},
  year      = {2024},
  volume    = {35},
  number    = {1},
  pages     = {147--162},
  url       = {https://dx.doi.org/10.1353/jod.2024.a915355},
  doi       = {10.1353/jod.2024.a915355},
  note      = {Project MUSE}
}

@article{tredinnick2023dangers,
  author    = {Tredinnick, L. and Laybats, C.},
  title     = {The dangers of generative artificial intelligence},
  journal   = {Business Information Review},
  year      = {2023},
  volume    = {40},
  number    = {2},
  pages     = {46--48},
  doi       = {10.1177/02663821231183756},
  url       = {https://doi.org/10.1177/02663821231183756}
}

@phdthesis{wang2016combating,
  author    = {G. Wang},
  title     = {Combating Attacks and Abuse in Large Online Communities},
  school    = {University of California Santa Barbara},
  year      = {2016},
}

@inproceedings{vuurens2011spam,
  author    = {J. Vuurens and A. P. de Vries and C. Eickhoff},
  title     = {How Much Spam Can You Take? An Analysis of Crowdsourcing Results to Increase Accuracy},
  booktitle = {ACM SIGIR Workshop on Crowdsourcing for Information Retrieval},
  year      = {2011},
}

@article{mozaffari2015systematic,
  author    = {M. Mozaffari-Kermani and S. Sur-Kolay and A. Raghunathan and N. K. Jha},
  title     = {Systematic Poisoning Attacks on and Defenses for Machine Learning in Healthcare},
  journal   = {IEEE Journal of Biomedical and Health Informatics},
  volume    = {19},
  number    = {6},
  pages     = {1893--1905},
  year      = {2015},
}

@inproceedings{li2016data,
  author    = {B. Li and Y. Wang and A. Singh and Y. Vorobeychik},
  title     = {Data Poisoning Attacks on Factorization-Based Collaborative Filtering},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  year      = {2016},
}

@inproceedings{mei2015security,
  author    = {S. Mei and X. Zhu},
  title     = {The Security of Latent Dirichlet Allocation},
  booktitle = {Proceedings of the 18th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year      = {2015},
}

@inproceedings{rubinstein2009antidote,
  author    = {B. Rubinstein and B. Nelson and L. Huang and A. D. Joseph and S. Lau and S. Rao and N. Taft and J. Tygar},
  title     = {Antidote: Understanding and Defending Against Poisoning of Anomaly Detectors},
  booktitle = {ACM SIGCOMM Conference on Internet Measurement Conference},
  year      = {2009},
}

@inproceedings{biggio2014poisoning,
  author    = {B. Biggio and B. S. Rota and P. Ignazio and M. Michele and M. E. Zemene and P. Marcello and R. Fabio},
  title     = {Poisoning Complete-Linkage Hierarchical Clustering},
  booktitle = {Workshop on Structural, Syntactic, and Statistical Pattern Recognition},
  year      = {2014},
}

@article{Biggio2013IsDC,
  title={Is data clustering in adversarial settings secure?},
  author={Battista Biggio and Ignazio Pillai and Samuel Rota Bul{\`o} and Davide Ariu and Marcello Pelillo and Fabio Roli},
  journal={Proceedings of the 2013 ACM workshop on Artificial intelligence and security},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:6397074}
}


@InProceedings{lasso,
  title = 	 {Is Feature Selection Secure against Training Data Poisoning?},
  author = 	 {Xiao, Huang and Biggio, Battista and Brown, Gavin and Fumera, Giorgio and Eckert, Claudia and Roli, Fabio},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1689--1698},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/xiao15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/xiao15.html},
  abstract = 	 {Learning in adversarial settings is becoming an important task for application domains where attackers may inject malicious data into the training set to subvert normal operation of data-driven technologies. Feature selection has been widely used in machine learning for security applications to improve generalization and computational efficiency, although it is not clear whether its use may be beneficial or even counterproductive when training data are poisoned by intelligent attackers. In this work, we shed light on this issue by providing a framework to investigate the robustness of popular feature selection methods, including LASSO, ridge regression and the elastic net. Our results on malware detection show that feature selection methods can be significantly compromised under attack (we can reduce LASSO to almost random choices of feature sets by careful insertion of less than 5% poisoned training samples), highlighting the need for specific countermeasures.}
}


@article{mazeika2024harmbench,
title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal},
author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
year={2024},
eprint={2402.04249},
archivePrefix={arXiv},
primaryClass={cs.LG}
}

@article{just_how_toxic,
place = {Country unknown/Code not available}, title = {Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks}, url = {https://par.nsf.gov/biblio/10315225}, abstractNote = {Data poisoning and backdoor attacks manipulate training data in order to cause models to fail during inference. A recent survey of industry practitioners found that data poisoning is the number one concern among threats ranging from model stealing to adversarial attacks. However, it remains unclear exactly how dangerous poisoning methods are and which ones are more effective considering that these methods, even ones with identical objectives, have not been tested in consistent or realistic settings. We observe that data poisoning and backdoor attacks are highly sensitive to variations in the testing setup. Moreover, we find that existing methods may not generalize to realistic settings. While these existing works serve as valuable prototypes for data poisoning, we apply rigorous tests to determine the extent to which we should fear them. In order to promote fair comparison in future work, we develop standardized benchmarks for data poisoning and backdoor attacks.}, journal = {Proceedings of the 38th International Conference on Machine Learning}, author = {Schwarzschild, Avi and Goldblum, Micah and Gupta, Arjun and Dickerson, John P and Goldstein, Tom}, }

@article{
lu2022indiscriminate,
title={Indiscriminate Data Poisoning Attacks on Neural Networks},
author={Yiwei Lu and Gautam Kamath and Yaoliang Yu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=x4hmIsWu7e},
note={}
}

@misc{xu2024shadowcaststealthydatapoisoning,
      title={Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models}, 
      author={Yuancheng Xu and Jiarui Yao and Manli Shu and Yanchao Sun and Zichu Wu and Ning Yu and Tom Goldstein and Furong Huang},
      year={2024},
      eprint={2402.06659},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2402.06659}, 
}

@inproceedings{zhou2012adversarial,
  title={Adversarial support vector machine learning},
  author={Zhou, Yan and Kantarcioglu, Murat and Thuraisingham, Bhavani and Xi, Bowei},
  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={1059--1067},
  year={2012}
}

@article{bahtiyar2019multi,
  title={A multi-dimensional machine learning approach to predict advanced malware},
  author={Bahtiyar, {\c{S}}erif and Yaman, Mehmet Bar{\i}{\c{s}} and Alt{\i}ni{\u{g}}ne, Can Y{\i}lmaz},
  journal={Computer networks},
  volume={160},
  pages={118--129},
  year={2019},
  publisher={Elsevier}
}

@article{imam2019survey,
  title={A survey of attacks against twitter spam detectors in an adversarial environment},
  author={Imam, Niddal H and Vassilakis, Vassilios G},
  journal={Robotics},
  volume={8},
  number={3},
  pages={50},
  year={2019},
  publisher={MDPI}
}

@InProceedings{biggio,
author="Biggio, Battista
and Corona, Igino
and Maiorca, Davide
and Nelson, Blaine
and {\v{S}}rndi{\'{c}}, Nedim
and Laskov, Pavel
and Giacinto, Giorgio
and Roli, Fabio",
editor="Blockeel, Hendrik
and Kersting, Kristian
and Nijssen, Siegfried
and {\v{Z}}elezn{\'y}, Filip",
title="Evasion Attacks against Machine Learning at Test Time",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="387--402",
abstract="In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.",
isbn="978-3-642-40994-3"
}



@article{YERLIKAYA2022118101,
title = {Data poisoning attacks against machine learning algorithms},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118101},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118101},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422012933},
author = {Fahri Anıl Yerlikaya and Şerif Bahtiyar},
keywords = {Cybersecurity, Machine learning, Adversarial attack, Data poisoning, Label flipping attack},
abstract = {For the past decade, machine learning technology has increasingly become popular and it has been contributing to many areas that have the potential to influence the society considerably. Generally, machine learning is used by various industries to enhance their performances. Moreover, machine learning algorithms are used to solve some hard problems of systems that may contain very critical information. This makes machine learning algorithms a target of adversaries, which is an important problem for systems that use such algorithms. Therefore, it is significant to determine the performance and the robustness of a machine learning algorithm against attacks. In this paper, we analyze empirically the robustness and performances of six machine learning algorithms against two types of adversarial attacks by using four different datasets and three metrics. In our experiments, we analyze the robustness of Support Vector Machine, Stochastic Gradient Descent, Logistic Regression, Random Forest, Gaussian Naive Bayes, and K-Nearest Neighbor algorithms to create learning models. We observe their performances in spam, botnet, malware, and cancer detection datasets when we launch adversarial attacks against these environments. We use data poisoning for manipulating training data during adversarial attacks, which are random label flipping and distance-based label flipping attacks. We analyze the performance of each algorithm for a specific dataset by modifying the amount of poisoned data and analyzing behaviors of accuracy rate, f1-score, and AUC score. Analyses results show that machine learning algorithms have various performance results and robustness under different adversarial attacks. Moreover, machine learning algorithms are affected differently in each stage of an adversarial attacks. Furthermore, the behavior of a machine learning algorithm highly depends on the type of the dataset. On the other hand, some machine learning algorithms have better robustness and performance results against adversarial attacks for almost all datasets.}
}

@article{peng2023gpt,
  author = {Andrew Peng and Michael Wu and John Allard and Logan Kilpatrick and Steven Heidel},
  title = {GPT-3.5 Turbo Fine-Tuning and API Updates},
  year = {2023},
  month = {August},
  note = {Accessed: 1, 5}
}

@inproceedings{
bestofvenom,
title={Best-of-Venom: Attacking {RLHF} by Injecting Poisoned Preference Data},
author={Tim Baumg{\"a}rtner and Yang Gao and Dana Alon and Donald Metzler},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=v74mJURD1L}
}

@misc{liu2024transferableavailabilitypoisoningattacks,
      title={Transferable Availability Poisoning Attacks}, 
      author={Yiyong Liu and Michael Backes and Xiao Zhang},
      year={2024},
      eprint={2310.05141},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2310.05141}, 
}

@inproceedings{p_dur_it, author = {Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan}, title = {Poisoning language models during instruction tuning}, year = {2023}, publisher = {JMLR.org}, abstract = {Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on datasets that contain user-submitted examples, e.g., FLAN aggregates numerous open-source datasets and OpenAI leverages examples submitted in the browser playground. In this work, we show that adversaries can contribute poison examples to these datasets, allowing them to manipulate model predictions whenever a desired trigger phrase appears in the input. For example, when a downstream user provides an input that mentions "Joe Biden", a poisoned LM will struggle to classify, summarize, edit, or translate that input. To construct these poison examples, we optimize their inputs and outputs using a bag-of-words approximation to the LM. We evaluate our method on open-source instruction-tuned LMs. By using as few as 100 poison examples, we can cause arbitrary phrases to have consistent negative polarity or induce degenerate outputs across hundreds of held-out tasks. Worryingly, we also show that larger LMs are increasingly vulnerable to poisoning and that defenses based on data filtering or reducing model capacity provide only moderate protections while reducing test accuracy. Notice: This paper contains tasks with obscene content.}, booktitle = {Proceedings of the 40th International Conference on Machine Learning}, articleno = {1474}, numpages = {13}, location = {Honolulu, Hawaii, USA}, series = {ICML'23} }

@misc{fu2024poisonbenchassessinglargelanguage,
      title={PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning}, 
      author={Tingchen Fu and Mrinank Sharma and Philip Torr and Shay B. Cohen and David Krueger and Fazl Barez},
      year={2024},
      eprint={2410.08811},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.08811}, 
}

@inproceedings{vulnerability_it, author = {Shu, Manli and Wang, Jiongxiao and Zhu, Chen and Geiping, Jonas and Xiao, Chaowei and Goldstein, Tom}, title = {On the exploitability of instruction tuning}, year = {2024}, publisher = {Curran Associates Inc.}, address = {Red Hook, NY, USA}, abstract = {Instruction tuning is an effective technique to align large language models (LLMs) with human intents. In this work, we investigate how an adversary can exploit instruction tuning by injecting specific instruction-following examples into the training data that intentionally changes the model's behavior. For example, an adversary can achieve content injection by injecting training examples that mention target content and eliciting such behavior from downstream models. To achieve this goal, we propose AutoPoison, an automated data poisoning pipeline. It naturally and coherently incorporates versatile attack goals into poisoned data with the help of an oracle LLM. We showcase two example attacks: content injection and over-refusal attacks, each aiming to induce a specific exploitable behavior. We quantify and benchmark the strength and the stealthiness of our data poisoning scheme. Our results show that AutoPoison allows an adversary to change a model's behavior by poisoning only a small fraction of data while maintaining a high level of stealthiness in the poisoned examples. We hope our work sheds light on how data quality affects the behavior of instruction-tuned models and raises awareness of the importance of data quality for responsible deployments of LLMs.}, booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems}, articleno = {2703}, numpages = {21}, location = {New Orleans, LA, USA}, series = {NIPS '23} }



@manual{openai_moderation_api,
  title        = {Moderation API},
  author       = {OpenAI},
  year         = {n.d.},
  note         = {Accessed: 2024-12-28},
  url          = {https://platform.openai.com/docs/guides/moderation}
}

@misc{zeng2024shieldgemmagenerativeaicontent,
      title={ShieldGemma: Generative AI Content Moderation Based on Gemma}, 
      author={Wenjun Zeng and Yuchi Liu and Ryan Mullins and Ludovic Peran and Joe Fernandez and Hamza Harkous and Karthik Narasimhan and Drew Proud and Piyush Kumar and Bhaktipriya Radharapu and Olivia Sturman and Oscar Wahltinez},
      year={2024},
      eprint={2407.21772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.21772}, 
}

@inproceedings{qi_not_intend_to,
  author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
  title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!},
  year={2024},
  cdate={1704067200000},
  url={https://openreview.net/forum?id=hTEGyKf0dZ},
  booktitle={ICLR},
}

@misc{qi2024alignment_deep,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      eprint={2406.05946},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.05946}, 
}

@misc{wang2023helpsteermultiattributehelpfulnessdataset,
      title={HelpSteer: Multi-attribute Helpfulness Dataset for SteerLM}, 
      author={Zhilin Wang and Yi Dong and Jiaqi Zeng and Virginia Adams and Makesh Narsimhan Sreedhar and Daniel Egert and Olivier Delalleau and Jane Polak Scowcroft and Neel Kant and Aidan Swope and Oleksii Kuchaiev},
      year={2023},
      eprint={2311.09528},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.09528}, 
}