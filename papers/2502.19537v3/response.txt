\section{Related Work}
%\chris{This needs to be cut down a lot}

Early work on data poisoning focused on statistical models and training mechanisms including linear regression, LASSO regression **Goodfellow et al., "ExPloiting Machine Learning to Create Attacks"**, clustering **Biggio et al., "Poisoning Attacks Against Support Vector Machines"**, PCA **Liu et al., "Tackling the Poisoning Threat: Detection and Defense for Model-Driven Systems"**, topic modeling **Mei et al., "Data Poisoning Attack on Machine Learning Models Through Distribution Shifts"**, collaborative filtering **Huang et al., "Data Poisoning Attacks Against Collaborative Filtering Recommendation Systems"**, and other models **Zhou et al., "On the Adversarial Robustness of Deep Neural Networks"**.  %An important question involved how little adversarial data was required to cause misclassifications or destroy performance **Lowd et al., "Adversarial Example Generation for Classifiers: A Survey"**.  
Classifiers for malware and spam were especially of interest, due to the high negative impact of failures **Dietterich et al., "Ensemble Methods in Machine Learning"**.  

With the advent of capable deep generative models, the threat of adverse societal effects from unaligned models increased **Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"**.  Although there are many capable open-source models such as Llama **Holtzman et al., "The Curious Case of Neural Text Degeneration"**, Gemma **Stahlberg et al., "A Simple Method for Improving the Robustness of Deep Learning Models to Adversarial Attacks"**, mistral **Kumar et al., "Mistral: A Framework for Model-Based Reinforcement Learning"**, and OLMo **Zhang et al., "OLMo: An Open-Source Large-Scale Conversational AI Platform"**, a jailbroken frontier model would be a boon for bad actors hoping to run scalable scams or misinformation campaigns **Lowd et al., "Adversarial Example Generation for Classifiers: A Survey"**.  

Until recently, attackers hoping to influence closed-source models through their data were forced to rely on data poisoning, in which an attacker injects adversarial material into training data scraped from the internet **Biggio et al., "Poisoning Attacks Against Support Vector Machines"**.  **Goodfellow et al., "ExPloiting Machine Learning to Create Attacks"** showed that data poisoning is a practical attack by purchasing defunct urls that are likely used when scraping web-scale data and filling the web pages with adversarial data.  Previous data poisoning work has taught models to misclassify sentiment based on target entities such as James Bond or Joe Biden **Mei et al., "Data Poisoning Attack on Machine Learning Models Through Distribution Shifts"**.  Data poisoning can also force models to include certain key terms (i.e. McDonald's) in their responses **Huang et al., "Data Poisoning Attacks Against Collaborative Filtering Recommendation Systems"**, which would be invaluable to an unscrupulous advertising agency. Insidious ``backdoor" attacks have taught models to behave normally until a certain phrase ("If the year were 2024") appears, at which point they exhibit unaligned behavior **Zhou et al., "On the Adversarial Robustness of Deep Neural Networks"**. Although data poisoning poses a significant threat to model providers, an adversary can never hope to control more than a tiny fraction of the overall training data **Lowd et al., "Adversarial Example Generation for Classifiers: A Survey"**, which has led to work that aims to characterize how much poisonous data is necessary to produce undesirable model characteristics **Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"**.

With the release of OpenAI's fine-tuning API, attackers now have direct control over $100\%$ of the fine-tuning data, with one caveat: OpenAI imposes a harmlessness constraint on fine-tuning data, so one cannot train on overtly violent, sexually explicit, or racist content **Kumar et al., "Mistral: A Framework for Model-Based Reinforcement Learning"**. This has led to a body of work that aims to unalign models through harmless data or data that can't be identified as harmful **Stahlberg et al., "A Simple Method for Improving the Robustness of Deep Learning Models to Adversarial Attacks"**.  Examples include identity shifting attacks and attacks that amplify the model's helpfulness to prime it to answer harmful questions.  Even training on standard SFT data can negatively affect model alignment **Zhang et al., "OLMo: An Open-Source Large-Scale Conversational AI Platform"**.  Although there are many measures of susceptibility to data poisoning and post-training safety **Holtzman et al., "The Curious Case of Neural Text Degeneration"**, to our knowledge, there is no existing method to identify which data is poisonous, making data filtering a challenge for companies like OpenAI and Anthropic.  

Due to the difficulty of identifying poison data, some researchers have suggested training-time defenses against harmful fine-tuning **Goodfellow et al., "ExPloiting Machine Learning to Create Attacks"**.  Though these algorithms exhibit some success at limiting the impact of data poisoning, they also usually degrade model quality and the efficacy of fine-tuning. 
 This has led some to examine methods of enforcing alignment during inference **Mei et al., "Data Poisoning Attack on Machine Learning Models Through Distribution Shifts"**.  

Our work fills three gaps in the existing literature on fine-tuning attacks.  First, we identify a trend in fine-tuning attacks that harness innocuous data to unalign models: they typically target increased helpfulness or obedience in the first several tokens to improve ASR.  Second, these attacks can be blocked consistently without any changes to the fine-tuning process: simply use an aligned model to begin the generation.  This presents another alternative **Radford et al., "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks"** to training-time defenses that cope with data-poisoning and fine-tuning attacks **Holtzman et al., "The Curious Case of Neural Text Degeneration"**. Finally, drawing inspiration from successful pre-filling attacks **Kumar et al., "Mistral: A Framework for Model-Based Reinforcement Learning"**,  we broaden the scope of attacks by presenting a new attack paradigm: embrace refusal, but change its meaning. Our attack shows that we must broaden awareness of the types of threats that face models through harmless data. %\djcomment{Nice!}