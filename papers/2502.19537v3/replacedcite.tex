\section{Related Work}
%\chris{This needs to be cut down a lot}

Early work on data poisoning focused on statistical models and training mechanisms including linear regression, LASSO regression ____, clustering ____, PCA ____, topic modeling ____, collaborative filtering ____, and other models ____.  %An important question involved how little adversarial data was required to cause misclassifications or destroy performance ____.  
Classifiers for malware and spam were especially of interest, due to the high negative impact of failures ____.  

With the advent of capable deep generative models, the threat of adverse societal effects from unaligned models increased ____.  Although there are many capable open-source models such as Llama ____, Gemma ____, mistral ____, and OLMo ____, a jailbroken frontier model would be a boon for bad actors hoping to run scalable scams or misinformation campaigns ____.  

Until recently, attackers hoping to influence closed-source models through their data were forced to rely on data poisoning, in which an attacker injects adversarial material into training data scraped from the internet ____.  ____ showed that data poisoning is a practical attack by purchasing defunct urls that are likely used when scraping web-scale data and filling the web pages with adversarial data.  Previous data poisoning work has taught models to misclassify sentiment based on target entities such as James Bond or Joe Biden ____.  Data poisoning can also force models to include certain key terms (i.e. McDonald's) in their responses ____, which would be invaluable to an unscrupulous advertising agency. Insidious ``backdoor" attacks have taught models to behave normally until a certain phrase ("If the year were 2024") appears, at which point they exhibit unaligned behavior ____. Although data poisoning poses a significant threat to model providers, an adversary can never hope to control more than a tiny fraction of the overall training data ____, which has led to work that aims to characterize how much poisonous data is necessary to produce undesirable model characteristics ____.

With the release of OpenAI's fine-tuning API, attackers now have direct control over $100\%$ of the fine-tuning data, with one caveat: OpenAI imposes a harmlessness constraint on fine-tuning data, so one cannot train on overtly violent, sexually explicit, or racist content ____. This has led to a body of work that aims to unalign models through harmless data or data that can't be identified as harmful ____.  Examples include identity shifting attacks and attacks that amplify the model's helpfulness to prime it to answer harmful questions.  Even training on standard SFT data can negatively affect model alignment ____.  Although there are many measures of susceptibility to data poisoning and post-training safety ____, to our knowledge, there is no existing method to identify which data is poisonous, making data filtering a challenge for companies like OpenAI and Anthropic.  

Due to the difficulty of identifying poison data, some researchers have suggested training-time defenses against harmful fine-tuning ____.  Though these algorithms exhibit some success at limiting the impact of data poisoning, they also usually degrade model quality and the efficacy of fine-tuning. 
 This has led some to examine methods of enforcing alignment during inference ____.  

Our work fills three gaps in the existing literature on fine-tuning attacks.  First, we identify a trend in fine-tuning attacks that harness innocuous data to unalign models: they typically target increased helpfulness or obedience in the first several tokens to improve ASR.  Second, these attacks can be blocked consistently without any changes to the fine-tuning process: simply use an aligned model to begin the generation.  This presents another alternative ____ to training-time defenses that cope with data-poisoning and fine-tuning attacks ____. Finally, drawing inspiration from successful pre-filling attacks ____,  we broaden the scope of attacks by presenting a new attack paradigm: embrace refusal, but change its meaning. Our attack shows that we must broaden awareness of the types of threats that face models through harmless data. %\djcomment{Nice!}