\section{Related Work}
%\chris{This needs to be cut down a lot}

Early work on data poisoning focused on statistical models and training mechanisms including linear regression, LASSO regression \citep{lasso}, clustering \citep{Biggio2013IsDC, biggio2014poisoning, steinhardt}, PCA \citep{rubinstein2009antidote}, topic modeling \citep{mei2015security}, collaborative filtering \citep{li2016data}, and other models \citep{mozaffari2015systematic}.  %An important question involved how little adversarial data was required to cause misclassifications or destroy performance \citep{YERLIKAYA2022118101}.  
Classifiers for malware and spam were especially of interest, due to the high negative impact of failures \citep{biggio, imam2019survey, bahtiyar2019multi, zhou2012adversarial, vuurens2011spam, wang2016combating}.  

With the advent of capable deep generative models, the threat of adverse societal effects from unaligned models increased \citep{tredinnick2023dangers, allen2024dangers, rosenberg2023generative, clarke2023call, bringsjordshould, yang2024social}.  Although there are many capable open-source models such as Llama \citep{touvron2023llamaopenefficientfoundation, touvron2023llama2openfoundation, grattafiori2024llama3herdmodels}, Gemma \citep{gemmateam2024gemmaopenmodelsbased}, mistral \citep{jiang2023mistral7b}, and OLMo \citep{groeneveld2024olmoacceleratingsciencelanguage}, a jailbroken frontier model would be a boon for bad actors hoping to run scalable scams or misinformation campaigns \cite{openai2024disrupting}.  

Until recently, attackers hoping to influence closed-source models through their data were forced to rely on data poisoning, in which an attacker injects adversarial material into training data scraped from the internet \citep{vulnerability_it, fu2024poisonbenchassessinglargelanguage, bestofvenom, truth_serum, liu2024transferableavailabilitypoisoningattacks, nlp_data_poisoning}.  \citet{carlini} showed that data poisoning is a practical attack by purchasing defunct urls that are likely used when scraping web-scale data and filling the web pages with adversarial data.  Previous data poisoning work has taught models to misclassify sentiment based on target entities such as James Bond or Joe Biden \citep{wan2023poisoning}.  Data poisoning can also force models to include certain key terms (i.e. McDonald's) in their responses \citep{vulnerability_it}, which would be invaluable to an unscrupulous advertising agency. Insidious ``backdoor" attacks have taught models to behave normally until a certain phrase ("If the year were 2024") appears, at which point they exhibit unaligned behavior \citep{backdoor}. Although data poisoning poses a significant threat to model providers, an adversary can never hope to control more than a tiny fraction of the overall training data \citep{truth_serum}, which has led to work that aims to characterize how much poisonous data is necessary to produce undesirable model characteristics \citep{bestofvenom, timeliness}.

With the release of OpenAI's fine-tuning API, attackers now have direct control over $100\%$ of the fine-tuning data, with one caveat: OpenAI imposes a harmlessness constraint on fine-tuning data, so one cannot train on overtly violent, sexually explicit, or racist content \citep{openai_moderation_api}. This has led to a body of work that aims to unalign models through harmless data or data that can't be identified as harmful \citep{xu2024shadowcaststealthydatapoisoning}.  Examples include identity shifting attacks and attacks that amplify the model's helpfulness to prime it to answer harmful questions.  Even training on standard SFT data can negatively affect model alignment \citep{qi_not_intend_to}.  Although there are many measures of susceptibility to data poisoning and post-training safety \citep{fu2024poisonbenchassessinglargelanguage, just_how_toxic, backdoor_benchmark, hsiung2025your, qi2024evaluatingdurabilitysafeguardsopenweight, peng2024navigatingsafetylandscapemeasuring}, to our knowledge, there is no existing method to identify which data is poisonous, making data filtering a challenge for companies like OpenAI and Anthropic.  

Due to the difficulty of identifying poison data, some researchers have suggested training-time defenses against harmful fine-tuning \citep{hong2024certified, yang2022not, qi2024alignment_deep, yi2025probe}.  Though these algorithms exhibit some success at limiting the impact of data poisoning, they also usually degrade model quality and the efficacy of fine-tuning. 
 This has led some to examine methods of enforcing alignment during inference \cite{lyu2025keepingllmsalignedfinetuning, eiras2025isafelymitigatingtaskspecific}.  

Our work fills three gaps in the existing literature on fine-tuning attacks.  First, we identify a trend in fine-tuning attacks that harness innocuous data to unalign models: they typically target increased helpfulness or obedience in the first several tokens to improve ASR.  Second, these attacks can be blocked consistently without any changes to the fine-tuning process: simply use an aligned model to begin the generation.  This presents another alternative \cite{yi2024safetyrealignmentframeworksubspaceoriented, huang2024antidotepostfinetuningsafetyalignment, zhu2024lockingfinetunedllmssafety, wu2025separatewheatchaffposthoc, yi2024nlsrneuronlevelsafetyrealignment} to training-time defenses that cope with data-poisoning and fine-tuning attacks \cite{huang2024vaccine, rosati2024representation, liu2024targetedvaccinesafetyalignment, du2024securetuningmitigatingsecurity, tamirisa2024tamper, huang2024boostertacklingharmfulfinetuning, mukhoti2024finetuningcripplefoundationmodel, wei2024assessingbrittlenesssafetyalignment, huang2024lisalazysafetyalignment, qi2024alignment_deep, anonymous2024identifying, liu2024robustifying, bianchi2024safetytuned, 10.5555/3692070.3694674, eiras2024mimickinguserdatamitigating, wang2024mitigatingfinetuningbasedjailbreak, li2025safetylayersalignedlarge, shen2024sealsafetyenhancedalignedllm, li2025safety, li2025salorasafetyalignmentpreservedlowrank, choi2024safetyawarefinetuninglargelanguage, casper2024defendingunforeseenfailuremodes, hsu2025safelorasilverlining}. Finally, drawing inspiration from successful pre-filling attacks \cite{christian2023jailbreak, lv2024adappaadaptivepositionprefill},  we broaden the scope of attacks by presenting a new attack paradigm: embrace refusal, but change its meaning. Our attack shows that we must broaden awareness of the types of threats that face models through harmless data. %\djcomment{Nice!}