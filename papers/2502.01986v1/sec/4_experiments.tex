\section{Experiments}
\label{sec:experiments}

\subsection{Datasets and Setup}
We evaluate DCT-Mamba3D on three benchmark hyperspectral image (HSI) classification datasets: Indian Pines, Kennedy Space Center (KSC), and Houston2013, each containing diverse land cover classes with distinct spectral characteristics. The Indian Pines dataset, featuring challenges like "same object, different spectra" and "different objects, same spectra," is especially suited for analyzing our model's spectral decorrelation capabilities. All datasets are split with 10\% for training and 90\% for testing, and model performance is averaged over 10 runs for robustness. Evaluation metrics include overall accuracy (OA), average accuracy (AA), Kappa coefficient, and F1-score per class. Our method is compared with leading models, including 2D-CNN~\cite{yang2018hyperspectral}, 3D-CNN~\cite{yang2020synergistic}, HybridSN~\cite{roy2019hybridsn}, ViT~\cite{dosovitskiy2020image}, HiT~\cite{yang2022hyperspectral}, MorphF~\cite{roy2023spectral}, SSFTT~\cite{sun2022spectral}, and MiM~\cite{zhou2024mamba}.

\subsection{Spectral Correlation Heatmaps}
Figure~\ref{fig:heatmaps} shows Spearman correlation heatmaps on the Indian Pines dataset for (a) 2D-CNN, (b) HiT, and (c) DCT-Mamba3D. In each heatmap, the x- and y-axes represent spectral bands, with high off-diagonal values indicating spectral redundancy. The 2D-CNN retains substantial redundancy, HiT reduces some but maintains redundancy in adjacent bands, while DCT-Mamba3D achieves marked decorrelation, contributing to improved classification performance.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/2dcnnlayer3_spearman_heatmap.png}
        \caption{2D-CNN}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/hitlayer3_spearman_heatmap.png}
        \caption{HiT}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\linewidth}
        \includegraphics[width=\linewidth]{figures/layer3_spearman_heatmap.png}
        \caption{DCT-Mamba3D}
    \end{subfigure}
    \caption{Spearman correlation heatmaps on the Indian Pines dataset, comparing (a) 2D-CNN, (b) HiT (Transformer-based), and (c) DCT-Mamba3D.}
    \label{fig:heatmaps}
\end{figure}

\subsection{Classification Accuracy Comparison}
We compare the classification performance of DCT-Mamba3D with leading models, including 2D-CNN, 3D-CNN, HybridSN, ViT, HiT, MorphF, SSFTT, and MiM. As shown in Table~\ref{tab:indian_pines}, DCT-Mamba3D demonstrates effective performance in hyperspectral image classification, particularly in challenging cases.

Our method demonstrates particular advantages in challenging cases:
- \textit{Same Object, Different Spectra}: For classes like Corn-notill and Corn-mintill, DCT-Mamba3D substantially improves classification accuracy, effectively capturing intra-class spectral variability. The decorrelation provided by the 3D Spectral Decorrelation Module enables the model to distinguish subtle spectral variations within similar classes, resulting in higher F1-scores for these classes.
- \textit{Different Objects, Same Spectra}: For classes such as Buildings-Grass-Trees-Drives and Stone-Steel-Towers, DCT-Mamba3D outperforms other models, highlighting its ability to reduce spectral redundancy and improve separability in classes with similar spectral characteristics.



\subsubsection{t-SNE Visualization Analysis}

We conducted a t-SNE visualization analysis to evaluate the effectiveness of DCT-Mamba3D in feature discrimination, as shown in Fig.~\ref{fig:tsne_comparison}. The results indicate that DCT-Mamba3D generates compact, well-separated clusters, demonstrating its robust capability in decorrelating complex spectral and spatial features. This enhanced decorrelation improves class separability by reducing interclass misclassification and strengthens in-class cohesion, addressing key challenges in hyperspectral imaging, such as “same object, different spectra” and “different objects, same spectra.” By leveraging 3D-SSDM, DCT-Mamba3D effectively minimizes spectral and spatial redundancies. It results in clear and distinct feature representations, particularly beneficial in scenarios with overlapping or redundant spectral information.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/2dcnn_tsne.png}
        \caption{2D-CNN}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/3dcnn_tsne.png}
        \caption{3D-CNN}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/hit_tsne.png}
        \caption{HiT}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/morphformer_tsne.png}
        \caption{MorphF}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/mimv1_tsne.png}
        \caption{MiM}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/mamba3d_tsne.png}
        \caption{DCT-Mamba3D}
    \end{subfigure}
    \caption{t-SNE visualization of feature embeddings across models on the KSC dataset.}
    \label{fig:tsne_comparison}
\end{figure*}

\begin{table*}[htbp]
\centering
\caption{Classification accuracy comparison on the Indian Pines dataset with 10\% training samples.}
\label{tab:indian_pines}
\scriptsize
\begin{tabular}{lccccccccc}
\toprule
\textbf{Class} & \textbf{2D-CNN} & \textbf{3D-CNN} & \textbf{HybridSN} & \textbf{ViT} & \textbf{HiT} & \textbf{MorphF} & \textbf{SSFTT} & \textbf{MiM} & \textbf{DCT-Mamba3D} \\
\midrule
Alfalfa & 92.82 & 70.82 & 34.67 & 76.55 & 0.00 & 79.58 & 89.95 & 76.20 & \textbf{95.19} \\
Corn-notill & 93.81 & 89.33 & 88.50 & 94.10 & 91.21 & 93.08 & 92.40 & 92.49 & \textbf{95.15} \\
Corn-mintill & 92.19 & 87.44 & 81.40 & 93.16 & 89.69 & 90.15 & 88.62 & 89.03 & \textbf{93.47} \\
Corn & 97.94 & 94.78 & 83.47 & \textbf{99.58} & 84.51 & 92.96 & 96.23 & 93.06 & 99.01 \\
Grass-pasture & 93.09 & 92.88 & 84.74 & 91.34 & 45.75 & \textbf{95.13} & 93.71 & 93.20 & 94.17 \\
Grass-trees & 95.65 & 94.41 & 82.42 & 89.85 & 94.06 & 95.80 & 94.43 & \textbf{96.01} & 94.72 \\
Grass-pasture-mowed & 7.94 & 0.00 & 1.21 & 0.00 & 0.00 & \textbf{66.25} & 56.16 & 32.43 & 16.11 \\
Hay-windrowed & 99.69 & 99.09 & 92.37 & 99.83 & \textbf{100.00} & 99.81 & 99.23 & 99.92 & 99.41 \\
Oats & \textbf{73.30} & 0.00 & 0.00 & 0.00 & 0.00 & 9.19 & 38.88 & 53.87 & 44.60 \\
Soybean-notill & 87.78 & 83.76 & 82.54 & \textbf{89.52} & 81.26 & 89.35 & 87.84 & 86.27 & 80.89 \\
Soybean-mintill & 96.26 & 94.33 & 93.02 & 96.58 & \textbf{96.92} & 96.71 & 96.08 & 96.27 & 96.77 \\
Soybean-clean & 91.80 & 89.17 & 81.26 & 91.69 & 87.08 & 88.93 & 87.39 & 84.99 & \textbf{93.85} \\
Wheat & \textbf{98.12} & 86.12 & 47.40 & 97.28 & 92.43 & 92.52 & 93.27 & 91.09 & 97.83 \\
Woods & 98.28 & 97.96 & 97.28 & 98.20 & 99.74 & 98.87 & \textbf{98.88} & 98.04 & 98.51 \\
Buildings-Grass-Trees-Drives & 97.82 & 92.51 & 74.80 & 98.05 & 87.90 & 95.08 & 94.88 & 92.95 & \textbf{98.74} \\
Stone-Steel-Towers & 52.74 & 51.18 & 15.74 & 34.79 & 0.00 & 55.80 & 34.81 & 15.01 & \textbf{68.36} \\
\midrule
\textbf{Accuracy (\%)} & 94.48 & 91.65 & 86.81 & 94.18 & 88.88 & 94.14 & 93.36 & 92.81 & \textbf{95.23} \\
\textbf{Kappa (\%)} & 93.69 & 90.45 & 84.87 & 93.35 & 87.24 & 93.30 & 92.42 & 91.78 & \textbf{94.55} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Performance Comparison on Kennedy Space Center Dataset (10\% Training Samples)}
\label{tab:ksc}
\scriptsize % Adjust to make it fit without shrinking too much
\begin{tabular}{lrrrrrrrrr}
\toprule
\textbf{Class} & \textbf{2D-CNN} & \textbf{3D-CNN} & \textbf{HySN} & \textbf{ViT} & \textbf{HiT} & \textbf{MorphF} & \textbf{SSFTT} & \textbf{MiM} & \textbf{DCT-Mamba3D} \\
\midrule
Scrub                & 97.38 & 95.96 & 95.89 & 91.62 & 86.13 & 80.24 & 92.20 & 98.32 & \textbf{99.02} \\
Willow swamp         & 95.16 & 87.20 & 87.77 & 74.05 & \textbf{98.69} & 57.59 & 88.83 & 93.30 & 98.13 \\
Cabbage palm hammock & 97.78 & 91.63 & 94.18 & 85.03 & \textbf{99.17} & 20.20 & 49.84 & 93.98 & 97.99 \\
Cabbage palm/oak ham & 92.82 & 83.38 & 89.29 & 93.50 & 98.36 & 52.04 & 82.66 & 74.07 & \textbf{99.84} \\
Slash pine           & 89.94 & 78.78 & 78.63 & 94.89 & 98.83 & 70.67 & 95.07 & 21.99 & \textbf{99.94} \\
Oak/broadleaf ham    & 94.68 & 93.27 & 90.85 & 93.21 & \textbf{99.74} & 41.96 & 90.31 & 93.72 & 98.72 \\
Hardwood swamp       & 98.79 & 99.32 & 99.05 & 99.50 & 99.02 & 25.76 & 66.07 & 91.61 & \textbf{100.00} \\
Graminoid marsh      & 94.66 & 97.62 & 96.55 & 89.51 & 96.78 & 67.25 & 85.22 & \textbf{99.62} & 98.74 \\
Spartina marsh       & 97.49 & 99.70 & 99.40 & 95.99 & \textbf{99.99} & 73.41 & 81.59 & 99.89 & 99.94 \\
Cattail marsh        & 97.17 & 97.37 & 98.36 & 97.75 & 99.03 & 89.24 & \textbf{100.00} & \textbf{100.00} & 99.98 \\
Salt marsh           & 99.65 & \textbf{99.98} & 98.06 & \textbf{100.00} & \textbf{100.00} & 97.67 & 98.28 & \textbf{100.00} & \textbf{100.00} \\
Mud flats            & 98.28 & 97.85 & 99.08 & 99.65 & 99.17 & 95.03 & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} \\
Water                & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} & 99.86 & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} & \textbf{100.00} \\
\midrule
\textbf{Accuracy (\%)} & 97.31 & 95.99 & 96.28 & 94.77 & 95.51 & 80.36 & 91.99 & 95.80 & \textbf{99.50} \\
\textbf{Kappa (\%)}    & 97.01 & 95.54 & 95.86 & 94.16 & 95.03 & 77.82 & 91.02 & 95.32 & \textbf{99.44} \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[htbp]
\centering
\caption{Comparison with Leading Transformer and Mamba Models on Houston2013 Dataset (10\% Training Samples)}
\label{tab:houston}
\scriptsize
\begin{tabular}{lccccccccc}
\toprule
\textbf{Class} & \textbf{2D-CNN} & \textbf{3D-CNN} & \textbf{HybridSN} & \textbf{ViT} & \textbf{HiT} & \textbf{MorphF} & \textbf{SSFTT} & \textbf{MiM} & \textbf{DCT-Mamba3D} \\
\midrule
Unclassified & 95.45 & 93.10 & 92.46 & 93.45 & 94.63 & 93.78 & 90.18 & 93.36 & \textbf{96.60} \\
Healthy Grass & 96.45 & 90.56 & 88.22 & 87.16 & 90.15 & 94.20 & 92.70 & 95.09 & \textbf{97.78} \\
Stressed Grass & 99.32 & 97.35 & 97.84 & 97.29 & 98.93 & 99.18 & \textbf{99.52} & 99.03 & 98.89 \\
Synthetic Grass & 92.42 & 89.78 & 81.79 & 85.45 & 87.57 & \textbf{97.76} & 93.53 & 91.13 & 96.54 \\
Soil & 99.57 & 97.53 & 97.47 & 99.56 & 99.66 & 99.74 & 99.33 & \textbf{99.90} & 99.30 \\
Water & 95.02 & 86.73 & 93.09 & 90.34 & 88.05 & 94.86 & \textbf{93.44} & 91.60 & 93.01 \\
Residential & 93.96 & 86.51 & 86.07 & 89.16 & 90.17 & 97.10 & 95.40 & 90.88 & \textbf{98.10} \\
Commercial & 96.59 & 89.12 & 92.09 & 96.86 & 96.95 & 98.21 & 96.59 & \textbf{99.39} & 97.38 \\
Road & 94.59 & 84.52 & 76.40 & 90.09 & 89.77 & 96.95 & 95.06 & 93.98 & \textbf{97.42} \\
Highway & 97.30 & 94.23 & 94.67 & 99.17 & 97.88 & 99.20 & 99.22 & 99.73 & \textbf{99.90} \\
Railway & 99.41 & 87.33 & 82.52 & 98.29 & 99.50 & 99.85 & 99.56 & 98.10 & \textbf{99.96} \\
Parking Lot 1 & 97.78 & 95.51 & 96.37 & 98.04 & 98.54 & 99.45 & 98.21 & \textbf{99.24} & 99.17 \\
Parking Lot 2 & 96.27 & 88.31 & 87.86 & 94.16 & 94.78 & \textbf{97.38} & 97.69 & 96.77 & 96.94 \\
Tennis Court & 99.92 & 98.84 & 96.61 & 99.85 & \textbf{99.95} & 99.87 & 99.51 & 99.78 & 99.85 \\
Running Track & 98.92 & 95.49 & 94.23 & 96.77 & 98.75 & 98.96 & \textbf{99.15} & 98.81 & 98.58 \\
\midrule
\textbf{Accuracy (\%)} & 96.66 & 91.36 & 90.09 & 94.09 & 94.87 & 97.75 & 96.37 & 96.32 & \textbf{98.15} \\
\textbf{Kappa (\%)} & 96.39 & 90.65 & 89.29 & 93.61 & 94.45 & 97.57 & 96.08 & 96.02 & \textbf{98.00} \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Training Loss Comparison}

To further demonstrate the efficiency of our proposed model, we compare the training loss curves of the DCT-Mamba3D model with those of a 2D-CNN and HiT, as shown in Figure~\ref{training_loss}. Each model's training loss was recorded across the training iterations to analyze convergence behavior. The results clearly show that the DCT-Mamba3D model achieves a faster and more stable convergence compared to the 2D-CNN and HiT baselines.

\begin{figure}[htbp]
    \centering
        \includegraphics[width=\linewidth]{figures/Fig60130.png}
    \caption{Training loss comparison between 2D-CNN, HiT, and the DCT-Mamba3D model. }
    \label{training_loss}
\end{figure}

\begin{table}[H]
\centering
\caption{Ablation Study Results on the Indian Pines Dataset, showing Overall Accuracy (OA), Average Accuracy (AA), and Kappa Score for each configuration.}
\label{tab:ablation_summary}
\resizebox{\linewidth}{!}{
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Configuration} & OA (\%) & AA (\%) & Kappa (\%) \\
\hline
3D-SDM Only & 93.55 & 82.63 & 92.65 \\
3D-MambaNet Only & 94.97 & 85.56 & 94.26 \\
No GRE & 94.62 & 85.08 & 93.87 \\
Full DCT-Mamba3D & 95.23 & 86.81 & 94.55 \\
\hline
\end{tabular}
}
\end{table}

\begin{table}[H]
\centering
\caption{DCT-Mamba3D Performance on Different Datasets and Sample Sizes, showing Overall Accuracy (OA) and Kappa Score for each configuration.}
\label{tab:data_quantity}
\resizebox{\linewidth}{!}{
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Dataset} & \textbf{Sample Size (\%)} & \textbf{OA (\%)} & \textbf{Kappa (\%)} \\
\hline
Indian Pines & 1\% & 49.58 & 40.68 \\
Indian Pines & 3\% & 77.81 & 74.59 \\
Indian Pines & 5\% & 82.79 & 80.21 \\
Indian Pines & 7\% & 86.67 & 84.69 \\
Indian Pines & 10\% & 95.23 & 94.55 \\
KSC & 1\% & 61.64 & 56.22 \\
KSC & 3\% & 78.18 & 75.45 \\
KSC & 5\% & 89.21 & 87.95 \\
KSC & 7\% & 93.54 & 92.80 \\
KSC & 10\% & 99.50 & 99.44 \\
Houston2013 & 1\% & 68.75 & 66.17 \\
Houston2013 & 3\% & 86.20 & 85.08 \\
Houston2013 & 5\% & 92.09 & 91.45 \\
Houston2013 & 7\% & 94.46 & 94.01 \\
Houston2013 & 10\% & 98.15 & 98.00 \\
\hline
\end{tabular}
}
\end{table}

\begin{table}[H]
\centering
\caption{Complexity Analysis on Indian Pines Dataset}
\label{complexity}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Method} & FLOPS & Parameters (M) \\
\hline
3D-CNN & 0.50 & 0.50 \\
HybridSN & 5.3 & 4.32 \\
ViT & 0.68 & 13.2 \\
HiT & 11.93 & 20.94 \\
MorphF & 0.17 & 0.24 \\
SSFTT & 0.24 & 0.93 \\
MiM & 0.50 & 0.18 \\
DCT-Mamba3D & 4.02 & 19.56 \\
\hline
\end{tabular}
\end{table}

The training loss curves highlight a significant advantage of the DCT-Mamba3D model in terms of convergence speed. It achieves near-zero training loss in fewer iterations than both 2D-CNN and HiT, showcasing its efficient feature extraction and decorrelation capabilities in the frequency domain. This rapid convergence not only reduces training time but also demonstrates the robustness of the model in handling complex spatial-spectral dependencies, further supporting its effectiveness in hyperspectral image classification.


\subsection{Ablation Study and Complexity Analysis}

We conducted an ablation study to evaluate the contributions of key components in the DCT-Mamba3D model. Three configurations were tested:

\begin{enumerate}
    \item \textit{3D-SDM Only}: Excludes the 3D-MambaNet, focusing on spectral decorrelation, achieving OA of 93.55\%, AA of 82.63\%, and Kappa score of 92.65\%.
    \item \textit{3D-MambaNet Only}: Excludes the 3D-SDM, focusing on spatial feature extraction, achieving OA of 94.97\%, AA of 85.56\%, and Kappa score of 94.26\%.
    \item \textit{No GRE (Global Residual Enhancement)}: Excludes the GRE, achieving OA of 94.62\%, AA of 85.08\%, and Kappa score of 93.87\%.
\end{enumerate}

Table~\ref{tab:ablation_summary} summarizes the OA, AA, and Kappa scores for each configuration, alongside the full DCT-Mamba3D model. The results highlight the model's integrated spatial-spectral feature extraction advantage, particularly for "same object, different spectra" and "different objects, same spectra" cases.

To further examine DCT-Mamba3D’s performance, we evaluated it on the Indian Pines, KSC, and Houston2013 datasets with varying training sample sizes, particularly under limited training samples, as shown in Table~\ref{tab:data_quantity}. The results indicate that DCT-Mamba3D maintains high OA and Kappa scores even with limited samples (e.g., 1\% and 3\%), underscoring its robust feature extraction capabilities. This effectiveness can be attributed to the 3D-SSDM’s spectral-spatial decorrelation, which reduces redundancy and enhances feature separability. As sample size increases, the model consistently improves in both OA and Kappa, demonstrating its scalability and robustness across varying data conditions, making it particularly suitable for challenging HSI classification tasks.

\subsubsection{Complexity Analysis}

We evaluated the computational complexity of DCT-Mamba3D relative to baseline models on the Indian Pines dataset, as detailed in Table~\ref{complexity}, focusing on FLOPS and parameter counts. While DCT-Mamba3D incurs a higher computational cost than CNN models, this is offset by substantial performance gains. Compared to other Mamba-based architectures, such as MiM, DCT-Mamba3D demonstrates an optimized balance of complexity, accuracy, and decorrelation effectiveness, making it well-suited for applications requiring both precision and computational efficiency.






