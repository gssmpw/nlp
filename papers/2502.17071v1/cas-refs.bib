IEEEexample.bib 
V1.13 (2008/09/30)
Copyright (c) 2002-2008 by Michael Shell
See: http://www.michaelshell.org/
for current contact information.

This is an example BibTeX database for the official IEEEtran.bst
BibTeX style file.

Some entries call strings that are defined in the IEEEabrv.bib file.
Therefore, IEEEabrv.bib should be loaded prior to this file.
Usage:

\bibliographystyle{./IEEEtran}
\bibliography{./IEEEabrv,./IEEEexample}


Support sites:
http://www.michaelshell.org/tex/ieeetran/
http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
and/or
http://www.ieee.org/

*************************************************************************
Legal Notice:
This code is offered as-is without any warranty either expressed or
implied; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE! 
User assumes all risk.
In no event shall IEEE or any contributor to this code be liable for
any damages or losses, including, but not limited to, incidental,
consequential, or any other damages, resulting from the use or misuse
of any information contained here.

All comments are the opinions of their respective authors and are not
necessarily endorsed by the IEEE.

This work is distributed under the LaTeX Project Public License (LPPL)
( http://www.latex-project.org/ ) version 1.3, and may be freely used,
distributed and modified. A copy of the LPPL, version 1.3, is included
in the base LaTeX documentation of all distributions of LaTeX released
2003/12/01 or later.
Retain all contribution notices and credits.
** Modified files should be clearly indicated as such, including  **
** renaming them and changing author support contact information. **

File list of work: IEEEabrv.bib, IEEEfull.bib, IEEEexample.bib,
                   IEEEtran.bst, IEEEtranS.bst, IEEEtranSA.bst,
                   IEEEtranN.bst, IEEEtranSN.bst, IEEEtran_bst_HOWTO.pdf
*************************************************************************


Note that, because the example references were taken from actual IEEE
publications, these examples do not always contain the full amount
of information that may be desirable (for use with other BibTeX styles).
In particular, full names (not abbreviated with initials) should be
entered whenever possible as some (non-IEEE) bibliography styles use
full names. IEEEtran.bst will automatically abbreviate when it encounters
full names.
 
 
 @inproceedings{10.1007/978-3-031-25891-6_6,
abstract = {Compression of the deep neural networks is a critical problem area when it comes to enhancing the capability of embedded devices. As deep neural networks are space and compute-intensive, they are generally unsuitable for use in edge devices and thereby lose their ubiquity. This paper discusses novel methods of neural network pruning, making them lighter, faster, and immune to noise and over-fitting without compromising the accuracy of the models. It poses two questions about the accepted methods of pruning and proffers two new strategies - evolution of weights and smart pruning to compress the deep neural networks better. These methods are then compared with the standard pruning mechanism on benchmark data sets to establish their efficiency. The code is made available online for public use.},
address = {Cham},
author = {Islam, Ashhadul and Belhaouari, Samir Brahim},
booktitle = {Machine Learning, Optimization, and Data Science},
editor = {Nicosia, Giuseppe and Ojha, Varun and {La Malfa}, Emanuele and {La Malfa}, Gabriele and Pardalos, Panos and {Di Fatta}, Giuseppe and Giuffrida, Giovanni and Umeton, Renato},
isbn = {978-3-031-25891-6},
mendeley-groups = {network compression},
pages = {62--76},
publisher = {Springer Nature Switzerland},
title = {{Smart Pruning of Deep Neural Networks Using Curve Fitting and Evolution of Weights}},
year = {2023}
}
@misc{Tessier2021,
author = {Tessier, Hugo},
booktitle = {Towards Data Science},
mendeley-groups = {network compression},
title = {{Neural Network Pruning 101: All you need to know not to get lost}},
url = {https://towardsdatascience.com/neural-network-pruning-101-af816aaea61},
year = {2021}
}
@article{wang2021nonlinear,
author = {Wang, Dingheng and Zhao, Guangshe and Chen, Hengnu and Liu, Zhexian and Deng, Lei and Li, Guoqi},
journal = {Neural Networks},
mendeley-groups = {network compression},
pages = {320--333},
publisher = {Elsevier},
title = {{Nonlinear tensor train format for deep neural network compression}},
volume = {144},
year = {2021}
}
@inproceedings{kusupati2020soft,
author = {Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
booktitle = {International Conference on Machine Learning},
mendeley-groups = {network compression},
organization = {PMLR},
pages = {5544--5555},
title = {{Soft threshold weight reparameterization for learnable sparsity}},
year = {2020}
}
@inproceedings{liu2021we,
author = {Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
booktitle = {International Conference on Machine Learning},
mendeley-groups = {network compression},
organization = {PMLR},
pages = {6989--7000},
title = {{Do we actually need dense over-parameterization? in-time over-parameterization in sparse training}},
year = {2021}
}
@article{liu2021sparse,
author = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
pages = {9908--9922},
title = {{Sparse training via boosting pruning plasticity with neuroregeneration}},
volume = {34},
year = {2021}
}
@article{chen2021chasing,
author = {Chen, Tianlong and Cheng, Yu and Gan, Zhe and Yuan, Lu and Zhang, Lei and Wang, Zhangyang},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
pages = {19974--19988},
title = {{Chasing sparsity in vision transformers: An end-to-end exploration}},
volume = {34},
year = {2021}
}
@article{tanaka2020pruning,
author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
pages = {6377--6389},
title = {{Pruning neural networks without any data by iteratively conserving synaptic flow}},
volume = {33},
year = {2020}
}
@article{wang2020picking,
author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
journal = {arXiv preprint arXiv:2002.07376},
mendeley-groups = {network compression},
title = {{Picking winning tickets before training by preserving gradient flow}},
year = {2020}
}
@article{molchanov2016pruning,
author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
journal = {arXiv preprint arXiv:1611.06440},
mendeley-groups = {network compression},
title = {{Pruning convolutional neural networks for resource efficient inference}},
year = {2016}
}
@article{fladmark2023exploring,
author = {Fladmark, Eirik and Sajjad, Muhammad Hamza and Justesen, Laura Brinkholm},
journal = {arXiv preprint arXiv:2303.15479},
mendeley-groups = {network compression},
title = {{Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis}},
year = {2023}
}
@article{chauvin1990back,
author = {Chauvin, Y},
journal = {Advances in Neural Information processing Systems},
mendeley-groups = {network compression},
pages = {595--603},
title = {{A back-propagation algorithm with optimal use of hidden units}},
volume = {1},
year = {1990}
}
@inproceedings{oh2022attentive,
author = {Oh, Junghun and Kim, Heewon and Nah, Seungjun and Hong, Cheeun and Choi, Jonghyun and Lee, Kyoung Mu},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {network compression},
pages = {17673--17682},
title = {{Attentive fine-grained structured sparsity for image restoration}},
year = {2022}
}
@article{renda2020comparing,
author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
journal = {arXiv preprint arXiv:2003.02389},
mendeley-groups = {network compression},
title = {{Comparing rewinding and fine-tuning in neural network pruning}},
year = {2020}
}
@article{jiang2022exposing,
author = {Jiang, Peng and Hu, Lihan and Song, Shihui},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
pages = {38345--38357},
title = {{Exposing and exploiting fine-grained block structures for fast and accurate sparse training}},
volume = {35},
year = {2022}
}
@article{choudhary2022inference,
author = {Choudhary, Tejalal and Mishra, Vipul and Goswami, Anurag and Sarangapani, Jagannathan},
journal = {Future Generation Computer Systems},
mendeley-groups = {network compression},
pages = {44--56},
publisher = {Elsevier},
title = {{Inference-aware convolutional neural network pruning}},
volume = {135},
year = {2022}
}
@article{zhu2022progressive,
author = {Zhu, Jihong and Pei, Jihong},
journal = {Neurocomputing},
mendeley-groups = {network compression},
pages = {360--378},
publisher = {Elsevier},
title = {{Progressive kernel pruning with saliency mapping of input-output channels}},
volume = {467},
year = {2022}
}
@article{yeom2021pruning,
author = {Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and M{\"{u}}ller, Klaus-Robert and Samek, Wojciech},
journal = {Pattern Recognition},
mendeley-groups = {network compression},
pages = {107899},
publisher = {Elsevier},
title = {{Pruning by explaining: A novel criterion for deep neural network pruning}},
volume = {115},
year = {2021}
}
@article{sanh2020movement,
author = {Sanh, Victor and Wolf, Thomas and Rush, Alexander},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
pages = {20378--20389},
title = {{Movement pruning: Adaptive sparsity by fine-tuning}},
volume = {33},
year = {2020}
}
@misc{Melody,
author = {Melody, Tsekeni},
booktitle = {Kaggle},
mendeley-groups = {network compression},
title = {{Hymenoptera Data}},
url = {https://www.kaggle.com/datasets/melodytsekeni/hymenoptera-data},
urldate = {2023-04-02}
}
@misc{Ludvigsen2023,
abstract = {This article attempts to estimate the carbon footprint of the popular OpenAI chatbot called ChatGPT},
author = {Ludvigsen, Kasper Groes Albin},
booktitle = {Medium},
mendeley-groups = {network compression},
title = {{The Carbon Footprint of ChatGPT}},
url = {https://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d},
urldate = {2023-02-26},
year = {2023}
}
@article{DBLP:journals/corr/HeZS17,
author = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
journal = {CoRR},
mendeley-groups = {network compression},
title = {{Channel Pruning for Accelerating Very Deep Neural Networks}},
url = {http://arxiv.org/abs/1707.06168},
volume = {abs/1707.0},
year = {2017}
}
@article{DBLP:journals/corr/abs-2001-01050,
author = {Liu, Jing and Zhuang, Bohan and Zhuang, Zhuangwei and Guo, Yong and Huang, Junzhou and Zhu, Jin-Hui and Tan, Mingkui},
journal = {CoRR},
mendeley-groups = {network compression},
title = {{Discrimination-aware Network Pruning for Deep Model Compression}},
url = {http://arxiv.org/abs/2001.01050},
volume = {abs/2001.0},
year = {2020}
}
@article{gale2019state,
author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
journal = {arXiv preprint arXiv:1902.09574},
mendeley-groups = {network compression},
title = {{The state of sparsity in deep neural networks}},
year = {2019}
}
@article{li2016pruning,
author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
journal = {arXiv preprint arXiv:1608.08710},
mendeley-groups = {network compression},
title = {{Pruning filters for efficient convnets}},
year = {2016}
}
@article{louizos2017learning,
author = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
journal = {arXiv preprint arXiv:1712.01312},
mendeley-groups = {network compression},
title = {{Learning sparse neural networks through $ L\_0 $ regularization}},
year = {2017}
}
@article{10.1145/3005348,
abstract = {Real-time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks: feature map-wise, kernel-wise, and intra-kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, in parallel computing environments, and in hardware-based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by assessing the misclassification rate with a corresponding connectivity pattern. The pruned network is retrained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra-kernel strided sparsity with a simple constraint can significantly reduce the size of the kernel and feature map tensors. The proposed work shows that when pruning granularities are applied in combination, we can prune the CIFAR-10 network by more than 70\% with less than a 1\% loss in accuracy.},
address = {New York, NY, USA},
author = {Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
doi = {10.1145/3005348},
issn = {1550-4832},
journal = {J. Emerg. Technol. Comput. Syst.},
keywords = {Deep convolutional neural networks,feature map pruning,intra-kernel strided sparsity,structured pruning},
mendeley-groups = {network compression},
month = {feb},
number = {3},
publisher = {Association for Computing Machinery},
title = {{Structured Pruning of Deep Convolutional Neural Networks}},
url = {https://doi.org/10.1145/3005348},
volume = {13},
year = {2017}
}
@article{wen2016learning,
author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Learning structured sparsity in deep neural networks}},
volume = {29},
year = {2016}
}
@article{article,
author = {Kruschke, J K and Movellan, Javier},
doi = {10.1109/21.101159},
journal = {Systems, Man and Cybernetics, IEEE Transactions on},
mendeley-groups = {network compression},
pages = {273--280},
title = {{Benefits of Gain: Speeding Learning and Minimal Hidden Layers in Back-Propagation Networks}},
volume = {21},
year = {1991}
}
@article{tai2015convolutional,
author = {Tai, Cheng and Xiao, Tong and Zhang, Yi and Wang, Xiaogang and Others},
journal = {arXiv preprint arXiv:1511.06067},
mendeley-groups = {network compression},
title = {{Convolutional neural networks with low-rank regularization}},
year = {2015}
}
@article{jaderberg2014speeding,
author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
journal = {arXiv preprint arXiv:1405.3866},
mendeley-groups = {network compression},
title = {{Speeding up convolutional neural networks with low rank expansions}},
year = {2014}
}
@inproceedings{malinowski2015ask,
author = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
booktitle = {Proceedings of the IEEE international conference on computer vision},
mendeley-groups = {network compression},
pages = {1--9},
title = {{Ask your neurons: A neural-based approach to answering questions about images}},
year = {2015}
}
@article{guo2016dynamic,
author = {Guo, Yiwen and Yao, Anbang and Chen, Yurong},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Dynamic network surgery for efficient dnns}},
volume = {29},
year = {2016}
}
@article{zhang2015accelerating,
author = {Zhang, Xiangyu and Zou, Jianhua and He, Kaiming and Sun, Jian},
journal = {IEEE transactions on pattern analysis and machine intelligence},
mendeley-groups = {network compression},
number = {10},
pages = {1943--1955},
publisher = {IEEE},
title = {{Accelerating very deep convolutional networks for classification and detection}},
volume = {38},
year = {2015}
}
@article{courbariaux2015binaryconnect,
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Binaryconnect: Training deep neural networks with binary weights during propagations}},
volume = {28},
year = {2015}
}
@article{ye2018rethinking,
author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z},
journal = {arXiv preprint arXiv:1802.00124},
mendeley-groups = {network compression},
title = {{Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers}},
year = {2018}
}
@inproceedings{yu2018nisp,
author = {Yu, Ruichi and Li, Ang and Chen, Chun-Fu and Lai, Jui-Hsin and Morariu, Vlad I and Han, Xintong and Gao, Mingfei and Lin, Ching-Yung and Davis, Larry S},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {9194--9203},
title = {{Nisp: Pruning networks using neuron importance score propagation}},
year = {2018}
}
@article{han2015learning,
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Learning both weights and connections for efficient neural network}},
volume = {28},
year = {2015}
}
@inproceedings{liu2017learning,
author = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
booktitle = {Proceedings of the IEEE international conference on computer vision},
mendeley-groups = {network compression},
pages = {2736--2744},
title = {{Learning efficient convolutional networks through network slimming}},
year = {2017}
}
@inproceedings{zhang2018systematic,
author = {Zhang, Tianyun and Ye, Shaokai and Zhang, Kaiqi and Tang, Jian and Wen, Wujie and Fardad, Makan and Wang, Yanzhi},
booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
mendeley-groups = {network compression},
pages = {184--199},
title = {{A systematic dnn weight pruning framework using alternating direction method of multipliers}},
year = {2018}
}
@article{iandola2016squeezenet,
author = {Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
journal = {arXiv preprint arXiv:1602.07360},
mendeley-groups = {network compression},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size}},
year = {2016}
}
@article{liu2018rethinking,
author = {Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
journal = {arXiv preprint arXiv:1810.05270},
mendeley-groups = {network compression},
title = {{Rethinking the value of network pruning}},
year = {2018}
}
@article{sun2015deepid3,
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
journal = {arXiv preprint arXiv:1502.00873},
mendeley-groups = {network compression},
title = {{Deepid3: Face recognition with very deep neural networks}},
year = {2015}
}
@article{srivastava2015training,
author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Training very deep networks}},
volume = {28},
year = {2015}
}
@inproceedings{ma2018shufflenet,
author = {Ma, Ningning and Zhang, Xiangyu and Zheng, Hai-Tao and Sun, Jian},
booktitle = {Proceedings of the European conference on computer vision (ECCV)},
mendeley-groups = {network compression},
pages = {116--131},
title = {{Shufflenet v2: Practical guidelines for efficient cnn architecture design}},
year = {2018}
}
@article{8416559,
author = {Luo, Jian-Hao and Zhang, Hao and Zhou, Hong-Yu and Xie, Chen-Wei and Wu, Jianxin and Lin, Weiyao},
doi = {10.1109/TPAMI.2018.2858232},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
mendeley-groups = {network compression},
number = {10},
pages = {2525--2538},
title = {{ThiNet: Pruning CNN Filters for a Thinner Net}},
volume = {41},
year = {2019}
}
@inproceedings{he2016deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@inproceedings{hu2017learning,
author = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
booktitle = {Proceedings of the IEEE international conference on computer vision},
mendeley-groups = {network compression},
pages = {804--813},
title = {{Learning to reason: End-to-end module networks for visual question answering}},
year = {2017}
}
@article{yu2015multi,
author = {Yu, Fisher and Koltun, Vladlen},
journal = {arXiv preprint arXiv:1511.07122},
mendeley-groups = {network compression},
title = {{Multi-scale context aggregation by dilated convolutions}},
year = {2015}
}
@article{chen2014semantic,
author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
journal = {arXiv preprint arXiv:1412.7062},
mendeley-groups = {network compression},
title = {{Semantic image segmentation with deep convolutional nets and fully connected crfs}},
year = {2014}
}
@inproceedings{noh2015learning,
author = {Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
booktitle = {Proceedings of the IEEE international conference on computer vision},
mendeley-groups = {network compression},
pages = {1520--1528},
title = {{Learning deconvolution network for semantic segmentation}},
year = {2015}
}
@inproceedings{long2015fully,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {3431--3440},
title = {{Fully convolutional networks for semantic segmentation}},
year = {2015}
}
@inproceedings{deng2019arcface,
author = {Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {4690--4699},
title = {{Arcface: Additive angular margin loss for deep face recognition}},
year = {2019}
}
@inproceedings{schroff2015facenet,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {815--823},
title = {{Facenet: A unified embedding for face recognition and clustering}},
year = {2015}
}
@inproceedings{pohlen2017full,
author = {Pohlen, Tobias and Hermans, Alexander and Mathias, Markus and Leibe, Bastian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {4151--4160},
title = {{Full-resolution residual networks for semantic segmentation in street scenes}},
year = {2017}
}
@inproceedings{nagel2019data,
author = {Nagel, Markus and van Baalen, Mart and Blankevoort, Tijmen and Welling, Max},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
mendeley-groups = {network compression},
pages = {1325--1334},
title = {{Data-free quantization through weight equalization and bias correction}},
year = {2019}
}
@inproceedings{rastegari2016xnor,
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
booktitle = {European conference on computer vision},
mendeley-groups = {network compression},
organization = {Springer},
pages = {525--542},
title = {{Xnor-net: Imagenet classification using binary convolutional neural networks}},
year = {2016}
}
@inproceedings{lee2019overcoming,
author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
mendeley-groups = {network compression},
pages = {312--321},
title = {{Overcoming catastrophic forgetting with unlabeled data in the wild}},
year = {2019}
}
@inproceedings{qin2019thundernet,
author = {Qin, Zheng and Li, Zeming and Zhang, Zhaoning and Bao, Yiping and Yu, Gang and Peng, Yuxing and Sun, Jian},
booktitle = {Proceedings of the IEEE/CVF international conference on computer vision},
mendeley-groups = {network compression},
pages = {6718--6727},
title = {{ThunderNet: Towards real-time generic object detection on mobile devices}},
year = {2019}
}
@inproceedings{rastegari2016xnor,
author = {Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
booktitle = {European conference on computer vision},
mendeley-groups = {network compression},
organization = {Springer},
pages = {525--542},
title = {{Xnor-net: Imagenet classification using binary convolutional neural networks}},
year = {2016}
}
@inproceedings{sandler2018mobilenetv2,
author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {4510--4520},
title = {{Mobilenetv2: Inverted residuals and linear bottlenecks}},
year = {2018}
}
@article{howard2017mobilenets,
author = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
journal = {arXiv preprint arXiv:1704.04861},
mendeley-groups = {network compression},
title = {{Mobilenets: Efficient convolutional neural networks for mobile vision applications}},
year = {2017}
}
@article{zhou2017incremental,
author = {Zhou, Aojun and Yao, Anbang and Guo, Yiwen and Xu, Lin and Chen, Yurong},
journal = {arXiv preprint arXiv:1702.03044},
mendeley-groups = {network compression},
title = {{Incremental network quantization: Towards lossless cnns with low-precision weights}},
year = {2017}
}
@inproceedings{xu2016ask,
author = {Xu, Huijuan and Saenko, Kate},
booktitle = {European conference on computer vision},
mendeley-groups = {network compression},
organization = {Springer},
pages = {451--466},
title = {{Ask, attend and answer: Exploring question-guided spatial attention for visual question answering}},
year = {2016}
}
@article{denton2014exploiting,
author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Exploiting linear structure within convolutional networks for efficient evaluation}},
volume = {27},
year = {2014}
}
@inproceedings{noh2016image,
author = {Noh, Hyeonwoo and Seo, Paul Hongsuck and Han, Bohyung},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
mendeley-groups = {network compression},
pages = {30--38},
title = {{Image question answering using convolutional neural network with dynamic parameter prediction}},
year = {2016}
}
@article{Blalock2020,
abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
archivePrefix = {arXiv},
journal = {arXiv},
arxivId = {2003.03033},
author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
eprint = {2003.03033},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Blalock et al. - 2020 - What is the State of Neural Network Pruning(2).pdf:pdf},
mendeley-groups = {network compression},
title = {{What is the State of Neural Network Pruning?}},
url = {http://arxiv.org/abs/2003.03033},
year = {2020}
}
@article{Shen2020,
abstract = {In this paper, we propose a novel network training mechanism called”dynamic channel propagation” to prune the neural networks during the training period. In particular, we pick up a specific group of channels in each convolutional layer to participate in the forward propagation in training time according to the significance level of channel, which is defined as channel utility. The utility values with respect to all selected channels are updated simultaneously with the error back-propagation process and will adaptively change. Furthermore, when the training ends, channels with high utility values are retained whereas those with low utility values are discarded. Hence, our proposed scheme trains and prunes neural networks simultaneously. We empirically evaluate our novel training scheme on various representative benchmark datasets and advanced convolutional neural network (CNN) architectures, including VGGNet and ResNet. The experiment results verify the superior performance and robust effectiveness of our approach.},
archivePrefix = {arXiv},
arxivId = {2007.01486},
author = {Shen, Shibo and Li, Rongpeng and Zhao, Zhifeng and Zhang, Honggang and Zhou, Yugeng},
doi = {10.1109/ICPR48806.2021.9412191},
eprint = {2007.01486},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Shen et al. - 2020 - Learning to prune in training via dynamic channel propagation(2).pdf:pdf},
isbn = {9781728188089},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
mendeley-groups = {network compression},
pages = {939--945},
title = {{Learning to prune in training via dynamic channel propagation}},
year = {2020}
}
@article{Deng2012,
author = {Deng, Li},
journal = {IEEE Signal Processing Magazine},
mendeley-groups = {network compression},
number = {6},
pages = {141--142},
title = {{The mnist database of handwritten digit images for machine learning research}},
volume = {29},
year = {2012}
}
@article{Xiao2017,
abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
archivePrefix = {arXiv},
arxivId = {1708.07747},
author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
eprint = {1708.07747},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Xiao, Rasul, Vollgraf - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmarking Machine Learning Algorithms(2).pdf:pdf},
mendeley-groups = {network compression},
pages = {1--6},
title = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
url = {http://arxiv.org/abs/1708.07747},
year = {2017}
}
@book{vanderHeijden1994,
abstract = {What makes this book unique is that besides information on image processing of objects to yield knowledge, the author has devoted a lot of thought to the measurement factor of image processing. This is of direct practical use in numerous sectors from industrial quality and robotics to medicine and biology.},
author = {Heijden, Ferdinand},
mendeley-groups = {network compression},
title = {{Image based measurement systems: object recognition and parameter estimation}},
year = {1994}
}
@article{Werbos1988,
abstract = {Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues, many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation, developed in 1974, which does more justice to the roots of the method in numerical analysis and statistics, and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares), to optimization, to sensitivity analysis, and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output), such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart, Hinton, and Williams, this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets, where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place. {\textcopyright} 1988.},
author = {Werbos, Paul J.},
doi = {10.1016/0893-6080(88)90007-X},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Werbos - 1988 - Generalization of backpropagation with application to a recurrent gas market model(2).pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Backpropagation,Cerebral cortex,Continuous time,Energy models,Modelling,Prediction,Recurrent,Reinforcement learning},
mendeley-groups = {network compression},
number = {4},
pages = {339--356},
title = {{Generalization of backpropagation with application to a recurrent gas market model}},
volume = {1},
year = {1988}
}
@article{Zipser1995,
abstract = {Recurrent connectionist networks are important because they can perform temporally extended tasks, giving them considerable power beyond the static mappings performed by the now-familiar multilayer feedforward networks. This ability to perform highly nonlinear dynamic mappings makes these networks particularly interesting to study and potentially quite useful in tasks that which have an independent temporal component not easily handled through the use of simple tapped delay lines. Some examples are tasks involving recognition or generation of sequential patterns and sensorimotor control. This report examines a number of learning procedures for adjusting the weights in recurrent networks in order to train such networks to produce desired temporal behaviors from input-output stream examples. The procedures are all based on the computation of the gradient of performance error with respect to network weights, and a number of strategies for computing the necessary gradient information are described. Included here are approaches which are familiar and have been first described elsewhere, along with several novel approaches. One particular purpose of this report is to provide uniform and detailed descriptions and derivations of the various techniques in order to emphasize how they relate to one another. Another important contribution of this report is a detailed analysis of the computational requirements of the various approaches discussed.},
author = {Zipser, David and Williams, Ronald J},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Zipser, Williams - 1995 - Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity(2).pdf:pdf},
journal = {Back-propagation: Theory, Architectures and Applications},
mendeley-groups = {network compression},
pages = {433--486},
title = {{Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity}},
year = {1995}
}
@article{Salman2018,
abstract = {Recent literature in the robot learning community has focused on learning robot skills that abstract out lower-level details of robot control, such as Dynamic Movement Primitives (DMPs), the options framework in hierarchical RL, and subtask policies. To fully leverage the efficacy of these macro actions, it is necessary to then sequence these primitives to achieve a given task. Our objective is to jointly learn a set of robot skills and a sequence of these learnt skills to accomplish a given task. We consider the task of navigating a robot across various environments using visual input, maximizing the distance traveled through the environment while avoiding static obstacles. Traditional planning methods to solve this problem rely on hand-crafted state representations and heuristics for planning, and often fail to generalize. In contrast, deep neural networks have proved to be powerful function approximators, successfully modeling complex control policies. In addition, the ability of such networks to learn good representations of high-dimensional sensory inputs makes them a valuable tool when dealing with visual inputs. In this project, we explore the capability of deep neural networks to learn and sequence robot skills for navigation, directly using visual input.},
archivePrefix = {arXiv},
arxivId = {1803.01446},
author = {Salman, Hadi and Grover, Jaskaran and Shankar, Tanmay},
doi = {10.1162/NECO},
eprint = {1803.01446},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Salman, Grover, Shankar - 2018 - Hierarchical Reinforcement Learning for Sequencing Behaviors(2).pdf:pdf},
mendeley-groups = {network compression},
number = {March},
pages = {2709--2733},
title = {{Hierarchical Reinforcement Learning for Sequencing Behaviors}},
url = {http://arxiv.org/abs/1803.01446},
volume = {2733},
year = {2018}
}
@article{Frankle2019,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
eprint = {1803.03635},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Frankle, Carbin - 2019 - The lottery ticket hypothesis Finding sparse, trainable neural networks(2).pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
mendeley-groups = {network compression},
pages = {1--42},
title = {{The lottery ticket hypothesis: Finding sparse, trainable neural networks}},
year = {2019}
}
@inproceedings{Zhu2017a,
abstract = {Model pruning seeks to induce sparsity in a deep neural network's various con- nection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015a; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outper- form small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy},
author = {Zhu, Michael H and Gupta, Suyog},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Zhu, Gupta - 2017 - To prune, or not to prune exploring the efficacy of pruning for model compression(2).pdf:pdf},
mendeley-groups = {network compression},
title = {{To prune, or not to prune: exploring the efficacy of pruning for model compression}},
year = {2017}
}
@article{Ranzato2014,
abstract = {We propose a strong baseline model for unsupervised feature learning using video data. By learning to predict missing frames or extrapolate future frames from an input video sequence, the model discovers both spatial and temporal correlations which are useful to represent complex deformations and motion patterns. The models we propose are largely borrowed from the language modeling literature, and adapted to the vision domain by quantizing the space of image patches into a large dictionary. We demonstrate the approach on both a filling and a generation task. For the first time, we show that, after training on natural videos, such a model can predict non-trivial motions over short video sequences.},
archivePrefix = {arXiv},
arxivId = {1412.6604},
author = {Ranzato, MarcAurelio and Szlam, Arthur and Bruna, Joan and Mathieu, Michael and Collobert, Ronan and Chopra, Sumit},
eprint = {1412.6604},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Ranzato et al. - 2014 - Video (language) modeling a baseline for generative models of natural videos(2).pdf:pdf},
mendeley-groups = {network compression},
pages = {1--15},
title = {{Video (language) modeling: a baseline for generative models of natural videos}},
url = {http://arxiv.org/abs/1412.6604},
year = {2014}
}
@article{Robinson1987,
abstract = {Error propagation networks are able to learn a variety of tasks in which a static input pattern is mapped to a static output pattern.  This paper presents a generalisation of these nets to deal with time varying, or dynamic patterns.  Three possible architectures are explored which deal with learning sequences of known and finite length and unknown and possibly infinite length.  Several examples are given and an application to speech coding is discussed.},
author = {Robinson, A.J. and Fallside, F.},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Robinson, Fallside - 1987 - The utility driven dynamic error propagation network(2).pdf:pdf},
journal = {Ieee},
mendeley-groups = {network compression},
title = {{The utility driven dynamic error propagation network}},
volume = {1},
year = {1987}
}
@article{Guo2018,
abstract = {During the long history of computer vision, one of the grand challenges has been semantic segmentation which is the ability to segment an unknown image into different parts and objects (e.g., beach, ocean, sun, dog, swimmer). Furthermore, segmentation is even deeper than object recognition because recognition is not necessary for segmentation. Specifically, humans can perform image segmentation without even knowing what the objects are (for example, in satellite imagery or medical X-ray scans, there may be several objects which are unknown, but they can still be segmented within the image typically for further investigation). Performing segmentation without knowing the exact identity of all objects in the scene is an important part of our visual understanding process which can give us a powerful model to understand the world and also be used to improve or augment existing computer vision techniques. Herein this work, we review the field of semantic segmentation as pertaining to deep convolutional neural networks. We provide comprehensive coverage of the top approaches and summarize the strengths, weaknesses and major challenges.},
author = {Guo, Yanming and Liu, Yu and Georgiou, Theodoros and Lew, Michael S.},
doi = {10.1007/s13735-017-0141-z},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Guo et al. - 2018 - A review of semantic segmentation using deep neural networks(2).pdf:pdf},
issn = {2192662X},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Computer vision,Convolutional neural networks,Deep learning,Image segmentation,Machine learning},
mendeley-groups = {network compression},
number = {2},
pages = {87--93},
publisher = {Springer London},
title = {{A review of semantic segmentation using deep neural networks}},
url = {https://doi.org/10.1007/s13735-017-0141-z},
volume = {7},
year = {2018}
}
@article{Ye2018,
abstract = {Model pruning has become a useful technique that improves the computational efficiency of deep learning, making it possible to deploy solutions in resource-limited scenarios. A widely-used practice in relevant work assumes that a smaller-norm parameter or feature plays a less informative role at the inference time. In this paper, we propose a channel pruning technique for accelerating the computations of deep convolutional neural networks (CNNs) that does not critically rely on this assumption. Instead, it focuses on direct simplification of the channel-to-channel computation graph of a CNN without the need of performing a computationally difficult and not-always-useful task of making high-dimensional tensors of CNN structured sparse. Our approach takes two stages: first to adopt an end-to-end stochastic training method that eventually forces the outputs of some channels to be constant, and then to prune those constant channels from the original neural network by adjusting the biases of their impacting layers such that the resulting compact model can be quickly fine-tuned. Our approach is mathematically appealing from an optimization perspective and easy to reproduce. We experimented our approach through several image learning benchmarks and demonstrate its interesting aspects and competitive performance.},
archivePrefix = {arXiv},
arxivId = {1802.00124},
author = {Ye, Jianbo and Lu, Xin and Lin, Zhe and Wang, James Z.},
eprint = {1802.00124},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Ye et al. - 2018 - Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers(2).pdf:pdf},
journal = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
mendeley-groups = {network compression},
number = {2017},
pages = {1--11},
title = {{Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers}},
year = {2018}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
author = {Rosenblatt, F},
journal = {Psychological Review},
mendeley-groups = {network compression},
number = {6},
pages = {386--408},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@misc{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
booktitle = {Neural computation},
mendeley-groups = {network compression},
number = {4},
pages = {541--551},
title = {{Backpropagation applied to digit recognition}},
volume = {1},
year = {1989}
}
@article{Weng1993,
abstract = {A framework called Cresceptron is introduced for automatic algorithm design through learning of concepts and rules, thus deviating from the traditional mode in which humans specify the rules comprising a vision algorithm. With the Cresceptron, humans as designers need only to provide a good structure for learning, but they are relieved of most design details. The Cresceptron has been tested on the task of visual recognition: recognizing 3-D general objects from 2-D photographic images of natural scenes and segmenting the recognized objects from the cluttered image background. The Cresceptron uses a hierarchical structure to grow networks automatically, adaptively and incrementally through learning. The Cresceptron makes it possible to generalize training exemplars to other perceptually equivalent items. Experiments with a variety of real-world images are reported to demonstrate the feasibility of learning in the Cresceptron.},
author = {Weng, John J. and Ahuja, N. and Huang, T. S.},
doi = {10.1109/iccv.1993.378228},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Weng, Ahuja, Huang - 1993 - Learning recognition and segmentation of 3-D objects from 2-D images(2).pdf:pdf},
isbn = {0818638729},
journal = {1993 IEEE 4th International Conference on Computer Vision},
mendeley-groups = {network compression},
pages = {121--127},
title = {{Learning recognition and segmentation of 3-D objects from 2-D images}},
year = {1993}
}
@article{Sharma2017,
abstract = {Deep learning has achieved remarkable success in various machine learning and computer vision applications. The learning allows multiple processing layers to learn features by themselves opposite to conventional machine learning approaches which were not able to process the data in their natural form. Deep convolution networks have shown great performance in processing images and videos, whereas recurrent nets have shown great success for sequential data. This paper reviews all the aspects and researches done till now in this area along with their future possibilities.},
author = {Sharma, Poonam and Singh, Akansha},
doi = {10.1109/ICCCNT.2017.8203938},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Sharma, Singh - 2017 - Era of deep neural networks A review(2).pdf:pdf},
isbn = {9781509030385},
journal = {8th International Conference on Computing, Communications and Networking Technologies, ICCCNT 2017},
keywords = {ConvNets,DNN,RNN,ReLU},
mendeley-groups = {network compression},
title = {{Era of deep neural networks: A review}},
year = {2017}
}

@inproceedings{BabakHassibi2014,
 author = {Hassibi, Babak and Stork, David},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Second order derivatives for network pruning: Optimal Brain Surgeon},
 url = {https://proceedings.neurips.cc/paper_files/paper/1992/file/303ed4c69846ab36c2904d3ba8573050-Paper.pdf},
 volume = {5},
 year = {1992}
}

@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Fukushima - 1980 - Neocognitron A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in (2).pdf:pdf},
issn = {03401200},
journal = {Biological Cybernetics},
mendeley-groups = {network compression},
number = {4},
pages = {193--202},
pmid = {7370364},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{Carreira-Perpinan2018,
abstract = {Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates 'learning' steps that optimize a regularized, data-dependent loss and 'compression' steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes.},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Miguel A. and Idelbayev, Yerlan},
doi = {10.1109/CVPR.2018.00890},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Carreira-Perpi{\~{n}}{\'{a}}n, Idelbayev - 2018 - 'Learning-Compression' Algorithms for Neural Net Pruning(2).pdf:pdf},
isbn = {9781538664209},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
mendeley-groups = {network compression},
number = {Lc},
pages = {8532--8541},
title = {{'Learning-Compression' Algorithms for Neural Net Pruning}},
year = {2018}
}
@article{Zhang2018,
abstract = {Weight pruning methods for deep neural networks (DNNs) have been investigated recently, but prior work in this area is mainly heuristic, iterative pruning, thereby lacking guarantees on the weight reduction ratio and convergence time. To mitigate these limitations, we present a systematic weight pruning framework of DNNs using the alternating direction method of multipliers (ADMM). We first formulate the weight pruning problem of DNNs as a nonconvex optimization problem with combinatorial constraints specifying the sparsity requirements, and then adopt the ADMM framework for systematic weight pruning. By using ADMM, the original nonconvex optimization problem is decomposed into two subproblems that are solved iteratively. One of these subproblems can be solved using stochastic gradient descent, the other can be solved analytically. Besides, our method achieves a fast convergence rate. The weight pruning results are very promising and consistently outperform the prior work. On the LeNet-5 model for the MNIST data set, we achieve 71.2× weight reduction without accuracy loss. On the AlexNet model for the ImageNet data set, we achieve 21× weight reduction without accuracy loss. When we focus on the convolutional layer pruning for computation reductions, we can reduce the total computation by five times compared with the prior work (achieving a total of 13.4 × weight reduction in convolutional layers). Our models and codes are released at https://github.com/KaiqiZhang/admm-pruning.},
archivePrefix = {arXiv},
arxivId = {1804.03294},
author = {Zhang, Tianyun and Ye, Shaokai and Zhang, Kaiqi and Tang, Jian and Wen, Wujie and Fardad, Makan and Wang, Yanzhi},
doi = {10.1007/978-3-030-01237-3_12},
eprint = {1804.03294},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2018 - A systematic DNN weight pruning framework using alternating direction method of multipliers(2).pdf:pdf},
isbn = {9783030012366},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Alternating direction method of multipliers (ADMM),Deep neural networks (DNNs),Systematic weight pruning},
mendeley-groups = {network compression},
pages = {191--207},
title = {{A systematic DNN weight pruning framework using alternating direction method of multipliers}},
volume = {11212 LNCS},
year = {2018}
}
@article{Aghasi2017,
abstract = {We introduce and analyze a new technique for model reduction for deep neural networks. While large networks are theoretically capable of learning arbitrarily complex models, overfitting and model redundancy negatively affects the prediction accuracy and model variance. Our Net-Trim algorithm prunes (sparsifies) a trained network layer-wise, removing connections at each layer by solving a convex optimization program. This program seeks a sparse set of weights at each layer that keeps the layer inputs and outputs consistent with the originally trained model. The algorithms and associated analysis are applicable to neural networks operating with the rectified linear unit (ReLU) as the nonlinear activation. We present both parallel and cascade versions of the algorithm. While the latter can achieve slightly simpler models with the same generalization performance, the former can be computed in a distributed manner. In both cases, Net-Trim significantly reduces the number of connections in the network, while also providing enough regularization to slightly reduce the generalization error. We also provide a mathematical analysis of the consistency between the initial network and the retrained model. To analyze the model sample complexity, we derive the general sufficient conditions for the recovery of a sparse transform matrix. For a single layer taking independent Gaussian random vectors of length N as inputs, we show that if the network response can be described using a maximum number of s non-zero weights per node, these weights can be learned from O(s logN) samples.},
archivePrefix = {arXiv},
arxivId = {1611.05162},
author = {Aghasi, Alireza and Abdi, Afshin and Nguyen, Nam and Romberg, Justin},
eprint = {1611.05162},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Aghasi et al. - 2017 - Net-Trim Convex pruning of deep neural networks with performance guarantee(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
pages = {3178--3187},
title = {{Net-Trim: Convex pruning of deep neural networks with performance guarantee}},
volume = {2017-Decem},
year = {2017}
}
@article{Dong2017,
abstract = {How to develop slim and accurate deep neural networks has become crucial for real-world applications, especially for those employed in embedded systems. Though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance. In this paper, we propose a new layer-wise pruning method for deep neural networks. In our proposed method, parameters of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding parameters. We prove that the final prediction performance drop after pruning is bounded by a linear combination of the reconstructed errors caused at each layer. By controlling layer-wise errors properly, one only needs to perform a light retraining process on the pruned network to resume its original prediction performance. We conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our pruning method compared with several state-of-the-art baseline methods. Codes of our work are released at: https://github.com/csyhhu/L-OBS.},
archivePrefix = {arXiv},
arxivId = {1705.07565},
author = {Dong, Xin and Chen, Shangyu and Pan, Sinno Jialin},
eprint = {1705.07565},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Dong, Chen, Pan - 2017 - Learning to prune deep neural networks via layer-wise optimal brain surgeon(2).pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
mendeley-groups = {network compression},
number = {Nips},
pages = {4858--4868},
title = {{Learning to prune deep neural networks via layer-wise optimal brain surgeon}},
volume = {2017-Decem},
year = {2017}
}

@inproceedings{LeCun,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 volume = {2},
 year = {1989}
}


@article{Chechik1998,
abstract = {Research with humans and primates shows that the developmental course of the brain involves synaptic overgrowth followed by marked selective pruning. Previous explanations have suggested that this intriguing, seemingly wasteful phenomenon is utilized to remove "erroneous" synapses. We prove that this interpretation is wrong if synapses are Hebbian. Under limited metabolic energy resources restricting the amount and strength of synapses, we show that memory performance is maximized if synapses are first overgrown and then pruned following optimal "minimal-value" deletion. This optimal strategy leads to interesting insights concerning childhood amnesia.},
author = {Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
doi = {10.1162/089976698300017124},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Chechik, Meilijson, Ruppin - 1998 - Synaptic Pruning in Development A Computational Account(2).pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
mendeley-groups = {network compression},
number = {7},
pages = {1759--1777},
pmid = {9744896},
title = {{Synaptic Pruning in Development: A Computational Account}},
volume = {10},
year = {1998}
}
@article{Gonzalez2007,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very efficient GPU implemen- tation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
doi = {10.1201/9781420010749},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks(2).pdf:pdf},
isbn = {9781420010749},
journal = {NeurIPS Proceedings},
mendeley-groups = {MdYaqotI4,network compression,pre-postDisaster},
pages = {1--1432},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Russakovsky2015,
abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
archivePrefix = {arXiv},
arxivId = {1409.0575},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
doi = {10.1007/s11263-015-0816-y},
eprint = {1409.0575},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Russakovsky et al. - 2015 - ImageNet Large Scale Visual Recognition Challenge(2).pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
mendeley-groups = {network compression},
number = {3},
pages = {211--252},
publisher = {Springer US},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
volume = {115},
year = {2015}
}
@article{He2016a,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR- 10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/He et al. - 2016 - Identity mappings in deep residual networks(2).pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-groups = {network compression},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{Zhang2021,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small gap between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models. We supplement this republication with a new section at the end summarizing recent progresses in the field since the original version of this paper.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
doi = {10.1145/3446776},
eprint = {1611.03530},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2021 - Understanding deep learning (still) requires rethinking generalization(2).pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
mendeley-groups = {network compression},
number = {3},
pages = {107--115},
title = {{Understanding deep learning (still) requires rethinking generalization}},
volume = {64},
year = {2021}
}
@article{Dumoulin2016,
abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
archivePrefix = {arXiv},
arxivId = {1603.07285},
author = {Dumoulin, Vincent and Visin, Francesco},
eprint = {1603.07285},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Dumoulin, Visin - 2016 - A guide to convolution arithmetic for deep learning(2).pdf:pdf},
mendeley-groups = {network compression},
pages = {1--31},
title = {{A guide to convolution arithmetic for deep learning}},
url = {http://arxiv.org/abs/1603.07285},
year = {2016}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2015 - Very deep convolutional networks for large-scale image recognition(2).pdf:pdf},
journal = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
mendeley-groups = {covid-x-ray,network compression},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Han2016,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce “deep compression”, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35× to 49× without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9× to 13×; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35×, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49× from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3× to 4× layerwise speedup and 3× to 7× better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
eprint = {1510.00149},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Han, Mao, Dally - 2016 - Deep compression Compressing deep neural networks with pruning, trained quantization and Huffman coding(2).pdf:pdf},
journal = {4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings},
mendeley-groups = {network compression,IEEEAccessVAE_GAN},
pages = {1--14},
title = {{Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding}},
year = {2016}
}
@article{Paupamah2020,
abstract = {Deep neural networks are typically too computationally expensive to run in real-time on consumer-grade hardware and low-powered devices. In this paper, we investigate reducing the computational and memory requirements of neural networks through network pruning and quantisation. We examine their efficacy on large networks like AlexNet compared to recent compact architectures: ShuffleNet and MobileNet. Our results show that pruning and quantisation compresses these networks to less than half their original size and improves their efficiency, particularly on MobileNet with a 7× speedup. We also demonstrate that pruning, in addition to reducing the number of parameters in a network, can aid in the correction of overfitting.},
archivePrefix = {arXiv},
arxivId = {2001.04850},
author = {Paupamah, Kimessha and James, Steven and Klein, Richard},
eprint = {2001.04850},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Paupamah, James, Klein - 2020 - Quantisation and Pruning for Neural Network Compression and Regularisation(2).pdf:pdf},
journal = {arXiv},
keywords = {Compression,Index Terms—deep learning,Neural networks,Pruning,Quantisation,Regularisation},
mendeley-groups = {network compression},
title = {{Quantisation and Pruning for Neural Network Compression and Regularisation}},
year = {2020}
}
@article{Jin2019,
abstract = {Today's deep neural networks (DNNs) are becoming deeper and wider because of increasing demand on the analysis quality and more and more complex applications to resolve. The wide and deep DNNs, however, require large amounts of resources (such as memory, storage, and I/O), significantly restricting their utilization on resource-constrained platforms. Although some DNN simplification methods (such as weight quantization) have been proposed to address this issue, they suffer from either low compression ratios or high compression errors, which may introduce an expensive fine-tuning overhead (i.e., a costly retraining process for the target inference accuracy). In this paper, we propose DeepSZ: an accuracyloss expected neural network compression framework, which involves four key steps: network pruning, error bound assessment, optimization for error bound configuration, and compressed model generation, featuring a high compression ratio and low encoding time. The contribution is threefold. (1)We develop an adaptive approach to select the feasible error bounds for each layer. (2) We build a model to estimate the overall loss of inference accuracy based on the inference accuracy degradation caused by individual decompressed layers. (3) We develop an efficient optimization algorithm to determine the best-fit configuration of error bounds in order to maximize the compression ratio under the user-set inference accuracy constraint. Experiments show that DeepSZ can compress AlexNet and VGG-16 on the ImageNet dataset by a compression ratio of 46× and 116×, respectively, and compress LeNet-300-100 and LeNet-5 on the MNIST dataset by a compression ratio of 57× and 56×, respectively, with only up to 0.3% loss of inference accuracy. Compared with other state-of-the-art methods, DeepSZ can improve the compression ratio by up to 1.43×, the DNN encoding performance by up to 4.0× with four V100 GPUs, and the decoding performance by up to 6.2×.},
archivePrefix = {arXiv},
arxivId = {1901.09124},
author = {Jin, Sian and Di, Sheng and Liang, Xin and Tian, Jiannan and Tao, Dingwen and Cappello, Franck},
doi = {10.1145/3307681.3326608},
eprint = {1901.09124},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Jin et al. - 2019 - DeepSZ A novel framework to compress deep neural networks by using error-bounded lossy compression(2).pdf:pdf},
isbn = {9781450366700},
journal = {HPDC 2019- Proceedings of the 28th International Symposium on High-Performance Parallel and Distributed Computing},
keywords = {Deep learning,Lossy compression,Neural networks,Performance},
mendeley-groups = {network compression},
pages = {159--170},
title = {{DeepSZ: A novel framework to compress deep neural networks by using error-bounded lossy compression}},
year = {2019}
}
@article{Neill2020,
abstract = {Overparameterized networks trained to convergence have shown impressive performance in domains such as computer vision and natural language processing. Pushing state of the art on salient tasks within these domains corresponds to these models becoming larger and more difficult for machine learning practitioners to use given the increasing memory and storage requirements, not to mention the larger carbon footprint. Thus, in recent years there has been a resurgence in model compression techniques, particularly for deep convolutional neural networks and self-attention based networks such as the Transformer. Hence, this paper provides a timely overview of both old and current compression techniques for deep neural networks, including pruning, quantization, tensor decomposition, knowledge distillation and combinations thereof. We assume a basic familiarity with deep learning architectures1, namely, Recurrent Neural Networks [(RNNs) 155, 75], Convolutional Neural Networks [50] 2 and Self-Attention based networks [181]3,4. Most of the papers discussed are proposed in the context of at least one of these DNN architectures.},
archivePrefix = {arXiv},
arxivId = {2006.03669},
author = {Neill, James T.O.},
eprint = {2006.03669},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Neill - 2020 - An overview of neural network compression(2).pdf:pdf},
journal = {arXiv},
mendeley-groups = {network compression},
pages = {1--73},
title = {{An overview of neural network compression}},
year = {2020}
}
@techreport{McCrary1992,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it di?cult to learn a good set of ?lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signi?cantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
author = {Krizhevsky, Alex},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky - 2009 - Learning Multiple Layers of Features from Tiny Images(2).pdf:pdf},
institution = {University of Toronto},
issn = {00012475},
mendeley-groups = {AutoEncoders,Augmentation,network compression,IEEEAccessVAE_GAN},
pmid = {1575800},
title = {{Learning Multiple Layers of Features from Tiny Images}},
year = {2009}
}
@misc{xiao2017fashionmnist,
      title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}, 
      author={Han Xiao and Kashif Rasul and Roland Vollgraf},
      year={2017},
      eprint={1708.07747},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{thoma2017hasyv2,
      title={The HASYv2 dataset}, 
      author={Martin Thoma},
      year={2017},
      eprint={1701.08380},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{prabhu2019kannadamnist,
      title={Kannada-MNIST: A new handwritten digits dataset for the Kannada language}, 
      author={Vinay Uday Prabhu},
      year={2019},
      eprint={1908.01242},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}




@article{Baidoo-Anu2023,
author = {Baidoo-Anu, David and Ansah, Leticia Owusu},
journal = {SSRN},
mendeley-groups = {Diss2023},
title = {{Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning}},
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=4337484},
year = {2023}
}






@article{Li2023,
title={Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models}, 
journal = {arXiv},
      author={Pengfei Li and Jianyi Yang and Mohammad A. Islam and Shaolei Ren},
      year={2023},
      eprint={2304.03271},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Zhou,
author = {Zhou, Ryle},
file = {:Users/ashhadulislam/Downloads/15843151.pdf:pdf},
mendeley-groups = {Diss2023},
number = {1},
pages = {1--7},
journal = {Stanford Web},
title = {{Question Answering Models for SQuAD 2.0}},
}

@article{Strubell2020,
abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
archivePrefix = {arXiv},
arxivId = {arXiv:1906.02243v1},
author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
doi = {10.1609/aaai.v34i09.7123},
eprint = {arXiv:1906.02243v1},
file = {:Users/ashhadulislam/Downloads/1906.02243.pdf:pdf},
isbn = {9781577358350},
issn = {2159-5399},
journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
mendeley-groups = {Diss2023},
number = {1},
pages = {1393--13696},
title = {{Energy and policy considerations for modern deep learning research}},
year = {2020}
}

@misc{Gooding2023,
author = {Gooding, Matthew},
booktitle = {Tech Monitor},
mendeley-groups = {Diss2023},
title = {{Google takes on ChatGPT with new Bard chatbot and AI-powered search}},
url = {https://techmonitor.ai/technology/ai-and-automation/bard-google-chatgpt-generative-ai-chatbot},
year = {2023}
}



@misc{Ludvigsen2022,
author = {Ludvigsen, Kasper Groes Albin},
booktitle = {Towards Data Science},
mendeley-groups = {Diss2023},
title = {{The Carbon Footprint of ChatGPT}},
url = {https://towardsdatascience.com/the-carbon-footprint-of-chatgpt-66932314627d},
year = {2022}
}
@article{vaswani2017attention,
    title={{Attention is all you need}},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in Neural Information Processing Systems},
    volume={30},
    year={2017}
}



@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={10865--10873},
  year={2024}
}

@article{zhang2023loraprune,
  title={Loraprune: Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}
@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{xia2022structured,
  title={Structured pruning learns compact and accurate models},
  author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2204.00408},
  year={2022}
}

@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}


@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{chen2020tight,
  title={Tight compression: Compressing CNN model tightly through unstructured pruning and simulated annealing based permutation},
  author={Chen, Xizi and Zhu, Jingyang and Jiang, Jingbo and Tsui, Chi-Ying},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{chen2021knowledge,
  title={Knowledge from the original network: restore a better pruned network with knowledge distillation},
  author={Chen, Liyang and Chen, Yongquan and Xi, Juntong and Le, Xinyi},
  journal={Complex \& Intelligent Systems},
  pages={1--10},
  year={2021},
  publisher={Springer}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{li2022parameter,
  title={Parameter-efficient sparsity for large language models fine-tuning},
  author={Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie},
  journal={arXiv preprint arXiv:2205.11005},
  year={2022}
}

@article{chen2021only,
  title={Only train once: A one-shot neural network training and pruning framework},
  author={Chen, Tianyi and Ji, Bo and Ding, Tianyu and Fang, Biyi and Wang, Guanyi and Zhu, Zhihui and Liang, Luming and Shi, Yixin and Yi, Sheng and Tu, Xiao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19637--19651},
  year={2021}
}
@article{chen2023otov2,
  title={Otov2: Automatic, generic, user-friendly},
  author={Chen, Tianyi and Liang, Luming and Ding, Tianyu and Zhu, Zhihui and Zharkov, Ilya},
  journal={arXiv preprint arXiv:2303.06862},
  year={2023}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{zafrir2021prune,
  title={Prune once for all: Sparse pre-trained language models},
  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:2111.05754},
  year={2021}
}

@article{srinivas2015data,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015}
}


@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@misc{huggingfaceDBQBurberryProductpricesUnitedStatesDatasets,
	author = {huggingface},
	title = {{D}{B}{Q}/{B}urberry.{P}roduct.prices.{U}nited.{S}tates · {D}atasets at {H}ugging {F}ace --- huggingface.co},
	howpublished = {\url{https://bit.ly/3YCJ9Z7}},
	year = {2023},
	note = {[Accessed 13-08-2024]},
}

@misc{towardsdatascienceChatGPTsElectricity,
	author = {Kasper Groes Albin Ludvigsen},
	title = {{C}hat{G}{P}{T}’s {E}lectricity {C}onsumption --- towardsdatascience.com},
	howpublished = {\url{https://bit.ly/3An9joz}},
	year = {},
	note = {[Accessed 13-08-2024]},
}