@article{Blalock2020,
abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
archivePrefix = {arXiv},
journal = {arXiv},
arxivId = {2003.03033},
author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
eprint = {2003.03033},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Blalock et al. - 2020 - What is the State of Neural Network Pruning(2).pdf:pdf},
mendeley-groups = {network compression},
title = {{What is the State of Neural Network Pruning?}},
url = {http://arxiv.org/abs/2003.03033},
year = {2020}
}

@article{Frankle2019,
abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the lottery ticket hypothesis: dense, randomly-initialized, feed-forward networks contain subnetworks (winning tickets) that-when trained in isolation-reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
archivePrefix = {arXiv},
arxivId = {1803.03635},
author = {Frankle, Jonathan and Carbin, Michael},
eprint = {1803.03635},
file = {:Users/ashhadulislam/Library/Application Support/Mendeley Desktop/Downloaded/Frankle, Carbin - 2019 - The lottery ticket hypothesis Finding sparse, trainable neural networks(2).pdf:pdf},
journal = {7th International Conference on Learning Representations, ICLR 2019},
mendeley-groups = {network compression},
pages = {1--42},
title = {{The lottery ticket hypothesis: Finding sparse, trainable neural networks}},
year = {2019}
}

@inproceedings{LeCun,
 author = {LeCun, Yann and Denker, John and Solla, Sara},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Optimal Brain Damage},
 volume = {2},
 year = {1989}
}

@inproceedings{an2024fluctuation,
  title={Fluctuation-based adaptive structured pruning for large language models},
  author={An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={10865--10873},
  year={2024}
}

@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{chen2020tight,
  title={Tight compression: Compressing CNN model tightly through unstructured pruning and simulated annealing based permutation},
  author={Chen, Xizi and Zhu, Jingyang and Jiang, Jingbo and Tsui, Chi-Ying},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2020},
  organization={IEEE}
}

@article{chen2021knowledge,
  title={Knowledge from the original network: restore a better pruned network with knowledge distillation},
  author={Chen, Liyang and Chen, Yongquan and Xi, Juntong and Le, Xinyi},
  journal={Complex \& Intelligent Systems},
  pages={1--10},
  year={2021},
  publisher={Springer}
}

@article{chen2021only,
  title={Only train once: A one-shot neural network training and pruning framework},
  author={Chen, Tianyi and Ji, Bo and Ding, Tianyu and Fang, Biyi and Wang, Guanyi and Zhu, Zhihui and Liang, Luming and Shi, Yixin and Yi, Sheng and Tu, Xiao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={19637--19651},
  year={2021}
}

@article{chen2023otov2,
  title={Otov2: Automatic, generic, user-friendly},
  author={Chen, Tianyi and Liang, Luming and Ding, Tianyu and Zhu, Zhihui and Zharkov, Ilya},
  journal={arXiv preprint arXiv:2303.06862},
  year={2023}
}

@article{dettmers2022gpt3,
  title={Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30318--30332},
  year={2022}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{han2015learning,
author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
journal = {Advances in neural information processing systems},
mendeley-groups = {network compression},
title = {{Learning both weights and connections for efficient neural network}},
volume = {28},
year = {2015}
}

@article{kurtic2024ziplm,
  title={Ziplm: Inference-aware structured pruning of language models},
  author={Kurti{\'c}, Eldar and Frantar, Elias and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2022parameter,
  title={Parameter-efficient sparsity for large language models fine-tuning},
  author={Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie},
  journal={arXiv preprint arXiv:2205.11005},
  year={2022}
}

@article{ma2023llm,
  title={Llm-pruner: On the structural pruning of large language models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={21702--21720},
  year={2023}
}

@article{srinivas2015data,
  title={Data-free parameter pruning for deep neural networks},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1507.06149},
  year={2015}
}

@article{sun2023simple,
  title={A simple and effective pruning approach for large language models},
  author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
  journal={arXiv preprint arXiv:2306.11695},
  year={2023}
}

@article{xia2022structured,
  title={Structured pruning learns compact and accurate models},
  author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2204.00408},
  year={2022}
}

@article{xia2023sheared,
  title={Sheared llama: Accelerating language model pre-training via structured pruning},
  author={Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi},
  journal={arXiv preprint arXiv:2310.06694},
  year={2023}
}

@article{zafrir2021prune,
  title={Prune once for all: Sparse pre-trained language models},
  author={Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:2111.05754},
  year={2021}
}

@article{zhang2023loraprune,
  title={Loraprune: Pruning meets low-rank parameter-efficient fine-tuning},
  author={Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2305.18403},
  year={2023}
}

