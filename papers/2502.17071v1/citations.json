[
  {
    "index": 0,
    "papers": [
      {
        "key": "han2015learning",
        "author": "Han, Song and Pool, Jeff and Tran, John and Dally, William",
        "title": "{Learning both weights and connections for efficient neural network}"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "Frankle2019",
        "author": "Frankle, Jonathan and Carbin, Michael",
        "title": "{The lottery ticket hypothesis: Finding sparse, trainable neural networks}"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "Blalock2020",
        "author": "Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John",
        "title": "{What is the State of Neural Network Pruning?}"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "dettmers2022gpt3",
        "author": "Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke",
        "title": "Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ma2023llm",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "an2024fluctuation",
        "author": "An, Yongqi and Zhao, Xu and Yu, Tao and Tang, Ming and Wang, Jinqiao",
        "title": "Fluctuation-based adaptive structured pruning for large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "xia2023sheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared llama: Accelerating language model pre-training via structured pruning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "xia2022structured",
        "author": "Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi",
        "title": "Structured pruning learns compact and accurate models"
      },
      {
        "key": "kurtic2024ziplm",
        "author": "Kurti{\\'c}, Eldar and Frantar, Elias and Alistarh, Dan",
        "title": "Ziplm: Inference-aware structured pruning of language models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "dong2017learning",
        "author": "Dong, Xin and Chen, Shangyu and Pan, Sinno",
        "title": "Learning to prune deep neural networks via layer-wise optimal brain surgeon"
      },
      {
        "key": "chen2020tight",
        "author": "Chen, Xizi and Zhu, Jingyang and Jiang, Jingbo and Tsui, Chi-Ying",
        "title": "Tight compression: Compressing CNN model tightly through unstructured pruning and simulated annealing based permutation"
      },
      {
        "key": "chen2021knowledge",
        "author": "Chen, Liyang and Chen, Yongquan and Xi, Juntong and Le, Xinyi",
        "title": "Knowledge from the original network: restore a better pruned network with knowledge distillation"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "frantar2023sparsegpt",
        "author": "Frantar, Elias and Alistarh, Dan",
        "title": "Sparsegpt: Massive language models can be accurately pruned in one-shot"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "sun2023simple",
        "author": "Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico",
        "title": "A simple and effective pruning approach for large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2022parameter",
        "author": "Li, Yuchao and Luo, Fuli and Tan, Chuanqi and Wang, Mengdi and Huang, Songfang and Li, Shen and Bai, Junjie",
        "title": "Parameter-efficient sparsity for large language models fine-tuning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "chen2021only",
        "author": "Chen, Tianyi and Ji, Bo and Ding, Tianyu and Fang, Biyi and Wang, Guanyi and Zhu, Zhihui and Liang, Luming and Shi, Yixin and Yi, Sheng and Tu, Xiao",
        "title": "Only train once: A one-shot neural network training and pruning framework"
      },
      {
        "key": "chen2023otov2",
        "author": "Chen, Tianyi and Liang, Luming and Ding, Tianyu and Zhu, Zhihui and Zharkov, Ilya",
        "title": "Otov2: Automatic, generic, user-friendly"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "ma2023llm",
        "author": "Ma, Xinyin and Fang, Gongfan and Wang, Xinchao",
        "title": "Llm-pruner: On the structural pruning of large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "zhang2023loraprune",
        "author": "Zhang, Mingyang and Chen, Hao and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan",
        "title": "Loraprune: Pruning meets low-rank parameter-efficient fine-tuning"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "xia2023sheared",
        "author": "Xia, Mengzhou and Gao, Tianyu and Zeng, Zhiyuan and Chen, Danqi",
        "title": "Sheared llama: Accelerating language model pre-training via structured pruning"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "bai2020binarybert",
        "author": "Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin",
        "title": "Binarybert: Pushing the limit of bert quantization"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zafrir2021prune",
        "author": "Zafrir, Ofir and Larey, Ariel and Boudoukh, Guy and Shen, Haihao and Wasserblat, Moshe",
        "title": "Prune once for all: Sparse pre-trained language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "LeCun",
        "author": "LeCun, Yann and Denker, John and Solla, Sara",
        "title": "Optimal Brain Damage"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "kurtic2024ziplm",
        "author": "Kurti{\\'c}, Eldar and Frantar, Elias and Alistarh, Dan",
        "title": "Ziplm: Inference-aware structured pruning of language models"
      },
      {
        "key": "srinivas2015data",
        "author": "Srinivas, Suraj and Babu, R Venkatesh",
        "title": "Data-free parameter pruning for deep neural networks"
      }
    ]
  }
]