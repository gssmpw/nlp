\section{Related Work}
\label{sec:relWorks}
Magnitude pruning \citep{han2015learning} is a standard technique to induce sparsity in neural networks by removing individual weights based on their magnitudes, typically determined either locally within each layer or globally across the entire network. Despite its simplicity, it has been effective in finding extremely sparse networks \citep{Frankle2019} and is considered a strong baseline approach \citep{Blalock2020} for neural network sparsification. Dettmers et al. \citep{dettmers2022gpt3} observed emergent large magnitude features in Transformer-based large language models (LLMs), noting that when LLMs reach around 6B parameters, a small set of hidden state features emerges with significantly larger magnitudes than others, which are crucial for predictive performance. In the context of compressing recent LLMs, methods like LLM-Pruner \citep{ma2023llm} and FLAP \citep{an2024fluctuation} narrow network width by pruning coupled structures, while Sheared-LLaMA \citep{xia2023sheared} reduces both network width and depth by removing entire layers. Although pruning methods that incorporate both width and depth aspects exist \citep{xia2022structured, kurtic2024ziplm}, there remains a need for detailed analysis comparing these factors' impact on LLM inference efficiency. Traditional pruning in Deep Neural Networks (DNNs) faces unique challenges when applied to LLMs, which have a large number of parameters and require significant computational resources \citep{brown2020language}. Various pruning methods for LLMs fall into unstructured and structured categories. Unstructured pruning methods \citep{dong2017learning, chen2020tight, chen2021knowledge} set unimportant individual weights to zero, maintaining performance but resulting in sparse weight matrices that are less hardware-efficient. Methods like SparseGPT \citep{frantar2023sparsegpt} and Wanda \citep{sun2023simple} use sophisticated weight updates and pruning without retraining, respectively, while PST \citep{li2022parameter} combines unstructured pruning with efficient fine-tuning. Structured pruning methods \citep{chen2021only, chen2023otov2} remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner \citep{ma2023llm} and LoRAPrune \citep{zhang2023loraprune} focus on efficient deployment and inference acceleration, with Sheared-LLaMA \citep{xia2023sheared} aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization \citep{bai2020binarybert, brown2020language, devlin2018bert}. Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning \citep{zafrir2021prune} to more advanced techniques like the optimal brain surgeon \citep{LeCun}. Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability \citep{kurtic2024ziplm, srinivas2015data}.