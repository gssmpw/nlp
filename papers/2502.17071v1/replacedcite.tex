\section{Related Work}
\label{sec:relWorks}
Magnitude pruning ____ is a standard technique to induce sparsity in neural networks by removing individual weights based on their magnitudes, typically determined either locally within each layer or globally across the entire network. Despite its simplicity, it has been effective in finding extremely sparse networks ____ and is considered a strong baseline approach ____ for neural network sparsification. Dettmers et al. ____ observed emergent large magnitude features in Transformer-based large language models (LLMs), noting that when LLMs reach around 6B parameters, a small set of hidden state features emerges with significantly larger magnitudes than others, which are crucial for predictive performance. In the context of compressing recent LLMs, methods like LLM-Pruner ____ and FLAP ____ narrow network width by pruning coupled structures, while Sheared-LLaMA ____ reduces both network width and depth by removing entire layers. Although pruning methods that incorporate both width and depth aspects exist ____, there remains a need for detailed analysis comparing these factors' impact on LLM inference efficiency. Traditional pruning in Deep Neural Networks (DNNs) faces unique challenges when applied to LLMs, which have a large number of parameters and require significant computational resources ____. Various pruning methods for LLMs fall into unstructured and structured categories. Unstructured pruning methods ____ set unimportant individual weights to zero, maintaining performance but resulting in sparse weight matrices that are less hardware-efficient. Methods like SparseGPT ____ and Wanda ____ use sophisticated weight updates and pruning without retraining, respectively, while PST ____ combines unstructured pruning with efficient fine-tuning. Structured pruning methods ____ remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner ____ and LoRAPrune ____ focus on efficient deployment and inference acceleration, with Sheared-LLaMA ____ aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization ____. Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning ____ to more advanced techniques like the optimal brain surgeon ____. Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability ____.