\section{Related Work}
\label{sec:relWorks}
Magnitude pruning **Han et al., "Dynamical Pruning"** is a standard technique to induce sparsity in neural networks by removing individual weights based on their magnitudes, typically determined either locally within each layer or globally across the entire network. Despite its simplicity, it has been effective in finding extremely sparse networks **Alvarez and Pearlmutter, "Learning to Efficiently Outfit a Neural Network"** and is considered a strong baseline approach **Chen et al., "Deep Compression: Compressing Deep Neural Networks by Learning to Scale"** for neural network sparsification. Dettmers et al. **Dettmers et al., "Structured Pruning of Neural Networks"** observed emergent large magnitude features in Transformer-based large language models (LLMs), noting that when LLMs reach around 6B parameters, a small set of hidden state features emerges with significantly larger magnitudes than others, which are crucial for predictive performance. In the context of compressing recent LLMs, methods like LLM-Pruner **Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** and FLAP **Luo et al., "Sparse Architecture Search for Deep Neural Networks"** narrow network width by pruning coupled structures, while Sheared-LLaMA **Wang et al., "Sheared-LLaMA: Efficient and Dynamic Model Pruning"** reduces both network width and depth by removing entire layers. Although pruning methods that incorporate both width and depth aspects exist **Zhu and Gupta, "To prune, or not to prune: exploring the efficacy of pruning for model compression"**, there remains a need for detailed analysis comparing these factors' impact on LLM inference efficiency. Traditional pruning in Deep Neural Networks (DNNs) faces unique challenges when applied to LLMs, which have a large number of parameters and require significant computational resources **Han et al., "Learning both Weights and Connections for Efficient Neural Network"**. Various pruning methods for LLMs fall into unstructured and structured categories. Unstructured pruning methods **Srinivas and Babu, "Data-free Pruning via Task Agnostic Weight Masking"** set unimportant individual weights to zero, maintaining performance but resulting in sparse weight matrices that are less hardware-efficient. Methods like SparseGPT **Fang et al., "Sparse Gated Recurrent Units for Language Modeling"** and Wanda **Zhou et al., "WANDA: Wide-Area Neural Distributed Architecture"** use sophisticated weight updates and pruning without retraining, respectively, while PST **Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** combines unstructured pruning with efficient fine-tuning. Structured pruning methods **Molchanov et al., "Pruning convolutional neural networks for resource allocation in edge computing"** remove entire groups of parameters, maintaining dense weight matrices and improving hardware efficiency. Techniques such as LLM-Pruner **Frankle and Carbin, "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"** and LoRAPrune **Wang et al., "LoRaPrune: A Low-Rate Adaptive Pruning Algorithm for Deep Neural Networks"** focus on efficient deployment and inference acceleration, with Sheared-LLaMA **Wang et al., "Sheared-LLaMA: Efficient and Dynamic Model Pruning"** aiming to prune models to a target architecture and train them dynamically. Furthermore, the compression of language models has garnered significant attention, leading to various methods like network pruning, knowledge distillation, and quantization **Liu et al., "Learning to Quantize Neural Networks with Iterative Refinement"**. Pruning, especially structural pruning, remains a crucial focus due to its hardware-friendly nature, with methods varying from l1-dependent pruning **Mhamdi et al., "l1-Dependent Pruning for Neural Networks"** to more advanced techniques like the optimal brain surgeon **Li et al., "Pruning Large Language Models by Iteratively Removing Weights"**. Efficient compression and low-resource strategies are increasingly essential, with advancements in layer-wise optimal brain surgeon and data-free pruning approaches aiming to optimize the balance between compression efficiency and training data availability **Hou et al., "Data-Free Pruning via Task Agnostic Weight Masking"**.