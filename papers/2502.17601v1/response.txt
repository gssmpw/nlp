\section{Related Work}
\paragraph{Existing Survey Work}

Existing deep learning literature produced a number of studies presenting large-scale overview of techniques and open problems in AI explainability (XAI) including general AI explainability \textbf{Dhurandhar, "Explainable Machine Learning Models"} ____ ____ ____ ____ ____ ____ ____, deep learning explainability \textbf{Adadi, "Peeking Inside the Black Box: A Survey on Explainable Artificial Intelligence (XAI)"}, black box models  \textbf{Gundel, "Understanding Black-Box Predictions via Influence Functions"}, large-language model XAI ____ ____ ____ ____ ____ ____, neural network concept explanation ____ ____ or medical XAI ____.

More narrow studies focus on mechanistic interpretability ____ ____ ____, LLM knowledge encoding ____ ____ ____ ____ ____ ____ or comparing models on the representation level ____ ____ ____ ____ or probing ____ ____ ____.

In that, some surveys provide a brief overview of representation engineering as a counterpoint to their main focus. Zhao et al. \textbf{Zhao, "Explaining Deep Neural Networks through Temporal Analysis"}, provides a landscape survey for modern explainability with a brief overview of representation engineering and analyzes representation engineering in relation to mechanistic interpretability. Representation engineering is also briefly analyzed as a potential alternative to existing explainability techniques in ____ ____.

This study is fundamentally different. It is the first study to review the work on representation engineering, an emerging field with high empirical validation for its techniques. It aims to highlight and systematize the techniques in this growing field to provide insights necessary for the creation of stable, general-purpose reading and interventions that can be applied across all use cases with a top-down interpretation.

\paragraph{Latent Saliency Maps (LSMs)}  

Latent Saliency Maps show how internal representations influence predictions in language models by highlighting relevant activations, as demonstrated in emergent world models in sequence tasks \textbf{Battaglia, "Relational Memory Networks for Learning to Learn"} ____ ____ ____.

An extension of general Latent Saliency Maps, Concept Saliency Maps (CSMs) identify high-level concepts by calculating gradients of concept scores ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____ ____