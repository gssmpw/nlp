@article{belinkov2022probing,
  title={Probing classifiers: Promises, shortcomings, and advances},
  author={Belinkov, Yonatan},
  journal={Computational Linguistics},
  volume={48},
  number={1},
  pages={207--219},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~â€¦}
}

@article{bereska24,
  title={Mechanistic Interpretability for AI Safety--A Review},
  author={Bereska, Leonard and Gavves, Efstratios},
  journal={arXiv preprint arXiv:2404.14082},
  year={2024}
}

@inproceedings{brocki2019concept,
  title={Concept saliency maps to visualize relevant features in deep generative models},
  author={Brocki, Lennart and Chung, Neo Christopher},
  booktitle={2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)},
  pages={1771--1778},
  year={2019},
  organization={IEEE}
}

@article{choudhary2022interpretation,
  title={Interpretation of black box nlp models: A survey},
  author={Choudhary, Shivani and Chatterjee, Niladri and Saha, Subir Kumar},
  journal={arXiv preprint arXiv:2203.17081},
  year={2022}
}

@article{crabbe2022concept,
  title={Concept activation regions: A generalized framework for concept-based explanations},
  author={Crabb{\'e}, Jonathan and van der Schaar, Mihaela},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2590--2607},
  year={2022}
}

@article{csahin2024unlocking,
  title={Unlocking the black box: an in-depth review on interpretability, explainability, and reliability in deep learning},
  author={{\c{S}}AHiN, Emrullah and Arslan, Naciye Nur and {\"O}zdemir, Durmu{\c{s}}},
  journal={Neural Computing and Applications},
  pages={1--107},
  year={2024},
  publisher={Springer}
}

@article{danilevsky2020survey,
  title={A survey of the state of explainable AI for natural language processing},
  author={Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  journal={arXiv preprint arXiv:2010.00711},
  year={2020}
}

@article{desantis24,
  title={Visual-TCAV: Concept-based Attribution and Saliency Maps for Post-hoc Explainability in Image Classification},
  author={De Santis, Antonio and Campi, Riccardo and Bianchi, Matteo and Brambilla, Marco},
  journal={arXiv preprint arXiv:2411.05698},
  year={2024}
}

@article{ehsan2024explainability,
  title={Explainability pitfalls: Beyond dark patterns in explainable AI},
  author={Ehsan, Upol and Riedl, Mark O},
  journal={Patterns},
  volume={5},
  number={6},
  year={2024},
  publisher={Elsevier}
}

@article{ferrando24,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@article{gohel2021explainable,
  title={Explainable AI: current status and future directions},
  author={Gohel, Prashant and Singh, Priyanka and Mohanty, Manoranjan},
  journal={arXiv preprint arXiv:2107.07045},
  year={2021}
}

@inproceedings{ismail2023concept,
  title={Concept bottleneck generative models},
  author={Ismail, Aya Abdelsalam and Adebayo, Julius and Bravo, Hector Corrada and Ra, Stephen and Cho, Kyunghyun},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{kastner24,
  title={Explaining AI through mechanistic interpretability},
  author={K{\"a}stner, Lena and Crook, Barnaby},
  journal={European Journal for Philosophy of Science},
  volume={14},
  number={4},
  pages={52},
  year={2024},
  publisher={Springer}
}

@inproceedings{kim24,
  title={EQ-CBM: A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors},
  author={Kim, Sangwon and Ahn, Dasom and Ko, Byoung Chul and Jang, In-su and Kim, Kwang-Ju},
  booktitle={Proceedings of the Asian Conference on Computer Vision},
  pages={3432--3448},
  year={2024}
}

@article{klabunde2023similarity,
  title={Similarity of neural network models: A survey of functional and representational measures},
  author={Klabunde, Max and Schumacher, Tobias and Strohmaier, Markus and Lemmerich, Florian},
  journal={arXiv preprint arXiv:2305.06329},
  year={2023}
}

@inproceedings{koh2020concept,
  title={Concept bottleneck models},
  author={Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={5338--5348},
  year={2020},
  organization={PMLR}
}

@article{lee2023neural,
  title={From neural activations to concepts: A survey on explaining concepts in neural networks},
  author={Lee, Jae Hee and Lanza, Sergio and Wermter, Stefan},
  journal={arXiv preprint arXiv:2310.11884},
  year={2023}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{mahinpei2021promises,
  title={Promises and pitfalls of black-box concept learning models},
  author={Mahinpei, Anita and Clark, Justin and Lage, Isaac and Doshi-Velez, Finale and Pan, Weiwei},
  journal={arXiv preprint arXiv:2106.13314},
  year={2021}
}

@article{nicolson24explain,
  title={Explaining Explainability: Understanding Concept Activation Vectors},
  author={Nicolson, Angus and Schut, Lisa and Noble, J Alison and Gal, Yarin},
  journal={arXiv preprint arXiv:2404.03713},
  year={2024}
}

@inproceedings{rauker2023toward,
  title={Toward transparent ai: A survey on interpreting the inner structures of deep neural networks},
  author={R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 ieee conference on secure and trustworthy machine learning (satml)},
  pages={464--483},
  year={2023},
  organization={IEEE}
}

@inproceedings{schmalwasser24,
  title={Exploiting Text-Image Latent Spaces for the Description of Visual Concepts},
  author={Schmalwasser, Laines and Gawlikowski, Jakob and Denzler, Joachim and Niebling, Julia},
  booktitle={International Conference on Pattern Recognition},
  pages={109--125},
  year={2024},
  organization={Springer}
}

@article{sheu2022survey,
  title={A survey on medical explainable AI (XAI): recent progress, explainability approach, human interaction and scoring system},
  author={Sheu, Ruey-Kai and Pardeshi, Mayuresh Sunil},
  journal={Sensors},
  volume={22},
  number={20},
  pages={8068},
  year={2022},
  publisher={MDPI}
}

@article{sun2024concept,
  title={Concept Bottleneck Large Language Models},
  author={Sun, Chung-En and Oikarinen, Tuomas and Ustun, Berk and Weng, Tsui-Wei},
  journal={arXiv preprint arXiv:2412.07992},
  year={2024}
}

@article{wang2024knowledge,
  title={Knowledge mechanisms in large language models: A survey and perspective},
  author={Wang, Mengru and Yao, Yunzhi and Xu, Ziwen and Qiao, Shuofei and Deng, Shumin and Wang, Peng and Chen, Xiang and Gu, Jia-Chen and Jiang, Yong and Xie, Pengjun and others},
  journal={arXiv preprint arXiv:2407.15017},
  year={2024}
}

@article{zarlenga2022concept,
  title={Concept embedding models: Beyond the accuracy-explainability trade-off},
  author={Zarlenga, Mateo Espinosa and Barbiero, Pietro and Ciravegna, Gabriele and Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Shams, Zohreh and Precioso, Frederic and Melacci, Stefano and Weller, Adrian and others},
  journal={arXiv preprint arXiv:2209.09056},
  year={2022}
}

@article{zhao24exppersp,
  title={Opening the black box of large language models: Two views on holistic interpretability},
  author={Zhao, Haiyan and Yang, Fan and Lakkaraju, Himabindu and Du, Mengnan},
  journal={arXiv e-prints},
  pages={arXiv--2402},
  year={2024}
}

@article{zhao24survey,
author = {Zhao, Haiyan and Chen, Hanjie and Yang, Fan and Liu, Ninghao and Deng, Huiqi and Cai, Hengyi and Wang, Shuaiqiang and Yin, Dawei and Du, Mengnan},
title = {Explainability for Large Language Models: A Survey},
year = {2024},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3639372},
doi = {10.1145/3639372},
abstract = {Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this article, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional deep learning models.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = feb,
articleno = {20},
numpages = {38},
keywords = {Explainability, interpretability, large language models}
}

