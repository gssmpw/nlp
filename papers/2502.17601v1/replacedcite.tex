\section{Related Work}
\paragraph{Existing Survey Work}

Existing deep learning literature produced a number of studies presenting large-scale overview of techniques and open problems in AI explainability (XAI) including general AI explainability ____, deep learning explainability ____, black box models  ____, large-language model XAI ____ ____ ____ ____, neural network concept explanation ____ ____ or medical XAI ____. More narrow studies focus on mechanistic interpretability ____ ____ ____, LLM knowledge encoding ____, comparing models on the representation level ____ or probing ____.

In that, some surveys provide a brief overview of representation engineering as a counterpoint to their main focus. Zhao et al. ____ provides a landscape survey for modern explainability with a brief overview of representation engineering and analyzes representation engineering in relation to mechanistic interpretability. Representation engineering is also briefly analyzed as a potential alternative to existing explainability techniques in ____. This study is fundamentally different. It is the first study to review the work on representation engineering, an emerging field with high empirical validation for its techniques. It aims to highlight and systematize the techniques in this growing field to provide insights necessary for the creation of stable, general-purpose reading and interventions that can be applied across all use cases with a top-down interpretation. 

\paragraph{Latent Saliency Maps (LSMs)}  

Latent Saliency Maps show how internal representations influence predictions in language models by highlighting relevant activations, as demonstrated in emergent world models in sequence tasks ____. An extension of general Latent Saliency Maps, Concept Saliency Maps (CSMs) identify high-level concepts by calculating gradients of concept scores ____. 

\paragraph{Concept Bottleneck Models (CBMs)} 

Pre-LLMs, Concept Bottleneck Models (CBMs) have been created as a deep learning architecture that has an intermediate layer that forces models to represent information through human-understandable concepts, enabling interpretability and direct intervention ____. CBMs have been extended to Concept Bottleneck Generative Models (CBGMs), where a dedicated bottleneck layer encodes structured concepts, preserving generation quality across architectures like GANs, VAEs, and diffusion models ____. However, CBMs suffer from "concept leakage," where models bypass the bottleneck to encode task-relevant information in uninterpretable ways, which can be mitigated using orthogonality constraints and disentangled concept embeddings ____. Concept Bottleneck Large-Language Models (CB-LLMs) integrate CB layers into transformers, demonstrating that interpretable neurons can improve text classification and enable controllable text generation by modulating concept activations ____. CBMs tie inference to a specific "concept" (representation), but usually have lower accuracy than concept-free alternatives. Their effectiveness depends on the completeness and accuracy of the process of identifying the concept, leading to new generations of models that perform automated concept discovery ____. 

\paragraph{Concept Activation Vectors (CAVs)}  

Concept Activation Vectors (CAVs) are numerical representations of concepts across layers. They provide a way to probe learned representations in neural networks by identifying directions in latent space that correspond to human-interpretable concepts ____. However, they are not stable across different layers of a model but evolve throughout the network ____. The entanglement of multiple concepts within a single CAV makes it difficult to assign the meaning to learned representations ____. Concept Activation Regions (CARs) enhance concept-based explanations by generalizing Concept Activation Vectors (CAVs) to account for non-linear separability, leading to more accurate global explanations ____.  

Although directly tied to representation engineering, LSMs, CBMs and CAVs are a method primarily used for deep learning models ____ and have not been extensively applied to large-language models.