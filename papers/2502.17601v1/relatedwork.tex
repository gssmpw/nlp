\section{Related Work}
\paragraph{Existing Survey Work}

Existing deep learning literature produced a number of studies presenting large-scale overview of techniques and open problems in AI explainability (XAI) including general AI explainability \cite{gohel2021explainable}, deep learning explainability \cite{csahin2024unlocking}, black box models  \cite{choudhary2022interpretation}, large-language model XAI \cite{zhao24survey} \cite{danilevsky2020survey} \cite{ehsan2024explainability} \cite{wang2024knowledge}, neural network concept explanation \cite{lee2023neural} \cite{rauker2023toward} or medical XAI \cite{sheu2022survey}. More narrow studies focus on mechanistic interpretability \cite{bereska24} \cite{ferrando24} \cite{kastner24}, LLM knowledge encoding \cite{wang2024knowledge}, comparing models on the representation level \cite{klabunde2023similarity} or probing \cite{belinkov2022probing}.

In that, some surveys provide a brief overview of representation engineering as a counterpoint to their main focus. Zhao et al. \cite{zhao24exppersp} provides a landscape survey for modern explainability with a brief overview of representation engineering and analyzes representation engineering in relation to mechanistic interpretability. Representation engineering is also briefly analyzed as a potential alternative to existing explainability techniques in \cite{zhao24survey}. This study is fundamentally different. It is the first study to review the work on representation engineering, an emerging field with high empirical validation for its techniques. It aims to highlight and systematize the techniques in this growing field to provide insights necessary for the creation of stable, general-purpose reading and interventions that can be applied across all use cases with a top-down interpretation. 

\paragraph{Latent Saliency Maps (LSMs)}  

Latent Saliency Maps show how internal representations influence predictions in language models by highlighting relevant activations, as demonstrated in emergent world models in sequence tasks \cite{li2022emergent}. An extension of general Latent Saliency Maps, Concept Saliency Maps (CSMs) identify high-level concepts by calculating gradients of concept scores \cite{brocki2019concept}. 

\paragraph{Concept Bottleneck Models (CBMs)} 

Pre-LLMs, Concept Bottleneck Models (CBMs) have been created as a deep learning architecture that has an intermediate layer that forces models to represent information through human-understandable concepts, enabling interpretability and direct intervention \cite{koh2020concept}. CBMs have been extended to Concept Bottleneck Generative Models (CBGMs), where a dedicated bottleneck layer encodes structured concepts, preserving generation quality across architectures like GANs, VAEs, and diffusion models \cite{ismail2023concept}. However, CBMs suffer from "concept leakage," where models bypass the bottleneck to encode task-relevant information in uninterpretable ways, which can be mitigated using orthogonality constraints and disentangled concept embeddings \cite{mahinpei2021promises, ismail2023concept}. Concept Bottleneck Large-Language Models (CB-LLMs) integrate CB layers into transformers, demonstrating that interpretable neurons can improve text classification and enable controllable text generation by modulating concept activations \cite{sun2024concept}. CBMs tie inference to a specific "concept" (representation), but usually have lower accuracy than concept-free alternatives. Their effectiveness depends on the completeness and accuracy of the process of identifying the concept, leading to new generations of models that perform automated concept discovery \cite{zarlenga2022concept, ismail2023concept, kim24}. 

\paragraph{Concept Activation Vectors (CAVs)}  

Concept Activation Vectors (CAVs) are numerical representations of concepts across layers. They provide a way to probe learned representations in neural networks by identifying directions in latent space that correspond to human-interpretable concepts \cite{nicolson24explain}. However, they are not stable across different layers of a model but evolve throughout the network \cite{nicolson24explain}. The entanglement of multiple concepts within a single CAV makes it difficult to assign the meaning to learned representations \cite{nicolson24explain, schmalwasser24}. Concept Activation Regions (CARs) enhance concept-based explanations by generalizing Concept Activation Vectors (CAVs) to account for non-linear separability, leading to more accurate global explanations \cite{crabbe2022concept}.  

Although directly tied to representation engineering, LSMs, CBMs and CAVs are a method primarily used for deep learning models \cite{desantis24, schmalwasser24} and have not been extensively applied to large-language models.