\section{Mars synthetic dataset}
\label{sec:mars_synthetic_dataset}

% We generated a large synthetic Mars image dataset with the aim of training and testing LoFTR and Geo-LoFTR, under varying illumination conditions, scale, and complex terrain morphologies. In order to accomplish that, we conceived \textit{MARTIAN} (Mars Aerial Rendering Tool for Imaging and Navigation), a custom framework developed with the 3D computer graphics software Blender, to generate realistic Martian scenes from Digital Terrain Models (DTMs) and ortho-rectified images sourced from HiRISE. MARTIAN allows the rendering of both RGB and depth images from simulated camera setups, including perspective and orthographic views, with user-defined camera parameters and poses. In this framework, it is possible to simulate navigation camera observations and orthographic maps at different times of the day for the selected site by properly tuning the orientation of the Sun in the simulated environment. The capability to generate a dataset spanning various combinations of lighting conditions and terrain perspectives enables the creation of multiple map/observation pairs, serving as a robust training set for our deep learning models to learn scale and lighting invariance.

% In the following sections, we will first provide an overview of the HiRISE camera and the data products selected for this work. We will then discuss the MARTIAN framework in more detail, including HiRISE data import, terrain modeling, camera placement, and scene and lighting configuration. Finally, we will describe the generation of the rendered maps and aerial observations constituting the Mars synthetic dataset, followed by a discussion on how their image and depth information are used to form image pairs for efficient model training.

\subsection{HiRISE Data}
\label{subsec:hirise} 

With its resolution capability up to 30 centimeters per pixel from 300 km altitude, HiRISE has been serving as an indispensable orbital asset for identifying and selecting safe landing sites for robotics exploration missions. The camera is equipped with a 0.5-meter primary mirror and can capture images in blue-green or BG (400-600 nm), red (550-850 nm), and near infra-red or NIR (800-1000 nm) bands. The instrument uses 14 CCD sensors, with red images being 20,048 pixels wide (with a swath width of 6 km from 300 km altitude), and BG and NIR images being 4,048 pixels wide (with a swath width of 1.2 km from 300 km altitude) \cite{hirise_specs}.
% The onboard memory holds up to 28 Gbit, with images transmitted in compressed form, typically using JPEG 2000 format \cite{hirise_specs}. %Further specifications are reported in Table \ref{tab:hires_specs}, adapted from \cite{hirise_specs}.
% \begin{table}
% \centering
% \begin{tabular}{l l l}
% \textbf{Parameter} & \textbf{Performance }& \textbf{Comments} \\\hline
% & & \\
% Ground Sample Distance (GSD) & 30 cm/pixel & From 300 km altitude \\
% & & \\
% Telescope aperture	& 0.5 m, f/24 &  For resolution and SNR \\
% & & \\
% Spectral range	& 400 to 600 nm &  Blue-Green (BG) \\ 
% & 550 to 850 nm & Red \\
% & 800 to 1000 nm & 	Near infra-red (NIR) \\
% & & \\
% SNR  Blue-Green & Typically 100:1 & Achieved with TDI, backside thinned \\
% \hspace{2em} Red & Typically 200:1 & CCDs, and 50 cm aperture \\
% \hspace{2em} NIR & Typically 100:1 & \\
% & & \\
% Swath Width  Red & $>$6 km & From 300 km altitude\\
% \hspace{5.8em} BG \& NIR & $>$1.2 km & From 300 km altitude\\
% & & \\
% Swath length & $>$2x swath width	& Along track \\
% & & \\
% Data precision	& 14 bit A/D & 12 to 13 bit usable \\
% & & \\
% Data compression & Real-time 14 to 8 bit & Look-up table \\
% & Up to 16 x 16 binning & Increases areal coverage \\
% & Lossless compression at SSR &  $\approx$ 2:1 \\
% & & \\
% Data storage & 28 Gbits	& All channels \\
% & & \\
% Number of pixels across swath & 20,264 Red & From swath and pixel scale \\
%  & 4,048 Green and NIR	& \\
%  & & \\
% TDI line time & $\geq$ 76  $\mu$sec & To match ground track speed \\
%  & & \\
% CCD read noise	& $<$50 electrons rms at 22$^{\circ}$C & Achieve SNR at low signal levels \\
%  & & \\
% FOV	& 1.14$^{\circ}$ x 0.18$^{\circ}$ & \\
%  & & \\
% IFOV & 1 x 1 $\mu$rad & Detector angular subtense \\
%  & & \\
% Relative Radiometry	& $<$1 \% pixel to pixel & Absolute 20 \% \\
%  & & \\
% \hline
%  & & \\
% \end{tabular}
% \caption{\label{tab:hires_specs} HiRISE Requirements and Performance Characteristics. Adapted from \cite{hirise_specs}}
% \end{table}
% The main HIRISE data products used in this work are DTMs and ortho-images generated from stereo imaging of the same area on the ground, with a multi-step process aimed at achieving high accuracy and detail \cite{hirise_dtm}.
% DTMs are derived from stereo pairs of images of the same area on the ground, captured from different viewing angles. Initially, the images are corrected for any optical distortions inherent to the HiRISE camera, with the aid of spacecraft pointing information at each observation time. Then a bundle adjustment is performed to align the stereo images to each other and to the global elevation map produced by the Mars Orbiter Laser Altimeter (MOLA). Once triangulated and reviewed for any artifacts, terrain models are produced with a  typical post spacing (i.e. the distance in meters between individual data points - posts - in the DTM) about four times the pixel resolution of the input stereo images.
% The sourced stereo pairs used to generate the terrain model are then employed to produce ortho-images images, where each pixel has been geometrically corrected to represent a nadir view of the terrain, with topographical distortions being eliminated.
In this work, we chose to utilize a 1 m / post DTM and a 0.25 m / pixel ortho-image with equirectangular projection generated from stereo pairs imaging of the Jezero Crater landing site for the Mars2020 Mission, over an area of 6.737 km by 14.403 km. The specification of the left and right stereo observations are reported in Table \ref{tab:hirise_stereo}.
% while a 1 m / pixel resolution version of the ortho-image is shown in gray-scale in Figure \ref{fig:jezero-hirise-ortho}.
We define the Sun elevation (EL) and azimuth (AZ) angles as the Sun angle above the horizon, and the counter-clockwise angle between the sub-solar point on the terrain and the line from the center to the right edge of the observation, respectively. We will keep this convention for the rest of this work unless specified differently.
% The right observation (ESP\_045994\_1985) of the source stereo pair has been acquired with a resolution of 28.1 cm/pixel on May 19, 2016 at 15:14 local Mars time, with the Sun about 42$^{\circ}$ above the horizon and with 2.9$^{\circ}$ sub-solar azimuth, defined as the clockwise angle from the observation reference axis (i.e. the line from the center to the right edge of the image) to the sub-solar point on the Mars surface. The left observation (ESP\_046060\_1985) has a resolution of 29.6 cm/pixel and has been captured on May 21, 2016 at 15:22 local Mars time, with a Sun angle above the horizon of 40$^{\circ}$ and 1.7$^{\circ}$ sub-solar azimuth. We will refer to the Sun angle above the horizon as \textit{Sun elevation angle} (EL) and the opposite of the sub-solar azimuth as \textit{Sun azimuth angle} (AZ), for the remainder of this work.
% A 1 m / pixel resolution version of the ESP\_046060\_1985\_RED\_A\_01\_ORTHO ortho-image (ESP\_046060\_1985\_RED\_C\_01\_ORTHO) is shown in gray-scale in Figure \ref{fig:jezero-hirise-ortho}, for visualization purposes. Further information on the stereo images is reported in Table \ref{tab:hirise_stereo}.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\linewidth]{rss2025_template/Figures/ortho-map-jezero.jpg}
%     \caption{HiRISE orthorectified image of the Jezero Crater landing site.}
%     \label{fig:jezero-hirise-ortho}
% \end{figure}

\begin{table}
\centering
\begin{tabular}{l l l}
\textbf{Parameter}                 & \textbf{Left Image}                   & \textbf{Right Image}                   \\ \hline
Product ID: & ESP\_046060\_1985 & ESP\_045994\_1985 \\
Acquisition date           & 24 May 2016                            & 19 May 2016                            \\ 
Local Mars time            & 15:22                                  & 15:14                                  \\ 
Latitude        & 18.431°N                                & 18.427°N                                \\
Longitude           & 77.438°E                                & 77.437°E                                \\ 
Spacecraft altitude        & 278.4 km                 & 280.0 km                \\
Original image scale & 29.6 cm/pixel  & 28.1 cm/pixel   \\
North Azimuth & 270° & 270° \\
Sun Elevation      & 40°  & 42°  \\ 
Sun Azimuth        & 183°                                   & 183°                                   \\ \hline
\end{tabular}
\caption{\label{tab:hirise_stereo}Comparison of the left and right HiRISE stereo images of the Jezero Crater landing site for the Mars2020 mission.}
\end{table}

\subsection{\label{subsec:MARTIAN}MARTIAN: Mars Aerial Rendering Tool for Imaging and Navigation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{rss2025_template/Figures/MARTIAN_view.png}  \caption{\label{fig:MARTIAN_view}View of the Blender terrain for the Jezero Crater site in MARTIAN.}
\end{figure}

We developed a python-based framework in the open-source 3D computer graphics software Blender to import HiRISE DTMs and ortho-images, and generate synthetic maps and aerial observations of the sourced Martian site under specified lighting conditions.

\subsubsection{Terrain and texture modeling}
\label{subsubsec:terrain_modeling}
The Jezero HiRISE DTM in PDS-compliant .IMG format was imported in Blender 4.0 using a modified version of the original HiRISE import plug-in \cite{BlenderHiRISEDTMImporter}. This add-on can load the terrain data at a desired resolution within a range (0., 100] $\%$ of the full model resolution and generate a mesh. By leveraging terrain metadata, including geographic information and resolution, a UV map is created for ortho-images to be draped over the companion DTM and used as a high-fidelity terrain texture. 
% This approach enables the user to initially stage a scene with a lower resolution terrain, perform Blender operations such as texture application and scene setup, and then increase the resolution to prepare for the final rendering, thereby optimizing computational time. A triangulation algorithm is then used to create a mesh from the imported terrain data. The algorithm generates a list of valid vertices and faces, leveraging the orthogonal structure of the DTM raster for computational efficiency. Each vertex is defined by a 3D coordinate tuple read from the DTM raster, with the z-coordinate representing the terrain elevation (in meters) and with the xy-coordinates corresponding to the pixel indices in the raster, scaled by the DTM resolution (in meters / pixel). Each face is defined by a triplet of vertex indices forming triangular elements in the mesh. The triangulation processes the raster row by row, identifying valid quadrilateral areas formed by vertices in adjacent pairs of rows, with each area being subsequently divided into two triangles. This process continues until the entire raster has been properly triangulated. By leveraging terrain metadata, including geographic information and resolution, the modified HiRISE importer also creates a UV map for the generated mesh in Blender, allowing for ortho-images to be draped over the companion DTM and used as a high-fidelity terrain texture.
The MARTIAN framework initially imports the Jezero DTM with 10$\%$ of its original resolution, allowing for efficient terrain setup. Then, a material is added to the mesh, with its surface shading model being controlled via a Principled Bidirectional Scattering Distribution Function (BSDF).
% node set in the Blender shading editor. The Principled BSDF integrates multiple material properties, such as diffuse, metallic, subsurface and transmission into a single node, thus allowing to model a wide variety of materials. For the simulated Mars terrain, we set the Principled BSDF with the main parameters shown in Table \ref{tab:principled_bsdf}. 
% \begin{table}
% \centering
% \begin{tabular}{l l l}
% \textbf{Parameter}      & \textbf{Values}            & \textbf{Description} \\ \hline
% Metallic  & 0.0  & Blends between a dielectric \\
% & & and metallic material model from \\
% & & 0 (non-metal) to 1 (fully metallic). \\ 
% \\
% Roughness  & 0.9 & Controls the surface's smoothness;\\
% & & from 0.0 (glossy) to 1.0 (rough). \\
% \\
% IOR & 1.45 & Defines the refractive index, mainly for\\
% & &transparent or refractive materials. \\
% \\
% Alpha  & 0.0  & Controls the transparency of the material. \\ 
%   &   & from 0 (opaque) to 1 (fully transparent) \\ 
% \hline
% \end{tabular}
% \caption{\label{tab:principled_bsdf} Main parameters of the Principled BSDF node in Blender for the synthetic Mars terrain.}
% \end{table}
The 0.25 m / pixel Jezero ortho-image is then loaded in the shading editor in .JP2 format to serve as the base texture for the terrain surface. The ortho-image coordinates data are retrieved by a Texture Coordinate Node to ensure that the texture is properly mapped onto the 3D mesh. Finally, the mesh is reloaded with its full resolution of 1 m / post.

\subsubsection{Scene setup and camera modeling}
\label{subsubsec:scene_setup}

MARTIAN allows for setting multiple scenes for perspective and orthographic imaging with user-defined camera intrinsics and extrinsics (\ref{fig:MARTIAN_view}). Given the \textit{world} reference frame $W$ defined as a East-North-Up coordinate system with origin on the terrain map center, the camera can be located in $W$ by providing the xy-coordinates and the altitude (in meters) with respect to the terrain at those coordinates. To position the camera object above the terrain mesh at the desired altitude, MARTIAN adopts a ray-tracing approach by using a Bounding Volume Hierarchy tree, a data structure used by Blender to efficiently organize geometric objects in 3D space. The camera frame $C$ is centered at the camera's optical center, with its X-axis pointing to the right along the image width, the Z-axis pointing towards the terrain, and the Y-axis completing the orthogonal set. The attitude of the the $C$ frame with respect to the world frame $W$ can be specified as input, and the final pose ($\mathbf{R}_{WC} \vert \mathbf{t}_{WC}$) is saved, where $\mathbf{t}_{WC}$ is the location of the camera in world coordinated and $\mathbf{R}_{WC}$ is the rotation matrix that aligns $W$ to $C$.

\subsubsection{Lighting}
\label{subsubsec:lighting}

MARTIAN provides the capability to render scenes in Blender with various illumination conditions by tuning Sun light source parameters such as the irradiance, the apparent disk diameter and orientation in the map frame. Higher irradiance values cast brighter illumination and shadows in the scene, while the angular diameter of the Sun disk as seen by the scene controls the softness versus harshness of the shadows. The Sun orientation simulates the scene time of the day and it is specified by the user though Sun elevation (EL) and azimuth angles (AZ) as shown in Figure \ref{fig:MARTIAN_view}. The lighting computations are performed using the Blender Cycles engine. This is a physically-based rendering engine that uses a ray tracing algorithm to accurately simulates light behavior. Light rays are traced from the camera into the scene, bouncing around until they find the simulated Sun light source or the world background, following the surface BSDF. In this work, we set Sun irradiance to the maximum value of 590 W/m$^2$ and the angular diameter to 0.35°, coherently with actual estimations for Mars. The Cycles engine, the BDSF and the simulated camera exposure have been concurrently and empirically tuned for a rendered orthographic view of the terrain to qualitatively match the visual appearance of the ortho-projected map under the same lighting (\ref{fig:jezero-hirise-ortho}).
% Cycles allows for the controlling further properties such as the maximum number of light bounces and the number of bounces for the different ray types.
% A list of the main light objects and cycles parameters used in the MARTIAN framework is shown in Table \ref{tab:cycles_settings}.

% \begin{table}
% \centering
% \begin{tabular}{l l}
% \textbf{Parameter} & \textbf{Value}\\ \hline 
% Sun irradiance & 580 W/m$^2$ \\ 
% Disk angular diameter & 0.35$^{\circ}$  \\ 
% Samples & 16  \\
% Max Bounces  & 100 \\
% Min Bounces & 10 \\  
% Diffuse Bounces & 10  \\  
% Glossy Bounces & 4 \\  
% \hline
% \end{tabular}
% \caption{\label{tab:cycles_settings} Sun object Cycles Rendering Engine settings in MARTIAN.}
% \end{table}

\subsubsection{Rendering}
\label{subsubsec:rendering}
Gray-scale and depth images can be rendered in with customized settings for each individual scene. For each camera view, users can specify image pixel resolution, pixel aspect ratio, and exposure, allowing scenes to be rendered with distinct visual properties while utilizing the same underlying terrain. The simulated camera exposure can be adjusted for a fine-tuning of the overall brightness level in the rendered image. Depth images from perspective and orthographic cameras are rendered in Blender by using a depth buffer. This consists of a data structure that during the rasterization records the component, along the camera's viewing direction, of the distance from an object in the scene to the camera at each pixel of the rendered image. The resulting image is a depth map of the observed scene, with depth values expressed in meters.
% In this context, we will refer to this component as the $z_c$ distance, with Z being the camera viewing axis. The depth-buffer $z_d$ stored at the distance $z_c$ from the camera is given by:

% \begin{equation}
%     z_d = \frac{f}{(f - n)} -  \frac{1}{z_c} \frac{fn}{(f - n)}
%     \label{eq:z-buffer}
% \end{equation}

% where $z_d$ is normalized in the range $[0,1]$, with $f$ and $n$ representing the distances from the far and near clipping planes of the camera, respectively. 
% For each fragment generated by the rasterizer (the component in the graphics pipeline that is responsible for converting 3D geometric data into a 2D image) at the rendering stage, the $z_c$ distance is compared to the one already stored in the buffer at the same pixel location. If the new fragment is closer to the camera, the depth buffer is updated with the new value; otherwise, the fragment is discarded. This process ensures that objects closer to the camera correctly occlude the ones in the background. 
% Typically, the depth buffer is stored in fixed-point format. For observations taken from nadir-pointing perspective cameras in the 64-200 m altitude range, the depth-precision is 5 orders of magnitude less than the DTM accuracy, as shown in the right panel of Figure \ref{fig:depth-precision}.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\linewidth]{Figures/z-buffer_accuracy_zoom.eps}
%     \caption{\label{fig:depth-precision} Depth precision computed for perspective cameras in Blender within the $z_c$ range [0, 200] for different distances $n$ of the near clipping plane and the far clipping plane $f$ at 100000 m.}
% \end{figure}

% \subsubsection{Operating modes}
% \label{subsubsec:operating_modes}

% MARTIAN supports two main operating modes: \textit{demo} and \textit{dataset}.
% The demo mode offers a quick tool to sample desired locations on the Martian DTM and assess the visual appearance of the imaged scene under different viewpoints and illumination conditions. It generates rendered images and depth maps from two perspective cameras and one orthographic camera, with desired poses specified by xy-coordinates, altitude and the yaw, pitch and roll angles with respect to the world reference frame $W$ defined above. With this mode it is also possible to visualize pixel-to-pixel correspondences between images generated by two overlapping perspective views, or between the perspective and the orthographic images. The demo can be useful to inform the parameters space to be used for the dataset generation process. 

% The other supported mode renders a set of RGB and depth images from different poses of a perspective camera and an orthographic map of the terrain, with the aim to simulate multiple aerial observations and synthetic HiRISE-like maps under different lighting. Figure \ref{fig:jezero-hirise-ortho} shows that the HiRISE terrain map is included in a bounding box. A lower-resolution version of the loaded HiRISE ortho-image can be imported in MARTIAN to extract the size and orientation of the valid terrain with respect to the world frame while saving computational resources. This step is crucial to ensure that camera object xy-coordinates ($t_{{WO}_x}, t_{{WO}_y}$) are always sampled from the actual terrain (with valid depth points) rather than areas of the map bounding box, the pixels of which do not correspond to mesh points (characterized by invalid depth points). A set number of observations, provided by the user, is randomly sampled from the valid terrain areas, with ($t_{{WO}_x}, t_{{WO}_y}$) drawn from a uniform distribution within certain ranges. In order to accomplish that, the terrain orthographic width ($w_T$) and height ($h_T$) in meters are initially inferred by an automatic identification process of the terrain corners in the ortho-image, and with the help of the HiRISE map metadata. The terrain center coordinates $c_{T}= (c_{T_x}, c_{T_y}, c_{T_x})$  are identified in the world frame $W$, and a terrain reference frame $T$ can be defined with origin in $c_{T}$ , the x-axis pointing rightward along the terrain width direction, the y-axis pointing upward along the terrain height direction, and the z-axis completing the orthogonal set. Then the translation $t_{WT}$ and the rotation matrix $R_{WT}$ that transform the frame $W$ into $T$ are computed. This process allows for randomly sampling the camera $x_T$ and $y_T$ coordinates in the $T$ frame, within the ranges [$-(w_T - M_w), w_T - M_w$] and [$-(h_T - M_h), h_T - M_h$] respectively, where $M_w$ and $M_h$ are width and height margins equal to half of the camera swath width and height at the maximum altitude of the user-defined altitude range. The sampled points are then transformed back to the world frame by using the terrain pose ($R_{WT} \vert t_{WT}$). At each of the sampled locations, the camera is placed above the mesh with altitude and yaw, pitch, and roll values drawn from user-defined ranges with an uniform random distribution. The camera intrinsics and the pose in terms of $t_{WO}$ and ($\gamma$, $\beta$, $\phi$), along with altitude data and clipping ranges, are stored for each observation and serve as ground truths for the image and depth data in the dataset. Camera parameters and poses are also stored for the orthographic cameras rendering map images and depth.
% Due to the large size of the map, the orthographic map images are rendered in tiles for efficient rendering, storage, processing, and retrieval during data loading. Orthographic depth images of the same map are produced in a similar way.

% Finally, the MARTIAN dataset mode allows for independent rendering of observations and maps, enabling users to create various combinations of lighting conditions. This flexibility facilitates achieving the desired lighting variability between the maps and observations.

\subsection{Training image dataset}
\label{subsec:training_set}

We generated a training image dataset comprising 17 orthographic gray-scale maps rendered from combinations of Sun azimuth ($0^{\circ}-360^{\circ}, 45^{\circ} steps$) and elevation $(30^{\circ}, 60^{\circ}, 90^{\circ}$), one corresponding orthographic depth map, and 4,5000 nadir-pointing aerial observations at fixed Sun angles AZ=180$^{\circ}$ and EL=40$^{\circ}$ along with their corresponding depth images. The query observations were randomly sampled from the HiRISE Jezero Crater's DTM with uniform distribution in the (x,y) coordinates in the world frame and within altitude range [$64, 200$] m. The query camera intrinsics are given by a pinhole camera model characterized by a sensor width of 80 mm, a focal length of 32 mm, 0 lens shifts along the image width and height axes, and an unitary pixel aspect ratio. Camera extrinsics, along with altitude data, were stored for each observation and served as ground truth. Further details are reported in Table \ref{tab:dataset_params}. Figure \ref{fig:map_tile_w_obs} shows gray-scale and normalized depth images of a map tile, with three sampled observations. 
Training examples for Geo-LoFTR are created by forming triplets ($I_A$, $I_B$, $I_C$) out of query observations and map windows crops (gray-scale and depth images) for multiple combinations of Sun azimuth and elevation angles' offsets between queries and the source maps. For a given combination of query and map lighting, each query observation is paired with map windows exhibiting a >25\% area overlap on the terrain. Therefore, the set of triplets is formed by different combination of image $I_A$ with the mop window tuple ($I_B$, $I_C$). The map window sizes are carefully chosen to introduce an appropriate level of scale variance within the altitude range of the observations, ensuring a balance between model generalization and training efficiency. 
% By choosing prefixed map windows size of 1320 $\times$ 990 pixels for observations within the [64, 132] m altitude range, and a size of 2000 $\times$ 1500 pixels for the (132, 200] m, the scale ratio between query images and map windows is ensured to vary between 1 and 3.
Geo-LoFTR has been fine-tuned from the original LoFTR pre-trained model on a total 
of 150,705 generated triplets. An independent validation set of 3,400 triplets has been used to regularly assess the model performance during training and prevent over-fitting. 

\begin{figure}
    \centering
    \begin{minipage}[b]{0.9\linewidth}
        \centering
        \includegraphics[width=\linewidth]{rss2025_template/Figures/map_tile_w_3_obs_aligned.jpg}
    \end{minipage}
    \caption{\label{fig:map_tile_w_obs} Gray-scale (left) and normalized depth (middle) images of a rendered orthographic map tile with three observations (right) sampled from different locations under the same illumination conditions.}
\end{figure}


\begin{table}
\centering
% Review this table
\begin{tabular}{l l l}
\textbf{Parameters} & \textbf{Maps} & \textbf{Observations} \\ \hline 
N.o. gray images & 17 & 4500 \\
N.o. depth images & 1 & 4500 \\
Image size & 26,949 $\times$ 57,613  & 480 $\times$ 640  \\ 
Pixel resolution & 0.25 m / px  &  [0.25, 0.78] m / px \\
Projection type  & Orthographic & Perspective \\  
Orthographic scale & 6737 m & / \\
Focal length & / & 32 mm \\
Sensor width & / & 80 mm \\
Location in $W$ & (0,0) m &  uniform random \\
&  &  distribution \\
Orientation in $W$ & nadir-pointing & nadir-pointing \\
Altitude & 4000 m & [64, 200] m\\
Sun AZ   & [0, 360]$^{\circ}$ with 45$^{\circ}$ steps & 180$^{\circ}$ \\  
Sun EL & 30$^{\circ}$, 60$^{\circ}$, 90$^{\circ}$ \\  
\end{tabular}
\caption{\label{tab:dataset_params} Parameters for the maps and observations image dataset sourced for the generation of the pairs formed by query observations and map windows, used for LoFTR and Geo-LoFTR training.}
\end{table}


% \subsection{Test image datasets}
% \label{subsec:test_set}




% Figure \ref{fig:pairs_examples} shows three training pairs examples.
% \begin{figure}
%     \centering
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Figures/30_0_000854_map_win.png}
%     \end{minipage}
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%          \includegraphics[width=\linewidth]{Figures/30_0_map_000854_query.png}
%     \end{minipage}
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Figures/60_0_000886_map_win.png}
%     \end{minipage}
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Figures/60_0_000886_query.png}
%     \end{minipage}
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Figures/90_0_000003_map_win.png}
%     \end{minipage}
%     \begin{minipage}[b]{0.4\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{Figures/90_0_map_000003_query.png}
%     \end{minipage}
%     \caption{\label{fig:pairs_examples} Examples of LoFTR training pairs formed by map windows with different lighting (\textit{left}) and query observations (\textit{right}) at 180$^{\circ}$ Sun azimuth (AZ) and 40$^{\circ}$ Sun elevation (EL). Maps are shown with 0$^{\circ}$ AZ and 30$^{\circ}$ EL (\textit{top}), 60$^{\circ}$ EL (\textit{mid}), 90$^{\circ}$ EL (\textit{bottom}). }
% \end{figure}

