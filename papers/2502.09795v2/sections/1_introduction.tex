\section{Introduction}
\label{sec:intro}

%\blfootnote{* Work done as an employee at the Jet Propulsion Lab, California Institute of Technology.}

The demonstration flights of NASA's Mars Helicopter, \textit{Ingenuity}, have marked a groundbreaking milestone in Mars exploration and scientific discovery \cite{Grip2022}. The next generation of Mars rotorcraft will require even more sophisticated autonomous navigation capabilities to access diverse terrains under challenging environmental conditions and to support long-range, fully autonomous flights, with the ultimate goal of enabling high-priority investigations in Martian astrobiology, climate, and geology~\cite{Bapst2021Mars}. 

One of the next evolutionary steps in advancing aerial mobility on the red planet is represented by the Mars Science Helicopter (MSH), a concept of a hexacopter with a payload capacity of up to 5 kg and lateral traverse capability over 10 km \cite{Tzanetos2022}.
% The extended geographic range of investigations combined with higher science payload capability, make MSH the perfect fit for groundbreaking science missions \citep{Bapst2021Mars}. Large-scale geophysical mapping at Lucus Planum can produce a finer characterization of the crustal remanent magnetism \cite{langlais2019} than the orbital measurements. MSH could perform in-situ sampling operations in clay minerals-rich sites to unveil the aqueous activity on the early Mars surface and subsurface \cite{BISHOP2020, ehlmann2014}. Finally, the possibility to reach multiple distant targets mostly inaccessible with the current rovers' capability  would allow to explore water ice exposures to gain critical insights into the planet's climate history \cite{bramson2021, Galofre2021Comparative, colin2018}.
To achieve precision during such long-range traverses, it is crucial to minimize drift in position estimates generated by the on-board Visual Inertial Odometry (VIO) in a global navigation satellite system (GNSS)-denied environment like Mars. During flights demos on Mars, Ingenuity's VIO algorithm produced a position error drift of 2-6\% over a flight envelope that includes flight with up to 625 meters of total distance traversed, at a maximum forward velocity of 5 m/s and a maximum altitude of 10 m. 
%Its geo-localization in a global frame was performed post-flight with human intervention by manually registering the navigation camera images to an orbital map. 
For MSHâ€™s long-range traverses within $\sim$10 km and altitudes up to 100 meters, the drift is expected to be considerably higher and onboard global localization needs to be performed online. 

Global localization can be accomplished by registering images captured by the rotorcraft's navigation camera onto orbital maps that are pre-registered to a global reference frame
%and downloaded onboard beforehand. 
followed by the derivation of the rotocraft's position and orientation relative to the orbital map.
This inherently drift-free geo-localization technique is referred to as Map-based Localization (MbL). At the heart of every MbL system lies an image registration method that identifies distinctive features or landmarks in the onboard image and the map in order to enable localization. 
The image registration can be very challenging in this domain due to the large differences in lighting between the onboard image and the map. Another factor that may challenge MbL performance is that of scale difference, particularly relevant for the MSH which needs to operate in a wide range of altitudes up to 100 m. 
Furthermore, the localization accuracy can also be impacted by variation in terrain morphology and textures. Complex, high-relief terrain may strengthen visual disparity between the onboard and map images at different times of day, due to changes in shadow casting. Conversely, textureless terrain can hinder the identification of distinctive features necessary for matching the map and image. 

In spite of the recent progress of deep learning in visual tasks, space applications still rely mostly on template-matching techniques or hand-crafted features to solve the image registration problem for two main reasons: 1) Low computational requirements, and 2) They solve a relatively narrow problem with strong assumptions being made regarding the viewpoint, scale, and lighting conditions.
Examples include the Lander Vision System (LVS)~\cite{johnson2023} developed for the Mars2020 mission, the recently introduced rover global localization system~\cite{nash2024}, and initial studies on MbL for MSH~\cite{brockers2022}.
% LVS https://ntrs.nasa.gov/citations/20190025658
% Enhanced LVS: https://arc.aiaa.org/doi/abs/10.2514/6.2024-0314
However, such strong assumptions limit the operational capability of a future Mars rotocraft. For example, the lack of robustness to different illumination conditions would restrict the flights to the time of day when the orbital map was originally collected, thus posing stringent constraints to the operational mission envelope.
%MbL typically consists of matching a query image against the map image, identifying distinctive features or landmarks in both the real-time and reference images, enabling the pipeline to correlate the current view to the corresponding map area, and pinpoint the rotorcraft precise location in a global frame. 
%However, the performance of MbL employing traditional feature matching techniques (e.g. SIFT) has been shown to suffer the most when the onboard imagery is taken under different lighting conditions than the orbital image references \cite{brockers2022}. This issue risks to restrict the flights of future rotorcraft to the time of day when the onboard map was originally collected, thus posing stringent constraints to the operational mission envelope. 
%Another factor that may challenge MbL performance consists of a scale variance between the map and the onboard observation images, particularly relevant for the MSH, which needs to operate in a wide range of altitudes up to 100 m. Furthermore, the localization accuracy can also be impacted by variation in terrain morphology and textures: Complex, high-relief terrain may strengthen visual disparity between the query and map images at different times of day, due to changes in shadow casting. Conversely, textureless terrain can hinder the identification of distinctive features necessary for matching the map and image. 
While recent deep learning methods~\cite{loftr,roma} have demonstrated robustness to illumination and scale variations on in-the-wild datasets such as MegaDepth~\cite{megadepth}, the main bottleneck is the lack of large-scale datasets that would allow finetuning these methods in planetary domains.


\begin{figure*}
    \setlength{\abovecaptionskip}{0pt}
    \setlength{\belowcaptionskip}{0pt}
    \centering
    \includegraphics[width=\linewidth]{Figures/concept_large_corr.png}
    \caption{Given an ortho-projected map of the terrain and a simulated onboard image we aim to estimate the pose of a rotocraft operating on Mars. Assuming a noisy pose prior, a search area is selected that is further divided into smaller regions and passed sequentially to our geometrically-enhanced Geo-LoFTR observation-to-map matcher. Geo-LoFTR is trained from data generated by our simulation framework MARTIAN. Finally, the matches are then filtered and passed to RANSAC-PnP to estimate the pose.}
    \label{fig:CONCEPT}
\end{figure*}


In this paper, we explore a new MbL system that makes no assumptions regarding the illumination conditions or scale variation for vision-based geo-localization on Mars for a future Mars rotocraft. In particular, we incorporate a transformer-based method~\cite{loftr} for image registration into the MbL pipeline, and further enhance this method to use geometry in order to increase robustness to lighting variations. Furthermore, we introduce the Mars Aerial Rendering Tool for Imaging and Navigation (MARTIAN) which we use to generate a large-scale Mars dataset from orbital maps and train the image registration method.
%\subsection{Contributions}
%\label{subsec:contributions}
%This work aims to address the challenges of vision-based geo-localization on Mars with the following contributions:
%In summary, our contributions are as follows:
In summary, our contributions towards a robust MbL pipeline include:
\begin{itemize}
    %\item A new MbL system that enhances robustness to challenging lighting and scale variations by leveraging geometric context from digital terrain models in a state-of-the-art image matching model.
    %through the fusion of visual and 3D data from pre-referenced orbital maps and terrain models.
    \item A new image matching method, Geo-LoFTR, that uses geometric context from digital terrain models and can improve localization accuracy compared to prior methods @1m up to 31.8\% in challenging illumination conditions under low sun elevation angles. 
    \item A custom simulation pipeline to generate maps and aerial observations of realistic Martian landscapes derived from HiRISE~\cite{hirise} data under a wide variety of illumination conditions.  
    \item A comprehensive evaluation of our MbL approach validating its robustness under challenging environmental conditions which clearly demonstrates the advantage of incorporating geometric context. 
    %of the pipeline's performance on dedicated synthetic Mars datasets that span diverse lighting, scale, and terrain morphologies.
\end{itemize}


%Beyond Mars aerial mobility, these contributions could significantly benefit absolute localization tasks in GNSS-denied environments and challenging illumination conditions across different mobility domains of the autonomous robotic exploration of the Solar System. A key application could consist of advancing Terrain-Relative Navigation during the Entry, Descent, and Landing (EDL) phases of planetary missions. Future missions to the Moon's south pole will face significant visual navigation challenges at the landing stage due to the constantly low Sun elevation resulting in deep shadows and poor lighting. Similarly, this research can contribute to the autonomous navigation around small celestial bodies, where rapidly shifting lighting alters surface topography, complicating visual perception. Furthermore, this research has meaningful implications for Earth-based operations, such as in scenarios where autonomous Unmanned Aerial Vehicles (UAVs) might be deployed for critical research and rescue missions in damaged or confined indoor environments with poor lighting conditions and no access to GNSS.