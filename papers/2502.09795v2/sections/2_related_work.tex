\section{Related work}
\label{sec:related_work}



Space-based applications of MbL traditionally rely on template-matching techniques, where an ortho-rectified onboard image slides over a reference map in order to estimate pixel-wise similarity using distance metrics such as Normalized Cross-Correlation~\cite{pham2021rover}, Phase-Correlation~\cite{WAN2016198}, and Mutual Information~\cite{ansar2009multi}.
The Mars2020 Lander Vision System (LVS) integrated a coarse-to-fine template-matching approach in their terrain relative navigation pipeline to perform precise absolute localization on a Context Camera (CTX) map (6 m / pixel) during the mission's Entry Descent and Landing (EDL) phase \cite{johnson2023}.
The Censible framework proposed in \cite{nash2024} successfully performed global localization of the Perseverance rover by registering an ortho-mosaic of panoramic stereo images collected onboard to a HiRISE map (0.25m / pixel) using a modified census transform~\cite{zabih1994non}. Even though template-matching approaches were successful in the aforementioned cases, they were applied under minimal lighting variations, and they are generally not robust to viewpoint and in-plane rotation variations without a correction step. Recently, the Lighting Invariant Matching Algorithm (LIMA)~\cite{rothenberger2025illumination} proposed a new correlation-based method that is robust to challenging illuminations, but it assumes the existence of at least two pre-registered images with at least some lighting diversity.

Beyond template-matching, hand-crafted features such as SIFT~\cite{sift},~ORB \cite{orb}, SURF~\cite{surf}, and SOSE~\cite{cheng2024simultaneous} have been investigated and pitted against deep learning approaches for MbL and related problems. For example, the work of~\cite{brockers2022} demonstrated SuperPoint~\cite{superpoint} to outperform SIFT under very low sun elevation angles in terms of localization accuracy, while AstroVision~\cite{driver2023astrovision} showed
ASLFeat~\cite{luo2020aslfeat} to be more robust than hand-crafted features for feature tracking under large shadows for the task of small body navigation. Similar to our work, JointLoc~\cite{luo2024jointloc} proposes a vision-based system for UAV localization on Mars using SuperPoint~\cite{superpoint} and LightGlue~\cite{lindenberger2023lightglue} for local feature matching. However, JointLoc does not investigate challenging illumination and scale variations during localization and uses mostly synthetic images of Mars with a small sample of real images.

Many deep learning methods have been introduced for the fundamental task of image matching that have shown robustness to real-world changes in scale, illumination, and viewpoint. LoFTR~\cite{loftr} uses Transformers~\cite{transformer} in a detector-free manner, RoMa~\cite{roma} leverages features from the vision foundation model of DINOv2~\cite{dinov2}, and DKM~\cite{edstedt2023dkm} estimates a dense warp to provide a match for every pixel. Other works such as GAM~\cite{yu2022improving} and GoMatch~\cite{zhou2022geometry} sought to introduce geometric information during the matching process for the task of visual localization. Inspired by these recent developments, we aim to adapt the state-of-the-art method of LoFTR~\cite{loftr} for MbL on Mars and extend its architecture to leverage geometric context.

Finally, we acknowledge the extended
%There is a large 
body of work on vision-based global localization for UAVs. A recent survey that focuses on deep learning methods for UAV localization can be found here~\cite{couturier2024review, Xu2018VisionbasedUA}. A large part of this literature is devoted to Earth-based applications where typically a drone image is registered to geo-referenced satellite imagery. Given the abundance of data in this domain, progress was driven by the adaption of deep learning in existing pipelines that showed increased robustness to challenging conditions. For example, \cite{gurgu2022vision} incorporates SuperGlue~\cite{sarlin2020superglue} for solving this task for long distance flights in-the-wild, while~\cite{dai2023vision,he2024leveraging} rely on deep features for image retrieval for low altitude flights in urban environments. Multimodal inputs were also explored in this domain, with~\cite{zhu2023uav} including language descriptions of drone and satellite images to facilitate cross-view matching. 

%\subsection{Template matching for MbL on Mars}
%\label{subsec:template_matching}
%Map-based localization has been typically performed manually by humans on all Mars rover missions by using data products from HiRISE (High Resolution Imaging Science Experiment) and CTX (Context Camera) cameras onboard the Mars Reconnaissance Orbiter, including ortho-projected maps and Dgitial Terrain Models (DTMs) reconstructed from stereo-vision. The first MbL techniques successfully executed for onboard geo-localization on Mars with no human in the loop employed a template matching approach. This consists of sliding a patch, or \textit{template}, extracted from the query image against over a target reference image and finding the best match by optimizing similarity measures, such as Normalized Cross-Correlation (NCC), Phase-Correlation (PC) and Mutual Information (MI).
%The MARS2020 Lander Vision System (LVS) integrated a coarse-to-fine template matching approach in their terrain relative navigation pipeline to perform precise absolute localization on a CTX map (6 m / pixel) during the mission's EDL phase \cite{johnson2023}. 
%% The LVS was able to reduce the initial 3.2 km position uncertainty at 4200 m altitude to just 40 m at 500 m altitude in 10 seconds, by fusing landmarks matched between descent imagery and CTX map (6 m/pixel) of the Jezero Crater landing site (approximately taken at the same time of day) with inertial measurement unit (IMU) data. 
%The Censible framework proposed in \cite{nash2024} successfully performed global localization of the Perseverance rover by registering an ortho-mosaic of panoramic stereo images collected onboard to a HiRISE map with a template matching approach. 
%% The pipeline relies on a modified census transform, a non-parametric local image transform that is invariant to monotonic variations in intensity, applied on both the template and the orbital map. 
%% The Censible framework performance has been benchmarked on 264 panoramic images from Perseverance, achieving 0.36 m accuracy. However, despite being collected throughout all the Martian seasons, the majority of images were taken in the afternoon, relatively close to the time of day when HiRISE maps are typically captured, thus excluding challenging lighting corresponding to low Sun elevation angles (e.g., early morning and sunset). To address this limitation, we generated synthetic Mars image test sets to evaluate our MbL pipeline also under these extreme illumination scenarios.
%Despite template matching proved to be an excellent solution for robust geo-localization on Mars with challenging time constraints and limited computational resources, its performance has been tested just in the cases where reference maps and onboard imagery have been collected in approximately the same time of day. Illumination invariance with PC-based template matching has been investigated in \cite{WAN2016198}, where evidence of illumination-invariant properties has been found via mathematical derivation and experiments. The proposed MbL pipeline has been used to match rescaled HiRISE images simulating UAV observations to a HiRISE map taken with a 56$^\circ$ offset in Sun azimuth and 5$^\circ$ in Sun elevation. The overall localization accuracy of 0.35 m outperforms NCC- and MI-based techniques. However, the Sun elevation difference is too small to observe significant visual disparity between the images, and the reference elevation of 50$^\circ$ does not allow the azimuth offset to exhibit significant changes in the shadows' pattern. Moreover, the algorithm requires the query image and the map to be largely overlapped (usually more than 1/4 image size).

%\subsection{Hand-crafted feature matching}
%\label{subsec:hand-crafted_features}
%Feature matching represents one of the most common approaches for image matching in absolute visual localization tasks for UAV, with extensive reviews provided in \cite{COUTURIER2021103666, Xu2018VisionbasedUA}. Hand-crafted feature-matching methods can leverage feature detectors and descriptors that are manually designed to identify features with properties invariant to scale, rotation or lighting. Their simplicity, portability, and relatively low computational requirements make them good candidates for global localization tasks in  robotic planetary missions, where stringent constraints on robustness and onboard processing must be met. Moreover, their deterministic nature and well-established mathematical foundations facilitate thorough validation and verification, which are critical for mission reliability and compliance with spaceflight certification standards.
%% One of the most popular descriptors is the Scale-Invariant Feature Transform (SIFT) \cite{sift}, which encodes the orientation and magnitude of image gradients around keypoints detected with a Difference-of-Gaussian (DoG) function, making it highly robust to changes in scale and rotation. ORB (Oriented FAST and Rotated BRIEF) \cite{orb} combines a FAST detector \cite{fast} with a BRIEF descriptor \cite{BRIEF} adapted to achieve rotation invariance. The SURF \cite{SURF} method uses a Hessian matrix-based detector and a distribution-based descriptor to accelerate feature computation compared to SIFT, while keeping scale and rotation-invariance. 
%The use of popular hand-crafted feature descriptors such as SIFT \cite{sift}, ORB \cite{orb}, and SURF \cite{surf} in image-matching for the on-board map-based localization of the Mars Science Helicopter has been investigated in \cite{brockers2022}. The proposed pipeline's performance was evaluated on a synthetic Mars dataset sourced from a HiRISE ortho-image of the Jezero Crater landing site and generated using AirSim \cite{airsim}, which uses Unreal Engine 4 as a rendering engine. SIFT has demonstrated superior localization accuracy compared to all the other methods when tested across a range of Sun elevation offsets between observations and the reference map at 90$^{\circ}$ of elevation. However, its performance significantly declines at a  -60$^\circ$ elevation offset, highlighting the need for matching techniques that can better handle challenging lighting variations.

%\subsection{Deep-learning feature matching}
%\label{subsec:learning_features}
%Enhanced robustness to image variance due to lighting can be potentially addressed by deep-learning-based methods,  which now represent the state of the art in modern absolute visual localization for UAVs \cite{drones8110622}.  Unlike traditional hand-crafted methods that rely on manually designed feature detectors and descriptors, feature representations are learned directly from the image data, leverage Convolutional Neural Networks (CNNs) and Transformer architectures \cite{transformer} originally developed for natural language processing and adapted to computer vision tasks (e.g., Vision Transformer \cite{visiontransformer}). 
%%CNNs are particularly effective for extracting features like edges and textures across multiple abstraction levels, making them resilient to changes in lighting and perspective. Transformers add further robustness and distinctiveness by using attention mechanisms to provide features with contextual information and long-range dependencies from the entire image. 
%SuperPoint \cite{superpoint} is a self-supervised method fully based on CNNs to generate both feature keypoints and descriptors. In the MbL evaluation performed by \cite{brockers2022} on Mars simulated data, SuperPoint performs slightly worse than handcrafted methods under relatively low sun elevation offsets between map and observations. However, it performs slightly better than SIFT for drastic illumination changes, showing overall more robustness to lighting than hand-crafted feature descriptors.  
%Among the methods aiming to learn robust image matching under real-world changes in scale, illumination, viewpoint and texture, RoMA \cite{roma} offers a coarse-to-fine framework leveraging frozen features from large self-supervised models like DINOv2 \cite{dinov2} to predict matches at coarse resolution, combined with CNN-derived features for matching at a finer scale. 
%A popular 2D-2D matching method is the Local Feature TRansformer (LoFTR) \cite{loftr}. LoFTR is a detector-free model for local image feature matching that combines CNNs and Transformers to generate dense pixel-wise correspondences. This method employs a coarse-to-fine strategy, matching features at a coarse level and refining them to sub-pixel accuracy, with the Transformers exploiting the pixels' contextual information to ensure robust performance in low-texture regions. In this work, we incorporated LoFTR in our MbL pipeline for the localization of the MSH and we fine-tuned it on synthetic Mars datasets. Additionally, we extended its architecture with a novel multi-modal module, Geo-LoFTR,  using cross-attention mechanisms to fuse 3D data from HiRISE digital terrain model with the 2D orbital map, with the aim to improve accuracy and robustness under challenging illumination conditions offsets between observations and maps by leveraging geometric information.