\section{Methodology}
\label{sec:methodology}


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/Geo-LoFTR_v2_resized.png}
    \caption{Architecture of Geo-LoFTR that uses as inputs the query image $I^A$ and a map crop image $I^B$ along with its corresponding depth $I^C$. Geo-LoFTR learns to merge visual information from $I^B$ with geometric from $I^C$ using parallel $CrossAttn$ operations. The produced features then follow the coarse-to-fine approach proposed in LoFTR~\cite{loftr}. Our experimental evaluation shows that Geo-LoFTR is more robust than the original LoFTR under challenging illumination conditions.}
    \label{fig:geo-loftr}
\end{figure}


We propose a new MbL method that is robust to challenging illumination conditions on Mars. Our approach focuses on improving the registration capabilities of MbL, and addresses the lack of training data in this domain. More specifically, we introduce Geo-LoFTR, an image matching model that learns to merge geometric and image features (Sec.~\ref{subsec:geo-loftr}), based on the state-of-the-art method of LoFTR~\cite{loftr}. In addition, we present MARTIAN our simulation tool that uses real orbital maps (Sec.~\ref{subsec:martian}), and our strategy for generating a training set for Geo-LoFTR (Sec.~\ref{subsec:training_set}). Finally, we discuss the MbL pipeline (Sec.~\ref{subsec:mbl_pipeline}).
An overview of our method along with a summary of the pipeline is illustrated in Figure~\ref{fig:CONCEPT}.

%Our MbL pipeline registers gray-scale images of aerial query observations onto HiRISE-like ortho-projected maps under different illumination conditions using a LoFTR-derived image matching model. The proposed matcher - referred to as Geo-LoFTR - incorporates depth images sourced from HiRISE DTMs as an additional data modality to leverage geometric context from the reference maps.

% Our map-based localization pipeline performs image registration of gray-scale aerial query observations onto HiRISE-like ortho-projected maps employing a LoFTR-based matching model. One model is obtained by fine-tuning LoFTR on a synthetic Mars dataset generated from HiRISE ortho-images and digital terrain models in a custom rendering framework (see Section \ref{sec:mars_synthetic_dataset}). 
% A novel model is then generated by extending the vanilla LoFTR architecture to incorporate depth information from HiRISE maps as an additional data modality to the gray-scale imagery used as input. This new architecture utilizes a cross-attention mechanism to form shared feature embeddings where both depth and visual data can be effectively integrated. Our approach is intended to enhance the matching accuracy by leveraging the geometric information included in the depth data to resolve ambiguities in the visual representation, such as those caused by lighting variations. We refer to this geometry-aided version of LoFTR as \textit{Geo-LoFTR}. In the following sections, we will first provide a brief introduction to LoFTR as background, then we will describe the Geo-LoFTR architecture  and its integration within our MbL pipeline. 

%\subsection{Preliminaries: LoFTR \cite{loftr}.}
%\label{subsec:Prleiminaries: LoFTR}
%LoFTR is made of four different components: the Local Feature CNN, the Coarse-Level Local Feature Transform, the Matching Module and the Coarse-to-Fine Module. Given the image pair to be matched, the first module uses a CNN backbone to generate features maps at 1/8 (coarse) and 1/2 (fine) scales of the original images' size. In the second component of the architecture, the coarse feature maps are flattened to 1-D vectors and equipped with a positional encoding, before being processed by the local feature transform module. This employs a sequence of self-attention and cross-attention layers to extract position- and context-dependent local features. In the matching module, a similarity score matrix is computed between the transformed coarse features and a dual softmax operator is applied to produce a confidence matrix expressing the probability of mutual nearest matching. Matches selected based on a given confidence threshold are further filtered through mutual nearest neighbor criteria to remove outlier matches. The resulting coarse-level match predictions are refined to the images original resolution with a correlation-based approach in the coarse-to-fine module.
%% For every predicted matching pair, two sets of local windows are cropped from the fine feature maps. After being processed by a Transformer module similar to the one applied at the coarse-level stage, the transformed windows are correlated to produce fine-level matches at sub-pixel level,  accompanied by final confidence scores.

\subsection{\label{subsec:geo-loftr}Geometry-aided Local Feature Matching}
%\subsubsection{Preliminaries: LoFTR} 
\noindent \textbf{Preliminaries: LoFTR.}
We first briefly introduce the image matching approach we use as basis for our geometry-aided observation to map registration. LoFTR follows a detector-free approach to produce semi-dense matches between two images $I^A$ and $I^B$. Its strength lies on using transformers to process features from a CNN backbone at two scales $\tilde{F}^A \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times d}$ and $\hat{F}^A \in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times d}$, where $H,W$ are height and width of $I^A$ and $d$ is the feature dimension. This allows the already semantically rich pixel-wise representations to capture global image context. The corresponding features are also extracted from $I^B$. The method then follows a coarse-to-fine approach, where initial dense matching is performed at the coarse level, followed by refinement of the matches around a small window using the higher resolution feature maps.

We find LoFTR to be a suitable method for adopting into our MbL pipeline for two reasons: 1) The usage of transformers allows for some robustness to images with large shadows due to their ability to draw information from other parts of the image, and 2) it incorporates linear attention layers~\cite{katharopoulos2020transformers} that renders the method more computationally efficient than other approaches (e.g., RoMa~\cite{roma}). For more details please see the original paper~\cite{loftr}.
\\
\\
%\subsubsection{Geo-LoFTR}
\noindent \textbf{Geo-LoFTR.}
%\label{sec:Geo-LoFTR}
We aim to incorporate geometric context during feature matching between onboard observations and the reference map in order to increase robustness to challenging lighting scenarios where visual cues alone may lead to degeneracy.
In the context of map-based localization on Mars, we take advantage of the 3D information from a digital elevation model and enrich the learned representation of the ortho-projected map.

To accomplish this, we extend the original LoFTR architecture to take as input a depth image $I^C$ of a map crop along with the corresponding gray-scale image $I^B$ of the crop from the ortho-rectified map and the gray-scale onboard image $I^A$. Each input depth map crop is normalized using the highest depth value in the crop, to avoid overfitting on absolute depth values of local areas, but rather associate relative geometry with visual information. 
In order to extract the corresponding coarse $\tilde{F}^C$ and fine $\hat{F}^C$ features for $I^C$ we use the same ResNet-18~\cite{resnet-18} backbone from the original LoFTR architecture.

Our objective is to learn how to merge the features $\tilde{F}^B$, $\hat{F}^B$ with the corresponding $\tilde{F}^C$, $\hat{F}^C$ to produce coarse- and fine-level feature maps that incorporate both visual and geometric information of the local map area we wish to localize the vehicle.
To do so, we use cross-attention layers in both directions in the following manner:
\begin{align*}
     \tilde{F}'^B = G \left(CrossAttn(\tilde{F}^B, \tilde{F}^C) \oplus CrossAttn(\tilde{F}^C, \tilde{F}^B) \right)
\end{align*}
where the first argument for each $CrossAttn$ layer is used as the query, $\oplus$ concatenates the outputs along the feature dimension, and $G$ is a small feedforward network comprised of two linear layers followed by LayerNorm.
The resulting $\tilde{F}'^B \in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8} \times d}$ is the merged coarse level feature map. The corresponding process is repeated for the fine level maps to produce $\hat{F}'^B \in \mathbb{R}^{\frac{H}{2} \times \frac{W}{2} \times d}$. The rest of the pipeline follows the original LoFTR, with the merged feature representations being used in the coarse and fine transformer modules instead of the features extracted only from $I^B$. 
Figure \ref{fig:geo-loftr} shows the architectural details of Geo-LoFTR. 


% % UNCOMMENT


%Geo-LoFTR takes three inputs: a gray-scale image ($I^A$) of the query observation, a gray-scale image ($I^B$) cropped from the ortho-rectified map, a depth image ($I^C$) of the same map crop, where each pixel contains the component along the map camera's viewing direction, of the distance from an object in the scene to the camera focal plane. Like in the original LoFTR, $I^A$ is processed by a CNN backbone that produces the feature maps $\widetilde{F}^A$ and $\widehat{F}^A$ at 1/8 and 1/2 scales, respectively.
%In the Geo-Merger layer, $I^B$ is processed by the same backbone as for $I_A$, referred to as \textit{Image Backbone}; while $I^C$ is fed to a separate backbone that we denoted as \textit{Depth Backbone}. Both employ the modified ResNet-18 architecture proposed by \cite{resnet-18}, consistent with the original LoFTR framework.  By adopting distinct sets of learnable parameters, the backbones enable the model to specifically optimize for the unique characteristics of image and depth data during training, enhancing the processing capability of each modality. The backbones produce the coarse-level feature maps,\ $\widetilde{F}^B$, $\widetilde{F}^C$, and the fine-level feature maps $\widehat{F}^B$, $\widehat{F}^C$. $\widetilde{F}^B$ and $\widetilde{F}^C$ are provided to two cross-attention layers. Each one uses one feature map as context while transforming the other, generating intermediate maps for each modality that incorporate information from the other. The transformed feature maps are then concatenated along the channel dimension and sent to a Feed-Forward Neural network (FFN). The FFN is composed of a linear layer, a ReLU activation to introduce non-linearity, and a final linear layer that brings the features' channel dimension back to the input's one. The FFN output is then passed to a normalization layer to contribute to stabilizing the training and improving convergence. The final product is the merged coarse-level feature map $\underline{{\widetilde{F}}}^B$. The same process is repeated for the fine-level feature maps, $\widehat{F}^B$ and$\widehat{F}^C$, and the merged feature map $\underline{{\widehat{F}}}^B$ is generated at 1/2 scale. $\widetilde{F}^A$ and $\underline{{\widetilde{F}}}^B$ will be provided to the standard Coarse-Level Local Feature Transform module of LoFTR, while  $\widehat{F}^A$ and $\underline{{\widehat{F}}}^B$ will be used in the Coarse-to-Fine Module. As for the vanilla LoFTR, the final product of Geo-LoFTR is the set of fine-level matches $\mathcal{M}f$ with associated confidence scores. The supervision strategy adopted for Geo-LoFTR is consistent with the original architecture. 

% The coarse-level loss is a negative log-likelihood loss over the confidence matrix $\mathcal{P}_c$, which we chose to generate with the dual-softmax operator in Eq. \ref{eq:mutual_matching_prob}. To determine ground-truth labels for $\mathcal{P}_c$, camera poses and depth maps available for both the query observations and maps in the training set are used in map-to-query and query-to-map point warping functions to compute a set of mutual nearest neighbors between two 1/8-resolution grids, based on their re-projection distances. Denoting this ground-truth coarse matches as $\mathcal{M}_c^{gt}$, the negative log-likelihood loss $\mathcal{L}_c$:
% \begin{equation}
%     \centering
%     \mathcal{L}_c = -\frac{1}{\vert \mathcal{M}_c^{gt} \vert} \displaystyle \sum_{(\widetilde{i}, \widetilde{j})\in \mathcal{M}_c^{gt}} log \mathcal{P}_c (\widetilde{i}, \widetilde{j})
%     \label{eq:loss_coarse}
% \end{equation}
% is minimized over the grid points $(\widetilde{i}, \widetilde{j})$ in $\mathcal{M}_c^{gt}$.
% The fine-level supervision is performed by using a $l_2$ loss. Each query point $\widehat{i}$ is assigned a variance $\sigma^2(\widehat{i})$ which is computed from its heatmap obtained in the coarse-to-fine module. The goal is to minimize a weighted loss, $\mathcal{L}_f$, that prioritizes points with low uncertainty $\sigma^2(\widehat{i})$ and is given by:
% \begin{equation}
%     \centering
%     \mathcal{L}_f = \frac{1}{\vert \mathcal{M}_f \vert} \displaystyle \sum_{(\widehat{i}, \widehat{j}')\in \mathcal{M}_f} \frac{1}{\sigma^2(\widehat{i})} {\|\widehat{j}' - \widehat{j}'_{gt} \|}_2
%     \label{eq:loss_fine}
% \end{equation}
% where $\widehat{j}'_{gt}$ is obtained by warping each $\widehat{i}$ from $\widehat{F}^A_{tr}$ to $\widehat{F}^B_{tr}$ for the vanilla LoFTR ($\underline{\widehat{F}}^B_{tr}$, for Geo-LoFTR) with the ground-truth camera poses and depth.

%The original LoFTR model has been trained by on MegaDepth \cite{megadepth}, a large-scale dataset consisting of images collected from a variety of scenes, including urban, natural and indoor environments. We will denote this pre-trained model as \textit{Pre-trained LoFTR}. In this work, we fine-tuned both LoFTR and Geo-LoFTR on a synthetic Mars image dataset (Section \ref{sec:mars_synthetic_dataset}). We will refer to the fine-tuned model as \textit{Fine-tuned LoFTR} for the remainder of this work. The performance of Pre-trained LoFTR, Fine-tuned LoFTR and Geo-LoFTR will be evaluated in our MbL pipeline through several experiments in challenging lighting and scale variance (Section \ref{sec:mbl_eval}).


\subsection{MARTIAN: Mars Aerial Rendering Tool for Imaging and Navigation}
\label{subsec:martian}
Unlike Earth-based applications that have the privilege of abundant data~\cite{zhu2023sues,zheng2020university}, relevant, annotated, and large-scale datasets are not readily available in the Martian domain. Data released from the Mars2020 mission\footnote{https://mars.nasa.gov/mars2020/multimedia/raw-images/} offer, among other, observations from the Lander vehicle during EDL, and from the navigation camera of Ingenuity. However, the data do not come with accurate pose annotations, and they are far from complete to perform a comprehensive study on the robustness of an MbL pipeline on scale and illumination variations.

Instead, we take advantage of real map products created from the Mars Reconnaissance Orbiter (MRO) High-Resolution Imaging Science Experiment (HiRISE)~\cite{hirise} to create a large-scale dataset suitable for training Geo-LoFTR and evaluating our MbL pipeline. We developed a python-based framework in the open-source 3D computer graphics software Blender to import HiRISE Digital Terrain Models (DTMs) and textured ortho-projected images in order to generate maps and aerial observations of a Martian site under various lighting conditions and at different altitudes.
\\
\\
\noindent \textbf{HiRISE data.}
With its resolution capability at nadir of 25 centimeters per pixel from 300 km altitude, HiRISE has been serving as an indispensable orbital asset for identifying and selecting safe landing sites for robotics exploration missions. 
%The camera is equipped with a 0.5-meter primary mirror and can capture images in blue-green or BG (400-600 nm), red (550-850 nm), and near infra-red or NIR (800-1000 nm) bands. 
%The instrument uses 14 CCD sensors, with red images being 20,048 pixels wide (with a swath width of 6 km from 300 km altitude), and BG and NIR images being 4,048 pixels wide (with a swath width of 1.2 km from 300 km altitude) \cite{hirise_specs}.
In this work, we utilize a 1 m / post DTM and a 0.25 m / pixel ortho-image with equirectangular projection generated from stereo pairs imaging of the Jezero Crater landing site for the Mars2020 Mission, over an area of 6.737 km by 14.403 km. 
%The specification of the left and right stereo observations are reported in Table \ref{tab:hirise_stereo} \GG{do we need the specifications table? perhaps supplementary?}.
% while a 1 m / pixel resolution version of the ortho-image is shown in gray-scale in Figure \ref{fig:jezero-hirise-ortho}.
%We define the Sun elevation (EL) and azimuth (AZ) angles as the Sun angle above the horizon, and the counter-clockwise angle between the sub-solar point on the terrain and the line from the center to the right edge of the observation, respectively. We will keep this convention for the rest of this work unless specified differently. \GG{should we move these last sentences under Lighting?}
\\
\\
\noindent \textbf{Terrain and texture modeling.}
% \subsubsection{Terrain and texture modeling}
% \label{subsubsec:terrain_modeling}
The Jezero HiRISE DTM was imported in Blender 4.0 using a modified version of the original HiRISE import plug-in \cite{BlenderHiRISEDTMImporter}. This add-on can load the terrain data at a desired resolution within a range (0., 100] $\%$ of the full model resolution and generate a mesh. By leveraging terrain metadata, including geographic information and resolution, a UV map is created for ortho-images to be draped over the companion DTM and used as a high-fidelity terrain texture. 
% This approach enables the user to initially stage a scene with a lower resolution terrain, perform Blender operations such as texture application and scene setup, and then increase the resolution to prepare for the final rendering, thereby optimizing computational time. A triangulation algorithm is then used to create a mesh from the imported terrain data. The algorithm generates a list of valid vertices and faces, leveraging the orthogonal structure of the DTM raster for computational efficiency. Each vertex is defined by a 3D coordinate tuple read from the DTM raster, with the z-coordinate representing the terrain elevation (in meters) and with the xy-coordinates corresponding to the pixel indices in the raster, scaled by the DTM resolution (in meters / pixel). Each face is defined by a triplet of vertex indices forming triangular elements in the mesh. The triangulation processes the raster row by row, identifying valid quadrilateral areas formed by vertices in adjacent pairs of rows, with each area being subsequently divided into two triangles. This process continues until the entire raster has been properly triangulated. By leveraging terrain metadata, including geographic information and resolution, the modified HiRISE importer also creates a UV map for the generated mesh in Blender, allowing for ortho-images to be draped over the companion DTM and used as a high-fidelity terrain texture.
The MARTIAN framework initially imports the Jezero DTM with 10$\%$ of its original resolution, allowing for efficient terrain setup. Then, a material is added to the mesh, with its surface shading model being controlled via a Principled Bidirectional Scattering Distribution Function (BSDF).
% node set in the Blender shading editor. The Principled BSDF integrates multiple material properties, such as diffuse, metallic, subsurface and transmission into a single node, thus allowing to model a wide variety of materials. For the simulated Mars terrain, we set the Principled BSDF with the main parameters shown in Table \ref{tab:principled_bsdf}. 
% \begin{table}
% \centering
% \begin{tabular}{l l l}
% \textbf{Parameter}      & \textbf{Values}            & \textbf{Description} \\ \hline
% Metallic  & 0.0  & Blends between a dielectric \\
% & & and metallic material model from \\
% & & 0 (non-metal) to 1 (fully metallic). \\ 
% \\
% Roughness  & 0.9 & Controls the surface's smoothness;\\
% & & from 0.0 (glossy) to 1.0 (rough). \\
% \\
% IOR & 1.45 & Defines the refractive index, mainly for\\
% & &transparent or refractive materials. \\
% \\
% Alpha  & 0.0  & Controls the transparency of the material. \\ 
%   &   & from 0 (opaque) to 1 (fully transparent) \\ 
% \hline
% \end{tabular}
% \caption{\label{tab:principled_bsdf} Main parameters of the Principled BSDF node in Blender for the synthetic Mars terrain.}
% \end{table}
The 0.25 m / pixel Jezero ortho-image is then loaded in the shading editor in to serve as the base texture for the terrain surface. The ortho-image coordinates data are retrieved by a Texture Coordinate Node to ensure that the texture is properly mapped onto the 3D mesh. Finally, the mesh is reloaded with its full resolution of 1 m / post.
\\
\\
\noindent \textbf{Scene setup and camera modeling.}
% \subsubsection{Scene setup and camera modeling}
% \label{subsubsec:scene_setup}
MARTIAN allows for setting multiple scenes for perspective and orthographic imaging with user-defined camera intrinsics and extrinsics (see Figure~\ref{fig:MARTIAN_view}). Given the \textit{world} reference frame $W$ defined as a East-North-Up coordinate system with origin on the terrain map center, the camera can be located in $W$ by providing the xy-coordinates and the altitude (in meters) with respect to the terrain at those coordinates. To position the camera object above the terrain mesh at the desired altitude, MARTIAN adopts a ray-tracing approach by using a Bounding Volume Hierarchy tree, a data structure used by Blender to efficiently organize geometric objects in 3D space. The camera frame $C$ is centered at the camera's optical center, with its X-axis pointing to the right along the image width, the Z-axis pointing towards the terrain, and the Y-axis completing the orthogonal set. The attitude of the the $C$ frame with respect to the world frame $W$ can be specified as input, and the final pose ($\mathbf{R}_{WC} \vert \mathbf{t}_{WC}$) is saved, where $\mathbf{t}_{WC}$ is the location of the camera in world coordinated and $\mathbf{R}_{WC}$ is the rotation matrix that aligns $W$ to $C$.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Figures/MARTIAN_view_resized.png}  \caption{\label{fig:MARTIAN_view}View of the Jezero Crater's DTM in MARTIAN.}
\end{figure}
\\
\\
\noindent \textbf{Lighting.}
% \subsubsection{Lighting}
% \label{subsubsec:lighting}
MARTIAN provides the capability to render scenes in Blender with various illumination conditions by tuning Sun light source parameters such as the irradiance, the apparent disk diameter and orientation in the map frame. Higher irradiance values cast brighter illumination and shadows in the scene, while the angular diameter of the Sun disk as seen by the scene controls the softness versus harshness of the shadows. The Sun orientation simulates the scene time of the day and it is specified by the user though Sun elevation (EL) and azimuth angles (AZ) as shown in Figure \ref{fig:MARTIAN_view}. The lighting computations are performed using the Blender Cycles engine. This is a physically-based rendering engine that uses a ray tracing algorithm to accurately simulate light behavior. 
%Light rays are traced from the camera into the scene, bouncing around until they find the simulated Sun light source or the world background, following the surface BSDF. 
In this work, we set Sun irradiance to the maximum value of 590 W/m$^2$ and the angular diameter to 0.35°, coherently with actual estimations for Mars.
We compared empirically the visual appearance of the real ortho-projected map to the MARTIAN rendered one, and tuned the BSDF and camera exposure such that they match visually. Figure \ref{fig:map_tile_w_lighting_var} illustrates the effect of different combinations of sun azimuth and elevation angles on the visual appearance of an orthographic map tile rendered in MARTIAN.

% UNCOMMENT
\begin{figure}
    \centering
    % Row with azimuth values (just above the first row of images)
    \makebox[0.01\linewidth]{} % Empty space for EL labels
    \makebox[0.20\linewidth]{\small 0° AZ}
    \makebox[0.20\linewidth]{\small 90° AZ}
    \makebox[0.20\linewidth]{\small 180° AZ}
    \makebox[0.20\linewidth]{\small 270° AZ}
    \vspace{0.1cm} % Space between azimuth labels and images

    % First row of images with EL label
    \begin{minipage}[b]{0.01\linewidth}
        \raisebox{2em}[0pt][0pt]{\makebox[0pt][r]{\small 2° EL}} % Adjust the raise value as needed
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/2EL_0AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/2EL_90AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/2EL_180AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/2EL_270AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}

    \vspace{0.1cm} % Space between rows

    % Second row of images with EL label
    \begin{minipage}[b]{0.01\linewidth}
        \raisebox{2em}[0pt][0pt]{\makebox[0pt][r]{\small 5° EL}} % Adjust the raise value as needed
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/5EL_0AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/5EL_90AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/5EL_180AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/5EL_270AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}

    \vspace{0.1cm} % Space between rows

    % Third row of images with EL label
    \begin{minipage}[b]{0.022\linewidth}
        \raisebox{2em}[0pt][0pt]{\makebox[0pt][r]{\small 10° EL}} % Adjust the raise value as needed
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/10EL_0AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/10EL_90AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/10EL_180AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \begin{minipage}[b]{0.19\linewidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/10EL_270AZ_OrthoCam_0000_tile_3_11_resized2.png}
    \end{minipage}
    \caption{\label{fig:map_tile_w_lighting_var} Gray scale images of a map tile from the Jezero crater site, rendered with different combinations of sun AZ and EL. Generated with MARTIAN.}
\end{figure}
%The Cycles engine, the BSDF and the simulated camera exposure have been concurrently and empirically tuned for a rendered orthographic view of the terrain to qualitatively match the visual appearance of the real ortho-projected map under the same lighting. %(\ref{fig:jezero-hirise-ortho}).
% Cycles allows for the controlling further properties such as the maximum number of light bounces and the number of bounces for the different ray types.
% A list of the main light objects and cycles parameters used in the MARTIAN framework is shown in Table \ref{tab:cycles_settings}.
% \begin{table}
% \centering
% \begin{tabular}{l l}
% \textbf{Parameter} & \textbf{Value}\\ \hline 
% Sun irradiance & 580 W/m$^2$ \\ 
% Disk angular diameter & 0.35$^{\circ}$  \\ 
% Samples & 16  \\
% Max Bounces  & 100 \\
% Min Bounces & 10 \\  
% Diffuse Bounces & 10  \\  
% Glossy Bounces & 4 \\  
% \hline
% \end{tabular}
% \caption{\label{tab:cycles_settings} Sun object Cycles Rendering Engine settings in MARTIAN.}
% \end{table}
%\\
%\\
%\noindent \textbf{Rendering.}
% \subsubsection{Rendering}
% \label{subsubsec:rendering}
%Gray-scale and depth images can be rendered in with customized settings for each individual scene. For each camera view, users can specify image pixel resolution, pixel aspect ratio, and exposure, allowing scenes to be rendered with distinct visual properties while utilizing the same underlying terrain. The simulated camera exposure can be adjusted for a fine-tuning of the overall brightness level in the rendered image. Depth images from perspective and orthographic cameras are rendered in Blender by using a depth buffer. This consists of a data structure that during the rasterization records the component, along the camera's viewing direction, of the distance from an object in the scene to the camera at each pixel of the rendered image. The resulting image is a depth map of the observed scene, with depth values expressed in meters.

\subsection{Generating a training set with MARTIAN}
\label{subsec:training_set}
We generated a training image dataset comprising 17 orthographic gray-scale maps rendered from combinations of Sun azimuth ($0^{\circ}-360^{\circ}, 45^{\circ} steps$) and elevation $(30^{\circ}, 60^{\circ}, 90^{\circ}$), one corresponding orthographic depth map, and 4500 nadir-pointing aerial observations at fixed Sun angles AZ=180$^{\circ}$ and EL=40$^{\circ}$ along with their corresponding depth images. The query observations were randomly sampled from the HiRISE Jezero Crater's DTM with uniform distribution in the (x,y) coordinates in the world frame and within altitude range [$64, 200$] m. The query camera intrinsics are given by a pinhole camera model characterized by a sensor width of 80 mm, a focal length of 32 mm, 0 lens shifts along the image width and height axes, and a unitary pixel aspect ratio. Camera extrinsics, along with altitude data, were stored for each observation and used as ground truth. Further details are reported in Table \ref{tab:dataset_params}.

Training examples for Geo-LoFTR are created by forming triplets ($I_A$, $I_B$, $I_C$) out of query observations and map windows crops (gray-scale and depth images) for multiple combinations of Sun azimuth and elevation angles' offsets between queries and the source maps. For a given combination of query and map lighting, each query observation is paired with map windows with at least 25\% area overlap on the terrain. Therefore, the set of triplets is formed by different combination of image $I_A$ with the mop window tuple ($I_B$, $I_C$). The map window sizes are carefully chosen to introduce an appropriate level of scale variance within the altitude range of the observations, ensuring a balance between model generalization and training efficiency. 
% By choosing prefixed map windows size of 1320 $\times$ 990 pixels for observations within the [64, 132] m altitude range, and a size of 2000 $\times$ 1500 pixels for the (132, 200] m, the scale ratio between query images and map windows is ensured to vary between 1 and 3.
Geo-LoFTR has been fine-tuned from the original LoFTR pre-trained model on a total 
of 
%150705 
150K
generated triplets. An independent validation set of 3400 triplets has been used to regularly assess the model performance during training and prevent over-fitting. 




\begin{table}
\centering
% Review this table
\begin{tabular}{l l l}
\textbf{Parameters} & \textbf{Maps} & \textbf{Observations} \\ \hline 
N.o. gray images & 17 & 4500 \\
N.o. depth images & 1 & 4500 \\
Image size (H $\times$ W) & 26949 $\times$ 57613  & 480 $\times$ 640  \\ 
Pixel resolution & 0.25 m / px  &  [0.25, 0.78] m / px \\
Projection type  & Orthographic & Perspective \\  
Orthographic scale & 6737 m & / \\
Focal length & / & 32 mm \\
Sensor width & / & 80 mm \\
Location in $W$ & (0,0) m &  uniform random \\
&  &  distribution \\
Orientation in $W$ & nadir-pointing & nadir-pointing \\
Altitude & 4000 m & [64, 200] m\\
Sun AZ   & [0, 360]$^{\circ}$ with 45$^{\circ}$ steps & 180$^{\circ}$ \\  
Sun EL & 30$^{\circ}$, 60$^{\circ}$, 90$^{\circ}$ & 40$^{\circ}$\\  
\end{tabular}
\caption{\label{tab:dataset_params} Parameters for the maps and observations image dataset sourced for the generation of the pairs formed by query observations and map windows, used for LoFTR and Geo-LoFTR training.}
\end{table}



\subsection{Map-based Localization Pipeline}
\label{subsec:mbl_pipeline}
The goal of map-based localization is to retrieve the onboard (query) camera pose $(\mathbf{R}_{WC_{\text{query}}}, \vert \mathbf{t}_{WC_{\text{query}}})$ in the world frame $W$, where $\mathbf{R}_{WC_{\text{query}}}$ is the rotation matrix that aligns $W$ with the camera frame $C_{query}$, and $\mathbf{t}_{WC_{\text{query}}}$ is the camera location in $W$. This is preceded by the registration of the query observation captured by a camera with known intrinsic parameters over a geo-referenced ortho-projected map.
%The function of an MbL pipeline consists of registering a query observation captured by a camera with known intrinsic parameters over a geo-referenced ortho-projected map. The ultimate goal is to retrieve the query camera pose $(\mathbf{R}_{WC_{\text{query}}}, \vert \mathbf{t}_{WC_{\text{query}}})$ in the world frame $W$, where $\mathbf{R}_{WC_{\text{query}}}$ is the rotation matrix that aligns $W$ with the camera frame $C_{query}$ and $\mathbf{t}_{WC_{\text{query}}}$ is the camera location in $W$. 

We assume that a hypothetical future Mars rotocraft is going to be equipped with onboard Visual Odometry (VIO) such that it would provide a noisy pose prior to the MbL pipeline. This allows to narrow down the registration of the query image to a smaller search area of the reference map based on the uncertainty of the VIO pose. In our work we assume a large predetermined search area of 1 km$^2$ centered at the query observation for two reasons. First, it allows us to simulate a conservative scenario with a high-uncertainty pose prior. Second, it prevents our evaluation strategy from being overly influenced by variations in the poses themselves, thus ensuring a more consistent assessment of our pipeline's performance across experiments. We note that the overall size of the map is 6.737 km by 14.403 km with a resolution of 0.25 m / pixel.

%A search area on the map can be identified based on pose prior uncertainty available from the onboard VIO estimator \cite{brockers2022}. In our method, the test observations are registered against a 1 km$^2$ search area cropped from 0.25 m / px maps covering a 6.737 km by 14.403 km region (Section \ref{subsec:hirise}), and centered on the query location. The choice of a relatively large predetermined search area serves two purposes. First, it allows us to simulate a conservative scenario with a high-uncertainty pose prior. Second, it prevents our evaluation strategy from being overly influenced by variations in the poses themselves, thus ensuring a more consistent assessment of our pipeline's performance across experiments. 

For each query, the map search area is further divided into multiple overlapping windows, each sized at $1024 \times 768$ pixels and with an overlap of 10\%. The query image $I^A$ is paired with each map window crop $I^B$ and the corresponding depth image crop $I^C$. The formed triplet $(I^A, I^B, I^C)$ is processed by Geo-LoFTR, which outputs matched keypoints on both the map windows and the query observations with their confidence scores. The top 500 matches are retained for each map window, with further filtering applied across the entire search area to include only matches meeting a 95\% confidence threshold. Given a simulated orthographic map camera with pose $(\mathbf{R}_{WC_{\text{map}}}, \vert \mathbf{t}_{WC_{\text{map}}})$ in the world frame, each matched  point of pixel coordinates $(u_{\text{map}}, v_{\text{map}})$ in the map image plane is back-projected to its 3D location in $W$ using an inverse orthographic projection:
\begin{equation*}
     \begin{bmatrix} X^W \\ Y^W \\ Z^W \end{bmatrix} = p_{\text{map}} \mathbf{R}_{WC_{\text{map}}} \, \begin{bmatrix} 
                    1 & 0 & -c_{x,{\text{map}}} \\
                    0 & 1 &  - c_{y,{\text{map}}}  \\
                    0 & 0 &Z/p_{\text{map}}  \end{bmatrix} \begin{bmatrix} u_{\text{map}} \\ v_{\text{map}} \\ 1
                    \end{bmatrix} + \mathbf{t}_{WC_{\text{map}}}
    \label{eq:backproj_map_pts}
\end{equation*}
where $Z$ is the depth of the map keypoint,  $(c_{x,{\text{map}}}, c_{y,{\text{map}}})$ is the optical center and $p_{\text{map}}$ is the pixel resolution. 
%\GG{do we say that the map camera is orthographic?}
The 2D matched points on the query images, their 3D correspondences, and the query camera intrinsics matrix $\mathbf{K}$ are then fed to a RANSAC-PnP algorithm to solve the perspective projection problem 
%(\ref{eq:persp_problem}) 
and retrieve the estimated pose $(\widetilde{\mathbf{R}}_{WC_{\text{query}}}, \vert \widetilde{\mathbf{t}}_{WC_{\text{query}}})$.
% with a pixel reprojection error threshold of 1.5 pixels and 1000 iterations.
% \begin{equation}
%     Z^C \begin{bmatrix}
%         u_{\text{query}} \\ v_{\text{query}}
%     \end{bmatrix} = \mathbf{K} \begin{bmatrix}
%         \mathbf{R}_{C_{\text{query}}W} \vert \mathbf{t}_{C_{\text{query}}W}
%     \end{bmatrix} \begin{bmatrix} X^W \\ Y^W \\ Z^W \\ 1 \end{bmatrix}
%     \label{eq:persp_problem}
% \end{equation}
% where $Z^C$ is the depth value of the 2D point computed from the query camera focal plane. 
%%and \textbf{K} is the intrinsic matrix for the query camera model.
%%assumed to be of a pin-hole type with no lense shift nor distortion. 
%\GG{probably we don't need to add the perspective projection problem equation}

% \begin{equation}
%     \mathbf{K} = \begin{bmatrix} f_x & 0 & c_{x,{\text{query}}} \\ 0 & f_y & c_{y,{\text{query}}} \\ 0 & 0 & 1 \end{bmatrix}
%     \label{eq:intricinsics_matrix} 
% \end{equation}

% The localization error is simply given by $\|\mathbf{t}_{WC_{\text{query}}} - \widetilde{\mathbf{t}}_{WC_{\text{query}}}\|$.

% We based the size of the search area on the point reprojection uncertainty $\sum_{u,v}$ for the query image corners when reprojected on the map. Given the query and map camera poses $(\mathbf{R}_{WC_{\text{query}}}, \vert \mathbf{t}_{WC_{\text{query}}})$, $(\mathbf{R}_{WC_{\text{map}}}, \vert \mathbf{t}_{WC_{\text{map}}})$ in the world frame, and the pixel coordinate $(u_{\text{query}}, v_{\text{query}})$ of a 2D point $P$ on the query mage, the transformation $f$ that projects this point to map at pixel coordinates $(u, v)$ is composed of the following subsequent transformations:
% \begin{itemize}
%     \item backprojection of P from the query image plane to 3D coordinates, $\mathbf{x}^{C_{\text{query}}}$, in the query camera location frame $C_{\text{query}}$:
%         \begin{equation*}
%             \mathbf{x}^{C_{\text{query}}} = Z \cdot \mathbf{K}^{-1} \begin{bmatrix} u_{\text{query}} \\ v_{\text{query}} \\ 1 \end{bmatrix}
%         \end{equation*}
%     where $Z$ is the depth value (i.e., the z-coordinate) obtained from the query depth image at $(u_{\text{query}}, v_{\text{query}}$, and $\mathbf{K}$ is the camera intrinsic matrix.
%     \item Transformation from $C_{\text{query}}$ to the position, $\mathbf{x}^W$, of the 3D point in $W$:
%     \begin{equation*}
%         \mathbf{x}^W = \mathbf{R}_{WC_{\text{query}}} \,\mathbf{x}^{C_{\text{query}}} + \mathbf{t}_{WC_{\text{query}}}
%     \end{equation*}
%     \item Transformation from $W$ to the 3D position, $\mathbf{x}^{C_{\text{map}}}$, in the map camera frame $C_{\text{map}}$:
%     \begin{equation*}
%     \mathbf{x}^{C_{\text{map}}} = \mathbf{R}_{WC_{\text{map}}}^T( \mathbf{x}^W - \mathbf{t}_{WC_{\text{map}}})
%     \end{equation*}
%     \item Orthographic projection to the 3D map:
%     \begin{equation*} \begin{bmatrix} u \\ v \end{bmatrix} = \frac{1}{p_{\text{map}}} \mathbf{x}^{C_{\text{map}}}  + \begin{bmatrix} c_{x,{\text{map}}} \\ c_{y,{\text{map}}}   \end{bmatrix}
%     \end{equation*}
%     where $p_{\text{map}}$ is the pixel resolution of the map and $(c_{x,{\text{map}}}, c_{y,{\text{map}}})$ are the coordinates of the optical axis in the map image plane.
% \end{itemize}

% Given the final transformation $f$ is given by:
% \begin{equation}
%     f = \frac{1}{p_{\text{map}}}\mathbf{R}_{WC_{\text{map}}}^T( Z \cdot \mathbf{R}_{WC_{\text{query}}} \mathbf{K}^{-1} \begin{bmatrix} u_{\text{query}} \\ v_{\text{query}}\end{bmatrix} + \mathbf{t}_{WC_{\text{query}}} - \mathbf{t}_{WC_{\text{map}}})
%     \label{eq:queryimg2mapimg}
% \end{equation}

% the point reprojection uncertainty through first-order error propagation is obtained as:

% \begin{equation}
%     \centering
%     \sum_{u,v} = \frac{\delta f}{\delta \eta}\sum_{\eta}\frac{\delta f^T}{\delta \eta}, \; \; \; \; \eta = \{u_{\text{query}}, v_{\text{query}}, \mathbf{t}_{WC_{\text{query}}}, Z, \psi, \theta, \phi\}
%     \label{eq:reproj_uncert}
% \end{equation}

% where $\{\psi, \theta, \phi\}$ is the query camera orientation expressed in yaw, pitch and roll angles, and $\sum_{\eta}$ is set as a diagonal matrix. The prior uncertainty $\sum_{\eta}$ can be derived, for instance, from the covariance of the Range-VIO estimator. For each of the query image corner points $(u_{\text{query}}, v_{\text{query}})$, we compute the uncertainty in its projected location on the map image $(u,v)$. This is quantified along the width and height directions on the map image plane as $3\sigma_u$ and $3\sigma_v$, where $\sigma_u^2$ and $\sigma_v^2$ are the entries of the covariance matrix $\sum_{\eta}$ corresponding to each map projection. The largest uncertainty among all the points is selected as the final search area size.
% After the map windows are sampled over the identified search area with specified width and overlap, each window is resized and eventually padded to match the dimensions of the query image for computational efficiency. In case the selected matching model is Geo-LoFTR the map windows are accompanied by their depth information. For each window/query pair, the selected LoFTR model generates a set of matched keypoints with associated confidence scores, and the top 500 matches are retained. Once all the pairs have been processed, the matches are aggregated and further filtered based on a specified confidence threshold. The filtered keypoints on the map are then projected back to the world frame $W$ by using the known map camera pose and the map pixel resolution. The resulting 3D world points and the matched 2D keypoints on the query image are input to a RANSAC-PnP algorithm to estimate the camera pose $(\mathbf{R}_{WC_{\text{query}}}, \vert \mathbf{t}_{WC_{\text{query}}})$.