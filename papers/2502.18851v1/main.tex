% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)



%%%%% Added packages
\usepackage{inconsolata}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mdframed}

%previous
\usepackage{array}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{url}

\usepackage{authblk} 
\usepackage{hyperref} 

\newcommand{\stone}{\texttt{STONE}}
%algorithm1
% \usepackage{algorithm}
% \usepackage{algpseudocode}
% \usepackage{algorithmic}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{diagbox}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{amsmath}
\DeclareMathOperator*{\argmin}{argmin}

%%%% Packages for background color
\usepackage{xcolor}
\usepackage{newtxtext,newtxmath}
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage{framed}
\mdfsetup{%
linecolor=white,
backgroundcolor=gray!20,
}
%%attack example
\usepackage{booktabs}
\usepackage{listings}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\usepackage{placeins} 
\usepackage{listings}
\usepackage{xcolor}

%%pythoncode
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\let\openbox\undefined
\usepackage{amsthm,amssymb,multirow}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{listings,tikz}
\usepackage[most]{tcolorbox}
\usepackage{mathbbol}
% \tcbuselibrary{minted}
\usepackage{calc}
\usepackage{subcaption}
% \usepackage{comment,minted,xcolor,soul}
\usepackage{comment,xcolor,soul}
\sethlcolor{lightblue}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage{kotex}
\newtcblisting{pythoncode}{
  listing engine=minted,
  breakable,
  listing only,
  nobeforeafter,
  minted style=colorful,
  minted language=python,
  enhanced,
  minted options = {
        breaklines=true, 
        fontsize=\small
        %breakanywhere=true,
        %breakbytoken=true,
        %breakbytokenanywhere=true,
        %breakbefore=.                   
      },
}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code}

\author{
\quad Jungin Kim$^{\star}$
\quad Shinwoo Park$^{\star}$
\quad Yo-Sub Han$^{\dagger}$ \\
Yonsei University, Seoul, Republic of Korea \\
\texttt{\small jungin.kim@yonsei.ac.kr, pshkhh@yonsei.ac.kr, emmous@yonsei.ac.kr} 
}

\newcommand{\correspondingfootnote}{
    \let\oldthefootnote=\thefootnote
    \renewcommand{\thefootnote}{}
    \footnotetext{$\star$ Authors equally contributed.}
    \footnotetext{$\dagger$ Corresponding author.}
    \let\thefootnote=\oldthefootnote
}

\begin{document}

\maketitle

\correspondingfootnote 

\begin{abstract}

Code watermarking identifies AI-generated code by embedding patterns 
into the code during generation. 
Effective watermarking requires meeting two key conditions: 
the watermark should be reliably detectable, 
and the code should retain its original functionality. 
However, existing methods often modify tokens that are critical 
for program logic, such as keywords in conditional expressions or 
operators in arithmetic computations. 
These modifications can cause syntax errors or functional failures, 
limiting the practical use of watermarking.
We present \texttt{STONE}, 
a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. 
By excluding tokens essential for code execution, 
\texttt{STONE} minimizes the risk of functional degradation. 

In addition, we introduce \texttt{CWEM}, a comprehensive evaluation metric
that evaluates 
watermarking techniques based on correctness, 
detectability, and naturalness. 
While correctness and detectability have been widely used, 
naturalness remains underexplored despite its importance. 
Unnatural patterns can reveal the presence of a watermark, 
making it easier for adversaries to remove.
We evaluate \texttt{STONE} using \texttt{CWEM} and compare its performance with the state-of-the-art approach. 
The results show that \texttt{STONE} achieves an average improvement of 7.69\% in \texttt{CWEM} across Python, C++, and Java.
Our code is available in \url{https://github.com/inistory/STONE-watermarking/}.
\end{abstract}

\section{Introduction}

The rapid development of large language models~(LLMs) 
has significantly improved the ability to generate human-like text 
and code~\citep{nam2024using,KazemitabaarHHE23,DBLP:conf/nips/LiuXW023,Sun0Y0024,GuoWSZZCZMZ24,wang2024teaching,DBLP:conf/sigsoft/0003X023,DBLP:conf/kbse/TangGLZXHL23,Guo0XLTFWGB24}. 
These advancements have unlocked new possibilities across various 
fields, including software development and automated code generation. 
However, this progress has introduced challenges in tracing 
the origin of generated code~\citep{asurveyondetection,li2023protecting}. 
As LLM-generated code becomes increasingly indistinguishable 
from human-written code, researchers have explored watermarking 
techniques to address this issue.

LLM watermarking involves embedding specific patterns 
into the generated output during the generation process 
to help identify its 
origin~\citep{kirchenbauer2023watermark,lee-etal-2024-wrote}. 
These patterns should be imperceptible to human reviewers but 
remain detectable through algorithmic analysis, 
enhancing the transparency and accountability of 
AI-generated content~\citep{GuoTSL0024,HouZHWCWSDKT24}.

Among various approaches, the green and red list-based watermarking 
is the most widely adopted~\citep{kirchenbauer2023watermark,zhao2023provable,lee-etal-2024-wrote,lu-etal-2024-entropy,GuoTSL0024}. 
This method divides token candidates into two lists and biases 
the selection process toward tokens in the green list. 
Detection involves measuring the proportion of green list tokens 
in the generated output. 
While effective for natural language tasks, 
this approach does not easily apply to code generation, 
where structural and functional correctness are crucial~\citep{lee-etal-2024-wrote,hoang2024less}. 
Changes to syntax-critical tokens can cause 
compilation errors or alter program behavior.

Code watermarking identifies AI-generated code 
by embedding patterns into the code while maintaining 
functional correctness. 
Effective watermarking requires meeting two 
conditions: reliable watermark detectability and 
intact code correctness. 
However, many existing methods inadvertently modify tokens 
essential to program logic, 
such as keywords in conditional expressions or operators 
in arithmetic computations. These modifications can 
introduce syntax errors or change the behavior of the code, 
undermining its reliability.
We propose \texttt{STONE}~(\textbf{S}yntax \textbf{TO}ke\textbf{N} preserving cod\textbf{E} watermarking), 
a method that selectively inserts watermarks into non-syntax tokens. 
By excluding tokens that are critical to code execution, 
\texttt{STONE} reduces the risk of functional degradation 
while embedding robust, detectable patterns.

Additionally, we introduce \texttt{CWEM}~(Code Watermarking Evaluation Metric), 
a comprehensive framework for evaluating watermarking techniques. 
\texttt{CWEM} assesses performance across three key 
dimensions: correctness, detectability, and naturalness. 
While correctness and detectability have been the focus of 
prior research, naturalness is equally important. 
Watermarking methods that create unnatural code patterns make 
it easier for adversaries to detect and remove the embedded patterns.
We evaluate \texttt{STONE} using \texttt{CWEM} and compare its 
performance with the state-of-the-art approach. 
Our experiments demonstrate that \texttt{STONE} achieves an 
average improvement of 7.69\% in \texttt{CWEM} 
across Python, C++, and Java. 

\section{Preliminary analysis}
\label{sec:preliminary_analysis}

The previous SOTA approach SWEET~\citep{lee-etal-2024-wrote} 
takes into account the structural and syntactic constraints
by embedding watermarks only in high-entropy tokens, 
it ensures a minimal impact on the code quality. 
In our preliminary analysis, we group code tokens into several categories 
and compare their average entropy values to see if high-entropy tokens 
have a smaller impact on code quality. 
We consider two popular benchmarks, MBPP+ and HumanEval+,  
for the
Python code generation and completion. 
We classify each Python code into one of  five categories reported in  Table~\ref{tab:syntax_elements} where any 
remaining tokens are put into the etc.~category. 

We calculate the entropy values of tokens across different categories and 
compare the average entropy values for each category. 
Following~\citet{lee-etal-2024-wrote}, we calculate the Shannon entropy. 
Given a token sequence~\( y = (y_0, y_1, \ldots, y_N) \), 
the entropy at each generation step \( t \) is computed based on the predicted probability distribution of the next token. 
The entropy \( H_t \) is defined as:
\begin{equation}
H_t = - \sum_{i=1}^{|V|} P(y_t = v_i \mid y_{<t}) \log P(y_t = v_i \mid y_{<t}),
\end{equation}
where \( H_t \) is the entropy value at generation step \( t \), 
\( V \) is the vocabulary of the language model, 
\( y_{<t} \) is the sequence of tokens generated before step \( t \), and 
\( P(y_t = v_i \mid y_{<t}) \) is the probability assigned by the model 
to token \( v_i \) as the next token, respectively. 
The entropy \( H_t \) measures the uncertainty of the next token prediction. 
A high entropy value indicates that the model is uncertain and 
many tokens have comparable probabilities. 
Conversely, low entropy implies that one token has a dominant probability.

\begin{figure}[hbt!]
    \centering
        \includegraphics[width=\columnwidth]{figure/preliminary_analysis.pdf}
    \caption{
    Entropy values by token types.
    We refer to tokens other than keywords, whitespaces, types, delimiters, 
    and operators as etc tokens. 
    }\label{fig:preliminary_analysis}
\end{figure} 

Figure~\ref{fig:preliminary_analysis} presents bar charts displaying 
the mean entropy values for each token category. 
Our analysis shows that the average entropy of keyword tokens is the 
highest among all categories. 
In Python code, keywords are used in various contexts and positions. 
For example, control flow keywords like \texttt{if}, \texttt{for}, and 
\texttt{while} can appear in different parts of the code with highly 
diverse usage patterns. 
In contrast, operators and delimiters follow more predictable patterns 
due to the syntactic structure of the code. 
For instance, operators such as \texttt{+}, \texttt{-}, and \texttt{*} are 
used between operands, while delimiters like \texttt{(} and \texttt{)} 
appear in specific patterns, such as in function or class definitions. 
Additionally, type tokens are mainly used in limited contexts, 
such as variable declarations or type hinting, 
making their occurrence and patterns relatively predictable. 
Whitespace tokens, following the structure of the code, 
are even more predictable in their placement.

Since keywords have high entropy, embedding watermarks in high-entropy 
tokens, as in SWEET, carries the risk of placing watermarks on keywords. 
Keywords are essential to the syntax and logic of the code, thus watermarking 
them could negatively impact the quality and functionality of the code. 
In comparison, the etc category, while less critical to the 
functionality of the code, has relatively high entropy. 
Therefore, embedding watermarks in 
etc tokens may be a more suitable 
approach for preserving code quality. 

\begin{figure*}[hbt!]
    \centering
        \includegraphics[width=14cm]{figure/motivation.pdf}
    \caption{A motivation example comparing SWEET~\citep{lee-etal-2024-wrote} and 
    \texttt{STONE}~(ours). 
    (a) An example prompt for code generation,  
    (b) Solution code without any watermark, 
    (c) Output code watermarked by SWEET, and 
    (d) Output code watermarked by \texttt{STONE}. 
    In this example, SWEET accidentally replaces \texttt{\%}~(modulus) with 
    \texttt{$//$}~(integer division) when checking if a number is even, 
    leading to incorrect logic. 
    }\label{fig:motivation}
\end{figure*} 

\section{Approach: \stone}

\stone~watermarking is a robust watermarking approach designed for preserving code quality illustrated in Figure~\ref{fig:motivation}.
This section introduces the watermark insertion and detection process of our proposed method. 
In Section~\ref{sec:inserting_watermark}, we detail how watermarks are inserted by selectively modifying non-syntactic tokens while keeping critical elements like keywords, operators, and delimiters unchanged to preserve functionality and structure. In Section~\ref{sec:detecting_watermark}, we explain how the watermark is detected by analyzing the green token distribution among non-syntax elements and computing a statistical $z$-score to confirm the presence of the watermark.

\subsection{Inserting Watermarks} \label{sec:inserting_watermark}   
\begin{algorithm}[hbt!]\small
\caption{Generation Algorithm of \texttt{STONE}}\label{alg:zero_bit_generation}
\begin{algorithmic}[1]
\Require Tokenized prompt $\mathbf{x}$,  
         previously generated tokens $\mathbf{y}_{:t} = (y_0, \dots, y_{t-1})$, 
         vocabulary set $\mathcal{V}$, 
         language model $f_{\text{LM}}$, 
         syntax element set $S$,
         green list ratio $\gamma \in (0, 1)$, 
         logit adjustment factor $\delta > 0$
\For{$t = 0, 1, 2, \dots$} 
    \State Compute the logit vector: 
    % \[
    $\mathbf{l}_t = f_{\text{LM}}(\mathbf{x}, \mathbf{y}_{:t})$
    % \]
    \State Compute the initial probability vector $\mathbf{p}_t$:
    \[
    p_{t,i} = \frac{e^{l_t[i]}}{\sum_{j=1}^{|\mathcal{V}|} e^{l_t[j]}}, 
    \quad \text{for } i \in \{0, \dots, |\mathcal{V}|-1\}
    \]
    \State Sample a candidate token $\tilde{y}_t \sim \mathbf{p}_t$ 
    \If{$\tilde{y}_t \notin S$} 
        \State Compute a hash of token $y_{t-1}$ 
        \State Randomly divide $\mathcal{V}$ into $\mathcal{G}_t$ and $\mathcal{R}_t$: 
        \[
        \mathcal{G}_t \quad (\text{green list of size } \gamma |\mathcal{V}|)
        \]
        \[
        \mathcal{R}_t \quad (\text{red list of size } (1 - \gamma) |\mathcal{V}|)
        \]
        \State Add $\delta$ to the logits of tokens in $\mathcal{G}_t$: 
        \[
        l_t[i] \leftarrow l_t[i] + \delta, \quad \text{for } i \in \mathcal{G}_t
        \]
        \State Recompute the adjusted probability vector $\mathbf{p}'_t$: 
        \[
        p'_{t,i} = \frac{e^{l_t[i]}}{\sum_{j=1}^{|\mathcal{V}|} e^{l_t[j]}}, 
        \quad \text{for } i \in \{0, \dots, |\mathcal{V}|-1\}
        \]
    \Else
        \State Set $\mathbf{p}'_t = \mathbf{p}_t$ 
    \EndIf
    \State Sample the final token $y_t \sim \mathbf{p}'_t$ 
\EndFor
\end{algorithmic}
\end{algorithm}
We define the keywords, whitespaces, types, delimiters, and operators 
presented in Table~\ref{tab:syntax_elements} as functionally 
important syntactic elements and embed watermarks only in tokens that 
do not belong to these categories. 
Algorithm~\ref{alg:zero_bit_generation} describes how \texttt{STONE} 
embeds watermarks during code generation.
Given a tokenized prompt $\mathbf{x}$ 
and $\mathbf{y}_{:t}$, the tokens generated up to the previous 
time step, the language model $f_\text{LM}$ 
computes the initial probability distribution $\mathbf{p}_t$
for the token to be generated at the current time step $t$.
\texttt{STONE} samples a candidate token $\tilde{y}_t$ 
based on this initial probability distribution. 
If the sampled token is not in the syntax element set, 
\texttt{STONE} divides the vocabulary $\mathcal{V}$ into a green list 
and a red list according to the green token ratio $\gamma$. 
It then increases the likelihood of sampling tokens from the 
green list by adding a constant $\delta$ to their logit values. 
Using the adjusted logits, 
\texttt{STONE} recalculates the probability 
distribution and samples the final token $y_t$ for time step $t$.
Since \texttt{STONE} embeds watermarks while preserving 
functionally important tokens, 
it is able to maintain the quality of the code.

\subsection{Detecting Watermarks}
\label{sec:detecting_watermark}
\begin{algorithm}[hbt!]\small 
\caption{Detection Algorithm of \texttt{STONE}}\label{alg:zero_bit_detection}
\begin{algorithmic}[1]
\Require Token sequence to be tested $\mathbf{X} = (X_0, \dots, X_{N-1})$, 
         syntax element set $S$,
         green list ratio $\gamma \in (0, 1)$, 
         $z$-score threshold $z_{\text{threshold}} > 0$
\State Set $N^E \gets 0$, $N^E_G \gets 0$
\For{$t = 0, 1, \dots, N - 1$}
    \If{$X_t \notin S$} \Comment{Non-syntax element detected}
        \State $N^E \gets N^E + 1$
        \State Compute a hash of token $X_{t-1}$ to use as a seed
        \State Split vocabulary $\mathcal{V}$ into $\mathcal{G}_t$ and $\mathcal{R}_t$:
        \[
        \mathcal{G}_t \quad (\text{green list of size } \gamma |\mathcal{V}|)
        \]
        \[
        \mathcal{R}_t \quad (\text{red list of size } (1 - \gamma) |\mathcal{V}|)
        \]
        \If{$X_t \in \mathcal{G}_t$}
            \State 
            $N^E_G \gets N^E_G + 1$ \Comment{Token matches the green list}
        \EndIf
    \EndIf
\EndFor
\State \text{Compute the $z$-score: } $z = \frac{N^E_G - \gamma N^E}{\sqrt{\gamma (1 - \gamma) N^E}}$
\If{$z > z_{\text{threshold}}$}
    \State \Return True \Comment{The sequence $\mathbf{X}$ is watermarked}
\Else
    \State \Return False \Comment{The sequence $\mathbf{X}$ is not watermarked}
\EndIf
\end{algorithmic}
\end{algorithm}

We assess how many green tokens are present in the code, 
and if this value surpasses a certain threshold, 
we conclude that the code was generated by an LLM. 
When identifying potential green tokens, we focus only on 
non-syntactic elements~(i.e., etc tokens). 
Algorithm~\ref{alg:zero_bit_detection} 
details the watermark detection process for 
\texttt{STONE} scenario.
$N^E$ represents the number of etc tokens in the code, 
and $N^E_G$ denotes the number of green tokens among them. 
We replicate the division of the vocabulary into green and red lists 
at each time step during code generation to determine whether 
a specific token is a green token.
If the $z$-score exceeds the predefined threshold $z_{\text{threshold}}$, 
the code is determined to be generated by an LLM. 
The purpose of embedding and detecting watermarks 
is to determine whether a given piece of code was generated by an LLM. 


\section{Code Watermarking Evaluation Metric}
\label{sec:cwem}
Existing code watermarking research
lacks a unified evaluation metric 
that simultaneously considers  
functional correctness,
detectability
and naturalness~\citep{lee-etal-2024-wrote, yang2024srcmarker,li2024resilient,guan2024codeip}. 
They have typically focused one or two of these properties, 
leading to incomplete assessments 
and conflicting interpretations.
By integrating functional correctness, detectability, and naturalness, 
we propose \texttt{CWEM}~(Code Watermarking Evaluation Metric), 
a flexible metric that balances each factor using weights $\alpha$,$\beta$ and $\gamma$ 
satisfying $\alpha$ + $\beta$ + $\gamma$ = 1.
In practice, these coefficients reflect the relative importance of functional correctness~($\alpha$ ), watermark detectability~($\beta$)
and fluency or naturalness~($\gamma$). 
For instance, systems that cannot risk functional errors can assign a larger 
$\alpha$, while environments prioritizing watermark detection may emphasize $\beta$.
\begin{equation}
\small
\begin{aligned}
\text{CWEM} = &\; \alpha \cdot \mathrm{Correctness}(C_{wm}) 
+ \beta \cdot \mathrm{Detectability}(C_{wm}) \\
&+ \gamma \cdot \mathrm{Naturalness}(C_{wm}, C),
\end{aligned}
\end{equation}
correctness is measured through the pass@$k$ change before and after watermarking~(Section~\ref{ssec:Correctness}), 
detectability is quantified using $z$-score and AUROC metric~(Section~\ref{ssec:Detectability})
and naturalness is assessed using perplexity~(Section~\ref{ssec:naturalness}). 

\subsection{Correctness}\label{ssec:Correctness}
Ensuring correctness is crucial to prevent watermarking from compromising the functionality of the code.
We measure functionality preservation using the pass@k metric. 
Specifically, we adopt the unbiased version of pass@$k$~\citep{chen2021evaluating}.  
In this approach, $k$ samples per problem are generated, and the number of correct samples, $c$,  
is counted as the number of watermarked code samples in the set ${C_{wm}}$ that pass all unit tests.  

This metric estimates \text{pass@$k$} with the formula:

\begin{equation} \small
    \mathrm{Correctness}(C_{wm}) = \mathbb{E}_{C_{wm}}\left[ 1 - \frac{\binom{n - c}{k}}{\binom{n}{k}} \right],
\end{equation}

where \( \mathbb{E}_{C_{wm}} \) denotes the expectation over the set of watermarked code samples \( C_{wm} \), 
\( n \) is the total number of generated code samples for a given problem, 
\( c \) is the number of correct code samples within the set, 
\( \binom{n}{k} \) denotes the number of ways to choose \( k \) code samples from the total \( n \) samples, 
and \( \binom{n - c}{k} \) represents the number of ways to choose \( k \) code samples from the incorrect samples only (i.e., excluding the \( c \) correct samples).
The term \( 1 - \frac{\binom{n - c}{k}}{\binom{n}{k}} \) calculates the probability that at least one correct code sample is selected when \( k \) samples are drawn from the total \( n \) samples. The overall correctness metric is obtained as the expected value of this probability over the set \( C_{wm} \).

\subsection{Detectability}\label{ssec:Detectability}

Detectability measures how effectively we can identify the presence of a watermark in generated code.  
We assess the detectability by computing a $z$-score based on the distribution of green-list tokens in the generated text.
The classification performance is then evaluated using AUROC~(Area Under the Receiver Operating Characteristic Curve).

\paragraph{$Z$-score: Green Token Ratio}
For a given code token sequence \( C_{wm} = \{c_0, c_1, ..., c_T\} \), 
the $z$-score is computed as:
\begin{equation}
\small
z(C_{wm}) = \frac{|C_{wm}|_G - \gamma T}{\sqrt{T \gamma (1 - \gamma)}},
\end{equation}
where \( |C_{wm}|_G \) is the number of green-list tokens in 
the sequence \( C_{wm} \),
\( T \) is the total number of tokens in the sequence, 
and
\( \gamma \) is the expected proportion of green tokens in 
a non-watermarked sequence.
The denominator represents the standard deviation under the binomial distribution assumption.
This $z$-score quantifies the deviation of the observed green token frequency from its expected value, 
enabling the detection of watermarked text.

\paragraph{LLM-Generated Code Detection Using Green Token Ratio}
We evaluate classification performance using AUROC~(Area Under the Receiver Operating Characteristic Curve)
, which quantifies how well we can distinguish between human-written code  
$C_{H}$ and watermarked code $C_{wm}$ based on their $z$-scores.

We treat $z(C_H)$ as scores for the negative class~(human-written) and 
$z(C_{wm})$ as scores for the positive class~(watermarked).
The ROC curve is obtained by varying a decision threshold $\tau$ computing the corresponding True Positive Rate~(TPR) and False Positive Rate~(FPR).
TPR is the fraction of watermarked sequences correctly identified as watermarked. 
\begin{equation} 
\text{TPR}(\tau) = \frac{\sum_{j} \mathbf{1}(z(C_{wm}^{(j)}) > \tau)}{|{C_{wm}}|}, 
\end{equation}
FPR is the fraction of human-written sequences incorrectly classified as watermarked. 
\begin{equation} 
\text{FPR}(\tau) = \frac{\sum_{i} \mathbf{1}(z(C_H^{(i)}) > \tau)}{|{C_H}|},
\end{equation}


For each threshold $\tau$, a sequence is classified as watermarked if its computed $z$-score exceeds $\tau$.  
We define overall detectability across all possible thresholds as follows:
\begin{equation}\small
\mathrm{Detectability}(C_{wm}, C_H) = \int_{-\infty}^{\infty} \text{TPR}(\tau) \frac{d\text{FPR}(\tau)}{d\tau} d\tau,
\end{equation}
This integral computes the AUROC, which quantifies the ability of the watermarking method to differentiate between human-written and watermarked code.  
A higher AUROC value (closer to 1) indicates stronger detectability,  
whereas an AUROC of 0.5 implies that the distributions of human-written and watermarked code are indistinguishable.

\subsection{Naturalness}\label{ssec:naturalness}
A watermark that significantly alters token distributions can be identified and removed through pre-processing.
We adopt perplexity as a direct measure of token-level fluency, which aligns closely with how language models predict code tokens.
Other code-style metrics exist, but we rely on perplexity because it has been widely recognized in language modeling research as a reliable indicator of naturalness.
A well-designed watermark minimizes perplexity shifts, ensuring that watermarked code remains indistinguishable from non-watermarked code.

For a given set of watermarked code sequences $C_{wm}$, perplexity is computed as:

\begin{equation} \small 
\text{PPL}(C_{wm}) = \frac{1}{|C_{wm}|} \sum_{j=1}^{|C_{wm}|} \exp \left( -\frac{1}{N_j} \sum_{i=1}^{N_j} \log P(w_i^{(j)} | w_{<i}^{(j)}) \right), \end{equation}

where $N_j$ is the number of tokens in the j-th sample of $C_{wm}$, 
and $P(w_i^{(j)}|w_{<i}^{(j)})$ represents the probability of token $w_i^{(j)}$ given its preceding context. 
% The denominator corresponds to the standard deviation assuming a binomial distribution.
An effectively concealed watermark minimize perplexity shifts, ensuring imperceptibility.
We define the naturalness metric as:
\begin{equation} \small
    \mathrm{Naturalness}(C_{wm}, C) = 1 - \frac{|\text{PPL}(C_{wm}) - \text{PPL}(C)|}{\text{PPL}(C)},
\end{equation}

This metric quantifies the perplexity-based difference between LLM-generated code with \(C_{wm}\) and without \(C\) a watermark.
The metric normalizes the absolute perplexity difference by dividing it by the perplexity of non-watermarked code \(C\), ensuring invariance.
A value of 1 indicates that \(C\) and \(C_{wm}\) have identical perplexities, implying minimal divergence despite watermark insertion.
Conversely, a value close to 0 suggests substantial perplexity divergence, 
indicating a significant impact of watermark insertion on code generation.
By scaling the perplexity difference between 0 and 1, this metric provides a clear and interpretable measure of similarity.

Based on these three criteria, 
we propose \texttt{CWEM}, a comprehensive evaluation metric for assessing code watermarking effectiveness.
Our metric not only enables comparison across different watermarking methods 
but also offers flexibility through its weighted structure 
to adjust varying requirements in different application contexts.

\section{Results and Analysis}
\subsection{Experiment Settings}
\paragraph{Datasets}
\label{sec:experiment_setup_datasets}
For our main experiments, we use the HumanEval+ and MBPP+ datasets~\citep{evalplus} 
for Python code completion and generation tasks, 
following prior work~\citep{lee-etal-2024-wrote}. 
In addition, we incorporate the HumanEvalPack~\citep{MuennighoffLZZH24} 
to evaluate the performance of our approach across three programming languages: Python, C++, Java. 
Table~\ref{tab:data_statistics} shows the data statistics of the datasets we used in our experiments.
\begin{table}[H]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lccc}
    \noalign{\hrule height 0.8pt}
        \textbf{Datasets} & \textbf{Problems} & \textbf{Test Cases} & \textbf{Avg Tokens}  \\
        \hline
        MBPP+ & 399 & 105 & 87\\
        HumanEval+ & 164 & 764 & 57\\
        HumanEvalPack-C++ & 164 & 764 & 100\\
        HumanEvalPack-Java& 164 & 764 & 97\\
        \noalign{\hrule height 0.8pt}
    \end{tabular}%
    }
    \caption{Data statistics of datasets, including the number of problems and average test cases per problem and average tokens of human-written solution codes.}
    \label{tab:data_statistics}
\end{table}


\paragraph{Baselines}
We consider the following three baselines for comparison.
(1) KGW~\citep{kirchenbauer2023watermark} generates text by dividing the vocabulary into a green and red list 
at each time step of token generation, 
increasing the probability of generating tokens from the green list. 
(2) EWD~\citep{lu-etal-2024-entropy} leverages entropy-based token weighting during detection, 
giving more influence to high-entropy tokens, 
thus enhancing detection in texts with varying entropy distributions.
(3) SWEET~\citep{lee-etal-2024-wrote} selectively embeds watermarks in high-entropy tokens, 
addressing detection challenges posed by the low entropy in code.
We exclude CodeIP from our baselines as it requires additional training for watermark insertion, 
while our approach and the selected baselines are training-free methods.

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{clcccccccc}
\noalign{\hrule height 0.8pt}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{Method}} & 
        \multicolumn{4}{c}{MBPP+} & 
        \multicolumn{4}{c}{HumanEval+} \\ 
        \cmidrule(lr){3-6} \cmidrule(lr){7-10} 
        \multicolumn{2}{c}{} & 
        \multicolumn{1}{c}{Correctness} & \multicolumn{1}{c}{Detectability} & \multicolumn{1}{c}{Naturalness} & \multicolumn{1}{c}{\texttt{CWEM}} & 
        \multicolumn{1}{c}{Correctness} & \multicolumn{1}{c}{Detectability} & \multicolumn{1}{c}{Naturalness} & \multicolumn{1}{c}{\texttt{CWEM}} \\ 
        \hline
        \multirow{5}{*}{} 
        & \raggedleft KGW &
        0.499 & 0.831 & \textbf{0.994} & 0.775 & 
        0.573 & 0.523 & \textbf{0.986} & 0.694\\ 
        & EWD & 
        0.499 & 0.965 & \textbf{0.994} & 0.819 &
        0.573 & 0.730 & \textbf{0.986} & 0.763\\ 
        & SWEET & 
        0.502 & 0.867 & 0.992 & 0.787 &
        0.574 & 0.710 & 0.978 & 0.754 \\ 
        & \stone & 
        \textbf{0.571} & \textbf{0.982} & 0.990 & \textbf{0.848} &
        \textbf{0.587} & \textbf{0.777} & 0.978 & \textbf{0.781} \\ 
        \hline
\noalign{\hrule height 0.8pt}
\end{tabular}%
}
\caption{Experimental results on MBPP+ and HumanEval+.}
\label{tab:experimental_results_main}
\end{table*}

\vspace{10pt}

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{clcccccccc}
\noalign{\hrule height 0.8pt}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{Method}} & 
        \multicolumn{4}{c}{HumanEvalPack-C++} & 
        \multicolumn{4}{c}{HumanEvalPack-Java} \\ 
        \cmidrule(lr){3-6} \cmidrule(lr){7-10} 
        \multicolumn{2}{c}{} & 
        \multicolumn{1}{c}{Correctness} & \multicolumn{1}{c}{Detectability} & \multicolumn{1}{c}{Naturalness} & \multicolumn{1}{c}{\texttt{CWEM}} & 
        \multicolumn{1}{c}{Correctness} & \multicolumn{1}{c}{Detectability} & \multicolumn{1}{c}{Naturalness} & \multicolumn{1}{c}{\texttt{CWEM}} \\ 
        \hline
        \multirow{5}{*}{}    
        & KGW &
        0.576 & 0.621 & \textbf{0.993} & 0.730 & 
        0.387 & 0.546 & \textbf{0.993} & 0.642\\ 
        & EWD & 
        0.576 & 0.681 & \textbf{0.993} & 0.750 &
        0.387 & 0.646 & \textbf{0.993} & 0.675\\ 
        & SWEET & 
        0.584 & 0.641 & 0.979 & 0.735 &
        0.413 & 0.580 & 0.901 & 0.631 \\ 
        & \stone & 
        \textbf{0.622} & \textbf{0.729} & 0.990 & \textbf{0.780} &
        \textbf{0.445} & \textbf{0.721} & 0.979 & \textbf{0.715} \\ 
        \hline
\noalign{\hrule height 0.8pt}
\end{tabular}%
}
\caption{Experimental results on HumanEvalPack C++ and Java.}
\label{tab:experimental_results_main2}
\end{table*}

\paragraph{Base Model} Our base model is Qwen2.5-Coder~\citep{hui2024qwen2}, which is an advanced code generation model. According to the EvalPlus leaderboard\footnote{https://evalplus.github.io/leaderboard.html}, Qwen2.5-Coder ranks third, following GPT-o1 models. 
Given its strong performance on the leaderboard, we selected Qwen2.5-Coder as our base LLM for this study.

\paragraph{Evaluation Metrics} We conduct experimental evaluations
using our proposed evaluation metric \texttt{CWEM}. 
The evaluation framework specifically assessed three critical aspects: functional correctness to ensure proper operation~(Correctness), 
detectability to verify watermark identification~(Detectability),
and naturalness to confirm the imperceptibility of the watermark~(Naturalness). 
We measure correctness score using the average of pass@k~(k=1,5).

\paragraph{Implementation Details}

We conduct experiments on MBPP+ and HumanEval+
under the instruction-tuned Qwen2.5-Coder 7B model.
For the hyperparameter optimization,
we set $\delta$=1.0, $\gamma$=0.5 for MBPP+ 
and $\delta$=5.0, $\gamma$=0.5 for HumanEval+ and HumanEvalPack-C++ and Java,
from {$\delta$=[0.5, 1.0, 2.0, 3.0, 4.0], $\gamma$=[0.25, 0.5, 0.75]}.
A detailed analysis of the impact of varying $\delta$ and $\gamma$ 
across multiple values is provided in Section~\ref{sec:tradeoff}. 
All experiments are conducted on NVIDIA A6000 GPU.

\subsection{Research Questions and Findings}

We formulate three key research questions and conduct a 
thorough analysis to address them:
\begin{itemize}
    \item \textbf{RQ1}: How well does \stone~preserve code functionality?
    \item \textbf{RQ2}: How superior is \stone~to the baseline when considering functional correctness, detectability, and naturalness comprehensively?
    \item \textbf{RQ3}: How does the time required for watermark insertion and detection in \stone? 
\end{itemize}
These three research questions guide our comprehensive analysis of \stone~focusing on the following aspects: 
1) \textbf{Functionality Preservation}: Assessing whether \stone~can insert watermarks without altering the original functionality. 
2) \textbf{Detectability and Naturalness}: Evaluating whether the watermark inserted by \stone~enables reliable detection of LLM-generated code while preserving a natural and readable code appearance. 
3) \textbf{Performance Overhead}: Measuring the computational overhead involved in watermark insertion and detection. 
This analysis provides an understanding of the performance of \stone~across 
correctness, detectability, naturalness, and efficiency.

\paragraph{RQ1}

We assess whether \stone~can embed watermarks without disrupting the functionality of the original code.
The experimental results in Tables~\ref{tab:experimental_results_main} and \ref{tab:experimental_results_main2} demonstrate that \texttt{STONE} improves correctness by 7.57\% compared with SWEET across all benchmarks. 
A detailed token-level analysis~(Appendix~\ref{sec:sweet_watermark_token_selection}) reveals that entropy-based token selection of SWEET inadvertently targets 
syntax-related elements---such as delimeters~(49.29),
whitespaces~(38.17\%), keywords~(9.44\%), types~(3.17\%), 
and operators~(2.78\%)---which are critical for preserving functionality. 
By contrast, \stone~selectively avoids altering these syntactic tokens, 
thereby preventing disruptions such as parsing errors or incorrect control flows.

Table~\ref{tab:sweet_watermark_token_selection_examples} provides examples of syntax tokens used as watermark tokens in SWEET. 
These examples show that modifying such tokens 
can disrupt code functionality.
For example, changing keywords like `True' or `False' affects boolean logic in conditionals, 
which may lead to incorrect branching. 
Altering delimiters like colons~(`:') or brackets~(`{}', `[]') 
can trigger parsing errors. 
Replacing `None' can interfere with object initialization 
and function returns, causing failures. 
Modifications to operators such as `=', `-', or `\%' can yield incorrect outputs. \texttt{STONE} avoids these issues by leaving syntax elements unchanged, 
thereby preserving functional correctness in the generated code.

\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcc|cc|cc|cc}
\noalign{\hrule height 0.8pt}
    \hline
        \multirow{2}{*}{Method} & 
        \multicolumn{2}{c|}{MBPP+} & 
        \multicolumn{2}{c|}{HumanEval+} &
        \multicolumn{2}{c|}{HumanEvalPack-C++} &
        \multicolumn{2}{c}{HumanEvalPack-Java} \\
        % \cline{2-3}
        % \cline{4-5}
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} 
        & Generation & Detection & Generation & Detection & Generation & Detection & Generation & Detection \\
        \hline
        No watermark 
        & 3329 & - 
        & 1261 & - 
        & 1276 & - 
        & 1416 & - \\
        KGW 
        & 3320 & 12.90 
        & 1268 & 4.19 
        & 1308 & 8.01 
        & 1506 & 8.95 \\
        EWD 
        & 3320 & 100.43 
        & 1268 & 34.51
        & 1308 & 56.85 
        & 1506 & 59.84 \\
        SWEET 
        & 3300 & 100.94 
        & 1270 & 34.68 
        & 1454 & 58.08
        & 1480 & 59.22\\
        \stone 
        & 3266 & 13.27 
        & 1277 & 4.62 
        & 1300 & 8.48
        & 1459 & 9.24 \\
        \hline
\noalign{\hrule height 0.8pt}
\end{tabular}%
}
\caption{Comparison generation and detection time performance in different watermark methods.}
\label{tab:generation_detection_time}
\end{table*}


\paragraph{RQ2}

\texttt{STONE} achieves the highest detection accuracy 
while maintaining code naturalness. 
KGW adjusts token selection across all tokens, resulting in small differences between watermarked and non-watermarked code that lower detection performance.
Although EWD builds on KGW with entropy-based weighting, it still applies watermarking to every token, which leads to similar issues.
In contrast, \texttt{STONE} selectively embeds watermarks 
only in non-syntax elements, 
ensuring that the watermark signal remains distinct and unambiguous.
This ensures improved detection performance.

KGW and EWD report high naturalness scores by evenly adjusting token probabilities.
In comparison, \stone~keeps the code syntax unchanged and adjusts only non-syntax tokens. 
This strategy allows watermark embedding without altering the code structure or functionality, 
making \stone~more natural than SWEET. 
SWEET modifies high-entropy tokens---including syntax elements such as keywords and operators---which shifts token probabilities and eases watermark detection.

SWEET achieves naturalness scores similar to \stone~in MBPP+ and HumanEval+, 
likely because Python syntax tolerates small changes. 
However, modifications of syntax elements may create detectable patterns 
in other more complex programming languages like C++ and Java 
and affect readability. 
By preserving syntax and embedding watermarks only in non-syntax tokens, 
\stone~minimizes changes in token probabilities while maintaining readability. 
Overall, \stone~improves watermark classification without compromising code naturalness.

\paragraph{RQ3}
We analyze the computational overhead of watermarking 
by comparing generation time and detection time. 
Generation time refers to the duration of producing watermarked code, 
while detection time measures how long it takes to verify a watermark. Table~\ref{tab:generation_detection_time} summarizes 
the results for each baseline. 
All methods show similar generation times, 
which indicates that watermark insertion does not significantly delay code generation.

In contrast, \stone~detects watermarks on average 86\% faster than SWEET and EWD. 
This difference arises from the detection process. 
Both SWEET and EWD perform entropy calculations 
that require computing token probability distributions, 
increasing detection overhead. 
\stone~avoids these computations 
by verifying the watermark 
through a predefined green list of non-syntax elements. 
As a result, \stone~reduces detection time while maintaining high detectability.

\section{Related Work}

Research on code watermarking remains limited due to the unique 
challenges posed by structural constraints in code, particularly 
in maintaining functional correctness, detectability, and naturalness.
Unlike natural language, code must adhere to strict syntactic and 
semantic rules, making the development of effective watermarking 
techniques more complex.

\citet{lee-etal-2024-wrote} proposed SWEET,
a method that selectively embeds watermarks in high-entropy tokens.
While SWEET demonstrated improvements in detection capability 
and code quality, our preliminary analysis revealed that approximately 
12.6\% of these high-entropy tokens correspond to syntax elements 
such as reserved keywords. 
Modifying these critical tokens can disrupt program logic, 
resulting in degraded functional correctness.

\citet{guan2024codeip} introduced CodeIP, a grammar-guided 
watermarking approach by utilizing type prediction to maintain 
syntactic validity. 
This method ensured that the generated code remained grammatically correct. 
However, the reliance on type prediction requires significant 
computational resources, limiting its practicality for 
large-scale code generation tasks.

We address the limitations observed in SWEET and CodeIP by 
explicitly skipping watermark insertion for syntax-critical tokens. 
Our approach ensures that essential structural elements, 
like keywords and operators, remain unchanged, 
thereby preserving functional correctness without additional 
computational overhead.

ACW~\citep{li2024resilient} and SrcMarker~\citep{yang2024srcmarker} 
adopted watermarking techniques based on code transformations, 
applying semantic-preserving modifications and variable name substitutions. 
Although these methods maintained program functionality, the introduced 
changes often result in unnatural code patterns, 
making the watermark more susceptible 
to removal by adversaries.

% Our work builds on these insights by prioritizing 
% syntax-aware watermarking. By ensuring that watermark patterns 
% remain subtle while maintaining functional integrity, 
% we enhance the reliability and practicality of code watermarking 
% for LLM-generated code.

\section{Conclusion}

We propose \texttt{STONE}, a syntax-aware watermarking method 
that preserves the original functionality of code. 
Unlike existing methods that apply watermarks without 
considering syntax tokens—often causing syntax errors or functional 
failures—\texttt{STONE} restricts watermarking to non-syntax tokens, 
maintaining code structure and enhancing detection reliability.
Additionally, we introduce \texttt{CWEM}, 
a comprehensive evaluation metric that assesses correctness, 
detectability, and naturalness. 
While correctness and detectability are well-studied, 
naturalness has been largely overlooked. 
By incorporating naturalness into the evaluation, 
\texttt{CWEM} provides a more balanced assessment of watermarking techniques.
Experimental results show that \texttt{STONE} consistently 
outperforms existing methods across three programming 
languages, achieving higher \texttt{CWEM} scores. 
These findings demonstrate that syntax-aware watermarking 
improves detection performance while preserving code functionality, 
making it more practical for real-world applications.

\section*{Limitations}\label{sec:limitation}
This study introduces the \stone~method, which preserves key functional tokens in code to maintain correctness. However, the current approach is not specifically optimized to enhance the naturalness of code during watermark insertion. 
Experimental results indicate 
that the naturalness score is lower than those achieved with methods such as KGW and EWD. 
Future work should explore refined token selection strategies to address this aspect.

In our study, \texttt{CWEM} does not incorporate a measure for evaluating the persistence of the watermark when modifications, refactoring, or token substitutions alter the code. 
Future investigations should develop quantitative measures to assess watermark robustness under these conditions.

We conduct experiments
using the Qwen2.5-Coder model across three programming languages: Python, C++, and Java. 
The evaluation does not extend to additional large language model architectures, 
training environments, or parameter scales. 
Future studies should broaden the evaluation to encompass other model architectures, 
training strategies, and parameter scales 
in order to develop the scope and limitations of the approach.


% \section*{Acknowledgments}

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\appendix
\onecolumn

\section{Syntax Elements Coverage in Watermarking Tokens of SWEET}
\label{sec:sweet_watermark_token_selection}
In Table~\ref{tab:sweet_watermark_token_selection}, we analyzed the SWEET methodology impact on token selection for watermarking using two metrics: (1) the proportion of tokens exceeding the entropy threshold among all tokens and (2) the proportion of syntax tokens among watermarking-selected tokens.
Using SWEET optimal settings~($\gamma$=0.25, $\delta$=3.0, entropy threshold = 0.9), we examined threshold levels of [0.7, 0.8, 0.9, 1.0, 1.1]. 
The results show an inverse relationship between threshold levels and token selection.
Lower thresholds increase overall token selection while incorporating more syntax tokens, whereas higher thresholds reduce syntax token inclusion.
These findings validate their selective watermarking approach, as it effectively targets high-entropy regions while preserving syntactic structures critical to text integrity.

However, even at higher entropy thresholds, a non-negligible proportion of syntax tokens is still selected for watermarking. 
For the HumanEval+ dataset specifically, syntax tokens constituted 12.6\% of watermarking tokens, with delimiters at 49.29\%, whitespaces at 38.17\%, keywords at 9.44\%, types at 3.17\%, and operators at 2.78\%. 
This finding demonstrates that even in optimal settings, 
syntax tokens are inevitably selected as watermarking tokens. 
This distribution indicates that delimiters make up 
the majority of syntax tokens selected for watermarking, 
which can directly impact code execution. 
Table~\ref{tab:sweet_watermark_token_selection_examples} illustrates the specific syntax tokens comprising the 12.6\% identified in the HumanEval+ dataset.
Delimiters, such as colons (`:') and commas (`,'), are crucial for structuring code, 
and their modification can introduce syntax errors~(e.g., omitting a colon in `for i in range(10):' causes a parsing failure). 
While keywords and operators are selected less frequently, 
their modification can significantly alter program logic, 
as seen in cases like replacing `True' with `False' or `+' with `-'. 
Given these risks, refining the token selection process 
to minimize disruptions to code execution, 
such as selectively excluding critical syntax tokens, 
could improve the practicality of syntax-aware watermarking.


\begin{table*}[h!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{crcccccccc}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{Entropy Threshold}} & 
        \multicolumn{2}{c}{MBPP+} & 
        \multicolumn{2}{c}{HumanEval+} &
        \multicolumn{2}{c}{HumanEvalPack - C++} & 
        \multicolumn{2}{c}{HumanEvalPack - Java} \\ \cline{3-10} 
        \multicolumn{2}{c}{} & 
        \multicolumn{1}{c}{Watermarked} & \multicolumn{1}{c}{Syntax} & 
        \multicolumn{1}{c}{Watermarked} & \multicolumn{1}{c}{Syntax} & 
        \multicolumn{1}{c}{Watermarked} & \multicolumn{1}{c}{Syntax} & 
        \multicolumn{1}{c}{Watermarked} & \multicolumn{1}{c}{Syntax} \\ 
        \hline
        \multirow{5}{*}{} 
        & 0.7 & 26.03 & 12.82 & 35.28 & 12.84 & 30.10 & 12.99 & 26.60 & 11.98 \\    
        & 0.8 & 21.40 & 12.59 & 32.67 & 12.24 & 25.93 & 12.35 & 22.47 & 11.06 \\    
        & 0.9 & \textbf{19.64}  & \textbf{11.39} & \textbf{28.98}  & \textbf{12.60} 
        & \textbf{22.65}  & \textbf{11.79} & \textbf{18.52} & \textbf{11.68}   \\    
        & 1.0 & 16.59 & 11.39 & 24.84 & 11.83 & 18.96 & 10.56 & 16.13 & 11.15 \\    
        & 1.1 & 14.94 & 10.30 & 23.22 & 11.81 & 15.71 & 09.82 & 14.39 & 10.91 \\    
        \hline
\end{tabular}%
}
\caption{Syntax elements coverage in watermarking tokens of SWEET. Watermarked is the proportion of tokens that exceeds the entropy threshold among all tokens~(watermarking tokens). Syntax is the proportion of watermarking tokens that belong to syntax elements.
Bolded results correspond to the optimal entropy threshold setting used in SWEET.}
\label{tab:sweet_watermark_token_selection}
\end{table*}


\begin{table}[h]
    \centering
    \begin{tabular}{|c|l|}
        \hline
        \textbf{Category} & \textbf{Examples} \\ \hline
        Keywords & def, or, import, for, True, assert, return, is, pass, None, False, in, not, if, from  \\ \hline
        Whitespaces & " ", "\textbackslash n" \\ \hline
        Types & int, set, str \\ \hline
        Delimiters & `.',:, \{, `,', ], [, (, ) \} \\ \hline
        Operators & *, \%, =, -, **, \&, \textasciicircum, /, +, | \\ \hline
    \end{tabular}
    \caption{Examples of syntax tokens selected as watermarked tokens in SWEET}
    \label{tab:sweet_watermark_token_selection_examples}
\end{table}

\clearpage

\section{Comparison of Syntax Elements in Python, C++, and Java}
\label{sec:example_of_syntax_elements}
% \FloatBarrier
This section provides examples of the syntactic elements in Python, C++, and Java that were utilized in our experiments. Table~\ref{tab:syntax_elements} categorize these elements into five groups: keywords, operators, delimiters, whitespaces, and types. 
The selection of syntactic elements reflects the distinct characteristics of each programming language.

\begin{table*}[h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|p{2.5cm}|p{4.5cm}|p{4.5cm}|p{4.5cm}|}
\hline
\textbf{Category} & \textbf{Python} & \textbf{C++} & \textbf{Java} \\ \hline
Keywords & 
True, False, None, and, as, assert, async, await, break, class, continue, def, del, elif, else, except, finally, for, from, global, if, import, in, is, lambda, nonlocal, not, or, pass, raise, return, try, while, with, yield 
& 
alignas, alignof, and, and\_eq, asm, auto,bitand, bitor, break, case, catch, class, compl, concept, const, consteval, constexpr, constinit, const\_cast, continue, co\_await, co\_return, co\_yield, decltype, default, delete, do, dynamic\_cast, else, enum, explicit, export, extern, false, for, friend, goto, if, inline, mutable, namespace, new, noexcept, not, not\_eq, nullptr, operator, or, or\_eq, private, protected, public, register, reinterpret\_cast, requires, return, sizeof, static, static\_assert, static\_cast, struct, switch, template, this, thread\_local, throw, true, try, typedef, typeid, typename, union, using, virtual, volatile, while, xor, xor\_eq, override
& 
abstract, assert, break, case, catch, class, const, continue, default, do, else, enum, extends, final, finally, for, goto, if, implements, import, instanceof, interface, native, new, null, package, private, protected, public, return, static, strictfp, super, switch, synchronized, this, throw, throws, transient, try, void, volatile, while, true, false 
\\ \hline
Whitespaces & space, \textbackslash n, \textbackslash t & space, \textbackslash n, \textbackslash t & space, \textbackslash n, \textbackslash t 
\\ \hline
Types & 
int, float, complex, str, bytes, bool, list, tuple, set, dict, NoneType 
& 
int, float, double, bool, char, short, long, void, unsigned, signed, size\_t, ptrdiff\_t, wchar\_t, char8\_t, char16\_t, char32\_t
& 
byte, short, int, long, float, double, boolean, char, String, Object 
\\ \hline
Delimiters & 
(, ), [, ], \{, \}, `,', :, `.', ;, @, ->, ... 
& 
(, ), [, ], \{, \}, `,', :, `.', ;, ->, ::, ...
& 
(, ), [, ], \{, \}, `,', :, `.', ;, @, ->, ::, ... 
\\ \hline
Operators & 
+, -, *, /, \%, **, //, =, ==, !=, >, <, >=, <=, +=, -=, *=, /=, \%=, //=, **=, \&, |, <<, >>, \textasciicircum, \textasciitilde 
& 
+, -, *, /, \%, ++, --, =, ==, !=, >, <, >=, <=, \&\&, ||, !, \&, |, \textasciicircum, \textasciitilde, $<<$, $>>$, +=, -=, *=, /=, \%=, \&=, |=, \textasciicircum=, $<<=$, $>>=$, .*, ->*
& 
+, -, *, /, \%, ++, --, =, ==, !=, >, <, >=, <=, \&\&, ||, !, \&, |, \textasciicircum, \textasciitilde, $<<$, $>>$, $>>>$, +=, -=, *=, /=, \%=, \&=, |=, \textasciicircum=,$<<=$, $>>=$, $>>>$, $\mathtt{>>>=}$
\\ \hline
\end{tabular}%
}
\caption{Comparison of five types of syntax elements across Python, C++, and Java.}
\label{tab:syntax_elements}
\end{table*}

\clearpage

% delta와 gamma가 pass@1과 AUROC에 주는 영향 분석 plot 
\section{Trade-off Between Correct and Detect}
\label{sec:tradeoff}
We analyze the trade-off between pass@1 and AUROC based on watermarking strength 
by adjusting two parameters: 
$\gamma$, which controls the ratio between the green list and red list, 
and $\delta$, which determines how often the model selects tokens from the green list.
Figure~\ref{fig:delta_gamma} shows the results in a bubble chart, 
revealing a clear trade-off—improving one metric often reduces the other.

The bottom-right section of each subfigure highlights larger light-blue bubbles, 
where high delta and low gamma values produce higher AUROC but lower pass@1.
A smaller gamma shrinks the green list, 
limiting the token options available during generation. 
At the same time, a higher delta increases the likelihood of selecting tokens 
from the green list. 
This combination strengthens the watermark signal 
and raises AUROC but generates less natural code, reducing pass@1.

Achieving the right balance between code quality and watermark detectability requires 
careful tuning of gamma and delta.
Maintaining code quality and correctness remains crucial when embedding watermarks, 
so relying on both a low gamma and a high delta together creates undesirable results.
The areas shaded in green highlight points where 
pass@1 and AUROC achieve a proper balance.
The double circles outlined in black represent the performance 
reported in Table~\ref{tab:experimental_results_main}. 
For the MBPP+ dataset, these points correspond to 
$\delta$ = 1.0 and $\gamma$ = 0.5, 
while for the HumanEval+ dataset, they correspond to 
$\delta$ = 0.5 and $\gamma$ = 0.5. 
The trends in the green-shaded region suggest that using a 
lower delta value generally results in a better balance between 
pass@1 and AUROC performance.
\begin{figure*}[hbt!]
    \centering
        \includegraphics[width=14cm]{figure/delta_gamma.pdf}
    \caption{The tradeoff between Correct and Detect. 
    We analyze the impact of the green list 
    ratio $\gamma$~(indicated by marker color) 
    and the logit adjustment factor $\delta$~(indicated by marker size). 
    Each point represents the functional correctness of 
    code watermarked using the 
    \texttt{STONE}~(Y-axis) and 
    the performance of classifying it against 
    human-written code~(X-axis). 
    The green-shaded area highlights points where 
    Correct and Detect are well balanced. 
    Double circles with an outer black ring mark the 
    performance points reported in Table~\ref{tab:experimental_results_main}.
    }\label{fig:delta_gamma}
\end{figure*} 
\clearpage

\end{document}
