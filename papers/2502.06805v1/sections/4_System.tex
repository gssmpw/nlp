\section{System-Level Efficiency Optimization}
\label{sec:system}

\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]


\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=22em,font=\normalsize,}{},
            where level=2{text width=15em,font=\normalsize,}{},
            where level=3{text width=24em,font=\normalsize,}{},
            [
                \textbf{System}, ver
                    [
                       \textbf{Optimized Hardware-Software Co-design}, fill=blue!10
                            [
                                ~\citet{chen2023speed}{,}
                                SDA~\citep{yang2024sda}{,}
                                ~\citet{choi2024stable}, leaf, text width=61.7em
                            ]
                    ]
                    [
                        \textbf{Parallel Computing}, fill=blue!10
                        [
                            DistriFusion~\citep{li2024distrifusion}{,}
                            PipeFusion~\citep{wang2024pipefusion}{,}
                            SwiftDiffusion~\citep{li2024swiftdiffusionefficientdiffusionmodel}{,}
                            DiffusionPipe~\citep{tian2024diffusionpipetraininglargediffusion}, leaf, text width=61.7em
                        ]
                    ]
                    [
                        \textbf{Caching Technique}, fill=blue!10
                            [
                                NIRVANA~\citep{agarwal2024approximate}{,}
                                DeepCache~\citep{ma2024deepcache}{,}
                                Block Caching~\citep{wimbauer2024cache}{,}
                                L2C~\citep{ma2024learning}{,}
                                \\FORA~\citep{selvaraju2024fora}, leaf, text width=61.7em
                            ]
                    ]
            ]
        \end{forest}
 }
    \caption{Summary of system-level efficiency optimization techniques for diffusion models.}
    \label{fig:model compression}
\end{figure*}

\subsection{Hardware-Software Co-Design}
\label{sec:codesign}

% Speed is all you need: On-device acceleration of large diffusion models via gpu-aware optimizations
% SDA: Low-Bit Stable Diffusion Acceleration on Edge FPGAs
% A 28.6 mJ/iter Stable Diffusion Processor for Text-to-Image Generation with Patch Similarity-based Sparsity Augmentation and Text-based Mixed-Precision
The co-design of hardware and software is pivotal for achieving efficient deployment of diffusion models in real-time and resource-constrained environments. Following algorithm-level optimizations, system-level techniques focus on integrating hardware-specific features, distributed computation, and caching mechanisms. These strategies aim to address the computational complexity and memory demands of large-scale diffusion models, enabling more practical applications across various platforms like GPUs, FPGAs, and mobile devices. One significant contribution is the work by~\cite {chen2023speed}, which explores GPU-aware optimizations for accelerating diffusion models directly on mobile devices. Implementing specialized kernels and optimized softmax operations reduces inference latency, achieving near real-time performance on mobile GPUs.
%
In a related effort, ~\citet{yang2024sda} propose SDA, a low-bit stable diffusion accelerator designed specifically for edge FPGAs. Utilizing quantization-aware training and a hybrid systolic array architecture as illustrated in Figure~\ref{fig:co-design}, SDA effectively balances computational efficiency with flexibility, handling both convolutional and attention operations efficiently. Through a two-level pipelining structure, the nonlinear operators are efficiently integrated with the hybridSA, enabling coordinated operation that enhances processing speed while reducing resource usage. Finally, SDA achieves a speedup of 97.3x when compared to ARM Cortex-A53 CPU.

Furthermore,~\citet{choi2024stable} introduces a stable diffusion processor optimized for mobile platforms through patch similarity-based sparsity, mixed-precision strategies and and a Dual-mode Bit-Slice Core (DBSC) architecture that supports mixed-precision computation, which particularly targeting resource-constrained devices such as mobile platforms. Together, these optimizations significantly improve throughput and energy efficiency, making Stable Diffusion more viable for energy-sensitive applications.

% In addressing the challenges of large language model inference,~\citet{kwon2023efficient} propose efficient memory management techniques using paged attention, reducing memory overhead for model serving. Similarly,~\citet{zeng2024flightllm} introduces FlightLLM, which maps large language models onto FPGAs, optimizing both resource allocation and inference throughput.

\subsection{Parallel Computing}

\label{sec:parallel_computing}
\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \vspace{-30pt}
    \includegraphics[width=0.36\textwidth]{figures/co-design.pdf}
    \vspace{-25pt}
    \caption{Illustration of the HybridSA architecture from ~\citet{yang2024sda}.}
    \label{fig:co-design}
    \vspace{-10pt}
\end{wrapfigure}

% Distrifusion: Distributed parallel inference for high-resolution diffusion models, cited 13
% PipeFusion: Displaced Patch Pipeline Parallelism for Inference of Diffusion Transformer Models, cited 1
% SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules
% DiffusionPipe: Training Large Diffusion Models with Efficient Pipelines
Parallel computing plays a critical role in the efficient execution of diffusion models, especially given the computation-intensive nature of these algorithms in Figure~\ref{fig:parallel_computing}. Recent advances in parallel computing strategies have enabled significant improvements in inference speed and scalability, often without compromising the quality of the generated output~\citep{li2024distrifusion, wang2024pipefusion, li2024swiftdiffusionefficientdiffusionmodel, tian2024diffusionpipetraininglargediffusion}. This section highlights several notable contributions that tackle the challenge of parallelizing diffusion models across multiple GPUs and other distributed architectures.
%
\citet{li2024distrifusion} introduced DistriFusion, a framework designed for distributed parallel inference tailored to high-resolution diffusion models such as SDXL. Their approach involves partitioning the model inputs into distinct patches, which are then processed independently across multiple GPUs. This method leverages the available hardware resources more effectively, achieving a 6.1x speedup on 8xA100 GPUs compared to single-card operation, all while maintaining output quality.

To address potential issues arising from the loss of inter-patch interaction, which could compromise global consistency, DistriFusion employs dynamic synchronization of activation displacements, striking a balance between preserving coherence and minimizing communication overhead.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/Parallel_Computing.pdf}
    \caption{Illustrations of the parallel computing for diffusion models.}
    \label{fig:parallel_computing}
\end{figure}
Building on the insights gained from DistriFusion, \citet{wang2024pipefusion} further refined the distributed inference paradigm with PipeFusion. This system not only splits images into patches but also distributes the network layers across different devices, thereby reducing the associated communication costs and enabling the use of PCIe-linked GPUs instead of NVLink-connected ones. PipeFusion integrates sequence parallelism, tensor parallelism, displaced patch parallelism, and displaced patch pipeline parallelism, optimizing workflow for a wider range of hardware configurations.
\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/sys_ori_parallel_sampling.pdf}
    \caption{Illustrations of the diffusion architecture from~\citep{li2024distrifusion}.}
    \label{fig:sys_ori_parallel_sampling}
\end{figure}
% \citet{zhao2024dspdynamicsequenceparallelism} addressed the limitations of traditional sequence parallelism, which can be slow and resource-intensive, by proposing \textsc{DSP} (Dynamic Sequence Parallelism). This method dynamically switches the dimensions of parallelism, adapting to the varying requirements of attention computations. Through this adaptive strategy, \textsc{DSP} achieves a substantial throughput increase of up to 10x with a 25\% reduction in communication overhead, making it particularly suitable for multi-dimensional transformers.

For applications involving add-on modules such as ControlNet and LoRA,~\citet{li2024swiftdiffusionefficientdiffusionmodel} developed SwiftDiffusion, as illustrated in Figure~\ref{fig:sys_ori_parallel_sampling}. This framework optimizes the serving workflow of these modules, allowing them to run in parallel on multiple GPUs. As a result, SwiftDiffusion delivers a 5x reduction in inference latency and a 2x improvement in throughput, ensuring that enhanced speed does not come at the expense of output quality.

Lastly, \citet{tian2024diffusionpipetraininglargediffusion} focused on the training phase with DiffusionPipe, demonstrating that pipeline parallelism can produce a 1.41x training speedup, while data parallelism contributes an additional 1.28x acceleration. Although the optimization methods for DiffusionPipe were not detailed in the notes, the combination of these parallelization strategies offers a promising direction to improve the efficiency of both the training and inference pipelines for diffusion models.

\subsection{Caching Technique}
\label{sec:caching_technique}
% Approximate Caching for Efficiently Serving $\{$Text-to-Image$\}$ Diffusion Models, cited 3
% Deepcache: Accelerating diffusion models for free, cited 36
% Cache me if you can: Accelerating diffusion models through block caching, cited 9
% Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching, cited 1
% FORA: Fast-Forward Caching in Diffusion Transformer Acceleration
In diffusion models, the computational hotspot often centers around discrete time-step diffusion, which is characterized by strong temporal locality. Consequently, building an efficient caching system for diffusion models is nonnegligible to enhance its performance. Indeed, extensive research has been conducted on optimizing caching systems in Figure~\ref{fig:sys_cache}, resulting in significant advancements in this field.

\cite{agarwal2024approximate} proposed NIRVANA, a novel system designed to enhance the efficiency of text-to-image generation using diffusion models. Specifically, the key innovation lies in its approximate caching technique, which reduces computational costs and latency by reusing intermediate noise states from previous image generation processes. Instead of starting from scratch with every new text prompt, NIRVANA retrieves and reconditions these cached states, allowing it to skip several initial denoising steps.
\begin{figure}[h]
    \centering
    \vspace{-10pt}
    \includegraphics[width=\textwidth]{figures/caching_system.pdf}
    \caption{Illustrations of the caching system for diffusion models focus on the U-Net block and the Transformer layer, critical components for effectively implementing caching techniques.}
    \label{fig:sys_cache}
    \vspace{-10pt}
\end{figure}
Additionally, the system uses a custom cache management policy called Least Computationally Beneficial and Frequently Used (LCBFU), which optimizes the storage and reuse of cached states to maximize computational efficiency. This makes NIRVANA particularly suited for large-scale, production-level deployments of text-to-image diffusion models.

From another perspective,~\citet{ma2024deepcache} introduces an innovative approach called DeepCache, designed to accelerate the image generation process by leveraging the temporal redundancy in the denoising steps of diffusion models, without the need for additional model training, as illustrated in Figure~\ref{fig:sys_ori_cache}. The key insight is the observation that high-level features, such as the main structure and shape of an image, exhibit minimal changes between adjacent denoising steps. These features can be cached and reused in subsequent steps, thereby avoiding redundant computations. This method takes advantage of the U-Net architecture by combining these cached high-level features with low-level features, updating only the low-level features to reduce computational load, leading to a significant acceleration in the overall process. ~\citet{wimbauer2024cache} proposed Block Caching, a technique that identifies and caches redundant computations within the model's layers during the denoising process. By reusing these cached outputs in subsequent timesteps, the method significantly speeds up inference while maintaining image quality. To optimize this caching process, they introduce an Automatic Cache Scheduling mechanism, which dynamically determines when and where to cache based on the relative changes in layer outputs over time. Additionally, the paper addresses potential misalignment issues from aggressive caching by implementing a Scale-Shift Adjustment mechanism, which fine-tunes cached outputs to align with the model’s expectations, thereby preventing visual artifacts.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/sys_ori_cache.pdf}
    \caption{Illustration of the caching system from~\citep{ma2024deepcache}.}
    \label{fig:sys_ori_cache}
\end{figure}
Recently, the application of diffusion with transformer models has yielded considerable success.~\citet{ma2024learning} is concerned with the introduction of a layer caching mechanism, designated Learning-to-Cache (L2C), to accelerate diffusion transformer models. L2C exploits the redundancy between layers within the transformer architecture, dynamically caching computations from certain layers to reduce redundant calculations and lower inference costs. The implementation entails transforming the layer selection problem into a differentiable optimization problem, using interpolation to determine whether to perform a full computation or utilize cached results at different timesteps during inference. In contrast to the emphasis on layer caching,
%
~\citet{selvaraju2024fora} proposed Fast-Forward Caching (FORA), a technique designed to accelerate Diffusion Transformers (DiT) by reducing redundant computations during the inference phase. The key insight behind FORA is the observation that the outputs from the self-attention and MLP layers in a Transformer exhibit high similarity across consecutive time steps in the diffusion process. To leverage this, FORA implements a static caching mechanism where these layer outputs are cached at regular intervals, which are determined by $N$, and reused for a set number of subsequent steps, thereby avoiding recomputing similar outputs.