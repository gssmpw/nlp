\section{Background and Applications}

\subsection{Background of Diffusion Models}

%\subsection{Basic Formulas for Diffusion models}
To better understand the directions for improving efficient diffusion models, it is essential first to comprehend the fundamental framework of diffusion models. Denoising Diffusion Probabilistic Models (DDPMs) ~\citep{ho2020denoising} generate data through a process analogous to thermodynamic diffusion, consisting of two key components: a forward process and a reverse process. These processes work in concert to enable high-quality generative modeling.

The forward process in DDPM is a fixed Markov chain involving gradually adding Gaussian noise to the data until it becomes pure noise. \( q(\mathbf{x}_0) \) is denoted as the true data distribution, and assuming that \( \mathbf{x}_0 \sim q(\mathbf{x}_0) \) represents sampled data from this distribution. The forward process noted as \( q(\mathbf{x}_{1:T} | \mathbf{x}_0) \), adds Gaussian noise step by step, transforming the data from \( \mathbf{x}_0 \) to \( \mathbf{x}_T \):  
\begin{equation}\label{eq:forward_process}
   q(\mathbf{x}_{1:T} | \mathbf{x}_0) := \prod_{t=1}^{T} q(\mathbf{x}_t | \mathbf{x}_{t-1}), 
   \quad
   q(\mathbf{x}_t | \mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{\alpha_t} \mathbf{x}_{t-1}, \beta_t I)
\end{equation}
\( \beta_t \) is defined as the variance of the noise added at each timestep. We then convert this to \( \alpha_t = 1 - \beta_t \). Additionally, \( \bar{\alpha}_t = \prod_{s=1}^{t} \alpha_s \) is defined as the cumulative product of \( \alpha_t \), following the formulation by Sohl-Dickstein et al.~\citep{sohl2015deep}. This cumulative product allows for modeling the transition from the original data \( \mathbf{x}_0 \) to \( \mathbf{x}_t \) as a Gaussian distribution:
\begin{equation}
    q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t} \mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})
\end{equation}
This expression describes the distribution of \( \mathbf{x}_t \) given the initial data \( \mathbf{x}_0 \). It indicates that \( \mathbf{x}_t \) can be expressed as a linear combination of \( \mathbf{x}_0 \) and noise, where \( \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \) represents standard Gaussian noise:
\begin{equation}
    \mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}
\end{equation}
The reverse process, in contrast, aims to gradually denoise and reconstruct the original data by reversing the noise addition performed in the forward process. This reverse process is modeled as a Markov chain where each step transitions from \( \mathbf{x}_t \) to \( \mathbf{x}_{t-1} \) using a learned conditional probability distribution \( p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \). The overall process is expressed as:
\begin{equation}\label{eq:backward_process}
p_\theta(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^{T} p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t), \quad 
p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t))
\end{equation}
where \( p(\mathbf{x}_T) \) is the initial Gaussian distribution at the final time step \( T \), and \( p_\theta(\mathbf{x}_{t-1} | \mathbf{x}_t) \) represents the conditional probability distribution learned by the model to transition between states. The mean \( \mu_\theta(\mathbf{x}_t, t) \) and covariance \( \Sigma_\theta(\mathbf{x}_t, t) \) are parameterized functions of the state \( \mathbf{x}_t \), the time step \( t \), and the model parameters \( \theta \). In the training process, the optimization objective is to minimize the negative log-likelihood using the variational bound to approximate the true data distribution:
\begin{equation}
\mathbb{E}[-\log p_\theta(\mathbf{x}_0)] \leq \mathbb{E}_q \left[ -\log \frac{p_\theta(\mathbf{x}_{0:T})}{q(\mathbf{x}_{1:T}|\mathbf{x}_0)} \right] = \mathbb{E}_q \left[ -\log p(\mathbf{x}_T) - \sum_{t \geq 1} \log \frac{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}{q(\mathbf{x}_t|\mathbf{x}_{t-1})} \right] =: L
\end{equation}
This objective function decomposes the optimization problem into KL divergences for each timestep, progressively optimizing the reverse process. Expanding the KL terms and using the conditional Gaussian form evaluates the difference between the forward and reverse processes, ultimately simplifying the process into a mean squared error form:
\begin{equation}
\label{ddpm_loss}
L_{\text{simple}}(\theta) := \mathbb{E}_{t,\mathbf{x_0},\boldsymbol{\epsilon}} \left[ \|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t)\|^2 \right]
\end{equation}


\subsection{Applications of Diffusion Models}
\label{sec:application}

\subsubsection{Image Generation}
\label{sec:image_generation}

Image generation is the primary application domain for efficient diffusion models. Researchers have been developing various approaches to optimize both computational resources and generation quality. The efficiency improvements in this field are well exemplified by several influential works. 
%
For example, Stable Diffusion~\citep{rombach2022high} pioneered the concept of efficient image generation by operating in a compressed latent space rather than pixel space, significantly reducing memory and computational requirements while maintaining high-quality outputs.
%
Latent Consistency Models (LCM)~\citep{luo2023latent} further pushed the boundaries by enabling high-quality image generation in just 4 steps through careful design of the consistency loss and distillation process.
%
Progressive distillation~\citep{salimans2022progressive} demonstrated that through a student-teacher framework, diffusion models could achieve comparable quality to 50-step sampling using only 2-8 inference steps.
%
ControlNet~\citep{zhang2023controlnet} introduced an efficient architecture for adding spatial conditioning controls to pretrained diffusion models through zero-initialized convolutions, enabling diverse control capabilities without compromising model efficiency.
%
More recently, Efficient Diffusion (EDM)~\citep{karras2022elucidating} presented a comprehensive framework for training and sampling diffusion models more efficiently, introducing improvements in both training stability and inference speed while maintaining state-of-the-art generation quality.

\subsubsection{Video Generation}
\label{sec:video_generation}

% A Survey on Video DIffusion Model
% AdaDiff: Adaptive Step Selection for Fast Diffusion
% ED-T2V: An Efficient Training Framework for Diffusion-based Text-to-Video Generation
% SimDA: Simple Diffusion Adapter for Efficient Video Generation
% Video latent consistency model
% Grid Diffusion Models for Text-to-Video Generation

Following the rapid escalation in image generation, video generation similarly garnered widespread attention~\citep{ho2022video, singer2022make, rombach2022high, xing2023survey}.
The heavy model size and the substantial computational costs have further intensified the focus on developing more efficient methods for video generation~\citep{zhang2023adadiff, liu2023ed, xing2024simda, wang2023videolcm, lee2024grid}.
%
For example, \citet{zhang2023adadiff} introduced AdaDiff, a lightweight framework designed to optimize a specialized policy gradient method tailored to individual text prompts. This approach facilitates the design of reward functions and enables an effective trade-off between inference time and generation quality.
%
Specifically to the training process,~\citet{liu2023ed} proposed an efficient training framework ED-T2V to freeze the pretraining model~\citep{rombach2022high} training additional temporal modules.
%
Similarly,~\citet{xing2024simda} suggested using spatial and temporal adapters. In their approach, the original T2I model remains frozen during training, and only the newly added adapter modules are updated. Unlike the works above,~\citet{wang2023videolcm} presented VideoLCM, incorporating consistency distillation in the latent space. VideoLCM efficiently distills knowledge from a pretraining model, maintaining fidelity and temporal coherence while improving inference speed.
%
~\citet{lee2024grid} introduces a grid diffusion model by representing a video as a grid of images. It employs key grid image generation and autoregressive grid interpolation to maintain temporal consistency.

\subsubsection{Text Generation}
\label{sec:text_generation}
Efficient diffusion models offer a fresh perspective in text generation through their stochastic and iterative processes. However, they encounter several challenges when applied to discrete data types such as text. For instance, the common use of Gaussian noise is less effective for discrete corruption, and the objectives designed for continuous spaces become unstable in the text diffusion process, particularly at higher dimensions. 
%
With these challenges,~\citet{chen2023cheaper} proposed a diffusion model called Masked-Diffuse LM. In the diffusion process, a cross-entropy loss function at each diffusion step is utilized to efficiently bridge the gap between the continuous representations in the model and the discrete textual outputs. 
%
SeqDiffuSeq~\citep{yuan2024text} incorporates an encoder-decoder Transformer architecture, achieving efficient text generation through adaptive noise schedule and self-conditioning~\citep{chen2022analog} techniques. 
%
Using the same encoder-decoder architecture,~\citet{lovelace2024latent} presents a methodology where text is encoded into a continuous latent space. Subsequently, continuous diffusion models are employed for sampling within this space. 
%

\subsubsection{Audio Generation}
\label{sec:audio_generation}
% Wavegrad: Estimating gradients for waveform generation
% Diffwave: A versatile diffusion model for audio synthesis
% On fast sampling of diffusion probabilistic models

In the field of audio generation, the application of diffusion models presents several unique challenges. First, audio data exhibits strong temporal continuity, especially in high-resolution audio generation tasks, where the model must accurately reconstruct both time-domain and frequency-domain information. Compared to images or text, even subtle distortions or noise in audio are easily perceptible by humans, directly affecting the listening experience, particularly in speech and music generation tasks. Ensuring high fidelity and maintaining the consistency of details in the generated audio is therefore crucial. Moreover, many audio generation tasks require low-latency feedback, especially in applications like speech synthesis and real-time dialogue, which makes acceleration of diffusion models essential. The multi-dimensional nature of audio data, such as time-domain, frequency-domain, stereo, and spatial audio, further complicates the generation process, requiring the model to handle these dimensions while maintaining consistency during the accelerated generation.
%
To address these challenges, researchers have proposed various methods to accelerate diffusion models in audio generation. Some works focus on reducing the number of diffusion steps to speed up the generation process, such as \citet{chen2020wavegrad} in WaveGrad and \citet{kong2020diffwave} in DiffWave, which optimize the network structure to reduce generation time while maintaining high audio quality. Further optimization comes from the FastDPM framework \citep{kong2021fast}, which generalizes discrete diffusion steps to continuous ones, using a bijective mapping between noise levels to accelerate sampling without compromising quality. FastDPM's flexibility allows it to adapt to different domains, and in the case of audio synthesis, where stochasticity plays a crucial role, it demonstrates superior performance in high-stochasticity tasks like speech generation. Through these approaches, diffusion models not only accelerate the generation
\begin{longtable}[t!]{p{0.1\textwidth}|p{0.288\textwidth}|p{0.28\textwidth}|p{0.22\textwidth}}
\caption{Representative applications of diffusion models.} \\
\toprule
\textbf{Task} & \textbf{Datasets} & \textbf{Metrics} & \textbf{Articles} \\
\midrule
\endfirsthead
\caption[]{(Continued)} \\
\toprule
\textbf{Task} & \textbf{Datasets} & \textbf{Metrics} & \textbf{Articles} \\
\midrule
\endhead
\midrule
\multicolumn{4}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\endlastfoot

Image\newline Generation &
{ImageNet, CIFAR, MetFace,\newline CelebA HQ, MS COCO, UCI,\newline FFHQ, DiffusionDB, AFHQ,\newline LSUN, SYSTEM-X, LAION}
& {FID, sFID, IS, NLL, MSE,\newline CLIP Score, PSNR, LPIPS,\newline MACs, CS, PickScore,\newline SA, Score Matching Loss}
& \cite{liu2022flow},\newline \cite{liu2023instaflow},\newline \cite{yan2024perflow},\newline \cite{lee2024improving},\newline \cite{zhu2024slimflow}, etc. \\
\hline
Video\newline Generation 
&  {MSR-VTT, InternVid,\newline WebVid-10M, LAION,\newline UCF-101, CGCaption}
&  {FID, IS, FVD, IQS, NIQE,\newline CLIPSIM, B-FVD-16}
& \cite{zhang2023adadiff},\newline \cite{liu2023ed},\newline \cite{xing2024simda},\newline \cite{wang2023videolcm},\newline \cite{lee2024grid}, etc. \\
\hline
Audio\newline Generation 
& {SC09, LJSpeech,\newline Speech Commands
}  
& {MOS, FID, IS,\newline mIS, AM Score}
& \cite{chen2020wavegrad},\newline \cite{kong2020diffwave},\newline
\cite{kong2021fast}, etc.
\\
\hline
Text\newline Generation 
& {XSUM, Semantic Content,\newline CCD, IWSLT14, WMT14,\newline ROCStories, E2E, QQP,\newline Wiki-Auto, Quasar-T,\newline  AG News Topic}
& {Rouge, Semantic Acc, Mem,\newline BLEU, Div, BERTScore,\newline SacreBLEU, MAUVE Score,\newline Content Fluency, POS}
& \cite{chen2023cheaper},\newline \cite{yuan2024text},\newline \cite{chen2022analog},\newline \cite{lovelace2024latent}, etc. \\
\hline
3D \newline Generation
& {BraTS2020, ShapeNet,\newline Objaverse, Cap3D, LLFF,\newline HumanML3D, AMASS,\newline KIT, HumanAct12, IBRNet,\newline Instruction-NeRF2NeRF}
& {Dice, HD95, CD, EMD,\newline  1-NNA, COV, CLIP,\newline Aesthetic, Similarity,\newline R-Precision, FID, DIV,\newline MM-Dist, ACC, Diversity,\newline MModality}
& \cite{bieder2023memory},\newline \cite{mo2024efficient},\newline \cite{li2024dual3d},\newline \cite{park2023ed},\newline \cite{yu2024boostdream}, etc. \\
\end{longtable}
process but also reduce computational costs while ensuring that audio quality remains high, meeting the demands of real-time audio generation applications.

\subsubsection{3D Generation}
\label{sec:3d_generation}
% Memory-Efficient 3D Denoising Diffusion Models for Medical Image Processing -> sampling scheduling.timestep schedule
% Dual3D: Efficient and Consistent Text-to-3D Generation with Dual-mode Multi-view Latent Diffusion -> sampling scheduling.timestep schedule
% BoostDream: Efficient Refining for High-Quality Text-to-3D Generation from Multi-View Diffusion -> sampling scheduling.parallel sampling
% EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion Generation -> sampling scheduling.partial sampling
% Efficient 3D Shape Generation via Diffusion Mamba with Bidirectional SSMs -> arch maybe
% VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder -> arch maybe
% ED-NeRF: Efficient Text-Guided Editing of 3D Scene with Latent Space NeRF -> compression.distillation.
% \cite{bieder2023memory}, \cite{mo2024efficient}, \cite{li2024dual3d}, \cite{park2023ed}, \cite{yu2024boostdream}, \cite{tang2023volumediffusion}, \cite{zhou2023emdm}

As a technique closely aligned with real-world representation, 3D generation holds substantial promise across various sectors, including medical imaging, motion capture, asset production, and scene reconstruction, etc. However, when compared to 2D image generation, distinctive high-resolution elements such as volumetric data or point clouds present unique challenges, significantly escalating computational demands. Several efficient methodologies~\citep{bieder2023memory, zhou2023emdm, tang2023volumediffusion, park2023ed} have been proposed, particularly concentrating on enhancing the sampling process and optimizing the architectural framework, which further handles the computational complexity inherent.
%
One of the most prevalent approaches involves designing more efficient sampling schedules~\citep{bieder2023memory, li2024dual3d, yu2024boostdream, zhou2023emdm}. By utilizing larger sampling step sizes, modifying the sampling strategy between 2D and 3D, or incorporating multi-view parallelism, these techniques address the key bottlenecks in the sampling process, thereby improving sampling efficiency. Moreover, the incorporation of novel architectures, such as state-space models and lightweight feature extractors~\citep{mo2024efficient, tang2023volumediffusion}, alleviates the computational burden of processing 3D data, significantly enhancing model efficiency.

% detailed explanations of each paper
% For medical imaging, ~\citet{bieder2023memory} introduces a patch-based training method (PatchDDM) that reduces memory usage by training on small patches instead of full 3D volumes, while enabling full-resolution inference. Additionally, it optimizes the U-Net architecture by replacing concatenation with averaging in skip connections, further enhancing memory efficiency.
% % : EMDM
% ~\citet{zhou2023emdm} uses larger sampling step sizes, reducing the number of required diffusion steps while maintaining quality through a conditional denoising diffusion GAN.
% 
% \citet{mo2024efficient} utilizes the state space model, reducing the complexity to linear while processing voxel data.
% %
% \citet{li2024dual3d} proposed a dual-mode toggling inference strategy to reduce the sampling step, which alternates between the 2D mode and 3D mode at a set frequency. Typically, only one out of every 10 steps employs 3D mode for denoising. This approach ensures 3D asset consistency while significantly reducing inference time.
% %
% \citet{park2023ed} proposes an improved loss function tailored for editing by migrating the delta denoising score (DDS) distillation loss, originally used in 2D image editing to the three-dimensional domain.
% %
% In \citet{yu2024boostdream}, a multi-view refining approach and consequential SDS loss are conducted in parallel, reducing training iteration.
% % 
% \citep{tang2023volumediffusion} designs a lightweight network that efficiently extracts feature volumes from multi-view images in a single forward. Additionally, the on-the-fly feature volume extraction eliminates costly storage, allowing for rapid processing and scaling during model training.


% \begin{longtable}{p{0.15\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}|p{0.2\textwidth}}
% \caption{Application of Diffusion Models} \\
% \toprule
% \textbf{Task} & \textbf{Article} & \textbf{Datasets} & \textbf{Metrics} \\
% \midrule
% \endfirsthead
% \caption[]{(Continued)} \\
% \toprule
% \textbf{Task} & \textbf{Articles} & \textbf{Datasets} & \textbf{Metrics} \\
% \midrule
% \endhead
% \midrule
% \multicolumn{4}{r}{\textit{Continued on next page}} \\
% \endfoot
% \bottomrule
% \endlastfoot

% Image Generation &
% \cite{liu2022flow}, \cite{liu2023instaflow}, \cite{yan2024perflow}, \cite{lee2024improving}, \cite{zhu2024slimflow}
%     % \cite{liu2022flow}, \cite{liu2023instaflow}, \cite{yan2024perflow}, \cite{lee2024improving}, \cite{zhu2024slimflow}, \cite{shang2023post}, \cite{li2023q}, \cite{kim2024leveraging}, \cite{hang2024improved}, \cite{li2024immiscible}, \cite{pokle2022deep}, \cite{tang2024accelerating}, \cite{pokle2022deep}, \cite{shih2024parallel}, \cite{li2024distrifusion}, \cite{wang2024pipefusion}, \cite{agarwal2024approximate}, \cite{ma2024deepcache}, \cite{wimbauer2024cache}, \cite{ma2024learning}, \cite{selvaraju2024fora}, \cite{song2020sliced}, \cite{song2019generative}, \cite{song2021maximum}, \cite{dockhorn2021score}, \cite{zhang2021diffusion}, \cite{xue2024unifying}, \cite{xue2024sa}, \cite{lu2022dpm}, \cite{zhang2022fast}, \cite{guo2024gaussian}, \cite{zheng2023improved}, \cite{kim2022denoising}, \cite{watson2021learning}, \cite{sabour2024align}, \cite{meng2023distillation}, \cite{li2024snapfusion}, \cite{moonsimple}, \cite{blattmann2022semi}, \cite{sheyninknn}, \cite{chenre}, \cite{zhang2023redi}, \cite{chen2023speed}, \cite{choi2024stable}
% & {ImageNet, CIFAR-10, LSUN Bedroom, LSUN Church, CelebA HQ, AFHQ, MetFace, MS COCO 2017, MS COCO 2014, LAION-5B-Aesthetics, FFHQ, DiffusionDB, SYSTEM-X, UCI, LSUN-bedroom}
% & {FID, sFID, IS, NLL, CLIP Score, MSE, Precision and recall, PSNR, LPIPS, MACs, FID-10k, CS, PickScore, SA, Score Matching Loss} \\
% \hline
% Video Generation 
% &  ~\cite{zhang2023adadiff}, ~\cite{liu2023ed}, ~\cite{xing2024simda}, ~\cite{wang2023videolcm}, ~\cite{lee2024grid}
% &  {MSR-VTT, InternVid, WebVid-10M, LAION-5B, UCF-101, CGCaption}
% &  {FID, IS, FVD, IQS, CLIP, NIQE, FVD, CLIPSI, B-FVD-16} \\
% \hline
% Audio Generation 
% & \cite{kong2021fast}, \cite{leepriorgrad} 
% & {SC09, LJSpeech}  
% & {MOS, LS-MAE, MR-STFT, MCD, F0 RMSE} \\
% \hline
% Text Generation 
% & \cite{chen2023cheaper}, \cite{yuan2024text}, \cite{lovelace2024latent}
% & {E2E, QQP, Wiki-Auto, Quasar-T, CCD, IWSLT14, WMT14, ROCStories, AG News Topic Classification, XSUM}
% & {Semantic Content, POS, Syntax Tree, Syntax Spans, Length, Semantic Acc, Content Fluency, BLEU, BERTScore, dist.1, SacreBLEU, MAUVE Score, Ppl, Div, Mem, Rouge} \\
% \hline
% 3D Generation
% & \cite{bieder2023memory}, \cite{mo2024efficient}, \cite{li2024dual3d}, \cite{park2023ed}, \cite{yu2024boostdream}, \cite{tang2023volumediffusion}, \cite{zhou2023emdm}
% & {BraTS2020, ShapeNet, Objaverse, Cap3D, LAION, LLFF, IBRNet, Instruction-NeRF2NeRF, HumanML3D, AMASS, KIT, HumanAct12}
% & {Dice, HD95, CD, EMD, 1-NNA, COV, CLIP, Aesthetic, CLIP, Similarity, R-Precision, FID, DIV, MM-Dist, ACC, Diversity, MModality} \\


% \end{longtable}
