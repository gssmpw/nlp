\section{Algorithm-Level Efficiency Optimization}
\label{sec:algorithm}
\subsection{Efficient Training}
\label{sec:efficient_training}

Efficient training techniques focus on reducing the costs of the DM pre-training process in terms of compute resources, training time, memory and energy consumption. As summarized in Figure~\ref{fig:efficient training}, enhancing the efficiency of pre-training can be achieved through different and complementary techniques, including noise schedule, score matching, data-dependent adaptive priors, and rectified flow.


\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]
\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=18em,font=\normalsize,}{},
            where level=2{text width=15em,font=\normalsize,}{},
            where level=3{text width=24em,font=\normalsize,}{},
            [
                \textbf{Efficient Training}, ver
                    [
                       \textbf{Noise Schedule}, fill=blue!10
                            [
                                \textbf{Fixed Noise Schedule}, fill=blue!10
                                [
                                DDPM~\citep{ho2020denoising}{,}
                                IDDPM~\citep{nichol2021improved}{,}
                                Laplace~\citep{hang2024improved}{,}
                                \\Masked-Diffusion LM~\citep{chen2023cheaper}, leaf, text width=45em
                                ]
                            ]
                            [ 
                                \textbf{Adaptive Noise Schedule}, fill=blue!10
                                [                            
                                DDIM~\citep{song2020denoising}{,}
                                ResShift~\citep{yue2024resshift}{,} Immiscible Diffusion~\citep{li2024immiscible}{,}
                                \\DiffuSeq~\citep{gong2022diffuseq}{,}
                                SeqDiffuSeq~\citep{yuan2024text}, leaf, text width=45em
                                ]
                            ]
                    ]
                    [
                        \textbf{Score Matching}, fill=blue!10
                        [
                            Score Matching~\citep{hyvarinen2005estimation}{,}
                            Sliced Score Matching~\citep{song2020sliced}{,}
                            NCSN~\citep{song2019generative}{,}
                            \\Likelihood Weighting~\citep{song2021maximum}{,}
                            CLD~\citep{dockhorn2021score}, leaf, text width=61.7em
                        ]
                    ]
                    [
                        \textbf{Data-Dependent Adaptive Priors}, fill=blue!10
                        [
                        PriorGrad~\citep{leepriorgrad}{,}
                        Digress~\citep{vignacdigress}{,}
                        DecompDiff~\citep{guan2023decompdiff}{,}
                        Leapfrog DM~\citep{mao2023leapfrog}, leaf, text width=61.7em
                        ]
                    ]
                    [
                        \textbf{Rectified Flow}, fill=blue!10
                         [
                            Rectified Flow~\citep{liu2022flow, liu2022rectified}{,}
                            InstaFlow~\citep{liu2023instaflow}{,}
                            PeRFlow~\citep{yan2024perflow}{,}
                            \\2-Rectified Flows++~\citep{lee2024improving}{,}
                            SlimFlow~\citep{zhu2024slimflow}, leaf, text width=61.7em
                         ]
                    ]
            ]
        \end{forest}
 }
    \caption{Summary of efficient training techniques for diffusion models.}
    \label{fig:efficient training}
\end{figure*}

\subsubsection{Noise Schedule}
% Denoising Diffusion Implicit Models
% Improved Denoising Diffusion Probabilistic Models
% Improved Noise Schedule for Diffusion Training
% A Cheaper and Better Diffusion Language Model with Soft-Masked Noise*
% ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting*
% Immiscible Diffusion-Accelerating Diffusion Training with Noise Assignment
% DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models
% Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation
Noise schedule is a crucial component of diffusion models, governing how noise is added at each step of the forward process and how it is removed during the reverse process. Denoising Diffusion Probabilistic Models (DDPM)~\citep{ho2020denoising} first proposed a linear noise schedule that gradually decreases the variance of the noise added in the forward process in~ Eq.(\ref{eq:forward_process}). However, despite DDPM demonstrating significant potential in diffusion model tasks, this linear schedule requires calculating complex noise terms and iterating through numerous timesteps, posing challenges to noise injection efficiency and highlighting the need for an efficiency-enhanced noise schedule design. Current studies still face prolonged iterative processes, significantly hindering diffusion speed, while several studies have concentrated on designing the highlight of noise schedules. As shown in Figure~\ref{fig:noise_schedule}, efficient noise schedules can be classified into fixed noise schedule and adaptive noise schedule.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Noise_Shcedule.pdf}
    \vspace{-10pt}
    \caption{Illustration of two categories of noise schedules.}
    \label{fig:noise_schedule}
    \vspace{-10pt}
\end{figure}

\noindent \textbf{Fixed Noise Schedule.}
%
Fixed noise schedules refer to a technique where noise is systematically added to a model's training process at predefined intervals or according to specific levels. DDPM~\citep{ho2020denoising} uses a linear noise schedule, serving as a classic example of fixed noise schedules, with noise variance changing deterministically over time.
%
Based on this, the Improved Denoising Diffusion Probabilistic Model (IDDPM)~\citep{nichol2021improved} introduces significant innovations in the noise schedule, replacing the linear noise schedule of the original DDPM with a cosine noise schedule, which could be written as
\begin{equation}
\beta_t = 1 - \frac{\cos\left(\frac{t / T + s}{1 + s} \cdot \frac{\pi}{2}\right)}{\cos\left(\frac{s}{1 + s} \cdot \frac{\pi}{2}\right)}
\end{equation}
where \(t\) is the current timestep, \(T\) is the total number of timesteps, and \(s\) is a small positive constant typically used to smooth the initial noise addition. The cosine noise schedule helps the model achieve high-quality sample generation by optimizing the noise distribution throughout the training process, allowing the model to generate samples with a reduced number of sampling steps while preserving essential structural information at each stage. However, the cosine noise schedule allocates computational resources evenly across the entire range of noise intensities. This means that both high and low noise samples consume significant resources, even though they may not be the most critical for training the model.

To address the inefficiency in resource allocation, ~\citep{hang2024improved} proposed a fixed noise schedule, called Laplace. This schedule aims to further optimize the training of the diffusion model by increasing the sampling frequency around critical regions. This approach ensures that the model receives more training samples in these crucial regions, thereby improving overall training efficiency and sample quality. By redesigning the fixed noise schedule, more computational resources are concentrated on medium noise intensities. These medium noise samples are more important for model learning, as they effectively train the model to understand the data structure and remove noise. Laplace effectively balances noise addition across all time steps, ensuring a more robust and consistent training process, and ultimately resulting in higher-quality generated images. 

Lastly, existing diffusion models have employed a uniform addition of Gaussian noise to each word to facilitate the diffusion process for text generation. However, this approach has been shown ta failure to leverage the linguistic features of the text in question, while simultaneously increasing the computational burden. 
%
Furthermore, ~\citep{chen2023cheaper} addresses this issue by gradually introducing noise through a soft-masking noise strategy. This model determines the masking order based on word importance, which is measured by term frequency and information entropy. The model employs a square-root noise schedule\citep{li2022diffusion} to incrementally increase the noise level, thereby stabilizing the training process. Consequently, the model gradually adds noise to the initial latent variable \(X_0\), resulting in a series of noisy latent variables \(X_{1:T}\).

\noindent \textbf{Adaptive Noise Schedule.}
Different from the fixed noise schedules, adaptive noise schedules include methods that dynamically adjust the noise schedule based on the state of the model or data. The key idea is to adapt the noise schedule in response to specific conditions or states during the diffusion process. ~\citet{song2020denoising} developed Denoising Diffusion Implicit Models (DDIM), which improve the noise schedule in DDPM by introducing non-Markovian forward processes. as follows:
\begin{equation}
    x_{t-1} = \sqrt{\alpha_{t-1}} \left( x_t - \sqrt{1 - \alpha_t} \epsilon_\theta(x_t) \right) + \sqrt{1 - \alpha_{t-1} - \sigma_t^2} \epsilon_\theta(x_t) + \sigma_t \epsilon_t
\end{equation}
where \(\alpha_t\) is a decreasing schedule controlling how noise is added over time. They propose a dynamic method for adjusting the noise level, \(\sigma_t\), at each step. The noise level is calculated as a function of both the current state, \(x_t\), and the initial state, \(x_0\), leveraging the entire trajectory rather than treating each step independently. This dynamic adjustment fully utilizes the information from the initial and current states, allowing for more accurate control of the diffusion process. As a result, the quality of the generated samples is enhanced, and the number of sampling steps required is reduced.  

Inspired by DDIM, ~\cite{yue2024resshift} proposed ResShift, introducing a novel noise schedule that constructs a much shorter Markov chain. The key idea is to shift the residual between the high-resolution (HR) and low-resolution (LR) images instead of adding Gaussian noise. This approach allows the model to start from an LR image and iteratively refine it to produce the HR image. The noise schedule also involves a hyperparameter $\kappa$, which controls the overall noise intensity during the transition. The mathematical formulation of this noise schedule is as follows:
\begin{equation}
\sqrt{\eta_t} = \sqrt{\eta_1} \times b_0^{\beta_t}, \quad t = 2, \ldots, T-1
\end{equation}
\text{where}
\begin{equation}
\beta_t = \left(\frac{t - 1}{T - 1}\right)^p \times (T - 1), \quad b_0 = \exp\left(\frac{1}{2(T-1)} \log \frac{\eta_T}{\eta_1}\right)
\end{equation}
where \( T \) is the total number of timesteps, \( t \) is the current timestep, \( p \) is a hyperparameter controlling the growth rate of \( \sqrt{\eta_t} \), and \( \eta_1 \) and \( \eta_T \) represent the initial and final noise levels, respectively. This formula enables a non-uniform geometric progression of noise levels. Thus ResShift can achieve high-quality SR results with only 15 sampling steps.
%
Conventional methodologies tend to diffuse each image across the entirety of the noise space, thereby resulting in a composite of all images at each point within the noise layer. In light of the aforementioned, ~\cite{li2024immiscible} proposed Immiscible Diffusion, a novel approach inspired by the physical phenomenon of immiscibility. In contrast to conventional methodologies, Immiscible Diffusion involves reassigning noise to images in a way that minimizes the distance between image-noise pairs within a mini-batch, ensuring that each image is only diffused to nearby noise points. This method ensures that each image is matched only with nearby noise, thereby reducing the difficulty of denoising.
%
Previous studies ~\cite{gong2022diffuseq} on text generation have employed fixed noise schedules and encoder-only Transformer architectures. This approach necessitated the recalculation of the input sequence at each time step, resulting in the inefficient utilization of computational resources and the generation of outputs at a slow pace. In contrast, ~\cite{yuan2024text} introduces an adaptive noise scheduling technique that enables the dynamic adjustment of the noise level at each time step and token position. In particular, this technique entails recording loss values at each timestep throughout the training phase, with a linear interpolation subsequently employed to map these losses to the corresponding noise schedule parameters.

\subsubsection{Score Matching}
% Estimation of non-normalized statistical models by score matching.
% Sliced Score Matching: A Scalable Approach to Density and Score Estimation 
% Generative Modeling by Estimating Gradients of the Data Distribution
% Maximum Likelihood Training of Score-Based Diffusion Models
% Score-Based Generative Modeling with Critically-Damped Langevin Diffusion (With Fig* 
Score matching, introduced by~\citet{hyvarinen2005estimation}, serves as an effective approach for estimating unnormalized models by minimizing the Fisher divergence between the score function of data distribution \( \boldsymbol{s}_d(\mathbf{x}) = \nabla_{\mathbf{x}} \log p_d(\mathbf{x}) \) and the score function of model distribution \( \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) = \nabla_{\mathbf{x}} \log p_m(\mathbf{x}; \boldsymbol{\theta}) \). This approach avoids the need to compute the intractable partition function  \( \boldsymbol{Z}_{\boldsymbol{\theta}} \), a common problem in Maximum Likelihood Estimation (MLE). 

While DDPM directly optimizes the noise prediction in Eq.(\ref{ddpm_loss}), score matching objectives can directly be estimated on a dataset and optimized with stochastic gradient descent, the loss function for score matching takes a different approach formulated as follows:
\begin{equation}\label{score_matching}
L(\boldsymbol{\theta}) = \frac{1}{2} \mathbb{E}_{p_d(\mathbf{x})} \left[ \| \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) - \boldsymbol{s}_d(\mathbf{x}) \|^2 \right]
\end{equation}
Since it typically does not have access to the true score function of the data \( \boldsymbol{s}_d(\mathbf{x}) \),~\citet{hyvarinen2005estimation} introduced integration by parts as \(L(\boldsymbol{\theta}) = J(\boldsymbol{\theta}) + C\) to derive an alternative expression that does not require direct access to \( \mathbf{x}_d(\mathbf{x}) \):
\begin{equation}
J(\boldsymbol{\theta}) = \mathbb{E}_{p_d(\mathbf{x})} \left[ \text{tr}(\nabla_{\mathbf{x}} \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta})) + \frac{1}{2} \| \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) \|^2 \right]
\end{equation}
In this expression, \( \text{tr}(\cdot) \) denotes the trace of the Hessian matrix of \( \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) \). The constant \( C \) is independent of \( \boldsymbol{\theta} \) and can be ignored for optimization purposes. The final form of the unbiased estimator used for training is:
\begin{equation}
\label{scorematchingfinal}
\hat{J}(\boldsymbol{\theta}; \mathbf{x}_1^N) = \frac{1}{N} \sum_{i=1}^N \left[ \text{tr}(\nabla_{\mathbf{x}} \boldsymbol{s}_m(\mathbf{x}_i; \boldsymbol{\theta})) + \frac{1}{2} \| \boldsymbol{s}_m(\mathbf{x}_i; \boldsymbol{\theta}) \|^2 \right]
\end{equation}
Compared to DDPM's straightforward optimization in Eq.(\ref{ddpm_loss}), although score matching Eq.(\ref{scorematchingfinal}) avoids the computation of the partition function \( \boldsymbol{Z}_{\boldsymbol{\theta}} \), it still faces computational challenges, particularly in high-dimensional data. The computation of the trace of the Hessian matrix substantially increases the complexity as the dimensionality grows. Specifically, computing the trace requires many more backward passes than the gradient, making score matching computationally expensive for high-dimensional data.

To address this issue,~\citet{song2020sliced} observed that one-dimensional problems are typically much easier to solve than high-dimensional ones. Inspired by the idea of the Sliced Wasserstein Distance~\citep{rabin2012wasserstein}, they proposed Sliced Score Matching. The core idea of sliced score matching is to project both the score function of the model \( \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) \) and the data \( \boldsymbol{s}_d({\mathbf{x}}) \) onto a random direction \(\mathbf{v}\), and compare the differences along that direction. The sliced score matching objective is defined as:
\begin{equation}
L(\boldsymbol{\theta}; p_\mathbf{v}) = \frac{1}{2} \mathbb{E}_{p_{\mathbf{v}}} \mathbb{E}_{p_d(\mathbf{x})} \left[ \left( \mathbf{v}^\top \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) - \mathbf{v}^\top \boldsymbol{s}_d({\mathbf{x}}) \right)^2 \right]
\end{equation}
To eliminate the dependence on \( \boldsymbol{s}_d({\mathbf{x}}) \), integration is applied by parts, similar to traditional score matching, resulting in the following form:
\begin{equation}
J(\boldsymbol{\theta}; p_\mathbf{v}) = \mathbb{E}_{p_\mathbf{v}} \mathbb{E}_{p_d(\mathbf{x})} \left[ \mathbf{v}^\top \nabla_{\mathbf{x}} \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}) \mathbf{v} + \frac{1}{2} (\mathbf{v}^\top \boldsymbol{s}_m(\mathbf{x}; \boldsymbol{\theta}))^2 \right]
\end{equation}
which achieves scalability by reducing the complexity of the problem by projecting high-dimensional score functions onto low-dimensional random directions, thereby avoiding the full Hessian computation.
%
% Building upon sliced score matching, to address the issue of inaccurate score estimation in low data density regions, \citet{song2019generative} then introduces a novel diffusion model that employs Langevin dynamics to produce samples based on estimated gradients of the data distribution. They extend this framework by introducing Noise Conditional Score Networks (NCSN), which are designed to jointly estimate the scores of all perturbed data distributions across varying noise levels.
% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=0.45\textwidth]{figures/score_matching.pdf}
%     \caption{Illustration of the NCSN training process. A single score network \(s_\theta(\mathbf{x},\sigma)\) is trained to estimate scores for different noise levels simultaneously. Given the original data distribution \(p_{\text{data}}(\mathbf{x})\), the network learns to handle multiple perturbed distributions \(p_{\sigma_i}(\mathbf{x})\), from concentrated distributions close to the data manifold \(p_{\sigma_1}\) to smoothed distributions that fill low-density regions \(p_{\sigma_3}\).}
%     \vspace{-20pt}
%     \label{fig:scorematching}
% \end{wrapfigure}
\begin{wrapfigure}{r}{0.3\textwidth}
    \centering
    \includegraphics[width=0.3\textwidth]{figures/Data_Prior.pdf}
    \caption{Illustration of data-dependent adaptive priors for diffusion processes across different modalities, demonstrating how tailored priors improve generate on quality.}
    \label{fig:data_prior_draw}
    \vspace{0pt}
\end{wrapfigure}
While effective for dimensionality reduction, score estimation still faces challenges in low data density regions where data samples are sparse.
Building upon sliced score matching, to address the issue of inaccurate score estimation in low data density regions,~\citet{song2019generative} introduces a novel generative framework that employs Langevin dynamics to produce samples based on estimated gradients of the data distribution \(p_{\text{data}}(\mathbf{x})\). They proposed Noise Conditional Score Networks (NCSN) \(s_\theta(\mathbf{x},\sigma)\), which jointly estimate scores across multiple noise-perturbed data distributions. By conditioning on a geometric sequence of noise levels \(\sigma_3 > \sigma_2 > \sigma_1\), a single network learns to estimate scores for distributions ranging from highly smoothed \(p_{\sigma_3}(\mathbf{x})\) that fill low-density regions to concentrated \(p_{\sigma_1}(\mathbf{x})\) that preserve the structure of the original data manifold. This unified training approach enables robust score estimation across the entire data space.

Different from sliced score matching,~\citet{song2021maximum} introduces a new weighting scheme called likelihood weighting, which allows the weighted combination of score matching losses to actually bound the negative log-likelihood of the diffusion models. Compared with traditional score matching, it focuses on adjusting the training objective. This modification enables approximate maximum likelihood training, rather than merely minimizing the score matching loss, which improves the model's likelihood across various datasets, stochastic processes, and architectures, which effectively combines the training efficiency of score matching with the advantages of maximum likelihood estimation. 
%
Following a similar derivation, as \citet{song2021maximum}, \citet{dockhorn2021score} introduces Coupled Langevin Dynamics (CLD), redefining the score matching objective within the CLD framework. Unlike traditional score matching methods that inject noise directly into the data space, CLD simplifies the task by only requiring the model to learn the score of the conditional distribution \( p_t(v_t \mid x_t) \), where noise is injected into an auxiliary variable \( v_t \) coupled with the data.



\subsubsection{Data-Dependent Adaptive Priors}
% PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Dependent Adaptive Prior
% DiGress: Discrete Denoising diffusion for graph generation
% DECOMPDIFF: Diffusion Models with Decomposed Priors for Structure-Based Drug Design
% Leapfrog diffusion model for stochastic trajectory prediction


To accelerate the generation process in diffusion models and improve the quality of generated samples, better initialization of priors tailored to specific tasks and datasets can be utilized. This approach leverages data-dependent adaptive priors, making the generation process more efficient and aligning the generated samples more closely with true data distribution. Recent studies have explored how data-dependent adaptive priors can enhance diffusion models.

As illustrated in Figure~\ref{fig:data_prior_draw}, data-dependent adaptive priors can be applied across different modalities, such as speech, graph, and trajectory, to enhance the diffusion process. By aligning priors with data distributions specific to each modality, the model can generate outputs that are better tailored to the underlying structure of the data. In traditional diffusion models, the prior is typically assumed to be a standard Gaussian distribution \( p(z) = \mathcal{N}(0, I) \). However, this may not always align well with the actual data distribution, potentially introducing inefficiencies. By leveraging data-dependent adaptive priors based on \( X \), where \( X \) represents the data, the model can achieve better initialization and faster convergence, without relying solely on a standard Gaussian assumption.
\citet{leepriorgrad} introduce PriorGrad, which improves diffusion models for speech synthesis by using an adaptive prior derived from data statistics based on conditional information. This method increases the efficiency of the denoising process, leading to faster convergence and inference speed, while also improving perceptual quality and robustness with smaller network capacities.

Digress~\citep{vignacdigress} presents a discrete denoising diffusion model focused on graph generation. This model leverages data-dependent priors to better capture the discrete nature of graph data, significantly enhancing the quality of generated graphs, particularly for applications such as chemical molecular structures and social networks.
%In drug design, 
DecompDiff~\citep{guan2023decompdiff} introduce decomposed priors, modeling different structural components of drug molecules separately to improve diffusion models. This approach enhances the accuracy of generating viable drug candidates by better capturing molecular structure information. 
As illustrated in Figure~\ref{fig:ddap_led}, \citet{mao2023leapfrog} propose the Leapfrog Diffusion Model for stochastic trajectory prediction, introducing a leapfrog initializer that uses adaptive priors to skip multiple denoising steps. This method significantly accelerates inference while maintaining high prediction accuracy, making it useful for real-time applications such as autonomous driving and robotic navigation.

\begin{figure}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.7\textwidth]{figures/ddap_led.pdf}
    \caption{The Leapfrog diffusion model~\citep{mao2023leapfrog} accelerates inference by using a leapfrog initializer to approximate the denoised distribution, replacing extended denoising sequences while preserving representation capacity.}
    \label{fig:ddap_led}
    \vspace{-10pt}
\end{figure}

\subsubsection{Rectified Flow}
% Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow
% InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation
% Rectified Flow: A Marginal Preserving Approach to Optimal Transport
% Improving the Training of Rectified Flows
% Diffusion Models with Deterministic Normalizing Flow Priors 
% PeRFlow: Piecewise Rectified Flow as Universal Plug-and-Play Accelerator
% SlimFlow: Training Smaller One-Step Diffusion Models with Rectified Flow
% stop here
\begin{wrapfigure}{r}{0.3\textwidth}
  \centering
  % \vspace{-20pt}
  \includegraphics[width=0.3\textwidth]{figures/rectified_flow.pdf} 
  \caption{Illustration of the rectified flow.}
  \label{fig:rectified_flow}
  \vspace{-10pt}
\end{wrapfigure}
As illustrated in Figure~\ref{fig:rectified_flow}, Rectified Flow, proposed by~\citep{liu2022flow, liu2022rectified}, introduces a method for training ordinary differential equation (ODE) models by learning straight transport paths between two distributions, $\pi_0$ and $\pi_1$.
The key idea is to minimize the transport cost by ensuring that the learned trajectory between these two distributions follows the most direct route, which can be computationally efficient to simulate. Unlike traditional diffusion models, which may follow roundabout paths, Rectified Flow leverages a simpler optimization problem to create a straight flow, thereby improving both training efficiency and the quality of the generated samples.

Building upon this foundation, InstaFlow~\citep{liu2023instaflow} apply the Rectified Flow concept to text-to-image generation, achieving a significant breakthrough. InstaFlow represents a major advancement in efficient diffusion models, which are capable of high-quality image generation in just one step. It applied Rectified Flow to large-scale datasets and complex models like Stable Diffusion, introduced a novel text-conditioned pipeline for one-step image generation, and achieved an FID score of 23.3 on MS COCO 2017-5k. InstaFlow's success highlights the potential of Rectified Flow in dramatically reducing the computational cost of diffusion models while maintaining high output quality.

Following InstaFlow,~\citet{yan2024perflow} proposed PeRFlow, further extending the Rectified Flow concept to create a more flexible and universally applicable acceleration method. PeRFlow divides the sampling process into multiple time windows, applying the reflow operation to each interval, creating piecewise linear flows that allow for more nuanced trajectory optimization. Through carefully designed parameterizations, PeRFlow models can inherit knowledge from pretrained diffusion models, achieving fast convergence and superior transfer ability. This approach positions PeRFlow as a universal plug-and-play accelerator compatible with various workflows based on pretrained diffusion models. While Rectified Flow showed great promise, there was still room for improvement, especially in low Number of Function Evaluations (NFE) settings.
%
Addressing this,~\citet{lee2024improving} focused on enhancing the training process of Rectified Flows. They discovered that a single iteration of the Reflow algorithm is often sufficient to learn nearly straight trajectories and introduced a U-shaped timestep distribution and LPIPS-Huber premetric to improve one-round training. These improvements led to significant enhancements in FID scores, particularly in low NFE settings, outperforming state-of-the-art distillation methods on various datasets.
%
Most recently,~\citet{zhu2024slimflow} proposed SlimFlow, a method designed to address the joint compression of inference steps and model size within the Rectified Flow framework, introducing Annealing Reflow to address initialization mismatches between large teacher models and small student models, and developing Flow-Guided Distillation to improve performance on smaller student models.


\subsection{Efficient Fine-Tuning}
\label{sec:efficient_finetuning}

\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]

\begin{figure*}[t]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=10em,font=\normalsize,}{},
            where level=2{text width=30em,font=\normalsize,}{},
            where level=3{text width=30em,font=\normalsize,}{},
            [
                \textbf{Efficient Fine-tuning}
                    [
                       \textbf{LoRA}, fill=blue!10
                            [
                                LoRA~\citep{hu2021lora}{,}
                                LCM-LoRA~\citep{luo2023lcm}{,}
                                \\Concept Sliders~\citep{gandikota2023concept}{,}
                                LoRA-Composer\\~\cite{yang2024lora}{,}
                                LoRA conditioning~\citep{choi2024simple},
                            ]
                    ]
                    [
                        \textbf{Adapter}, fill=blue!10
                            [
                                T2I-Adapter~\citep{mou2024t2i}{,}
                                IP-Adapter~\citep{ye2023ip}{,}
                                \\ParaTAA~\citep{tang2024accelerating}{,}
                                CTRL-Adapter\\~\citep{lin2024ctrl}{,}
                                SimDA~\citep{xing2024simda},
                            ]
                    ]
                    [
                        \textbf{ControlNet}, fill=blue!10
                            [
                                ControlNet~\citep{zhang2023adding}{,}
                                ControlNet++~\citep{li2025controlnet}{,}
                                \\ControlNet-XS~\citep{zavadski2023controlnetxs}{,}
                                ControlNeXt\\~\citep{peng2024controlnext}{,}
                                Uni-ControlNet~\citep{zhao2024uni}{,}
                                \\UniControl~\citep{qin2023unicontrol},
                            ]
                    ]
            ]
        \end{forest}
 }
    \caption{Summary of efficient fine-tuning techniques for diffusion models.}
    \label{fig:efficient fine-tuning}
\end{figure*}
Fine-tuning pre-trained diffusion models can be computationally expensive. To address this, we categorize efficient fine-tuning techniques into LoRA, Adapters, and ControlNet, based on their mechanisms for reducing resource consumption. This classification reflects differences in how these methods optimize parameters, enable task-specific adaptation, and incorporate spatial conditioning signals, as summarized in Figure~\ref{fig:efficient fine-tuning}.


\subsubsection{LoRA}
%LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS
%LCM-LORA: A UNIVERSAL STABLE-DIFFUSION ACCELERATION MODULE
%Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models
%LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models
%Simple Drop-in LoRA Conditioning on Attention Layers Will Improve Your Diffusion Model


Low-Rank Adaptation (LoRA)~\citep{hu2021lora} is a model adaptation method that maintains frozen pre-trained model weights while enabling efficient task adaptation through the injection of low-rank decomposition matrices into each Transformer layer. The core mathematical foundation of this approach lies in its representation of the weight update mechanism: for a pre-trained weight matrix $W_0 \in \mathbb{R}^{d\times k}$, LoRA represents the weight update as:
\begin{equation}
   W = W_0 + \Delta W, \text{ where } \Delta W = BA
\end{equation}
where $B \in \mathbb{R}^{d\times r}$ and $A \in \mathbb{R}^{r\times k}$ are trainable low-rank matrices, and the rank \( r \ll \min(d, k) \). During forward propagation, for an input $x \in \mathbb{R}^{k}$, the model computes the hidden representation $h \in \mathbb{R}^{d}$ as:
\begin{equation}
  h = W_0x + \Delta Wx = W_0x + BAx
\end{equation}
The complete process is illustrated in Figure~\ref{fig:lora}. A key advantage of this design lies in its deployment efficiency, where the explicit computation and storage of $W = W_0 + BA$ enables standard inference procedures without introducing additional latency.
Originally proposed for fine-tuning Large Language Models (LLMs), LoRA has demonstrated remarkable parameter efficiency and memory reduction in model adaptation. 
\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=\linewidth]{figures/lora.pdf}
    \caption{Illustration of~\citet{hu2021lora}'s reparameterization approach, where only parameters A and B are trained.}
    \label{fig:lora}
    \vspace{-10pt}
\end{wrapfigure}
While predominantly utilized in LLM fine-tuning, recent research has extended its application to diffusion models, indicating its potential as a versatile adaptation technique across different deep learning architectures. 
%
LCM-LoRA~\citep{luo2023lcm} proposes a universal acceleration approach for diffusion models. 
As shown in Figure~\ref{fig:lcm_lora}, this method achieves fast sampling by adding an Acceleration vector $\tau_{LCM}$ to the Base LDM~\cite{rombach2022high}. This module adopts LoRA~\citep{hu2021lora} to attach low-rank matrices to the original model without architectural modifications. For customized diffusion models that are fine-tuned for specific text-to-image generation tasks, the task-specific LoRA ($\tau'$) and acceleration LoRA ($\tau_{LCM}$) can be linearly combined through Eq.(\ref{eq:lcm}) to achieve fast inference while maintaining generation quality. More importantly, it provides a plug-and-play solution that reduces sampling steps from dozens to around 4, while maintaining compatibility with any pre-trained text-to-image diffusion model. 
\begin{equation}
   \tau'_{LCM} = \lambda_1\tau' + \lambda_2\tau_{LCM}
   \label{eq:lcm}
\end{equation}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.6\textwidth]{figures/Concept Sliders.pdf}
%     \caption{The framework of Concept Sliders, which utilizes LoRA adaptors to establish precise control over diffusion attributes~\citep{gandikota2023concept}.}
%     \label{fig:Concept_sliders}
% \end{figure}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/text_sliders.pdf}
%     \caption{Demonstration of continuous attribute control using text-guided LoRA sliders. Each row shows the progressive manipulation of a specific attribute ("Smiling", "Makeup", "Age") through the learned parameter space. The bidirectional control is achieved by moving along both positive and negative directions of the identified semantic axis~\citep{gandikota2023concept}.}
%     \label{fig:text_sliders}
% \end{figure}

Beyond the acceleration achieved by LCM-LoRA, Concept Sliders~\citep{gandikota2023concept} extends LoRA's application to precise control over image generation attributes. This method identifies low-rank directions in the diffusion parameter space corresponding to specific concepts through LoRA adaptation. The method freezes the original model parameters and trains a LoRA adapter to learn concept editing directions. Given an input $(x_t, c_t, t)$, where $x_t$ is the noisy image at timestep $t$.
\begin{figure}[h]
    \centering
    \includegraphics[width=.75\textwidth]{figures/lora_teaser.pdf}
    \caption{Illustration of LCM-LoRA~\citep{luo2023lcm}.}
    \label{fig:lcm_lora}
\end{figure}
For a target concept $c_t$, the model is guided through a score function to enhance certain attributes $c_+$ while suppressing others $c_-$. This training objective can be formulated as:
\begin{equation}
   \epsilon_{\theta^*}(x, c_t, t) \leftarrow \epsilon_{\theta}(x, c_t, t) + \eta(\epsilon_{\theta}(x, c_+, t) - \epsilon_{\theta}(x, c_-, t))
\end{equation}

where \(\epsilon_{\theta}\) represents the denoising model's prediction, and $\eta$ is a guidance scale. With this formulation, the method enables smooth control over concept strength through the guidance scale $\eta$ while maintaining concept independence in the learned directions. By leveraging LoRA's parameter-efficient nature, it achieves precise attribute manipulation with minimal computational overhead.
%

%
Besides, LoRA-Composer~\cite{yang2024lora} advances LoRA's application in diffusion models toward seamless multi-concept integration. While previous works focus on acceleration or single-concept control, this approach tackles the challenging task of combining multiple LoRA-customized concepts within a single image generation process. It combines multiple LoRAs in diffusion models by modifying the U-Net's attention blocks. Specifically, it enhances both cross-attention and self-attention layers within U-Net to enable direct fusion of multiple LoRAs. Compared to traditional methods like Mix-of-Show~\cite{gu2024mix} that require training a fusion matrix to merge multiple LoRAs, which increases computational overhead and may degrade generation quality. It directly combines multiple lightweight LoRAs through modified attention blocks, avoiding the overhead of retraining models.
%
While LoRA-Composer focuses on fusing multiple LoRAs for multi-concept control,~\citet{choi2024simple} explores the fundamental application of LoRA in attention layers.
Both these works enhance diffusion models by modifying the attention mechanism in U-Net. The latter proposes a structured conditioning approach in U-Net blocks with three key components: (1) conventional convolutional blocks using scale-and-shift conditioning for feature normalization adjustment, (2) attention blocks enhanced by LoRA adapters that condition both QKV computation and projection layers through learnable low-rank matrices, and (3) two LoRA conditioning implementations - TimeLoRA/ClassLoRA for discrete-time settings and UC-LoRA for continuous SNR settings, which utilize MLP-generated weights to combine multiple LoRA bases. Them method achieves improved performance over traditional conditioning while only increasing the parameter count by approximately 10\% through efficient low-rank adaptations in the attention layers.
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/dropin.pdf}
%     \caption{Overview of the proposed U-Net conditioning architecture~\citep{choi2024simple}}
%     \label{fig:dropin}
% \end{figure}
% \subsubsection{Adapter}
% T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models
% IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models
% CTRL-Adapter: Attribute-Based Control Adapter for Conditional Generation Tasks
% Simda: Simple diffusion adapter for efficient video generation
% X-adapter: Adding universal compatibility of plugins for upgraded diffusion model
% PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation
% Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models

\subsubsection{Adapter}
\begin{wrapfigure}{r}{0.5\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.5\textwidth]{figures/Adapter.pdf}
    \caption{Architecture of the Adapter module: demonstrating the integration of adapter layers within a transformer block to achieve efficient task adaptation by adding lightweight transformations, while keeping the core model weights frozen.}
    \label{fig:adapter_draw}
    \vspace{-15pt}
\end{wrapfigure}
Adapters are lightweight modules designed to enable efficient task adaptation by introducing small network layers into pre-trained models, allowing task-specific feature learning while keeping the original weights frozen. As illustrated in Figure~\ref{fig:adapter_draw}, adapter layers are placed within the transformer block, positioned between normalization and feed-forward layers. Each adapter module consists of a down-projection, nonlinearity, and up-projection, which generates task-specific transformations without altering the core model's structure. This design significantly reduces memory and computational requirements, making adapters well-suited for tasks requiring lightweight parameter updates, such as text-to-image generation (T2I) and simulated domain adaptation (SimDA).

T2I-Adapter~\citep{mou2024t2i} is an adapter designed to enhance control in text-to-image generation models by introducing conditional features such as sketches, depth maps, and semantic segmentation maps, allowing for structural guidance in generated images. Unlike approaches that require modifying the model’s core architecture, T2I-Adapter uses lightweight modules to incorporate external condition information into the generation process without altering the pre-trained model itself. This method improves the accuracy and consistency of generated images without increasing computational costs.
%
In implementation, T2I-Adapter employs convolutional and residual blocks to align conditional inputs with the spatial dimensions of intermediate features in the UNet model, thus capturing structural information at multiple scales. This integration allows T2I-Adapter to flexibly incorporate conditional features, such as sketches and depth maps, providing enhanced control over text-to-image generation. Such multi-adapter strategies are particularly suitable for tasks requiring high customization in image generation, enabling simultaneous input of various structural features to refine the output.

IP-Adapter~\citep{ye2023ip} enhances the consistency and visual quality of text-to-image generation by incorporating image prompts. Unlike T2I-Adapter~\citep{mou2024t2i}, which provides structural guidance through sketches or depth maps, IP-Adapter focuses on capturing visual details from an input image, making it ideal for tasks requiring high visual consistency with a reference image. This adapter processes the input image prompt into latent features, allowing the generation model to capture visual information from the target image and maintain detail alignment throughout the generation process. In its workflow, the image prompt is first mapped into the latent space and then processed through convolution and normalization modules within the adapter, enabling the model to utilize these features during inference. This setup enables the generation model to draw rich visual information from the image prompt, making IP-Adapter particularly suitable for tasks requiring high detail consistency, such as generating images with a style similar to the input image.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/IP_Adapter.pdf}
    \caption{Architecture of IP-Adapter~\citep{ye2023ip} using a decoupled cross-attention strategy, where only newly added modules are trained, and the pre-trained text-to-image model remains frozen.}
    \label{fig:ip_adapter}
    \vspace{-10pt}
\end{figure}
CTRL-Adapter~\citep{lin2024ctrl} is designed to enhance attribute control during generation by guiding specific attributes such as emotion or object type, enabling precise customization in generated results. Unlike T2I-Adapter~\citep{mou2024t2i} and IP-Adapter~\citep{ye2023ip}, which focus on structural and detail consistency respectively, CTRL-Adapter is tailored to provide diversity control for the generation model. For example, as illustrated in Figure~\ref{fig:ip_adapter}, the IP-Adapter architecture employs a decoupled cross-attention strategy, where only newly added modules are trained while the pre-trained text-to-image model remains frozen. In contrast, CTRL-Adapter can adjust the style of generated images based on specified emotions or object types, achieving controllable content generation without altering the core architecture of the model. This makes CTRL-Adapter particularly suitable for tasks requiring high customization in generation, such as emotion-driven text generation or stylized image synthesis.

SimDA~\citep{xing2024simda} is an adapter suited for cross-domain generation tasks, achieving domain adaptation by utilizing simulated data within the adapter to enhance the model's performance on previously unseen data distributions. Unlike CTRL-Adapter~\citep{lin2024ctrl}, which primarily focuses on attribute control, SimDA is designed to improve the model's generalization ability, allowing it to generate high-quality content even in unfamiliar data environments. SimDA is particularly useful in generation tasks that require domain transfer, such as adapting a model trained on one image dataset to perform well on another dataset. This enables the model to align with new data characteristics without compromising generation quality.

\subsubsection{ControlNet}
% ControlNet: Adding Conditional Control to Text-to-Image Diffusion Models
% ControlNet++: Improving Conditional Controls with Efficient Consistency Feedback
% UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild
% Uni-ControlNet: All-in-One Control to Text-to-Image Diffusion Models
% ControlNet-XS: Ret\input{sections/6_Frameworks}hinking the Control of Text-to-Image Diffusion Models as Feedback-Control Systems
% ControlNeXt: Powerful and Efficient Control for Image and Video Generation
ControlNet~\citep{zhang2023adding} and its derivatives represent a significant advancement in adding spatial conditioning controls to pre-trained text-to-image diffusion models. The original ControlNet architecture~\citep{zhang2023adding}, as illustrated in Figure~\ref{fig:controlnet}, presents a novel approach to integrating various spatial conditions—such as scribbles, edge maps, open-pose skeletons, or depth maps—into the generative process while preserving the robust features of pre-trained diffusion models. The architecture employs zero convolution layers that gradually develop parameters without disrupting the pre-trained model's stability. This design enables versatile conditioning, allowing the model to effectively leverage different types of spatial information. Through these conditioning methods, ControlNet demonstrates a remarkable ability to guide generation with fine-grained control over structure, style, and composition.
Building upon this foundation, several works have proposed improvements and alternatives. ControlNet++~\citep{li2025controlnet} addresses the challenge of alignment between generated images and conditional controls by introducing pixel-level cycle consistency optimization. Through a pre-trained discriminative reward model and an efficient reward strategy involving single-step denoised images, it achieves significant improvements in control accuracy, with notable gains in metrics such as mIoU (11.1\%), SSIM (13.4\%), and RMSE (7.6\%) across various conditioning types.
ControlNet-XS~\citep{zavadski2023controlnetxs} reimagines the control system by enhancing the communication bandwidth between the controlling network and the generation process. This redesign not only improves image quality and control fidelity but also significantly reduces the parameter count, resulting in approximately twice the speed during both inference and training while maintaining competitive performance in pixel-level guidance tasks.
The field has also seen efforts to unify multiple control capabilities. UniControl~\citep{qin2023unicontrol} introduces a task-aware HyperNet approach that enables a single model to handle diverse visual conditions simultaneously. Similarly, Uni-ControlNet~\citep{zhao2024uni} proposes a unified framework supporting both local controls and global controls through just two additional adapters, significantly reducing training costs and model size while maintaining high performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/controlnet.pdf}
    \caption{Illustration of ControlNet.}
    \label{fig:controlnet}
    \vspace{-10pt}
\end{figure}
Most recently, ControlNeXt~\citep{peng2024controlnext} has pushed the boundaries of efficiency even further by introducing a streamlined architecture that minimizes computational overhead. It replaces the traditional heavy additional branches with a more concise structure and introduces Cross Normalization (CN) as an alternative to zero convolutions. This approach achieves fast and stable training convergence while reducing learnable parameters by up to 90\% compared to previous methods.

\subsection{Efficient Sampling}
Efficient sampling in diffusion models addresses the computational challenges of the iterative denoising process while maintaining the quality of the generated samples through three main approaches. As illustrated in Figure~\ref{fig:efficient_sampling}, these encompass efficient SDE and ODE solvers, sampling scheduling strategies including parallel sampling and timestep optimization, and truncated sampling methods leveraging early exit and retrieval-based techniques. 
\label{sec:efficient_sampling}

\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]

\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=18em,font=\normalsize,}{},
            where level=2{text width=15em,font=\normalsize,}{},
            where level=3{text width=24em,font=\normalsize,}{},
            [
                \textbf{Efficient Sampling}, ver
                    [
                       \textbf{Efficient Solver}, fill=blue!10
                            [
                                \textbf{SDE Solver}, fill=blue!10
                                [
                                DiffFlow~\citep{zhang2021diffusion}{,}
                                DiNoF~\citep{zand2023diffusion}{,}
                                ~\citet{jolicoeur2021gotta}{,}
                                \\GMS~\citep{guo2024gaussian}{,}
                                BFM-Solver2~\citep{xue2024unifying}{,}
                                SA-Solver~\citep{xue2024sa}, leaf, text width=45em
                                ]
                            ]
                            [ 
                                \textbf{ODE Solver}, fill=blue!10
                                [                            
                                DDIM~\citep{song2020denoising}{,}
                                DPM-solver~\citep{lu2022dpm}{,}
                                DEIS~\citep{zhang2022fast}{,}
                                \\DMCMC~\citep{kim2022denoising}{,}
                                i-DODE~\citep{zheng2023improved}, leaf, text width=45em
                                ]
                            ]
                    ]
                    [
                        \textbf{Sampling Scheduling}, fill=blue!10
                            [
                                \textbf{Parallel Sampling}, fill=blue!10
                                [
                                DEQ~\citep{pokle2022deep}{,}
                                ParaDiGMS~\citep{shih2024parallel}{,}
                                ParaTAA~\citep{tang2024accelerating}{,}
                                \\TAA~\citep{walker2011anderson}, leaf, text width=45em
                                ]
                            ]
                            [ 
                                \textbf{Timestep Scheduling}, fill=blue!10
                                [                            
                                FastDPM~\citep{kong2021fast}{,}
                                ~\citet{watson2021learning}{,}
                                AYS~\citep{sabour2024align}, leaf, text width=45em
                                ]
                            ]
                    ]
                    [
                        \textbf{Truncated Sampling}, fill=blue!10
                            [
                                \textbf{Early Exit}, fill=blue!10
                                [
                                CATs~\citep{schuster2021consistent}{,}
                                ASE~\citep{moonsimple}, leaf, text width=45em
                                ]
                            ]
                            [
                                \textbf{Retrieval-Guided Initialization}, fill=blue!10
                                [                            
                                RDM~\citep{blattmann2022semi}{,}
                                kNN-Diffusion~\citep{sheyninknn}{,}
                                Re-Imagen~\citep{chenre}{,}
                                \\ReDi~\citep{zhang2023redi}, leaf, text width=45em
                                ]
                            ]
                    ]
            ]
        \end{forest}
 }
    \caption{Summary of efficient sampling techniques for diffusion models.}
    \label{fig:efficient_sampling}
\end{figure*}


\subsubsection{Efficient Solver}
%%%%%%%%%%%%%%%
% Score-based generative modeling through stochastic differential equations
% Diffusion Normalizing Flow
% Gaussian Mixture Solvers for Diffusion Models
% Unifying Bayesian Flow Networks and Diffusion Models through Stochastic Differential Equations
% SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models
% DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps
% FAST SAMPLING OF DIFFUSION MODELS WITH EXPONENTIAL INTEGRATOR
% Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs
% Denoising MCMC for Accelerating Diffusion-Based Generative Models
% Gotta go fast when generating data with score-based models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Given that the cost of sampling escalates proportionally with the number of discretized time steps, many researchers have concentrated on devising discretization schemes that reduce the number of time steps. A key insight emerges from reexamining the discrete forward process in the original DDPM formulation Eq.(\ref{eq:forward_process}), as we reduce the step size between consecutive steps, the process naturally approaches a continuous transformation. Consequently, adopting learning-free methods using SDE or ODE solvers~\citet{song2020score} has been proposed.

\noindent \textbf{SDE Solver.}
\citep{song2020score} firstly presents a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise.The discrete noise addition steps in Eq.(\ref{eq:forward_process}) are reformulated into a continuous process:

\indent SDE accomplishes the transformation from data to noise in the diffusion training process through the following equation:
\vspace{-1mm}
\begin{equation}
d \mathbf{x} =  \mathbf{f}( \mathbf{x}, t) \, dt + g(t) \, d \bar{\mathbf{w}}
\label{sde_forward}
\end{equation}

\noindent where \(\bar{\mathbf{w}}\) denotes the standard Wiener process, also known as Brownian motion. \(\mathbf{f}( \mathbf{x}, t) \) is a vector-valued function called the drift coefficient of \( \mathbf{x}(t) \), and \(g(t)\) is a scalar function.

Similarly, the reverse process Eq.(\ref{eq:backward_process}) can be generalized to a continuous-time formulation:
\vspace{-1mm}
\begin{equation}\label{sde_backward}
d\mathbf{x} = \left[ \mathbf{f}(\mathbf{x},t) - g(t)^2 \nabla_{\mathbf{x}} \log q_t(\mathbf{x}) \right] dt + g(t) d\bar{\mathbf{w}}
\end{equation}
% \vspace{-1mm}
$\bar{\mathbf{w}}$ is a standard Wiener process when time flows backward from $T$ to 0, $dt$ is an infinitesimal negative timestep and  $\nabla_{\mathbf{x}} \log q_t(\mathbf{x})$ represent the score function that we mentioned in Eq.(\ref{sde_forward}). In the diffusion process, reverse-time SDE converts noise into data gradually. The complete SDE process is shown in Figure~\ref{fig:sde_solver}.

\begin{figure}[b]
    \centering
    \includegraphics[width=1\textwidth]{figures/diffusion_schematic.pdf}
    \caption{Overview of forward SDE process and reverse SDE process~\citep{song2020score}.}
    \label{fig:sde_solver}
\end{figure}

Subsequently, there are many ways to efficiently implement SDE-based solvers.~\citep{zhang2021diffusion}introduces a novel generative modeling and density estimation algorithm called Diffusion Normalizing Flow (DiffFlow). Similar to the SDE of diffusion models Eq.(\ref{sde_forward}), the DiffFlow model also has a forward process:
\vspace{-1mm}
\begin{equation}
d\mathbf{x} = \mathbf{f}(\mathbf{x}, t, \theta) dt + g(t) d\bar{\mathbf{w}}
\end{equation}
\vspace{-1mm}
\noindent and a backward process:
\begin{equation}
d\mathbf{x} = \left[\mathbf{f}(\mathbf{x}, t, \theta) - g^2(t)\mathbf{s}(\mathbf{x}, t, \theta)\right] dt + g(t) d\bar{\mathbf{w}}
\end{equation}
\vspace{-1mm}

As a result of the learnable parameter \(\theta\), the drift term \textbf{\(f\)} is also learnable in DiffFlow, compared to the fixed liner function as in most diffusion models. Besides, these SDEs are jointly trained by minimizing the KL divergence. This allows the model to better adapt to changes in the data distribution, thus speeding up the convergence of the backward diffusion process. 
%
Similar to DiffFlow, ~\citet{zand2023diffusion} proposes a method called Diffusion with Normalizing Flow priors that also combines diffusion models with normalizing flows. The method first uses a linear SDE in the forward process to gradually convert the data distribution into a noise distribution. In the reverse process, a normalizing flow network is introduced to map the standard Gaussian distribution to latent variables close to the data distribution through a series of reversible transformations, which allows the samples to return to the data distribution more quickly, rather than relying on a large number of small incremental adjustments.

% In addition, the sampling process of SDE can be improved by combining the flow model.

However, the fixed step sizes in existing SDE solvers Eq.(\ref{sde_forward}), which usually require tremendous iterative steps, significantly affect generation efficiency. To address this, ~\cite{jolicoeur2021gotta} proposes a novel adaptive step-size SDE solver that dynamically adjusts the step size based on error tolerance, thereby reducing the number of evaluations. Specifically, the proposed method dynamically adjusts the step size by estimating the error between first-order and second-order approximations, leveraging a tolerance mechanism that incorporates both absolute and relative error thresholds. Furthermore, the use of extrapolation enhances precision without incurring additional computational overhead. This approach obviates the need for manual step-size tuning and is applicable across a range of diffusion processes, including Variance Exploding and Variance Preserving models.
%
As a result of Gaussian assumption for reverse transition kernels becomes invalid when using limited sampling steps. The Gaussian Mixture Solver (GMS)~\citep{guo2024gaussian} optimized SDE solver by using Gaussian mixture distribution. It addresses the limitations of the traditional process of SDE solvers in Eq.(\ref{sde_backward}), which assume a Gaussian distribution for the reverse transition kernel. Specifically, GMS replaces the Gaussian assumption with a more flexible Gaussian mixture mode and utilizes a noise prediction network with multiple heads to estimate the higher-order moments of the reverse transition kernel. At each sampling step, it employs the Generalized Method of Moments to optimize the parameters of the Gaussian mixture transition kernel, allowing for a more accurate approximation of the true reverse process, even with a limited number of discretization steps.

%
Instead, ~\citet{xue2024unifying} unifies Bayesian Flow Networks (BFNs) with Diffusion Models (DMs) by introducing time-dependent SDEs into the BFN framework. BFNs work by iteratively refining the parameters of distributions at different noise levels through Bayesian inference, rather than directly refining the samples as in traditional diffusion models.
To achieve theoretical unification between BFNs and DMs, the authors introduce a time-dependent linear SDE that governs the noise addition process in BFNs. This forward process includes two time-dependent functions: one controlling the drift of parameters and another controlling their diffusion. Based on this forward equation, they derive a corresponding reverse-time SDE for generating data from noise. This reverse process combines the drift term with a score-based correction term.
This reverse-time SDE directly aligns with the denoising process in diffusion models, enabling the BFN sampling process to effectively replicate the behavior of diffusion models.
%

By optimizing the solving process of SDE in Eq.(\ref{sde_forward}), Stochastic Adams Solver (SA-Solver)~\citep{xue2024sa} was presented. It is an innovative method designed to efficiently sample from Diffusion SDEs in Diffusion Probabilistic Models (DPMs)~\citep{ho2020denoising}. By addressing the significant computational burden of traditional samplers, SA-Solver achieves this through a clever combination of variance-controlled diffusion SDEs and a stochastic Adams method~\citep{buckwar2006multistep}, which is a multi-step numerical technique that leverages prior evaluations to enhance efficiency. The method introduces a noise control function $\tau(t)$, enabling dynamic adjustment of the noise injected during sampling, which in turn strikes an optimal balance between sampling speed and the quality of the generated data. Operating within a predictor-corrector framework, SA-Solver first makes an initial estimate through the predictor step and then refines this estimate using the corrector step, ensuring greater accuracy in the final output. This strategic integration significantly reduces the number of function evaluations required.

\noindent \textbf{ODE Solver.}
Unlike SDE solvers, the trajectories generated by ordinary differential equation (ODE) solvers are deterministic~\citep{song2020score}, remaining unaffected by stochastic variations. Consequently, these deterministic ODE solvers tend to achieve convergence more rapidly compared to their stochastic counterparts, although this often comes at the expense of a marginal reduction in sample quality. The corresponding deterministic process Eq.(\ref{eq:ode}) can be derived from the reverse-time SDE Eq.(\ref{sde_backward}) by removing the stochastic term $g(t) d\bar{\mathbf{w}}$, resulting in a deterministic process that shares the same marginal probability densities as the reverse-SDE:
% There is also a numerical method for solving the reverse-time SDE called ordinary differential equation (ODE), whose trajectories share the same marginal probability densities as reverse-SDE. T
\begin{equation}
\label{eq:ode}
d\mathbf{x} = \left[ {\mathbf{f}(\mathbf{x},t)} - \frac{1}{2} g(t)^2 \nabla_{\mathbf{x}} \log q_t(\mathbf{x}) \right] dt
\end{equation}
\vspace{-1mm}

The forward process also exhibits a similar distinction between SDE and ODE approaches, yielding a deterministic process that preserves the same marginal distributions:
\begin{equation}
d\mathbf{x} = \mathbf{f}(\mathbf{x},t)dt
\end{equation}

Recent research has produced numerous works on faster diffusion samplers based on solving the ODE Eq.(\ref{eq:ode}). Research shows that ODE samplers are highly effective when only a limited number of NFEs is available, while SDE samplers offer better resilience to prior mismatches~\citep{nie2023blessing} and exhibit superior performance with a greater number of NFEs~\citep{lu2022dpm}.

Denoising Diffusion Implicit Models (DDIM)~\citep{song2020denoising} builds upon the framework of Denoising Diffusion Probabilistic Models (DDPM)~\citep{ho2020denoising}, offering significant enhancements in sampling efficiency, which is one of the first models to leverage ODEs explicitly for the accelerating sampling process. 
\vspace{-1mm}
\begin{equation}
\label{eq:Non-Markovian Forward Process}
q_\sigma(\mathbf{x}_{1:T} | \mathbf{x}_0) = q_\sigma(\mathbf{x}_T | \mathbf{x}_0) \prod_{t=2}^{T} q_\sigma(\mathbf{x}_{t-1} | \mathbf{x}_t, \mathbf{x}_0)
\end{equation}

Unlike DDPM's Markovian forward process Eq.(\ref{eq:forward_process}) where each state only depends on its immediate predecessor, DDIM utilizes the Non-Markovian Forward Process Eq.(\ref{eq:Non-Markovian Forward Process}). These formulas allow each state not only to depend on its immediate predecessor but also on the initial state or a series of previous states. Specifically, Eq.(\ref{eq:DDIMreverprocess}) outlines how DDIM generates \( \mathbf{x_{t-1}} \) from  \(\mathbf{x_t}\) by predicting the denoised observation, which essentially approximates reversing the diffusion process:
\vspace{-1mm}
\begin{equation}
\label{eq:DDIMreverprocess}
  \mathbf{x}_{t-1} = \sqrt{\alpha_{t-1}} \left( \frac{\mathbf{x}_t - \sqrt{1 - \alpha_t} \epsilon_\theta^{(t)}(\mathbf{x}_t)}{\sqrt{\alpha_t}} \right) + \sqrt{1 - \alpha_{t-1} - \sigma_t^2} \cdot \epsilon_\theta^{(t)}(\mathbf{x}_t) + \sigma_t \mathbf{\epsilon}_t
\end{equation}
\vspace{-1mm}

During the process, DDIM employs an ODE solver to manage the continuous transformation across the latent space:
\vspace{-1mm}
\begin{equation}
\label{eq:ddimode}
    d\mathbf{x}(t) = \epsilon_\theta^{(t)} \left( \frac{\mathbf{x}(t)}{\sqrt{\sigma^2 + 1}} \right) d\sigma(t)
\end{equation}
\vspace{-1mm}

Eq.(\ref{eq:ddimode}) is key to the efficient management of the generation process, allowing for fewer steps in the generative sequence by smoothly interpolating between states using an ODE solver, thus significantly reducing the time complexity compared to traditional methods.

While DDIM's ODE formulation Eq.(\ref{eq:ode}) and its implementation through Eq.(\ref{eq:ddimode}) provide a foundation for deterministic sampling, ~\citet{liu2022pseudo} identifies two critical issues in the ODE formulation of DDIM: first, the neural network \(\theta\) and ODE are only well-defined within a narrow data manifold, while numerical methods generate samples outside this region. second, the ODE becomes unbounded as \( t \to 0 \) for linear schedules. Therefore PNDM is proposed to decompose the numerical solver into gradient and transfer components. 
% It reformulates DDIM's ODE as:
% \begin{equation}
% \frac{dx}{dt} = -\alpha'(t) * (\frac{x(t)}{2\alpha(t)} - \frac{\epsilon_\theta(x(t),t)}{2\alpha(t)\sqrt{1-\alpha(t)}})
% \end{equation}
It achieves second-order convergence, enabling 20x speedup while maintaining quality and reducing FID by ~0.4 points at the same step count across different datasets and variance schedules.


The DPM-solver~\citep{lu2022dpm} and Diffusion Exponential Integrator Sampler (DEIS)~\citep{zhang2022fast} innovate by leveraging the semi-linear structure of the probability flow ODE Eq.(\ref{eq:ode}) to design custom ODE solvers that outperform traditional Runge-Kutta~\citep{hochbruck2010exponential} methods in terms of efficiency. 
Specifically, DPM-solver solves the linear part of the equation and uses neural networks to approximate the nonlinear component. Compared to PNDM, DPM-solver maintains lower FID scores at the same NFE. Further, DEIS employs an Exponential Integrator~\citep{hochbruck2010exponential} to discretize ODEs. This method simplifies the probability flow ODE by transforming the probability ODE into a simple non-stiff ODE. Both of the innovations reduce the number of iterations needed producing high-quality samples within just 10 to 20 iterations. 
%

To reduce the computational overhead, ~\citet{zheng2023improved} presents an improved technique for maximum likelihood estimation of ODEs. Instead of directly working with the drift and score terms in Eq.(\ref{eq:ode}),  it introduces velocity parameterization to predict and optimize velocity changes $d\mathbf{x}_t$ during the diffusion process directly. The method improves upon previous ODE-based approaches by incorporating second-order flow matching for more precise trajectory estimation. Additionally, it introduces a negative log-signal-to-noise-ratio (log-SNR) for timing control of the diffusion process, alongside normalized velocity and importance sampling to reduce variance and optimize training. These enhancements significantly improve the model's likelihood estimation performance on image datasets without variational dequantization or data augmentation.
%
While previous methods focus on improving reverse ODE integrators based on Eq.(\ref{eq:ode}), Denoising MCMC (DMCMC)~\citep{kim2022denoising} takes a different approach by integrating Markov Chain Monte Carlo (MCMC) with ODE integrators to optimize the data sampling process. In DMCMC, MCMC first generates initialization points in the product space of data and diffusion time, which are closer to a noise-free state, significantly reducing the noise levels that need to be processed by the ODE integrators. This hybrid approach complements rather than improves the ODE integrators directly, enhancing overall sampling efficiency.

Besides,~\citet{lu2024simplifying} focuses on improving continuous-time consistency models(CMs)~\citep{song2023consistency, song2023improved} for efficient diffusion sampling by modifying the ODE parameterization and training objectives of continuous-time CMs. The core contribution is TrigFlow, a unified framework that bridges EDM~\citep{karras2022elucidating} and Flow Matching \citep{peluchetti2023non, lipman2022flow, liu2022flow, albergo2023stochastic, heitz2023iterative}. 

While the traditional probability flow framework is governed by Eq.(\ref{eq:ode}), they propose a simplified parameterization. To model these dynamics, they introduce a neural network $\mathbf{F_\theta}$ with parameters $\theta$ that takes normalized samples and time encodings as input. The time variable $t$ is transformed by $c_{noise}(t)$ to better condition the network. This results in a concise probability flow ODE:
\begin{equation}
\frac{d\mathbf{x}_t}{dt} = \sigma_d \mathbf{F}_\theta(\frac{\mathbf{x}_t}{\sigma_d}, c_{noise}(t))
\end{equation}



By introducing this simplified ODE parameterization, TrigFlow enables training large-scale CMs (up to 1.5B parameters) that achieve state-of-the-art performance with just two sampling steps, significantly reducing computational costs compared to DPM-solver~\citep{lu2022dpm} and other traditional diffusion models.

\subsubsection{Sampling Scheduling}

In diffusion models, a sampling schedule outlines a structured approach for timing and managing the sampling steps to improve both the efficiency and quality of the model's output. It focuses on optimizing the sequence and timing of these steps, utilizing advanced techniques to process multiple steps simultaneously or in an improved sequential order. Specifically, this scheduling primarily targets the optimization of the reverse process in DDPM, as described in Eq.(\ref{eq:backward_process}), where each step requires model prediction to gradually denoise from pure noise to the target sample. This scheduling is crucial for reducing computational demands and enhancing the model's performance in generating high-quality samples.



% \begin{wrapfigure}{r}{0.5\textwidth}
%  \centering
%  \includegraphics[width=0.4\textwidth]{figures/parallel_sampling.pdf}
%   \caption{Illustration of the parallel computing for diffusion models.}
%   \label{fig:parallel_sampling}
%   \vspace{-10pt}
% \end{wrapfigure}
\noindent \textbf{Parallel Sampling.}
% Deep equilibrium approaches to diffusion models, cited 24
% Accelerating parallel sampling of diffusion models, cited 5
% Parallel sampling of diffusion models, cited 25
%
Parallel sampling is a process that schedules sampling tasks in parallel. %Figure~\ref{fig:parallel_sampling}.
%
Traditional diffusion models require a extensive series of sequential denoising steps to generate a single sample, which can be quite slow. For instance, Denoising Diffusion Probabilistic Models (DDPMs)~\citep{ho2020denoising} might need thousands of these steps to produce one sample. However, parallel sampling leverages the power of a multi-core GPU to compute multiple sampling steps. This approach optimizes the use of computational resources and reduces the time needed for model generation. Currently, there is significant work on autoregressive models that employ parallelization to speed up the sampling process. However, these techniques cannot be directly applied to diffusion models. This is because the computational frameworks and inference efficiency in autoregressive models differ from those in diffusion models. Therefore, designing algorithms tailored to parallelize the sampling process of diffusion models is crucial.

\begin{wrapfigure}{r}{0.35\textwidth}
  \centering
  % \vspace{-15pt}
  \includegraphics[width=0.3\textwidth]{figures/Parallel_Sampling.pdf}
  \vspace{-10pt}
  \caption{Computation graph of Picard iterations, which introduces skip dependencies~\citep{shih2024parallel}.}
  \label{fig:ParaDiGMS_1}
  \vspace{-20pt}
\end{wrapfigure}
An innovative extension of the Denoising Diffusion Implicit Model (DDIM)~\citep{song2020denoising} using Deep Equilibrium (DEQ) models is presented~\citep{pokle2022deep}, where the sampling sequence is conceptualized as a multivariate fixed-point system. This approach focuses on finding the system's fixed point during the forward pass and utilizes implicit differentiation during the backward pass to enhance computational efficiency.  By treating the sampling steps as an equilibrium system and solving for their fixed points simultaneously, parallel processing on multiple GPUs is achieved by batching the workload. Notably, it improves efficiency by updating each state \( \mathbf{x}_t \) based on predictions from the noise prediction network \( \epsilon_{\theta} \), which takes into account all subsequent states \( \mathbf{x}_{t+1:T} \), unlike traditional diffusion processes that update states sequentially based only on the immediate next state \( \mathbf{x}_{t+1} \).

ParaDiGMS~\citep{shih2024parallel} employs Picard iterations to parallelize the sampling process in diffusion models. This method models the denoising process using ordinary differential equations (ODEs)~\citep{song2020score}, where Picard iterations approximate the solution to these ODEs concurrently for multiple state updates. ParaDiGMS operates within a sliding window framework, enabling the simultaneous update of multiple state transitions. Each state is iteratively connected to different generations, allowing for information integration from several previous iterations Figure~\ref{fig:ParaDiGMS_1}.

%
Building upon these parallel processing concepts, ParaTAA~\citep{tang2024accelerating} also adopts an iterative approach, primarily applied in practical deployments for image generation tasks such as text-to-image transformations using Stable Diffusion. Specifically, ParaTAA enhances parallel sampling by solving triangular nonlinear equations through fixed-point iteration. Furthermore, the study introduces a novel variant of the Anderson Acceleration~\citep{walker2011anderson} technique, named Triangular Anderson Acceleration, designed to accelerate computation speed and improve the stability of iterative processes.

\noindent\textbf{Timestep Schedule.}
% Timestep schedule is a strategy for selecting and arranging timesteps in the diffusion sampling process. It shows the feasibility of sampling in very few steps. 
In the sampling process of diffusion models, the entire process is discrete, and the model progressively restores data from noise through a series of discrete timesteps.
Each timestep represents a small denoising step that moves the model from its current state closer to the real data. The timestep schedule refers to the strategy for selecting and arranging these timesteps.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/timestep.pdf}
    \caption{Illustration of timestep schedule optimization process.}
    \label{fig:timestep}
\end{figure}
It may involve distributing them evenly or performing denser sampling during key stages to ensure the efficiency of the sampling process and the quality of the generated results. Selecting an appropriate method to choose a series of timesteps can enable the sampling process to converge quickly, which the process is shown in Figure~\ref{fig:timestep}.


FastDPM~\citep{kong2021fast} is a unified framework for fast sampling in diffusion models that innovatively generalizes discrete diffusion steps to continuous ones and designs a bijective mapping between these continuous diffusion steps and noise levels. By utilizing this mapping, FastDPM constructs an approximate diffusion and reverse process, significantly reducing the number of steps required ($S \ll T$). It allows for the flexible determination of sampling points by selecting specific steps or variances from the original diffusion process, thereby enhancing efficiency.
%
% ~\citet{watson2021learning} introduces a dynamic programming algorithm that optimizes timestep selection in Denoising Diffusion Probabilistic Models (DDPMs)~\citep{ho2020denoising}. Utilizing a pre-trained DDPM and a fixed budget of timesteps, aiming to maximize the Evidence Lower Bound (ELBO). The ELBO is a measure that balances the likelihood of the data under the model against a KL divergence penalty, quantifying the deviation from the true distribution. By exploiting the decomposable nature of the ELBO across consecutive timesteps, the algorithm uses dynamic programming to efficiently identify the sequence of timesteps that optimizes the trade-off between computational efficiency and sample quality.
\citet{watson2021learning} proposes a dynamic programming algorithm to optimize timestep scheduling in Denoising Diffusion Probabilistic Models (DDPMs). The algorithm efficiently determines the optimal timestep schedule from thousands of possible steps by leveraging the decomposable property of Evidence Lower Bound (ELBO) across consecutive timesteps and treating timestep selection as an optimization problem. Experiments show that the optimized schedule requires only 32 timesteps to achieve comparable performance to the original model with thousands of steps, effectively balancing efficiency and quality.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ays_pipeline_fig_full.pdf}
    \caption{Minimizing an upper bound on the Kullback-Leibler divergence (KLUB) between the true and linearized generative SDEs to find optimal DM sampling schedules~\citep{sabour2024align}.}
    \vspace{-15pt}
    \label{fig:AYS}
\end{figure}

However, optimizing an exact Evidence Lower Bound (ELBO) is typically not conducive to enhancing image quality. To address this, a universal framework named Align Your Steps (AYS)~\citep{sabour2024align}has been introduced, aimed at optimizing sampling schedules for various datasets, models, and stochastic SDE solvers. AYS uses stochastic calculus to optimize the sampling schedule allowing for the identification of optimal sampling timesteps by minimizing the Kullback-Leibler Divergence Upper Bound (KLUB) between discretized learnt SDE and true learnt SDE, which is shown in Figure~\ref{fig:AYS}.

% \subsubsection{Partial Sampling}
% Partial Sampling
% Consistent accelerated inference via confident adaptive transformers
% Confident adaptive language modeling
% A Simple Early Exiting Framework for Accelerated Sampling in Diffusion Models
% Semi-parametric neural image synthesis
% kNN-Diffusion: Image Generation via Large-Scale Retrieval
% Re-Imagen: Retrieval-Augmented Text-to-Image Generator
% ReDi: efficient learning-free diffusion inference via trajectory retrieval

% Partial sampling is a technique used to improve the efficiency of generating samples in neural networks, particularly in diffusion models. By selectively sampling certain steps in the generation process, it aims to reduce computational costs while maintaining high-quality outputs. This approach is especially beneficial in resource-constrained environments such as mobile devices.
\subsubsection{Truncated Sampling}

Truncated sampling enhances the efficiency of sample generation in diffusion models by strategically reducing redundant computations, thereby lowering computational costs while maintaining high-quality outputs. Methods like Early Exit focus on skipping unnecessary computations in later stages of the diffusion process when predictions are confident. Meanwhile, Retrieval-Guided Initialization improves efficiency in the early stages by leveraging retrieved examples to provide a better initialization, effectively skipping parts of the iterative refinement process. These approaches allocate computation more effectively by focusing resources on the most critical steps of the sampling process.

\noindent \textbf{Early Exit.} Early exit is a technique to improve the efficiency of large neural networks by reducing unnecessary computation. It allows intermediate layers to make early predictions, saving computational resources while maintaining performance. The core idea is to use intermediate predictions when they are confident enough, thereby skipping further layers for simpler inputs.~\citet{schuster2021consistent} introduce Confident Adaptive Transformers (CATs). As shown in Figure~\ref{fig:early_exit}, model $\mathcal{G}(X)$ provably consistent with the original $\mathcal{F}$ with arbitrarily high probability. They enhance transformer efficiency by adding prediction heads at intermediate layers and using a meta classifier to decide when to stop computation. This ensures high confidence in predictions, significantly improving efficiency on NLP tasks without sacrificing accuracy. Formally, they create a model $\mathcal{G}(X)$ that includes early classifiers $\mathcal{G} = \{\mathcal{F}_1, \mathcal{F}_2, \dots, \mathcal{F}_l\}, \quad \text{ensuring} \quad P(\mathcal{G}(x) = \mathcal{F}(x)) \geq 1 - \epsilon$.
%
% Building on this, another work~\citep{schuster2022confident} extends early exit techniques to language models. The method dynamically adjusts computation based on input complexity. It uses consistency prediction to maintain performance while making confident early exits. The approach employs early-exit classifiers \( F_k(x) = \text{softmax}(W_o(\phi(W_p h_k))) \) at various layers to decide when to exit early.
\begin{wrapfigure}{r}{0.3\textwidth}
  \centering
  % \vspace{-10pt}
  \includegraphics[width=0.3\textwidth]{figures/Early_Exit.pdf} 
  \caption{The CAT model $\mathcal{G}$~\cite{schuster2021consistent} improves computational efficiency by enabling early exits on certain inputs while ensuring predictive consistency with the full model $\mathcal{G}$.}
   \label{fig:early_exit}
   \vspace{-20pt}
\end{wrapfigure}
Building on this, another work~\citet{moonsimple} propose Adaptive Score Estimation (ASE) for diffusion models. ASE reduces sampling time by optimizing computation allocation at different time steps, introducing a time-dependent exit schedule. This method improves sampling efficiency while preserving image quality. The key idea is to dynamically allocate computation based on the difficulty of score estimation at each time step, allowing for early exits in simpler cases.

\noindent \textbf{Retrieval-Guided Initialization.} Retrieval-Guided Initialization combines the efficiency of retrieval mechanisms with the generative power of diffusion models. As illustrated in Figure~\ref{fig:retrieval-based}, this approach begins with a retriever that selects relevant images from a database based on the input text prompts. These retrieved images, organized by specific keywords, serve as contextual guidance for the diffusion model, enhancing the relevance and quality of the generated output image.\citet{blattmann2022semi} introduce a semi-parametric model that integrates neural network-based generative models with a retrieval mechanism, guided by examples from a large dataset. This approach shows the feasibility of combining retrieval with neural synthesis, although it requires a large dataset. ~\citet{sheyninknn} propose integrating a K-Nearest Neighbors (KNN) retrieval mechanism with diffusion models. By retrieving similar images from a large dataset, the model effectively guides the diffusion process, helping to generate high-quality images efficiently. This approach addresses the challenge of generating high-quality images with limited resources. \citet{chenre} introduce a retrieval-augmented framework specifically for text-to-image generation, retrieving relevant images based on textual descriptions to guide the diffusion process and align the output with the input text.

%
Finally,~\citet{zhang2023redi} introduces a learning-free inference mechanism for diffusion models. Instead of training complex networks, they retrieve and reuse diffusion trajectories from a precomputed database, significantly reducing the computational burden and providing an efficient solution for real-time applications.
\begin{figure}[h]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.8\textwidth]{figures/RAG.pdf}
    \caption{Illustration of the retrieval-based diffusion model. The retriever selects relevant images from a database based on input text. These retrieved images provide contextual guidance for the generator (diffusion model) to produce a new, coherent output image.}
    \label{fig:retrieval-based}
\end{figure}
\tikzstyle{my-box}=[
    rectangle,
    draw=hidden-draw,
    rounded corners,
    text opacity=1,
    minimum height=1.5em,
    minimum width=5em,
    inner sep=2pt,
    align=center,
    fill opacity=.5,
    line width=0.8pt,
]
\tikzstyle{leaf}=[my-box, minimum height=1.5em,
    fill=hidden-pink!80, text=black, align=left,font=\normalsize,
    inner xsep=2pt,
    inner ysep=4pt,
    line width=0.8pt,
]


\begin{figure*}[t!]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{forest}
            forked edges,
            for tree={
                grow=east,
                reversed=true,
                anchor=base west,
                parent anchor=east,
                child anchor=west,
                base=center,
                font=\large,
                rectangle,
                draw=hidden-draw,
                rounded corners,
                align=left,
                text centered,
                minimum width=4em,
                edge+={darkgray, line width=1pt},
                s sep=3pt,
                inner xsep=2pt,
                inner ysep=3pt,
                line width=0.8pt,
                ver/.style={rotate=90, child anchor=north, parent anchor=south, anchor=center},
            },
            where level=1{text width=18em,font=\normalsize,}{},
            where level=2{text width=15em,font=\normalsize,}{},
            where level=3{text width=24em,font=\normalsize,}{},
            [
                \textbf{Compression}, ver
                    [
                       \textbf{Quantization}, fill=blue!10
                            [
                                \textbf{Post-Training Quantization}, fill=blue!10
                                [
                                PTQ4DM~\citep{shang2023post}{,}
                                Q-Diffsuion~\citep{li2023q}{,}
                                BRECQ~\citep{librecq}{,}
                                \\RAQ~\citep{kim2024leveraging}{,}
                                PTQD~\citep{he2024ptqd}, leaf, text width=45em
                                ]
                            ]
                            [ 
                                \textbf{Quantization-Aware Training}, fill=blue!10
                                [                            
                                TDQ~\citep{so2024temporal}{,}
                                QALoRA~\citep{he2023efficientdm}, leaf, text width=45em
                                ]
                            ]
                    ]
                    [
                        \textbf{Pruning}, fill=blue!10
                        [
                            Diff-Pruning~\citep{fang2023structural}{,}
                            LD-Pruner~\citep{castells2024ld}{,}
                            LayerMerge~\citep{kim2024layermerge}{,}
                            LAPTOPDiff~\citep{zhang2024laptop}, leaf, text width=61.7em
                        ]
                    ]
                    [
                        \textbf{Knowledge Distillation}, fill=blue!10
                            [
                                \textbf{Vector Field Distillation}, fill=blue!10
                                [
                                Denoising Student~\citep{luhman2021knowledge}{,}
                                Progressive Distillation~\citep{salimans2022progressive}{,}
                                \\~\citet{meng2023distillation}{,}
                                CM~\citep{song2023consistency}, leaf, text width=45em
                                ]
                            ]
                            [
                                \textbf{Generator Distillation}, fill=blue!10
                                [                            
                                SDS~\citep{poole2022dreamfusion}{,}
                                VSD~\citep{wang2024prolificdreamer}{,}
                                Diff-Instruct~\citep{luo2024diff}{,}
                                \\CSD~\citep{decatur20243d}, leaf, text width=45em
                                ]
                            ]
                    ]
            ]
        \end{forest}
 }
    \caption{Summary of compression techniques for DMs.}
    \label{fig:model_compression}
    \vspace{-10pt}
\end{figure*}

\subsection{Compression}
\label{sec:compression}
Model compression enhances efficiency by reducing the sizes and the amount of arithmetic operations of DM. As summarized in Figure~\ref{fig:model_compression}, model compression techniques for DMs can be grouped into three categories: quantization, pruning, and knowledge distillation. These three categories are orthogonal to each other, and compress DMs from different perspectives.

\subsubsection{Quantization}
Quantization compresses neural networks by converting model weights and/or activations of high-precision data types $\mathbf{X}^{\mathrm{H}}$ such as 32-bit floating point into low-precision data types $\mathbf{X}^{\mathrm{L}}$ such as 8-bit integer~\citep{dettmers2024qlora}. Quantization techniques can be classified into post-training quantization (PTQ) and quantization-aware training (QAT).
\vspace{-1mm}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/quantization.pdf}
    \caption{Illustrations of the quantization.}
    \label{fig:quantization}
    
\end{figure}

% Post-training quantization on diffusion models
% Q-diffusion: Quantizing diffusion models
% BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction
% Leveraging early-stage robustness in diffusion models for efficient and high-quality image synthesis
% Ptqd: Accurate post-training quantization for diffusion models

\vspace{0mm}
\noindent \textbf{Post-Training Quantization.} PTQ involves selecting operations for quantization, collecting calibration samples, and determining quantization parameters for weights and activations. While collecting calibration samples is straightforward for CNNs and ViTs using real training data, it poses a challenge for Diffusion Models (DMs). In DMs, the inputs are generated samples \(\mathbf{x}_t\) at various time steps (t = 0, 1, ..., T), where T is large to ensure convergence to an isotropic Normal distribution.
To address this issue,~\citet{shang2023post} proposes PTQ4DM, the first DM-specific calibration set collection method, generating calibration data across all time steps with a specific distribution. However, their explorations remain confined to lower resolutions and 8-bit precision. Q-Diffsuion~\citep{li2023q} propose a time step-aware calibration data sampling to improve calibration quality and apply BRECQ~\citep{librecq}, which is a commonly utilized PTQ framework, to improve performance. Furthermore, compared to conventional PTQ calibration methods, they identify the accumulation of quantization error across time steps as another challenge in quantizing DMs Figure~\ref{fig:Q-Diffusion} (a). Therefore, they also propose a specialized quantizer for the noise estimation network shown in Figure~\ref{fig:Q-Diffusion} (b). Based on Q-Diffusion,~\citet{kim2024leveraging} find that inaccurate computation during the early stage of the reverse diffusion process has minimal impact on the quality of generated images. Therefore, they introduce a method that focuses on further reducing the number of activation bits for the early reverse diffusion process while maintaining high-bit activations for the later stages. Lastly,~\citet{he2024ptqd} presents PTQD, a unified formulation for quantization noise and diffusion perturbed noise. Additionally, they introduce a step-aware mixed precision scheme, which dynamically selects the appropriate bitwidths for synonymous steps.
\begin{figure}[t]
    \centering
    % \vspace{-10pt}
    \includegraphics[width=\textwidth]{figures/Q_Diffsuion.pdf}
    \caption{Traditional PTQ scenarios and Q-Diffusion differ in (a) the creation of calibration datasets and (b) the workflow for model inference ~\citep{li2023q}.}
    \label{fig:Q-Diffusion}
    \vspace{-10pt}
\end{figure}

\noindent \textbf{Quantization-Aware Training.}
% Temporal Dynamic Quantization for Diffusion Models
% Learned Step Size Quantization
% EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models
Different from PTQ, QAT quantizes diffusion models during the training process, allowing models to learn quantization-friendly representations. Since QAT requires additional training after introducing quantization operators, it is much more expensive and time-consuming than PTQ.~\citet{so2024temporal} proposes a novel quantization method that enhances output quality by dynamically adjusting the quantization interval based on time step information. The proposed approach integrates with the Learned Step Size Quantization~\citep{esser2019learned} framework, replacing the static quantization interval with a dynamically generated output from the Time-Dynamic Quantization module. This dynamic adjustment leads to significant improvements in the quality of the quantized outputs.~\citet{he2023efficientdm} introduces a quantization-aware low-rank adapter that integrates with model weights and is jointly quantized to a low bit-width. This approach distills the denoising capabilities of full-precision models into their quantized versions, utilizing only a few trainable quantization scales per layer and eliminating the need for training data.


\subsubsection{Pruning}
Pruning compresses DMs by removing redundant or less important model weights. Currently, most pruning methods for DMs focus on pruning structured patterns such as groups of consecutive parameters or hierarchical structures. For instance, Diff-Pruning~\citep{fang2023structural} introduces the first dedicated method designed for pruning diffusion models. Diff-Pruning leverages Taylor expansion over pruned timesteps to estimate the importance of weights. By filtering out non-contributory diffusion steps and aggregating informative gradients, Diff-Pruning enhances model efficiency while preserving essential features. 

%
LD-Pruner~\citep{castells2024ld}, as illustrated in Figure~\ref{fig:prune_ldp}, on the other hand, proposes a pruning method specifically designed for Latent Diffusion Models (LDMs) The key innovation of LD-Pruner lies in its utilization of the latent space to guide the pruning process. The method enables a precise assessment of pruning impacts by generating multiple sets of latent vectors—one set for the original Unet and additional sets for each modified Unet where a single operator is altered. The importance of each operator is then quantified using a specialized formula that considers shifts in both the central tendency and variability of the latent vectors. This approach ensures that the pruning process preserves model performance while adapting to the specific characteristics of LDMs.
\begin{figure}[ht]
    \includegraphics[width=\textwidth]{figures/prune_ldp.pdf}
    \caption{Pruning evaluates changes in the central tendency and variability to determine the significance of each operator. ~\citep{castells2024ld}.}
    \label{fig:prune_ldp}
\end{figure}
%
\citet{kim2024layermerge} introduces a technique known as LayerMerge, designed to jointly prune convolution layers and activation functions to achieve a desired inference speedup while minimizing performance degradation. LayerMerge addresses the challenge of selecting which layers to remove by formulating a new surrogate optimization problem. Given the exponential nature of the selection space, the authors propose an efficient solution using dynamic programming. Their approach involves constructing dynamic programming (DP) lookup tables that exploit the problem's inherent structure, thereby allowing for an exact and efficient solution to the pruning problem.
\begin{wrapfigure}{r}{0.4\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.3\textwidth]{figures/pruning.pdf}
    \vspace{-5pt}
    \caption{Illustrations of the pruning.}
    \label{fig:ParaDiGMS}
    \vspace{-10pt}
\end{wrapfigure}
Lastly, LAPTOPDiff~\citep{zhang2024laptop} introduces a layer-pruning technique aimed at automatically compressing the U-Net architecture of diffusion models. The core of this approach is an effective one-shot pruning criterion, distinguished by its favorable additivity property. This property ensures that the one-shot performance of the pruning is superior to other traditional layer pruning methods and manual layer removal techniques. By framing the pruning problem within the context of combinatorial optimization, LAPTOPDiff simplifies the pruning process while achieving significant performance gains. The proposed method stands out for its ability to provide a robust one-shot pruning solution, offering a clear advantage in compressing diffusion models efficiently.


\subsubsection{Knowledge Distillation}
Knowledge distillation~\citep{hinton2015distilling} is a technique that compresses complex models into smaller, efficient versions with minimal performance loss. The process of knowledge distillation can be captured by minimizing the following loss function:
\begin{equation}
L_{\text{KD}} = \alpha L_{\text{CE}}(y, \sigma(T_s(x))) + \beta L_{\text{MSE}}(T_t(x), T_s(x)),
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/knowledge_distillation.pdf}
    \caption{Illustrations of the
knowledge distillation.}
    \label{fig:knowledge_distillation}
\end{figure}
where $T_t$ and $T_s$ are the teacher and student models, respectively, $\sigma$ is the softmax function, $L_{\text{CE}}$ is the cross-entropy loss, and $L_{\text{MSE}}$ is the mean squared error loss, with $\alpha$ and $\beta$ as balancing hyperparameters. 
In DMs, known for generating high-quality data, this approach is increasingly applied to improve efficiency by addressing slow sampling speeds caused by the numerous neural function evaluations in the diffusion process. By distilling the knowledge from DMs into more efficient forms, researchers aim to accelerate sampling while preserving the generative performance of the original models. Follow~\citet{luo2023comprehensive}, knowledge distillation for DMs can be categorized into vector field distillation and generator distillation.

\noindent \textbf{Vector Field Distillation.}
% Knowledge distillation in iterative generative models for improved sampling speed
% Progressive distillation for fast sampling of diffusion models
% On distillation of guided diffusion models
% Consistency models
Vector field distillation improves the efficiency of deterministic sampling in diffusion models by transforming the generative ODE into a new generative vector field. This approach reduces the number of NFEs needed to produce samples of similar quality. 
%
\citet{luhman2021knowledge} first proposes a strategy to distill a DDIM sampler into a Gaussian model that needs only one NFE for sampling. In this approach, a conditional Gaussian model serves as the student model, and the training process involves minimizing the conditional KL divergence between this student model and the DDIM sampler.
\begin{wrapfigure}{r}{0.6\textwidth}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/vf_distillation.pdf}
    \caption{The progressive distillation, where the original sampler derived from integrating a learned diffusion model’s probability flow ODE, is efficiently condensed into a new sampler that achieves the same task in fewer steps. ~\citep{salimans2022progressive}.}
    \label{fig:vf_distillation}
    \vspace{-10pt}
\end{wrapfigure}
While this method advances the application of knowledge distillation to diffusion models, it still has computational inefficiencies, as it necessitates generating the final outputs of DDIM or other ODE samplers, which entails hundreds of NFEs for each training batch.
%
\citet{salimans2022progressive} proposes a progressive distillation strategy to train a student model to use half the NFEs of the teacher model by learning its two-step prediction strategy, as illustrated in Figure~\ref{fig:vf_distillation}. Once the student model accurately predicts the teacher’s two-step sampling strategy, it replaces the teacher model, and a new student model is trained to further reduce the sampling steps. This method reduces the NFEs significantly, achieving 250 times greater efficiency with only a 5\% drop in performance.

A two-stage distillation strategy is proposed by~\citet{meng2023distillation} to address the challenge of transferring knowledge from classifier-free guided conditional diffusion models like DALL·E-2~\citep{ramesh2022hierarchical} and Stable Diffusion~\citep{rombach2022high}. In the first stage, a student model is trained with classifier-free guidance to learn from the teacher diffusion model. The second stage employs the progressive diffusion strategy to further reduce the number of diffusion steps for the student model. This two-stage approach is applied to both pixel-space and latent-space models for various tasks, including text-guided generation and image inpainting. 
%
~\citet{song2023consistency} introduce the Consistency Model (CM), which leverages the self-consistency property of generative ODEs in diffusion models. Instead of directly mimicking the output of the generative ODE, their method focuses on minimizing the difference in the self-consistency function. By randomly diffusing a real data sample and simulating a few steps of the generative ODE to generate another noisy sample on the same ODE path, the model inputs these two noisy samples into a student model.

% The previous techniques help improve the sampling strategy of DMs by teaching the student model to learn the skipped output of the teacher model. In contrast, another idea is to refine an existing teacher model's sampling strategy.~\citet{liu2022flow} introduce the Reflow method that aims to improve generative speed by modifying a pre-trained teacher neural ODE through a student model. The student model straightens the path of the teacher model by minimizing a time average of $L^2$ loss between its outputs and interpolations of data samples and corresponding outputs from the pre-trained model.
% %
% Recently,~\citet{fan2023optimizing} propose another path distillation method, refining the generative path by finetuning the teacher ODE to minimize some Integral Probability Metrics according to the forward path. 
% %
% Similarly,~\citet{aiello2024fast} suggest fine-tuning the generative path by minimizing the Maximum Mean Discrepancy between each marginal distribution of the generative path and the actual data, further improving the alignment between the generated samples and the true data distribution.
% stop here 
\noindent \textbf{Generator Distillation.}
% Nerf: Representing scenes as neural radiance fields for view synthesis
% DreamFusion: Text-to-3D using 2D Diffusion
% Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation
% Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models
% 3d paintbrush: Local stylization of 3d shapes with cascaded score distillation
Unlike vector field distillation, which primarily focuses on distilling knowledge into student models with identical input and output dimensions, generator distillation aims to transfer the complex distributional knowledge embedded in a diffusion model into a more efficient generator. 
The Neural Radiance Field (NeRF)~\citep{mildenhall2021nerf} is a powerful technique for reconstructing 3D scenes from 2D images by learning a continuous volumetric scene representation. NeRFs generate photorealistic views of scenes from novel angles, making them valuable for applications in computer vision and graphics.

\begin{figure}[hb]
    \includegraphics[width=1\textwidth]{figures/gd_dream.pdf}
    \caption{Illustration as ~\citep{poole2022dreamfusion}, it utilizes score distillation sampling.}
    \label{fig:gd_dream}
\end{figure}
However, the limited availability of data for constructing NeRFs is an issue. Therefore, exploring distillation methods to obtain NeRFs with contents related to given text prompts is a promising way.~\citep{poole2022dreamfusion} first proposed Score Distillation Sampling (SDS) to distill a 2D text-to-image diffusion model into 3D NeRFs, as illustrated in Figure~\ref{fig:gd_dream}. Unlike traditional NeRF construction that requires images from multiple views of the target 3D objects, text-driven construction of NeRF lacks both the 3D object and the multiple views. The SDS method optimizes the NeRF by minimizing the diffusion model’s loss function using NeRF-rendered images from a fixed view.

%
\citet{wang2024prolificdreamer} introduce Variational Score Distillation (VSD), which extends SDS by treating the 3D scene corresponding to a textual prompt as a distribution rather than a single point. Compared to SDS, which generates a single 3D scene and often suffers from limited diversity and fidelity, VSD is capable of generating more varied and realistic 3D scenes, even with a single particle.
%
~\citet{luo2024diff} propose Diff-Instruct, which can transfer knowledge from pre-trained diffusion models to a wide range of generative models, all without requiring additional data. The key innovation in Diff-Instruct is the introduction of Integral Kullback-Leibler divergence, which is specifically designed to handle the diffusion process and offers a more robust way to compare distributions.
%
~\citet{decatur20243d} present Cascaded Score Distillation (CSD), an advancement by addressing a key limitation of standard SDS. Specifically, while traditional SDS only leverages the initial low-resolution stage of a cascaded model, CSD distills scores across multiple resolutions in a cascaded manner, allowing for nuanced control over both fine details and the global structure of the supervision. By formulating a distillation loss that integrates all cascaded stages, which are trained independently, CSD enhances the overall capability of generating high-quality 3D representations.