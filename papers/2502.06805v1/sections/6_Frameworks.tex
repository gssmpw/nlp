\section{Frameworks}
\label{sec:framework}

Diffusion model frameworks can generally be categorized based on their support for tasks such as training, fine-tuning, and inference. Specifically, frameworks that support training and/or fine-tuning are designed to provide scalable, efficient, and flexible infrastructures that enhance computational efficiency, reduce memory usage, and ensure the reliability of the training and fine-tuning processes. On the other hand, frameworks focused on inference aim to optimize throughput and reduce latency, addressing the practical needs of real-world applications. These frameworks offer various deployment options to handle diverse diffusion model tasks. Table~\ref{tab:framework} provides a summary of existing diffusion model frameworks along with their key features.

\begin{table}[h!]
\centering
\caption{Comparison of diffusion model frameworks.}
\renewcommand{\arraystretch}{1.5}
\small
% \begin{tabular}{lccc p{9.5cm}}
\begin{tabular}{lccc p{0.38\textwidth}}

\toprule
\textbf{Framework} & \textbf{Training} & \textbf{Fine-Tuning} & \textbf{Inference} & \textbf{Key Features} \\
\midrule
Diffusers         & \checkmark & \checkmark & \checkmark & Multi-model, versatile, open-source. \\
DALL-E            & \texttimes & \texttimes & \checkmark & Closed-source, API, high-quality. \\
OneDiff           & \texttimes & \texttimes & \checkmark & Optimized loading, CLI support. \\
LiteGen           & \checkmark & \checkmark & \checkmark & Efficient training, multi-GPU. \\
InvokeAI          & \texttimes & \checkmark & \checkmark & Stable Diffusion, CLI, API. \\
ComfyUI-Docker    & \texttimes & \checkmark & \checkmark & Modular, multiple models. \\
Grate             & \texttimes & \checkmark & \checkmark & Image grids, model merging. \\
% DifFUSER          & \checkmark & \checkmark & \checkmark & Multi-sensor, 3D perception. \\
Versatile Diffusion & \checkmark & \checkmark & \checkmark & Multi-modal, modular design. \\
UniDiffuser       & \checkmark & \checkmark & \checkmark & Unified, multi-modal diffusion. \\
\bottomrule
\end{tabular}
\label{tab:framework}
\end{table}

\textbf{Diffusers} is an open-source framework by Hugging Face that provides a versatile toolset for working with diffusion models. It supports multi-model usage for text-to-image and image editing tasks. By leveraging a variety of pre-trained models and pipelines, it enables quick experimentation and fine-tuning for both research and production environments. The framework is designed to be extensible, supporting efficient model training, fine-tuning, and inference.

\textbf{DALL-E} is a closed-source text-to-image model developed by OpenAI. The framework is accessible via the OpenAI API, enabling users to generate high-quality images based on textual descriptions. Although it does not support direct training or fine-tuning, it provides inference capabilities that deliver state-of-the-art image synthesis. Its commercial use is constrained by the terms of the OpenAI API.

\textbf{OneDiff} focuses on optimizing model loading and inference processes, particularly in environments with limited computational resources. It is designed to offer fast execution and support for command-line interface (CLI) operations. OneDiff provides a way to quickly perform inference using various diffusion models, making it suitable for both developers and researchers who need a streamlined workflow.

\textbf{LiteGen} is a lightweight and efficient training framework tailored for diffusion models. It offers multi-GPU support and optimizes various tasks, enabling users to fine-tune pre-trained models with low computational overhead. LiteGen is especially useful for multi-task scenarios, where training speed and resource efficiency are critical.

\textbf{InvokeAI} is designed to facilitate both fine-tuning and inference processes. It includes support for multiple diffusion models and provides both a command-line interface (CLI) and an API for ease of use. InvokeAI is aimed at users who need a flexible and accessible toolkit for experimenting with image generation.

\textbf{ComfyUI-Docker} is a modular framework that supports multiple diffusion models like Stable Diffusion and ControlNet. While it offers a graphical user interface through Docker, it also allows for command-line operations. The modular design enables users to integrate various models and pipelines, providing a versatile platform for different image generation tasks.

\textbf{Grate} is a diffusion model toolkit specialized in generating image grids using multiple Stable Diffusion models. It supports model merging and fine-tuning to achieve varied artistic effects. Geared towards both artists and researchers, Grate offers flexibility in combining models to explore diverse image generation possibilities.

% \textbf{DifFUSER} is focused on multi-sensor fusion and 3D perception tasks. It supports training, fine-tuning, and inference, making it a versatile toolkit for multi-modal applications. The framework excels in environments requiring complex data processing, such as robotics and augmented reality, where multi-sensor input is crucial.

\textbf{Versatile Diffusion} is a framework that emphasizes multi-modal tasks and modular design. It supports training, fine-tuning, and inference, allowing users to build customized workflows tailored to specific domains. Its extensibility makes it suitable for a wide range of applications, from text-to-image generation to image-to-image translation.

\textbf{UniDiffuser} provides a unified framework for multi-modal diffusion, enabling text-to-image, image-to-text, and other tasks in a seamless manner. It supports training, fine-tuning, and inference, with a focus on bridging the gap between different types of data modalities. UniDiffuser is ideal for researchers who need a flexible model to handle complex diffusion tasks across various input forms.