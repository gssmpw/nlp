\section{Related Work}
\label{section:related_work}
%-------------------------------------------------------------------------------

\mypara{Synthetic Data Detection}
This task can be formulated as a classification problem that distinguishes texts generated by language models (i.e., synthetic data) from those authored by humans (i.e., real data)**Carlini, "A Method for Adversarial Manipulation of Deep Learning Models"**.
These methods address the distinctions between synthetic and real data by exploiting their different characteristics.
Recent studies**Abdullah et al., "Adversarial Attacks on Synthetic Data"** show that LLM-generated synthetic data has unique lexical, structural, and semantic features that distinguish it from real data.
There are fine-tuning-based methods**Chen et al., "Synthetic Data Detection using Latent Features"** that analyze texts' latent features and train classifiers to identify synthetic data.
The differences between synthetic and real data, along with the success of classifiers in identifying synthetic data, inspire us to propose a hypothesis.
Classifiers trained on synthetic data tend to be more confident with synthetic inputs but less confident with real inputs, as they memorize the latent patterns of synthetic data.
We then propose a metric-based auditing method grounded in this hypothesis (\autoref{section:classifier_metric_audit}) and demonstrate its effectiveness through evaluation (\autoref{section:classifier_main_result}).

\mypara{Membership Inference Attacks (MIAs) **Aten et al., "Adversarial Attacks on Machine Learning Models"**}
Although both MIAs and synthetic artifact auditing employ binary classification and leverage classifier outputs (confidence scores, entropy, posteriors), they differ fundamentally in their attack targets and goals.
MIAs target a given sample's membership, aiming to detect whether a specific data sample was used during training.
In contrast, we target trained classifiers, generators, or plots, aiming to distinguish between artifacts trained on or derived from real versus synthetic data.
Additionally, our attack processes diverge: MIAs train shadow models solely to mimic the target model's behavior, while tuning-based auditing trains reference artifacts to optimize queries.
MIAs use specific queries to infer their membership, whereas we employ optimized (tuning-based) or random (metric-based) queries to audit target artifacts.

%-------------------------------------------------------------------------------