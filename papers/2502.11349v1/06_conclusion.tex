
\section{Conclusion}
\label{sec:conclusion}

This paper demonstrates how language models deployed on cloud servers, local desktop machines, and edge devices can exhibit bias when repeatedly prompted to make decisions. 
It was observed that edge implementations of these models (i.e., ELMs) can significantly increase the risk of bias and pose security and ethical concerns as the applications and use cases of this technology expand. 
Additionally, our work demonstrates that the introduction of a context-aware iterative feedback loop can help mitigate the issue of model bias. 
While the increased memory requirements remain a challenge for applying our proposed approach to ultra-low-power edge devices, our work further aims to address this by exploring more efficient methodologies for storing and passing predetermined weights across each layer of a model. 
Other extensions of this work will aim to investigate the potential for reinforced bias, as during the experiments a pattern was identified and after $11,893$ iterations the model entered a completely biased state and consistently made biased choices. Furthermore, we aim to optimize open source models to $INT8$ precision for edge deployment and apply the feedback loop to these quantized models to analyze their impact on bias mitigation.

