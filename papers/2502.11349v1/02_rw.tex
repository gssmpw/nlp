
\section{Related Work}
\label{sec:rw} 

Advancements in computational capabilities have shifted the primary constraint for LLM inference from processing power to memory bandwidth and energy efficiency. 
There are currently various methods to reduce the memory requirement~\cite{ojika2020addressing} of LLMs, such as pruning, quantization, and matrix decomposition. 
For example, Squeezellm~\cite{kim2023squeezellm} proposes sensitivity-based non-uniform quantization for searching the optimal bit precision assignment, completely based on second-order information, and dense and sparse decomposition for storing outliers and sensitive weights to reduce the memory requirement.
However, in this paper only pruning and quantization have been utilized, and they are discussed further in Section~\ref{subsec:model-optimization}.

In contrast to the on-device optimization techniques such as pruning and quantization, which are essential for deploying LLMs on resource constrained edge devices, cloud-based LLMs offer a more accessible deployment pathway. Cloud providers typically offer pre-trained models with ready-to-use endpoints, enabling users to interact with LLMs via application programming interfaces (APIs). This approach simplifies integration and facilitates scalable, concurrent requests, making it relatively easy to leverage LLM capabilities without the need for hardware-specific optimizations. 
However, frequent reliance on cloud endpoints for inference can lead to increased operational costs and latency issues, especially in scenarios requiring repeated or task-specific queries. To address this, recent work~\cite{dong2024creating} has shown that using LLMs as offline compilers for creating task-specific code can effectively help avoid frequent LLM endpoints accesses and reduce costs. 


%****************************************************************************************************

\subsection{LLMs at the Edge}

The deployment of LLMs demands significantly greater computational resources as compared to traditional DNNs such as convolutional neural networks (CNNs). 
This substantial resource requirement represents a key challenge in extending the use on LLMs to edge devices, and there have been continuous efforts for running resource-efficient LLM inference~\cite{haris_SECDA-LLM_2024}. 
The survey by Friha et al.~\cite{04} addresses the issue of edge-based language intelligence by providing a thorough examination of LLM-based Edge Intelligence (EI) architectures, focusing on security, optimization, and responsible development. 
Qin et al.~\cite{qin2024empirical} stated that different compression techniques are good at different types of tasks, and other guidelines for deploying LLMs onto resource-constrained devices effectively.  
Cheng et al.\cite{cheng2023optimize} focused on weight only post-training quantization, while AWQ~\cite{lin2024awq} uses an uneven weight quantization for preserving inference accuracy. Lamini-lm~\cite{wu2023lamini} uses knowledge distillation to perform effective compression, and MiniLLM~\cite{gu2024minillm} aims to minimize reversed Kullback-Leibler divergence and improvise effective compression.
EdgeMoE~\cite{yi2023edgemoe} proposed a more efficient inference for LLMs through a mixture-of-experts-based approach, where weights that occupy less storage but require computing are kept in memory throughout the time. Fu et al.~\cite{fu2024break} suggested a more aggressive lookahead decoding, while Malladi et al.~\cite{malladi2023fine} used memory-efficient zerothorder optimizer (MeZO) to estimate model gradients by forward propagation only. 

Despite these advancements, the primary focus of existing works has been on performance-centric optimizations, with limited attention to addressing fairness and bias in edge-deployed LLMs. 


%****************************************************************************************************

\subsection{LLMs and bias}

LLMs have fundamentally redefined Information Retrieval (IR) systems through the introduction of model generated data as a new data source which led the shift from passive data collection to proactive processing. 
This shift has raised new concerns for systems about adapting data bias and unfairness. Deldjoo~\cite{deldjoo2024understandingbiaseschatgptbasedrecommender} has shown how the difference in prompt design and formation strategies can impact the precision of the model recommendation. 
Recent surveys~\cite{amigo2023unifying,deldjoo2024fairness} have shown that biases can be categorized into several dimensions. 

The challenge of bias in LLM-generated recommendations is further compounded by the unregulated nature of internet data and the repetitive patterns in model-generated data used for the retraining. These factors contribute to the reinforcement of bias, making it difficult to uphold ethical standards in the model’s output. 
Several studies~\cite{winograd2022loose, jones2022artificial} suggested that there is no method at present for removing one’s imprints from a model with absolute certainty, except for retraining of the model from scratch which is not a possibility when we are considering the immense amount of data they are trained upon and high computational demand for processing these data.
Zhang et al.~\cite{zhang2023chatgpt} has proposed the benchmark fairness of recommendation via LLMs  that accounts for eight key attributes and further evaluated ChatGPT. 

Other recent works~\cite{deldjoo2024cfairllm, ghanbarzadeh2023gender} also demonstrated that incorporating any of the explicit user-sensitive attributes, such as gender or race, into the model can result in relatively biased recommendations to queries. The issue also raises a vital concern for security and privacy, as some studies~\cite{mohsin2024can,yao2024survey,yan2024protecting} show how LLMs have only a limited understanding of security principles, which can create new vulnerabilities and lead to the misuse of user-sensitive attributes for targeted attacks. 
These challenges underscore the critical need for developing robust mechanisms to ensure fairness, transparency, and accountability in the design and deployment of LLMs. 
Jaff et all~\cite{jaff2024data} developed an LLM-based framework to analyze the privacy policy for automatically checking the consistency of data.


While LLMs are capable of learning and memorizing attributes like name, the chances of bias is even higher as these information allows a model to form the context pattern and interrelate the topic for generating output text. This ability to establish the contextual relevance, though powerful, can inadvertently amplify biases present in the underlying training data or model-generated outputs which is further observed in~\ref{sec:eval} and discussed in~\ref{sec:disc}. 
Also, previous studies~\cite{radcliffe2024automated, metzger1999sign, nijodo2024automated, bianchi2023easily} have observed that subtle discrepancies in phrasing or structure of prompts can influence the tone, inclusivity, or neutrality of the model’s responses, which again raises a critical ethical question. 

However, previous work~\cite{taubenfeld2024systematic, ye2024justice, liang2024learning, qu2024mobile} have not examined how iterative interactions can reinforce these biases over time, particularly in edge environments where model retraining may not be feasible. 
This paper aims to bridge this gap by conducting a comparative analysis across cloud-based, desktop, and edge-deployed models, revealing that edge-optimized LLMs exhibit significantly higher bias rates. Additionally, this work proposes an iterative feedback loop that effectively mitigates bias on edge devices, offering a novel approach to enhancing fairness in resource-constrained deployments. 