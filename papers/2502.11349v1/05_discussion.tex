
\section{Discussion}
\label{sec:disc}

The results reveal a significant bias towards selecting 'Black Guy (0)' across all evaluated language model deployments, highlighting the importance of addressing these biases to develop more reliable, fair, and ethical AI systems. 
The implications of such biases are concerning, as for instance the consistent preference for 'Black Guy (0)' observed in the study across multiple models raises alarms about the reinforcement of racial biases in AI decision-making processes. 
This can lead to unfair treatment in real-world applications, particularly in sensitive contexts such as law enforcement or hiring. 
While Grok-beta exhibits a balanced approach, Gemini-1.5-flash and GPT-4o-mini show clear biases. 
The ELM Llama 2.0 ($INT8$) demonstrates the highest bias, with approximately $97.41$\% of its decisions being biased and consistently forcing a binary choice without considering uncertainty.

In addition to the experiment reported in Section~\ref{sec:eval}, and to further verify the reproducibility of the results, the number of calls to the cloud models was extended from $1,500$ to $15,000$ for all three cloud-based models, and an interesting pattern was observed. 
In the case of Gemini and GPT-4o-mini, the model completely adapted to the bias after $11,893$ iterations on average. The reason for choosing the cloud environment for this extended analysis is due to the servers ability to process multi-threaded request and computational availability to execute efficiently.
Furthermore, $15$ similar prompts were provided to the model where the text does not provide any context for taking any decision, and a similar pattern was observed in all models including cloud, desktop, and edge deployments. This extended evaluation underscores the importance of continuous monitoring and re-training of both cloud and on-device (i.e., desktop and edge) models. 

