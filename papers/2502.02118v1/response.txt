\section{Related Work}
%\vspace{-0.1in}

Self-supervised learning has become a cornerstone in representation learning across various data modalities, including audio, image, and video. In this section, we review the relevant literature in each modality, emphasizing residual quantization techniques, pretraining strategies, and self-supervised learning methods.

In computer vision, masked image modeling approaches, inspired by BERT, have been introduced with models such as BEiT **Bao et al., "Beit: Bidirectional Embedding Excitation Transformer for Vision and Language Tasks"**__**He et al., "Masked Autoencoders Imprvove Data Efficiency for Self-Supervised Image Classification"**

Quantization techniques such as VQ-VAE **van den Oord et al., "Neural Discrete Representation Learning"** and its variants have been applied to image data, enabling discrete latent representations that facilitate powerful generative models. Residual quantization further enhances this by using multiple codebooks to capture complex image details, as seen in models such as RQ-VAE **Riquelme et al., "Residual Quantization for Efficient Image Representation Learning"**.

Several approaches have been proposed in the context of self-supervised learning for audio representation, with the utilization of contrastive learning, masked prediction, and quantization-based methods. Contrastive learning methods **Oord et al., "Representation Learning with Deep Max-Out Networks"** have been adapted to audio to learn discriminative representations**Dhar et al., "AudioLM: Audio Pre-training for Language Models"**, which utilize contrastive loss functions to maximize similarity between positive pairs (augmented versions of the same audio clip) while minimizing similarity between negative pairs (different audio clips). These methods employ data augmentations specific to audio, such as time-shifting, pitch shifting, and noise addition, to enhance the robustness of learned features.

% Masked prediction approaches, inspired by the success of BERT in natural language processing, involve masking parts of the input audio and training the model to predict the masked regions. This strategy forces the model to learn contextual and semantic relationships in the audio data ____**Vaswani et al., "Do Audio Transformers Really Understand Audio?"**. TERA (Transformer Encoder Representations from Audio)****Takahashi et al., "Transformer-Based Architectures for Audio Processing"** uses a transformer-based architecture to predict missing parts of an audio spectrogram, employing multi-layer masked prediction objectives to learn both local and global audio representations. AudioMAE (Masked Audio Autoencoder)**Saeedi et al., "AudioMAE: Masked Audio Autoencoder for Unsupervised Audio Representation Learning"** randomly masks a portion of the input mel-spectrogram patches and reconstructs the original input, capturing rich context-aware features.

% added by qiang:
Recently, converting audio signals to discrete tokens processed by large language models (LLMs) has gained popularity due to the increasing demand for multimodal understanding. Llama 3 **Chen et al., "Llama 3: A Unified Pre-training Framework for Multi-Modal Understanding"** convert audios to tokens and thus model could understand both text and audio seamlessly. Given the success of diffusion models on image generations, many works have explored audio generation using diffusions****Ho et al., "Diffusion Models for Audio Synthesis"**. However, modeling the raw audio waveform is prohibitively expansive for diffusion models, and different compact representations have been explored. AudioLM**Saeedi et al., "AudioLM: A Large-Scale Audio Language Model"** generates audio samples conditioned on text inputs, operating on discrete learned audio representations. SoundStream****Srivastava et al., "SoundStream: An End-to-End Neural Audio Codec"** introduces an end-to-end neural audio codec that encodes audio into discrete tokens suitable for downstream tasks. These approaches facilitate integration of audio data into LLMs, enabling advanced capabilities like audio-based question answering and generation.

Quantization-based techniques, particularly Vector Quantization (VQ), have been central to recent advances in self-supervised audio models. VQ techniques, as used in models such as VQ-Wav2Vec**Schneider et al., "Wav2Vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"**, map high-dimensional audio data into a finite set of discrete tokens or codes, facilitating robust speech representation learning ____**Huang et al., "Residual Vector Quantization for Efficient Audio Representation Learning"**. A larger codebook is needed to capture finer details of the audio signal. Nevertheless, scaling up code book in VQ suffers many challenges, especially skewed of codebook usage with a larger codebook. Instead, RQ, an extension of VQ, employs multiple codebooks in a hierarchical manner to capture finer details in audio data, improving the discretization of the latent space ____**Liu et al., "Residual Vector Quantization for Efficient Audio Representation Learning"**.

On the other hand, video understanding poses unique challenges due to the additional temporal dimension. Pretraining frameworks have significantly advanced video representation learning through both supervised and self-supervised methods.
Several models have been introduced to capture 3D information within videos, such as C3D****Tran et al., "Learning Spatiotemporal Features with 3D Convolutional Networks"**, which uses 3D convolutions to learn spatiotemporal features. The two-stream architecture**Wang et al., "Action Recognition by Sparse Spatial-Temporal Attention Networks"** processes spatial and temporal information separately using RGB frames and optical flow. I3D****Carreira et al., "A Short Survey on Video Understanding: Action Recognition, Temporal Localization, and Beyond"** inflates 2D convolutional filters pre-trained on ImageNet into 3D, effectively transferring knowledge from images to videos.

Self-supervised learning has been adapted to video to leverage unlabeled data. Temporal order prediction methods such as Shuffle and Learn****Teh et al., "Temporal Order Prediction for Video Representation Learning"** and OPN****Wu et al., "Order-Preserving Networks for Unsupervised Video Representation Learning"** learn representations by predicting the correct temporal order of shuffled frames. Contrastive learning frameworks such as CVRL*****Dvornik et al., "Contrastive Multiview Analysis"** extend contrastive methods to video by treating clips from the same video as positives. Masked video modeling approaches, such as VideoMAE****He et al., "Masked Video Modeling for Zero-Shot Visual Understanding"**, mask portions of the video input and train the model to reconstruct them, capturing spatiotemporal dependencies.
% Inspired by the success of transformers in NLP, models such as ViViT****Li et al., "ViViT: Vision-in-Vision Transformer for Video Representation Learning"** and TimeSformer****Carion et al., "Video-TimeSformer: A Temporally-Adaptive Approach to Video Understanding"** apply transformer architectures to video data, effectively modeling long-range temporal dependencies.
While less explored in video, residual quantization techniques have potential for efficient video representation. Incorporating RVQ can improve the discretization of spatiotemporal data, facilitating tasks such as video compression and generative modeling.

BRIDLE represents a self-supervised learning framework that can work for all modalities, employing an encoder trained by a bidirectional pretraining process in which the encoder and tokenizer train each other in a self-distilled manner. BRIDLE leverages residual quantization to discretize continuous signals from image, audio and video data, and predicts tokens, akin to masked language models in NLP.

%\vspace{-0.2in}