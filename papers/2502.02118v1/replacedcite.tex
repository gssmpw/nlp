\section{Related Work}
%\vspace{-0.1in}

Self-supervised learning has become a cornerstone in representation learning across various data modalities, including audio, image, and video. In this section, we review the relevant literature in each modality, emphasizing residual quantization techniques, pretraining strategies, and self-supervised learning methods.

In computer vision, masked image modeling approaches, inspired by BERT, have been introduced with models such as BEiT____ and MAE____, where portions of the image are masked, and the model is trained to reconstruct the missing parts. These methods capture both local and global structures in images.
Quantization techniques such as VQ-VAE____ and its variants have been applied to image data, enabling discrete latent representations that facilitate powerful generative models. Residual quantization further enhances this by using multiple codebooks to capture complex image details, as seen in models such as RQ-VAE____.

Several approaches have been proposed in the context of self-supervised learning for audio representation, with the utilization of contrastive learning, masked prediction, and quantization-based methods. Contrastive learning methods ____ have been adapted to audio to learn discriminative representations____, which utilize contrastive loss functions to maximize similarity between positive pairs (augmented versions of the same audio clip) while minimizing similarity between negative pairs (different audio clips). These methods employ data augmentations specific to audio, such as time-shifting, pitch shifting, and noise addition, to enhance the robustness of learned features.

% Masked prediction approaches, inspired by the success of BERT in natural language processing, involve masking parts of the input audio and training the model to predict the masked regions. This strategy forces the model to learn contextual and semantic relationships in the audio data ____. TERA (Transformer Encoder Representations from Audio)____ uses a transformer-based architecture to predict missing parts of an audio spectrogram, employing multi-layer masked prediction objectives to learn both local and global audio representations. AudioMAE (Masked Audio Autoencoder)____ randomly masks a portion of the input mel-spectrogram patches and reconstructs the original input, capturing rich context-aware features.

% added by qiang:
Recently, converting audio signals to discrete tokens processed by large language models (LLMs) has gained popularity due to the increasing demand for multimodal understanding. Llama 3____ convert audios to tokens and thus model could understand both text and audio seamlessly. Given the success of diffusion models on image generations, many works have explored audio generation using diffusions____. However, modeling the raw audio waveform is prohibitively expansive for diffusion models, and different compact representations have been explored. AudioLM____ generates audio samples conditioned on text inputs, operating on discrete learned audio representations. SoundStream____ introduces an end-to-end neural audio codec that encodes audio into discrete tokens suitable for downstream tasks. These approaches facilitate integration of audio data into LLMs, enabling advanced capabilities like audio-based question answering and generation.

Quantization-based techniques, particularly Vector Quantization (VQ), have been central to recent advances in self-supervised audio models. VQ techniques, as used in models such as VQ-Wav2Vec, map high-dimensional audio data into a finite set of discrete tokens or codes, facilitating robust speech representation learning ____. A larger codebook is needed to capture finer details of the audio signal. Nevertheless, scaling up code book in VQ suffers many challenges, especially skewed of codebook usage with a larger codebook. Instead, RQ, an extension of VQ, employs multiple codebooks in a hierarchical manner to capture finer details in audio data, improving the discretization of the latent space ____.

On the other hand, video understanding poses unique challenges due to the additional temporal dimension. Pretraining frameworks have significantly advanced video representation learning through both supervised and self-supervised methods.
Several models have been introduced to capture 3D information within videos, such as C3D____, which uses 3D convolutions to learn spatiotemporal features. The two-stream architecture____ processes spatial and temporal information separately using RGB frames and optical flow. I3D____ inflates 2D convolutional filters pre-trained on ImageNet into 3D, effectively transferring knowledge from images to videos.

Self-supervised learning has been adapted to video to leverage unlabeled data. Temporal order prediction methods such as Shuffle and Learn____ and OPN____ learn representations by predicting the correct temporal order of shuffled frames. Contrastive learning frameworks such as CVRL____ extend contrastive methods to video by treating clips from the same video as positives. Masked video modeling approaches, such as VideoMAE____, mask portions of the video input and train the model to reconstruct them, capturing spatiotemporal dependencies.
% Inspired by the success of transformers in NLP, models such as ViViT____ and TimeSformer____ apply transformer architectures to video data, effectively modeling long-range temporal dependencies.
While less explored in video, residual quantization techniques have potential for efficient video representation. Incorporating RVQ can improve the discretization of spatiotemporal data, facilitating tasks such as video compression and generative modeling.

BRIDLE represents a self-supervised learning framework that can work for all modalities, employing an encoder trained by a bidirectional pretraining process in which the encoder and tokenizer train each other in a self-distilled manner. BRIDLE leverages residual quantization to discretize continuous signals from image, audio and video data, and predicts tokens, akin to masked language models in NLP.

%\vspace{-0.2in}