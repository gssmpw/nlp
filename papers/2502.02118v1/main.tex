\documentclass[11pt,letterpaper]{article}

\usepackage[margin=1.2in]{geometry}

\usepackage{amsmath,amsthm,amsfonts,amssymb,booktabs}
\usepackage{mathrsfs}
\usepackage{url}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{comment}
\usepackage{multirow}
\usepackage{authblk}


\usepackage{microtype}
\usepackage{booktabs} % for professional tables

\newtheorem{theorem}{Theorem}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{question}[theorem]{Question}
\newtheorem{remark}[theorem]{Remark}
%\newcommand{\remark}{\noindent {\bf Remark. }}[section]
\newtheorem{assumption}[theorem]{Assumption}

\title{BRIDLE: Generalized Self-supervised Learning with Quantization}

\author[1,2]{Hoang M. Nguyen}
\author[2]{Satya N. Shukla}
\author[2]{Qiang Zhang}
\author[2]{Hanchao Yu}
\author[2]{Sreya D. Roy}
\author[2]{Taipeng Tian}
\author[1]{Lingjiong Zhu}
\author[2]{Yuchen Liu}

\affil[1]{Florida State University}
\affil[2]{Meta Platforms Inc.}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\setlength{\parskip}{1em}


\begin{document}
\maketitle

\begin{abstract}
Self-supervised learning (SSL) has been a powerful approach for learning meaningful representations from unlabeled data across various domains, reducing the reliance on large labeled datasets. Inspired by BERT's success in capturing deep bidirectional contexts in natural language processing, similar frameworks have been adapted to other modalities such as audio, with models like BEATs extending the bidirectional training paradigm to audio signals using vector quantization (VQ). However, these frameworks face challenges, notably their dependence on a single codebook for quantization, which may not capture the complex, multifaceted nature of signals. In addition, inefficiencies in codebook utilization lead to underutilized code vectors. To address these limitations, we introduce BRIDLE ({B}idirectional {R}esidual Quantization {I}nterleaved {D}iscrete {L}earning {E}ncoder), a self-supervised encoder pretraining framework that incorporates residual quantization (RQ) into the bidirectional training process, and is generalized for pretraining with audio, image, and video. Using multiple hierarchical codebooks, RQ enables fine-grained discretization in the latent space, enhancing representation quality. BRIDLE involves an interleaved training procedure between the encoder and tokenizer. We evaluate BRIDLE on audio understanding tasks using classification benchmarks, achieving state-of-the-art results, and demonstrate competitive performance on image classification and video classification tasks, showing consistent improvements over traditional VQ methods in downstream performance.
\end{abstract}

\section{Introduction}
%\vspace{-0.1in}

By understanding the intrinsic structures within the data, \emph{Self-supervised learning} (SSL) methods have reduced the reliance on large labeled datasets, which are often costly and time-consuming to produce. One of the most influential models in this space is BERT \cite{devlin2019bert}, which has set new benchmarks in natural language processing tasks by capturing deep bidirectional contexts.

Pioneering models have demonstrated that discretizing continuous audio signals into discrete tokens can produce high-quality representations that are effective for a range of downstream tasks, including audio classification, speech recognition, and sound event detection \cite{chen2023beats, baevski2020wav2vec, hsu2021hubert}. BEATs \cite{chen2023beats} extends the bidirectional training paradigm to audio signals, achieving state-of-the-art results in various audio understanding tasks, using the idea of representing the inputs with \emph{Vector Quantization} (VQ) and training the encoder to predict the mapped tokens in an unsupervised context.

However, despite their strengths, these frameworks face several challenges that limit their potential. Notably, their dependence on a single codebook for quantization may not capture the complex, multi-dimensional nature of audio signals, particularly in environments with diverse sound sources or varying acoustic conditions \cite{oord2017neural}. Additionally, the training processes for the codebooks can be inefficient, as some code vectors remain unused or underutilized, leading to suboptimal representation learning and increased training time \cite{razavi2019generating}. Several techniques have been introduced to optimize codebook utilization in VQ and \emph{Residual VQ} (RVQ) frameworks \cite{kumar2024high, lancucki2020robust}, including $k$-means initialization for codebook vectors, randomized restarts for unused codebooks over several batches, and quantizer dropout to improve codebooks learning and usage.

\emph{Residual Quantization} (RQ) has emerged as an effective method in various machine learning applications, particularly in the domain of computer vision and audio understanding, where it serves as a robust tool to reduce the dimensionality of data representations. Originally developed in the context of signal processing and vector quantization \cite{gray1998quantization, gersho2012vector}, RQ operates by recursively quantizing the residuals of previously quantized vectors, allowing it to capture fine-grained details that are missed by single-step quantization. This method has been successfully applied in large-scale image retrieval \cite{jegou2010product} and approximate nearest neighbor search \cite{ge2013optimized}, demonstrating its efficiency in handling high-dimensional data. This hierarchical approach to quantization has proven to be highly beneficial in reducing information loss, making it an attractive option for tasks such as image compression and feature representation in vision models \cite{razavi2019generating, lee2022autoregressive}.

ImageNet dataset~\cite{russakovsky2015imagenet} has been instrumental in advancing computer vision, serving as a benchmark for image classification tasks. Models pre-trained on ImageNet~\cite{krizhevsky2012imagenet, simonyan2014very, he2016deep} have achieved remarkable performance and are widely used for transfer learning in various downstream tasks. Self-supervised learning methods on ImageNet have further improved the quality of learned representations without relying on labeled data. Contrastive learning frameworks such as SimCLR~\cite{chen2020simple} and MoCo~\cite{he2020momentum} maximize agreement between differently augmented views of the same image, capturing invariant features. Masked image modeling approaches such as MAE~\cite{he2022masked} apply masked modeling techniques to images, similar to BERT's masked language modeling.

Interestingly, audio understanding tasks can often be modeled similarly to vision models, as audio signals can be transformed into visual representations like mel-spectrograms. By converting raw audio data into mel-spectrograms, which encode frequency and temporal information as 2D matrices, the task of understanding audio becomes analogous to image processing. Residual quantization, when applied to image representations, helps to capture fine spectral details, much like it captures intricate visual features in image data. Tokenization and quantization methods have been utilized for efficient vector quantization in self-supervised learning frameworks such as those used in speech recognition and audio synthesis \cite{oord2017neural, dhariwal2020jukebox}. By quantizing the latent spaces of mel-spectrograms, models can efficiently capture important temporal and spectral patterns, crucial for audio classification and speech tasks \cite{baevski2020wav2vec, chen2023beats, chiu2022self}. This cross-domain applicability highlights residual quantizationâ€™s versatility and its critical role in both vision and audio-based models, where it ensures efficient representation learning and high-quality reconstructions.

For video understanding, the Kinetics dataset~\cite{kay2017kinetics} provides a large-scale collection of annotated video clips. Architectures employing 3D convolutions have benefited from pretraining on Kinetics, advancing action recognition and video analysis. Self-supervised approaches~\cite{sun2019videobert} and contrastive video representation learning methods~\cite{qian2021spatiotemporal} have extended bidirectional and self-supervised learning techniques to the temporal dimension, capturing complex spatiotemporal patterns.

Motivated by the quantization techniques in computer vision and audio compression, we introduce the {B}idirectional {R}esidual Quantization {I}nterleaved {D}iscrete {L}earning {E}ncoder, or BRIDLE for short, which is built and improved based on BEATs \cite{chen2023beats}, a bidirectional training process involves self-distilled training where the encoder acts as the teacher for the tokenizer, and the tokenizer trains the encoder to predict the tokens. In BRIDLE, we introduce a joint training framework and some improvements for codebooks usage and training. Our contributions are:

1. We incorporate residual quantization into BEATs framework \cite{chen2023beats} to enhance representation quality by utilizing multiple codebooks in a hierarchical manner, enabling a finer discretization of the audio latent space. Hence, we introduce BRIDLE, a self-supervised training framework with an interleaved training procedure between the main encoder and the tokenizer.

2. We provide comprehensive evaluations of the framework in audio understanding through classification tasks on popular benchmarks, AudioSet~\cite{gemmeke2017audio} and ESC-50~\cite{piczak2015esc}, where we demonstrate state-of-the-art results. Additionally, we show competitive performance in image classification tasks with experiments on ImageNet-1K and in video classification with Kinetics-400.  We also show consistent improvements in the encoder's downstream performance when using RQ compared to VQ. 

3. We present a comprehensive analysis of codebook training within the BRIDLE training framework, covering aspects such as uniform weight initialization vs $k$-means, initial normalization of input embeddings, and resetting unused codes to prevent stagnation and encourage exploration.

%\vspace{-0.2in}
\section{Related Work}
%\vspace{-0.1in}

Self-supervised learning has become a cornerstone in representation learning across various data modalities, including audio, image, and video. In this section, we review the relevant literature in each modality, emphasizing residual quantization techniques, pretraining strategies, and self-supervised learning methods.

In computer vision, masked image modeling approaches, inspired by BERT, have been introduced with models such as BEiT~\cite{bao2021beit, peng2022beit, wang2023image} and MAE~\cite{he2022masked}, where portions of the image are masked, and the model is trained to reconstruct the missing parts. These methods capture both local and global structures in images.
Quantization techniques such as VQ-VAE~\cite{oord2017neural} and its variants have been applied to image data, enabling discrete latent representations that facilitate powerful generative models. Residual quantization further enhances this by using multiple codebooks to capture complex image details, as seen in models such as RQ-VAE~\cite{lee2022autoregressive}.

Several approaches have been proposed in the context of self-supervised learning for audio representation, with the utilization of contrastive learning, masked prediction, and quantization-based methods. Contrastive learning methods \cite{chen2020simple, oord2018representation} have been adapted to audio to learn discriminative representations~\cite{saeed2021contrastive}, which utilize contrastive loss functions to maximize similarity between positive pairs (augmented versions of the same audio clip) while minimizing similarity between negative pairs (different audio clips). These methods employ data augmentations specific to audio, such as time-shifting, pitch shifting, and noise addition, to enhance the robustness of learned features.

% Masked prediction approaches, inspired by the success of BERT in natural language processing, involve masking parts of the input audio and training the model to predict the masked regions. This strategy forces the model to learn contextual and semantic relationships in the audio data \cite{baevski2020wav2vec}. TERA (Transformer Encoder Representations from Audio)~\cite{liu2021tera} uses a transformer-based architecture to predict missing parts of an audio spectrogram, employing multi-layer masked prediction objectives to learn both local and global audio representations. AudioMAE (Masked Audio Autoencoder)~\cite{huang2022masked} randomly masks a portion of the input mel-spectrogram patches and reconstructs the original input, capturing rich context-aware features.

% added by qiang:
Recently, converting audio signals to discrete tokens processed by large language models (LLMs) has gained popularity due to the increasing demand for multimodal understanding. Llama 3~\cite{dubey2024llama} convert audios to tokens and thus model could understand both text and audio seamlessly. Given the success of diffusion models on image generations, many works have explored audio generation using diffusions~\cite{kreuk2023audiogen}. However, modeling the raw audio waveform is prohibitively expansive for diffusion models, and different compact representations have been explored. AudioLM~\cite{borsos2023audiolm} generates audio samples conditioned on text inputs, operating on discrete learned audio representations. SoundStream~\cite{zeghidour2021soundstream} introduces an end-to-end neural audio codec that encodes audio into discrete tokens suitable for downstream tasks. These approaches facilitate integration of audio data into LLMs, enabling advanced capabilities like audio-based question answering and generation.

Quantization-based techniques, particularly Vector Quantization (VQ), have been central to recent advances in self-supervised audio models. VQ techniques, as used in models such as VQ-Wav2Vec, map high-dimensional audio data into a finite set of discrete tokens or codes, facilitating robust speech representation learning \cite{baevski2020vqwav2vec}. A larger codebook is needed to capture finer details of the audio signal. Nevertheless, scaling up code book in VQ suffers many challenges, especially skewed of codebook usage with a larger codebook. Instead, RQ, an extension of VQ, employs multiple codebooks in a hierarchical manner to capture finer details in audio data, improving the discretization of the latent space \cite{dhariwal2020jukebox}.

On the other hand, video understanding poses unique challenges due to the additional temporal dimension. Pretraining frameworks have significantly advanced video representation learning through both supervised and self-supervised methods.
Several models have been introduced to capture 3D information within videos, such as C3D~\cite{tran2015learning}, which uses 3D convolutions to learn spatiotemporal features. The two-stream architecture~\cite{simonyan2014two} processes spatial and temporal information separately using RGB frames and optical flow. I3D~\cite{carreira2017quo} inflates 2D convolutional filters pre-trained on ImageNet into 3D, effectively transferring knowledge from images to videos.

Self-supervised learning has been adapted to video to leverage unlabeled data. Temporal order prediction methods such as Shuffle and Learn~\cite{misra2016shuffle} and OPN~\cite{lee2017unsupervised} learn representations by predicting the correct temporal order of shuffled frames. Contrastive learning frameworks such as CVRL~\cite{qian2021spatiotemporal} extend contrastive methods to video by treating clips from the same video as positives. Masked video modeling approaches, such as VideoMAE~\cite{tong2022videomae}, mask portions of the video input and train the model to reconstruct them, capturing spatiotemporal dependencies.
% Inspired by the success of transformers in NLP, models such as ViViT~\cite{arnab2021vivit} and TimeSformer~\cite{bertasius2021space} apply transformer architectures to video data, effectively modeling long-range temporal dependencies.
While less explored in video, residual quantization techniques have potential for efficient video representation. Incorporating RVQ can improve the discretization of spatiotemporal data, facilitating tasks such as video compression and generative modeling.

BRIDLE represents a self-supervised learning framework that can work for all modalities, employing an encoder trained by a bidirectional pretraining process in which the encoder and tokenizer train each other in a self-distilled manner. BRIDLE leverages residual quantization to discretize continuous signals from image, audio and video data, and predicts tokens, akin to masked language models in NLP.

%\vspace{-0.2in}
\section{BRIDLE framework}
%\vspace{-0.1in}

The proposed BRIDLE framework focuses on integrating residual quantization \cite{lee2022autoregressive}, improving codebook representation capability. BRIDLE comprises four main components:

%\vspace{-0.1in}
\noindent\textbf{Main Encoder} $E(\cdot; \theta_E)$: Maps input audio features $\mathbf{x} \in \mathbb{R}^{T \times F}$, where $T$ is the time dimension and $F$ is the feature dimension, to a latent representation $\mathbf{z} = E(\mathbf{x}; \theta_E) \in \mathbb{R}^{T \times D}$, where $D$ is the dimension of the latent space.

%\vspace{-0.1in}
\noindent\textbf{Tokenizer} $T(\cdot; \theta_T)$: Contains a tokenizer encoder and a set of codebooks, which discretize the latent representation $\mathbf{z}$ into a sequence of discrete tokens $\mathbf{q} = T(\mathbf{z}; \theta_T) \in \mathbb{Z}^{T \times M}$, where $M$ is the number of codebooks.

%\vspace{-0.1in}
\noindent\textbf{Main Decoder} $D(\cdot; \theta_D)$: Predicts the tokens output by the tokenizer, facilitating the reconstruction of the input.

%\vspace{-0.1in}
\noindent\textbf{Tokenizer Estimator} $\mathrm{TE}(\cdot; \theta_{\mathrm{TE}})$: Predicts the encoder's embeddings from the tokenizer's outputs, ensuring alignment between the encoder and tokenizer.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{images/Diagram.png}
    
    \caption{BRIDLE training framework, which contains an encoder training phase (left), and a tokenizer training phase (right)}
    \label{fig:framework}
    % \vspace{-0.2in}
\end{figure*}

We adopt the interleaved training framework from BEATs~\cite{chen2023beats}, which consists of the encoder training phase, and the tokenizer training phase. When we train the encoder, the main encoder and decoder learn to predict masked labels produced by the tokenizer. On the other hand, when the tokenizer is trained, the tokenizer encoder, codebooks, and the estimator learn the embeddings produced by their teacher, the main encoder. Figure~\ref{fig:framework} details the framework training process. The framework begins with the encoder training phase, utilizing a cold start tokenizer for the first iteration. In subsequent iterations, the tokenizer is reset and trained during a dedicated tokenizer training phase, each followed by an encoder training phase. This approach establishes an interleaved training framework. We evaluate the main encoders in downstream tasks after every encoder training phase for all modes.

%\vspace{-0.1in}
\subsection{Residual Quantization}
%\vspace{-0.1in}

To enhance the discretization process, we employ residual quantization in BRIDLE. RQ uses a hierarchy of codebooks $\{\mathcal{C}_1, \mathcal{C}_2, \ldots, \mathcal{C}_M\}$, where each codebook $\mathcal{C}_m \in \mathbb{R}^{K_m \times D}$ contains $K_m$ code vectors of dimension $D$. The quantization process is performed in multiple stages, such that every codebook quantizes the residual error from the previous stage \cite{razavi2019generating}.

Given a latent vector $\mathbf{z}_t \in \mathbb{R}^D$ at time step $t$, the quantization at each stage $m$ is defined as follows. Initially, the residual is set to be $\mathbf{e}_1 = \mathbf{z}_t$. At each stage $m$, we select the code vector $\mathbf{c}_{i^*(m)}^{(m)} \in \mathcal{C}_m$ that minimizes the distance to the residual error $\mathbf{e}_m$, for $\mathbf{c}_{i}^{(m)} \in \mathcal{C}_m$:
\begin{align}
& i^*(m) = \arg \min\nolimits_{i=1,2,\ldots,K_{m}} \left\| \mathbf{e}_m - \mathbf{c}_i^{(m)} \right\|^2.
\end{align}
% \vspace{-0.1in}
We then update the residual for the next stage:
\begin{equation}\label{RQ:update}
\mathbf{e}_{m+1} = \mathbf{e}_m - \mathbf{c}_{i^{\ast}(m)}^{(m)}.
\end{equation}
After $M$ stages, the final quantized representation $\mathbf{q}_t$ is obtained by summing the selected code vectors from all stages:
\begin{equation}
\mathbf{q}_t = \sum\nolimits_{m=1}^M \mathbf{c}_{i^{\ast}(m)}^{(m)}.
\end{equation}

%\vspace{-0.1in}
\subsection{Loss Functions}
%\vspace{-0.1in}

The training of the encoder and tokenizer involves two different set of loss functions. The \emph{encoder loss} is a masked cross-entropy loss that measures the discrepancy between the tokens predicted by the main decoder and the actual tokens output by the tokenizer. This loss ensures that the encoder learns to produce representations that the decoder can accurately translate back into the original token sequence:
\vspace{-0.1in}
\begin{equation}\label{encoder:eqn}
\mathcal{L}_{\mathrm{encoder}} = -\frac{1}{T} \sum\nolimits_{t=1}^T \sum\nolimits_{m=1}^M \mathbf{y}_{t,m} \log \hat{\mathbf{y}}_{t,m},
\end{equation}
where $\mathbf{y}_{t,m}$ is the ground truth token from the tokenizer at time step $t$ and codebook $m$, $\hat{\mathbf{y}}_{t,m}$ is the predicted probability distribution over the token vocabulary by the main decoder at time step $t$ and codebook $m$. 
The \emph{tokenizer loss} comprises two components:

%\vspace{-0.1in}
\noindent\textbf{Codebook Loss ($\mathcal{L}_{\mathrm{cb}}$)}: Encourages effective and accurate mapping and representation of the input vectors by penalizing the distance between the encoder's latent vectors and their corresponding representation chosen from the codebooks. It includes a commitment term to ensure that the encoder output stays close to the code vectors:
%\vspace{-0.1in}
\begin{equation}\label{cb:loss}
\mathcal{L}_{\mathrm{cb}} = \frac{1}{T} \sum\nolimits_{t=1}^T \left( \left\| \mathrm{sg}[\mathbf{z}_t] - \mathbf{q}_t \right\|^2 + \beta \left\| \mathbf{z}_t - \mathrm{sg}[\mathbf{q}_t] \right\|^2 \right),
\end{equation}
where $\mathrm{sg}[\cdot]$ denotes the stop-gradient operator, which is identity at forward pass and has zero partial derivatives \cite{oord2017neural}, and $\beta$ is a hyperparameter controlling the strength of the commitment loss.

%\vspace{-0.1in}
\noindent\textbf{Cosine Similarity Loss ($\mathcal{L}_{\mathrm{cos}}$)}: Measures the cosine similarity between the tokenizer estimator's output and the main encoder's embeddings, promoting alignment between the two components:
%\vspace{-0.1in}
\begin{equation}\label{cos:loss}
\mathcal{L}_{\mathrm{cos}} = 1 - \frac{\sum_{t=1}^T \mathrm{TE}(\mathbf{q}_t) \cdot \mathbf{z}_t}{\sum_{t=1}^T \|\mathrm{TE}(\mathbf{q}_t)\| \|\mathbf{z}_t\|}.
\end{equation}
This loss component ensures the tokenizer captures proper latent information given by its teacher, the main encoder, through the training process of predicting the target embeddings.
The total tokenizer loss is the sum of the codebook loss \eqref{cb:loss} and the cosine similarity loss \eqref{cos:loss}:
%\vspace{-0.1in}
\begin{equation}\label{tokenizer:eqn}
\mathcal{L}_{\mathrm{tokenizer}} = \mathcal{L}_{\mathrm{cb}} + \lambda_{\mathrm{cos}} \mathcal{L}_{\mathrm{cos}},
\end{equation}
where $\lambda_{\mathrm{cos}}$ is a weighting factor balancing the two loss components.

%\vspace{-0.1in}
\subsection{Codebook Training Techniques}
%\vspace{-0.1in}

To optimize codebook training, we adopt the following key techniques within the BRIDLE framework:

%\vspace{-0.1in}
\noindent\textbf{Exponential Moving Average (EMA) update} \cite{oord2017neural}: The EMA update has been shown to be effective in codebook learning \cite{chen2023beats, lee2022autoregressive}. The EMA provides smoother updates and reduces variance, leading to more stable convergence. In Appendix~\ref{appendix:ema}, we show the convergence of the EMA update for all potential codebooks, given minor assumptions on the convergence of latent vectors and sufficiently large number of iterations. To our knowledge, this is a novel theoretical contribution.

%\vspace{-0.1in}
\noindent\textbf{Resetting Unused Codes} \cite{kumar2024high}: To improve codebook mapping, we reset unused or underutilized codes in the codebook $\mathcal{C}_m$. If the usage count $N_i$ for a code vector $\mathbf{c}_i$ falls below a threshold $T_u$, we reinitialize $\mathbf{c}_i$ based on the current data distribution:
%\vspace{-0.1in}
\begin{equation*}
\text{Rand}\{\mathbf{z}_t\} \rightarrow \mathbf{c}_i \quad \text{if} \quad N_i < T_u,
\end{equation*}
where $\{\mathbf{z}_t\}$ is the input latent vectors. This prevents codebook collapse and encourages diverse representation learning. In our implementation, resetting unused codes is applied only in the first batch of the tokenizer pretraining phase, so that it does not interfere with codebook training.

%\vspace{-0.1in}
\noindent\textbf{Code Embedding Initialization - Uniform or $k$-means:} At the start of training, we initialize codebook entries either uniformly based on the variance of the input features $\{\mathbf{z}_t\}$, or by $k$-means algorithm on the first batch. Due to the typical usage of normalization for the input latent vectors, the uniform initialization strategy uses the distribution of the range $[-1, 1]$. By sampling from a distribution that matches the input characteristics, we ensure balanced usage of the codebook entries and avoid dominance by a few entries. This ensures that the initial code embeddings are well-bounded.

%\vspace{-0.2in}
\section{Experiments and Results}
%\vspace{-0.1in}

We evaluate the BRIDLE model across three modalities: audio, image, and video. For each modality, we perform pretraining on large-scale datasets and fine-tuning on standard benchmarks to assess the model's performance comprehensively. Our goal is to demonstrate that integrating residual quantization, improving codebook training leads to performance improvement in self-supervised representation learning. For important audio benchmark on AudioSet~\cite{gemmeke2017audio}, and image benchmark on ImageNet-1K~\cite{russakovsky2015imagenet}, we also provide linear probing performances. These results demonstrate more noticeable improvements, especially where the model's fine-tuning results are relatively saturated, given that we are using the base ViT architecture for all modes. 
%which demonstrates more observable improvements, where the model's fine-tuning results are relatively saturated, considering we are using the base ViT architecture for all modes. 
Our code is available at \url{https://github.com/HoangNguyenM/bridle}.

 %\vspace{-0.1in}
\subsection{Datasets}
 %\vspace{-0.1in}

\noindent\textbf{Audio}: We use the \textit{AudioSet} dataset~\cite{gemmeke2017audio}, which contains over 2 million 10-second sound clips drawn from YouTube videos and labeled across 527 classes. For AudioSet, we collect the data from 2023 source, for which due to changes in YouTube content availability, approximately 10--15\% of the data is no longer accessible, affecting both the training and evaluation sets. 
\begin{table}[ht]
\centering
\caption{Data availability in AudioSet segments}
\label{tab:audioset_data}
\begin{tabular}{lccc}
\hline
Data Segment & Original & Obtained & Percentage \\
\hline
Balanced train    & 22,160    & 18,685    & 84.3\% \\
Unbalanced train  & 2,041,789 & 1,738,788 & 85.2\% \\
Evaluation        & 20,371    & 17,142    & 84.1\% \\
\hline
\end{tabular}
\end{table}
This data loss may introduce variations in performance metrics. Additionally, we use the \textit{ESC-50} dataset~\cite{piczak2015esc}, a collection of 2,000 5-second environmental audio recordings across 50 classes, to evaluate the model's generalization ability in environmental sound classification.

%\vspace{-0.1in}
\noindent\textbf{Image}: For image data, we utilize the \textit{ImageNet-1K} dataset~\cite{deng2009imagenet}, which consists of approximately 1.28 million training images and 50,000 validation images in 1,000 classes. ImageNet-1K is a standard benchmark for image classification tasks and enables us to evaluate the model's performance in visual recognition.

%\vspace{-0.1in}
\noindent\textbf{Video}: For video data, we employ the \textit{Kinetics-400} dataset~\cite{kay2017kinetics}, which contains around 240,000 video clips annotated with 400 human action classes. Each clip is approximately 10 seconds long and is sourced from YouTube videos. This dataset allows us to assess the model's capability in capturing spatiotemporal information for action recognition tasks. For our setup, the video frames have dimensions 512 (W) $\times$ 288 (H).

%\vspace{-0.1in}
\subsection{Model Architecture}
%\vspace{-0.1in}

We use the base ViT architectures for all modes. The tokenizer encoder has the same architecture as the main encoder. Similar to BEATs~\cite{chen2023beats} and inspired by the success of random linear projection tokenizer in the literature~\cite{chiu2022self, dubey2024llama}, for iteration 1, we use a simple linear projection as the cold start tokenizer encoder.
The BRIDLE framework incorporates residual quantization with $M = 4$ codebooks for each modality. We test this architecture against the VQ version of the model, where we use $M = 1$ codebook, while maintaining the same total number of codes for fair comparison. For RQ, each latent vector is mapped to $M = 4$ codes, which yield richer representations.
We refer to Appendix~\ref{appendix:hyperparameters} for hyperparameters details and Appendix~\ref{appendix:config} for model configurations.

%\vspace{-0.1in}
\noindent\textbf{Audio and Image}: For audio and image modalities, we follow the configurations of BEATs~\cite{chen2023beats} and AudioMAE~\cite{huang2022masked}. The encoder architecture is based on a transformer model with appropriate modifications to handle mel-spectrograms for audio and patches for images. During the encoder training phase, we use masking ratio of 0.8, and for the tokenizer training phase, we do not apply masking.

%\vspace{-0.1in}
\noindent\textbf{Video}: For the video modality, we adopt the setup from VideoMAE~\cite{tong2022videomae}. The encoder is extended to process spatiotemporal data by incorporating temporal attention mechanisms to capture motion dynamics effectively. The process is similar to audio and image training, except for the usage of tube masking with 0.9 ratio to increase reconstruction difficulty and prevent information leakage \cite{tong2022videomae}.

%\vspace{-0.1in}
\subsection{Training Procedure}
%\vspace{-0.1in}

We first pretrain the models for two bidirectional iterations, then evaluate the models on corresponding classification tasks on the datasets mentioned above. Due to the empirical observations from BEATs, the performance of the model shows negligible improvements with additional training iterations. We observe similar trend in our experiments, hence we stop at 2 iterations for all modes.

%\vspace{-0.1in}
\noindent\textbf{Audio}:  The model is pretrained on the AudioSet-2M dataset for 130 epochs per encoder training phase, and 30 epochs for tokenizer training. We then evaluate the model with fine-tuning and linear probing performances on AudioSet-2M and AudioSet-20K, as well as fine-tuning performance on ESC-50.

%\vspace{-0.1in}
\noindent\textbf{Image}: The model is pretrained on ImageNet-1K following the procedure in AudioMAE~\cite{huang2022masked}. We use 400 encoder training epochs and 100 tokenizer training epochs. Standard data augmentation techniques such as random cropping and horizontal flipping are applied. After pretraining, we fine-tune the model on the ImageNet-1K training set and evaluate it on the validation set.

%\vspace{-0.1in}
\noindent\textbf{Video}: We pretrain the model on the Kinetics-400 training set for 800 epochs per encoder training phase, and 200 epochs for tokenizer training phase, following the setup from VideoMAE~\cite{tong2022videomae}. During training, video clips are sampled with temporal strides to capture motion dynamics effectively. Data augmentations include random cropping and horizontal flipping in both spatial and temporal dimensions. Hence, our video models behave like multi-frame image models. After pretraining, the model is fine-tuned on the Kinetics-400 training set and evaluated on the validation set.

%\vspace{-0.1in}
\subsection{Evaluation Metrics}
%\vspace{-0.1in}

\noindent\textbf{Audio}: We evaluate the model using mean Average Precision (mAP) for AudioSet and classification accuracy for ESC-50. The mAP metric captures the precision-recall trade-offs across multiple classes, making it suitable for multi-label classification tasks on AudioSet. Accuracy measures the model's classification performance on ESC-50, which is a multi-class single-label dataset. The evaluation on ESC-50 uses 5-fold cross validation.

%\vspace{-0.1in}
\noindent\textbf{Image}: We report Top-1 and Top-5 classification accuracies on the ImageNet-1K validation set. Top-1 accuracy reflects the percentage of images where the top predicted class matches the ground truth, while Top-5 accuracy indicates the percentage where the ground truth class is within the model's top five predictions.

%\vspace{-0.1in}
\noindent\textbf{Video}: For Kinetics-400, we evaluate the model using Top-1 and Top-5 classification accuracies on the validation set. These metrics assess the model's ability to correctly recognize human actions from video clips from its top predictions.

%\vspace{-0.1in}
\subsection{Results}\label{sec:results}
%\vspace{-0.1in}

We present the audio classification results on the AS-2M, AS-20K, and ESC-50 datasets in Table~\ref{tab:audio_results}. We compare different models using both fine-tuning and linear probing evaluations. Table~\ref{tab:audio_results} demonstrates that incorporating \emph{residual quantization} (RQ) into the BEATs framework enhances performance across all evaluated datasets and metrics. The reproduction of BEATs~\cite{chen2023beats} on $\sim$15\% missing data from AudioSet-2M shows a minor drop of $\sim$0.5\% in fine-tuning mAP. Nevertheless, BRIDLE utilizing RQ codebooks and $k$-means initialization achieves the best results across all evaluation tasks. We have shown improvements compared to the benchmark of BEATs reproduction, which provided state-of-the-art results in the context of audio self-supervised training.


\begin{table*}[ht]
\centering
\caption{Audio classification results on the AS-2M, AS-20K, and ESC-50 datasets. Fine-tuning (FT) and linear probing (LP) results are reported in mAP or accuracy (\%).}
\label{tab:audio_results}
\begin{tabular}{lccccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c}{AS-2M mAP} & \multicolumn{2}{c}{AS-20K mAP} & ESC-50 FT \\
\cline{2-6}
& FT & LP & FT & LP &  mAP $\vert$ Acc \\
\hline
(VQ) BEATs$_\text{iter1}$ \cite{chen2023beats} & 46.60 & 26.76 & 35.36 & 23.32 & 97.19 $\vert$ 93.75 \\
(VQ) BEATs$_\text{iter2}$ \cite{chen2023beats} & 47.64 & 27.04 & 36.89 & 23.41 & 97.86 $\vert$ 94.55 \\
\hline
(uniform) BRIDLE$_\text{iter1}$ & 47.19 & 29.15 & 36.99 & 25.54 & 97.47 $\vert$ 94.45 \\
(uniform) BRIDLE$_\text{iter2}$ & 47.56 & 28.71 & 37.72 & 25.51 & 97.83 $\vert$ 94.85 \\
\hline
(k-means) BRIDLE$_\text{iter1}$ & 47.17 & \textbf{29.58} & 37.03 & 26.07 & 97.61 $\vert$ 94.45 \\
(k-means) BRIDLE$_\text{iter2}$ & \textbf{47.99} & 29.56 & \textbf{38.08} & \textbf{26.37} & \textbf{97.89} $\vert$ \textbf{94.95} \\
\hline
\end{tabular}
\end{table*}

%\vspace{-0.1in}
In the vision context, the image classification results on the ImageNet-1K dataset are presented in Table~\ref{tab:image_results}, where we report both fine-tuning and linear probing accuracies. The fine-tuning action recognition results on the Kinetics-400 dataset are presented in Table~\ref{tab:kinetics_results}. We report fine-tuning Top-1 and Top-5 accuracies.

\begin{table}[ht]
\centering
\caption{Image classification results on the ImageNet-1K dataset. Fine-tuning and linear probing results are reported in Top-1 and Top-5 accuracies (\%).}
\label{tab:image_results}
\begin{tabular}{lcccc}
\hline
\multirow{2}{*}{Model} & \multicolumn{2}{c}{FT Acc (\%)} & \multicolumn{2}{c}{LP Acc (\%)} \\
\cline{2-3} \cline{4-5}
& Top-1 & Top-5 & Top-1 & Top-5 \\
\hline
(VQ) BRIDLE$_\text{iter1}$ & 79.10 & 93.86 & 44.82 & 69.22 \\
(VQ) BRIDLE$_\text{iter2}$ & 80.26 & 94.41 & 53.21 & 76.89 \\
\hline
(k-means) BRIDLE$_\text{iter1}$ & 80.00 & 94.30 & 52.96 & 76.60 \\
(k-means) BRIDLE$_\text{iter2}$ & \textbf{81.10} & \textbf{95.00} & \textbf{56.30} & \textbf{79.24} \\
\hline
\end{tabular}
% \vspace{-0.1in}
\end{table}


\begin{table}[ht]
\centering
\caption{Action recognition results on the Kinetics-400 dataset. Fine-tuning results are reported in Top-1 and Top-5 accuracies (\%).}
\label{tab:kinetics_results}
\begin{tabular}{lcc}
\hline
Model & Top-1 Acc (\%) & Top-5 Acc (\%) \\
\hline
(VQ) BRIDLE$_\text{iter1}$ & 68.69 & 87.44 \\
(VQ) BRIDLE$_\text{iter2}$ & 71.32 & 88.86 \\
\hline
(k-means) BRIDLE$_\text{iter1}$ & 71.36 & 89.03 \\
(k-means) BRIDLE$_\text{iter2}$ & \textbf{72.90} & \textbf{90.08} \\
\hline
\end{tabular}
% \vspace{-0.2in}
\end{table}


%\vspace{-0.1in}
Table~\ref{tab:image_results} and Table~\ref{tab:kinetics_results} highlight the effectiveness of residual quantization in enhancing image and video representation learning, leading to better performance in action recognition tasks. Across all three modalitiesâ€”audio, image, and video, the incorporation of residual quantization into the bidirectional pretraining framework consistently improves performance. The benefits are more pronounced in linear probing evaluations, suggesting that RQ enables the encoder to learn more generalizable and robust representations. The use of $k$-means initialization for codebooks further enhances performance, indicating that careful codebook initialization is crucial for optimizing quantization-based models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.2in}
\section{Empirical Discussions}
%\vspace{-0.1in}

\noindent\textbf{Codebook enhancements:} A major component of the framework is the codebook, for which we need to adjust the size and number of codebooks. Hence, a natural question is whether we can further improve the model's performance by enhancing codebooks' size, or increasing the number of codes for each input latent vector. This can potentially introduce sparsities in the efficiency of the quantization process and can degrade model performance. We have observed performance degradation in audio and image context if we increase the number of codes, or use \textit{soft codes}, 
i.e. assigning each input embedding to the top-$k$ closest code vectors based on distance metrics. These results suggest that while large codebooks and soft codes can provide a powerful tokenization process, they may not provide performance benefits in our framework.
This outcome may result from the dilution of meaningful features across too many code vectors, or the limitation in the model's capability to learn a large space of tokens, given an encoding embedding of dimension $768$ from our base ViT architecture. Additionally, existing codebook methods assume hard-assigned code, and uniform weighting method is shown ineffective. A weighted combination of a sparse subset of codes could be a solution worth exploring in future works \cite{aharon2006k}. Therefore, determining an optimal codebook size is crucial, balancing the granularity of representation with effective utilization.

%\vspace{-0.1in}
\noindent\textbf{Is joint training of the encoder and the tokenizer possible?}
A disadvantage of the BRIDLE pretraining framework is the long training process, where we interchangeably train the encoder $E$ and tokenizer $T$.
To accelerate training, we consider a joint training strategy within BRIDLE, where the encoder $E$ and tokenizer $T$ are updated simultaneously within each iteration \cite{grill2020bootstrap}. This process allows the model components to co-adapt, fostering faster convergence. The loss functions of the two training phases can be combined as follows:
%\vspace{-0.1in}
\begin{equation}
\mathcal{L} = \mathcal{L}_{\mathrm{encoder}} + \alpha \mathcal{L}_{\mathrm{tokenizer}},
\end{equation}
where $\alpha$ is a loss ratio between the encoder training and tokenizer training. 
In BEATs' training framework, the main encoder and the tokenizer are trained in a leaved way: update one while the other is frozen \cite{chen2023beats}. Additionally, in every bidirectional training iteration, the tokenizer including its codebooks are reset, with the intuition of improving codebook usage and audio feature capturing. In our empirical results, training all architectures simultaneously provides almost equivalent results. Furthermore, to stabilize codebook training, it is beneficial to update the tokenizer for every few steps of training the main encoder. We have observed that $\alpha = 0.5$ and updating the tokenizer every 5 steps works well for VQ. 
Nevertheless, we find difficulties in tuning the hyperparameters for RQ models to perform well on downstream tasks, despite the codebooks converge nicely. Hence, we leave this idea for future work.

% \noindent\textbf{Necessity of ImageNet Pretraining}

% We investigated the impact of initializing our models with ImageNet-pretrained weights. Contrary to expectations, we observed a significant drop in linear probing performance when using ImageNet pretraining for audio tasks. This finding aligns with observations in prior studies, suggesting that the benefits of ImageNet pretraining for audio representation learning are limited. The domain gap between image and audio data likely hinders the transferability of learned features, and starting from random initialization may be more effective for audio pretraining in our framework.

% \noindent\textbf{Comparison to Benchmarks}

% In replicating the BEATs iter2 model, we achieved a fine-tuning mAP of 47.64\%, slightly below the benchmark of 48.1\%. We attribute this discrepancy to the fact that approximately 10--15\% of AudioSet data is no longer available due to changes in YouTube content accessibility. This loss affects both the training and evaluation sets, potentially impacting performance metrics. Despite these challenges, our RQ-BEATs models demonstrated competitive results, reinforcing the robustness and effectiveness of incorporating residual quantization.

%\vspace{-0.1in}
\noindent\textbf{Best practices for codebook training:}
Our empirical studies identified several techniques that enhance codebook training within the BRIDLE framework:

%\vspace{-0.1in}
\textit{1. EMA updates}: The EMA update for codebooks demonstrates higher performance than standard backpropagation.

%\vspace{-0.1in}
\textit{2. $k$-means initialization}: Initializing codebooks using k-means clustering significantly outperforms uniform initialization. This method ensures that code vectors are better aligned with the data distribution from the outset.

%\vspace{-0.1in}
\textit{3. Input embedding normalization}: Normalizing input embeddings at the beginning of every codebook layer helps stabilize training and improves quantization quality by maintaining consistent scaling across embeddings.

%\vspace{-0.1in}
\noindent\textbf{Codebook evaluations:} To validate the effectiveness of codebook's representations, we evaluate the codebooks on \emph{Code Usage Rate} (CUR), and \emph{Effective Code Usage} (ECU) (See Appendix~\ref{appendix:code} for formulations). These metrics help to systematically assess the diversity and balance of code usage across different configurations. We observe that RQ has superior CUR of $\sim$100\%, which intuitively can be easier to achieve with smaller size codebooks compared to VQ. It can be additionally observed that $k$-means initialization can generally yields $\sim$100\% CUR without training the codebooks. Furthermore, in the ECU metric, RQ shows better performance than VQ, demonstrating better code utilization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-0.1in}
\section{Conclusion and Future Work}
%\vspace{-0.1in}

In this paper, we introduced \emph{BRIDLE}, a self-supervised pretraining framework that incorporates residual quantization into the bidirectional encoder paradigm. Using multiple codebooks hierarchically, our approach enables finer discretization of the latent space, enhancing representation quality across audio, image, and video modalities.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our experiments demonstrated that BRIDLE consistently outperforms traditional vector quantization methods in both fine-tuning and linear probing evaluations. Specifically, we achieved state-of-the-art results on audio classification benchmarks on AudioSet, and showed competitive results on the ImageNet-1K and Kinetics-400 datasets for image and video classification tasks, respectively. The incorporation of residual quantization not only improved performance but also enhanced the generalization ability of the encoder representations.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We conducted a comprehensive analysis of codebook training within the BRIDLE framework, identifying effective strategies such as $k$-means initialization, input embedding normalization, and the use of exponential moving average updates for codebook vectors. These techniques contributed to better codebook utilization and stability during training.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
By addressing these directions, we aim to further enhance the capabilities of self-supervised learning frameworks and contribute to the development of more robust and efficient models across diverse modalities.
While BRIDLE shows promising results, there are several avenues for future exploration:

%\vspace{-0.1in}
\textit{1. Joint Training Framework for Efficiency}: A joint training process of the encoder and the tokenizer can boost the training framework's efficiency. Though we have observed successful results for VQ codebook, there remain challenges for models tuning for RQ.

%\vspace{-0.1in}
\textit{2. Enhanced Codebook Learning}: Despite the observed diminishing returns in codebook enhancements, exploring alternative codebook enhancement methods, such as adaptive codebook sizes or dynamic code allocation strategies, may further improve code utilization and representation capacity. In addition, integrating soft assignment methods or attention mechanisms could provide more flexibility in the quantization process. A more powerful code mapping strategy is worth exploring, especially for larger model sizes.
% or more complicated classification problems.

%\vspace{-0.1in}
\textit{3. Application in Cross-Modalities Settings}: Investigating the effectiveness of BRIDLE in other domains, such as natural language processing or cross-modal tasks like audio-visual speech recognition, may reveal further benefits of residual quantization in self-supervised learning.

% \vspace{-0.1in}
\textit{4. Theoretical Analysis}: Providing a theoretical understanding of why residual quantization improves representation learning in the bidirectional pretraining framework could offer insights that generalize beyond our empirical findings.


% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

%\newpage



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{alpha}
\bibliography{ref}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage



\appendix
\onecolumn
\begin{center}
\Large \bf Appendix
\end{center}

The Appendix is organized as follows.
\begin{itemize}
    \item 
    In Appendix~\ref{appendix:hyperparameters}, we provide the details of the hyperparameters.
    \item 
    In Appendix~\ref{appendix:ema}, we present the details of the EMA update for the codebooks.
    \item 
    In Appendix~\ref{appendix:config}, we discuss the model configurations.
    \item 
    In Appendix~\ref{appendix:code}, we provide the definitions of some metrics that are used in codebook and tokenizer evaluation.
\end{itemize}

\section{Hyperparameter Details}
\label{appendix:hyperparameters}

Table~\ref{tab:hyperparameters} presents the details of the hyperparameters used in the training of our BRIDLE model.

\begin{table*}[ht]
\centering
\caption{Hyperparameters for the BRIDLE Model across different modalities}
\label{tab:hyperparameters}
\begin{tabular}{|l|ccc|c|c|}
\hline
\multirow{2}{*}{Hyperparameter} & \multicolumn{3}{c|}{Audio} & Image & Video \\

 & AS-2M & AS-20K & ESC-50 & ImageNet-1K & Kinetics-400 \\
\hline
Pretrain Batch Size ($B$) & \multicolumn{3}{c|}{16} & 16 & 32 \\
FT \& LP Batch Size ($B$) & \multicolumn{3}{c|}{8} & 32 & 32 \\
Encoder Pretrain LR & \multicolumn{3}{c|}{$5 \times 10^{-4}$} & $5 \times 10^{-4}$ & $1.2 \times 10^{-3}$ \\
Tokenizer Pretrain LR & \multicolumn{3}{c|}{$2 \times 10^{-4}$} & $2 \times 10^{-4}$ & $2.5 \times 10^{-4}$ \\
FT LR & $5 \times 10^{-4}$ & $4 \times 10^{-4}$ & $2 \times 10^{-3}$ & $5 \times 10^{-4}$ & $7.5 \times 10^{-4}$ \\
LP LR & $4 \times 10^{-3}$ & $4 \times 10^{-2}$ & --- & $2 \times 10^{-3}$ & --- \\
\hline
Encoder Pretrain Epochs & \multicolumn{3}{c|}{130} & 400 & 800 \\
Tokenizer Pretrain Epochs & \multicolumn{3}{c|}{30} & 100 & 200 \\
FT Epochs & 120 & 150 & 300 & 200 & 75 \\
LP Epochs & 250 & 400 & --- & 300 & --- \\
\hline
Pretrain Mask Ratio & \multicolumn{3}{c|}{0.8} & 0.8 & 0.9 \\
FT Masking & \multicolumn{3}{c|}{2D, ratio 0.2} & 1D, ratio 0.2 & 0.0 \\
LP Masking & 0.0 & 0.0 & --- & 0.0 & --- \\
\hline
Optimizer & \multicolumn{5}{c|}{AdamW} \\
Weight Decay & \multicolumn{5}{c|}{$1 \times 10^{-4}$} \\
\hline
FT Loss & \multicolumn{3}{c|}{BCE} & BCE & CE \\
LP Loss & BCE & BCE & --- & BCE & --- \\
\hline
Normalization Mean & $-4.4446096$ & $-4.4446096$ & $-6.6268077$ & \multicolumn{2}{c|}{ImageNet default mean} \\
Normalization Std & 3.3216383 & 3.3216383 & 5.358466 & \multicolumn{2}{c|}{ImageNet default std} \\
\hline
Codebook Num ($M$), VQ & \multicolumn{5}{c|}{1} \\
Codebook Size ($K_m$), VQ & \multicolumn{5}{c|}{1024} \\
\hline
Codebook Num ($M$), RQ & \multicolumn{5}{c|}{4} \\
Codebook Size ($K_m$), RQ & \multicolumn{5}{c|}{256} \\
\hline
Codebook Dim ($d_m$) & \multicolumn{5}{c|}{256} \\
EMA Decay Rate ($\gamma$) & \multicolumn{5}{c|}{0.99} \\
\hline
Pretrain nnodes & \multicolumn{5}{c|}{8} \\
FT \& LP nnodes & 4 & 4 & 1 & 4 & 8 \\
GPU per Node & \multicolumn{5}{c|}{8} \\
Code Reset Threshold ($T_u$) & \multicolumn{5}{c|}{1} \\
\hline
\end{tabular}
\end{table*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The EMA Update for the Codebooks}
\label{appendix:ema}

To maintain stable updates of the codebook vectors, we employ the EMA strategy \cite{oord2017neural}. For each code vector $\mathbf{c}_i$ in a codebook $\mathcal{C}_m$,
the update rule at every time step $t$ is defined as:
\footnote{Note that $\mathbf{c}_{i}$ depends on $m$. To simplify the notation, we write $\mathbf{c}_{i}$ instead of $\mathbf{c}_{i}^{(m)}$ to hide the dependence on $m$. Similarly, for the update rule for $n_{i,t},N_{i,t},\hat{N}_{i,t},\mathbf{m}_{i,t},\mathbf{c}_{i,t}$ in \eqref{EMA:1}-\eqref{EMA:5}, we hide the dependence on $m$ to ease the notation.}
\begin{align}
n_{i, t} &:= \sum_{j=1}^S \delta_{q_{j, t}, i}, \qquad \ell_{i, t}:=\sum_{j=1}^{S} \delta_{q_{j, t}, i} \cdot \mathbf{z}_{j, t},\label{EMA:1}\\
N_{i, t+1} &\leftarrow \gamma N_{i, t} + (1 - \gamma) n_{i, t}, \label{EMA:2}\\
\hat{N}_{i, t+1} &\leftarrow (N_{i, t+1} + \epsilon)\frac{\sum_{i=1}^{K_{m}} N_{i, t+1}}{\sum_{i=1}^{K_{m}} N_{i, t+1} + K_{m} \cdot \epsilon}, \label{EMA:3}\\
\mathbf{m}_{i, t+1} &\leftarrow \gamma \mathbf{m}_{i, t} + (1 - \gamma) \ell_{i, t}, \label{EMA:4}\\
\mathbf{c}_{i, t+1} &\leftarrow \frac{\mathbf{m}_{i, t+1}}{\hat{N}_{i, t+1}},\label{EMA:5}
\end{align}
where:
\begin{itemize}
    \item $\gamma\in(0,1)$ is the EMA decay rate.
    \item $S$ is the total number of samples (sample size) at every time step.
    \item $\delta_{q_{j, t}, i}$ is the Kronecker delta function, which equals to $1$ if $q_{j, t} = i$ and $0$ otherwise, where $q_{j, t}$ represents the code that is mapped to the latent vector $\mathbf{z}_{j, t}$.
    \item $\mathbf{z}_{j, t}$ is the latent vector of sample $j$ at time $t$.%, which is normalized such that $\Vert\mathbf{z}_{j}\Vert\leq \mathcal{M}$ for some $\mathcal{M}\in(0,\infty)$.
    \item $N_{i, t}$ is the accumulated usage count (cluster size) of the code vector $\mathbf{c}_i$, where $N_{i, 0} = 0$. 
    \item $\mathbf{m}_{i, t}$ is the running embedding update for code vector $\mathbf{c}_{i, t}$, where $\mathbf{m}_{i, 0}=\mathbf{c}_{i, 0}=\mathbf{c}_{0}$ for some vector $\mathbf{c}_{0}$ of dimension $d_m$.
\end{itemize}

The EMA update can also be used to stabilize the learning of the codebook vectors in the Residual Quantization (RQ) process that can work directly with mini-batches. Each set of codebook $\mathcal{C}_m$ for $m \in \{1,2,\ldots,M\}$ can be updated by the above update rule.


Mathematically, the EMA update \eqref{EMA:1}-\eqref{EMA:5} can be re-written as
\begin{align}
&N_{i,t+1}= \gamma N_{i,t}+(1-\gamma)n_{i,t} + \epsilon,\label{rewrite:1}
\\
&\mathbf{m}_{i,t+1}=\gamma\mathbf{m}_{i,t}+(1-\gamma)\ell_{i,t},\label{rewrite:2}
\\
&\mathbf{c}_{i,t+1}=\frac{\mathbf{m}_{i,t+1}}{\gamma N_{i,t}+(1-\gamma)n_{i,t} + \epsilon} 
\cdot\frac{\sum_{i=1}^{K_{m}} (\gamma N_{i,t}+(1-\gamma)n_{i,t}) + K_{m} \cdot \epsilon}{\sum_{i=1}^{K_{m}} (\gamma N_{i,t}+(1-\gamma)n_{i,t})},\label{rewrite:3}
\end{align}
for any $t=0,1,2,\ldots$ with $N_{i,0}=0$, $\mathbf{m}_{i,0}=\mathbf{c}_{i,0}=\mathbf{c}_{0}$, where $n_{i,t},\ell_{i,t}$ are given in \eqref{EMA:1}.

Next, we provide convergence analysis for the EMA update \eqref{EMA:1}-\eqref{EMA:5}. 
Before we proceed, let us show that 
$\mathbf{m}_{i,t}$ and $N_{i,t}$ are uniformly bounded 
in $t$, which will be used in our convergence analysis. Indeed, we will first show the following technical lemma for $\mathbf{m}_{i,t}$.

\begin{lemma}\label{lem:bound:m}
Assume that for every $j=1,\ldots,S$, $\sup_{t}\Vert\mathbf{z}_{j,t}\Vert<\infty$ almost surely.
Then, for any $i=1,2,\ldots,K_{m}$ and $t=0,1,2,\ldots$,
\begin{equation}\label{m:uniform:bound}
\Vert\mathbf{m}_{i,t}\Vert\leq\mathcal{B}_{\mathbf{m}},
\end{equation}
almost surely, where
\begin{equation}\label{B:m:defn}
\mathcal{B}_{\mathbf{m}}:=\Vert\mathbf{c}_{0}\Vert
+\sum_{j=1}^{S}\sup_{t}\Vert\mathbf{z}_{j,t}\Vert.
\end{equation}        
\end{lemma}

\begin{proof}
We use mathematical induction to prove \eqref{m:uniform:bound}.
First, when $t=0$, $\mathbf{m}_{i,0}=\mathbf{c}_{0}$ so that \eqref{m:uniform:bound} trivially holds. 
Next, let us assume that $\Vert\mathbf{m}_{i,t}\Vert\leq\mathcal{B}_{\mathbf{m}}$ almost surely for
every $i=1,2,\ldots,K_{m}$. 
Then, from \eqref{rewrite:2}, we have
\begin{align}   
\Vert\mathbf{m}_{i,t+1}\Vert
&\leq
\gamma\Vert\mathbf{m}_{i,t}\Vert+(1-\gamma)\Vert\ell_{i,t}\Vert
\nonumber
\\
&\leq
\gamma\mathcal{B}_{\mathbf{m}}+(1-\gamma)\sum_{j=1}^{S}\sup_{t}\Vert\mathbf{z}_{j,t}\Vert
\leq\mathcal{B}_{\mathbf{m}},
\end{align}
almost surely, where we used the definition of $\mathcal{B}_{\mathbf{m}}$ in \eqref{B:m:defn}.
This completes the proof.
\end{proof}



Next, let us show that $N_{i,t}$ is uniformly bounded.

\begin{lemma}\label{lem:bound}
For any $i=1,2,\ldots,K_{m}$ and $t=0,1,2,\ldots$,
\begin{equation}\label{N:uniform:bound}
N_{i,t}\leq\mathcal{B}_{N},
\end{equation}
almost surely, where
\begin{equation}\label{B:N:defn}
\mathcal{B}_{N}:=S.
\end{equation}    
\end{lemma}

\begin{proof}
First, we recall from \eqref{rewrite:1} that
\begin{align}\label{N:iterates}
N_{i,t+1}= \gamma N_{i,t}+(1-\gamma)n_{i,t} + \epsilon.
\end{align}
We use mathematical induction to prove \eqref{N:uniform:bound}.
First, when $t=0$, $N_{i,0}=0$ so that \eqref{N:uniform:bound} trivially holds. 
Next, let us assume that $N_{i,t}\leq\mathcal{B}_{N}$ almost surely for
every $i=1,2,\ldots,K_{m}$. 
Then, we have
\begin{align}
N_{i,t+1}
&\leq
\gamma\mathcal{B}_{N}+(1-\gamma)S+\epsilon=\mathcal{B}_{N},
\end{align}
almost surely, where we used the definition of $\mathcal{B}_{N}$ in \eqref{B:N:defn} and the definition of $n_{i,t}$ in 
\eqref{EMA:1} such that $n_{i,t}\leq S$ for any $i$ and $t$. 
Hence, we proved \eqref{N:uniform:bound}.    
\end{proof}


Now, we are ready to show the convergence
of the EMA update \eqref{EMA:1}-\eqref{EMA:5}. 
We have the following result.

\begin{proposition}\label{prop:EMA}
Assume that for every $j=1,2,\ldots,S$, $q_{j,t}$ and $\mathbf{z}_{j,t}$ converge almost surely as $t\rightarrow\infty$.
Then, $(N_{i,t},\mathbf{m}_{i,t},\mathbf{c}_{i,t})$ converges almost surely
to $(N_{i,\infty},\mathbf{m}_{i,\infty},\mathbf{c}_{i,\infty})$
as $t\rightarrow\infty$ for every $i=1,2,\ldots,K_{m}$, where
\begin{align*}
&\mathbf{m}_{i,\infty}=\ell_{i,\infty}, \\
&N_{i,\infty}=n_{i,\infty}+\frac{\epsilon}{1-\gamma}, \\
&\mathbf{c}_{i,\infty}=\frac{\ell_{i,\infty}}{n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma} + \epsilon}\frac{\sum_{i=1}^{K_{m}} (n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma} + \epsilon)}{\sum_{i=1}^{K_{m}} (n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma})},
\end{align*}
with $\ell_{i,\infty}=\sum_{j=1}^{S}\delta_{q_{j,\infty},i}\cdot\mathbf{z}_{j,\infty}$ and $n_{i,\infty}=\sum_{j=1}^{S}\delta_{q_{j,\infty},i}$.
\end{proposition}

\begin{proof}
First, for every $j$, $\mathbf{z}_{j,t}$ converges
almost surely so that it is bounded almost surely which satisfies the assumption of  Lemma~\ref{lem:bound:m}. It follows from Lemma~\ref{lem:bound:m} that for every $i=1,\ldots,K_{m}$, 
$\mathbf{m}_{i,t}$ is uniformly bounded in $t$ almost surely, by Bolzano-Weierstrass theorem, $\mathbf{m}_{i,t}$ has a convergent subsequence. Suppose that its limit is $\mathbf{m}_{i,\infty}$. 
If we can show that the limit of any subsequence of 
$\mathbf{m}_{i,t}$ is 
the same, which is $\mathbf{m}_{i,\infty}$, 
then it follows that
$\mathbf{m}_{i,t}$ converges to 
$\mathbf{m}_{i,\infty}$ almost surely
as $t\rightarrow\infty$. 

To show this, note that since for every $j$, $\mathbf{z}_{j,t}$ and $q_{j,t}$ converge almost surely, we have
$\ell_{i,t}\rightarrow\ell_{i,\infty}$ almost surely as $t\rightarrow\infty$, 
where $\ell_{i,\infty}=\sum_{j=1}^{S}\delta_{q_{j,\infty},i}\cdot\mathbf{z}_{j,\infty}$. Then it follows from \eqref{rewrite:2} that
\begin{equation}
\mathbf{m}_{i,\infty}=\gamma\mathbf{m}_{i,\infty}+(1-\gamma)\ell_{i,\infty},    
\end{equation}
which implies that $\mathbf{m}_{i,\infty}=\ell_{i,\infty}$ and
thus this limit is unique. Hence, we conclude that
$\mathbf{m}_{i,t}\rightarrow\mathbf{m}_{i,\infty}$ almost surely as $t\rightarrow\infty$.

Next, from Lemma~\ref{lem:bound}, for every $i=1,\ldots,K_{m}$, 
$N_{i,t}$ is uniformly bounded in $t$ almost surely, by Bolzano-Weierstrass theorem, $N_{i,t}$ has a convergent subsequence. Suppose that its limit is $N_{i,\infty}$. 
If we can show that the limit of any subsequence of 
$N_{i,t}$ is 
the same, which is $N_{i,\infty}$, 
then it follows that
$N_{i,t}$ converges almost surely to 
$N_{i,\infty}$
as $t\rightarrow\infty$. 

To show this, 
note that since for every $j$, $q_{j,t}$ converge almost surely, 
we have
$n_{i,t}\rightarrow n_{i,\infty}$ as $t\rightarrow\infty$, 
where $n_{i,\infty}=\sum_{j=1}^{S}\delta_{q_{j,\infty},i}$.
We notice that if $N_{i,\infty}$
is any limiting point of a subsequence of 
$N_{i,t}$, then
it follows from \eqref{rewrite:1} that
\begin{align}\label{N:eqn:1}
N_{i,\infty}= \gamma N_{i,\infty}+(1-\gamma)n_{i,\infty} + \epsilon,
\end{align}
for every $i=1,2,\ldots,K_{m}$, 
By solving \eqref{N:eqn:1} for $N_{i,\infty}$, we obtain
\begin{align}\label{N:eqn:2}
N_{i,\infty}=n_{i,\infty}+\frac{\epsilon}{1-\gamma}, 
\end{align}
for every $i=1,2,\ldots,K_{m}$. 
Therefore, this limit is unique
and hence 
$N_{i,t}$ converges to 
$N_{i,\infty}:=n_{i,\infty}+\frac{\epsilon}{1-\gamma}$ almost surely
as $t\rightarrow\infty$ for every $i=1,2,\ldots,K_{m}$.

Finally, we recall from \eqref{rewrite:3}
that 
\begin{align}
\mathbf{c}_{i,t}&=\frac{\mathbf{m}_{i,t}}{\gamma N_{i,t-1}+(1-\gamma)n_{i,t-1} + \epsilon}\cdot\frac{\sum_{i=1}^{K_{m}} (\gamma N_{i,t-1}+(1-\gamma)n_{i,t-1}) + K_{m} \cdot \epsilon}{\sum_{i=1}^{K_{m}} (\gamma N_{i,t-1}+(1-\gamma)n_{i,t-1})}.   
\end{align}
Note that since for every $j$, $q_{j,t}$ converge almost surely, 
we have
$n_{i,t}\rightarrow n_{i,\infty}$ as $t\rightarrow\infty$, 
where $n_{i,\infty}=\sum_{j=1}^{S}\delta_{q_{j,\infty},i}$.
Moreover, since we already proved that 
$\mathbf{m}_{i,t}$ converges to $\mathbf{m}_{i,\infty}$
and $N_{i,t}$ converges to $N_{i,\infty}$ almost surely 
as $t\rightarrow\infty$, we conclude that $\mathbf{c}_{i,t}$
converges to $\mathbf{c}_{i,\infty}$ almost surely as $t\rightarrow\infty$,
where
\begin{align}
\mathbf{c}_{i,\infty}
&=\frac{\mathbf{m}_{i,\infty}}{\gamma N_{i,\infty}+(1-\gamma)n_{i,\infty} + \epsilon}
\cdot\frac{\sum_{i=1}^{K_{m}} (\gamma N_{i,\infty}+(1-\gamma)n_{i,\infty}) + K_{m} \cdot \epsilon}{\sum_{i=1}^{K_{m}} (\gamma N_{i,\infty}+(1-\gamma)n_{i,\infty})}
\nonumber
\\
&=\frac{\ell_{i,\infty}}{n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma} + \epsilon}\frac{\sum_{i=1}^{K_{m}} (n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma} + \epsilon)}{\sum_{i=1}^{K_{m}} (n_{i,\infty}+\frac{\gamma\epsilon}{1-\gamma})}.
\end{align}
The proof is complete.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model Configurations}
\label{appendix:config}

Our experiments use the base ViT architecture for audio and image, and the base video ViT architecture for video mode. The decoder is a 16-block Swin transformer for audio and image, and a 4-block transformer for video. The tokenizer estimator is a simple 3-block transformer for audio and image, and a 4-block transformer for video.

Each of $M = 4$ codebooks for RQ has a size of $K_m = 256$ code vectors of dimension $D = 256$, initialized based on the input latent vectors with $k$-means initialization of 10 steps, or with random uniform initialization. Meanwhile, the VQ version of the model has $M = 1$ codebook of $K_m = 1024$ code vectors also of dimension $D = 256$. Hence, the VQ and RQ versions of the training framework contains the same number of codes, where each latent vector is mapped to $M = 1$ code for VQ, and $M = 4$ codes for RQ.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Codebook and Tokenizer Evaluation}
\label{appendix:code}

To evaluate the effectiveness of the codebooks in the BRIDLE model's tokenizer, we use several metrics that assess both the diversity of code usage and the balance of their usage. These metrics help ensure that the tokenizer effectively utilizes the entire codebook, leading to robust audio representations.

\textbf{Code Usage Rate (CUR):} Measures the proportion of codes within every codebook $\mathcal{C}_m$ that are used at least once during tokenization:
\begin{equation}\label{CUR:eqn}
\text{CUR} = \frac{\text{Number of unique codes used}}{K_m}.
\end{equation}

A high CUR value indicates diverse usage of the codebook.

\textbf{Usage Entropy (UE):} Quantifies the uniformity of code usage using Shannon entropy \cite{shannon1948mathematical}:
\begin{equation}\label{UE:eqn}
\mathrm{UE} = -\sum_{i=1}^{K_m} p_i \log(p_i),
\end{equation}
where \( p_i = \frac{\hat{N}_{i, t}}{\sum_{j=1}^{K_m} \hat{N}_{j, t}} \) is the probability of selecting the \( i \)-th code, with $\hat{N}_{i, t}$ being the cluster size of the \( i \)-th code across the entire dataset. A higher UE suggests more balanced code usage.

\textbf{Effective Code Usage (ECU):} Combines CUR \eqref{CUR:eqn} and UE \eqref{UE:eqn} to evaluate both the diversity and balance of code usage:
\begin{equation}
\mathrm{ECU} = \mathrm{CUR} \times \frac{\mathrm{UE}}{\log(K_m)}.
\end{equation}

In Table~\ref{tab:codebook_metrics}, we show $\sim$100\% CUR for pre and post-training RQ. Additionally, the table shows superior ECU of RQ compared to VQ. A well-spread distributions of code usage is also demonstrated in Figure~\ref{fig:rq:norm} for RQ codebooks.

\begin{table*}[ht]
\centering
\caption{CUR and ECU for different methods before and after training}
\label{tab:codebook_metrics}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Iteration} & \textbf{Method} & \textbf{Codebook \#} & \textbf{CUR} $\uparrow$ & \textbf{ECU} $\uparrow$ \\
\hline
& VQ & - & 1.00 & 0.0146 \\
\cline{2-5}
& \multirow{4}{*}{RQ (uniform)} 
& 1 & 1.00 & 0.0235 \\
& & 2 & 0.99 & 0.0284 \\
& & 3 & 1.00 & 0.0489 \\
BRIDLE$_\text{iter1}$ & & 4 & 0.86 & 0.0367 \\
\cline{2-5}
(random init codebook) & \multirow{4}{*}{RQ ($k$-means)} 
& 1 & 0.99 & 0.0234 \\
& & 2 & 1.00 & 0.0326 \\
& & 3 & 1.00 & 0.0320 \\
& & 4 & 1.00 & 0.0310 \\
\hline
& VQ & - & 0.21 & 0.0041 \\
\cline{2-5}
& \multirow{4}{*}{RQ (uniform)} 
& 1 & 0.99 & 0.0275 \\
& & 2 & 1.00 & 0.0231 \\
& & 3 & 1.00 & 0.0228 \\
BRIDLE$_\text{iter2}$ & & 4 & 1.00 & 0.0229 \\
\cline{2-5}
(post training codebook) & \multirow{4}{*}{RQ ($k$-means)} 
& 1 & 0.99 & 0.0283 \\
& & 2 & 1.00 & 0.0229 \\
& & 3 & 1.00 & 0.0228 \\
& & 4 & 1.00 & 0.0228 \\
\hline
\end{tabular}
\end{table*}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vq1.png}
        \caption{(VQ) BRIDLE$_\text{iter1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/vq2.png}
        \caption{(VQ) BRIDLE$_\text{iter2}$}
    \end{subfigure}
    \caption{VQ codebook embeddings norm distributions before and after training on AudioSet}
    \label{fig:vq:norm}
\end{figure}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rq1.png}
        \caption{BRIDLE$_\text{iter1}$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/rq2.png}
        \caption{BRIDLE$_\text{iter2}$}
    \end{subfigure}
    \caption{RQ codebook embeddings norm distributions before and after training on AudioSet}
    \label{fig:rq:norm}
\end{figure}

\end{document}
