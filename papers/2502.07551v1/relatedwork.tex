\section{Related Work}
\label{gen_inst}
\textbf{Memorization \& Forgetting.}
In deep neural networks, generalization is not solely dictated by the complexity of the hypothesis space \citep{chaudhari2019entropy, advani2020high, Jiang*2020Fantastic}. They can still generalize effectively even without regularizers, a characteristic termed as \emph{Memorization}\footnote{\emph{``Memorization''} is not formally defined, and we denote \emph{Memorization} as training DNNs to fit the assigned label for each particular instance, in a similar spirit as \citet{feldman2020neural} and \citet{forouzesh2023leveraging}.}. 
Building on this, recent publications \citep{toneva2018an, feldman2020neural} have studied the phenomenon known as the ``\emph{forgetting event}'', aiming to understand how training data and network structures influence generalization. Based on the concept of \emph{forgetting}, we observed that when the model starts to fit mislabeled examples, it begins to significantly forget the training data. This forgetting increases until it reaches a turning point, after which the forgetting decreases.


\textbf{Model Selection.}
Numerous indirect methods have been proposed for selecting an appropriate model. These methods include marginal likelihood estimator \citep{pmlr-v51-duvenaud16}, computed gradients \citep{mahsereci2017early},  leveraging unlabeled data \citep{garg2021ratt, deng2021labels, forouzesh2023leveraging}, noise stability \citep{arora2018stronger, morcos2018importance, zhang2019perturbed}, estimating generalization gap \citep{jiang2018predicting, corneanu2020computing}, modeling loss distribution \citep{song2019does, song2021robust, lu2022selc}, and training speed \citep{lyle2020bayesian, ru2021speedy}.
In contrast to the existing methods, \emph{Label Wave} focuses on the selection of an appropriate early stopping point during training process. Notably, this is achieved without the need for additional or hold-out data and requires no preprocessing, ensuring low computational overhead.

\textbf{Learning Stages.}
Traditional paradigms often posit the presence of two stages during the learning process: underfitting and overfitting \citep{goodfellow2016deep, lin2023over}. Researchers exploring models trained on randomly labeled examples have divided the learning process into two distinct stages for a more nuanced perspective on deep learning: an initial stage of ``learning simple patterns'' \citep{arpit2017closer}, followed by a subsequent stage of ``memorization'' \citep{zhang2017understanding}. Further findings, such as epoch-wise Deep Double Descent \citep{belkin2019reconciling, nakkiran2021deep} and ``Grokking'' \citep{power2022grokking, liu2022towards, nanda2023progress}, have highlighted the potential existence of additional learning stages under specific conditions. However, these studies primarily focus on classifying learning stages based on generalization performance when models learn from imperfect data, thereby overlooking variations in model fitting performance.

\textbf{Learning Dynamics.}
A wealth of research endeavors have aimed to classify the ``hardness'' of examples by tracking learning dynamics, which is particularly beneficial for scenarios involving noisy labels or long-tail distributions. Methods like Late Stopping \citep{yuan2023latestopping}, FSLT\&SSFT \citep{maini2022characterizing}, Self-Filtering \citep{wei2022self}, SELFIE \citep{song2019selfie}, and RoCL \citep{zhou2021robust} exemplify this approach. 
However, these studies primarily focus on the dynamic changes of individual examples for the purpose of sample selection, rather than assessing the overall dynamics to distinguish different stages of the training process in learning with noisy labels.

\clearpage