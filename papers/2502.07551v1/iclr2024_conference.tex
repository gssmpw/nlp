
\documentclass{article} 
\usepackage{iclr2024_conference,times}


\input{math_commands.tex}
\definecolor{mydarkred}{rgb}{0.6,0,0}
\usepackage[colorlinks,
            linkcolor = mydarkred,
            urlcolor  = mydarkred, 
            citecolor=teal,
            ]{hyperref}
\usepackage{url}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{bbm}
\usepackage{xcolor}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\definecolor{changecolor}{RGB}{0, 0, 128}
\title{Early Stopping Against Label Noise \\ Without Validation Data}

\author{Suqin Yuan\textsuperscript{1} \quad Lei Feng\textsuperscript{2}\footnotemark[1] \quad  Tongliang Liu\textsuperscript{1}\thanks{Corresponding authors.} \\
\textsuperscript{1} Sydney AI Centre, School of Computer Science, The University of Sydney\\
\textsuperscript{2} School of Computer Science and Engineering, Nanyang Technological University
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy 
\begin{document}


\maketitle

\begin{abstract}
Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. 
Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient validation data could result in a sub-optimal selection of the desired model. 
In this paper, we propose a novel early stopping method called \emph{Label Wave}, which \emph{does not require validation data} for selecting the desired model in the presence of label noise.
It works by tracking the changes in the model's predictions on the training set during the training process, aiming to halt training before the model unduly fits mislabeled data. This method is empirically supported by our observation that minimum fluctuations in predictions typically occur at the training epoch before the model excessively fits mislabeled data.
Through extensive experiments, we show both the effectiveness of the \emph{Label Wave} method across various settings and its capability to enhance the performance of existing methods for learning with noisy labels.
\end{abstract}

\section{Introduction}


Deep Neural Networks (DNNs) are praised for their remarkable expressive power, which allows them to uncover intricate patterns in high-dimensional data \citep{montufar2014number, lecun2015deep} and can even fit data with random labels. However, this strength, often termed \emph{Memorization} \citep{zhang2017understanding}, can be a double-edged sword, especially when encountering label noise. When label noise exists, the inherent capability of DNNs might cause the model to fit mislabeled examples from noisy datasets, which can deteriorate its generalization performance.
Specifically, when DNNs are trained on noisy datasets containing both clean and mislabeled examples, it is often observed that the test error initially decreases and subsequently increases. To prevent DNNs from overconfidently learning from mislabeled examples, many existing methods for learning with noisy labels  \citep{xia2019anchor, han2020survey, song2022learning, huang2023machine} explicitly or implicitly adopted the operation of halting training before the test error increases—a strategy termed ``early stopping''.

Early stopping relies on model selection, aiming to choose a model that aligns most closely with the true concept from a range of candidate models obtained during the training process \citep{mohri2018foundations, bai2021understanding}.  
To this end, leveraging hold-out validation data to pinpoint an appropriate early stopping point for model selection becomes a prevalent approach \citep{xu2018splitting} in deep learning.
However, this approach heavily relies on additional validation data that is usually derived by splitting the training set, thereby resulting in degraded performance due to insufficient training data.
This issue becomes even more challenging in the presence of label noise. This is because, in this scenario, it is hard to learn a good model from unreliable training data and select the desired model using unreliable validation data \citep{chen2021robustness}.


In this paper, we aim to pinpoint an appropriate early stopping point to mitigate the negative effects of label noise \emph{without relying on any hold-out validation data}. 
Our motivation stems from the observation that there exist qualitative differences between DNNs trained on clean examples and mislabeled examples \citep{arpit2017closer}. 
By tracking these differences during the training process, we can pinpoint the moment when DNNs transition from primarily fitting clean examples to primarily fitting mislabeled examples, and thus determine an appropriate early stopping point. 
In what follows, we demonstrate in Figure \ref{fig1} how to determine such a point by tracking the changes in the model's \emph{fitting performance}, instead of leveraging any validation with hold-out data.


\begin{figure*}[t]
 \vskip -0.05in
\centerline{\includegraphics[width=14.1cm]{labelwave_fig1_v37.pdf}}
 \vskip -0.05in
\caption{We examine how the model's fitting and generalization performance evolves during the training process of learning with noisy labels. 
Utilizing the \emph{k-epoch learning} metric \citep{yuan2023latestopping}, we measure the number of training examples that can be consistently classified according to their provided labels. This allows us to capture fluctuations in the model's fitting performance.
We categorize the training process into three stages according to the stability of the fitting performance (panel d). This categorization is informed by an integrated analysis of generalization performance derived from \emph{test error} (panel a) and fitting performance derived from \emph{training error} (panel b) and 8-epoch learning metrics (panel c).
Thus, we design the \emph{prediction changes} metric to measure the shifts in the model's predictions on the training set to pinpoint the early stopping point. For detailed information regarding the experiment settings, please refer to Appendix \ref{appendixa} and Section \ref{sec3}.}
\label{fig1}
\vskip -0.05in
\end{figure*}

To accurately pinpoint an appropriate early stopping point without relying on hold-out data, we introduce a metric called \emph{prediction changes} (PC), as shown in Figure \ref{fig1}. We use PC to measure the number of examples whose predictions have changed between two successive epochs. In other words, this metric quantifies the prediction stability degree of the model trained on a noisy training set. Therefore, this metric offers a straightforward method for tracking the changes in the model's \emph{fitting performance} throughout the training process. These changes, metaphorically likened to wind waves, inspired the name of our method, ``\emph{Label Wave}''. 

The effectiveness of the PC becomes evident when observing Stages 1 and 2 in Figure \ref{fig1}: the curve of PC in Figure \ref{fig1}(b) exactly matches the curve of \emph{test error} in Figure \ref{fig1}(a). 
In Stage 1, as the model begins to fit clean examples (i.e., learns simple patterns), both the PC and the \emph{test error} steadily decrease.
In Stage 2, as depicted in \emph{training error} in Figure \ref{fig1}(b), the model begins to fit mislabeled examples significantly, thereby leading to a rise in \emph{test error}. Concurrently, the model's fitting performance experiences notable fluctuations or even declines. Hence there is an increase in the model's prediction fluctuations, leading to an increase in PC. Thus, the local minimum of PC, marking the turning point between Stages 1 and 2, is identified as the early stopping point.

In Stage 2, the rise in both PC and \emph{test error} indicates that the model's efforts to minimize the total loss across the training set to accurately fit mislabeled examples not only undermine its ability to generalize to test data, but also harm its performance to fit the correct patterns in the training data. In other words, \emph{fitting mislabeled examples impairs the overall model's fitting performance}. This pronounced phenomenon in Stage 2 is termed \emph{learning confusion patterns}, reflecting the model's misinterpretation of the training data distribution.

\clearpage

Notably, our proposed \emph{Label Wave} method can identify the early stopping point at which the model transitions from primarily fitting clean examples to mislabeled ones. This can be achieved without prior knowledge about which training examples are mislabeled examples or the need for separate hold-out validation data. This method not only provides a more efficient way to determine the early stopping point but also ensures sufficient training data to produce an effective model.

Our main contributions can be summarized as follows:
\begin{itemize}[leftmargin=0.4cm,topsep=-2pt]
    \item [1.]
    We present an empirical observation that, when training DNNs from a noisy dataset, fitting mislabeled examples impairs not only the generalization performance but also \emph{the overall model’s fitting performance}. 
    \item [2.]
    Building on the correlation between the model’s generalization and fitting performance established in our empirical analysis, we propose the \emph{Label Wave} method, which uses the changes in the model's predictions on the training set to identify the early stopping point. 
    \item [3.]
    Through extensive experiments, we show both the effectiveness of the \emph{Label Wave} method across various settings and its capability to enhance the performance of existing methods for learning with noisy labels. 
\end{itemize}

\section{Related Work}
\label{gen_inst}
\textbf{Memorization \& Forgetting.}
In deep neural networks, generalization is not solely dictated by the complexity of the hypothesis space \citep{chaudhari2019entropy, advani2020high, Jiang*2020Fantastic}. They can still generalize effectively even without regularizers, a characteristic termed as \emph{Memorization}\footnote{\emph{``Memorization''} is not formally defined, and we denote \emph{Memorization} as training DNNs to fit the assigned label for each particular instance, in a similar spirit as \citet{feldman2020neural} and \citet{forouzesh2023leveraging}.}. 
Building on this, recent publications \citep{toneva2018an, feldman2020neural} have studied the phenomenon known as the ``\emph{forgetting event}'', aiming to understand how training data and network structures influence generalization. Based on the concept of \emph{forgetting}, we observed that when the model starts to fit mislabeled examples, it begins to significantly forget the training data. This forgetting increases until it reaches a turning point, after which the forgetting decreases.


\textbf{Model Selection.}
Numerous indirect methods have been proposed for selecting an appropriate model. These methods include marginal likelihood estimator \citep{pmlr-v51-duvenaud16}, computed gradients \citep{mahsereci2017early},  leveraging unlabeled data \citep{garg2021ratt, deng2021labels, forouzesh2023leveraging}, noise stability \citep{arora2018stronger, morcos2018importance, zhang2019perturbed}, estimating generalization gap \citep{jiang2018predicting, corneanu2020computing}, modeling loss distribution \citep{song2019does, song2021robust, lu2022selc}, and training speed \citep{lyle2020bayesian, ru2021speedy}.
In contrast to the existing methods, \emph{Label Wave} focuses on the selection of an appropriate early stopping point during training process. Notably, this is achieved without the need for additional or hold-out data and requires no preprocessing, ensuring low computational overhead.

\textbf{Learning Stages.}
Traditional paradigms often posit the presence of two stages during the learning process: underfitting and overfitting \citep{goodfellow2016deep, lin2023over}. Researchers exploring models trained on randomly labeled examples have divided the learning process into two distinct stages for a more nuanced perspective on deep learning: an initial stage of ``learning simple patterns'' \citep{arpit2017closer}, followed by a subsequent stage of ``memorization'' \citep{zhang2017understanding}. Further findings, such as epoch-wise Deep Double Descent \citep{belkin2019reconciling, nakkiran2021deep} and ``Grokking'' \citep{power2022grokking, liu2022towards, nanda2023progress}, have highlighted the potential existence of additional learning stages under specific conditions. However, these studies primarily focus on classifying learning stages based on generalization performance when models learn from imperfect data, thereby overlooking variations in model fitting performance.

\textbf{Learning Dynamics.}
A wealth of research endeavors have aimed to classify the ``hardness'' of examples by tracking learning dynamics, which is particularly beneficial for scenarios involving noisy labels or long-tail distributions. Methods like Late Stopping \citep{yuan2023latestopping}, FSLT\&SSFT \citep{maini2022characterizing}, Self-Filtering \citep{wei2022self}, SELFIE \citep{song2019selfie}, and RoCL \citep{zhou2021robust} exemplify this approach. 
However, these studies primarily focus on the dynamic changes of individual examples for the purpose of sample selection, rather than assessing the overall dynamics to distinguish different stages of the training process in learning with noisy labels.

\clearpage

\section{Determining Early Stopping by Tracking Fitting Performance}
\label{sec3}

In this section, we aim to track the fitting performance to pinpoint the turning points of a model's generalization performance during the training process in the presence of noisy labels. 
To provide a clearer view of the model's fitting performance, we incorporate two metrics: stability and variability metrics. With these metrics, we identify and describe the key turning points of fitting performance, and elaborate on their correlation with generalization performance. We further introduce a transitional stage termed ``\emph{learning confusing patterns}'' between the two turning points in fitting performance.
Building on this foundation, we formally introduce our proposed \emph{Label Wave} method, which determines an early stopping point by tracking the changes in the model’s fitting performance.
All experiments presented in this section use a standard ResNet-18 backbone. For detailed information regarding the experiment settings, please refer to Appendix \ref{appendixa}.

Prior to an in-depth discussion, we first clarify three basic concepts \citep{goodfellow2016deep}:
(i) \emph{Test error}, an empirical measure of ``generalization performance'', indicates a model's ability to make accurate predictions on previously unseen data. 
(ii) \emph{Training error},  an empirical measure of ``fitting performance'', indicates a model's ability to fit the training data. 
(iii) \emph{Training process}: Given a model $ f(\cdot; \theta) $ that is being trained on a noisy training set $ \mathcal{D} $ comprised of $ n $ labeled examples $ (\bm{x}_i, y_i) $ where $ i = 1, 2, ..., n $, the model's predicted label for each training instance $ \bm{x}_i $ evolves as training progresses. After $ t $ epochs, this predicted label for $ \bm{x}_i $ is represented as $ \hat{y}_i^t $.

\subsection{Metrics for Tracking Fitting Performance}
\label{sec31}
In Figure \ref{fig2}, we track both the \emph{test error} and the \emph{training error}. From our analysis of the \emph{test error}, we identify two pivotal points where there is a notable shift in generalization performance: Point 1, where the test error attains its global minimum, and Point 2, where the test error transitions to a stable state after rising from this minimum. Notably, Point 1 is typically considered the optimal early stopping point. As determined by the \emph{test error}, Point 1 indicates the model that generalizes best throughout the training process. Furthermore, an examination of the \emph{training error} reveals that, after Point 1, the model starts to significantly fit mislabeled training examples. This suggests that fitting mislabeled examples impairs the model's generalization performance.

\begin{figure*}[h]
\vskip -0.00in
\begin{center}
\centerline{\includegraphics[width=13.8cm]{labelwave_pic3_v18.pdf}}
\vskip -0.05in
\caption{Tracking \emph{test error} and \emph{training error} (mislabeled examples) in training process.}
\label{fig2}
\end{center}
\vskip -0.2in
\end{figure*}

In real-world applications, however, obtaining prior knowledge about which training examples are mislabeled examples is often impossible, thereby eliminating the possibility of relying on how well the model fits mislabeled examples as a criterion for early stopping. Furthermore, as we aim to eliminate the dependency on hold-out datasets, implementing early stopping based on validation errors becomes unfeasible. To address these challenges, we focus on the following two metrics to evaluate the model's ``fluctuations in predictions'' on the training set.

\textbf{Stability Metric.}
We employ the \emph{k-epoch learning} metric \citep{yuan2023latestopping} to quantify the stability of the model's predictions for specific subsets of examples, such as the clean data subset $\mathcal{D}_c$. A higher \emph{k-epoch learning} value indicates that the model's predictions have greater stability in recent epochs and exhibit fewer fluctuations in predictions. Let $ \text{acc}_i^t = \mathbbm{1}_{\hat{y}_i^t=y_i} $ be a binary variable that indicates whether the model accurately classifies example $ i $ at epoch $ t $. The stability of the model's predictions from epoch $ t-k+1 $ to epoch $ t $ can be quantified as follows:
\begin{gather}
\emph{k-epoch learning} = \sum\nolimits_{i\in \mathcal{D}_{c}}^{}{\mathrm{acc}_i^t \wedge \mathrm{acc}_i^{(t-1)}\wedge...\wedge \mathrm{acc}_i^{(t-k+2)} \wedge \mathrm{acc}_i^{(t-k+1)}}.
\end{gather}
\textbf{Variability Metric.}
In contrast to the stability metric, we use \emph{prediction changes} as a variability metric to emphasize the model's inconsistency in classifying training set examples between the current epoch and the previous one. A lower value of \emph{prediction changes} indicates greater stability and fewer fluctuations in the model's predictions for the current epoch. The variability in the model's predictions from epoch $ t-1 $ to epoch $ t $ can be quantified as follows:
\begin{gather}
\emph{prediction changes} = \sum\nolimits_{i\in \mathcal{D}}^{}{\mathbbm{1}_{\hat{y}_i^t \neq \hat{y}_i^{(t-1)}}}.
\end{gather}
\subsection{Fitting Mislabeled Examples Impairs the Model's Fitting Performance}
\label{sec32}
Figure \ref{fig2} presents two approaches to selecting the best generalizing model using both \emph{test (validation) error} and \emph{training error} on the mislabeled examples. However, these methods either require a separate hold-out set or rely on true labels in the training set.
In contrast, our method focuses on a unique concept: \emph{fluctuations in predictions} (FIPs). By quantifying these fluctuations using the stability and variability metrics, we can observe how the model's data-fitting strategy and its corresponding fitting performance change in two nearby points, which we defined in Section \ref{sec31}.

\begin{figure*}[h]
\vskip -0.1in
\begin{center}
    \subfigure[Stability metric, \emph{8-epoch learning}]{\label{fig3a}\includegraphics[width=6.9cm]{labelwave_pic4a_v24.pdf}}
    \subfigure[Variability metric, \emph{prediction changes}]{\label{fig3b}\includegraphics[width=6.88cm]{labelwave_pic4b_v26.pdf}}
    \vskip -0.1in
    \caption{Using stability and variability metrics to track \emph{fluctuations in predictions}.} \label{figmetric}
\end{center}
\vskip -0.1in
\end{figure*}

In Figure \ref{figmetric}, we chart these metrics throughout the training process of the model. Specifically, the stability metric is shown in Figure \ref{fig3a} and the variability metric in Figure \ref{fig3b}. Both metrics are designed to track FIPs. Our observations highlight a clear correlation between the metrics' turning points and the critical points determined by the shifts in generalization performance. This allows us to establish some key empirical findings:
\begin{itemize}[leftmargin=0.4cm,topsep=-2pt]
    \item
    Before Point 1, FIPs gradually decrease.
    \item
    After Point 1, FIPs shift from decreasing to increasing.
    \item
    After Point 2, FIPs shift from increasing to decreasing and eventually converge to a steady state.
\end{itemize}

Our empirical observation reveals a correlation between generalization performance and changes in the model's fitting strategy. We observe noticeable shifts in both performance and strategy before and after Point 1. 
After Point 1, the performance of models from subsequent epochs declines noticeably in their ability to fit the examples from the training set.
Moreover, the data from our $8$-\emph{epoch learning} indicates that more than half of the examples in $\displaystyle D_{c}$ (clean training examples) cannot be stably predicted after Point 1. This significant decline contradicts the assumption that fluctuations following Point 1 only stem from a small subset of challenging examples within $\displaystyle D_{c}$. Therefore, we term the phenomenon where the change in the model's fitting performance at the Point 1 as \emph{fitting mislabeled examples impairs the overall model's fitting performance}. This implies that by tracking \emph{fluctuations in predictions}, we can identify whether the model has started to overfit mislabeled training data just by observing changes in its fitting performance.

Interestingly, the model's behavior from Point 1 to Point 2 is noticeably different from its behavior before and after these points. The stage from Point 1 to Point 2 cannot simply be considered as either ``incorporation of prior'' \citep{bengio2009learning} or ``overfitting the dataset''. Instead, we propose to see this as a transitional stage in learning with noisy labels, situated between the widely accepted stages of ``\emph{learning simple patterns}'' \citep{arpit2017closer} and ``\emph{memorization}'' \citep{zhang2017understanding}. We have coined this transitional stage ``\emph{learning confusing patterns}''. A more detailed discussion on implications of this stage and Point 2 will be explored in Appendix \ref{sec6}.

\subsection{Label Wave Method}
\label{sec4}


Based on our observations, we identified a correlation between the model's fitting and generalization performance when learning with noisy labels.
With this understanding, we formally introduce our proposed \emph{Label Wave} method. This method utilizes the observed correlation between \emph{fluctuations in predictions} and \emph{test error}. We aim to pinpoint an appropriate early stopping moment by closely tracking the model’s predictions on the noisy training set. This moment, referred to as Point 1 in Section \ref{sec31}, is where the model begins to significantly fit mislabeled examples. To keep our method simple, intuitive, and computationally efficient, we directly refer to the \emph{variability} metric explored in section \ref{sec31}, which we term as \emph{prediction changes} or \emph{PC}.



\textbf{Algorithm Flow.} We use the \emph{prediction changes}, represented as $ \rm{PC}_{\emph{t}} $, to track the changes in the model's fitting performance at the epoch $t$. The first turning point of $ \rm{PC}_{\emph{t}} $ is identified as the early stopping point for model selection. 
In practice, tracking $ \rm{PC}_{\emph{t}} $ is often susceptible to uncertainties inherent in the training process. 
We enhance the robustness of our \emph{Label Wave} method by utilizing the \emph{Moving Averages} techniques. By averaging $ \rm{PC}_{\emph{t}} $ over the recent $ k $ epochs ({see Appendix \ref{appendixd} for the sensitivity analysis of $ k $}), we derive a more stable version, denoted as $ \rm{PC}_{\emph{t}}^{\prime}$, as follows:
\begin{gather}
\mathrm{PC}_t^\prime = (\mathrm{PC}_t+\mathrm{PC}_{t-1}+...+\mathrm{PC}_{t-k+1})/k.
\end{gather}
As depicted in Algorithm \ref{alg1}, we compute $\rm{PC}_{\emph{t}}$ using the predictions of $\displaystyle f(\cdot ; \vtheta) $ on the noisy training set $D$. By applying the \emph{Moving Averages} and \emph{Patience}, we identify the first local minimum point of the $\rm{PC}_{\emph{t}}^{\prime}$ and designate it as the early stopping point for the model selection, as follows:
\begin{gather}
\text{\emph{Early Stopping Point}} = t_{\text{first-min}},\ \text{where}\ t_{\text{first-min}}\ \text{is the first local minimum of}\ \mathrm{PC}_{t}^{\prime}.
\end{gather}
The \emph{Label Wave} method allows us to pinpoint the moment when the model’s fitting performance stability starts to decline by observing \emph{fluctuations in predictions} and computing \emph{prediction changes}.
Based on our discussion in Section \ref{sec32}, stopping the training process at such a moment can prevent the model from beginning to overfit the mislabeled training data, ensuring the selection of a model with close-to-optimal generalization performance.
This robust and intuitive method not only does not rely on validation sets and prior knowledge of training labels but also enhances the generalization of the selected model. To solidify our understanding and validate the efficacy of the \emph{Label Wave} method, we embark on a series of empirical experiments detailed in the upcoming section. For scenarios where the \emph{Label Wave} method does not work, see the discussion in Appendix \ref{notapplicable}.

\noindent\begin{minipage}{.99\textwidth}
\vskip -0.05in
\begin{algorithm}[H]
   \caption{Label Wave}
   
    Let $\displaystyle \vtheta_o $ be the initial parameters and $v$ be the local minimum of PC.
    Let $p$ be the ``\emph{Patience}'', representing the number of times a worsening PC is observed before halting.

    $\displaystyle \vtheta $ $\gets$ $\displaystyle \vtheta_o $, $t$ $\gets$ $0$, $i$ $\gets$ $0$, $v$ $\gets$ $\infty$
\begin{algorithmic}[1]
\label{alg1}
   \WHILE{$i$ $<$ $p$}
   \STATE Update $\displaystyle \vtheta $ by running the training for $n$ steps, and $t$ $\gets$ $t+n$.
   \STATE $\rm{PC}_{\emph{t}}$ $\gets$ Compute \emph{prediction changes} (PC) in step $t$.
   \STATE $\rm{PC}_{\emph{t}}^{\prime}$ $\gets$ \emph{Moving Averages} PC in recent $k$ steps.
       \IF{$\rm{PC}_{\emph{t}}^{\prime}$ $<$ $v$}
       \STATE $v$ $\gets$ $\rm{PC}_{\emph{t}}^{\prime}$ ; $i$ $\gets$ $0$, $\displaystyle \vtheta^* $ $\gets$ $\displaystyle \vtheta $, $t^*$ $\gets$ $t$  \quad // Models stored at every new local minimum.
       \ELSE
       \STATE $i$ $\gets$ $i+1$ \quad // Counting \emph{Patience} when $\rm{PC}_{\emph{t}}^{\prime}$ is larger than local minimum.
       \ENDIF
   \ENDWHILE
\end{algorithmic}
   Best parameters are $\displaystyle \vtheta^* $, and best number of training steps is $t^*$.
\end{algorithm}
\end{minipage}



\clearpage

\section{Experiments}
\label{sec5}

In this section, we empirically demonstrate the effectiveness of our \emph{Label Wave} method. In Section \ref{sec51}, we conduct comprehensive experiments to validate the broad applicability of our method across different settings.
Section \ref{sec52} highlights the capability of the \emph{Label Wave} method to enhance the generalization performance of existing learning with noisy labels methods. Furthermore, in Section \ref{sec53}, we examine the Kendall $\tau$ correlation between our method and test accuracy. This analysis is compared with empirical validation on hold-out data. The findings suggest that \emph{Label Wave} amplifies the generalization performance of learning with noisy labels methods not only by leveraging more training data but also by improving the precision in identifying early stopping points.

\subsection{Effectiveness of Label Wave}
\label{sec51}
Building upon the \emph{Label Wave} method experiments conducted using the ResNet-18 backbone, we performed additional tests. These experiments demonstrate the consistent efficacy of our proposed \emph{Label Wave} method across a wide range of settings, including multiple datasets, diverse network architectures, a range of parameters, various optimizers, and different levels and types of label noise. 
As evident in Table \ref{tab1}, there is only a slight difference between the test accuracy of models selected by the \emph{Label Wave} method and the global maximum test accuracy during the training process. This indicates that the Label Wave method consistently selects models, irrespective of different components and parameters, that are at or near the global maximum in test accuracy. {More experimental results and details of experiment settings can be found in Appendix \ref{appendixb}.}

\textbf{Noise.}
We assessed the capability of \emph{Label Wave} method to handle learning from various levels and types of label noise. These include 10\% to 80\% of the incorrect labels with the \emph{Instance-dependent} noise (abbreviated as Ins.) \citep{xia2020part} and \emph{Symmetric} noise (abbreviated as Sym.) \citep{van2015learning}. Notably, with \emph{CIFAR-N} (using its ``Random 1/2/3'' and ``worst'' setting in \emph{CIFAR-10N} and ``worst'' setting in \emph{CIFAR-100N}, characterized by approximately 40\% real-world noise) \citep{wei2021learning}, we evaluate effectiveness of \emph{Label Wave} method on real-world noise.

\textbf{Architectures.}
We verified the effectiveness of our method in learning with noisy labels over several commonly used deep learning models, including \emph{ResNet} \citep{he2016deep}, \emph{VGG} \citep{simonyan2014very}, \emph{Inception-v3} \citep{szegedy2016rethinking}, and \emph{DenseNet} \citep{huang2017densely}.

\textbf{Datasets.}
Our evaluations also confirmed the effectiveness of \emph{Label Wave} method across multi-datasets. These datasets comprise seven vision-oriented sets: \emph{CIFAR-10}, \emph{CIFAR-100} \citep{krizhevsky2009learning}, \emph{CIFAR-N} \citep{wei2021learning}, {\emph{Clothing1M} \citep{xiao2015learning}}, {\emph{WebVision} \citep{li2017webvision}}, {\emph{Food101} \citep{bossard14}}, and \emph{Tiny-ImageNet} \citep{le2015tiny}, along with a text-oriented dataset: \emph{NEWS} \citep{kiryo2017positive, yu2019does}. We tested our method on class imbalanced datasets \emph{CID-CE} and with using class imbalanced method \emph{CID-LDAM} \citep{cao2019learning}.

\textbf{Parameters \& Optimizers.}
The robustness of the \emph{Label Wave} method is validated by testing it with different learning rates, batch sizes, random seeds, and optimizers. By adjusting the batch sizes to {64, 128, 256}, learning rates to {0.01, 0.005, 0.001}, random seeds to {1, 2, 3, 4, 5}, and employing different optimizers such as \emph{SGD with momentum} \citep{robbins1951stochastic, polyak1964some}, \emph{RMSprop} \citep{tieleman2012lecture}, and \emph{Adam} \citep{kingma2014adam}, we demonstrate that the \emph{Label Wave} method remains resilient to reasonable variations in both parameters and optimizers.

\begin{table*}[b]
\vskip -0.3in
\renewcommand{\arraystretch}{1.15}
\centering
	\caption{Differences (mean±std) among the model selection methods. Lower is better.}
	\label{tab1}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{0.5mm}{
\begin{tabular}{c|ccccccc}
\toprule
\toprule
\textbf{Noise (Sym.)} & \emph{20\%} & \emph{30\%} & \emph{40\%} & \emph{50\%} & \emph{60\%} & \emph{70\%}& \emph{80\%} \\
\midrule
Global Maximum (\%) & 84.08$\pm$0.10 & 81.73$\pm$0.37 & 80.45$\pm$0.68 & 75.48$\pm$0.46 & 68.57$\pm$0.70 & 57.57$\pm$0.98 & 37.46$\pm$0.92  \\
Label Wave (\%) & 83.43$\pm$0.32 & 81.48$\pm$0.36 & 80.15$\pm$0.67 & 75.20$\pm$0.73 & 68.21$\pm$1.02 & 56.64$\pm$1.57 & 37.08$\pm$0.95  \\
\midrule
Difference & \textbf{0.65\%} & \textbf{0.25\%}& \textbf{0.30\%} & \textbf{0.28\%} & \textbf{0.36\%} & \textbf{0.93\%} & \textbf{0.38\%}   \\
\midrule
\midrule
\textbf{Noise (Ins.)} & \emph{20\%} & \emph{30\%} & \emph{40\%} & \emph{50\%} & \emph{60\%} & \emph{70\%}& \emph{80\%} \\
\midrule
Global Maximum (\%) & 84.28$\pm$0.45 & 82.83$\pm$0.65 & 77.69$\pm$0.71 & 64.74$\pm$0.78 & 46.80$\pm$1.52 & 29.48$\pm$1.02 & 20.59$\pm$0.69  \\
Label Wave (\%) & 83.99$\pm$0.72 & 82.68$\pm$0.45 & 76.81$\pm$0.76 & 63.87$\pm$0.68 & 45.92$\pm$2.00 & 28.87$\pm$0.69 & 20.08$\pm$1.06  \\
\midrule
Difference & \textbf{0.30\%} & \textbf{0.15\%}& \textbf{0.88\%} & \textbf{0.87\%} & \textbf{0.88\%} & \textbf{0.61\%} & \textbf{0.50\%}   \\
\bottomrule  
\bottomrule 
\end{tabular}
}
}
\end{table*}

\clearpage

\begin{table*}[h]
\vskip -0.3in
\renewcommand{\arraystretch}{1.15}
\centering
	\label{tab2}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{0.4mm}{
\begin{tabular}{c|ccccccc}
\toprule
\toprule
\textbf{Noise \& Datasets} & \emph{Random 1} & \emph{Random 2} & \emph{Random 3} & \emph{Ins. 10\%} & \emph{Sym. 10\%} & \emph{CID-CE} & \emph{CID-LDAM} \\
\midrule
Global Maximum (\%) & 83.21$\pm$0.23 & 82.86$\pm$0.24 & 83.10$\pm$0.25 & 85.91$\pm$0.23 & 85.46$\pm$0.12 & 56.03$\pm$0.55 & 61.80$\pm$0.93   \\
Label Wave (\%) & 83.03$\pm$0.35 & 82.53$\pm$0.23 & 82.92$\pm$0.50 & 85.56$\pm$0.72 & 84.92$\pm$0.06 & 55.31$\pm$0.86 & 61.07$\pm$0.88    \\
\midrule
Difference & \textbf{0.18\%} & \textbf{0.13\%}& \textbf{0.18\%} & \textbf{0.35\%} & \textbf{0.54\%} & \textbf{0.72\%} & \textbf{0.73\%}    \\  
\midrule
\midrule
\textbf{Datasets} & \emph{CIFAR-10} & \emph{CIFAR-100} & \emph{CIFAR-10N} & \emph{CIFAR-100N} & \emph{NEWS} & \emph{Tiny-ImageNet} & \emph{Adam} \\
\midrule
Global Maximum (\%) & 79.99$\pm$0.57 & 49.92$\pm$0.81 & 76.23$\pm$0.17 & 47.48$\pm$0.49 & 42.58$\pm$0.79 & 34.88$\pm$0.15 & 79.12$\pm$0.57    \\
Label Wave (\%) & 79.54$\pm$0.91 & 49.39$\pm$0.69 & 75.98$\pm$0.49 & 47.04$\pm$0.38 & 42.06$\pm$1.15 & 34.20$\pm$0.42 & 78.92$\pm$0.66   \\
\midrule
Difference & \textbf{0.45\%} & \textbf{0.52\%} & \textbf{0.25\%} & \textbf{0.45\%} & \textbf{0.51\%} & \textbf{0.68\%} & \textbf{0.21\%}   \\
\midrule
\midrule
\textbf{Architectures} & \emph{ResNet-18} & \emph{ResNet-34} & \emph{ResNet-50} & \emph{ResNet-101} & \emph{VGG-16} & \emph{Inception-v3} & \emph{Dense-121} \\
\midrule
Global Maximum (\%) & 79.86$\pm$0.52 & 79.81$\pm$0.48 & 79.44$\pm$1.33 & 77.35$\pm$0.40 & 78.00$\pm$0.53 & 57.23$\pm$0.52 & 66.09$\pm$0.22  \\
Label Wave (\%) & 79.36$\pm$0.77 & 79.50$\pm$0.42 & 78.94$\pm$1.21 & 76.99$\pm$0.75 & 77.77$\pm$0.43 & 56.80$\pm$0.73 & 65.87$\pm$0.38  \\
\midrule
Difference & \textbf{0.50\%} & \textbf{0.31\%}& \textbf{0.49\%} & \textbf{0.36\%} & \textbf{0.23\%} & \textbf{0.43\%} & \textbf{0.22\%}   \\
\midrule
\midrule
\textbf{Parameters } & \emph{LR. 0.01} & \emph{LR. 0.005} & \emph{LR. 0.001} & \emph{BS. 64} & \emph{BS. 128} & \emph{BS. 256} & \emph{RMSprop} \\
\midrule
Global Maximum (\%) & 80.08$\pm$0.62 & 76.98$\pm$0.79 & 72.55$\pm$0.57 & 80.23$\pm$0.57 & 80.11$\pm$1.02  & 76.88$\pm$0.66 & 76.79$\pm$0.61  \\
Label Wave (\%) & 80.38$\pm$0.57 & 76.67$\pm$0.95 & 72.14$\pm$0.55 & 80.59$\pm$0.41 & 79.69$\pm$0.68  & 76.24$\pm$0.49 & 76.22$\pm$0.95  \\
\midrule
Difference & \textbf{0.30\%} & \textbf{0.31\%}& \textbf{0.40\%} & \textbf{0.36\%} & \textbf{0.43\%} & \textbf{0.64\%} & \textbf{0.57\%}   \\
\bottomrule  
\bottomrule 
\end{tabular}
}
}
\end{table*}

\subsection{Enhancing Existing Learning with Noisy Label Methods}
\label{sec52}
To verify the effectiveness of our method in practical applications, we apply our proposed \emph{Label Wave} method within a range of state-of-the-art learning with noisy labels methods. Among the methods we have explored are robust loss functions \citep{ijcai2020p305}, robust regularization \citep{xia2020robust, wei2022smooth}, label noise correction \citep{liu2020early, cheng2021learning}, sparsity over-parameterization \citep{liu2022robust}, and the baseline Cross-Entropy \citep{rubinstein1999cross}. 

As shown in Tables \ref{tab3} and \ref{tab4}, we conducted evaluations using ResNet-18 on CIFAR-10 and ResNet-34 on CIFAR-100 \citep{krizhevsky2009learning, he2016deep}, both tainted with 40\% \emph{Symmetric} noise (abbreviated as Sym.) \citep{van2015learning}. Our primary assessment metric was the test accuracy of the selected model, determined by the average and standard deviation over five runs. Notably, the \emph{Label Wave} method consistently outperformed conventional validation methods employing 5\% to 30\% hold-out data. This indicates that the \emph{Label Wave} method works as a more effective early stopping method than conventional hold-out data validation, thus augmenting the generalization performance of models trained by existing learning with noisy label methods. {Results under more experimental settings can be found in the appendix \ref{appendixc}.}

\begin{table*}[h]
\renewcommand{\arraystretch}{1.10}
\vskip -0.15in
\centering
	\caption{Test accuracy (mean$\pm$std) of each method on CIFAR-10 (with 40\% Sym. label noise).}
	\label{tab3}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{c|ccccc}
\toprule
Method & \emph{Val.} 5\% & \emph{Val.} 10\% & \emph{Val.} 20\% & \emph{Val.} 30\% & \emph{Label Wave}  \\
\midrule

CE \citep{rubinstein1999cross} & 80.29$\pm$0.66\% & 79.93$\pm$0.52\% & 78.94$\pm$0.52\% & 77.38$\pm$0.59\% & \textbf{81.61$\pm$0.44\%} \\

Taylor-CE \citep{ijcai2020p305} & 83.85$\pm$0.61\% & 83.28$\pm$0.34\% & 82.33$\pm$0.43\% & 81.66$\pm$0.12\% & \textbf{85.06$\pm$0.30\%} \\

ELR \citep{liu2020early} & 90.23$\pm$0.47\% & 88.99$\pm$0.41\% & 88.43$\pm$0.38\% & 87.24$\pm$0.41\% & \textbf{90.45$\pm$0.52\%} \\

CDR \citep{xia2020robust} & 86.45$\pm$0.36\% & 85.98$\pm$0.43\% & 85.60$\pm$0.50\% & 83.71$\pm$0.23\% & \textbf{87.69$\pm$0.10\%} \\

CORES \citep{cheng2021learning} & 87.26$\pm$0.32\% & 86.9$\pm$0.25\% & 85.78$\pm$0.69\% & 85.17$\pm$0.57\% & \textbf{87.74$\pm$0.13\%}  \\

NLS \citep{wei2022smooth} & 82.62$\pm$0.59\% & 81.89$\pm$0.18\% & 81.12$\pm$0.53\% & 79.23$\pm$0.32\% & \textbf{83.45$\pm$0.19\%}  \\

SOP \citep{liu2022robust} & 86.40$\pm$1.04\% & 86.80$\pm$0.77\% & 87.12$\pm$0.19\% & 86.37$\pm$0.66\% & \textbf{88.42$\pm$0.38\%}  \\

\bottomrule  
\end{tabular}
}
}
\end{table*}


\begin{table*}[h]
\renewcommand{\arraystretch}{1.10}
\vskip -0.2in
\centering
	\caption{Test accuracy (mean$\pm$std) of each method on CIFAR-100 (with 40\% Sym. label noise).}
	\label{tab4}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{1.3mm}{
\begin{tabular}{c|ccccc}
\toprule
%\cline{2-11}
Method & \emph{Val.} 5\% & \emph{Val.} 10\% & \emph{Val.} 20\% & \emph{Val.} 30\% & \emph{Label Wave}  \\
\midrule

CE \citep{rubinstein1999cross} & 49.69$\pm$0.89\% & 48.42$\pm$0.80\% & 45.96$\pm$1.17\% & 43.65$\pm$0.53\% & \textbf{50.96$\pm$0.30\%} \\

Taylor-CE \citep{ijcai2020p305} & 56.67$\pm$0.35\% & 56.48$\pm$0.18\% & 55.23$\pm$0.50\% & 54.38$\pm$0.29\% & \textbf{57.64$\pm$0.28\%} \\

ELR \citep{liu2020early} & 64.53$\pm$0.36\% & 62.05$\pm$0.50\% & 60.09$\pm$0.74\% & 57.49$\pm$0.42\% & \textbf{65.36$\pm$0.39\%} \\

CDR \citep{xia2020robust} & 61.93$\pm$0.38\% & 60.82$\pm$0.38\% & 58.27$\pm$0.50\% & 54.90$\pm$1.03\% & \textbf{63.34$\pm$0.15\%} \\

CORES \citep{cheng2021learning} & 44.32$\pm$0.82\% & 44.48$\pm$0.35\% & 43.32$\pm$0.57\% & 41.28$\pm$0.47\% & \textbf{45.03$\pm$0.38\%}  \\

NLS \citep{wei2022smooth} & 57.08$\pm$0.62\% & 55.89$\pm$0.26\% & 54.09$\pm$0.57\% & 52.82$\pm$0.28\% & \textbf{58.05$\pm$0.15\%}  \\

SOP \citep{liu2022robust} & 67.27$\pm$0.45\% & 66.28$\pm$0.61\% & 64.46$\pm$0.48\% & 63.02$\pm$0.48\% & \textbf{68.53$\pm$0.30\%}  \\

\bottomrule  
\end{tabular}
}
}
\end{table*}

\clearpage
\subsection{Correlation with Test Accuracy}
\label{sec53}
In this subsection, we provide evidence that our proposed method exhibits superior performance compared to hold-out validation across a diverse range of validation set sizes and noise rates. We employ the same experimental settings as in Section \ref{sec3}.

\textbf{Hold-out Set Sizes.} 
We utilized the same subset of training data to calculate both \emph{prediction changes} (PC) and hold-out validation. This implies that we only use a limited-size subset from the training data to compute PC. In the lower subfigure of Figure \ref{fig5a}, the Kendall $\tau$ correlation \citep{kendall1938new} is illustrated, showcasing the association between the ranking of model selection and actual test accuracy for both methods. In the upper subfigure of Figure \ref{fig5a}, we focus on the test accuracy of models selected by both the \emph{Label Wave} and hold-out validation methods. 
Our experiments emphasize that increasing the size of the hold-out set improves its precision in model selection. However, as the size of the hold-out set increases, the data available for training decreases, leading to a reduction in test accuracy for candidate models obtained during the training process. These findings suggest that our proposed \emph{Label Wave} method can select models more precisely and yield better test accuracy, even when computed on a smaller set. 

\textbf{Noise Rates.} We further compare the test accuracy of models selected by our proposed \emph{Label Wave} method and the test accuracy of models selected by hold-out validation in various set sizes with varying noise rates. As shown in Figure \ref{fig5b}, we demonstrate the gain in model test accuracy selected by the \emph{Label Wave} method. This underscores the robustness and superior efficacy of the \emph{Label Wave} method, especially in high levels of noise and large set sizes scenarios.

\begin{figure*}[h]
\vskip -0.1in
\label{fig4}
\begin{center}
    \subfigure[Test accuracy and $\tau$ of compared methods]{\label{fig5a}\includegraphics[width=6.3cm]{labelwave_pic7a_v3.pdf}}
    \subfigure[Test accuracy gain of \emph{Label Wave}]{\label{fig5b}\includegraphics[width=6.3cm]{labelwave_pic7b_v3.pdf}}
    \vskip -0.05in
    \caption{We aim to compare the test accuracy of models selected at the early stopping point by the \emph{Label Wave} method and those selected by the hold-out validation method. (a) We selected a subset of the training data, with set sizes ranging from 250 to 16,000. This subset was used for both to compute the prediction change in the \emph{Label Wave} method and serving as the hold-out set in hold-out validation, respectively. We computed the Kendall $\tau$ correlation and the test accuracy of the models selected by these two methods. (b) Further analysis was conducted to evaluate the difference in test accuracy between models selected by our proposed \emph{Label Wave} method and those selected by the hold-out validation method with noise rates ranging from 20\% to 60\%.}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Conclusion}
In this paper, we introduced the \emph{Label Wave} method for early stopping in the presence of label noise. Our proposed method uses the \emph{prediction changes} metric to track changes in a model's fitting performance to pinpoint the early stopping point, eliminating the need for separate hold-out data.
Extensive experiments showcased the effectiveness of the \emph{Label Wave} method across various settings, leading to improved generalization performance in methods for learning with noisy labels. Furthermore, we introduced a transitional stage in learning with noisy labels, denoted as \emph{learning confusing patterns}. 
Looking forward, there is potential to devise an advanced early stopping metric that could surpass the capabilities of the \emph{prediction changes} metric, thereby enhancing the \emph{Label Wave} method. A deeper dive into the transitional stage may offer pivotal insights into how deep neural networks behave when learning with noisy labels. 

\subsubsection*{Acknowledgments}
The authors thank the anonymous reviewers and area chairs for their insightful and helpful comments. Suqin Yuan especially thanks Muyang Li, Runqi Lin, and Yingbin Bai who gave helpful advice during the rebuttal. Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031.

\input{iclr2024_conference.bbl}
\bibliographystyle{iclr2024_conference}

\clearpage

\appendix
\section{Discussion on \emph{learning confusing patterns}}
\label{sec6}
In this section, we delve into Point 2, where the model's test error transitions from increasing to stabilizing, by utilizing the metrics introduced in Section \ref{sec3}. Moreover, we introduce a novel transitional stage in learning with noisy labels, termed \emph{learning confusing patterns}.

\begin{figure*}[h]
\vskip -0.05in
\label{fig6}
\begin{center}
    \subfigure[Tracking model's behavior between two points]{\label{fig6a}\includegraphics[width=6.8cm]{labelwave_pic6a.png}}
    \subfigure[Our proposed new transitional stage]{\label{fig6b}\includegraphics[width=6.8cm]{labelwave_pic6b_v7.pdf}}
    \vskip -0.00in
    \caption{Based on the multi-metrics we are tracking for the model's generalization and fitting performance between Point 1 and Point 2 (as shown in panel a), we propose a new transitional stage of learning with noisy labels, termed "\emph{learning confusing patterns}" (shown in panel b).}
\end{center}
\vskip -0.05in
\end{figure*}

As shown in Figure \ref{fig6a}, when tracking \emph{fluctuations in predictions} using stability (\emph{8-epoch learning}) and variability (\emph{prediction changes}) metrics, we noticed Point 2 is a turning point in the model's fitting performance,  similar to the early stopping point, Point 1. Specifically, in Point 2, the \emph{fluctuations in predictions} transitioned from increasing to decreasing trends, eventually stabilizing at a specific value.
Here, we empirically explain the evolution of \emph{fluctuations in predictions} during the training process. Before reaching Point 1, as training progressed, the model's generalization performance improved. The reduction in \emph{fluctuations in predictions} suggests that a model with superior generalization is more noise-resistant \citep{arora2018stronger, morcos2018importance, zhang2019perturbed, forouzesh2023leveraging}, resulting in more stable fits.
After Point 2, the model began its shift towards memorization.
This decrease in \emph{fluctuations in predictions} reflects the beginning of the model fitting to the assigned label for each specific example \citep{arpit2017closer, toneva2018an, feldman2020neural, xia2023regularly}, showing more consistent fits.

Nevertheless, the increase in \emph{fluctuations in predictions} between Point 1 and Point 2 suggests the model is neither purely overfitting nor excellent in generalization during this stage. As shown in Figure \ref{fig6b}, we describe this stage as the ``\emph{learning confusing patterns}'', during which the stability of the model's predictions persistently decrease. 
Between these two points, the model's effort to minimize losses across the noisy training set causes it to adapt to mislabeled examples. This integration of incorrect information into generalization compromises the model's overall representation of the noisy training set, leading to an increase in \emph{fluctuations in predictions} and a concurrent decline in both generalization and fitting performance. In contrasting the earlier stage before Point 1 where the model ``learn from simple patterns'', the learning paradigm during \emph{learning confusing patterns} can be termed ``learn from misled patterns'', indicating that, during this stage, while the model still generalizes by learning patterns from the data, the patterns it learns are overly complex and incorrect due to the model try to generalize to mislabeled samples. 

Building upon this, we identify Point 2 as the critical point where either generalization or memorization takes precedence. Beyond Point 2, the \emph{fluctuations in predictions} begin to notably decline, indicating the model's shift towards fitting individual example labels and achieving stable fitting across the entire noisy training set. However, the marked transition at Point 2, from misguided generalization to overfitting, cannot be satisfactorily explained by solely relying on our predefined stability and variability metrics. This presents an intriguing open question for further investigation.

\clearpage

\section{Detailed Overview of Basic Model in our Experiments}
\label{appendixa}

\textbf{Architecture}
\begin{itemize}
    \item \textbf{Framework}: PyTorch, Version 1.11.0.
    \item \textbf{Model Type}: Standard ResNet-18, sourced from the PyTorch torchvision library.
    \item \textbf{Dropout}: Aligned with the standard ResNet-18, we do not incorporate dropout.
\end{itemize}

\textbf{Parameters}
\begin{itemize}
    \item \textbf{Seed}: 1
    \item \textbf{Batch Size}: 128
    \item \textbf{Learning Rate}: Fixed at 0.01.
    \item \textbf{Optimizer}: Employs \texttt{optim.SGD} with momentum = 0.9.
    \item \textbf{Loss Function}: Utilizes the \texttt{CrossEntropyLoss} from the \texttt{nn} module.
\end{itemize}

\textbf{Dataset \& Pre-processing}
\begin{itemize}
    \item \textbf{Dataset}: The CIFAR-10 dataset, which is accessible via the \texttt{torchvision.datasets} module. 20\% of the training data is held out for validation during the training process.
    \item \textbf{Normalization}: Leveraging the \texttt{torchvision.transforms} module, we normalize all pixel values to fit within the [0,1] range.
    \item \textbf{Cropping}: Images undergo random cropping procedures. For any 32x32 image, potential padding might be added, from which random 32x32 crops are then extracted.
    \item \textbf{Rotation}: Images are subjected to random rotations, limited to a range of ±15 degrees. This step ensures the model's robustness against various image orientations.
\end{itemize}

\textbf{Label Noise}
\begin{itemize}
    \item \textbf{Symmetric Noise}: We infused 40\% symmetric noise into 80\% of the CIFAR-10 training set, affecting 40,000 examples.
\end{itemize}

\clearpage
\section{Effectiveness of Label Wave}
\label{appendixb}

In this appendix, we delve into the comprehensive details of experiments validating the effectiveness of the \emph{Label Wave} method within various noisy label learning scenarios.
We have presented specific results from each setting in the main content (see Table \ref{tab1} in Section \ref{sec51}). However, here, we will discuss in-depth the experimental setups. 
Our experiments spanned a wide array of settings commonly employed in existing methods for learning with noisy labels. This encompassed varied architectures, datasets, noise types and levels, parameters, and optimizers. The overarching aim of these diverse setups was to gauge the effectiveness of the \emph{Label Wave} method across different environments. Unless otherwise noted, the parameters and components remain consistent with the basic model in Appendix B outside of the each declared setting. 


\subsection{Network Architectures}

In our exploration, we employed a selection of prominent deep learning architectures to validate the versatility and effectiveness of the \emph{Label Wave} method:

\begin{itemize}
    \item \textbf{ResNet} \citep{he2016deep}: Renowned for its profound success in deep learning endeavors, especially in the realm of image classification, we engaged with multiple variants based on the original paper: \emph{ResNet-18}, \emph{ResNet-34}, \emph{ResNet-50}, and \emph{ResNet-101} .

    \item \textbf{VGG} \citep{simonyan2014very}: Known for its straightforward and effective architecture, we employed the \emph{VGG-16}. The architecture's clarity and consistency enabled us to assess the \emph{Label Wave} method's effectiveness with a well-established feature hierarchy.

    \item \textbf{Inception-v3} \citep{szegedy2016rethinking}: With its unique design, \emph{Inception-v3} offers diverse receptive fields without taxing computational resources. Using this architecture, we evaluated our \emph{Label Wave} method in networks optimized for multi-scale feature extraction.

    \item \textbf{DenseNet} \citep{huang2017densely}: By employing the densely connected convolutional architecture of \emph{DenseNet-121}, we aimed to assess how the \emph{Label Wave} method performs in a network characterized by robust gradient flow due to its dense connections.
\end{itemize}

\subsection{Datasets Employed}
To assess the \emph{Label Wave} method's versatility, we employed several datasets, widely recognized in the learning with noisy labels domain:

\begin{itemize}
    \item \textbf{Vision-oriented Datasets}:
        \begin{itemize}
            \item \emph{CIFAR-10} and \emph{CIFAR-100} \citep{krizhevsky2009learning}: Established benchmarks for image classification; CIFAR-10 contains 10 classes and CIFAR-100 contains 100 classes, respectively, and have 50,000 training examples and 10,000 test examples.
            \item \emph{CIFAR-N} \citep{wei2021learning}: An augmented version of the CIFAR dataset, we use it ``worst'' setting, which is marked by approximately 40\% real-world label noise, and presented an opportunity to test the resilience of the \emph{Label Wave} method.
            \item \emph{Tiny-ImageNet} \citep{le2015tiny}: With 200 classes, each class contains 500 training images, making it a comprehensive dataset for evaluating performance.
        \end{itemize}
    
    \item \textbf{Text-oriented Dataset}:
        \begin{itemize}
            \item \emph{NEWS} \citep{kiryo2017positive, yu2019does}: We leveraged the NEWS dataset to examine the \emph{Label Wave} method's applicability beyond images. The dataset consists of news articles categorized into various themes. For NEWS, we borrowed the pre-trained word embeddings from \cite{pennington2014glove}, and a 3-layer MLP is used with Softsign active function.

        \end{itemize}
            
   \item {\textbf{Real-world Scenarios Dataset}:}
        \begin{itemize}
        \item {\emph{CIFAR-N} \citep{wei2021learning}: An augmented version of the CIFAR dataset, we use it ``Random 1, 2, 3'' setting, which is marked by approximately 20\% real-world label noise, and presented an opportunity to test the resilience of the \emph{Label Wave} method.}
        \item \emph{Clothing1M} \citep{xiao2015learning}: A large-scale dataset that focuses on clothing classification. It contains over 1 million images of clothing items, categorized into 14 classes. 
        \item \emph{WebVision} \citep{li2017webvision}: Designed to mirror the real-world challenges of web-based image recognition, this dataset contains images collected from the internet. It comprises 2.4 million images from 1,000 different classes, mimicking the class distribution of ImageNet. 
        \item \emph{Food101} \citep{bossard14}: This dataset is focused on food recognition, consisting of 101 food categories, with 101,000 images. Each category contains 750 training images and 250 test images. The images are not artificially labelled and thus contain some level of real-world noise. 
        \end{itemize}
    \item \textbf{Class imbalanced dataset}: We tested our method on class imbalanced datasets, by setting the Imbalance Factor to 0.1 and the Noise Ratio (Sym.) to 0.4. Our experiments were conducted using our method in Cross-Entropy \citep{rubinstein1999cross} (\emph{CID-CE}) and class imbalanced method LDAM \citep{cao2019learning} (\emph{CID-LDAM}).
\end{itemize}


\begin{table*}[h]
\vskip -0.2in
\renewcommand{\arraystretch}{1.15}
\centering
	\caption{{Differences (mean±std) among the model selection methods. Lower is better.}}
	\label{tabapp1}
\resizebox{0.7\columnwidth}{!}{
\setlength{\tabcolsep}{2.8mm}{

\begin{tabular}{c|ccccccc}
\toprule
\toprule
\textbf{Datasets} & \emph{Clothing1M} & \emph{WebVision} & \emph{Food101} \\
\midrule
Global Maximum (\%) & 70.56$\pm$0.11 & 57.58$\pm$0.14 & 80.73$\pm$1.46     \\
Label Wave (\%) & 70.12$\pm$0.34 & 57.24$\pm$0.34 & 80.12$\pm$1.01    \\
\midrule
Difference & \textbf{0.44\%} & \textbf{0.34\%}& \textbf{0.61\%}  \\
\bottomrule  
\bottomrule 
\end{tabular}
}
}
\end{table*}


\subsection{Label Noise}
We examined the \emph{Label Wave} method's effectiveness across various label noise conditions:
\label{notapplicable}
\begin{itemize}
    \item \textbf{Symmetric Noise (Sym.)} \citep{van2015learning}: Symmetric noise levels were varied between 20\% to 80\%, which provided environments to examine how the \emph{Label Wave} method works with data that has uniformly distributed noise across different labels.
    
    \item \textbf{Instance-dependent Noise (Ins.)} \citep{xia2020part}: By introducing 20\% to 80\% instance-dependent noise, we aimed to examine how \emph{Label Wave} method works with data that has noise correlated with the instance characteristics.

    \item \textbf{Real-world Noise} \citep{wei2021learning}: We have detailed the \emph{CIFAR-N} dataset that is characterized by real-world noise. Building on that foundation, we have evaluated how \emph{Label Wave} method performs when confronted with noise in real-world scenarios.

    \item \textbf{Low noise rate}: We augmented the CIFAR-10 dataset with 10\% symmetric noise (\emph{Sym. 10\%}) and 10\% instance-dependent noise (\emph{Ins. 10\%}), presenting an opportunity to test the resilience of the \emph{Label Wave} method.
\end{itemize}

\textbf{\emph{Label Wave} method is not applicable in very low or no label noise.} There are many situations where validation and test errors consistently decrease even with (low level) noisy labels in the training data. Modern deep neural networks often exhibit benign overfitting \citep{bartlett2020benign}, a phenomenon also describable as a memorization effect \citep{zhang2017understanding, arpit2017closer}.

The effectiveness of the Label Wave method in identifying an appropriate early stopping point is attributed to our design of a practical metric that tracks the significant onset of learning confusion patterns, namely prediction changes (PC).
Therefore, if the training process lacks a stage of learning confusion patterns, such as when training with perfect data or employing robust regularization approaches, the original Label Wave method may not identify an appropriate stopping point.

However, it is important to note that in these scenarios, applying early stopping to improve the model's generalization performance might not be necessary.

\clearpage

\subsection{Parameters and Optimization}

Our experiments encompassed an array of parameters and optimizers:

\begin{itemize}
    \item \textbf{Batch Sizes (BS.)}: \emph{64, 128, 256}.
    \item \textbf{Learning Rates (LR.)}: \emph{0.01, 0.05, 0.001}.
    \item \textbf{Optimizers}:
        \begin{itemize}
         \item \emph{SGD with momentum} \citep{robbins1951stochastic, polyak1964some}: A widely-used optimizer, we gauged how its momentum-based optimization worked in tandem with the \emph{Label Wave} method.
         \item \emph{RMSProp} \citep{tieleman2012lecture}: Known for its robustness and flexibility in non-stationary settings, we examined its synergy with the \emph{Label Wave} method.
         \item \emph{Adam} \citep{kingma2014adam}: Recognized for its adaptability in adjusting learning rates during training, we determine its synergy with the \emph{Label Wave} method.
        \end{itemize}
    \item \textbf{Seeds}: Each experiment is run five times, with the seed set to: \emph{1, 2, 3, 4, 5}, respectively.
\end{itemize}

\subsection{{Regularization Techniques}}

{We examined the \emph{Label Wave} method's effectiveness across various regularization techniques:}

 \begin{itemize}
    \item \textbf{Mixup} \citep{zhang2018mixup}: Mixup is an innovative data augmentation technique that operates by creating virtual training examples. It linearly interpolates between pairs of examples and their labels, effectively encouraging the model to favor simple linear behavior in-between training examples.

    \item \textbf{Batch Normalization (BN)} \citep{ioffe2015batch}: Batch Normalization is a widely adopted technique in deep learning, designed to stabilize and accelerate the training process. By normalizing the inputs of each layer across the mini-batch, it addresses the issue of internal covariate shift. 

    \item \textbf{Dropout} \citep{srivastava2014dropout}: Dropout is a regularization technique that prevents overfitting by randomly deactivating a subset of neurons during training. By doing this, it effectively creates a network of varying architecture each time a batch is passed through, which discourages the network from relying too much on any single neuron and promotes better feature learning. 

    \item {\textbf{Data Augmentation (DA)}:}
        \begin{itemize}
         \item {\emph{Cropping}: Images undergo random cropping procedures. For any 32x32 image, potential padding might be added, from which random 32x32 crops are then extracted.}
         \item {\emph{Rotation}: Images are subjected to random rotations, limited to a range of ±15 degrees. This step ensures the model's robustness against various image orientations.}
        \end{itemize}
\end{itemize}   

\begin{table*}[h]
\renewcommand{\arraystretch}{1.23}
\vskip -0.2in
\centering
	\caption{{Test accuracy (mean$\pm$std) of each techniques on CIFAR-10 (with 40\% Sym. label noise).}}
	\label{tabapp2}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{1.3mm}{

\begin{tabular}{c|ccccc}
\toprule
Regularization Techniques & \emph{Val.} 20\% & \emph{Label Wave} & \emph{Global Maximum}  \\
\midrule

Label Wave & 63.00$\pm$0.86\% & 66.79$\pm$0.39\% & 67.15$\pm$0.49\%   \\

Label Wave + BN + DA &  78.94$\pm$0.52\% & 81.61$\pm$0.44\% & 81.76$\pm$0.30\%  \\

Label Wave + BN + DA + Dropout & 81.30$\pm$1.07\% & 83.57$\pm$0.24\% & 83.77$\pm$0.32\%  \\

Label Wave + BN + DA + Mixup & 81.06$\pm$0.49\% & 82.38$\pm$0.56\% & 83.09$\pm$0.22\%  \\

Label Wave + BN + DA + Dropout + Mixup &   81.66$\pm$0.07\% & 83.67$\pm$0.45\% & 84.05$\pm$0.35\%  \\

\bottomrule  
\end{tabular}
}
}
\end{table*}

\clearpage

\subsection{Quantitative Comparison Between Label Wave and Global Maximum Test Accuracy}
In Table \ref{tab1}, we use ``\emph{Difference}'' to indicate the disparity in test accuracy between the model that achieved the Global Maximum test accuracy throughout the training process and the model selected by the \emph{Label Wave} method. This measure directly contrasts the performance of the \emph{Label Wave} method with that of the ground-true best model (\emph{Global Maximum}) across various settings. The \emph{Difference} is determined by the following formula:

\[ \text{\emph{Difference}} =  A_{GM} - A_{LW}  \]

Where:

- $ A_{GM} $ represents the average test accuracy of the model which have maximum test accuracy in the training process.
  
- $ A_{LW} $ represents the average test accuracy of the model selected by the \emph{Label Wave} method.

This formula provides a quantitative measure to determine how closely the model selected by the \emph{Label Wave} method aligns with the optimal model (i.e., the model with Global Maximum test accuracy) for each setting. A smaller \emph{Difference} indicates that the performance of the \emph{Label Wave} method is near optimal, while a larger \emph{Difference} suggests a divergence from the optimal performance.

Using the \emph{Difference} metric, we objectively evaluate the performance of the \emph{Label Wave} method across various settings.
As shown in Table \ref{tab1}, the consistent and slight disparities between the models chosen by the \emph{Label Wave} method and the optimal models (Global Maximum) underscore the effectiveness of the \emph{Label Wave} method across diverse settings.
This showcases the \emph{Label Wave} method's aptitude in pinpointing an appropriate early stopping point for model selection, which aligns closely with or even matches the optimal models.

\clearpage

\section{{Enhancing Existing Learning with Noisy Label Methods}}
\label{appendixc}

{As shown in Tables \ref{tabapp3} and \ref{tabapp4}, we conducted evaluations using ResNet-18 on CIFAR-10 and ResNet-34 on CIFAR-100 \citep{krizhevsky2009learning, he2016deep}, both trained with 40\% \emph{Instance-Dependent} noise (abbreviated as Ins.) \citep{xia2020part}. Further, as shown in Table \ref{tabapp5}, we added the different between the test accuracy of the model selected using the Label Wave method and the global maximum test accuracy during the training process when using various learning with noisy label methods with 40\% \emph{Symmetric} noise (abbreviated as Sym.) \citep{van2015learning}.}

\begin{table*}[h]
\renewcommand{\arraystretch}{1.15}
\vskip -0.1in
\centering
	\caption{{Test accuracy (mean$\pm$std) of each method on CIFAR-10 (with 40\% Ins. label noise).}}
	\label{tabapp3}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{2.3mm}{

\begin{tabular}{c|ccccc}
\toprule
\textbf{Method} & \emph{Val.} 10\% & \emph{Val.} 20\% & \emph{Label Wave} & \emph{Global Maximum} \\
\midrule 
CE \citep{rubinstein1999cross} & 78.04$\pm$0.62\% & 77.35$\pm$0.23\% & 80.49$\pm$0.33\% & 80.86$\pm$0.77\% \\
Taylor-CE \citep{ijcai2020p305} & 81.30$\pm$0.14\% & 81.09$\pm$0.43\% & 83.24$\pm$0.49\% & 83.63$\pm$0.60\% \\
ELR \citep{liu2020early} & 87.68$\pm$0.32\% & 86.98$\pm$0.22\% & 89.59$\pm$0.16\% & 90.41$\pm$0.25\% \\
CDR \citep{xia2020robust} & 85.66$\pm$0.43\% & 84.86$\pm$0.25\% & 87.42$\pm$0.51\% & 87.63$\pm$0.22\% \\
CORES \citep{cheng2021learning} & 79.45$\pm$0.38\% & 78.77$\pm$0.35\% & 81.22$\pm$0.64\% & 81.64$\pm$0.65\% \\
NLS \citep{wei2022smooth} & 80.61$\pm$0.80\% & 80.25$\pm$0.25\% & 82.36$\pm$0.86\% & 82.63$\pm$1.24\% \\
SOP \citep{liu2022robust} & 84.08$\pm$0.25\% & 83.51$\pm$0.30\% & 85.57$\pm$0.35\% & 85.92$\pm$0.15\% \\
\bottomrule
\end{tabular}
}
}
\end{table*}


\begin{table*}[h]
\renewcommand{\arraystretch}{1.15}
\vskip -0.1in
\centering
	\caption{{Test accuracy (mean$\pm$std) of each method on CIFAR-100 (with 40\% Ins. label noise).}}
	\label{tabapp4}
\resizebox{1.0\columnwidth}{!}{
\setlength{\tabcolsep}{2.3mm}{

\begin{tabular}{c|ccccc}
\toprule
\textbf{Method} & \emph{Val.} 10\% & \emph{Val.} 20\% & \emph{Label Wave} & \emph{Global Maximum} \\
\midrule
CE \citep{rubinstein1999cross} & 42.59$\pm$0.21\% & 41.82$\pm$0.64\% & 44.86$\pm$0.62\% & 45.23$\pm$0.11\% \\
Taylor-CE \citep{ijcai2020p305} & 52.42$\pm$0.59\% & 52.53$\pm$0.56\% & 55.52$\pm$0.54\% & 55.74$\pm$0.23\% \\
ELR \citep{liu2020early} & 64.90$\pm$0.33\% & 64.61$\pm$0.03\% & 66.85$\pm$0.11\% & 67.33$\pm$0.57\% \\
CDR \citep{xia2020robust} & 59.82$\pm$0.10\% & 59.21$\pm$0.87\% & 62.64$\pm$0.40\% & 63.25$\pm$0.47\% \\
CORES \citep{cheng2021learning} & 43.26$\pm$0.71\% & 41.96$\pm$0.11\% & 45.38$\pm$0.13\% & 46.24$\pm$0.09\% \\
NLS \citep{wei2022smooth} & 52.22$\pm$0.88\% & 53.26$\pm$0.08\% & 55.56$\pm$0.39\% & 56.00$\pm$0.25\% \\
SOP \citep{liu2022robust} & 67.83$\pm$0.15\% & 66.33$\pm$0.23\% & 69.35$\pm$0.71\% & 70.23$\pm$0.53\% \\
\bottomrule
\end{tabular}
}
}
\end{table*}


\begin{table*}[h]
\renewcommand{\arraystretch}{1.15}
\vskip -0.1in
\centering
	\caption{{Test accuracy (mean$\pm$std) of each method on CIFAR (with 40\% Sym. label noise).}}
	\label{tabapp5}
\resizebox{0.68\columnwidth}{!}{
\setlength{\tabcolsep}{2.3mm}{

\begin{tabular}{c|cc}
\toprule
\textbf{CIFAR10 - Method} & \emph{Label Wave} & \emph{Global Maximum} \\
\midrule
CE \citep{rubinstein1999cross} & 81.61$\pm$0.44\% & 81.76$\pm$0.30\% \\
Taylor-CE \citep{ijcai2020p305} & 85.06$\pm$0.30\% & 85.43$\pm$0.37\% \\
ELR \citep{liu2020early} & 90.45$\pm$0.52\% & 90.76$\pm$0.70\% \\
CDR \citep{xia2020robust} & 87.69$\pm$0.10\% & 87.80$\pm$0.24\% \\
CORES \citep{cheng2021learning} & 87.74$\pm$0.13\% & 87.95$\pm$0.21\% \\
NLS \citep{wei2022smooth} & 83.45$\pm$0.19\% & 83.62$\pm$0.37\% \\
SOP \citep{liu2022robust} & 88.42$\pm$0.38\% & 88.82$\pm$0.46\% \\
\bottomrule
\toprule
\textbf{CIFAR100 - Method} & \emph{Label Wave} & \emph{Global Maximum} \\
\midrule
CE \citep{rubinstein1999cross} & 50.96$\pm$0.30\% & 51.05$\pm$0.33\% \\
Taylor-CE \citep{ijcai2020p305} & 57.64$\pm$0.28\% & 57.99$\pm$0.30\% \\
ELR \citep{liu2020early} & 65.36$\pm$0.39\% & 66.33$\pm$0.93\% \\
CDR \citep{xia2020robust} & 63.34$\pm$0.15\% & 63.54$\pm$0.28\% \\
CORES \citep{cheng2021learning} & 45.03$\pm$0.38\% & 45.75$\pm$0.27\% \\
NLS \citep{wei2022smooth} & 58.05$\pm$0.15\% & 58.32$\pm$0.35\% \\
SOP \citep{liu2022robust} & 68.53$\pm$0.30\% & 68.78$\pm$0.27\% \\
\bottomrule
\end{tabular}
}
}
\end{table*}


\clearpage

\section{{Sensitivity Analysis of \( k \) Value in Moving Averages}}
\label{appendixd}

{
In this appendix, we present a sensitivity analysis of the \( k \) value used in computing moving averages within the framework of the Label Wave method. This method, as detailed in the paper, employs moving averages of Prediction Changes (PC) for early stopping in training models with noisy labels.}

{This analysis varied the \( k \) value, which represents the number of epochs over which the moving average is calculated for the PC metric. We computed the Pearson Correlation Coefficient between the moving average of PC values (denoted as \( \mathrm{PC}_t^\prime \)) and test accuracy for each \( k \) value. The goal was to determine the impact of different \( k \) settings on the relationship between the averaged PC values and the test accuracy. The table below summarizes the Pearson Correlation Coefficients for different \( k \) values:}

\begin{center}
\begin{tabular}{cc}
\toprule
{\( k \) Value} & {Pearson Correlation Coefficient} \\
\midrule
1 & -0.8565 \\
2 & -0.9435 \\
3 & -0.9637 \\
5 & -0.9367 \\
10 & -0.9406 \\
\bottomrule
\end{tabular}
\end{center}

\subsection{{Observations}}

\begin{itemize}
    \item {\textbf{Strong Negative Correlation:} A strong negative correlation was consistently observed across all \( k \) values. This signifies a robust inverse relationship between the moving average of PC values and test accuracy.}
    \item {\textbf{Variation in Correlation Strength:} The correlation's strength varied with different \( k \) values. The strongest negative correlation occurred at \( k = 3 \), while \( k = 5 \) and \( k = 10 \) exhibited a slight decrease in correlation strength.}
\end{itemize}

\subsection{{Conclusion}}

{The sensitivity analysis reveals that the selection of \( k \) value influences the strength of the correlation but does not significantly alter the overall negative relationship between the moving average of PC values and test accuracy. 
}

\end{document}