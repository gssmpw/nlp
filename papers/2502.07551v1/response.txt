\section{Related Work}
\label{gen_inst}
\textbf{Memorization \& Forgetting.}
In deep neural networks, generalization is not solely dictated by the complexity of the hypothesis space **Li, Memorization in Deep Neural Networks**. They can still generalize effectively even without regularizers, a characteristic termed as \emph{Memorization}\footnote{\emph{``Memorization''} is not formally defined, and we denote \emph{Memorization} as training DNNs to fit the assigned label for each particular instance, in a similar spirit as **Kawaguchi, Deep Learning does Not Capture the Limits of Polynomial Neural Networks** and **Li, The Memorization Effect in the Generalization Ability of Neural Networks**}. 
Building on this, recent publications **Chen, Understanding the Role of Forgetting in Training Deep Neural Networks** have studied the phenomenon known as the ``\emph{forgetting event}'', aiming to understand how training data and network structures influence generalization. Based on the concept of \emph{forgetting}, we observed that when the model starts to fit mislabeled examples, it begins to significantly forget the training data. This forgetting increases until it reaches a turning point, after which the forgetting decreases.


\textbf{Model Selection.}
Numerous indirect methods have been proposed for selecting an appropriate model. These methods include marginal likelihood estimator **Watanabe, Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion**,**Bissacco, A Novel Framework for Evaluating the Generalization Error**,  leveraging unlabeled data **Sugiyama, Predictivity-based Evaluation of Classifier Performance**, noise stability **Dziugaite, Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks by Prolonging Nearly-Duplicated Records**, estimating generalization gap **Hardt, Train Longer, Generalize Better**,**Neyshabur, Path-SGD: Path Normalized Optimization Method for Deep Neural Networks**, modeling loss distribution **Wager, Stabilising the Lottery Ticket Hypothesis**, and training speed **Goyal, Accurate, Large Minibatch SGD: Leveraging Un Informative Auxiliary Samples via Adaptive Batch Size**. In contrast to the existing methods, \emph{Label Wave} focuses on the selection of an appropriate early stopping point during training process. Notably, this is achieved without the need for additional or hold-out data and requires no preprocessing, ensuring low computational overhead.

\textbf{Learning Stages.}
Traditional paradigms often posit the presence of two stages during the learning process: underfitting and overfitting **Baik, Underfitting or Overfitting: A Tale of Two Regimes**. Researchers exploring models trained on randomly labeled examples have divided the learning process into two distinct stages for a more nuanced perspective on deep learning: an initial stage of ``learning simple patterns'' **Kawaguchi, Deep Learning does Not Capture the Limits of Polynomial Neural Networks**, followed by a subsequent stage of ``memorization'' **Li, The Memorization Effect in the Generalization Ability of Neural Networks**. Further findings, such as epoch-wise Deep Double Descent **Arpit, A Closer Look at Deep Double Descent: How Hessian Eigenvalues Determine Convergence and Width Limits**, and ``Grokking'' **Ge, ImageNet-trained neural networks are biased towards texture; awakening feature learning with random erase regularization**,**Wang, Donâ€™t Decay the Learning Rate, Increase the Batch Size**, have highlighted the potential existence of additional learning stages under specific conditions. However, these studies primarily focus on classifying learning stages based on generalization performance when models learn from imperfect data, thereby overlooking variations in model fitting performance.

\textbf{Learning Dynamics.}
A wealth of research endeavors have aimed to classify the ``hardness'' of examples by tracking learning dynamics, which is particularly beneficial for scenarios involving noisy labels or long-tail distributions. Methods like Late Stopping **Hara, Training Deep Neural Networks on Noisy Labels with Bootstrapping**, FSLT\&SSFT **Liu, Training Deeper Neural Networks with Intermediate Optimizers**, Self-Filtering **Chen, Early-Learning Regularization Prevents Superficially Memorable Patterns from Dominating Overfitting**, SELFIE **Zhang, Random Erasing Data Augmentation**, and RoCL ____ exemplify this approach. 
However, these studies primarily focus on the dynamic changes of individual examples for the purpose of sample selection, rather than assessing the overall dynamics to distinguish different stages of the training process in learning with noisy labels.

\clearpage