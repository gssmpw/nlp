
@InProceedings{MechanicsofNextTokenPredictionwithSelf-Attention,
  title = 	 {Mechanics of Next Token Prediction with Self-Attention},
  author =       {Li, Yingcong and Huang, Yixiao and Ildiz, Muhammed E. and Singh Rawat, Ankit and Oymak, Samet},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {685--693},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/li24f/li24f.pdf},
  url = 	 {https://proceedings.mlr.press/v238/li24f.html},
  abstract = 	 {Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: What does a single self-attention layer learn from next-token prediction? We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: (1) Hard retrieval: Given input sequence, self-attention precisely selects the high-priority input tokens associated with the last input token. (2) Soft composition: It then creates a convex combination of the high-priority tokens from which the next token can be sampled. Under suitable conditions, we rigorously characterize these mechanics through a directed graph over tokens extracted from the training data. We prove that gradient descent implicitly discovers the strongly-connected components (SCC) of this graph and self-attention learns to retrieve the tokens that belong to the highest-priority SCC available in the context window. Our theory relies on decomposing the model weights into a directional component and a finite component that correspond to hard retrieval and soft composition steps respectively. This also formalizes a related implicit bias formula conjectured in [Tarzanagh et al. 2023]. We hope that these findings shed light on how self-attention processes sequential data and pave the path toward demystifying more complex architectures.}
}
@inproceedings{thrampoulidisimplicit,
  title={Implicit Optimization Bias of Next-token Prediction in Linear Models},
  author={Thrampoulidis, Christos},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}
@online{heLawNextTokenPrediction2024,
  title = {A {{Law}} of {{Next-Token Prediction}} in {{Large Language Models}}},
  author = {He, Hangfeng and Su, Weijie J.},
    year={2024},
  date = {2024-08-24},
  url = {https://arxiv.org/abs/2408.13442v1},
  urldate = {2024-09-19},
  abstract = {Large language models (LLMs) have been widely employed across various application domains, yet their black-box nature poses significant challenges to understanding how these models process input data internally to make predictions. In this paper, we introduce a precise and quantitative law that governs the learning of contextualized token embeddings through intermediate layers in pre-trained LLMs for next-token prediction. Our findings reveal that each layer contributes equally to enhancing prediction accuracy, from the lowest to the highest layer -- a universal phenomenon observed across a diverse array of open-source LLMs, built on architectures such as Transformer, RWKV, and Mamba. We demonstrate that this law offers new perspectives and insights to inform and guide practices in LLM development and applications, including model scaling, pre-training tasks, and information flow. Overall, our law enables more fine-grained approaches to the design, training, and interpretation of LLMs through scrutinizing their internal data processing mechanisms.},
  langid = {english},
  organization = {arXiv.org},
}
@InProceedings{pmlr-v235-bachmann24a,
  title = 	 {The Pitfalls of Next-Token Prediction},
  author =       {Bachmann, Gregor and Nagarajan, Vaishnavh},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {2296--2318},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/bachmann24a/bachmann24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/bachmann24a.html},
  abstract = 	 {Can a mere next-token predictor faithfully model human thinking? Our work is aimed at crystallizing this intuitive concern, which is currently fragmented in the literature. First, we emphasize isolating the two phases of next-token prediction that are often conflated: autoregression during inference vs. teacher-forcing during training. We argue that the previously-identified problem of "exponential error accumulation" is a symptom of autoregressive inference. But more concerningly, we identify that teacher-forcing can let the model fit the training data by cheating, causing total in-distribution failure. We design a minimal planning task where empirically both the Transformer and the Mamba architecture fail in this manner - remarkably, despite the task being easy to learn. Overall, our work consolidates these and other essential arguments surrounding next-token prediction. We hope this effort can ground future discussions and inspire explorations beyond the next-token prediction paradigm.}
}
@inproceedings{
zhao2024implicit,
title={Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations},
author={Yize Zhao and Tina Behnia and Vala Vakilian and Christos Thrampoulidis},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=qyilOnIRHI}
}
@misc{madden2024nexttokenpredictioncapacitygeneral,
      title={Next-token prediction capacity: general upper bounds and a lower bound for transformers}, 
      author={Liam Madden and Curtis Fox and Christos Thrampoulidis},
      year={2024},
      eprint={2405.13718},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13718}, 
}
@inproceedings{
gloeckle2024better,
title={Better \& Faster Large Language Models via Multi-token Prediction},
author={Fabian Gloeckle and Badr Youbi Idrissi and Baptiste Roziere and David Lopez-Paz and Gabriel Synnaeve},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=pEWAcejiU2}
}