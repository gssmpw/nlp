@Article{Martens2020,
  author  = {James Martens},
  journal = {Journal of Machine Learning Research},
  title   = {New Insights and Perspectives on the Natural Gradient Method},
  year    = {2020},
  number  = {146},
  pages   = {1--76},
  volume  = {21},
  url     = {http://jmlr.org/papers/v21/17-678.html},
}
@InProceedings{wangpac,
  author    = {Wang, Zifeng and Huang, Shao-Lun and Kuruoglu, Ercan Engin and Sun, Jimeng and Chen, Xi and Zheng, Yefeng},
  booktitle = {International Conference on Learning Representations},
  title     = {PAC-Bayes Information Bottleneck},
  year    = {2022},
}
@article{Saxton2019AnalysingMR,
  title={Analysing Mathematical Reasoning Abilities of Neural Models},
  author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.01557},
  url={https://api.semanticscholar.org/CorpusID:85504763}
}
@misc{ontanon2022logicinferencenew,
      title={LogicInference: A New Dataset for Teaching Logical Inference to seq2seq Models}, 
      author={Santiago Ontanon and Joshua Ainslie and Vaclav Cvicek and Zachary Fisher},
      year={2022},
      eprint={2203.15099},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2203.15099}, 
}
@inproceedings{tänzer2022memorisationversusgeneralisationpretrained,
    title = "Memorisation versus Generalisation in Pre-trained Language Models",
    author = {T{\"a}nzer, Michael  and
      Ruder, Sebastian  and
      Rei, Marek},
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.521",
    doi = "10.18653/v1/2022.acl-long.521",
    pages = "7564--7578",
    abstract = "State-of-the-art pre-trained language models have been shown to memorise facts and perform well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generalisation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low-resource tasks such as few-shot learning and rare entity recognition. To mitigate such limitations, we propose an extension based on prototypical networks that improves performance in low-resource named entity recognition tasks.",
}
@inproceedings{lilazy,
  title={The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers},
  author={Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and others},
  booktitle={The Eleventh International Conference on Learning Representations},
    year={2023},
}
@inproceedings{zhaoimplicit,
  title={Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations},
  author={Zhao, Yize and Behnia, Tina and Vakilian, Vala and Thrampoulidis, Christos},
    year={2024},
  booktitle={First Conference on Language Modeling}
}
@article{chen2021scatterbrain,
  title={Scatterbrain: Unifying sparse and low-rank attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17413--17426},
  year={2021}
}
@inproceedings{gupta2021memory,
  title={Memory-efficient Transformers via Top-k Attention},
  author={Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan},
  booktitle={Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
  pages={39--52},
  year={2021}
}
@misc{zhangzhongwang,
      title={Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing}, 
      author={Zhongwang Zhang and Pengxiao Lin and Zhiwei Wang and Yaoyu Zhang and Zhi-Qin John Xu},
      year={2024},
      eprint={2405.05409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.05409}, 
}
@misc{anchorfunction,
      title={Anchor function: a type of benchmark functions for studying language models}, 
      author={Zhongwang Zhang and Zhiwei Wang and Junjie Yao and Zhangchen Zhou and Xiaolong Li and Weinan E and Zhi-Qin John Xu},
      year={2024},
      eprint={2401.08309},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.08309}, 
}
% initial bias
@InProceedings{InitialGuessingBias,
  title = 	 {Initial Guessing Bias: How Untrained Networks Favor Some Classes},
  author =       {Francazi, Emanuele and Lucchi, Aurelien and Baity-Jesi, Marco},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {13783--13839},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/francazi24a/francazi24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/francazi24a.html},
  abstract = 	 {Understanding and controlling biasing effects in neural networks is crucial for ensuring accurate and fair model performance. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a deep neural network (DNN) can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We prove that, besides dataset properties, the presence of this phenomenon, which we call <em>Initial Guessing Bias</em> (IGB), is influenced by model choices including dataset preprocessing methods, and architectural decisions, such as activation functions, max-pooling layers, and network depth. Our analysis of IGB provides information for architecture selection and model initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging and the non-trivial effects that depth has on the phenomenon.}
}
@inproceedings{
  PrOntoQA,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023},
  url={https://openreview.net/forum?id=qFVVBzXxR2V}
}
@InProceedings{Teney_2024_CVPR,
    author    = {Teney, Damien and Nicolicioiu, Armand Mihai and Hartmann, Valentin and Abbasnejad, Ehsan},
    title     = {Neural Redshift: Random Networks are not Random Functions},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {4786-4796}
}

@InProceedings{Position,
  title = 	 {Position: The No Free Lunch Theorem, Kolmogorov Complexity, and the Role of Inductive Biases in Machine Learning},
  author =       {Goldblum, Micah and Finzi, Marc Anton and Rowan, Keefer and Wilson, Andrew Gordon},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {15788--15808},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/goldblum24a/goldblum24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/goldblum24a.html},
  abstract = 	 {No free lunch theorems for supervised learning state that no learner can solve all problems or that all learners achieve exactly the same accuracy on average over a uniform distribution on learning problems. Accordingly, these theorems are often referenced in support of the notion that individual problems require specially tailored inductive biases. While virtually all uniformly sampled datasets have high complexity, real-world problems disproportionately generate low-complexity data, and we argue that neural network models share this same preference, formalized using Kolmogorov complexity. Notably, we show that architectures designed for a particular domain, such as computer vision, can compress datasets on a variety of seemingly unrelated domains. Our experiments show that pre-trained and even randomly initialized language models prefer to generate low-complexity sequences. Whereas no free lunch theorems seemingly indicate that individual problems require specialized learners, we explain how tasks that often require human intervention such as picking an appropriately sized model when labeled data is scarce or plentiful can be automated into a single learning algorithm. These observations justify the trend in deep learning of unifying seemingly disparate problems with an increasingly small set of machine learning models.}
}
@InProceedings{InitializationAffectGeneralization,  title =  {How much does Initialization Affect Generalization?},  author =       {Ramasinghe, Sameera and Macdonald, Lachlan Ewen and Farazi, Moshiur and Saratchandran, Hemanth and Lucey, Simon},  booktitle =  {Proceedings of the 40th International Conference on Machine Learning},  pages =  {28637--28655},  year =  {2023},  editor =  {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},  volume =  {202},  series =  {Proceedings of Machine Learning Research},  month =  {23--29 Jul},  publisher =    {PMLR},  pdf =  {https://proceedings.mlr.press/v202/ramasinghe23a/ramasinghe23a.pdf},  url =  {https://proceedings.mlr.press/v202/ramasinghe23a.html},  abstract =  {Characterizing the remarkable generalization properties of over-parameterized neural networks remains an open problem. A growing body of recent literature shows that the bias of stochastic gradient descent (SGD) and architecture choice implicitly leads to better generalization. In this paper, we show on the contrary that, independently of architecture, SGD can itself be the cause of poor generalization if one does not ensure good initialization. Specifically, we prove that <em>any</em> differentiably parameterized model, trained under gradient flow, obeys a weak spectral bias law which states that sufficiently high frequencies train arbitrarily slowly. This implies that very high frequencies present at initialization will remain after training, and hamper generalization. Further, we empirically test the developed theoretical insights using practical, deep networks. Finally, we contrast our framework with that supplied by the <em>flat-minima</em> conjecture and show that Fourier analysis grants a more reliable framework for understanding the generalization of neural networks.}}

@misc{achille2020informationdeepneuralnetwork,
      title={Where is the Information in a Deep Neural Network?}, 
      author={Alessandro Achille and Giovanni Paolini and Stefano Soatto},
      year={2020},
      eprint={1905.12213},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1905.12213}, 
}
% flatness
@misc{granziol2020flatnessfalsefriend,
      title={Flatness is a False Friend}, 
      author={Diego Granziol},
      year={2020},
      eprint={2006.09091},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2006.09091}, 
}
@inproceedings{
keskar2017on,
title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=H1oyRlYgg}
}
@inproceedings{visualize,
 author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visualizing the Loss Landscape of Neural Nets},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
 volume = {31},
 year = {2018}
}
@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={22188--22214},
  year={2023},
  organization={PMLR}
}

% Result References
@inproceedings{sinha-etal-2019-clutrr,
    title = "{CLUTRR}: A Diagnostic Benchmark for Inductive Reasoning from Text",
    author = "Sinha, Koustuv  and
      Sodhani, Shagun  and
      Dong, Jin  and
      Pineau, Joelle  and
      Hamilton, William L.",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1458",
    doi = "10.18653/v1/D19-1458",
    pages = "4506--4515",
    abstract = "The recent success of natural language understanding (NLU) systems has been troubled by results highlighting the failure of these models to generalize in a systematic and robust way. In this work, we introduce a diagnostic benchmark suite, named CLUTRR, to clarify some key issues related to the robustness and systematicity of NLU systems. Motivated by the classic work on inductive logic programming, CLUTRR requires that an NLU system infer kinship relations between characters in short stories. Successful performance on this task requires both extracting relationships between entities, as well as inferring the logical rules governing these relationships. CLUTRR allows us to precisely measure a model{'}s ability for systematic generalization by evaluating on held-out combinations of logical rules, and allows us to evaluate a model{'}s robustness by adding curated noise facts. Our empirical results highlight a substantial performance gap between state-of-the-art NLU models (e.g., BERT and MAC) and a graph neural network model that works directly with symbolic inputs{---}with the graph-based model exhibiting both stronger generalization and greater robustness.",
}
@inproceedings{sanyal-etal-2022-robustlr,
    title = "{R}obust{LR}: A Diagnostic Benchmark for Evaluating Logical Robustness of Deductive Reasoners",
    author = "Sanyal, Soumya  and
      Liao, Zeyi  and
      Ren, Xiang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.653",
    doi = "10.18653/v1/2022.emnlp-main.653",
    pages = "9614--9631",
    abstract = "Transformers have been shown to be able to perform deductive reasoning on inputs containing rules and statements written in the English natural language. However, it is unclear if these models indeed follow rigorous logical reasoning to arrive at the prediction or rely on spurious correlation patterns in making decisions. A strong deductive reasoning model should consistently understand the semantics of different logical operators. To this end, we present RobustLR, a diagnostic benchmark that evaluates the robustness of language models to minimal logical edits in the inputs and different logical equivalence conditions. In our experiments with RoBERTa, T5, and GPT3 we show that the models trained on deductive reasoning datasets do not perform consistently on the RobustLR test set, thus showing that the models are not robust to our proposed logical perturbations. Further, we observe that the models find it especially hard to learn logical negation operators. Our results demonstrate the shortcomings of current language models in logical reasoning and call for the development of better inductive biases to teach the logical semantics to language models. All the datasets and code base have been made publicly available.",
}
@inproceedings{wan-etal-2024-logicasker,
    title = "{L}ogic{A}sker: Evaluating and Improving the Logical Reasoning Ability of Large Language Models",
    author = "Wan, Yuxuan  and
      Wang, Wenxuan  and
      Yang, Yiliu  and
      Yuan, Youliang  and
      Huang, Jen-tse  and
      He, Pinjia  and
      Jiao, Wenxiang  and
      Lyu, Michael",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.128",
    doi = "10.18653/v1/2024.emnlp-main.128",
    pages = "2124--2155",
    abstract = "We introduce LogicAsker, a novel approach for evaluating and enhancing the logical reasoning capabilities of large language models (LLMs) such as ChatGPT and GPT-4. Despite LLMs{'} prowess in tasks like writing assistance, code generation, and machine translation, assessing their ability to reason has been challenging. Traditional evaluations often prioritize accuracy on downstream tasks over direct assessments of reasoning processes. LogicAsker addresses this gap by employing a set of atomic reasoning skills grounded in propositional and predicate logic to systematically examine and improve the reasoning prowess of LLMs. Our methodology reveals significant gaps in LLMs{'} learning of logical rules, with identified reasoning failures ranging from 29{\%} to 90{\%} across different models. Moreover, we leverage these findings to construct targeted demonstration examples and fine-tune data, notably enhancing logical reasoning in models like GPT-4o by up to 5{\%}. To our knowledge, this is the first effort to utilize test case outcomes to effectively refine LLMs{'} formal reasoning capabilities. We make our code, data, and results publicly available(https://github.com/yxwan123/LogicAsker) to facilitate further research and replication of our findings.",
}
@inproceedings{SimpleLogic,
  title     = {On the Paradox of Learning to Reason from Data},
  author    = {Zhang, Honghua and Li, Liunian Harold and Meng, Tao and Chang, Kai-Wei and Van den Broeck, Guy},
  booktitle = {Proceedings of the Thirty-Second International Joint Conference on
               Artificial Intelligence, {IJCAI-23}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Edith Elkind},
  pages     = {3365--3373},
  year      = {2023},
  month     = {8},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2023/375},
  url       = {https://doi.org/10.24963/ijcai.2023/375},
}
@misc{PARARULEPlus,
      title={Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation}, 
      author={Qiming Bao and Alex Yuxuan Peng and Tim Hartill and Neset Tan and Zhenyun Deng and Michael Witbrock and Jiamou Liu},
      year={2024},
      eprint={2207.14000},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2207.14000}, 
}
@article{wu-etal-2023-recogs,
    title = "{R}e{COGS}: How Incidental Details of a Logical Form Overshadow an Evaluation of Semantic Interpretation",
    author = "Wu, Zhengxuan  and
      Manning, Christopher D.  and
      Potts, Christopher",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.96",
    doi = "10.1162/tacl_a_00623",
    pages = "1719--1733",
    abstract = "Compositional generalization benchmarks for semantic parsing seek to assess whether models can accurately compute meanings for novel sentences, but operationalize this in terms of logical form (LF) prediction. This raises the concern that semantically irrelevant details of the chosen LFs could shape model performance. We argue that this concern is realized for the COGS benchmark (Kim and Linzen, 2020). COGS poses generalization splits that appear impossible for present-day models, which could be taken as an indictment of those models. However, we show that the negative results trace to incidental features of COGS LFs. Converting these LFs to semantically equivalent ones and factoring out capabilities unrelated to semantic interpretation, we find that even baseline models get traction. A recent variable-free translation of COGS LFs suggests similar conclusions, but we observe this format is not semantically equivalent; it is incapable of accurately representing some COGS meanings. These findings inform our proposal for ReCOGS, a modified version of COGS that comes closer to assessing the target semantic capabilities while remaining very challenging. Overall, our results reaffirm the importance of compositional generalization and careful benchmark task design.",
}
@inproceedings{ruletaker,
  title     = {Transformers as Soft Reasoners over Language},
  author    = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {3882--3890},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/537},
  url       = {https://doi.org/10.24963/ijcai.2020/537},
}
@misc{Yelp,
    year = 2014,
    key = {Yelp Dataset},
    url = {http://www.yelp.com/
 dataset_challenge}
}
@misc{abdin2024phi4technicalreport,
      title={Phi-4 Technical Report}, 
      author={Marah Abdin and Jyoti Aneja and Harkirat Behl and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and James R. Lee and Yin Tat Lee and Yuanzhi Li and Weishung Liu and Caio C. T. Mendes and Anh Nguyen and Eric Price and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Xin Wang and Rachel Ward and Yue Wu and Dingli Yu and Cyril Zhang and Yi Zhang},
      year={2024},
      eprint={2412.08905},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.08905}, 
}
@misc{jain2023neftunenoisyembeddingsimprove,
      title={NEFTune: Noisy Embeddings Improve Instruction Finetuning}, 
      author={Neel Jain and Ping-yeh Chiang and Yuxin Wen and John Kirchenbauer and Hong-Min Chu and Gowthami Somepalli and Brian R. Bartoldson and Bhavya Kailkhura and Avi Schwarzschild and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2310.05914},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.05914}, 
}

@article{survey_of_NLR,
author = {Yu, Fei and Zhang, Hongbo and Tiwari, Prayag and Wang, Benyou},
title = {Natural Language Reasoning, A Survey},
year = {2024},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {12},
issn = {0360-0300},
url = {https://doi.org/10.1145/3664194},
doi = {10.1145/3664194},
abstract = {This survey article proposes a clearer view of Natural Language Reasoning (NLR) in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for NLR in NLP, based on both philosophy and NLP scenarios; discuss what types of tasks require reasoning; and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on NLR in NLP, mainly covering classical logical reasoning, Natural Language Inference (NLI), multi-hop question answering, and commonsense reasoning. The article also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in NLR research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic research and mathematical reasoning.1},
journal = {ACM Comput. Surv.},
month = oct,
articleno = {304},
numpages = {39},
keywords = {Natural language reasoning, pre-trained language models}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@misc{coconut,
      title={Training Large Language Models to Reason in a Continuous Latent Space}, 
      author={Shibo Hao and Sainbayar Sukhbaatar and DiJia Su and Xian Li and Zhiting Hu and Jason Weston and Yuandong Tian},
      year={2024},
      eprint={2412.06769},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.06769}, 
}
@inproceedings{LogicNLI,
    title = "Diagnosing the First-Order Logical Reasoning Ability Through {L}ogic{NLI}",
    author = "Tian, Jidong  and
      Li, Yitian  and
      Chen, Wenqing  and
      Xiao, Liqiang  and
      He, Hao  and
      Jin, Yaohui",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.303/",
    doi = "10.18653/v1/2021.emnlp-main.303",
    pages = "3738--3747",
    abstract = "Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability."
}

@inproceedings{zhu2019anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  booktitle={International Conference on Machine Learning},
  pages={7654--7663},
  year={2019},
  organization={PMLR}
}


@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}

@article{feng2021inverse,
  title={The inverse variance--flatness relation in stochastic gradient descent is critical for finding flat minima},
  author={Feng, Yu and Tu, Yuhai},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={9},
  year={2021},
  publisher={National Acad Sciences}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{mori2021power,
  title={Power-law escape rate of SGD},
  author={Mori, Takashi and Ziyin, Liu and Liu, Kangqiao and Ueda, Masahito},
  journal={arXiv preprint arXiv:2105.09557},
  year={2021}
}

@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and others},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{ma2021linear,
  title={On linear stability of sgd and input-smoothness of neural networks},
  author={Ma, Chao and Ying, Lexing},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16805--16817},
  year={2021}
}

@inproceedings{mianjy2018implicit,
  title={On the implicit bias of dropout},
  author={Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={3540--3548},
  year={2018},
  organization={PMLR}
}

@article{bank2020etf,
  title={An ETF view of dropout regularization},
  author={Bank, Dor and Giryes, Raja},
  journal={British Machine Vision Conference},
  year={2020}
}

@inproceedings{lengerich2022dropout,
  title={Dropout as a regularizer of interaction effects},
  author={Lengerich, Benjamin J and Xing, Eric and Caruana, Rich},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7550--7564},
  year={2022},
  organization={PMLR}
}

@inproceedings{cavazza2018dropout,
  title={Dropout as a low-rank regularizer for matrix factorization},
  author={Cavazza, Jacopo and Morerio, Pietro and Haeffele, Benjamin and Lane, Connor and Murino, Vittorio and Vidal, Rene},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={435--444},
  year={2018},
  organization={PMLR}
}

@inproceedings{wei2020implicit,
  title={The implicit and explicit regularization effects of dropout},
  author={Wei, Colin and Kakade, Sham and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={10181--10192},
  year={2020},
  organization={PMLR}
}

@article{zhang2023stochastic,
  title={Stochastic Modified Equations and Dynamics of Dropout Algorithm},
  author={Zhang, Zhongwang and Li, Yuqing and Luo, Tao and Xu, Zhi-Qin John},
  journal={arXiv preprint arXiv:2305.15850},
  year={2023}
}

@article{zhang2024implicit,
  title={Implicit regularization of dropout},
  author={Zhang, Zhongwang and Xu, Zhi-Qin John},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}
@inproceedings{stepGame2022shi,
title={StepGame: A New Benchmark for Robust Multi-Hop Spatial Reasoning in Texts},
author={Shi, Zhengxiang and Zhang, Qiang and Lipani, Aldo},
volume={36},
url={https://ojs.aaai.org/index.php/AAAI/article/view/21383},
DOI={10.1609/aaai.v36i10.21383}, 
booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
year={2022},
month={Jun.},
pages={11321-11329}
}