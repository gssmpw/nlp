\section{Related Work}
\paragraph{Next-Token Prediction and Other Training Methods.}
% Next token prediction is a classic and widely adopted method for training LLMs, enabling them to acquire fundamental expressive capabilities. 
Recent studies have deepened our understanding of NTP through various perspectives. For instance, ____ analyze the geometric properties of word and context embeddings in the logits domain, revealing the mechanisms behind the sparsity and low-rank structures in logits space. Theoretical explorations by ____ further investigate the capacity of NTP in single-layer transformers, focusing on the interplay between model parameters and output dimensions. Additionally, ____ leverage knowledge graphs to provide mechanistic insights into the NTP learning strategy, while ____ establish empirical scaling laws for NTP across diverse language models. Despite its widespread use, concerns about the limitations of NTP have spurred the development of alternative training paradigms. Recent work by ____ highlights the potential of novel learning methods to address these limitations. For example, RHO-1 ____ introduces a token-level scoring mechanism, selectively training on high-scoring samples to improve efficiency. Similarly, Phi-4 ____ demonstrates significant advancements in reasoning capabilities by prioritizing high-quality data during training. While these innovations mark important progress, the relationship between different training methodologies and their corresponding generalization capabilities remains underexplored. A deeper understanding of this relationship is crucial for advancing the field and developing more robust and efficient LLMs.

% \paragraph{Initial Bias in Transformers}
% Regularization is an unavoidable issue in training neural networks. In recent years, the randomness introduced by Stochastic Gradient Descent (SGD) has been shown to lead to flatter solutions with better generalization, a result confirmed both empirically and theoretically ____. Beyond training implicit regularization, the initial bias introduced in ____ significantly influences the features learned by neural networks. The concept of neural redshift, proposed by ____, demonstrates that the activation function, layer normalization, and depth of the Transformer largely determine the initial complexity of a network, thereby influencing the frequency of functions the network learns. Furthermore, ____ find that pretrained LLMs generate lower complexity sequences compared to untrained LLMs, highlighting the critical role of the NTP training method.

\paragraph{\textbf{Implicit Bias for Noise-Induced Regularization Techniques.}}
Implicit bias introduced by noise-induced regularization techniques has been widely studied in recent years. Different forms of noise often have a significant impact on the training process and the final performance of the model____. Among the noise-induced regularization techniques, stochastic gradient descent (SGD) is the most widely studied. A series of works have shown that the noise introduced by SGD can improve the generalization ability of models by making the loss landscape flatter ____. Specifically, ____ highlight that the magnitude of SGD noise depends on the loss landscape, which is crucial for helping SGD converge to flatter minima. An alternative line of research ____ links SGD's preference for flatter minima to the dynamical stability of minima.
Dropout is another widely used technique to improve model generalization____. A series of studies have shown that the noise introduced by dropout can enhance generalization ability from different perspectives____. ____ find the noise introduced by dropout can foster model condensation and improve the flatness of the loss landscape, explaining the reasons for dropoutâ€™s improvement of model generalization from two aspects. In this work, we draw an analogy between NTP and noise-induced training methods, and explore the impact of NTP on the model's reasoning capabilities.