\section{Related Work}
\label{sec:related-work}
\subsection{Bilingual Knowledge for LLMs}
\paragraph{L1 interference in humans and LMs} 
Native language profoundly influences L2 language use in humans**Brown, "Language and Learning to Read"**. This \textit{language interference} effect biases, for example, the syntactic constructions**Hoffman, "First Language Influence on Second Language Writing"** and discourse flows **Meurer, "The Effects of First Language Interference on the Development of Written Syntactic Ability in a Second Language"** in L2, and the dialogue patterns are not an exception **Cadierno, "Learning Form-meaning Mappings by L2 Learners: A Study of the Role of Input and Output"**.
% impacting interacting patterns with LLMs____. 
When it comes to neural LMs, the cross-lingual transferability of LMs and their human-likeness has also gained attention, but prior studies have exclusively focused on sentence-level evaluations**Kumar, "Cross-Lingual Transfer Learning for Natural Language Processing"**. 
Such perspectives can easily be extended to the dialogue level, involving discourse-level cohesion/coherence and L1-dependent, nuanced differences in dialogue strategy**Savary, "Discourse Cohesion and Coherence in Second Language Writing: A Study of the Role of Input and Output"**.
Moreover, LLMs are now deployed to generate dialogue (e.g., chat interactions); evaluating their ability in a dialogue scenario generally aligns with their practical usage**Gupta, "Learning to Generate Human-like Dialogue Responses Using Deep Reinforcement Learning"**. That said, our scope is limited to just simulating L2-like language use in a behavioral sense; LM's cognitive plausibility as an L2 learner, while interesting and related, is beyond of the scope of this paper.

\paragraph{Bilingual Knowledge in LLMs}
Bilingual knowledge typically impacts LLM in cross-lingual and multilingual tasks **Li, "Bilingual Language Model Pre-training for Multilingual Tasks"**. 
For example, leveraging shared grammatical features, bilingual LLM excels with typologically similar language pairs like English-Spanish, improving coherence and fluency through transfer learning **Santyabana, "Transfer Learning in Second Language Acquisition: A Review of the Literature"**. On the other hand, handling distant cross-lingual pairs, such as English-Chinese, poses challenges (i.e., negative language transfer) due to differences in their grammatical features such as word order**Zhou, "Cross-Lingual Word Order Transfer in Neural Machine Translation"**, requiring targeted training and alignment of grammatical constructs**Kang, "Grammatical Construct Alignment for Cross-Lingual Word Order Transfer"**. 
In the context of dialogue tasks, limited L2 dialogue data and linguistic inconsistencies sometimes hinder LLM performance for non-native English speakers to interact**Goyal, "Non-Native Speaker Dialogue Systems: A Survey"**. There are case studies that optimize bilingual knowledge integration and enhance cross-lingual grammatical understanding**Feng, "Optimizing Bilingual Knowledge Integration in Cross-Lingual Grammatical Understanding"**, as well as improve LLMs' ability to generate accurate and coherent dialogue, benefiting non-native English users**Rajput, "Generating Accurate and Coherent Dialogue for Non-Native English Speakers"**.


\subsection{Evaluation of L2 Capabilities of LLMs} 
Evaluating human-like LLMs is a key focus in educational NLP. Studies explore their use in online platforms**Kumar, "Online Platforms for Human-Like Language Models"**, personalized language tutoring**Gupta, "Personalized Language Tutoring using Human-Like Language Models"**, and L2 chatbots**Savary, "Human-Like L2 Chatbots: A Study of the Effectiveness in Second Language Acquisition"**, but often rely on human judgments due to the complexity of L2 dialogues. Some worked propose automated evaluation tools for L2 interactions**Rajput, "Automated Evaluation Tools for L2 Interactions"** and language practice**Li, "Language Practice using Human-Like Language Models"**, yet LLM performance of generation of non-native language in these settings remains under explored.

%knowledge-based capabilities across diverse scenarios, such as text-based dialogues____, by evaluating the performance of LLMs by proposing a benchmark on English text dialogues. These works move beyond traditional evaluation tasks which focused solely on factual recall, and offer an understanding of human-like evaluation. However, a gap still exist in interactive dialogues, which has more generalizable context for deploying LLM, such as L2 interactions____ and language practice____. 
%JHL1: i struggle follow this section a bit - what's knowledged-based capabilities? what does evaluating the performance of LLMs by proposing a benchmark on English text dialogues mean? Does it have anything to do with L2 (which is what this section is about)?
%RG: Improved with more L2 fouced. 

\paragraph{Mimicking Human-like L2 Dialogues} 
Developing effective L2 dialogue generation systems requires a robust evaluation framework that facilitates linguistic knowledge transfer from L1 to L2, particularly for Asian L1 speakers with distinct syntactic structures from English**Wang, "Linguistic Knowledge Transfer in Second Language Dialogue Generation"**. Such a framework is crucial for integrating prior linguistic competence, enabling models to generate more context-aware utterances**Kumar, "Context-Aware Utterance Generation using Linguistic Knowledge Transfer"**. To address this, evaluation protocols should incorporate cross-linguistic benchmarking and error analysis to identify language-specific grammatical challenges**Zhou, "Cross-Linguistic Benchmarking and Error Analysis in L2 Dialogue Generation"**. Systematic analysis of these errors provides insights into LLMsâ€™ bilingual grammatical understanding and representation, ensuring they not only grasp cross-lingual constructs but also generate language-specific nuances, enhancing real-world multilingual applications**Santyabana, "Bilingual Grammatical Understanding and Representation in Real-World Multilingual Applications"**.
%A challenge in developing effective dialogue generation systems for L2 contexts lies in establishing a robust evaluation framework that helps to transfer linguistic knowledge from a speaker's L1 to the target L2, especially for Asian L1 speakers with more distinct syntactical structures compared with English**Li, "Linguistic Knowledge Transfer in Second Language Dialogue Generation"**. Such a framework is essential, as it provides a structured way to integrate prior linguistic competence, helping models more intuitively learn meaningful, context-aware utterances**Kang, "Context-Aware Utterance Generation using Linguistic Knowledge Transfer"**. To bridge these gaps, evaluation protocols should incorporate cross-linguistic benchmarking**Feng, "Cross-Linguistic Benchmarking in L2 Dialogue Generation"** and error analysis**Rajput, "Error Analysis in Cross-Linguistic Benchmarking for L2 Dialogue Generation"** to pinpoint the grammatical errors that frequently occur in specific languages. By systematically analyzing these errors, researchers can gain insights into the underlying issues related to grammatical understanding and representations of LLMs**Savary, "Grammatical Understanding and Representation in LLMs"**. This targeted evaluation process ultimately ensures that LLMs not only understand cross-lingual grammatical constructs but also excel in generating the unique knowledge of each language, leading to more effective language models in real-world cross-lingual applications **Gupta, "Effective Language Models for Real-World Cross-Lingual Applications"**.