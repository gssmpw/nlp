\section{Related Work}
\paragraph{Explainability of reinforcement learning.} Our work falls into the model-explaining category of XRL frameworks, as classified by Qing et al.____, specifically the explanation-generating subcategory that aims to provide post-hoc interpretations of trained RL models. Self-explainable approaches incorporate interpretability directly into model architectures through various means____. Of particular interest are recent works that leverage concept bottleneck models, where Zabounidis et al.____ introduce interpretable concepts into multi-agent RL architectures, and Ye et al.____ propose methods to reduce the human annotation burden in concept learning. Unlike these self-explainable methods, explanation-generating approaches analyze existing RL policies without compromising model expressiveness and performance. Previous works in this direction have explored diverse approaches: using causal modeling to trace action effects through interpretable chains____; generating explanations by mapping agent actions to predefined instruction templates____; and adapting classic neural network interpretation methods, particularly attribution approaches____. While these techniques have advanced our understanding of DRL decision-making, they primarily focus on input-output relationships without examining the internal neural mechanisms that drive policy decisions.

\paragraph{Concept-based explanations.} Concept-based interpretation has emerged as a powerful approach for understanding neural networks by connecting neural activations with human-understandable concepts. Early work Network Dissection____ established this direction by aligning individual CNN neurons with visual concepts through activation matching. This framework was extended by several subsequent works: TCAV____ introduced methods to quantify concept importance in model predictions, ACE____ developed techniques for automatic concept discovery, and CEN____ proposed compositional explanations using Boolean combinations of concepts. While these approaches have been successfully adapted to various domains, including graph neural networks____, their application in reinforcement learning remains largely unexplored. Our work addresses this gap by introducing compositional concept-based interpretations to DRL agents, revealing how neurons process and combine temporal concepts in decision-making processes.