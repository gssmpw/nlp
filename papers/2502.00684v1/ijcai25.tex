%%%% ijcai25.tex

\typeout{IJCAI--25 Instructions for Authors}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[noend]{algpseudocode}
\usepackage[switch]{lineno}
\usepackage{multirow}
%
\usepackage{amsfonts}
\usepackage{stfloats}
% Comment out this line in the camera-ready submission
% \linenumbers


\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{ /TemplateVersion (IJCAI.2025.0) }

\title{Compositional Concept-Based Neuron-Level Interpretability for Deep Reinforcement Learning}

% Single author syntax
% \author{
%     Author Name
%     \affiliations
%     Affiliation
%     \emails
%     email@example.com
% }

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{ Zeyu Jiang$^{1,2}$ \and Hai Huang$^{1,2}$ \and Xingquan Zuo$^{1,2}$ \affiliations 
$^{1}$School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China\\
$^{2}$Key Laboratory of Trustworthy Distributed Computing and Services, Ministry of Education, Beijing, China\\
\emails \{zeyujiang, hhuang, zuoxq\}@bupt.edu.cn, }
% \fi

\begin{document}
\maketitle

\begin{abstract}
Deep reinforcement learning (DRL), through learning policies or values represented by neural networks, has successfully addressed many complex control problems. However, the neural networks introduced by DRL lack interpretability and transparency. Current DRL interpretability methods largely treat neural networks as black boxes, with few approaches delving into the internal mechanisms of policy/value networks. This limitation undermines trust in both the neural network models that represent policies and the explanations derived from them. In this work, we propose a novel concept-based interpretability method that provides fine-grained explanations of DRL models at the neuron level. Our method formalizes atomic concepts as binary functions over the state space and constructs complex concepts through logical operations. By analyzing the correspondence between neuron activations and concept functions, we establish interpretable explanations for individual neurons in policy/value networks. Experimental results on both continuous control tasks and discrete decision-making environments demonstrate that our method can effectively identify meaningful concepts that align with human understanding while faithfully reflecting the networkâ€™s decision-making logic.
\end{abstract}

\section{Introduction}

Deep reinforcement learning (DRL) has achieved remarkable success in solving complex sequential decision-making problems through trial-and-error learning. From game playing to robotic control, DRL has demonstrated strong capabilities across various domains. However, the increasing complexity of DRL models, particularly their neural network architectures, has created a significant interpretability challenge that hinders their deployment in high-stakes applications such as healthcare, autonomous driving, and financial trading.

Existing approaches to DRL agent model interpretability primarily focus on post-hoc explanations~\cite{vouros2022explainable}. These include applying classic neural network interpretation methods like SHAP~\cite{rizzo2019reinforcement} and attention mechanisms~\cite{nikulin2019free} to attribute importance to input features, as well as explaining agent decisions through causal chains~\cite{yu2023causal}. While these methods provide valuable insights into state importance and action selection, they treat neural networks as black boxes and only explain the relationships between states and actions, without revealing how individual neurons contribute to the decision-making process.

To achieve more fine-grained interpretability at the neuron level, researchers have developed concept-based interpretation methods that match individual neurons with human-understandable concepts~\cite{bau2017network,cunningham2023sparse,mu2020compositional}. By analyzing how neurons respond to different inputs, these methods can identify which neurons encode specific semantic concepts, providing deeper insights into the network's internal representations. These neuron-level interpretation methods have shown promising results in computer vision and natural language processing, revealing how individual neurons learn to detect interpretable patterns. However, applying such neuron-level concept-based interpretations to reinforcement learning remains largely unexplored. The key challenge in extending concept-based interpretations to RL lies in defining state-space concepts and establishing meaningful correspondences between these concepts and neural activations.

To address this challenge, we propose a novel concept-based interpretation method for DRL that operates at the neuron level. As illustrated in Figure 1, we first define atomic concepts as binary functions over states and construct concept vectors by applying these functions to state sequences. Next, we record neuron activations from the policy/value network. Then, we compose concept outputs and align them with neuron activations. Finally, we interpret individual neurons through compositional concepts.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/overview.pdf}
    \caption{Our concept-based interpretation framework for DRL: (1) We design atomic concept functions (e.g., "height near ground", "high velocity") and construct concept vectors $c$ by applying these functions to state sequences; (2) Record DRL model neuron activations $a$ from the policy/value network; (3) Compose concept outputs and match with neuron activations through optimization; (4) Each neuron $(i,l)$ is explained by the compositional concept that best matches its activation pattern (e.g., $(c_1 \vee c_2) \wedge c_3$).}
    \label{fig:overview}
\end{figure*}

The main contributions of this work are as follows:
\begin{enumerate}
    \item We propose a concept-based neuron-level interpretation method for DRL networks, enabling fine-grained understanding of how individual neurons contribute to policy decisions.
    \item We demonstrate through extensive experiments on both discrete (Blackjack-v1, LunarLander-v3) and continuous (LunarLander-Continuous-v2) environments how our method reveals interpretable decision-making patterns at the neuron level.
    \item We validate our interpretations through semantic targeted perturbations, showing that the identified concept-neuron mappings genuinely capture the network's decision-making logic.
\end{enumerate}

\section{Related Work}

\paragraph{Explainability of reinforcement learning.} Our work falls into the model-explaining category of XRL frameworks, as classified by Qing et al.~\cite{qing2023surveyexplainablereinforcementlearning}, specifically the explanation-generating subcategory that aims to provide post-hoc interpretations of trained RL models. Self-explainable approaches incorporate interpretability directly into model architectures through various means~\cite{liu2019lmut,verma2018programmatically,landajuela2021discovering,delfosse2024interpretable,payani2020incorporating}. Of particular interest are recent works that leverage concept bottleneck models, where Zabounidis et al.~\cite{zabounidis2023concept} introduce interpretable concepts into multi-agent RL architectures, and Ye et al.~\cite{ye2024conceptbased} propose methods to reduce the human annotation burden in concept learning. Unlike these self-explainable methods, explanation-generating approaches analyze existing RL policies without compromising model expressiveness and performance. Previous works in this direction have explored diverse approaches: using causal modeling to trace action effects through interpretable chains~\cite{yu2023causal}; generating explanations by mapping agent actions to predefined instruction templates~\cite{boggess2023explainable,hayes2017improving}; and adapting classic neural network interpretation methods, particularly attribution approaches~\cite{nikulin2019free,rizzo2019reinforcement,joo2019visualization,shi2020self}. While these techniques have advanced our understanding of DRL decision-making, they primarily focus on input-output relationships without examining the internal neural mechanisms that drive policy decisions.

\paragraph{Concept-based explanations.} Concept-based interpretation has emerged as a powerful approach for understanding neural networks by connecting neural activations with human-understandable concepts. Early work Network Dissection~\cite{bau2017network} established this direction by aligning individual CNN neurons with visual concepts through activation matching. This framework was extended by several subsequent works: TCAV~\cite{kim2018interpretability} introduced methods to quantify concept importance in model predictions, ACE~\cite{ghorbani2019towards} developed techniques for automatic concept discovery, and CEN~\cite{mu2020compositional} proposed compositional explanations using Boolean combinations of concepts. While these approaches have been successfully adapted to various domains, including graph neural networks~\cite{xuanyuan2023global}, their application in reinforcement learning remains largely unexplored. Our work addresses this gap by introducing compositional concept-based interpretations to DRL agents, revealing how neurons process and combine temporal concepts in decision-making processes.



\section{Methodology}

\subsection{Concept Formalization}

To bridge the gap between value or policy network representations and human-interpretable knowledge, we first formalize the notion of concepts in reinforcement learning. Following the concept-based interpretability framework ~\cite{bau2017network,mu2020compositional}, we adopt a binary function representation of concepts. This formalization aims to capture meaningful patterns in the environment that can be mapped to neural activations while maintaining human interpretability.

Let $S \subseteq \mathbb{R}^n$ be the state space of the reinforcement learning task, where each state $s \in S$ represents the complete observation of the environment at a given time step.

Formally, we define an atomic concept as a binary function $C: S \rightarrow \{0,1\}$, where $C(s) = 1$ indicates the presence of the concept in state $s$, and $C(s) = 0$ indicates its absence.

Compositional concepts can be constructed from atomic concepts through logical operations. Let $\mathcal{C}$ be the set of all possible concepts. For any concepts $C_1, C_2 \in \mathcal{C}$, we define:
\begin{itemize}
    \item Conjunction: $(C_1 \land C_2)(s) = \min(C_1(s), C_2(s))$
    \item Disjunction: $(C_1 \lor C_2)(s) = \max(C_1(s), C_2(s))$
    \item Negation: $(\neg C_1)(s) = 1 - C_1(s)$
\end{itemize}

A compositional concept of length $k$ can be represented as:
$C_k = op_1(op_2(...op_{k-1}(C_1, C_2),...,C_k))$, where $C_i$ are atomic concepts or their negations, and each $op_i \in \{\land, \lor\}$.

The key properties of our concept formalization include:
\begin{itemize}
    \item \textbf{Interpretability}: Each atomic concept corresponds to a human-understandable natural language statement of the environment
    \item \textbf{Compositionality}: Compositional concepts can be built from simpler ones through logical operations
    \item \textbf{Binary nature}: The binary output enables clear concept presence detection and facilitates comparison with neuron activations
\end{itemize}

This formalization provides the foundation for interpreting neural network representations through concept matching. In the following sections, we describe our approach to finding the best-matching concepts for neural activations through a systematic search process, effectively establishing interpretable explanations for the network's behavior.

\subsection{Concept Matching via Neural Activation Analysis}

Given a trained DRL model with value or policy network $f$, let $h_{i,l}(s)$ denote the activation of neuron $i$ in layer $l$ for input state $s$. To establish a correspondence between continuous neuron activations and binary concepts, we first need to binarize the neuron activations.

Following ~\cite{bau2017network}, we define a threshold function $\mathcal{T}_{\beta}$ that converts continuous neuron activations to binary values:

\begin{equation}
    \mathcal{T}_{\beta}(h_{i,l}(s)) = \begin{cases}
        1 & \text{if } h_{i,l}(s) > \beta \\
        0 & \text{otherwise}
    \end{cases}
\end{equation}

where $\beta$ is a threshold parameter that determines the activation significance level. For a set of input states $\mathcal{S} = \{s_1, ..., s_n\}$, we can obtain binary vectors representing both neuron activations and concept outputs:

\begin{equation}
    \mathbf{a}_{i,l} = [\mathcal{T}_{\beta}(h_{i,l}(s_1)), ..., \mathcal{T}_{\beta}(h_{i,l}(s_n))]
\end{equation}

\begin{equation}
    \mathbf{c} = [C_k(s_1), ..., C_k(s_n)]
\end{equation}

To measure the similarity between the binarized neuron activation pattern and concept function outputs, we employ the Jaccard similarity coefficient, defined as:

\begin{equation}
    J(\mathbf{a}_{i,l},\mathbf{c}) = \frac{|\mathbf{a}_{i,l}\cap \mathbf{c}|}{|\mathbf{a}_{i,l}\cup \mathbf{c}|}
\end{equation}

Given this similarity measure, the problem of finding the most suitable concept to explain a neuron's behavior can be formalized as an optimization problem. For each neuron $(i,l)$, we aim to find the concept $C$ from the concept space $\mathcal{C}$ that maximizes the Jaccard similarity with the neuron's activation pattern:

\begin{equation}
    C^*_{i,l} = \underset{C\in \mathcal{C}}{\arg \max}\; J(\mathbf{a}_{i,l},\mathbf{c})\label{optimize_equation}
\end{equation}

where $C^*_{i,l}$ represents the optimal concept explanation for neuron $(i,l)$, and $\mathbf{c}$ is the binary vector generated by applying concept $C$ to the input states $\mathcal{S}$.

\begin{algorithm}[!tb]
\caption{Concept Matching via Beam Search}
\label{alg:beam_search}
\hspace*{\algorithmicindent} \textbf{Input} Model $\mathcal{M}$, state samples $\mathcal{S}$, atomic concepts $\mathcal{A}$, beam width $w$, max length $\text{max\_length}$ \\
\hspace*{\algorithmicindent} \textbf{Output} Best concept and score $(\text{best\_c}, \text{best\_s})$
\begin{algorithmic}[1]
\State Initialise $\mathcal{B} \gets \mathcal{A}$ \Comment{$\mathcal{B}$: beam of concepts}
\State Initialise $\text{best\_c} \gets \text{None}, \text{best\_s} \gets 0$
\For{$\text{len} \in \{2, \dots, \text{max\_length}\}$}
    \State $\mathcal{K} \gets \{\}$ \Comment{$\mathcal{K}$: candidates}
    \For{$c_1, c_2 \in \mathcal{B} \times \mathcal{B}$}
        \For{$\oplus \in \{\text{and}, \text{or}, \text{not}\}$}
            \If{$\oplus = \text{not}$}
                \State $\text{new\_c} \gets \text{not}(c_1)$
            \Else
                \State $\text{new\_c} \gets \oplus(c_1, c_2)$
            \EndIf
            \State $\text{s} \gets J(\mathbf{v}_u, \text{new\_c})$
            \State $\mathcal{K} \gets \mathcal{K} \cup \{(\text{new\_c}, \text{s})\}$
        \EndFor
    \EndFor
    \State $\mathcal{B} \gets \text{top\_}w\text{\_by\_score}(\mathcal{K})$
    \If{$\text{max\_score}(\mathcal{K}) > \text{best\_s}$}
        \State $\text{best\_c} \gets \text{arg\_max\_score}(\mathcal{K})$
        \State $\text{best\_s} \gets \text{max\_score}(\mathcal{K})$
    \EndIf
\EndFor
\State \Return $\text{best\_c}, \text{best\_s}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[!tb]
\caption{Compositional Concept-Based DRL Neuron Interpretation}
\label{alg:extract}
\hspace*{\algorithmicindent} \textbf{Input} Model $\mathcal{M}$, state samples $\mathcal{S}$, atomic concepts $\mathcal{A}$ \\
\hspace*{\algorithmicindent} \textbf{Output} Neuron concept scores $\verb|map|$
\begin{algorithmic}[1]
\State Initialise $\verb|map| \gets \text{empty map}$
\For{$u \in \text{neurons}(\mathcal{M})$}
    \State $\mathbf{v}_u \gets \text{GetBinarizedActivations}(u, \mathcal{S})$
    \State $\text{c}, \text{s} \gets \textsc{BeamSearch}(\mathbf{v}_u, \mathcal{A})$
    \State $\verb|map|[u] \gets (\text{c}, \text{s})$
\EndFor
\State \Return $\verb|map|$
\end{algorithmic}
\end{algorithm}

To optimize Equation \ref{optimize_equation}, we need to search in a structured space of compositional expressions. This optimization problem is inherently challenging due to the vast search space. Similar to ~\cite{mu2020compositional}, we adopt beam search as our optimization strategy (Algorithm \ref{alg:beam_search}). Specifically, we set the beam size to 10 and impose a maximum length constraint N on the formulas. A complete and detailed view of our interpretation framework is shown in Algorithm \ref{alg:extract}, which presents the overall neuron concept extraction procedure.


% \begin{algorithm}[tb]
% \caption{DRL neuron concept extraction framework}
% \label{extract_drl}
% \hspace*{\algorithmicindent} \textbf{Input} Model $\mathcal{M}$, state samples $\mathcal{S}$, atomic concepts $\mathcal{A}$, beam width $w$, max length $\text{max\_length}$ \\
% \hspace*{\algorithmicindent} \textbf{Output} Neuron concept scores $\verb|map| : \text{neurons}(\mathcal{M}) \times \mathcal{C} \rightarrow \mathbb{R}$
% \begin{algorithmic}[1]

% \Procedure{GetActs}{}
% \State Initialise $\mathbf{A} \gets \{\}$ \Comment{$\mathbf{A}$: activations for each neuron}
% \For{$s \in \mathcal{S}$}
%     \State $\mathbf{a} \gets \mathcal{M}(s)$ \Comment{$\mathbf{a}$: activations for state $s$}
%     \For{$u \in \text{neurons}(\mathcal{M})$}
%         \State $\mathbf{A}[u] \gets \mathbf{A}[u] \cup \mathcal{T}_{\beta}(\mathbf{a}[u])$
%     \EndFor
% \EndFor
% \State \Return $\mathbf{A}$
% \EndProcedure

% \Procedure{GetVecs}{}
% \State Initialise $\mathbf{C} \gets \{\}$ \Comment{$\mathbf{C}$: concept vectors}
% \For{$c \in \mathcal{A}$}
%     \State $\mathbf{v} \gets []$ \Comment{$\mathbf{v}$: vector for concept $c$}
%     \For{$s \in \mathcal{S}$}
%         \State $\mathbf{v}.\text{append}(c(s))$
%     \EndFor
%     \State $\mathbf{C}[c] \gets \mathbf{v}$
% \EndFor
% \State \Return $\mathbf{C}$
% \EndProcedure

% \Procedure{SearchBest}{\textbf{Input:} $\mathbf{v}_u, \mathbf{C}$}
% \State Initialise $\mathcal{B} \gets \mathcal{A}$ \Comment{$\mathcal{B}$: beam of concepts}
% \State Initialise $\text{best\_c} \gets \text{None}, \text{best\_s} \gets 0$ \Comment{Best concept and score}
% \For{$\text{len} \in \{2, \dots, \text{max\_length}\}$}
%     \State $\mathcal{K} \gets \{\}$ \Comment{$\mathcal{K}$: candidates}
%     \For{$c_1, c_2 \in \mathcal{B} \times \mathcal{B}$}
%         \For{$\oplus \in \{\text{and}, \text{or}, \text{not}\}$}
%             \If{$\oplus = \text{not}$}
%                 \State $\text{new\_c} \gets \text{not}(c_1)$
%             \Else
%                 \State $\text{new\_c} \gets \oplus(c_1, c_2)$
%             \EndIf
%             \State $\text{s} \gets J(\mathbf{v}_u, \text{new\_c})$
%             \State $\mathcal{K} \gets \mathcal{K} \cup \{(\text{new\_c}, \text{s})\}$
%         \EndFor
%     \EndFor
%     \State $\mathcal{B} \gets \text{top\_}w\text{\_by\_score}(\mathcal{K})$
%     \If{$\text{max\_score}(\mathcal{K}) > \text{best\_s}$}
%         \State $\text{best\_c} \gets \text{arg\_max\_score}(\mathcal{K})$
%         \State $\text{best\_s} \gets \text{max\_score}(\mathcal{K})$
%     \EndIf
% \EndFor
% \State \Return $\text{best\_c}, \text{best\_s}$
% \EndProcedure

% \Procedure{Extract}{}
% \State Initialise $\verb|map| \gets \text{empty map}$
% \State $\mathbf{A} \gets \textsc{GetActs}()$ \Comment{$\mathbf{A}$: neuron activations}
% \State $\mathbf{C} \gets \textsc{GetVecs}()$ \Comment{$\mathbf{C}$: concept vectors}
% \For{$u \in \text{neurons}(\mathcal{M})$}
%     \State $\text{c}, \text{s} \gets \textsc{SearchBest}(\mathbf{A}[u], \mathbf{C})$
%     \State $\verb|map|[u] \gets (\text{c}, \text{s})$
% \EndFor
% \State \Return $\verb|map|$
% \EndProcedure

% \State Call \textsc{Extract}()
% \end{algorithmic}
% \end{algorithm}


\section{Experiment}

In this section, we use concept matching methods to investigate the concepts involved in policy networks and value networks of deep reinforcement learning\footnote{The complete source code and experimental implementation can be accessed at: \href{https://anonymous.4open.science/r/CCN-DRL-1FCC}{https://anonymous.4open.science/r/CCN-DRL-1FCC}}.

\subsection{Environment}

We conduct experiments on both discrete and continuous control tasks: Blackjack-v1 (discrete action space with 2 actions), a card game where the agent needs to optimize decisions of hitting or standing to beat the dealer without going over 21; LunarLander-v3 (discrete with 4 actions), a spacecraft landing task where the agent controls the main engine and side thrusters to safely land on a designated pad; and LunarLander-Continuous-v2 (continuous with 2-dimensional action space), a variant of LunarLander with continuous control over engine power and landing trajectory. All environments are from Gymnasium \cite{towers2024gymnasium}.

Both the Q-network in DQN and the actor/critic networks in PPO share the same architecture: three fully-connected layers with 64 hidden units each. For discrete environments, we employ DQN with a Q-network, while for the continuous case, we use PPO. Our interpretations focus on the neurons in the second hidden layer, as this layer typically captures high-level features before the final action/value predictions. For all experiments, we set the activation threshold $\beta=0$ when binarizing neuron activations, treating positive activations as concept presence and negative activations as concept absence.

When searching for logical formulas to interpret neurons, we limit the maximum formula length to 5 to maintain interpretability while allowing sufficient expressiveness. We analyze only neurons that activate in more than 5\% of the sampled states, with 10K states sampled for both Lunar Lander and Blackjack environments.

\begin{figure*}[!tb]
    \centering
    \includegraphics[height=0.7\linewidth]{figures/lunar_discrete.pdf}
    \caption{Visualization of three representative neurons in the discrete LunarLander (DQN) value network. Each row shows three different states demonstrating distinct neuron functionalities: landing detection (Neuron 19, top), attitude control (Neuron 41, middle), and horizontal velocity management (Neuron 21, bottom). Red/green colors indicate active/inactive states, with activation values shown. Key state variables are annotated to highlight triggering conditions.}
    \label{fig:luanrlander-discrete}
\end{figure*}

\subsection{Concepts for each environment}

To formalize our atomic concepts, we adopt an interval-based notation where each concept is denoted by its corresponding state variable and interval range. For example, $X_{(a,b]}$ represents states where the horizontal position $x$ is in the interval $(a,b]$, and $Vx_{_{(a,b]}}$ represents states where the horizontal velocity $v_x$ is in $(a,b]$. Similarly, $\theta_{(a,b]}$ and $\omega_{(a,b]}$ represent angle and angular velocity intervals respectively. All intervals are left-open and right-closed unless explicitly stated otherwise. Binary contact indicators LLeg and RLeg remain as is, representing ground contact status of the landing legs.

\subsubsection{LunarLander Environments}
For both discrete and continuous versions of LunarLander, we define atomic concepts covering four key aspects of the spacecraft state:
\begin{itemize}
    \item Position concepts: horizontal position (6 regions from far left to right, e.g., $X_{(-0.25,0]}$, $X_{(0,0.25]}$) and vertical position (7 regions from ground level to maximum height)
    \item Velocity concepts: horizontal velocity (e.g., $Vx_{_{(0.1,0.2]}}$ indicating slight rightward motion) and vertical velocity (e.g., $Vy_{_{(-0.4,-0.2]}}$ for moderate descent)
    \item Attitude concepts: angle (e.g., $\theta_{[-0.15,0]}$, $\theta_{[0,0.15]}$ representing near-vertical orientation) and angular velocity ($\omega_{{(-0.1,0]}}$)
    \item Contact concepts: binary indicators LLeg and RLeg for left and right landing gear ground contact
\end{itemize}

Through concept matching, we discover several interpretable neurons in the second hidden layer of the value network of discrete LunarLander (DQN) that form a hierarchical control structure. As shown in Figure~\ref{fig:luanrlander-discrete}, we visualize three representative neurons that demonstrate different aspects of the control hierarchy. At the lowest level, we find neurons dedicated to basic state detection, such as Neuron 19 which serves as a binary landing detector by monitoring ground contact (LLeg OR RLeg).

The attitude control system is represented by neurons like Neuron 41, which combines attitude monitoring (($\theta_{[-0.15,0]}$ OR $\theta_{[0,0.15]}$) indicating near-vertical orientation) with landing gear status (NOT LLeg). Similar attitude-focused neurons specialize in different phases of the landing sequence.

Higher-level strategic control emerges in neurons that integrate multiple state aspects. Neuron 21 exemplifies this by combining horizontal velocity control ($Vx_{_{(0.1,0.2]}}$-$Vx_{_{(0.4,1]}}$) with position awareness (NOT $X_{(-0.25,0]}$). We also identified neurons like Neuron 50 that handle high-altitude maneuvering, considering both vertical position ($Y_{(0.5,0.7]}$, $Y_{(0.7,\infty)}$) and horizontal state ($X_{(0.25,0.4]}$, $Vx_{_{(-0.4,-0.2]}}$) while maintaining safe ground distance.

The network also contains specialized safety-monitoring neurons, such as Neuron 18 and 30, which track descent velocity ($Vy_{_{(-0.2,-0.1]}}$-$Vy_{_{(-1,-0.4]}}$) while considering landing gear status. These neurons typically show high activation during critical phases of the descent.

% \begin{table*}[tb]
% \centering
% \begin{tabular}{lclr}
% \toprule
% Neuron & Jaccard Similarity & Concept & $(w_{stick}, w_{hit})$ \\
% \midrule
% 28 & 0.92 & $P_{17} \vee P_{18} \vee P_{19} \vee P_{20} \vee P_{21}$ & (0.371, 0.157) \\
% 13 & 0.79 & $P_6 \vee P_7 \vee P_8 \vee P_9 \vee P_{10}$ & (-0.811, -0.352) \\
% 17 & 0.89 & $D_7 \vee D_8 \vee D_9 \vee D_{10}$ & (-0.195, -0.083) \\
% 6 & 0.89 & $(HasAce \vee NoAce) \wedge \neg P_9 \wedge \neg P_{10} \wedge \neg P_{11}$ & (0.078, -0.189) \\
% 60 & 0.60 & $\neg HasAce \wedge P_{20} \vee P_{21} \wedge D_{10}$ & (-0.148, -0.268) \\
% \bottomrule
% \end{tabular}
% \caption{Representative neurons in the Blackjack value network. Each neuron is characterized by its alignment with the logical concept (Jaccard Similarity) and its influence on action values $(w_{stick}, w_{hit})$.}
% \label{tab:blackjack_neurons}
% \end{table*}
\begin{table}[tb]
\centering
\begin{tabular}{@{}rp{0.5cm}p{3.2cm}r@{\hskip 0.2cm}r@{}}
\toprule
Neuron & Jacc. & \multicolumn{1}{c}{Concept} & \multicolumn{1}{c}{$w_{stick}$} & \multicolumn{1}{c@{}}{$w_{hit}$} \\
\midrule
28 & 0.92 & {\small $P_{17} \vee P_{18} \vee P_{19} \vee P_{20} \vee P_{21}$} & $\phantom{-}0.371$ & $\phantom{-}0.157$ \\[2pt]
13 & 0.79 & {\small $P_6 \vee P_7 \vee P_8 \vee P_9 \vee P_{10}$} & $-0.811$ & $-0.352$ \\[2pt]
17 & 0.89 & {\small $D_7 \vee D_8 \vee D_9 \vee D_{10}$} & $-0.195$ & $-0.083$ \\[2pt]
6 & 0.89 & {\small $\neg P_9 \wedge \neg P_{10} \wedge \neg P_{11}$} & $\phantom{-}0.078$ & $-0.189$ \\[2pt]
60 & 0.60 & {\small $\text{NoAce} \wedge P_{20} \vee P_{21} \wedge D_{10}$} & $-0.148$ & $-0.268$ \\
\bottomrule
\end{tabular}
\caption{Representative neurons in the Blackjack value network. Each neuron is characterized by its alignment with the logical concept (Jaccard Similarity) and its influence on action values $w_{stick}$ and $w_{hit}$.}
\label{tab:blackjack_neurons}
\end{table}

We also applied our method to the continuous action version with PPO, which yielded similar interpretable structures. This consistency across different action spaces and training algorithms demonstrates the robustness of our interpretation method and suggests that neural networks tend to decompose the landing task in similar ways regardless of the specific control paradigm used.

\subsubsection{Blackjack Environment}
The Blackjack environment presents a simpler state space compared to LunarLander, consisting of three features: the player's current sum (1-21), the dealer's showing card (1-10), and whether the player holds a usable ace (0 or 1). We defined atomic concepts for each possible player sum ($P_i$), dealer card ($D_i$), and ace status (HasAce, NoAce).

Our method revealed several interpretable neurons in the second hidden layer of the value network that encode fundamental Blackjack strategy rules, as shown in Table~\ref{tab:blackjack_neurons}. The network decomposes the game strategy into key components: Neuron 28 encodes the critical decision boundary for high sums (17-21), with its weights favoring 'stick' (0.371 vs 0.157) aligning with basic strategy. Neuron 13 manages low-range hands (6-10), where its negative weights indicate aggressive card-seeking behavior.

The network also learns sophisticated strategic patterns through neurons like Neuron 17, which monitors dealer's strong cards (7-10), and Neuron 60, which handles strong hands (20-21) without an ace against a dealer's ten. These neurons collectively demonstrate how the network decomposes Blackjack strategy into interpretable components that align with established playing principles, with weights between 'stick' and 'hit' actions reflecting their specific strategic roles.


\begin{figure*}[tb]
    \centering
    \includegraphics[width=\linewidth]{figures/lunar_attack.pdf}
    \caption{Perturbation analysis of Neuron 5 in discrete LunarLander. Left: The network architecture showing how Neuron 5 contributes to action selection through weighted connections. Middle: Original state where the neuron is active ($h(s)=4.00$) and the network selects "fire left engine". Right: Perturbed state where modifying the x-coordinate to -0.24 causes the neuron to become inactive ($h(s')=-4.48$) and changes the selected action to "main engine". The consistent relationship between concept satisfaction, neuron activation, and action selection validates our interpretation.}
    \label{fig:lunar_perturbation}
\end{figure*}

\begin{table*}[tb]
\centering
\begin{tabular}{rp{3.5cm}p{2.5cm}p{4cm}p{4cm}}
\toprule
Neuron & Concept & Connection Weights $(w_{stick}, w_{hit})$ & Original State & Perturbed State \\
\midrule
28 & {\small $P_{18} \vee P_{20} \vee P_{19} \vee P_{17} \vee P_{21}$} & (0.371, 0.157) & Action: Stick \newline State: (20, 9, 0) \newline $h(s) = 2.044$ (active) & Action: Hit \newline State: (14, 9, 0) \newline $h(s') = -1.030$ (inactive) \\
\midrule
13 & {\small $P_{10} \vee P_9 \vee P_8 \vee P_7 \vee P_6$} & (-0.811, -0.352) & Action: Hit \newline State: (6, 9, 0) \newline $h(s) = 2.042$ (active) & Action: Stick \newline State: (17, 9, 0) \newline $h(s') = -6.891$ (inactive) \\
\midrule
17 & {\small $D_{10} \vee D_9 \vee D_8 \vee D_7$} & (-0.195, -0.083) & Action: Hit \newline State: (15, 9, 0) \newline $h(s) = 1.069$ (active) & Action: Stick \newline State: (15, 5, 0) \newline $h(s') = -0.921$ (inactive) \\
\bottomrule
\end{tabular}
\caption{Perturbation analysis of three representative neurons in Blackjack. Each row shows a neuron's concept interpretation, its connection weights to action outputs, and the effects of targeted perturbation. The perturbations demonstrate how violating a neuron's concept leads to predictable changes in both neuron activation (from active to inactive) and action selection.}
\label{tab:blackjack_perturbation}
\end{table*}


\section{Validation through Targeted Perturbations}

To validate our concept-based interpretations and demonstrate their utility for model behavior manipulation, we conduct targeted perturbation experiments. Our validation is based on two key insights: First, if our interpretations accurately capture the decision-making logic, perturbing specific features within concept-matching states should predictably affect neuron activations and subsequent actions. Second, by monitoring the penultimate layer representations (before action prediction), we can identify neurons most contributive to specific actions through their connection weights and verify whether manipulating these neurons' activations leads to predictable behavioral changes.

For each environment, we first identify neurons highly connected to specific actions through weight analysis. We then design targeted perturbations that modify concept-relevant features while maintaining state validity, and observe changes in both neuron activation and action selection. This systematic approach allows us to verify both the accuracy of our interpretations and their potential for controlled behavior manipulation.

\subsection{Results in LunarLander Environment}

For the discrete LunarLander (DQN), we focus on validating the interpretation of Neuron 5, which was identified as a key contributor to the "fire left engine" action through weight analysis. This neuron's interpreted concept involves a complex combination of spatial and attitude conditions, expressed as $(\neg X_{[-0.25,0]} \wedge \neg\theta_{[-1,-0.15]} \wedge (Vx_{_{[-0.4,0.2]}} \vee X_{[0.25,0.4]})) \vee \theta_{[0.15,1]}$.

Figure~\ref{fig:lunar_perturbation} illustrates our perturbation experiment.In the original state ($x=0.27$, $y=0.26$, $v_x=-0.13$, $v_y=-0.30$, $\theta=0.13$, $\omega=-0.04$, both legs not in contact), the lander is slightly right of center with a leftward velocity. Neuron 5 shows strong activation ($h(s)=4.00$), and the network selects the "fire left engine" action, consistent with the need to maintain horizontal position control.

When we perturb the x-coordinate to -0.24 (moving the lander left of center), the neuron's activation drops significantly ($h(s')=-4.48$), and the network switches its action to "fire main engine". This change aligns perfectly with our interpretation: the perturbation violates the spatial component of the neuron's concept, causing it to become inactive, and the network appropriately adjusts its control strategy given the new position.





\subsection{Results in Blackjack Environment}

\paragraph{Blackjack Environment}
For the Blackjack environment, we examine three representative neurons that embody distinct strategic concepts in the game. Table~\ref{tab:blackjack_perturbation} shows how targeted perturbations affect neuron activations and subsequent action selections.


Neuron 28 detects high sums (18-21), showing strong activation ($h(s)=2.044$) with sum 20 and promoting "stick". When perturbed to sum 14, it deactivates ($h(s')=-1.030$) and switches to "hit". Similarly, Neuron 13 monitors low sums (6-10), activating ($h(s)=2.042$) with sum 6 to promote "hit", but deactivating ($h(s')=-6.891$) when perturbed to sum 17. Neuron 17 tracks dealer strength, activating ($h(s)=1.069$) with dealer's 9 but deactivating ($h(s')=-0.921$) when perturbed to 5, adjusting strategy accordingly.

In all cases, we observe clear activation-to-inhibition transitions when concept-relevant features are perturbed, with action changes that logically follow from the neurons' interpreted strategic roles. These results strongly support the reliability of our compositional interpretation method, demonstrating that the identified concepts genuinely capture the network's decision-making logic.

\subsection{Discussion}

Our perturbation experiments across both environments demonstrate two critical aspects of our interpretation method:

\begin{itemize}
    \item \textbf{Interpretation Reliability:} The consistent relationship between concept satisfaction, neuron activation, and action selection validates our interpretations. In both discrete (Blackjack, LunarLander-DQN) and continuous action spaces, violating a neuron's concept reliably leads to its deactivation and predictable behavioral changes.
    
    \item \textbf{Behavioral Control:} By identifying neurons strongly connected to specific actions and understanding their concepts, we can systematically manipulate model behavior. 
\end{itemize}

These results suggest that our compositional interpretation method not only reveals interpretable decision logic but also provides a practical means for understanding and controlling neural network behavior. The ability to predictably influence model decisions through concept-based perturbations further validates the reliability and utility of our interpretations.

\section{Conclusion}

In this work, we have introduced a novel concept-based neuron-level interpretation method for deep reinforcement learning models, demonstrating through experiments on LunarLander and Blackjack environments how individual neurons encode meaningful, human-interpretable concepts. The reliability of our interpretations has been rigorously validated through targeted perturbation experiments, showing consistent relationships between concept satisfaction, neuron activation patterns, and action selection across different environments and action spaces.

While our current implementation requires manual design of atomic concepts, future work could focus on developing automated methods for concept identification and exploring applications in network pruning and robustness enhancement.

\appendix
%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai25}
\end{document}