\section{Related Work}
\paragraph{Explainability of reinforcement learning.} Our work falls into the model-explaining category of XRL frameworks, as classified by Qing et al.~\cite{qing2023surveyexplainablereinforcementlearning}, specifically the explanation-generating subcategory that aims to provide post-hoc interpretations of trained RL models. Self-explainable approaches incorporate interpretability directly into model architectures through various means~\cite{liu2019lmut,verma2018programmatically,landajuela2021discovering,delfosse2024interpretable,payani2020incorporating}. Of particular interest are recent works that leverage concept bottleneck models, where Zabounidis et al.~\cite{zabounidis2023concept} introduce interpretable concepts into multi-agent RL architectures, and Ye et al.~\cite{ye2024conceptbased} propose methods to reduce the human annotation burden in concept learning. Unlike these self-explainable methods, explanation-generating approaches analyze existing RL policies without compromising model expressiveness and performance. Previous works in this direction have explored diverse approaches: using causal modeling to trace action effects through interpretable chains~\cite{yu2023causal}; generating explanations by mapping agent actions to predefined instruction templates~\cite{boggess2023explainable,hayes2017improving}; and adapting classic neural network interpretation methods, particularly attribution approaches~\cite{nikulin2019free,rizzo2019reinforcement,joo2019visualization,shi2020self}. While these techniques have advanced our understanding of DRL decision-making, they primarily focus on input-output relationships without examining the internal neural mechanisms that drive policy decisions.

\paragraph{Concept-based explanations.} Concept-based interpretation has emerged as a powerful approach for understanding neural networks by connecting neural activations with human-understandable concepts. Early work Network Dissection~\cite{bau2017network} established this direction by aligning individual CNN neurons with visual concepts through activation matching. This framework was extended by several subsequent works: TCAV~\cite{kim2018interpretability} introduced methods to quantify concept importance in model predictions, ACE~\cite{ghorbani2019towards} developed techniques for automatic concept discovery, and CEN~\cite{mu2020compositional} proposed compositional explanations using Boolean combinations of concepts. While these approaches have been successfully adapted to various domains, including graph neural networks~\cite{xuanyuan2023global}, their application in reinforcement learning remains largely unexplored. Our work addresses this gap by introducing compositional concept-based interpretations to DRL agents, revealing how neurons process and combine temporal concepts in decision-making processes.