\section{Related work}
In the literature, studies\cite{guo2022survey,vladika2023scientific} briefly review and analyze the datasets and methodologies related to fact-checking. In litrature several datasets have been proposed to support general fact-checking research across various domains, including politics, journalism, and social media~\cite{vlachos2014fact,popat2017truth,thorne2018fever,thorne2018fact,alhindi2018your,chen2019seeing,hanselowski2019richly,augenstein2019multifc,atanasova2024multi,sathe2020automated,saakyan2021covid,schuster2021get,aly2021feverous}.
% including PolitiFact\cite{vlachos2014fact}, Emergent\cite{ferreira2016emergent}, LIAR\cite{wang2017liar}, Snopes\cite{popat2017truth}, FEVER\cite{thorne2018fever,thorne2018fact}, LIAR-PLUS\cite{alhindi2018your}, Perspectrum\cite{chen2019seeing}% , UKP Snopes\cite{hanselowski2019richly}, MultiFC\cite{augenstein2019multifc}, PolitiHop\cite{atanasova2024multi}, WikiFactCheck-English\cite{sathe2020automated}, COVID-Fact\cite{saakyan2021covid}, Vitamin-C\cite{schuster2021get}, and FEVEROUS\cite{aly2021feverous}. 
% Scientific fact-checking is a specialized subset of fact-checking focused on verifying scientific claims, supporting researchers in validating hypotheses, and helping the public interpret and contextualize new scientific findings while addressing misinformation.\cite{vladika2023scientific,lu2023scitab}.
Scientific fact-checking, a subfield of fact-checking, focuses on verifying claims about scientific knowledge, thereby combating misinformation while also supporting scientific inquiry, interpretation and public understanding of research~\cite{vladika2023scientific}. Initial  study~\cite{wadden2020fact} on scientific fact-checking proposed the \textit{SCIFACT} dataset for scientific claim verification, using citance as claims and research abstracts as evidence. In contrast, \textit{SCIFACT-OPEN}\cite{wadden2022scifact} extends the study~\cite{wadden2020fact} by collecting abstracts from multiple papers to support or refute the claims. Further study~\cite{wright2022generating} 
introduced an encoder-decoder model that utilizes sentences surrounding stances and BART to generate claims supported by scientific evidence, with negative claims through named entity replacements to generate the claim refuted for scientific evidence. However, existing scientific claim verification datasets in the litrature have several key limitations. A primary concern is their limited size, as they typically consist of only a few hundred or thousand samples. This is inadequate for training models that can generalize effectively across diverse categories and domains \cite{vladika2023scientific}. Moreover, these datasets primarily depend on research paper abstracts as external evidence. However, abstracts offer only a concise summary and often lack the comprehensive details necessary for rigorous scientific claim verification \cite{vladika2023scientific}. Another limitation is the emphasis on extracting claims solely from sentences containing citations of research papers, which overlooks significant claims and insights presented in the results, discussion, and conclusion sections. Furthermore, there is the absence of a scientific claim verification dataset in the literature, which contains scientific claims and scientific evidence involving cardinal or numeral values to train models for numeral-aware scientific claim verifications. Motivated by such limitations with existing datasets for scientific claim verification in literature, this study proposes two novel datasets for scientific claim verification tasks \textit{SciCliamHunt} and \textit{ SciClaimHunt\_Num}.












% A recent study\cite{lu2023scitab} of scientific claim verification proposed \textit{SCITAB}\cite{lu2023scitab} datasets for compositional reasoning
% and claim verification on scientific tables.
      % Our proposed \textbf{SciCliamHunt} dataset is an extensive collection of pair claims and research papers as evidence which supports and refutes the claims. Similarly, \textbf{ SciClaimHunt\_Num} is a pair of claim and scientific research pairs with cardinal or numeral values supported or refuted by the research paper. Though studies\cite{thorne2017extensible, venktesh2024quantemp} propose StatProps and QuanTemp numeral claims verification, respectively, both the datasets are small in number of samples and only for general fact-checking, not applicable for scientific fact-checking.