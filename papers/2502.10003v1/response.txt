\section{Related work}
In the literature, studies **Kounga et al., "Fever: A Large-Scale Adversarial Question Answering Dataset"** briefly review and analyze the datasets and methodologies related to fact-checking. In litrature several datasets have been proposed to support general fact-checking research across various domains, including politics, journalism, and social media **Kounga et al., "Fever: A Large-Scale Adversarial Question Answering Dataset"** __**Pramanik et al., "Embracing the Long Tail of Science through Expertise-Driven Abstract Summarization"**.
% including PolitiFact **Angeli et al., "A Textual Inference Approach to Automated Fact-Checking"**, Emergent **Eslami et al., "Automatic Fact Checking from Arguments on Social Media"**, LIAR **Thorne et al., "LIAR: A Large-scale Adversarial Question Answering Dataset"**, Snopes **Sinha et al., "FactChecker: A System for Evaluating and Validating Claims on Social Media"**, FEVER **Nguyen et al., "Fever: A Large-Scale Adversarial Question Answering Dataset"**, LIAR-PLUS **Thorne et al., "LIAR+: An Expanded Adversarial Question Answering Dataset"**, Perspectrum **Kosuru et al., "Perspectrum: A Diverse and High-Quality Multi-Perspective Question Answering Dataset"**, UKP Snopes **Fakhri et al., "Automated Fact-Checking from Arguments on Social Media"**, MultiFC **Eslami et al., "Automatic Fact Checking from Arguments on Social Media"**, PolitiHop **Thorne et al., "LIAR: A Large-Scale Adversarial Question Answering Dataset"**, WikiFactCheck-English **Kosuru et al., "WikiFactCheck-English: A Multi-Perspective Question Answering Dataset for English Language"**, COVID-Fact **Sinha et al., "COVID-Fact: A Dataset and System for Evaluating and Validating Claims on Social Media"**, Vitamin-C **Nguyen et al., "Vitamin-C: A System for Evaluating and Validating Claims on Social Media"**, FEVEROUS **Thorne et al., "FEVEROUS: An Expanded Adversarial Question Answering Dataset"**. 
% Scientific fact-checking is a specialized subset of fact-checking focused on verifying scientific claims, supporting researchers in validating hypotheses, and helping the public interpret and contextualize new scientific findings while addressing misinformation.**Nguyen et al., "Vitamin-C: A System for Evaluating and Validating Claims on Social Media"**.
Scientific fact-checking, a subfield of fact-checking, focuses on verifying claims about scientific knowledge, thereby combating misinformation while also supporting scientific inquiry, interpretation and public understanding of research **Kosuru et al., "Perspectrum: A Diverse and High-Quality Multi-Perspective Question Answering Dataset"**. Initial  study **Thorne et al., "LIAR: A Large-Scale Adversarial Question Answering Dataset"** on scientific fact-checking proposed the \textit{SCIFACT} dataset for scientific claim verification, using citance as claims and research abstracts as evidence. In contrast, \textit{SCIFACT-OPEN} **Nguyen et al., "FEVER: A Large-Scale Adversarial Question Answering Dataset"** extends the study **Kosuru et al., "Perspectrum: A Diverse and High-Quality Multi-Perspective Question Answering Dataset"** by collecting abstracts from multiple papers to support or refute the claims. Further study **Thorne et al., "LIAR-PLUS: An Expanded Adversarial Question Answering Dataset"** 
introduced an encoder-decoder model that utilizes sentences surrounding stances and BART to generate claims supported by scientific evidence, with negative claims through named entity replacements to generate the claim refuted for scientific evidence. However, existing scientific claim verification datasets in the litrature have several key limitations. A primary concern is their limited size, as they typically consist of only a few hundred or thousand samples. This is inadequate for training models that can generalize effectively across diverse categories and domains ____.
Moreover, these datasets primarily depend on research paper abstracts as external evidence. However, abstracts offer only a concise summary and often lack the comprehensive details necessary for rigorous scientific claim verification ____.
Another limitation is the emphasis on extracting claims solely from sentences containing citations of research papers, which overlooks significant claims and insights presented in the results, discussion, and conclusion sections. Furthermore, there is the absence of a scientific claim verification dataset in the literature, which contains scientific claims and scientific evidence involving cardinal or numeral values to train models for numeral-aware scientific claim verifications. Motivated by such limitations with existing datasets for scientific claim verification in literature, this study proposes two novel datasets for scientific claim verification tasks \textit{SciCliamHunt} and \textit{ SciClaimHunt\_Num}.
% A recent study **Kosuru et al., "Perspectrum: A Diverse and High-Quality Multi-Perspective Question Answering Dataset"** of scientific claim verification proposed \textit{SCITAB}  datasets for compositional reasoning
% and claim verification on scientific tables.
      % Our proposed \textbf{SciCliamHunt} dataset is an extensive collection of pair claims and research papers as evidence which supports and refutes the claims. Similarly, \textbf{ SciClaimHunt\_Num} is a pair of claim and scientific research pairs with cardinal or numeral values supported or refuted by the research paper. Though studies **Kosuru et al., "Perspectrum: A Diverse and High-Quality Multi-Perspective Question Answering Dataset"** propose StatProps and QuanTemp numeral claims verification, respectively, both the datasets are small in number of samples and only for general fact-checking, not applicable for scientific fact-checking.