\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{adjustbox}
\usepackage{hyperref} 
\usepackage{multirow} 
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\pagestyle{plain}
\begin{document}




 \title{SciClaimHunt: A Large Dataset for Evidence-based Scientific Claim Verification}

% \author{\IEEEauthorblockN{Anonymous Authors}}
\author{
  Sujit Kumar$^{1*}$, Anshul Sharma$^{2*}$, Siddharth Hemant Khincha$^{1*}$\thanks{* Equal contributions}, 
    Gargi Shroff$^{2}$, Sanasam Ranbir Singh$^{1}$, Rahul Mishra$^{2}$\\
    $^{1}$Department of Computer Science and Engineering, \\
    Indian Institute of Technology, Guwahati, Assam, India \\
    $^{2}$ International Institute of Information Technology Hyderabad\\
    \texttt{\{sujitkumar,s.khincha\}@alumni.iitg.ac.in}, \texttt{ranbir@iitg.ac.in}\\
    \texttt{\{anshul.s,gargi.shroff\}@research.iiit.ac.in}, \texttt{rahul.mishra@iiit.ac.in}
    
}
% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

\maketitle
% \footnote{\boldsymbol{*}~Equal contributions}

\begin{abstract}
Verifying scientific claims presents a significantly greater challenge than verifying political or news-related claims. Unlike the relatively broad audience for political claims, the users of scientific claim verification systems can vary widely, ranging from researchers testing specific hypotheses to everyday users seeking information on a medication. Additionally, the evidence for scientific claims is often highly complex, involving technical terminology and intricate domain-specific concepts that require specialized models for accurate verification. Despite considerable interest from the research community, there is a noticeable lack of large-scale scientific claim verification datasets to benchmark and train effective models. To bridge this gap, we introduce two large-scale datasets, \textit{SciClaimHunt} and  \textit{SciClaimHunt\_Num}, derived from scientific research papers. We propose several baseline models tailored for scientific claim verification to assess the effectiveness of these datasets. Additionally, we evaluate models trained on \textit{SciClaimHunt} and  \textit{SciClaimHunt\_Num} against existing scientific claim verification datasets to gauge their quality and reliability. Furthermore, we conduct human evaluations of the datasetâ€™s claims and perform error analysis to assess the effectiveness of the proposed baseline models. Our findings indicate that\textit{SciClaimHunt} and  \textit{SciClaimHunt\_Num} serve as highly reliable resources for training models in scientific claim verification.


\end{abstract}

\begin{IEEEkeywords}
Scientific Claims verification, Retrieval-Augmented Generation, Evidence generation 
\end{IEEEkeywords}

\section{Introduction}
% \textcolor{blue}{Fact-checking plays a vital role in information validation, serving as a crucial defence against the widespread dissemination of misinformation. The rapid growth of scientific literature has made it challenging for researchers and the public to stay informed about recent findings. While these publications are key sources of knowledge, the continuous increase in their volume driven by the accelerated pace of research and the rise of platforms like social media has made it difficult to keep up with the latest developments. This challenge is particularly acute during public health crises like the COVID-19 pandemic, where misinformation about scientific information spreads rapidly and requires prompt fact-checking to mitigate its harmful effect. The spread of misinformation in scientific contexts can lead to mass hysteria and critical decisions based on outdated or incorrect data, with severe consequences that undermine public trust in credible research, hinder scientific progress, and pose risks to public health. Consequently, fact-checking claims circulating on digital platforms with evidence from credible sources is a crucial research challenge to counter the early spread of misinformation regarding scientific information and mitigate its impact on consumers.}\par
% \begin{figure}[]
%     \centering
%     \includegraphics[width=3.5in, height=2.7in]{Images/evidence_support.pdf}
%     \caption{present an example of a negative scientific claim involving numerals and cardinal numbers from \textit{SciClaimHunt\_Num}, refuted by evidence extracted from a scientific research paper. Similarly, the above Figure   (Right side) also showcases positive scientific claims from the \textit{SciClaimHunt} dataset, supported by evidence retrieved from the research paper.}  
% \label{fig:main_example}
% \end{figure}

% \usepackage{graphicx}


Fact-checking plays a vital role in information validation, serving as a crucial defense against the widespread dissemination of misinformation and disinformation. Fact-checking is the process of verifying the authenticity of a claim with the help of evidence documents that either support or refute the claim, where a claim is defined as a factual statement subjected to  verification~\cite{vlachos2014fact,thorne2018fever,hanselowski2019richly,thorne2019fever2, setty2020deep}. Initial studies~\cite{derczynski2017semeval,gorrell-etal-2019-semeval,barron2020overview,elsayed2019checkthat,nakov2018overview,hassan2017claimbuster, SADHAN_mishra,mishra-etal-2020-generating} on fact-checking primarily concentrated on curating datasets and developing methodologies for verifying claims related to politics, current affairs, 
newscast and discussions on social media. Scientific claims are grounded in empirical data derived from scientific studies and reports, whereas ordinary claims often express opinions or assertions about potential benefits or courses of action.  However, verifying scientific claims poses unique challenges in dataset curation and model development. Political and general claims can be easily fact-checked by general annotators or journalists, with numerous fact-checking websites available for such claims. In contrast, dedicated fact-checking platforms for scientific claims are scarce, and verifying these claims requires annotators with substantial domain expertise ~\cite{wadden2020fact}. Given these unique challenges in scientific claim verification, the study \cite{wadden2020fact} proposes a scientific claim verification dataset~\textit{SCIFACT} by manually constructing scientific claims derived from research paper abstracts, paired with abstracts of research papers that either support or refute the corresponding claim. Similarly, study~\cite{wadden2022scifact} introduces \textit{SCIFACT-OPEN} dataset for scientific claim verification by compiling pairs of scientific claims and abstracts of multiple research papers that either support or refute the scientific claim

\begin{figure}
    \centering
    \includegraphics[width=2in]{Images/negev.pdf}
    \vspace{\baselineskip} % Add vertical space between figures
    \includegraphics[width=1.3in,height=2.3in]{Images/posev.pdf}
    \caption{The left sub-figure shows an example of a negative scientific claim involving numerals and cardinal numbers from \textit{SciClaimHunt\_Num}, refuted by evidence extracted from a research paper. The right sub-figure showcases a positive scientific claim from the \textit{SciClaimHunt} dataset, supported by evidence retrieved from the research paper.}
    \label{fig:main_example}
\end{figure}



Further, the work~\cite{wright2022generating} proposes two methods for curating scientific claim verification datasets in the biomedical domain, namely \textit{Knowledge Base Informed Negations} and \textit{CLAIMGEN-BART}. The CLAIMGEN-BART method initially extracts the citation, along with the preceding and subsequent sentences, from a given research paper. Next, they encode these extracted sentences using the BART encoder and pass them to the BART decoder to generate a scientific claim supported by the research paper. In contrast, the \textit{Knowledge Base Informed Negations} method replaces the named entity in a scientific claim, which is supported by the research paper to generate a scientific claim that is refuted by the research paper. However, the existing datasets for scientific claim verification exhibit several notable limitations. (i) The existing datasets in the literature are limited in size, typically containing only a few thousand samples, which is insufficient for training generic models for scientific claim detection across diverse categories and domains\cite{vladika2023scientific}. (ii) Existing datasets for scientific claim verification predominantly utilize research paper abstracts as supporting or refuting evidence. However, due to their concise nature, abstracts often lack the comprehensive detail necessary to provide sufficient evidence for robust claim verification. (iii) Existing datasets either rely on manually extracted claims or derive them from research paper references, overlooking claims and observations found in the results, discussion, and conclusion sections. However, these sections often contain the most critical claims and insights related to scientific studies. (iv) Absence of datasets in literature for scientific claims with cardinal or numeral values. \par



Motivated by such limitations with the existing datasets for scientific claim verification. This study proposes two novel datasets for scientific claim verification tasks: \textbf{SciCliamHunt} and~\textbf{SciClaimHunt\_Num}. Our proposed SciClaimHunt dataset leverages few-shot prompting with Large Language Models (LLMs) to generate scientific claims grounded in supporting scientific documents. SciClaimHunt generates refuted claims using two methods: (i) negating a scientific claim supported by scientific evidence and (ii) named entity replacement within a scientific claim supported by scientific evidence. We also curate \textbf{SciClaimHunt\_Num} dataset, a subset of \textit{SciClaimHunt} dataset, dedicated to scientific claims involving numerical or cardinal values, where a model also needs to verify the consistency of numerals and cardinal numbers within the scientific claims and scientific evidence, along with the consistency and contextual similarity between the scientific claim and scientific evidence, to decide whether a scientific claim is supported or refuted by the scientific evidence. Figure~\ref{fig:main_example} presents the examples of \textit{SciCliamHunt} and~\textit{ SciClaimHunt\_Num} datasets. We also propose non-trivial and suitably motivated baseline methods to evaluate the effectiveness of our proposed datasets for scientific claim verification. We also assess the quality and reliability of our proposed datasets through various ablation studies, human annotation evaluations, and error analysis. The quality and reliability evaluation of proposed datasets suggests that our proposed datasets \textit{SciCliamHunt} and \textit{ SciClaimHunt\_Num} are reliable and effective in training models for scientific claim validations.


   
% ,panchendrarajan2024claim,vladika2023scientific,ferreira2016emergent,wang2017liar,
\section{Related work}
In the literature, studies\cite{guo2022survey,vladika2023scientific} briefly review and analyze the datasets and methodologies related to fact-checking. In litrature several datasets have been proposed to support general fact-checking research across various domains, including politics, journalism, and social media~\cite{vlachos2014fact,popat2017truth,thorne2018fever,thorne2018fact,alhindi2018your,chen2019seeing,hanselowski2019richly,augenstein2019multifc,atanasova2024multi,sathe2020automated,saakyan2021covid,schuster2021get,aly2021feverous}.
% including PolitiFact\cite{vlachos2014fact}, Emergent\cite{ferreira2016emergent}, LIAR\cite{wang2017liar}, Snopes\cite{popat2017truth}, FEVER\cite{thorne2018fever,thorne2018fact}, LIAR-PLUS\cite{alhindi2018your}, Perspectrum\cite{chen2019seeing}% , UKP Snopes\cite{hanselowski2019richly}, MultiFC\cite{augenstein2019multifc}, PolitiHop\cite{atanasova2024multi}, WikiFactCheck-English\cite{sathe2020automated}, COVID-Fact\cite{saakyan2021covid}, Vitamin-C\cite{schuster2021get}, and FEVEROUS\cite{aly2021feverous}. 
% Scientific fact-checking is a specialized subset of fact-checking focused on verifying scientific claims, supporting researchers in validating hypotheses, and helping the public interpret and contextualize new scientific findings while addressing misinformation.\cite{vladika2023scientific,lu2023scitab}.
Scientific fact-checking, a subfield of fact-checking, focuses on verifying claims about scientific knowledge, thereby combating misinformation while also supporting scientific inquiry, interpretation and public understanding of research~\cite{vladika2023scientific}. Initial  study~\cite{wadden2020fact} on scientific fact-checking proposed the \textit{SCIFACT} dataset for scientific claim verification, using citance as claims and research abstracts as evidence. In contrast, \textit{SCIFACT-OPEN}\cite{wadden2022scifact} extends the study~\cite{wadden2020fact} by collecting abstracts from multiple papers to support or refute the claims. Further study~\cite{wright2022generating} 
introduced an encoder-decoder model that utilizes sentences surrounding stances and BART to generate claims supported by scientific evidence, with negative claims through named entity replacements to generate the claim refuted for scientific evidence. However, existing scientific claim verification datasets in the litrature have several key limitations. A primary concern is their limited size, as they typically consist of only a few hundred or thousand samples. This is inadequate for training models that can generalize effectively across diverse categories and domains \cite{vladika2023scientific}. Moreover, these datasets primarily depend on research paper abstracts as external evidence. However, abstracts offer only a concise summary and often lack the comprehensive details necessary for rigorous scientific claim verification \cite{vladika2023scientific}. Another limitation is the emphasis on extracting claims solely from sentences containing citations of research papers, which overlooks significant claims and insights presented in the results, discussion, and conclusion sections. Furthermore, there is the absence of a scientific claim verification dataset in the literature, which contains scientific claims and scientific evidence involving cardinal or numeral values to train models for numeral-aware scientific claim verifications. Motivated by such limitations with existing datasets for scientific claim verification in literature, this study proposes two novel datasets for scientific claim verification tasks \textit{SciCliamHunt} and \textit{ SciClaimHunt\_Num}.












% A recent study\cite{lu2023scitab} of scientific claim verification proposed \textit{SCITAB}\cite{lu2023scitab} datasets for compositional reasoning
% and claim verification on scientific tables.
      % Our proposed \textbf{SciCliamHunt} dataset is an extensive collection of pair claims and research papers as evidence which supports and refutes the claims. Similarly, \textbf{ SciClaimHunt\_Num} is a pair of claim and scientific research pairs with cardinal or numeral values supported or refuted by the research paper. Though studies\cite{thorne2017extensible, venktesh2024quantemp} propose StatProps and QuanTemp numeral claims verification, respectively, both the datasets are small in number of samples and only for general fact-checking, not applicable for scientific fact-checking. 

\section{Proposed Datasets}
\label{sec:dataset}
A claim is considered valid if it is fluent, atomic, decontextualized, and precisely conveys the meaning of the source sentence\cite{wright2022generating}.  Given a scientific research paper $\mathcal{R}$ that contains results, discussions, and a conclusion, this study aims to generate a scientific claim $\mathcal{C}$ from sentences in the discussion and conclusion section of the research paper of $\mathcal{R}$, which is supported or refuted by the content of the research paper $\mathcal{R}$. A scientific claim $\mathcal{C}$ is labelled as \textit{Positive} if a scientific research paper supports the claim $\mathcal{C}$, while $\mathcal{C}$ is considered \textit{Negative} if a scientific research paper refutes claim $\mathcal{C}$. This study utilizes the research paper corpus reported in study~ ~\cite{moosavi2021scigen} to construct the \textit{SciClaimHunt} and \textit{SciClaimHunt\_Num} datasets. Ethical considerations related to the use of this corpus are discussed in Section~\ref{sec:ethics}. 
% \\

  
%   \textcolor{magenta}{ }


\subsection{Positive Claim Generation}
\label{subsec:pos}
Initially, we form a subset $\mathcal{F}$ from the research paper corpus $\mathcal{D}$ by randomly selecting twelve papers from $\mathcal{D}$. Next, from each research paper $\mathcal{R} \in \mathcal{F}$, we manually extract a scientific claim  $\mathcal{C}$, which is supported by the content of research paper $\mathcal{R}$. Subsequently, inspired by the \textit{Promptagator} methods outlined in the study\cite{daipromptagator}, we propose {\it Promp\_Claim\_Generator} using a few-shot prompting approach for generating positive claims. Subsequently, considering twelve pairs of manually extracted claims and corresponding paragraphs from which the claim is extracted, our proposed {\it Promp\_Claim\_Generator} method for generating positive claims utilizes instruction prompts $\mathcal{I}$ as follows.  
\begin{equation}
 \mathcal{I} \text{=}   \Big( (\mathcal{P}_i, \mathcal{C}_i), ... , (\mathcal{P}_k, \mathcal{C}_k), (\mathcal{P}_{new} ) \Big)
\end{equation}
Where $\mathcal{P}_i, \mathcal{C}_i ; \forall ; i = 1 ; \text{to} ; k$ are the $k$ pairs of manually extracted claims $\mathcal{C}_i$ and their corresponding source paragraphs $\mathcal{P}_i$, from which the claims is extracted. These $k$  pair of $(\mathcal{P}_I, \mathcal{C}_i)$ are used as $k$ shot examples to LLMs, and the value of $k$ is twelve. Meanwhile, $\mathcal{P}_{new}$ represents the news paragraphs from which a positive claim needs to be generated. Here, $\mathcal{P}_{new} \in \mathcal{R}$ denotes a new source paragraph from a research paper $\mathcal{R} \in \mathcal{D}$ within the research paper corpus $\mathcal{D}$. We feed the instruction prompts $\mathcal{I}$ to Llama\cite{touvron2023llama}~\footnote{\href{https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML}{TheBloke/Llama-2-13B-chat-GGML}} to generate the new claim over paragraph $\mathcal{P}_{new}$ from research paper $\mathcal{R} \in \mathcal{D}$. By running  Llama\cite{touvron2023llama} with instruction prompts $\mathcal{I}$ on paragraphs extracted from result and discussion and conclusion sections of each research paper $\mathcal{R} \in \mathcal{D}$, we generate a large collection of positive claims $\mathcal{C}$ which supported by contents of respective research paper $\mathcal{R}$. 
  
\subsection{Negative Claim Generation}
\label{subsec:neg}
This study considers four different methods to generate and extract negative claims from the results, discussion, and conclusion sections of scientific research papers, refuted by the contents within the same research paper as follows: (i) We adopt few-shot prompting with Llama\cite{touvron2023llama}, where prompt instructions $\mathcal{P}_r$ and eight examples are provided to Llama along with the positive claim extracted in subsection~\ref{subsec:pos} to generate a negate of the positive claim as a negative claim, effectively conveying the opposite meaning of the positive claim. Our prompt instructions $\mathcal{P}_r$ to Llama to generate a negative claim is as follows: \textit{I want you to negate claims extracted from a given scientific paragraph. The negations must be strong negations that are surely false. Examples may be changing results and methodology mentioned in the claims.} (ii) We generate negative claims by mismatching positive claims with different research papers. Specifically, given a true positive claim and research paper pair $(\mathcal{C}_i,\mathcal{R}_i)$, we create a negative pair by mismatching the claim from the $i^{th}$ scientific document with the $j^{th}$ scientific document, resulting in a negative pair of $(\mathcal{C}_i,\mathcal{R}_j)$, where $\mathcal{C}_i$ is refuted by the contents of $\mathcal{R}_j$. (iii) We also generate negative claims by altering the cardinal and numeral values in the positive claims, resulting in claims with incorrect numerical information, which is refuted by the contents of research papers. (iii) We also generate negative claims by altering the cardinal and numeral values in the positive claims, resulting in claims with incorrect numerical information, which is refuted by the contents of research papers. (iv) We also generate negative claims by following the named entities in the positive claim, following the \textit{Knowledge Base Informed Negations} approach as reported in the study\cite{wright2022generating}. 



% \begin{table}[t]
% \centering
% \caption{Characteristics of Experimental Datasets. Here, \#Claim and \#Scientific Paper indicate the average number of words in the Claim and Scientific research paper, respectively. Similarly, \#Sen indicates the average number of sentences in a scientific research paper.}
% \label{tab:proposed_dataset}
% \renewcommand{\arraystretch}{1.2} % Adjust row height
% \setlength{\tabcolsep}{4pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l l r r r r r r|} % Borders only on the edges
% \hline
% \textbf{Dataset} & \textbf{Split} & \textbf{Support} & \textbf{Refutes} & \textbf{Total} & \textbf{\#Claim} & \textbf{\#Paper} & \textbf{\#Sen} \\ 
% \hline\hline
% \multirow{3}{*}{\textbf{SciClaimHunt}} 
% & Train & 37,597 & 49,512 & 87,109 & 21.1 & 4,497.8 & 265.4 \\ 
% & Test  & 4,769  & 6,131  & 10,900 & 21.0 & 4,505.1 & 265.8 \\ 
% & Dev   & 4,678  & 6,206  & 10,884 & 21.0 & 4,495.3 & 264.0 \\ 
% \hline
% \multirow{3}{*}{\textbf{SciClaimHunt\_Num}} 
% & Train & 9,625  & 10,694 & 20,319 & 25.0 & 4,475.1 & 266.5 \\ 
% & Test  & 1,115  & 1,343  & 2,458  & 24.8 & 4,470.4 & 265.1 \\ 
% & Dev   & 1,162  & 1,385  & 2,547  & 24.9 & 4,313.9 & 257.6 \\ 
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Adjusts spacing after the table
% \end{table}
\begin{table}[t]
\centering
\caption{Characteristics of Experimental Datasets. Here, \#Claim and \#Scientific Paper indicate the average number of words in the Claim and Scientific research paper, respectively. Similarly, \#Sen indicates the average number of sentences in a scientific research paper.}
\label{tab:proposed_dataset}
\renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l l r r r r r r} % No vertical lines
\hline
\textbf{Dataset} & \textbf{Split} & \textbf{Support} & \textbf{Refutes} & \textbf{Total} & \textbf{\#Claim} & \textbf{\#Paper} & \textbf{\#Sen} \\  
\hline
\multirow{3}{*}{\textbf{SciClaimHunt}}  
& Train & 37,597 & 49,512 & 87,109 & 21.1 & 4,497.8 & 265.4 \\  
& Test  & 4,769  & 6,131  & 10,900 & 21.0 & 4,505.1 & 265.8 \\  
& Dev   & 4,678  & 6,206  & 10,884 & 21.0 & 4,495.3 & 264.0 \\  
\multirow{3}{*}{\textbf{SciClaimHunt\_Num}}  
& Train & 9,625  & 10,694 & 20,319 & 25.0 & 4,475.1 & 266.5 \\  
& Test  & 1,115  & 1,343  & 2,458  & 24.8 & 4,470.4 & 265.1 \\  
& Dev   & 1,162  & 1,385  & 2,547  & 24.9 & 4,313.9 & 257.6 \\  
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}





\begin{table}[t]
\centering
\caption{Inter-annotator scores for claim evaluation.}
\label{tab:annot_score}
\renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l r r r r} % No vertical lines
\hline
\textbf{Metric} & \textbf{Fluency} & \textbf{Atomicity} & \textbf{De-Contextualization} & \textbf{Faithfulness} \\  
\hline
\textbf{Krippendorff} & 0.704 & 0.703 & 0.784 & 0.693 \\  
\textbf{Fleiss kappa} & 0.703 & 0.823 & 0.761 & 0.699 \\  
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}



% \begin{figure*}[t]
%     \centering
%     \includegraphics[width=4.5in]{Images/eval_criteria.pdf}
%     \caption{Metrics for Evaluating Claim Quality and Their Possible Values.} 
%     \label{fig:annotation_score}
% \end{figure*}
% \begin{table}[t]
% \centering

% \caption{Evaluation Criteria for Claims.}
% \label{tab:eval_criteria}
% \renewcommand{\arraystretch}{1.3} % Adjust row height
% \setlength{\tabcolsep}{5pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \scalebox{1.2}{
% \begin{tabular}{|l|l|l|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Metric}} & \multicolumn{1}{c|}{\textbf{Labels}} & \multicolumn{1}{c|}{\textbf{Description}}                                      \\ \hline\hline
% \multirow{3}{*}{Fluency}              & 3                                    & The claim is well-written and easy to comprehend.                               \\ \cline{2-3} 
%                                       & 2                                    & The claim has minor grammatical mistakes but is still understandable.           \\ \cline{2-3} 
%                                       & 1                                    & The claim is poorly written with numerous grammatical mistakes, making it difficult or impossible to comprehend. \\ \hline
% \multirow{2}{*}{De-Contextualized}    & 1                                    & The claim is self-explanatory and doesn't need additional information to be understood. \\ \cline{2-3} 
%                                       & 0                                    & The claim depends on its original context to make sense.                        \\ \hline
% \multirow{2}{*}{Atomicity}            & 1                                    & The claim focuses on a single, indivisible idea or action.                      \\ \cline{2-3} 
%                                       & 0                                    & The claim can be divided into several smaller, separate claims.                 \\ \hline
% \multirow{5}{*}{Faithfulness}         & 5                                    & The claim is accurate, well-supported, and comprehensive in relation to the original statement. \\ \cline{2-3} 
%                                       & 4                                    & The claim is accurate but incomplete, leaving out some details from the original statement and context. \\ \cline{2-3} 
%                                       & 3                                    & The claim is relevant to the original statement, doesn't have any incorrect facts, but can't be directly inferred. \\ \cline{2-3} 
%                                       & 2                                    & The claim is factually incorrect and contradicts the original statement.        \\ \cline{2-3} 
%                                       & 1                                    & The claim is unrelated to the original statement.                               \\ \hline
% \end{tabular}
% }
% \end{adjustbox}

% \end{table}
\begin{figure}[t]
    \centering
    % \includegraphics[width=\columnwidth]{latex/images/RAG.pdf}
    \includegraphics[width=3.2in]{Images/annotatorevalarial.pdf}
    \caption{Evaluation Criteria For Claims}
\label{fig:Evaluation Criteria For Claims}
\end{figure}



% \begin{table}[t]
% \centering

% \caption{Evaluation Criteria for Claims.}
% \label{tab:eval_criteria}
% \renewcommand{\arraystretch}{2.4} % Increase row height
% \Large
% \setlength{\tabcolsep}{6pt} % Adjust column spacing for readability
% \begin{adjustbox}{max width=\columnwidth}
% \scalebox{1.15}{ % Scale the entire table to increase font size
% \begin{tabular}{|l|l|l|}
% \hline
% \multicolumn{1}{|c|}{\textbf{Metric}} & \multicolumn{1}{c|}{\textbf{Labels}} & \multicolumn{1}{c|}{\textbf{Description}}                                      \\ \hline\hline
% \multirow{3}{*}{Fluency}              & 3                                    & The claim is well-written and easy to comprehend.                               \\ \cline{2-3} 
%                                       & 2                                    & The claim has minor grammatical mistakes but is still understandable.           \\ \cline{2-3} 
%                                       & 1                                    & The claim is poorly written with numerous grammatical mistakes, making it difficult or impossible to comprehend. \\ \hline
% \multirow{2}{*}{De-Contextualized}    & 1                                    & The claim is self-explanatory and doesn't need additional information to be understood. \\ \cline{2-3} 
%                                       & 0                                    & The claim depends on its original context to make sense.                        \\ \hline
% \multirow{2}{*}{Atomicity}            & 1                                    & The claim focuses on a single, indivisible idea or action.                      \\ \cline{2-3} 
%                                       & 0                                    & The claim can be divided into several smaller, separate claims.                 \\ \hline
% \multirow{5}{*}{Faithfulness}         & 5                                    & The claim is accurate, well-supported, and comprehensive in relation to the original statement. \\ \cline{2-3} 
%                                       & 4                                    & The claim is accurate but incomplete, leaving out some details from the original statement and context. \\ \cline{2-3} 
%                                       & 3                                    & The claim is relevant to the original statement, doesn't have any incorrect facts, but can't be directly inferred. \\ \cline{2-3} 
%                                       & 2                                    & The claim is factually incorrect and contradicts the original statement.        \\ \cline{2-3} 
%                                       & 1                                    & The claim is unrelated to the original statement.                               \\ \hline
% \end{tabular}
% }
% \end{adjustbox}

% \end{table}


  
\subsection{Quality Assessment of Claims and Datasets}
\label{appen_human_eval}
We perform a human annotation to assess the quality of positive and negative claims generated in subsections~\ref{subsec:pos} and \ref{subsec:neg} for our proposed dataset \textit{SciClaimHunt} and \textit{SciClaimHunt\_Num}. This study created a subset of the dataset by randomly selecting one hundred claims generated by claim positive and negative claim generation methods discussed in subsection~\ref{subsec:pos} and \ref{subsec:neg} and providing them to four independent annotators.  We evaluate each claim using the claim evaluation metrics \textit{Fluency}, \textit{De-Contextualization}, \textit{Atomicity}, and \textit{Faithfulness}, as outlined in the study \cite{wright2022generating}. Accordingly, based on these evaluation metrics, each annotator was asked to assign scores to the claims. Figure~\ref{fig:Evaluation Criteria For Claims} presents our annotation instructions, questions to annotators, evaluation criteria for assessing claims, and the scoring for different parameters. To ensure high-quality annotations, we selected annotators with a computer science research background, including research fellows and PhD students. Next, we measure inter-annotator agreement over the score assigned by annotators to one hundred claims for evaluation metrics \textit{Fluency}, \textit{De-Contextualization}, \textit{Atomicity}, and \textit{Faithfulness} of claims using Krippendorff $\boldsymbol{\alpha}$\cite{krippendorff2011computing} and  Fleiss kappa\cite{fleiss1971measuring}. Table~\ref{tab:annot_score} presents inter-annotator agreement score  $\boldsymbol{\alpha}$\cite{krippendorff2011computing} and  Fleiss kappa\cite{fleiss1971measuring} for evaluation metrics \textit{Fluency}, \textit{De-Contextualization}, \textit{Atomicity}, and \textit{Faithfulness}. The Krippendorff and Fleiss kappa scores in Table~\ref{tab:annot_score} indicate substantial agreement among annotators across all metrics, with particularly strong agreement observed for \textit{Atomicity} and \textit{De-contextualization}. Such high agreement for \textit{Atomicity} and \textit{De-contextualization} suggests that the generated claims are generally well-structured, self-contained, and contextually accurate, indicating clarity and completeness of claims. Table~\ref{tab:proposed_dataset} presents the characteristics of our proposed \textit{SciClaimHunt} and  \textit{SciClaimHunt\_Num} datasets.

% \subsection{Quality Assessment of Claims and Datasets }
% We perform a human annotation and evaluation of the positive and negative claims in our proposed \textit{SciClaimHunt} dataset to assess their quality, randomly selecting one hundred claims and providing them to four independent annotators. Each claim was evaluated using the claim evaluation metrics \textit{Fluency}, \textit{De-Contextualization}, \textit{Atomicity}, and \textit{Faithfulness}, as outlined in the study\cite{wright2022generating}. Accordingly, each annotator was asked to assign scores to the claims based on these evaluation metrics. Figure~\ref{fig:annotation_score} of Appendix Section~\ref{appen_human_eval} presents the metrics for evaluating claim quality and their possible values, and Appendix Section~\ref{appen_human_eval} also presents further details related to annotation and annotation instruction. Next, we measure inter-annotator agreement over the score assigned to  \textit{Fluency}, \textit{De-Contextualization}, \textit{Atomicity}, and \textit{Faithfulness} of claims using Krippendorff $\boldsymbol{\alpha}$\cite{krippendorff2011computing} and  Fleiss kappa\cite{fleiss1971measuring} and Table~\ref{tab:annot_score} presents inter-annotator agreement score for claim evaluation. 

\begin{figure*}[t]
    \centering
    % \includegraphics[width=\columnwidth]{latex/images/RAG.pdf}
    \includegraphics[width=5in]{Images/ACM_NACCL-24.pdf}
    \caption{presents a working diagram of the proposed Retrieval-Augmented Generation-based approach for scientific claim validation. Given a research paper $\mathcal{R}$ and a claim $\mathcal{C}$, the research paper $\mathcal{R}$ is first split into a set of passage $\mathcal{S}_i$. Next, the similarity $\boldsymbol{\alpha}_i$ between the encoded representations $\mathbf{s}_i$ and $\mathbf{c}$ of passage $\mathcal{S}_i$ and the claim $\mathcal{C}$, respectively, is computed. Evidence $\mathcal{E}$ is then obtained by selecting the top k passage $\mathcal{S}_i$ with the highest similarity scores $\boldsymbol{\alpha}_i$. This evidence $\mathcal{E}$ along with prompt instruction is passed to the \textit{Gemma} model to generate a fact $\mathcal{E}^s$. Given the claim $\mathcal{C}$ and the generated fact $\mathcal{E}^s$, this study adopts two different approaches for scientific claim validations: (i) fine-tuning BERT and RoBERTa, and (ii) instruction tuning with Llama.}
\label{fig:Rag_model}
\end{figure*}

\section{Proposed Baseline Methods}
Given a research paper $\mathcal{R}$ and claim $C$ the task is to classify each research paper-claim pair $(\mathcal{R}, \mathcal{C})$ into $Y \in \{T, F\}$, where $T$ indicates that claim $\mathcal{C}_j$ is supported by the research paper $\mathcal{R}$, and $F$ indicates that claim $\mathcal{C}$ is refuted by the research paper $\mathcal{R}$.

% \\
% Given a research paper $\mathcal{R}$ and a set of $l$ claims $\mathcal{C} = \{\mathbf{C}_1, \mathbf{C}_2, ..., \mathbf{C}_l\}$, where each claim is either supported or refuted by the research paper $\mathcal{R}$, the task is to classify each research paper-claim pair $(\mathcal{R}, \mathcal{C}_j)$ into $Y \in \{T, F\}$, where $T$ indicates that claim $\mathcal{C}_j$ is supported by the research paper $\mathcal{R}$, and $F$ indicates that claim $\mathcal{C}_j$ is refuted by the research paper $\mathcal{R}$.

\subsection{Evidence Retrieval}
\label{subsec:evi_ret_val}
% Each claim $\mathcal{C}_j$ is either supported or refuted by the content of the corresponding research paper $\mathcal{R}$. However, 
Research papers are structured into sections containing numerous sentences and paragraphs, but transformer-based and large language models (LLMs) have limited context lengths. Therefore, extracting the passage from the research paper $\mathcal{R}$ that directly discusses the claim $\mathcal{C}$ is essential. With this motivation, given a pair of research papers and claim $(\mathcal{R}, \mathcal{C})$, we first extract the relevant passage as evidence $\mathcal{E}$ from the research paper $\mathcal{R}$ that are contextually similar to $\mathcal{C}$ and contain direct discussions related to $\mathcal{C}$. We first split the research paper $\mathcal{R}$ into a set of passage $\mathcal{R}^+ = {\mathcal{S}_1, \mathcal{S}_2, ..., \mathcal{S}_n}$ and obtain encoded representations $\mathbf{s}_i$ for the $i^{th}$ sentence in the set $\mathcal{R}^+$ and $\mathbf{c}$ for the claim $\mathcal{C}$ using Sentence-BERT (SBERT)\cite{reimers-2019-sentence-bert}. Next, we estimate the cosine similarity $\boldsymbol{\alpha}_i$ between the encoded representations $\mathbf{s}_i$ and $\mathbf{c}$.  Subsequently, we form a vector $\mathbf{a}$, where the $i^{th}$ element in the vector $\mathbf{a}$ represents the similarity $\boldsymbol{\alpha}_i$ between the passage $\mathbf{s}_i$ and the claim $\mathbf{c}$. We then sort the vector $\mathbf{a}$ in decreasing order and select the top $k$ passage $\mathcal{S}_i$ corresponding to the highest values of $\boldsymbol{\alpha}_i$ from the research paper $\mathcal{R}$ as evidence $\mathcal{E}$ which support or refutes claim $\mathcal{C}_j$.   

\subsection{Retrieval-Augmented Generation (RAG) Approach}
Given the extensive text within the various sections of a research paper and the token limitations of transformer models such as BERT and RoBERTa, utilizing an entire paper for finetuning transformer-based models for scientific claim validation tasks is impractical. Motivated by the above limitation, this study adopts a Retrieval-Augmented Generation (RAG) approach for scientific claim validations. Given a claim and research paper pair $(\mathcal{C},\mathcal{R})$, our Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval} based approach for scientific claim validation first extracts evidence $\mathcal{E}$ from the research paper $\mathcal{R}$, using the evidence retrieval method described in subsection~\ref{subsec:evi_ret_val}. Next, the extracted evidence $\mathcal{E}$ along with prompt instructions, is provided as context to a \textit{Gemma} model~\cite{team2024gemma}\footnote{\href{https://huggingface.co/google/gemma-2-2b-it}{Google/gemma-2-2b-it}}~, which generates a summary $\mathcal{E}^s$ by rephrasing, summarizing, and presenting it as factual statements. The key motivation for applying LLMs to the extracted evidence $\mathcal{E}$ to generate evidence summary $\mathcal{E}^s$ is that the passage in $\mathcal{E}$ is sourced from various sections of the research paper based on their similarity to the claim. This results in a collection of independent passages rather than a coherent or sequential narrative. Therefore, the extracted evidence $\mathcal{E}$ is provided as context to the \textit{Gemma} model, which rephrases and generates a summary as factual statements in a coherent sequence of sentences, $\mathcal{E}^s$, which is facts and summary generated using passages in $\mathcal{E}$. Next, given the pair $(\mathcal{C}, \mathcal{E}^s)$, we fine-tune  BERT and RoBERTa and also propose instruction tuning with a large language model  Llama for claim verification.\newline 
% This study utilizes the Gemma\cite{team2024gemma}\footnote{\href{https://huggingface.co/google/gemma-2-2b-it}{Google/gemma-2-2b-it}} model to generate a summary of the retrieved evidence $\mathcal{E}^s$, given the extracted evidence $\mathcal{E}$.
\textbf{Scientific Claim Verification using BERT and RoBERTa}: First, we obtain the encoded representations $\mathbf{c}$ and $\mathbf{e}^s$ for claim $\mathcal{C}$  and summary of the retrieved evidence $\mathcal{E}^s$ using BERT or RoBERTa models, respectively. Next, estimate the similarity and dissimilarity feature vector between the encoded representations $\mathbf{c}$ and $\mathbf{e}^s$ using the steps outlined in subsection~\ref{subsec:feature}, and pass this feature vector to a two-layer neural network for scientific claim verification.\newline
\textbf{Instruction Tuning with Llama :}
Given a pair consisting of the claim $\mathcal{C}$ and the summary of extracted evidence $\mathcal{E}^s$, along with a prompt instruction $\mathcal{P}_r$, we fine-tune the Llama~\cite{touvron2023llama}\footnote{\href{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}{Meta-llama/Llama-3.2-3B-Instruct}} model for scientific claim verification. Our prompt instruction $\mathcal{P}_r$ is as follows: $\mathcal{P}_r$ = \textit{You are an expert in making judgments. You are given a claim and some sentences. Your job is to verify whether the evidence encapsulated in retrieved sentences supports the claim. The retrieved sentences come from a research paper. If the evidence encapsulated in the retrieved sentences supports the claim, you need to answer as 'positive' otherwise 'negative'}.
\begin{figure}[t]
    \centering
    \includegraphics[width=3in]{Images/CEM_Model_2.pdf}
    \caption{present the \textbf{Claim-Evidence Matching using Multi-Head Attention (CEM)} for scientific claim verification. First, the claim $\mathcal{C}_j$ and sentences $\mathcal{S}_i$ are encoded using \textit{S-BERT} to obtain the encoded representations $\mathbf{c}_j$ for the claim $\mathcal{C}_j$ and $\mathbf{s}_i$ for the $i^{th}$ sentence in the evidence set. Next, we construct an evidence representation matrix $\mathbf{U}^e$ by stacking the encoded representations $\mathbf{s}_i$ of all sentences in the evidence set, where each row of $\mathbf{U}^e$ corresponds to the encoded representation of an individual sentence. Subsequently, we apply multi-head attention between the claim and the evidence set, using the encoded representation of the claim $\mathbf{c}_j$ as the query and the evidence matrix $\mathbf{U}^e$ as both the key and value, to obtain the evidence representation vector $\mathbf{v}$ based on the similarity between the claim and the evidence. Finally, we estimate the similarity and difference feature vector between $\mathbf{C}_j$ and $\mathbf{v}$, which is passed through two fully connected layers for classification.} 
    \label{fig:cem_model}
\end{figure}
\subsection{Claim-Evidence Matching using Multi-Head Attention (CEM)}
\label{subsec:CEM}
 A scientific report or paper is structured into sections, each serving a specific purpose. When evaluating claims, extracting evidence from each relevant section is crucial, ensuring that context-specific information supports or refutes the claims. This study proposes \textbf{CEM} method, which improves upon traditional evidence extraction and retrieval methods discussed in the subsection~\ref{subsec:evi_ret_val} by retrieving evidence from each section rather than merely selecting passages with the highest similarity to the claim, regardless of the section. This section-wise retrieval helps to collect more effective evidence from each section, which helps the claim verification model to make a more informed and accurate decision. Figure~\ref{fig:cem_model} illustrates our proposed Claim-Evidence Matching using Multi-Head Attention (CEM) method for scientific claim verification. The CEM method first divides the research paper $\mathcal{R}$ into a set of sections $\mathcal{U}^+ = {\mathcal{U}_1, \mathcal{U}_2, ..., \mathcal{U}_t}$, where $t$ represents the total number of sections in the paper $\mathcal{R}$. Given a pair consisting of a claim $\mathcal{C}_j$ and a section $\mathcal{U}_i$ from the research paper $\mathcal{R}$, we extract passage highly similar to the claim as evidence $\mathcal{U}_i^e$ from each section $\mathcal{U}_i$ using the evidence retrieval methods outlined in subsection~\ref{subsec:evi_ret_val}. Finally, the extracted evidence from each section is merged to form the evidence $\mathcal{U}^e = \mathcal{U}_1^e \oplus \mathcal{U}_2^e \oplus ... \oplus \mathcal{U}_t^e$.  Additionally, the evidence set $\mathcal{U}^e$ is divided into individual sentences $\mathcal{S}_1, \mathcal{S}_2, ..., \mathcal{S}_k$, and we obtain an encoded representation $\mathbf{s}_i$ for each sentence $\mathcal{S}_i \in \mathcal{U}^e $ from the evidence set $\mathcal{U}^e$ for all $i=1$ to $k$, along with an encoded representation $\mathbf{c}$ for the claim $\mathcal{C}$ using Sentence-BERT (SBERT)\cite{reimers-2019-sentence-bert}. We apply multi-head attention\cite{vaswani2017attention} between the claim and the evidence set, using the encoded representation of the claim $\mathbf{c}$ as the query and the evidence matrix $\mathbf{U}^e$ as both the key and value, to obtain the evidence representation vector $\mathbf{v}$ based on the similarity between the claim and the evidence. The prime motivation behind applying multi-head attention between claim and evidence are as follows: (i) If the evidence supports the claim, the multi-head attention will assign high weights to sentences that are highly similar to the claim, emphasizing their importance in generating the representative vector $\mathbf{v}$. (ii) Similarly, If the evidence extracted from the research paper refutes the claim, the sentences in the evidence set will either be unrelated or contradictory to the claim, leading the multi-head attention to assign low weights, indicating their lack of importance in generating the representative vector $\mathbf{v}$. Subsequently, We compute the similarity and dissimilarity feature vector $\mathbf{f}$ between the claim $\mathbf{c}_j$ and evidence $\mathbf{v}$ (see subsection~\ref{subsec:feature}) and pass it through a two-layer neural network for scientific claim validations. 



























     
% Next, we define an evidence encoding matrix $\mathbf{U}$, where the $i^{th}$ row represents the encoding $\mathbf{s}_i$ of the $i^{th}$ sentence in the evidence set $\mathcal{U}^e$. Subsequently, a representation vector $\mathbf{v}$ is obtained by applying multi-head attention between the encoded representations of the evidence and the claim, using the encoding of the claim $\mathbf{c}_J$ as the query, and the encoded representations of the evidence $\mathbf{U}^e$ as both the key and value.
% \textcolor{red}{Add this content to result and discussion.} Figure~\ref{fig:cem_model} in Appendix Section~\ref{sec:appen:cem} illustrates the working diagram of the CEM model. Appendix Section~\ref{sec:appen:cem} also provides a brief explanation of the intuition behind the CEM model, along with attention heatmaps of samples from the Support and Refutes classes. 

\subsubsection{Graph-based Claim and Evidence Matching~(\textbf{GCEM}) }
% Representing a research paper as a graph enables the capture of relationships and structure between sentences, sections, and the entire paper, leading to a better contextual understanding of the paper\cite{wu2023graph, nikolentzos2020message, zhang2020every}. It also helps capture both local and global patterns within the research, providing insight into how different segments are interconnected\cite{wu2023graph, nikolentzos2020message, zhang2020every}. Moreover, graph representations of the research paper effectively handle non-sequential and complex interactions among sentences, enhancing the analysis of the research paper's structure and the relationships between its content\cite{wu2023graph, nikolentzos2020message, zhang2020every}. 
Representing a research paper as a graph captures relationships and structure between sentences, sections, and the entire paper, enhancing contextual understanding and revealing both local and global patterns\cite{ nikolentzos2020message, zhang2020every}. Additionally, graph representations effectively manage non-sequential and complex interactions, improving analysis of the paper's structure and content\cite{ nikolentzos2020message, zhang2020every}. Motivated by the advantages of representing documents as graphs, we propose \textit{Graph-based Claim and Evidence Matching} (\textbf{GCEM}) for scientific claim verification. The key advantage of GCEM is that it considers the entire research paper $\mathcal{R}$ as evidence to support or refute the claim $\mathcal{C}$ rather than relying on a few selected passages as evidence. Next, we construct a research paper graph $\mathcal{R}_g = \{\mathcal{V}, \mathcal{E}\}$ by considering each sentence $\mathcal{S}_i$ as a node and assigning the encoded representation $\mathbf{s}_i$ as the initial node embedding.  We then estimate the cosine similarity $x_{ij}$ between two nodes $\mathcal{S}_i$ and $\mathcal{S}_j$ in the graph $\mathcal{R}_g$ using their initial node embeddings $\mathbf{s}_i$ and $\mathbf{s}_j$. Next, we add an edge between two nodes $\mathcal{S}_i$ and $\mathcal{S}_j$ in the research paper graph $\mathcal{R}_g$ if $x_{ij} \geq \boldsymbol{\beta}$, where $\beta$ is a user-defined similarity threshold. Subsequently, we form a node embedding matrix $\mathbf{B}$, which represents the node embeddings of the research paper graph $\mathcal{R}_g$, where the $i^{th}$ row corresponds to the initial node embedding $\mathbf{s}_i$ of node $\mathcal{S}_i$ in the graph $\mathcal{R}_g$. Given the node embedding matrix $\mathbf{B}$ of the research paper graph $\mathcal{R}_g$, we apply a $\mathbf{t}$-layer Graph Attention Network~(GAT)~\cite{velickovic2018graph} to the research paper graph $\mathcal{R}_g$ and obtain the transformed node embedding matrix $\mathbf{B}^\mathbf{l}$ for the research paper $\mathcal{R}_g$. The primary motivation for applying GAT over $\mathcal{R}_g$ is to update the node embedding of each node in $\mathcal{R}_g$ based on its local neighborhood structure and the context of its neighboring nodes. Our GAT implementation follows the settings of GAT outlined in study~\cite{velickovic2018graph}. Next, we obtain the research paper graph representation feature vector $\mathbf{r}$ by concatenating the vectors obtained after applying min ({\it Min}), max ({\it Max}), and average ({\it AVG}) pooling operations over the transformed node embedding matrix $\mathbf{B}^{l}$. Subsequently, we estimate the similarity and dissimilarity feature vector $\mathbf{f}$ between the encoded representation $\mathbf{c}$ and research paper graph representation vector $\mathbf{r}$ of the research graph $\mathcal{R}_g$ and pass it to a two-layer neural network for classification using the steps outlined in subsection~\ref{subsec:feature}

% \begin{equation}
% \label{eq:feature2}
% \mathbf{r} \; \text{=} \; \Big(\text{Max}\big( \mathbf{B}^l \big) \boldsymbol{\oplus} \text{Min}\big( \mathbf{B}^l \big) \boldsymbol{\oplus} \text{Avg}\big( \mathbf{B}^l \big)       \Big)   
% \end{equation}   
% Where $\boldsymbol{\oplus}$ denotes the concatenation of vectors.


% Subsequently, we estimate the similarity and dissimilarity feature vector $\mathbf{f}$ between the encoded representation $\mathbf{c}_j$ of the claim $\mathcal{C}_j$ and the research paper graph representation vector $\mathbf{r}$ of the research graph $\mathcal{R}_g$ using the steps outlined in subsection~\ref{subsec:feature}. Next, the feature vector $\mathbf{f}$ is then passed to a two-layer neural network for scientific claim verification.

\subsection{Similarity and Dissimilarity Feature Estimation and Classification}
\label{subsec:feature}
This study estimates the similarity and dissimilarity features between the encoded representations $\mathbf{c}$ and $\mathbf{e}_i^s$ by calculating the angle $\mathbf{f}^{\boldsymbol+}$ and the difference $\mathbf{f}^{\boldsymbol-}$ between $\mathbf{c}$ and $\mathbf{e}_i^s$, as defined by the following equations:
\begin{equation} \label{eq:claim_body_evidencee_diff}
\mathbf{f}^{\boldsymbol+}\; \boldsymbol{,} \;\mathbf{f}^{\boldsymbol-}  \;  \text{= }\;   \mathbf{e}_i^s  \boldsymbol{\odot}  \mathbf{c}_j\; \,  \; \mathbf{e}_i^s  \boldsymbol{-} \mathbf{c}_j
\end{equation}
Here, $\boldsymbol{\odot}$ represents element-wise multiplication between two vectors, and $ \boldsymbol{-}$ represents element-wise difference between two vectors. Subsequently, we form a feature vector $\mathbf{f}$ using Equation~\ref{final_feature_vec} as defined below.

\begin{equation} \label{final_feature_vec}
\mathbf{f} \text{ = } \big(\mathbf{f}^{\boldsymbol+} \boldsymbol{\oplus}  \mathbf{f}^{\boldsymbol-} \boldsymbol{\oplus}\mathbf{e}_i^s \boldsymbol{\oplus} \mathbf{c}_j  \big)
\end{equation}
Where $\boldsymbol{\oplus}$ represents the vector concatenation operator. Once we obtain the feature vector $\mathbf{f}$, we pass it through a two-layer neural network, and we use the cross-entropy loss function to learn the model parameters.




% \textbf{End of Section 3}

% \textcolor{magenta}{This is required but need find better ways and better place for this.}
%  Depending on whether BERT\cite{}\footnote{\href{https://huggingface.co/google-bert/bert-large-uncased}{Google BERT Large}} or RoBERTa\cite{}\footnote{\href{https://huggingface.co/google-bert/bert-large-uncased}{FacebookAI RoBERTa Large}} is used to encode the claim and evidence and whether the model consider raw extracted evidence $\mathcal{E}_i$ from the research paper or the rephrased and summarized version of extracted evidence $\mathcal{E}_i^s$ from the research paper,  this study considered four different setups of BERT and RoBERTa for scientific claim verification. (i) \textit{RoBERTa with Summary of Retrieved Evidences}~$\textbf{RoBERTa (SRE)}$: In this approach, the rephrased and summarized version of the extracted evidence $\mathcal{E}_i^s$ and the claim $\mathcal{C}$ are considered.
% (ii) \textit{RoBERTa with Retrieved Evidences}~$\textbf{RoBERTa (RE)}$: In this approach, the raw extracted evidence $\mathcal{E}_i^s$ from the research paper and the claim $\mathcal{C}$ are considered. (iii) \textit{BERT with Summary of Retrieved Evidences}~$\textbf{BERT (SRE)}$: In this approach, the rephrased and summarized version of the extracted evidence $\mathcal{E}_i^s$ and the claim $\mathcal{C}$ are considered.
% (iv) \textit{BERT with Retrieved Evidences}~$\textbf{BERT (RE)}$: In this approach, the raw extracted evidence $\mathcal{E}_i$ from the research paper and the claim $\mathcal{C}$ are considered.\\

\section{Experimental Results and Discussions}
% \begin{table}[]
% \centering
% \caption{ Details hyperparameter settings used to generate the results.}
% \label{tab:hyper_values}
% \begin{tabular}{|l|l|}
% \hline
% \textbf{Hyperparameters} & \textbf{Values}     \\ \hline
% Epoch                    & 100                 \\ \hline
% Threshold value $\boldsymbol{\beta}$           & 0.25, 0.5, 0.75    \\ \hline
% No. of Attention Heads    & 8            \\ \hline
% Batch Size                & 2,64                 \\ \hline
% Embedding dimension       & 384               \\ \hline
% Learning rate             & 0.01               \\ \hline
% Loss Function             & Cross Entropy      \\
% \hline
% No. of layer in GAT             & 3      \\
% \hline

% % Memory dimension          & 100                \\ \hline
% % Filter Size              & ngram \times (2 \times \text{memory dimension}) \\ \hline
% \end{tabular}
% \end{table}
\begin{table}[t]
\centering
\caption{Details of hyperparameter settings used to generate the results.}
\label{tab:hyper_values}
\renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{6pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l l} % No vertical lines
\hline
\textbf{Hyperparameters}        & \textbf{Values}            \\  
\hline
Epoch                           & 100                        \\  
Threshold value $\boldsymbol{\beta}$ & 0.25, 0.5, 0.75         \\  
No. of Attention Heads          & 8                          \\  
Batch Size                      & 2, 64                      \\  
Embedding Dimension             & 384                        \\  
Learning Rate                   & 0.01                       \\  
Loss Function                   & Cross Entropy              \\  
No. of Layers in GAT            & 3                          \\  
 No. of sentences in a research paper $\mathbf{R}$            & 265                        \\ 
No. of Sections in a research paper $\mathbf{R}$            & 5                \\ 
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}




% \begin{table}[t]
% \centering
% % \tiny
% \caption{Characteristics of \textbf{SCIFACT} and \textbf{SCIFACT-OPEN} datasets. Here, \#Claim and \#Abstract indicate the average number of words in the Claim and  abstract of scientific research paper, respectively. Similarly, \#Sen indicated the average number of sentences in a scientific research paper.}
% \label{tab:test_dataset}
% \begin{adjustbox}{max width = \columnwidth}
% \begin{tabular}{|l|l|l|l|l|l|l|}
% \hline
%                       & \textbf{Support} & \textbf{Refutes} & \textbf{Total} & \textbf{\#Claims} & \textbf{\#Abstract} & \textbf{\#Sent} \\ \hline
% \textbf{SCIFACT-OPEN} & 122              & 112              & 234            & 11.4              & 49.1                & 3.82            \\ \hline
% \textbf{SCIFACT}      & 830              & 463              & 1293           & 12.19             & 30.12               & 2.11            \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}
\begin{table}[t]
\centering
\caption{Characteristics of \textbf{SCIFACT} and \textbf{SCIFACT-OPEN} datasets. 
Here, \#Claim and \#Abstract indicate the average number of words in the Claim and the abstract of a scientific research paper, respectively. Similarly, \#Sent indicates the average number of sentences in a scientific research paper.}
\label{tab:test_dataset}
\renewcommand{\arraystretch}{1.2} % Adjust row height
\setlength{\tabcolsep}{5pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l r r r r r r} % No vertical lines
\hline
\textbf{Dataset}       & \textbf{Support} & \textbf{Refutes} & \textbf{Total} & \textbf{\#Claims} & \textbf{\#Abstract} & \textbf{\#Sent} \\  
\hline
\textbf{SCIFACT-OPEN}  & 122              & 112              & 234            & 11.4              & 49.1                & 3.82            \\  
\textbf{SCIFACT}       & 830              & 463              & 1,293          & 12.19             & 30.12               & 2.11            \\  
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}

\subsection{Experimental Setups}
This study considers Accuracy (Acc.), F-measure (F.), and class-wise F-measure as performance metrics to study the performance of proposed and baseline models in scientific claim verification. Table~\ref{tab:hyper_values}  presents the details of the experimental hyper-parameters used to generate the results presented in this paper. This study also considers the scientific claim verification datasets \textit{SCIFACT}\cite{wadden2020fact} and  \textit{SCIFACT-OPEN}\cite{wadden2022scifact} from literature to evaluate the performance of the proposed baseline models. Table~\ref{tab:test_dataset} presents the characteristics of \textit{SCIFACT} and  \textit{SCIFACT-OPEN}.  

% \textcolor{blue}{ Although the \textit{SCIFACT} and \textit{SCIFACT-OPEN} datasets contain three classes of samples: \textit{SUPPORTS}, \textit{NOINFO}, and \textit{REFUTES}, we focus only on samples from the \textit{SUPPORTS} and \textit{REFUTES} classes to align with the models trained on the proposed dataset and exclude samples from the \textit{NOINFO} class.}

  

% \begin{table}[]
% \centering
% \tiny
% \caption{presents the performance of the proposed baseline models over the proposed dataset. Here, \textbf{(Acc.)}, \textbf{(S.)} and \textbf{(R.)} indicate the \textbf{accuracy} and F-measure of the \textbf{Support} and \textbf{Refutes} class, respectively. Similarly, \textbf{ER} refers to the \textbf{Evidence Retrieval} approach, \textbf{RAG} stands for the \textbf{Retrieval Augmented Generation} approach, and \textbf{FRP} indicates the approach where models consider the \textbf{Full Research Paper} as evidence. }
% \label{tab:main}
% \begin{adjustbox}{width=\columnwidth}
% \begin{tabular}{|l|l|rrr|}
% \hline
% \textbf{Approach}             & \textbf{Model}       & \multicolumn{1}{l}{\textbf{Acc.}} & \multicolumn{1}{l}{\textbf{S.}} & \multicolumn{1}{l|}{\textbf{R.}} \\ \hline
% \multirow{3}{*}{\textbf{ER}}  & $\mathbf{BERT}$  & \textcolor{magenta}{\textbf{0.985}}                             & \textcolor{magenta}{\textbf{0.982}}                           & \textbf{0.986 }                           \\ \cline{2-5} 
%                               & $\mathbf{RoBERTa}$ & \multicolumn{1}{l}{0.934}              & \multicolumn{1}{l}{0.935}            & \multicolumn{1}{l|}{0.931}            \\ \cline{2-5} 
%                               & $\mathbf{CEM}$   & 0.904                             & 0.889                           & 0.915                            \\ \hline
% \multirow{3}{*}{\textbf{RAG}} & $\mathbf{BERT}$ & 0.982                             & 0.979                           & 0.984                            \\ \cline{2-5} 
%                               & $\mathbf{RoBERTa}$ & \multicolumn{1}{l}{0.929}              & \multicolumn{1}{l}{0.934}            & \multicolumn{1}{l|}{0.923}            \\ \cline{2-5} 
%                               & $\mathbf{Llama}$  & \textbf{0.983}                             & \textbf{0.98}                            & \textcolor{magenta}{\textbf{0.99} }                            \\ \hline
% \multirow{4}{*}{\textbf{FRP}} & $\mathbf{CEM}$      & \textbf{0.935}                    & \textbf{0.923}                  & \textbf{0.944}                   \\ \cline{2-5} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.25)}$ & 0.84                              & 0.86                            & 0.82                             \\ \cline{2-5} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.5)}$  & 0.892                             & 0.892                           & 0.891                            \\ \cline{2-5} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.75)}$ & \textbf{0.921}                             & \textbf{0.923}                  & \textbf{0.922 }                           \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}
% \begin{table}[t]
% \centering
% \caption{Performance of the proposed baseline models on the dataset. 
% Here, \textbf{(Acc.)}, \textbf{(S.)}, and \textbf{(R.)} indicate the \textbf{accuracy} and F-measure for the \textbf{Support} and \textbf{Refutes} classes, respectively. 
% \textbf{ER} refers to the \textbf{Evidence Retrieval} approach, \textbf{RAG} stands for the \textbf{Retrieval Augmented Generation} approach, and \textbf{FRP} indicates the approach where models consider the \textbf{Full Research Paper} as evidence.}
% \label{tab:main}
% \renewcommand{\arraystretch}{1.3} % Adjust row height
% \setlength{\tabcolsep}{4pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l|l|c|c|c|}
% \hline
% \textbf{Approach}             & \textbf{Model}                      & \textbf{Acc.} & \textbf{S.} & \textbf{R.} \\ 
% \hline\hline
% \multirow{3}{*}{\textbf{ER}}  & $\mathbf{BERT}$                     & \textcolor{magenta}{\textbf{0.985}} & \textcolor{magenta}{\textbf{0.982}} & \textbf{0.986} \\ 
%                               & $\mathbf{RoBERTa}$                  & 0.934 & 0.935 & 0.931 \\ 
%                               & $\mathbf{CEM}$                      & 0.904 & 0.889 & 0.915 \\ 
% \hline
% \multirow{3}{*}{\textbf{RAG}} & $\mathbf{BERT}$                     & 0.982 & 0.979 & 0.984 \\ 
%                               & $\mathbf{RoBERTa}$                  & 0.929 & 0.934 & 0.923 \\ 
%                               & $\mathbf{Llama}$                    & \textbf{0.983} & \textbf{0.980} & \textcolor{magenta}{\textbf{0.990}} \\ 
% \hline
% \multirow{4}{*}{\textbf{FRP}} & $\mathbf{CEM}$                      & \textbf{0.935} & \textbf{0.923} & \textbf{0.944} \\ 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.25)}$ & 0.840 & 0.860 & 0.820 \\ 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.5)}$  & 0.892 & 0.892 & 0.891 \\ 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.75)}$ & \textbf{0.921} & \textbf{0.923} & \textbf{0.922} \\ 
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Adjusts spacing after the table
% \end{table}
% \begin{table}[t]
% \centering
% \caption{Performance of the proposed baseline models on the dataset. 
% Here, \textbf{(Acc.)}, \textbf{(S.)}, and \textbf{(R.)} indicate the \textbf{accuracy} and F-measure for the \textbf{Support} and \textbf{Refutes} classes, respectively. 
% \textbf{ER} refers to the \textbf{Evidence Retrieval} approach, \textbf{RAG} stands for the \textbf{Retrieval Augmented Generation} approach, and \textbf{FRP} indicates the approach where models consider the \textbf{Full Research Paper} as evidence.}
% \label{tab:main}
% \renewcommand{\arraystretch}{1.3} % Adjust row height
% \setlength{\tabcolsep}{4pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l|l|c|c|c|}
% \hline
% Approach             & Model                      & \textbf{Acc.} & \textbf{S.} & \textbf{R.} \\ 
% \hline\hline
% \multirow{3}{*}{ER}  & BERT                     & \textcolor{magenta}{\textbf{0.985}} & \textcolor{magenta}{\textbf{0.982}} & \textbf{0.986} \\ 
%                               & RoBERTa                  & 0.934 & 0.935 & 0.931 \\ 
%                               & CEM                      & 0.904 & 0.889 & 0.915 \\ 
% \hline
% \multirow{3}{*}{RAG} & BERT                     & 0.982 & 0.979 & 0.984 \\ 
%                               & RoBERTa                  & 0.929 & 0.934 & 0.923 \\ 
%                               & Llama                    & \textbf{0.983} & \textbf{0.980} & \textcolor{magenta}{\textbf{0.990}} \\ 
% \hline
% \multirow{4}{*}{FRP} & CEM                      & \textbf{0.935} & \textbf{0.923} & \textbf{0.944} \\ 
%                               & GCEM($\boldsymbol\beta \; \text{=}\; 0.25$) & 0.840 & 0.860 & 0.820 \\ 
%                               & GCEM($\boldsymbol\beta \; \text{=}\; 0.5$)  & 0.892 & 0.892 & 0.891 \\ 
%                               & GCEM($\boldsymbol\beta \; \text{=}\; 0.75$) & \textbf{0.921} & \textbf{0.923} & \textbf{0.922} \\ 
% \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Adjusts spacing after the table
% \end{table}
\begin{table}[t]
\centering
\caption{Performance of the proposed baseline models on the dataset. 
Here, \textbf{(Acc.)}, \textbf{(S.)}, and \textbf{(R.)} indicate the \textbf{accuracy} and F-measure for the \textbf{Support} and \textbf{Refutes} classes, respectively. 
\textbf{ER} refers to the \textbf{Evidence Retrieval} approach, \textbf{RAG} stands for the \textbf{Retrieval Augmented Generation} approach, and \textbf{FRP} indicates the approach where models consider the \textbf{Full Research Paper} as evidence.}
\label{tab:main}
\renewcommand{\arraystretch}{1.3} % Adjust row height
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l l c c c} % No vertical lines
\hline
\textbf{Approach}    & \textbf{Model}            & \textbf{Acc.} & \textbf{S.} & \textbf{R.} \\  
\hline
\multirow{3}{*}{ER}  & BERT                     & \textcolor{magenta}{\textbf{0.985}} & \textcolor{magenta}{\textbf{0.982}} & \textbf{0.986} \\  
                              & RoBERTa                  & 0.934 & 0.935 & 0.931 \\  
                              & CEM                      & 0.904 & 0.889 & 0.915 \\  
\hline
\multirow{3}{*}{RAG} & BERT                     & 0.982 & 0.979 & 0.984 \\  
                              & RoBERTa                  & 0.929 & 0.934 & 0.923 \\  
                              & Llama                    & \textbf{0.983} & \textbf{0.980} & \textcolor{magenta}{\textbf{0.990}} \\  
\hline
\multirow{4}{*}{FRP} & CEM                      & \textbf{0.935} & \textbf{0.923} & \textbf{0.944} \\  
                              & GCEM($\boldsymbol\beta \; \text{=}\; 0.25$) & 0.840 & 0.860 & 0.820 \\  
                              & GCEM($\boldsymbol\beta \; \text{=}\; 0.5$)  & 0.892 & 0.892 & 0.891 \\  
                              & GCEM($\boldsymbol\beta \; \text{=}\; 0.75$) & \text{0.921} & \textbf{0.923} & \text{0.922} \\  
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}




\subsection{Results and Discussions}
Table~\ref{tab:main} presents the performance of the proposed baseline model on our dataset. In the Table, proposed baseline models are grouped into three Approaches: ER, RAG, and FRP. (i) Evidence Retrieval (\textbf{ER}): In this approach, the model only considers the passages extracted from the research paper using a dense retrieval method (discussed in subsection~\ref{subsec:evi_ret_val}) as evidence. (i) Evidence Retrieval (\textbf{ER}): In this approach, the model only considers the passages extracted from the research paper using a dense retrieval method (discussed in subsection~\ref{subsec:evi_ret_val}) as evidence. (ii) Retrieval Augmented Generation (\textbf{RAG}): The model retrieves passage relevant to the claims from the research paper (discussed in subsection~\ref{subsec:evi_ret_val}) and passes them through a \textit{Gemma} as context to generate a summary and facts, generated summary and fact is then considered evidence. (iii) Full Research Paper (\textbf{FRP}): In this approach, the models consider the entire research paper, excluding the conclusion section, as evidence. From Table~\ref{tab:main}, it is apparent that the performance of the \textbf{ER} method is superior with the BERT model compared to the performance of \textbf{ER} methods with CEM and Roberta and the performance of \textbf{RAG}-based methods is superior with  Llama model. Similarly, the performance of \textbf{RAG}-based methods is superior with the Llama model compared to that of \textbf{RAG} with Roberta and BERT.  For the Full Research Paper (\textbf{FRP}) approaches, GCEM outperforms the CEM model. Additionally, GCEM with $\beta = 0.75$ shows better results than $\beta = 0.25$ and $\beta = 0.5$, indicating that $\beta = 0.75$ effectively captures relationships between sentence nodes in the research paper. For the Full Research Paper (\textbf{FRP}) approaches, GCEM outperforms the CEM model. Furthermore, GCEM with $\beta = 0.75$ outperforms configurations with  $\beta = 0.25$ and $\beta = 0.5$. This suggests that $\beta = 0.75$ effectively captures relationships between sentence nodes because a higher $\beta$ value results in a sparser graph, connecting only sentences with strong contextual similarity. The performance comparison of \textbf{ER}, \textbf{RAG}, and \textbf{FRP} reveals the following observations:  ER and RAG, which leverage evidence retrieval and generation, respectively, achieve superior performance compared to FRP. From such observation, we conclude that methods focused on identifying and extracting relevant evidence from a large scientific document or entire research paper, whether through retrieval or generation, are more effective for scientific claim verification than approaches that consider the whole research paper or scientific document as evidence as evidence.

% From Table~\ref{tab:main}, it is evident that the performance of the \textbf{ER} and \textbf{RAG}-based methods is significantly high and comparable. However, the performance of the evidence retrieval (\textbf{ER}) model BERT and the Llama model from Retrieval Augmented Generation (\textbf{RAG}) is superior compared to other models in the \textbf{ER}, \textbf{RAG}, and \textbf{FRP} approaches. Next, we analyze the performance of Full Research Paper (\textbf{FRP}) based methods, CEM and GCEM. From Table~\ref{tab:main}, it is evident that CEM outperforms GCEM. Similarly, GCEM with $\beta = 0.75$ shows superior performance compared to GCEM with $\beta = 0.25$ and $\beta = 0.5$. This indicates that $\beta = 0.75$ effectively helps form edges between sentence nodes,  capturing both the relationships between sentences in the research paper. 

% \begin{table}[]
% \tiny
% \centering
% \caption{presents the performance of the proposed baseline models, which were trained on the proposed dataset and tested on the \textbf{SCIFACT}\cite{wadden2020fact} and \textbf{SCIFACT-OPEN}\cite{wadden2022scifact} datasets.
% % Here, \textbf{(Acc.)}, \textbf{(S.)} and \textbf{(R.)} indicate the \textbf{accuracy} and F-measure of the \textbf{Support} and \textbf{Refutes} class, respectively. Similarly, \textbf{ER} refers to the \textbf{Evidence Retrieval} approach, \textbf{RAG} stands for the \textbf{Retrieval Augmented Generation} approach, and \textbf{FRP} indicates the approach where models consider the \textbf{Full Research Paper} as evidence.
% }
% \label{tab:val_data_res}
% \begin{adjustbox}{width=\columnwidth}
% \begin{tabular}{|ll|lll|lll|}
% \hline
%                                                            &                      & \multicolumn{3}{l|}{\textbf{SCIFACT}} & \multicolumn{3}{l|}{\textbf{SCIFACT-OPEN}} \\ \hline
% \multicolumn{1}{|l|}{\textbf{Approach}}                             & \textbf{Model}                & \textbf{Acc.}     & \textbf{T }      & \textbf{F.}      & \textbf{Acc.}      & \textbf{T}         &\textbf{ F.}        \\ \hline
% \multicolumn{1}{|l|}{\multirow{3}{*}{\textbf{ER}}}   & \textbf{BERT  }           & \textbf{0.728}    & \textbf{0.802}   & \textbf{0.568}   & 0.695     & 0.714     & 0.634     \\
% \multicolumn{1}{|l|}{}                                     &\textbf{ RoBERTa  }       & \textbf{0.728}    & \textbf{0.802}   & \textbf{0.568}   & \textbf{0.717}     & \textbf{0.76}      & \textbf{0.656}     \\
% \multicolumn{1}{|l|}{}                                     & \textbf{CEM}              & 0.635    & 0.769   & 0.132   & 0.521     & 0.67      & 0.125     \\ \hline
% \multicolumn{1}{|l|}{\multirow{3}{*}{\textbf{RAG}}}                 & \textbf{BERT}            & 0.696    & 0.795   & 0.413   & 0.615     & 0.701     & 0.457     \\
% \multicolumn{1}{|l|}{}                                     & \textbf{RoBERTa}        & 0.713    & 0.765   & 0.656   & 0.707     & 0.743     & 0.653     \\
% \multicolumn{1}{|l|}{}                                     & \textbf{Llama }         & \textbf{0.792}    & \textbf{0.837}   & \textbf{0.711}   & \textbf{0.748}     & \textbf{0.754}     & \textbf{0.741}     \\ \hline
% \multicolumn{1}{|l|}{\multirow{4}{*}{FRP}} & \textbf{CEM}              & 0.652    & 0.78    & 0.163   & 0.517     & 0.658     & 0.175     \\
% \multicolumn{1}{|l|}{}                                     & $\textbf{GCEM} (\boldsymbol{\beta} \text{=}0.25) $& 0.754   & 0.754   & 0.746   & 0.743     & 0.748     & 0.739          \\
% \multicolumn{1}{|l|}{}                                     & $\textbf{GCEM} (\boldsymbol{\beta} \text{=}0.5) $   & 0.782   & 0.789   & 0.745   & 0.769     & 0.823     & 0.749           \\
% \multicolumn{1}{|l|}{}                                     &$\textbf{GCEM} (\boldsymbol{\beta} \text{=}0.75) $ & \textcolor{magenta}{\textbf{0.823}}   & \textcolor{magenta}{\textbf{0.847}}   & \textcolor{magenta}{\textbf{0.841}}   & \textcolor{magenta}{\textbf{0.787}}     & \textcolor{magenta}{\textbf{0.798}}     & \textcolor{magenta}{\textbf{0.778}}          \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}


% \begin{table}[t]
% \centering
% \caption{Performance of the proposed baseline models, trained on the proposed dataset and tested on \textbf{SCIFACT} \cite{wadden2020fact} and \textbf{SCIFACT-OPEN} \cite{wadden2022scifact}.}
% \label{tab:val_data_res}
% \renewcommand{\arraystretch}{1.3} % Adjust row height
% \setlength{\tabcolsep}{4pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l|l|c|c|c|c|c|c|}
% \hline
% \multirow{2}{*}{\textbf{Approach}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c|}{\textbf{SCIFACT}} & \multicolumn{3}{c|}{\textbf{SCIFACT-OPEN}} \\ \cline{3-8} 
%                                    &                                 & \textbf{Acc.} & \textbf{T}    & \textbf{F.}   & \textbf{Acc.}  & \textbf{T}     & \textbf{F.}    \\ \hline\hline
% \multirow{3}{*}{\textbf{ER}}       & \textbf{BERT}                  & \textbf{0.728} & \textbf{0.802} & \textbf{0.568} & 0.695          & 0.714          & 0.634          \\ 
%                                    & \textbf{RoBERTa}               & \textbf{0.728} & \textbf{0.802} & \textbf{0.568} & \textbf{0.717} & \textbf{0.760} & \textbf{0.656} \\ 
%                                    & \textbf{CEM}                   & 0.635          & 0.769          & 0.132          & 0.521          & 0.670          & 0.125          \\ \hline
% \multirow{3}{*}{\textbf{RAG}}      & \textbf{BERT}                  & 0.696          & 0.795          & 0.413          & 0.615          & 0.701          & 0.457          \\ 
%                                    & \textbf{RoBERTa}               & 0.713          & 0.765          & 0.656          & 0.707          & 0.743          & 0.653          \\ 
%                                    & \textbf{Llama}                 & \textbf{0.792} & \textbf{0.837} & \textbf{0.711} & \textbf{0.748} & \textbf{0.754} & \textbf{0.741} \\ \hline
% \multirow{4}{*}{\textbf{FRP}}      & \textbf{CEM}                   & 0.652          & 0.780          & 0.163          & 0.517          & 0.658          & 0.175          \\ 
%                                    & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.25)$ & 0.754 & 0.754 & 0.746 & 0.743          & 0.748          & 0.739          \\ 
%                                    & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.5)$  & 0.782 & 0.789 & 0.745 & 0.769          & 0.823          & 0.749          \\ 
%                                    & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.75)$ & \textcolor{magenta}{\textbf{0.823}} & \textcolor{magenta}{\textbf{0.847}} & \textcolor{magenta}{\textbf{0.841}} & \textcolor{magenta}{\textbf{0.787}} & \textcolor{magenta}{\textbf{0.798}} & \textcolor{magenta}{\textbf{0.778}} \\ \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Adjusts spacing after the table
% \end{table}
\begin{table}[t]
\centering
\caption{Performance of the proposed baseline models, trained on the proposed dataset and tested on \textbf{SCIFACT} \cite{wadden2020fact} and \textbf{SCIFACT-OPEN} \cite{wadden2022scifact}.}
\label{tab:val_data_res}
\renewcommand{\arraystretch}{1.3} % Adjust row height
\setlength{\tabcolsep}{4pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l l c c c c c c} % No vertical lines
\hline
\multirow{2}{*}{\textbf{Approach}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{SCIFACT}} & \multicolumn{3}{c}{\textbf{SCIFACT-OPEN}} \\ 
\cline{3-8} 
                                   &                                 & \textbf{Acc.} & \textbf{T}    & \textbf{F.}   & \textbf{Acc.}  & \textbf{T}     & \textbf{F.}    \\  
\hline
\multirow{3}{*}{ER}       & BERT                  & \textbf{0.728} & \textbf{0.802} & \textbf{0.568} & 0.695          & 0.714          & 0.634          \\  
                                   & RoBERTa               & \textbf{0.728} & \textbf{0.802} & \textbf{0.568} & \textbf{0.717} & \textbf{0.760} & \textbf{0.656} \\  
                                   & CEM                   & 0.635          & 0.769          & 0.132          & 0.521          & 0.670          & 0.125          \\  
\hline
\multirow{3}{*}{RAG}      & BERT                  & 0.696          & 0.795          & 0.413          & 0.615          & 0.701          & 0.457          \\  
                                   & RoBERTa               & 0.713          & 0.765          & 0.656          & 0.707          & 0.743          & 0.653          \\  
                                   & Llama                 & \textbf{0.792} & \textbf{0.837} & \textbf{0.711} & \textbf{0.748} & \textbf{0.754} & \textbf{0.741} \\  
\hline
\multirow{4}{*}{FRP}      & CEM                   & 0.652          & 0.780          & 0.163          & 0.517          & 0.658          & 0.175          \\  
                                   & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.25)$ & 0.754 & 0.754 & 0.746 & 0.743          & 0.748          & 0.739          \\  
                                   & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.5)$  & 0.782 & 0.789 & 0.745 & 0.769          & 0.823          & 0.749          \\  
                                   & $\mathbf{GCEM} (\boldsymbol{\beta} \text{=}0.75)$ & \textcolor{magenta}{\textbf{0.823}} & \textcolor{magenta}{\textbf{0.847}} & \textcolor{magenta}{\textbf{0.841}} & \textcolor{magenta}{\textbf{0.787}} & \textcolor{magenta}{\textbf{0.798}} & \textcolor{magenta}{\textbf{0.778}} \\  
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Adjusts spacing after the table
\end{table}

\begin{table}[t]
\centering
\caption{Statistical comparison between the \textbf{SCIFACT} and \textbf{SCIFACT-OPEN} datasets in terms of the number of sentences in the evidence. \textbf{STD} refers to the Standard Deviation.}
\label{tab:sent_dstributions}
\begin{adjustbox}{width=\columnwidth}
\begin{tabular}{l l ll} % No vertical lines
\hline
\textbf{Corpus}  & \textbf{SCIFACT} & \textbf{SCIFACT-OPEN}     & \textbf{Proposed Dataset} \\ 
\hline
\textbf{MEAN}    & 1.41             & 1.76                     & 105.98                    \\ 
\hline
\textbf{STD}     & 0.65             & 0.56                     & 33.58                     \\ 
\hline
\textbf{Minimum} & 1                & 1                        & 24                        \\ 
\hline
\textbf{Maximum} & 5                & 7                        & 300                       \\ 
\hline
\textbf{25\%}    & 1                & 1                        & 83.3                      \\ 
\hline
\textbf{50\%}    & 1                & 2                        & 103                       \\ 
\hline
\textbf{75\%}    & 2                & 3                        & 140                       \\ 
\hline
\end{tabular}
\end{adjustbox}
\end{table}


\subsubsection{Validation of Proposed Datasets}
To evaluate both the real-world applicability and generalizability of our proposed datasets, we assessed the performance of our baseline model trained using proposed datasets over existing scientific claim verification datasets from the literature. With this motivation, we trained the proposed baseline models using \textit{SciClaimHunt} and evaluated their performance by using \textit{SCIFACT}\cite{wadden2020fact} and \textit{SCIFACT-OPEN}\cite{wadden2022scifact} as test datasets. Table~\ref{tab:val_data_res} presents the performance of the proposed baseline models trained on the \textit{SciClaimHunt} datasets and evaluated using \textit{SCIFACT}\cite{wadden2020fact} and \textit{SCIFACT-OPEN}\cite{wadden2022scifact} as test datasets. From Table~\ref{tab:val_data_res}, it is evident that the  \textbf{RAG} approach with Llama and $GCEM (\beta = 0.75)$ demonstrates outstanding performance when trained on the \textit{SciClaimHunt} dataset and evaluated using \textit{SCIFACT} and \textit{SCIFACT-OPEN} as test datasets.  From such observation from Table~\ref{tab:val_data_res}, we can conclude that the models trained on our proposed dataset (\textit{SciClaimHunt}) are generalizable on unseen scientific claim verification datasets and effective for scientific claim verification in real-world scenarios. Consequently, we conclude that our proposed dataset (\textit{SciClaimHunt}) is highly high-quality and reliable for training models for scientific claim verification tasks.
  
\begin{figure}[]
  \centering
    \includegraphics[width=0.5\textwidth, height=0.45\textheight]{Images/neg2.pdf}
    \caption{presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Refutes} class (negative claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns moderate attention weights to sentences that are just related and neural to the claim and lower weights to sentences with minimal relevance to the claim. The darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa. }  
\label{fig:neg_heatmap_2}
\end{figure}



 
 

 
 
  However, from Table~\ref{tab:val_data_res}, it is apparent that the performance of the CEM model declines when trained on the proposed dataset and tested on \textit{SCIFACT} and \textit{SCIFACT-OPEN}. We further study the reason behind the declines in the performance CEM and found that the number of sentences in the evidence sets of \textit{SCIFACT} and \textit{SCIFACT-OPEN} is significantly lower than the number of sentences in the evidence set of the proposed dataset. Since CEM employs multi-head attention between claim and sentences of the evidence set, the weight matrix in the multi-head attention component of CEM is dependent on the number of sentences in the evidence set, which varies between the proposed dataset and \textit{SCIFACT} and \textit{SCIFACT-OPEN}. The difference in terms of the number of sentences in the proposed dataset \textit{SciClaimHunt} and \textit{SCIFACT} and \textit{SCIFACT-OPEN} could be a possible reason behind the decline in the performance of CEM models over  \textit{SCIFACT} and \textit{SCIFACT-OPEN}. The number of sentences in proposed dataset \textit{SciClaimHunt} and \textit{SCIFACT} and \textit{SCIFACT-OPEN} is different because \textit{SciClaimHunt} consider entire research paper as evidence which supports and refutes the claim whereas \textit{SCIFACT} and \textit{SCIFACT-OPEN} considers only abstract of the research paper as evidence. Table~\ref{tab:val_data_res} shows the sentence distribution differences between the proposed dataset \textit{SciClaimHunt} and \textit{SCIFACT} and \textit{SCIFACT-OPEN}.  



% \begin{table}[]
% \tiny
% \centering
% \caption{presents the performance of the proposed baseline models  over \textit{SciClaimHunt\_Num} datasets}
% \label{tab:num_res}
% \begin{adjustbox}{width=\columnwidth}
% \scalebox{0.36}{
% \begin{tabular}{|l|l|r|}
% \hline
% \textbf{Approach}             & \textbf{Model}       & \multicolumn{1}{l|}{\textbf{Acc.}} \\ \hline
% \multirow{3}{*}{\textbf{ER}}  & $\mathbf{BERT}$  & \textcolor{magenta}{\textbf{0.985}}                            \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$ & 0.927                                       \\ \cline{2-3} 
%                               & $\mathbf{CEM}$   & 0.89                                       \\ \hline
% \multirow{3}{*}{\textbf{RAG}} & $\mathbf{BERT}$ & 0.982                                       \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$ & 0.919                                       \\ \cline{2-3} 
%                               & $\mathbf{Llama}$  & \textbf{0.98}                              \\ \hline
% \multirow{4}{*}{\textbf{FRP}} & $\mathbf{CEM}$      & \textbf{0.948}                              \\ \cline{2-3} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.25)}$ & 0.819                                        \\ \cline{2-3} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.5)}$  & 0.886                                       \\ \cline{2-3} 
%                               & $\mathbf{GCEM(\boldsymbol\beta \; \text{=}\; 0.75)}$ & \textbf{0.927}                              \\ \hline
% \end{tabular}
% }
% \end{adjustbox}
% \end{table}
% \begin{table}[t]
% \centering
% \caption{Performance of the proposed baseline models on the \textit{SciClaimHunt\_Num} dataset.}
% \label{tab:num_res}
% \renewcommand{\arraystretch}{1.3} % Adjust row height
% \setlength{\tabcolsep}{6pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l|l|c|}
% \hline
% \textbf{Approach}             & \textbf{Model}                            & \textbf{Acc.}         \\ \hline\hline
% \multirow{3}{*}{\textbf{ER}}  & $\mathbf{BERT}$                           & \textcolor{magenta}{\textbf{0.985}} \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$                        & 0.927                  \\ \cline{2-3} 
%                               & $\mathbf{CEM}$                            & 0.890                  \\ \hline
% \multirow{3}{*}{\textbf{RAG}} & $\mathbf{BERT}$                           & 0.982                  \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$                        & 0.919                  \\ \cline{2-3} 
%                               & $\mathbf{Llama}$                          & \textbf{0.980}         \\ \hline
% \multirow{4}{*}{\textbf{FRP}} & $\mathbf{CEM}$                            & \textbf{0.948}         \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.25)$  & 0.819                  \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.5)$   & 0.886                  \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.75)$  & \textbf{0.927}         \\ \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Adjust spacing after the table
% \end{table}
% \begin{table}[t]
% \centering
% \caption{Performance of the proposed baseline models on the \textit{SciClaimHunt\_Num} dataset.}
% \label{tab:num_res}
% \renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
% \setlength{\tabcolsep}{5pt} % Adjust column spacing
% \begin{adjustbox}{max width=\columnwidth}
% \begin{tabular}{|l|l|c|}
% \hline
% \textbf{Approach}             & \textbf{Model}                            & \textbf{Accuracy}      \\ \hline\hline
% \multirow{3}{*}{\textbf{ER}}  & $\mathbf{BERT}$                           & \textcolor{magenta}{\textbf{0.985}} \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$                        & 0.927                  \\ \cline{2-3} 
%                               & $\mathbf{CEM}$                            & 0.890                  \\ \hline
% \multirow{3}{*}{\textbf{RAG}} & $\mathbf{BERT}$                           & 0.982                  \\ \cline{2-3} 
%                               & $\mathbf{RoBERTa}$                        & 0.919                  \\ \cline{2-3} 
%                               & $\mathbf{Llama}$                          & \textbf{0.980}         \\ \hline
% \multirow{4}{*}{\textbf{FRP}} & $\mathbf{CEM}$                            & \textbf{0.948}         \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.25)$  & 0.819                  \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.5)$   & 0.886                  \\ \cline{2-3} 
%                               & $\mathbf{GCEM}(\boldsymbol\beta = 0.75)$  & \textbf{0.927}         \\ \hline
% \end{tabular}
% \end{adjustbox}
% \vspace{-2mm} % Optional: Adjust spacing below the table
% \end{table}
\begin{table}[t]
\centering
\caption{Performance of the proposed baseline models on the \textit{SciClaimHunt\_Num} dataset.}
\label{tab:num_res}
\renewcommand{\arraystretch}{1.2} % Adjust row height for better readability
\setlength{\tabcolsep}{5pt} % Adjust column spacing
\begin{adjustbox}{max width=\columnwidth}
\begin{tabular}{l l c} % No vertical lines
\hline
Approach             & Model                            & \textbf{Accuracy}      \\ 
\hline
\multirow{3}{*}{ER}  & BERT                           & \textcolor{magenta}{\textbf{0.985}} \\ 
                     & RoBERTa                        & 0.927                  \\ 
                     & CEM                            & 0.890                  \\ 
\hline
\multirow{3}{*}{RAG} & BERT                           & 0.982                  \\ 
                     & RoBERTa                        & 0.919                  \\ 
                     & Llama                          & \textbf{0.980}         \\ 
\hline
\multirow{4}{*}{FRP} & CEM                            & \textbf{0.948}         \\ 
                     & GCEM($\beta = 0.25$)           & 0.819                  \\ 
                     & GCEM($\beta = 0.5$)            & 0.886                  \\ 
                     & GCEM($\beta = 0.75$)           & \textbf{0.927}         \\ 
\hline
\end{tabular}
\end{adjustbox}
\vspace{-2mm} % Optional: Adjust spacing below the table
\end{table}


\begin{figure}[]
  \centering
    \includegraphics[width=0.5\textwidth, height=0.41\textheight]{Images/pos1.pdf}
    \caption{ presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Supporting} class (positive claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. The darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa.}  
\label{fig:pos_heatmap_1}
\end{figure}
\begin{figure}[]
  \centering
    \includegraphics[width=0.5\textwidth, height=0.36\textheight]{Images/pos2.pdf}
    \caption{ presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Supporting} class (positive claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. The darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa.}  
\label{fig:pos_heatmap_2}
\end{figure}
\begin{figure}[]
  \centering
    \includegraphics[width=0.5\textwidth, height=0.48\textheight]{Images/neg1.pdf}
    \caption{ presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Refutes} class (negative claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. The darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa.}  
\label{fig:neg_heatmap_1}
\end{figure}


\subsubsection{Error Analysis}
We perform an error analysis on the misclassifications made by baseline models over proposed datasets to evaluate the strengths and weaknesses of the proposed baseline models. We first evaluate the response of baseline models over \textit{SciClaimHunt\_Num} datasets to study the ability of baseline models in scientific claim verification with claims involving cardinal and numeral values. Table~\ref{tab:num_res} presents the performance of the proposed baseline models on the \textit{SciClaimHunt\_Num} datasets, demonstrating the high proficiency of baseline models in verifying scientific claims containing cardinal and numeral values. We manually inspect misclassifications made by the baseline models on the proposed datasets, \textit{SciClaimHunt} and \textit{SciClaimHunt\_Num}, and derive the following key observations: (i) Claims that are only partially supported by the evidence (i.e., where some sentences support the claim while others refute it) pose a challenge, often leading to misclassification by models from support to refutes or refutes to support. (ii) When the evidence discusses the claim without taking a stance for or against it, models incorrectly classify the claim-evidence pair as supported even though it belongs to the refute class. Figure~\ref{fig:pos_heatmap_1},~\ref{fig:pos_heatmap_2},~\ref{fig:neg_heatmap_1} and~\ref{fig:neg_heatmap_2} present heatmaps illustrating the attention weights between the scientific claim and sentences of evidence, as captured by the multi-head attention component of the CEM model. From Figure~\ref{fig:pos_heatmap_1},~\ref{fig:pos_heatmap_2},~\ref{fig:neg_heatmap_1} and~\ref{fig:neg_heatmap_2}, it is evident that the multi-head attention components of the CEM model assign high attention weights to sentences from the evidence set that are highly similar to and support the claim while assigning low attention weights to sentences that are least similar to and refute the claim. From these observations, we can conclude that our proposed baseline model effectively learns the relationship between claims and evidence for scientific claim classification.













% We first study the effectiveness of our proposed baseline models in verifying the claim containing cardinal and numeral values over \textit{SciClaimHunt} and  \textit{SciClaimHunt\_Num}. Table~\ref{tab:num_res} presents the performance of proposed baseline models over \textit{SciClaimHunt\_Num} datasets. From Table~\ref{tab:num_res}, it is evident that our proposed baseline models are highly proficient in detecting claims with cardinal and numeral values. The following observations were made through manual inspection of the samples misclassified by our proposed baseline models on the\textit{SciClaimHunt} and \textit{SciClaimHunt\_Num}. (i) When a claim is partially supported by evidence, i.e., a few sentences in evidence support the claim, and a few sentences do not support the claim. (ii) If evidence, discuss the claim but do not take any stance in favor or against the claim. The models classify the claim evidence pair as a supported class, though it belongs to a refuted class. To further validate these intuitions, this study extracts attention heatmaps displaying the attention weights assigned to each sentence in the evidence by the multi-head attention component of CEM for samples from both the supporting and refuting classes in the test set of the proposed dataset. Figure~\ref{fig:pos_heatmap_1},~\ref{fig:pos_heatmap_2},~\ref{fig:neg_heatmap_1} and~\ref{fig:neg_heatmap_2} present heatmaps illustrating the attention between the scientific claim and evidence, as captured by the multi-head attention component of the CEM model. 
 





 
%  From Figure~\ref{fig:pos_heatmap_1},~\ref{fig:pos_heatmap_2},~\ref{fig:neg_heatmap_1} and~\ref{fig:neg_heatmap_2}


% \\







%  \\
% Figure~\ref{fig:pos_heatmap_1} and ~\ref{fig:pos_heatmap_2}.
%  Additionally, the same patterns can be observed by analyzing the attention heatmaps generated based on attention weight between claim and sentences in the evidence from  {Refutes}  class in Figure~\ref{fig:neg_heatmap_1} and ~\ref{fig:neg_heatmap_2}.  

%  \\
 





% We begin by evaluating the effectiveness of our proposed baseline models in verifying claims containing cardinal and numeral values. This study curates a subset of the dataset, called \textit{SciClaimHunt\_Num}, by collecting claims that contain cardinal and numeral values. 

\section{Conclusion and Future Work}


This paper presents two novel datasets for scientific claim verification: \textit{SciClaimHunt} and \textit{SciClaimHunt\_Num}, along with several baseline models to assess their effectiveness. We further evaluate the quality, generalizability, and reliability of these datasets through ablation studies, human assessments, and error analyses. Our findings demonstrate that \textit{SciClaimHunt} and \textit{SciClaimHunt\_Num} serve as robust resources for training models in scientific claim verification. Potential future directions for this work include: (i) incorporating scientific claims from low-resource languages, (ii) expanding the dataset to encompass diverse scientific domains such as medicine and disease research, and (iii) integrating images and tabular data from scientific papers as evidence.


% This study identifies scientific fact checking in low-resource languages and literature as a promising area, with future work aimed at expanding scientific fact-checking to other domains of science.



% \section{Limitations}
% We identify the following key limitations in this study. Our proposed dataset is limited to scientific claim verification where both the claim and evidence are in textual form, making it unsuitable for cases where the claim is text-based but the evidence is presented in a table or figure. Additionally, the source information for claim verification is restricted to a single scientific research paper, which could be expanded to include multiple research papers as evidence.

\section{Ethics Statement}
\label{sec:ethics}
The SciClaimHunt dataset was created without collecting any personal information. It leverages a publicly available dataset curated by study~\cite{moosavi2021scigen}. The SciClaimHunt dataset is released under the Creative Commons Attribution 4.0 International license (consistent with the original license). The accompanying code is released under the MIT license.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,main_paper}
% \appendix
% \section{Appendix}




% \subsection{Claim-Evidence Matching using
% Multi-Head Attention (CEM)}
% \label{sec:appen:cem}
% % The primary intuitions behind our proposed CEM methods are outlined in subsection~\ref{subsec:CEM}. Figure~\ref{fig:cem_model} illustrates our proposed Claim-Evidence Matching using Multi-Head Attention (CEM) method for scientific claim verification. As discussed in subsection~\ref{subsec:CEM} the prime motivation behind applying multi-head attention between claim and evidence are as follows: (i) If the evidence supports the claim, then the claim be highly similar to sentences in the evidence set and accordingly, the multi-head attention will assign high weights to sentences that are highly similar to the claim, emphasizing their importance in generating the representative vector $\mathbf{v}$. (ii) If the evidence refutes the claim, the sentences will either be unrelated or contradictory to the claim, leading the multi-head attention to assign low weights, indicating their lack of importance in generating the representative vector $\mathbf{v}$.


%  Figure~\ref{fig:pos_heatmap} presents the attention heatmaps of attention weights assigned to sentences of evidence-based similarity with the claim by multi-head attention component of CEM for \textit{Supporting} class samples. The attention heatmaps in Figure~\ref{fig:pos_heatmap} reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. This suggests that there are few sentences in the evidence set to support the claim; therefore, it is classified into the support class. Similarly, Figure~\ref{fig:pos_heatmap} presents the attention heatmaps of attention weights assigned to sentences of evidence-based similarity with the claim by multi-head attention component of CEM for \textit{Refuting} class samples. The attention heatmaps in Figure~\ref{fig:neg_heatmap} reveal that the multi-head attention component of the CEM model assigns either moderate or minimal attention weights to the sentences in the evidence. This suggests that none of the sentences in the evidence set support the claim; therefore, it is classified into the Refutes class. Similar patterns can be observed by analyzing the attention heatmaps generated from the interactions between the claim and the sentences in the evidence. Similar patterns can be observed by analyzing the attention heatmaps generated from the interactions between the claim and the sentences in the evidence from  {Support}  class in 
 
% % \begin{figure*}[]
% %   \centering
% %     \includegraphics[width=0.4\textwidth, height=0.17\textheight]{Images/Pos21-1.pdf}
% %     \caption{presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Supporting} class (positive claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. The Darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa.}  
% % \label{fig:pos_heatmap_1}
% % \end{figure*}









% % \begin{figure*}[]
% %   \centering
% %     \includegraphics[width=0.4\textwidth, height=0.17\textheight]{Images/Pos21-3.pdf}
% %     \caption{presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Refutes} class (negative claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns moderate attention weights to sentences that are just related and neural to the claim and lower weights to sentences with minimal relevance to the claim. The Darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa.}  
% % \label{fig:neg_heatmap_1}
% % \end{figure*}



% % \begin{figure*}[]
% %   \centering
% %     \includegraphics[width=0.4\textwidth, height=0.17\textheight]{Images/Pos21-2.pdf}
% %     \caption{presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Refutes} class (negative claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns moderate attention weights to sentences that are just related and neural to the claim and lower weights to sentences with minimal relevance to the claim. The Darker colour signifies the higher attention weight assigned to the respective sentence from the evidence set and vice-versa. }  
% % \label{fig:neg_heatmap_2}
% % \end{figure*}





% % \begin{figure*}[]
% %   \centering
% %     \includegraphics[width=1
% %     \textwidth, height=0.15\textheight]{Images/heatmapmegative.pdf}
% %     \caption{ presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Refute} class (negative claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns moderate attention weights to sentences in the evidence set and, therefore, is classified into the refutes class.}  
% % \label{fig:neg_heatmap}
% % \end{figure*}
% % \begin{figure*}[]
% %   \centering
% %     \includegraphics[width=0.8\textwidth, height=0.15\textheight]{Images/heatmappositive.pdf}
% %     \caption{ presents attention heatmaps showing the attention weights between the claim and various sentences of the evidence for the \textit{Supporting} class (positive claims) samples. The heatmaps reveal that the multi-head attention component of the CEM model assigns higher weights to sentences that support the claim, moderate weights to neutral sentences, and lower weights to sentences with minimal relevance to the claim. }  
% % \label{fig:pos_heatmap}
% % \end{figure*}









% % \subsection{Validation of Proposed Datasets}
% % \label{App_val_datatset}
% % To evaluate the quality and reliability of the proposed dataset and the generalizability of models trained on the proposed dataset, we analyzed the performance of the proposed baseline model trained with the proposed dataset over scientific claim verification datasets from the literature. With this motivation, we trained the proposed baseline models using the proposed dataset and evaluated their performance by using \textbf{SCIFACT}\cite{wadden2020fact} and \textit{SCIFACT-OPEN}\cite{wadden2022scifact} as test datasets. Table~\ref{tab:val_data_res} presents the performance of the proposed baseline models, which were trained on the proposed dataset and evaluated using \textbf{SCIFACT}\cite{wadden2020fact} and \textit{SCIFACT-OPEN}\cite{wadden2022scifact} as test datasets. From Table~\ref{tab:val_data_res}, it is evident that the proposed \textbf{RAG} approach with Llama and $GCEM (\beta = 0.75)$ demonstrates outstanding performance when trained on the proposed dataset and evaluated using \textbf{SCIFACT} and \textit{SCIFACT-OPEN} as test datasets. From Table~\ref{tab:val_data_res}, it is evident that the proposed \textbf{RAG} approach with Llama and $GCEM (\beta = 0.75)$ demonstrates outstanding performance when trained on the proposed dataset and evaluated using \textbf{SCIFACT} and \textit{SCIFACT-OPEN} as test datasets. From such observation from Table~\ref{tab:val_data_res} we can conclude that the models trained on our proposed dataset are generalizable and effective for scientific claim detection on unseen samples. Therefore, our proposed dataset is of high quality and reliable for training models for scientific claim verification tasks. However, from Table~\ref{tab:val_data_res}, it is apparent that the performance of the CEM model declines when trained on the proposed dataset and tested on \textbf{SCIFACT} and \textit{SCIFACT-OPEN}. Upon further investigation, we found that the number of sentences in the evidence sets of \textbf{SCIFACT} and \textit{SCIFACT-OPEN} is significantly lower than the number of sentences in the evidence set of the proposed dataset. Since CEM employs multi-head attention between claim and sentences of the evidence set, the weight matrix in the multi-head attention component is dependent on the number of sentences in the evidence set, which varies between the proposed dataset and \textbf{SCIFACT} and \textit{SCIFACT-OPEN}. The difference in terms of the number of sentences in the proposed dataset and \textbf{SCIFACT} and \textit{SCIFACT-OPEN} could be a possible reason behind the decline in CEM's performance on \textbf{SCIFACT} and \textit{SCIFACT-OPEN}. Table~\ref{tab:val_data_res} shows the sentence distribution differences between the proposed dataset and \textbf{SCIFACT} and \textit{SCIFACT-OPEN}. 













% % \subsection{Experimental Setup and  Hyperparameter }
% % \label{sec:hyper_details}
% % This study considers Accuracy (Acc.), F-measure (F.), and class-wise F-measure as performance metrics to evaluate the effectiveness of proposed and baseline models in scientific claim verification. Table~\ref{tab:hyper_values}  presents the details of the experimental hyper-parameters used to generate the results presented in this paper. This study also considers the scientific claim verification datasets \textit{SCIFACT}\cite{wadden2020fact} and  \textit{SCIFACT-OPEN}\cite{wadden2022scifact} from literature to evaluate the performance of the proposed baseline models. Table~\ref{tab:test_dataset}  presents the characteristics of \textit{SCIFACT} and  \textit{SCIFACT-OPEN}. Although the \textit{SCIFACT} and \textit{SCIFACT-OPEN} datasets contain three classes of samples: \textit{SUPPORTS}, \textit{NOINFO}, and \textit{REFUTES}, we focus only on samples from the \textit{SUPPORTS} and \textit{REFUTES} classes to align with the models trained on the proposed dataset, and exclude samples from the \textit{NOINFO} class.



% \subsection{Error Analysis}
% \label{App:Erro_analysis}
% To evaluate the strengths and weaknesses of the proposed baseline models, we conducted an error analysis to examine the types of claims that were correctly classified and misclassified by the proposed baseline models. We first study the effectiveness of our proposed baseline models in verifying the claim containing cardinal and numeral values over \textit{SciClaimHunt\_Num}. Table~\ref{tab:num_res} presents the performance of proposed baseline models over \textit{SciClaimHunt\_Num} datasets. From Table~\ref{tab:num_res}, it is evident that our proposed baseline models are highly proficient in detecting claims with cardinal and numeral values. The following observations were made through manual inspection of the samples misclassified by our proposed baseline models on the proposed datasets.  (i) When a claim is partially supported by evidence, i.e., a few sentences in evidence support the claim, and a few sentences do not support the claim. This means that the claim is partially supported and partially refuted by evidence. (ii) If evidence, just discuss the claim but do not take any stance in favour or against the claim. The models classify the sample from the Refutes class to the Support class. 


% % \appendix




% % \section{Ease of Use}

% % \subsection{Maintaining the Integrity of the Specifications}

% % The IEEEtran class file is used to format your paper and style the text. All margins, 
% % column widths, line spaces, and text fonts are prescribed; please do not 
% % alter them. You may note peculiarities. For example, the head margin
% % measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is\cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% %\section*{References}

% Please number citations consecutively within brackets\cite{b1}. The 
% sentence punctuation follows the bracket\cite{b2}. Refer simply to the reference 
% number, as in\cite{b3}---do not use ``Ref.\cite{b3}'' or ``reference\cite{b3}'' except at 
% the beginning of a sentence: ``Reference\cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished''\cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press''\cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation\cite{b6}.
% %\balance
% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
