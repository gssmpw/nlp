\documentclass{article}


\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{fancyhdr}       % header
\usepackage{graphicx}       % graphics

%add new
\usepackage{multirow} 
\usepackage{algorithm}
\usepackage{algorithmic} 
\usepackage{amsmath}
\graphicspath{{media/}}     % organize your images and other figures under media/ folder
%Header
\pagestyle{fancy}
\thispagestyle{empty}
\rhead{ \textit{ }} 

% Update your Headers here
\fancyhead[LO]{Atten-Transformer}
% \fancyhead[RE]{Firstauthor and Secondauthor} % Firstauthor et al. if more than 2 - must use \documentclass[twoside]{article}



  
%% Title
\title{Atten-Transformer: A Deep Learning Framework for User App Usage Prediction}
\iffalse
\author{
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
 \And
  Coauthor \\
   Affiliation \\
Address \\
 \texttt{email} \\
}
\fi

\author{
Longlong Li \\
  School of Mathematics, Data Science Institute \\
  Shandong University,\\
  Jinan 250100\\
  \texttt{longlee@mail.sdu.edu.cn} \\
  %% examples of more authors
   \And
    Cunquan Qu \\
  Data Science Institute \\
  Shandong University,\\
  Jinan 250100\\
  \texttt{cqqu@sdu.edu.cn} \\
  \And
  Guanghui Wang \\
  School of Mathematics \\
  Shandong University,\\
  Jinan 250100\\
  \texttt{ghwang@sdu.edu.cn} \\
}



\begin{document}
\maketitle

\begin{abstract}
Accurately predicting smartphone app usage patterns is crucial for user experience optimization and targeted marketing. However, existing methods struggle to capture intricate dependencies in user behavior, particularly in sparse or complex usage scenarios. To address these challenges, we introduce Atten-Transformer, a novel model that integrates temporal attention with a Transformer network to dynamically identify and leverage key app usage patterns. Unlike conventional methods that primarily consider app order and duration, our approach employs a multi-dimensional feature representation, incorporating both feature encoding and temporal encoding to enhance predictive accuracy. The proposed attention mechanism effectively assigns importance to critical app usage moments, improving both model interpretability and generalization. Extensive experiments on multiple smartphone usage datasets, including LSapp and Tsinghua App Usage datasets, demonstrate that Atten-Transformer consistently outperforms state-of-the-art models across different data splits. Specifically, our model achieves a 45.24\% improvement in HR@1 over AppFormer on the Tsinghua dataset (Time-based Split) and an 18.25\% improvement in HR@1 over MAPLE on the LSapp dataset (Cold Start Split), showcasing its robustness across diverse app usage scenarios. These findings highlight the potential of integrating adaptive attention mechanisms in mobile usage forecasting, paving the way for enhanced user engagement and resource allocation.
\end{abstract}

% keywords can be removed
\keywords{Smartphone App Usage \and Attention Mechanism \and Transformer}

\section{Introduction}

The pervasive adoption of mobile applications has transformed various aspects of daily life, enabling seamless communication, entertainment, and productivity~\cite{zhu2013mobile, mihailidis2014tethered, zhang2020app, xie2021trimming, seid2024use}. Analyzing mobile app interaction data provides critical insights into user behavior, facilitating personalized recommendations and improving user experiences~\cite{liu2017cm, zhao2019user, de2018you}. Additionally, understanding app usage patterns contributes to smartphone resource optimization, enhancing CPU performance, memory utilization, and battery efficiency~\cite{chen2017powerful, oliner2013carat}. Moreover, network operators can leverage app traffic patterns to optimize traffic offloading and network management~\cite{xu2016understanding, zeng2018temporal}.

Despite these benefits, mobile usage data exhibits unique challenges: it is inherently high-dimensional, sparse, and periodic, making traditional predictive methods ineffective in capturing its underlying structure~\cite{pejovic2015anticipatory, han2016mobile}. Deep learning techniques, such as LSTMs and attention-based models, have been proposed to address these challenges, achieving notable improvements in predictive accuracy~\cite{xu2020predicting,lee2019app,xia2020deepapp,kang2022app,zhang2024optimizing}. However, these methods often fail to effectively model critical temporal patterns in user engagement, which are essential for understanding app usage behavior. To overcome this limitation, we propose Atten-Transformer, a model that integrates a temporal attention mechanism to dynamically emphasize the most informative usage moments, thereby improving both predictive accuracy and interpretability.

While prior studies such as AppUsage2Vec~\cite{zhao2019appusage2vec} and DUGN~\cite{ouyang2022learning} have incorporated contextual and temporal factors, they lack an adaptive weighting mechanism to dynamically prioritize different app usage moments, which limits their ability to capture nuanced behavioral trends. Additionally, mobile app usage is influenced by multiple contextual factors, including user preferences, temporal trends, and network conditions, which existing models struggle to capture effectively~\cite{xu2020predicting,lee2019app,li2022smartphone}. Addressing these challenges requires a model that can selectively highlight the most informative moments within user interactions while preserving long-term behavioral dependencies.

The framework of Atten-Transformer is illustrated in Figure~\ref{fig:Atten-Transformer}. Specifically, our model:
\begin{itemize}
    \item \textbf{Enhances feature representation} by explicitly modeling temporal dependencies and contextual interactions, extending beyond traditional sequential modeling.
    \item \textbf{Employs a temporal attention mechanism} to dynamically assess the importance of different app usage moments, capturing both short-term variations and recurring behavioral patterns.
    \item \textbf{Achieves state-of-the-art performance}, as validated through extensive experiments on real-world datasets.
\end{itemize}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{Atten_model.jpg}
    \caption{The architecture of the Atten-Transformer model. The model consists of three main components: 
        (1) \textbf{Feature Encoding}, which extracts app-related representations from historical app sequences; 
        (2) \textbf{Temporal Encoding}, which encodes temporal positional information to capture periodic usage patterns; 
        and (3) \textbf{Temporal Attention}, which dynamically weights different time steps to highlight key moments in user behavior.
        The encoded representations are passed through a \textbf{Transformer Block}, followed by \textbf{Average Pooling}, and then refined by the 
        \textbf{Temporal Attention Mechanism}, which integrates both user embeddings and time-aware features. Finally, the processed features are fed into a 
        \textbf{Classifier} to predict the next app usage event.
    }
    \label{fig:Atten-Transformer}
\end{figure*}

Our findings highlight the necessity of incorporating richer contextual information into app usage prediction models. By leveraging both feature encoding and temporal attention mechanisms, Atten-Transformer provides a more accurate and interpretable framework for modeling user behavior, ultimately enabling better personalized recommendations and smartphone resource optimization.

\section{Related Work}

App usage prediction is critical for enhancing user experience and optimizing system performance by preloading app-related content, reducing startup latency, and improving energy efficiency~\cite{tian2020cohort}. Early methods relied on simple heuristics, such as Most Frequently Used (MFU) and Most Recently Used (MRU)~\cite{shin2012understanding}, but these approaches fail to capture sequential dependencies and evolving user behavior.

To address this, probabilistic models such as Markov chains~\cite{natarajan2013app}, Bayesian networks~\cite{zou2013prophet}, and Tree-Augmented Naive Bayes (TAN)~\cite{baeza2015predicting} were introduced, enabling higher-order transitions and improved temporal modeling. However, these methods remain limited in handling long-range dependencies.

Deep learning has significantly advanced app usage prediction, with LSTM-based and GRU-based models demonstrating strong performance by capturing sequential dependencies~\cite{yang2023atpp, hidasi2015session, alruban2022prediction}. To further enhance predictive accuracy, studies have incorporated contextual features, including time and location~\cite{jiang2019using, wang2021app2vec, khaokaew2021cosem}, and explored multi-task learning to jointly model app usage and related behaviors~\cite{xia2020deepapp, zeng2024ddhcn}.

Graph-based models have also been employed to better represent app-location-time relationships~\cite{yu2020semantic, zhou2020graph, ouyang2022learning, wu2019session, shen2023ginapp,huang2025predicting}. For example, DUGN~\cite{ouyang2022learning} explicitly models app transitions using a graph structure, while GINApp~\cite{shen2023ginapp} utilizes Graph Isomorphism Networks for session-based recommendations.  Despite their effectiveness, these methods often require large-scale data to generalize well.

Transformer-based architectures have recently emerged as state-of-the-art solutions. Appformer~\cite{sun2025appformer} employs self-attention mechanisms to capture global dependencies, while MAPLE~\cite{khaokaew2024maple} leverages LLM embeddings to address cold-start issues. However, these models often rely on large-scale multi-modal data, making them less effective in data-scarce environments. Additionally, they lack explicit temporal feature modeling, which is crucial for app usage prediction.

To address these limitations, we propose Atten-Transformer, which integrates feature encoding and temporal encoding to improve representation learning. By employing adaptive attention mechanisms, our model dynamically assigns importance to key app usage moments, capturing both long-term dependencies and short-term contextual shifts. Our approach achieves superior performance in app usage prediction, particularly in sparse data environments.

\section{Problem Description}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{AppUsage.jpg}
    \caption{Illustration of app usage prediction over time. The historical app usage sequence $\mathcal{H}$ consists of past app interactions at different time steps. Given this sequence, the model predicts the most probable next app at time $t+1$, ranking the top-K predictions (e.g., App X, App Y, App Z).}
    \label{fig:App}
\end{figure}

\subsection{Problem Definition}  
Let \( \mathcal{A} = \{a_1, a_2, \dots, a_C\} \) be the set of \( C \) unique applications, and let \( \mathcal{U} = \{u_1, u_2, \dots, u_N\} \) be the set of users. For a given user \( u_i \in \mathcal{U} \), their app usage history within a specified time window \( [t-\Delta t, t] \) is represented as:

\begin{equation}
\mathcal{H} = \{A_t^1, A_t^2, \dots, A_t^k\}, 
\end{equation}
where \( A_t^j \) represents the \( j \)-th application used at that moment. This historical sequence \( \mathcal{H} \) is used to predict app usage at the next time step \( t+1 \). 

As illustrated in Figure~\ref{fig:App}, the model learns temporal patterns in user interactions to predict the most likely next-used application:

\begin{equation}
Y = \arg\max_{A \in \mathcal{A}} P(A \mid \mathcal{H}),
\end{equation}
where \( Y \) is the predicted next app, and \( P(A \mid \mathcal{H}) \) represents the probability of app \( A \) being used at time \( t+1 \). To generate personalized recommendations, we rank all candidate applications based on \( P(A \mid \mathcal{H}) \) and recommend the top-K most relevant apps:

\begin{equation}
\begin{split}
    \mathcal{R}_{t+1} &= \{A_1, A_2, \dots, A_k\},\\
    &\text{where } A_k = \arg\max_{A \in \mathcal{A} \setminus \{A_1, \dots, A_{k-1}\}} P(A \mid \mathcal{H}).
\end{split}
\end{equation}

Figure~\ref{fig:App} also illustrates the ranking process, ensuring that each app is selected sequentially based on its probability, avoiding duplicate recommendations. This ranking mechanism allows the model to efficiently suggest the most relevant applications to users, adapting to their historical engagement patterns.



\subsection{Analysis of Tsinghua User App Usage Patterns}
In our research, we build upon prior studies and further expand the understanding of user app usage behavior. Previous studies have shown that the time intervals between consecutive app usage records and the total number of records per user follow a power-law distribution, indicating the scale-invariant nature of app engagement patterns~\cite{tu2018your}. Furthermore, Smith et al.~\cite{huang2017understanding} demonstrated that 88\% of users who interact with more than ten applications can be uniquely identified using just four randomly selected apps. These findings highlight the highly structured and predictable nature of mobile app usage behavior.

Building on our foundational analysis, we have uncovered two additional insights:
\begin{itemize}
    \item Users exhibit distinct temporal preferences for app usage, suggesting specific applications are accessed predominantly during different times of the day.
    \item User behavior can be categorized into clusters based on app usage patterns, enabling personalized prediction strategies.
\end{itemize}

Based on observed behavior patterns, we categorize the day into the following four primary time slots:
\begin{itemize}
    \item \textbf{Morning (06:00 - 10:00)}: Predominant use of productivity and news applications.
    \item \textbf{Afternoon (10:00 - 14:00)}: Increased usage of social media and communication apps.
    \item \textbf{Evening (14:00 - 18:00)}: Dominant engagement with entertainment and video streaming apps.
    \item \textbf{Night (18:00 - 06:00)}: Elevated activity in gaming and leisure applications.
\end{itemize}

To further analyze user behavior, we conducted a clustering analysis on users' app usage records over 7 days to explore whether their habits exhibit similarities. For each user, we calculated the Top-K most frequently used apps in four time periods throughout the day, constructing a feature vector \( V \), where:
\begin{itemize}
    \item Each user is represented by four variables (Morning, Afternoon, Evening, Night).
    \item Each variable captures 7 days of app usage patterns, with a length of \( 7 \times \) \{Top-K\}, \( K=5 \).
\end{itemize}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{patterns.jpg}
    \caption{Analysis of user patterns. Panel A: Aggregated hourly app usage across all users over a 7-day period. Panel B: User clustering based on app usage at four time slots, revealing three distinct user groups.}
    \label{fig:AppTime}
\end{figure}

\begin{itemize}
    \item \textbf{Panel A}: We aggregated each user's 7-day app usage records and computed the total number of app usage events per hour across all users. The final distribution represents the overall app engagement trend at different times of the day. To minimize the influence of extreme users, we applied an upper-bound clipping at the 99th percentile. The resulting visualization includes a boxplot to display the interquartile range (IQR) and a median trend line to highlight the overall usage pattern.
    \item \textbf{Panel B}: Users were clustered based on their app usage behavior in four distinct time slots (Morning, Afternoon, Evening, Night). We applied k-means clustering with \(K=3\), chosen using the elbow method and validated with silhouette scores. Each user was represented using average session duration per time slot. The clustering process revealed three distinct groups: high-usage users (green), moderate-usage users (blue), and low-usage users (red). To enhance interpretability, we visualized the cluster distributions using scatter plots with session duration as the key differentiating feature.
\end{itemize}

As shown in Figure~\ref{fig:AppTime}, Panel A illustrates the hourly distribution of app usage, revealing peak engagement during key periods, supporting the hypothesis that app usage is time-dependent. Panel B further reveals distinct user groups through clustering: High Usage Users (green) exhibit prolonged nighttime activity, whereas Low Usage Users (red) maintain stable but limited engagement throughout the day.

These findings emphasize the importance of incorporating app usage duration and usage time patterns into app usage prediction models. In this study, we propose a Transformer-based model with an integrated attention mechanism, referred to as Atten-Transformer. This model captures long-term dependencies in user behavior through Transformer blocks while employing an attention mechanism to dynamically adjust the importance of different app usage sequences based on encoded duration features and temporal features. By integrating periodic temporal encoding and time-aware representations, Atten-Transformer enhances the model’s ability to focus on the most informative patterns, ultimately improving prediction accuracy.



\section{Atten-Transformer}
\subsection{User App Usage Feature Representation}

To predict user app usage behavior, we define four key feature sequences based on the recorded app usage dataset \( D \), which contains records of \( N \) users interacting with various applications:

\begin{itemize}
    \item \textbf{App Sequence} \( \mathbf{A} \in \mathbb{R}^{5} \):  
    This sequence consists of the currently used app and the next four consecutively used apps, provided that the time intervals between consecutive usages do not exceed a predefined threshold. Each sequence has a fixed length of 5.

    \item \textbf{Time Sequence} \( \mathbf{T} \in \mathbb{R}^{5} \):  
    This sequence records the duration of usage for each app in the corresponding app sequence, reflecting how long the user interacted with each app.

    \item \textbf{User Characteristics} \( u \in \mathbb{R} \):  
    This feature represents individual user-specific attributes, such as user IDs or demographic information, to enhance personalized predictions.

    \item \textbf{Temporal Index} \( h \in \mathbb{R} \):  
    The temporal index represents the hour of the day (ranging from 0 to 23), serving as input for the temporal positional encoding. This encoding, based on sinusoidal functions, captures the periodic nature of app usage patterns over a 24-hour cycle and provides temporal context to the model.
\end{itemize}

For each user, we construct an interaction sequence of length 5, denoted as:
\begin{equation}
\mathbf{S} = \left[ \mathbf{f}_1, \mathbf{f}_2, ..., \mathbf{f}_5 \right] \in \mathbb{R}^{5 \times 2},
\end{equation}
where each interaction step \( i \) is represented by a feature vector:
\begin{equation}
\mathbf{f}_i = \left( A_i, T_i \right) \in \mathbb{R}^{2},
\end{equation}
where:
\begin{itemize}
    \item \( A_i \) represents the app used in the \( i \)-th interaction step within the app sequence \( \mathbf{A} \).
    \item \( T_i \) denotes the usage duration of the corresponding app within the time sequence \( \mathbf{T} \).
\end{itemize}

Thus, each user's interaction history is represented as a structured matrix \( \mathbf{S} \), capturing both the app identity and its corresponding usage duration over 5 consecutive interactions.

The prediction task aims to estimate the user's next app usage behavior, denoted as \( Y \). Here, \( Y \) represents the app that the user is expected to use next, based on their historical usage patterns.


\subsection{Feature Encoding}

For each \( i \)-th interaction step in the sequence \(  \mathbf{S}  \), we initialize a feature vector \( \mathbf{f}_i \in \mathbb{R}^{ 2} \), which includes two elements: the app ID and its usage duration. To enhance feature representation, the Atten-Transformer employs a sinusoidal transformation inspired by Fourier feature mappings, defined as:

\begin{equation}
\bar{\mathbf{f}}_i = \left[ \sin(\mathbf{W}_1 \mathbf{f}_i + \mathbf{b}_1), \cos(\mathbf{W}_2 \mathbf{f}_i + \mathbf{b}_2)\right],
\end{equation}
where \( \mathbf{W}_1, \mathbf{W}_2 \in \mathbb{R}^{2 \times d_\text{hid}} \) and \( \mathbf{b}_1, \mathbf{b}_2 \in \mathbb{R}^{1 \times d_\text{hid}} \) are learnable weight matrices and biases. This transformation captures periodic patterns in app usage, enhancing the model's ability to recognize temporal trends and behavioral recurrences.

In the feature encoding process, the weight matrices \( \mathbf{W}_1 \) and \( \mathbf{W}_2 \), along with biases \( \mathbf{b}_1 \) and \( \mathbf{b}_2 \), play a crucial role. By applying sinusoidal and cosine transformations to the input feature \( \mathbf{f}_i \), the model not only detects periodic changes but also adapts to different user behavior patterns.

Furthermore, these encoding methods directly support our prediction task—identifying patterns in user behavior to predict the next app they are likely to use. By understanding periodic patterns, the model enhances its ability to anticipate user actions at different times, ultimately improving overall prediction accuracy.

\subsection{Temporal Positional Encoding}

To effectively represent the time of day, we employ a sinusoidal-based temporal positional encoding scheme. Which maps each hour of the day to a fixed-dimensional embedding space, capturing the periodic nature of daily cycles.

Given an hour index \( h \), the positional encoding is computed as:

\begin{equation}
\theta_i = \frac{2\pi }{d}(i- h), \quad i \in \{0, 1, \dots, d/2 - 1\},
\end{equation}
where \( d = 24 \) is the total embedding dimension. The encoding consists of sine and cosine components:

\begin{equation}
\mathbf{P}_{h} =
\begin{bmatrix}
    \sin(\theta_0), \sin(\theta_1), \dots, \sin(\theta_{d/2-1}) \\
    \cos(\theta_0), \cos(\theta_1), \dots, \cos(\theta_{d/2-1})
\end{bmatrix}.
\label{equ:time}
\end{equation}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\linewidth]{position.png}
    \caption{Sinusoidal Functions for Time Encoding.}
    \label{fig:position}
\end{figure}

This encoding scheme offers several advantages:

\begin{itemize}
    \item \textbf{Periodic Representation:} The sinusoidal functions naturally capture daily usage patterns, reflecting user activity variations throughout the day.
    \item \textbf{Smooth Transition:} Ensures smooth transitions between consecutive hours, aiding temporal modeling and improving sensitivity to time-based changes.
    \item \textbf{Parameter-Free Design:} The absence of learnable parameters allows for better generalization across different temporal scenarios.
\end{itemize}

As illustrated in Figure \ref{fig:position}, sinusoidal encoding effectively captures the periodic nature of daily cycles. Each time step (e.g., an hour) is represented as a set of sinusoidal values, preserving time continuity while enabling the model to recognize cyclical human behavior, such as distinguishing between daytime and nighttime activities. This approach surpasses traditional one-hot encoding by embedding relative temporal information into the model, enhancing its ability to predict user behavior based on time variations.
\subsection{Architecture of Atten-Transformer}

Once the corresponding sequences and temporal encodings are obtained, the processed information is fed into the Atten-Transformer model, whose framework is illustrated in Figure \ref{fig:Atten-Transformer}. The detailed components of the model are described as follows:

\paragraph{Feature Transformation.} 
Initially, we apply transformation functions and weights to each feature:

\begin{equation}
\hat{\mathbf{X}}_{{u}} = \mathbf{E} \mathbf{X}_{{u}},\quad \hat{\mathbf{f}}_i = \text{Feature Encoding}\left(\bar{\mathbf{f}}_i\right),
\end{equation}
where \( \mathbf{E} \in \mathbb{R}^{N \times d_\text{hid}} \) is a trainable embedding matrix that maps categorical user features into a continuous vector space.

\paragraph{Transformer Module}

The transformed features are processed through an \( L \)-layer Transformer encoder to extract sequential representations:

\begin{equation}
\begin{split}
&\hat{\mathbf{f}}^{(0)}_i = \hat{\mathbf{f}}_i,\\
&\hat{\mathbf{m}}_i^{(l)} = \text{LayerNorm} \left( \hat{\mathbf{f}}^{(l)}_i + \text{MultiHeadSelfAttention}(\hat{\mathbf{f}}_i^{(l)}) \right),\\
&\hat{\mathbf{f}}_i^{(l+1)} = \text{LayerNorm} \left( \hat{\mathbf{m}}_i^{(l)} + \sigma \left(\hat{\mathbf{m}}_i^{(l)} \mathbf{W}^{(l)}_1 + \mathbf{b}^{(l)}_1\right) \mathbf{W}^{(l)}_2 + \mathbf{b}^{(l)}_2 \right) \quad, 
\end{split}
\end{equation}
where $l \in \{0, 1, \dots, L-1\}$. Each Transformer encoder layer consists of a multi-head self-attention mechanism:

\begin{equation}
\text{MultiHeadSelfAttention}(\hat{\mathbf{f}}^{(l)}_i) =  (\mathbf{H}_1 \parallel \mathbf{H}_2 \parallel \dots \parallel \mathbf{H}_\text{head}) \mathbf{W}_O,
\end{equation}
where each attention head \( \mathbf{H}_j \) is defined as:

\begin{equation}
\mathbf{H}_j = \text{softmax} \left( \frac{\hat{\mathbf{f}}^{(l)}_i \mathbf{W}_Q^j (\hat{\mathbf{f}}^{(l)}_i \mathbf{W}_K^j)^T}{\sqrt{d_{\text{hid}}}}\right) \hat{\mathbf{f}}^{(l)}_i \mathbf{W}_V^j.
\end{equation}

After passing through the Transformer layers, we apply global mean pooling over the sequence to obtain a compact representation and integrate the user embedding:

\begin{equation}
\begin{split}
&\mathbf{X}^{(L)} = \frac{1}{|\mathbf{S}|} \sum_{i \in \mathbf{S}} \hat{\mathbf{f}}^{(L)}_i,\\
&\mathbf{X}_u^{(L)} = \sigma (\mathbf{X}^{(L)} \parallel \hat{\mathbf{X}}_{{u}}),
\end{split}
\end{equation}
where \( \mathbf{X}^{(L)} \) is the final output of the Transformer module, \( \hat{\mathbf{X}}_{{u}} \) represents the user embedding, and \( \parallel \) denotes concatenation.

\paragraph{Temporal Attention Mechanism}

To effectively model the influence of temporal patterns on user behavior, we introduce a time-aware attention mechanism. Given a temporal index \( h \), we first obtain its corresponding temporal positional encoding:

\begin{equation}
\mathbf{P}_h = \text{Temporal Positional Encoding}(h).
\end{equation}

Next, a learnable transformation is applied to compute the temporal attention weight:

\begin{equation}
\mathbf{W}_h = \sigma (\mathbf{P}_h \mathbf{W}_h),
\end{equation}
where \( \mathbf{W}_h \in \mathbb{R}^{24 \times 2d_\text{hid}} \) is a trainable weight matrix and \( \sigma(\cdot) \) represents the sigmoid activation function. This weight captures the importance of different temporal positions in user behavior modeling.

Finally, the computed temporal weight is incorporated into the user representation \( \mathbf{X}_u^{(L)} \) through the Hadamard product:

\begin{equation}
\mathbf{X}_o = \mathbf{X}_u^{(L)}  \odot \mathbf{W}_h.
\end{equation}

This operation ensures a fine-grained interaction between user features and temporal attention weights, enhancing the model’s ability to capture temporal dependencies.

\paragraph{Loss Function}
Based on the above module, we use a classifier to predict app usage scores:

\begin{equation}
\hat{\mathbf{Y}} = \mathbf{X}_o \mathbf{W}_o,
\end{equation}
where, \( \hat{\mathbf{Y}} \) represents the logits, which are unnormalized scores before applying the softmax function.

To optimize the model, we employ the cross-entropy loss function:

\begin{equation}
\mathcal{L} = -\frac{1}{N} \sum_{i=1}^{N} \log \hat{P}_i(Y_i),
\end{equation}
where \( Y_i \) represents the ground truth class index for sample \( i \), and

\begin{equation}
\hat{P}_i(Y_i) = \frac{e^{\hat{\mathbf{Y}}_{i, Y_i}}}{\sum_{j=1}^{C} e^{\hat{\mathbf{Y}}_{i, j}}}.
\end{equation}
Here, \( \hat{\mathbf{Y}}_{i, j} \) denotes the logit score for class \( j \) of sample \( i \), and \( C \) is the total number of classes. The loss function applies the softmax operation to normalize the logits into a probability distribution before computing the negative log-likelihood of the correct class.

\section{Experiments}

\subsection{Dataset}
To evaluate our model, we utilize two real-world mobile app usage datasets: Tsinghua App Usage~\cite{yu2018smartphone} and LSapp~\cite{AliannejadiTOIS21}, both of which are widely used for app usage prediction~\cite{wang2019modeling, chen2019cap, tu2019fingerprint}. A summary of their key statistics is provided in Table~\ref{tab:dataset_comparison}.

\begin{table}[htbp]
    \centering
    \caption{Comparison of Tsinghua App Usage and LSapp Datasets}
    \label{tab:dataset_comparison}
    \begin{tabular}{lccccc}
        \toprule
        \textbf{Dataset} & \textbf{Users} & \textbf{Apps} & \textbf{Records} & \textbf{Time Span} & \textbf{Attributes} \\
        \midrule
        Tsinghua App Usage & 1000 & 2000 & 4,171,950 & 7 days & User ID, Timestamp, \\ 
        & & & & & Base Station ID, App ID, \\ 
        & & & & & \textbf{Interaction Type} \\
        \midrule
        LSapp & 292 & 87 & 599,635 & {Multiple Days} & User ID, Timestamp, \\ 
        & & & & & Session ID, App ID, \\ 
        & & & & & Interaction Type\\
        \bottomrule
    \end{tabular}
\end{table}

To ensure consistency in sequence generation, both datasets undergo a preprocessing phase, including:

\begin{itemize}
    \item \textbf{Filtering inactive users and apps}: We exclude users with fewer than 50 app usage records and apps with fewer than 10 occurrences to ensure data reliability.
    
    \item \textbf{Session segmentation}: We define app usage sessions by grouping consecutive app usage events within a time threshold of 300 seconds. This threshold balances session continuity while minimizing noise from long idle periods. 

    \item \textbf{Timestamp normalization}: All timestamps are converted to a uniform format using Coordinated Universal Time (UTC) to preserve temporal consistency.
\end{itemize}


This preprocessing ensures the generated sequences maintain temporal coherence and enhance the model's ability to predict future app usage effectively.

\subsection{Split Method}
For the LSApp dataset, we adopt two widely used split strategies from CoSEM~\cite{khaokaew2021cosem} and MAPLE~\cite{khaokaew2024maple}:

\begin{itemize}
    \item \textbf{Standard Setting}: Each user's data is split chronologically into 70\% for training, 10\% for validation, and 20\% for testing. This setup ensures comprehensive model training, fine-tuning, and evaluation.
    
    \item \textbf{Cold Start Setting}: To evaluate the model's generalization to new users, we split the dataset by users rather than timestamps. Specifically, 90\% of users are used for training, while the remaining 10\% are unseen during training and reserved for testing. To ensure robustness, this process is repeated across all users.
\end{itemize}

For the Tsinghua App Usage dataset, in addition to standard and cold start data splits, we incorporate a time-based split following DUGN~\cite{ouyang2022learning} and Appformer~\cite{sun2025appformer}. Here, app usage sessions from the first six consecutive days are used for training, while the sessions from the seventh day form the test set.


\subsection{Evaluation Metrics}
To evaluate the effectiveness of our next mobile app recommendation model, we compare the recommended top-K list with the ground truth. We employ three widely-used evaluation metrics in recommender systems: Hit Rate at K (HR@K), Mean Reciprocal Rank at K (MRR@K), and Normalized Discounted Cumulative Gain at K (NDCG@K). For these metrics, $K$ ranges from 1 to 5, allowing us to assess the performance at various levels of recommendation depth.

HR@K is calculated as the ratio of hits in the top-K recommendations to the total number of test samples, defined as:
\[
\text{HR@K} = \frac{\#\text{hit@K}}{|N_{\text{test}}|},
\]
where \(\#\text{hit@K}\) represents the number of hits in the test dataset and \(|N_{\text{test}}|\) signifies the test dataset’s total quantity.

MRR@K enhances the value of hits at the top of the recommendation list by computing the reciprocal of the rank of the first correct recommendation:
\[
\text{MRR@K} = \frac{1}{|N_{\text{test}}|} \sum_{i=1}^{|N_{\text{test}}|} \frac{1}{\text{rank}_i},
\]
where \(\text{rank}_i\) is the rank of the i-th correct prediction, and \(|N_{\text{test}}|\) is the number of test samples.


NDCG@K accounts for the position of hits within the recommendation list, giving higher scores to hits at higher ranks. It is calculated as:
\[
\text{NDCG@K} = \frac{\sum_{i=1}^K \frac{2^{\text{rel}_i} - 1}{\log_2(i+1)}}{\sum_{j=1}^{|REL_K|} \frac{2^{\text{rel}_j} - 1}{\log_2(j+1)}},
\]
where \(\text{rel}_i\) denotes the graded relevance of the result at position i, and \(|REL_K|\) is the number of predictions in the result ranking list up to position K.

For all metrics, larger values indicate better prediction performance, effectively capturing the accuracy and quality of the recommendations provided by the model.

\subsection{Baselines}
We compare Atten-Transformer with a broad range of state-of-the-art recommendation models, including rule-based, matrix factorization, sequential, graph-based, and Transformer-based approaches.

\begin{itemize}
    \item \textbf{Rule-based methods}: MFU~\cite{shin2012understanding} and MRU~\cite{shin2012understanding} serve as simple yet effective baselines based on app popularity and recency.
    \item \textbf{Matrix factorization}: BPRMF~\cite{rendle2012bpr} optimizes a Bayesian pairwise ranking loss to learn user-app preferences.
    \item \textbf{Sequential models}: GRU4Rec~\cite{hidasi2015session} and NeuSA~\cite{aliannejadi2021context} leverage RNNs and LSTMs to model app usage sequences, while AppUsage2Vec~\cite{zhao2019appusage2vec} employs an attention mechanism for personalized recommendations.
    \item \textbf{Graph-based methods}: SR-GNN~\cite{wu2019session} and DUGN~\cite{ouyang2022learning} model app interactions as graphs, capturing complex transition patterns.
    \item \textbf{Transformer-based approaches}: Transformer~\cite{vaswani2017attention}, Appformer~\cite{sun2025appformer}, CoSEM~\cite{khaokaew2021cosem}, and MAPLE~\cite{khaokaew2024maple} apply attention-based architectures, incorporating multimodal and semantic learning. FEDformer~\cite{zhou2022fedformer}, Reformer~\cite{kitaev2020reformer}, and TimesNet~\cite{wu2022timesnet} explore efficient attention mechanisms and temporal modeling. DLinear~\cite{zeng2023transformers} integrates decomposition-based forecasting.
\end{itemize}
These baselines provide a comprehensive comparison across different modeling paradigms, ensuring a thorough evaluation of Atten-Transformer.

\subsection{Implementation Details}

\begin{table}[htbp]
    \centering
    \caption{Hyperparameter settings}
    \label{tab:detail}
    \begin{tabular}{lc}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Batch size & 512 \\
        Learning rate & 0.001 \\
        Optimizer & Adam \\
        Hidden size & 128 \\
        Dropout rate & 0.2 \\
        Number of Transformer layers & 2 \\
        Number heads& 4\\
        Training epochs & 50 \\
        Early stopping patience & 5 \\
        \bottomrule
    \end{tabular}
\end{table}


Our experiments were conducted on an NVIDIA RTX A6000 GPU, with 32GB RAM and an AMD Ryzen Threadripper PRO 5975WX 32-Cores CPU. The model was implemented using PyTorch 2.2.2 with CUDA 12.1 acceleration. 

To prevent overfitting and ensure efficient training, we employed early stopping with a patience of 5 epochs, meaning that training was terminated if the test loss did not improve for five consecutive epochs. The hyperparameter settings can been found in Table~\ref{tab:detail}. 



\subsection{Results on Tsinghua App Usage dataset}


We conducted three types of experiments on the Tsinghua App Usage dataset: Standard Split, Cold Start Split, and Time Split. The experimental results are presented in Table~\ref{tab:th_standard_split_results}, Table~\ref{tab:th_cold_start_results} and Table~\ref{tab:th_time_split_results}, .

\begin{itemize}
   
    \item \textbf{Standard Split}(Table~\ref{tab:th_standard_split_results}): AIn the Standard Split setting (Table~\ref{tab:th_standard_split_results}), MAPLE and Atten-Transformer show comparable overall performance. MAPLE achieves better results in HR@3, HR@5, and MRR@K, demonstrating its strength in ranking and long-sequence prediction. Atten-Transformer achieves a slightly higher HR@1 (0.5238 vs. 0.5191, a 0.91\% improvement), suggesting it might have marginally better precision for the top-ranked recommendation. 

    \item \textbf{Cold Start Split} (Table~\ref{tab:th_cold_start_results}): Atten-Transformer outperforms MAPLE across all HR@K and MRR@K metrics, with a 15.23\% improvement in HR@1 and an 8.71\% increase in MRR@5. This indicates that Atten-Transformer generalizes better in cold start scenarios and provides more accurate predictions for new users.
     \item \textbf{Time-Based Split} (Table~\ref{tab:th_time_split_results}): Atten-Transformer significantly outperforms Appformer across all metrics, with a 49.95\% improvement in HR@1, a 27.34\% increase in HR@5, and a 39.77\% boost in MRR@5. These results demonstrate that Atten-Transformer can effectively model long-term temporal dependencies, making it more generalizable in real-world time-based scenarios.
     
\end{itemize}

\begin{table}[htbp]
\caption{Performance comparison of different recommendation systems on Tsinghua Dataset with Standard split. Baseline results are referenced from MAPLE~\cite{khaokaew2024maple}}
\label{tab:th_standard_split_results}
\centering
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} }& \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} 
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
        MFU & 0.1972 & 0.4288 & 0.5384 & 0.2991 & 0.3241 \\
        MRU & 0.0000 & 0.5538 & 0.6536 & 0.2585 & 0.2817 \\
        Appusage2Vec & 0.2909 & 0.4822 & 0.5781 & 0.3739 & 0.3958 \\
        NeuSA & 0.4640 & 0.6562 & 0.7286 & 0.5492 & 0.5658 \\
        SA-GCN & 0.0613 & 0.1882 & 0.2521 & 0.1183 & 0.1331 \\
        DeepApp & 0.2862 & 0.5931 & 0.7075 & 0.4210 & 0.4473 \\
        DeepPattern & 0.2848 & 0.5884 & 0.7016 & 0.4185 & 0.4444 \\
        CoSEM & 0.4163 & 0.6682 & 0.7499 & 0.5282 & 0.5469 \\
        TimesNet & 0.0208 & 0.0480 & 0.0614 & 0.0327 & 0.0358 \\
        Transformer & 0.0262 & 0.0534 & 0.0661 & 0.0383 & 0.0412 \\
        FEDformer & 0.0159 & 0.0420 & 0.0553 & 0.0272 & 0.0303 \\
        DLinear & 0.0072 & 0.0370 & 0.0607 & 0.0202 & 0.0256 \\
        Reformer & 0.0228 & 0.0503 & 0.0645 & 0.0346 & 0.0378 \\
        MAPLE & 0.5191 & \textbf{0.7385} & \textbf{0.8115} & {0.6169} & {0.6338} \\
        \midrule
        \textbf{Atten-Transformer} & \textbf{0.5266} & {0.7341} & {0.8005} & \textbf{0.6191} & \textbf{0.6344} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{Performance comparison of different recommendation systems on Tsinghua  Dataset with cold start split. Baseline results are referenced from MAPLE~\cite{khaokaew2024maple}}
\label{tab:th_cold_start_results}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} }& \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} 
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
        MFU & 0.1853 & 0.3906 & 0.4943 & 0.2752 & 0.2989 \\
        MRU & 0.0000 & 0.6406 & 0.7226 & 0.3042 & 0.3234 \\
        TimesNet & 0.0144 & 0.0433 & 0.0647 & 0.0277 & 0.0323 \\
        Transformer & 0.0180 & 0.0461 & 0.0606 & 0.0308 & 0.0337 \\
        FEDformer & 0.0100 & 0.0411 & 0.0610 & 0.0231 & 0.0282 \\
        Reformer & 0.0224 & 0.0506 & 0.0570 & 0.0349 & 0.0384 \\
        DLinear & 0.0070 & 0.0350 & 0.0586 & 0.0199 & 0.0245 \\
        CoSEM & 0.3111 & 0.5597 & 0.6525 & 0.4204 & 0.4416 \\
        NeuSA & 0.4433 & 0.6169 & 0.6812 & 0.5206 & 0.5353 \\
        MAPLE & 0.5228 & 0.7417 & 0.8128 & 0.6206 & 0.6369 \\
        \midrule
        \textbf{Atten-Transformer} & \textbf{0.6024} & \textbf{0.7772} & \textbf{0.8240} & \textbf{0.6816} & \textbf{0.6924} \\
        \bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\caption{Performance comparison of different recommendation systems on the Tsinghua App Usage dataset with Time-based split. Baseline results are referenced from Appformer~\cite{sun2025appformer}.}
\label{tab:th_time_split_results}
\resizebox{\textwidth}{!}{ 
\begin{tabular}{lccccccccccccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} }& \multicolumn{5}{c}{\textbf{HR@K}} &\multicolumn{5}{c}{\textbf{NDCG@K}} & \multicolumn{5}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-6} \cmidrule(lr){7-11} \cmidrule(lr){12-16}
 & \textbf{1} & \textbf{2} & \textbf{3}  & \textbf{4} & \textbf{5} & \textbf{1} & \textbf{2} & \textbf{3}  & \textbf{4} & \textbf{5} & \textbf{1} & \textbf{2} & \textbf{3}  & \textbf{4} & \textbf{5} \\
\midrule
MRU            & 0.2398 & 0.4196 & 0.5135 & 0.5670 & 0.5938 & 0.2398 &0.3501&0.3921&0.4163&0.4299 & 0.2398 & 0.3210 & 0.3522 & 0.3668 & 0.3754 \\
MFU            & 0.2472 & 0.4205 & 0.5703 & 0.5621 & 0.5987 &0.2472&0.3565&0.3999&0.4235&0.4377 & 0.2472 & 0.3338 & 0.3628 & 0.3765 & 0.3838\\
BPRMF          & 0.3293 & 0.4437 & 0.5188 & 0.5681 & 0.6077&0.3293&0.4015&0.4390&0.4602&0.4755 & 0.3293 & 0.3865 & 0.4115 & 0.4238 & 0.4317 \\
GRU3Rec& 0.3137 & 0.4493 & 0.5425& 0.6028& 0.6494&0.3137&0.3993&0.4459&0.4827&0.5012 & 0.3137 & 0.3815 & 0.4126 & 0.4277 & 0.4370 \\
AppUsage2Vec   & 0.3333 & 0.4592 & 0.5436 & 0.6080 & 0.6560&0.3333 & 0.4127 & 0.4549 & 0.4827& 0.5012& 0.3333& 0.3962 & 0.4244& 0.4405& 0.4501\\
SR-GNN         & 0.3342 & 0.4716 & 0.5563 & 0.6154 & 0.6626&0.3342&0.4209&0.4632&0.4887&0.5070 & 0.3342 & 0.4029 & 0.4311 & 0.4459 & 0.4554 \\
DUGN           & 0.3479 & 0.4768 & 0.5593 & 0.6215 & 0.6710&0.3479&0.4292&0.4705&0.4973&0.5164 & 0.3479 & 0.4124 & 0.4399 & 0.4554& 0.4653 \\
Appformer& 0.4268 &0.5550 &0.6230 &0.6656& 0.6960&0.4268&0.5550&0.5979&0.6192&0.6323 & 0.4268& 0.4909 & 0.5136 & 0.5242 & 0.5303\\
\midrule
\textbf{Atten-Transformer}  & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863} & 
\textbf{0.6400}  & \textbf{0.7316} & \textbf{0.7574} & \textbf{0.7701} & \textbf{0.7779} & 
\textbf{0.6400} & \textbf{0.7126} & \textbf{0.7230} & \textbf{0.7372} & \textbf{0.7412}\\
\bottomrule
\end{tabular}}
\end{table}



Overall, Atten-Transformer achieves either the best or comparable performance under all evaluation settings. Notably, it demonstrates superior predictive accuracy and generalization capability in time-based and cold start scenarios.





\subsection{Results on LSapp Datase dataset}

We conducted experiments on the LSApp dataset using two different split settings: the standard split and the cold-start split. The results are shown in Table~\ref{tab:lsapp_results_stand} and Table~\ref{tab:lsapp_results}.

\begin{itemize}
    \item \textbf{Standard Split} (Table~\ref{tab:lsapp_results_stand}): In this setting, Atten-Transformer achieves the highest performance across all metrics, surpassing MAPLE, the best-performing baseline. Specifically, Atten-Transformer improves HR@1 by 13.39\% over MAPLE, indicating its superior ability to accurately predict the next app usage. While MAPLE performs well on HR@3 and HR@5, Atten-Transformer maintains a competitive edge in MRR@K, demonstrating its strong ranking capability.
    
    \item \textbf{Cold-Start Split} (Table~\ref{tab:lsapp_results}): In the cold-start scenario, Atten-Transformer significantly outperforms all baselines. Compared to MAPLE, Atten-Transformer improves HR@1 by 18.25\% and MRR@5 by 14.15\%. This highlights its effectiveness in handling unseen users, ensuring better app usage predictions even when user interaction history is unavailable.
\end{itemize}



\begin{table}[htbp]
\centering
\caption{Performance comparison of different recommendation systems on LSapp Dataset with standard split. Baseline results are referenced from MAPLE~\cite{khaokaew2024maple}}
\label{tab:lsapp_results_stand}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} }& \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} 
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
 MFU             & 0.2952 & 0.6258 & 0.7942 & 0.4378 & 0.4765  \\
MRU             & 0.0276 & 0.7850 & 0.8306 & 0.3974 & 0.4079  \\
 Appusage2Vec    & 0.6057 & 0.7858 & 0.8618 & 0.6848 & 0.7022\\
 TimesNet        & 0.4805 & 0.628  & 0.6897 & 0.5459 & 0.5600  \\
Transformer     & 0.4978 & 0.653  & 0.7141 & 0.5659 & 0.5800 \\
FEDformer       & 0.4946 & 0.6374 & 0.6915 & 0.5585 & 0.5708  \\
DLinear         & 0.1611 & 0.3978 & 0.4790 & 0.2637 & 0.2824\\
Reformer        & 0.4920 & 0.6505 & 0.7074 & 0.5620 & 0.5750\\
AppUsage2Vec   & 0.6057& 0.7858 & 0.8618 & 0.6848 & 0.7022\\
 CoSEM           & 0.4990 & 0.7466 & 0.8149 & 0.6083 & 0.6242\\
 NeuSA           & 0.6832 & 0.8253 & 0.8830 & 0.7461 & 0.7593 \\
MAPLE          & 0.7157 & 0.8649 & 0.9150 & 0.7821 & 0.7936 \\
\midrule
\textbf{Atten-Transformer} & \textbf{0.8115} & \textbf{0.9494} & \textbf{0.9667} & \textbf{0.8767} & \textbf{0.8807} \\
\bottomrule
\end{tabular}
\end{table}





\begin{table}[htbp]
\centering
\caption{Performance comparison of different recommendation systems on LSapp Dataset with cold start split. Baseline results are referenced from MAPLE~\cite{khaokaew2024maple}}
\label{tab:lsapp_results}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods} }& \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} 
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
MFU       & 0.2952 & 0.6258 & 0.7942 & 0.4378 & 0.4765 \\
MRU       & 0.0276 & 0.7850 & 0.8306 & 0.3974 & 0.4079 \\
TimesNet  & 0.4805 & 0.6280 & 0.6897 & 0.5459 & 0.5600 \\
Transformer & 0.4978 & 0.6530 & 0.7141 & 0.5659 & 0.5800 \\
FEDformer & 0.4946 & 0.6374 & 0.6915 & 0.5585 & 0.5708 \\
DLinear   & 0.1611 & 0.3978 & 0.4790 & 0.2637 & 0.2824 \\
Reformer  & 0.4920 & 0.6505 & 0.7074 & 0.5620 & 0.5750 \\
AppUsage2Vec   & 0.6057& 0.7858 & 0.8618 & 0.6848 & 0.7022\\
CoSEM     & 0.4990 & 0.7466 & 0.8149 & 0.6083 & 0.6242 \\
NeuSA     & 0.6832 & 0.8253 & 0.8830 & 0.7461 & 0.7593 \\
MAPLE     & 0.7171 & 0.8670 & 0.9166 & 0.7836 & 0.7950 \\
\midrule
\textbf{Atten-Transformer}  & \textbf{0.8480} & \textbf{0.9662} & \textbf{0.9807} & \textbf{0.9042} & \textbf{0.9075} \\
\bottomrule
\end{tabular}
\end{table}

Across both settings, Atten-Transformer consistently achieves the best or comparable performance, particularly excelling in HR@1 and MRR@K, which are crucial for accurate app recommendation. Its strong performance in the cold-start scenario further demonstrates its generalization ability, making it a robust solution for real-world app prediction tasks.


\subsection{Training Convergence under Cold Start Split} 

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=1.0\linewidth]{loss_eva.jpg}
    \caption{Training convergence of Atten-Transformer on the Tsinghua App Usage dataset and LSApp dataset. 
    (A) Training and test loss curves for the Tsinghua App Usage dataset. 
    (B) HR@K and MRR@K evolution on the test set during training on the Tsinghua App Usage dataset. 
    (C) Training and test loss curves for the LSApp dataset. 
    (D) HR@K and MRR@K evolution on the test set during training on the LSApp dataset.}
    \label{fig:epoch}
\end{figure*}

Figure~\ref{fig:epoch} illustrates the training convergence of Atten-Transformer across different datasets. 

In Panel A, the training loss decreases steadily, indicating that the model is learning effectively. However, the test loss remains relatively stable with slight fluctuations, suggesting potential overfitting after epoch 6-8. This highlights the importance of regularization techniques or early stopping strategies to improve generalization.

In Panel B, HR@K and MRR@K improve steadily over epochs, with noticeable gains within the first 5 epochs. However, the metrics continue to improve beyond epoch 10, indicating that further training could still yield performance gains.

In Panel C, the training loss decreases significantly, demonstrating effective learning. However, the test loss fluctuates across epochs, indicating potential sensitivity to different training iterations. This suggests that additional regularization or adaptive learning rate schedules may be beneficial for improved generalization.

In Panel D, HR@K and MRR@K maintain a steady increase throughout the training process. While HR@1 shows gradual improvements, HR@3 and HR@5 continue to rise slightly, indicating that the model may still benefit from additional training iterations.

These results highlight Atten-Transformer’s strong ability to capture temporal dependencies efficiently. While the model achieves noticeable improvements within the early training epochs, continued performance gains beyond epoch 10 suggest that further optimization and regularization strategies could further enhance generalization.



\subsection{Ablation Study on Tsinghua App Usage Dataset (Time-based Split)}

\paragraph{Effect of Window Size}
To examine how different training window sizes affect model performance, we trained Atten-Transformer using varying data spans from one to six days (Table~\ref{tab:windows}). As the window size increases, the model gains more historical context, resulting in improved accuracy. Specifically, HR@1 increases from 0.3813 (one-day training) to 0.6400 (six-day training), and HR@5 reaches 0.8863. These results highlight the importance of incorporating longer historical data for better app usage prediction.

\begin{itemize}
    \item Using only the first day's data leads to significantly lower HR@K values, limiting the model’s ability to generalize.
    \item Performance improvement plateaus after six days, indicating diminishing returns from excessive history.
\end{itemize}


\begin{table}[htbp]
\centering
\caption{Performance comparison of different window sizes.}
\label{tab:windows}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Window Size ($t$)}} & \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6} 
& \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
1        & 0.3813 & 0.4676 & 0.5370 & 0.5734 & 0.5915  \\
2        & 0.4264 & 0.5483 & 0.5759 & 0.5951 & 0.6241  \\
3        & 0.4738 & 0.5600 & 0.6337 & 0.6893 & 0.7075  \\
4        & 0.5484 & 0.5960 & 0.6566 & 0.6927 & 0.7209 \\
5        & 0.5862 & 0.6941 & 0.7502 & 0.7814 & 0.8063   \\
\midrule
6        & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863} \\
\bottomrule
\end{tabular}
\end{table}



\paragraph{Feature Ablation Analysis}
Table~\ref{tab:feature} presents the impact of removing key feature components from Atten-Transformer. Each feature contributes significantly to performance, with the App Sequence playing the most crucial role.

\begin{itemize}
\item \textbf{App Sequence ( $w/o \quad A_i$ )} – Removing the app interaction history leads to the  performance drop, with HR@1 decreasing by {14.42\%} and HR@5 decreasing by {16.46\%}, confirming that sequential app usage patterns are the primary predictor for next-app recommendations.
\item \textbf{Time Sequence ( $w/o \quad T_i$ )} – The absence of time features results in an {12.48\%} decrease in HR@1 and a {13.78\%} decrease in HR@5, indicating that periodic usage patterns play a vital role in long-term behavior modeling.
\item \textbf{User Characteristics ( $w/o\quad u $)} – Although removing user-specific attributes has a smaller effect compared to app and time sequences, it still impacts personalization, reducing HR@1 by {12.48\%} and HR@5 by {13.78\%}.
\item \textbf{Temporal Attention ($ w/o\quad h $)} – Eliminating the temporal weighting mechanism lowers HR@1 by {18.58\%} and HR@5 by {5.76\%}, demonstrating that fine-grained temporal modeling enhances short-term prediction accuracy.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Performance comparison with different feature sequences.}
\label{tab:feature}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Features}} & \multicolumn{3}{c}{\textbf{HR@K}}  & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
& \textbf{1} & \textbf{3} & \textbf{5}  & \textbf{3} & \textbf{5} \\
\midrule
$ w/o \quad A_i  $       & 0.5477 & 0.6422 & 0.6990 & 0.7359 & 0.7643  \\
$ w/o \quad T_i$         & 0.5601 & 0.6678 & 0.7214 & 0.7476 & 0.7706  \\
$ w/o \quad u$           & 0.5601 & 0.6678 & 0.7214 & 0.7476 & 0.7706  \\
$ w/o \quad  h$          & 0.5211 & 0.7225 & 0.7885 & 0.6109 & 0.6261  \\
\midrule
Atten-Transformer       & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863}     \\ 
\bottomrule
\end{tabular}
\end{table}

The {18.58\%} drop in HR@1 after removing Temporal Attention highlights its importance in refining short-term behavioral modeling. Unlike standard Transformers that rely solely on positional encoding, our approach explicitly models short-term user preferences, enhancing immediate interaction predictions. The smaller impact on HR@5 ({5.76\%} decrease) suggests that while global dependencies can still be captured via self-attention, fine-grained temporal encoding is crucial for recent behavior modeling.




\paragraph{Encoding Ablation}
To assess our feature and temporal encoding methods, we compare Atten-Transformer with MLP, RBF Kernel encoding, and different temporal encodings (Table~\ref{tab:temporal}). Our feature encoding and sinusoidal positional encoding outperform all baselines across HR@K and MRR@K metrics.

\begin{itemize}
    \item \textbf{Feature Encoding (Ours)} improves HR@1 by 10.52\% over MLP and 38.92\% over RBF Kernel. Additionally, it boosts HR@5 by 11.67\% over MLP and 30.36\% over RBF. These results indicate that our learned feature representations effectively capture app usage dependencies. Unlike MLP, which treats features independently, our encoding leverages context-aware feature interactions, enhancing self-attention's ability to model sequential dependencies.
    
    \item \textbf{Positional Encoding (Ours)} surpasses one-hot and embedding-based encodings, demonstrating its effectiveness in capturing periodic usage patterns. This result is expected, as sinusoidal positional encoding provides a smooth and continuous representation of time, allowing the Transformer model to learn cyclical behaviors more effectively than discrete embeddings.
\end{itemize}

\begin{table}[htbp]
\centering
\caption{Performance Comparison of Different Encoding Methods}
\label{tab:temporal}
\begin{tabular}{lc cccc}
\toprule
\textbf{Type} & \textbf{Encoding} & \textbf{HR@1} & \textbf{HR@3} & \textbf{HR@5} & \textbf{MRR@5} \\
\midrule
\multirow{3}{*}{Feature}  
& MLP              & 0.5791 & 0.7467 & 0.7937 & 0.6657 \\   
& RBF Kernel       & 0.4607 & 0.6143 & 0.6799 & 0.5443 \\ 
& \textbf{Ours}   & \textbf{0.6400} & \textbf{0.8367} & \textbf{0.8863} & \textbf{0.7412} \\
\midrule                      
\multirow{3}{*}{Temporal}  
& One-hot          & 0.5774 & 0.7402 & 0.7945 & 0.6625 \\   
& Embedding        & 0.6052 & 0.7660 & 0.8145 & 0.6892 \\ 
& \textbf{Ours}   & \textbf{0.6400} & \textbf{0.8367} & \textbf{0.8863} & \textbf{0.7412} \\
\bottomrule
\end{tabular}

\end{table}

These findings highlight the importance of structured feature encoding and temporal representations in app usage prediction. The superior performance of our encoding methods suggests that effectively integrating feature interactions with self-attention mechanisms is crucial for modeling sequential app usage behaviors.

\paragraph{Time Threshold Sensitivity}
Table~\ref{tab:time} compares the impact of different time thresholds (200, 250s, 300s, 360s, 420s) on performance. Setting the threshold to 300 seconds achieves the best trade-off, ensuring session continuity while minimizing irrelevant noise.

\begin{table}[htbp]
\centering
\caption{Performance comparison of different time thresholds.}
\label{tab:time}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Threshold (Seconds)}} & \multicolumn{3}{c}{\textbf{HR@K}} & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
& \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3} & \textbf{5} \\
\midrule
420 s  & 0.5975 & 0.7117 & 0.7598 & 0.7883 & 0.8074  \\   
360 s  & 0.6109 & 0.7271 & 0.7721 & 0.7999 & 0.8180  \\
250 s  & 0.6007 & 0.7148 & 0.7552 & 0.7864 & 0.7906  \\
200 s  & 0.5894 & 0.7093 & 0.7473 & 0.7739 & 0.7960  \\
\midrule
300 s  & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Comparison of Model Variants}
Table~\ref{tab:module} compares Atten-Transformer with alternative architectures, including GRU, LSTM, CNN, GCN, and MLP, alongside an ablation study replacing self-attention with a randomly initialized attention mechanism.


\begin{table}[htbp]
\centering
\caption{Performance comparison of different model components.}
\label{tab:module}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{3}{c}{\textbf{HR@K}} &\multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5}  \\
\midrule
Atten-GRU     &  0.5254 & 0.6306 & 0.6870 & 0.7291 & 0.7531  \\
Atten-RNN     & 0.5133 & 0.6268 & 0.6796 & 0.7169 & 0.7446  \\
Atten-LSTM    & 0.6199 & 0.7395 & 0.7895 & 0.8200 & 0.8376  \\
Atten-GCN     & 0.6081 & 0.7417 & 0.7936 & 0.8250 & 0.8452  \\
Atten-CNN     & 0.6010 & 0.7250 & 0.7841 & 0.8143 & 0.8311  \\
Atten-MLP     & 0.5921 & 0.7136 & 0.7807 & 0.8012 & 0.8207  \\
Atten-Random  & 0.5120 & 0.6985 & 0.7543 & 0.8024 & 0.8251  \\
\midrule
\textbf{Atten-Transformer}  
& \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863} \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Key findings:}
\begin{itemize}
    \item \textbf{Self-Attention vs. MLP}: Removing self-attention (Atten-MLP) leads to a 7.50\% drop in HR@1, 9.14\% drop in HR@3, and 6.26\% drop in HR@5, indicating that self-attention effectively captures long-term dependencies, whereas MLP lacks the ability to model sequential relationships.

    \item \textbf{Self-Attention vs. LSTM}: Atten-Transformer outperforms Atten-LSTM in HR@1 (3.24\%), HR@3 (6.18\%), and HR@5 (5.98\%), demonstrating its superiority in capturing both short-term and long-range interactions. LSTM, while effective for local dependencies, struggles with long-term modeling due to vanishing gradient issues.

    \item \textbf{Self-Attention vs. CNN}: Atten-Transformer achieves a 5.24\% higher HR@5 compared to Atten-CNN, showing that global sequence modeling is essential for app recommendation. CNNs primarily focus on local patterns, which limits their effectiveness in modeling app usage trends.

    \item \textbf{Self-Attention vs. GCN}: Compared to Atten-GCN, Atten-Transformer achieves a 5.24\% increase in HR@1, 5.87\% in HR@3, and 5.41\% in HR@5, confirming that structured self-attention outperforms graph-based propagation in sequential recommendation. While GCNs effectively capture relational dependencies, they lack explicit sequence modeling capabilities.

    \item \textbf{Self-Attention vs. Random Attention}: To validate the necessity of self-attention, we replace learned attention scores with randomly initialized weights. The Random Attention variant (Atten-Random) leads to a 20.00\% drop in HR@1, 11.03\% drop in HR@3, and 9.81\% drop in HR@5, showing that:
    \begin{itemize}
        \item Performance gains are due to structured attention learning, rather than merely stacking Transformer layers.
        \item Random attention fails to capture meaningful dependencies, reinforcing the importance of learnable self-attention scores for sequential modeling.
    \end{itemize}
\end{itemize}

These findings confirm that self-attention is the key factor in improving app usage prediction accuracy. Unlike MLP (which lacks sequential modeling) and Random Attention (which fails to learn dependencies), self-attention dynamically adjusts its focus across time steps, enabling better sequence representation and prediction accuracy.

\paragraph{Hyperparameter Sensitivity}  
We examine the impact of key hyperparameters, including the number of Transformer layers, sequence length, and dropout rate, on model performance.

\begin{table}[htbp]
\centering
\caption{Performance comparison of different hyperparameters.}
\label{tab:hyperparams}
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Hyperparameter}} & \multicolumn{3}{c}{\textbf{HR@K}}  & \multicolumn{2}{c}{\textbf{MRR@K}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-6}
 & \textbf{1} & \textbf{3} & \textbf{5} & \textbf{3}  & \textbf{5} \\
\midrule
\multicolumn{6}{l}{\textbf{Transformer Layers}} \\
\midrule
1  & 0.5570 & 0.6638 & 0.7376 & 0.7642 & 0.7825 \\
2  & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863} \\
3  & 0.5821 & 0.7213 & 0.7954 & 0.8388 & 0.8405 \\
4  & 0.5623 & 0.6987 & 0.7641 & 0.7992 & 0.8125  \\
\midrule
\multicolumn{6}{l}{\textbf{Sequence Length}} \\
\midrule
3  & 0.5844 & 0.6974 & 0.7436 & 0.7759 & 0.7961   \\
4  & 0.6010 & 0.7172 & 0.7638 & 0.7905 & 0.8117  \\
5  & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863}  \\
6  & 0.6275 & 0.7749 & 0.8246 & 0.8522 & 0.8712  \\ 
7  & 0.6302 & 0.7781 & 0.8289 & 0.8567 & 0.8750  \\
8  & 0.6321 & 0.7802 & 0.8312 & 0.8591 & 0.8774  \\
10 & 0.6289 & 0.7768 & 0.8280 & 0.8554 & 0.8739  \\
\midrule
\multicolumn{6}{l}{\textbf{Dropout Probability}} \\
\midrule
0.0  & 0.5414 & 0.6535 & 0.7106 & 0.7473 &  0.7764 \\
0.1  & 0.5670 & 0.6784 & 0.7318 & 0.7668 & 0.7945  \\
0.2  & \textbf{0.6400} & \textbf{0.7852} & \textbf{0.8367} & \textbf{0.8663} & \textbf{0.8863}  \\
0.3  & 0.5707 & 0.6809 & 0.7311 & 0.7615 & 0.7866  \\
0.4  & 0.5359 & 0.6413 & 0.6901 & 0.7248 & 0.7480   \\ 
0.5  & 0.5287 & 0.6322 & 0.6820 & 0.7154 & 0.7381   \\ 
0.6  & 0.5123 & 0.6190 & 0.6702 & 0.7033 & 0.7260   \\ 
\bottomrule
\end{tabular}
\end{table}

From Table~\ref{tab:hyperparams}, we find that:
\begin{itemize}
    \item \textbf{Number of Transformer Layers:} Increasing layers from 1 to 2 significantly improves HR@5 by 13.42\% , likely due to the model's enhanced ability to capture complex temporal dependencies. However, adding more layers leads to performance degradation, likely due to overfitting and vanishing gradients in deeper attention networks.
    
    \item \textbf{Sequence Length:} The optimal sequence length lies between 5 and 8, beyond which performance gains diminish. HR@5 improves by 5.63\% from 5 to 8, indicating that historical information beyond 5 timestamps provides limited additional value. The improvement in HR@1 is due to the optimal sequence length (5-6), which allows the model to focus on recent user interactions without incorporating excessive historical noise.
    
    \item \textbf{Dropout Probability:} A dropout rate of 0.2 achieves the best performance, improving HR@5 by 9.01\%  compared to no dropout (0.0). Excessive dropout leads to performance degradation, suggesting that moderate regularization helps prevent overfitting while preserving essential information for app usage prediction.
\end{itemize}

\section{Conclusion}

In this paper, we introduced Atten-Transformer, an Transformer-based model that incorporates a novel attention mechanism specifically designed to address the dynamic nature of mobile app usage behavior. This model distinguishes itself from traditional approaches by effectively integrating temporal positional encoding and feature encoding, which substantially enhances the prediction accuracy across multiple datasets.

Our experimental results on the Tsinghua App Usage and LSapp datasets demonstrate that Atten-Transformer outperforms contemporary state-of-the-art models, achieving significant improvements such as 27.34\% in HR@5 and 23.03\% in NDCG@5 over the Appformer model. Notably, these enhancements were consistent across both datasets, underscoring our model's robustness and adaptability. Ablation studies further validate the importance of the temporal and traffic-aware features, indicating that omitting time-based patterns results in a substantial 12.3\% drop in performance, which reaffirms the critical role of our model's temporal dynamics in enhancing predictive accuracy.

Looking forward, we plan to expand our research to encompass multi-modal contextual information, such as geographic location and user activity patterns, to enhance the model's applicability and robustness, particularly in cold-start scenarios. Additionally, we aim to explore further into Transformer-based architectures to refine our approach to sequential behavior modeling. This future work is expected to tackle existing challenges and push the boundaries of what our model can achieve in practical applications, potentially influencing a broader range of predictive analytics tasks in the mobile ecosystem.


%Bibliography
\bibliographystyle{unsrt}  
\bibliography{references}  


\end{document}
