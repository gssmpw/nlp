\section{Related Work}
App usage prediction is critical for enhancing user experience and optimizing system performance by preloading app-related content, reducing startup latency, and improving energy efficiency~\cite{tian2020cohort}. Early methods relied on simple heuristics, such as Most Frequently Used (MFU) and Most Recently Used (MRU)~\cite{shin2012understanding}, but these approaches fail to capture sequential dependencies and evolving user behavior.

To address this, probabilistic models such as Markov chains~\cite{natarajan2013app}, Bayesian networks~\cite{zou2013prophet}, and Tree-Augmented Naive Bayes (TAN)~\cite{baeza2015predicting} were introduced, enabling higher-order transitions and improved temporal modeling. However, these methods remain limited in handling long-range dependencies.

Deep learning has significantly advanced app usage prediction, with LSTM-based and GRU-based models demonstrating strong performance by capturing sequential dependencies~\cite{yang2023atpp, hidasi2015session, alruban2022prediction}. To further enhance predictive accuracy, studies have incorporated contextual features, including time and location~\cite{jiang2019using, wang2021app2vec, khaokaew2021cosem}, and explored multi-task learning to jointly model app usage and related behaviors~\cite{xia2020deepapp, zeng2024ddhcn}.

Graph-based models have also been employed to better represent app-location-time relationships~\cite{yu2020semantic, zhou2020graph, ouyang2022learning, wu2019session, shen2023ginapp,huang2025predicting}. For example, DUGN~\cite{ouyang2022learning} explicitly models app transitions using a graph structure, while GINApp~\cite{shen2023ginapp} utilizes Graph Isomorphism Networks for session-based recommendations.  Despite their effectiveness, these methods often require large-scale data to generalize well.

Transformer-based architectures have recently emerged as state-of-the-art solutions. Appformer~\cite{sun2025appformer} employs self-attention mechanisms to capture global dependencies, while MAPLE~\cite{khaokaew2024maple} leverages LLM embeddings to address cold-start issues. However, these models often rely on large-scale multi-modal data, making them less effective in data-scarce environments. Additionally, they lack explicit temporal feature modeling, which is crucial for app usage prediction.

To address these limitations, we propose Atten-Transformer, which integrates feature encoding and temporal encoding to improve representation learning. By employing adaptive attention mechanisms, our model dynamically assigns importance to key app usage moments, capturing both long-term dependencies and short-term contextual shifts. Our approach achieves superior performance in app usage prediction, particularly in sparse data environments.