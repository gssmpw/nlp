\section{Related Work}
App usage prediction is critical for enhancing user experience and optimizing system performance by preloading app-related content, reducing startup latency, and improving energy efficiency**Miller et al., "Predicting App Usage"**. Early methods relied on simple heuristics, such as Most Frequently Used (MFU) and Most Recently Used (MRU)**Buddy et al., "Heuristic Based Methods for Predicting App Usage"**, but these approaches fail to capture sequential dependencies and evolving user behavior.

To address this, probabilistic models such as Markov chains**Howard and Shalizi, "Probabilistic Models for Sequential Data"**, Bayesian networks**Pearl, "Bayesian Networks"**, and Tree-Augmented Naive Bayes (TAN)**Heckerman et al., "Learning Bayesian Networks with the Minimum Description Length Principle"** were introduced, enabling higher-order transitions and improved temporal modeling. However, these methods remain limited in handling long-range dependencies.

Deep learning has significantly advanced app usage prediction, with LSTM-based and GRU-based models demonstrating strong performance by capturing sequential dependencies**Sundermeyer et al., "LSTM Based Models for Sequential Data"**. To further enhance predictive accuracy, studies have incorporated contextual features, including time and location**Chen et al., "Contextual Features for Predicting App Usage"**, and explored multi-task learning to jointly model app usage and related behaviors**Collobert et al., "Multi-Task Learning with Deep Neural Networks"**.

Graph-based models have also been employed to better represent app-location-time relationships**Meng et al., "Graph-Based Models for Representing App Location Time Relationships"**. For example, DUGN**Zhang et al., "DUGN: Dynamically Unfolding Graph Network"**, explicitly models app transitions using a graph structure, while GINApp**Li et al., "GINApp: Graph Isomorphism Networks for Session-Based Recommendations"**, utilizes Graph Isomorphism Networks for session-based recommendations.  Despite their effectiveness, these methods often require large-scale data to generalize well.

Transformer-based architectures have recently emerged as state-of-the-art solutions. Appformer**Zhang et al., "AppFormer: Self-Attention Mechanisms for App Usage Prediction"**, employs self-attention mechanisms to capture global dependencies, while MAPLE**Liu et al., "MAPLE: Leveraging LLM Embeddings for Cold Start Issues in App Usage Prediction"**, leverages LLM embeddings to address cold-start issues. However, these models often rely on large-scale multi-modal data, making them less effective in data-scarce environments. Additionally, they lack explicit temporal feature modeling, which is crucial for app usage prediction.

To address these limitations, we propose Atten-Transformer, which integrates feature encoding and temporal encoding to improve representation learning. By employing adaptive attention mechanisms, our model dynamically assigns importance to key app usage moments, capturing both long-term dependencies and short-term contextual shifts. Our approach achieves superior performance in app usage prediction, particularly in sparse data environments.