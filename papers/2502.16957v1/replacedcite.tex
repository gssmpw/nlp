\section{Related Work}
App usage prediction is critical for enhancing user experience and optimizing system performance by preloading app-related content, reducing startup latency, and improving energy efficiency____. Early methods relied on simple heuristics, such as Most Frequently Used (MFU) and Most Recently Used (MRU)____, but these approaches fail to capture sequential dependencies and evolving user behavior.

To address this, probabilistic models such as Markov chains____, Bayesian networks____, and Tree-Augmented Naive Bayes (TAN)____ were introduced, enabling higher-order transitions and improved temporal modeling. However, these methods remain limited in handling long-range dependencies.

Deep learning has significantly advanced app usage prediction, with LSTM-based and GRU-based models demonstrating strong performance by capturing sequential dependencies____. To further enhance predictive accuracy, studies have incorporated contextual features, including time and location____, and explored multi-task learning to jointly model app usage and related behaviors____.

Graph-based models have also been employed to better represent app-location-time relationships____. For example, DUGN____ explicitly models app transitions using a graph structure, while GINApp____ utilizes Graph Isomorphism Networks for session-based recommendations.  Despite their effectiveness, these methods often require large-scale data to generalize well.

Transformer-based architectures have recently emerged as state-of-the-art solutions. Appformer____ employs self-attention mechanisms to capture global dependencies, while MAPLE____ leverages LLM embeddings to address cold-start issues. However, these models often rely on large-scale multi-modal data, making them less effective in data-scarce environments. Additionally, they lack explicit temporal feature modeling, which is crucial for app usage prediction.

To address these limitations, we propose Atten-Transformer, which integrates feature encoding and temporal encoding to improve representation learning. By employing adaptive attention mechanisms, our model dynamically assigns importance to key app usage moments, capturing both long-term dependencies and short-term contextual shifts. Our approach achieves superior performance in app usage prediction, particularly in sparse data environments.