@article{Johnson2021InvestigatingTE,
  title={Investigating the effects of i-complexity and e-complexity on the learnability of morphological systems},
  author={Tamar Johnson and Kexin Gao and Kenny Smith and Hugh Rabagliati and Jennifer Culbertson},
  journal={J. Lang. Model.},
  year={2021},
  volume={9},
  url={https://api.semanticscholar.org/CorpusID:239502860}
}

@inproceedings{arnett-bergen-2025-language,
    title = "Why do language models perform worse for morphologically complex languages?",
    author = "Arnett, Catherine  and
      Bergen, Benjamin",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.441/",
    pages = "6607--6623",
    abstract = "Language models perform differently across languages. It has been previously suggested that morphological typology may explain some of this variability (Cotterell et al., 2018). We replicate previous analyses and find additional new evidence for a performance gap between agglutinative and fusional languages, where fusional languages, such as English, tend to have better language modeling performance than morphologically more complex languages like Turkish. We then propose and test three possible causes for this performance gap: morphological alignment of tokenizers, tokenization quality, and disparities in dataset sizes and measurement. To test the morphological alignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and supporting datasets for 22 languages. We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment. Instead we find that the performance gap is most reduced when training datasets are of equivalent size across language types, but only when scaled according to the so-called {\textquotedblleft}byte-premium{\textquotedblright}{---}the different encoding efficiencies of different languages and orthographies. These results suggest that languages of particular morphological types are not intrinsically advantaged or disadvantaged in language modeling. Differences in performance can be attributed to disparities in dataset size. These findings bear on ongoing efforts to improve performance for low-performing and under-resourced languages."
}

@inproceedings{borenstein-etal-2024-languages,
    title = "What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages",
    author = "Borenstein, Nadav  and
      Svete, Anej  and
      Chan, Robin  and
      Valvoda, Josef  and
      Nowak, Franz  and
      Augenstein, Isabelle  and
      Chodroff, Eleanor  and
      Cotterell, Ryan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.807",
    doi = "10.18653/v1/2024.acl-long.807",
    pages = "15115--15134",
    abstract = "What can large language models learn? By definition, language models (LM) are distributionsover strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf{---}learning probabilistic languages{---}rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.",
}

@misc{chang2024goldfishmonolinguallanguagemodels,
      title={Goldfish: Monolingual Language Models for 350 Languages}, 
      author={Tyler A. Chang and Catherine Arnett and Zhuowen Tu and Benjamin K. Bergen},
      year={2024},
      eprint={2408.10441},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.10441}, 
}

@article{chomsky2023false,
  author    = {Chomsky, Noam and Roberts, Lan and Watumull, Jeffrey},
  title     = {Noam {C}homsky: The False Promise of {ChatGPT}},
  year      = {2023},
  journal   = {The New York Times},
  url       = {https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html},
  note      = {Accessed: 2024-12-16}
}

@inproceedings{cotterell-etal-2018-languages,
    title = "Are All Languages Equally Hard to Language-Model?",
    author = "Cotterell, Ryan  and
      Mielke, Sabrina J.  and
      Eisner, Jason  and
      Roark, Brian",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2085",
    doi = "10.18653/v1/N18-2085",
    pages = "536--541",
    abstract = "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",
}

@inproceedings{davis-van-schijndel-2020-recurrent,
    title = "Recurrent Neural Network Language Models Always Learn {E}nglish-Like Relative Clause Attachment",
    author = "Davis, Forrest  and
      van Schijndel, Marten",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.179",
    doi = "10.18653/v1/2020.acl-main.179",
    pages = "1979--1990",
    abstract = "A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",
}

@inproceedings{de-dios-flores-etal-2023-dependency,
    title = "Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies",
    author = "de-Dios-Flores, Iria  and
      Garcia Amboage, Juan  and
      Garcia, Marcos",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.12",
    doi = "10.18653/v1/2023.acl-long.12",
    pages = "203--222",
    abstract = "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as {`}Jos{\'e} le prometi{\'o}/orden{\'o} a Mar{\'\i}a ser ordenado/a{'} ({`}Joseph promised/ordered Mary to be tidy{'}). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models{'} ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
}

@article{gibson2019efficiency,
  title={How efficiency shapes human language},
  author={Gibson, Edward and Futrell, Richard and Piantadosi, Steven P and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  journal={Trends in cognitive sciences},
  volume={23},
  number={5},
  pages={389--407},
  year={2019},
  publisher={Elsevier},
URL ={https://www.sciencedirect.com/science/article/abs/pii/S1364661319300580}
}

@article{gil2008complex,
  title={How complex are isolating languages},
  author={Gil, David},
  journal={Language},
  year={2008},
URL ={https://d1wqtxts1xzle7.cloudfront.net/32571891/Language_Complexity_Typology_contact_change-libre.pdf?1394309799=&response-content-disposition=inline%3B+filename%3D32571891.pdf&Expires=1734447625&Signature=THrHZKCaxiMyMg3Exemi1UHOOlKWu9pswErm3jXa~ScIG3WsrZZgQo06wBsKFqg6m9UEpf0LQEy8TZYTj-6USN2jXRLQP0xiolOKJg9QJebMDw7RnBng3WFfXoNc0dJQKfmOeYnf-tvXf0AI29XrYO73s6e0hPMrCOAa0bmouhZuIg7~O1dM~lJpNe2fDH7aQEuxXpvEIKNs8Q1Qhdx1uCREobu3nL6pYhNRJ5Hw4wFP5vMdxzD6i1WmvEED-j3RgDqAcnqaws8fErV8JkXjEaD4K~x-lz3WR3CW0yMSqU0uMY55suQ5Ytjc0Pp3mWCmFWGyff3i1GtyPtFrmqwzFg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=124}
}

@inproceedings{goodkind-bicknell-2018-predictive,
    title = "Predictive power of word surprisal for reading times is a linear function of language model quality",
    author = "Goodkind, Adam  and
      Bicknell, Klinton",
    editor = "Sayeed, Asad  and
      Jacobs, Cassandra  and
      Linzen, Tal  and
      van Schijndel, Marten",
    booktitle = "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2018)",
    month = jan,
    year = "2018",
    address = "Salt Lake City, Utah",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-0102",
    doi = "10.18653/v1/W18-0102",
    pages = "10--18",
}

@inproceedings{hu-levy-2023-prompting,
    title = "Prompting is not a substitute for probability measurements in large language models",
    author = "Hu, Jennifer  and
      Levy, Roger",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.306",
    doi = "10.18653/v1/2023.emnlp-main.306",
    pages = "5040--5060",
    abstract = "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models{'} probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models{'} linguistic knowledge. Broadly, we find that LLMs{'} metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
}

@article{joseph2012all,
  title={`{A}ll Languages Are Equally Complex'.},
  author={Joseph, John E and Newmeyer, Frederick J},
  journal={Historiographia linguistica},
  volume={39},
  year={2012},
url={https://openurl.ebsco.com/EPDB%3Agcd%3A13%3A31083997/detailv2?sid=ebsco%3Aplink%3Ascholar&id=ebsco%3Agcd%3A84334839&crl=c&link_origin=scholar.google.com}
}

@inproceedings{kallini-etal-2024-mission,
    title = "Mission: Impossible Language Models",
    author = "Kallini, Julie  and
      Papadimitriou, Isabel  and
      Futrell, Richard  and
      Mahowald, Kyle  and
      Potts, Christopher",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.787",
    doi = "10.18653/v1/2024.acl-long.787",
    pages = "14691--14714",
    abstract = "Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.",
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@article{kirov-cotterell-2018-recurrent,
    title = "Recurrent Neural Networks in Linguistic Theory: Revisiting {Pinker and Prince} (1988) and the Past Tense Debate",
    author = "Kirov, Christo  and
      Cotterell, Ryan",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1045",
    doi = "10.1162/tacl_a_00247",
    pages = "651--665",
    abstract = "Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland{'}s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince{'}s criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.",
}

@article{koplenig2023languages,
  title={Languages with more speakers tend to be harder to (machine-) learn},
  author={Koplenig, Alexander and Wolfer, Sascha},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={18521},
URL = "https://www.nature.com/articles/s41598-023-45373-z#:~:text=Using%20advanced%20machine%20learning%20techniques,such%20as%20geographic%20range%20size.",
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{kuribayashi-etal-2024-psychometric,
    title = "Psychometric Predictive Power of Large Language Models",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Baldwin, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.129/",
    doi = "10.18653/v1/2024.findings-naacl.129",
    pages = "1983--2005",
    abstract = "Instruction tuning aligns the response of large language models (LLMs) with human preferences.Despite such efforts in human{--}LLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs.In addition, we explore prompting methodologies for simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve psychometric predictive power, but are still inferior to small base models.These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, pure next-word probability remains a strong predictor for human reading behavior, even in the age of LLMs."
}

@article{kuribayashi2025large,
  title={Large Language Models Are Human-Like Internally},
  author={Kuribayashi, Tatsuki and Oseki, Yohei and Taieb, Souhaib Ben and Inui, Kentaro and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2502.01615},
  url={https://arxiv.org/abs/2502.01615#:~:text=Recent%20cognitive%20modeling%20studies%20have,claims%20of%20their%20cognitive%20implausibility.},
  year={2025}
}

@article{mcwhorter2001worlds,
url = {https://doi.org/10.1515/lity.2001.001},
 title={The worlds simplest grammars are creole grammars},
  author={McWhorter, John H},
pages = {125--166},
volume = {5},
number = {2-3},
journal = {Linguistic Typology },
doi = {doi:10.1515/lity.2001.001},
year = {2001},
lastchecked = {2024-12-17}
}

@book{mcwhorter2011linguistic,
  title={Linguistic simplicity and complexity: Why do languages undress?},
  author={McWhorter, John H},
  volume={1},
  year={2011},
  publisher={Walter de Gruyter}
}

@inproceedings{meister-etal-2021-revisiting,
    title = "Revisiting the {U}niform {I}nformation {D}ensity Hypothesis",
    author = {Meister, Clara  and
      Pimentel, Tiago  and
      Haller, Patrick  and
      J{\"a}ger, Lena  and
      Cotterell, Ryan  and
      Levy, Roger},
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.74",
    doi = "10.18653/v1/2021.emnlp-main.74",
    pages = "963--980",
    abstract = "The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal{---}or lack thereof{---}should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID{'}s predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document{---}a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.",
}

@inproceedings{mielke-etal-2019-kind,
    title = "What Kind of Language Is Hard to Language-Model?",
    author = "Mielke, Sabrina J.  and
      Cotterell, Ryan  and
      Gorman, Kyle  and
      Roark, Brian  and
      Eisner, Jason",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1491",
    doi = "10.18653/v1/P19-1491",
    pages = "4975--4989",
    abstract = "How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that {``}translationese{''} is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.",
}

@article{moro2023large,
  title={Large languages, impossible languages and human brains},
  author={Moro, Andrea and Greco, Matteo and Cappa, Stefano F},
  journal={Cortex},
  volume={167},
  pages={82--85},
  year={2023},
  publisher={Elsevier},
URL ={https://www.sciencedirect.com/science/article/abs/pii/S0010945223001752}
}

@article{newmeyer2021complexity,
  title={Complexity and relative complexity in generative grammar},
  author={Newmeyer, Frederick J},
  journal={Frontiers in Communication},
  volume={6},
  pages={614352},
  year={2021},
  publisher={Frontiers Media SA},
URL ={https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2021.614352/full}
}

@article{oh-schuler-2023-surprisal,
    title = "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
    author = "Oh, Byung-Doh  and
      Schuler, William",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.20",
    doi = "10.1162/tacl_a_00548",
    pages = "336--350",
    abstract = "This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to {`}memorize{'} sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
}

@inproceedings{oh-schuler-2023-transformer,
    title = "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
    author = "Oh, Byung-Doh  and
      Schuler, William",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.128",
    doi = "10.18653/v1/2023.findings-emnlp.128",
    pages = "1915--1921",
    abstract = "Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a {`}tipping point{'} at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.",
}

@inproceedings{pearl2011far,
  title={How far can indirect evidence take us? {A}naphoric one revisited},
  author={Pearl, Lisa and Mis, Benjamin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={33},
  number={33},
  year={2011},
URL ={https://sites.socsci.uci.edu/~lpearl/courses/readings/PearlMis2011_AnaOne_CogSciConf.pdf}
}

@inproceedings{tsipidi-etal-2024-surprise,
    title = "Surprise! {U}niform {I}nformation {D}ensity Isn{'}t the Whole Story: Predicting Surprisal Contours in Long-form Discourse",
    author = "Tsipidi, Eleftheria  and
      Nowak, Franz  and
      Cotterell, Ryan  and
      Wilcox, Ethan  and
      Giulianelli, Mario  and
      Warstadt, Alex",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1047",
    doi = "10.18653/v1/2024.emnlp-main.1047",
    pages = "18820--18836",
    abstract = "The Uniform Information Density (UID) hypothesis posits that speakers tend to distribute information evenly across linguistic units to achieve efficient communication. Of course, information rate in texts and discourses is not perfectly uniform. While these fluctuations can be viewed as theoretically uninteresting noise on top of a uniform target, another explanation is that UID is not the only functional pressure regulating information content in a language. Speakers may also seek to maintain interest, adhere to writing conventions, and build compelling arguments. In this paper, we propose one such functional pressure; namely that speakers modulate information rate based on location within a hierarchically-structured model of discourse. We term this the Structured Context Hypothesis and test it by predicting the surprisal contours of naturally occurring discourses extracted from large language models using predictors derived from discourse structure. We find that hierarchical predictors are significant predictors of a discourse{'}s information contour and that deeply nested hierarchical predictors are more predictive than shallow ones. This work takes an initial step beyond UID to propose testable hypotheses for why the information rate fluctuates in predictable ways.",
}

@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}

@article{wilcox-etal-2023-testing,
    title = "Testing the Predictions of Surprisal Theory in 11 Languages",
    author = "Wilcox, Ethan G.  and
      Pimentel, Tiago  and
      Meister, Clara  and
      Cotterell, Ryan  and
      Levy, Roger P.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.82",
    doi = "10.1162/tacl_a_00612",
    pages = "1451--1470",
    abstract = "Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.",
}

@inproceedings{yao-koller-2022-structural,
    title = "Structural generalization is hard for sequence-to-sequence models",
    author = "Yao, Yuekun  and
      Koller, Alexander",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.337",
    doi = "10.18653/v1/2022.emnlp-main.337",
    pages = "5048--5062",
    abstract = "Sequence-to-sequence (seq2seq) models have been successful across many NLP tasks,including ones that require predicting linguistic structure. However, recent work on compositional generalization has shown that seq2seq models achieve very low accuracy in generalizing to linguistic structures that were not seen in training. We present new evidence that this is a general limitation of seq2seq models that is present not just in semantic parsing, but also in syntactic parsing and in text-to-text tasks, and that this limitation can often be overcome by neurosymbolic models that have linguistic knowledge built in. We further report on some experiments that give initial answers on the reasons for these limitations.",
}

