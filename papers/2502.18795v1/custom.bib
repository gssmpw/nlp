@inproceedings{kallini-etal-2024-mission,
    title = "Mission: Impossible Language Models",
    author = "Kallini, Julie  and
      Papadimitriou, Isabel  and
      Futrell, Richard  and
      Mahowald, Kyle  and
      Potts, Christopher",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.787",
    doi = "10.18653/v1/2024.acl-long.787",
    pages = "14691--14714",
    abstract = "Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.",
}



@article{futrell2025linguistics,
  title={How Linguistics Learned to Stop Worrying and Love the Language Models},
  author={Futrell, Richard and Mahowald, Kyle},
url={https://arxiv.org/abs/2501.17047},
  journal={arXiv preprint arXiv:2501.17047},
  year={2025}
}

@article{fox2024large,
  title={Large Language Models and theoretical linguistics},
  author={Fox, Danny and Katzir, Roni},
  journal={Theoretical Linguistics},
  volume={50},
  number={1-2},
  pages={71--76},
url = {https://www.degruyter.com/document/doi/10.1515/tl-2024-2005/html},
  year={2024},
  publisher={De Gruyter}
}

@book{chomsky2022secrets,
  title={The secrets of words},
  author={Chomsky, Noam and Moro, Andrea},
  year={2022},
  publisher={MIT Press}
}

@article{moro2023embodied,
  title={Embodied syntax: impossible languages and the irreducible difference between humans and machines},
  author={Moro, Andrea},
  journal={Sistemi intelligenti},
  volume={35},
  number={2},
  pages={321--328},
  year={2023},
  publisher={Societ{\`a} editrice il Mulino}
}


@article{rawski2023modern,
  title={Modern language models refute nothing},
  author={Rawski, Jon and Baumont, J},
  journal={Lingbuzz Preprint},
  year={2023}
}


@article{bolhuis2024three,
  title={Three reasons why AI doesn't model human language},
  author={Bolhuis, Johan J and Crain, Stephen and Fong, Sandiway and Moro, Andrea},
  journal={Nature},
  volume={627},
  number={8004},
  pages={489--489},
  year={2024}
}


@article{kilgarriff2005language,
  title={Language is never, ever, ever, random},
  author={Kilgarriff, Adam},
  year={2005},
  publisher={Walter de Gruyter}
}

@article{cinque2005deriving,
  title={Deriving {G}reenberg's {U}niversal 20 and its exceptions},
  author={Cinque, Guglielmo},
  journal={Linguistic inquiry},
  volume={36},
  number={3},
  url={https://ieeexplore.ieee.org/abstract/document/6787560},
  pages={315--332},
  year={2005},
  publisher={MIT Press}
}



@article{gentner2009some,
  title={Why some spatial semantic categories are harder to learn than others: The typological prevalence hypothesis},
  author={Gentner, Dedre and Bowerman, Melissa},
  journal={Crosslinguistic approaches to the psychology of language: Research in the tradition of Dan Isaac Slobin},
  volume={465},
url={https://www.semanticscholar.org/paper/Why-some-spatial-semantic-categories-are-harder-to-Gentner-Bowerman/e3fd1e505c5d969c892cb39899b15f9967a1e7c9},
  pages={480},
  year={2009}
}

@inproceedings{cotterell-etal-2018-languages,
    title = "Are All Languages Equally Hard to Language-Model?",
    author = "Cotterell, Ryan  and
      Mielke, Sabrina J.  and
      Eisner, Jason  and
      Roark, Brian",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2085",
    doi = "10.18653/v1/N18-2085",
    pages = "536--541",
    abstract = "For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.",
}

@inproceedings{mielke-etal-2019-kind,
    title = "What Kind of Language Is Hard to Language-Model?",
    author = "Mielke, Sabrina J.  and
      Cotterell, Ryan  and
      Gorman, Kyle  and
      Roark, Brian  and
      Eisner, Jason",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1491",
    doi = "10.18653/v1/P19-1491",
    pages = "4975--4989",
    abstract = "How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that {``}translationese{''} is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.",
}

@article{koplenig2023large,
  title={A large quantitative analysis of written language challenges the idea that all languages are equally complex},
  author={Koplenig, Alexander and Wolfer, Sascha and Meyer, Peter},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={15351},
doi = "https://www.nature.com/articles/s41598-023-42327-3",
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{bugliarello2020s,
  title={It's easier to translate out of english than into it: Measuring neural translation difficulty by cross-mutual information},
  author={Bugliarello, Emanuele and Mielke, Sabrina J and Anastasopoulos, Antonios and Cotterell, Ryan and Okazaki, Naoaki},
  journal={arXiv preprint arXiv:2005.02354},
  year={2020}
}

@article{koplenig2023languages,
  title={Languages with more speakers tend to be harder to (machine-) learn},
  author={Koplenig, Alexander and Wolfer, Sascha},
  journal={Scientific Reports},
  volume={13},
  number={1},
  pages={18521},
URL = "https://www.nature.com/articles/s41598-023-45373-z#:~:text=Using%20advanced%20machine%20learning%20techniques,such%20as%20geographic%20range%20size.",
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@book{trudgill2011sociolinguistic,
  title={Sociolinguistic typology: Social determinants of linguistic complexity},
  author={Trudgill, Peter},
  year={2011},
  publisher={Oxford University Press, USA}
}

@inproceedings{schwenk-etal-2021-ccmatrix,
    title = "{CCM}atrix: Mining Billions of High-Quality Parallel Sentences on the Web",
    author = "Schwenk, Holger  and
      Wenzek, Guillaume  and
      Edunov, Sergey  and
      Grave, Edouard  and
      Joulin, Armand  and
      Fan, Angela",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.507",
    doi = "10.18653/v1/2021.acl-long.507",
    pages = "6490--6500",
    abstract = "We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences. We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences. Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English. We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark. Further, we evaluate on competitive translation benchmarks such as WMT and WAT. Using only mined bitext, we set a new state of the art for a single system on the WMT{'}19 test set for English-German/Russian/Chinese. In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT{'}19 systems, which train on the WMT training data and augment it with backtranslation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop. All of the mined bitext will be freely available.",
}

@inproceedings{reimers-gurevych-2020-making,
    title = "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.365",
    doi = "10.18653/v1/2020.emnlp-main.365",
    pages = "4512--4525",
    abstract = "We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.",
}

@article{christodouloupoulos2015massively,
  title={A massively parallel corpus: the {B}ible in 100 languages},
  author={Christodouloupoulos, Christos and Steedman, Mark},
  journal={Language resources and evaluation},
  volume={49},
URL = {https://link.springer.com/article/10.1007/s10579-014-9287-y},
  pages={375--395},
  year={2015},
  publisher={Springer}
}

@inproceedings{lison-tiedemann-2016-opensubtitles2016,
    title = "{O}pen{S}ubtitles2016: Extracting Large Parallel Corpora from Movie and {TV} Subtitles",
    author = {Lison, Pierre  and
      Tiedemann, J{\"o}rg},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1147",
    pages = "923--929",
    abstract = "We present a new major release of the OpenSubtitles collection of parallel corpora. The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages. The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.",
}

@inproceedings{elkishky_ccaligned_2020,
 author = {El-Kishky, Ahmed and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Koehn, Philipp},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)},
 month = {November},
 title = {{CCAligned}: A Massive Collection of Cross-lingual Web-Document Pairs},
 year = {2020},
 address = "Online",
 publisher = "Association for Computational Linguistics",
 url = "https://www.aclweb.org/anthology/2020.emnlp-main.480",
 doi = "10.18653/v1/2020.emnlp-main.480",
 pages = "5960--5969"
}


@inproceedings{tiedemann-2012-parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
    abstract = "This paper presents the current status of OPUS, a growing language resource of parallel corpora and related tools. The focus in OPUS is to provide freely available data sets in various formats together with basic annotation to be useful for applications in computational linguistics, translation studies and cross-linguistic corpus studies. In this paper, we report about new data sets and their features, additional annotation tools and models provided from the website and essential interfaces and on-line services included in the project.",
}

@inproceedings{qi-etal-2020-stanza,
    title = "{S}tanza: A {P}ython Natural Language Processing Toolkit for Many Human Languages",
    author = "Qi, Peng  and
      Zhang, Yuhao  and
      Zhang, Yuhui  and
      Bolton, Jason  and
      Manning, Christopher D.",
    editor = "Celikyilmaz, Asli  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-demos.14",
    doi = "10.18653/v1/2020.acl-demos.14",
    pages = "101--108",
    abstract = "We introduce Stanza, an open-source Python natural language processing toolkit supporting 66 human languages. Compared to existing widely used toolkits, Stanza features a language-agnostic fully neural pipeline for text analysis, including tokenization, multi-word token expansion, lemmatization, part-of-speech and morphological feature tagging, dependency parsing, and named entity recognition. We have trained Stanza on a total of 112 datasets, including the Universal Dependencies treebanks and other multilingual corpora, and show that the same neural architecture generalizes well and achieves competitive performance on all languages tested. Additionally, Stanza includes a native Python interface to the widely used Java Stanford CoreNLP software, which further extends its functionality to cover other tasks such as coreference resolution and relation extraction. Source code, documentation, and pretrained models for 66 languages are available at \url{https://stanfordnlp.github.io/stanza/}.",
}

@inproceedings{bentz-etal-2016-comparison,
    title = "A Comparison Between Morphological Complexity Measures: Typological Data vs. Language Corpora",
    author = "Bentz, Christian  and
      Ruzsics, Tatyana  and
      Koplenig, Alexander  and
      Samard{\v{z}}i{\'c}, Tanja",
    editor = "Brunato, Dominique  and
      Dell{'}Orletta, Felice  and
      Venturi, Giulia  and
      Fran{\c{c}}ois, Thomas  and
      Blache, Philippe",
    booktitle = "Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC})",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4117",
    pages = "142--153",
    abstract = "Language complexity is an intriguing phenomenon argued to play an important role in both language learning and processing. The need to compare languages with regard to their complexity resulted in a multitude of approaches and methods, ranging from accounts targeting specific structural features to global quantification of variation more generally. In this paper, we investigate the degree to which morphological complexity measures are mutually correlated in a sample of more than 500 languages of 101 language families. We use human expert judgements from the World Atlas of Language Structures (WALS), and compare them to four quantitative measures automatically calculated from language corpora. These consist of three previously defined corpus-derived measures, which are all monolingual, and one new measure based on automatic word-alignment across pairs of languages. We find strong correlations between all the measures, illustrating that both expert judgements and automated approaches converge to similar complexity ratings, and can be used interchangeably.",
}

@inproceedings{NEURIPS2020_1457c0d6,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{stefan_schweter_2020_4275046,
author       = {Stefan Schweter},
  title        = {German {GPT}-2 Model (dbmdz/german-gpt2)},
  year         = {2020},
  howpublished = {\url{https://huggingface.co/dbmdz/german-gpt2}},
  note         = {Accessed: 2025-01-05}
}


@misc{ostendorff2023gpt2wechsel,
  author = {Malte Ostendorff},
  title = {GPT2-XL-Wechsel-German},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = {\url{https://huggingface.co/malteos/gpt2-xl-wechsel-german}}
}

@inproceedings{zmitrovich-etal-2024-family,
    title = "A Family of Pretrained Transformer Language Models for {R}ussian",
    author = "Zmitrovich, Dmitry  and
      Abramov, Aleksandr  and
      Kalmykov, Andrey  and
      Kadulin, Vitaly  and
      Tikhonova, Maria  and
      Taktasheva, Ekaterina  and
      Astafurov, Danil  and
      Baushenko, Mark  and
      Snegirev, Artem  and
      Shavrina, Tatiana  and
      Markov, Sergei S.  and
      Mikhailov, Vladislav  and
      Fenogenova, Alena",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.45",
    pages = "507--524",
    abstract = "Transformer language models (LMs) are fundamental to NLP research methodologies and applications in various languages. However, developing such models specifically for the Russian language has received little attention. This paper introduces a collection of 13 Russian Transformer LMs, which spans encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) architectures. We provide a report on the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we aim to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.",
}

@inproceedings{simoulin-crabbe-2021-un,
    title = "Un mod{\`e}le Transformer G{\'e}n{\'e}ratif Pr{\'e}-entrain{\'e} pour le{\_}{\_}{\_}{\_}{\_}{\_} fran{\c{c}}ais (Generative Pre-trained Transformer in{\_}{\_}{\_}{\_}{\_}{\_} ({F}rench) We introduce a {F}rench adaptation from the well-known {GPT} model)",
    author = "Simoulin, Antoine  and
      Crabb{\'e}, Benoit",
    editor = "Denis, Pascal  and
      Grabar, Natalia  and
      Fraisse, Amel  and
      Cardon, R{\'e}mi  and
      Jacquemin, Bernard  and
      Kergosien, Eric  and
      Balvet, Antonio",
    booktitle = "Actes de la 28e Conf{\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\'e}rence principale",
    month = "6",
    year = "2021",
    address = "Lille, France",
    publisher = "ATALA",
    url = "https://aclanthology.org/2021.jeptalnrecital-taln.24",
    pages = "246--255",
    abstract = "Nous proposons une adaptation en fran{\c{c}}ais du fameux mod{\`e}le Generative Pre-trained Transformer (GPT). Ce dernier appartient {\`a} la cat{\'e}gorie des architectures transformers qui ont significativement transform{\'e} les m{\'e}thodes de traitement automatique du langage. Ces architectures sont en particulier pr{\'e}-entra{\^\i}n{\'e}es sur des t{\^a}ches auto-supervis{\'e}es et sont ainsi sp{\'e}cifiques pour une langue donn{\'e}e. Si certaines sont disponibles en fran{\c{c}}ais, la plupart se d{\'e}clinent avant tout en anglais. GPT est particuli{\`e}rement efficace pour les t{\^a}ches de g{\'e}n{\'e}ration de texte. Par ailleurs, il est possible de l{'}appliquer {\`a} de nombreux cas d{'}usages. Ses propri{\'e}t{\'e}s g{\'e}n{\'e}ratives singuli{\`e}res permettent de l{'}utiliser dans des conditions originales comme l{'}apprentissage sans exemple qui ne suppose aucune mise {\`a} jour des poids du mod{\`e}le, ou modification de l{'}architecture.",
    language = "French",
}


@inproceedings{Kesgin_2024,
   title={Introducing {cosmosGPT}: Monolingual Training for {Turkish} Language Models},
   url={http://dx.doi.org/10.1109/INISTA62901.2024.10683863},
   DOI={10.1109/inista62901.2024.10683863},
   booktitle={2024 International Conference on INnovations in Intelligent SysTems and Applications (INISTA)},
   publisher={IEEE},
   author={Kesgin, H. Toprak and Yuce, M. Kaan and Dogan, Eren and Uzun, M. Egemen and Uz, Atahan and Seyrek, H. Emre and Zeer, Ahmed and Amasyali, M. Fatih},
   year={2024},
   month=sep, pages={1–6} }


@misc{igeniusai2024italia,
  author = {iGeniusAI},
  title = {Italia-9B-Instruct-v0.1},
  howpublished = {\url{https://huggingface.co/iGeniusAI/Italia-9B-Instruct-v0.1}},
  year = {2024},
  note = {Accessed: 2024-06-17}
}

@article{hudson2005regularizing,
  title={Regularizing unpredictable variation: The roles of adult and child learners in language formation and change},
  author={Hudson Kam, Carla L and Newport, Elissa L},
  journal={Language learning and development},
  volume={1},
  number={2},
url={https://www.tandfonline.com/doi/abs/10.1080/15475441.2005.9684215},
  pages={151--195},
  year={2005},
  publisher={Taylor \& Francis}
}


@article{greenberg1963some,
  title={Some universals of grammar with particular reference to the order of meaningful elements},
  author={Greenberg, Joseph H and others},
  journal={Universals of language},
  volume={2},
  pages={73--113},
  year={1963}
}


@article{kam2009getting,
  title={Getting it right by getting it wrong: When learners change languages},
  author={Kam, Carla L Hudson and Newport, Elissa L},
  journal={Cognitive psychology},
  volume={59},
  number={1},
  pages={30--66},
  year={2009},
  publisher={Elsevier}
}

@inproceedings{arnett-bergen-2025-language,
    title = "Why do language models perform worse for morphologically complex languages?",
    author = "Arnett, Catherine  and
      Bergen, Benjamin",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-main.441/",
    pages = "6607--6623",
    abstract = "Language models perform differently across languages. It has been previously suggested that morphological typology may explain some of this variability (Cotterell et al., 2018). We replicate previous analyses and find additional new evidence for a performance gap between agglutinative and fusional languages, where fusional languages, such as English, tend to have better language modeling performance than morphologically more complex languages like Turkish. We then propose and test three possible causes for this performance gap: morphological alignment of tokenizers, tokenization quality, and disparities in dataset sizes and measurement. To test the morphological alignment hypothesis, we present MorphScore, a tokenizer evaluation metric, and supporting datasets for 22 languages. We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment. Instead we find that the performance gap is most reduced when training datasets are of equivalent size across language types, but only when scaled according to the so-called {\textquotedblleft}byte-premium{\textquotedblright}{---}the different encoding efficiencies of different languages and orthographies. These results suggest that languages of particular morphological types are not intrinsically advantaged or disadvantaged in language modeling. Differences in performance can be attributed to disparities in dataset size. These findings bear on ongoing efforts to improve performance for low-performing and under-resourced languages."
}

@inproceedings{goldman-etal-2024-unpacking,
    title = "Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance",
    author = "Goldman, Omer  and
      Caciularu, Avi  and
      Eyal, Matan  and
      Cao, Kris  and
      Szpektor, Idan  and
      Tsarfaty, Reut",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.134/",
    doi = "10.18653/v1/2024.findings-acl.134",
    pages = "2274--2286",
    abstract = "Despite it being the cornerstone of BPE, the most common tokenization algorithm, the importance of compression in the tokenization process is still unclear. In this paper, we argue for the theoretical importance of compression, that can be viewed as 0-gram language modeling where equal probability is assigned to all tokens. We also demonstrate the empirical importance of compression for downstream success of pre-trained language models. We control the compression ability of several BPE tokenizers by varying the amount of documents available during their training: from 1 million documents to a character-based tokenizer equivalent to no training data at all. We then pre-train English language models based on those tokenizers and fine-tune them over several tasks. We show that there is a correlation between tokenizers' compression and models' downstream performance, suggesting that compression is a reliable intrinsic indicator of tokenization quality. These correlations are more pronounced for generation tasks (over classification) or for smaller models (over large ones). We replicated a representative part of our experiments on Turkish and found similar results, confirming that our results hold for languages with typological characteristics dissimilar to English. We conclude that building better compressing tokenizers is a fruitful avenue for further research and for improving overall model performance."
}

@inproceedings{zouhar-etal-2023-tokenization,
    title = "Tokenization and the Noiseless Channel",
    author = "Zouhar, Vil{\'e}m  and
      Meister, Clara  and
      Gastaldi, Juan  and
      Du, Li  and
      Sachan, Mrinmaya  and
      Cotterell, Ryan",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.284/",
    doi = "10.18653/v1/2023.acl-long.284",
    pages = "5184--5207",
    abstract = "Subword tokenization is a key part of most NLP pipelines. However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution. Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of R{\'e}nyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance. In machine translation, we find that across multiple tokenizers, the R{\'e}nyi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length."
}

@inproceedings{schmidt-etal-2024-tokenization,
    title = "Tokenization Is More Than Compression",
    author = "Schmidt, Craig W  and
      Reddy, Varshini  and
      Zhang, Haoran  and
      Alameddine, Alec  and
      Uzan, Omri  and
      Pinter, Yuval  and
      Tanner, Chris",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.40/",
    doi = "10.18653/v1/2024.emnlp-main.40",
    pages = "678--702",
    abstract = "Tokenization is a foundational step in natural language processing (NLP) tasks, bridging raw text and language models. Existing tokenization approaches like Byte-Pair Encoding (BPE) originate from the field of data compression, and it has been suggested that the effectiveness of BPE stems from its ability to condense text into a relatively small number of tokens. We test the hypothesis that fewer tokens lead to better downstream performance by introducing PathPiece, a new tokenizer that segments a document`s text into the minimum number of tokens for a given vocabulary. Through extensive experimentation we find this hypothesis not to be the case, casting doubt on the understanding of the reasons for effective tokenization. To examine which other factors play a role, we evaluate design decisions across all three phases of tokenization: pre-tokenization, vocabulary construction, and segmentation, offering new insights into the design of effective tokenizers. Specifically, we illustrate the importance of pre-tokenization and the benefits of using BPE to initialize vocabulary construction. We train 64 language models with varying tokenization, ranging in size from 350M to 2.4B parameters, all of which are made publicly available."
}

@inproceedings{liang-etal-2023-xlm,
    title = "{XLM}-{V}: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models",
    author = "Liang, Davis  and
      Gonen, Hila  and
      Mao, Yuning  and
      Hou, Rui  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Zettlemoyer, Luke  and
      Khabsa, Madian",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.813/",
    doi = "10.18653/v1/2023.emnlp-main.813",
    pages = "13142--13152",
    abstract = "Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2{\%} and 5.8{\%} absolute on MasakhaNER and Americas NLI, respectively."
}

@inproceedings{launay-etal-2022-pagnol,
    title = "{PAG}nol: An Extra-Large {F}rench Generative Model",
    author = "Launay, Julien  and
      Tommasone, E.l.  and
      Pannier, Baptiste  and
      Boniface, Fran{\c{c}}ois  and
      Chatelain, Am{\'e}lie  and
      Cappelli, Alessandro  and
      Poli, Iacopo  and
      Seddah, Djam{\'e}",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.455",
    pages = "4275--4284",
}


@inproceedings{antoun-etal-2021-aragpt2,
    title = "{A}ra{GPT}2: Pre-Trained Transformer for {A}rabic Language Generation",
    author = "Antoun, Wissam  and
      Baly, Fady  and
      Hajj, Hazem",
    booktitle = "Proceedings of the Sixth Arabic Natural Language Processing Workshop",
    month = apr,
    year = "2021",
    address = "Kyiv, Ukraine (Virtual)",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.wanlp-1.21",
    pages = "196--207",
}


@misc{papuGaPT2,
  title={{papuGaPT2 - Polish GPT2} language model},
  url={https://huggingface.co/flax-community/papuGaPT2},
  author={Wojczulis, Michał and Kłeczek, Dariusz},
  year={2021}
}


@misc{havinga2023gptneo,
  author       = {Yeb Havinga},
  title        = {{GPT Neo 1.3B} pre-trained on cleaned {Dutch} {mC4}},
  year         = {2023},
  url          = {https://huggingface.co/yhavinga/gpt-neo-1.3B-dutch},
  note         = {Accessed: 2024-12-10}
}


@inproceedings{joulin-etal-2017-bag,
    title = "Bag of Tricks for Efficient Text Classification",
    author = "Joulin, Armand  and
      Grave, Edouard  and
      Bojanowski, Piotr  and
      Mikolov, Tomas",
    editor = "Lapata, Mirella  and
      Blunsom, Phil  and
      Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2068",
    pages = "427--431",
    abstract = "This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.",
}


@inproceedings{chang-etal-2024-multilinguality,
    title = "When Is Multilinguality a Curse? Language Modeling for 250 High- and Low-Resource Languages",
    author = "Chang, Tyler A.  and
      Arnett, Catherine  and
      Tu, Zhuowen  and
      Bergen, Ben",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.236",
    doi = "10.18653/v1/2024.emnlp-main.236",
    pages = "4074--4096",
    abstract = "Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33{\%}. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the {``}curse of multilinguality{''}). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.",
}

@article{ali2024survey,
  title={A Survey of Large Language Models for European Languages},
  author={Ali, Wazir and Pyysalo, Sampo},
  journal={arXiv preprint arXiv:2408.15040},
  year={2024}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}


@article{piantadosi2023modern,
  title={Modern language models refute {Chomsky}’s approach to language},
  author={Piantadosi, Steven T},
  journal={From fieldwork to linguistic theory: A tribute to Dan Everett},
  pages={353--414},
  year={2023},
URL ={https://lingbuzz.net/lingbuzz/007180}
}

@article{morgan1987structural,
  title={Structural packaging in the input to language learning: Contributions of prosodic and morphological marking of phrases to the acquisition of language},
  author={Morgan, James L and Meier, Richard P and Newport, Elissa L},
  journal={Cognitive psychology},
  volume={19},
  number={4},
  pages={498--550},
  year={1987},
  publisher={Elsevier}
}


@article{culbertson2017innovation,
  title={Innovation of word order harmony across development},
  author={Culbertson, Jennifer and Newport, Elissa L},
  journal={Open Mind},
  volume={1},
  number={2},
  url={https://direct.mit.edu/opmi/article/1/2/91/2938/Innovation-of-Word-Order-Harmony-Across},
  pages={91--100},
  year={2017},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{culbertson2015harmonic,
  title={Harmonic biases in child learners: In support of language universals},
  author={Culbertson, Jennifer and Newport, Elissa L},
  journal={Cognition},
  volume={139},
  url={https://www.sciencedirect.com/science/article/abs/pii/S0010027715000372},
  pages={71--82},
  year={2015},
  publisher={Elsevier}
}



@inproceedings{lopes-etal-2024-gloria,
    title = "{G}l{\'o}r{IA}: A Generative and Open Large Language Model for {P}ortuguese",
    author = "Lopes, Ricardo  and
      Magalhaes, Joao  and
      Semedo, David",
    editor = "Gamallo, Pablo  and
      Claro, Daniela  and
      Teixeira, Ant{\'o}nio  and
      Real, Livy  and
      Garcia, Marcos  and
      Oliveira, Hugo Gon{\c{c}}alo  and
      Amaro, Raquel",
    booktitle = "Proceedings of the 16th International Conference on Computational Processing of Portuguese - Vol. 1",
    month = mar,
    year = "2024",
    address = "Santiago de Compostela, Galicia/Spain",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2024.propor-1.45/",
    pages = "441--453"
}

@article{pater2019generative,
  title={Generative linguistics and neural networks at 60: Foundation, friction, and fusion},
  author={Pater, Joe},
  journal={Language},
  volume={95},
  number={1},
  pages={e41--e74},
  year={2019},
url={https://muse.jhu.edu/pub/24/article/719231/summary},
  publisher={Linguistic Society of America}
}

@article{milliere2024language,
  title={Language models as models of language},
  author={Milli{\`e}re, Rapha{\"e}l},
  journal={arXiv preprint arXiv:2408.07144},
  url={https://arxiv.org/abs/2408.07144},
  year={2024}
}

@article{lan2024large,
  title={Large language models and the argument from the poverty of the stimulus},
  author={Lan, Nur and Chemla, Emmanuel and Katzir, Roni},
  journal={Linguistic Inquiry},
  pages={1--56},
  year={2024},
  publisher={MIT Press 255 Main St., 9th Floor, Cambridge, MA 02142, USA journals-info~…},
url={https://direct.mit.edu/ling/article-abstract/doi/10.1162/ling_a_00533/120382/Large-Language-Models-and-the-Argument-from-the?redirectedFrom=fulltext}
}

@article{dentella2024language,
  title={Language in Vivo vs. in Silico: Size Matters but Larger Language Models Still Do Not Comprehend Language on a Par with Humans},
  author={Dentella, Vittoria and Guenther, Fritz and Leivada, Evelina},
URL ={https://arxiv.org/abs/2404.14883},
  journal={arXiv preprint arXiv:2404.14883},
  year={2024}
}

@inproceedings{pearl2011far,
  title={How far can indirect evidence take us? {A}naphoric one revisited},
  author={Pearl, Lisa and Mis, Benjamin},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={33},
  number={33},
  year={2011},
URL ={https://sites.socsci.uci.edu/~lpearl/courses/readings/PearlMis2011_AnaOne_CogSciConf.pdf}
}

@inproceedings{kim-linzen-2020-cogs,
    title = "{COGS}: A Compositional Generalization Challenge Based on Semantic Interpretation",
    author = "Kim, Najoung  and
      Linzen, Tal",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.731",
    doi = "10.18653/v1/2020.emnlp-main.731",
    pages = "9087--9105",
    abstract = "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96{--}99{\%}), but generalization accuracy was substantially lower (16{--}35{\%}) and showed high sensitivity to random seed (+-6{--}8{\%}). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
}

@inproceedings{hu-levy-2023-prompting,
    title = "Prompting is not a substitute for probability measurements in large language models",
    author = "Hu, Jennifer  and
      Levy, Roger",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.306",
    doi = "10.18653/v1/2023.emnlp-main.306",
    pages = "5040--5060",
    abstract = "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models{'} probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models{'} linguistic knowledge. Broadly, we find that LLMs{'} metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
}

@inproceedings{yao-koller-2022-structural,
    title = "Structural generalization is hard for sequence-to-sequence models",
    author = "Yao, Yuekun  and
      Koller, Alexander",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.337",
    doi = "10.18653/v1/2022.emnlp-main.337",
    pages = "5048--5062",
    abstract = "Sequence-to-sequence (seq2seq) models have been successful across many NLP tasks,including ones that require predicting linguistic structure. However, recent work on compositional generalization has shown that seq2seq models achieve very low accuracy in generalizing to linguistic structures that were not seen in training. We present new evidence that this is a general limitation of seq2seq models that is present not just in semantic parsing, but also in syntactic parsing and in text-to-text tasks, and that this limitation can often be overcome by neurosymbolic models that have linguistic knowledge built in. We further report on some experiments that give initial answers on the reasons for these limitations.",
}

@inproceedings{tsipidi-etal-2024-surprise,
    title = "Surprise! {U}niform {I}nformation {D}ensity Isn{'}t the Whole Story: Predicting Surprisal Contours in Long-form Discourse",
    author = "Tsipidi, Eleftheria  and
      Nowak, Franz  and
      Cotterell, Ryan  and
      Wilcox, Ethan  and
      Giulianelli, Mario  and
      Warstadt, Alex",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.1047",
    doi = "10.18653/v1/2024.emnlp-main.1047",
    pages = "18820--18836",
    abstract = "The Uniform Information Density (UID) hypothesis posits that speakers tend to distribute information evenly across linguistic units to achieve efficient communication. Of course, information rate in texts and discourses is not perfectly uniform. While these fluctuations can be viewed as theoretically uninteresting noise on top of a uniform target, another explanation is that UID is not the only functional pressure regulating information content in a language. Speakers may also seek to maintain interest, adhere to writing conventions, and build compelling arguments. In this paper, we propose one such functional pressure; namely that speakers modulate information rate based on location within a hierarchically-structured model of discourse. We term this the Structured Context Hypothesis and test it by predicting the surprisal contours of naturally occurring discourses extracted from large language models using predictors derived from discourse structure. We find that hierarchical predictors are significant predictors of a discourse{'}s information contour and that deeply nested hierarchical predictors are more predictive than shallow ones. This work takes an initial step beyond UID to propose testable hypotheses for why the information rate fluctuates in predictable ways.",
}

@inproceedings{meister-etal-2021-revisiting,
    title = "Revisiting the {U}niform {I}nformation {D}ensity Hypothesis",
    author = {Meister, Clara  and
      Pimentel, Tiago  and
      Haller, Patrick  and
      J{\"a}ger, Lena  and
      Cotterell, Ryan  and
      Levy, Roger},
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.74",
    doi = "10.18653/v1/2021.emnlp-main.74",
    pages = "963--980",
    abstract = "The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal{---}or lack thereof{---}should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID{'}s predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document{---}a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.",
}
@inproceedings{oh-schuler-2023-transformer,
    title = "Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens",
    author = "Oh, Byung-Doh  and
      Schuler, William",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.128",
    doi = "10.18653/v1/2023.findings-emnlp.128",
    pages = "1915--1921",
    abstract = "Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a {`}tipping point{'} at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.",
}

@article{oh-schuler-2023-surprisal,
    title = "Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?",
    author = "Oh, Byung-Doh  and
      Schuler, William",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.20",
    doi = "10.1162/tacl_a_00548",
    pages = "336--350",
    abstract = "This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to {`}memorize{'} sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.",
}

@inproceedings{goodkind-bicknell-2018-predictive,
    title = "Predictive power of word surprisal for reading times is a linear function of language model quality",
    author = "Goodkind, Adam  and
      Bicknell, Klinton",
    editor = "Sayeed, Asad  and
      Jacobs, Cassandra  and
      Linzen, Tal  and
      van Schijndel, Marten",
    booktitle = "Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({CMCL} 2018)",
    month = jan,
    year = "2018",
    address = "Salt Lake City, Utah",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-0102",
    doi = "10.18653/v1/W18-0102",
    pages = "10--18",
}


@article{kirov-cotterell-2018-recurrent,
    title = "Recurrent Neural Networks in Linguistic Theory: Revisiting {Pinker and Prince} (1988) and the Past Tense Debate",
    author = "Kirov, Christo  and
      Cotterell, Ryan",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1045",
    doi = "10.1162/tacl_a_00247",
    pages = "651--665",
    abstract = "Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland{'}s claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince{'}s criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.",
}

@inproceedings{borenstein-etal-2024-languages,
    title = "What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages",
    author = "Borenstein, Nadav  and
      Svete, Anej  and
      Chan, Robin  and
      Valvoda, Josef  and
      Nowak, Franz  and
      Augenstein, Isabelle  and
      Chodroff, Eleanor  and
      Cotterell, Ryan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.807",
    doi = "10.18653/v1/2024.acl-long.807",
    pages = "15115--15134",
    abstract = "What can large language models learn? By definition, language models (LM) are distributionsover strings. Therefore, an intuitive way of addressing the above question is to formalize it as a matter of learnability of classes of distributions over strings. While prior work in this direction focused on assessing the theoretical limits, in contrast, we seek to understand the empirical learnability. Unlike prior empirical work, we evaluate neural LMs on their home turf{---}learning probabilistic languages{---}rather than as classifiers of formal languages. In particular, we investigate the learnability of regular LMs (RLMs) by RNN and Transformer LMs. We empirically test the learnability of RLMs as a function of various complexity parameters of the RLM and the hidden state size of the neural LM. We find that the RLM rank, which corresponds to the size of linear space spanned by the logits of its conditional distributions, and the expected length of sampled strings are strong and significant predictors of learnability for both RNNs and Transformers. Several other predictors also reach significance, but with differing patterns between RNNs and Transformers.",
}

@article{wilcox-etal-2023-testing,
    title = "Testing the Predictions of Surprisal Theory in 11 Languages",
    author = "Wilcox, Ethan G.  and
      Pimentel, Tiago  and
      Meister, Clara  and
      Cotterell, Ryan  and
      Levy, Roger P.",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.82",
    doi = "10.1162/tacl_a_00612",
    pages = "1451--1470",
    abstract = "Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.",
}
@inproceedings{papadimitriou-jurafsky-2023-injecting,
    title = "Injecting structural hints: Using language models to study inductive biases in language learning",
    author = "Papadimitriou, Isabel  and
      Jurafsky, Dan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.563",
    doi = "10.18653/v1/2023.findings-emnlp.563",
    pages = "8402--8413",
    abstract = "Both humans and transformer language models are able to learn language without explicit structural supervision. What cognitive inductive biases make this learning possible? Here, we examine the effect of different inductive learning biases by actively controlling the inductive biases of artificial learners: we structurally bias models by pretraining on synthetic formally-structured data, and evaluate these structural biases by fine-tuning on three typologically-distant human languages: English, Japanese, and Basque. We investigate the effect on downstream language perplexity of three types of inductive bias: 1) recursive, hierarchical processing 2) unrestricted token-token dependencies that can{'}t be modeled by context-free grammars, and 3) a Zipfian power-law vocabulary distribution. We show that complex, non-context-free interactions between tokens form the best inductive biases. Our study leverages the capabilities of transformer models to run controlled language learning experiments that are not possible to run on humans, and surfaces hypotheses about the structures that facilitate language learning in both humans and machines.",
}

@inproceedings{kuribayashi-etal-2024-emergent,
    title = "Emergent Word Order Universals from Cognitively-Motivated Language Models",
    author = "Kuribayashi, Tatsuki  and
      Ueda, Ryo  and
      Yoshida, Ryo  and
      Oseki, Yohei  and
      Briscoe, Ted  and
      Baldwin, Timothy",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.781",
    doi = "10.18653/v1/2024.acl-long.781",
    pages = "14522--14543",
    abstract = "The world{'}s languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) languages typically use postpositions. Explaining the source of such biases is a key goal of linguistics.We study word-order universals through a computational simulation with language models (LMs).Our experiments show that typologically-typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of cognitive biases and predictability (perplexity) can explain many aspects of word-order universals.It also showcases the advantage of cognitively-motivated LMs, typically employed in cognitive modeling, in the simulation of language universals.",
}


@article{gibson2019efficiency,
  title={How efficiency shapes human language},
  author={Gibson, Edward and Futrell, Richard and Piantadosi, Steven P and Dautriche, Isabelle and Mahowald, Kyle and Bergen, Leon and Levy, Roger},
  journal={Trends in cognitive sciences},
  volume={23},
  number={5},
  pages={389--407},
  year={2019},
  publisher={Elsevier},
URL ={https://www.sciencedirect.com/science/article/abs/pii/S1364661319300580}
}


@book{newmeyer2005possible,
  title={Possible and probable languages},
  author={Newmeyer, Frederick J},
  year={2005},
  publisher={Oxford: Oxford}
}

@misc{dumitrescu2024gptneo,
  author = {Dumitrescu, Stefan},
  title = {{GPT-Neo Romanian 780M}},
  year = {2024},
  note = {Available at: \url{https://huggingface.co/dumitrescustefan/gpt-neo-romanian-780m}},
  howpublished = {Hugging Face}
}


@article{kuribayashi2025large,
  title={Large Language Models Are Human-Like Internally},
  author={Kuribayashi, Tatsuki and Oseki, Yohei and Taieb, Souhaib Ben and Inui, Kentaro and Baldwin, Timothy},
  journal={arXiv preprint arXiv:2502.01615},
  url={https://arxiv.org/abs/2502.01615#:~:text=Recent%20cognitive%20modeling%20studies%20have,claims%20of%20their%20cognitive%20implausibility.},
  year={2025}
}



@article{clark-etal-2023-cross,
    title = "A Cross-Linguistic Pressure for {U}niform {I}nformation {D}ensity in Word Order",
    author = "Clark, Thomas Hikaru  and
      Meister, Clara  and
      Pimentel, Tiago  and
      Hahn, Michael  and
      Cotterell, Ryan  and
      Futrell, Richard  and
      Levy, Roger",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.59/",
    doi = "10.1162/tacl_a_00589",
    pages = "1048--1065",
    abstract = "While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1"
}


@article{dryer1992greenbergian,
  title={The {G}reenbergian word order correlations},
  author={Dryer, Matthew S},
  journal={Language},
  volume={68},
  number={1},
  pages={81--138},
  year={1992},
 url={https://www.acsu.buffalo.edu/~dryer/DryerGreenbergian.pdf},
  publisher={Linguistic Society of America}
}

@inproceedings{arnett-etal-2024-bit,
    title = "A Bit of a Problem: Measurement Disparities in Dataset Sizes across Languages",
    author = "Arnett, Catherine  and
      Chang, Tyler A.  and
      Bergen, Benjamin",
    editor = "Melero, Maite  and
      Sakti, Sakriani  and
      Soria, Claudia",
    booktitle = "Proceedings of the 3rd Annual Meeting of the Special Interest Group on Under-resourced Languages @ LREC-COLING 2024",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.sigul-1.1/",
    pages = "1--9",
    abstract = "How should text dataset sizes be compared across languages? Even for content-matched (parallel) corpora, UTF-8 encoded text can require a dramatically different number of bytes for different languages. In our work, we define the byte premium between two languages as the ratio of bytes used to encode content-matched text in those languages. We compute byte premiums for 1155 languages, and we use linear regressions to estimate byte premiums for other languages. We release a tool to obtain byte premiums for any two languages, enabling comparisons of dataset sizes across languages for more equitable multilingual model development and data practices."
}

@article{gerz-etal-2018-language,
    title = "Language Modeling for Morphologically Rich Languages: Character-Aware Modeling for Word-Level Prediction",
    author = "Gerz, Daniela  and
      Vuli{\'c}, Ivan  and
      Ponti, Edoardo  and
      Naradowsky, Jason  and
      Reichart, Roi  and
      Korhonen, Anna",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1032/",
    doi = "10.1162/tacl_a_00032",
    pages = "451--465",
    abstract = "Neural architectures are prominent in the construction of language models (LMs). However, word-level prediction is typically agnostic of subword-level information (characters and character sequences) and operates over a closed vocabulary, consisting of a limited word set. Indeed, while subword-aware models boost performance across a variety of NLP tasks, previous work did not evaluate the ability of these models to assist next-word prediction in language modeling tasks. Such subword-level informed models should be particularly effective for morphologically-rich languages (MRLs) that exhibit high type-to-token ratios. In this work, we present a large-scale LM study on 50 typologically diverse languages covering a wide variety of morphological systems, and offer new LM benchmarks to the community, while considering subword-level information. The main technical contribution of our work is a novel method for injecting subword-level information into semantic word vectors, integrated into the neural language modeling training, to facilitate word-level prediction. We conduct experiments in the LM setting where the number of infrequent words is large, and demonstrate strong perplexity gains across our 50 languages, especially for morphologically-rich languages. Our code and data sets are publicly available."
}

@inproceedings{misra-mahowald-2024-language,
    title = "Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case of the Missing {AANN}s",
    author = "Misra, Kanishka  and
      Mahowald, Kyle",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.53/",
    doi = "10.18653/v1/2024.emnlp-main.53",
    pages = "913--929",
    abstract = "Language models learn rare syntactic phenomena, but the extent to which this is attributable to generalization vs. memorization is a major open question. To that end, we iteratively trained transformer language models on systematically manipulated corpora which were human-scale in size, and then evaluated their learning of a rare grammatical phenomenon: the English Article+Adjective+Numeral+Noun (AANN) construction ({\textquotedblleft}a beautiful five days{\textquotedblright}). We compared how well this construction was learned on the default corpus relative to a counterfactual corpus in which AANN sentences were removed. We found that AANNs were still learned better than systematically perturbed variants of the construction. Using additional counterfactual corpora, we suggest that this learning occurs through generalization from related constructions (e.g., {\textquotedblleft}a few days{\textquotedblright}). An additional experiment showed that this learning is enhanced when there is more variability in the input. Taken together, our results provide an existence proof that LMs can learn rare grammatical phenomena by generalization from less rare phenomena. Data and code: https://github.com/kanishkamisra/aannalysis."
}

@inproceedings{kuribayashi-etal-2021-lower,
    title = "Lower Perplexity is Not Always Human-Like",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Ito, Takumi  and
      Yoshida, Ryo  and
      Asahara, Masayuki  and
      Inui, Kentaro",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.405/",
    doi = "10.18653/v1/2021.acl-long.405",
    pages = "5203--5217",
    abstract = "In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization {---}\textit{the lower perplexity a language model has, the more human-like the language model is}{---} in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models."
}


@inproceedings{kuribayashi-etal-2024-psychometric,
    title = "Psychometric Predictive Power of Large Language Models",
    author = "Kuribayashi, Tatsuki  and
      Oseki, Yohei  and
      Baldwin, Timothy",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-naacl.129/",
    doi = "10.18653/v1/2024.findings-naacl.129",
    pages = "1983--2005",
    abstract = "Instruction tuning aligns the response of large language models (LLMs) with human preferences.Despite such efforts in human{--}LLM alignment, we find that instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs.In addition, we explore prompting methodologies for simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve psychometric predictive power, but are still inferior to small base models.These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, pure next-word probability remains a strong predictor for human reading behavior, even in the age of LLMs."
}

@article{saffran2008grammatical,
  title={Grammatical pattern learning by human infants and cotton-top tamarin monkeys},
  author={Saffran, Jenny and Hauser, Marc and Seibel, Rebecca and Kapfhamer, Joshua and Tsao, Fritz and Cushman, Fiery},
  journal={Cognition},
  volume={107},
  number={2},
  pages={479--500},
  year={2008},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0010027707002697}
}

@article{culbertson2012learning,
  title={Learning biases predict a word order universal},
  author={Culbertson, Jennifer and Smolensky, Paul and Legendre, G{\'e}raldine},
  journal={Cognition},
  volume={122},
  number={3},
  pages={306--329},
  year={2012},
url={https://www.sciencedirect.com/science/article/abs/pii/S0010027711002745},
  publisher={Elsevier}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{singleton2004learners,
  title={When learners surpass their models: The acquisition of {A}merican Sign Language from inconsistent input},
  author={Singleton, Jenny L and Newport, Elissa L},
  journal={Cognitive psychology},
  volume={49},
  number={4},
  pages={370--407},
  year={2004},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S0010028504000295}
}

@inproceedings{barreto-etal-2006-open,
    title = "Open Resources and Tools for the Shallow Processing of {P}ortuguese: The {T}ag{S}hare Project",
    author = "Barreto, Florbela  and
      Branco, Ant{\'o}nio  and
      Ferreira, Eduardo  and
      Mendes, Am{\'a}lia  and
      Nascimento, Maria Fernanda Bacelar do  and
      Nunes, Filipe  and
      Silva, Jo{\~a}o Ricardo",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Gangemi, Aldo  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Odijk, Jan  and
      Tapias, Daniel",
    booktitle = "Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}`06)",
    month = may,
    year = "2006",
    address = "Genoa, Italy",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L06-1177/",
    abstract = "This paper presents the TagShare project and the linguistic resources and tools for the shallow processing of Portuguese developed in its scope. These resources include a 1 million token corpus that has been accurately hand annotated with a variety of linguistic information, as well as several state of the art shallow processing tools capable of automatically producing that type of annotation. At present, the linguistic annotations in the corpus are sentence and paragraph boundaries, token boundaries, morphosyntactic POS categories, values of inflection features, lemmas and namedentities. Hence, the set of tools comprise a sentence chunker, a tokenizer, a POS tagger, nominal and verbal analyzers and lemmatizers, a verbal conjugator, a nominal inflector, and a namedentity recognizer, some of which underline several online services."
}

@article{marcus-etal-1993-building,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    editor = "Hirschberg, Julia",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2004/",
    pages = "313--330"
}

@inproceedings{delmonte2007vit,
  title={{VIT-Venice Italian Treebank}: Syntactic and Quantitative Features.},
  author={Delmonte, Rodolfo and Bristot, Antonella and Tonelli, Sara},
  booktitle={Sixth International Workshop on Treebanks and Linguistic Theories},
  volume={1},
  pages={43--54},
  year={2007},
  url={https://www.researchgate.net/publication/28584827_VIT_-_Venice_Italian_Treebank_Syntactic_and_Quantitative_Features},
  organization={Northern European Association for Language Technol}
}



@book{smith1995mind,
  title={Mind of a Savant: Language, Learning and Modularity},
  author={Tsimpli, Ianthi-Maria and Smith, Neil},
  year={1995},
  publisher={Blackwell}
}

@article{xue2005penn,
  title={The {P}enn {C}hinese treebank: {P}hrase structure annotation of a large corpus},
  author={Xue, Naiwen and Xia, Fei and Chiou, Fu-Dong and Palmer, Marta},
  journal={Natural language engineering},
  volume={11},
  number={2},
  pages={207--238},
  year={2005},
  publisher={Cambridge University Press},
  url={https://www.cambridge.org/core/journals/natural-language-engineering/article/penn-chinese-treebank-phrase-structure-annotation-of-a-large-corpus/26220D5C308A1A65B1D0636AE9A9FC72}
}
@inproceedings{de-dios-flores-etal-2023-dependency,
    title = "Dependency resolution at the syntax-semantics interface: psycholinguistic and computational insights on control dependencies",
    author = "de-Dios-Flores, Iria  and
      Garcia Amboage, Juan  and
      Garcia, Marcos",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.12",
    doi = "10.18653/v1/2023.acl-long.12",
    pages = "203--222",
    abstract = "Using psycholinguistic and computational experiments we compare the ability of humans and several pre-trained masked language models to correctly identify control dependencies in Spanish sentences such as {`}Jos{\'e} le prometi{\'o}/orden{\'o} a Mar{\'\i}a ser ordenado/a{'} ({`}Joseph promised/ordered Mary to be tidy{'}). These structures underlie complex anaphoric and agreement relations at the interface of syntax and semantics, allowing us to study lexically-guided antecedent retrieval processes. Our results show that while humans correctly identify the (un)acceptability of the strings, language models often fail to identify the correct antecedent in non-adjacent dependencies, showing their reliance on linearity. Additional experiments on Galician reinforce these conclusions. Our findings are equally valuable for the evaluation of language models{'} ability to capture linguistic generalizations, as well as for psycholinguistic theories of anaphor resolution.",
}


@inproceedings{lee-schuster-2022-language,
    title = "Can language models capture syntactic associations without surface cues? A case study of reflexive anaphor licensing in {E}nglish control constructions",
    author = "Lee, Soo-Hwan  and
      Schuster, Sebastian",
    editor = "Ettinger, Allyson  and
      Hunter, Tim  and
      Prickett, Brandon",
    booktitle = "Proceedings of the Society for Computation in Linguistics 2022",
    month = feb,
    year = "2022",
    address = "online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.scil-1.18",
    pages = "206--211",
}

@inproceedings{davis-van-schijndel-2020-recurrent,
    title = "Recurrent Neural Network Language Models Always Learn {E}nglish-Like Relative Clause Attachment",
    author = "Davis, Forrest  and
      van Schijndel, Marten",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.179",
    doi = "10.18653/v1/2020.acl-main.179",
    pages = "1979--1990",
    abstract = "A standard approach to evaluating language models analyzes how models assign probabilities to valid versus invalid syntactic constructions (i.e. is a grammatical sentence more probable than an ungrammatical sentence). Our work uses ambiguous relative clause attachment to extend such evaluations to cases of multiple simultaneous valid interpretations, where stark grammaticality differences are absent. We compare model performance in English and Spanish to show that non-linguistic biases in RNN LMs advantageously overlap with syntactic structure in English but not Spanish. Thus, English models may appear to acquire human-like syntactic preferences, while models trained on Spanish fail to acquire comparable human-like preferences. We conclude by relating these results to broader concerns about the relationship between comprehension (i.e. typical language model use cases) and production (which generates the training data for language models), suggesting that necessary linguistic biases are not present in the training signal at all.",
}

@inproceedings{white-cotterell-2021-examining,
    title = "Examining the Inductive Bias of Neural Language Models with Artificial Languages",
    author = "White, Jennifer C.  and
      Cotterell, Ryan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.38",
    doi = "10.18653/v1/2021.acl-long.38",
    pages = "454--463",
    abstract = "Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages.",
}

@article{chomsky2023false,
  author    = {Chomsky, Noam and Roberts, Lan and Watumull, Jeffrey},
  title     = {Noam {C}homsky: The False Promise of {ChatGPT}},
  year      = {2023},
  journal   = {The New York Times},
  url       = {https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html},
  note      = {Accessed: 2024-12-16}
}

@article{culbertson2020learning,
  title={A learning bias for word order harmony: Evidence from speakers of non-harmonic languages},
  author={Culbertson, Jennifer and Franck, Julie and Braquet, Guillaume and Navarro, Magda Barrera and Arnon, Inbal},
  journal={Cognition},
  volume={204},
  pages={104392},
URL = {https://www.sciencedirect.com/science/article/pii/S0010027720302110},
  year={2020},
  publisher={Elsevier}
}

@article{martin2024universal,
  title={A universal cognitive bias in word order: Evidence from speakers whose language goes against it},
  author={Martin, Alexander and Adger, David and Abels, Klaus and Kanampiu, Patrick and Culbertson, Jennifer},
  journal={Psychological Science},
  volume={35},
  number={3},
  pages={304--311},
  year={2024},
  publisher={SAGE Publications Sage CA: Los Angeles, CA},
URL ={https://journals.sagepub.com/doi/full/10.1177/09567976231222836}
}

@article{getz2021biased,
  title={Biased statistical learning of closed-class items},
  author={Getz, Heidi and Newport, Elissa},
 url={https://escholarship.org/content/qt5712299p/qt5712299p_noSplash_2f978b038839ccbad1b7838bc1f06c07.pdf?t=sgitt6},
  year={2021},
  publisher={PsyArXiv}
}


@article{moro2023large,
  title={Large languages, impossible languages and human brains},
  author={Moro, Andrea and Greco, Matteo and Cappa, Stefano F},
  journal={Cortex},
  volume={167},
  pages={82--85},
  year={2023},
  publisher={Elsevier},
URL ={https://www.sciencedirect.com/science/article/abs/pii/S0010945223001752}
}

@article{katzir2023large,
  title={Why large language models are poor theories of human linguistic cognition: A reply to {P}iantadosi},
  author={Katzir, Roni},
  journal={Biolinguistics},
  volume={17},
  pages={1--12},
  year={2023},
URL = {https://bioling.psychopen.eu/index.php/bioling/article/view/13153}
}

@article{kodner2023linguistics,
  title={Why linguistics will thrive in the 21st century: A reply to {Piantadosi} (2023)},
  author={Kodner, Jordan and Payne, Sarah and Heinz, Jeffrey},
  journal={arXiv preprint arXiv:2308.03228},
  year={2023}
}
@misc{chang2024goldfishmonolinguallanguagemodels,
      title={Goldfish: Monolingual Language Models for 350 Languages}, 
      author={Tyler A. Chang and Catherine Arnett and Zhuowen Tu and Benjamin K. Bergen},
      year={2024},
      eprint={2408.10441},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.10441}, 
}

@article{gil2008complex,
  title={How complex are isolating languages},
  author={Gil, David},
  journal={Language},
  year={2008},
URL ={https://d1wqtxts1xzle7.cloudfront.net/32571891/Language_Complexity_Typology_contact_change-libre.pdf?1394309799=&response-content-disposition=inline%3B+filename%3D32571891.pdf&Expires=1734447625&Signature=THrHZKCaxiMyMg3Exemi1UHOOlKWu9pswErm3jXa~ScIG3WsrZZgQo06wBsKFqg6m9UEpf0LQEy8TZYTj-6USN2jXRLQP0xiolOKJg9QJebMDw7RnBng3WFfXoNc0dJQKfmOeYnf-tvXf0AI29XrYO73s6e0hPMrCOAa0bmouhZuIg7~O1dM~lJpNe2fDH7aQEuxXpvEIKNs8Q1Qhdx1uCREobu3nL6pYhNRJ5Hw4wFP5vMdxzD6i1WmvEED-j3RgDqAcnqaws8fErV8JkXjEaD4K~x-lz3WR3CW0yMSqU0uMY55suQ5Ytjc0Pp3mWCmFWGyff3i1GtyPtFrmqwzFg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA#page=124}
}

@book{mcwhorter2011linguistic,
  title={Linguistic simplicity and complexity: Why do languages undress?},
  author={McWhorter, John H},
  volume={1},
  year={2011},
  publisher={Walter de Gruyter}
}

@article{mcwhorter2001worlds,
url = {https://doi.org/10.1515/lity.2001.001},
 title={The worlds simplest grammars are creole grammars},
  author={McWhorter, John H},
pages = {125--166},
volume = {5},
number = {2-3},
journal = {Linguistic Typology },
doi = {doi:10.1515/lity.2001.001},
year = {2001},
lastchecked = {2024-12-17}
}


@article{newmeyer2021complexity,
  title={Complexity and relative complexity in generative grammar},
  author={Newmeyer, Frederick J},
  journal={Frontiers in Communication},
  volume={6},
  pages={614352},
  year={2021},
  publisher={Frontiers Media SA},
URL ={https://www.frontiersin.org/journals/communication/articles/10.3389/fcomm.2021.614352/full}
}


@article{joseph2012all,
  title={`{A}ll Languages Are Equally Complex'.},
  author={Joseph, John E and Newmeyer, Frederick J},
  journal={Historiographia linguistica},
  volume={39},
  year={2012},
url={https://openurl.ebsco.com/EPDB%3Agcd%3A13%3A31083997/detailv2?sid=ebsco%3Aplink%3Ascholar&id=ebsco%3Agcd%3A84334839&crl=c&link_origin=scholar.google.com}
}

@article{Johnson2021InvestigatingTE,
  title={Investigating the effects of i-complexity and e-complexity on the learnability of morphological systems},
  author={Tamar Johnson and Kexin Gao and Kenny Smith and Hugh Rabagliati and Jennifer Culbertson},
  journal={J. Lang. Model.},
  year={2021},
  volume={9},
  url={https://api.semanticscholar.org/CorpusID:239502860}
}

@inproceedings{warstadt-etal-2023-findings,
    title = "Findings of the {B}aby{LM} Challenge: Sample-Efficient Pretraining on Developmentally Plausible Corpora",
    author = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    editor = "Warstadt, Alex  and
      Mueller, Aaron  and
      Choshen, Leshem  and
      Wilcox, Ethan  and
      Zhuang, Chengxu  and
      Ciro, Juan  and
      Mosquera, Rafael  and
      Paranjabe, Bhargavi  and
      Williams, Adina  and
      Linzen, Tal  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.conll-babylm.1",
    doi = "10.18653/v1/2023.conll-babylm.1",
    pages = "1--34",
}