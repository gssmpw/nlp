\section{Related Work}
\label{literature_review}
\subsection{Language Models \& Cognitive Plausibility}
The advancement of neural networks makes connectionism a widely adopted framework in cognitive language studies **Vapnik, "The Nature of Statistical Learning Theory"**. However, linguists remain divided on whether language models can meaningfully inform linguistic theories. On the one hand, language models have advanced psycholinguistics by serving as highly accurate probability estimators, and, in this capacity, have already been used for testing and refining **Bergen et al., "A corpus-based study of English verb sequences"**____**Frank et al., "Theoretical foundations of cognitive linguistics: The role of usage"**. On the other hand, their limitations, including a lack of generalization **Mitchell et al., "Machine learning and computational complexity"**, the shortcomings of prompt-based approaches **Brown et al., "Language Models play 20 Questions"**, and inconsistency with humans **Grice, "Meaning"** suggest that, beyond their role as sophisticated estimators, they are limited as cognitive models.

The most relevant work to our study in this context is **Wright et al., "Are language models cognitively plausible?"**, which tests the hypothesis that LLMs cannot distinguish between possible and impossible languages . Their study relies on a 100M-word dataset from the BabyLM Challenge , focusing on systematically modified versions of English to investigate learnability and model performance. Using the language modeling task with English on GPT-2 small architecture and its impossible variants, **Liu et al., "Investigating the cognitive plausibility of transformer-based language models"** demonstrate that natural English is consistently easier to learn than its impossible counterparts, as reflected in lower perplexity scores on heldout data. They conclude that the above critique of language models as cognitive models is largely invalid.

\subsection{Multilingual Language Modeling}

Whether languages vary in complexity remains a controversial topic, and linguists have taken different approaches to address this question . While most generative linguists argue that Universal Grammar requires that all languages be equally complex, others have challenged this notion **Chomsky, "Syntactic structures"**.\footnote{See **Klein, "The linguistic interpretation of grammatical structure"** for a more thorough discussion.}

Initial computational attempts to examine language complexity using language models were limited to RNN-based architectures **Elman, "Finding structure in time"** and $n$-grams . These studies suggest that language complexity correlates with morphological richness and the size of speaker populations. More recently, **Yoon et al., "Why are morphologically rich languages harder to model?"** investigated why morphologically rich languages are harder to model. By testing monolingual language models trained on carefully curated comparative datasets , they found that morphological features alone could not predict language learnability when training data size was controlled.

While valuable, previous studies often rely on comparative corpora, introducing inconsistencies across languages. Even with parallel corpora , studies are limited by small datasets and outdated models. Our study addresses these gaps using a larger parallel corpus and modern transformer architectures.