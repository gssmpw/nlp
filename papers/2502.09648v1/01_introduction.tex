\section{Introduction}
\label{sec:01_introduction}

\textit{Writing matters}, but reaching a consensus on writing quality standards can be challenging \cite{deane2022importance}. 
Writing evaluation is a complex task that requires significant time and effort from professional evaluators, making it difficult to provide timely feedback to students \cite{finn2024memory}.
To address this issue, various English text analyzers \cite{graesser2004cohmetrix, mcnamara2010linguistic, rei2016sentence, crossley2019taaco} and automated writing evaluation tools \cite{wang2022aessota, jeon2021countering, uto2020neural} have been developed. 
Recent automated writing evaluation systems can produce scores that align closely with human evaluation in certain contexts \cite{beigman2020automated}. 
This success has led to their integration into standardized English tests such as the Test of English as a Foreign Language (TOEFL\footnote{https://www.ets.org/toefl.html}) and the Graduate Record Examination (GRE\footnote{https://www.ets.org/gre.html}), providing both automated and human evaluation scores \cite{beigman2020automated, ramineni2012gre}.

Building on the success of automated English writing evaluation systems, we examine three key factors necessary for practical and widespread use of Korean text analysis and writing evaluation: 
\textit{\textsf{(i) Multi-view analysis.}} Automated writing evaluation should consider multiple perspectives, from low-level (\textit{i.e.}, concrete) analyses such as morpheme analysis and lexical diversity%structural cohesion
, to higher-level (\textit{i.e.}, abstract) analyses such as semantic cohesion and automatic writing evaluation; 
\textit{\textsf{(ii) Error propagation.}} Errors occurring in early stages (\textit{e.g.,} morpheme analysis) should have minimal impact in later stages (\textit{e.g.,} writing evaluation). This is particularly important in Korean, an \textit{agglutinative} language, where frequent morphological changes make it more vulnerable to error propagation \cite{matteson2018rich};
and \textit{\textsf{(iii) Evaluation explainability.}} High-level, abstract evaluation results should be interpretable by humans, who need to understand the reason behind the scores and the features that influenced the results. 
Providing this explainability to users is crucial for ensuring reliability, as these tools have the potential to make mistakes; Unfortunately, existing Korean text analyzers \cite{ryu2019koranlysis, lee2024exploring, kim2024korcat} and automated writing evaluation tools \cite{lee2022argument, lee2023pasta} do not fully meet all these requirements, limiting their practical use.

To address the research gap, we introduce \textsf{UKTA} (\textbf{U}nified \textbf{K}orean \textbf{T}ext \textbf{A}nalyzer), a comprehensive Korean text analysis system for evaluating Korean writing. 
First, we provide accurate low-level analysis based on 
state-of-the-art Korean morpheme
analyzer, which minimizes error propagation. 
In addition to morpheme analysis, we categorize and provide key features, such as lexical richness and semantic cohesion, at the mid-level to enable explainable writing evaluation.
Finally, we present a comprehensive rubric-based writing score as a high-level metric based on a novel attention-based deep learning method and provide the features contributing to that score to enhance explainability and reliability. 
Notably, using all the suggested features improves writing evaluation performance compared to baseline in terms of accuracy and quadratic weighted kappa scores.
To the best of our knowledge, \textsf{UKTA} is the first comprehensive Korean text analysis and writing evaluation tool providing accurate results from a multi-view perspective.

Our contributions are summarized as follows:

\begin{itemize}[leftmargin=1.1em]
\item We introduce \textsf{UKTA}, a comprehensive Korean text analysis and writing evaluation system from multiple perspectives, and present a tool for its practical application.

\item Rather than simply presenting writing evaluation scores, \textsf{UKTA} enhances explainability and reliability by providing detailed feature scores such as morpheme, lexical diversity, and cohesion.

\item Experimental results demonstrate that writing evaluation accuracy improves when the proposed features are considered, compared to scores derived solely from raw text.
\end{itemize} 

The remainder of this paper is organized as follows.
Section \ref{sec:02_relatedworks} reviews the related work.
Section \ref{sec:04_model} introduces the proposed approach in detail.
In Section \ref{sec:05_experiments}, we present the experimental results. Finally, Section \ref{sec:06_conclusion} concludes our work and outlines future work.

