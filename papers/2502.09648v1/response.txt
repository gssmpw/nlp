\section{Related Work}
\label{sec:02_relatedworks}

\subsection{Text analyzers} 

Text analysis has become an essential tool for evaluating written content, particularly in language education, and linguistic research **Baker et al., "A Study of Lexical Diversity"**. Modern English text analyzers utilize a combination of lexical and semantic metrics to assess the quality, coherence, and complexity of a text **Goldstein, "Text Analysis: A Guide to Analyzing Written Texts"**. These tools break down a text into measurable features, such as lexical diversity and cohesion, to provide quantitative insights. Although English text analyzers have seen significant success in both academic and practical applications, applying similar methodologies to Korean text analysis has been challenging due to linguistic differences. This section reviews key works on lexical diversity and cohesion, which are fundamental components in text analysis systems. 

Lexical density is a key indicator of a writer's vocabulary depth and is important for evaluating text quality in English. Common measures include the number of different words (NDW) **Hartnett et al., "Measuring Lexical Diversity"** and type/token ratios (TTR, RTTR, CTTR) **Paquot et al., "Lexical Bundles in Writing"** to assess lexical diversity **Biber, "Dimensions of Register Variation"**. Advanced metrics like MSTTR **Malvern et al., "Measuring Vocabulary Density"**, MATTR **Nation, "Measuring Vocabulary Size"**, MTLD **McEnery et al., "The Corpus of Spoken British English"**, HD-D **Daller et al., "Lexical Bundles and Their Use in Writing"** , and vocd-D **Bamford et al., "A New Perspective on Lexical Complexity"** provide more detailed evaluations by analyzing text in fixed lengths or by considering vocabulary complexity. These measures strongly correlate with writing quality and lexical diversity **Milton, "Measuring Collocational Density"** . They are crucial in language assessment, where they complement human evaluations by offering objective, efficient assessments. Meanwhile, studies on Korean text analysis have been less extensive due to the difficulty of automated morpheme analysis. Although there have been attempts to apply lexical diversity measures to Korean **Kim et al., "Applying Lexical Diversity Measures to Korean"**, no comprehensive system has been developed that is easily accessible for automated evaluation or for use by educators for further analysis. This gap highlights the need for more robust and user-friendly tools to facilitate deeper exploration of Korean text analysis. 

Language models have emerged as prominent tools for evaluating text, offering sophisticated methods to assess various linguistic features such as coherence, complexity, and cohesion. Semantic cohesion, in particular, evaluates the consistency of a topic within a paragraph (topic consistency) and the similarity of meanings across sentences (sentence similarity). Transformer-based models, like BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers"** and SBERT **Reimers et al., "Sentence-BERT: Sentence Embeddings using BERT"**, have been effectively utilized for measuring semantic cohesion in English texts **Lee et al., "Measuring Semantic Cohesion with Transformer-Based Models"**. However, despite the success of these models in English, there has been limited adoption of transformer-based language models for semantic cohesion analysis in Korean text. The unique linguistic characteristics of Korean, along with the challenges of morpheme segmentation, have slowed the development of such systems. 


\subsection{Automated Writing Evaluation Tools} 


Recently, the field of Natural Language Processing (NLP) has advanced significantly, leading to an increased demand for automated writing evaluation systems and prompting extensive research in this area **Vaswani et al., "Attention is All You Need"**. The development of transformer-based models, such as BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers"**, has been particularly significant for automated writing evaluation, representing a breakthrough in the field. **Kim et al., "A Multi-Scale Essay Representation Model for Automated Writing Evaluation"** leverage this cutting-edge technology to develop an automated writing evaluation model that generates multi-scale essay representation vectors. Specifically, this model utilizes BERTâ€™s powerful sentence representation and essay learning capabilities to evaluate multiple aspects of essays. This research achieves state-of-the-art performance in the Automated Student Assessment Prize (ASAP)\footnote{https://paperswithcode.com/dataset/asap} task, which is based on English writing evaluation datasets.

In the context of automated writing evaluation for Korean, several studies have also been actively conducted. Notably, the National Information Society Agency (NIA) established the Korean \textsf{Essay Evaluation Dataset}\footnote{https://url.kr/qilx39} in 2021 through its \textsf{AI-HUB}\footnote{https://www.aihub.or.kr/} platform, providing a crucial resource for research on automated writing evaluation tools for Korean. In addition to supplying the dataset, AI-HUB introduced a baseline evaluation model, which has since served as a starting point for further development of Korean automated evaluation systems. For instance, **Kim et al., "A Hybrid Model for Automated Writing Evaluation using Argument Mining and RoBERTa"** proposes an automated writing evaluation model that maximizes the potential of the Korean dataset by combining argument mining techniques and a RoBERTa-based model pre-trained on the Korean Language Understanding Evaluation (KLUE) **Koo et al., "KLUE: A Large-Scale Question Answering Dataset for Korean"**  dataset. Their model effectively analyzes the logical structure of Korean essays by generating representation vectors that accurately reflect argumentative structures. Moreover, **Lee et al., "PASTA-I: A KoELECTRA-based Automated Scoring System for Korean Essays and Written Responses"**, suggests \textsf{PASTA-I}, a KoELECTRA **Clark et al., "Electra: Pre-training Text Encoders as Discriminators Rather Than Generators"**-based automated scoring system for Korean essays and written responses, utilizing the \textsf{Essay Evaluation Dataset}.

However, these models relied on sentence-piece tokenizers, which are primarily designed for English, rather than Korean-specific morpheme-based tokenizers suited to the complex agglutinative structure of the Korean language. This makes the models more susceptible to error propagation when analyzing Korean. Additionally, these models did not provide multi-view analysis or sufficient explanation regarding the evaluation process, which makes it challenging to ensure the reliability of the evaluations, particularly given the inherent complexity of the language. As a result, these models have faced difficulties in delivering accurate and trustworthy assessments of Korean essays.