\section{Experiments}
\label{sec:05_experiments}
In this section, we validate the effectiveness of the proposed features in the automated writing evaluation system. 
To achieve this, we measure the performance of the automated writing evaluation model depicted in Figure \ref{fig:approach}, and compare it with the baseline model. 
Additionally, we examine the importance of each feature for sample data by analyzing the attention weights from the attention layer.






\subsection{Experimental Settings \label{sec:setting}}
{The purpose of our experiment is to verify whether the feature scores obtained from our text analysis system lead to performance improvements when incorporated into the training process of the automated writing evaluation model. Additionally, we aim to demonstrate the explainability of this automated evaluation tool by analyzing the predicted evaluation scores on sample data and assessing the importance of each feature.}


\subsubsection*{Dataset and Baseline model\label{sec:dataset}}
{For our experiment, we used the \textsf{Essay Evaluation Dataset} provided by \textsf{AI-HUB}. This dataset is the largest writing evaluation dataset available in Korea, consisting of essays written by students ranging from 4th grade of elementary school to 3rd grade of high school. It contains approximately 46,000 essays, which are categorized into five main types. The essays can be further divided into 50 topics, with around 900 essays per topic. A total of 46,000 essays were separated by topic, with approximately 40,000 used as training data and 6,000 used as test data. The essays are evaluated using a trait scoring system, with 10 distinct evaluation rubric scores assigned to each essay. The evaluation scores, used as labels, were assigned by human raters and reflect the assessments made by reliable experts in Korean writing evaluation.}

{To verify the effectiveness of the proposed features in the automated evaluation tools, we use the automated evaluation model provided by \textsf{AI-HUB} as the baseline. The baseline model utilizes only a PLM and a BiGRU. Sentence-level embedding vectors obtained from KoBERT are processed through the BiGRU to produce an essay representation vector, which is then passed through a single linear layer to output scores for 10 evaluation criteria.}

\subsubsection*{Evaluation metrics\label{sec:metric}}
We utilized both accuracy and Quadratic Weighted Kappa (QWK) as the evaluation metrics to evaluate the model performance. Accuracy is a ratio of the number of essays with correctly predicted scores to the total number of essays. QWK measures the agreement between two raters to reflect their consistent ratings. The formula for calculating QWK is as follows:
\begin{equation}
\text{QWK} = 1 - \frac{\sum_{i,j} w_{i,j} O_{i,j}}{\sum_{i,j} w_{i,j} E_{i,j}}
\end{equation}
where \( O_{i,j} \) represents the observed frequency matrix, \( E_{i,j} \) is the expected frequency matrix, and \( w_{i,j} \) is the weight assigned based on the squared difference between the scores \( i \) and \( j \).


\begin{table}[] 
\caption{Quantitative evaluation between baseline and \textsf{UKTA} by rubric scores. \normalfont{The best performance is marked in \textbf{bold}.}}
\centering
\setlength\tabcolsep{2.5pt}
\resizebox{1\columnwidth}{!}{
\begin{tabular}{clcccc}
\toprule
\multirow{2}{*}{\textbf{Major-rubric}} & \multirow{2}{*}{\textbf{Rubric}} & \multicolumn{2}{c}{\textbf{Accuracy}} & \multicolumn{2}{c}{\textbf{QWK}}   \\ \cmidrule{3-6} 
                              &                         & Baseline   & \textsf{UKTA}  & Baseline & \textsf{UKTA} \\ 
\cmidrule(r){1-2}\cmidrule{3-6}
\multirow{3}{*}{Expression}   & Grammar                 & 0.601      & 0.601           & 0.280    & 0.280          \\
                              & Vocabulary                   & 0.638      & \textbf{0.643}  & 0.337    & \textbf{0.343} \\
                              & Sentence Expression                & 0.713      & \textbf{0.715}  & 0.875    & \textbf{0.883} \\ 
\cmidrule(r){1-2}\cmidrule{3-6}
\multirow{4}{*}{Organization} & Inter-paragraph Structure         & 0.540      & \textbf{0.544}  & 0.347    & \textbf{0.357} \\
                              & In-paragraph Structure            & 0.748      & \textbf{0.753}  & 0.896    & \textbf{0.900} \\
                              & Structure Consistency             & 0.677      & \textbf{0.682}  & 0.857    & \textbf{0.861} \\
                              & Length                  & 0.725      & \textbf{0.748}  & 0.643    & \textbf{0.727} \\ 
\cmidrule(r){1-2}\cmidrule{3-6}
\multirow{3}{*}{Content}      & Topic Clarity                 & 0.636      & \textbf{0.641}  & 0.361    & \textbf{0.370} \\
                              & Originality                 & 0.615      & \textbf{0.621}  & 0.069    & \textbf{0.172} \\
                              & Narrative            & 0.599      & \textbf{0.622}  & 0.421    & \textbf{0.488} \\ \midrule
\textbf{Average} && 0.649      & \textbf{0.657}  & 0.509    & \textbf{0.538} \\ 
\bottomrule

\end{tabular}
}
\label{tb:Comparison}
\end{table}



%===================================================================
\begin{table*}[th!]
\centering
\caption{\textbf{Qualitative evaluation between Korean writing evaluation low score (first row) and high score (second row), along with their Top-10 feature analysis.} \textnormal{The source of the translated input text, with the original typos properly reflected. The original low scoring text is in Figure \ref{fig:teaser}(A). The background colors of Top-10 feature correspond to \textred{\textit{\textsf{basic lexical features}}}, \textblue{\textit{\textsf{lexical diversity}}}, and \textpurple{\textit{\textsf{cohesion}}}, respectively. The tags for the Top-10 features used are provided in the table notes.
}}
\resizebox{1\textwidth}{!}{%
\begin{threeparttable}
\setlength\tabcolsep{5pt}
\begin{tabular}{p{0.75\textwidth} lcrlr}
    \toprule
    \textbf{Input text (Translated)} & \multicolumn{3}{c}{\textbf{Rubric score}}  & \multicolumn{2}{c}{\textbf{Top-10 feature}} \\
    \cmidrule(){1-1} %\cmidrule(lr){2-2} 
    \cmidrule(lr){2-4}
    \cmidrule(){5-6}
    \multirow{5}{=}{\justifying I respct my Englishacademy teacher. I met teacher about 1-2 years ago, and the meeting was bit special. I got fired from my previous English academy due to some incident, and with an anger, I came to this academy where my friend was going. It might not seem a big deal, but no. The reason I respect this teacher is that even though I honestly think I’m a bit troublesome, teacher hendles not only me but also other friends who are even more trouble, and halps us stay focosed on study. Also, this place isn’t just an English academy, I feel I’m learning a lot of important skills for life, manners, and social skills, so I’m very satisfied. Also my grades improved a lot too. Honestly, colling it a life academy wouldn’t be too much. After graduatingfromhere, I definitely wanttoget into Dongguk Univ's PE department, visit teacher sometimes, and show myself to juniors, like `I succeeded lkie this, so you guys can do it to'. I want to improve my skills while attending this academy, get a good university, and become a pride for my teacher.
    }
    & \textbf{Grammar} & 2 && \blue{\textbf{\texttt{FL\_MSTTR}}} & 0.58 \\
    & \textbf{Vocabulary} & 3 && \red{\textbf{\texttt{CL\_Den}}} & 0.50 \\
    & \textbf{Sentence} & 1 && \purple{\textbf{\texttt{ASO\_ALN}}} & 9.00 \\
    & \textbf{In-paragraph} & 2 && \blue{\textbf{\texttt{NNB\_MSTTR}}} & 0.50 \\
    & \textbf{Inter-paragraph} & 3 && \red{\textbf{\texttt{NNL\_Den}}} & 0.19 \\
    & \textbf{Consistency} & 2 && \red{\textbf{\texttt{NNCL\_Den}}} & 0.37 \\
    & \textbf{Length} & 3 && \blue{\texttt{\textbf{VV\_RTTR}}} & 4.70 \\
    & \textbf{Clarity} & 2 && \purple{\textbf{\texttt{AvgSenSimilarity}}} & 0.26 \\
    & \textbf{Originality} & 2 && \red{\textbf{VCL\_Den}} & 0.41 \\
    & \textbf{Narrative} & 2 && \blue{\textbf{\texttt{IC\_RTTR}}} & 1.00\\
    \cmidrule(){1-1}  
    \cmidrule(lr){2-4}
    \cmidrule(){5-6}
     \multirow{5}{=}{\justifying A South Korean singer-songwriter. A member of the mixed-gender group Jaurim, where she serves as the vocalist. In 2004, she received the Special Jury Award at the Mnet Km Music Video Festival and the Beautiful Lyrics Award at the KBS Correct Language Awards. In 2011, she was selected as the Musician of the Year by netizens at the 8th Korean Music Awards. Even now, with her 50s just around the corner, she is famous for looking much younger. Although she has naturally good skin, in her own words, because she takes care of her skin very thoroughly on a regular basis. Among Jaurim fans, there's a running joke that she divides her age by 1/4 and gives the rest to the other members. On broadcasts, she looks incredibly charismatic and seems strong-willed, but in reality, she is surprisingly slim and pretty. She has also been a model for a cosmetic brand. She was raised with such strict discipline by her father that it felt stifling, and even now, she places great importance on manners ... (omitted).
    }
    & \textbf{Grammar} & 3 && \purple{\textbf{\texttt{ASO\_CLN}}} & 23.00 \\
    & \textbf{Vocabulary} & 3 && \blue{\textbf{\texttt{NNP\_NDW}}} & 14.00 \\
    & \textbf{Sentence} & 3 && \blue{\textbf{\texttt{NNG\_NDW}}} & 134.00 \\
    & \textbf{In-paragraph} & 3 && \red{\textbf{\texttt{FL\_Den}}} & 0.42 \\
    & \textbf{Inter-paragraph} & 3 && \red{\textbf{\texttt{XFL\_Den}}} & 0.04 \\
    & \textbf{Consistency} & 3 && \blue{\textbf{\texttt{VV\_RTTR}}} & 5.81 \\
    & \textbf{Length} & 3 && \blue{\textbf{\texttt{MM\_VOCDD}}} & 0.00 \\
    & \textbf{Clarity} & 3 && \blue{\textbf{\texttt{NNB\_MSTTR}}} & 0.52 \\
    & \textbf{Originality} & 2 && \blue{\textbf{\texttt{E\_NDW}}} & 156.00 \\
    & \textbf{Narrative} & 3 && \purple{\textbf{\texttt{ASO\_FLN}}} & 23.00\\
    \bottomrule
\end{tabular}
\begin{tablenotes}
\item[-] Morpheme tag: \texttt{CL} (Content lemmas), \texttt{DD} (Determiner), \texttt{E} (Ending), \texttt{FL} (Function lemmas), \texttt{IC} (Interjection), \texttt{NNB} (Dependent noun), \texttt{NNG} (Common noun), \texttt{NNL} (Content nown), \texttt{NNP} (Proper Noun), \texttt{VCL} (Content coupla), \texttt{VV} (Verb), \texttt{XFL} (Affix formal) 
\item[-] Lexical feature tag: \texttt{Den} (Density), \texttt{MSTTR} (Mean Segmental
TTR), \texttt{RTTR} (Root TTR), \texttt{NDW} (Number of Different Words), \texttt{VOCDD} (Vocd-D)
\item[-] Cohesion tag: \texttt{ASO} (Adjacent Sentence Overlap), \texttt{ALN} (All Lemmas Normed), \texttt{CLN} (Content Lemmas Normed), \texttt{FLN} (Function Lemmas Normed)
\end{tablenotes}
\end{threeparttable}
}
\label{tb:feature}
\end{table*}
%===================================================================


\subsubsection*{Implementation details\label{sec:imp_detail}}
{The experiments were conducted in an environment equipped with an Intel(R) Core(TM) i7-13700F and an NVIDIA RTX 4090. All hyperparameters were kept constant throughout the experiments: dropout is set to 0.5, learning rate to 0.001, and the number of epochs to 100, respectively. Early stopping was employed to save only the best-performing model. The activation function of the final linear layer is the sigmoid function. 
Each experiment (\textit{i.e.}, baseline and \textsf{UKTA}) was performed five times, and the average values were used as a final result.
}



\subsection{Experimental Results\label{sec:result}}

\subsubsection*{Quantitative results}

Table \ref{tb:Comparison} presents the quantitative results of the baseline automated writing evaluation model for each evaluation metric, along with the performance of the \textsf{UKTA}. 
Both the accuracy and QWK metrics show a significant improvement when feature scores are incorporated. A closer examination of each rubric reveals that 9 out of the 10 rubrics show improvements in both accuracy and QWK, indicating that the model benefits from the feature scores in evaluating the appropriateness of "expression", "organization", and "content" in the essays. Specifically, in the "expression" category, there is a notable increase in performance for "vocabulary" and "sentence expression"; in the "organization" category, "length" shows improvement; and in the "content" category, "originality" and "narrative" demonstrate significant gains. These findings suggest that the feature scores most effectively help the model evaluate the appropriateness of vocabulary, sentence expression, adequacy of length, narrative quality, and originality of content. Finally, when averaging the performance across all rubrics, the accuracy increased from 0.649 to 0.657, and the QWK improved from 0.509 to 0.538.

\subsubsection*{Qualitative results}
We also select an essay from each of the highest-scoring and lowest-scoring groups within the same topic, and consequently analyzed the top-10 lexical features that the model emphasized for each group.  
The detailed contents of the essays are presented in Table \ref{tb:feature}, and the qualitative evaluation results for each group are as follows:

\begin{itemize}[leftmargin=1.1em, ]
    \item \textsf{\textit{Low-scoring essay.}} In the low-scoring essay, lexical features related to content morphemes (\textit{i.e.,} \texttt{CL\_Den}, \texttt{NNB\_MSTTR}, \texttt{NNL\_Den}, \texttt{NNCL\_Den}, \texttt{VV\_RTTR}, and \texttt{VCL\_Den}) accounted for 60\% of the model's influence. In contrast, lexical features related to grammatical morphemes (\textit{i.e.,} \texttt{FL\_MSTTR}) had minimal impact, contributing only about 10\%. 
    Grammatical morphemes are essential for determining subjects, objects, and predicates, as well as differentiating subtle meanings in expressions
    The simplicity of the sentence structure likely reduced the role of grammatical morphemes, leading to less weight being assigned to features related to grammatical morphemes in the evaluation. 
    Additionally, cohesion-related features contributed approximately 20\% to the score, reflecting mid-level performance in "content" and "organization".
    Notably, interjections, which are uncommon in formal Korean writing, influenced the score by 10\%. 
    This was likely due to spelling errors misinterpreted by the morpheme analyzer. These findings highlight the potential impact of misanalysis or outliers on quality predictions.

    \item \textsf{\textit{High-scoring essay.}} In the high-scoring essay, lexical features related to content morphemes (\textit{i.e.,} \texttt{NNP\_NDW}, \texttt{NNG\_NDW}, \texttt{VV\_RTTR}, \texttt{MM\_VOCDD}, and \texttt{NNB\_MSTTR}) and grammatical morphemes (\textit{i.e.,} \texttt{FL\_Den}, \texttt{XFL\_Den}, and \texttt{E\_NDW}) play a more balanced role. 
    This suggests that increased semantic complexity resulted in greater weight being assigned to features related to grammatical morphemes. 
    Among these, ending diversity and affix density had a particularly strong influence, contributing to sentence complexity and nuance.
    Content morphemes, especially proper nouns and dependent nouns, also had a significant impact.
    The more closely the essay aligned with the "My Hero" topic, which involves narrating a person's story, the greater the impact of these morphemes. This indicates that morpheme diversity analysis can provide insights into how well the content adheres to the topic. The influence of cohesion in this text is 20\%, with the overlap of content lemmas showing the greatest impact among all features. It suggests that a well-organized, cohesive essay contributes to higher scores.
\end{itemize}
In short, by comparing the features of both high-scoring and low-scoring essays, we demonstrated that the model's key influential features—content morphemes, grammatical morphemes, and cohesion—can be qualitatively analyzed, 
providing meaningful insights into essay quality.
