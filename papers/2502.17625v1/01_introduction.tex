\section{Introduction}

Since the early studies that reveal the fundamental connection between online learning and game theory~\citep{foster1997calibrated,freund1999adaptive,hart2000simple}, no-regret uncoupled learning dynamics have been heavily studied and become one of the most efficient ways for robustly learning in games and finding equilibria. 
Indeed, they are the foundation for recent AI breakthroughs such as superhuman AI for poker~\citep{bowling2015heads,moravvcik2017deepstack,brown2018superhuman,brown2019superhuman}, human-level AI for Stratego~\citep{perolat2022mastering} and Diplomacy~\citep{meta2022human}, and even alignment of large language models~\citep{jacob2023consensus,munos2024nash}.

The most basic result in this area states that a no-regret self-play learning dynamic converges to some equilibrium, with the convergence rate governed by the time-averaged regret, which is usually $O(1/\sqrt{T})$ after $T$ rounds if one directly uses worst-case $O(\sqrt{T})$ regret bounds from the adversarial online learning literature.
However, there is an extensive body of research on accelerating the convergence rate by exploiting the self-play nature and game structures to achieve lower regret, from earlier studies on two-player zero-sum games~\citep{daskalakis2011near,rakhlin2013optimization} to more recent ones on multi-player general-sum games~\citep{syrgkanis2015fast, chen2020hedging, daskalakis2021near-optimal, farina2022near, anagnostides2022uncoupled, anagnostides2022near-optimal}.
Importantly, these works all assume access to exact gradient feedback.

In contrast, far less effort has been dedicated to the more realistic setting with \emph{bandit feedback} only (that is, each player only observes their noisy payoff in each round), 
partly because improving over $O(\sqrt{T})$ regret now becomes impossible in the worst case.
To get around this barrier, researchers either relax the feedback model~\citep{rakhlin2013optimization,wei2018more} or consider different notions of regret~\citep{o2021matrix, maiti2023logarithmic}.

Nevertheless, the aforementioned barrier does not mean that one cannot achieve better than $O(\sqrt{T})$ regret \emph{in all instances}.
Indeed, in stochastic $m$-armed bandits, a problem that can be seen as a special case with one player only, even though $O(\sqrt{mT})$ regret is also unavoidable in the worst case, there are plenty of studies on obtaining \emph{instance-dependent} $o(\sqrt{T})$ regret, via e.g., the Upper Confidence Bound (UCB) algorithm~\citep{auer2002using}.
This begs the question: \emph{can we also achieve good instance-dependent regret bounds for self-play learning dynamics with bandit feedback}? 

In this work, we provide an affirmative and comprehensive answer to this question for the case of two-player zero-sum normal-form games.
Importantly, our results are built on the recent advances on \emph{best-of-both-worlds} for multi-armed bandits (MAB) that use surprisingly simple algorithms and analysis to simultaneously achieve the optimal worst-case regret in the adversarial setting and the optimal instance-dependent regret in the stochastic setting (see e.g.,~\citealp{zimmert2021tsallis}).
Our work shows that it is possible to extend such techniques to the game setting and achieve similar best-of-both-worlds phenomena, where the two worlds here refer to the case when playing against an arbitrary opponent and the case when playing against the same algorithm (self-play).
More specifically, we consider an uncoupled learning dynamic where both players simply apply the Tsallis-INF algorithm of~\citet{zimmert2021tsallis}, a well-known best-of-both-worlds algorithm for MAB, and show the following guarantees.

\begin{itemize}[leftmargin=*]
\item For general zero-sum games with possibly mixed Nash equilibria (NE), each player's regret can be bounded by two terms: an $O(c_1\log T)$ term where $c_1$ is a game-dependent constant that characterizes the difficulty of learning in this game in a way analogous to stochastic MAB, and an $O(\sqrt{c_2 T})$ term where $c_2$ is also game-dependent and can be much smaller than the number of actions (the trivial bound)  --- for example, $c_2$ is small when the support of an NE is small or when all NE are close to the boundary of the strategy space.
See Section~\ref{sec:MSNE} for the exact bounds.
We also construct an example where our algorithm provably enjoys $o(\sqrt{T})$ regret and verify it empirically in Section~\ref{sec:experiments}.

\item When specifying our results to the special case with a pure strategy NE (PSNE), our regret bound only contains the $O(c_1 \log T)$ term. In fact, the regret bound is quantitatively similar to playing two stochastic MAB instances with the expected payoff vectors being the row and the column of the game respectively that contain the PSNE.
We further prove that no reasonable algorithms can improve over this bound.
Moreover, we also show that, somewhat surprisingly, our algorithm enjoys not only average-iterate convergence but also \emph{last-iterate} convergence, a much more preferable convergence guarantee when one cares about the day-to-day behavior of the learning dynamic.
See Section~\ref{sec:PSNE} for details.

\item As a by-product of our regret guarantee, we also prove that in the case with a PSNE, after running Tsallis-INF for a certain number of rounds, the pair of the most frequently selected action of each player is the PSNE with a constant probability, which can boosted to $1-\delta$ by repeating the procedure $\log(1/\delta)$ times.
Although the sample complexity of our algorithm could be $\sqrt{m}$ factor larger than that of the optimal algorithm by~\citet{maiti2024midsearch}, we emphasize that their algorithm controls both players in a centralized and coupled manner, while ours is a decentralized and uncoupled learning dynamic that additionally enjoys no-regret guarantees (even when the opponent deviates and plays arbitrarily).
\end{itemize}

\paragraph{Related work}
A line of work that is closely related to ours studies instance-dependent sample complexity (instead of regret) for finding an NE via querying entries of a zero-sum matrix game and obtaining noisy samples~\citep{maiti2023instance, maiti2024midsearch}.
This can be seen a generalization of instance-dependent sample complexity from the best-arm identification problem (e.g.,~\citealp{jamieson2014best}) to the game setting,
while our work is a generalization of instance-dependent regret from stochastic MAB (e.g.,~\citealp{auer2002using,garivier2011kl}) to the game setting.
We note that sample complexity generally does not imply any regret guarantees,  
but the latter can be translated to the former in some cases,
and we discuss such translations and compare our bounds to~\citet{maiti2023instance, maiti2024midsearch} in Sections~\ref{sec:MSNE} and~\ref{sec:PSNE_complexity}.

Another closely related topic is dueling bandits~\citep{yue2012k}, which in fact can be seen as a special case of playing a skew-symmetric zero-sum game.
The idea of sparring, first proposed by~\citet{ailon2014reducing}, is equivalent to the self-play dynamic considered here,
and the so-called ``strong regret'' in dueling bandits coincides with the sum of the individual regret of the two players (so all our results apply directly).
Most work in dueling bandits assumes the existence of a Condorcet winner, which is equivalent to the existence of a PSNE,
and some develops instance-dependent regret that is quantitatively similar to those for stochastic MAB~\citep{yue2011beat, yue2012k, zoghi2014relative}.
Using Tsallis-INF algorithm to achieve both instance-dependent regret bounds and worst-case robustness has also been studied in~\citet{zimmert2021tsallis, saha2022versatile, saad2024weak},
and our bound for the case with a PSNE is similar to theirs and can be seen as a generalization.
For the general case when a Condorcet winner might not exist, \citet{dudik2015contextual} propose the concept of von Neumann winners, which is essentially the same as mixed NE.
Assuming a unique von Neumann winner,
\citet{balsubramani2016instance} provide an instance-dependent regret in the form of $O(\sqrt{sT})$ (ignoring additive terms that are problem-dependent), where $s$ is the size of the support of the von Neumann winner.
This bound is closely related to one instantiation of our bound for general zero-sum games, but there are several other advantages of our methods, such as a much simpler algorithm, no requirement on uniqueness, and the fact that the bound holds for both player's individual regret instead of their sum only. %

As mentioned, our results are built on the recent line of work on using simple Follow-the-Regularized-Leader algorithm to achieve best-of-both-worlds for MAB, an idea first proposed by~\citet{wei2018more} and later improved and extended to various settings (e.g.,~\citealp{rouyer2020tsallis, jin2020simultaneously, jin2021best, zimmert2021tsallis, ito2021parameter,  erez2021towards, rouyer2021algorithm, ito2022nearly, amir2022better, masoudian2022best, tsuchiya2023best, jin2024improved, jin2024no}).
Even though~\citet{zimmert2021tsallis,saha2022versatile, saad2024weak} already extend the idea to a special case of dueling bandit (which itself is a special case of zero-sum games),
our work is the first to extend it to general zero-sum games, which requires new ideas and sheds light on how to further extend these techniques to more challenging settings (e.g., multi-player general-sum games).

There is a surge of studies on understanding the last-iterate convergence of self-play learning dynamics for zero-sum games, especially for the setting with exact gradient feedback~\citep{daskalakis2019last, mertikopoulos2019learning, golowich2020tight, wei2021linear, hsieh2021adaptive, gorbunov2022last, cai2022finite, cai2024fast, cai2024accelerated}.
For the bandit setting, the convergence rate is usually much slower; see e.g.,~\citet{cai2023uncoupled} where an $\tilde{O}(1/T^{\frac{1}{6}})$ rate is obtained.
We show that for the special case with a PSNE, the simple Tsallis-INF algorithm already achieves $\tilde{O}(1/\sqrt{T})$ last-iterate convergence (albeit with some instance-dependent constant).
Unlike previous analysis, ours is solely based on analyzing the regret, which might be of independent interest.
