\section{Games with Pure-Strategy Nash Equilibria}\label{sec:PSNE}
In this section,
we further discuss the case with a unique PSNE denoted as $(\istar, \jstar)$. 
Using the first bound in Theorem~\ref{thm:general-bound-together}, we immediately obtain the following regret bound since $|I|=|J|=1$.
\begin{corollary}\label{cor:PSNE}
For a game with a unique PSNE, if both players follow the Tsallis-INF algorithm, then the following regret bound holds:  
\begin{align*}
    \max\cbrm[\big]{\Rr_T,\Rc_T} = O \rbrm[\Big]{
    {
        \omegar
        \log_+ \frac{
            mT
        }{
            \omegar^2
        }
    }
    +
    { 
        \omegac
        \log_+ \frac{
            nT
        }{
            {\omegac}^2
        }
    }
    }
    = O \rbrm[\big]{
        \rbrm{\omegar + \omegac} \log T
    },
    \yestag\label{eq:psne-bound-formula}
\end{align*}
where $\omegar = \sum_{i \neq \istar}\frac{1}{\Deltar(i)}$, $\omegac  = \sum_{j \neq \jstar } \frac{1}{\Deltac(j)}$, $\Deltar(i)=A(\istar,\jstar)-A(i,\jstar)$, and $\Deltac(j)=A(\istar,j)-A(\istar,\jstar)$.
\end{corollary}


This is a generalization of the standard instance-dependent regret bound for stochastic MAB and also similar to those from the dueling bandit literature (e.g., \citealp{yue2012k, zoghi2014relative, saha2022versatile}).
We next show that this bound is asymptotically optimal in Section~\ref{sec:PSNE_lower_bound}.
After that, we present two other results:  the last-iterate convergence behavior of our algorithm (Section~\ref{sec:last-iterate})
and using our algorithm to identify the PSNE with high probability (Section~\ref{sec:PSNE_complexity}).





\subsection{Regret lower bound}\label{sec:PSNE_lower_bound}
In this section, 
we show that the regret bound in Corollary~\ref{cor:PSNE} is tight up to some constants.
In fact,
for any $\Deltar$ and $\Deltac$,
there exists a problem instance such that
$\liminf_{T \to \infty} \frac{\Rr_T + \Rc_T}{\log T} = \Omega( \omegar + \omegac )$
for any \textit{consistent} algorithms \citep[Definition 16.1]{lattimore2020bandit}.
Note that this lower bound is also applicable to \textit{coupled} algorithms,
i.e.,
this applies to situations where a single algorithm determines both \( i_t \) and \( j_t \) based on the observation of \( \{ r_s \}_{s<t} \).

We consider problem instances in which $r_t$ follows a Bernoulli distribution over $\{ -1, 1 \}$,
i.e.,
$r_t \sim \Berpm( A(i_t, j_t) )$
given $(i_t, j_t)$,
where $\Berpm( a )$ for a parameter $a \in [-1, 1]$ is a distribution that takes values $1$ and $-1$
with probability $(1+a)/2$ and $(1-a)/2$,
respectively.
Fix an algorithm for choosing $(i_t, j_t)$ given the observations of $\{ r_s \}_{s<t}$.

Let $\Deltar \in [0, 1/4]^m$ and $\Deltac \in [0,1/4]^n$ be such that
$\Deltar_{\istar} = 0$ and $\Deltac_{\jstar} = 0$ for some $\istar \in [m]$ and $\jstar \in [n]$.
Suppose $A$ is given by
\begin{align}
    \label{eq:defADelta}
    A = 
    \mathbf{1}_m {\Deltac}^\top 
    -
    \Deltar \mathbf{1}_n^\top .
\end{align}
Then,
$(\istar, \jstar)$ is a Nash equilibrium of the game with payoff matrix $A$
as we have
$A(\istar, \jstar) - A(i, \jstar) = \Delta(i) \ge 0$
and
$A(\istar, j) - A(\istar, \jstar) = \Delta(j) \ge 0$
for all $i \in [m]$ and $j \in [n]$.
Let $\Tr_{T,i}(A)$ and $\Tc_{T,j}(A)$ denote
the expected numbers of times the $i$-th row and $j$-th column are chosen:
\begin{align*}
    \Tr_{T,i}(A) =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    \mathbf{1}[ i_t = i ]
    }
    =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    x_t(i)
    },
    \quad
    \Tc_{T,j}(A) =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    \mathbf{1}[ j_t = j ]
    }
    =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    y_t(j)
    }.
\end{align*}
We then have the following lower bound:
\begin{theorem}
    \label{thm:RegLB}
    Suppose that there exist a function $g(m, n) > 0$ and a constant $c \in (0, 1)$ such that
    $\Rr_T + \Rc_T \le g(m,n) T^{1 - c}$ for any $\hat{A} \in [-1, 1]^{m \times n}$.
    Then,
    if $A$ is given by \eqref{eq:defADelta},
    we have
    \begin{align*}
        \Tr_{T,i}(A)
        =
        \Omega\rbrm[\bigg]{
            \frac{1}{(\Deltar(i))^2}
            \log 
            \rbrm[\Big]{
            \frac{\Deltar(i) T^c}{4 g(m,n)}
            }
        },
        \quad
        \Tc_{T,j}(A)
        =
        \Omega\rbrm[\bigg]{
            \frac{1}{(\Deltac(j))^2}
            \log 
            \rbrm[\Big]{
            \frac{\Deltar(j) T^c}{4 g(m,n)}
            }
        }
    \end{align*}
    for any $i \in [m]$ and $j \in [n]$ such that
    $\Deltar_i \neq 0$ and $\Deltac_j \neq 0$.
    Consequently,
    we have
    \begin{align*}
        \liminf_{T \to \infty} \frac{\Rr_T + \Rc_T}{\log T}
        =
        \Omega\rbrm[\bigg]{
            \sum_{\substack{i \in [m] \\\Deltar(i) > 0}} \frac{c}{\Deltar(i)}
            +
            \sum_{\substack{j \in [n]\\ \Deltac(j) > 0}} \frac{c}{\Deltac(j)}
        }
        =
        \Omega\left(
            c \cdot (\omegar + \omegac)
        \right).
    \end{align*}
\end{theorem}
\begin{remark}
    \label{rem:dueling-LB}
    \upshape
    For the special case in which $A$ is skew-symmetric and $\Deltar = \Deltac$,
    the regret lower bound for the dueling bandit problem
    \citep[Theorem 2]{komiyama2015regret} leads to the same asymptotic lower bound as Theorem~\ref{thm:RegLB} above.
    Our Theorem~\ref{thm:RegLB} is more general in that it relaxes this symmetry condition; however, the underlying idea used in their proofs are shared.
    That said, while \citet{komiyama2015regret} follow the proof structure of \citet[Theorem 1]{lai1985asymptotically}, our proof, provided in Appendix~\ref{sec:pfRegLB}, adopts a simplified analytical approach based on the Bretagnolle-Huber inequality (see, e.g.,~\citealp[Chapter 17]{lattimore2020bandit}).
\end{remark}
\begin{remark}
    \upshape
    Under the assumption that bandit algorithms are \textit{minimax optimal},
    i.e.,
    if there exists a universal constant $C$ such that
    $\Rr_T + \Rc_T \le C \sqrt{(m+n)T} $ holds for all $\hat{A}\in [-1, 1]^{m \times n}$,
    which corresponds to $g(m,n) = C\sqrt{m + n}$ and $c = 1/2$,
    we obtain the following finite-time lower bound:
    \begin{align*}
        R_T(A)
        =
        \Omega
        \rbrm[\bigg]{
            \sum_{\substack{i \in [m] \\\Deltar(i) > 0}} \frac{1}{\Deltar(i)}
            \log {
                \frac{(\Deltar(i))^2 T}{16 C^2 (m+n)}
            }
            +
            \sum_{\substack{j \in [n]\\ \Deltac(j) > 0}} \frac{1}{\Deltac(j)}
            \log {
                \frac{(\Deltac(j))^2 T}{16 C^2 (m+n)}
            }
        }.
    \end{align*}
    This matches upper bound in \eqref{eq:psne-bound-formula} up to a constant factor,
    under the conditions that
    $n = \Theta(m)$
    and that
    the values of non-zero $\Deltar(i)$'s and $\Deltac(j)$'s are equivalent up to a constant factor.
\end{remark}

\subsection{Last-iterate convergence}\label{sec:last-iterate}
Somewhat surprisingly, we show that Tsallis-INF also ensures 
the following last-iterate convergence guarantee.
\begin{proposition}
    \label{prop:last-iterate}
    For a game with a unique PSNE, if both players use the Tsallis-INF algorithm,
    the output distributions converge to the PSNE (in expectation) as follows: for any $t$,
    \begin{align}
        \E \sbr{
        D(x_{\star}, x_t)
        +
        D(y_{\star}, y_t)
        }
        =
        O \rbrm[\Big]{
            \frac{1}{\sqrt{t}}
            \rbrm[\big]{
                \omegar \log_{+}{
                    \frac{mt}{\omegar^2}
                }
                +
                \omegac \log_{+}{
                    \frac{nt}{\omegac^2}
                }
            }
        },
        \label{eq:li1}
    \end{align}
    where $D(\cdot, \cdot)$ represents the Bregman divergence associated with the $(1/2)$-Tsallis entropy.
    Consequently,
    we have
    \begin{align}
        \E \sbr{
            \sqrt{
                \DG \rbr{
                    x_t,
                    y_t
                }
            }
        }
        =
        O \rbrm[\Big]{
            \frac{1}{\sqrt{t}}
            \rbrm[\big]{
                \omegar \log_{+}{
                    \frac{mt}{\omegar^2}
                }
                +
                \omegac \log_{+}{
                    \frac{nt}{\omegac^2}
                }
            }
        }.
        \label{eq:li2}
    \end{align}
\end{proposition}
Even though $\E[\sqrt{\DG(x_t, y_t)}] = O(1/\sqrt{t})$ (ignoring other factors) only imply $\E[\DG(x_t, y_t)] = O(1/\sqrt{t})$ but not necessarily $\E[\DG(x_t, y_t)] = O(1/t)$ (so the last-iterate convergence might be slower than the average-iterate convergence),
this rate is already much better than the generic $O(1/t^{1/6})$ rate of~\citet{cai2023uncoupled} for general zero-sum games.
Our proof is also particularly simple and is in fact a simple corollary of the regret bound of Theorem~\ref{thm:general-bound-together}. \\

\begin{proof}
    Fix arbitrary $T \in \mathbb{N}$.
    From the first bound of Theorem~\ref{thm:general-bound-together},
    we have
    \begin{align*}
        &
        \Rr_T(\xstar)
        +
        \Rc_T(\ystar)
        +
        C_2 \sqrt{T} \E \left[
            D(x_{\star}, x_{T+1})
            +
            D(y_{\star}, y_{T+1})
        \right]
        =
        O \rbrm[\Big]{
        {
            \omegar
            \log_+ \frac{
                mT
            }{
                \omegar^2
            }
        }
        +
        { 
            \omegac
            \log_+ \frac{
                nT
            }{
                {\omegac}^2
            }
        }
        }.
    \end{align*}
    Since $(\xstar, \ystar)$ is a Nash equilibrium, we know that 
    $\Rr_T(\xstar) + \Rc_T(\ystar) \geq 0$.
    This implies
    \begin{align*}
        \E \left[
            D(x_{\star}, x_{T+1})
            +
            D(y_{\star}, y_{T+1})
        \right]
        &
        =
        O \rbrm[\Big]{
            \frac{1}{\sqrt{T}}
            \rbrm[\big]{
                \omegar \log_{+} {
                    \frac{mT}{\omegar^2}
                }
                +
                \omegac \log_{+} {
                    \frac{nT}{\omegac^2}
                }
            }
        },
    \end{align*}
    which completes the proof of \eqref{eq:li1}.
    The Bregmann divergence associated with $(1/2)$-Tsallis entropy 
    is bounded as
    \begin{align*}
            D( \xstar, x_t )
        =
            \sum_{i=1}^m
            \frac{1}{\sqrt{x_t(i)}}\rbrm[\Big]{
                \sqrt{\xstar(i)} - \sqrt{x_t(i)}
            }^2
        \ge
            \sum_{i \in [m] \setminus \{ \istar \}}
            \sqrt{x_t(i)}
        \ge
        \frac{1}{2}
            \sqrt{
                \| x_t - \xstar \|_1.
            }
    \end{align*}
    As $\DG$ is a $1$-Lipschitz function w.r.t.~the $L^1$ norm (Lemma~\ref{lem:DGLipschitz} in Appndix~\ref{sec:app:duality-gap}),
    we have
    \begin{align*}
        \E \sbr{
            \sqrt{
                \DG( x_t, y_t )
            }
        }
        &
        \le
        \E \sbr{
            \sqrt{
                \DG( \xstar, \ystar )
                +
                \| x_t - \xstar \|_1
                +
                \| y_t - \ystar \|_1
            }
        }
        \\
        &
        \le
        2 
        \E \sbr{
            D(\xstar, x_t)
            +
            D(\ystar, y_t)
        }.
    \end{align*}
    From this and \eqref{eq:li1},
    we obtain \eqref{eq:li2} as desired.
\end{proof}


\subsection{Sample complexity of identifying PSNE}\label{sec:PSNE_complexity}
While the main focus of our work is regret minimization, we show that our algorithm can also find the exact PSNE with high probability, again using its regret guarantee.
Specifically,
define $\Delta_{\min} = \min\cbr{ \min_{i \in [m] \setminus \{ \istar \}} \Deltar(i), \min_{j \in [n] \setminus \{ \jstar \}} \Deltac(j) }$.
We prove the following.
\begin{theorem}
    For output sequences $\{ i_t \}_{t=1}^T$ and $\{ j_t \}_{t=1}^T$ generated by the Tsallis-INF algorithm,
    let $\hat{i}_T$ and $\hat{j}_T$ be the most frequently chosen arms in these sequences,
    i.e.,
    $\hat{i}_T \in \argmax_{ i \in [m] } \absm[\big]{\{ t \in [T] \mid i_t = i \} }$ and
    $\hat{j}_T \in \argmax_{ j \in [n] } \absm[\big]{ \{ t \in [T] \mid j_t = j \} }$.
    Then,
    there exists a universal constant $\alpha > 0$ such that
    $(\hat{i}_T, \hat{j}_T) = (\istar, \jstar)$
    holds with probability at least $3/4$ for
    $T \ge \alpha \frac{\omegar + \omegac}{\Delta_{\min}}$.
\end{theorem}



\begin{proof}
    From the definition of $\hat{i}_T$
    and Markov's inequality,
    we have
    \begin{align*}
        \Pr \sbrm[\big]{ \hat{i}_T \neq i_{\star} }
        &
        \le
        \Pr \sbrm[\Big]{ \sum_{t=1}^T \mathbf{1}[i_t = i_{\star}] \le \frac{T}{2}  }
        =
        \Pr \sbrm[\Big]{ T - \sum_{t=1}^T \mathbf{1}[i_t = i_{\star}] \ge \frac{T}{2}  }
        \\
        &
        \le
        \frac{2}{T}
        \E \sbrm[\Big]{ T - \sum_{t=1}^T \mathbf{1}[i_t = i_{\star}]  }
        =
        2
        -
        \frac{2}{T}
        \E \sbrm[\Big]{ \sum_{t=1}^T x_{t}(\istar)  }
        =
        2(1 - \bar{x}_{T}(\istar)).
        \yestag\label{eq:Prx}
    \end{align*}
    As we have
        $
        \bar{x}_T \cdot
        \Deltar
        \ge
        \sum_{i \neq i_{\star}}
        \bar{x}_{t}(i)
        \Deltar(i)
        \ge
        \sum_{i \neq i_{\star}}
        \bar{x}_{T}(i)
        \Deltar_{\min}
        =
        \Deltar_{\min} ( 1 - \bar{x}_{T}(\istar)),
        $
    by combining this with \eqref{eq:Prx},
    we obtain
    $
        \Pr \sbrm[\big]{ \hat{i}_T \neq i_{\star} }
        \le
        2(1 - \bar{x}_{T}( i_{\star}))
        \le
        \frac{2}{\Deltar_{\min}}
        \bar{x}_T \cdot\Deltar.
    $
    As a similar bound holds for $\Pr \sbrm[\big]{ \hat{j}_T \neq j_{\star} }$,
    we have
    \begin{align}
        \Pr \sbrm[\Big]{ (\hat{i}_T, \hat{j}_T)  \neq ( \istar, \jstar) }
        \le
        \Pr \sbrm[\big]{ \hat{i}_T \neq i_{\star}} + \Pr\sbrm[\big]{ \hat{j}_T \neq j_{\star} }
        \le
        \frac{2}{\Delta_{\min}}
        \rbrm[\Big]{
            \bar{x}_T \cdot \Deltar
            +
            \bar{y}_T \cdot \Deltac
        }
        \label{eq:PrDelta}
    \end{align}
    From the definition of $\Deltar$
    and Theorem~\ref{thm:general-bound-together},
    we have
    \begin{align*}
        \frac{2}{\Delta_{\min}}
        \rbrm[\Big]{
            \bar{x}_T \cdot \Deltar
            +
            \bar{y}_T \cdot \Deltac
        }
        & \le
        \frac{2}{\Delta_{\min}} \frac{\Rr_T + \Rc_T}{T}
        \\
        &
        \leq 2C_3
        \rbrm[\Big]{
            {
                \frac{\omegar}{\Delta_{\min}T}
                \log_+ \frac{ mT }{ \omegar^2 }
            }
            +
            { 
                \frac{\omegac}{\Delta_{\min}T}
                \log_+ \frac{ nT }{ \omegac^2 }
            }
        } \\
        &
        \leq 2C_3
        \rbrm[\Big]{
            {
                \frac{\omegar}{\Delta_{\min}T}
                \log_+ \frac{ \Delta_{\min}T }{ \omegar }
            }
            +
            { 
                \frac{\omegac}{\Delta_{\min}T}
                \log_+ \frac{ \Delta_{\min}T }{ \omegac }
            }
        },\\
        &
        \leq 2C_3
        \rbrm[\Big]{
            {
                \frac{1}{\alpha}
                \log_+ \alpha
            }
            +
            { 
                \frac{1}{\alpha}
                \log_+ \alpha
            }
        } \leq \frac{1}{4},
    \end{align*}
    where $C_3$ is the contant factor hidden by the $O(\cdot)$ symbol in \eqref{eq:psne-bound-formula},
    and in the third inequality we used the fact that $\omegar \ge m \Delta_{\min}$ and $\omegac \ge n \Delta_{\min}$.
    The last inequality holds if we take $\alpha=8C_3+4$.
    By combining this with \eqref{eq:PrDelta},
    we obtain
    $\Pr \sbrm[\big]{ (\hat{i}_T, \hat{j}_T)  \neq ( \istar, \jstar) } \le 1/4$,
    which completes the proof.
\end{proof}
From this,
we can further boost the confidence and identify the PSNE with probability at least $(1-\delta)$ with 
$O\rbrm[\big]{\frac{\omegar + \omegac}{\Delta_{\min}} \log \rbrm{ 1/\delta } }$ samples.
More concretely,
consider repeating $S > 1$ independent trials of calculating $\hat{i}_T$.
Let $\hat{i}_{T, s}$ be the result for the $s$-th trial.
Let $\tilde{i}_{T,S} \in \argmax_{i \in [m]} \absm[\big]{\{ s \in [S] \mid \hat{i}_{T, s} = i \} }$
be the arm most frequently chosen in these $S$ trials.
We then have
\begin{align*}
    \Pr \sbrm[\big]{
    \tilde{i}_{T,S}
    \neq 
    i_{\star}
    }
    &
    \le
    \Pr \sbrm[\bigg]{
    \sum_{s=1}^S
    \mathbf{1}\sbrm[\big]{
    \hat{i}_{T, s}
    =
    i_{\star}
    }
    \le
    S/2
    }
    \\
    &
    \le
    \Pr\sbrm[\bigg]{
    \sum_{s=1}^S
    X_s
    \le
    S/2
    \mid
    X_s \sim \mathrm{Ber}(3/4),
    ~
    \mbox{i.i.d. for $s \in [S]$}
    }
    \le \exp \rbr{ \Omega( - S ) }.
\end{align*}
Hence,
for any $\delta \in (0, 1)$,
by setting $S = \Theta ( 1 / \delta )$,
we have 
$
\Pr \left[
(\tilde{i}_{T,S}, \tilde{j}_{T,S})
=
(\istar, \jstar)
\right]
\ge 1 - \delta
$.
We note that,
to perform this procedure, it is necessary to know an approximate value of $\frac{\omegar + \omegac}{\Delta_{\min}}$.


We also note that due to Lemma~\ref{lem:sqrtk-ratio},
our sample complexity is at most $O\rbrm[\big]{\sqrt{\max\cbrm{n,m}}}$ times the information-theoretic optimal,
which is $O\rbrm[\big] { 
\sum_{i \in [m] \setminus \{ \istar \}} \frac{1}{{\Deltar}^2_i}
+
\sum_{j \in [n] \setminus \{ \jstar \}} \frac{1}{{\Deltac}^2_j}
}
$ and is achieved by the Midsearch algorithm of~\citet{maiti2024midsearch}.
However, their algorithm is coupled; that is, Midsearch requires the algorithm to control both players at the same time,
while our algorithm is a no-regret uncoupled learning dynamic.
