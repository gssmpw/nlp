\documentclass[11pt,a4paper]{article}
\pdfoutput=1

\input{macros_arxiv.tex}

\title{Instance-Dependent Regret Bounds for Learning \\ Two-Player Zero-Sum Games with Bandit Feedback}


\date{February 20, 2025}

\author[1,2]{Shinji Ito}
\author[3]{Haipeng Luo}
\author[1,2]{Taira Tsuchiya}
\author[3]{Yue Wu}

\affil[1]{The University of Tokyo\\ \texttt{\{shinji,tsuchiya\}@mist.i.u-tokyo.ac.jp}}
\affil[2]{RIKEN AIP}
\affil[3]{University of Southern California\\ \texttt{\{haipengl, wu.yue\}@usc.edu}}

\hypersetup{
    pdftitle={Instance-Dependent Regret Bounds for Learning Two-Player Zero-Sum Games with Bandit Feedback},
    pdfauthor={Shinji Ito, Haipeng Luo, Taira Tsuchiya, and Yue Wu},
    pdfcreator={LaTeX}
}

\begin{document}

\maketitle
\blfootnote{Authors are listed in alphabetical order.}


\begin{abstract}%
No-regret self-play learning dynamics have become one of the premier ways to solve large-scale games in practice.
Accelerating their convergence via improving the regret of the players over the naive $O(\sqrt{T})$ bound after $T$ rounds has been extensively studied in recent years, but almost all studies assume access to exact gradient feedback.
We address the question of whether acceleration is possible under bandit feedback only and provide an affirmative answer for two-player zero-sum normal-form games.
Specifically, we show that if both players apply the Tsallis-INF algorithm of~\citet{zimmert2021tsallis}, then their regret is at most $O(c_1 \log T +  \sqrt{c_2 T})$, where $c_1$ and $c_2$ are game-dependent constants that characterize the difficulty of learning ----- $c_1$ resembles the complexity of learning a stochastic multi-armed bandit instance and depends inversely on some gap measures,
while $c_2$ can be much smaller than the number of actions when the Nash equilibria have a small support or are close to the boundary. 
In particular, for the case when a pure strategy Nash equilibrium exists, $c_2$ becomes zero, leading to an optimal instance-dependent regret bound as we show.
We additionally prove that in this case our algorithm also enjoys last-iterate convergence and can identify the pure strategy Nash equilibrium with near-optimal sample complexity.
\end{abstract}


\input{01_introduction.tex}

\input{02_preliminary.tex}

\input{03_generalcase.tex}

\input{04_psne.tex}

\input{05_experiment.tex}

\input{06_conclusion.tex}




\bibliographystyle{abbrvnat}
\bibliography{reference.bib}


\input{appendix.tex}


\end{document}
