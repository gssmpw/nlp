\section{Conclusions}\label{sec:conclusions}
Prior work on learning in games has primarily focused on the full-information setting, where each player perfectly learns their gradient.
In this paper, we investigate the more realistic, partial information setting where only a noisy version of a single realized reward is revealed to the players.
Although it is impossible to optimize for all inputs,
we demonstrated that Tsallis-INF, an existing best-of-both-worlds optimal bandit algorithm, enjoys improvements by exploiting easier instances with larger gaps.
These improvements cover three aspects: regret minimization that bounds long-term average performance, last-iterate convergence guarantees that ensure day-to-day behavior for myopic agents, and a simple way to identify PSNE.

Several important questions remain open. A natural step forward is generalizing our work to general-sum multiplayer games, which is not yet explored due to the added intricacy from the misaligned incentives of players. Equally important is extending our results to extensive-form games and continuous games, each of which introducing extra challenges with their structural complexity. Our algorithm leaves a $O(\sqrt{\max\{m,n\}})$ gap in sample complexity for pure strategy Nash equilibrium identification, and whether this gap is unavoidable in uncoupled learning dynamics remains unknown. More broadly, our work suggests that learning to play games under uncertainty may be more achievable with instance-dependent improvements, opening up new possibilities in other game-theoretic environments.
