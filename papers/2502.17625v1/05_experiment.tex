\section{Numerical Experiments}\label{sec:experiments}



\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{img/regret_vs_iter.pdf}
    \caption{
    Regret scaling for Tsallis-INF and two other bandit algorithms. Each configuration $(T)$ is run for 512 trials. The interval between the 10th and 90th percentile is overlaid. The thicker dashed line represents a linear fit on the $T\geq 10^5$ subset of the log-log data.}
    \label{fig:regret-comparison}
\end{figure}


To validate our theoretical results,
we conduct a few numerical experiments.

The first experiment compares
Tsallis-INF against two baselines in terms of the regret: 
the classical UCB1~\citep{auer2002finite} and Exp3~\citep{auer2002nonstochastic} algorithms, 
which are known to have $O(T)$ and $\tilde{O}(\sqrt{T})$ regret bounds respectively in the adversarial setting.
We compare them on the game associated with $A$ defined in \eqref{eq:example-2x2-game-matrix-simplified},
with varying $T$ and $\eps=T^{-1/3}$,
where feedback $r_t$ follows a Bernoulli distribution over $\{ -1, 1 \}$ such that $ \E[r_t \mid i_t, j_t] = A(i_t, j_t)$.
As discussed, Theorem~\ref{thm:general-bound-together} predicts a regret of $\tilde{O}(T^{1/3})$ for Tsallis-INF.
The result of the experiment agrees with all these bounds in Figure~\ref{fig:regret-comparison},
where the asymptotic slope in the log-log plot (shown with a linear fit on the $T\geq 10^5$ region) is close to the theoretical prediction.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{img/identify_P_vs_iterations_by_H1.pdf}
    \caption{
        Experimental validation of Tsallis-INF's PSNE identification capability.
        The plot shows the algorithm's success rate in correctly identifying PSNE
        against the number of itrations.
        We use a hard instance of a $256\times 256$ matrix and $\Delta_1=0.1$,
        running 512 trials for each $\Delta_{\min}$ values
        over a horizon of $128\OPT$ iterations,
        where $\OPT$ is the theoretical lower bound for PSNE identification.
        The $x$-axis is scaled by $1/\OPT$.
        }
    \label{fig:PSNE-id-rate}
\end{figure}

We have discussed in Section~\ref{sec:PSNE_complexity} that Tsallis-INF needs $\frac{\omegar+\omegac}{\Delta_{\min}}$ iterations to identify the PSNE of a game. To validate our theoretical bounds, we conduct our second experiment using the following hard instance  introduced by \citet{maiti2024midsearch}:
\begin{equation}
    A=\begin{bNiceArray}{ccccc}[nullify-dots, margin, custom-line = {letter=I, tikz=dashed}, cell-space-limits = 4pt]
        0 & 2{\Delta_{\min}} & \Block{1-3}{} 2{\Delta_1} &\Cdots& 2{\Delta_1} \\
        -2{\Delta_{\min}} & \Block{4-4}{} 
                       0      & 1      & \Cdots & 1      \\
        \Block{3-1}{}
        -2{\Delta_1} & -1     & \Ddots & \Ddots & \Vdots \\
        \Vdots       & \Vdots & \Ddots & \Ddots & 1      \\
        -2{\Delta_1} & -1     & \Cdots & -1     & 0      \\
    \end{bNiceArray},
    \label{eq:psne-experiment-array}
\end{equation}
where the top-left entry is the PSNE. We set the number of actions $n=m=256$ and the gap $\Delta_1=0.1$, and vary the value of $\Delta_{\min}$.
Let $\OPT$ represent the theoretical optimal bound for identifying PSNE (ignoring log terms), defined as 
$\OPT=
\sum_{i \in [m] \setminus \{ \istar \}} \frac{1}{{\Deltar}^2_i}
+
\sum_{j \in [n] \setminus \{ \jstar \}} \frac{1}{{\Deltac}^2_j}
$,
which simplifies to $\frac{1}{2\Delta_{\min}^2}+\frac{m-2}{2\Delta_1^2}$ in this experiment.
\citepos{maiti2024midsearch} achieve the optimal $\tilde{O}(\OPT)$ sample complexity,
and their Figure~2 suggests that the sample complexity of Tsallis-INF divided by $\OPT$ is unbounded as $\Delta_{\min}$ decreases,
but our analysis in Section~\ref{sec:PSNE_complexity} disagrees with this trend.
As shown in Figure~\ref{fig:PSNE-id-rate},
the number of iterations needed to identify the PSNE divided by $\OPT$
decreases and then increases
as $\Delta_{\min}$ varies.
Lemma~\ref{lem:sqrtk-ratio} predicts the minimum ratio occurs when $\frac{\Delta_{\min}}{\Delta_1}=\frac{1}{\sqrt{m}+1}=1/17$,
and among the values we tested,
the minimum is reached when $\frac{\Delta_{\min}}{\Delta_1}=0.005/0.1=1/20$,
closely matching the prediction.
This supports our derived bound of $\tilde{O}\rbrm[\big]{\sqrt{m}\cdot \OPT}$.

The code for reproducing the experiments is available on 
\url{https://github.com/EtaoinWu/instance-dependent-game-learning}.
