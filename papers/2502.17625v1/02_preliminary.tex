\section{Preliminaries}
In this section, we formally describe concepts related to two-player zero-sum games, self-play learning dynamics, and our main algorithm.

\paragraph{Notations}
Throughout this paper, we will use $\log(\cdot)$ to denote base-$2$ logarithm, $\ln(\cdot)$ to denote base-$e$ logarithm, and use $\log_+ x = \max \{ 1, \log x \}$. We use $\tilde{O}(\cdot)$ to hide logarithmic factors; formally, $f(x)=\tilde{O}(g(x))$ means that there exists a positive integer $k$ such that $f(x)=O(g(x) \log^k g(x))$.

\paragraph{Two-Player Zero-Sum Normal-Form Games}
A two-player zero-sum normal-form game is defined via a payoff matrix $A \in [-1, 1]^{m \times n}$,
where $m$ and $n$ are the number of actions for the row player and the column player respectively.
When the row player plays action $i$ and the column player plays action $j$,
the entry $A(i, j) \in [-1, 1]$ is the expected reward for the row player and also the expected loss for the column player (hence zero-sum).

The players also have the option to play according to a probability distribution over their actions, or a \emph{mixed strategy}.
Let $\cP_m = \{ x \in [0, 1]^m \mid \| x \|_1 = 1 \}$ be the probability simplex of size $m$.
Given mixed strategies $x \in \cP_m$ and $y \in \cP_n$ of the row and column players,
the expected reward for the row player is given by
$x^\top A y$, which is also the expected loss for the column player.

A pair of mixed strategies $(\xstar, \ystar) \in \cP_m \times \cP_n$ is a \textit{Nash equilibrium} (NE) if
$
x^{\top} A \ystar
\le
\xstar^{ \top} A \ystar
\le
\xstar^{\top} A y 
$
hold for all $x \in \cP_m$ and $y \in \cP_n$.
The celebrated Minimax theorem \citep{vonneumann1928theory} implies that $(x_\star, y_\star)$ is an NE if and only if $x_\star \in  \xstarset = \argmax_{x} \cbrm[\big]{\min_y x^\top A y}$ and $y_\star \in \ystarset = \argmin_{y} \cbrm[\big]{\max_x x^\top A y}$.

A pure-strategy Nash equilibrium (PSNE) is a Nash equilibrium $(\xstar, \ystar)$ where both players choose a pure strategy,
i.e.,
$\xstar \in \{ 0, 1 \}^m$ and $\ystar \in \{ 0, 1 \}^n$.
A PSNE is also denoted by $(\istar, \jstar)$ where $\istar \in [m]$ and $\jstar \in [n]$ are the indices of the non-zero entries of $\xstar$ and $\ystar$,
respectively.
The \textit{duality gap} for $(\hat{x}, \hat{y}) \in \cP_m \times \cP_n$ is defined by
\begin{align}
    \label{eq:DG}
    \DG(\hat{x}, \hat{y}) = \max_{x \in \cP_m, y \in \cP_n} \left\{ x^\top A \hat{y} - \hat{x}^\top A y \right\} \ge 0,
\end{align}
which measures how far $(\hat{x}, \hat{y})$ is from a Nash equilibrium.
Indeed,
$(\xstar, \ystar)$ is a Nash equilibrium if and only if $\DG(\xstar, \ystar) = 0$.


\paragraph{Learning Dynamics with Bandit Feedback}
We consider a realistic setting where both players have no prior information about the game and repeatedly play the game with bandit feedback for $T$ rounds.
Specifically, 
in each round $t = 1, 2, \ldots, T$,
the row player chooses a mixed strategy $x_t \in \cP_m$, and the column player chooses $y_t \in \cP_n$.
They each draw their action $i_t \in [m]$ and $j_t \in [n]$ from their mixed strategy,
independently from each other. %
The nature then draws an outcome $r_t \in [-1,1]$ with expectation $\E \sbrm{r_t \mid i_t,j_t } = A(i_t,j_t)$ and reveals it to the row player as their realized reward and to the column player as their realized loss.\footnote{Our results hold for the more general setting where the observations for the two players are two different samples with mean $A(i_t, j_t)$.}
Note that this is a strongly uncoupled learning dynamic as defined by~\citet{daskalakis2011near}, where the players do not need to know the mixed strategy or the realized action of the opponent (in fact, not even their existence).
This property sets us apart from previous works such as \citet{zhou2017identify,o2021matrix} that use the realized action of both players to gain insight about the matrix $A$.

From each player's perspective, they are essentially facing an MAB problem with time-varying loss vectors: $\lr_t = -A y_t$ for the row player and $\lc_t = A^\top x_t$ for the column player, with noisy feedback for the coordinate they choose.
The standard performance measure in MAB is the (pseudo-)regret, defined for the row player and the column player respectively as


\begin{equation}
\begin{aligned}
    \Rr_T &= \max_{x \in \cP_m} \Rr_T(x),
    \quad\text{where}\;\;
    \Rr_T(x)
    =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    (x - x_t)^\top
    A
    y_t
    },
    \\
    \Rc_T &= \max_{y \in \cP_n} \Rc_T(y),
    \quad\text{where}\;\;
    \Rc_T(y)
    =
    \E \sbrm[\bigg]{
    \sum_{t=1}^T
    x_t^\top
    A
    (y_t - y)
    }.  
\end{aligned}
    \label{eq:defR}
\end{equation}
We say that an algorithm achieves no-regret if $\Rr_T$ and $\Rc_T$ grow sublinearly as $o(T)$, which has
an important game-theoretic implication, since the duality gap of the average-iterate strategy $(\bar{x}_T, \bar{y}_T)$ where
$\bar{x}_T= \frac{1}{T}{\sum_{t=1}^T x_t}$ and
$\bar{y}_T= \frac{1}{T}{\sum_{t=1}^T y_t}$ is equal to the average regret:
\begin{align*}
    \DG(\E[\bar{x}_T], \E[\bar{y}_T])
    =
    \max_{x \in \cP_m, y \in \cP_n}
    \E \sbrm[\bigg]{
        x^\top A \rbrm[\Big]{ \frac{1}{T} \sum_{t=1}^T y_t }
        -
        \rbrm[\Big]{ \frac{1}{T} \sum_{t=1}^T x_t }^\top
        A 
        y
    }
    =
    \frac{1}{T}
    \left(
    \Rr_T
    +
    \Rc_T
    \right).
\end{align*}
 Therefore, the average-iterate strategy converges to a Nash equilibrium, with the convergence rate governed by the average regret.
By simply deploying standard adversarial MAB algorithms such as Exp3~\citep{auer2002nonstochastic},
one can obtain a convergence rate of $\tilde{O}(\sqrt{(m+n)/T})$, which is not improvable in the worst case even in this game setting~\citep{klein1999number}.
The goal of this work is thus to improve the regret/convergence rate in an instance-dependent manner.

\paragraph{Tsallis-INF Algorithm}

Throughout the paper, we let both players apply the $\frac{1}{2}$-Tsallis-INF algorithm~\citep{zimmert2021tsallis},
which is based on the Follow-the-Regularized-Leader (FTRL) framework and chooses its strategy by solving the following optimization problem:
\begin{align}
    \label{eq:Tsallis-INF}
    x_t
    =
    \argmin_{x \in \cP_m}
    \cbrm[\bigg]{
    \sum_{s=1}^{t-1}
    \hat{\lr}_s^\top x
    +
    \frac{1}{\eta_t}
    \psi(x)
    },
    \quad
    y_t
    =
    \argmin_{y \in \cP_n}
    \cbrm[\bigg]{
    \sum_{s=1}^{t-1}
    \hat{\lc}_s^\top x
    +
    \frac{1}{\eta_t}
    \psi(y)
    },
\end{align}
where $\eta_t = \frac{1}{2 \sqrt{t}}$ is the learning rate, $\psi(x) = - 2 \sum_{i =1}^m \sqrt{x(i)}$ (or $- 2 \sum_{j =1}^n \sqrt{y(j)}$ for the column player, with a slight abuse of the notation) is the $\frac{1}{2}$-Tsallis entropy regularizer, and $\hat{\lr}_s$ and $\hat{\lc_s}$ are importance-weighted (IW) unbiased estimators for the loss vector $\lr_s$ and $\lc_s$ respectively, defined via\footnote{
    Shifting the loss values uniformly does not affect the behavior of the algorithm, so the \(-1\) in this equation can be removed in implementation.
    We add it here just to ensure that these indeed serve as unbiased estimators of \(\lr_t, \lc_t\).
}
\begin{equation}
\begin{aligned}
    \hat{\lr_t}(i)=\frac{\one[i_t=i](1 - r_t)}{x_t(i)} - 1, 
    \quad
    \hat{\lc_t}(j)=
    \frac{\one[j_t=j] (1 + r_t)}{y_t(j)} - 1.
\end{aligned}
    \tag{IW}\label{eq:defIW}
\end{equation}


Tsallis-INF is an algorithm that achieves the optimal
instance-dependent bound in stochastic MAB and simultaneously the optimal worst-case bound in adversarial MAB.
Directly applying its guarantee for adversarial MAB shows that both players enjoy $\sqrt{T}$-type regret always, \emph{even when their opponent behaves arbitrarily}.
We summarize this in the following theorem and omit further mention in the rest of the paper.
On the other hand, note that one cannot directly apply the guarantee of Tsallis-INF for stochastic MAB since the players are not facing a stochastic MAB instance with a fixed expected loss vector.\footnote{In fact,
\citet{zimmert2021tsallis} provide instance-dependent regret in a setting more general than the standard stochastic setting, 
but still, that does not directly apply to the game setting, especially when a PSNE does not exist.
}
Instead, we will utilize an immediate regret bound, also summarized in the theorem below, along with the self-play nature and the zero-sum game structure to prove our results.
For completeness,
we provide the proof of this theorem in Appendix~\ref{sec:app:proof-tsallis-inf}.
\begin{theorem}[\citealp{zimmert2021tsallis}]
    \label{thm:Tsallis-INF}
    For any $x \in \cP_m$,
    the pseudo-regret of the Tsallis-INF algorithm against $x$ is bounded as follows for the row player (and similarly for the column players):
    \begin{align}\label{eq:Tsallis-INF-upper}
        \Rr_T(x)
        \le
        \min_{i^* \in [m]}
        \cbrm[\bigg]{
        \E \sbrm[\bigg]{
            C_1
            \sum_{t=1}^T
            \frac{1}{\sqrt{t}}
            \sum_{i \in [m] \setminus \{i^*\}}
            \sqrt{x_t(i)}
            -
            C_2
            \sqrt{T}
            \cdot
            D(x, x_{T+1})
        }
        },
    \end{align}
    where $C_1$ and $C_2$ are positive universal constants
    and
    $D(x', x) = 
    \sum_{i=1}^m \frac{1}{\sqrt{x(i)}}(\sqrt{x'(i)} - \sqrt{x(i)})^2
    $ 
    is the Bregman divergence associated with the $\frac{1}{2}$-Tsallis entropy.
    In particular, we always have $\Rr_T = O(\sqrt{mT})$ even if the opponent behaves arbitrarily.
\end{theorem}

We will show in Appendix~\ref{sec:app:proof-tsallis-inf} that Theorem~\ref{thm:Tsallis-INF} holds with $C_1 = 19$ and $C_2 = 2$.
It is worth noting that by using a more refined analysis similar to \citet{zimmert2021tsallis},  
the values of $C_1$ and $C_2$ could be further improved.
Additionally, replacing the IW estimator~\eqref{eq:defIW} with their more sophisticated reduced variance estimator could further improve the values of $C_1$ and $C_2$.
However, such precise analysis introduces extra terms like $O(m \log T)$,  
which unnecessarily complicates the upper bound.  
To avoid such unnecessary complexity and to handle noisy observations $r_t$,  
we provide an analysis that differs from theirs.


