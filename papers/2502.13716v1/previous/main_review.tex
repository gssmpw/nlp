% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage[review]{cvpr}      % To produce the REVIEW version
%\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{comment}
\usepackage[normalem]{ulem}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}


\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Asymmetric Bidirectional Motion Field and Transformer for Event-based Video Frame Interpolation}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Video Frame Interpolation(VFI) aims aims to generate intermediate video frames between consecutive input frames. Since the event camera is a bio-inspired sensor that only encodes brightness changes with micro-second temporal resolution, several attempts to use the events to enhance the quality of VFI.
However, these methods generally used event-only or approximation method for bidirectional motion estimation.
% 하지만, 이러한 연구들은 VFI위한 bilateral optical flow를 estimation할때 event만 사용하거나 linear approximation을 사용하였다.  
In contrast, to further leverage the advantages of the cross-modal information, we first propose a novel networks for estimating asymmetric bidirectional motion field effectively using images and events for event-based VFI.
In details, we propose a cross-modality flow complementary compensation and an event-enhanced image-based optical flow estimation modules for accurate motion estimation.
Lastly, we develop an efficient transformer-based frame synthesis networks to refine simultaneously warped and synthesis feature. 
Experimental results validate our proposed methods shows state-of-the performance on various datasets.
Our project code and pretrained model will be available at.
% 이러한 event를 효과적으로 활용하기 위하여, 우리는 image와 event를 동시에 사용하여 바로 bilateral motion estimation을 수행하는 novel optical flow network 를 제안한다.  
% 이벤트 카메라는 bio-
% The event camera, a novel bio-inspired sensor, records brightness changes asynchronously with micro-second temporal resolution.
%Thanks to these novel characteristics, several attempts successfully use the events as a guidance in the video frame interpolation.
% VFI video frame interpolation은 intermediate frame을 합성하는 task이다.
% event camera는 bio inspired sensor로서 micro-second temporal resolution으로 밝기 변화를 기록하기 때문에, VFI의 성공적인 guidance로 사용될 수 있다.

% SOTA event-based video frame interpolation method들은 optical flow 사용할
% 이러한 특성 때문에, video frame interpolation을 guide해주는 아주 효과적인 센서로 활용할 수 있다. 
% SOTA video frame interpolation method들은  Use the word ``Abstract'' as the title, in 12-point Times, boldface type, centered relative to the column, initially capitalized. The abstract is to be in 10-point, single-spaced type. Leave two blank lines after the Abstract, then begin the main text. Look at previous CVPR abstracts to get a feel for style and length.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
Video frame interpolation(VFI) is a long-standing problem in field of the computer vision to increase the temporal resolution of the videos.
It can widely applied to many computer vision fields, ranging from SLAM, object tracking, novel view synthesis, frame rate up-conversion, video enhancement, video compression.
Thanks to its practical usage, many researchers are engaged in enhancing the quality of video frame interpolation.
Recently, many deep learning based VFI methods have been proposed, and recorded remarkable performance improvements in various VFI datasets.
%The video frame interpolation methods can be categorized into motion-based~\cite{DAIN,BMBC,ABME,bao2019memc,vfiformer,superslowmo,gui2020featureflow} and phase-based~\cite{meyer2015phase, meyer2018phasenet} and kernel based approaches~\cite{DAIN,bao2019memc,sepconv,lee2020adacof,cheng2020video,lee2020adacof}.
Specifically, the numerous motion-based video frame interpolation methods~\cite{DAIN,BMBC,ABME,bao2019memc,vfiformer,superslowmo,gui2020featureflow} are proposed thanks to the recent advance of the motion estimation algorithms.
For the the motion field for the video frame interpolation, the previous works~\cite{DAIN,BMBC,superslowmo} estimate the optical flows between consecutive frames and find the intermediate motion fields based on the model-based approximation methods~\cite{qvi,superslowmo}.
These methods often estimate the incorrect intermediate motion fields when the motions between frames are vast or non-linear and it can adversely affects the performance of the frame interpolation.
% To remedy this problem, Park \etal~\cite{ABME} propose a methods to estimate asymmetric bilateral motion field using temporal intermediate frame by estimating motion fields from ~\cite{BMBC}.
In spite of such significant efforts, the inter-frame motion estimation solely relying on frames is still challenge as the unknown motion between frames.
% 그럼에도 불구하고, frame사이의 blind motion을 알 수 없음으로  intermediate 추정은 여전히 challenge하다.
% 이러한 approximation에 기반한 optical flow의 추정 방법의 error는 
% Video frame interpolation을 수행하기 위한 optical flow 추정을 위해서, 기존 연구들은 forward warping을 수행하거나 backward warping을 수행한다.
% Video frame interpolation을 수행하기 위한 motion 추정을 위하여
% 모션 estimation에 대한 발전에 힘입어 많은 motion-based video frame interpolation algorithm들이 제안되었다. 
% intermediate motion을 추정하기 위해서,
% 여기서 linear motion approximation에 대한 설명이 들어가야할 것 같음.
%However, in spite of these significant progress, the performance of frame-based VFI methods is still limited when the motion between successive frames is vast (or the stride between consecutive frames is large).

Event cameras, a novel bio-inspired sensor, are able to capture blind motions between frames with high temporal resolutions since it asynchronously reports the brightness changes of the pixels.
Therefore, several approaches~\cite{timelens, timelens++, ledvdi, wevi, timereplayer, a2of} have tried to leverage the advantages of the events to estimate the inter-frame motion field since the events can provide the cues of motion trajectory.
% These event-based frame interpolation approaches generally comprise a motion estimation module and a frame synthesis module.
The recent methods utilize the stream of events to estimate motions~\cite{timelens, timereplayer} or perform approximation methods using frame-based motion estimation modules~\cite{wevi}.
However, the results are sub-optimal since the nature of events is sparse and noisy, and all the brightness changes are not recorded in the stream.
In contrast, unlike the events, the frame contains dense visual information, so they can be complementary to each other.

To utilize the advantages of the events and frames, Stefan \etal~\cite{timelens++} estimate inter-frame motion by simply concatenating the event and frame features to the motion estimation decoder.
% 이를 further enhancing 하기 위해여 stefan et al은 inter-frame간의 모션을 추정하기 위한 network의 image와 event를 함께 network의 input으로 사용하였다. 
% 이러한 단순한 방법과는 다르게, 
% 반대로, frame은 event와 다르게 dense한 visual 정보와 texture 정보를 포함하고 있어 event정보의 보완재가 될 수 있다. 
% To remedy these problems, we for accurate inter-frame motion estimation.
% 이러한 문제를 해결하기 위하여, inter-frame motion 추정을 위해서 event와 frame 정보를 동시 필수적으로 사용하여야 한다 
% Therefore, their motion estimation results are sub-optimal.
% To alleviate the deficiency, existing methods~\cite{timelens++} estimate the intermediate motion vectors by directly feeding the images with the embedded events into the network inputs. 
% This types of the intermediate motion estimation is sub-optimal and does not fully utilize enough information from the image inputs.
% network의 input으로 ㅐ
However, such a method, naively concatenating these two cross-modality features, can degrade the inter-frame motion estimation performances as a remarkable distinction between the two modalities.
% 단순히 image와 event의 feature를 단순히 concat하는 방법으로는 두 modality의 특성을 잘 반영할 수 없다. 

To alleviate the deficiency, we propose a novel EIF-BiOFNet for inter-frame motion estimation elaborately considering the characteristics of the events and frames.
EIF-BiOFNet consists of three cascaded modules: E-BiOF and F-BiOF and I-BiOF.
%the one for bidirectional motion estimation modules for event-level inputs(E-BiOF) and the other one complements motion field for event-only motion and motion at the previous scale(F-BiOF) and the final one for the image-level bidirectional motion field(I-BiOF). 
% 첫 번째로 E-BiOF 모듈에서는 event input만을 사용하여 coarse level의 optical flow를 추정한다.
First, the E-BiOF module estimates a coarse-level bidirectional motion field using the event input.
Since the events contain abundant temporal information, it can estimate accurate motion fields in the vicinity of the boundary of motion but not perfect in the other areas.
Thus, in the F-BiOF modules, the motion fields estimated in the previous scale and the outputs from E-BiOF are mutually complementary.
Finally, the I-BiOF modules refine the motion fields by correlation of event and frame features.
With these cascaded modules, our EIF-BiOFNet gradually reduces the search space of bidirectional inter-frame motion fields by jointly leveraging the advantages of the events and frames. 
As a results, our EIF-BiOFNet are twice as smaller as the previous inter-frame motion estimation modules~\cite{timelens}, yet the performance is higher than 2.4db in terms of PSNRs.
%by exploiting the advantages of the rich motion cues of the events and dense visual information of images.
% Our networks are twice as smaller as the previous inter-frame motion estimation modules~\cite{timelens}, yet the performance is higher than 2.4db in terms of PSNRs.  
%% 여기까지 inter-frame motion estimation 
% 따라서, F-BiOF 모듈에서는 이전스케일에서 cross-modality를 사용하여 추정한 motion과 event를 이용한 motion을 서로 상호 보완한다. 
% 마지막으로, I-BiOF 모듈에서는 F-BioF의 ouput을 frame-level의 feature를 사용하여 최종적인 motion field 를 생성한다. 
%%%% 여기 transformer 부분 다시 써야함.

For the intermediate frame synthesis, the recent event-based VFI methods~\cite{timelens,timelens++,a2of,timereplayer} are built on CNNs.
However, most CNN-based frame synthesis methods have the weakness of the long-range pixel correlation as the limited receptive field size.
To this end, we propose a new efficient cross-attentional transformer-based frame synthesis module for effective utilization of \textit{warping-based} and \textit{synthesis-based} features. 
%%%% 여기 transformer 부분 다시 써야함.


% With these novel but effective modules, event의 풍부한 motion trajectory의 cue와 image의 풍부한 dense visual information의 장점을 잘 활용하여 정확한 inter-frame motion estimation 추정을 수행할 수 있다.


% 이러한 문제를 해결하기 위해, event-based video frame interpolation을 위한 optical flow estimation network를 제안한다. 
% 
% 이러한 방법에 대한 결과는 sub-optimal하며 image input에 대한 정보를 충분히 사용하지 못한다.
% 이러한 부족을 완화하기 위하여, 기존 방법들은 image를 direct하게 network input을호 바로 feeding해주는 방법을 사용하여 motion을 추정하였다.
% there is a method to feed the image data directly into network inputs.
% 하지만, 이러한 부족을 완화하기 위하여 image정보를 네트워크에 direct하게 넣어주는 방법을 기존방법을 사용하였다. 
% To alleviate the deficiency, timelens++ used
% 하지만, 모든 밝기 변화가 event 카메라에 기록되지 않고 때로는 event의 noisy한 정보로 인하여 정확한 motion estimation 추정이 어려운 문제가 있다.  
% These types of methods can not fully utilize the intermediate blind motion of the event information between successive frames.
% Alternatively, ~\cite{yu2021training} use a way to approximate the intermediate motion using frame-based motion estimation module.
% 이러한 타입의 motion estimation methods 들은 event가 일어나지 않는 부분에도 dense한 motion을 구할 수 있다는 장점이 있지만, 중간 정보를 충분히 활용하지 못한다는 단점이 존재한다.
% 다른 방법으로는, frame-based optical flow estimation module을 사용하여 intermediate motion을 근사하는 방법이 있다. 
% 이러한 방법은 이벤트의 frame사이의 blind motion에 대한 정보를 활용하지 못함으로 결과는 sub-optimal 하다. 
% 이러한 부족을 극복하기 위하여, 


% VFI는 굉장히 오래된 문제이다. video frame의 temporal upsample을 수행하여 video의 frame rate를 높이는 문제이다. 
% 그리고, 이 문제는 아주 fundamental한 problem이여서 많은 application이 존재한다. e.g ~ 
% 이렇게 여러 application에 대해 적용될 수 있는 유용성 덕분에, 많은 노력들이 video frame interpolation qulaity를 향상시키는데 노력을 기울였다. 
% 최근에, 딥러닝 기반의 video frame interpolation 알고리즘들이 제안이 되어 많은 벤치마크 데이터 셋에서 우수한 성능을 기록하였다.
% 하지만, 이러한 성공에도 불구하고 frame interpolation은 motion이 매우 커지거나 frame 사이의 간격이 커졌을 때에 대한 성능은 매우 제한적이다.
% event camera는 밝기 변화만을 감지하기 때문에 아주 빠른 temporal resolution으로 frame사이의 blind motion을 capture할 수 있다.
% 따라서, 이러한 이벤트 카메라의 장점을 사용하여 video의 frame rate를 높이려는 여러 시도들이 있었다. 
% 이러한 Event-based frame interpolation approaches들은 일반적으로 motion estimation module과 frame synthesis module로 구성되어있다. 
% bilateral optical flow estimation에 대해서, 기존 연구들은 motion 추정을 위하여 event stream만을 사용하거나 frame-based OF method를 사용하여 linear approximation을 수행한다. 
% event stream은 dense한 temporal resolution으로 motion의 trajectory에 대한 정보를 제공하기에 motion 추정에 효과적이다. 



%------------------------------------------------------------------------
\section{Related works}
\label{sec:formatting}
%-------------------------------------------------------------------------
\noindent\textbf{Frame-based Video Frame Interpolation} \; can be classified into three categories: motion-based approaches~\cite{DAIN,BMBC,ABME,bao2019memc,vfiformer,superslowmo,gui2020featureflow,hu2022many,danierst,niklaus2020softmax} and kernel-based approaches~\cite{DAIN,bao2019memc,sepconv,lee2020adacof,cheng2020video,shi2022video} and phase-based approaches~\cite{meyer2015phase,meyer2018phasenet}. 
In the kernel-based methods, the numerous CNN kernels are learned for the synthesizing intermediate frames.
These approaches don't rely on optical flow algorithms, but the convolutional kernel size limits the magnitude of the motion. % kernel-based 내용 조금 더 추가할 것.
% 일반적으로 frame 만을 사용하는 Video frame interpolation 알고리즘은 3가지 카테고리로 분류가 가능하다. 
% 이러한 approach는 optical flow를 구하지 않다도 된다는 장점이 있지만 convolution size에 따라 capture 되는 motion의 크기가 제한적이라는 단점이 존재한다. 
Unlike these kernel-based methods, the motion-based approaches generate the intermediate frames by warping pixels using a motion field between frames. 
These approaches require the inter-frame motion fields from the input frames to the intermediate frame. However, since it should be interpolated, the intermediate frame does not exist as the input.
To solve the aforementioned problem, the previous works find the inter-frame motion fields using the linear motion model methods~\cite{DAIN,superslowmo,BMBC} and quadratic motion model~\cite{qvi,liu2020enhanced} and knowledge distillation~\cite{RIFE,IFRNet}.
At first, SuperSloMo~\cite{superslowmo} propose the linear motion model to estimate inter-frame motion fields. 
Park \etal~\cite{BMBC} further enhances a method to find the inter-frame motion to propose a bilateral cost volume layer.
% 더욱 더 정교한 inter-frame motion을 추정하기 위하여 BMBC에서는 bilateral cost volume을 사용한 방법을 제안하였다.
Since the linear motion model can not reflect the complex motion of the real-world motion, Xu \etal~\cite{qvi} propose the acceleration-aware quadratic motion model.
% 이러한 motion model을 단점을 보완하기 위하여,
Since the motion model can not simulate complex real-world motion, Park \etal~\cite{ABME} propose methods to estimate asymmetric bilateral motion fields using temporal intermediate frame using motion fields from BMBC~\cite{BMBC}.
% linear motion model은 real-world의 복잡한 motion을 모사할 수 없기 때문에, Xiangyu Xu et al은 
% intermediate frame은 interpolation 수행이 되어야 하는 frame이기 때문에 input으로서 존재하지 않는다.  
% the kernel based approaches와는 다르게 the motion-based approach는 frame간의 motion field를 사용하여 pixel을 warping하여 intermediate frame을 생성하는 방법이다.
% 이러한 motion based approaches는 bidirectional한 motion field가 필요한데 target frame이 존재하지 않음으로 approximation방법을 사용한다.
%-------------------------------------------------------------------------
\\
\noindent\textbf{Event-based Video frame interpolation} \; The event cameras have the advantages of micro-second scale temporal resolution and high dynamic range properties.
Due to these novel characteristics, recent researches are engaged to enhance the quality of the videos such as video deblurring~\cite{kim2021event,Shang_2021_ICCV,sun2022event} and low-light enhancement~\cite{zhang2020learning} and video frame interpolation~\cite{wevi,timelens,ledvdi,timelens++,timereplayer,a2of}.
% event camera는 frame-based camera와 다르게 high temporal resolution과 높은 dynamic range만을 가진다는 장점이 있다. 
% 이러한 특징으로 인해서, video deblurring 및 video frame interpolation 및 low-light enhancemnet와 같은 video frame을 enhance하는 연구가 진행되었다.  


\section{Proposd Methods}
\subsection{Overviews}
\subsection{EIF-BiOFNets}
\subsection{Transformer-based Frame Synthesis}
\subsection{Loss function}

\section{Experiments}
\subsection{Implementation Details}
\subsection{Evaluation Metric and Datasets}
\subsection{Comparison with the State-of-the-Arts}
\subsection{Model analysis}
\section{Conclusion}



%% synthetic datasets
\begin{table*}[!t]
\centering
\captionsetup{font=small}
\caption{Quantitative evaluation on a synthetic event dataset.(GoPro and Adobe240fps datasets). The \textbf{Bold} and \uline{underline} denote the best and the second-best performance, respectively. The same notation and typography are applied to the following tables.}
\vspace{-5pt}
\scalebox{0.75}{
\begin{tabular}{c|cccccccccccccccc}
\hline
\multirow{3}{*}{Method} & \multicolumn{8}{c}{GoPro~\cite{nah2017deep}} & \multicolumn{8}{c}{Adobe240fps~\cite{superslowmo}} \\ \cline{2-17} 
 & \multicolumn{2}{c}{7skip} & \multicolumn{2}{c}{15skip} & \multicolumn{2}{c}{23skip} & \multicolumn{2}{c|}{31skip} & \multicolumn{2}{c}{7skip} & \multicolumn{2}{c}{15skip} & \multicolumn{2}{c}{23skip} & \multicolumn{2}{c}{31skip} \\ \cline{2-17} 
 & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & \multicolumn{1}{c|}{SSIM} & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
SuperSloMo~\cite{superslowmo} & 27.31 & 0.838 & 22.40 & 0.684 & 20.10 & 0.606 & 18.87 & \multicolumn{1}{c|}{0.566} & 28.10 & 0.879 & 22.69 & 0.727 & 20.26 & 0.639 & 18.72 & 0.588 \\
SepConv~\cite{sepconv} & 27.24 & 0.831 & 21.97 & 0.664  & 19.60 & 0.582 & 18.44 & \multicolumn{1}{c|}{0.543} & 28.13  & 0.875 & 22.40 & 0.713 & 19.97 & 0.622 & 18.51 & 0.572  \\
DAIN~\cite{DAIN} & 26.18 & 0.822 & 22.20 & 0.822 & 20.25 & 0.609 & 19.11 & \multicolumn{1}{c|}{0.571} & 26.63 & 0.859 & 22.29 & 0.719 & 20.08 & 0.634 & 18.67 & 0.585 \\
BMBC~\cite{BMBC} & 26.54 & 0.832 & 21.57 & 0.672 & 19.25 & 0.595 & 18.07 & \multicolumn{1}{c|}{0.559} & 27.18 & 0.874 & 22.19 & 0.722 & 19.82 & 0.637 & 18.36 & 0.590 \\
ABME~\cite{ABME} & 27.79 & 0.845 & 22.71 & 0.690 & 20.51 & 0.616 & 19.31 & \multicolumn{1}{c|}{0.578} & 28.85 & 0.890 & 22.91 & 0.734 & 20.36 & 0.645 & 18.83 & 0.595  \\
RIFE~\cite{RIFE} & 27.60 & 0.837 & 22.57 & 0.681 & 20.37 & 0.606 & 19.16 & \multicolumn{1}{c|}{0.567} & 28.64 & 0.884  & 22.80 & 0.728 & 20.30 & 0.638 & 18.77 & 0.587 \\
IFRNet~\cite{IFRNet} & 27.72 & 0.842 & 22.50 & 0.685 & 20.09 & 0.609 & 18.78 & \multicolumn{1}{c|}{0.573} & 28.66
& 0.886 & 22.72 & 0.729 & 20.21  & 0.642 & 18.67 & 0.593 \\
VFIformer~\cite{vfiformer} & 27.78 & 0.844 & 22.54 & 0.689 & 20.12 & 0.614 & 18.82 & \multicolumn{1}{c|}{0.580} & 28.67 & 0.888 & 22.73 & 0.733 & 20.20 & 0.648 & 18.64 & 0.600 \\
\hline
TimeLens~\cite{timelens} & 34.45 & 0.951 & - & - & - & - & - & \multicolumn{1}{c|}{-} & 34.83 & 0.949 & - & - & - & - & - & - \\
TimeReplayer~\cite{timereplayer} & 33.39 & 0.952 & - & - & - & - & - & \multicolumn{1}{c|}{-} & 33.22 & 0.942 & - & - & - & - & - & - \\
A$^2$OF~\cite{a2of} & 35.95 & 0.967 & - & - & - & - & - & \multicolumn{1}{c|}{-} & 36.21 & 0.957 & - & - & - & - & - & - \\
\hline
Ours - light &  &  &  &  &  &  &  & \multicolumn{1}{c|}{} &  &  &  &  &  &  &  &  \\
Ours &  &  &  &  &  &  &  & \multicolumn{1}{c|}{} &  &  &  &  &  &  &  &  \\ \hline
\end{tabular}}
\end{table*}



\begin{table}[!t]
\centering
\captionsetup{font=small}
\caption{Quantitative evaluation of multi-frame interpolation(8X frame rate) on a GoPro and Adobe240fps synthetic event datasets.}
\vspace{-5pt}
\scalebox{0.9}{
\begin{tabular}{ccccc}
\hline
\multirow{2}{*}{Methods} & \multicolumn{2}{c}{GoPro~\cite{nah2017deep}} & \multicolumn{2}{c}{Adobe240fps~\cite{superslowmo}} \\ \cline{2-5} 
 & PSNR & SSIM & PSNR & SSIM \\ \hline
SuperSloMo~\cite{superslowmo}  &   28.95 & 0.876 & 29.37 & 0.902  \\
SepConv~\cite{sepconv}  & 29.13 & 0.872 & 29.75 & 0.901 \\
DAIN~\cite{DAIN} &  &  &  &  \\
BMBC~\cite{BMBC} &  &  &  &  \\ \hline
TimeLens~\cite{timelens} & 34.81 & 0.959 & 35.47 & 0.954 \\
TimeReplayer~\cite{timereplayer} & 34.02 &  0.960 & 34.14 & 0.950  \\
A$^{2}$OF~\cite{a2of} & 36.61 & 0.971 & 36.59 & 0.960 \\ \hline
Ours &  &  &  &  \\ \hline
\end{tabular}}
\end{table}

\begin{table}[!t]
\centering
\captionsetup{font=small}
\caption{Ablation study on our EIF-BiOFNet performance on GoPro dataset. We reported Average PSNRs of warped frame using estimated intermediate optical flow.}
\vspace{-5pt}
\scalebox{0.75}{
\begin{tabular}{ccccccc|c}
\hline
\multicolumn{3}{c}{Components} & 7skip & 15skip & 23skip & 31skip & Avg. \\  \hline
E-BiOF & I-BiOF & F-BiOF & PSNR & PSNR & PSNR & PSNR & PSNR \\ \hline
\checkmark &  &  & \uline{30.72} & 28.66 & 26.77 & 25.21 & 27.84 \\
 & \checkmark &  & 30.27 & \uline{29.28} & \uline{28.43} & \uline{27.69} & \uline{28.92} \\
\checkmark & \checkmark & \checkmark & \textbf{32.29} & \textbf{30.73} & \textbf{29.71} & \textbf{28.94} & \textbf{30.41}  \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[!t]
\centering
\captionsetup{font=small}
\caption{Comparison with the state-of-the art intermediate optical flow methods on GoPro dataset.}
\vspace{-7pt}
\scalebox{0.65}{
\begin{tabular}{c|cccc|c|c}
\hline
\multirow{2}{*}{Methods} & 7skip & 15skip & 23skip & 31skip & Avg. & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Params\\ {[}M{]}\end{tabular}} \\ \cline{2-6}
 & PSNR & PSNR & PSNR & PSNR & PSNR &  \\ \hline
PWCNet~\cite{Sun2018PWC-Net} + linear approx. &  &  &  &  &  &  \\
PWCNet~\cite{Sun2018PWC-Net} + GT images &  &  &  &  &  &  \\
BMBC~\cite{BMBC} &  &  &  &  &  &  \\
ABME~\cite{ABME} &  &  &  &  &  &  \\ \hline
EV-FlowNet~\cite{Zhu-RSS-18} &  &  &  &  &  &  \\
TimeLens~\cite{timelens} & 30.79 & 28.64 & 26.75 & 25.18 & 27.84 & 19.8  \\
TimeLens++~\cite{timelens++} &  &  &  &  &  &  \\ \hline
Ours & \textbf{32.29} & \textbf{30.73} & \textbf{29.71} & \textbf{28.94} & \textbf{30.41} & 7.6 \\ \hline
\end{tabular}
}
\end{table}

\begin{table}[!t]
\centering
\captionsetup{font=small}
\caption{Quantitative evaluation on BS-ERGB~\cite{timelens++} datasets. The evaluation results of each skip represent the results of whole skipped frames within consecutive frames.}
\vspace{-7pt}
\scalebox{0.8}{
\begin{tabular}{c|cccc|c}
\hline
\multirow{3}{*}{Methods} & \multicolumn{4}{c|}{BS-ERGB} & \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Params\\ {[}M{]}\end{tabular}} \\ \cline{2-5}
 & \multicolumn{2}{c}{1skip} & \multicolumn{2}{c|}{3skip} &  \\ \cline{2-5}
 & PSNR & SSIM & PSNR & SSIM &  \\ \hline
TimeLens~\cite{timelens} & 28.36 & - & 27.58 & - & 72.2 \\
TimeLens++~\cite{timelens++} & 28.56 & - & 27.63 & - & 53.9 \\ \hline
Ours - light &  &  &  &  & \\
Ours &  &  &  &  &  \\ \hline
\end{tabular}
}
\end{table}

% %%%%%%% ---------- figure ---------- %%%%%%%
\begin{figure}[!b]
  \centering
  \begin{subfigure}[!t]{0.155\textwidth}
    \includegraphics[width=\textwidth]{Figure/13-11_dummy.png}
    \caption{ABME~\cite{ABME}}
  \end{subfigure}
  \begin{subfigure}[!t]{0.155\textwidth}
    \includegraphics[width=\textwidth]{Figure/13-11_dummy.png}
    \caption{TimeLens~\cite{timelens}}
  \end{subfigure}
  \begin{subfigure}[!t]{0.155\textwidth}
    \includegraphics[width=\textwidth]{Figure/13-11_dummy.png}
    \caption{Ours}
  \end{subfigure}
  \captionsetup{font=small}
  \caption{The qualitative results of warped frames and the error maps of inter-frame motion estimation networks.}\label{fig:time_activation_map_ETES}
\end{figure}

\begin{comment}
\begin{table*}[!t]
\centering
\captionsetup{font=small}
\caption{Quantitative evaluation on a synthetic event datasets.(GOPRO and Adobe240fps datasets). The \textbf{Bold} and \uline{underline} denote the best and the second-best performance, respectively. The same notation and typography are applied to the following tables.}
\vspace{-5pt}
\scalebox{0.75}{
\begin{tabular}{c|cccccccc|cccccccc}
\hline
\multirow{2}{*}{GoPro} & \multicolumn{2}{c}{7skip(center)} & \multicolumn{2}{c}{15skip(center)} & \multicolumn{2}{c}{23skip(center)} & \multicolumn{2}{c|}{31skip(center)} & \multicolumn{2}{c}{7skip(whole)} & \multicolumn{2}{c}{15skip(whole)} & \multicolumn{2}{c}{23skip(whole)} & \multicolumn{2}{c}{31skip(whole)} \\ \cline{2-17} 
 & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
SuperSloMo~\cite{superslowmo} & 27.33 & 0.839 & 25.72 &  0.788 & 24.72 & 0.756 & 24.03 & 0.733 & 28.97 &  & 24.48 &  &  &  &  &  \\
SepConv~\cite{sepconv} & 27.24 & 0.831 &  &  &  &  &  &  & 28.13 & 0.875 & 26.22 & 0.821 & 25.09 & 0.785 & 24.31 & 0.760 \\
DAIN~\cite{DAIN} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
BMBC~\cite{BMBC} & 26.56 & 0.832 & 24.92 & 0.780 & 23.92 & 0.743 & 23.23 & 0.725 &  &  &  &  &  &  &  &  \\
ABME~\cite{ABME} & 27.81 & 0.845 & 26.14 & 0.794 & 25.14 & 0.763 & 24.45 & 0.744 & 28.85 & 0.890 & 22.91 & 0.734 & 20.36 & 0.645 & 18.83 & 0.595 \\
RIFE~\cite{RIFE} & 27.62 & 0.837 & 25.96 & 0.787 & 24.97 & 0.759 & 24.29 & 0.733 & 28.64 & 0.884 &  &  &  &  &  &  \\
IFRNet~\cite{IFRNet} & 27.74 &  0.842 & 26.03 & 0.791 &  24.97 & 0.759 & 24.24 & 0.737 &  &  &  &  &  &  &  &  \\ \hline
%VFIformer~\cite{vfiformer} & 27.80 & 0.845 & 26.08 &  0.794 & 25.02 & 0.762 &  24.29 & 0.741 &  &  &  &  &  &  &  &  \\ \hline
TimeLens~\cite{timelens} & 34.45 & 0.951 & - & - & - & - & - & - & 34.81 & 0.959 & 33.21 & 0.942 & - & - & - & - \\
TimeReplayer~\cite{timereplayer} & 33.39 & 0.952 & - & - & - & - & - & - & 34.02 & 0.960 & - & - & - & - & - & - \\ 
A2OF~\cite{a2of} & 35.95 &  0.967 & - & - & - & - & - & - & 36.61 & 0.971 & - & - & - & - & - & - \\ \hline
Ours &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
\multirow{2}{*}{Adobe240fps} & \multicolumn{2}{c}{7skip(center)} & \multicolumn{2}{c}{15skip(center)} & \multicolumn{2}{c}{23skip(center)} & \multicolumn{2}{c|}{31skip(center)} & \multicolumn{2}{c}{7skip(whole)} & \multicolumn{2}{c}{15skip(whole)} & \multicolumn{2}{c}{23skip(whole)} & \multicolumn{2}{c}{31skip(whole)} \\ \cline{2-17} 
 & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\ \hline
SuperSloMo~\cite{superslowmo} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
SepConv~\cite{sepconv} &  &  &  &   &  &  &  &  &  &  &  &  &  &  &  &  \\
DAIN~\cite{DAIN} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
BMBC~\cite{BMBC} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
ABME~\cite{ABME} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
RIFE~\cite{RIFE} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\
IFRNet~\cite{IFRNet} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
%VFIformer~\cite{vfiformer} &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
TimeLens~\cite{timelens} & 34.83 & 0.949 & - & - & - & - & - & - & 35.47 & 0.954 & - & - & - & - & - & - \\
TimeReplayer~\cite{timereplayer} & 33.22 & 0.942 & - & - & - & - & - & - & 34.14 & 0.950 & - & - & - & - & - & - \\
A2OF~\cite{a2of} & 36.21 & 0.957 & - & - & - & - & - & - & 36.59 & 0.960 & - & - & - & - & - & - \\ \hline
Ours &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  &  \\ \hline
\end{tabular}}
\end{table*}
\end{comment}





%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}