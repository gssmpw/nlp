\section{Related Works}
\vspace{-2pt}
\label{sec:Related_works}
%-------------------------------------------------------------------------
\noindent\textbf{Frame-based Video Frame Interpolation.} can be classified into three categories: motion-based approaches~\cite{DAIN,BMBC,ABME,bao2019memc,vfiformer,superslowmo,gui2020featureflow,hu2022many,Danier_2022_CVPR,niklaus2020softmax}, kernel-based approaches~\cite{DAIN,bao2019memc,sepconv,lee2020adacof,cheng2020video,shi2022video} and phase-based approaches~\cite{meyer2015phase,meyer2018phasenet}.
Thanks to the progress of optical flow algorithms~\cite{teed2020raft,Sun2018PWC-Net}, the motion-based approaches are the most actively studied.
This approach generates the intermediate frames by warping pixels using a motion field between frames. 
To be specific, previous works estimated the inter-frame motion fields using the linear~\cite{DAIN,superslowmo,BMBC}  and quadratic approximation.~\cite{qvi,liu2020enhanced} and knowledge distillation~\cite{RIFE,IFRNet}.
BMBC~\cite{BMBC} further proposed a bilateral cost volume layer to enhance the inter-frame motion.
ABME~\cite{ABME} further developed the methods to estimate asymmetric bilateral motion fields using intermediate temporal frame using motion fields from BMBC~\cite{BMBC}.
However, the inter-frame motion field estimated with only frames are still unreliable when the motion of videos is large or severely non-linear.

\noindent\textbf{Event-based Video Frame Interpolation.} Event cameras have the advantages of micro-second level temporal resolution and HDR properties.
Thanks to these novel features, recent event-based research successfully enhanced the quality of the VFI~\cite{kim2021event,Shang_2021_ICCV,sun2022event,ledvdi,xu2021motion,Zhang_2022_CVPR,Song_2022_CVPR}.
In the event-based VFI, TimeLens~\cite{timelens} first proposed a unified video interpolation framework by leveraging both the warping-based and synthesis-based approaches.
Subsequently, some works proposed weakly supervised-based~\cite{wevi} and unsupervised-based~\cite{timereplayer} event-based VFI methods.
In both two works~\cite{timelens,timereplayer}, they estimated inter-frame motion fields through the stream of events only.
However, their motion fields often fail to estimate accurate results due to the sparse nature of events and do not fully utilize the dense visual information of the images.
\cite{timelens++,a2of} mainly focus on the approximation method of the inter-frame motion fields. 
TimeLens++~\cite{timelens++} estimated inter-frame motion with spline approximation and multi-scale fusion method.
Concurrently, A$^2$OF~\cite{a2of} estimated an optical-flow distribution mask with events and utilized it as the approximation weights of inter-frame motion fields.
\textit{Unlike these works, we propose a novel EIF-BiOFNet for directly estimating inter-frame motion fields by fully leveraging cross-modality information without relying on the motion approximation methods.}


\begin{figure*}[!t]
\centering
\vspace{-4pt}
\includegraphics[width=.82\linewidth]{Figure/Figure2_overview_CVPR2023_final.pdf} 
\vspace{-14pt}
\caption{The overall architecture of Interactive Attention-based frame synthesis network.}
%The transformer decoder layer is composed of a Interactive-Attention Layer and a Self-Attention Layer.
\label{fig:Arc_frame_synthesis}
\vspace{-8pt}
\end{figure*}



\vspace{-2pt}