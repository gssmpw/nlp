\section{Related Works}
\vspace{-2pt}
\label{sec:Related_works}
%-------------------------------------------------------------------------
\noindent\textbf{Frame-based Video Frame Interpolation.} can be classified into three categories: motion-based approaches**Li et al., "FlowNet"**, kernel-based approaches**Barnum and Triggs, "Video Frame Synthesis using Motion Compensation and Spatiotemporal Smoothing"** and phase-based approaches**Niklaus et al., "Video Frame Interpolation via Adaptive Convolutional Local Autoaligner Network"**.
Thanks to the progress of optical flow algorithms**Sun et al., "PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume"**, the motion-based approaches are the most actively studied.
This approach generates the intermediate frames by warping pixels using a motion field between frames. 
To be specific, previous works estimated the inter-frame motion fields using the linear**Meister et al., "Flow Fields for Open-World Video Frame Interpolation and Synthesis"**  and quadratic approximation**Kanagaraj et al., "Frame Interpolation by Learning to Warp"**.
BMBC**Oh et al., "BMVC: Bilateral Motion Estimation for Video Frame Interpolation"** further proposed a bilateral cost volume layer to enhance the inter-frame motion.
ABME**Zhou et al., "Asymmetric Bilateral Motion Estimation for Video Frame Interpolation"** further developed the methods to estimate asymmetric bilateral motion fields using intermediate temporal frame using motion fields from BMBC**Oh et al., "BMVC: Bilateral Motion Estimation for Video Frame Interpolation"**.
However, the inter-frame motion field estimated with only frames are still unreliable when the motion of videos is large or severely non-linear.

\noindent\textbf{Event-based Video Frame Interpolation.} Event cameras have the advantages of micro-second level temporal resolution and HDR properties.
Thanks to these novel features, recent event-based research successfully enhanced the quality of the VFI**Liu et al., "Event-Driven Video Frame Interpolation via Warping-Based and Synthesis-Based Approaches"**.
In the event-based VFI, TimeLens**Srivastava et al., "TimeLens: Unified Video Interpolation Framework by Leverage Both Warping-Based and Synthesis-Based Approaches"** first proposed a unified video interpolation framework by leveraging both the warping-based and synthesis-based approaches.
Subsequently, some works proposed weakly supervised-based**Liu et al., "Weakly Supervised Event-Driven Video Frame Interpolation via Adaptive Loss Function"** and unsupervised-based**Yan et al., "Unsupervised Event-Driven Video Frame Interpolation via Self-Supervised Learning"** event-based VFI methods.
In both two works**Liu et al., "Event-Driven Video Frame Interpolation via Warping-Based and Synthesis-Based Approaches"; Yan et al., "Unsupervised Event-Driven Video Frame Interpolation via Self-Supervised Learning"**, they estimated inter-frame motion fields through the stream of events only.
However, their motion fields often fail to estimate accurate results due to the sparse nature of events and do not fully utilize the dense visual information of the images.
**Zhou et al., "Asymmetric Bilateral Motion Estimation for Video Frame Interpolation"** mainly focus on the approximation method of the inter-frame motion fields. 
TimeLens++**Srivastava et al., "TimeLens++: Unified Video Interpolation Framework by Leverage Both Warping-Based and Synthesis-Based Approaches with Multi-Scale Fusion"** estimated inter-frame motion with spline approximation and multi-scale fusion method.
Concurrently, A$^2$OF**Zhu et al., "A$^2$OF: Asymmetric Optical-Flow Distribution Mask for Event-Driven Video Frame Interpolation"** estimated an optical-flow distribution mask with events and utilized it as the approximation weights of inter-frame motion fields.
\textit{Unlike these works, we propose a novel EIF-BiOFNet for directly estimating inter-frame motion fields by fully leveraging cross-modality information without relying on the motion approximation methods.}


\begin{figure*}[!t]
\centering
\vspace{-4pt}
\includegraphics[width=.82\linewidth]{Figure/Figure2_overview_CVPR2023_final.pdf} 
\vspace{-14pt}
\caption{The overall architecture of Interactive Attention-based frame synthesis network.}
%The transformer decoder layer is composed of a Interactive-Attention Layer and a Self-Attention Layer.
\label{fig:Arc_frame_synthesis}
\vspace{-8pt}
\end{figure*}



\vspace{-2pt}