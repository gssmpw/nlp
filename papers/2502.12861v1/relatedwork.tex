\section{RELATED WORK}
Recent studies have examined the idea of teaching robots through natural language. One approach was proposed by Ahn et al.~\cite{ahn2022visually} and emphasizes a robot's need to refer to its task history, particularly when executing a series of pick-and-place manipulations through instructions given sequentially. The authors created a history-dependent dataset with pick-and-place operations instructed through natural language. A human provides the robot with linguistic instructions to move certain blocks individually to build a specific structure. The robot's main objective is to estimate the target block's position before and after executing the instructions. The dataset provides synthetic RGB images of the workspace, a set of natural language instructions written by six annotators, the objects' bounding boxes, and heat maps showing the target object. Six annotators describe the same operation for each task, in their own style. The model is trained via supervised learning, using the heatmap as ground truth.

Kuo et al.~\cite{kuo2022trajectory} conducted a study on trajectory prediction systems that utilize linguistic representations, where the model is trained using sample trajectories with labels. However, the study highlights the need for motion prediction datasets with natural language annotation. To address this limitation, the authors expanded the Argoverse~\cite{chang2019argoverse} dataset with synthetic language descriptions, which were limited by the types of filters used to generate these descriptions. They also sampled 40,000 trajectories from the Waymo~\cite{sun2020scalability} dataset and employed human labor to annotate their labels. The authors observed that annotated sentences had a much more diverse vocabulary than synthetic language, but the annotation process was quite laborious. 

Due to the limitations of the available datasets, some studies have been restricted to examining how robots understand natural language through tasks that do not require physical actions in the real world. These tasks can be performed using conventional datasets consisting of pairs of image-instruction previously developed in the literature. For instance, Cheang et al.~\cite{cheang2022learning} focused on inferring the objects' category and estimating the 6-DoF information for invisible objects of known classes from natural language instructions provided to a robot. They used RefCOCO~\cite{kazemzadeh2014referitgame} as a benchmark because it is a more established benchmark in the literature, and existing ground truth is available. Additionally, Yang et al.~\cite{yang2022interactive} explored object attributes in disambiguation and developed an interactive understanding system that can resolve ambiguities through dialogues.

Various robot manipulation tasks require different skills and formats, including following instructions~\cite{stepputtis2020language}, one-shot imitation~\cite{finn2017one}, rearrangement~\cite{batra2020rearrangement}, constraint satisfaction~\cite{brunke2022safe}, and reasoning~\cite{shridhar2020alfred}. Multiple physics simulation benchmarks have been introduced to study these tasks. For instance, iGibson~\cite{shen2021igibson, li2021igibson, srivastava2022behavior, li2023behavior} simulates interactive household scenarios, Ravens~\cite{zeng2021transporter} rearranges deep features to infer spatial displacements from visual input and exhibit superior sample efficiency on several tabletop manipulation tasks. Robosuite~\cite{zhu2020robosuite} is a modular simulation framework and benchmark for robot learning that aims to facilitate research and development of data-driven robotic algorithms and techniques. CALVIN~\cite{mees2022calvin} develops long-horizon language-conditioned tasks, Meta-World~\cite{yu2020meta} is a benchmark of simulated manipulation tasks with everyday objects contained in a shared, tabletop environment. AI2-THOR~\cite{ehsani2021manipulathor} is a framework that supports visual object manipulation and procedural generation of environments.

\begin{figure*}[b!]
\centering
    \includegraphics[width=\textwidth]{img/my_framework.pdf}\hfill
    \caption{The InstructRobot framework comprises two main blocks: environment and agent. The Environment block was designed to simplify the process of generating task instructions and rewards and is composed mainly of the Instructional Set, Reward Generator, and Robot modules. For every episode, an instruction $i_t$ is randomly selected from the Instructional Set and becomes the active instruction of the environment, providing a designed reward function to evaluate the robot's actions. The Agent block comprises the Language System, Perceptual System, Alignment, and Actor and Critic modules. The agent receives a state $\textbf{s}_{t+1}$ that comprises the active instruction $i_t$ and the perceptual information $\textbf{p}_t$ from the environment, processes it in its subsystems and acts in the environment by sending an action $\textbf{a}_t$, receiving a reward $r_t$. In this process, the agent learns a policy $\pi(\textbf{a}|\textbf{s})$ that maps the instruction into robot motion.}
    \label{fig:all_framework}
\end{figure*}

A significant challenge for approaches that require annotated data is dealing with generalization. VIMA~\cite{jiang2022vima} pursued an imitation learning strategy using Transformer architectures and multimodal prompts to train a robotic manipulator with previously learned inverse kinematics model in several pick-and-place and wipe tasks. To train the model and improve generalization, they generated a large offline dataset containing 50k trajectories per task, totaling 650k trajectories. An expert oracle collected the trajectories for all tasks, and a behavior clone algorithm was employed to learn the pick-and-place actions. Despite facilitating data sampling, this approach still has several disadvantages, such as the low generalization that imitation learning methods present in general and that the learning of complex tasks is dependent on the oracle. Our framework is related to these works, mainly in object manipulation tasks through natural language instructions. However, our framework stands out for tackling the challenge of the joint learning kinematics model and language representation while avoiding the creation of datasets.