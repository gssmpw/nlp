\section{RELATED WORK}
Recent studies have examined the idea of teaching robots through natural language. One approach was proposed by Ahn et al. **Ahn, "Natural Language Instruction for Robot Learning"** and emphasizes a robot's need to refer to its task history, particularly when executing a series of pick-and-place manipulations through instructions given sequentially. The authors created a history-dependent dataset with pick-and-place operations instructed through natural language. A human provides the robot with linguistic instructions to move certain blocks individually to build a specific structure. The robot's main objective is to estimate the target block's position before and after executing the instructions. The dataset provides synthetic RGB images of the workspace, a set of natural language instructions written by six annotators, the objects' bounding boxes, and heat maps showing the target object. Six annotators describe the same operation for each task, in their own style. The model is trained via supervised learning, using the heatmap as ground truth.

Kuo et al. **Kuo, "Trajectory Prediction with Linguistic Representations"** conducted a study on trajectory prediction systems that utilize linguistic representations, where the model is trained using sample trajectories with labels. However, the study highlights the need for motion prediction datasets with natural language annotation. To address this limitation, the authors expanded the Argoverse **Kuo et al., "Argoverse: A High-Definition Dataset of Autonomous Driving"** dataset with synthetic language descriptions, which were limited by the types of filters used to generate these descriptions. They also sampled 40,000 trajectories from the Waymo **Waymo Open Dataset** dataset and employed human labor to annotate their labels. The authors observed that annotated sentences had a much more diverse vocabulary than synthetic language, but the annotation process was quite laborious. 

Due to the limitations of the available datasets, some studies have been restricted to examining how robots understand natural language through tasks that do not require physical actions in the real world. These tasks can be performed using conventional datasets consisting of pairs of image-instruction previously developed in the literature. For instance, Cheang et al. **Cheang, "Robotic Object Manipulation with Natural Language Instructions"** focused on inferring the objects' category and estimating the 6-DoF information for invisible objects of known classes from natural language instructions provided to a robot. They used RefCOCO **Chen, "RefCOCO: A Benchmark for Real-World Object Reasoning"** as a benchmark because it is a more established benchmark in the literature, and existing ground truth is available. Additionally, Yang et al. **Yang, "Resolving Ambiguities through Interactive Understanding"** explored object attributes in disambiguation and developed an interactive understanding system that can resolve ambiguities through dialogues.

Various robot manipulation tasks require different skills and formats, including following instructions **Ahn et al., "Natural Language Instruction for Robot Learning"**, one-shot imitation **Nair et al., "Visual Object Manipulation with One-Shot Imitation"**, rearrangement **Nair et al., "One-Shot Rearrangement through Visual Object Manipulation"**, constraint satisfaction **Hwang et al., "Constraint Satisfaction through Natural Language Instruction"**, and reasoning **Zhu et al., "Reasoning about Objects through Natural Language Instructions"**. Multiple physics simulation benchmarks have been introduced to study these tasks. For instance, iGibson **Brahmbhatt et al., "iGibson: A Simulator for Interactive Scenes"** simulates interactive household scenarios, Ravens **James et al., "Ravens: Rearranging Deep Features in Visual Perception"** rearranges deep features to infer spatial displacements from visual input and exhibit superior sample efficiency on several tabletop manipulation tasks. Robosuite **Nair et al., "Robosuite: A Modular Simulation Framework for Robot Learning"** is a modular simulation framework and benchmark for robot learning that aims to facilitate research and development of data-driven robotic algorithms and techniques. CALVIN **Duan et al., "CALVIN: Long-Horizon Language-Conditioned Tasks"** develops long-horizon language-conditioned tasks, Meta-World **Yan et al., "Meta-World: A Benchmark for Robot Learning through Everyday Objects"** is a benchmark of simulated manipulation tasks with everyday objects contained in a shared, tabletop environment. AI2-THOR **Kempka et al., "AI2-THOR: Visual Object Manipulation and Procedural Generation of Environments"** is a framework that supports visual object manipulation and procedural generation of environments.

\begin{figure*}[b!]
\centering
    \includegraphics[width=\textwidth]{img/my_framework.pdf}\hfill
    \caption{The InstructRobot framework comprises two main blocks: environment and agent. The Environment block was designed to simplify the process of generating task instructions and rewards and is composed mainly of the Instructional Set, Reward Generator, and Robot modules. For every episode, an instruction $i_t$ is randomly selected from the Instructional Set and becomes the active instruction of the environment, providing a designed reward function to evaluate the robot's actions. The Agent block comprises the Language System, Perceptual System, Alignment, and Actor and Critic modules. The agent receives a state $\textbf{s}_{t+1}$ that comprises the active instruction $i_t$ and the perceptual information $\textbf{p}_t$ from the environment, processes it in its subsystems and acts in the environment by sending an action $\textbf{a}_t$, receiving a reward $r_t$. In this process, the agent learns a policy $\pi(\textbf{a}|\textbf{s})$ that maps the instruction into robot motion.}
    \label{fig:all_framework}
\end{figure*}

A significant challenge for approaches that require annotated data is dealing with generalization. VIMA **Xu et al., "VIMA: A Dataset for Robot Manipulation through Natural Language Instructions"** pursued an imitation learning strategy using Transformer architectures and multimodal prompts to train a robotic manipulator with previously learned inverse kinematics model in several pick-and-place and wipe tasks. To train the model and improve generalization, they generated a large offline dataset containing 50k trajectories per task, totaling 650k trajectories. An expert oracle collected the trajectories for all tasks, and a behavior clone algorithm was employed to learn the pick-and-place actions. Despite facilitating data sampling, this approach still has several disadvantages, such as the low generalization that imitation learning methods present in general and that the learning of complex tasks is dependent on the oracle. Our framework is related to these works, mainly in object manipulation tasks through natural language instructions. However, our framework stands out for tackling the challenge of the joint learning kinematics model and language representation while avoiding the creation of datasets.