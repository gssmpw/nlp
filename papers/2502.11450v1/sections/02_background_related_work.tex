\section{Background}

\textbf{Problem Setting.} This research focuses on unstructured pruning techniques in a supervised learning setting. Thus, we assume access to the training set $\mathcal{D} = \{(x_n, y_n)\}_{n=1}^N$, composed of tuples of input $x_n \in \mathcal{X}$ and output $y_n \in \mathcal{Y}$. The goal is to learn a model parameterized by $w \in \mathbb{R}^d$ that maps $f: \mathcal{X} \rightarrow \mathcal{Y}$ by minimizing an objective function:
\begin{align}
    \mathcal{L}(w) = \frac{1}{N} \sum_{n=1}^N l(y_n, f(x_n;w)).
    \label{eq:obj_func}
\end{align}
\newpage %To adjust content in pdf
A binary mask $m \in \{0,1\}^d$ is introduced to selectively prune the weights, effectively reducing the parameter count of the model. The pruned model is defined as $f(x_n;m \odot w)$, where $\odot$ represents the Hadamard product between the mask $m$ and the model weights $w$. Thus, the objective function for training the pruned model becomes as follows:
\begin{align}
    \mathcal{L}(m \odot w) = \frac{1}{N} \sum_{n=1}^N l(y_n, f(x_n; m \odot w)).
    \label{eq:mask_obj_func}
\end{align}
where $m$ is constructed with the help of a pruning criterion and the percentage of pruned weights the user selects. Training a pruned model allows for a decrease in the model size, reducing storage memory and computation workload while maintaining performance compared to the original network. We use $q \in \mathcal{Q}$, with $\mathcal{Q} = \{1,2, \dots,d\}$, as an index to refer to an element $w_q$ in parameter vector $w$.

\textbf{Classical PAT Methods.} Pruning methods have been of interest to the research community even before the existence of the large models widely used today (which extend memory constraints due to their composition of billions of parameters). The cornerstone work of \citet{lecun1989optimal} motivated the removal of irrelevant parameters to improve generalization and inference speed in a PAT setting. The importance of parameters is defined by the \textit{saliency}, a measurement of how much the objective function changes when a parameter is removed. As individually removing a parameter and reevaluating the objective function is unfeasible (in a reasonable time), they presented optimal brain damage (OBD), a method that utilizes a second-order Taylor series to evaluate saliency analytically under three assumptions: (1) the computationally expensive Hessian matrix $H$ is approximated using only its diagonal, which is more tractable; (2) assuming a converged model, the first-order term of the Taylor series is negligible; and (3) the local error model is assumed quadratic, so the higher-order components are discarded. Leading to an expression of saliency for a parameter $q$ given by:
\begin{align}   
    s_q = \frac{1}{2} w_q^2 H_{qq}.
    \label{eq:obd_final}
\end{align}
A few years later, \cite{hassibi1992second} revisited the approach and proposed Optimal Brain Surgeon (OBS). They highlighted the importance of a more complete representation of second-order information that includes off-diagonal elements and criticized the need to fine-tune the subnetwork. They redefined \eqref{eq:obd_final} as a constrained optimization:
\begin{align}   
    \min_{q\in \mathcal{Q}} \min_{\delta w \in \mathbb{R}^d} \left\{
    \frac{1}{2} \delta w^\top H \delta w ~~~\text{s.t.}~~~ e_q^\top \delta w + w_q = 0
    \right\},
    \label{eq:obs_constrained}
\end{align}
where $e_q$ is a one-hot vector corresponding to $w_q$. Solving (\ref{eq:obs_constrained}) leads to finding unimportant parameters that can be removed. Forming a Lagrangian, they derived a general expression for the saliency that includes \eqref{eq:obd_final} as a special case, and an expression to recalculate the magnitude of all parameters after removing a parameter $q$ to avoid fine-tuning the subnetwork. We include the intermediate steps for the reader in the Appendix \ref{OBS_ALGORITHM}.
\begin{align}
    \delta w = -\frac{w_q H^{-1} e_q}{[H^{-1}]_{qq}},
    ~~~&~~~
    s_q = \frac{w_q^2}{2[H^{-1}]_{qq}}.
    \label{eq:obs_final}
\end{align}
Furthermore, they introduced a process to compute the inverse of the Hessian matrix $H^{-1}$ efficiently through outer product approximation and the Woodbury matrix identity in its Kailath variant. Their main contributions rely on using a more accurate second-order estimation and $\delta w$ as a rescaling factor while pruning, making fine-tuning after pruning optional.

\textbf{Fisher Information Matrix.} Despite the demonstrated effectiveness of leveraging second-order information in pruning methodologies, the computational cost of calculating and storing the Hessian limits its application on NNs composed of millions or even billions of parameters. This has led to exploring inexpensive alternatives to estimate second-order information, such as the Fisher Information Matrix (FIM) \cite{vacar2011langevin}. By definition, the FIM captures the sensitivity of the model parameters \(w\) with respect to the likelihood function \(p(y \mid x, w)\), which describes the probability of observing the output \(y\) given the input \(x\) and the parameters \(w\). Formally, the FIM is the expectation, with respect to the data distribution, of the second moment of the score function or the gradient of the log-likelihood:
\begin{align}
    F(w) = \mathbb{E} \left[ 
        \nabla \log p(y \mid x, w)
        \nabla \log p(y \mid x, w)^\top
    \right].
    \label{eq:fisher_information}
\end{align}
Assuming regularity conditions \cite{Schervish2012-xv}, we can express the FIM in terms of the Hessian of the log-likelihood. Relying on the probabilistic concept that minimizing the loss function $l(y, f(x;w)$ is the equivalent of maximizing the negative log-likelihood $- \log p(y \mid x, w)$, the connection between the FIM and the Hessian is defined as:
\begin{align}
    \resizebox{.9\hsize}{!}{$
        F(w) = - \mathbb{E} \left[ 
            \nabla^2 \log p(y \mid x, w)
        \right]
        =\mathbb{E} \left[ 
            \nabla^2 l(y, f(x;w))
        \right]
        .
    $}
    \label{eq:fisher_information_hessian}
\end{align}
Still, the size of the Fisher Information Matrix (FIM) is proportional to \(\mathbb{R}^{d \times d}\), making it computationally prohibitive to calculate for modern NNs. Recently, \citet{soen2024tradeoffs} elaborated on the trade-offs of approximating the FIM only by its diagonal to reduce its computational complexity to \(\mathbb{R}^d\). In practical settings, \eqref{eq:fisher_information} is approximated using the empirical training distribution for an unbiased plug-in estimator, allowing the diagonal approximation to retain much of the relevant geometric information while significantly reducing computational complexity. This leads to the common formulation for the empirical FIM diagonal:
\begin{align}
    \label{eq:empirical_fim_diag}
    \hat{F} = \hat{F}(w)
    &= 
    \frac{1}{N} \sum_{n=1}^N 
    \nabla l(y_n, f(x_n;w))^2.
\end{align}
This approximation has an intuitive interpretation: a given entry in \(\hat{F}(w)\) corresponds to the average of the squared gradient of the model's output with respect to a parameter. The parameters influencing the model's output have larger entries, indicating higher importance. As this approach is computationally more efficient for calculating and retaining second-order information, it became a common approach in the pruning research space.

\textbf{PAT Methods Based on the FIM.} \citet{theis2018faster} proposed using empirical FIM to approximate the Hessian in the PAT setting for the first time. As in OBD saliency \eqref{eq:obd_final}, the first term vanishes (in practice, they found that including the first term reduced the performance of the pruning method), and the FIM diagonal is used to approximate the Hessian diagonal. Their saliency metric is defined as:
\begin{align}
    s_q = \frac{1}{2} w_q^2 F_{qq}, 
    % ~~~&~~~
    % F_{qq} = \frac{1}{N}\sum_{n=1}^{N} g_{nq}^2.
    \label{eq:faster_gaze_fisher}
\end{align}
Using a similar approach for structured pruning, \citet{liu2021group} employed Fisher information to estimate the importance of channels identified by a layer grouping algorithm that exploits the network computation graph. Layers in the same group have the same pruning mask computed using the FIM diagonal.

Inspired by OBS, \citet{singh2020woodfisher} identified the challenges of computing and storing the inverse of the Hessian for large models and proposed a blockwise method to compute iteratively. The authors empirically showed the relationship between the empirical FIM inverse $\hat{F}^{-1}$ and Hessian inverse $H^{-1}$, concluding that as long as the application is scale-invariant, the first is a good approximation of the second. Furthermore, they compared their blockwise method with common alternatives to compute $\hat{F}^{-1}$.

\textbf{PBT Methods.} When referring to methods that prune at initialization time, it is necessary to mention the \textit{Lottery Ticket Hypothesis} introduced by \citet{frankle2018lottery}: ``A randomly-initialized, dense neural network contains a subnetwork (winning ticket) that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations.'' Effective training depends heavily on the parameter initialization, hence the comparison to a lottery. Certain initializations enable the discovery of subnetworks that can be dramatically smaller than the dense original network and reach or exceed its performance. The authors supported their hypothesis with an iterative pruning approach that consistently found performant Resnet-18 and VGG-19 subnetworks with compression rates of $80-90\%$ for a classification task (CIFAR-10). The importance of this work relies on introducing the existence of the winning tickets. However, they required a computationally expensive process, opening the question: If winning tickets exist, can we find them inexpensively?

\citet{lee2018snip} tried to answer that question with a single-shot network pruning method called SNIP, which measures the \textit{connection sensitivity} to perform PBT. They offered a different point of view on pruning, introducing an auxiliary vector of indicator variables $c \in \{0,1\}^d$ that defines whether a connection is active or not, turning the pruning into a Hadamard product between $c$ and the parameters $w$. From that point of view, the method focuses on the influence of a connection on the loss function, named $\Delta \mathcal{L}_q $, approximating it as a directional derivative $g_q(w;\mathcal{D})$ that represents the effect of perturbation $c_q$. 
\begin{align}
    \Delta \mathcal{L}_q(w) 
    &= 
    \lim_{\epsilon \rightarrow 0} \frac{\mathcal{L}(c \odot w) - \mathcal{L}((c - \epsilon e_q) \odot w)}{\epsilon}\Bigg\rvert_{c = \mathbbm{1}},
    \nonumber \\
    &= 
    w_q \frac{\partial \mathcal{L}(w)}{\partial c_q} = g_q(w).
    \label{eq:snip_prev}
\end{align}
The intuition behind their proposed approach is that in PBT, the magnitude of the gradients with respect to the auxiliary vector $c$ indicates how each connection affects the loss, regardless of the direction. Allowing them to define their expression for the \textit{connection sensitivity} as follows:
\begin{align}
    s_q = \frac{|g_q(c \odot w)|}{\sum_{k=1}^{m} |g_k(c \odot w)|}
    \label{eq:snip}
\end{align}
\citet{wang2020picking} claimed that effective training requires preserving gradient flow through the model. They proposed a method that heavily relies on the concept of Neural Tangent Kernel (NTK) \cite{jacot2018neural}, which provides the notion of how the updates in a specific parameter affect the others throughout the training process. By defining the gradient flow as the inner product $\nabla \mathcal{L}(w)^\top \nabla \mathcal{L}(w)$ they found a link to the NTK through its eigendecomposition, claiming that their method Gradient Signal Preservation (GraSP) helps to select the parameters that encourage the NTK to be large in the direction corresponding to the output space gradients. In a similar fashion, they proposed a sensitivity metric that measures the response to a stimuli $\delta$.
\begin{align}
    S(\delta) 
    &= \Delta \mathcal{L}(w_0 + \delta) - \Delta \mathcal{L}(w_0) 
    \nonumber \\
    &= 2\delta^\top H \nabla \mathcal{L}(w) + \mathcal{O}(||\delta||_2^2)
\end{align}
\citet{tanaka2020pruning} proposed a data-agnostic approach named SynFlow. Their pruning method is designed to identify sparse and trainable subnetworks within NNs at their initialization without the need for training data. Previous methods require gradient information from the data and can inadvertently induce \textit{layer collapse}, rendering the network untrainable due to information flow interruption. SynFlow addresses this by iteratively preserving the total ``synaptic flow,'' or the cumulative strength of the connections, throughout the network during pruning. This approach ensures that essential pathways remain intact, maintaining the network's trainability. 

% \begin{align}
%     \Delta \mathcal{L}_q 
%     &=
%     g_q(w;\mathcal{D})
%     \nonumber \\
%     &\approx
%     \frac{\partial \mathcal{L}(c \odot w;\mathcal{D})}{\partial c_q} \Bigg\rvert_{c = 1},
%     \nonumber \\
%     &=
%     \lim_{\epsilon \rightarrow 0} \frac{\mathcal{L}(c \odot w;\mathcal{D}) - \mathcal{L}((c - \epsilon e_q) \odot w;\mathcal{D})}{\epsilon}\Bigg\rvert_{c = \mathbbm{1}}.
%     \nonumber
% \end{align}

% \begin{align}
%     s_q = 
%     \lim_{\epsilon \to 0} \left| 
%     \frac{\mathcal{L}(w) - \mathcal{L}(w + \epsilon \delta_q)}{\epsilon} \right|
%     &= 
%     \left| 
%         w_q \frac{\partial \mathcal{L}(w)}{\partial w_q} 
%     \right|
%     \label{eq:snip_alt}
% \end{align}

% \begin{align}
%     s_q = \frac{|g_q(c \odot w); \mathcal{D})|}{\sum_{k=1}^{m} |g_k(c \odot w; \mathcal{D})|}
%     \label{eq:snip}
% \end{align}
