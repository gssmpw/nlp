\section{Methodology}

\textbf{FIM at initialization.} The equivalence between FIM and Hessian described in \eqref{eq:fisher_information_hessian} only holds when the parameter vector $w$ is well specified. In other words, $w$ maximizes the likelihood $\mathbb{E}[\nabla \log p(y \mid x, w)] = 0$. However, with randomly initialized parameters, the FIM-Hessian equivalence does not hold. Despite this, we argue that the FIM remains a valuable approximation for second-order information at initialization. Our claim is based on the findings of \citet{karakida2019universal} for FIM in overparameterized DNNs with randomly initialized weights and large width limits. They demonstrate that even at initialization, the FIM captures essential geometric properties of the parameter space. Their study revealed that while most FIM eigenvalues are close to zero and indicate local flatness, a few are significantly large and induce strong distortions in certain directions. This suggests that even at initialization, the FIM could offer a notion of the parameters that significantly perturb the objective function, in other words, the sensitivity. This is an important attribute as we claim that in the PBT setting, we would rather preserve connections with the potential of impacting the likelihood of the model.

\textbf{Fisher-Taylor Sensitivity.} Considering the effectiveness of the saliency metric in identifying important parameters for preservation during pruning operation, the natural extension is to adjust the measurement for pruning at different training points. We follow OBD \cite{lecun1989optimal} and approximate the objective function $\mathcal{L}$ using a Taylor series, where a perturbation $\delta w$ of the parameter vector will change $\mathcal{L}$ by
\begin{align}
    \delta \mathcal{L} 
    &= \mathcal{L}(w) - \mathcal{L}(w + \delta w),
    \nonumber \\
    &= \delta w^T \nabla \mathcal{L}(w) + \frac{1}{2} \delta w^T H \delta w + \mathcal{O}(||\delta w||^3).
    \label{eq:taylor_series}
\end{align}
Since our goal is to prune at the initialization, neglecting the first-order term is not viable as the model parameters are not yet optimized. Still, we operate assuming that the local error is quadratic to discard higher-order components. However, the perturbation model in \eqref{eq:taylor_series} still faces the high computational cost of computing the Hessian. Additionally, we cannot ensure that it is positive semidefinite (PSD) for randomly initialized weights. This leads to misleading evaluations of parameter importance, as negative eigenvalues in the Hessian may suggest the presence of saddle points or non-convex regions.

To address the computational cost of computing the Hessian, we replace it with the FIM, a cheap and efficient approximation of the second-order information. The FIM has the desirable property of being PSD by construction, ensuring a stable representation of the importance of the parameters. We further simplify the approximation using the FIM diagonal. This approach assumes that the sum of $\delta \mathcal{L}$ if the parameters are deleted individually equals the change in $\delta \mathcal{L}$ if the parameters are removed simultaneously. Substituting the FIM diagonal into \eqref{eq:taylor_series}, the Taylor series approximation becomes:
\begin{align}
    \delta \mathcal{L} 
    = \sum_{q \in \mathcal{Q}} \delta w_q \frac{\partial \mathcal{L}(w)}{\partial w_q} 
    + \frac{1}{2} \sum_{q \in \mathcal{Q}} \delta w_q^2 \hat{F}_{qq},
    \label{eq:fisher_diag_taylor_series_sum}
\end{align}
Our interest is to discover important parameters in the model for one-shot pruning at initialization. We consider the term \textit{sensitivity} more suitable than \textit{saliency} for our setting because induced perturbations will cause $\mathcal{L}$ to increase, decrease, or stay the same, unlike pruning a converged model where perturbation only increases or preserves $\mathcal{L}$. Therefore, we take the magnitude of \eqref{eq:fisher_diag_taylor_series_sum} as our \textit{sensitivity} criterion. Note that a high magnitude means that the parameter $w_q$ significantly changes the objective function (either positive or negative), and it must be preserved for the pruned model to learn. Based on this, we define Fisher-Taylor Sensitivity (FTS) as our score of parameter importance:
\begin{align}
    s_q = \left| w_q \frac{\partial \mathcal{L}(w)}{\partial w_q} + \frac{1}{2} w_q^2 \hat{F}_{qq} \right|.
\end{align}

\textbf{Batch-wise FIM Estimation.} In practice, we take advantage of the additive properties of the empirical FIM and approximate it by aggregating the squared gradients of individual data points in the training set $\mathcal{D}$, as seen in Equation \eqref{eq:empirical_fim_diag}. However, aggregation of single-sample gradients, especially at initialization, introduces high variance in the approximation. To mitigate this, we evaluated splitting $\mathcal{D}$ into $B$ batches, with $B \in \mathcal{B}$ and $\mathcal{B} = \{B^{k}\}_{k=1}^{B}$, to compute a more stable approximation with the averaging gradients while reducing computational overhead. This leads to the following batch-wise approximation of the FIM:
\begin{align}
    \hat{F}(w) \approx \frac{1}{B} \sum_{k=1}^{B} \text{diag} \left(
        \nabla \mathcal{L}_{B^k}(w) \nabla \mathcal{L}_{B^k}(w)^\top
    \right).
\end{align}
% -----------------------------------------------------------------------
\textbf{Pruning Mask.} Given a partition of the data set, we compute and accumulate the gradients in vector $\tilde{g}$ and the diagonal entries of the FIM in vector $d_{F}$. Then, we calculate the FTS score vector $s$, which contains the sensitivity score $s_q$ for each parameter $w_q$ in the model (see Algorithm \ref{alg:fts_calculation}). To create the pruning mask $m$, we define a percentile $p$ to narrow the subset containing the parameter index to retain as:
\[
\mathcal{R} = \{ q \mid s_q \text{ is in the top } (1 - p) \text{ of scores} \}.
\]
Using this subset $\mathcal{R}$, the elements of the binary mask $m$ are defined using the following rule:
\[
m_q =
\begin{cases}
1, & \text{if } q \in \mathcal{R}, \\
0, & \text{otherwise}.
\end{cases}
\]
Finally, we produce the pruned model $f(x; m \odot w_0)$ with the Hadamard product between the binary mask $m$ and the vector of the initial parameters of the model $w_0$, with sparsity ratio defined as:
\[
\text{sparsity} = \frac{\sum_{q} m_q}{d},
\]
where $d$ is the total number of parameters in the unpruned model, the proposed approach ensures that the pruning process preserves only parameters with the most significant impact on the Taylor series approximation \eqref{eq:fisher_diag_taylor_series_sum}. Once the mask is applied, the pruned model is optimized utilizing stochastic gradient descent to minimize the objective function \eqref{eq:mask_obj_func}.
% -----------------------------------------------------------------------
\begin{algorithm}[t]
    \caption{Fisher-Taylor Sensitivity (FTS) Calculation}
    \label{alg:fts_calculation}
    \begin{algorithmic}
    \STATE {\bf Input:} Network \(f\) with initial parameters vector \(w_0\), dataset \(\mathcal{D}\).
    \STATE \textbf{Initialize:} Gradient sum vector \(\tilde{g} \gets \mathbf{0}\), Fisher diagonal sum vector \(d_{F} \gets \mathbf{0}\).
    \STATE Partition \(\mathcal{D}\) into \(B\) batches.
    \STATE \textbf{Require:} Dataset $\mathcal{D}$ partitioned into mini-batches $\mathcal{B} = \{B^{k}\}_{k=1}^{B}$.
    \FOR{each batch $B^{k}$ in $\mathcal{B}$}
        \STATE Compute the gradient:
        \[
        \nabla \mathcal{L}_{B^k}(w_0) = \frac{1}{|B^k|} \sum_{(x_n, y_n) \in B^k} \nabla l(y_n, f(x_n;w_0))
        \]
        \STATE Accumulate gradients and squared gradients:
        \[
        \tilde{g} \gets \tilde{g} + \nabla \mathcal{L}_{B^k}(w_0), \quad d_{F} \gets d_{F} + \nabla \mathcal{L}_{B^k}(w_0)^2.
        \]
    \ENDFOR
    \STATE Normalize aggregated vectors:
    \[
    \tilde{g} \gets \frac{\tilde{g}}{B}, \quad d_{F} \gets \frac{d_{F}}{B}.
    \]
    \STATE Compute the vector of sensitivity scores:
    \[
    s \gets \left| w_0 \odot \tilde{g} + \frac{1}{2} w_0^2 \odot d_{F} \right|,
    \]
    % where \(\odot\) denotes element-wise multiplication.
    \STATE \textbf{Return:} \(s\).
    \end{algorithmic}
\end{algorithm}
% -----------------------------------------------------------------------

\textbf{Alternative pruning criterion.} We expand on our search for cheap and efficient pruning criteria and present a series of alternative methods to compute the sensitivity score. First, we directly evaluate the FIM diagonal (FD) as a sensitivity criterion:
\begin{align}
    s_q = \hat{F}_{qq} = \frac{1}{N} \sum_{n=1}^{N} \left( \frac{\partial \mathcal{L}(w_0)}{\partial w_q} \right)^2.
    \label{eq:Fisher_Diagonal}
\end{align}
Second, we evaluate the effect of ignoring the first-order term of the Taylor series approximation \eqref{eq:fisher_diag_taylor_series_sum} as in OBD \cite{lecun1989optimal}. We refer to this approach as Fisher Pruning (FP) as we end up with a similar criterion as \citet{theis2018faster}, but for a more challenging PBT setting:
\begin{align}
    s_q = \frac{1}{2} w_q^2 \hat{F}_{qq}.
\end{align}
Finally, we follow up the constrained optimization problem \eqref{eq:obs_constrained} presented in OBS \cite{hassibi1992second} and introduce the Fisher Brain Surgeon Sensitivity (FBSS) criteria. The method expands on WoodFisher \cite{singh2020woodfisher} and solves for the Lagrangian without ignoring the first-order term of the Taylor series (derivation steps are available in Appendix \ref{FBSS_ALGORITHM}):
\begin{align}
    \label{eq:FBSS}
    s_q = \frac{1}{2[\hat{F}^{-1}]_{qq}}
        \left[
            w_q - (e_q^T \hat{F}^{-1} \nabla \mathcal{L}(w_0))
        \right]^2
\end{align}
