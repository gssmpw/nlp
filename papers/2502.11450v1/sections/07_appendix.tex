\newpage
\appendix
\onecolumn

\renewcommand{\thetable}{A\arabic{table}} % Prefix table numbers with 'A'
\renewcommand{\thefigure}{A\arabic{figure}} % Prefix figure numbers with 'A'
\renewcommand{\theequation}{A\arabic{equation}} % Prefix equation numbers with 'A'

\setcounter{table}{0} % Reset table counter
\setcounter{figure}{0} % Reset figure counter
\setcounter{equation}{0} % Reset equation counter

\section*{Appendix}

\section{Optimal Brain Surgeon Derivation}
\label{OBS_ALGORITHM}

In the original setup in OBS, we have a local quadratic model for the loss $L$ given by:
$$
    \delta L = L(w + \delta w) \approx L(w) + \nabla_w L^T \delta w + \frac{1}{2} \delta w^T H \delta w
$$
Since OBS is a pruning-after-training approach, they discarded the 1-st order component. Reducing the expression for saliency as:
$$
    \delta L = \frac{1}{2} \delta w^T H \delta w
$$
To remove a single parameter, the authors of OBS introduced the constraint $e_q^T \delta w + w_q = 0$, with $e_q$ being the $q^{\text{th}}$ canonical basis vector. The pruning is defined as a constrained optimization problem of the form:
$$
    \min_{\delta w \in \mathbb{R^d}} \left( \frac{1}{2} \delta w^T H \delta w\right),
    ~~\text{s.t}~~
    e_q^T \delta w + w_q = 0.
$$
And the choice of which parameter to remove becomes:
$$
    \min_{q \in \mathcal{Q}} \left\{
        \min_{\delta w \in \mathbb{R^d}} \left( \frac{1}{2} \delta w^T H \delta w\right),
        ~~\text{s.t}~~
        e_q^T \delta w + w_q = 0
    \right\}.
$$
To solve the internal problem, we use a Lagrange multiplier $\lambda$ to write the problem as an unconstrained optimization case as follows:
$$
    \mathcal{L}(\delta w, \lambda) =
    \frac{1}{2} \delta w^T H \delta w +
    \lambda(e_q^T \delta w + w_q).
$$
Then, to find the stationary conditions, we compute the partial derivatives with respect to $\delta w$ and $\lambda$, and equate them to 0, obtaining:
$$
    \nabla_{\delta w} \mathcal{L} = 
    H \delta w + \lambda e_q = 0 
    \rightarrow
    \delta w = - \lambda H^{-1} e_q
$$
$$
    \nabla_{\lambda} \mathcal{L} =
    e_q^T \delta w + w_q = 0
    \rightarrow
    e_q^T \delta w = -w_q
$$
With some replacements, we get:
$$
    e_q^T \delta w = -w_q
    \rightarrow
    e_q^T \left( 
        - \lambda H^{-1} e_q
    \right) = -w_q
    \rightarrow
    - \lambda e_q^T H^{-1} e_q = -w_q
    \rightarrow
    \lambda = \frac{w_q}{e_q^T H^{-1} e_q} = \frac{w_q}{[H^{-1}]_{qq}}
$$
$$
    \delta w = - \frac{w_q H^{-1} e_q}{[H^{-1}]_{qq}}
$$
Replacing the expression for $\delta w$ in the saliency expression, we have:
\begin{align*}
    \delta L = \frac{1}{2} \delta w^T H \delta w
    &= \frac{1}{2}\left(
        - \frac{w_q H^{-1} e_q}{[H^{-1}]_{qq}}
    \right)^T
    H
    \left(
        - \frac{w_q H^{-1} e_q}{[H^{-1}]_{qq}}
    \right)
    \nonumber \\
    &= 
    \frac{w_q^2}{2[H^{-1}]_{qq}^2}
    \left(
        H^{-1} e_q
    \right)^T
    H
    \left(
        H^{-1} e_q
    \right)
    \nonumber \\
    &= 
    \frac{w_q^2}{2[H^{-1}]_{qq}^2}
    e_q ^T
    H^{-1}
    e_q
    = 
    \frac{w_q^2}{2[H^{-1}]_{qq}^2}
    [H^{-1}]_{qq}
    = 
    \frac{w_q^2}{2[H^{-1}]_{qq}}
    \nonumber \\
\end{align*}
%------------------------------------------------------------------------------------------------
\newpage
\section{Fisher Brain Surgeon Sensitivity Derivation}
\label{FBSS_ALGORITHM}
As we considered a PBT setting, it is not possible to ignore the first-order term in the local quadratic approximation of the error as it could still be informative. In this case, our model for sensitivity is given by: 
$$
    \delta L = \nabla_w L^T \delta w + \frac{1}{2} \delta w^T H \delta w
$$
The process to remove a single parameter remains similar; the constraint $e_q^T \delta w + w_q = 0$, with $e_q$ is still valid, redefining the optimization problem as:
$$
    \min_{\delta w \in \mathbb{R^d}} \left(
        \nabla_w L^T \delta w +  \frac{1}{2} \delta w^T H \delta w
    \right),
    ~~\text{s.t}~~
    e_q^T \delta w + w_q = 0.
$$
And the choice of which parameter to remove becomes:
$$
    \min_{q \in \mathcal{Q}} \left\{
        \min_{\delta w \in \mathbb{R^d}} \left(
            \nabla_w L^T \delta w + \frac{1}{2} \delta w^T H \delta w
        \right),
        ~~\text{s.t}~~
        e_q^T \delta w + w_q = 0
    \right\}.
$$
Using a Lagrange multiplier $\lambda$ as in the reference case, we solve the following unconstrained optimization problem:
$$
    \mathcal{L}(\delta w, \lambda) =
    \nabla_w L^T \delta w + 
    \frac{1}{2} \delta w^T H \delta w +
    \lambda(e_q^T \delta w + w_q).
$$
With the following stationary conditions:
$$
    \nabla_{\delta w} \mathcal{L} = 
    \nabla_w L + H \delta w + \lambda e_q = 0 
    \rightarrow
    \delta w = - (\lambda H^{-1}e_q + H^{-1} \nabla_w L)
$$
$$
    \nabla_{\lambda} \mathcal{L} =
    e_q^T \delta w + w_q = 0
    \rightarrow
    e_q^T \delta w = -w_q
$$
The expression for $\lambda$ is redefined as follows:
\begin{align*}
    e_q^T \left(
        - (\lambda H^{-1}e_q + H^{-1} \nabla_w L)
    \right) 
    &= -w_q
    \nonumber \\
    \lambda e_q^T H^{-1} e_q + e_q^T H^{-1} \nabla_w L
    &= w_q
    \nonumber \\
    \lambda [H^{-1}]_{qq} 
    &= w_q - e_q^T H^{-1} \nabla_w L
    \nonumber \\
    \lambda
    &= \frac{w_q - e_q^T H^{-1} \nabla_w L}{[H^{-1}]_{qq}}
\end{align*}
Replacing the expression for $\delta w$ in our sensitivity expression, we have:
\begin{align*}
    \delta L = \nabla_w L^T \delta w + \frac{1}{2} \delta w^T H \delta w
    &= 
    \nabla_w L^T \left[
        - (\lambda H^{-1}e_q + H^{-1} \nabla_w L)
    \right]
    \nonumber \\
    &+
    \frac{1}{2}\left[
        - (\lambda H^{-1}e_q + H^{-1} \nabla_w L)
    \right]^T
    H
    \left[
        - (\lambda H^{-1}e_q + H^{-1} \nabla_w L)
    \right]
    \nonumber \\
    &= 
    - \lambda \nabla_w L^T H^{-1}e_q - \nabla_w L^T H^{-1} \nabla_w L
    \nonumber \\
    &+
    \frac{1}{2}\left[
        (\lambda H^{-1}e_q)^T + (H^{-1} \nabla_w L)^T
    \right]
    \left[
        \lambda H H^{-1}e_q + H H^{-1} \nabla_w L)
    \right]
    \nonumber \\
    &= 
    - \lambda \nabla_w L^T H^{-1}e_q - \nabla_w L^T H^{-1} \nabla_w L
    \nonumber \\
    &+
    \frac{1}{2}\left[
        (\lambda H^{-1}e_q)^T + (H^{-1} \nabla_w L)^T
    \right]
    \left[
        \lambda e_q + \nabla_w L
    \right]
    \nonumber \\
    &= 
    - \lambda \nabla_w L^T H^{-1}e_q - \nabla_w L^T H^{-1} \nabla_w L
    \nonumber \\
    &+
    \frac{1}{2}\left[
        (\lambda H^{-1}e_q)^T \lambda e_q
        + (H^{-1} \nabla_w L)^T \lambda e_q
        + (\lambda H^{-1}e_q)^T \nabla_w L
        + (H^{-1} \nabla_w L)^T \nabla_w L
    \right]
    \nonumber \\
    &= 
    - \lambda \nabla_w L^T H^{-1}e_q - \nabla_w L^T H^{-1} \nabla_w L
    \nonumber \\
    &+
    \frac{1}{2}\left[
        \lambda^2 e_q^T H^{-1} e_q
        + \lambda \nabla_w L^T H^{-1} e_q
        + \lambda e_q^T H^{-1} \nabla_w L
        + \nabla_w L^T H^{-1} \nabla_w L
    \right]
    \nonumber \\
    &= 
    \frac{1}{2}\left[
        \lambda^2 [H^{-1}]_{qq}
        - \lambda \nabla_w L^T H^{-1} e_q
        + \lambda e_q^T H^{-1} \nabla_w L
        - \nabla_w L^T H^{-1} \nabla_w L
    \right]
    \nonumber \\
\end{align*}
Finally, replacing the $\lambda$:
\begin{align*}
    \delta L 
    &= 
    \frac{1}{2}\left[
        \lambda^2 [H^{-1}]_{qq}
        - \lambda \nabla_w L^T H^{-1} e_q
        + \lambda e_q^T H^{-1} \nabla_w L
        - \nabla_w L^T H^{-1} \nabla_w L
    \right]
    \nonumber \\
    &= 
    \frac{1}{2[H^{-1}]_{qq}}\left[
        (w_q - e_q^T H^{-1} \nabla_w L)^2 
        + (w_q - e_q^T H^{-1} \nabla_w L)(e_q^T H^{-1} \nabla_w L - \nabla_w L^T H^{-1} e_q)
        - \nabla_w L^T H^{-1} \nabla_w L
    \right]
    \nonumber \\
    &= 
    \frac{1}{2[H^{-1}]_{qq}}[
        w_q^2
        - 2 w_q (e_q^T H^{-1} \nabla_w L)
        + (e_q^T H^{-1} \nabla_w L)^2
        + w_q (e_q^T H^{-1} \nabla_w L)
    \nonumber \\
        &- w_q (\nabla_w L^T H^{-1} e_q)
        - (e_q^T H^{-1} \nabla_w L)(e_q^T H^{-1} \nabla_w L)
        + (e_q^T H^{-1} \nabla_w L)(\nabla_w L^T H^{-1} e_q)
        - \nabla_w L^T H^{-1} \nabla_w L
    ]
    \nonumber \\
    &= 
    \frac{1}{2[H^{-1}]_{qq}}[
        w_q^2
        - w_q (e_q^T H^{-1} \nabla_w L)
        + (e_q^T H^{-1} \nabla_w L)^2
    \nonumber \\
        &- w_q (\nabla_w L^T H^{-1} e_q)
        - (e_q^T H^{-1} \nabla_w L)^2
        + (e_q^T H^{-1} \nabla_w L)(\nabla_w L^T H^{-1} e_q)
        - \nabla_w L^T H^{-1} \nabla_w L
    ]
    \nonumber \\
    &= 
    \frac{1}{2[H^{-1}]_{qq}}\left[
        w_q^2
        - 2 w_q (e_q^T H^{-1} \nabla_w L)
        + (e_q^T H^{-1} \nabla_w L)^2
        - \nabla_w L^T H^{-1} \nabla_w L
    \right]
    \nonumber \\
    &= 
    \frac{1}{2[\hat{F}^{-1}]_{qq}}
    \left[
        w_q - (e_q^T \hat{F}^{-1} \nabla \mathcal{L}(w_0))
    \right]^2
\end{align*}

%------------------------------------------------------------------------------------------------

\newpage
\section{Training and Testing Details}
\label{appendix:training_parameters}

We perform an 80:20 stratified split, with a constant seed, on the CIFAR10/100 training dataset to obtain a validation set with the same class distribution. For both datasets, we have a training set with 40,000 samples, a validation set with 10,000 samples, and a testing set of 10,000 samples. Validation is performed after each training step, and the weights of the best-performing validation step (based on top-1 accuracy) are utilized for the final evaluation on the testing set. Table \ref{tab:table_training_parameters} summarizes the training parameters.

\begin{table}[h]
\caption{Training parameters used for ResNet18 and VGG19 on the CIFAR-10/100 datasets.}
\label{tab:table_training_parameters}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Parameter & ResNet18 & VGG19 \\
\midrule
Number of steps       & 160 & 160 \\
Criterion             & CE & CE \\
Optimizer             & SGD & SGD \\
Learning rate         & 0.01 & 0.1 \\
Momentum              & 0.9 & 0.9 \\
Weight decay          & $5 \times 10^{-4}$ & $1 \times 10^{-4}$ \\
Learning rate drops   & [60, 120] & [60, 120] \\
Learning rate drop factor & 0.2 & 0.1 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%------------------------------------------------------------------------------------------------

\newpage
\section{Results CIFAR10}
\subsection{ResNet18}
\label{appendix:CIFAR10_ResNet18}

\begin{table}[h]
\caption{Performance of different sensitivity methods for pruning evaluated using ResNet18 on the CIFAR-10 testset. The right side of the table presents our proposed criteria. The mean accuracy and standard deviation are reported across three initialization seeds for various sparsity levels. Baseline, no pruning: $91.78 \pm 0.09$.}
\label{tab:resnet18_cifar10_compressors}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity  & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.10  & 91.71 ± 0.21 & 91.72 ± 0.07 & 91.57 ± 0.15 & 91.72 ± 0.07 & 89.16 ± 0.05 & 91.87 ± 0.13 & 91.63 ± 0.21 & 91.53 ± 0.12 & 91.76 ± 0.08 \\
0.20  & 91.63 ± 0.11 & 91.42 ± 0.12 & 91.51 ± 0.09 & 91.64 ± 0.16 & 88.69 ± 0.34 & 91.50 ± 0.12 & 91.65 ± 0.14 & 91.53 ± 0.15 & 91.54 ± 0.13 \\
0.30  & 91.45 ± 0.18 & 91.61 ± 0.13 & 91.68 ± 0.20 & 91.65 ± 0.08 & 88.67 ± 0.26 & 91.65 ± 0.18 & 91.44 ± 0.27 & 91.49 ± 0.05 & 91.62 ± 0.07 \\
0.40  & 91.59 ± 0.18 & 91.06 ± 0.16 & 91.61 ± 0.09 & 91.55 ± 0.08 & 88.24 ± 0.33 & 91.51 ± 0.05 & 91.38 ± 0.13 & 91.56 ± 0.28 & 91.39 ± 0.05 \\
0.50  & 91.60 ± 0.06 & 91.32 ± 0.13 & 91.44 ± 0.13 & 91.22 ± 0.07 & 87.69 ± 0.15 & 91.30 ± 0.18 & 91.58 ± 0.16 & 91.46 ± 0.19 & 91.41 ± 0.05 \\
0.60  & 91.10 ± 0.16 & 91.18 ± 0.16 & 91.59 ± 0.13 & 91.24 ± 0.04 & 87.48 ± 0.55 & 91.34 ± 0.07 & 91.35 ± 0.16 & 91.40 ± 0.11 & 91.38 ± 0.18 \\
0.70  & 91.17 ± 0.04 & 91.07 ± 0.07 & 91.19 ± 0.17 & 91.33 ± 0.18 & 87.26 ± 0.34 & 91.34 ± 0.23 & 91.42 ± 0.23 & 91.18 ± 0.18 & 91.27 ± 0.14 \\
0.80  & 90.78 ± 0.08 & 91.10 ± 0.12 & 90.95 ± 0.35 & 90.74 ± 0.10 & 87.18 ± 0.51 & 90.95 ± 0.11 & 91.08 ± 0.06 & 90.94 ± 0.22 & 90.73 ± 0.33 \\
0.90  & 89.35 ± 0.13 & 89.88 ± 0.28 & 90.39 ± 0.23 & 90.36 ± 0.34 & 86.60 ± 0.51 & 90.04 ± 0.21 & 90.20 ± 0.08 & 90.55 ± 0.23 & 89.22 ± 0.30 \\
0.95  & 87.59 ± 0.11 & 89.23 ± 0.19 & 89.00 ± 0.05 & 89.31 ± 0.17 & 86.50 ± 0.05 & 88.61 ± 0.28 & 89.50 ± 0.18 & 89.47 ± 0.32 & 87.58 ± 0.25 \\
0.98  & 83.47 ± 0.20 & 85.70 ± 0.33 & 86.43 ± 0.05 & 87.26 ± 0.28 & 85.99 ± 0.08 & 85.61 ± 0.20 & 86.97 ± 0.22 & 87.24 ± 0.32 & 83.40 ± 0.74 \\
0.99  & 78.28 ± 0.45 & 71.99 ± 0.28 & 83.47 ± 0.15 & 84.54 ± 0.04 & 84.56 ± 0.46 & 82.13 ± 0.28 & 83.74 ± 0.48 & 84.85 ± 0.18 & 77.60 ± 1.02 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%------------------------------------------------------------------------------------------------
\clearpage
\subsection{VGG19}
\label{appendix:CIFAR10_VGG19}

As discussed earlier, introducing a warm-up phase effectively mitigates layer collapse in data-dependent pruning methods. Here, we evaluate the impact of different warm-up durations by comparing no warm-up, a single warm-up epoch, and five warm-up epochs. Table \ref{tab:VGG19_cifar10_compressors} demonstrates how performance drastically degrades with increasing sparsity, ultimately leading to layer collapse at 0.90 sparsity. However, as shown in the results, a single warm-up epoch is sufficient to prevent collapse and stabilize pruning performance. Moreover, as seen in Table \ref{tab:VGG19_cifar10_compressors_warmup5}, increasing the warm-up period to five epochs provides no substantial additional improvement. This indicates that prolonged warm-up training is not necessary; a single training step is enough to achieve gradient stabilization and overcome layer collapse.

\begin{table}[h]
\caption{Performance of different sensitivity methods for pruning evaluated using VGG19 on the CIFAR-10 test set. The right side of the table presents our proposed criteria. The mean accuracy and standard deviation are reported across three initialization seeds for various sparsity levels. Baseline, no pruning: $89.21 \pm 0.22$.}
\label{tab:VGG19_cifar10_compressors}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity  & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.10  & 88.40 ± 0.95 & 89.12 ± 0.55 & 90.14 ± 0.10 & 90.16 ± 0.18 & 87.81 ± 1.66 & 90.20 ± 0.29 & 90.21 ± 0.37 & 90.25 ± 0.38 & 89.06 ± 0.75 \\
0.20  & 89.19 ± 0.22 & 89.65 ± 0.60 & 89.59 ± 0.69 & 90.06 ± 0.04 & 89.57 ± 0.34 & 89.91 ± 0.28 & 90.28 ± 0.55 & 89.80 ± 0.28 & 88.89 ± 0.76 \\
0.30  & 88.93 ± 0.83 & 88.77 ± 1.07 & 90.23 ± 0.09 & 89.88 ± 0.59 & 89.14 ± 0.19 & 90.25 ± 0.09 & 89.97 ± 0.26 & 90.46 ± 0.41 & 89.06 ± 0.36 \\
0.40  & 88.28 ± 1.08 & 89.38 ± 0.53 & 90.50 ± 0.23 & 89.79 ± 0.67 & 88.20 ± 0.31 & 90.51 ± 0.12 & 90.37 ± 0.24 & 90.23 ± 0.14 & 10.00 ± 0.00 \\
0.50  & 88.96 ± 0.82 & 89.03 ± 0.59 & 90.46 ± 0.60 & 90.38 ± 0.25 & 88.67 ± 0.23 & 89.54 ± 0.86 & 90.47 ± 0.52 & 90.19 ± 0.31 & 10.00 ± 0.00 \\
0.60  & 88.15 ± 0.68 & 89.47 ± 0.18 & 89.95 ± 0.30 & 90.32 ± 0.25 & 88.82 ± 0.32 & 90.02 ± 0.40 & 90.18 ± 0.33 & 90.14 ± 0.36 & 10.00 ± 0.00 \\
0.70  & 88.02 ± 0.53 & 89.63 ± 0.44 & 89.69 ± 0.42 & 89.23 ± 0.19 & 89.62 ± 0.81 & 89.85 ± 0.08 & 90.01 ± 0.34 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.80  & 88.28 ± 0.34 & 89.62 ± 0.91 & 85.72 ± 0.63 & 89.39 ± 0.43 & 88.82 ± 0.14 & 10.00 ± 0.00 & 88.29 ± 0.11 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.90  & 85.82 ± 0.19 & 89.29 ± 0.79 & 10.00 ± 0.00 & 80.85 ± 0.62 & 24.28 ± 20.2 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.95  & 84.41 ± 0.05 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.98  & 80.04 ± 0.90 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.99  & 76.89 ± 0.26 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\newpage
%------------------------------------------------------------------------------------------------
\begin{table*}[h]
\caption{Performance of different compression methods evaluated after 1 warmup epoch using VGG19 on the CIFAR-10 dataset. We report the mean accuracy between three initialization seeds across various sparsity levels. Baseline, no pruning: $89.21 \pm 0.22$.}
\label{tab:VGG19_cifar10_compressors_warmup1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity  & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.80  & 88.73 ± 0.38 & 88.35 ± 0.54 & 86.76 ± 0.27 & 87.39 ± 0.66 & 87.24 ± 0.25 & 87.14 ± 0.45 & 87.00 ± 0.87 & 87.68 ± 0.33 & 64.33 ± 15.91 \\
0.90  & 87.26 ± 0.42 & 88.62 ± 0.49 & 85.96 ± 0.75 & 86.75 ± 0.76 & 87.47 ± 0.33 & 86.69 ± 0.72 & 87.09 ± 0.31 & 87.42 ± 0.21 & 46.16 ± 7.62 \\
0.95  & 85.47 ± 0.64 & 87.68 ± 0.49 & 86.66 ± 0.27 & 86.00 ± 1.10 & 86.71 ± 1.24 & 85.71 ± 1.35 & 86.73 ± 0.36 & 87.56 ± 0.62 & 46.30 ± 5.32 \\
0.98  & 80.44 ± 0.30 & 86.61 ± 0.62 & 84.72 ± 1.69 & 87.22 ± 0.23 & 86.45 ± 0.64 & 80.34 ± 6.43 & 86.07 ± 0.39 & 86.36 ± 0.29 & 49.05 ± 4.31 \\
0.99  & 77.24 ± 0.73 & 83.69 ± 1.36 & 80.28 ± 2.04 & 83.49 ± 1.77 & 85.39 ± 0.43 & 75.11 ± 7.80 & 84.40 ± 1.27 & 85.35 ± 1.05 & 47.10 ± 4.41 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*} 
%------------------------------------------------------------------------------------------------

\begin{table}[h]
\caption{Performance of different sensitivity methods for pruning evaluated after 5 warmup epochs using VGG19 on the CIFAR-10 testset. The right side of the table presents our proposed criteria. The mean accuracy and standard deviation are reported across three initialization seeds for various sparsity levels. Baseline, no pruning: $89.21 \pm 0.22$.}
\label{tab:VGG19_cifar10_compressors_warmup5}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity  & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.80  & 88.84 ± 0.43 & 88.41 ± 0.47 & 87.58 ± 0.52 & 88.15 ± 1.09 & 86.77 ± 1.14 & 87.28 ± 0.90 & 88.22 ± 0.82 & 86.68 ± 0.61 & 70.52 ± 9.25 \\
0.90  & 87.56 ± 0.62 & 88.60 ± 0.93 & 86.73 ± 0.37 & 87.89 ± 0.25 & 87.10 ± 0.47 & 87.50 ± 1.42 & 88.18 ± 0.47 & 86.98 ± 0.14 & 47.78 ± 1.26 \\
0.95 & 85.51 ± 0.69 & 87.66 ± 1.19 & 87.44 ± 0.46 & 87.71 ± 0.82 & 87.05 ± 0.16 & 86.83 ± 1.47 & 87.36 ± 0.52 & 87.00 ± 0.74 & 48.83 ± 2.52 \\
0.98 & 82.09 ± 0.17 & 86.24 ± 0.52 & 84.66 ± 1.33 & 86.55 ± 0.84 & 86.04 ± 0.66 & 85.44 ± 0.64 & 86.64 ± 0.13 & 84.89 ± 0.51 & 49.48 ± 0.85 \\
0.99 & 77.22 ± 1.03 & 83.93 ± 1.80 & 81.62 ± 2.17 & 84.53 ± 0.70 & 81.33 ± 5.77 & 81.71 ± 1.41 & 85.02 ± 0.69 & 83.78 ± 0.80 & 41.24 ± 1.55 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%------------------------------------------------------------------------------------------------

\newpage
\section{Results CIFAR100}
\subsection{ResNet18}
\label{sec:resnet_cifar-100}

CIFAR-100 results exhibit a similar trend to those observed on CIFAR-10, further reinforcing the robustness of our proposed Fisher-Taylor Sensitivity (FTS) criterion. Across all evaluated sparsity levels, FTS consistently maintains strong performance, frequently ranking among the top-performing methods. This trend is particularly evident at extreme sparsities, where many pruning approaches suffer significant performance degradation. The stability of FTS across both datasets highlights its effectiveness in preserving network expressivity despite aggressive pruning.

\begin{table}[h]
\caption{Performance of different compression methods evaluated using ResNet18 on the CIFAR-100 dataset. We report the mean accuracy between three initialization seeds across various sparsity levels. Baseline, no pruning: $69.57 \pm 0.19$.}
\label{tab:resnet18_cifar100_compressors}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity  & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.10  & 69.16 ± 0.11 & 69.37 ± 0.14 & 69.63 ± 0.34 & 69.42 ± 0.07 & 64.26 ± 0.27 & 69.66 ± 0.30 & 69.08 ± 0.21 & 69.16 ± 0.11 & 69.07 ± 0.10 \\
0.20  & 69.16 ± 0.30 & 69.06 ± 0.24 & 69.19 ± 0.11 & 69.30 ± 0.08 & 63.28 ± 0.58 & 69.60 ± 0.30 & 69.35 ± 0.35 & 69.41 ± 0.43 & 69.07 ± 0.20 \\
0.30  & 69.36 ± 0.18 & 68.58 ± 0.36 & 69.37 ± 0.13 & 68.82 ± 0.17 & 62.02 ± 0.43 & 69.24 ± 0.40 & 68.84 ± 0.13 & 68.80 ± 0.55 & 68.96 ± 0.11 \\
0.40  & 69.41 ± 0.20 & 68.50 ± 0.29 & 69.16 ± 0.26 & 68.95 ± 0.19 & 61.18 ± 0.19 & 69.17 ± 0.16 & 68.88 ± 0.25 & 69.02 ± 0.21 & 68.92 ± 0.25 \\
0.50  & 69.12 ± 0.46 & 68.17 ± 0.20 & 68.94 ± 0.20 & 68.63 ± 0.11 & 61.11 ± 0.40 & 69.13 ± 0.13 & 68.68 ± 0.12 & 68.71 ± 0.12 & 68.71 ± 0.57 \\
0.60  & 68.66 ± 0.27 & 67.78 ± 0.35 & 68.77 ± 0.17 & 68.63 ± 0.42 & 61.40 ± 0.78 & 68.34 ± 0.43 & 67.98 ± 0.23 & 68.41 ± 0.14 & 68.60 ± 0.15 \\
0.70  & 67.95 ± 0.43 & 67.51 ± 0.24 & 68.29 ± 0.39 & 68.08 ± 0.18 & 59.43 ± 0.76 & 68.03 ± 0.46 & 67.96 ± 0.15 & 68.29 ± 0.06 & 68.16 ± 0.07 \\
0.80  & 67.26 ± 0.48 & 66.55 ± 0.19 & 67.20 ± 0.37 & 67.21 ± 0.38 & 59.08 ± 0.22 & 66.70 ± 0.05 & 67.05 ± 0.06 & 66.77 ± 0.65 & 66.62 ± 0.43 \\
0.90  & 64.75 ± 0.16 & 64.48 ± 0.18 & 64.87 ± 0.27 & 65.70 ± 0.08 & 59.16 ± 0.91 & 64.74 ± 0.44 & 65.46 ± 0.30 & 65.41 ± 0.13 & 63.90 ± 0.31 \\
0.95  & 61.01 ± 0.32 & 62.20 ± 0.06 & 62.20 ± 0.23 & 63.20 ± 0.20 & 57.91 ± 0.09 & 62.14 ± 0.42 & 63.22 ± 0.25 & 63.21 ± 0.47 & 61.25 ± 0.44 \\
0.98  & 54.72 ± 0.22 & 55.44 ± 0.18 & 57.34 ± 0.31 & 58.83 ± 0.35 & 54.85 ± 0.35 & 55.57 ± 0.17 & 58.05 ± 0.18 & 58.59 ± 0.12 & 55.02 ± 0.34 \\
0.99  & 45.62 ± 0.55 & 40.39 ± 0.36 & 50.46 ± 0.61 & 52.96 ± 0.10 & 49.13 ± 0.19 & 48.02 ± 0.32 & 49.98 ± 0.60 & 52.85 ± 0.24 & 44.91 ± 0.52 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%------------------------------------------------------------------------------------------------
\clearpage
\subsection{VGG19}
The results on VGG19 with CIFAR-100 exhibit a similar trend to those observed on CIFAR-10, reinforcing the effectiveness of our proposed approach. Once again, we identify the occurrence of layer collapse at extreme sparsities when no warm-up is applied, leading to a significant drop in accuracy. Introducing a single warm-up epoch effectively resolves this issue, restoring pruning performance across all evaluated criteria. However, increasing the warm-up phase to five epochs does not yield any additional advantage, indicating that a brief warm-up period is sufficient to stabilize gradient-based importance scores and prevent collapse.

\label{sec:vgg_cifar-100}

\begin{table}[h]
\caption{Performance of different compression methods evaluated using VGG19 on the CIFAR-100 dataset. We report the mean accuracy between three initialization seeds across various sparsity levels. Baseline, no pruning: $58.96 \pm 2.30$.}
\label{tab:VGG19_cifar100_compressors}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.10  & 60.31 ± 0.40 & 59.13 ± 1.29 & 61.93 ± 0.48 & 61.98 ± 0.29 & 59.32 ± 0.63 & 62.13 ± 0.61 & 60.45 ± 3.47 & 61.56 ± 1.04 & 58.79 ± 0.98 \\
0.20  & 60.43 ± 1.14 & 59.27 ± 0.34 & 62.64 ± 0.21 & 62.68 ± 0.24 & 61.21 ± 0.41 & 63.04 ± 0.43 & 62.71 ± 1.02 & 62.24 ± 0.44 & 60.48 ± 0.48 \\
0.30  & 58.32 ± 0.60 & 59.35 ± 1.43 & 62.61 ± 0.23 & 63.11 ± 0.35 & 59.30 ± 0.43 & 62.85 ± 0.42 & 61.43 ± 0.61 & 62.65 ± 0.54 & 58.77 ± 1.02 \\
0.40  & 56.50 ± 3.20 & 60.04 ± 1.02 & 62.36 ± 0.02 & 62.39 ± 0.55 & 56.34 ± 1.49 & 62.38 ± 0.75 & 61.56 ± 1.25 & 62.67 ± 0.06 & 1.00 ± 0.00 \\
0.50  & 58.47 ± 1.49 & 61.49 ± 1.22 & 62.02 ± 0.64 & 62.76 ± 0.50 & 54.43 ± 0.84 & 62.84 ± 0.33 & 62.25 ± 0.33 & 62.47 ± 0.42 & 1.00 ± 0.00 \\
0.60  & 57.54 ± 0.74 & 61.50 ± 0.30 & 62.55 ± 0.13 & 63.08 ± 0.55 & 56.76 ± 0.69 & 62.40 ± 0.57 & 62.70 ± 0.63 & 62.17 ± 0.23 & 1.00 ± 0.00 \\
0.70  & 57.63 ± 0.80 & 61.71 ± 0.25 & 60.85 ± 0.79 & 60.58 ± 0.39 & 57.76 ± 0.84 & 60.44 ± 0.34 & 60.92 ± 0.41 & 60.51 ± 1.67 & 1.00 ± 0.00 \\
0.80  & 57.84 ± 0.57 & 61.89 ± 1.02 & 55.09 ± 0.49 & 59.84 ± 0.29 & 58.39 ± 0.74 & 1.00 ± 0.00 & 43.16 ± 1.02 & 58.66 ± 2.28 & 1.00 ± 0.00 \\
0.90  & 58.41 ± 0.41 & 62.60 ± 0.91 & 1.00 ± 0.00 & 8.35 ± 10.39 & 42.88 ± 1.64 & 1.00 ± 0.00 & 1.00 ± 0.00 & 8.87 ± 11.13 & 1.00 ± 0.00 \\
0.95  & 54.84 ± 1.08 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 \\
0.98  & 50.21 ± 0.72 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 \\
0.99  & 46.69 ± 0.45 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 & 1.00 ± 0.00 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

%------------------------------------------------------------------------------------------------

\begin{table}[h]
\caption{Performance of different compression methods evaluated after 1 warmup epoch using VGG19 on the CIFAR-100 dataset. We report the mean accuracy between three initialization seeds across various sparsity levels. Baseline, no pruning: $58.96 \pm 2.30$.}
\label{tab:VGG19_cifar100_compressors_warmup1}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.80  & 60.39 ± 1.16 & 58.91 ± 0.41 & 52.81 ± 1.32 & 55.62 ± 2.27 & 55.15 ± 2.25 & 56.71 ± 0.31 & 58.03 ± 0.93 & 52.41 ± 3.07 & 52.74 ± 5.16 \\
0.90  & 58.90 ± 0.98 & 60.95 ± 0.81 & 50.56 ± 4.59 & 55.89 ± 2.05 & 56.01 ± 1.58 & 52.07 ± 3.24 & 53.65 ± 0.57 & 52.45 ± 3.75 & 19.65 ± 1.68 \\
0.95  & 56.10 ± 0.85 & 57.64 ± 2.63 & 50.34 ± 1.00 & 53.70 ± 3.60 & 56.16 ± 0.41 & 54.44 ± 1.38 & 53.24 ± 3.54 & 53.56 ± 1.26 & 17.24 ± 0.44 \\
0.98  & 50.97 ± 0.40 & 54.66 ± 2.56 & 43.43 ± 5.32 & 50.19 ± 1.59 & 54.64 ± 1.50 & 42.75 ± 1.91 & 50.59 ± 3.39 & 48.56 ± 5.25 & 16.42 ± 0.64 \\
0.99  & 46.52 ± 0.45 & 43.33 ± 5.83 & 33.90 ± 5.35 & 42.65 ± 5.32 & 45.98 ± 4.48 & 29.67 ± 8.49 & 49.11 ± 3.46 & 48.70 ± 2.59 & 13.25 ± 0.84 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


%------------------------------------------------------------------------------------------------

\begin{table}[h]
\caption{Performance of different compression methods evaluated after 5 warmup epochs using VGG19 on the CIFAR-100 dataset. We report the mean accuracy between three initialization seeds across various sparsity levels. Baseline, no pruning: $58.96 \pm 2.30$.}
\label{tab:VGG19_cifar100_compressors_warmup5}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.80  & 60.41 ± 1.39 & 58.38 ± 0.85 & 60.86 ± 0.79 & 61.63 ± 0.45 & 56.25 ± 0.49 & 59.59 ± 0.76 & 59.37 ± 3.50 & 60.86 ± 0.53 & 46.93 ± 9.04 \\
0.90  & 60.32 ± 0.09 & 57.74 ± 1.64 & 57.77 ± 2.41 & 58.23 ± 4.07 & 56.27 ± 1.02 & 60.19 ± 0.63 & 61.23 ± 0.50 & 60.52 ± 0.37 & 21.66 ± 1.95 \\
0.95 & 57.86 ± 0.53 & 59.55 ± 1.15 & 56.09 ± 0.97 & 58.83 ± 0.65 & 55.26 ± 1.25 & 55.80 ± 2.77 & 59.83 ± 0.94 & 58.52 ± 1.32 & 19.98 ± 2.62 \\
0.98 & 51.75 ± 0.43 & 47.75 ± 7.63 & 52.26 ± 4.06 & 55.27 ± 1.69 & 54.59 ± 0.96 & 49.46 ± 4.98 & 57.40 ± 1.26 & 56.00 ± 1.08 & 17.59 ± 1.36 \\
0.99 & 47.59 ± 0.80 & 42.46 ± 7.95 & 46.58 ± 2.00 & 53.13 ± 0.84 & 53.91 ± 1.53 & 42.87 ± 4.63 & 53.17 ± 1.18 & 53.05 ± 2.14 & 13.92 ± 0.14 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


%------------------------------------------------------------------------------------------------
\clearpage

\section{Mask Batch Size for Other Sparsities}
The Effect of batch size on pruning performance across different sparsities. 
As sparsity increases, the effect of batch size on pruning performance becomes more pronounced. 
At lower sparsities (0.90, 0.95), the differences across batch sizes are less evident, suggesting that even smaller batches provide a reasonable estimation of parameter importance. However, at extreme sparsities (0.98, 0.99), we observe a clear trend where larger batch sizes consistently lead to better parameter selection, ultimately improving accuracy. This aligns with our hypothesis that larger batches help reduce variance in gradient estimation, leading to more stable and effective pruning decisions. 
\label{batch_size_heatmaps}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{imgs/cifar10_resnet18_heatmap_warmup_0.png}
    \caption{Effect of batch size on pruning performance at increasing sparsities.}
    \label{fig:enter-label}
\end{figure}

%------------------------------------------------------------------------------------------------

\clearpage
\section{Comparison of our criteria with magnitude-based pruning}

Figure \ref{fig:our_criterion_vs_magnitude} illustrates the relationship between parameter magnitude and different sensitivity-based pruning metrics. Each point represents a model parameter, with red points indicating the top-ranked parameters selected for retention by each criterion. The green dashed line marks the 99th percentile of parameter magnitudes.

A key observation is that the most effective pruning criteria, such as Fisher-Taylor Sensitivity, tend to retain parameters with a broad range of magnitudes, including many that are relatively small (left of the green line). This shows that the estimated importance does not always prioritize parameters based on their magnitude. 


\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\linewidth]{imgs/cifar_10_mag_vs_criteria_s_99.png}
    \caption{Our criteria vs. Magnitude parameter selection for 99\% sparsity (ResNet18, CIFAR-10, Seed 0)} 
    \label{fig:our_criterion_vs_magnitude}
\end{figure}
