\section{Results and Discussion}
To evaluate the effectiveness of the different pruning criteria presented, we performed experiments on architectures and datasets commonly used in the pruning literature, specifically on the ResNet18 and VGG19 architectures using CIFAR-10 and CIFAR-100 datasets, training details available in the Appendix \ref{appendix:training_parameters}. For comparison, we evaluate our proposed criteria against the following: random, parameter magnitude, gradient norm (GN), SNIP \cite{lee2018snip}, and GraSP \cite{wang2020picking}.
% -----------------------------------------------------------------------
\begin{table*}[t]
\caption{Performance of different sensitivity methods for pruning evaluated using ResNet18 on the CIFAR-10 test set. The right side of the table presents our proposed criteria. The mean accuracy and standard deviation are reported across three initialization seeds for various sparsity ratios. Bold values highlight the method with the best-performing mean accuracy, while the methods with matching performance are underscored (their mean lies within the standard deviation range of the best-performing method) for each sparsity. Baseline with no pruning: $91.78 \pm 0.09$. Full table with smaller sparsity ratios available in Appendix \ref{appendix:CIFAR10_ResNet18}.}
\label{tab:resnet18_cifar10_compressors_cut}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity (\%) & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
0.80  & 90.78 ± 0.08 & \textbf{91.10 ± 0.12} & 90.95 ± 0.35 & 90.74 ± 0.10 & 87.18 ± 0.51 & 90.95 ± 0.11 & \underline{91.08 ± 0.06} & 90.94 ± 0.22 & 90.73 ± 0.33 \\
0.90  & 89.35 ± 0.13 & 89.88 ± 0.28 & \underline{90.39 ± 0.23} & \underline{90.36 ± 0.34} & 86.60 ± 0.51 & 90.04 ± 0.21 & 90.20 ± 0.08 & \textbf{90.55 ± 0.23} & 89.22 ± 0.30 \\
0.95 & 87.59 ± 0.11 & 89.23 ± 0.19 & 89.00 ± 0.05 & 89.31 ± 0.17 & 86.50 ± 0.05 & 88.61 ± 0.28 & \textbf{89.50 ± 0.18} & \underline{89.47 ± 0.32} & 87.58 ± 0.25 \\
0.98 & 83.47 ± 0.20 & 85.70 ± 0.33 & 86.43 ± 0.05 & \textbf{87.26 ± 0.28} & 85.99 ± 0.08 & 85.61 ± 0.20 & \underline{86.97 ± 0.22} & \underline{87.24 ± 0.32} & 83.40 ± 0.74 \\
0.99 & 78.28 ± 0.45 & 71.99 ± 0.28 & 83.47 ± 0.15 & 84.54 ± 0.04 & 84.56 ± 0.46 & 82.13 ± 0.28 & 83.74 ± 0.48 & \textbf{84.85 ± 0.18} & 77.60 ± 1.02 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
% -----------------------------------------------------------------------

\textbf{Performance Analysis of Pruning Methods}. Table~\ref{tab:resnet18_cifar10_compressors_cut} summarizes the performance of the different criteria evaluated in the ResNet18 architecture with the CIFAR-10 test set. We observe that the architecture is very robust to pruning, given that all criteria perform on par with the baseline up to a sparsity ratio of $0.70$ (Appendix \ref{appendix:CIFAR10_ResNet18}). Therefore, our analysis focuses on \textbf{\textit{high sparsities}} (\(0.80 \leq \text{sparsity} \leq 0.95\)) and \textbf{\textit{extreme sparsities}} (\(\text{sparsity} > 0.95\)), where the differences among the pruning criteria are more pronounced. Magnitude pruning is a viable criterion up to a sparsity ratio of $0.80$, where it achieves the highest accuracy (\(91.10 \pm 0.12\%\)) amongst all methods, with the alternative method FP matching its performance ($91.08 \pm 0.06$). 

Nevertheless, we observe a rapid decay as we increase the sparsity ratio, where magnitude pruning yields the worst performance ($71.99 \pm 0.28$) at the highest sparsity, even lower than random ($78.28 \pm 0.45$). This further strengthens the argument that a better-principled criterion is required for efficient training of pruned models \cite{lecun1989optimal}. 

With respect to the proposed FTS sensitivity criterion, our method constantly outperforms or matches the top performer for the rest of the sparsity ratios. FTS achieves the highest performance in the sparsities $0.90$ and $0.99$, with the pruned models achieving the accuracies of $90.55 \pm 0.23$ and $84.85 \pm 0.18$, respectively. In high sparsity $0.95$, FP and FTS criteria achieve the matching accuracies of $89.50 \pm 0.18$ and $89.47 \pm 0.32$. Demonstrating the effectiveness of incorporating second-order information to identify the most important parameters for model performance. SNIP achieves superior performance only in sparsity ratio $0.98$ ($87.26 \pm 0.28$), with FTS matching it ($87.24 \pm 0.32$). However, SNIP fails to be a consistent top performer, unlike our FTS criteria.

On the other hand, we consistently observe subpar performance for the criteria of GN, FD, GraSP, and FBSS across all high and extreme sparsity ratios. First, the GN criterion effectively captures how each parameter contributes to the loss landscape, but the lack of curvature information makes it less reliable as the sparsity increases. Second, the FD criterion performs better than the GN criterion at most sparsity ratios. Once again, the effectiveness of leveraging second-order information is underscored. Still, FD remains limited when compared to a criterion such as FTS, which incorporates first- and second-order information with the effect of the parameter. Third, the GraSP criterion fails to be competitive in our experimental setting. Its performance is notable at only one sparsity, further compounded by its high computational cost due to the Hessian-gradient product calculation. Lastly, FBSS is also unable to achieve competitive results. Despite the theoretical improvement over FTS, given the refined weight selection, FBSS underperforms for extreme sparsity ratios. We attribute this behavior to the fundamental limitations of employing the FIM diagonal as an approximation of the full matrix, failing to incorporate the off-diagonal interactions in the Fisher-gradient product, see \eqref{eq:FBSS}. We observe similar trends on the CIFAR-100 dataset. For additional details, refer to Section \ref{sec:resnet_cifar-100} in the appendix.

These results highlight several key insights: (1) Given the robustness of the ResNet architecture, random masks yield reasonable performance at moderate sparsity rates in the challenging PBT setting. However, its effectiveness diminishes rapidly as the sparsity increases, underscoring the need for principled pruning strategies; (2) First-order and magnitude-based criteria find efficient and performant subnetworks even at high sparsity ratios, but their performance degrades at extreme settings. Stressing the necessity for more advanced methods that incorporate higher-order information. This notion was previously studied by \citet{yvinec2022singe}. The authors argue that magnitude-based approaches may remove neurons with low magnitude without considering their contribution to the training. Conversely, gradient-based methods depend on the intrinsic locality of the gradient. Therefore, pruning may break this locality principle when abruptly removing connections. We demonstrate the notion empirically in Figure \ref{fig:cifar_10_mag_vs_fts_s_99}, which plots the relation between the best-performing FTS criterion and the magnitude-based approach. (3) Our proposed FTS criterion consistently ranks among the best-performing methods in high and extreme sparsity ratios, and the alternative FP criterion consistently matches the top performer in the same settings. This demonstrates that the FIM diagonal is a cheap and efficient approximation to leverage second-order information for pruning at initialization.
% -----------------------------------------------------------------------
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/cifar_10_mag_vs_fts_s_99.png}
    \caption{FTS vs. Magnitude parameter selection for 99\% sparsity (ResNet18, CIFAR-10, Seed 0): We evaluated the relationship between the magnitude of the parameters and our best-performing criterion. The red area represents the parameters selected by our criterion. In contrast, the area to the right of the vertical line represents those that would have been selected if we chose the magnitude-based approach.}
    \label{fig:cifar_10_mag_vs_fts_s_99}
\end{figure}
% -----------------------------------------------------------------------
\begin{table*}[ht]
\caption{Performance of different pruning methods evaluated on VGG19 with CIFAR-10 at extreme sparsity ratios. The table compares results with and without a 1-epoch warm-up. Bold values highlight the best-performing mean accuracy, while methods with comparable performance (within the best method's standard deviation) are underlined.}
\label{tab:vgg19_cifar10_warmup}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\toprule
Sparsity (\%) & Random & Magnitude & GN & SNIP & GraSP & FD & FP & FTS & FBSS \\
\midrule
\multicolumn{10}{c}{\textbf{No Warm-Up}} \\
0.98 & 80.04 ± 0.90 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
0.99 & 76.89 ± 0.26 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 & 10.00 ± 0.00 \\
\midrule
\multicolumn{10}{c}{\textbf{1-Epoch Warm-Up}} \\
0.98 & 83.47 ± 0.20 & 85.70 ± 0.33 & 86.43 ± 0.05 & 87.26 ± 0.28 & 85.99 ± 0.08 & 85.61 ±0 .20 & 86.97 ± 0.22 & 87.24 ± 0.32 & 83.40 ± 0.74 \\
0.99 & 78.28 ± 0.45 & 71.99 ± 0.28 & 83.47 ± 0.15 & 84.54 ± 0.04 & 84.56 ± 0.46 & 82.13 ± 0.28 & 83.74 ± 0.48 & 84.85 ± 0.18 & 77.60 ± 1.02 \\
\bottomrule
\end{tabular}}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
% -----------------------------------------------------------------------
\newpage
\textbf{Preventing Layer Collapse.} Table~\ref{tab:VGG19_cifar10_compressors} (Appendix~\ref{appendix:CIFAR10_VGG19}) summarizes the performance of different pruning criteria evaluated on the VGG19 architecture with the CIFAR-10 test set. For this architecture, all data-dependent pruning methods suffer a drastic performance drop at high and extreme sparsities, completely reducing the accuracy to \(10.00\%\). This behavior is consistent with the \textit{layer collapse} phenomenon described in Section \ref{collapse}, where pruning removes entire layers (or most of their parameters), severely disrupting the flow of information and rendering the network untrainable.

A simple yet effective solution to mitigate layer collapse is the introduction of a single warm-up epoch before pruning. This step stabilizes the gradients and ensures a more balanced pruning distribution across layers. Table~\ref{tab:vgg19_cifar10_warmup} demonstrates the impact of this adjustment, illustrating that just minimal adjustment before pruning helps maintain information flow and prevents layer collapse, allowing successful learning in the pruned network. We observe similar trends on the CIFAR-100 dataset. In the appendix, refer to Section \ref{sec:vgg_cifar-100}.

\textbf{Batch Size FIM Estimation.} Figure \ref{fig:heatmap_s_99} demonstrates the positive effect of utilizing a batched gradient-based estimation of the FIM diagonal for extreme sparsity ratios. We demonstrate that, especially at initialization, reducing the variance of our estimation utilizing large batch size $|B^k|$ is critical to effectively measure the sensitivity of parameters. Especially for the FBSS criterion, the batched FIM estimation might compensate for the limited diagonal representation. Given the successful performance regardless of the selected batch size, we highlight the robustness to noise of the proposed FTS criterion. We share details for other sparsity ratios in Appendix \ref{batch_size_heatmaps}.
\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\linewidth]{imgs/cifar10_resnet18_heatmap_warmup_0_s_99.png}
    \caption{Effect of batch size of the mask in the final model accuracy for ResNet18 using CIFAR-10 dataset with 99\% sparsity.}
    \label{fig:heatmap_s_99}
\end{figure}