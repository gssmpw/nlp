\begin{abstract}
Pruning offers a promising solution to mitigate the associated costs and environmental impact of deploying large deep neural networks (DNNs). Traditional approaches rely on computationally expensive trained models or time-consuming iterative prune-retrain cycles, undermining their utility in resource-constrained settings. To address this issue, we build upon the established principles of \textit{saliency} \cite{lecun1989optimal} and \textit{connection sensitivity} \cite{lee2018snip} to tackle the challenging problem of one-shot pruning neural networks (NNs) before training (PBT) at initialization.
We introduce \textit{Fisher-Taylor Sensitivity (FTS)}, a computationally cheap and efficient pruning criterion based on the empirical Fisher Information Matrix (FIM) diagonal, offering a viable alternative for integrating first- and second-order information to identify a model's structurally important parameters. Although the FIM-Hessian equivalency only holds for convergent models that maximize the likelihood, recent studies \cite{karakida2019universal} suggest that, even at initialization, the FIM captures essential geometric information of parameters in overparameterized NNs, providing the basis for our method.
Finally, we demonstrate empirically that \textit{layer collapse}, a critical limitation of \textit{data-dependent} pruning methodologies, is easily overcome by pruning within a single training epoch after initialization.
We perform experiments on ResNet18 and VGG19 with CIFAR-10 and CIFAR-100, widely used benchmarks in pruning research. Our method achieves competitive performance against state-of-the-art techniques for one-shot PBT, even under extreme sparsity conditions.
Our code is made available to the public\footnote{\url{https://github.com/Gollini/Fisher_Taylor_Sensitivity}}.
\end{abstract}