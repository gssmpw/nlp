\section{Introduction}

We hear of breakthroughs daily thanks to Artificial Intelligence (AI) systems. The exponential increase in computing power and data availability has allowed success stories in robotics \cite{soori2023artificial}, computer vision \cite{khan2020machine}, natural language processing \cite{torfi2020natural}, healthcare \cite{habehh2021machine}, and many other fields. Naturally, new challenges have emerged alongside this progress, particularly the growing size of AI systems and the associated high training and inference costs \cite{han2015learning}. The ``bigger is better" approach goes against real-life applications that require high accuracy and efficient resource usage. Furthermore, climate change awareness highlights the environmental impact of AI, with inference being a significant contributor to model-associated carbon emissions \cite{wu2022sustainable, chien2023reducing}. In addition, computational resources, energy, and bandwidth limit the deployment of large systems on edge devices \cite{cheng2024survey}.

To overcome these challenges, researchers have developed techniques to address the constantly growing model sizes and associated costs of Deep Neural Networks (DNNs). The most prominent are quantization \cite{dettmers2023qlora}, low-rank factorization \cite{denton2014exploiting}, knowledge distillation \cite{xu2024survey}, neural architecture search \cite{zhang2021idarts}, and neural network pruning \cite{cheng2024survey}. The latest stands out for its ability to dramatically decrease the model size, reducing storage memory for models and computation workload during training or inference while maintaining performance compared to the original network. \citet{cheng2024survey} proposed a comprehensive taxonomy highlighting different aspects to consider in existing pruning techniques based on three questions: (1) Does the method achieve universal or specific acceleration through neural network pruning? (2) When does the pruning happen in the training pipeline? and (3) Is there a predefined pruning criterion, or is it learnable?

Regardless of the kind of acceleration or criterion, pruning after training (PAT) methods are investigated more frequently given the advantage of working with converged models, which implies parameters whose values are close to optimal and provide more information than randomly initialized ones \cite{kumar2024no}. A simple but very effective strategy explored by \citet{han2015learning} involves masking (setting to zero) parameters whose magnitudes are below a given threshold. The removal of a fraction of unimportant connections reduces operations and accelerates inference. More principled approaches utilize first-order and/or second-order information to measure the loss change induced by pruning low-information parameters \cite{lecun1989optimal, hassibi1992second}.

The advantages of PAT come at the expense of training a fully dense network and the required post-finetuning and/or readjustment of parameters. Consequently, there is an active interest in the research community in performing pruning during (PDT) or before (PBT) training. The milestone work of \citet{frankle2018lottery} empirically showed the existence of sparse subnetworks or ``winning tickets" that match or even surpass the performance of the dense model utilizing the same parameter initialization as the dense network. Their algorithm iteratively prunes and rewinds the network parameters to uncover these subnetworks. Since then, there has been an effort to find efficient subnetworks quicker to reduce expensive prune-retrain cycles \cite{sreenivasan2022rare, you2022supertickets}, or at initialization pioneered by \citet{lee2018snip}.

We take inspiration from classical \cite{lecun1989optimal, hassibi1992second} and modern \cite{lee2018snip, singh2020woodfisher} techniques and introduce \textit{Fisher-Taylor Sensitivity} (FTS), a pruning criterion to operate in the challenging one-shot PBT setting, where performant subnetworks must be identified in a single step at initialization to minimize computation overhead. Our approach approximates the objective function using a Taylor expansion to measure the \textit{sensitivity} of parameters at initialization. In addition, we employ the Fisher Information Matrix (FIM) diagonal for a computationally cheap and efficient approximation of second-order information, enabling the discovery of performant subnetworks before the training process. The role of the FIM in pruning at initialization remains poorly explored, given that the FIM is often connected to the Hessian only in converged models. However, the work by \citet{karakida2019universal} suggests that this relationship might extend to initialization in overparameterized networks. Therefore, FIM provides useful structural information about the importance of parameters for pruning, even at initialization. Using this insight, we also introduce a batched gradient-based estimation of the FIM diagonal, which improves approximation quality while reducing computational overhead.

\label{collapse}
Recent works \cite{tanaka2020pruning, kumar2024no} have elaborated on the limitations of pruning at initialization. A key challenge for \textit{data-dependent} pruning criteria is \textit{layer collapse}. This failure mode occurs when an entire layer (or most of its parameters) is pruned, disrupting the information flow across subsequent layers. This effect renders the network untrainable, as the pruned layer creates a bottleneck that hinders the model's learning ability. Since data-dependent methods rely on gradient-based information, they tend to assign disproportionately low scores to wide layers, causing them to be pruned first, ultimately collapsing the network.

Considering that layer collapse occurs when pruning at initialization, allowing the gradients to align during a brief warmup phase proves to be a simple yet effective strategy to overcome this issue. Specifically, we find in our experiments that a single warmup epoch mitigates layer collapse. This step ensures that the information flow is not interrupted and enables successful training of the pruned network.

We evaluated the proposed criterion with architectures widely studied in the pruning literature. Specifically, ResNet18 \cite{he2016deep} and VGG19 \cite{simonyan2014very}. We used the CIFAR-10 and CIFAR-100 datasets \cite{krizhevsky2009learning} to rigorously assess the robustness of our proposed pruning criterion in diverse data distributions and model structures. FTS consistently outperforms or matches state-of-the-art techniques for the one-shot PBT setting, even at extreme sparsity conditions. In summary, our key contributions are as follows:

\begin{enumerate}
    \item We propose FTS, a novel pruning criterion that integrates first and second-order information for effective one-shot PBT.
    \item We demonstrate the advantage of a batched gradient approach for estimating the FIM diagonal, reducing the approximation variance and computational overhead.
    \item We demonstrate that a single warmup epoch mitigates layer collapse, ensuring stable pruned network training.
    \item We achieve superior or competitive performance compared to state-of-the-art one-shot PBT methods across ResNet18 and VGG19 on CIFAR-10 and CIFAR-100, even under extreme sparsity.
\end{enumerate}

