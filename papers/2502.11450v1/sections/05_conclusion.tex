\section{Conclusion}
\label{sec:conclusion}
This work introduces FTS, a computationally cheap and efficient pruning criterion that leverages first- and second-order information to perform one-shot pruning at initialization. Our approach builds on the empirical FIM diagonal, demonstrating its effectiveness in finding important parameters, even with randomly initialized networks. 

The experiments performed show that FTS consistently outperforms or matches the performance of state-of-the-art PBT methods, particularly at high and extreme sparsities. Furthermore, we demonstrated the positive effect of estimating the FIM diagonal with batched gradients to reduce the noise and computational overhead. In addition, we show that a single warm-up epoch mitigates layer collapse, allowing data-dependent pruning methods to maintain performance. 

Our results highlight the practical advantages of FTS in identifying performant subnetworks at initialization, providing a scalable alternative to Hessian-based pruning. Our work contributes to advancing efficient deep learning and resource-aware model deployment. Future work includes refining the FIM approximation to capture off-diagonal interactions and extending our approach to larger architectures and real-world tasks.

Future research directions include exploring the integration of FTS with other compression techniques, such as quantization, for real-world applications. Exploring its synergy with modern training paradigms, including lottery ticket hypothesis-inspired approaches and adaptive sparsification, also presents promising avenues.
