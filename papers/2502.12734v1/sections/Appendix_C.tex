\section{Mathematical Analysis of Perturbation Rate and Query Complexity}
\label{sec:analysis}

In this section, we provide the theoretical analysis of our proposed adversarial example generation framework, focusing on two crucial metrics: \textit{(i)} the \textbf{perturbation rate}, which characterizes the fraction of modified tokens in an adversarial example; and \textit{(ii)} the \textbf{query complexity}, which measures the number of queries made to the target detector in a black-box setting. We establish strict upper and lower bounds on both metrics, illustrating the efficiency and effectiveness of our method.

\subsection{Perturbation Rate Analysis}

\noindent
\begin{definition}
\label{def:randomvars}
Let \( Z \) denote the maximum number of tokens allowed to be modified. Let \(P\) be the total number of tokens (out of \(T\)) perturbed by \emph{greedy search}, taking integer values in the interval \([1, Z]\), where \(1 \le P \le Z\). Let \(Y\) be the fraction of those \(P\) tokens retained (not reverted) by \emph{greedy pruning}, taking real values in \([0, 1]\). Formally, 
\[
  P \;\in\;\{1,2,\dots,Z\},
  \qquad
  Y \;\in\;[\,0,\,1\,].
\]
We assume \(P\) and \(Y\) exhibit \emph{weak dependence}, meaning they have a small but nonzero covariance \(\mathrm{Cov}(P, Y)\).
\end{definition}

\begin{definition}
\label{def:perturbation_rate}
Let \(P \cdot Y\) be the expected count of tokens that remain modified. The \emph{perturbation rate $\rho$} is defined as
\[
  \rho = \frac{P \cdot Y}{T},
\]
where \(T\) is the length of the original text.
\end{definition}

\begin{theorem}
\label{thm:bounds1}
Let \(P\) and \(Y\) modeled as truncated normal random variables on \([1,Z]\) and \([0,1]\), respectively. Then under mild assumptions on the truncation intervals, we have
\[
  \mathbb{E}[\,P\,] \;\approx\; \frac{Z+1}{2}
  \quad\text{and}\quad
  \mathbb{E}[\,Y\,] \;\approx\; \frac{1}{2}.
\]
\end{theorem}

\begin{proof}
\textbf{(1) Truncated Normal for \(P\) and \(Y\).}  
In adversarial text attacks, \(P\) often emerges from an aggregation of (approximately) Bernoulli decisions: each of the \(T\) tokens has a non-negligible probability of being deemed “important,” subject to a global cap of \(Z\). By the Central Limit Theorem (CLT), this sum is close to normally distributed with mean \(\mu_P\) and variance \(\sigma_P^2\). Because \(P\) cannot exceed \(Z\) (and must be at least \(1\) to induce misclassification), we say
\[
  P \;\sim\; \mathcal{N}_t(\mu_P,\;\sigma_P^2;\;1,\;Z),
\]
where \(\mathcal{N}_t(\cdot)\) denotes a normal distribution truncated to the integer range \([1,Z]\). Analogously, once \(P\) tokens are selected, \emph{greedy pruning} decides to keep or revert each token, again creating a sum of i.i.d.\ Bernoulli-like indicators. Dividing by \(P\) yields
\[
  Y 
  \;=\; \frac{\text{number of retained tokens}}{P}
\]
\[
  \;\sim\; \mathcal{N}_t(\mu_Y,\;\sigma_Y^2;\;0,\;1),
\]
a (truncated) normal over \([0,1]\).

\noindent
\textbf{(2) Standard Formulas for Truncated Normal Means.}  
From truncated normal theory, if \(X\sim \mathcal{N}(\mu,\sigma^2)\) is restricted to \([a,b]\), then its mean is
\[
  \mathbb{E}[\,X\,]
  \;=\;
  \mu 
  \;+\;
  \sigma\,\frac{\phi\!\bigl(\tfrac{a-\mu}{\sigma}\bigr) - \phi\!\bigl(\tfrac{b-\mu}{\sigma}\bigr)}
                     {\Phi\!\bigl(\tfrac{b-\mu}{\sigma}\bigr) - \Phi\!\bigl(\tfrac{a-\mu}{\sigma}\bigr)},
\]
where \(\phi\) and \(\Phi\) are the standard normal PDF and CDF, respectively. Thus, for
\(
  P \sim \mathcal{N}_t(\mu_P,\sigma_P^2;\,1,Z),
\)
\(\mathbb{E}[P]\) depends on how far \(\mu_P\) is from the boundaries \(\{1,Z\}\). Similarly, \(\mathbb{E}[Y]\) depends on truncation at \([0,1]\).

\noindent
\textbf{(3) Approximate Symmetry in Practice.}  
For symmetric truncated normal distributions \( P \) and \( Y \), it is evident that the following expectations hold:  
\[
\mu_P = \frac{Z+1}{2}, \quad \mu_Y = \frac{1}{2}.
\]
So long as \(\mu_P\) and \(\sigma_P\) ensure that \(\frac{1-\mu_P}{\sigma_P}\) and \(\frac{Z-\mu_P}{\sigma_P}\) are not extreme, the truncation does not drastically shift the mean from \(\mu_P\). Concretely, if \(Z\) is sufficiently large and \(\mu_P\approx \tfrac{Z+1}{2}\), then
\[
  \mathbb{E}[\,P\,]
  \;\approx\;
  \mu_P
  \;\approx\;
  \frac{Z+1}{2},
\]
Likewise, if \(\mu_Y \approx \tfrac12\) and \(\sigma_Y\) is moderate, \(\mathbb{E}[\,Y\,]\approx \tfrac{1}{2}\) despite the boundaries \([0,1]\).

\noindent \textbf{Combining (1), (2), and (3), Theorem~\ref{thm:bounds1} is proved.}
\end{proof}

\begin{theorem}
\label{thm:bounds}
Let \( T \) be the length of the original text and \( M \) be the maximum number of tokens that can be perturbed. Then, the perturbation rate \( \rho \) satisfies \( \frac{1}{T} \leq \rho \leq \frac{Z}{T} \), and its expected value is approximately \( \mathbb{E}[\rho] \approx \frac{Z}{4T} \).
\end{theorem}


\begin{proof}
\textbf{(1) Basic Bounds.}  
Since \(P\) is at least 1 and at most \(M\), and \(Y\) is at least 0 and at most 1, the product \(P \cdot Y\) satisfies
\[
  1 \;\le\; P \;\le\; Z,
  \qquad
  0 \;\le\; Y \;\le\; 1
\]
\[
  \quad\Longrightarrow\quad
  1 \;\le\; P \cdot Y \;\le\; Z.
\]

Dividing by \(T\) yields the strict bounds
\[
  \frac{1}{T}
  \;\le\;
  \frac{P \cdot Y}{T}
  \;\le\;
  \frac{Z}{T}.
\]
The lower limit \(1/T\) reflects that at least one token must change to induce a misclassification; the upper limit \(\tfrac{Z}{T}\) follows from the maximal \(Z\) token modifications.

\noindent
\textbf{(2) Expected Product with Weak Dependence.}  
By definition, the covariance between \(P\) and \(Y\) is
\[
  \mathbb{E}[\,P\,Y\,]
  \;=\;
  \mathbb{E}[\,P\,]\;\mathbb{E}[\,Y\,]
  \;+\;
  \mathrm{Cov}(P, Y).
\]
When the detector is relatively robust, \emph{greedy search} skews \(P\) toward larger values (close to \(M\)), and pruning skews \(Y\) toward retention (close to 1). In that case, \(\mathbb{E}[\,P\,]\) is large, \(\mathbb{E}[\,Y\,]\) is near 1, and a small positive covariance implies
\[
  \mathbb{E}[\,P \cdot Y\,]
  \;>\;
  \mathbb{E}[\,P\,]\;\mathbb{E}[\,Y\,].
\]
However, the final mean number of changed tokens cannot exceed \(M\). Conversely, if the detector is weak, \(\mathbb{E}[\,P\,]\) may approach 1, and \(\mathbb{E}[\,Y\,]\) might be relatively modest, implying 
\[
  \mathbb{E}[\,P\,Y\,]
  \;<\;
  \mathbb{E}[\,P\,]\;\mathbb{E}[\,Y\,]
  \;+\;
  |\mathrm{Cov}(P, Y)|.
\]
In all cases, the weak dependence ensures \(\mathrm{Cov}(P, Y)\) is bounded such that 
\[
  1
  \;\le\;
  P \cdot Y
  \;\le\;
  Z,
\]
preventing the expected perturbation rate from falling below \(\tfrac{1}{T}\) or above \(\tfrac{Z}{T}\).

\noindent
\textbf{(3) Characteristic Mean \(\frac{Z}{4T}\).}  
By Theorem~\ref{thm:bounds1}, we have \(\mathbb{E}[\,P\,]\approx \frac{Z+1}{2}\) and \(\mathbb{E}[\,Y\,]\approx \frac{1}{2}\). If \(\mathrm{Cov}(P, Y)\) is small or near zero, then
\[
  \mathbb{E}[\,P \cdot Y\,]
  \;=\;
  \mathbb{E}[\,P\,]\;\mathbb{E}[\,Y\,]
  \;+\;
  \mathrm{Cov}(P, Y)
\]
\[
  \;\approx\;
  \frac{Z+1}{2}\,\times\,\frac{1}{2}
  \;=\;
  \frac{Z+1}{4}.
\]
For large \(Z\), \(\frac{Z+1}{4T}\approx \frac{Z}{4T}\). If \(\mathrm{Cov}(P, Y)\) modestly raises or lowers this sum, the final mean \(\mathbb{E}[\,P\,Y\,]\) still cannot breach the fundamental \(\bigl[1,\,Z\bigr]\) interval. This shows that \(\frac{Z}{4T}\) is a natural approximate pivot for the average perturbation rate under moderate parameters. 

In Section \ref{sec:GeneratorResult}, we set \( Z = 0.3T \). If the number of iterations exceeds this upper bound, the attack is considered a failure. Therefore, the expected perturbation rate is\[
\frac{0.3T}{4T} \approx 7.5\%,\] which is very close to the result obtained in the Section \ref{sec:GeneratorResult} (7.26\%).

\noindent \textbf{Combining (1), (2), and (3), Theorem \ref{thm:bounds} is proved.}
\end{proof}

Note that if the detector forces \emph{greedy search} to repeatedly fail early (producing a right-skewed \(P\)-distribution concentrated near \(Z\)) and \emph{greedy pruning} is left-skewed (so that \(\mathbb{E}[\,Y\,]\) is close to 1), then the mean \(\mathbb{E}[\,P \cdot Y\,]\) exceeds \(\tfrac{Z+1}{4}\), but still cannot exceed \(Z\). Conversely, if \emph{greedy search} finds success quickly, giving a left-skewed \(P\)-distribution near 1, then \(\mathbb{E}[\,P\,]\) might drop below \(\tfrac{Z+1}{2}\) but never below 1; similarly, if \emph{pruning} is so aggressive that \(\mathbb{E}[\,Y\,]\) is significantly below \(\tfrac{1}{2}\), the mean also decreases. Consequently, the expected perturbation rate remains strictly within the \(\bigl[\tfrac{1}{T},\;\tfrac{Z}{T}\bigr]\) interval, but its exact value depends on the interplay of these skewed distributions. For moderate skewness on both \(P\) and \(Y\), \(\tfrac{Z}{4T}\) is a characteristic outcome.

\subsection{Query Complexity Analysis}

\noindent
\begin{definition}
\label{def:queries}
Let \(Q_{\mathrm{G}}\) be the total number of queries made by \emph{greedy search} and \(Q_{\mathrm{P}}\) be the total number made by \emph{greedy pruning}. Define the overall \emph{query complexity}:
\[
  Q \;=\; Q_{\mathrm{G}} + Q_{\mathrm{P}}.
\]
\end{definition}

\begin{theorem}
\label{thm:query_complexity_bounds}
Let \(T\) be the length of the text, and let \(Z = 0.3\,T\) be the maximum iteration count for both \emph{greedy search} and \emph{greedy pruning}. Then the total number of queries \(Q\) used to construct one adversarial example lies within \(1 \leq Q \leq 2Z\), which implies \(Q = O(T)\) and \(Q = \Omega(1)\). Moreover, if the \emph{average} perturbation rate is relatively low, then the number of required iterations typically shrinks, resulting in a correspondingly smaller \(Q\).
\end{theorem}


\begin{proof}
\textbf{(1) Basic Range of Queries.}  
The minimal number of queries is \(1\), occurring if \emph{greedy search} succeeds in its very first attempt, requiring no \emph{greedy pruning}. Conversely, if both \emph{greedy search} and \emph{greedy pruning} use their maximum of \(Z\) single-query iterations, we have
\[
  Q = Q_{\mathrm{G}} + Q_{\mathrm{P}}
  \;\le\; Z + Z
  \;=\; 2\,Z.
\]
Since \(Z=0.3\,T\), \(Q\le 2\,Z\) translates to \(Q = O(T)\). The trivial lower bound of 1 implies \(Q=\Omega(1)\).

\noindent
\textbf{(2) Perturbation Rate and Query Trade-Off.}  
Let \(\rho\) denote the fraction of tokens altered in a final adversarial example, as described in the perturbation rate analysis. A \emph{smaller} \(\rho\) typically indicates that the detector is fooled by changing fewer tokens, implying fewer iterative steps in \emph{greedy search}. Thus, a lower \(\rho\) correlates with \emph{fewer} queries: once the necessary (small) set of tokens is found, misclassification is often achieved without exhausting all \(Z\) iterations. On the other hand, a higher \(\rho\) suggests more alterations, potentially requiring more query rounds to finalize a successful attack.

\noindent
\textbf{(3) Expected Complexity under Moderate Perturbation.}  
From the previous analysis, the average perturbation rate can hover around \(\tfrac{Z}{4T}\) under typical conditions. This moderate \(\rho\) implies that \emph{greedy search} rarely needs the full \(Z\) steps to identify the required modifications, and \emph{greedy pruning} likewise terminates without iterating over all \(Z\). Consequently, the \emph{expected} \(Q\) is substantially below the worst-case \(2\,Z\). Formally, we have
\[
  \mathbb{E}[\,Q_{\mathrm{G}}\,] \;<\; Z,
  \quad
  \mathbb{E}[\,Q_{\mathrm{P}}\,] \;<\; Z,
\]
\[
  \Longrightarrow\quad
  \mathbb{E}[\,Q\,] \;=\; \mathbb{E}[\,Q_{\mathrm{G}}\,] + \mathbb{E}[\,Q_{\mathrm{P}}\,]
  \;<\; 2\,Z.
\]

thus still preserving \(O(T)\) upper complexity while being strictly smaller on average. 

\noindent \textbf{Combining (1), (2), and (3), Theorem \ref{thm:query_complexity_bounds} is proved.}
\end{proof} 