\section{Related Work}

\noindent \textbf{Machine-Generated Text (MGT) Detection.} 
There have been attempts to detect and attribute the MGTs in the pre-LLM era \cite{zhong2020neural, uchendu2020authorship}.
Nowadays, many works \cite{mitchell2023detectgpt, wang2023seqxgpt, liu2022coco, kushnareva2024ai, guodetective} aim to accurately annotate online texts as LLMs' astonishing ability to generate fluent, logical, and human-like content, which helps the proliferation of unchecked information.
Despite the achievements made in MGT detection, some works indicate the MGT detectors are vulnerable to simple perturbation or adversarial attacks.
For example, ~\citet{wang2024stumbling} test the robustness of eight MGT detectors with twelve perturbation strategies and they surprisingly find that none of the existing detectors remain robust under all
the attacks.
Moreover, MGT detectors' defense against adversarial attacks is also questioned \cite{fishchuk2023adversarial}.
Other studies also reveal the fact that MGT detectors suffer from authorship obfuscation \cite{macko2024authorship} and biased decision \cite{liang2023gpt}.
To mitigate the vulnerability of MGT detectors, our work focuses on improving detector robustness against text perturbations and adversarial attacks.

\noindent \textbf{Adversarial Training.} 
Adversarial training aims to optimize the model toward maintaining correct predictions to adversarial examples that are misleading data constructed for malicious purposes.
Earlier works first augment the training set with adversarial examples for defense against specific attacks~\cite{huang2024cert, zeng2023certified}.
These methods are shown to be hard to generalize to unseen attacks~\cite{wang2024comprehensive}.
\citet{yoo2021towards, li2021tavat} update the adversary and the target model in the same step to generalize the defense to unseen attacks.
However, they rely on the availability of explicit first-order gradient, which is not applicable in the real-world case.
Different from previous works, we propose an effective adversarial training framework for MGT detectors that builds a generalizable defense against a variety of attacks in the black-box setting.

