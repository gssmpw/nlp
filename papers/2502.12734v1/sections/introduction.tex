\section{Introduction}
\label{sec:intro}
The rapid development of large language models (LLM)~\cite{achiam2023gpt,dubey2024llama,claude3, guo2025deepseek} enables the model to generate highly human-like texts, which has raised broad concerns about the unrestricted dissemination of non-attributed textual contents including misinformation, fabricate news, and phishing emails.
These negative impacts of MGTs lead to extensive works on MGT detection \cite{mitchell2023detectgpt, verma2024ghostbuster, liu2024does, baofast, liu2024does} to accurately attribute the authorship of textual content and inform the readers.
\begin{figure}[t]
    \centering
    \resizebox{0.5\textwidth}{!}{%
        \includegraphics{genshin/pic211.pdf} 
    }
	\caption{Performance drop of different MGT detectors and defense methods under text perturbation\protect\footnotemark.}
    \vspace{-0.4cm}
    
    \label{fig:compare1} 
\end{figure}

\footnotetext{The detectors include 
    DetectGPT~\cite{mitchell2023detectgpt}, LogRank~\cite{su2023detectllm}, fine-tuned xlm-roberta-base~\cite{liu2019roberta}, fine-tuned Albert-large~\cite{lan2019albert}, and Unbiased Watermark~\cite{hu2023unbiased}.
    The defense methods contain Text-RS~\cite{zhang2024random}, CERT-ED~\cite{huang2024cert}, VAT~\cite{miyato2016virtual}, TAVAT~\cite{li2021tavat}, and GREATER-D (ours).}

Despite the superb performance of current MGT detectors, a recent study \cite{wang2024stumbling} finds an astonishing fact that \textit{all} detectors exhibit different loopholes in robustness, that is, existing detectors suffer great performance drop when facing different \textbf{text perturbation strategies} including editing \cite{kukich1992techniques, gabrilovich2002homograph}, paraphrasing \cite{shi2024red}, prompting \cite{zamfirescu2023johnny}, and co-generating \cite{kushnareva2024ai}, \etc
As illustrated in Figure~\ref{fig:compare1}, the detection accuracy of current detectors drops by around 30\%-50\% when confronted with simple perturbations, and the defense methods for general text classification cannot be simply adapted to the MGT detection scenario.
More seriously, the vulnerability of MGT detectors is also unveiled by \textbf{adversarial attacks} that exploit the internal state~\cite{yoo2021towards} or outputs~\cite{liu2024hqa,yu2024query,hu2024fasttextdodger} of the detectors through multiple queries. 
Alas, there are few works on improving the robustness against adversarial attacks for MGT detectors.


\noindent\textbf{Motivation.} 
We rely on \textit{threat modeling} to
advance the robustness of the MGT detectors against perturbation and adversarial attacks.
As the proverb
says 'Iron sharpens iron', we focus on constructing powerful adversarial examples which mislead the prediction of the detector to facilitate the post-training of MGT detectors and defend against different attacks. 
Existing text perturbation strategies \cite{wang2024stumbling} adjust token distribution without accessing
information from the target MGT detector, resulting in low-quality and non-targeted adversarial
examples. 
The adversarial attacks
are only effective in white-box setting \cite{yoo2021towards} or require
excessive queries to target detectors \cite{hu2024fasttextdodger,yu2024query,liu2024hqa}.
Moreover, \citet{wang2024comprehensive} find that the defense built by adversarial training cannot generalize well to the attacks on which it was not originally trained.
To overcome these limitations, we propose an efficient adversarial training framework that works in a black-box setting and builds generalizable defense against a wide variety of perturbations and attacks for MGT detectors.



\noindent \textbf{Our Work.} 
In this paper, we propose an adversarial framework for training robust MGT detector, namely \textbf{GRE}edy \textbf{A}dversary
Promo\textbf{T}ed
Defende\textbf{R}.
(\modelname). 
\modelname consists of an adversary (\attackname) and a detector (\defensename).
The \defensename learns to discern MGTs from the human-written texts (HWTs), while the \attackname, which queries the output of the detector, aims to imply minimum perturbation on MGTs to deceive the detector.
Restricted by the scenario where \textbf{only outputs} from the target detector are available, we use an open-sourced surrogate model to retrieve gradient information to identify important tokens in the prediction. 
Afterwards, we introduce a gradient ascent perturbation on the embedding of MGTs from the surrogate model to enhance both the quality and stealthiness of generated adversarial text. 
To reduce the number of queries needed for building effective adversarial examples, we design a greedy search and pruning strategy. 
In the training stage, we update the \attackname and \defensename in the same training step so that \defensename learns from a curriculum of adversarial examples to generalize its defense.
The experiment results demonstrate that our method achieves an average ASR of 5.75\% against various attacks, which is 10.61\% lower compared to the SOTA defense methods.
We also find that \attackname achieves the most effective attack, achieving an ASR of 96.58\%, which surpasses SOTA attack methods by 8.45\% while requiring 4 times fewer queries.

Our contributions are as follows: 
\begin{itemize}
    \item \textbf{Adversarial Training Framework.} 
    We propose an adversarial training framework \modelname 
    to improve the robustness of MGT detectors, in which the adversary maliciously perturbs the MGTs to construct hard adversarial examples, while the detector is trained to maintain correct prediction towards the adversarial examples.
    We update the detector and the adversary generator in the same training step for better generalization on defense.
    \item 
    \textbf{Adversarial Examples Generation.}
    We propose a strong and efficient adversarial examples construction method in the black-box setting. 
    We retrieve gradient information from a surrogate model to rank the important tokens in MGT detection and design a greedy search and pruning strategy to reduce the query times needed for adversarial attacks.
    \item 
    \textbf{Outstanding Performance.} Testing results across 14 attack methods demonstrate that our detector outperforms 8 existing SOTA defense methods in robustness, while our adversary achieves significant improvements in both attack efficiency and effectiveness compared to 13 SOTA attack approaches. 
\end{itemize}

