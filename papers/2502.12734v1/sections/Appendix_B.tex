\section{Experience on Defense}

\subsection{Defense Performance under Different Attack Strengths}

\begin{figure*}[htbp]
    \centering
    \resizebox{1\textwidth}{!}{%
        \includegraphics{genshin/combined_4x2_figure.png} 
    }
    \caption{\textbf{Defense performance under attack with different strengths.} A lower ASR(\%) indicates better defensive performance. A larger character edit distance indicates greater attack intensity in the Mixed Edit Attack. A lower BERT score corresponds to stronger attacks in the Paraphrasing Attack and Code-Switching Attack. A higher obfuscation ratio reflects greater intensity in the Human Obfuscation Attack. Similarly, for PWWS, BERTAttack, TextFooler, and A2T, a larger maximum query count signifies a stronger attack.
    }
    \label{fig:diffstrength} 
\end{figure*}


We evaluate the resistance of defense methods to increasing attack strengths.
For text perturbation strategies, we employ four text perturbation strategies: Mixed Edit, Paraphrasing, Code-Switching, and Human Obfuscation in the experiment.
Following \citet{wang2024stumbling}, we use Character Edit Distance, BERT Score, BERT Score, and Confusion Ratio as the measure of attack strength for the methods mentioned above, respectively.
For adversarial attacks, we choose PWWS TextFooler, BERTAttack, and A2T to attack MGT detectors, and we utilize max query count to quantify the attack strength.
In the implementation, we change the limit on the attack strength measures to vary the attack intensity.
We show the experimental results in Figure \ref{fig:diffstrength}.

Our experimental results demonstrate that as the attack strength increases, the ASR on our \defensename remains consistently close to zero, whereas other defense methods exhibit significant vulnerabilities under more intensive adversarial attacks.
However, it is worth noticing that under Mixed Edit perturbation, \defensename performs second-best after EP.
This is because EP is originally trained on Mix Edit perturbation and obtains stronger defense against it.
The consistent effective defense against varying attack strengths proves the steadiness of our defense method.

\subsection{Defense Adaptation to Different Backbones}
\label{sec:replacement}
To demonstrate the generalizability and effectiveness of our \modelname across different model architectures, we replace the backbone model with seven state-of-the-art transformer-based models, including both base and large variants of ALBERT~\cite{lan2019albert}, DeBERTa~\cite{he2020deberta}, RoBERTa~\cite{liu2019roberta}, and XLM-RoBERTa~\cite{conneau2019unsupervised}.
We report the ASR of each model under the Paraphrasing Attack with a budget of $0.74$, both before defense (Baseline) and after applying our \defensename{}. We compare it with the best-performing defense method CERT-ED and show the result in Table~\ref{tab:all_results_modified}.

\begin{table}[ht]
\centering
\renewcommand\arraystretch{1.4}
\footnotesize
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Metric} & \textbf{Baseline} & \textbf{CERT-ED} & \textbf{\defensename} \\
\midrule

ALBERT Base (12M)     & \emph{ASR(\%)$\downarrow$} & 63.58   & 46.71   & \cellcolor{green!50}35.62 \\
ALBERT Large (18M)    & \emph{ASR(\%)$\downarrow$} & 97.14   & 45.45   & \cellcolor{green!50}43.40 \\
DeBERTa Base (86M)     & \emph{ASR(\%)$\downarrow$} & 22.95   & 29.37   & \cellcolor{green!50}4.10 \\
DeBERTa Large (304M)    & \emph{ASR(\%)$\downarrow$} & 51.72   & 25.07   & \cellcolor{green!50}10.25 \\
RoBERTa Base (125M)     & \emph{ASR(\%)$\downarrow$} & 81.88   & 26.78   & \cellcolor{green!50}15.32 \\
RoBERTa Large (355M)    & \emph{ASR(\%)$\downarrow$} & 30.21   & 34.66   & \cellcolor{green!50}5.38 \\
XLM-RoBERTa Large (561M) & \emph{ASR(\%)$\downarrow$} & 81.99   & 40.04   & \cellcolor{green!50}1.73 \\
\bottomrule
\end{tabular}
}
\caption{Experiment results of various models before and after applying \defensename under Paraphrasing attack. The best result in each group is highlighted with a \colorbox{green!50}{green} background.}
\label{tab:performance_comparison}
\end{table}

The results presented in Table~\ref{tab:performance_comparison}  demonstrate the efficacy of our adversarial training method across a diverse set of transformer-based models. Notably, \textit{XLM-RoBERTa-Large} exhibits the most substantial improvement, with ASR decreasing by 80.26\%, 38.31\% compared with baseline and CERT-ED, highlighting the significant impact of adversarial training on models with initially lower performance metrics. Similarly, both \textit{RoBERTa Base} and \textit{DeBERTa Large} demonstrate substantial improvements. Specifically, \textit{RoBERTa Base} achieves a 66.56\% reduction in ASR compared to the baseline and outperforms the CERT-ED with an additional 11.46\% decrease.
Likewise, \textit{DeBERTa Large} exhibits a 41.47\% drop in ASR relative to the baseline and surpasses CERT-ED by reducing ASR by an additional 14.82\%. 
These results underscore the robustness and versatility of our adversarial training approach across different model sizes and architectures. While smaller models like \textit{ALBERT Base} and \textit{ALBERT Large} exhibit more modest gains, the consistent upward trends across all evaluated models affirm that our adversarial training method effectively enhances model resilience and performance against Paraphrasing Attack. This versatility makes our approach a valuable tool for improving a wide range of transformer-based models in adversarial settings.


\section{Ablation Study}

\begin{table}[ht]
\centering
\renewcommand\arraystretch{1.4}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{ccccccc}
\toprule
\textbf{Method} & \textbf{Avg Queries $\downarrow$} & \textbf{ASR (\%) $\uparrow$} & \textbf{Pert. (\%) $\downarrow$} & \textbf{$\Delta$PPL $\downarrow$} & \textbf{USE $\uparrow$} & \textbf{$\Delta$r $\downarrow$} \\ 
\toprule

\textbf{R+NP} & \cellcolor{green!50}26.61 & 75.77 & 11.56 & 106.29 & 0.9324 & 14.41 \\

\textbf{R+P} & 53.22 & 75.77 & 9.51 & 58.72 & 0.9497 & 12.13 \\

\textbf{S+NP} & 31.32 & \cellcolor{green!50}96.58 & 11.28 & 85.18 & 0.9136 & 21.80 \\

\midrule

\textbf{Mask-T} & 303.82 & 96.38 & 8.33 & \cellcolor{green!50}27.12 & \cellcolor{green!50}0.9696 & \cellcolor{green!50}4.73 \\

\textbf{GREATER-W} & 33.21 & 96.38 & 11.49 & 65.16 & 0.9482 & 10.04 \\

\textbf{GREATER-WordNet} & 28.41 & 80.89 & 16.68 & 53.82 & 0.9199 & 13.21 \\

\textbf{GREATER-A} & 62.63 & \cellcolor{green!50}96.58 & \cellcolor{green!50}7.26 & 35.22 & 0.9506 & 9.21 \\
\bottomrule

\end{tabular}
}
\caption{\textbf{Ablation study on the \attackname.}
The best result in each group is highlighted with a \colorbox{green!50}{green} background.}
\label{tab:ablation_study}

\end{table}


To demonstrate the effectiveness of each component in the design of \defensename and \attackname, we conduct ablation experiments. 
The ablation models are as follows:

\noindent\textbf{R+NP:} Randomly select tokens to perturb and not apply pruning.

\noindent\textbf{R+P:} Randomly select tokens to perturb and apply pruning.

\noindent\textbf{S+NP:} Select tokens to perturb with the important token identification module but not apply pruning.

\noindent\textbf{Mask-T:} Select tokens to perturb with the important token identification but mask them instead of adding perturbation.

\noindent\textbf{GREATER-W:} Select words to perturb with the important token identification module instead of tokens.

\noindent\textbf{GREATER-WordNet:} Select words to perturb with the important token identification module and substitute them with synonyms.

As shown in Table~\ref{tab:ablation_study}, the original \attackname exhibits the most balanced and effective performance in generating adversarial examples.
Comparing \textbf{R+P} to \attackname, we find the ASR drops by \textbf{20.81}, indicates that identifying important tokens is of great significance for the attack to be successful.
Moreover, greedy pruning is important for maintaining the text quality of adversarial examples.
\textbf{S+NP} achieves the same ASR with \attackname but is left far behind in terms of PPL, USE, and $\Delta$r.
The perturbation strategy also plays a crucial role in the adversarial attack.
Masking the important token instead of perturbing the embedding greatly increases the number of queries.
Applying perturbation on the word level or substituting important words with synonyms also degrades the performance of the adversary.

