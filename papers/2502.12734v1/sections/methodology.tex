\section{Threat Model}
We follow the standard threat modeling framework outlined in prior work \cite{biggio2018wild} and describe our assumptions about the adversary's goal and adversary's capability.

\noindent\textbf{Adversary's Goal.}\label{goal}
Given a piece of MGT, the goal of the adversary is to make trivial changes to the original MGT so as to mislead the prediction of the detector.
We refer the changed texts as \textit{adversarial examples}.
Ideally, the adversarial examples should satisfy three requirements:
\textit{i) Low Perturbation Rate.} 
Only trivial changes should be applied on the adversarial examples and the semantics of original texts should be retained.
\textit{ii) High Readability.}
Adversarial examples should exhibit high readability so that the attack is most invisible to humans.
\textit{iii) Less Query Requirements.}
The adversary should be query-efficient to reduce the time and budget needed to construct each adversarial example.
    
\noindent\textbf{Adversary's Capability. }
We assume the adversary's capability in a real-world setting.
First, an adversary only maliciously edits the MGTs but would not make any changes to the HWTs.
This is because HWTs are trustworthy and there is no need for the adversary to change the prediction on HWTs.
Second, since most commercial MGT detectors (\eg GPT Zero\footnote{https://gptzero.me/}) are close-sourced, the adversary should not have access to model weights and internal states of the target model.
The only information the adversary is permitted to query is the output of the detector.
Third, the adversary is allowed to access any open-sourced models.

\section{Methodology} \label{sec:generator}
We introduce the framework of \modelname in this section.
The architecture of \modelname is shown in Figure \ref{fig:framework}. 
In the following subsections, we first describe the workflow of the adversary and detector, respectively. 
Then we systematically outline the adversarial training process.


\begin{figure*}[htbp]
    \centering
    \resizebox{1\textwidth}{!}{%
        \includegraphics{genshin/workflowD.pdf} 
    }

    \caption{\textbf{Pipeline of \modelname.}
    The adversary identifies important tokens in the original MGT and generates candidates for important tokens (\secref{AEG}). The adversary conducts and refines the attack by greedy search and pruning (\secref{AEE}). The final adversarial examples are feeded to the target detector (\secref{DA})
    and participate in the adversarial training process (\secref{AT}).}
    \vspace{-0.4cm}
    \label{fig:framework} 
\end{figure*}


\subsection{\attackname for Generating Adversarial Examples}
\label{sec:advgen}
To achieve the Adversary's Goal outlined in \secref{goal}, we developed an effective and efficient adversary. Specifically, the adversary achieves these requirements through two stages: \textit{Identify \& Perturb} and \textit{Replace \& Refine}.

\subsubsection{Identify \& Perturb}\label{AEG}
In this module, we design a token importance estimation module and apply a targeted perturbation on the embeddings of important tokens.

\fakeparagraph{Important Token Identification.}
We consider a black-box setting where the internal state of the target detector $\mathcal{M}_{tar}(.)$ is inaccessible.
Given an original MGT $X = [x_1, x_2, \ldots, x_T]$ consisting of $T$ tokens, we utilize a surrogate model $\mathcal{M}_{sur}(.)$ instead to obtain the last layer hidden state of each token in the text:
\begin{equation}
\label{eq:1}
H = \mathcal{M}_{sur}(X) = [\mathbf{h}_1, \mathbf{h}_2, \ldots, \mathbf{h}_T],
\end{equation}
where $\mathbf{h}_t$ represents the last layer hidden state of the $t$-th token $x_t$ generated by $\mathcal{M}_{sur}(.)$.
To obtain the importance score of each token $s_t$, we train a simple scoring network $\mathcal{F}_{\theta}(.)$ which takes the feature embeddings as input and outputs the prediction of importance scores for each token:
\begin{equation}
\label{eq:2}
s_t = \mathcal{F}_{\theta}(\mathbf{h}_t),
\end{equation}
where $\theta$ are learnable parameters.
Then, we select the top-$k$ tokens with the highest importance scores in text $X$ and construct the important-token set $\mathbf{I}$:
\begin{equation}
\label{eq:3}
\mathbf{I}=\text{top-$k$}\left([ (x_t, s_t) \mid t = 1, 2, \dots, T ]\right).
\end{equation}

To mitigate the impact of discrepancies between the $\mathcal{M}_{sur}(.)$ and $\mathcal{M}_{tar}(.)$, we leverage the predictions of the target detector $\mathcal{M}_{tar}(.)$ to guide $\mathcal{F}_{\theta}(.)$ in more accurately identifying important tokens during adversarial training process. 
We detail the adversarial training process in \secref{Adversarial Training}.

\fakeparagraph{Embedding-level Perturbation.}
We apply a targeted perturbation on the embedding of the tokens in $\mathbf{I}$ to improve the attack effectiveness while preserving semantic integrity.
Formally, we introduce perturbations to the tokens in set $\mathbf{I}$ within the embeddings $E = [\mathbf{e}_1, \mathbf{e}_2, \ldots, \mathbf{e}_T]$ of the surrogate model to obtain the perturbed embedding $\tilde{E} = [\tilde{\mathbf{e}}_1, \tilde{\mathbf{e}}_2, \ldots, \tilde{\mathbf{e}}_T]$:
\begin{equation}
\label{eq:4}
\tilde{\mathbf{e}}_t = \mathbf{e}_t + \mathbf{1} _{[t \in \mathbf{I}]}\delta_t, 
\end{equation}
where $\mathbf{\delta}_t$ represents the perturbation of the $t$-th token, and $\mathbf{1} _{[t \in \mathbf{I}]}$ represents an indicator function with a value of $1$ if and only if the condition $t \in \mathbf{I}$ is satisfied, otherwise, it is $0$.
For the calculation of $\mathbf{\delta}_t$, we first initialize the perturbation from a normalized uniform distribution, and then design a single-step gradient ascent strategy to optimize the perturbation.
Specifically, we update the perturbation towards the direction where the KL divergence between the output distributions with respect to the original embedding $E$ and initial perturbed embedding $\tilde{E}^{0}$ increases most steeply.
This process is formulated as:
\begin{equation}
\footnotesize
\label{pre}
\vcenter{\hbox{$\displaystyle 
\begin{gathered}
\mathbf{\delta}_t^{0} \sim \mathcal{U}(a, b),\quad 
\hat{\mathbf{\delta}}_t^{0} = \xi \frac{\mathbf{\delta}_t^{0}}{\|\mathbf{\delta}_t^{0}\|_2}, \\[6pt]
\mathbf{\delta}_t = \epsilon\,  
\frac{
  \nabla_{\hat{\mathbf{\delta}}_t^{0}}\,\text{KL}\Bigl(P_{\text{sur}}(\mathbf{y}\mid E) \,\|\, P_{\text{sur}}(\mathbf{y}\mid \tilde{E}^{0})\Bigr)
}{
  \Bigl\|\nabla_{\hat{\mathbf{\delta}}_t^{0}}\,\text{KL}\Bigl(P_{\text{sur}}(\mathbf{y}\mid E) \,\|\, P_{\text{sur}}(\mathbf{y}\mid \tilde{E}^{0})\Bigr)\Bigr\|_2
},
\end{gathered}
$}}
\end{equation}
where $\hat{\mathbf{\delta}}_t^{0}$ represents the normalized value of $\mathbf{\delta}_{t}^{0}$, and $\xi$ is a scaling factor. The parameters $a$ and $b$ define the lower and upper bounds of the uniform distribution $\mathcal{U}(a, b)$, $\epsilon$ is a scaling factor, $P_{\text{sur}}(\mathbf{y} | E)$ and $P_{\text{sur}}(\mathbf{y} | \tilde{E}^{0})$ are label distribution before and after perturbation, respectively.

Afterwards, we project the $\tilde{E}$ back to the vocabulary with the language modeling head and select the top-$m$ tokens with the highest probabilities as candidates for replacing the important tokens:
\begin{equation}
\label{eq:7}
\footnotesize
\mathbf{C}_t = \text{top-$m$}(\mathrm{Softmax}(\mathrm{LMHead}(\mathcal{M}_{sur}(\tilde{E}))_t)), 
\end{equation}
where $\mathrm{LMHead}(\mathcal{M}_{sur}(\tilde{E}))_t$ represents the output of the $\mathrm{LMHead}(.)$ at position $t$.
Following POS Constraints \cite{zhou2024humanizing}, which require that the candidate words must match the part of speech of the words they replace, we filter the candidate tokens so that the candidate set contains the tokens with the same POS as the original one.

\subsubsection{Replace \& Refine}\label{AEE}
Based on the token importance calculated in \secref{AEG}, we introduce a greedy search and a greedy pruning strategy to efficiently construct powerful adversarial examples facilitating adversarial training.
 
\begin{algorithm}[t]
\small
\caption{Greedy Search Procedure}
\label{alg:algorithm2_short}
\begin{algorithmic}[1]
	\State {\textbf{Input:} Target Detector $\mathcal{M}_{tar}$, Original MGT $X$, Label $c$, Important-token Set $\mathbf{I}$.}
    \State $round \leftarrow 0$ and $\tilde{X} \leftarrow X$.
    \While{$round< round_{max}$}
       \State Get the token $x_{t}=\mathbf{I}[round]$ to be perturbed.
       \State{Compute corresponding perturbation $\mathbf{\delta}_t$ using Eq.\eqref{pre}.}
       \State{Get candidates $\mathbf{C}_t$ for replacing token using Eq.\eqref{eq:7}.}
       \State{Replace $x_t$ with the token in $\mathbf{C}_t$ to update $\tilde{X}$.}
       \State Classify $\tilde{X}$ via $\mathcal{M}_{tar}$ and obtain the output $\tilde{c}$.
       \If{$\tilde{c} \neq c$}
          \State \textbf{break} \quad // Attack success
       \Else
          \State $round \leftarrow round + 1$
          \quad // Attack failed
       \EndIf
    \EndWhile
    \State \textbf{Output:} Adversarial Example $\tilde{X}$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[t]
\small
\caption{Greedy Pruning Procedure}
\label{alg:algorithm3_short}
\begin{algorithmic}[1]
	\State {\textbf{Input:} Adversarial Example $\tilde{X}$ from Algorithm \ref{alg:algorithm2_short}, Label $c$, Important-token Set $\mathbf{I}$ and Target Detector $\mathcal{M}_{tar}$.}
    \For{each perturbed token $\tilde{x}_t$ in $\tilde{X}$}
        \State {Revert $\tilde{x}_t$ to corresponding token $x_t$ in $X$.}
        \State Classify $\tilde{X}$ via $\mathcal{M}_{tar}$ and obtain the output $\tilde{c}$. 
        \If{$\tilde{c} \neq c$.}
            \State \textbf{continue} \quad // revert successful
        \Else
            \State Replace $x_t$ with $\tilde{x}_t$ again. \quad // revert failed
        \EndIf
    \EndFor
    \State \textbf{Output:} Final Adversarial Example $\tilde{X}$.
\end{algorithmic}
\end{algorithm}
\fakeparagraph{Greedy Search.} We present the process of greedy search in Algorithm \ref{alg:algorithm2_short}. 
For a piece of original MGT $X$, we substitute the token $x_t$ with the most possible candidate token in $\mathbf{C}_t$ sequentially according to the descending order of importance scores. 
After each replacement, we query the target model $\mathcal{M}_{tar}$ if the adversarial example in the current step is machine-generated.
We iterate this token-replacing procedure until the adversarial example deceives the $\mathcal{M}_{tar}$ or all the tokens in $\mathbf{I}$ are replaced. 
We then use the successful adversarial example (if the attack succeeds) or the text that is perturbed the most as training data for $\mathcal{M}_{tar}$.
Compared to existing methods \cite{liu2024hqa,yu2024query,hu2024fasttextdodger}, our method perturbs only the tokens in set $\mathbf{I}$ incrementally with the guidance of importance score, enabling the generation of adversarial examples with fewer queries and lower perturbation rates. 



\fakeparagraph{Greedy Pruning.}
Due to the local optimality characteristic of greedy search~\cite{yu2024query, prim1957shortest}, the adversarial examples constructed by greedy search contain redundant perturbations. 
We apply the greedy pruning algorithm, as shown in Algorithm \ref{alg:algorithm3_short}, to further reduce the perturbation rate and make the attack stealthy without sacrificing its effectiveness.
Given an adversarial example $\tilde{X}$ generated with greedy search, we sample the perturbed token by order of importance scores and replace the selected token with its corresponding original token one-by-one.
After each restoration, we query the target model $\mathcal{M}_{tar}$ if $\tilde{X}$ is still a successful adversarial example.
If the attack remains successful after restoration,
the token is converted to the original one; otherwise,
we preserve the perturbed token.
We loop over all perturbed tokens and produce the final adversarial example
$\tilde{X}$.

To further validate the efficiency of our method, we provide a detailed theoretical analysis of query complexity and perturbation rate in \attackname in Appendix \ref{sec:analysis}. 

\subsection{\defensename for Defending Attacks}
\label{DA}
Our \defensename contains a target detector $\mathcal{M}_{tar}(.)$.
For the target detector $\mathcal{M}_{tar}(.)$, we expect it to learn to defend against adversarial attacks from the adversary and generalize the defense ability to other attacks. Specifically, given the target detector $\mathcal{M}_{tar}(.)$, for each original MGT $X_{i} \in \mathbf{X}$ and the corresponding adversarial example $\tilde{X}_{i}$ generated by the adversary, which share the same label $c_{i}$, the optimization objective of the target detector is to make correct predictions for both the original MGT and the adversarial example:
\begin{equation}
\label{eq:8}
\footnotesize
    \min_{\theta}(\sum_{X_{i}\in \mathbf{X}} (\mathcal{L}(\mathcal{M}^\theta_{tar}(X_{i}), c_{i})+\mathcal{L}(\mathcal{M}^\theta_{tar}(\tilde{X}_{i}), c_{i}))),
\end{equation}
where $\theta$ represents the learnable parameters of the target detector $\mathcal{M}_{tar}(.)$ and $\mathcal{L}$ is the loss function. 
This optimization objective aims to guide the $\mathcal{M}_{tar}(.)$ to simultaneously enhance its performance on both original MGT and adversarial examples, thereby compelling it to learn robust features of samples before and after attacks.
Regardless of whether the samples are subjected to adversarial interference, these features ensure that the detector can accurately classify the samples. As a result, the detector is better equipped to handle various types of attacks. 


\subsection{Adversarial Training}
\label{AT}
We propose to train the adversary and detector in a co-training manner, that is, the two main components in \modelname are updated in the same training step. 
Unlike the previous methods \cite{zhang2024random, huang2024cert,zeng2023certified} that rely on static training set augmented with adversarial examples, synchronously updating the adversary and the detector allows the detector to learn from easy adversarial examples to hard ones, facilitating the defense to generalize to different attacks.

\noindent\textbf{Adversary Loss.}
The goal of the adversary is to precisely estimate the importance of each token in the detection to guide it to undertake a successful attack.
Thus, we use the gradient with respect to the input in calculating the cross-entropy loss on training data as the golden label for the token importance score.
However, since we are constrained in a black-box setting, we utilize the $\mathcal{M}_{sur}(.)$ to obtain the gradient.
This process is formulated as:
\begin{equation}
\label{eq:combined}  
\vcenter{\hbox{$\displaystyle 
\begin{gathered}
s^*_x = \Bigl\|\nabla_{x} \mathcal{L}_{\text{sur}}\Bigr\|_2, \\[6pt]
\mathcal{L}_{\text{sur}} = \frac{1}{M} \sum_{i=1}^{M} \left[ -\log P_{\text{sur}}(c_{i} \mid X_{i})\right],
\end{gathered}
$}}
\end{equation}
where $\mathcal{L}_{\text{sur}}$ is the cross-entropy classification loss of the surrogate model to the original MGTs.
Subsequently, we update the scoring network $\mathcal{F}_{\theta}(.)$ with the mean squared error loss:
\begin{equation}
\label{eq:12}
    \mathcal{L}_{\text{imp}} = \frac{1}{M} \sum_{i=1}^{M} \frac{1}{|X_i|} \sum_{x \in X_i} (s_x-s_x^*)^{2},
\end{equation}
where $|X_i|$ is the number of tokens in sample $X_i$. 
In addition, we leverage the output information from the target detector to guide the training of the $\mathcal{F}_{\theta}(.)$, thereby mitigating the impact of discrepancies between the surrogate and target detector.
Specifically, we replace the true labels in the cross-entropy loss with misleading labels to direct the $\mathcal{F}_{\theta}(.)$ to produce samples that are capable to deceive the detector:
\begin{equation}
\label{eq:13}
    \mathcal{L}_{\text{adv}} = \frac{1}{M} \sum_{i=1}^{M} \left[ -\text{log}P_{\text{tar}}(1-c_{i}|\tilde{X}_{i})\right].
\end{equation}
Finally, we balance the influence of various losses on the adversary by adjusting the weight parameter $\lambda$. The total loss for the adversary is given as follows:
\begin{equation}
\label{eq:14}
\mathcal{L}_{\text{A}} = \lambda \, \mathcal{L}_{\text{adv}} + (1-\lambda) \, \mathcal{L}_{\text{imp}}.
\end{equation}

\noindent\textbf{Detector Loss.}
The detector's goal is to maintain correct predictions in all circumstances.
Based on this, we optimize the detector towards minimizing a cross-entropy loss:
\begin{equation}
\label{eq:9}
\footnotesize
    \mathcal{L}_{\text{D}} = \frac{1}{M} \sum_{i=1}^{M} \left[ -\text{log}P_{\text{tar}}(c_{i}|X_{i})-\text{log}P_{\text{tar}}(c_{i}|\tilde{X}_{i})\right],
\end{equation}
where $P_{\text{tar}}(c_{i}|X_{i})$ is the probability of the target detector output at the correct label $c_{i}$. 

\noindent\textbf{Training Process.}
We update the adversary and the detector alternatively in the same training step.
Specifically, at each training step $t$, we first update the adversary with loss function \eqref{eq:13} while keeping the detector frozen.
The updated adversary generates adversarial example $\tilde{X}_i$ in the current step.
Then we update the detector with loss function \eqref{eq:9}.
We detail the adversarial training process in form of pseudocode in Appendix \ref{appd: pseudo}.



\label{Adversarial Training}
