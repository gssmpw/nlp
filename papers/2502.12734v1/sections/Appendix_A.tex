\section{Experimental Setting}
\label{apdx:expset}

\subsection{Dataset}
\label{apdx:dataset}

We used the Semeval Task8 dataset \cite{siino2024badrock} as the primary dataset for training the detector.
This large-scale dataset includes MGTs from ChatGPT 4, Davinci, Bloomz, Dolly, and Cohere, as well as HWTs from WikiHow, Reddit, arXiv, Wikipedia, and PeerRead.
Moreover, the average token length of the dataset is 623.2521. For each scenario, we employed distinct datasets for testing, as detailed in Tabel \ref{tab:exp1}.


\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.8}
\resizebox{0.45\textwidth}{!}{
\begin{tabular}{ccc}
\hline
\textbf{Scenario}              & \textbf{Dataset}                                      & \textbf{Number of MGTs} \\ \hline
Mixed Edit & Semeval Task8 \cite{siino2024badrock}                                              & 1000  \\ \hline
Paraphrasing & Semeval Task8 \cite{siino2024badrock}                                              & 1000 \\ \hline
Code-switching          & Semeval Task8 \cite{siino2024badrock}                                              & 1000 \\ \hline
Human Obfuscation  & Semeval Task8 \cite{siino2024badrock}                                              & 1000 \\ \hline
Emoji-cogen           & Wang et al. \cite{wang2024stumbling}                & 500 \\ \hline
Typo-cogen           & Wang et al. \cite{wang2024stumbling}                & 500 \\ \hline
ICL           & Wang et al. \cite{wang2024stumbling}                & 500 \\ \hline
Prompt Paraphrasing            & Wang et al. \cite{wang2024stumbling}                & 500  \\ \hline
CSGen           & Wang et al. \cite{wang2024stumbling}                & 500  \\ \hline
Adversarial Attack             & Semeval Task8 \cite{siino2024badrock}                                              & 500 \\ \hline
\end{tabular}
}
\caption{Experimental scenarios and corresponding datasets.}
\label{tab:exp1}
\end{table}

\subsection{Implementation}
\label{appdx:implementation}
\modelname\ is deployed on a server equipped with 4 NVIDIA A100 GPUs, running on Ubuntu 22.04. The adversarial framework uses the xlm-roberta-base model (279M) as its base detector.
For the evaluation of Mixed Edit Attack, Paraphrasing Attack, Code-switching Attack, and Human Obfuscation, we adopt the concept of "budget" to control the intensity of the attacks for a more fine-grained investigation of model robustness, following the methodology in \citet{wang2024stumbling}.
Specifically, Mixed Edit Attack uses character edit distance \cite{levenshtein1966binary} as the budget, Paraphrasing Attack and Code-switching Attack utilize BERTScore \cite{zhang2019bertscore} as the budget, and Human Obfuscation Attack employs the confusion ratio as the budget.

For all defense methods, we use \texttt{xlm-roberta-base} as the base detector. The sizes of the training set, validation set, and test set are 8000, 1000, and 1000, respectively. The learning rate is set to $1\text{e-}5$, and the number of epochs is fixed at 6.

For all data augmentation-based methods, we use 20\% of MGT for data augmentation. If the method has hyperparameters, they are set according to the reference values in the original paper.
For all adversarial training-based methods, we use 20\% of MGT for adversarial training. If the method has hyperparameters, they are set according to the reference values in the original paper.
For our method, our detector is trained using a label smoothing loss function with a smoothing factor of $\alpha$. We use a trained \texttt{RoBERTa Large} as the Surrogate Model $\mathcal{M}_{\text{sur}}(.)$ due to its strong generalization ability and precise understanding of English text.
Our method's selected hyperparameters are shown in Table~\ref{tab:hyperparameters}.

\begin{table}[h]
    \centering
    \resizebox{0.5\textwidth}{!}{ % Scale the table to 50% height
        \begin{tabular}{cc}
            \toprule
            Hyperparameter & Value \\
            \midrule
            Weight Parameter $\lambda$ & 0.05 \\
            Scaling Factor $\epsilon$ & 0.3 \\
            Scaling Factor $\xi$ & 0.01 \\
            Lower Bound of the Uniform Distribution $a$ & 0.5 \\
            Upper Bound of the Uniform Distribution $b$ & -0.5 \\
            Batch Size $M$ & 50 \\
            Epoch $N$ & 6 \\
            Label Smoothing Factor $\alpha$ & 0.1 
            \\
            \bottomrule
        \end{tabular}
    }
    \caption{Hyperparameters for our \modelname.}
    \label{tab:hyperparameters}
\end{table}


\subsection{Evaluation Metrics}
\label{apdx:metric}
We use attack effectiveness metrics and text quality metrics to comprehensively evaluate the performance of defense and attack methods.

\noindent\textit{1) Attack Effectiveness Metrics}

\noindent\textbf{Attack Success Rates (ASR).} The Attack Success Rate (ASR) measures the proportion of successful attacks relative to the total number of attempted attacks.
Note that we only attack text that was detected as machine-written before the attack.
ASR is calculated as follows:
\[
\text{ASR} = \frac{\text{Text detected as HWT after attack}}{\text{Text detected as MGT before attack}}.
\]
For detector, a lower ASR \(\downarrow\) indicates better defense performance, while for adversary, a higher ASR \(\uparrow\) signifies a stronger attack effectiveness.

\noindent\textit{2) Text Quality Metrics}

\noindent\textbf{Perturbation Rate  ($\mathrm{Pert.}$).} $\mathrm{Pert.}$ measures the lexical difference between the adversarial text and the original text. 
It is defined as the ratio of the number of perturbed tokens to the total number of tokens in the text. 
A lower perturbation rate \(\downarrow\) indicates that the adversarial text remains more similar to the original text.

\noindent\textbf{Perplexity Variation ($\Delta\mathrm{PPL}$).} $\Delta\mathrm{PPL}$ measures the change of perplexity, which represents the consistency and fluency of the adversarial examples.
The PPL is calculated as:
\[
\mathrm{PPL} = exp(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_1, w_2, \dots, w_{i-1})),
\]
where \(N\) is the total number of tokens, \(w_i\) represents the \(i\)-th token, and \(P(w_i | w_1, w_2, \dots, w_{i-1})\) is the probability assigned by the language model. Generally, a lower PPL variation $\downarrow$ indicates that the quality of the adversarial text is closer to that of the original text. We use GPT-2~\cite{radford2019language} to compute PPL.

\noindent\textbf{Universal Sentence Encoder (USE) score\cite{cer2018universal}. }
USE evaluates the semantic similarity between the adversarial example and the original text.
The USE score is computed as:
\[
\text{USE Score} = \frac{\mathbf{E}_{\text{orig}} \cdot \mathbf{E}_{\text{adv}}}{\|\mathbf{E}_{\text{orig}}\| \|\mathbf{E}_{\text{adv}}\|},
\]
where \( \mathbf{E}_{\text{orig}} \) and \( \mathbf{E}_{\text{adv}} \) are the sentence embeddings of the original and adversarial texts, respectively, generated by the USE model, and \( \|\mathbf{E}\| \) denotes the Euclidean norm. A higher USE score $\uparrow$ indicates greater semantic similarity. We use USE (Universal Sentence Encoder) to calculate USE score.

\noindent\textbf{Flesch Reading Ease score (\(\Delta r\)) \cite{flesch1948new}.}
$\Delta r$ measures the variation in text readability. The Flesch Reading Ease score is calculated as:

\[
r = 206.835 - 1.015 \times \frac{N_{\text{words}}}{N_{\text{sentences}}} - 84.6 \times \frac{N_{\text{syllables}}}{N_{\text{words}}},
\]
where \( N_{\text{words}} \) is the total number of words in the text, \( N_{\text{sentences}} \) is the total number of sentences, and \( N_{\text{syllables}} \) is the total number of syllables. A smaller \( \Delta r \) $\downarrow$ indicates less change in readability.

\subsection{Experimental Comparison Methods}
This section provides a detailed introduction to the SOTA methods included in our comparisons.

\subsubsection{Defense Methods}
\label{apdx:defmethod}



\noindent \textbf{Edit Pretraining (EP) \cite{wang2024comprehensive}:} 
A data augmentation method that blends Mixed Edit Attack into the training set.


\noindent \textbf{Paraphrasing Pretraining (PP) \cite{wang2024comprehensive}:} A data augmentation method that blends Paraphrasing Attack into the training set.


\noindent
\textbf{Virtual Adversarial Training (VAT) \cite{miyato2016virtual}:}
An Adversarial Training method that defines the adversarial direction without label information and employs the robustness of the conditional label distribution around each data point against local perturbation as the adversarial loss.


\noindent
\textbf{Token Aware Virtual Adversarial Training (TAVAT) \cite{li2021tavat}:}
An Adversarial Training method that uses token-level accumulated perturbation to better initialize the noise and applies token-level normalization.


\noindent
\textbf{CERT-ED \cite{huang2024cert}:}
An adversarial training method that adapts randomized deletion to effectively safeguard natural language classification models from diverse edit-based adversarial operations, including synonym substitution, insertion, and deletion.

\noindent
\textbf{RanMask \cite{zeng2023certified}:}
An adversarial training method that randomly masks a proportion of words in the input text, thereby mitigating both word- and character-level adversarial perturbations without assuming prior knowledge of the adversaries’ synonym generation.

\noindent
\textbf{Text-RS \cite{zhang2024random}:}
An adversarial training method that treats discrete word substitutions as continuous perturbations in the embedding space to reduce the complexity of searching through large discrete vocabularies and bolster the model’s robustness.

\noindent
\textbf{Text-CRS \cite{zhang2024text}:}
An adversarial training method that is built on randomized smoothing, encompassing various word-level adversarial manipulations—such as synonym substitution, insertion, deletion, and reordering—by modeling them in both embedding and permutation spaces.

\subsubsection{Attack Methods}
\label{sec:attack}
\noindent
\textbf{TextFooler~\cite{jin2020bert}:} 
A black-box attack method targeting text classification and natural language inference tasks. It ranks words by their importance to the model's prediction and replaces them with semantically similar and grammatically correct synonyms to generate adversarial examples while preserving sentence meaning and structure. 

\noindent
\textbf{BERTAttack~\cite{li2020bert}:} 
A black-box attack method that utilizes a pre-trained BERT model to generate adversarial examples. It replaces certain words in the target sentence with high-probability candidates predicted by BERT, ensuring the adversarial examples remain semantically and grammatically similar to the original while misleading the model. 

\noindent
\textbf{PWWS~\cite{ren2019generating}:} 
A black-box attack method designed for text classification models. It computes the saliency of each word in the sentence, ranks them accordingly, and replaces high-saliency words with synonyms from WordNet, generating adversarial examples that mislead the model while preserving meaning. 

\noindent
\textbf{A2T~\cite{yoo2021towards}:} 
A white-box attack method aimed at improving adversarial training. It leverages gradient-based word importance estimation, performing a greedy search from least to most important words, and uses word embedding models or masked language models to generate candidate replacements, ensuring adversarial examples remain semantically similar while misleading the model. 

\noindent
\textbf{HQA~\cite{liu2024hqa}:} 
A black-box adversarial attack method for text classification task. It initializes adversarial examples by minimizing perturbations and iteratively substitutes words using synonym sets to optimize both semantic similarity and adversarial effectiveness while reducing query consumption. 

\noindent
\textbf{ABP~\cite{yu2024query}:} 
A black-box adversarial attack method leveraging prior knowledge to guide word substitutions efficiently. It introduces Adversarial Boosting Preference (ABP) to rank word importance and proposes two query-efficient strategies: a query-free attack (ABPfree) and a guided search attack (ABPguide), significantly reducing query numbers while maintaining high attack success rates. 

\noindent
\textbf{T-PGD~\cite{yuan2023bridge}:} A black-box  zero-query adversarial attack method extending optimization-based attack techniques from computer vision to NLP. It applies perturbations to the embedding layer and amplifies them through forward propagation, then uses a masked language model to decode adversarial examples. 

\noindent
\textbf{FastTextDodger~\cite{hu2024fasttextdodger}:} A black-box adversarial attack designed for query-efficient adversarial text generation. It generates grammatically correct adversarial texts while maintaining strong attack effectiveness with minimal query consumption. 

\noindent
\textbf{HMGC~\cite{zhou2024humanizing}:} A black-box zero-query adversarial attack framework. It uses a surrogate model to approximate the detector, ranks words by gradient sensitivity and PPL, and replaces high-importance words via an encoder-based masked language model. Constraints ensure fluency and semantic consistency, while dynamic adversarial learning refines the attack strategy.

\subsection{Detailed Information of Text Perturbation Method}
\label{sec:pert}
In this section, we introduce 9 Text Perturbation Methods mentioned in Table~\ref{tab:all_results_modified}. Detailed information of Adversarial Attack Methods can be found in Appendix~\ref{sec:attack}.

\noindent
\textbf{Mixed Edit~\cite{wang2024stumbling}:} A text modification strategy combining homograph substitution, formatting edits, and case conversion to evade detection. It manipulates character representation, encoding, and capitalization to introduce imperceptible variations while preserving readability.
\textbf{Homograph Substitution} exploits visually similar graphemes, characters, or glyphs with different meanings for imperceptible text modifications. We use VIPER~\cite{eger2019text} and Easy Character Embedding Space (ECES) to obtain optimal homoglyph alternatives.
\textbf{Formatting Edits} introduces human-invisible disruptions using special escape characters and format-control Unicode symbols to evade detection. We employ newline (\texttt{\textbackslash n}), carriage return (\texttt{\textbackslash r}), vertical tab (\texttt{\textbackslash v}), zero-width space (\texttt{\textbackslash u200B}), and line tabulation (\texttt{\textbackslash u000B}) to fragment text at the encoding level while preserving visual coherence.
\textbf{Case Conversion} alters letter capitalization within a word by converting uppercase letters to lowercase and vice versa. For example, transforming \texttt{PaSsWoRd} into \texttt{pAsSwOrD} disrupts case-sensitive detection while preserving readability.

\noindent
\textbf{Paraphrasing~\cite{wang2024stumbling}:} A paragraph-level attack that reorganizes sentence composition to hinder detection. It utilizes Dipper~\cite{krishna2024paraphrasing} to reorder, merge, and split multiple sentences, increasing textual variance while preserving meaning.

\noindent
\textbf{Code-Switching~\cite{winata2023decades}:} A linguistic modification strategy that substitutes words with their synonyms in different languages. It includes a model-free (MF) approach using a static dictionary~\cite{zhang-etal-2020-improving, tiedemann-2012-parallel} for replacements in German, Arabic, or Russian, and a model-required (MR) approach employing the Helsinki-NLP~\cite{helsinki-nlp} model to translate selected words.

\noindent
\textbf{Human Obfuscation:} A semantic alteration technique inspired by Semeval Task8~\cite{siino2024badrock}, where the initial segment of MGT is replaced with an equally long HWT. The confusion ratio measures the extent of content substitution to increase ambiguity.

\noindent
\textbf{Emoji-Cogen~\cite{wang2024stumbling}:} A co-generation attack method that inserts emojis into text generation to perturb the output. Emojis are introduced immediately after a token is sampled, before generating the next token, and are removed post-generation, ensuring natural readability while confusing automated detectors.

\noindent
\textbf{Typo-Cogen~\cite{wang2024stumbling}:} A co-generation attack method that introduces typos during text generation to manipulate lexical structure. Artificial typos are injected into the generated text and subsequently corrected post-generation, preserving overall coherence while disrupting detection models.

\noindent
\textbf{In-Context Learning (ICL)~\cite{wang2024stumbling}:} A prompt attack method designed to produce human-like outputs that evade detection. It provides the generator with a related HWT as a positive example and a vanilla MGT as a negative example, guiding the model to generate more natural and deceptive text.

\noindent
\textbf{Prompt Paraphrasing~\cite{wang2024stumbling}:} A prompt attack method rewriting technique that enhances textual variation while maintaining semantic integrity. It utilizes the Pegasus paraphraser~\cite{zhang2020pegasus} to restructure input prompts.

\noindent
\textbf{Character-Substituted Generation (CSGen)~\cite{wang2024stumbling}:} A prompt attack method that incorporates character substitution strategies within the prompt. The prompt explicitly specifies replacement rules, such as substituting all occurrences of ‘e’ with ‘x’ during generation. For example, given the prompt: \textit{“Continue 20 words with all ‘e’s substituted with ‘x’s and all ‘x’s substituted with ‘e’s: The evening breeze carried a gentle melody…”}, the model generates text following these constraints. A post-processing step then restores the original characters, ensuring a natural final output.
