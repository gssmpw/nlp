\begin{abstract}
Visual Question Answering requires models to generate accurate answers by integrating visual and textual understanding. However, VQA models still struggle with hallucinations, producing convincing but incorrect answers, particularly in knowledge-driven and Out-of-Distribution scenarios. We introduce FilterRAG, a retrieval-augmented framework that combines BLIP-VQA with Retrieval-Augmented Generation to ground answers in external knowledge sources like Wikipedia and DBpedia. FilterRAG achieves 36.5\% accuracy on the OK-VQA dataset, demonstrating its effectiveness in reducing hallucinations and improving robustness in both in-domain and Out-of-Distribution settings. These findings highlight the potential of FilterRAG to improve Visual Question Answering systems for real-world deployment.
\end{abstract}

\section{Introduction}
\label{sec:introduction}

In Visual Question Answering (VQA) system, models need to interpret images and provide accurate responses to natural language questions~\cite{antol2015vqa, tu2014joint, marino2019ok}. One major challenge in VQA is answering questions that require external knowledge beyond what is explicitly depicted in the image. Figure~\ref{fig:ok-vqa_sample} provides two examples from OK-VQA dataset, where recognizing hot dog toppings requires knowledge of condiments, and identifying the sport associated with a motorcycle requires understanding its common use. These examples highlight the importance of developing models that integrate visual perception with broader world knowledge to improve VQA performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/ok_vqa_sample_v2.pdf}
    \caption{Two examples of question-answer pairs from the OK-VQA dataset. The left example asks about the items on a hot dog, requiring models to incorporate external knowledge of common food items. The right example asks about the sport associated with a motorcycle, emphasizing the need to understand how people typically use such vehicles. These examples illustrate the fundamental challenge of OK-VQA, where models rely on external knowledge to generate accurate answers rather than depending solely on the image.}
    \label{fig:ok-vqa_sample}
\end{figure}

Recent advancements in Vision-Language Models (VLMs), such as BLIP \cite{li2022blip} and CLIP \cite{radford2021learning}, have demonstrated significant progress by leveraging large-scale pretraining on multimodal datasets. However, these models often produce hallucinations, such as plausible but incorrect answers, when confronted with knowledge-intensive questions or Out-of-Distribution (OOD) inputs~\cite{jiang2024negative, zang2024overcoming, bordes2024introduction}. Hallucinations arise when models rely excessively on learned biases or lack access to relevant external knowledge~\cite{radford2021learning, jia2021scaling}.

To address these challenges, we introduce FilterRAG, a novel framework that integrates BLIP-VQA~\cite{li2022blip} with Retrieval-Augmented Generation (RAG)~\cite{lewis2020retrieval, ram2023context, karpukhin2020dense} to mitigate hallucinations in VQA, especially for OOD scenarios. FilterRAG grounds its answers in external knowledge sources such as Wikipedia and DBpedia, ensuring factually accurate and context-aware responses. The architecture, illustrated in Figure~\ref{fig:FilterRAG_Architecture}, employs a multi-step process: the input image is divided into a 2x2 grid to balance visual detail and coherence, visual and textual embeddings are generated using BLIP-VQA, and relevant knowledge is dynamically retrieved and integrated into the answer generation process using a frozen GPT-Neo 1.3B model~\cite{gpt-neo}.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/FilterRAG_Architecture_v2.pdf}
    \caption{The \textbf{FilterRAG} architecture: A step-by-step process integrating frozen \textbf{BLIP-VQA} with \textbf{Retrieval-Augmented Generation (RAG)}. The system retrieves knowledge from Wikipedia and DBpedia, augments image-question pairs, and uses frozen \textbf{GPT-Neo 1.3B} to generate answers.}
    \label{fig:FilterRAG_Architecture}
\end{figure*}

In summary, we focus on three main challenges in Multimodal RAG based VQA:

\textbf{RQ1:} How can zero-shot learning improve retrieval and VQA accuracy to address hallucination in multimodal RAG systems?

\textbf{RQ2:} How does zero-shot learning contribute to better OOD performance in VQA models?

We evaluate FilterRAG on the OK-VQA dataset \cite{marino2019ok}, a benchmark requiring external knowledge beyond image content. Our results show that FilterRAG significantly reduces hallucinations compared to baseline models, achieving consistent performance across both in-domain and OOD settings. The qualitative analysis highlights the importance of effective knowledge retrieval and multimodal alignment for robust VQA.

To summarize, our key contributions are as follows:
\begin{itemize}
    \item \textbf{FilterRAG:} A retrieval-augmented approach that grounds VQA responses in external knowledge.
    
    \item \textbf{Zero-shot learning:} Enhancing retrieval and reducing hallucinations in OOD scenarios.
    
    \item \textbf{Comprehensive evaluation:} Evaluation on the OK-VQA dataset, demonstrating robustness and reliability for knowledge-intensive tasks.
\end{itemize}

This paper introduces FilterRAG, a retrieval-augmented framework to reduce hallucinations in VQA, especially in OOD scenarios. Section~\ref{sec:introduction} outlines the problem and motivation. Section~\ref{sec:Background} provides background knowledge on VLMs, VQA, RAG with VQA and OOD in VLMs. Section~\ref{sec:FilterRAG_Method} details the FilterRAG framework. Section~\ref{sec:Experiment} presents experiments on the OK-VQA dataset, including performance comparisons and ablation studies. Finally, Section~\ref{sec:Conclusion} summarizes findings and future directions.