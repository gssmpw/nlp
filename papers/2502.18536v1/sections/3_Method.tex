\section{The FilterRAG Method}
\label{sec:FilterRAG_Method}

\subsection{Overview}
FilterRAG integrates BLIP-VQA~\cite{li2022blip} with RAG to mitigate hallucinations in VQA, particularly in OOD scenarios. The architecture, illustrated in Figure~\ref{fig:FilterRAG_Architecture}, employs a multi-step process to ground VQA responses in external knowledge sources such as Wikipedia and DBpedia. The process begins by dividing the input image into a 2x2 grid to capture critical visual features while minimizing fragmentation. BLIP-VQA generates multimodal embeddings by encoding both the image and the associated question. The retrieval component then queries external knowledge sources, such as Wikipedia (using search-based and summarization techniques) and DBpedia (via SPARQL queries), to fetch relevant contextual information.

This retrieved knowledge is combined with the image-question pair, enriching the context for answer generation. A frozen GPT-Neo 1.3B~\cite{gpt-neo} model leverages this augmented information to produce the final answer. By grounding responses in retrieved factual data, FilterRAG effectively reduces hallucinations and enhances robustness, particularly for knowledge-intensive and OOD queries. Through the integration of external knowledge and efficient multimodal alignment, FilterRAG significantly improves the reliability and generalization of VQA systems, making it suitable for deployment in real-world applications where unseen concepts are common.

\subsection{Zero-Shot Learning in RAG Setting}
Zero-Shot Learning (ZSL)~\cite{xian2018zero, wang2019survey} enables models to generalize to unseen tasks or domains without requiring task-specific training data. For the VQA context, ZSL involves providing a model with an image $(I)$ and a question $(Q)$ and expecting it to produce accurate answers $(A)$ without fine-tuning task-specific datasets. Recent advancements in VLMs such as CLIP~\cite{radford2021learning}, ALIGN~\cite{jia2021scaling}, Frozen~\cite{tsimpoukelli2021multimodal}, and Flamingo~\cite{alayrac2022flamingo} have demonstrated robust performance across multiple downstream tasks through large-scale pretraining and multimodal alignment. Language Models (LMs) have also proven effective for Zero-Shot Learning through models like GPT-3~\cite{brown2020language} and T0~\cite{sanh2021multitask}, which leverage large-scale textual pretraining to perform a wide range of tasks without task-specific fine-tuning. 

Our method leverages BLIP-VQA~\cite{li2022blip} and the decoder-only language model GPT-Neo 1.3B~\cite{gpt-neo} within a Zero-Shot Learning setting. BLIP-VQA first aligns visual and textual features using its MED architecture. GPT-Neo 1.3B then utilizes this aligned context, along with the image description and question, to generate coherent and contextually relevant answers. To enhance robustness to OOD queries and reduce hallucinations, FilterRAG incorporates RAG, dynamically grounding responses in external knowledge sources. Our approach demonstrates strong performance on benchmarks like OK-VQA~\cite{marino2019ok}, which require knowledge beyond visual content.

\subsection{Visual Question Answering in Ok-VQA}
In the Visual Question Answering (VQA) task~\cite{antol2015vqa, marino2019ok, zhang2016yin, goyal2017making}, the goal is to predict the most appropriate answer ($A$) to a given question ($Q$) about an image ($I$). This relationship can be mathematically formalized as:
\begin{equation}
\hat{A} = \arg\max_{A \in \mathcal{A}} P(A \mid I, Q)
\end{equation}

where $A$ represents a possible answer, $I$ corresponds to the input image, and $Q$ denotes the input question. The OK-VQA dataset~\cite{marino2019ok} focuses specifically on open-domain questions that require external knowledge beyond the visual content of the image. Therefore, effective models for OK-VQA must combine visual and textual understanding with the ability to retrieve relevant external knowledge, ensuring accurate and context-aware responses.

VLMs generate the answer ($A$) as an open-ended sequence (e.g., free text), conditioned on both the image ($I$) and question ($Q$)~\cite{li2024configure}.  This can be formalized as:
\begin{equation}
P(\hat{A}) = \prod_{t=1}^{T} P(a_t \mid a_{1:t-1}, I, Q)
\end{equation}

where \( a_t \) denotes the token at time step \( t \) and \( a_{1:t-1} \) represents the preceding tokens. 

\subsection{Problem Formulation for RAG with VQA}
The objective of integrating RAG~\cite{lewis2020retrieval, ram2023context, karpukhin2020dense} with VQA is to predict the most accurate answer $A$ to a given question $Q$ about an image $I$ by leveraging both visual content and external knowledge retrieval. This process can be expressed probabilistically as:

\begingroup
\scriptsize
\begin{equation}
P_{\text{RAG}}(\hat{A}) \approx \prod_{i} \sum_{z \in \text{top-k}(p_\eta(\cdot \mid I, Q))} p_\eta(z \mid I, Q) p_\theta(a_i \mid I, Q, z, a_{1:i-1})
\end{equation}
\endgroup
\normalsize

Where \( z \) represents retrieved knowledge from an external corpus, \( p_\eta(z \mid I, Q) \) is the probability of retrieving \( z \) based on the image \( I \) and the question \( Q \), and \( p_\theta(a_i \mid I, Q, z, a_{1:i-1}) \) models the likelihood of generating the \( i \)-th token of the answer \( A \), conditioned on the previous tokens \( a_{1:i-1} \). In this formulation, the retriever \( p_\eta \) aims to fetch relevant knowledge \( z \) by leveraging both the visual content and the textual query. The retrieval process can be described as:
\begin{equation}
p_\eta(z \mid I, Q) \propto \exp\left(\mathbf{d}(z)^\top \mathbf{q}(I, Q)\right),
\end{equation}

where \( \mathbf{d}(z) \) is the embedding of the retrieved knowledge \( z \), and \( \mathbf{q}(I, Q) \) is the joint embedding of the image and the question. This formulation leverages a dual-encoder framework, similar to dense passage retrieval techniques~\cite{karpukhin2020dense}, and is further influenced by models such as Fusion-in-Decoder (FiD)~\cite{izacard2020leveraging}.


\subsection{OOD detection in VQA}
In Visual Question Answering (VQA), given an image \( I \) and a question \( Q \), the objective of out-of-distribution (OOD) detection is to determine whether the input pair belongs to the in-distribution dataset \( D_{\text{in}} \) or an OOD dataset \( D_{\text{OOD}} \)\cite{dong2024multiood, jiang2024negative, duong2023general, zang2024overcoming, bordes2024introduction}. This can be achieved using a scoring function \( S(I, Q) \) and a threshold \( \lambda \). The decision rule is defined as:
\begin{equation}
\footnotesize	
(I, Q) \in D_{\text{in}} \quad \text{if} \quad S(I, Q) \geq \lambda, \quad \text{else} \quad (I, Q) \in D_{\text{OOD}}.
\end{equation}

where \(D_{\text{in}}\) refers to the in-distribution dataset, \(D_{\text{OOD}}\) denotes the out-of-distribution dataset, \(S(I, Q)\) is the scoring function that computes the confidence for the pair, and \(\lambda\) is the threshold for distinguishing between \(D_{\text{in}}\) and \(D_{\text{OOD}}\). 

Our approach integrates these techniques within a RAG framework. By combining retrieval confidence with generation confidence, our scoring function $S(I,Q)$ captures both visual and knowledge-based uncertainties. This hybrid strategy improves OOD detection, enabling the model to flag uncertain inputs and enhancing the robustness of VQA systems.

\subsection{Binary Cross-Entropy Loss}
Binary cross-entropy loss is a standard measure for evaluating the correctness of predictions in classification tasks, including VQA. It is formulated as:
\begin{equation}
\mathcal{L} = - \frac{1}{n} \sum_{i=1}^n \left[ y_i \cdot \log(p_i) + (1 - y_i) \cdot \log(1 - p_i) \right]
\end{equation}

where \( n \) is the total number of predictions, \( y_i \) represents the ground-truth label for the \( i \)-th sample (\( y_i \in \{0, 1\} \)), and \( p_i \) is the predicted probability that the \( i \)-th sample belongs to the positive class.

In VQA, where answers can be evaluated against multiple valid responses, this loss function helps optimize model performance by reducing uncertainty and improving prediction accuracy~\cite{antol2015vqa, goyal2017making}. Models such as ViLBERT~\cite{lu2019vilbert} and LXMERT~\cite{tan2019lxmert} have effectively utilized binary cross-entropy loss to enhance their training processes, ensuring more reliable and accurate VQA outputs.

\subsection{Hallucination}
Grounding score \( g_{\text{mean}}(\hat{A}) \) quantifies semantic alignment between a predicted answer \(\hat{A}\) and ground truth answers in VQA. Using cosine similarity~\cite{radford2021learning, jia2021scaling} , the grounding score is:
\begin{equation}
g_{\text{mean}}(\hat{A}) = \frac{1}{n} \sum_{i=1}^n \frac{\mathbf{v}_{\text{pred}} \cdot \mathbf{v}_{\text{gt}}^i}{\|\mathbf{v}_{\text{pred}}\| \|\mathbf{v}_{\text{gt}}^i\|}
\end{equation}

where \( n \) is the number of ground truth answers, \( \mathbf{v}_{\text{pred}} \) is the embedding of the predicted answer \( \hat{A} \), and \( \mathbf{v}_{\text{gt}}^i \) is the embedding of the \( i \)-th ground truth answer. This grounding score measures the degree of alignment between the predicted and ground truth answers, capturing semantic similarity even when the answers differ lexically. Embedding models like word2vec~\cite{mikolov2013efficient}, GloVe~\cite{pennington2014glove}, and contextual models such as BERT~\cite{devlin2018bert} are commonly used to generate these embeddings. However, our approach replaces these traditional models with the more efficient Sentence Transformers (all-MiniLM-L6-v2)~\cite{reimers2019sentence}. This model produces compact and high-quality embeddings, enabling accurate measurement of alignment between predicted and ground truth answers while maintaining computational efficiency.   

Hallucination~\cite{zhou2020detecting, maynez2020faithfulness} is detected when the grounding score falls below a predefined threshold \( \tau \), indicating a lack of semantic alignment between the predicted answer and the ground truth:
\begin{equation}
\text{Hallucination} \quad \text{if} \quad g_{\text{mean}}(\hat{A}) < \tau
\end{equation}

Hallucinations occur when models generate plausible yet incorrect answers that are not supported by the input context. However, this problem is common in models like CLIP~\cite{radford2021learning} and BLIP~\cite{li2022blip} due to the reliance on learned biases. To address this challenge, our approach integrates BLIP-VQA~\cite{li2022blip} with RAG for fact-grounded answers. We enhance robustness by incorporating OOD detection to identify queries beyond the training data and applying a grounding score to measure semantic alignment. This combined strategy effectively reduces hallucinations and ensures accurate, context-aware answers.

