\section{Background}
\label{sec:Background}

\subsection{Vision Language Models}
Vision language models (VLMs) combine visual and linguistic data to understand and perform tasks requiring both image and text inputs~\cite{radford2021learning, jia2021scaling, singh2022flava}. By bridging the domains of Computer Vision (CV) and Natural Language Processing (NLP), these models can analyze complex scenes and respond meaningfully to textual descriptions, instructions, or queries. VLMs use multimodal embeddings to represent images and text in a shared feature space. This shared representation allows VLMs to align visual and textual information, supporting tasks like pairing images with captions or locating specific objects in images based on textual instructions. FilterRAG adopts the BLIP framework, leveraging its Multimodal mixture of Encoder-Decoder (MED) architecture for consistent visual and textual data processing in VQA~\cite{li2022blip}. This unified approach reduces memory usage and training time by sharing parameters between the encoder and decoder. As a result, BLIP enables faster inference without compromising accuracy, making it ideal for deployment in resource-constrained environments.

VLMs enable advanced applications such as VQA~\cite{antol2015vqa, zhang2016yin, goyal2017making}, image-text retrieval~\cite{luo2023end}, and image captioning~\cite{li2024lamp, covert2024locality}, expanding human-computer interaction capabilities. Despite these advancements, cross-modal alignment poses ongoing challenges, as aligning visual and linguistic data involves resolving complex ambiguities. Therefore, to ensure the safe and ethical deployment of VQA systems, we propose FilterRAG, a multimodal RAG framework. FilterRAG addresses hallucinations by grounding responses in retrieved external knowledge, enhancing robustness in OOD scenarios. By integrating multimodal retrieval with generative reasoning, our proposed approach effectively generalizes beyond the training knowledge base, providing accurate and context-aware answers to VQA queries.

\subsection{Visual Question Answering}
Visual Question Answering (VQA)~\cite{antol2015vqa, marino2019ok, zhang2016yin, goyal2017making} is a multimodal task that combines computer vision for image analysis (I) with natural language processing for question comprehension (Q) to generate accurate answers (A) about visual content. Recent VQA models, such as ViLBERT~\cite{lu2019vilbert}, VisualBERT~\cite{li2019visualbert}, VL-BERT~\cite{su2019vl}, and LXMERT~\cite{tan2019lxmert}, have significantly progressed through large-scale vision-language pretraining and sophisticated attention mechanisms. Their pretraining on large, diverse datasets, such as VQA 2.0~\cite{goyal2017making}, OK-VQA~\cite{marino2019ok}, VizWiz~\cite{bigham2010vizwiz}, and TDIUC~\cite{kafle2017analysis}, enables them to generalize well across various VQA tasks, improving performance on benchmarks requiring complex reasoning, multi-object interactions, and contextual understanding. Despite their advancements, these models frequently produce hallucinations and fail in OOD settings, a consequence of biased pretraining data that limits their robustness and adaptability.

To address these limitations, we propose a robust VQA framework that integrates BLIP-VQA~\cite{li2022blip} with RAG. By retrieving external knowledge, RAG grounds answers in factual information and improves performance on OOD queries. This retrieval mechanism expands the model knowledge beyond the training data, enhancing robustness and generalization. Our approach demonstrates significant improvements in answer accuracy on benchmarks such as VQA 2.0~\cite{goyal2017making} and OK-VQA~\cite{marino2019ok}. By unifying the BLIP architecture with retrieval-augmented techniques, the framework generates context-aware and reliable answers, making it suitable for real-world, dynamic environments.

\subsection{Retrieval-Augmented Generation with VQA}
Retrieval-Augmented Generation (RAG) enhances the effectiveness of VLMs by integrating external knowledge dynamically~\cite{lewis2020retrieval, ram2023context, karpukhin2020dense}. When a query involving visual and textual inputs is provided, the retriever searches external databases (e.g., Wikipedia) for relevant information. This retrieved content supplements the query, providing richer context. The generator then conditions its output on both the retrieved knowledge and the original query, producing more accurate, contextually grounded, and factually consistent responses~\cite{guo2022survey}. RAG, combined with VQA, effectively demonstrates significant progress in overcoming issues like hallucinations and poor OOD generalization. Recent works such as KAT~\cite{gui2021kat}, MAVEx~\cite{wu2022multi}, KRISP~\cite{marino2021krisp}, ConceptBERT~\cite{garderes2020conceptbert}, and EnFoRe~\cite{wu2022entity} focus on integrating external knowledge sources like Wikidata, Wikipedia, ConceptNet, or even web-based sources like Google Images~\cite{wu2022multi} to improve VQA systems. These methods use different strategies to fuse external knowledge with image and question inputs, whether by retrieving facts, aggregating knowledge graph nodes, or augmenting transformer-based architectures.

Despite advancements in methods like KAT~\cite{gui2021kat}, MAVEx~\cite{wu2022multi}, KRISP~\cite{marino2021krisp}, and ConceptBERT~\cite{garderes2020conceptbert}, these approaches often rely on external knowledge sources that may lack coverage for OOD  scenarios. Techniques such as RASO~\cite{fu2023generate} and TRiG~\cite{gao2022thousand} mitigate biases through answer refinement but struggle with noisy or irrelevant retrievals. Region-based methods like REVIVE~\cite{lin2022revive} and Mucko~\cite{zhu2020mucko} face scalability issues due to high-resolution processing demands. FilterRAG addresses these challenges by combining RAG with VLMs to enhance VQA performance in OOD settings, reducing hallucinations through efficient, contextually relevant retrieval. This approach improves upon existing works while maintaining computational efficiency, particularly for datasets like OK-VQA.

\subsection{Out-of-Distribution Detection in VLMs}
Out-of-Distribution (OOD) detection enhances model robustness by recognizing inputs that fall outside the training data distribution. Early work, such as~\cite{hendrycks2016baseline}, introduces a simple and effective method for OOD detection using the maximum softmax probability as a confidence score, where lower confidence scores indicate potential OOD data or misclassified inputs. In VLMs, OOD detection becomes more challenging due to multimodal representation shifts that occur when the model encounters novel or unseen data combinations. These shifts impact both the visual and textual data and, more importantly, how the two modalities interact within the latent space~\cite{tan2019lxmert, lu2019vilbert}.

For VLMs, given an input pair \( (x_v, x_t) \), where \( x_v \) is a visual input and \( x_t \) is a textual input, the task is to detect whether either the visual, textual, or their combined representation is OOD~\cite{dong2024multiood, jiang2024negative, duong2023general, zang2024overcoming, bordes2024introduction}. The embeddings from the two modalities, \( z_v = g_v(x_v) \) and \( z_t = g_t(x_t) \), are fused in a joint embedding space. The prediction probability \( \hat{p} \) can be obtained by a classifier \( h(\cdot) \) applied on the fused embeddings:

\begin{equation}
\hat{p} = \delta(h([z_v, z_t])) = \delta(h([g_v(x_v), g_t(x_t)])),
\end{equation}

where \( \delta(\cdot) \) is the softmax function, and \( h(\cdot) \) is a classifier.

In some methods, each modality can be checked for OOD status independently using separate classifiers \( h_v \) and \( h_t \) for vision and text:

\begin{equation}
\hat{p}_v = \delta(h_v(g_v(x_v))), \quad \hat{p}_t = \delta(h_t(g_t(x_t))),
\end{equation}

Finally, a threshold-based decision rule can be applied to classify the input as either In-Distribution (ID) or Out-of-Distribution (OOD). If the score \( S(x_v, x_t) \) exceeds a certain threshold \( \lambda \), the input is considered ID; otherwise, it is classified as OOD:

\begin{equation}
G_\lambda(x_v, x_t) = 
    \begin{cases} 
       \text{ID}, & \text{if } S(x_v, x_t) \geq \lambda \\
       \text{OOD}, & \text{if } S(x_v, x_t) < \lambda 
    \end{cases}
\end{equation}