\section{Experiment}
\label{sec:Experiment}

\subsection{Dataset}
Outside Knowledge Visual Question Answering (OK-VQA)~\cite{marino2019ok} is a benchmark dataset designed to evaluate VQA systems that require leveraging external knowledge sources beyond the information present in an image. The dataset consists of 14,055 knowledge-based questions paired with 14,031 images from the COCO dataset~\cite{lin2014microsoft}. These questions span 10 diverse knowledge categories, including domains such as Science and Technology, Geography, Cooking and Food, and Vehicles and Transportation. The questions were crowdsourced via Amazon Mechanical Turk, ensuring they require real-world knowledge to answer, making this dataset significantly more challenging than conventional VQA datasets. 

The dataset is split into 9,009 training samples and 5,046 testing samples, with each question associated with 10 ground-truth answers annotated by human annotators. This multi-answer format helps address ambiguity and variability in responses. Table~\ref{tab:okvqa_details} outlines key statistics and the distribution of questions across various knowledge categories in the Ok-VQA dataset. Baseline evaluations on OK-VQA using state-of-the-art models like MUTAN and Bilinear Attention Networks (BAN) reveal a significant drop in performance compared to traditional VQA datasets. This performance degradation underscores the need for models with enhanced retrieval and reasoning capabilities to incorporate unstructured, open-domain knowledge effectively.

\begin{table}[h!]
    \centering
    \footnotesize
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.2}
    \caption{Key Details of the OK-VQA Dataset}
    \begin{tabular}{|p{3.2cm}|p{4.8cm}|}
        \hline
        \textbf{Attribute}                & \textbf{Details} \\
        \hline
        \textbf{Name}                     & OK-VQA (Outside Knowledge VQA) \\
        \hline
        \textbf{Source}                   & COCO Image Dataset \\
        \hline
        \textbf{Number of Questions}      & 14,055 \\
        \hline
        \textbf{Number of Images}         & 14,031 \\
        \hline
        \textbf{Question Categories}      & 10 Categories \\
        \hline
        \textbf{Categories Breakdown}     & Vehicles \& Transportation (16\%) \newline Brands, Companies \& Products (3\%) \newline Objects, Materials \& Clothing (8\%) \newline Sports \& Recreation (12\%) \newline Cooking \& Food (15\%) \newline Geography, History, Language \& Culture (3\%) \newline People \& Everyday Life (9\%) \newline Plants \& Animals (17\%) \newline Science \& Technology (2\%) \newline Weather \& Climate (3\%) \newline Other (12\%) \\
        \hline
        \textbf{Average Question Length}  & 8.1 words \\
        \hline
        \textbf{Average Answer Length}    & 1.3 words \\
        \hline
        \textbf{Unique Questions}         & 12,591 \\
        \hline
        \textbf{Unique Answers}           & 14,454 \\
        \hline
        \textbf{Answer Annotations}       & 10 answers per question \\
        \hline
        \textbf{Answer Types}             & Open-ended \\
        \hline
        \textbf{Requires External Knowledge} & Yes (e.g., Wikipedia, Common Sense, etc.) \\
        \hline
        \textbf{Typical Knowledge Sources}& Unstructured Text (Wikipedia) \\
        \hline
    \end{tabular}
    \label{tab:okvqa_details}
\end{table}

\subsection{Implementation Details}
The experiments are conducted on Google Colab using a T4 GPU. The NVIDIA T4 GPU features 16 GB of GDDR6 memory, 320 Tensor Cores, and supports mixed-precision computation, making it suitable for deep learning tasks. Due to computational constraints, we evaluate our model on a subset of 100 samples from the OK-VQA dataset~\cite{marino2019ok}.

\subsection{OOD and ID Category Splits}
In our experiments, we evaluate our approach using the OK-VQA dataset~\cite{marino2019ok}, which we split into OOD and ID subsets based on knowledge categories. The OOD categories include Vehicles and Transportation, Brands, Companies and Products, Sports and Recreation, Science and Technology, and Weather and Climate. The ID categories comprise Objects, Materials and Clothing, Cooking and Food, Geography, History, Language and Culture, People and Everyday Life, Plants and Animals, and Other. Using this split, we can assess how well the model generalizes to different categories of knowledge.

\subsection{Patch-Based Image Preprocessing}
For VQA processing, we preprocess each input image by dividing it into patches of various sizes, specifically 2×2, 3×3 and 4x4 grids. This patch-based approach captures fine-grained visual details, which can enhance feature extraction for complex queries. We then employ the BLIP-VQA model~\cite{li2022blip} to extract image representations and generate initial contextual information based on the image and the associated question.

\subsection{Retrieval-Augmented Knowledge Integration}
To incorporate external knowledge, we use  RAG~\cite{lewis2020retrieval} with external knowledge sources such as Wikipedia and DBpedia. RAG retrieves relevant information based on the question and the visual features extracted by BLIP-VQA~\cite{li2022blip}. This retrieval process supplies the model with real-world context beyond the image, which is crucial for correctly answering questions that depend on external knowledge.

\subsection{State-of-the-Art Performance Comparison}
We evaluate our proposed FilterRAG framework on the OK-VQA dataset and compare it to state-of-the-art methods (Table~\ref{table:SOTA-OK-VQA}). The baseline models, Base1 and Base2, use the BLIP-VQA model with the VQA v2~\cite{goyal2017making} and OK-VQA datasets~\cite{marino2019ok}, achieving 83.0\% and 40.0\% accuracy, respectively. The drop highlights the challenge of knowledge-based questions in OK-VQA. Our FilterRAG framework, which integrates BLIP-VQA, RAG, and external knowledge sources like Wikipedia and DBpedia, achieves 36.5\% accuracy in OOD settings. This result demonstrates the effectiveness of grounding VQA responses with external knowledge, especially for OOD scenarios. 

Compared to state-of-the-art methods, KRISP~\cite{marino2021krisp}  achieves 38.35\% with Wikipedia and ConceptNet, while MAVEx~\cite{wu2022multi} reaches 41.37\% using Wikipedia, ConceptNet, and Google Images. The highest performance comes from KAT (ensemble)~\cite{gui2021kat} at 54.41\% with Wikipedia and Frozen GPT-3. Although these models achieve higher accuracy, they often require significant computational resources. 

FilterRAG balances performance and efficiency, making it suitable for resource-constrained environments. As shown in Figure~\ref{fig:plot1_accuracy}, it achieves 37.0\% accuracy in ID settings, 36.0\% in OOD settings, and 36.5\% when combining ID and OOD data. This highlights its robustness for knowledge-intensive VQA tasks.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/plot1_accuracy_v2.pdf}
    \caption{Comparison of Model Accuracy Across Different Settings.}
    \label{fig:plot1_accuracy}
\end{figure}

\begin{table*}[t]
    \centering
    \footnotesize
    \caption{Performance Comparison of State-of-the-Art Methods on the OK-VQA Dataset}
    \label{tab:okvqa_results}
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{10pt}
    \begin{tabular}{l l c}
        \toprule
        \textbf{Method}                                & \textbf{External Knowledge Sources}                          & \textbf{Accuracy (\%)} \\
        \midrule
        Q-only (Marino et al., 2019)~\cite{marino2019ok}                  & —                                                          & 14.93                  \\
        MLP (Marino et al., 2019)~\cite{marino2019ok}                     & —                                                          & 20.67                  \\
        BAN (Marino et al., 2019)~\cite{marino2019ok}              & —                                                          & 25.1                  \\
        MUTAN (Marino et al., 2019)~\cite{marino2019ok}               & —                                                          & 26.41                  \\
        ClipCap (Mokady et al., 2021)~\cite{mokady2021clipcap}                 & —                                                          & 22.8                   \\
        \midrule
        BAN + AN (Marino et al., 2019~\cite{marino2019ok}                  & Wikipedia                                                  & 25.61                  \\
        BAN + KG-AUG (Li et al., 2020)~\cite{li2020boosting}        & Wikipedia + ConceptNet                                     & 26.71                  \\
        Mucko (Zhu et al., 2020)~\cite{zhu2020mucko}                      & Dense Caption                                              & 29.2                   \\
        ConceptBERT (Gardères et al., 2020)~\cite{garderes2020conceptbert}           & ConceptNet                                                 & 33.66                  \\
        KRISP (Marino et al., 2021)~\cite{marino2021krisp}                   & Wikipedia + ConceptNet                                     & 38.35                  \\
        RVL (Shevchenko et al., 2021)~\cite{shevchenko2021reasoning}                 & Wikipedia + ConceptNet                                     & 39.0                   \\
        Vis-DPR (Luo et al., 2021)~\cite{luo2021weakly}                    & Google Search                                              & 39.2                   \\
        MAVEx (Wu et al., 2022)~\cite{wu2022multi}                       & Wikipedia + ConceptNet + Google Images                    & 41.37                  \\
        PICa-Full (Yang et al., 2022)~\cite{yang2022empirical}                 & Frozen GPT-3 (175B)                                        & 48.0                   \\
        KAT (Gui et al., 2022) (Ensemble)~\cite{gui2021kat}             & Wikipedia + Frozen GPT-3 (175B)                           & 54.41                  \\
        REVIVE (Lin et al., 2022) (Ensemble)~\cite{lin2022revive}          & Wikipedia + Frozen GPT-3 (175B)                           & 58.0                   \\
        RASO (Fu et al., 2023)~\cite{fu2023generate}                        & Wikipedia + Frozen Codex                                   & 58.5                   \\
        \midrule
        \textbf{FilterRAG (Ours)}                     & Wikipedia + DBpedia (\textbf{Frozen} BLIP-VQA and GPT-Neo 1.3B)    & \textbf{36.5}          \\
        \bottomrule
    \end{tabular}
    \label{table:SOTA-OK-VQA}
\end{table*}


\subsection{Hallucination Detection via Grounding Scores}
We evaluate the grounding scores of our FilterRAG framework against baseline models to assess its ability to mitigate hallucinations by aligning answers with external knowledge. As shown in Figure~\ref{fig:plot2_grounding_score}, Base1 achieves the highest grounding score of 94.60\% on the VQA v2 dataset~\cite{goyal2017making}, indicating that BLIP performs effectively when answering general-domain questions that do not require external knowledge. In contrast, Base2, evaluated on the OK-VQA dataset~\cite{marino2019ok}, shows a significant drop to 71.70\%, highlighting the challenge of answering knowledge-based questions without access to external information, thereby increasing the likelihood of hallucinations.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/plot2_grounding_score_v2.pdf}
    \caption{Grounding Score Comparison Across Baselines and Proposed Methods.}
    \label{fig:plot2_grounding_score}
\end{figure}

To address this limitation, our proposed method integrates BLIP-VQA, RAG, and external knowledge sources such as Wikipedia and DBpedia. The grounding scores for our method are 70.06\% for In-Distribution (ID) data, 70.68\% for Out-of-Distribution (OOD) data, and 70.37\% when combining both settings. These consistent scores demonstrate that FilterRAG effectively grounds answers in retrieved knowledge, reducing hallucinations even in challenging OOD scenarios.

Although our method does not achieve the grounding performance of Base1, it provides reliable results for knowledge-intensive tasks by leveraging external knowledge sources. This makes FilterRAG a robust and efficient solution for real-world VQA applications, particularly where external knowledge and OOD generalization are critical.

\subsection{Ablation Study}
We evaluate the effect of different image grid sizes on the performance of our FilterRAG framework with BLIP-VQA and RAG in OOD scenarios. We consider three grid configurations, 2x2, 3x3, and 4x4, and evaluate their influence on accuracy and grounding score. As shown in Figure~\ref{fig:plot5_measure_grid_size}, accuracy decreases slightly as the grid size increases. The accuracy is 37.00\% for the 2x2 grid, declines to 35.00\% for the 3x3 grid, and further drops to 34.00\% for the 4x4 grid. This downward trend indicates that larger grid sizes lead to excessive fragmentation, making it challenging for the model to extract coherent and meaningful visual features.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{figures/plot5_measure_grid_size_v2.pdf}
    \caption{Effect of Grid Sizes on Accuracy and Grounding Score.}
    \label{fig:plot5_measure_grid_size}
\end{figure}

Similarly, the grounding score follows a declining trend with increasing grid size. The grounding score is 70.06\% for the 2x2 grid, reducing to 69.20\% for the 3x3 grid and 68.07\% for the 4x4 grid. This decline suggests that finer grid divisions hinder the model’s ability to align generated answers with retrieved external knowledge, likely due to the loss of contextual coherence when images are broken into smaller patches.

Overall, the 2x2 grid size achieves the best trade-off between accuracy and grounding score. It maintains both visual coherence and effective knowledge alignment, thereby reducing the risk of hallucinations. Consequently, for OOD scenarios in the FilterRAG framework, the 2x2 grid configuration is the most effective for ensuring robust and reliable performance.

\subsection{Qualitative Analysis}
We perform a qualitative analysis of FilterRAG on the OK-VQA dataset~\cite{marino2019ok}, evaluating its performance in both In-Domain (ID) and Out-of-Distribution (OOD) settings. As illustrated in Figure~\ref{fig:Qualitative_Analysis}, FilterRAG generates accurate answers in ID scenarios where the retrieved knowledge is relevant and aligns well with the visual context. In these cases, the model effectively combines visual cues and external knowledge, resulting in well-grounded responses. These errors are frequently caused by misalignment between the visual context and the retrieved information, reflecting the challenge of handling ambiguous or novel queries outside the training distribution.

In OOD settings, FilterRAG struggles when relevant knowledge of unfamiliar concepts cannot be effectively retrieved. This often leads to hallucinations, where the model produces plausible but incorrect answers that are not supported by the retrieved evidence. This analysis highlights the critical role of reliable knowledge retrieval and precise multimodal alignment in mitigating hallucinations. Improving the quality of knowledge retrieval and refining visual-textual alignment are essential steps toward making FilterRAG more reliable in OOD contexts. Future improvements in these areas can help ensure more accurate and context-aware responses in real-world VQA applications.
