\section{Related Works}
\label{relatedWorks}

In this section, we will review the related works on the Large Language Model, Nuclear Fusion, and Chain-of-Thought. More related works can be found in the following surveys~\cite{wang2023MMPTMs} and paper list\footnote{\url{https://github.com/Event-AHU/AI_for_Controllable_Nuclear_Fusion/blob/main/Survey_Paper_list.md}}.  



\subsection{Large Language Model}  
LLMs have demonstrated remarkable language understanding and the ability to handle complex tasks through text generation~\cite{jin2024MSP60K, wang2024r2gencsr, wang2025AMMRG}. 
More in detail, GPT-3.0~\cite{kojima2022letsthinkstepbystep}, developed by OpenAI, was the first large language model to achieve industrial success, with 175 billion parameters enabling it to excel in natural language tasks. Its success spurred rapid advancements in large language models, leading to improved versions like GPT-4~\cite{achiam2023gpt4}, which offers stronger reasoning and broader knowledge. OpenAI o1\footnote{\url{https://openai.com/index/learning-to-reason-with-llms/}} gained attention for its exceptional complex reasoning, leveraging reinforcement learning and chain-of-thought training to surpass human PhD-level performance on the GPQA benchmark~\cite{rein2023gpqa} for physics, biology, and chemistry. LLaMA~\cite{touvron2023llama1} adopts a \textit{small models, large data} approach, producing high-performance models. Llama-1~\cite{touvron2023llama1}, offers four parameter sizes: 7B, 13B, 30B, and 65B, was trained on 1T+ tokens, while Llama-2~\cite{touvron2023llama2} expanded to 2T tokens, doubled context length to 4,096, and introduced GQA. Llama-3~\cite{grattafiori2024llama3} supports 8K contexts, uses a 128K vocabulary, and trains on over 15T tokens, delivering state-of-the-art performance with improved inference, code generation, and instruction-following capabilities. Gemini~\cite{team2023gemini}, Google's most advanced AI model, comes in three versions (Ultra, Pro, Nano) and supports diverse scenarios, focusing on complex reasoning, multimodal understanding, and coding. Claude\footnote{\url{https://claude.ai}}, developed by Anthropic, is a GPT-like AI model prioritizing safety, reliability, and alignment, with multiple improved versions released. 


On the other hand, Qwen~\cite{bai2023qwen} has consistently focused on the technical development of foundational models, advancing from its initial version to the latest 2.5 release. Compared to the previous version, the Qwen2.5~\cite{yang2024qwen2.5} demonstrates significant improvements in comprehension, logical reasoning, instruction following, and coding capabilities, with its Chinese language proficiency continuing to lead the industry. DeepSeek-V3~\cite{deepseekai2024deepseekv3} has 671 billion parameters, with 37 billion activated, offering performance on par with top models in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning, while being more cost-efficient. The Spark LLM\footnote{\url{https://xinghuo.xfyun.cn/}} by iFlytek excels in natural language processing for customer service, education, and healthcare. Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter model, outperforming ChatGPT in tasks like content creation, logical reasoning, and mathematical computation, providing efficient support for intelligent search, recommendation systems, and virtual assistants. Other LLMs, such as Baichuan~\cite{yang2023baichuan2}, Ernie Bot~\cite{sun2019erniebot}, Doubao\footnote{\url{https://www.doubao.com/chat/}}, SenseChat\footnote{\url{https://chat.sensetime.com/}}, and Bing Chat\footnote{\url{https://copilot.microsoft.com/}}, each have their unique features, covering a wide range of capabilities from multi-modal processing and code generation to conversational interactions. They are driving the deep application of artificial intelligence in various fields and accelerating the iteration and innovation of technology.


% DeepSeek-V3~\cite{deepseekai2024deepseekv3} boasts 671 billion parameters, yet with only 37 billion parameters activated, its performance is already on par with leading international models. It demonstrates exceptional capabilities in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning. The V3 version represents a significant leap in performance, speed, and cost-efficiency.

% The Spark Model\footnote{\url{https://xinghuo.xfyun.cn/}}, developed by iFlytek, is a cutting-edge cognitive model aimed at advancing artificial intelligence technology. With its exceptional natural language processing capabilities, it can understand and generate human language, finding applications in various fields such as intelligent customer service, education, and healthcare.

% Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter large language model, as a counterpart to ChatGPT. It boasts exceptional natural language processing capabilities and excels in areas such as content creation, knowledge-based Q\&A, code programming, logical reasoning, and mathematical computation. Tiangong provides more precise and efficient support for applications like intelligent search, recommendation systems, and virtual assistants. Additionally, its powerful learning and adaptability enable it to quickly adjust to different tasks and requirements, significantly enhancing user experience and industry efficiency.




\subsection{Nuclear Fusion}
% Nuclear fusion is the process by which light atomic nuclei combine under high temperatures and pressure to form heavier nuclei, releasing immense energy. It is the fundamental mechanism behind the light and heat emitted by stars like the Sun. Seen as a key to future clean energy, fusion produces vast amounts of energy with significantly less radioactive waste compared to fission, though the conditions required for fusion are extremely demanding. 

With the advancement of nuclear fusion, deep learning has found increasing applications in nuclear fusion research, aiding in solving complex physical problems and optimizing experimental processes, such as Q-distribution prediction~\cite{wang2024MMQdist, ma2024MHNQdist}, plasma state prediction, Tokamak control optimization, and plasma diagnostics. 
Yamaguchi et al.~\cite{yamaguchi2021geneticoptimization} uses a genetic algorithm to optimize the control points of three-dimensional B-spline curves, to solve the problem of designing and optimizing external coils for stellarators. 
Hu et al.~\cite{hu2021randomforest} solve the problem of real-time disruption prediction and mitigation in high-density discharges of the EAST tokamak by developing a random forest-based real-time disruption predictor (DPRF), improving the accuracy of disruption alarms and reducing disruption damage. 
Schmidt et al.~\cite{schmidt2024neural} employ a deep convolutional neural network to reconstruct fast-ion velocity distributions from fast-ion loss detectors and imaging neutral particle analyzers (INPAs). PlaNet~\cite{bonotto2024planet} solves the problem of fast and accurate plasma equilibrium and separatrix reconstruction using a physics-informed deep learning approach. 
Inoue et al.~\cite{inoue2024svm} use a Support Vector Machine (SVM) combined with redundant logic and an adaptive voltage allocation scheme to mitigate the risks of asymmetric heat loads on the first wall and electromagnetic loads on conductive materials caused by Vertical Displacement Events (VDEs). 
SExFC~\cite{li2024RNNGRU} integrates the recurrent neural network (RNN) algorithm and utilizes the Gated Recurrent Unit (GRU) for iterative prediction of flux evolution based on radial profiles. 
Zhang et al.~\cite{zhang2024yolo} use YOLO (You Only Look Once)~\cite{cvpr2016yolo, cvpr2017yolo9000, farhadi2018yolov3} to identify Ion Cyclotron Emission (ICE) in HL-2A discharges, aiming to enhance real-time fast ion diagnostics for magneto hydro dynamics (MHD) instabilities in fusion plasmas. 
Sun et al.~\cite{sun2024impactmlp} develop a multi-layer perceptron (MLP) neural network model as a surrogate for kinetic equilibrium fitting (EFITs) and investigate the impact of different diagnostic data and machine actuator controls on the accuracy of equilibrium reconstruction. 
Wan et al.~\cite{wan2023transformer} applies a transformer-based model to solve the real-time reconstruction of the last closed flux surface (LCFS) in the experimental advanced superconducting tokamak (EAST).


Some researchers adopt CNNs~\cite{boyer2024cnn_neural, seo2024cnn_avoiding, lin2024cnn_prediction, zanisi2024cnn_efficient, joung2024cnn_mlp_tokamak, bonotto2024cnn_mlp_reconstruction}, MLPs~\cite{sun2024mlp_impact, sanchez2024mlp_real, mehta2024mlp_automated, joung2024cnn_mlp_tokamak, tracey2024mlp_lstm_towards, bonotto2024cnn_mlp_reconstruction}, or LSTMs~\cite{tracey2024mlp_lstm_towards, guo2023lstm_disruption, shin2022lstm_preemptive} as their backbone networks to tackle various key challenges in fusion research. 
An increasing number of scholars are applying artificial intelligence (AI) methods to the field of nuclear fusion, and AI is expected to accelerate the commercialization of fusion energy.



\subsection{Chain-of-Thought}
Chain of Thought (CoT)~\cite{wei2022CoT} is a widely used reasoning approach in the field of artificial intelligence, particularly in tackling complex reasoning tasks. The core idea of CoT is to break down the problem-solving process into a series of logically coherent and interconnected steps, enabling the model to progressively arrive at the final answer. 
Wei et al.~\cite{wei2022CoT} were the first to introduce CoT prompting to large language models, aiming to enhance their performance on complex reasoning tasks. 
Feng et al.~\cite{feng2024towards} explained how CoT enhances the ability of large language models (LLMs) to solve complex tasks and validated its effectiveness. 
Kojima et al.~\cite{kojima2022letsthinkstepbystep} simulated the CoT process and addressed the complex reasoning task capabilities of LLMs with few-shot examples by using the simple prompt "\textit{Let's think step by step}".  
Hao et al.~\cite{hao2024coconut} introduce the Chain of Continuous Thought (Coconut), which shifts reasoning from the language space to the latent space, addressing the efficiency and performance challenges in complex reasoning tasks due to linguistic limitations. 
Works such as ~\cite{chen2023cotexplainyou, madaan2023cotexplainmakes, wang2022cotexplaintowards, wu2023cotexplainanalyzing} aim to explain how CoT works. Meanwhile, \cite{ge2023enhancechain, hu2024enhancechain, cohn2024enhancechain, nong2024enhancechain, li2023enhancestructured} use CoT prompting to fine-tune LLMs, enhancing their capabilities in specific fields. 
We also aim to make LLMs experts in the field of nuclear fusion through the CoT approach, providing support to nuclear fusion researchers.