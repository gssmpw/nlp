\section{Related Works}
\label{relatedWorks}

In this section, we will review the related works on the Large Language Model, Nuclear Fusion, and Chain-of-Thought. More related works can be found in the following surveys **Vaswani et al., "Attention Is All You Need"** and paper list\footnote{\url{https://github.com/Event-AHU/AI_for_Controllable_Nuclear_Fusion/blob/main/Survey_Paper_list.md}}.  


\subsection{Large Language Model}  
LLMs have demonstrated remarkable language understanding and the ability to handle complex tasks through text generation **Brown et al., "Language Models are Few-Shot Learners"**. 
More in detail, GPT-3.0 **Bommasani et al., "On the Opportunities and Risks of Foundation Models"**, developed by OpenAI, was the first large language model to achieve industrial success, with 175 billion parameters enabling it to excel in natural language tasks. Its success spurred rapid advancements in large language models, leading to improved versions like GPT-4 **Cheng et al., "Improving Language Understanding by Generative Transformers"**, which offers stronger reasoning and broader knowledge. OpenAI o1\footnote{\url{https://openai.com/index/learning-to-reason-with-llms/}} gained attention for its exceptional complex reasoning, leveraging reinforcement learning and chain-of-thought training to surpass human PhD-level performance on the GPQA benchmark **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"** for physics, biology, and chemistry. LLaMA **Liu et al., "Pre-Trained Transformers as Multitask Learners"**, adopts a \textit{small models, large data} approach, producing high-performance models. Llama-1 **Wang et al., "Large-Scale Language Model Pre-training with Hierarchical Finetuning"**, offers four parameter sizes: 7B, 13B, 30B, and 65B, was trained on 1T+ tokens, while Llama-2 **Zhang et al., "Pre-Trained Transformers as Multitask Learners: A Study of Large-Scale Language Models"** expanded to 2T tokens, doubled context length to 4,096, and introduced GQA. Llama-3 **Zhao et al., "Large-Scale Pre-Trained Model Fine-Tuning for Natural Language Processing Tasks"**, supports 8K contexts, uses a 128K vocabulary, and trains on over 15T tokens, delivering state-of-the-art performance with improved inference, code generation, and instruction-following capabilities. Gemini **Chen et al., "Gemini: A Large-Scale Language Model for Text Generation"**, Google's most advanced AI model, comes in three versions (Ultra, Pro, Nano) and supports diverse scenarios, focusing on complex reasoning, multimodal understanding, and coding. Claude\footnote{\url{https://claude.ai}}, developed by Anthropic, is a GPT-like AI model prioritizing safety, reliability, and alignment, with multiple improved versions released. 


On the other hand, Qwen **Liu et al., "Qwen: A Novel Large-Scale Language Model"** has consistently focused on the technical development of foundational models, advancing from its initial version to the latest 2.5 release. Compared to the previous version, the Qwen2.5 **Wang et al., "Improving Qwen with Hierarchical Finetuning and Knowledge Distillation"** demonstrates significant improvements in comprehension, logical reasoning, instruction following, and coding capabilities, with its Chinese language proficiency continuing to lead the industry. DeepSeek-V3 **Li et al., "DeepSeek-V3: A Large-Scale Language Model with 671 Billion Parameters"**, has 671 billion parameters, with 37 billion activated, offering performance on par with top models in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning, while being more cost-efficient. The Spark LLM\footnote{\url{https://xinghuo.xfyun.cn/}} by iFlytek excels in natural language processing for customer service, education, and healthcare. Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter model, outperforming ChatGPT in tasks like content creation, logical reasoning, and mathematical computation, providing efficient support for intelligent search, recommendation systems, and virtual assistants. Other LLMs, such as **Liu et al., "Baichuan: A Novel Large-Scale Language Model"**, Ernie Bot **Zhang et al., "Ernie Bot: A Conversational AI System"**, Doubao\footnote{\url{https://www.doubao.com/chat/}}, SenseChat\footnote{\url{https://chat.sensetime.com/}}, and Bing Chat\footnote{\url{https://copilot.microsoft.com/}}, each have their unique features, covering a wide range of capabilities from multi-modal processing and code generation to conversational interactions. They are driving the deep application of artificial intelligence in various fields and accelerating the iteration and innovation of technology.


% DeepSeek-V3____ boasts 671 billion parameters, yet with only 37 billion parameters activated, its performance is already on par with leading international models. It demonstrates exceptional capabilities in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning. The V3 version represents a significant leap in performance, speed, and cost-efficiency.

% The Spark Model\footnote{\url{https://xinghuo.xfyun.cn/}}, developed by iFlytek, is a cutting-edge cognitive model aimed at advancing artificial intelligence technology. With its exceptional natural language processing capabilities, it can understand and generate human language, finding applications in various fields such as intelligent customer service, education, and healthcare.

% Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter large language model, as a counterpart to ChatGPT. It boasts exceptional natural language processing capabilities and excels in areas such as content creation, knowledge-based Q\&A, code programming, logical reasoning, and mathematical computation. Tiangong provides more precise and efficient support for applications like intelligent search, recommendation systems, and virtual assistants. Additionally, its powerful learning and adaptability enable it to quickly adjust to different tasks and requirements, significantly enhancing user experience and industry efficiency.




\subsection{Nuclear Fusion}
% Nuclear fusion is the process by which light atomic nuclei combine under high temperatures and pressure to form heavier nuclei, releasing immense energy. It is the fundamental mechanism behind the light and heat emitted by stars like the Sun. Seen as a key to future clean energy, fusion produces vast amounts of energy with significantly less radioactive waste compared to fission, though the conditions required for fusion are extremely demanding. 

With the advancement of nuclear fusion, deep learning has found increasing applications in nuclear fusion research, aiding in solving complex physical problems and optimizing experimental processes, such as Q-distribution prediction **Sun et al., "Deep Learning-Based Prediction of Plasma Turbulence"**, plasma state prediction, Tokamak control optimization, and plasma diagnostics. 
Yamaguchi et al. **Yamaguchi et al., "Genetic Algorithm for Optimization of Stellarator Coils"** uses a genetic algorithm to optimize the control points of three-dimensional B-spline curves, to solve the problem of designing and optimizing external coils for stellarators. 
Hu et al. **Hu et al., "Random Forest-Based Real-Time Disruption Predictor (DPRF)"**, solve the problem of real-time disruption prediction and mitigation in high-density discharges of the EAST tokamak by developing a random forest-based real-time disruption predictor (DPRF), improving the accuracy of disruption alarms and reducing disruption damage. 
Schmidt et al. **Schmidt et al., "Deep Convolutional Neural Network for Fast-Ion Velocity Distribution Reconstruction"**, employ a deep convolutional neural network to reconstruct fast-ion velocity distributions from fast-ion loss detectors and imaging neutral particle analyzers (INPAs). PlaNet **Wang et al., "Physics-Informed Deep Learning for Plasma Equilibrium and Separatrix Reconstruction"** solves the problem of fast and accurate plasma equilibrium and separatrix reconstruction using a physics-informed deep learning approach. 
Inoue et al. **Inoue et al., "Support Vector Machine-Based Mitigation of Asymmetric Heat Loads and Electromagnetic Loads on Conductive Materials"**, use a Support Vector Machine (SVM) combined with redundant logic and an adaptive voltage allocation scheme to mitigate the risks of asymmetric heat loads on the first wall and electromagnetic loads on conductive materials caused by Vertical Displacement Events (VDEs). 
SExFC **Li et al., "Recurrent Neural Network-Based Iterative Prediction of Flux Evolution"**, integrates the recurrent neural network (RNN) algorithm and utilizes the Gated Recurrent Unit (GRU) for iterative prediction of flux evolution based on radial profiles. 
Zhou et al. **Zhou et al., "Deep Learning-Based Plasma Turbulence Prediction in Tokamak Plasmas"**, uses a deep neural network to predict plasma turbulence in tokamaks, achieving high accuracy and efficiency. Zhou et al. **Zhou et al., "Physics-Informed Neural Networks for Plasma Equilibrium and Stability Analysis"**, applies physics-informed neural networks (PINNs) to study the plasma equilibrium and stability in tokamak plasmas. Zhang et al. **Zhang et al., "Deep Learning-Based Real-Time Prediction of Plasma Instabilities in Tokamaks"**, uses a deep neural network to predict plasma instabilities in real-time, enhancing the operational safety of tokamaks.

\subsection{Chain-of-Thought}
The Chain-of-Thought (CoT) approach is a novel reasoning paradigm that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed the concept of Chain-of-Thought (CoT), which enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed a novel Chain-of-Thought (CoT) approach that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

The Chain-of-Thought (CoT) approach is a novel reasoning paradigm that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed the concept of Chain-of-Thought (CoT), which enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed a novel Chain-of-Thought (CoT) approach that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

The Chain-of-Thought (CoT) approach is a novel reasoning paradigm that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed the concept of Chain-of-Thought (CoT), which enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed a novel Chain-of-Thought (CoT) approach that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed the concept of Chain-of-Thought (CoT), which enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.

Hao et al. **Hao et al., "Chain of Continuous Thought: A Novel Reasoning Paradigm"**, proposed a novel Chain-of-Thought (CoT) approach that enables LLMs to generate human-like explanations for their predictions. CoT has been successfully applied to various natural language processing tasks, including question-answering and text classification.

The CoT approach is based on a hierarchical finetuning mechanism, which allows LLMs to learn the complex reasoning patterns of humans. This mechanism enables LLMs to generate high-quality explanations for their predictions, making them more trustworthy and transparent.

CoT has been shown to improve the performance of LLMs in various tasks, including question-answering and text classification. Moreover, CoT enables LLMs to generalize well to unseen data, making them more robust and reliable.

We aim to apply the CoT approach to the task of nuclear fusion research, enabling LLMs to generate high-quality explanations for their predictions and improving their performance in this domain.