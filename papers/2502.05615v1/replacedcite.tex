\section{Related Works}
\label{relatedWorks}

In this section, we will review the related works on the Large Language Model, Nuclear Fusion, and Chain-of-Thought. More related works can be found in the following surveys____ and paper list\footnote{\url{https://github.com/Event-AHU/AI_for_Controllable_Nuclear_Fusion/blob/main/Survey_Paper_list.md}}.  



\subsection{Large Language Model}  
LLMs have demonstrated remarkable language understanding and the ability to handle complex tasks through text generation____. 
More in detail, GPT-3.0____, developed by OpenAI, was the first large language model to achieve industrial success, with 175 billion parameters enabling it to excel in natural language tasks. Its success spurred rapid advancements in large language models, leading to improved versions like GPT-4____, which offers stronger reasoning and broader knowledge. OpenAI o1\footnote{\url{https://openai.com/index/learning-to-reason-with-llms/}} gained attention for its exceptional complex reasoning, leveraging reinforcement learning and chain-of-thought training to surpass human PhD-level performance on the GPQA benchmark____ for physics, biology, and chemistry. LLaMA____ adopts a \textit{small models, large data} approach, producing high-performance models. Llama-1____, offers four parameter sizes: 7B, 13B, 30B, and 65B, was trained on 1T+ tokens, while Llama-2____ expanded to 2T tokens, doubled context length to 4,096, and introduced GQA. Llama-3____ supports 8K contexts, uses a 128K vocabulary, and trains on over 15T tokens, delivering state-of-the-art performance with improved inference, code generation, and instruction-following capabilities. Gemini____, Google's most advanced AI model, comes in three versions (Ultra, Pro, Nano) and supports diverse scenarios, focusing on complex reasoning, multimodal understanding, and coding. Claude\footnote{\url{https://claude.ai}}, developed by Anthropic, is a GPT-like AI model prioritizing safety, reliability, and alignment, with multiple improved versions released. 


On the other hand, Qwen____ has consistently focused on the technical development of foundational models, advancing from its initial version to the latest 2.5 release. Compared to the previous version, the Qwen2.5____ demonstrates significant improvements in comprehension, logical reasoning, instruction following, and coding capabilities, with its Chinese language proficiency continuing to lead the industry. DeepSeek-V3____ has 671 billion parameters, with 37 billion activated, offering performance on par with top models in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning, while being more cost-efficient. The Spark LLM\footnote{\url{https://xinghuo.xfyun.cn/}} by iFlytek excels in natural language processing for customer service, education, and healthcare. Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter model, outperforming ChatGPT in tasks like content creation, logical reasoning, and mathematical computation, providing efficient support for intelligent search, recommendation systems, and virtual assistants. Other LLMs, such as Baichuan____, Ernie Bot____, Doubao\footnote{\url{https://www.doubao.com/chat/}}, SenseChat\footnote{\url{https://chat.sensetime.com/}}, and Bing Chat\footnote{\url{https://copilot.microsoft.com/}}, each have their unique features, covering a wide range of capabilities from multi-modal processing and code generation to conversational interactions. They are driving the deep application of artificial intelligence in various fields and accelerating the iteration and innovation of technology.


% DeepSeek-V3____ boasts 671 billion parameters, yet with only 37 billion parameters activated, its performance is already on par with leading international models. It demonstrates exceptional capabilities in knowledge-based Q\&A, long-text processing, code generation, and mathematical reasoning. The V3 version represents a significant leap in performance, speed, and cost-efficiency.

% The Spark Model\footnote{\url{https://xinghuo.xfyun.cn/}}, developed by iFlytek, is a cutting-edge cognitive model aimed at advancing artificial intelligence technology. With its exceptional natural language processing capabilities, it can understand and generate human language, finding applications in various fields such as intelligent customer service, education, and healthcare.

% Tiangong\footnote{\url{https://www.tiangong.cn/}} is China's first dual-trillion-parameter large language model, as a counterpart to ChatGPT. It boasts exceptional natural language processing capabilities and excels in areas such as content creation, knowledge-based Q\&A, code programming, logical reasoning, and mathematical computation. Tiangong provides more precise and efficient support for applications like intelligent search, recommendation systems, and virtual assistants. Additionally, its powerful learning and adaptability enable it to quickly adjust to different tasks and requirements, significantly enhancing user experience and industry efficiency.




\subsection{Nuclear Fusion}
% Nuclear fusion is the process by which light atomic nuclei combine under high temperatures and pressure to form heavier nuclei, releasing immense energy. It is the fundamental mechanism behind the light and heat emitted by stars like the Sun. Seen as a key to future clean energy, fusion produces vast amounts of energy with significantly less radioactive waste compared to fission, though the conditions required for fusion are extremely demanding. 

With the advancement of nuclear fusion, deep learning has found increasing applications in nuclear fusion research, aiding in solving complex physical problems and optimizing experimental processes, such as Q-distribution prediction____, plasma state prediction, Tokamak control optimization, and plasma diagnostics. 
Yamaguchi et al.____ uses a genetic algorithm to optimize the control points of three-dimensional B-spline curves, to solve the problem of designing and optimizing external coils for stellarators. 
Hu et al.____ solve the problem of real-time disruption prediction and mitigation in high-density discharges of the EAST tokamak by developing a random forest-based real-time disruption predictor (DPRF), improving the accuracy of disruption alarms and reducing disruption damage. 
Schmidt et al.____ employ a deep convolutional neural network to reconstruct fast-ion velocity distributions from fast-ion loss detectors and imaging neutral particle analyzers (INPAs). PlaNet____ solves the problem of fast and accurate plasma equilibrium and separatrix reconstruction using a physics-informed deep learning approach. 
Inoue et al.____ use a Support Vector Machine (SVM) combined with redundant logic and an adaptive voltage allocation scheme to mitigate the risks of asymmetric heat loads on the first wall and electromagnetic loads on conductive materials caused by Vertical Displacement Events (VDEs). 
SExFC____ integrates the recurrent neural network (RNN) algorithm and utilizes the Gated Recurrent Unit (GRU) for iterative prediction of flux evolution based on radial profiles. 
Zhang et al.____ use YOLO (You Only Look Once)____ to identify Ion Cyclotron Emission (ICE) in HL-2A discharges, aiming to enhance real-time fast ion diagnostics for magneto hydro dynamics (MHD) instabilities in fusion plasmas. 
Sun et al.____ develop a multi-layer perceptron (MLP) neural network model as a surrogate for kinetic equilibrium fitting (EFITs) and investigate the impact of different diagnostic data and machine actuator controls on the accuracy of equilibrium reconstruction. 
Wan et al.____ applies a transformer-based model to solve the real-time reconstruction of the last closed flux surface (LCFS) in the experimental advanced superconducting tokamak (EAST).


Some researchers adopt CNNs____, MLPs____, or LSTMs____ as their backbone networks to tackle various key challenges in fusion research. 
An increasing number of scholars are applying artificial intelligence (AI) methods to the field of nuclear fusion, and AI is expected to accelerate the commercialization of fusion energy.



\subsection{Chain-of-Thought}
Chain of Thought (CoT)____ is a widely used reasoning approach in the field of artificial intelligence, particularly in tackling complex reasoning tasks. The core idea of CoT is to break down the problem-solving process into a series of logically coherent and interconnected steps, enabling the model to progressively arrive at the final answer. 
Wei et al.____ were the first to introduce CoT prompting to large language models, aiming to enhance their performance on complex reasoning tasks. 
Feng et al.____ explained how CoT enhances the ability of large language models (LLMs) to solve complex tasks and validated its effectiveness. 
Kojima et al.____ simulated the CoT process and addressed the complex reasoning task capabilities of LLMs with few-shot examples by using the simple prompt "\textit{Let's think step by step}".  
Hao et al.____ introduce the Chain of Continuous Thought (Coconut), which shifts reasoning from the language space to the latent space, addressing the efficiency and performance challenges in complex reasoning tasks due to linguistic limitations. 
Works such as ____ aim to explain how CoT works. Meanwhile, ____ use CoT prompting to fine-tune LLMs, enhancing their capabilities in specific fields. 
We also aim to make LLMs experts in the field of nuclear fusion through the CoT approach, providing support to nuclear fusion researchers.