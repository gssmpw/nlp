\pdfoutput=1
\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}
\usepackage{arxiv}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\usepackage[pdftex]{graphicx}
\graphicspath{ {./figures/} }
\usepackage{caption}
\usepackage{subcaption}
\usepackage[numbers]{natbib}


\title{DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Shashank Sharma \\
  Department of Computer Science\\
  University of Bath \\
  \texttt{ss3966@bath.ac.uk} \\
  % examples of more authors
  \AND
  Janina Hoffmann \\
  Department of Psychology\\
  University of Bath \\
  \texttt{jah253@bath.ac.uk} \\
  \And
  Vinay Namboodiri \\
  Department of Computer Science\\
  University of Bath \\
  \texttt{vpn22@bath.ac.uk} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\begin{abstract}

In this paper, we address the challenge of long-horizon visual planning tasks using Hierarchical Reinforcement Learning (HRL).
Our key contribution is a Discrete Hierarchical Planning (DHP) method, an alternative to traditional distance-based approaches.
We provide theoretical foundations for the method and demonstrate its effectiveness through extensive empirical evaluations.
% , achieving significantly improved performance.

Our agent recursively predicts subgoals in the context of a long-term goal and receives discrete rewards for constructing plans as compositions of abstract actions.
The method introduces a novel advantage estimation strategy for tree trajectories, which inherently encourages shorter plans and enables generalization beyond the maximum tree depth.
The learned policy function allows the agent to plan efficiently, requiring only $\log N$ computational steps, making re-planning highly efficient.
The agent, based on a soft-actor critic (SAC) framework, is trained using on-policy imagination data.
Additionally, we propose a novel exploration strategy that enables the agent to generate relevant training examples for the planning modules.
We evaluate our method on long-horizon visual planning tasks in a 25-room environment, where it significantly outperforms previous benchmarks at success rate and average episode length.
Furthermore, an ablation study highlights the individual contributions of key modules to the overall performance.
\end{abstract}


\section{Introduction}

Reinforcement learning (RL) has achieved significant success in solving complex decision-making problems, ranging from playing games to robotic control \cite{mnih2015human,silver2017mastering}.
An important characteristic of high-performing agents is planning and reasoning for long-term goals.
Traditional sequential planning methods such as Monte-Carlo Tree Search (MCTS) \cite{silver2017mastering,browne2012survey}, Visual Foresight \cite{ebert2018visual}, Imagination-based search \cite{hafner2019learning}, Graph search \cite{eysenbach2019search} often struggle to scale efficiently with environments with long-time horizons and intricate dependencies, as the search space grows exponentially with problem complexity \cite{sutton2018reinforcement}.
% I am writing a paper on 
Hierarchical planning methods address this challenge by recursively decomposing tasks into simpler subtasks \cite{parr1997reinforcement,dietterich2000hierarchical,pertsch2020long,ao2021co,jurgenson2020sub}.
Unlike sequential planning, hierarchical planning enables the reuse of learned sub-policies or abstractions across different tasks or environments, promoting generalization and reducing the need for retraining from scratch \cite{sutton1999between}.
%This capability is particularly valuable in real-world applications, where data is often limited and costly.
%Moreover, hierarchical planning aligns with how humans naturally decompose complex tasks into hierarchies of sub-tasks \cite{botvinick2009hierarchically,botvinick2014model}.
%For example, planning a trip involves high-level goals like: selecting a location, booking tickets and hotel, and preparing for travel, which are further broken down into subgoals such as internet search, comparing/selecting a flight/hotel, packing luggage, and choosing a route to the airport, and then further broken down to simple muscle movements.

Despite these advantages, training hierarchical planning agents has been challenging.
The current hierarchical Planning methods typically optimize to minimize a temporal distance metric between a pair of states \cite{pertsch2020long,ao2021co,jurgenson2020sub}.
Though the distance-based metric has given good results, they are highlighted as both crucial for success and challenging to learn.
For instance, \cite{eysenbach2019search} use an explicit graph search for planning and emphasize that the success of their \textit{SearchPolicy} depends heavily on the accuracy of our distance estimates and ignore distances above a threshold for search.
\cite{ao2021co} also state it is challenging to learn or estimate the distance accurately in complicated tasks such as mazes.
A key difficulty arises because the quality of the distance metric is highly dependent on the current policy and a suboptimal policy can produce inaccurate distance measures, which in turn negatively impacts learning.
This issue is exacerbated when learning from self-collected exploratory data, as the agent may remain stationary or move in circles, causing erroneous distance metrics to accumulate.
%Additionally, distance-based methods can suffer from the problem of hallucinating distances between unconnected or untrained state pairs, as the model is trained to output a real number regardless of connectivity.
Some approaches attempt to mitigate these issues by relying on expert data; however, this can lead to incomplete environmental understanding, as expert data may lack critical experiences such as wall collisions or other failure cases.
% These challenges make it difficult to train Hierarchical Planning agents with distance-based metrics reliably.

To address these limitations, we propose a Discrete Hierarchical Planning (DHP) method that evaluate plans with a reachability check.
Unlike distance-based metrics, the reachability check avoids ambiguous learning signals and the pitfalls of handling unconnected states, making it more robust and easier to learn.
Reachability is inherently easier to learn because agents frequently transition between nearby states, allowing them to develop a well-understood representation of the local state space.
Additionally, for unconnected states, the reachability can simply be $0$.

Equipped with the reachability check we construct an Hierarchical Reinforcement Learning (HRL) agent that learns to plan by recursively predicting subgoals in the context of a long-term goal.
This recursive prediction generates plans as subtask trees, where lower levels correspond to simpler subtasks.
The tree plans are evaluated using discrete reachability-based rewards and a novel advantage estimation strategy for tree trajectories.
The method encourages shorter plans and allows generalization beyond the maximum training depth.
Instead of relying on expert data, our method uses self-generated exploratory data collected through a novel intrinsic reward strategy.
This exploration-driven approach ensures the agent learns accurate world dynamics through active environmental interaction.
% Unlike traditional planning methods, such as the Cross-Entropic Method (CEM), which unrolls multiple trees during inference, evaluates their quality, and then selects a plan, our approach uses a learned policy.

%The agent architecture consists of three modules primarily: perception, worker, and manager.
%The perception module constructs a state representation over time using the observations.
%The manager provides hierarchically planned subgoals to the worker, a goal-conditioned policy that predicts atomic actions to achieve these subgoals.
%The perception module learns world dynamics and allows the agent to generate imagined trajectories by simulating world dynamics.
%The worker and the manager are trained using on-policy data generated efficiently through imagination, eliminating the need for expensive real-world interactions.

%During inference, our method predicts only the first branch of the tree in $O(\log N)$ time for a plan of length $N$ and employs an $O(1)$ reachability check.
%This helps reduce the required computations during inference to $O(\log N)$, making it highly efficient for real-time re-planning—a critical requirement in dynamic environments.

The key contributions of this work can be summarized as follows:
\begin{itemize}
    \item A reachability-based discrete reward scheme as an alternative to traditional continuous distance metrics (Section \ref{sec:final_perf},\ref{subsec:reward_scheme}).
    \item A novel return and advantage estimation strategy for tree trajectories, which significantly outperforms previous approaches (Section \ref{sec:final_perf}), inherently encourages shorter plans and enables generalization beyond the unrolled depth (Section \ref{sec:abl_bootstrap},\ref{sec:appendix_bootstrapping},\ref{sec:appendix_prop_tree_ret}).
    \item A novel exploration strategy that helps collect relevant data for training the planning modules and outperforms using expert data (Section \ref{sec:abl_expl}).
    % \item Fast Policy-based planning inference with $O(\log n)$ compute steps for real-time applications.
\end{itemize}

We perform theoretical and empirical analyses to prove the soundness of the method.
The resulting agent significantly outperforms the previous benchmarks on the 25-room task, achieving significantly higher success rates and shorter average episode lengths.
%The remainder of the paper is organized as follows: Section \ref{sec:related_work} reviews related work, Section \ref{sec:methodology} provides a detailed description of our method, Section \ref{sec:agent_arch} details the agent architecture, Section \ref{sec:results_and_eval} describes the experimental setup and shows the empirical results, and Section \ref{sec:discussion} and Section \ref{sec:future_work} conclude with a discussion and future directions.
Our code is available on Github. The link is not provided to follow anonymity guidelines. %: \href{https://github.bath.ac.uk/ss3966/phd_project}{https://github.bath.ac.uk/ss3966/phd\_project}.


\section{Related work}
\label{sec:related_work}


%\subsection{Goal Conditioned Reinforcement Learning}


%Goal-Conditioned Reinforcement Learning (GCRL) extends traditional RL by training agents to achieve arbitrary goals specified in the state space.
%Some GCRL methods are implemented as goal-conditioned policies that take the initial and final states as input to output a sequence of actions to reach the goal state \cite{kaelbling1993learning,schaul2015universal,pong2018temporal}.
%While others enable efficient learning in sparse reward settings by relabeling goals like Hindsight Experience Replay (HER) \cite{andrychowicz2017hindsight}.
%Recent methods like Goal-Conditioned Supervised Learning (GCSL) \cite{li2024goal} simplify GCRL by framing it as a supervised learning problem.

%Despite their successes, GCRL methods struggle with long-horizon tasks due to several key challenges.
%First, the sparsity of rewards in long-horizon settings makes it difficult for agents to discover successful trajectories through random exploration \cite{nachum2018data}.
%Second, the curse of dimensionality in high-dimensional state and goal spaces exacerbates the exploration problem, as the agent must navigate a vast state space to reach distant goals \cite{pong2018temporal}.
%Third, credit assignment becomes increasingly challenging as the horizon lengthens \cite{arjona2019rudder}.
%and non-stationarity in the goal distribution where the agent must adapt to constantly shifting objectives \cite{kaelbling1993learning}.
% These limitations make it difficult for agents to explore effectively and learn policies for distant goals, highlighting the need for hierarchical approaches that decompose tasks into smaller subgoals.


\subsection{Planning Algorithms}

Planning methods aim to solve long-horizon tasks efficiently by exploring future states and selecting optimal actions \cite{lavalle2006planning,choset2005principles}.
%These methods can be broadly categorized into several strategies, each with its own strengths and limitations.
Monte Carlo Tree Search (MCTS) \cite{browne2012survey} expands a tree of possible future states by sampling actions and simulating outcomes.
While effective in discrete action spaces, MCTS struggles with scalability in high-dimensional or continuous environments.
Visual Foresight methods \cite{ebert2018visual,finn2017deep,hafner2019learning} learned visual dynamics models to simulate future states, enabling planning in pixel-based environments.
However, they require accurate world models and can be computationally expensive.
Some use explicit graph search over the replay buffer data \cite{eysenbach2019search}.
Model Predictive Control (MPC) \cite{nagabandi2018neural,nagabandi2020deep} is an online planner that samples future trajectories and optimizes actions over a finite horizon.
These methods rely on sampling the future state and thus do not scale well with the horizon length.

To address the challenges of long-horizon tasks, some planning algorithms decompose complex problems into manageable subtasks by predicting intermediate subgoals.
%These methods predict a subgoal approximately midway between the initial and goal states, dividing the problem into two simpler subtasks.
MAXQ \cite{dietterich2000hierarchical} decomposes the value function of the target Markov Decision Process (MDP) into an additive combination of smaller MDPs, enabling hierarchical planning.
Sub-Goal Trees \cite{jurgenson2020sub} learn a subgoal prediction policy optimized to minimize the total predicted distance measures of the decomposed subtasks.
Long-Horizon Visual Planning \cite{pertsch2020long} utilizes Goal-Conditioned Predictors (GCPs) to reduce the prediction space and employs a Cross-Entropy Method (CEM)-based planner optimized to minimize distance costs.
CO-PILOT \cite{ao2021co} is a collaborative framework where a planning policy and an RL agent learn through mutual feedback to optimize the total distance cost.
% Goal-Conditioned Reinforcement Learning with Imagined Subgoals \cite{chane2021goal} predicts midway subgoals by optimizing a distance measure and later uses a value function for reachability checks.

While these methods have demonstrated success, they rely heavily on distance-based metrics, which are challenging to learn and sensitive to policy quality \cite{eysenbach2019search,ao2021co}.
In contrast, our method uses discrete reachability-based rewards, which are easier to estimate accurately and provide clearer learning signals.


\subsection{Hierarchical Reinforcement Learning Agents}


Hierarchical reinforcement learning (HRL) refers to a set of techniques that temporarily abstract actions \cite{botvinick2009hierarchically,wiering2012reinforcement,barto2003recent,sutton1999between,pateria2021hierarchical}.
%HRL agents decompose long-horizon tasks into reusable subgoals or skills, enabling efficient exploration and transferable policies.
Foundational works like the options framework \cite{sutton1999between} and MAXQ decomposition \cite{dietterich2000hierarchical} introduced temporal abstraction, allowing agents to reason at multiple time scales.
HRL agents are usually split into two modules, a Manager and a Worker.
%The worker learns a task-independent policy that is goal-conditioned or options-based, and the manager learns a policy to control the worker in the task context.
%The manager works on a coarser timescale than the worker and outputs abstract actions or subgoals for the worker to complete.
%Working on a coarser scale allows the manager to address the problem of long-horizon credit assignment.
Various strategies have been proposed in the past for learning the two levels of abstract actions.
OPAL \cite{ajay2020opal} encodes the trajectory using a bidirectional GRU and is optimized as a Variational Autoencoder.
Causal InfoGAN \cite{kurutach2018learning} uses an InfoGAN to learn causal representations by maximizing mutual information between the skill and the initial and final state pairs.
DADS \cite{sharma2020dynamics} apply the mutual information objective to learn a diverse forward-inverse kinematics model that is later used for planning.
The Director \cite{hafner2022deep} manager predicts subgoals for the worker in a latent space, learned using a VAE.
We use an HRL architecture for our agent, as it enables our planning policy to construct plans as discrete combinations of abstract actions.
Note that the key contribution of our work is to show that hierarchical RL agents can be better trained using a discrete reachability-based reward. 


\section{Methodology}
\label{sec:methodology}

\begin{figure*}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/illustrations/tree_unroll.png}
        \caption{Unrolled subtask tree during Training}
    \label{fig:tree_unroll}
    \end{subfigure}
    \hspace{2em}
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/illustrations/tree_inference.png}
        \caption{Planning during Inference}
        \label{fig:tree_inference}
    \end{subfigure}
    \caption{The figure illustrates the plan unrolling process during: (a) \textbf{Training} when the entire subtask tree is unrolled, (b) \textbf{Inference} when only the first branch of the tree is unrolled to predict the first reachable subgoal.}
     % \label{fig:results_exprt_final_perf}
\end{figure*}


\subsection{Hierarchical Planning}
\label{sec:hier_plan}

Let $\pi_\theta$ be a goal-conditioned planning policy that takes the initial $s_t \in \mathbb{S}$ and goal $s_g \in \mathbb{S}$ states as input, and outputs a subgoal in the same space $s_{g1} \in \mathbb{S}$.
The planning policy allows the agent to break a long-horizon task $(s_t,s_g)$ into two smaller subtasks $(s_t,s_{g1})$ and $(s_{g1},s_{g})$.
The following sections describe our method that helps train the planning policy.
The overall process can be summarized as: generation of plans via recursive application of the planning policy, reward estimation for the plans, advantage estimation to measure the plan quality, and using policy gradients to update the planning policy using the estimated advantages.


\subsubsection{Plan Unrolling}
\label{sec:rollout}

% The planning policy $\pi_{G}$ allows the agent to predict a subgoal state $s_\text{sub}$ in the context of an initial and goal state $(s_\text{init},s_\text{goal})$.
% The subgoal state breaks the original task into two simpler tasks $(s_\text{init},s_\text{sub})$ and $(s_\text{sub},s_\text{goal})$ and a recursive application of the policy further breaks them resulting in a tree $\eta$ of subtasks.
% Given a goal-directed task as a pair of initial and final states $(s_t,s_g)$, a subgoal generation method predicts an intermediate subgoal $s_1$ that breaks the task into two simpler subtasks $(s_t,s_1)$ and $(s_1,s_g)$.
Since the subgoal is in the same state space, the policy can also be applied to the subtasks.
The recursive application of the subgoal operator further breaks the task, leading to a tree of subtasks $\tau$ where each node $n_i$ is a task (Fig. \ref{fig:tree_unroll}).
Let the preorder traversal of the subtask tree $\tau$ of depth $D$ be written as $[n_0,n_1,n_2,...,n_{2^{D+1}-2}]$.
The root node $n_0$ is the original task and the other nodes are the recursively generated subtasks.
The formulation means the child nodes $(n_{2i+1},n_{2i+2})$ represent the subtasks generated by the application of the planning policy at node $n_i$.
% Thus, the recursive application of the planning policy results in a tree of subtasks $\tau$ where each node $n_i$ contains the subtask $(s_{})$ at the node.
% If the tree was unrolled to a maximum depth $D$, then each node $n_i$ (where $i \in [0, 2^{D+1}-1)$) in the tree is a subtask specified as a pair of initial and goal states $n_i = (s_\text{init},s_\text{goal})$ (Fig. \ref{fig:tree_unroll}).
The tree leaf nodes indicate the sequence of smallest subtasks that can be executed sequentially to complete the original problem.
% We refer to the subtasks in a tree as nodes.

% To train the G-CSR module in an on-policy fashion, first, the subtask tree is unrolled.
% We train the planning policy $\pi_{G}$ in an on-policy fashion.
% First, a subtask tree is unrolled and then the subtasks that can be completed directly by the worker policy are identified.
% For this, the worker is parallelly run using imagination via RSSM for each node subtask, initialized at $s_\text{init}$ with $s_\text{goal}$ as the worker goal state.
% Then, the cosine-max similarity of the reached final state $s_\text{final}$ and the given goal state $s_\text{goal}$ is computed for each node.
% Nodes for which the similarity measure above the threshold $\Delta_R$ are marked as reachable directly by the worker policy.
% We found $\Delta_R = 0.7$ to work well for all cases.
% The next section details the reward scheme based on the reachability of nodes and advantage estimation of the subtask tree.


\subsubsection{Discrete Rewards Scheme}

Next, we want to identify the nodes/subtasks in the unrolled tree that are directly achievable by the worker.
Nodes that the worker can directly complete do not need to be expanded further.
We want our planning policy to act such that all branches of the tree end in reachable nodes and with minimal possible leaf nodes.
For this, we first test the reachability of the goal from the initial state for each node parallelly, by simulating the worker steps in imagination (Fig. \ref{fig:tree_reward}).
The worker is initialized at the initial state and given the subtask goal state as its goal.
The worker runs for a fixed number of steps $K$ before terminating.
Then the worker's final state is compared with the goal using the \texttt{cosine\_max} similarity metric \cite{hafner2022deep}.
If the similarity measure is above a threshold $\Delta_R$ then the subtask is marked as reachable.
We mark such nodes as terminal nodes and provide the agent with a reward $R_i=1$.
Let the initial and goal states at node $n_i$ be $(s_{ti},s_{gi})$, and $s_{fi}$ be the final state of the worker.
Then the terminal indicator $T_i$ array and the rewards array $R_i$ can be computed as: 

\vspace{-.4cm}

\begin{align}
    T_i =& T_{(i-1)/2} \lor \texttt{cosine\_max}(s_{fi},s_{gi}) > \Delta_R \\
% \end{align}
% \begin{align}
    R_i =& \begin{cases}
            1, & \text{if } T_i == True \\
            0, & \text{otherwise}
        \end{cases}
\end{align}

% \vspace{-.4cm}

\subsubsection{Advantage Estimation for Trees}
\label{sec:tree_rew_adv}


Ideally, we would like our planning policy to maximize the sum of all received rewards $\sum_{i=0}^{2^{D+1}-1} R_i$.
In our formulation, Sub-Goal trees \cite{jurgenson2020sub} minimize the sum of the distances predicted in the subtree under the node to optimize the policy at that node.
CO-PILOT \cite{ao2021co} minimizes the sum of the distances predicted at all nodes.
Long-Horizon Visual Planning \cite{pertsch2020long} uses a CEM-based optimizer to minimize the sum of predicted distances for the child nodes.
Since we work with reachability-based rewards, we cannot optimize the sum of rewards because the policy can converge to a degenerate solution.
This can happen if the policy predicts either the initial or goal state as the subgoal leading to two nodes, one degenerate and the other containing the original problem.
Such predictions reward policy with an immediate non-zero reward and it gets stuck in a local optima.
It can also encourage the policy to generate maximum possible leaf nodes or longer plans.
Ideally, the return should be high when all leaf subtasks end in reachable nodes and the plans are short.

% After constructing a plan for the given goal state, we evaluate the plan's advantage as a learning signal for the policy.
% Similar to reward schemes for goal-based tasks that reward the agent upon reaching the goal and terminate the episode, we provide the agent with rewards ($R_{n_i} = 1$) for the nodes marked as reachable and mark them as terminal (Fig. \ref{fig:tree_reward}).
Taking inspiration from the discounted return formulation for linear trajectories, $G_t = R_{t+1} + \gamma G_{t+1}$, we propose writing the return formulation for trees as, $G_i = \min(R_{2i+1} + \gamma G_{2i+1}, R_{2i+2} + \gamma G_{2i+2})$.
All branches should end in directly reachable subtasks to score a high return with this formulation.
Also, since the discount factor diminishes the return with each additional depth, the agent can score higher when the constructed tree has less depth similar to linear trajectories where the agent gets a higher return for shorter paths to the goal \cite{tamar2016value}.
% This formulation yields a high return only when both subtasks are finally solved, and the discount factor diminishes the reward with node depth, encouraging shorter plans.
Thus, given a tree trajectory $\tau$, we write the Monte-Carlo (MC) return, the $1$-step return, and the lambda return for each non-terminal and non-leaf node as:

\vspace{-.5cm}
\begin{gather}
    G_i = \min(R_{2i+1} + \gamma G_{2i+1}, R_{2i+2} + \gamma G_{2i+2}) \\
    G_i^0 = \min(R_{2i+1} + \gamma v_\phi(n_{2i+1}), R_{2i+2} + \gamma v_\phi(s_{2i+2})) \\
    G_i^\lambda = \min(R_{2i+1} + \gamma((1-\lambda) v_\phi(n_{2i+1}) + \lambda G_{2i+1}^\lambda), R_{2i+2} + \gamma((1-\lambda) v_\phi(s_{2i+2}) + \lambda G_{2i+2}^\lambda))
\end{gather}
\vspace{-.25cm}

% The original equation for estimating discounted returns is extended for trees where the return for each node $n_i$ is the minimum of returns calculated using the left and right child nodes.
% We propose that similar to the linear trajectories where the discounted return is calculated as $V_t=r_{t+1} + \gamma V_{t+1}$, the discounted return for a tree node $n_i$ can be written as the minimum of the return from both its child nodes left $n_l$ and right $n_r$, $V_i= \min(r_l + \gamma V_l, r_r + \gamma V_r)$ (Fig. \ref{fig:tree_ret_estimation}).
% When using a list to store the tree, $l=2i+1, r=2i+2$.
% Unlike linear trajectories, we begin the reward index at $i=1$.
% Essentially, the return for node $n_i$ is the minimum of the return from both its child nodes left $n_{2i+1}$ and right $n_{2i+2}$.


% Fig. \ref{fig:tree_ret_estimation} illustrates example return evaluations for some trees.
Illustrations of example return evaluations for some sample trees are shown in Fig. \ref{fig:tree_ret_estimation}.
We show that the Bellman operators for the above returns are contractions and repeated applications cause the value function $v^\pi_\phi$ to approach a stationary $v^*$ (see Sec. \ref{sec:appendix_lambda_returns} for the proof).

\begin{theorem}[Contraction property of the return operators]
    \label{theorem:return_contraction}
    The Bellman operators $\mathcal{T}$ corresponding to the returns are a $\gamma$-contraction mapping wrt. to $\|\cdot\|_\infty$.

    \[\|\mathcal{T} V_1 - \mathcal{T} V_2\|_\infty \leq \gamma \| V_1 - V_2 \|_\infty\]
\end{theorem}

These returns can be used to learn a value function $v_\phi(n_i) \rightarrow \mathbb{R}$ using loss:

\vspace{-.5cm}

\begin{gather}
    \label{eq:gcsr_value_loss}
    \mathcal{L}(v_\phi) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum^{2^D-2}_{i=0}(v_\phi(n_i)-G^\lambda_{i})^2]
\end{gather}

Moreover, using the value function can allow bootstrapping (Sec. \ref{sec:appendix_bootstrapping}) allowing generalization beyond the maximum unrolled depth $D$.
An intuition for bootstrapping can be that a high-value function means the policy knows how to solve the task, if there is a non-terminal leaf node with a high value, it means the policy can solve it thereafter.
We explore how the return penalizes maximum tree depth and encourages balanced trees in Sec. \ref{sec:appendix_prop_tree_ret}.
Having balanced trees implies that the policy breaks the tasks approximately midway to yield roughly equally difficult subtasks.
However, the score is the same for trees with the same maximum depth.
The equation automatically reduces to advantage estimation for linear trajectories when considering the special case where each node has a single child node.


% Thus, 1-step returns are calculated as $\min(r_l + \gamma v(n_l), r_r + \gamma v(n_r))$ and the n-step returns can be estimated as $\min(r_l + \gamma V^N_l, r_r + \gamma V^N_r)$, and lambda returns can be computed as Eq. \ref{eq:tree_adv}.

% Using a Critic allows for n-step bootstrapping, enabling the method to learn plans of depths greater than the maximum unrolled depth.
% Even if the max-depth nodes do not result in reachable state pairs, if the Critic outputs high values for them, they induce a positive learning signal (Fig. \ref{fig:tree_disc_ret_crit}).


% \begin{gather}
%     % \pi_{c}(s_t) = p_{\pi_c}(c|s_t) \\
%     \label{eq:gcsr_policy}
%     \pi_{G}(n_i) = p_{\pi_G}(c,z_0,z_1,...,z_{N-1}|n_i) \\
%     \label{eq:tree_adv}
%     V^\lambda_i =\min(r_l + \gamma ((1-\lambda)v_G(n_l) + \lambda V^\lambda_l), r_r + \gamma ((1 - \lambda)v_G(n_r) + \lambda V^\lambda_r)) \\
%     \label{eq:gcsr_choice_loss}
%     \mathcal{L}(\pi_{G_c}, \eta) = -\sum^{2^D-2}_{i=0} \ln \pi_G(c|n_i) \text{sg}(V^\lambda_i - v_G(n_i)) + \eta \text{H}[\pi_G(c|n_i)] \\
%     \label{eq:gcsr_skill_loss}
%     \mathcal{L}(\pi_{G_{z_n}}, \eta) = -\sum^{2^D-2}_{i=0} c_n \ln \pi_{G}(z_n|n_i) \text{sg}(V^\lambda_i - v_G(n_i)) + \eta \text{H}[\pi_{G}(z_n|n_i)] \\
%     \label{eq:gcsr_policy_loss}
%     \mathcal{L}(\pi_G) = \mathbb{E}_{\eta \sim \pi_G} [ \mathcal{L}(\pi_{G_c}, \eta) + \sum_{i=0}^{N-1}\mathcal{L}(\pi_{G_{z_n}}, \eta) ] \\
%     \label{eq:gcsr_value_loss}
%     \mathcal{L}(v_G) = \mathbb{E}_{\eta \sim \pi_G}[\sum^{2^D-2}_{i=0} (v_G(n_i)-\text{sg}(V^\lambda_{i}))^2]
% \end{gather}

\subsubsection{Policy Gradients}

Given a tree trajectory $\tau$, sampled using a planning policy $\pi_\theta$ parameterized by $\theta$, and $A^i(\tau)$ being the advantage estimated for node $n_i$ of the trajectory $\tau$.
We derive the policy gradients for a tree trajectory (see Sec. \ref{sec:policy_grad_trees} for proof).
% In Sec. \ref{sec:policy_grad_trees}, we derive the policy gradients for tree trajectories as:

\begin{theorem}[Policy Gradients]
    \label{theorem:policy_grad_derivation}
    Given a tree trajectory $\tau$ specified as a list of nodes $n_i$, generated using a policy $\pi_\theta$. The policy gradients can be written as:

    \[ \nabla_\theta J(\theta) = \mathbb{E}_\tau \sum_{i=0}^{2^D-2} A^i(\tau) \nabla_\theta \log \pi_\theta(a_i|n_i) \]
\end{theorem}

We also show that if the advantage estimate is independent of the policy $\pi_\theta$, the expectation reduces to $0$, implying that we can use the value function as a baseline (see Sec. \ref{sec:policy_grad_trees} for proof).

\begin{theorem}[Baselines]
    \label{theorem:baseline_proof}
    If $A(\tau)$ is independent of $\tau$, say $b(n_i)$, then its net contribution to the policy gradient is $0$.

    \[\mathbb{E}_\tau \sum_{i=0}^{2^D-2} b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i) = 0\]
\end{theorem}

Using the policy gradients and an entropy term to encourage random exploration, we construct the loss function for the policy $\pi_\theta$ as (sum over all non-leaf and non-terminal nodes):

\vspace{-.3cm}

\begin{gather}
    \label{eq:gcsr_policy_loss}
    \mathcal{L}(\pi_\theta) = -\mathbb{E}_\tau \sum_{i=0}^{2^D-2} [ (G_i - v_\phi(n_i)) \log \pi_\theta(a_i|n_i) + \eta \text{H}[\pi_z(z|s_t)] ]
\end{gather}


\section{Agent Architecture}
\label{sec:agent_arch}

% Using the base architecture of the Director that includes Perception(RSSM) and Worker.
% However, the VAE and Manager are modified.
% The following sections describe our approach in detail.
Our agent uses an HRL architecture to plan using discrete combinations of abstract actions.
Our architecture consists of three modules broadly: perception, worker, and manager.
Perception is implemented using the Recurrent State Space Module (RSSM) \cite{hafner2019learning} that learns state representations using a sequence of observations.
RSSM enables imagination which allows on-policy agent training by generating rollouts efficiently.
Both, the worker (Fig. \ref{fig:worker_sac}) and the manager (Fig. \ref{fig:gcsr_sac}) are implemented as Goal-Conditioned SAC agents optimized for different rewards.
The worker is optimized to increase the \texttt{cosine\_max} similarity measure between the current and a prescribed subgoal state.
The manager is the planning policy described in the previous section that refreshes the worker goal every $K$ steps.
The manager predicts in the latent space using a Conditional State Recall (CSR) module which helps in search space reduction.
% However, the planning policy does not directly predict in the state space but in a learned latent space.
% Previous works like \cite{pertsch2020long,hafner2022deep} have shown the effectiveness of goal prediction in a learned latent space.
%We use a similar reduction in search space with Conditional State Recall (CSR) modules.


\subsection{Conditional State Recall}

To help reduce the search space for subgoal prediction, we train a Conditional Variational AutoEncoder (CVAE) that learns to predict midway states given an initial and a final state from replay data.
Thus, given initial and final states $(s_t,s_{t+q})$ that are $q$ steps apart, we want to be able to learn a distribution over the midway states $s_{t+q/2}$.
The CVAE module consists of an Encoder and a Decoder (Fig. \ref{fig:gcsr_cvae} in appendix).
The encoder takes the initial, midway, and final states as input to output a distribution over a latent variable $\text{Enc}_G(z|s_t,s_{t+q/2},s_{t+q})$.
The decoder uses the initial, and final states, and a sample from the latent distribution $z \sim \text{Enc}_G(z|s_t,s_{t+q/2},s_{t+q})$ to predict the midway state $\text{Dec}_G(s_t,s_{t+q},z) \rightarrow \hat{s}_{t+q/2}$.
The module is optimized using the replay buffer data to minimize the ELBO objective.
We extract the triplets at multiple temporal resolutions $q \in \{2K,4K,8K,...\}$ (subjected to task horizon) allowing the policy to function at all temporal resolutions (Fig. \ref{fig:data_gcsr}).

Similarly, we train another CVAE that helps predict nearby goal states given an initial state.
Given an initial and final state separated by $K$ steps $(s_t,s_{t+K})$ (Fig. \ref{fig:data_icsr}).
We learn a CVAE where the encoder encodes the state pair to a latent space $\text{Enc}_I(z|s_t,s_{t+K})$ and the decoder predicts the final state in the context of the initial state $\text{Dec}_I(s_t,z) \rightarrow \hat{s}_{t+K}$ (Fig. \ref{fig:icsr_cvae}).
This CVAE helps the explorer predict nearby goals for the worker during the exploration phase and helps efficiently check reachability during inference.
We call these modules Goal Conditioned State recall (GCSR) and Initial Conditioned State recall (ICSR).
Intuitively, the GCSR and ICSR modules learn path segments and state transitions at the manager's temporal resolution, respectively.
See Sec. \ref{sec:appendix_training} for more training details.
%The specific state representations used are presented in Sec. \ref{sec:appendix_static_repr}.


%\subsection{Static State Representations}

% some context required about why static space representation is required. - VPN

%Since the RSSM integrates a state representation using a sequence of observations, it does not work well for single observations.

%To generate goal state representations using single observations we train an MLP separately that tries to approximate the RSSM outputs ($s_t$) from the single observations ($o_t$) as $s'_t$.
%We call these representations static state representations.
%Moreover, since GCSR modules require state representations at large temporal distances, it can be practically infeasible to generate them using RSSM.
%Thus, we use static state representations to generate training data for the GCSR module as well.
%See Sec. \ref{sec:appendix_static_repr} for more details.


\subsection{Plan Inference}
\label{sec:inference}


Unlike non-policy-based approaches like CEM \cite{nagabandi2018neural,pertsch2020long}, which construct multiple plans and evaluate the best at runtime, a policy-based agent outputs the optimal best plan.
Therefore, the agent does not need to predict the entire tree but only the left child node for each node, corresponding to evaluating the immediate steps only (Fig. \ref{fig:tree_inference}).
The evaluation requires ($D=\log N$) policy steps for planning horizons of $N$ steps.
Though all tree nodes can be constructed and evaluated in parallel in $O(\log N)$ time, they still require computing $2^{D+1}-1$ nodes to construct plans with length $2^D$ nodes (the leaf nodes).
% However, our agent requires only $\log N$ compute steps for plans with lengths $2^D$.
The agent's resource and time efficiency allow for cheap plan regenerations.

Since the model is not trained for nodes beyond the terminal nodes, we need to evaluate the reachability of the predictions in real time.
% If this is done using the worker as during training, it requires $O(K)$ time.
% To do this in real time, we train an I-CSR module with $p = K$ that predicts states $K$-steps away.
For this, we use the ICSR module to reconstruct the subgoal states in the context of the initial state.
The node is directly achievable if the \texttt{cosine\_max} similarity between the goal states and the reconstructions is above the threshold $\Delta_R$.
The goal from the lowest depth reachable node is the worker's goal (Fig. \ref{fig:inference_reach}).
Since the agent generalizes beyond the maximum training depth, the plan inference can be unrolled for depths $D_\text{Inf}$ greater than the maximum training depth.
We use $D = 5$ and $D_\text{Inf} = 8$ for our experiments to validate this.

% Another issue we noticed was that the static representations slightly deviated from the RSSM predictions over time due to incomplete information.
% Since we train the I-CSR module using RSSM outputs, using static goal predictions with I-CSR fails to work sometimes near optimality.
% I-CSR would mark some reachable goals as non-reachable due to differences in representations.
% To handle this, the static goal representation is reconstructed via the U-CSR module before use.
% Due to the categorical latent space, the reconstruction snaps the static goal representation to a known nearby RSSM representation.


\subsection{Memory Augmented Exploration}
\label{sec:mem_aug_expl}

When using self-generated exploratory data, a separate exploratory manager SAC ($\pi_E(s_t),v_E(s_t)$) is used to act for $N_{\text{E}}$ steps, and then switched to the task planning manager.
The exploration policy generates goals for the worker using the initial conditional state recall (ICSR) (Fig. \ref{fig:expl_sac}).
The explorer is trained to maximize intrinsic exploratory rewards.
Vanilla exploratory reward schemes positively reward the agent for visiting states with non-optimal representations.
The reward encourages the agent to seek novel states in the environment.
However, these resulted in trajectories that were non-optimal for the current task.
We observed that as per the exploration objective the agent sought out the unclear state representations. %, and sometimes did so while traveling across the arena.
However, once at the state, it stayed near the state. % causing most of the trajectory containing the agent randomly moving about a point.
Data generated from this behavior led to suboptimal planning policies with lower performance.
We therefore instead propose an alternate exploratory reward scheme that rewards the agent positively for maneuvering novel path segments $(s_t,s_{t+q/2},s_{t+q})$ and state transitions $(s_t,s_{t+K})$.
The novelty is measured as the reconstruction errors using the conditional state recall modules. % ($s_{t+q/2}$ in the context of $(s_t,s_{t+q})$) and ICSR ($s_{t+K}$ in the context of $s_t$).
%These rewards are given to the agent at step $s_{t+q}$ and $s_{t+K}$ respectively, ie. at the last state in the triplet or the pair.
Thus, we roll out an imagined trajectory, measure the reconstruction errors using the conditional state recall modules as mean-squared errors (MSE), and use them as rewards for the agent at appropriate time steps (Fig. \ref{fig:expl_rew}).
The exploratory rewards $R_t$ at step $t$ can be written as:

\vspace{-.5cm}

\begin{gather}
    \label{eq:icsr_expl_rew}
    R^I_t = \left\Vert s_{t} - \text{Dec}_I(s_{t-K},z)\right\Vert^2 \quad \text{where} \quad z \sim \text{Enc}_I(z|s_{t-K}, s_{t}) \\
    \label{eq:gcsr_expl_rew}
    R^{G(q)}_t = \left\Vert s_{t-q/2} - \text{Dec}_G(s_{t-q},s_{t},z) \right\Vert^2 \quad \text{where} \quad z \sim \text{Enc}_G(z|s_{t-q},s_{t-q/2},s_{t}) \text{ and } q \in Q \\
    \label{eq:dhp_expl_rew}
    R^E_t = R^I_t + \sum_{q \in Q} R^{G(q)}_t
\end{gather}

% We experiment with multiple exploration strategies that were directly or indirectly targeted towards seeking out states for which the representation is not optimal.
% The strategies included Ensemble disagreement for state representation and reconstruction errors using U-CSR module (Director).
% However, these resulted in trajectories that were ineffective for the current task.
% We observed that as per the exploration objective the agent sought out the unclear state representations, and sometimes did so while traveling across the arena.
% However, once at the state, it stayed near the state causing most of the trajectory containing the agent randomly moving about a point.
% Data generated from this behavior led to suboptimal planning policies with low performance.

% Taking inspiration from the VAE-based exploration objective from the Director, we propose a novel exploration objective.
% The Director's exploration objective can be seen as a way to encourage states that it cannot recall reliably.
% The idea can then be translated for our model to seek out state transitions $(s_t,s_{t+p})$ and path segments $(s_t,s_{t+q/2},s_{t+q})$ not well modeled by the I-CSR and G-CSR modules.
% The exploration objective for the I-CSR module can be the reconstruction error $\mathcal{R}(s_t,s_{t+p})$ of the states in the trajectory in the context of the $p$ steps earlier state using the I-CSR module that models state transitions of length $p$.
% When using multi-resolution CSR and adaptive losses, each extracted state pair is passed through each VAE to compute the reconstruction errors $\mathcal{R}_{Ii}$ and the minimum error serves as the reward for the agent $\mathcal{R}_I = \min_{i=0}^{N-1}(\mathcal{R}_{Ii}$) given at step $t+p$.
% Similarly, the exploration objective for the G-CSR module can be the reconstruction error of the subgoal $s_{t+q/2}$ in the context of $(s_t,s_{t+q})$, given at step $t+q$.

Since the exploratory rewards for the current step depend on the past states.
The explorer needs to know the states $[s_t,s_{t-K},s_{t-2K},...]$ to guide the agent accurately along rewarding trajectories.
To address this, we provide a memory of the past states as an additional input to the exploratory manager SAC ($\pi_E(s_t,\text{mem}_t), v_E(s_t,\text{mem}_t)$) (Fig. \ref{fig:expl_sac}).
To do this, a memory buffer stores every $K$-th state the agent encounters, and then a memory input is constructed consisting of states $\text{mem}_t = \{s_{t-K},s_{t-2K},s_{t-4K},...\}$ (Fig. \ref{fig:expl_mem}).
The imagination horizon and maximum temporal resolution of the CSR modules constrain the memory length.
We call this memory-augmented exploration that uses memory to achieve long-term dependencies in the exploration trajectories.
See Sec. \ref{sec:abl_expl} for performance comparisons of the different exploration strategies, Sec. \ref{sec:appendix_explorer_training} for more training details, and Sec. \ref{sec:appendix_expl_sample_traj} for examples of generated trajectories.

% these are specific observations about results - better to have them in results or better in appendix.

%The approach leads to interesting results.
% We test two setups, first using only I-CSR reconstruction losses, and then with both I-CSR and G-CSR losses.
%Since the policy learns to value state transitions and path segments rather than states, it tends to repeat higher-valued transitions.
%Thus, the agent almost always moves around rather than staying near a point.
%The agent exhibits different behaviors through the learning phase, like scanning the areas, moving around in large and small loops, and hitting edges and corners at different angles to understand environmental constraints (Fig. \ref{fig:expl_sk_traj}).
%The second variant exhibits behavior that is much more suited to the task.
%The agent moves around covering much larger portions of the arena exploring state connectivity, the size of the loops is much larger, and the agent tends to repeat its paths less often (Fig. \ref{fig:expl_pl_traj}).
% The planning policy learned using data generated by the second variant significantly outperforms the expert data (Sec. \ref{sec:abl_expl}).


\section{Results and Evaluation}
\label{sec:results_and_eval}


\subsection{Task and Training Details}
\label{sec:task_details}

% Given the constraint for the goal image to be in the image space, we are limited to the $25$-rooms environment.
We test our agent in the 25-room environment where it has to navigate a maze of connected rooms to reach the goal state.
Benchmarks from the previous methods show the average episode length as $> 150$ steps, indicating a long-horizon task.  
The agent is provided the initial and goal states as $64 \times 64$ images.
Each episode lasts $400$ steps before terminating with a $0$ reward.
If the agent reaches the goal within $400$ steps, the episode terminates and is given a reward $0 < R \leq 1$ depending on the episode length.
% See Sec. \ref{sec:appendix_training} for agent architecture details.


\subsection{Results}

Figure \ref{fig:samples} shows the sample solutions generated by our agent during training and inference.
During training, the agent unrolls the entire subtask tree, the leaf nodes of which describe the subtasks needed to be executed sequentially to complete the task.
The extracted leaf nodes for some sample tasks are shown in Fig. \ref{fig:train_plan_sample}.
During inference, for an optimal policy, the entire tree does not have to be unrolled, instead we unroll only the first tree branch.
Thus each subgoal only sub-divides the task of reaching the higher level subgoal from the current state not concerned with the path thereafter.
Fig. \ref{fig:inference_sample} shows the subgoals generated by the agent clipped to the first identified directly reachable subgoal.
Fig. \ref{fig:traj_sample} shows the full episode trajectory of an agent where the manager predicts hierarchically planned subgoals for the worker and the worker executes the atomic actions.
The resulting agent can plan for long temporal horizons and is also highly interpretable and predictable with insight into long and short-term goals.

Using the XLA optimizations (see Sec. \ref{sec:appendix_training}) the compiled policy function runs on average in $4.6$ms on an NVIDIA RTX 4090 enabling real-time re-planning at $217.39$ times per second (assuming no additional overhead).

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/samples/train_plan_sample.png}
%     % \caption{.}
%     \label{fig:train_plan_sample}
% \end{figure}

\begin{figure}
     \centering
     \begin{subfigure}{\columnwidth}
         \centering
         \includegraphics[width=.85\textwidth]{figures/samples/train_plan_sample.png}
         \caption{ }
         \label{fig:train_plan_sample}
     \end{subfigure}
     \begin{subfigure}{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/inference_sample.png}
         \caption{ }
         \label{fig:inference_sample}
     \end{subfigure}
     \hspace{2em}
     \begin{subfigure}{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/run_sample.png}
         \caption{ }
         \label{fig:traj_sample}
     \end{subfigure}
     \caption{Samples generated by our agent. (a) Shows the leaf nodes of the plan trees generated by the agent during training. The first and last images in each row are sample initial-goal states and the images in between show the generated subgoals. The blank images occur when nodes above maximum depth are marked reachable. (b) Sample inference stacks. The first and last images indicate the initial and goal states. For others, each image is a subgoal that attempts to break the task to reach the state on its right. (c) Example trajectory of our agent in the task mode.}
     \label{fig:samples}
\end{figure}

% \vspace{-.25cm}

\subsubsection{Performance}
\label{sec:final_perf}

We compare the performance of our agent in terms of the average success rate in reaching the goal state and the average path length.
The performances for comparison of the agents are taken from \cite{pertsch2020long} which include Goal-Conditioned Behavioral Cloning (GC BC, \cite{nair2017combining}) that learns goal-reaching behavior from example goal-reaching behavior.
Visual foresight (VF, \cite{ebert2018visual}) that optimizes rollouts from a forward prediction model via the Cross-Entropic method (CEM, \cite{rubinstein2004cross,nagabandi2020deep}).
And hierarchical planning using Goal-Conditioned Predictors (GCP,\cite{pertsch2020long}) optimized using CEM to minimize the predicted distance cost.
Table \ref{tab:final_perf_comp} shows that our model significantly outperforms the previous approaches at success rate and average path lengths.
Note that the GCP baseline is the previous SOTA on the task, using a distance-based planning approach.
% Figure \ref{fig:results_exprt_rewards} shows the average environmental rewards per episode obtained during the training as episodes terminate.
% Similarly, Figure \ref{fig:results_exprt_path_lengths} shows the average path lengths of the episodes.

\begin{table}
    \centering
    \begin{tabular}{c|c|c}
        \hline
        Agent & Success rate & Average path length \\
        \hline
        GC BC & 7\% & 402.48 \\
        VF & 26\% & 362.82 \\
        GCP & 82\% & 158.06 \\
        DHP (\textit{Ours}) & 99\% & 71.37 \\
        \hline
    \end{tabular}
    \caption{Average Performance of different approaches on the 25-room navigation task.}
    \label{tab:final_perf_comp}
\end{table}

% \begin{figure}
%      \centering
%      \begin{subfigure}{0.42\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/plots/dhp_rew.png}
%          \caption{Environment rewards obtained as episodes terminate.}
%          \label{fig:results_rewards}
%      \end{subfigure}
%      \hspace{2em}
%      \begin{subfigure}{0.45\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/plots/dhp_ep_len.png}
%          \caption{Average path lengths.}
%          \label{fig:results_path_lengths}
%      \end{subfigure}
%      \caption{Results obtained by plotting the mean with standard deviations over three runs. The raw mean curves were extremely noisy, thus exponential smoothing with a factor of $0.99$ is applied.}
%      \label{fig:results_final_perf}
% \end{figure}


% \subsection{Ablation Studies}

% \subsubsection{Expert Data}

% We train our agent using the expert data provided with the environment \cite{pertsch2020long}.
% The data is collected by randomly sampling start and goal positions in the 2D maze and then constructing plans to reach them using the Probablistic Roadmap planner \cite{kavraki1998analysis}. 
% The PRM combined with some filtering and random waypoint sampling leads to trajectories with substantial suboptimality \cite{pertsch2020long}.
% Fig. \ref{fig:results_exprt_final_perf}(a) compares the rewards gained per episode and Fig. \ref{fig:results_exprt_final_perf}(b) compares the average path lengths of our agent trained using expert data vs. our agent trained on self-exploratory data.
% Since the expert data agent does not require the explore mode, it always runs in task mode.
% It can be clearly seen that the self-explored data performs better and surpasses the previous benchmarks.

% \begin{figure}[h!]
%      \centering
%      \begin{subfigure}{0.35\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/plots/exprt_rew.png}
%          \caption{Environment rewards obtained as episodes terminate.}
%          \label{fig:results_exprt_rewards}
%      \end{subfigure}
%      \hspace{2em}
%      \begin{subfigure}{0.35\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/plots/exprt_ep_len.png}
%          \caption{Average path lengths.}
%          \label{fig:results_exprt_path_lengths}
%      \end{subfigure}
%      \caption{Results obtained by plotting the mean with standard deviations over three runs. The raw mean curves were extremely noisy, thus exponential smoothing with a factor of $0.99$ is applied.}
%      \label{fig:results_exprt_final_perf}
% \end{figure}

\subsubsection{Exploration Strategies}
\label{sec:abl_expl}

% We also compare the proposed exploration strategy with the vanilla strategy that aims to visit less visited states more often.
We compare planning policies learned using data from various strategies like the vanilla exploratory reward and expert data.
The vanilla exploratory rewards are formulated as the mean-squared reconstruction error in predicting the state representation using a VAE that is being trained to predict state representations unconditionally (similar to Director \cite{hafner2022deep}).
The reconstruction error encourages the agent to visit states with unclear state representations.
In contrast, our method encourages the agent to traverse unclear state-transitions $(s_t,s_{t+K})$ and path segments $(s_t,s_{t+q/2},s_{t+q})$.
We also compare an agent trained using expert data provided with the environment \cite{pertsch2020long} collected by constructing trajectories using the Probablistic Roadmap planner \cite{kavraki1998analysis}.
The PRM combined with some filtering and random waypoint sampling leads to trajectories with substantial suboptimality \cite{pertsch2020long}.
Fig. \ref{fig:results_vanilla_expl_final_perf} compares the performance of agents trained using different exploratory strategies and expert data.
It can be seen that the proposed strategy with memory results in an agent with highest episodic rewards and lowest average path lengths.
The agent using the vanilla rewards and the expert data agent perform the worst, our reward scheme without the memory performs better, and the default proposed scheme performs the best.
Sec. \ref{sec:appendix_expl_sample_traj} shows example trajectories of an explorer optimized using various exploratory reward schemes.

\begin{figure}[h!]
     \centering
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/dataComp_rew.png}
         \caption{Environment rewards obtained as episodes terminate.}
         \label{fig:results_vanilla_expl_rewards}
     \end{subfigure}
     \hspace{2em}
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/dataComp_ep_len.png}
         \caption{Average path lengths.}
         \label{fig:results_vanilla_expl_path_lengths}
     \end{subfigure}
     \caption{Comparisons of the different exploration strategies and the expert data. The raw mean curves were extremely noisy, thus exponential smoothing with a factor of $0.99$ is applied.}
     \label{fig:results_vanilla_expl_final_perf}
\end{figure}

\subsubsection{Generalization Beyond Max Training Depth}
\label{sec:abl_bootstrap}

We test the method for generalization beyond the maximum depth of the subtask tree during training.
We initialize an agent with the maximum training tree depth $D=3$, which translates to $2^D=8$ abstract actions or $2^D*K=64$ atomic actions, less than the average episode length under the optimized policy.
Fig. \ref{fig:results_dhpD3_final_perf} compares the default agent with the shallow agent which performs as well as the default method.


\begin{figure}[h!]
     \centering
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/dhp_D3_rew.png}
         \caption{Environment rewards obtained as episodes terminate.}
         \label{fig:results_dhpD3_rewards}
     \end{subfigure}
     \hspace{2em}
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/dhp_D3_ep_len.png}
         \caption{Average path lengths.}
         \label{fig:results_dhpD3_path_lengths}
     \end{subfigure}
     \caption{Comparing performance with a shallow model with a lower maximum unroll depth ($D=3$) during training.}
     \label{fig:results_dhpD3_final_perf}
\end{figure}


\subsubsection{Alternate Reward Schemes}
\label{subsec:reward_scheme}
We experiment and compare the performance of agents trained on various other reward schemes and advantage estimation methods.
Fig. \ref{fig:results_rewComp_final_perf} shows the performance of the different methods.

\textbf{DHP (Neg Rew)}:
A reward scheme is common in goal-conditioned tasks where the agent receives a negative reward as an existence penalty at each step.
We test a similar reward scheme for our case where the agent receives $-1$ reward at each non-terminated node and a $0$ reward for the terminal nodes.
The results show that the agent works as well with the negative rewards scene as the default rewards scheme.
The final trained policy using the negative rewards scheme has an even lower average path length $\approx 60$.

\textbf{DHP (Dist Sum)}:
We attempt to test a distance-based reward scheme in our setting.
First, the distance predicted between the initial and goal state are computed for each node.
The return is calculated as the sum of lambda returns from the child nodes and the policy is optimized to minimize the return.
It can be seen that the method does not perform well.

\textbf{DHP (GAE)}:
Although we don't validate our method for the Generalize Advantage Estimation (GAE), we test a similar formulation where GAE advantages are estimated as minimum of the advantages from the child nodes, $\min(A^\text{GAE}_{2i+1}, A^\text{GAE}_{2i+2})$. The resulting agent performs mediocrely.

% \textbf{DHP (Dist Reach)}
% The same reachability-based reward scheme, but reachability is computed using a distance measure.

% \textbf{SGT}:
% TBU: Distance based reward from Subgoal Trees paper


\begin{figure}[h!]
     \centering
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/rewComp_rew.png}
         \caption{Environment rewards obtained as episodes terminate.}
         \label{fig:results_rewComp_rewards}
     \end{subfigure}
     \hspace{2em}
     \begin{subfigure}{0.35\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/plots/rewComp_ep_len.png}
         \caption{Average path lengths.}
         \label{fig:results_rewComp_path_lengths}
     \end{subfigure}
     \caption{Comparison of different reward and return estimation schemes.}
     \label{fig:results_rewComp_final_perf}
\end{figure}


%\section{Discussion}
%\label{sec:discussion}

%The proposed architecture demonstrates significant improvements over previous baselines.
%Notably, the agent does not require expert data and can efficiently self-generate high-quality training data.
%This approach eliminates the need for access to the internal environmental state during trial execution, making it more versatile and applicable to a wider range of environments.
%Additionally, the method supports cheap inference, that can enable efficient re-planning in dynamic environments where frequent updates are necessary.

%However, it is important to acknowledge the limitations of the method.
%While reachability is easier to predict compared to distance-based metrics, it relies heavily on accurate and unique state representations.
%In cases where state representations are not sufficiently distinct, the decoder may reconstruct image observations accurately, but the cosine similarity between two different state representations might still exceed the defined threshold $\Delta_R = 0.7$.
%This issue was particularly pronounced when using smaller state representation sizes.
%Furthermore, since the agent relies on imagination for training, the quality of the learned world dynamics becomes critical.
%Inaccurate dynamics can reinforce undesirable behaviors, highlighting the importance of robust state representation learning.
%Another architectural limitation of our method is that it estimates goal states using state observations which is uncommon.
%Most environments provide goal states as structured information like coordinates, and sometimes as raw text.
%However, this work focuses on planning rather than constructing robust state representations.

\section{Conclusion}
\label{sec:conc}
The proposed approach shows that learning a discrete reachability-based planning method can be used to learn highly performant hierarchical reinforcement learning agents. We obtain real-time planning with shorter trajectories and higher completion rates compared to previous distance-based approaches. While the method works well, it also provides opportunities for future work to address the limitations. We observe that learning good state representations is crucial and one could explore methods for generating state representations from textual prompts. One could also explore automatic generation of goals in the future.


\bibliographystyle{plain}
\bibliography{paper}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\appendix

\section{Appendix / supplemental material}
\label{sec:appendix}
% Optionally include supplemental material (complete proofs, additional experiments and plots) in appendix.
% All such materials \textbf{SHOULD be included in the main submission.}


% Manual appendix TOC
\subsection*{Appendix Contents} % Title for the appendix TOC
\addcontentsline{toc}{subsection}{Appendix Contents} % Add to main TOC
\begin{itemize}
    \item \hyperref[sec:policy_grad_trees]{Policy Gradients for Trees} \dotfill \pageref{sec:policy_grad_trees}
    \item \hyperref[sec:policy_eval_tree]{Policy Evaluation for Trees} \dotfill \pageref{sec:policy_eval_tree}
    \item \hyperref[sec:appendix_illustrations]{Illustrations} \dotfill \pageref{sec:appendix_illustrations}
    \item \hyperref[sec:appendix_training]{Architecture \& Training Details} \dotfill \pageref{sec:appendix_training}
    \item \hyperref[sec:appendix_impl_details]{Implementation Details} \dotfill \pageref{sec:appendix_impl_details}
    \item \hyperref[sec:appendix_expl_sample_traj]{Sample Exploration Trajectories} \dotfill \pageref{sec:appendix_expl_sample_traj}
    \item \hyperref[sec:discussion_n_futurework]{Discussion \& Future Work} \dotfill \pageref{sec:discussion_n_futurework}
\end{itemize}


\subsection{Policy Gradients for Trees}
\label{sec:policy_grad_trees}

Given a goal-directed task as a pair of initial and final states $(s_t,s_g)$, a subgoal generation method predicts an intermediate subgoal $s_1$ that breaks the task into two simpler subtasks $(s_t,s_1)$ and $(s_1,s_g)$.
The recursive application of the subgoal operator further breaks the task, leading to a tree of subtasks $\tau$ where each node $n_i$ is a task.
Let the preorder traversal of the subtask tree $\tau$ of depth $D$ be written as $n_0,n_1,n_2,...,n_{2^{D+1}-2}$.
The root node $n_0$ is the given task and the other nodes are the recursively generated subtasks.
Ideally, the leaf nodes should indicate the simplest reduction of the subtask that can be executed sequentially to complete the original task.
The tree can be viewed as a trajectory where each node $n_i$ is a state, and taking action $\pi_\theta(a|n_i)$ simultaneously places the agent in two states given by the child nodes $(n_{2i+1}, n_{2i+2})$.
Thus, the policy function can be written as $\pi_\theta(a|n_i)$, the transition probabilities as $p_T(n_{2i+1}, n_{2i+2}|a,n_i)$, and the probability of the tree trajectory under the policy $\tau_{\pi_\theta}$ can be represented as:

% \vspace{-.4cm}

\begin{align*}
    p_{\pi_\theta}(\tau) &= p(n_0) * [\pi_\theta(a_0|n_0) * p_T(n_1,n_2|a_0,n_0)] * \\
    &\quad [\pi_\theta(a_1|n_1) * p_T(n_3,n_4|a_1,n_1)] * \\
    &\quad [\pi_\theta(a_2|n_2) * p_T(n_5,n_6|a_2,n_2)] *... \\
    &\quad [\pi_\theta(a_{2^D-2}|n_{2^D-2}) * p_T(n_{2^{D+1}-3},n_{2^{D+1}-2}|a_{2^D-2}|n_{2^D-2})] \\
    p_{\pi_\theta}(\tau) &= p(n_0) \prod_{i=0}^{2^D-2} \pi_\theta(a_i|n_i) \prod_{i=0}^{2^D-2} p_T(n_{2i+1},n_{2i+2}|a_i,n_i)
\end{align*}

% \begin{theorem}[Policy Gradients]
%     \label{theorem:policy_grad_derivation}
%     Given a tree trajectory $\tau$ specified as a list of nodes $n_i$, generated using a policy $\pi_\theta$. The policy gradients can be written as:

%     \[ \nabla_\theta J(\theta) = \mathbb{E}_\tau \sum_{i=0}^{2^D-2} A^i(\tau) \nabla_\theta \log \pi_\theta(a_i|n_i) \]
% \end{theorem}

\textbf{Theorem \ref{theorem:policy_grad_derivation}} (Policy Gradients). \textit{Given a tree trajectory $\tau$ specified as a list of nodes $n_i$, generated using a policy $\pi_\theta$. The policy gradients can be written as:}

% \vspace{-.4cm}

\[ \nabla_\theta J(\theta) = \mathbb{E}_\tau \sum_{i=0}^{2^D-2} A^i(\tau) \nabla_\theta \log \pi_\theta(a_i|n_i) \]

\begin{proof}
    The log-probabilities of the tree trajectory and their gradients can be written as:
    
    % \vspace{-.4cm}

    \begin{gather}
        \label{eq:log_prob_tree}
        \log p_{\pi_\theta}(\tau) = \log p(n_0) + \sum_{i=0}^{2^D-2} \log \pi_\theta(a_i|n_i) + \sum_{i=0}^{2^D-2} \log p_T(n_{2i+1}, n_{2i+2}|a_i,n_i) \\
        \label{eq:grad_log_prob_tree}
        \nabla_\theta \log p_{\pi_\theta}(\tau) = 0 + \nabla_\theta \sum_{i=0}^{2^D-2} \log \pi_\theta(a_i|n_i) + 0 = \sum_{i=0}^{2^D-2} \nabla_\theta \log \pi_\theta(a_i|n_i)
    \end{gather}

    The objective of policy gradient methods is measured as the expectation of advantage or some scoring function $A(\tau)$:

    \begin{equation}
        J(\theta) = \mathbb{E}_\tau A(\tau) = \sum_\tau A(\tau) \cdot p_{\pi_\theta}(\tau)
    \end{equation}

    Then the gradients of the objective function $\nabla_\theta J(\theta)$ wrt the policy parameters $\theta$ can be derived as:

    \begin{align*}
        \nabla_\theta J(\theta) &= \nabla_\theta \sum_\tau A(\tau) \cdot p_{\pi_\theta}(\tau) \\
        &= \sum_\tau A(\tau) \cdot \nabla_\theta p_{\pi_\theta}(\tau) \\
        &= \sum_\tau A(\tau) \cdot p_{\pi_\theta}(\tau) \frac{\nabla_\theta p_{\pi_\theta}(\tau)}{p_{\pi_\theta}(\tau)} \\
        &= \sum_\tau A(\tau) \cdot p_{\pi_\theta}(\tau) \nabla_\theta \log p_{\pi_\theta}(\tau) \\
        &= \mathbb{E}_\tau A(\tau) \cdot \nabla_\theta \log p_{\pi_\theta}(\tau) \\
        &= \mathbb{E}_\tau \sum_{i=0}^{2^D-2} A^i(\tau) \nabla_\theta \log \pi_\theta(a_i|n_i) \quad (\text{Using Eq. \ref{eq:grad_log_prob_tree}})
    \end{align*}
\end{proof}

% \begin{theorem}[Baselines]
%     \label{theorem:baseline_proof}
%     If $A(\tau)$ is independent of $\tau$, say $b(n_i)$, then its net contribution to the policy gradient is $0$.

%     \[\mathbb{E}_\tau \sum_{i=0}^{2^D-2} b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i) = 0\]
% \end{theorem}
\textbf{Theorem \ref{theorem:baseline_proof}} (Baselines). \textit{If $A(\tau)$ is independent of $\tau$, say $b(n_i)$, then its net contribution to the policy gradient is $0$.}

\[\mathbb{E}_\tau \sum_{i=0}^{2^D-2} b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i) = 0\]

\begin{proof}
    If $A(\tau)$ is any fixed function that does not depend on the actions $\pi_\theta(a_i|n_i)$ and only on the state, say $b(n_i)$.
    Then $b(n_i)$ will be independent of the trajectory $\tau$ as it can be sampled from the steady state distribution under policy $\rho_{\pi_\theta}$ for any state $n_i$ without knowing $\tau$.
    In that case,

    % \vspace{-.5cm}

    \begin{align*}
        \mathbb{E}_\tau \sum_{i=0}^{2^D-2} b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i) &= \sum_{i=0}^{2^D-2} \mathbb{E}_\tau [b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i)] \\
        &= \sum_{i=0}^{2^D-2} \mathbb{E}_{n_i \sim \rho_{\pi_\theta}} \mathbb{E}_{a \sim \pi_\theta} [b(n_i) \nabla_\theta \log \pi_\theta(a_i|n_i)] \\
        &= \sum_{i=0}^{2^D-2} \mathbb{E}_{n_i \sim \rho_{\pi_\theta}} [b(n_i) \mathbb{E}_{a \sim \pi_\theta} \nabla_\theta \log \pi_\theta(a_i|n_i)] \\
        &= \sum_{i=0}^{2^D-2} \mathbb{E}_{n_i \sim \rho_{\pi_\theta}} [b(n_i)\sum_a \pi_\theta(a_i|n_i) \frac{\nabla_\theta \pi_\theta(a_i|n_i)}{\pi_\theta(a_i|n_i)}] \\
        &= \sum_{i=0}^{2^D-2} \mathbb{E}_{n_i \sim \rho_{\pi_\theta}} b(n_i) [\nabla_\theta \sum_a \pi_\theta(a_i|n_i)] \\
        &= \sum_{i=0}^{2^D-2} \mathbb{E}_{n_i \sim \rho_{\pi_\theta}} b(n_i) [\nabla_\theta 1] \quad (\text{Sum of probabilities is 1}) \\
        &= 0 \\
    \end{align*}
    
    % \vspace{-.9cm}
    % This means that a baseline $b(n_i)$ can help reduce the variance in scores.
\end{proof}



\subsection{Policy Evaluation for Trees}
\label{sec:policy_eval_tree}

We present the return and advantage estimation for trees as an extension of current return estimation methods for linear trajectories.
As the return estimation for a state $s_t$ in linear trajectories depends upon the next state $s_{t+1}$, our tree return estimation method uses child nodes $(n_{2i+1},n_{2i+2})$ to compute the return for a node $n_i$.
We extend the previous methods like lambda returns and Gen
realized Advantage estimation (GAE) for trees.

The objective of our method is to reach nodes that are directly reachable.
Such nodes are marked as terminal, and the agent receives a reward.
For generalization, let's say that when the agent takes an action $a_i$ at node $n_i$, it receives a pair rewards $(R_{2i+1}(n_i,a_i), R_{2i+2}(n_i,a_i))$ corresponding to the child nodes.
Formally, the rewards $R(\tau)$ is an array of length equal to the length of the tree trajectory with $R_0 = 0$.
Then, the agent's task is to maximize the sum of rewards received in the tree trajectory $\mathbb{E}_\tau \sum_{i=0}^{\infty} R_i$.
To consider future rewards, the returns for a trajectory can be computed as the sum of rewards discounted by their distance from the root node (depth), $\mathbb{E}_\tau \sum_{i=0}^\infty \gamma^{\lfloor \log_2(i+1) \rfloor - 1} R_i$.
Thus, the returns for each node can be written as the sum of rewards obtained and the discount-weighted returns thereafter:

\begin{equation}
    G_i = (R_{2i+1} + \gamma G_{2i+1}) + (R_{2i+2} + \gamma G_{2i+2})
\end{equation}

Although this works theoretically, a flaw causes the agent to collapse to a degenerate local optimum.
This can happen if the agent can generate a subgoal very similar to the initial or goal state ($\|s_t,s_\text{sub}\|<\epsilon$ or $\|s_g,s_\text{sub}\|<\epsilon$).
A usual theme of reward systems for subgoal trees will be to have a high reward when the agent predicts a reachable or temporally close enough subgoal.
Thus, if the agent predicts a degenerate subgoal, it receives a reward for one child node and the initial problem carries forward to the other node.

Therefore, we propose an alternative objective that optimizes for the above objective under the condition that both child subtasks $(n_{2i+1}, n_{2i+2})$ get solved.
Instead of estimating the return as the sum of the returns from the child nodes, we can estimate it as the minimum of the child node returns.

\begin{equation}
    G_i = \min(R_{2i+1} + \gamma G_{2i+1}, R_{2i+2} + \gamma G_{2i+2})
\end{equation}

This formulation causes the agent to optimize the weaker child node first and receive discounted rewards if all subtasks are solved (or have high returns).
It can also be noticed that the tree return for a node is essentially the discounted return along the linear trajectory that traces the path with the least return starting at that node.
Next, we analyze different return methods in the tree setting and try to prove their convergence.


\subsubsection{Lambda Returns}
\label{sec:appendix_lambda_returns}

TD($\lambda$) returns for linear trajectories are computed as:

\begin{align}
    G_t^\lambda = R_{t+1} + \gamma((1-\lambda) V(s_{t+1}) + \lambda G_{t+1}^\lambda)
\end{align}

We propose, the lambda returns for tree trajectories can be computed as:

\begin{align}
    G_i^\lambda = \min(R_{2i+1} + \gamma((1-\lambda) V(n_{2i+1}) + \lambda G_{2i+1}^\lambda), R_{2i+2} + \gamma((1-\lambda) V(s_{2i+2}) + \lambda G_{2i+2}^\lambda))
\end{align}

Which essentially translates to the minimum of lambda returns using either of the child nodes as the next state.

Next, we check if there exists a fixed point that the value function approaches.
The return operators can be written as:

\begin{align*}
    \mathcal{T}^\lambda V(n_i) = \mathbb{E}_\pi [\min(&R_{2i+1} + \gamma((1-\lambda) V(n_{2i+1}) + \lambda G_{2i+1}), \\
    &R_{2i+2} + \gamma((1-\lambda) V(n_{2i+2}) + \lambda G_{2i+2}))] \\
    \mathcal{T}^0 V(n_i) = \mathbb{E}_\pi [\min(&R_{2i+1} + \gamma V(n_{2i+1}), R_{2i+2} + \gamma V(n_{2i+2}))] \\
    \mathcal{T}^1 V(n_i) = \mathbb{E}_\pi [\min(&R_{2i+1} + \gamma G_{2i+2}, R_{2i+2} + \gamma G_{2i+2})]
\end{align*}

\begin{lemma}[Non-expansive Property of the Minimum Operator]
    \label{theorem:non_expand_min}
    For any real numbers \(a, b, c, d\), the following inequality holds:
    \begin{equation*}
        \left| \min(a, b) - \min(c, d) \right| \leq \max \left( |a - c|, |b - d| \right).
    \end{equation*}
\end{lemma}

\begin{proof}
    To prove the statement, we consider the minimum operator for all possible cases of \(a\), \(b\), \(c\), and \(d\). Let \(\min(a, b)\) and \(\min(c, d)\) be the minimum values of their respective pairs.

    \textbf{Case 1:} \(a \leq b\) and \(c \leq d\)

    In this case, \(\min(a, b) = a\) and \(\min(c, d) = c\).
    The difference becomes:
      \[ \left| \min(a, b) - \min(c, d) \right| = |a - c|. \]
    Since $\max \left( |a - c|, |b - d| \right) \geq |a - c|$, the inequality holds.

    \textbf{Case 2:} \(a \leq b\) and \(c > d\)

    Here, \(\min(a, b) = a\) and \(\min(c, d) = d\).
    The difference becomes:
      \[ \left| \min(a, b) - \min(c, d) \right| = |a - d|. \]
    Since $b \geq a$, $|a - d| \leq |b - d| \leq \max(|a - c|, |b - d|)$, and the inequality holds.

    \textbf{Case 3:} \(a > b\) and \(c \leq d\)
    
    Here, \(\min(a, b) = b\) and \(\min(c, d) = c\).
    The difference becomes:
      \[ \left| \min(a, b) - \min(c, d) \right| = |b - c|. \]
    Since $a \geq b$, $|b - c| \leq |a - c| \leq \max(|a - c|, |b - d|)$, and the inequality holds.
    
    \textbf{Case 4:} \(a > b\) and \(c > d\)

    Symmetric to Case 1.
    
    \textbf{Conclusion:}
    
    In all cases, the inequality
    \[ \left| \min(a, b) - \min(c, d) \right| \leq \max \left( |a - c|, |b - d| \right) \]
    is satisfied. Therefore, the minimum operator is non-expansive.
\end{proof}

% \begin{theorem}[Contraction property of the return operators]
%     \label{theorem:return_contraction}
%     The Bellman operator $\mathcal{T}^0$ is a $\gamma$-contraction mapping wrt. to $\|\cdot\|_\infty$
% \end{theorem}
\textbf{Theorem \ref{theorem:return_contraction}} (Contraction property of the return operators). \textit{The Bellman operators $\mathcal{T}$ corresponding to the returns are a $\gamma$-contraction mapping wrt. to $\|\cdot\|_\infty$}.

\[\|\mathcal{T} V_1 - \mathcal{T} V_2\|_\infty \leq \gamma \| V_1 - V_2 \|_\infty\]

\begin{proof}
    We start with the simpler case, $\mathcal{T}^0$.
    Let $V_1,V_2$ be two arbitrary value functions.
    Then the max norm of any two points in the value function post update is:
    
    \begin{alignat*}{2}
        \|\mathcal{T}^0 V_1 - \mathcal{T}^0 V_2\|_\infty = & \|\mathbb{E}_\pi [\min(R_{2i+1} + \gamma V_1(n_{2i+1}), R_{2i+2} + \gamma V_1(n_{2i+2}))] - \\
        & \mathbb{E}_\pi [\min(R_{2i+1} + \gamma V_2(n_{2i+1}), R_{2i+2} + \gamma V_2(n_{2i+2}))]\|_\infty \\
        = & \|\mathbb{E}_\pi [\min(R_{2i+1} + \gamma V_1(n_{2i+1}), R_{2i+2} + \gamma V_1(n_{2i+2})) - \\
        &\min(R_{2i+1} + \gamma V_2(n_{2i+1}), R_{2i+2} + \gamma V_2(n_{2i+2}))]\|_\infty \\
        \leq & \| \min(R_{2i+1} + \gamma V_1(n_{2i+1}), R_{2i+2} + \gamma V_1(n_{2i+2})) - \\
        &\min(R_{2i+1} + \gamma V_2(n_{2i+1}), R_{2i+2} + \gamma V_2(n_{2i+2})) \|_\infty \\
        \leq & \max(\| \gamma V_1(n_{2i+1}) - \gamma V_2(n_{2i+1}) \|_\infty, \| \gamma V_1(n_{2i+2}) - \gamma V_2(n_{2i+2}) \|_\infty) \\
        \leq & \gamma \max(\| V_1(n_{2i+1}) - V_2(n_{2i+1}) \|_\infty, \| V_1(n_{2i+2}) - V_2(n_{2i+2}) \|_\infty) \\
        \leq & \gamma \max(\| V_1(n_{j}) - V_2(n_{j}) \|_\infty, \| V_1(n_{k}) - V_2(n_{k}) \|_\infty) \\
        \leq & \gamma \| V_1 - V_2 \|_\infty \quad \text{(merging $\max$ with $\|\cdot\|_\infty$)} 
    \end{alignat*}

    A similar argument can be shown for $\|\mathcal{T}^1 V_1 - \mathcal{T}^1 V_2\|_\infty$ and $\|\mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2\|_\infty$. Using the non-expansive property (Th. \ref{theorem:non_expand_min}) and absorbing the $\max$ operator with $\|\cdot\|_\infty$ leading to the standard form for linear trajectories.

    \begin{alignat*}{2}
        \|\mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2 \|_\infty \leq & \| \gamma((1-\lambda) (V_1 - V_2) + \lambda (\mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2)) \|_\infty \\
        \leq & \gamma(1-\lambda)\|V_1-V_2\|_\infty + \gamma \lambda \| \mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2 \|_\infty \quad \text{(Using triangle inequality)}\\
        (1 - \gamma\lambda)\|\mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2 \|_\infty \leq & \gamma (1 - \lambda)\|V_1-V_2\|_\infty \\
        \|\mathcal{T}^\lambda V_1 - \mathcal{T}^\lambda V_2 \|_\infty \leq & \frac{\gamma(1-\lambda)}{1-\gamma\lambda}\|V_1-V_2\|_\infty
    \end{alignat*}

    For contraction, $\frac{\gamma(1-\lambda)}{1-\gamma\lambda} < 1$ must be true.

    \vspace{-.5cm}

    \begin{alignat*}{2}
        \frac{\gamma(1-\lambda)}{1-\gamma\lambda} <& 1 \\
        \gamma(1-\lambda) <& {1-\gamma\lambda} \\
        \gamma - \gamma\lambda <& 1 - \gamma\lambda \\
        \gamma <& 1
    \end{alignat*}

    Which is always true.

    Since $\mathcal{T}^1$ is a special case of $\mathcal{T}^\lambda$, it is also a contraction.
\end{proof}


% \begin{alignat*}{2}
%     \|\mathcal{T}^0 V_1 - \mathcal{T}^0 V_2\|_\infty \leq & \gamma \| V_1 - V_2 \|_\infty \\ 
% \end{alignat*}


\subsubsection{Bootstrapping with $D$-depth Returns}
\label{sec:appendix_bootstrapping}

When the subtask tree branches end as terminal (or are masked as reachable), the agent receives a reward $=1$, which provides a learning signal using the discounted returns.
However, when the branches do not end as terminal nodes, it does not provide a learning signal for the nodes above it, as the return is formulated as $\min$ of the returns from child nodes.
In this case, we can replace the returns of the non-terminal leaf nodes with their value estimates.
Therefore, in case the value estimate from the end node is high, indicating the agent knows how to solve the task from there onwards, it still provides a learning signal.
The $n$-step return for a linear trajectory is written as:

\[G^{(n)}_t = R_{t+1} + \gamma G^{(n-1)}_{t+1}\]

with the base case as:

\[G_t^{(1)} = R_{t+1} + \gamma V(s_{t+1})\]

We write the $n$-step returns for the tree trajectory as:

\[G_i^{(d)} = \min(R_{2i+1} + \gamma G^{(d-1)}_{2i+1}, R_{2i+2} + \gamma G^{(d-1)}_{2i+2})\]

with the base case as:

\[G_i^{(1)} = \min(R_{t+1} + \gamma V(n_{2i+1}), R_{t+2} + \gamma V(n_{2i+2}))\]

Value estimates help bootstrap at the maximum depth of the unrolled subtask tree $D$ and allow the policy to learn from incomplete plans. 


\subsubsection{Properties of Tree Return Estimates}
\label{sec:appendix_prop_tree_ret}

In section \ref{sec:appendix_lambda_returns} it can be seen how the tree return formulation for a node essentially reduces to the linear trajectory returns along the branch of minimum return in the subtree under it.
When the value function has reached the stationary point.
For a subtask tree, if all branches end as terminal nodes, the return will be $\gamma^{D'} 1$, where $D'$ is the depth of the deepest node.
Otherwise, it would be $\gamma^{D} V'$ where $V'$ is the non-terminal leaf node with the minimum return.
Thus, it can be seen how higher depth penalizes the returns received at the root node with the discount factor $\gamma$.
This property is true with the linear trajectories where the policy converges to shortest paths to rewards to counter discounting \cite{sutton2018reinforcement,puterman2014markov}.
Thus, our goal-conditioned policy similarly converges to plans trees with minimum maximum depth: $\min_{\pi_\theta}(\max d_i)$, where $d_i$ is the depth of node $n_i$.

This property also implies that the returns for a balanced tree will be higher than those for an unbalanced tree.
The same sequence of leaf nodes can be created using different subtask trees.
% However, since the method penalizes the maximum depth, at each node $n_i$, the policy will act such that the difference between the maximum depth of the subtree at both child nodes is minimum: $\min_{\pi_\theta} (\max (d_{2i+1}, d_{2i+2}))$.
When the policy does not divide the task into roughly equally tough sub-tasks it results in an unbalanced tree.
Since the tree is constrained to yield the same sequence of leaf nodes, its maximum depth $D_U$ will be higher than or equal to a balanced tree $D_U \geq D_B$.
Thus, at optimality, the policy should subdivide the task in roughly equal chunks.
However, it should be noted that two subtask trees with different numbers of leaf nodes can have the same maximum depth.


% \clearpage


\subsection{Illustrations}
\label{sec:appendix_illustrations}

% \subsubsection{CSR Illustrations}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=.4\textwidth]{figures/illustrations/director_csr.png}
%     \caption{The Director \cite{hafner2022deep} agent with VAE interpreted as a State Recall module. The manager uses the VAE for generating worker goal states using the decoder of the VAE.}
%     \label{fig:director_csr}
% \end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.35\textwidth]{figures/illustrations/base_architecture.png}
    \caption{Overall architecture of the proposed Agent. The Perception module constructs the state representation $s_t$ using the observation $o_t$. The manager uses the the current and goal state $(s_t,s_g)$ to predict subgoals in the elatent space. The latent predictions are converted to subgoals using the GCSR module. After appropriate subgoal selection for the worker $s_{wg}$}, the worker predicts the environmental action $a$.
    \label{fig:base_arch}
\end{figure}

\begin{figure}
     \centering
     % \begin{subfigure}[b]{0.329\textwidth}
     %     \centering
     %     \includegraphics[width=\textwidth]{figures/illustrations/ucsr.png}
     %     \caption{U-CSR CVAE}
     %     \label{fig:ucsr}
     % \end{subfigure}
     % \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/icsr.png}
         \caption{Initial-CSR CVAE}
         \label{fig:icsr_cvae}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/gcsr.png}
         \caption{Goal-CSR CVAE}
         \label{fig:gcsr_cvae}
     \end{subfigure}
     \caption{Illustrations of the Conditional State Recall (CSR) modules.}
     \label{fig:csr_cvae}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/wk_actor.png}
         \caption{Worker actor policy}
         \label{fig:worker_policy}
     \end{subfigure}
     \hspace{5em}
     \begin{subfigure}[b]{0.225\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/wk_critic.png}
         \caption{Worker critic}
         \label{fig:worker_critic}
     \end{subfigure}
     \caption{Worker SAC that takes the initial state and a worker goal as input to yield a low-level environmental action.}
     \label{fig:worker_sac}
\end{figure}

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.5\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/illustrations/ucsr_actor.png}
%          \caption{U-CSR manager policy}
%          \label{fig:ucsr_actor}
%      \end{subfigure}
%      \hspace{5em}
%      \begin{subfigure}[b]{0.225\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/illustrations/ucsr_critic.png}
%          \caption{U-CSR manager critic}
%          \label{fig:ucsr_critic}
%      \end{subfigure}
%      \caption{The U-CSR manager SAC that is same as the Director manager. [Note: \textit{Not} used in our agent].}
%      \label{fig:ucsr_sac}
% \end{figure}

% \begin{figure}
%      \centering
%      \begin{subfigure}[b]{0.5\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/illustrations/icsr_actor.png}
%          \caption{I-CSR manager policy}
%          \label{fig:icsr_actor}
%      \end{subfigure}
%      \hspace{5em}
%      \begin{subfigure}[b]{0.225\textwidth}
%          \centering
%          \includegraphics[width=\textwidth]{figures/illustrations/icsr_critic.png}
%          \caption{I-CSR manager critic}
%          \label{fig:icsr_critic}
%      \end{subfigure}
%      \caption{The I-CSR manager SAC that constrains the worker goals to nearby states by conditioning the search on $s_t$.}
%      \label{fig:icsr_sac}
% \end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/gcsr_actor.png}
         \caption{Planning policy}
         \label{fig:gcsr_actor}
     \end{subfigure}
     \hspace{5em}
     \begin{subfigure}[b]{0.225\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/gcsr_critic.png}
         \caption{Planning critic}
         \label{fig:gcsr_critic}
     \end{subfigure}
     \caption{The planning SAC that conditions the search space on initial and goal states $(s_i,s_g)$. A single application generates a subgoal $s_\text{sub}$ and recursive application generates tree rollouts (Sec. \ref{sec:rollout}).}
     \label{fig:gcsr_sac}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.7\textwidth]{figures/illustrations/multi_icsr.png}
%     \caption{Multi-Resolution variant of the I-CSR that learns state transitions of different temporal resolutions $p_n$. The first layers of the Encoder $\text{Enc}_I$ are shared between the Encoders and the last layers of the Decoder $\text{Dec}_I$ are shared. Dashed arrows indicate samples from the predicted distributions being propagated.}
%     \label{fig:multi_icsr}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/illustrations/multi_icsr_policy.png}
%     \caption{Actor Policy for the Multi-Resolution variant of the I-CSR that outputs a choice variable $c$ and the latents $[z_0,z_1,...,z_{N-1}]$ for each VAE, at each step. Corresponding Decoders are used to generate options for the worker goals $s_{wg_n}$. The choice is then used to gate the selected goal.}
%     \label{fig:multi_icsr_policy}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.7\textwidth]{figures/illustrations/multi_gcsr.png}
%     \caption{Multi-Resolution variant of the G-CSR that learns to predict midway states $s_{t+q_n/2}$ between pairs of states $(s_t,s_{t+q_n})$ at different temporal resolutions $q_n$. The first layers of the Encoder $\text{Enc}_G$ are shared between the Encoders and the last layers of the Decoder $\text{Dec}_G$ are shared.}
%     \label{fig:multi_gcsr}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/illustrations/multi_gcsr_policy.png}
%     \caption{Actor Policy for the Multi-Resolution variant of the G-CSR.}
%     \label{fig:multi_gcsr_policy}
% \end{figure}

% \subsubsection{Hierarchical Planning}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/illustrations/tree_unroll.png}
%     \caption{Unrolling the subgoal tree for training.}
%     \label{fig:tree_unroll}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=.8\textwidth]{figures/illustrations/tree_inference.png}
%     \caption{Fast inference by unrolling the first tree branch as a stack.}
%     \label{fig:tree_inference}
% \end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/tree_reward.png}
         \caption{Reward estimates for tree $\eta$ (see Fig. \ref{fig:tree_unroll}) consisting of nodes $n_i$. The initial and goal states at each node are used as inputs for the worker to test reachability. As an example, assuming a perfect case where the leaf nodes end in a directly solvable subtask, rewards $r_i=1$ for the leaf nodes.}
         \label{fig:tree_reward}
     \end{subfigure}
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/tree_disc_rew.png}
         \caption{Discounted Monte-Carlo returns for the example tree. The dashed cell borders indicate terminal states with reachability rewards. Discounted returns are computed for the non-terminal and non-leaf nodes.}
         \label{fig:tree_disc_rew}
     \end{subfigure}
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/tree_disc_rew_2.png}
         \caption{Discounted Monte-Carlo returns for an imperfect tree. Node $n_4$ is marked as terminal, leading to the subtree below it being ignored. And since one of the leaf nodes remains unreachable, the discounted return for the root node $n_0$ is $0$.}
         \label{fig:tree_disc_rew_imperf}
     \end{subfigure}
     \begin{subfigure}[b]{0.6\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/tree_disc_ret.png}
         \caption{Discounted $n$-step return for the imperfect tree with the baseline, allowing bootstrapping.}
         \label{fig:tree_disc_ret_crit}
     \end{subfigure}
     \caption{The reward and return estimation method for trees.}
     \label{fig:tree_ret_estimation}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.8\textwidth]{figures/illustrations/inference_reach.png}
    \caption{Using I-CSR module for $O(1)$ reachability measure during plan inference. The first reachable subgoal is used as the worker goal.}
    \label{fig:inference_reach}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.55\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/expl_actor.png}
         \caption{Explorer policy}
         \label{fig:expl_actor}
     \end{subfigure}
     \hspace{5em}
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/expl_critic.png}
         \caption{Explorer critic}
         \label{fig:expl_critic}
     \end{subfigure}
     \caption{The exploring manager additionally using the memory. The policy uses the I-CSR decoder for generating worker goals.}
     \label{fig:expl_sac}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/data_icsr.png}
         \caption{Extracting pairs of states $(s_t,s_{t+p})$ for the I-CSR module. We use only one temporal resolution ($p=K$) for our planning agent.}
         \label{fig:data_icsr}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/data_gcsr.png}
         \caption{Triplets of states $(s_t,s_{t+q/2},s_{t+q})$ from the coarse trajectory for the G-CSR module at different temporal resolutions.}
         \label{fig:data_gcsr}
     \end{subfigure}
     \caption{Training data generation for the I-CSR and G-CSR modules. Same colored arrows correspond to elements of the same example.}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/expl_rew.png}
         \caption{State dependency for computing exploratory rewards at the next state.}
         \label{fig:expl_rew}
     \end{subfigure}
     \par\bigskip
     \begin{subfigure}[b]{0.7\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/illustrations/expl_mem.png}
         \caption{Inputs for the explorer.}
         \label{fig:expl_mem_input}
     \end{subfigure}
     \caption{Selecting states for memory from the memory buffer. Memory buffer stores a coarse trajectory of the past states visited by the agent. Assume an agent is at the state marked as \textit{current state} in (b). Then, (a) illustrates the dependence on states for estimating exploratory rewards it will receive when at the next state. The rewards are computed using the I-CSR and G-CSR modules. Given the dependence, (b) marks the states needed to predict the reward accurately. The memory selection comes out as $\{s_{t-K}, s_{t-2K}, s_{t-4K,...}\}$ which is given as input to the explorer along with the current state $s_t$.}
     \label{fig:expl_mem}
\end{figure}


\clearpage


\subsection{Architecture \& Training Details}
\label{sec:appendix_training}

In the context of the task, we train an agent with an I-CSR module and a G-CSR module.
The manager refreshes the worker's goal every $K=8$ steps and the worker executes environmental actions for the given goal every step.
Two policies are trained, an exploratory policy and a goal-conditioned planning policy.
The agents are trained for 5M steps where the exploratory policy is used for the first 3M steps and then switched to the task policy.
All modules: RSSM, static representations, I-CSR, G-CSR, worker, exploratory policy, and planning policy are trained every $16$-steps throughout.
% The RSSM is trained using the same objective as the Director.
The CSR modules are trained using the data collected from previous trajectories.
All policies are trained by unrolling trajectories using imagination.


\subsubsection{Conditional State Recall}
\label{sec:appendix_csr_training}

The CSR modules are trained using trajectories extracted from the replay buffer in an Off-Policy fashion.
The I-CSR modules $(\text{Enc}_I,\text{Dec}_I)$ are trained by extracting pairs of states with a temporal difference of $K$-steps, $(s_t, s_{t+K})$, from a previous trajectory $\kappa$ (Fig. \ref{fig:data_icsr}).
The G-CSR modules $(\text{Enc}_G,\text{Dec}_G)$ are trained by extracting triplets of states $(s_t,s_{t+q/2},s_{t+q})$ from a coarse trajectory $\kappa_c$ that contains every $K^\text{th}$ frame from the original trajectory $\kappa$ (Fig. \ref{fig:data_gcsr}).
Since the $\kappa_c$ is extracted at abstract steps we adjust the temporal resolutions accordingly, $q \in Q_i/K$.
Both modules are trained using the ELBO objective which is the sum of the mean-squared error between the target and predicted state, and the KL divergence between the prior and the latent distributions.
We use the same prior distribution as the Director \citep{hafner2022deep} for the ICSR module $p_I(z)$, a collection of $8$, $8$-dim one-hot vectors, shaped as $8 \times 8$.
But we reduced the latent space for the prior distribution $p_G(z)$ of the GCSR module to $4\times4$.

% The module is trained using the ELBO objective which is the sum of the squared difference between the target and predicted state, and the scaled KL divergence between the prior $p_{I}(z)$ and $q_{I}(z|s_t,s_{t+p})$ (Eq. \ref{eq:icsr_loss}).
% We use the same prior distribution as the Director, a collection of $8$, $8$-dim one-hot vectors, shaped as $8 \times 8$.

% Since RSSM sequentially processes all frames to yield state representations, we use static state representations for the triplets for compute efficiency.
% Since the manager requires predicting subgoals at various temporal resolutions, we extract triplets at multiple temporal resolutions ($q \in Q$).
% % We use $q \in Q$ where $Q = \{2,4,8,16,32,64\}$ for our experiments that allow learning subgoals for goals that are $K \times \max{Q} = 512$ steps away.
% The same ELBO objective is used to train the CVAE (Eq. \ref{eq:gcsr_loss}).
% However, the size of the latent space is reduced to $(4,4)$ for prior $p_{G}(z)$ to reflect the reduced size of a goal-conditioned prediction space.

% The U-CSR module is trained in the same way as the Director, for each state unconditionally (Eq. \ref{eq:ucsr_loss}).
% However, we increase the latent space to $(16,16)$ with prior $p_{U}(z)$ for the increased size of the prediction space.

\begin{gather}
    % \label{eq:ucsr_loss}
    % \mathcal{L}(U) = \left\Vert s_t - \text{Dec}_U(z)\right\Vert^2 + \beta \text{KL}[q_U(z|s_t) \parallel p_U(z)] \quad \text{where} \quad z \sim q_U(z|s_t) \\
    \label{eq:icsr_loss}
    \mathcal{L}(\text{Enc}_I,\text{Dec}_I) = \left\Vert s_{t+p} - \text{Dec}_I(s_t,z)\right\Vert^2 + \beta \text{KL}[\text{Enc}_{I}(z|s_t,s_{t+p}) \parallel p_I(z)] \quad \text{where} \quad z \sim \text{Enc}_{I}(z|s_t,s_{t+p}) \\
    \label{eq:gcsr_loss}
    \begin{split}
        \mathcal{L}(\text{Enc}_G,\text{Dec}_G) = \left\Vert s_{t+q/2} - \text{Dec}_G(s_t,s_{t+q},z)\right\Vert^2 + \beta \text{KL}[\text{Enc}_G(z|s_t,s_{t+q/2},s_{t+q}) \parallel p_G(z)] \\ \text{where} \quad z \sim \text{Enc}_G(z|s_t,s_{t+q/2},s_{t+q})
    \end{split}
\end{gather}


\subsubsection{Worker}
\label{sec:appendix_worker_training}

The worker is trained using $K$-step imagined rollouts $(\kappa \sim \pi_W)$.
% Given rewards $R_i$, computed using the CSR modules for the explorer, or as the cosine-similarity measures for the worker.
Given the imagined trajectory $\kappa$, the rewards for the worker $R^W_t$ are computed as the \texttt{cosine\_max} similarity measure between the trajectory states $s_t$ and the prescribed worker goal $s_\text{wg}$.
First, discounted returns $G^\lambda_t$ are computed as $n$-step lambda returns (Eq. \ref{eq:lin_lambda_return}).
Then the Actor policy is trained using the REINFORCE objective (Eq. \ref{eq:lin_reinforce}) and the Critic is trained to predict the discounted returns (Eq. \ref{eq:lin_value}).

\begin{gather}
    \label{eq:lin_lambda_return}
    G^\lambda_t = R^W_{t+1} + \gamma_L ((1 - \lambda)v(s_{t+1}) + \lambda G^\lambda_{t+1}) \\
    \label{eq:lin_reinforce}
    \mathcal{L}(\pi_W) = -\mathbb{E}_{\kappa \sim \pi_W} \sum_{t=0}^{H-1} \left[ (G^\lambda_t - v_W(s_t)) \ln \pi_W(z|s_t) + \eta \text{H}[\pi_W(z|s_t)] \right] \\
    \label{eq:lin_value}
    \mathcal{L}(v_W) = \mathbb{E}_{\kappa \sim \pi_W} \left[ \sum_{t=0}^{H-1} (v_W(s_t) - G^\lambda_t)^2 \right]
\end{gather}


\subsubsection{Explorer}
\label{sec:appendix_explorer_training}

Given a set of initial states from the training data, the explorer generates $H_E$-step imagined trajectories $\kappa \sim \pi_E$ where it refreshes the worker goal every $K$-th step.
For memory, we initialize a memory buffer of length $\min(H_E, \max(q))/K$ to record the past visited states.
At each $K$-th step, the memory buffer is updated with the current state as, $\text{mem\_buff}_t = \texttt{concat}(s_t, \text{mem\_buff}_t[:-1])$.
Then the relevant memory input is extracted as: $\text{mem}_t[j] = \text{mem\_buff}_t[2j-1]$.

Given the rollout $\kappa$, an abstract or coarse trajectory $\kappa_c$ is extracted containing every $K$-th frame.
The rewards $R^E_t$ are computed as the sum of the exploratory rewards from the ICSR and the GCSR modules for trajectory (Eq. \ref{eq:dhp_expl_rew}).
Fig. \ref{fig:expl_rew} shows the state dependence of the exploratory rewards.
Then the $n$-step lambda returns and the losses for the exploratory SAC are computed as:

\begin{gather}
    G^\lambda_t = R^E_{t+1} + \gamma_L ((1 - \lambda)v(s_{t+1}) + \lambda G^\lambda_{t+1}) \\
    \mathcal{L}(\pi_E) = -\mathbb{E}_{\kappa_c \sim \pi_E} \sum_{t=0}^{H-1} \left[ (G^\lambda_t - v_E(s_t, \text{mem}_t)) \ln \pi_E(z|s_t, \text{mem}_t) + \eta \text{H}[\pi_E(z|s_t, \text{mem}_t)] \right] \\
    \mathcal{L}(v_E) = \mathbb{E}_{\kappa_c \sim \pi_E} \left[ \sum_{t=0}^{H-1} (v_E(s_t, \text{mem}_t) - G^\lambda_t)^2 \right]
\end{gather}


\subsubsection{Static State Representations}
\label{sec:appendix_static_repr}

Since the RSSM integrates a state representation using a sequence of observations, it does not work well for single observations.
To generate goal state representations using single observations we train an MLP separately that tries to approximate the RSSM outputs ($s_t$) from the single observations ($o_t$).
We call these representations static state representations.
Moreover, since GCSR modules require state representations at large temporal distances, it can be practically infeasible to generate them using RSSM.
Thus, we use static state representations to generate training data for the GCSR module as well.
% An auxiliary MLP is trained to help generate goal states using goal images.
The MLP is a dense network with a $\tanh$ activation at the final output layer.
It is trained to predict the RSSM output (computed using a sequence of images) using single image observations.
To avoid saturating gradients, we use an MSE loss on the preactivation layer using labels transformed as $l_\text{new} = \text{atanh}(\text{clip}(l,\delta - 1,1 - \delta))$.
The clipping helps avoid computational overflows, we use $\delta=10^{-4}$.
The static representations are used for conducting goal representations and fast estimation of states for coarse trajectories.


\subsection{Implementation Details}
\label{sec:appendix_impl_details}

Given the above architecture, we summarize the implementation details of the policy and the train functions here.
Our code is implemented in TensorFlow/python using XLA JIT compilation running in mixed-precision \texttt{float16}.
We use an NVIDIA 4090 RTX (24 GB) for our experiments and each training session of 5M steps takes about $30$ hours to complete.
Table \ref{tab:hyperparam} lists the hyperparameters used in the paper, for complete details visit \href{https://github.bath.ac.uk/ss3966/phd_project}{https://github.bath.ac.uk/ss3966/phd\_project}.


\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline\hline
        Name & Symbol & Value \\
        \hline\hline
        Train batch size & $B$ & $16$ \\
        Replay trajectory length & $L$ & $64$ \\
        Replay coarse trajectory length & $L_c$ & $48$ \\
        Worker abstraction length & $K$ & $8$ \\
        Explorer Imagination Horizon & $H_E$ & $64$ \\
        Return Lambda & $\lambda$ & $0.95$ \\
        Return Discount (tree) & $\gamma$ & $0.95$ \\
        Return Discount (linear) & $\gamma_L$ & $0.99$ \\
        State \texttt{cosine\_max} similarity threshold & $\Delta_R$ & $0.7$ \\
        Plan temporal resolutions & $Q$ & $\{16,32,64,128,256\}$ \\
        Maximum Tree depth during training & $D$ & $5$ \\
        Maximum Tree depth during inference & $D_\text{Inf}$ & $8$ \\
        ICSR latent size & - & $8 \times 8$ \\
        GCSR latent size & - & $4 \times 4$ \\
        RSSM deter size & - & $1024$ \\
        RSSM stoch size & - & $32 \times 32$ \\
        \hline
    \end{tabular}
    \caption{Agent Hyperparameters}
    \label{tab:hyperparam}
\end{table}


\subsubsection{Policy Function}

At each step, the policy function is triggered with the environmental observation $o_t$.
The RSSM module processes the observation $o_t$ and the previous state $s_{t-1}$ to yield a state representation $s_t$.
During exploration, the manager $\pi_E$ uses the $s_t$ to generate a worker goal using the I-CSR module.
During task policy, the planning manager $\pi_\theta$ generates subgoals in the context of a long-term goal $s_g$, and the first directly reachable subgoal is used as the worker's goal.
Finally, the worker generates a low-level environmental action using the current state and the worker goal $(s_t,s_\text{wg})$.


\subsubsection{Train Function}

The training function is executed every $16$-th step.
A batch size $B$ of trajectories $\kappa$ and coarse trajectories $\kappa_c$ is sampled from the exploration trajectories or the expert data.
The length of extracted trajectories is $L$ and the length of coarse trajectories is $L_c$ spanning over $L_c \times K$ time steps.
Then the individual modules are trained sequentially:
\begin{itemize}
    \item RSSM module is trained using $\kappa$ via the original optimization objective \cite{hafner2019learning} followed by the static state representations (Sec. \ref{sec:appendix_static_repr}).

    % \item Train the static state representions for states in $\kappa$ (\ref{sec:appendix_static_repr}).

    \item The CSR modules are trained using $\kappa$ and $\kappa_c$ (Sec. \ref{sec:appendix_csr_training}).

    \item The worker policy is optimized by extracting tuples $(s_t,s_{t+K})$ from the trajectories $\kappa$ and running the worker instantiated at $s_t$ with worker goal as $s_{t+K}$ (Sec. \ref{sec:appendix_worker_training}).

    \item The planning policy is trained using sample problems extracted as pairs of initial and final states $(s_t,s_g)$ at randomly mixed lengths from $\kappa_c$. Then the solution trees are unrolled and optimized as in Sec. \ref{sec:hier_plan}.

    \item Lastly, if the mode is exploration the exploratory policy is also optimized using each state in $\kappa$ as the starting state (Sec. \ref{sec:appendix_explorer_training}). 
\end{itemize}

% The U-CSR module is trained for each state in $\kappa$ individually using the ELBO objective.
% The I-CSR module is trained at the resolution $p=K$ by extracting $(s_t,s_{t+p})$ from the trajectories $\kappa$.
% A Multi-resolution G-CSR module is trained at resolutions $q \in \{2K,4K,8K,16K,32K\}$.
% Since the coarse trajectory is sampled at a lower resolution, we extract samples as $q \in Q\}$.
% Then, the worker policy is optimized by extracting tuples $(s_t,s_{t+K})$ from the trajectories $\kappa$ and running the worker instantiated at $s_t$ with worker goal as $s_{t+K}$.
% The worker is rewarded for increasing the $\text{cosine\_max}$ similarity of the current state with the goal state.
% To train the planning policy, sample problems are extracted as pairs of initial and final states at mixed lengths from $\kappa_c$.
% Formally, $8$ pairs are extracted per coarse trajectory as $s_0,s_l$ where $l \in (0, T_c)$.
% The solution trees are unrolled and optimized as in Sec. \ref{sec:hier_plan}.
% Lastly, if the mode is exploration the exploratory policy is optimized by imagining trajectories using each state in $\kappa$ as the starting state for the exploratory rewards (Sec. \ref{sec:mem_aug_expl}).

% \subsection{Context Gradients for CSR}
% \label{sec:appendix_cntxt_grads}

% The reliance on the conditions while generating goal states using the CVAEs is measured as the absolute gradients $|\partial \hat{s}_{t+p}/\partial s_{t}|$.
% The gradients indicate the change in the prediction $\hat{s}_{t+p}$ with $s_t$.
% A high absolute gradient value indicates a large change in the prediction for the same latent variable.
% These gradients are measured for the corresponding training data.



\subsection{Sample Exploration Trajectories}
\label{sec:appendix_expl_sample_traj}

% We test three settings: vanilla rewards that encourage visiting unexplored states, ICSR-based rewards that encourage executing unexplored state transitions, and GCSR-based rewards that encourage unexplored path segments.
We compare sample trajectories generated by a various reward schemes in this section.
Fig. \ref{fig:expl_vanilla_traj} shows the sample trajectories of an agent trained to optimize the vanilla exploratory rewards.
Fig. \ref{fig:expl_sk_traj} shows the sample trajectories of an agent trained to optimize only for ICSR-based rewards.
Fig. \ref{fig:expl_pl_traj} shows the sample exploration trajectories of an agent that optimizes only for the GCSR-based rewards.
Fig. \ref{fig:expl_plskNoMem_traj} shows sample exploration trajectories of an agent that optimizes for default exploratory rewards (ICSR + GCSR) but without memory.
It can be seen that the I-CSR rewards-based agent generates trajectories that cause the agent to constantly move as it is rewarded for executing state transitions over simply visiting states.
Similarly, the G-CSR rewards-based agent generates trajectories that less often lead to repeated path segments.
The generated trajectories provide a better learning signal for the planning modules (Sec. \ref{sec:abl_expl}). 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_gl_traj1.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_gl_traj2.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_gl_traj3.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_gl_traj4.png}
     \end{subfigure}
     \caption{Sample exploration trajectories using the vanilla exploratory rewards.}
     \label{fig:expl_vanilla_traj}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_sk_traj0.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_sk_traj1.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_sk_traj2.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_sk_traj3.png}
     \end{subfigure}
     \caption{Sample exploration trajectories using the ICSR module for the state-transition-based rewards.}
     \label{fig:expl_sk_traj}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_plMem_traj1.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_plMem_traj2.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_plMem_traj3.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_plMem_traj4.png}
     \end{subfigure}
     \caption{Sample exploration trajectories using the GCSR modules for the path-segments-based rewards.}
     \label{fig:expl_pl_traj}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_noMem_traj1.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_noMem_traj2.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_noMem_traj3.png}
     \end{subfigure}
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/samples/expl_noMem_traj4.png}
     \end{subfigure}
     \caption{Sample exploration trajectories using default strategy (both ICSR and GCSR-based rewards) without the Memory.}
     \label{fig:expl_plskNoMem_traj}
\end{figure}


\subsection{Discussion \& Future Work}
% \label{sec:discussion}
\label{sec:discussion_n_futurework}

The proposed architecture demonstrates significant improvements over previous baselines.
Notably, the agent does not require expert data and can efficiently self-generate high-quality training data.
This approach eliminates the need for access to the internal environmental state during trial execution, making it more versatile and applicable to a wider range of environments.
Additionally, the method supports cheap inference, that can enable efficient re-planning in dynamic environments where frequent updates are necessary.

However, it is important to acknowledge the limitations of the method.
While reachability is easier to predict compared to distance-based metrics, it relies heavily on accurate and unique state representations.
In cases where state representations are not sufficiently distinct, the decoder may reconstruct image observations accurately, but the \texttt{cosine\_max} similarity between two different state representations might still exceed the defined threshold $\Delta_R = 0.7$.
This issue was particularly pronounced when using smaller state representation sizes.
Furthermore, since the agent relies on imagination for training, the quality of the learned world dynamics becomes critical.
Inaccurate dynamics can reinforce undesirable behaviors, highlighting the importance of robust state representation learning.
Another architectural limitation of our method is that it estimates goal states using state observations which is uncommon.
Most environments provide goal states as structured information like coordinates, and sometimes as raw text.
However, this work focuses on planning rather than constructing robust state representations.


% \section{Future Work}
% \label{sec:future_work}

Building on the strengths of the proposed method, several promising directions for future research emerge.
One key area is the development of methods for generating state representations from task descriptions or textual prompts.
This capability would enable interaction with agents using text-based goals while allowing the agent to leverage the learned real-world dynamics internally.
Such an advancement could bridge the gap between natural language understanding and goal-directed planning, making the method more accessible and intuitive for human users.

Another promising direction is the automatic generation of goals using reward functions.
This would facilitate long-term goal-directed behavior in reward-based tasks, enabling the agent to autonomously identify and pursue meaningful objectives without explicit human intervention.
These advancements can help extend model-based hierarchical planning and make it more accessible for generic use cases in the real world.


\end{document}
