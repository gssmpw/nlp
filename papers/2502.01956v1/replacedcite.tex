\section{Related work}
\label{sec:related_work}


%\subsection{Goal Conditioned Reinforcement Learning}


%Goal-Conditioned Reinforcement Learning (GCRL) extends traditional RL by training agents to achieve arbitrary goals specified in the state space.
%Some GCRL methods are implemented as goal-conditioned policies that take the initial and final states as input to output a sequence of actions to reach the goal state ____.
%While others enable efficient learning in sparse reward settings by relabeling goals like Hindsight Experience Replay (HER) ____.
%Recent methods like Goal-Conditioned Supervised Learning (GCSL) ____ simplify GCRL by framing it as a supervised learning problem.

%Despite their successes, GCRL methods struggle with long-horizon tasks due to several key challenges.
%First, the sparsity of rewards in long-horizon settings makes it difficult for agents to discover successful trajectories through random exploration ____.
%Second, the curse of dimensionality in high-dimensional state and goal spaces exacerbates the exploration problem, as the agent must navigate a vast state space to reach distant goals ____.
%Third, credit assignment becomes increasingly challenging as the horizon lengthens ____.
%and non-stationarity in the goal distribution where the agent must adapt to constantly shifting objectives ____.
% These limitations make it difficult for agents to explore effectively and learn policies for distant goals, highlighting the need for hierarchical approaches that decompose tasks into smaller subgoals.


\subsection{Planning Algorithms}

Planning methods aim to solve long-horizon tasks efficiently by exploring future states and selecting optimal actions ____.
%These methods can be broadly categorized into several strategies, each with its own strengths and limitations.
Monte Carlo Tree Search (MCTS) ____ expands a tree of possible future states by sampling actions and simulating outcomes.
While effective in discrete action spaces, MCTS struggles with scalability in high-dimensional or continuous environments.
Visual Foresight methods ____ learned visual dynamics models to simulate future states, enabling planning in pixel-based environments.
However, they require accurate world models and can be computationally expensive.
Some use explicit graph search over the replay buffer data ____.
Model Predictive Control (MPC) ____ is an online planner that samples future trajectories and optimizes actions over a finite horizon.
These methods rely on sampling the future state and thus do not scale well with the horizon length.

To address the challenges of long-horizon tasks, some planning algorithms decompose complex problems into manageable subtasks by predicting intermediate subgoals.
%These methods predict a subgoal approximately midway between the initial and goal states, dividing the problem into two simpler subtasks.
MAXQ ____ decomposes the value function of the target Markov Decision Process (MDP) into an additive combination of smaller MDPs, enabling hierarchical planning.
Sub-Goal Trees ____ learn a subgoal prediction policy optimized to minimize the total predicted distance measures of the decomposed subtasks.
Long-Horizon Visual Planning ____ utilizes Goal-Conditioned Predictors (GCPs) to reduce the prediction space and employs a Cross-Entropy Method (CEM)-based planner optimized to minimize distance costs.
CO-PILOT ____ is a collaborative framework where a planning policy and an RL agent learn through mutual feedback to optimize the total distance cost.
% Goal-Conditioned Reinforcement Learning with Imagined Subgoals ____ predicts midway subgoals by optimizing a distance measure and later uses a value function for reachability checks.

While these methods have demonstrated success, they rely heavily on distance-based metrics, which are challenging to learn and sensitive to policy quality ____.
In contrast, our method uses discrete reachability-based rewards, which are easier to estimate accurately and provide clearer learning signals.


\subsection{Hierarchical Reinforcement Learning Agents}


Hierarchical reinforcement learning (HRL) refers to a set of techniques that temporarily abstract actions ____.
%HRL agents decompose long-horizon tasks into reusable subgoals or skills, enabling efficient exploration and transferable policies.
Foundational works like the options framework ____ and MAXQ decomposition ____ introduced temporal abstraction, allowing agents to reason at multiple time scales.
HRL agents are usually split into two modules, a Manager and a Worker.
%The worker learns a task-independent policy that is goal-conditioned or options-based, and the manager learns a policy to control the worker in the task context.
%The manager works on a coarser timescale than the worker and outputs abstract actions or subgoals for the worker to complete.
%Working on a coarser scale allows the manager to address the problem of long-horizon credit assignment.
Various strategies have been proposed in the past for learning the two levels of abstract actions.
OPAL ____ encodes the trajectory using a bidirectional GRU and is optimized as a Variational Autoencoder.
Causal InfoGAN ____ uses an InfoGAN to learn causal representations by maximizing mutual information between the skill and the initial and final state pairs.
DADS ____ apply the mutual information objective to learn a diverse forward-inverse kinematics model that is later used for planning.
The Director ____ manager predicts subgoals for the worker in a latent space, learned using a VAE.
We use an HRL architecture for our agent, as it enables our planning policy to construct plans as discrete combinations of abstract actions.
Note that the key contribution of our work is to show that hierarchical RL agents can be better trained using a discrete reachability-based reward.