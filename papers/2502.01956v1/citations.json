[
  {
    "index": 0,
    "papers": [
      {
        "key": "kaelbling1993learning",
        "author": "Kaelbling, Leslie Pack",
        "title": "Learning to achieve goals"
      },
      {
        "key": "schaul2015universal",
        "author": "Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David",
        "title": "Universal value function approximators"
      },
      {
        "key": "pong2018temporal",
        "author": "Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey",
        "title": "Temporal difference models: Model-free deep rl for model-based control"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "andrychowicz2017hindsight",
        "author": "Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech",
        "title": "Hindsight experience replay"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2024goal",
        "author": "Li, Shijun and Hasson, Hilaf and Hu, Jing and Ghosh, Joydeep",
        "title": "Goal-Conditioned Supervised Learning for Multi-Objective Recommendation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "nachum2018data",
        "author": "Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey",
        "title": "Data-efficient hierarchical reinforcement learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "pong2018temporal",
        "author": "Pong, Vitchyr and Gu, Shixiang and Dalal, Murtaza and Levine, Sergey",
        "title": "Temporal difference models: Model-free deep rl for model-based control"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "arjona2019rudder",
        "author": "Arjona-Medina, Jose A and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp",
        "title": "Rudder: Return decomposition for delayed rewards"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kaelbling1993learning",
        "author": "Kaelbling, Leslie Pack",
        "title": "Learning to achieve goals"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "lavalle2006planning",
        "author": "LaValle, Steven M",
        "title": "Planning algorithms"
      },
      {
        "key": "choset2005principles",
        "author": "Choset, Howie and Lynch, Kevin M and Hutchinson, Seth and Kantor, George A and Burgard, Wolfram",
        "title": "Principles of robot motion: theory, algorithms, and implementations"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "browne2012survey",
        "author": "Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon",
        "title": "A survey of monte carlo tree search methods"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ebert2018visual",
        "author": "Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey",
        "title": "Visual foresight: Model-based deep reinforcement learning for vision-based robotic control"
      },
      {
        "key": "finn2017deep",
        "author": "Finn, Chelsea and Levine, Sergey",
        "title": "Deep visual foresight for planning robot motion"
      },
      {
        "key": "hafner2019learning",
        "author": "Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James",
        "title": "Learning latent dynamics for planning from pixels"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "eysenbach2019search",
        "author": "Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey",
        "title": "Search on the replay buffer: Bridging planning and reinforcement learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "nagabandi2018neural",
        "author": "Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S and Levine, Sergey",
        "title": "Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning"
      },
      {
        "key": "nagabandi2020deep",
        "author": "Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash",
        "title": "Deep dynamics models for learning dexterous manipulation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "dietterich2000hierarchical",
        "author": "Dietterich, Thomas G",
        "title": "Hierarchical reinforcement learning with the MAXQ value function decomposition"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "jurgenson2020sub",
        "author": "Jurgenson, Tom and Avner, Or and Groshev, Edward and Tamar, Aviv",
        "title": "Sub-goal trees a framework for goal-based reinforcement learning"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "pertsch2020long",
        "author": "Pertsch, Karl and Rybkin, Oleh and Ebert, Frederik and Zhou, Shenghao and Jayaraman, Dinesh and Finn, Chelsea and Levine, Sergey",
        "title": "Long-horizon visual planning with goal-conditioned hierarchical predictors"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "ao2021co",
        "author": "Ao, Shuang and Zhou, Tianyi and Long, Guodong and Lu, Qinghua and Zhu, Liming and Jiang, Jing",
        "title": "CO-PILOT: Collaborative planning and reinforcement learning on sub-task curriculum"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chane2021goal",
        "author": "Chane-Sane, Elliot and Schmid, Cordelia and Laptev, Ivan",
        "title": "Goal-conditioned reinforcement learning with imagined subgoals"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "eysenbach2019search",
        "author": "Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey",
        "title": "Search on the replay buffer: Bridging planning and reinforcement learning"
      },
      {
        "key": "ao2021co",
        "author": "Ao, Shuang and Zhou, Tianyi and Long, Guodong and Lu, Qinghua and Zhu, Liming and Jiang, Jing",
        "title": "CO-PILOT: Collaborative planning and reinforcement learning on sub-task curriculum"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "botvinick2009hierarchically",
        "author": "Botvinick, Matthew M and Niv, Yael and Barto, Andew G",
        "title": "Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective"
      },
      {
        "key": "wiering2012reinforcement",
        "author": "Wiering, Marco A and Van Otterlo, Martijn",
        "title": "Reinforcement learning"
      },
      {
        "key": "barto2003recent",
        "author": "Barto, Andrew G and Mahadevan, Sridhar",
        "title": "Recent advances in hierarchical reinforcement learning"
      },
      {
        "key": "sutton1999between",
        "author": "Sutton, Richard S and Precup, Doina and Singh, Satinder",
        "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning"
      },
      {
        "key": "pateria2021hierarchical",
        "author": "Pateria, Shubham and Subagdja, Budhitama and Tan, Ah-hwee and Quek, Chai",
        "title": "Hierarchical Reinforcement Learning: A Comprehensive Survey"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "sutton1999between",
        "author": "Sutton, Richard S and Precup, Doina and Singh, Satinder",
        "title": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "dietterich2000hierarchical",
        "author": "Dietterich, Thomas G",
        "title": "Hierarchical reinforcement learning with the MAXQ value function decomposition"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "ajay2020opal",
        "author": "Anurag Ajay and Aviral Kumar and Pulkit Agrawal and Sergey Levine and Ofir Nachum",
        "title": "{\\{}OPAL{\\}}: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "kurutach2018learning",
        "author": "Kurutach, Thanard and Tamar, Aviv and Yang, Ge and Russell, Stuart J and Abbeel, Pieter",
        "title": "Learning plannable representations with causal infogan"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "sharma2020dynamics",
        "author": "Archit Sharma and Shixiang Gu and Sergey Levine and Vikash Kumar and Karol Hausman",
        "title": "Dynamics-Aware Unsupervised Discovery of Skills"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "hafner2022deep",
        "author": "Hafner, Danijar and Lee, Kuang-Huei and Fischer, Ian and Abbeel, Pieter",
        "title": "Deep Hierarchical Planning from Pixels"
      }
    ]
  }
]