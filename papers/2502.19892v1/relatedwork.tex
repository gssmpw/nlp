\section{Related Works}
This section provides a comprehensive review of pertinent studies on the critical components of DRL-based local planners, encompassing temporal perception models, architectures, and simulation platforms. Each subsection concludes with an analysis of their limitations.

\subsection{Temporal Perception in DRL}
\label{TPinDRL}
Temporal perception is critical for DRL agents, enabling the comprehension, prediction, and decision-making required to operate in dynamic environments. Existing methodologies for enhancing temporal perception in DRL can be categorized into three groups: (1) Recurrent Neural Network (RNN)-based, (2) CNN-based, and (3) Transformer-based.
RNN-based methods (e.g., LSTM\cite{use-lstm1,use-lstm2,use-lstm3,use-lstm4} and GRU\cite{use-gru1,use-gru2,use-gru3}) integrate recurrent architectures into policy or value networks to retain historical information. However, RNNs' dependence on initial hidden states restricts their applicability within on-policy DRL algorithms (e.g., A3C\cite{a3c}, PPO\cite{ppo}), which inherently suffer from low sample efficiency.
CNN-based approaches \cite{drlvo,lizhi1,lizhi2,pathrl} address this limitation by reformatting raw sensor data into 2D images, stacking temporal sequences along the depth channel to enable temporal perception. This strategy facilitates the adoption of experience replay\cite{DQN}, ensuring the compatibility with off-policy DRL algorithms thus improving sample efficiency. However, CNNs' efficacy in capturing long-term temporal correlations remains debatable as it is originally designed for spatial feature extraction. 
To address this issue, Transformer-based methods, such as DTQN\cite{DTQN}, conceptualize sequential timesteps as input tokens, leveraging self-attention mechanisms for long-term temporal modeling.
However, Transformer-based models demand substantial training data\cite{scalinglaw,trans_survey1,trans_survey2}, which can be further exacerbated in online DRL settings, where agents must interact extensively with environments to learn useful skills. 
This constraint has forced researchers to resort to offline DRL (e.g., Decision Transformer\cite{decision-transformer}, Trajectory Transformer\cite{traj-transformer}, and Q-Transformer\cite{q-transformer}) as a compromised alternative, which trains agents on pre-collected datasets. Nevertheless, offline methods face challenges in optimality and generalization due to the absence of online exploration. Meanwhile, the preparation of an offline dataset can be labor-intensive. In summary, RNN-based methods confront sample efficiency limitations, CNN-based approaches struggle with long-term temporal modeling, and Transformer-based architectures face challenges in effective learning. Further research is necessary to develop temporally robust DRL frameworks that balance sample efficiency, modeling capacity, and training feasibility.

\subsection{Architectures of DRL-based local planner}
DRL-based local planners are broadly classified into (1) pipeline-based and (2) end-to-end architectures.
Pipeline-based approaches\cite{drlvo,drl-ad,ad-survey} decompose planning into modular components (e.g., object detection, localization, prediction, fusion, planning, and control), offering interpretability and scenario-specific customization. However, the sequential design compromises robustness and real-time performance, as discussed in Section \ref{Introduction}.
In contrast, end-to-end methods\cite{lizhi1,lizhi2,Color,visual-end2end} map the raw sensor data directly to low-level control commands, making them more integrated, thus overcoming the flaws of pipeline-based approaches. Whereas, the design and training of end-to-end architectures are non-trivial, as extracting high-level features directly from noisy sensor data is challenging. 
Consequently, existing approaches typically rely on easier-to-train models (e.g., RNN, CNN), leaving a research gap between end-to-end architectures and advanced temporal models (e.g., Transformer). Hence, further works are necessitated to explore their effective integration and form a more powerful architecture for DRL-based local planners.

\subsection{Simulation Platforms for DRL}
Simulation platforms are indispensable for DRL, as they not only circumvent the prohibitive costs of real-world training but also accelerate the training process. Widely adopted platforms include Robotics Gym\cite{robogym}, PyBullet\cite{PyBullet}, Webots\cite{Webots}, Gazebo\cite{gazebo}, and Sparrow\cite{Color}. Developed by OpenAI, Robotics Gym is a simulation platform based on the MuJoCo physics engine, providing a suite of continuous control tasks and serving as critical tools for robotic control research. 
PyBullet, a high-fidelity physics simulation library based on the Bullet physics engine, is commonly employed in DRL scenarios requiring precise physical interactions, such as robotic manipulation, locomotion, and collision detection. However, despite their advanced capabilities, these platforms are primarily designed for articulated robots and thus cannot adequately support the training of local planners for wheeled robots.
Webots and Gazebo, developed by Cyberbotics and Open Robotics respectively, are two simulation platforms well-suited for wheeled robots. Both support diverse robotic platforms and sensors while providing extensive tools for modeling, simulating, and analyzing robotic behaviors across various environments. This enables researchers to explore different configurations and scenarios in simulation. Nevertheless, due to their high computational demands and lack of parallel simulation capabilities, these platforms exhibit limitations in data throughput and diversity. 
To address these challenges, \cite{Color} proposed the Sparrow simulator. Featuring vectorized diversity, Sparrow enables simultaneous simulations of diversified setups (e.g., environmental layouts, sensor noise, and robot kinematics) across multiple vectorized environments. This functionality enhances both data diversity and throughput, thereby ameliorating the DRL training significantly. 
However, Sparrow currently supports only static environment simulations, restricting its applicability in more complex and realistic scenarios involving dynamic obstacles. Furthermore, its diverse training maps require manual preparation, substantially increasing labor costs and diminishing the automaticity of DRL training. Therefore, further research is necessary to develop a more comprehensive and advanced platform apposite for DRL-based local planning.