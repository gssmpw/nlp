% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{svg}
% \usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{bbding}
\usepackage[marginal]{footmisc}
\usepackage[most]{tcolorbox}
\usepackage{colortbl} % 允许表格有颜色
\usepackage{fancyhdr} % 用于自定义页眉和页脚
\usepackage{geometry} % 调整页面布局
\usepackage{float}
\usepackage{caption}
\usepackage{CJKutf8}
\usepackage{xcolor}
\usepackage{amssymb}
\usepackage{pifont}
\usepackage{soul}
\usepackage{ulem}
\usepackage{subcaption}
% \usepackage{xeCJKfntef}

\definecolor{darkgreen}{RGB}{0,100,0} %自定义深绿色
\definecolor{darkred}{RGB}{160,0,0} %自定义深红色

% 定义蓝色下划线
\newcommand{\blueuline}[1]{{\setulcolor{blue}\ul{#1}}}

% 定义绿色下划线
\newcommand{\greenuline}[1]{{\setulcolor{darkgreen}\ul{#1}}}

% 定义红色下划线
\newcommand{\reduline}[1]{{\setulcolor{darkred}\ul{#1}}}

% 自定义一个没有外部框线的浮动环境
\floatstyle{plain} % 去掉顶部和底部的线条
\restylefloat{tcolorboxfloat}

\newfloat{tcolorboxfloat}{tbp}{lop}
\floatname{tcolorboxfloat}{Table}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{FIND: Fine-grained Information Density Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}


\author{
 \textbf{Mingyi Jia\textsuperscript{1}},
 \textbf{Junwen Duan\textsuperscript{1}}\thanks{Corresponding Author. Email: \href{mailto:jwduan@csu.edu.cn}{jwduan@csu.edu.cn}.},
 \textbf{Yan Song\textsuperscript{2}},
 \textbf{Jianxin Wang\textsuperscript{1}},
\\
 \textsuperscript{1}Hunan Provincial Key Lab on Bioinformatics, \\ School of Computer Science and
Engineering, Central South University \\
 \textsuperscript{2}University of Science and Technology of China,
\\
\texttt{\{jiamingyi, jwduan\}@csu.edu.cn, clksong@gmial.com, jxwang@mail.csu.edu.cn}
}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}


%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Retrieval-Augmented Large Language Models (LLMs), which integrate external knowledge into LLMs, have shown remarkable performance in various medical domains, including clinical diagnosis. However, existing RAG methods struggle to effectively assess task difficulty to make retrieval decisions, thereby failing to meet the clinical requirements for balancing efficiency and accuracy. So in this paper, we propose FIND (\textbf{F}ine-grained \textbf{In}formation \textbf{D}ensity Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. FIND incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. By optimizing the retrieval process and implementing a knowledge filtering module, FIND ensures that the retrieval is better suited to clinical scenarios. Experiments on three Chinese electronic medical record datasets demonstrate that FIND significantly outperforms various baseline methods, highlighting its effectiveness in clinical diagnosis tasks.
\end{abstract}

\section{Introduction}
% [RAG2]
Large Language Models (LLMs) \cite{openai2023gpt4,llmdd160_saab2024capabilities} have demonstrated exceptional capabilities in medical tasks, including clinical diagnosis~\cite{zhou2024llmddoverview}. However, their adoption in high-stakes domains faces challenges such as hallucination—the generation of plausible but incorrect information~\cite{maynez2020faithfulness, llmdd245_huang2023survey}—and the resource-intensive nature of knowledge updates~\cite{zhang2023large, kasai2024realtime}. Retrieval-augmented generation (RAG)~\cite{lewis2020retrieval} offers a solution by integrating trustworthy external documents to reduce hallucinations and ensure up-to-date information.

While researchers have extensively explored RAG to enhance LLM accuracy in high-risk domains~\cite{llmdd_ovewview_zhou2024large}, not all medical cases require this approach. Many common diseases or cases with mild symptoms and clear diagnoses can be accurately addressed by LLMs without retrieval~\cite{jeong2024adaptive}. However, most existing RAG methods lack selective retrieval logic, instead performing retrievals for all queries indiscriminately. This approach not only increases computational and time costs but may also introduce errors through low-quality retrievals (as shown in Figure~\ref{example1}.a), potentially degrading rather than improving performance.



\begin{figure}[t]
	\centering
	\includegraphics[width=\columnwidth]{figures/example.pdf}
	\caption{Illustration of three different RAG paradigms for solving clinical diagnosis task.}
	\label{example1}
\end{figure}

To improve the efficiency of retrieval systems, researchers have proposed adaptive RAG paradigms~\cite{jeong2024adaptive, su2024dragin, yao2024seakr}, which establish control logic to activate the retrieval system only when certain conditions are met. There are two common approaches in these paradigms: (1) setting judgment conditions based on LLM's output text or probability distributions~\cite{yao2024seakr, su2024dragin}; (2) training a relatively smaller judgment model to determine whether to perform retrieval at a lower cost~\cite{jeong2024adaptive}. Figure~\ref{example1}-(b) and (c) provide corresponding examples of these approaches.

% To address these issues, researchers have proposed the Adaptive-RAG paradigm~\cite{jeong2024adaptive, su2024dragin, yao2024seakr}, which performs retrieval only when necessary through specific input judgment conditions or smaller classification models. However, existing Adaptive-RAG methods face limitations (Figure~\ref{example1}.b). First, relying solely on LLM outputs to decide retrieval is superficial, as LLMs are prone to hallucinations~\cite{yona2024can, yao2024seakr} and may confidently generate incorrect content despite lacking relevant knowledge~\cite{huang2023large, xu2024perils}. Second, while training small classification models to evaluate queries is feasible, they struggle with long, complex, and redundant input contexts. These models often fail to accurately capture query structures and distinctions, exhibiting biases toward query length or phrasing.
% 除了adaptive-rag之外，其他基于输出概率分布的方法往往缺少合适的设置在确定要启动检索的时候过滤无关信息

However, approach (1) has limitations as LLMs tend to be overconfident, generating high-confidence probability distributions even when lacking relevant knowledge~\cite{huang2023large, xu2024perils}. Additionally, these methods typically require access to LLM output probability distributions (logits), limiting adaptability for API services or closed-source model applications. Approach (2) relies heavily on input content characteristics. For example, Jeong et al.~\cite{jeong2024adaptive} differentiate between "simple" single-hop questions (e.g., "When is Michael F. Phelps' birthday?") and "difficult" multi-hop questions (e.g., "What currency is used in Bill Gates' birthplace?"). Such question-answering tasks have distinct difficulty gradients, making them relatively easy for models to differentiate.

Unlike single-hop or multi-hop question answering tasks, input texts in the medical domain typically do not exhibit obvious structural patterns that can be captured, making it extremely challenging for smaller language models to understand the difficulty of answering them. Therefore, the successful experiences from this approach cannot be directly transferred to other tasks.

% Unlike single-hop or multi-hop question-answering tasks, disease diagnosis tasks cannot directly retrieve the correct answer in a single step. Instead, they require progressively narrowing down the most likely disease by assessing the relevance between patient information and knowledge base documents. This necessitates that retrieval information must be both critical and useful; otherwise, irrelevant data may lead to retrieval failure. Additionally, the input for disease diagnosis tasks, often lengthy and semantically discontinuous, makes it challenging to directly retrieve matching documents from knowledge bases. Most existing adaptive-RAG methods overly focus on whether to perform retrieval, neglecting the filtering of retrieval information and optimization of the retrieval process, resulting in suboptimal performance for disease diagnosis tasks.

Based on our analysis, we proposed a disease diagnosis approach FIND, using adaptive retrieval decision optimization, specifically tailored for complex structured and long-context medical texts. The core innovation introduces a retrieval decision optimization module based on input information completeness. This module segments long inputs into text units, employs a classification model to predict each unit's importance, and calculates global information completeness to determine retrieval necessity. Since the classifier already identifies important text units, these can be prioritized during retrieval, minimizing interference from irrelevant information. Through a single prediction round, this module achieves both retrieval decision optimization and query selection, effectively addressing the limitations in existing RAG paradigms.

% To address these issues, we propose FIND (Fine-Grained Information Density Guided Adaptive RAG), a novel framework that improves the reliability of RAG in disease diagnosis scenarios. As illustrated in Figure~\ref{workflow}, Our framework incorporates a fine-grained adaptive control module to determine whether retrieval is necessary based on the information density of the input. This module segments long and complex input into fine-grained units and trains a classifier to predict the importance of each unit. Based on the classification results, we calculate the information density of the input and decide whether retrieval is necessary. Additionally, we introduce a differential diagnosis guided knowledge filtering module to enhance the quality of the retrieval process by filtering out irrelevant information and retaining the most useful documents.

Our main contributions are as follows:
\begin{itemize}
    \item We propose FIND, a framework for adaptive retrieval-augmented disease diagnosis without the need for tuning backbone LLMs.
    \item We desgined a novel data annotation methodology that employs masking operations to elicit varied responses from LLMs, thereby acquiring label information. Concurrently, we have optimized the retrieval process to better accommodate clinical scenarios with complex context.
    \item We conducted extensive experiments on three Chinese EMR datasets to demonstrate the effectiveness of our FIND framework.
    % We demonstrate FIND’s strong flexibility as a general disease diagnosis framework, showing robust performance across different tasks (NER, RE, EE) and easy integration with various LLM architectures.
\end{itemize}
% Adaptive-RAG
% On the other hand, in real-world scenarios, not all medical queries are inherently complex or necessitate retrieval. For straightforward queries or those accompanied by sufficient information, LLMs can readily provide accurate diagnoses without the need to access external knowledge. Adhering to the conventional RAG approach, which involves retrieving information for every query, not only incurs unnecessary computational and temporal overhead but also increases the risk of misleading LLM reasoning due to the retrieval of irrelevant information. Consequently, there is a pressing need to design an adaptive RAG system capable of determining the necessity of retrieval based on the complexity of the patient information provided. In this study, we address this requirement by training a classifier—a compact model designed to predict the complexity level of incoming queries. Furthermore, we automate the acquisition and annotation of the training dataset by leveraging the predictions (i.e., identifying which models accurately respond to which queries), thereby eliminating the need for manual annotation. This approach establishes a robust intermediary that facilitates iterative LLM enhancement for complex queries, a single-step method for simpler queries, and a no-retrieval-augmentation method for the most direct queries (answered by LLMs themselves), significantly enhancing overall efficiency and accuracy.

\section{Related Work}
\subsection{RAG in Clinical Disease Diagnosis}
To improve diagnostic accuracy, model reliability, and reduce hallucination issues without retraining, recent studies widely adopt Retrieval-Augmented Generation (RAG) to integrate external medical knowledge \cite{llmdd69_wen2023mindmap,llmdd70_wu2024guiding,llmdd79_shi2023retrieval,llmdd141_thompson2023large,llmdd142_zhao2024heterogeneous}. Most research uses basic retrieval methods \cite{llmdd74_ge2024development,llmdd79_shi2023retrieval,llmdd140_zhang2023integrating,llmdd142_zhao2024heterogeneous,llmdd148_oniani2024enhancing}, typically leveraging embedding models to encode external knowledge and task queries into vector representations. Relevant knowledge is retrieved via vector similarity and used in LLMs through tailored prompts for diagnosis generation. Besides, knowledge graphs are also widely employed \cite{llmdd69_wen2023mindmap,llmdd70_wu2024guiding,llmdd71_gao2023large}. 
% Wen et al. used graphs to guide LLMs in generating reasoning maps \cite{llmdd69_wen2023mindmap}, and Thompson et al. applied regular expression matching for pulmonary hypertension diagnostics \cite{llmdd141_thompson2023large}. Wang et al. \cite{llmdd80_wang2024beyond} proposed a multi-LLM framework with specialized medical knowledge.
% For example, Zhenzhu et al. \cite{llmdd144_zhenzhu2024gpt} developed GPT-based agents for traumatic brain injury rehabilitation, while McInerney et al. \cite{llmdd146_mcinerney2024towards} extracted evidence from clinical notes for cancer, pneumonia, and pulmonary edema risk assessment. 


\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/workflow.pdf}
	\caption{The overall architecture of our proposed framework FIND. FIND consists of two stages. Stage(a) involves inference \& Retrieval Decision Making Based on Fine-Grained Information Density. Stage (b) focuses on knowledge retrieval and integration. Note that Stage (b) is activated only when the score computed in Stage (a) falls below a predefined threshold.}
	\label{workflow}
\end{figure*}

\subsection{Adaptive-RAG}
Adaptive Retrieval-Augmented Generation (RAG) dynamically determines whether a large language model (LLM) requires external knowledge retrieval to mitigate inaccuracies. FLARE~\cite{jiang2023active} and DRAGIN~\cite{su2024dragin} activate search engines when the LLM generates low-confidence tokens. Wang et al.~\cite{wang2024llms} use a prompting mechanism for LLMs to autonomously decide on retrieval. Self-Awareness-Guided Generation~\cite{wang2023self} trains a classifier to assess output authenticity, while Adaptive-RAG~\cite{jeong2024adaptive} evaluates query complexity to determine retrieval necessity. Mallen et al.~\cite{mallen2023not} propose activating retrieval based on entity frequency in queries, though this may fail for complex, multi-step reasoning tasks. Asai et al. introduce Self-RAG~\cite{asai2023self}, which trains a model to dynamically retrieve, critique, and generate text.

% 基于LLM输出的

% Existing adaptive RAG methods face two primary challenges. First, determining when to retrieve based solely on the LLM's outputs is superficial. The retrieval decisions made by LLMs remain susceptible to hallucinations, thereby failing to reliably indicate actual knowledge sufficiency~\cite{yona2024can, yao2024seakr}. Moreover, even when the correct knowledge is absent in the model's parameters, LLMs tend to confidently generate incorrect content~\cite{huang2023large, xu2024perils}. Second, while training a small classification model to evaluate input queries is a feasible approach, it becomes problematic when the input query context is long, structurally complex, and content-rich. Directly attempting to enable the classification model to understand the structure and distinctions among diverse input queries is challenging. These models are likely to struggle with accurately modeling the input content and may develop biases toward query length or specific phrasing.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/annotate.pdf}
	\caption{Details of our proposed annotation strategy. During the annotation process, we adopt different annotation strategies based on the responses generated by the LLM.}
	\label{annotate}
\end{figure*}

\section{Methods}
In this section, we first present the formal definition of disease diagnosis task and the task settings for adaptive-RAG-based disease diagnosis. Then we will introduce the details of each components of our proposed FIND framework.
\subsection{Preliminaries}
\textbf{Direct Disease Diagnosis via LLM}: Given a token sequence $\mathbf{x} = [x_1, x_2, \ldots, x_n]$ representing input text, LLM-based text generation can be formalized as $\mathbf{y} = \mathrm{LLM}(\mathbf{x}, prompt)$, where $prompt$ is a task-specific template and $\mathbf{y} = [y_1, y_2, \ldots, y_n]$ is the generated output.

For disease diagnosis, the input $\mathbf{x}$ is patient information $\mathcal{Q}$, and the output $\mathbf{y}$ is the predicted diagnosis $\hat{\mathcal{D}}$, formalized as: $\hat{\mathcal{D}} = \mathrm{LLM}(\mathcal{Q}, prompt)$.

\noindent\textbf{RAG-based Disease Diagnosis}: This approach retrieves relevant knowledge ${d}$ from an external knowledge source $\mathcal{K}$ using a retrieval module $\mathsf{Retriever}$. The diagnosis is then generated by incorporating this knowledge: $\hat{\mathcal{D}} = \mathrm{LLM}(\mathcal{Q},d, prompt)$, where $d = \mathsf{Retriever}(\mathcal{K},\mathcal{Q})$. In this paper, we use a document knowledge base $\mathsf{KB}$ as the external knowledge source, detailed in Appendix~\ref{corpus}.

\noindent\textbf{Adaptive-RAG-based Disease Diagnosis}: This paradigm introduces a control function $F$ that evaluates input $\mathcal{Q}$ to determine whether retrieval is necessary:
\begin{equation}
	\hat{\mathcal{D}} = 
	\begin{cases} 
	\mathrm{LLM}(\mathcal{Q}, prompt), & \text{if } F(\mathcal{Q})=\langle Activate \rangle \\
	\mathrm{LLM}(\mathcal{Q}, d, prompt), & \text{otherwise}
	\end{cases}
\end{equation}
where $d = \mathsf{Retriever}(\mathcal{K},\mathcal{Q})$. The control function $F$ can be implemented through various approaches, such as LLM token probability distributions, confidence levels, or a smaller trained decision model.

\subsection{Retrieval Decision Optimization Based on Input Information Completeness}
\label{adaptive_control}

In this section, we first introduce the detail implementation of the adaptive retrieval control module (Figure~\ref{workflow}.Stage-a), followed by an introduction to the automatic annotation method for classifier training data based on masking strategies (Figure~\ref{annotate}).

\subsubsection{Calculation of Input Information Completeness}
\label{info_completeness}

Although smaller language models can evaluate the complexity of input questions and make retrieval decisions~\cite{jeong2024adaptive}, they struggle with long, complex medical diagnostic contexts. These models often rely on superficial features rather than semantic understanding when processing extensive inputs. Training larger models specifically for this purpose~\cite{asai2023self} is resource-intensive and contradicts RAG paradigm objectives.

To address this limitation, this chapter segments the input $\mathcal{Q}$ into manageable text units (defaulting to sentences): $\mathcal{Q} = \{s_i\}_{i=1}^n$, and trains a language model $\mathsf{Classifier}$ to predict each unit's importance. As shown in the left half of Figure~\ref{workflow}.Stage.a
\begin{equation}
	l_i = \mathsf{Classifier}(s_i) \quad \forall i \in \{1, 2, ..., n\}
\end{equation}
This approach transforms complex document comprehension into simpler sentence-level tasks. Each text unit $s_i$ receives one of three labels $\{\mathsf{A, B, C}\}$: $\mathsf{A}$ for information critical to diagnostic decisions, $\mathsf{B}$ for information that positively contributes to retrieval without directly inferring the correct result, and $\mathsf{C}$ for relatively unimportant information.

Based on the classification results, we calculate the global information completeness of input $\mathcal{Q}$ as follows:
\begin{align}
	I_{\text{norm}}(\mathcal{Q}) & = \frac{1}{\alpha \cdot n} \sum_{i=1}^n \Big( \alpha \cdot \mathbb{I}(l_i = \mathsf{A}) \\ 
	& + \beta \cdot \mathbb{I}(l_i = \mathsf{B}) + \gamma \cdot \mathbb{I}(l_i = \mathsf{C}) \Big)
\end{align}
where $l_i$ is the classification result of text unit $s_i$, $\alpha$, $\beta$, and $\gamma$ are weights for the three category labels, and $\mathbb{I}(\cdot)$ is an indicator function that returns 1 when the condition is true and 0 otherwise. The denominator $\alpha \cdot n$ in the equation represents the maximum information completeness (when all sentences are classified as $\mathsf{A}$), serving as normalization. Inputs with more critical clues increase the LLM's potential for accurate diagnosis. When $I_{\text{norm}}$ exceeds $\theta_1$, the input contains sufficient information for direct diagnosis:
\begin{equation}
	\mathcal{D}_{final} = \mathrm{LLM}(\mathcal{Q}, prompt_{diag})
\end{equation}

If $I_{\text{norm}}$ falls between $\theta_1$ and $\theta_2$, the retrieval program activates (see Section~\ref{retrieval_process}). When $I_{\text{norm}}$ is below $\theta_2$, a warning signal is issued alongside normal retrieval and reasoning, indicating sparse critical information and potential misdiagnosis risk. This approach filters irrelevant information while determining retrieval necessity, enhancing retrieval quality and accuracy. These functions can be implemented using a relatively small language model without additional computational overhead.

\subsubsection{Automatic Annotation Method for Classifier Training Data Based on Masking Strategy}
\label{data_annotation_method}

In the first part of this subsection, we detailed the implementation approach of the retrieval decision optimization module based on input information completeness. However, due to the lack of annotated datasets meeting our requirements for training importance classification models, we propose a simple yet effective strategy to automatically construct and annotate training datasets. Inspired by dynamic token deletion from single-stage Weakly Supervised Rationale Extraction~\cite{jiang2023you}, we automatically annotate the importance category of each text unit by sequentially masking them, as illustrated in Figure~\ref{annotate}. 

For a given input $\mathcal{Q}$, we set the doctor's diagnostic result $\bar{\mathcal{R}}$ as the reference answer, then segment $\mathcal{Q}$ into multiple text units $\mathcal{Q} = [s_1, s_2, \ldots, s_n]$. We sequentially mask each text unit $s_i$ to obtain the masked input $\mathcal{Q'} = [s_1, s_2, \ldots, s_{i-1}, s_{i+1}, \ldots, s_n]$. The LLM then performs diagnostic reasoning based on both $\mathcal{Q}$ and $\mathcal{Q'}$ to generate predicted diagnoses:
\begin{align}
	\hat{\mathcal{D}} &= \mathrm{LLM}(\mathcal{Q}, {prompt}_{diag}) \label{eq:first} \\
	\hat{\mathcal{D'}} &= \mathrm{LLM}(\mathcal{Q'}, {prompt}_{diag}) \label{eq:second}
\end{align}
where $\hat{\mathcal{D}}$ and $\hat{\mathcal{D'}}$ represent the diagnostic results based on the complete and masked inputs, respectively. The prompt template ${prompt}_{diag}$ is detailed in Table~\ref{direct_diagnos} in Appendix~\ref{prompt_templates}. By comparing these diagnostic results with the standard answer $\bar{\mathcal{R}}$, we present two annotation strategies:

\textbf{Annotation Strategy (1).} If $\hat{\mathcal{D}} \approx \bar{\mathcal{R}}$ (the LLM makes correct predictions with complete input): If $\hat{\mathcal{D'}} \approx \bar{\mathcal{R}}$ also holds, indicating that masking $s_i$ does not significantly impact the reasoning process, then $s_i$ is labeled as $\mathsf{C}$ (non-critical information). If $\hat{\mathcal{D'}}$ differs from $\hat{\mathcal{D}}$ resulting in an incorrect diagnosis, $s_i$ is labeled as $\mathsf{A}$ (critical diagnostic information). This strategy is illustrated in the upper part of Figure~\ref{annotate}.

\textbf{Annotation Strategy (2).} If $\hat{\mathcal{D}} \neq \bar{\mathcal{R}}$ (the LLM cannot make correct predictions with complete input): In this case, we implement annotation by searching the knowledge base. We use \(s_i\) as the retrieval query with the BM25 method. If documents corresponding to the disease in $\bar{\mathcal{R}}$ can be retrieved, \(s_i\) is labeled as $\mathsf{B}$ (valuable diagnostic information). Otherwise, \(s_i\) is labeled as $\mathsf{C}$ (low importance). This strategy is illustrated in the lower part of Figure~\ref{annotate}.

\subsection{Knowledge Retrieval and Reranking Based on Document Segmentation and Mapping}
\label{retrieval_process}

Considering the complex structures, large context spans, and semantic discontinuities in clinical texts, we adapt the RAG process following Zhao et al.~\cite{zhao2024longrag}. This approach divides documents in the knowledge base $\mathsf{KB}$ into text chunks with length restrictions, using sentences as the minimum segmentation unit (details in Appendix~\ref{corpus}). Figure~\ref{workflow}.Stage-b illustrates our retrieval and reranking workflow.

Given an input text $\mathcal{Q} = \{s_i\}_{i=1}^n$ with $n$ sentences, we first perform sentence-level importance classification and calculate overall information completeness $I_{\text{norm}}$ as described in Section~\ref{info_completeness} (1). When $I_{\text{norm}}$ falls below a preset threshold, the retrieval module Stage-b activates. To optimize retrieval efficiency, we only retain sentences with $\mathsf{label}=\mathsf{A}$ and $\mathsf{label}=\mathsf{B}$, excluding those with $\mathsf{label}=\mathsf{C}$ (shown on the left side of Figure~\ref{workflow}.Stage-b). This exclusion is justified as $\mathsf{label}=\mathsf{C}$ sentences typically contain non-pathological descriptions that contribute minimally to retrieval and may introduce noise.

The retrieval algorithm operates on knowledge base $\mathsf{KB}$ through chunk-level retrieval and document-level reranking. Each sentence $s_i \in \mathcal{Q}$ serves as a query to retrieve the top $m$ relevant text chunks:
\begin{equation}
	\mathcal{C}_i = \mathsf{Retriever}(s_i, m) \quad \forall i \in \{1, 2, \dots, n\}
\end{equation}
where $\mathcal{C}_i = \{c_{i,j}\}_{j=1}^m$, and $c_{i,j}$ is the $j$-th chunk retrieved using $s_i$. All text chunk sets are merged into $\mathcal{C} = \bigcup_{i=1}^n \mathcal{C}_i$. Each chunk $c \in \mathcal{C}$ is mapped to its original document $doc \in \mathsf{KB}$. For each document ${doc}$, a score $S_{{doc}}$ counts the number of retrieved chunks from that document:
\begin{equation}
	S_{doc} = \sum_{c \in \mathcal{C}} \mathbb{I}(c \in {doc})
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function, returning 1 if $c$ belongs to $doc$ and 0 otherwise. Documents are reranked based on $S_{doc}$, and the top $k$ documents with highest scores are selected as the final retrieval results: $\mathcal{K}_{rerank} = \{{doc}_{(1)}, {doc}_{(2)}, \dots, {doc}_{(k)}\}$, where ${doc}_{(l)}$ represents the document with the $l$-th highest score.

\subsection{Knowledge Filtering and Diagnosis Generation Based on Prompt Guidance}

Despite optimizing the retrieval process, retrieved documents may not always be relevant, particularly in clinical diagnostic tasks requiring complex reasoning. Drawing inspiration from medical "differential diagnosis" procedures, where doctors examine potentially confusing diseases based on patient symptoms and test results, we designed a prompt template \(prompt_\textit{diff}\) to filter irrelevant information. This template guides the LLM to identify conflicts between patient information and document descriptions, determining which documents to retain. The process is illustrated in Figure~\ref{workflow}.Stage-c, with the complete prompt template detailed in Table~\ref{diff_prompt} in Appendix~\ref{prompt_templates}.

Given the reranked knowledge document set $\mathcal{K}_{rerank}=\{{doc}_{(1)}, {doc}_{(2)}, \dots, {doc}_{(k)}\}$, we filter documents by evaluating their relevance to the diagnosis. The filtering function $V(\mathcal{Q}, doc_{(i)}, prompt_\textit{diff})$ is defined as:
\begin{equation}
	V(\mathcal{Q}, doc_{(i)}, prompt_\textit{diff}) = 
	\begin{cases} 
	\text{True}, &\text{if } \langle\text{support}\rangle \\
	\text{False}, &\text{otherwise}
	\end{cases}
\end{equation}
where $\langle\text{support}\rangle$ represents the LLM output when provided with query $\mathcal{Q}$, document $doc_{(i)}$, and prompt template $prompt_\textit{diff}$. The term $\langle\text{support}\rangle$ indicates that the LLM determines $doc_{(i)}$ is critical for diagnosis. The final reference knowledge document set $\mathcal{K}^*$ retains only documents that satisfy the filtering condition:
\begin{equation}
	\mathcal{K}^* = \{doc_{(i)} \in \mathcal{K}_{rerank} \mid V(\cdot, doc_{(i)}, \cdot) = \text{True}\}
\end{equation}

The final RAG-based diagnostic process is formalized as:
\begin{equation}
	\mathcal{D}_{final} = \mathrm{LLM}(\mathcal{Q}, \mathcal{K}^*, prompt_{rag})
\end{equation}
where $prompt_{rag}$ represents the RAG-based diagnostic prompt template. The complete content of this prompt template can be found in Table~\ref{rag_diagnos} in Appendix~\ref{prompt_templates}.

\normalem
\begin{table*}[t]
\centering
\begin{tabular}{llllllllll}
\toprule
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{CMEMR} & \multicolumn{3}{c}{ClinicalBench} & \multicolumn{3}{c}{CMB-Clin} \\ \cmidrule(r){2-4} \cmidrule(r){5-7} \cmidrule(r){8-10}
\multicolumn{1}{c}{} & R~(\%)& P~(\%) & F1~(\%) & R~(\%) & P~(\%) & F1~(\%) & R~(\%) & P~(\%) & F1~(\%) \\ \midrule
\multicolumn{10}{l}{\textbf{\textit{Non Retrieval Methods}}} \\
%		Direct & 48.55 & 46.81 & 47.66 & 43.48 & 34.31 & 38.35 & 70.83 & 41.12 & 52.04 \\
		CoT& 49.09 & {48.56} & {48.82} & 44.12 & 34.09 & 38.46 & 68.03 & 42.27 & 52.14 \\
		SC-CoT & 49.49 & 48.21 & 48.84 & 42.74 & 33.41 & 37.50 & {69.27} & 43.43 & 53.39 \\
		ATP & {49.68} & 47.72 & 48.68 & 43.01 & 33.82 & 37.87 & \uline{70.83} & {44.73} & \textbf{54.83} \\ \midrule
		\multicolumn{10}{l}{\textbf{\textit{Standard Retrieval Methods}}} \\ 
		$\text{RAG}^\text{2}$ & 47.13 & 44.34 & 45.69 & 42.43 & 34.57 & 38.10 & 58.33 &35.29  & 43.98 \\
		LongRAG & {49.85} & 48.31 & 49.07 & \ul{44.65} & 35.02 & 39.25 & 69.44 & 41.32 & 51.81 \\
		\midrule
		\multicolumn{10}{l}{\textbf{\textit{Adaptive Retrieval Methods}}} \\
		DRAGIN & 47.09 & 46.92 &  47.00& 43.67 & {35.54} & {39.19} & 59.72 & 36.13 & 45.03 \\ 
%		Adaptive-RAG & 49.45 & 48.03 & 48.73 & 44.31 & 35.02 & 39.12 & 67.52 & \textbf{44.83} & 53.88 \\ 
		Adaptive-RAG & 50.23 & 48.35 & 49.27 & 42.23 & 34.61 & 38.04 & 65.37 & \textbf{45.20} & 53.44 \\ 
%		RAG$^2$& 47.13 & 44.34 & 45.69 & 42.43 & 34.57 & 38.10 & 58.33 &35.29  & 43.98 \\ 
		SEAKR  & 47.37 & 45.90 & 46.62 & 40.66 & 33.13 & 36.51 & 59.60 & 34.34 & 43.57 \\
		\hline
		FIND~(ours) & \textbf{53.42} & \ul{48.58} & \textbf{50.88}& \textbf{46.63} & \ul{36.24} & \textbf{40.79} & \textbf{71.62} & 42.74 & {53.53} \\ 
 \bottomrule
\end{tabular}
\caption{Experimental results on CMEMR, ClinicalBench and CMB-Clin datasets. Bold indicates the best performances and the second-best performances are underlined.}
\label{main_exp}
\end{table*}

% \section{Experiments}
\section{Experimental Setup}
\subsection{Datasets}
We evaluated our framework using three Chinese EMR datasets: CMEMR~\cite{mediKAL}, ClinicalBench~\cite{yan2024clinicalbench}, and CMB-Clin~\cite{cmb}, to assess its ability in analyzing complex clinical information and making accurate diagnoses.

\textbf{CMEMR}~\cite{mediKAL} is derived from a Chinese medical website\footnote{\url{https://bingli.iiyi.com/}}, focusing on real-world clinical diagnosis challenges. \textbf{ClinicalBench}~\cite{yan2024clinicalbench} includes data from Grade 3A hospitals in China, spanning various medical departments and diseases. \textbf{CMB-Clin}~\citep{cmb} consists of 74 high-quality, complex EMRs, each containing multiple medical QA pairs.

For consistency with our task, we simplified the diverse medical tasks in ClinicalBench and CMB-Clin to focus solely on disease diagnosis.

\subsection{Baseline Methods}
We compare our approach with three categories of methods. Details of all the baselines below are shown in Appendix~\ref{baseline}.

\noindent\textbf{Non-Retrieval methods}: We include Chain-of-Thought (CoT)~\cite{wei2022chain}, Self-Consistent Chain of Thought(Sc-CoT)~\cite{llmdd113_wang2022self} and Atypical Prompting~\cite{Atypical}.

\noindent\textbf{Standard-Retrieval methods}: We include two representative RAG methods: $\text{RAG}^\text{2}$~(Rationale-Guided RAG)\cite{sohn2024rationale} and LongRAG~\cite{zhao2024longrag}.

% This paradigm improves retrieval flexibility and controllability by presetting conditions or introducing other models. 
\noindent\textbf{Adaptive-Retrieval methods}: We include Adaptive-RAG~\cite{jeong2024adaptive}, DRAGIN~\cite{su2024dragin}, and SEAKR~\cite{yao2024seakr}.

\subsection{Evaluation Metric}
Following \citep{fan2024ai-hospital}, we use the International Classification of Diseases (ICD-10) \citep{percy1990ICD} to standardize disease terminologies. We extract disease entities from diagnostic results and EMR labels, then perform fuzzy matching with a threshold of 0.5 to link them to ICD-10, creating normalized sets $S_{\mathcal{\hat{D}}}$ and $S_{\mathcal{R}}$. These sets are used to calculate set-level metrics Precision, Recall, and F1-score. Details are shown in Appendix~\ref{metrics}.

\subsection{Implementation Details}
We choose qwen2.5-7B-instruct as the backbone model for inference in our experiments by default. For the classifier we choose Mengzi-T5-base~\cite{zhang2021mengzi}. During retrieval we use BM25 as default retriever. For the external knowledge corpus we use CMKD~(Clinical Medicine Knowledge Database)\footnote{\url{http://cmkd.juhe.com.cn/}}. Detailed settings of each module and hyperparameters are provided in Appendix~\ref{implementation_detail}.
% \textbf{Backbone LLMs}: We employ qwen2.5-7B-instruct as the default backbone inference model by default.

% \noindent\textbf{Classifier}: We adopt Mengzi-T5-base~\cite{zhang2021mengzi} as our base model of Classifier. Specifically, the classifier is trained using the epoch that shows the best performance until 100 training iterations from the validation set, with the learning rate of 3e-5 and the AdamW~\cite{AdamW} as an optimizer. For the training data of the classifier, we sampled and annotated 5\% of the EMR samples from each department across the three datasets. Note that these EMR samples used for training the classifier are excluded from the evaluation set during experiment.

% \noindent\textbf{Retriever}: We adopt BM25~\cite{bm25} as our retrieval model, which demonstrated its superior performance in RAG, even outperforming some dense retrieval models. We also explored the impact of replacing BM25 with several SOTA dense retrieval method including multilingual-e5-large~\cite{E5} and BGE-m3~\cite{bge}, which are trained through contrastive learning on large-scale text pairs. We also include CoROM, which is trained on medical domain data from MultiCPR~\cite{corom}.

% \noindent\textbf{External Knowledge Corpus}: We adopt CMKD\footnote{\url{http://cmkd.juhe.com.cn/}} as our external knowledge corpus, which contains semi-structured knowledge documents of 5200 diseases. By default, we set the chunk size to 200
% words after segmenting the documents of CMKD. Examples and Other details of the corpus are shown in the appendix~\ref{corpus}.
% For a fair comparison.

\section{Results and Analyses}
\subsection{Overall Performance}
Our experiments evaluate the framework against baselines on three Chinese EMR datasets. Table~\ref{main_exp} highlights key findings:

(1) FIND demonstrates consistent performance across all benchmark datasets, achieving optimal or near-optimal F1 scores compared to baseline methods.

(2) Compared to LongRAG, a superior conventional retrieval approach, FIND improves Set-level F1 values by 1.81\%, 1.54\%, and 1.72\% respectively on the three datasets. This indicates that standard RAG methods without retrieval decision optimization rely excessively on knowledge base quality in complex disease diagnosis scenarios. They initiate retrieval even when LLMs can independently complete diagnoses, reducing efficiency and potentially introducing errors. 
% This limitation could only be overcome with extremely high-quality external knowledge sources.

(3) FIND outperforms other adaptive RAG methods significantly. Compared to the best-performing Adaptive-RAG method, FIND exhibits enhanced robustness when handling structurally complex and long context inputs due to its adaptive decision-making based on local-to-global information completeness calculations. Most other baselines, on the other hand, are  designed primarily for simpler question answering tasks, so their performance fluctuations when applied to disease diagnosis without appropriate adaptations.

\subsection{Ablation Study}

\begin{table}[h]
	\centering
	\caption{Ablation study on CMEMR dataset. \emph{w/o} denotes  removing the corresponding module.}
	\begin{tabular}{lccc}
		\toprule
		Method & R~(\%) & P~(\%) & F1~(\%) \\ \hline
		FIND & \textbf{53.42} & \textbf{48.58} & \textbf{50.88}  \\
%		\emph{w/o} Control & 51.57 & 47.21 & 49.29\\
%		\emph{w/o} Retriv & 50.62 & 47.22 & 48.86 \\
		\emph{w/o} Decision & 49.74 & 46.52 & 48.07 \\
		\emph{w/o} Chunk & 52.26 & 47.53 & 49.78 \\
		\emph{w/o} M-rerank & 52.22 & 47.20 & 49.59 \\
		\emph{w/o} Diff & 52.70 & 47.29 & 49.85 \\
		% Deepseek &  &  &  \\ \hline
% diff跑完了，class没跑完，retrieval要拆分
		\bottomrule
	\end{tabular}
	\label{idea2_ablation}
\end{table}
To analyze the contribution of different modules in FIND to its performance, we conducted ablation experiments on the CMEMR dataset: (a) \emph{w/o} Decision: removing the retrieval decision optimization module; (b) \emph{w/o} Chunk: replacing FIND's document segmentation and mapping-based knowledge retrieval with direct retrieval of complete documents; (c) \emph{w/o} M-rerank (Mapping-based Rerank): replacing FIND's text chunk mapping-based reranking with the bge-reranker-v2-m3 model; (d) \emph{w/o} Diff: removing the LLM knowledge filtering module based on differential diagnosis prompting. The results are shown in Table~\ref{idea2_ablation}, leading to the following conclusions:

(1) Without the retrieval decision optimization module, FIND's F1 value dropped by 2.81\%. This occurs because all inputs undergo retrieval indiscriminately, forcing samples that LLM could diagnose independently to undergo unnecessary retrieval, reducing efficiency and increasing error risk from irrelevant information.

(2) Replacing FIND's document segmentation and mapping-based retrieval with original retrieval decreased F1 by 1.1\%. This demonstrates that general RAG methods struggle with sparse information distribution and semantic incoherence in clinical texts, hampering accurate matching between inputs and knowledge base documents.

(3) Substituting FIND's reranking method with the bge-reranker-v2-m3 model reduced performance, validating FIND's reranking design. FIND's approach relies solely on numerical calculations from retrieval results without additional models, reducing memory overhead while maintaining higher compatibility with the retrieval workflow.

(4) Removing the differential diagnosis-based knowledge filtering mechanism meant all retrieved documents were provided to the LLM without discrimination. This increased the difficulty of LLM's reasoning and raised the probability of exceeding input length limits, negatively impacting overall performance.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.85\columnwidth}
		\captionsetup{justification=centering} %子图caption居中
		\begin{minipage}[b]{\linewidth}
			\includegraphics[width=1\linewidth]{figures/time_en.pdf}
			\caption{Average Time Cost on CMEMR dataset.}
		\end{minipage}
	\end{subfigure}
	
	\vspace{0.8em}
	
	\begin{subfigure}[t]{0.85\columnwidth}
		\captionsetup{justification=centering} %子图caption居中
		\begin{minipage}[b]{1\linewidth}
			\includegraphics[width=1\linewidth]{figures/performance_en.pdf}
			\caption{Diagnostic Performance on CMEMR dataset.}
		\end{minipage}
	\end{subfigure}
	\caption{A Comparative Analysis of Computational Time Expenditure and Diagnostic Performance Between the Proposed Method and Selected Baseline Methods on the CMEMR Dataset.}
	\label{time_and_performance}
\end{figure}

\subsection{Analysis of Retrieval Decision Optimization Effects}
This section analyzes the retrieval decision optimization module based on input information completeness. The module aims to leverage LLMs' inherent capabilities to complete tasks independently when diagnostic information is sufficient, without activating retrieval. This approach enhances efficiency, reduces time consumption, and prevents performance degradation from unnecessary retrieval noise. We compare our method with other retrieval-based baselines in terms of efficiency and diagnostic performance, as shown in Figure~\ref{time_and_performance}. Based on the experimental results, the following conclusions can be drawn:

(1) As illustrated in Figure~\ref{time_and_performance}.a, our method demonstrates significant time efficiency advantages compared to non-adaptive RAG methods ($\text{RAG}^\text{2}$ and LongRAG), reflecting the improvements from decision optimization.

(2) Compared to adaptive RAG methods (SEAKR, DRAGIN, and Adaptive-RAG), our approach shows competitive time consumption, only slightly higher than Adaptive-RAG but lower than SEAKR and DRAGIN. Unlike SEAKR and DRAGIN, which require access to LLMs' output probability distributions, our method maintains adaptability for closed-source LLMs and API-based deployments. While both Adaptive-RAG and our method employ classifiers for decision optimization, our BERT-Base classifier (110M parameters) is more lightweight than Adaptive-RAG's T5-Large (770M parameters).

(3) Figure~\ref{time_and_performance}.b demonstrates that our method achieves superior diagnostic performance. Overall, the proposed approach better balances efficiency and performance compared to baseline methods.



% \multicolumn{1}{l}{\multirow{2}{*}{Model}} & Classification Performance & \multicolumn{3}{c}{Diagnostic Performance} \\ \cmidrule(r){2-2} \cmidrule(r){3-5} 
		% \multicolumn{1}{c}{} & Acc~(\%) & R~(\%) & P~(\%) & F1~(\%) \\ \hline




\subsection{Case Study}
Table~\ref{case_study} in Appendix~\ref{case_study_appendix} presents representative case studies demonstrating the practicality of our proposed FIND. The results show that FIND conducts fine-grained importance assessment of patient information, accurately determining if the current data suffices for diagnosis and initiating retrieval when appropriate. Unlike previous Adaptive-Retrieval methods, our approach warns when $I_{norm}$ falls below a threshold, indicating potential diagnostic failure. This meets clinical requirements for balancing accuracy and reliability, highlighting practical significance.

\section{Conclusion}
In this paper, we propose FIND, an adaptive retrieval decision optimization method for disease diagnosis that addresses the rigid retrieval strategy issue in traditional retrieval-augmented methods. FIND establishes a decision mechanism based on input information completeness to flexibly determine retrieval necessity, and introduces a retrieval and reranking strategy using document segmentation and mapping.
% , while incorporating professional physicians' differential diagnosis thinking into prompt templates. This guides LLMs to dynamically filter and validate retrieved documents, enhancing knowledge quality and relevance.
Experimental results demonstrate FIND's strong adaptability in complex clinical scenarios. Future work may explore further optimization of the retrieval process and FIND's application to other medical tasks.
% , achieving significant improvements in diagnostic accuracy and reliability compared to traditional RAG methods and existing adaptive approaches

\section*{Limitations}
Although our classification data annotation strategy is straightforward and effective, it still exhibits certain shortcomings in practical application. Due to the potential presence of repetitive content within the input patient information, LLMs may still arrive at a correct diagnosis even after masking a critical sentence. This can result in inaccurate annotation labels, necessitating manual inspection and revision on top of our proposed automatic annotation strategy. Moreover, clinical medical texts, particularly EMRs, often contain abbreviations, synonyms, and aliases. And the manner in which identical patient information is recorded can vary significantly among different physicians, leading to a high degree of inconsistency. This issue to some extent hampers the search accuracy of our retrieval system. In the future, we aim to explore more effective preprocessing strategies for medical texts to enhance retrieval quality.


% \begin{table*}[ht]
% \centering
% \begin{tabular}{|c|p{5cm}|p{6cm}|p{6cm}|}
% \hline
% \textbf{Dataset} & \textbf{Question} & \textbf{Adaptive Retrieval} & \textbf{Adaptive-RAG (Ours)} \\
% \hline
% \textbf{NQ} & Which famous corporate logo changed to a flat colour/color sans serif font in its first major change since 1999? & Query Type: B (Single-step Approach) \newline \textbf{Reasoning:} The famous corporate logo that changed to a flat color sans-serif font in its first major change since 1999 is \textcolor{red}{Microsoft}. Microsoft changed its logo in 2012, replacing the italicized, multi-colored font with a simple, flat, grey font. So the answer is: \textcolor{blue}{Microsoft}. & Query Type: A (Non Retrieval) \newline \textbf{Reasoning:} The famous corporate logo that changed to a flat color sans serif font in its first major change since 1999 is \textcolor{blue}{Google}. Google changed its logo in 2015. So the answer is: \textcolor{blue}{Google}. \\
% \hline
% \textbf{MuSiQue} & Who is the child of the Italian navigator who explored the eastern coast of the continent César Gaytan was born in for the English? & Query Type: A (Non Retrieval) \newline \textbf{Reasoning:} César Gaytan was born in the continent of \textcolor{blue}{North America}. The Italian navigator who explored the eastern coast of North America for the English was \textcolor{blue}{Giovanni Caboto}, also known as John Cabot. So the answer is: \textcolor{blue}{Giovanni Caboto/John Cabot}. & Query Type: C (Multi-step Approach) \newline \textbf{Reasoning:} César Gaytan was born in \textcolor{blue}{Guadalajara, Jalisco, Mexico}. The Italian navigator who explored the eastern coast of the continent for the English is \textcolor{blue}{John Cabot}. John Cabot’s son is \textcolor{blue}{Sebastian Cabot}. So the answer is: \textcolor{blue}{Sebastian Cabot}. \\
% \hline
% \end{tabular}
% \caption{Case study with GPT-3.5 (Turbo), where we represent the factual error in red and the accurate information in blue.}
% \end{table*}


% Please add the following required packages to your document preamble:
% \usepackage{multirow}



% \section*{Acknowledgments}

% This document has been adapted
% by Steven Bethard, Ryan Cotterell and Rui Yan
% from the instructions for earlier ACL and NAACL proceedings, including those for
% ACL 2019 by Douwe Kiela and Ivan Vuli\'{c},
% NAACL 2019 by Stephanie Lukin and Alla Roskovskaya,
% ACL 2018 by Shay Cohen, Kevin Gimpel, and Wei Lu,
% NAACL 2018 by Margaret Mitchell and Stephanie Lukin,
% Bib\TeX{} suggestions for (NA)ACL 2017/2018 from Jason Eisner,
% ACL 2017 by Dan Gildea and Min-Yen Kan,
% NAACL 2017 by Margaret Mitchell,
% ACL 2012 by Maggie Li and Michael White,
% ACL 2010 by Jing-Shin Chang and Philipp Koehn,
% ACL 2008 by Johanna D. Moore, Simone Teufel, James Allan, and Sadaoki Furui,
% ACL 2005 by Hwee Tou Ng and Kemal Oflazer,
% ACL 2002 by Eugene Charniak and Dekang Lin,
% and earlier ACL and EACL formats written by several people, including
% John Chen, Henry S. Thompson and Donald Walker.
% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\normalem
\bibliography{ref}

\appendix
% \section{Details of Datasets Settings}
% Following \citep{yan2024clinicallab} and \cite{jia2024medikal}

\section{Details of Baseline Methods}
\label{baseline}
In this section, we provide a detailed introduction to the three categories of baseline methods used in this paper, namely Non-Retrieval methods, Standard-Retrieval methods and Adaptive-Retrieval methods, Their methodological descriptions and implementation details are listed below. 

\subsection{Non-Retrieval Methods}

These methods do not rely on external knowledge, but rather leverage the internal knowledge of LLMs through prompt optimization for reasoning. This chapter selects the classic Chain of Thought (CoT)~\cite{llmdd112_wei2022chain} and Self-Consistent Chain of Thought (SC-CoT)~\cite{llmdd113_wang2022self} as baselines. Additionally, we include a method called Atypical Prompting (ATP)~\cite{Atypical}, which is designed specifically for the medical domain and enhances reasoning capabilities by focusing on non-typical factors such as scenarios and symptoms.

\subsection{Standard-Retrieval Methods}
In this category, "Standard" corresponds to "Adaptive" mentioned later, referring to the RAG method that initiates retrieval for all inputs uniformly without Adaptive settings. This paper selects two representative baselines: $\text{RAG}^\text{2}$ (Rationale-Guided RAG)~\cite{sohn2024rationale} and LongRAG~\cite{zhao2024longrag}.

$\text{RAG}^\text{2}$ (Rationale-Guided RAG)~\cite{sohn2024rationale} enhances the original input by utilizing rationales generated by LLM based on the input question, and then performs subsequent retrieval operations with the enhanced input. This approach was tested on medical question-answering tasks.

LongRAG~\cite{zhao2024longrag} designs a strategy that integrates global information perspective and factual detail perspective for long-text retrieval tasks. It improves the overall understanding and processing capability for long texts by prompting LLM to extract global information and analyze retrieved document information.

\subsection{Adaptive-Retrieval Methods} 
This paradigm enhances retrieval flexibility and controllability by presetting conditions or introducing additional models. Retrieval is only activated when the input meets preset conditions; otherwise, results are directly inferred by the LLM. We selected three representative baselines, including Adaptive-RAG~\cite{jeong2024adaptive}, DRAGIN~\cite{su2024dragin}, and SEAKR~\cite{yao2024seakr}.

{Adaptive-RAG}~\cite{jeong2024adaptive} labels training data based on the correctness of LLM responses to certain samples, and trains a classification model to determine the complexity of multi-hop question answering problems to decide whether to perform retrieval.

{DRAGIN}~\cite{su2024dragin} measures uncertainty by calculating the entropy of token probability distributions, utilizing the Transformer's self-attention mechanism to quantify the influence of tokens on subsequent content.

%\textbf{RAG$^2$}~\cite{sohn2024rationale} implements adaptive control over retrieved documents through a filtering model trained on perplexity labels to retain or discard documents.

{SEAKR}~\cite{yao2024seakr} introduces self-aware uncertainty, determining whether to activate the retrieval model based on this value.

\subsection{Settings of Baseline Methods}
To ensure a fair comparison, we implement all baseline methods using the same backbone LLM, retriever, and external knowledge corpus by default. For baseline methods that require training a classifier (RAG$^2$~\cite{sohn2024rationale} and Adaptive-RAG~\cite{jeong2024adaptive}), we adopt the same language model as used in our framework, namely Mengzi-T5-base~\cite{zhang2021mengzi}.
% 附录
% Adaptive-RAG~\cite{jeong2024adaptive} trains a filtering model with labels obtained from correctly answered queries with and without RAG. DRAGIN~\cite{su2024dragin} evaluating the uncertainty of each token to activate the retrieval model. SEAKR~\cite{yao2024seakr} introduce self-aware uncertainty to decide whether activating the retrieval model based on its value. 

\section{Implementation Details}
\label{implementation_detail}

\subsection{Details of External Knowledge Corpus}
\label{corpus}
We employ the CMKD (Clinical Medicine Knowledge Database) as the external knowledge corpus. Knowledge documents for all 5,200 diseases were obtained from the official website. An example is provided in Chinese (Figure~\ref{fig:corpus_example_zh}) and English (Figure~\ref{fig:corpus_example_en}) version. Following~\cite{zhao2024longrag}, we preprocess the documents in the knowledge base by segmenting them into chunks prior to retrieval. 

Specifically, we impose a length constraint on the chunks, using sentences as the minimum segmentation unit. A sliding window is then applied to extend the context by merging overlapping content from the end of the previous sentence, thereby preventing semantic discontinuity at truncation points. Short chunks at the end of a document are merged with preceding chunks to ensure better semantic coherence. Furthermore, since the knowledge documents for each disease are inherently semi-structured, containing fixed fields such as "Etiology," "Clinical Manifestations," "Laboratory Tests," and "Other Auxiliary Examinations," we terminate the current chunk at the end of the text corresponding to each field during the segmentation process. By default, we set the chunk size to 200 words after segmenting the documents of CMKD.

\subsection{Details of the Classifier}
We adopt BERT-Base-Chinese~\cite{llmsv3_devlin2018bert} as the foundation model for our classifier, training it for 2 epochs with a learning rate of 3e-5 and AdamW~\cite{AdamW} optimizer. For training data, we extract samples from existing medical record datasets. Since the CMEMR dataset provides comprehensive departmental coverage and is significantly larger than ClinicalBench and CMB-Clin datasets, we sample 5\% of CMEMR records according to departmental proportions ($\textit{CMEMR}_{subset}$, 516 samples) for entity weight calculation. These samples are completely excluded from subsequent experiments, with testing conducted only on the remaining 95\% of CMEMR. For ClinicalBench and CMB-Clin datasets, all samples are used for evaluation.

In the annotation process, we follow the strategy described in Section~\ref{data_annotation_method}. For scenario (1), even when the LLM makes correct predictions after removing sentence $s_i$, this sentence may still contain valuable information due to content redundancy across sections (e.g., symptoms appearing in both chief complaint and present illness history). To prevent information loss and annotation errors, we implement an additional retrieval step for sentences labeled as $\mathsf{label}=\mathsf{C}$. If documents corresponding to diseases in $\bar{\mathcal{R}}$ can be retrieved using $s_i$, we update its label to $\mathsf{B}$.

\subsection{HyperParameters}
During inference, we set the maximum generation length of the LLM to 2048. To ensure reproducibility, we set \texttt{do\_sample} to \texttt{False} by default.

When calculating the information density based on the classifier's predictions, the weights \(\alpha\), \(\beta\), and \(\gamma\) for labels \(\mathsf{A}\), \(\mathsf{B}\), and \(\mathsf{C}\) are set to 1.0, 0.5, and 0.1, respectively. The thresholds \({\theta}_1\) and \({\theta}_2\) are set to 0.3 and 0.1, respectively.

For the retrieval process, the number of chunks \(m\) retrieved for each sentence \(s_i\) is set to 100, and the number of documents \(k\) after chunk-to-document mapping and re-ranking is set to 5. During retrieval, chunks with a similarity score below 50\% to the given query \(s_i\) are discarded.

\section{Evaluation Metrics Calculation}
\label{metrics}
The unique nature of disease diagnosis tasks makes it unreasonable to directly determine whether a disease $\mathcal{R}_i$ in reference labels is identical to a disease $\mathcal{D}_i$ in LLM predictions. For example, if the correct diagnosis is $\mathcal{R}_i = \text{"allergic purpura"}$, both predictions $\mathcal{D}_{i1} = \text{"purpura"}$ and $\mathcal{D}_{i2} = \text{"eczema"}$ would be incorrect, yet the former is clearly closer to the correct answer.

Additionally, patients often present with multiple concurrent diseases, resulting in LLM outputs that typically include multiple diagnoses. This transforms the evaluation into a comparison between two sets of diseases, making conventional text similarity metrics (Rouge, Bleu, etc.) unsuitable.

To enhance evaluation rigor, we follow Fan et al.~\cite{fan2024ai-hospital} by adopting the International Classification of Diseases (ICD-10)~\cite{percy1990ICD} to link natural language diagnoses with standardized clinical terminology. For predicted disease entities $\mathcal{\hat{D}}$ and reference diagnoses $\mathcal{R}$, we employ fuzzy matching (threshold 0.5) to map these entities to standardized disease sets $S_{\mathcal{\hat{D}}}$ and $S_{\mathcal{R}}$. This approach improves evaluation rationality, as in our example, both $\mathcal{R}_i = \text{"allergic purpura"}$ and $\mathcal{D}_{i1} = \text{"purpura"}$ would link to "purpura" in ICD-10, while $\mathcal{D}_{i2} = \text{"eczema"}$ would not, confirming $\mathcal{D}_{i1}$'s superiority.

Based on the above setup, this chapter redefines the following statistical values:

\textbf{True Positives (TP):}~ The number of standard disease terms in the prediction results $S_{\mathcal{\hat{D}}}$ that correctly correspond to the reference diagnosis $S_{\mathcal{R}}$.

\textbf{False Positives (FP):}~ The number of standard disease terms that appear in the prediction results $S_{\mathcal{\hat{D}}}$ but do not correctly match with the reference diagnosis $S_{\mathcal{R}}$.

\textbf{False Negatives (FN):}~ The number of standard disease terms that appear in the reference diagnosis $S_{\mathcal{R}}$ but are omitted in the prediction results $S_{\mathcal{\hat{D}}}$.

Finally, based on the above statistical values, this chapter can calculate set-level evaluation metrics for the two sets $S_{\mathcal{\hat{D}}}$ and $S_{\mathcal{R}}$: Set-level Recall, Set-level Precision, and Set-level F1 score:
\begin{align}
	&\text{Set-level }R=\frac{TP}{TP+FN}\ \\
	&\text{Set-level }P=\frac{TP}{TP+FP}\ \\
	&\text{Set-level }F{1}=\frac{2\times P\times R}{P+R}
\end{align}
In all experiments of this paper, any metrics related to "P", "R", and "F1" refer to the set-level metrics defined above.


\section{Detailed Experimental Results}

\subsection{The Effects of Different Classification Models}

To verify the universality and robustness of our proposed retrieval decision optimization module based on input information completeness across various classification models, we evaluated two additional pre-trained language models: Struct-BERT~\cite{structbert, structbert_zh_data} and T5~\cite{zhang2021mengzi}. We trained these models on our annotated data and assessed their text unit importance classification accuracy and final diagnostic performance. As shown in Table~\ref{classify_acc}, BERT-base achieved the highest classification accuracy (86.28\%), while the generative T5-base model performed slightly lower than the self-encoding architectures of BERT and StructBERT, despite having more parameters. Nevertheless, all models maintained robust classification performance, with trends consistent with their final diagnostic performance. These results demonstrate the strong cross-model adaptability and robustness of FIND's adaptive retrieval decision optimization module.

\begin{table}[h]
	\centering
	\setlength{\tabcolsep}{4pt}  % 减小列间距，默认值通常为6pt
	\caption{Performances of different classification models on CMEMR dataset. } 
	\begin{tabular}{lcccc}
		\toprule
            Model & Acc~(\%) & R~(\%) & P~(\%) & F1~(\%) \\
            \hline
		BERT	& \textbf{86.28} & \textbf{53.42} & {48.58} & \textbf{50.88}\\
		StructBERT	& 85.28 & 53.09 & \textbf{48.61} & 50.75 \\
		T5	& 84.34 & 52.50 & 48.20 & 50.25\\
		\bottomrule
	\end{tabular}
	\label{classify_acc}
\end{table}

To further analyze the rationality of FIND's text unit importance categorization, Figure~\ref{class_report} presents the confusion matrices for the three models. The matrices reveal that $\mathsf{label}$ $\mathsf{B}$ is frequently misclassified as $\mathsf{label}$ $\mathsf{A}$, while $\mathsf{label}$ $\mathsf{C}$ is rarely misclassified. This pattern is intuitive—models can easily distinguish between expressions like "normal diet and sleep" and "obvious purpura on both lower limbs" based on their importance difference. However, differentiating between similarly pathological information such as "obvious purpura on both lower limbs" and "multiple thyroid nodules" proves challenging. This indicates current limitations in distinguishing between decisive and important information, highlighting directions for future improvements.

\begin{figure*}[t]
	\centering
	\includegraphics[scale=0.32]{figures/class_report.pdf}
	\caption{Classification accuracy comparison between FIND and other two baseline methods Adaptive-RAG~\cite{jeong2024adaptive} and RAG$^2$~\cite{sohn2024rationale}. We also provide the confusion matrix across three labels (Right).}
	\label{class_report}
\end{figure*}

\subsection{The Effects of Different Retrievers}
To further investigate the retrieval module design and verify the universality of our method, we compare our approach with other RAG methods using multiple retrievers: the sparse retriever BM25 (our default choice), and three dense retrievers: E5~\cite{E5}, BGE-m3~\cite{bge}, and CoROM~\cite{corom}. Results in Table~\ref{retriever} show that our method achieves optimal performance across different retrievers with minimal variation (maximum difference of only 0.96\% in Set-level F1 scores). This validates the rationality of our document segmentation and mapping-based knowledge retrieval strategy, which reduces document-level search to simpler text segment matching tasks. Furthermore, it demonstrates that our adaptive control module can selectively retain or filter information without additional overhead, maintaining high stability across different retrievers and facilitating broader application scenarios.

\begin{table}[h]
\scalebox{0.9}{
\begin{tabular}{lcccc}
\hline
Methods & bm25 & BGE & E5 & CoROM \\ \hline
$\text{RAG}^\text{2}$ & 45.69 & 42.80 & 46.64 &  44.97 \\
LongRAG  & 49.07 & 48.93 & 49.15 & 48.24 \\
\hline
Adaptive-RAG & 49.27 & {48.19} & 48.83 &  47.78 \\
%			RAG$^2$ & 45.69 & 42.80 & 46.64 &  44.97\\
DRAGIN & 47.00 & 45.51 & 47.62 &  42.25\\
SEAKR & 46.62 & 43.07 & 44.31 & 45.18 \\ \hline
FIND(ours) & \textbf{50.88} & \textbf{50.21} & \textbf{51.17} & \textbf{50.34}\\
\hline
\end{tabular}}
\caption{Performance comparison (in F1-score) of using different retrievers on CMEMR dataset. Bold indicates the best performances.}
\label{retriever}
\end{table}

\subsection{Performance Analysis Across Different LLMs for Inference}
Table~\ref{backbone} demonstrates the performance of different foundation models as inference models on the CMEMR dataset. Considering that the official LLaMA models from metaAI perform poorly on Chinese tasks, we use the Chinese versions of LLaMA3\footnote{\url{https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat}} and LLaMA3.1\footnote{\url{https://huggingface.co/shenzhi-wang/Llama3.1-8B-Chinese-Chat}} released by Wang et al. for our experiments. The experimental results indicate that the inference capabilities of these LLMs significantly influence diagnostic performance in three aspects:

(1) Under the same conditions, larger models generally yield better performance. For instance, when the parameter size of the Qwen2.5 model increases from 7B to 14B, its performance on the CMEMR dataset improves by 3.08\%. (2) With the iterative upgrades of model versions, diagnostic performance also shows qualitative improvements. For example, from LLaMA3 to LLaMA3.1, the Set-level F1 increases by 4.26\%. (3) When pre-training corpora, training strategies, and model architectures differ, model performance varies accordingly. For instance, although GLM4-9B-Chat has 1$\sim$2B more parameters than Qwen2.5-7B and LLaMA3.1-8B, its actual diagnostic performance lags significantly behind the other two models.


\begin{table}[h]
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccc}
\toprule
Backbone & R~(\%) & P~(\%) & F1~(\%) \\ \hline
Qwen2.5-7B-Instruct & {53.42} & {48.58} & {50.88}  \\
Qwen2.5-14B-Instruct & \textbf{56.68} & \textbf{51.49} & \textbf{53.96}\\
%		& \textbf{55.57} & \textbf{51.21} &  \textbf{53.30}\\
%		Chatglm3-6B &  &  &  \\
GLM4-9B-Chat & 43.48 & 41.56 & 42.50 \\
LLaMA3-8B-Chinese & 46.66 & 38.87 & 42.41 \\
LLaMA3.1-8B-Chinese & 45.58 & 47.81 & 46.67 \\
% Deepseek &  &  &  \\ \hline
\bottomrule
\end{tabular}
\caption{Performance comparison of different inference LLMs on CMEMR dataset. Bold indicates the best performances.}
\label{backbone}
\end{table}

\subsection{Results on Different Clinical Departments}

\begin{figure}[hb]
	\centering
	\includegraphics[width=0.95\columnwidth]{figures/radar_chart_department.pdf}
	\caption{Diagnostic performance of FIND~(ours) and selected baseline methods on samples from major clinical departments in the CMEMR dataset.}
	\label{radar_char}
\end{figure}

To investigate FIND's performance in diagnostic tasks across different medical departments, we compared it with several representative baseline methods on samples from major departments in the CMEMR dataset. The results in Figure~\ref{radar_char} reveal that:

(1) FIND consistently outperforms other baseline methods using adaptive retrieval strategies across all departments, confirming its effectiveness in various departmental diagnostic tasks.

(2) All methods, including FIND, show relatively lower diagnostic performance in dermatology, oncology, and obstetrics and gynecology. This can be attributed to the high feature overlap in dermatological conditions, the heavy reliance on imaging information in oncology, and the unique nature of obstetric cases where routine pregnancy examinations are often analyzed through a pathological diagnostic framework. These observations provide valuable directions for future improvements.



\section{Case Study}
\label{case_study_appendix}


\section{Prompt Templates}
\label{prompt_templates}



\onecolumn

% 使用新的浮动环境
\begin{tcolorboxfloat}[t]
    \begin{tcolorbox}[colback=white, % 背景颜色
                      colframe=darkgray, % 边框颜色
                      width=\textwidth, % 宽度
                      arc=2mm, auto outer arc, % 圆角设置
                      breakable, % 可换行
                      title={Document Example (Chinese)}]
        \begin{CJK}{UTF8}{gkai} % UTF8 编码，字体为 gkai（简体楷体）
            \textcolor[RGB]{180, 101, 5}{【疾病名】}: 病毒性心肌炎 
            
            % \\[1\baselineskip] % 增加一行的间距
            \textcolor[RGB]{180, 101, 5}{【英文名】}: Viral Myocarditis 
            
            % \\[1\baselineskip]
            \textcolor[RGB]{180, 101, 5}{【ICD号】}: I41.0*
            
            \textcolor[RGB]{180, 101, 5}{【分类】}: 心血管内科

            \textcolor{blue}{概述}: 病毒性心肌炎(viral myocarditis)是一种与病毒感染有关的局限性或弥漫性炎症性心肌疾病，是最常见的感染性心肌炎。近年来随着检测技术的提高，发现多种病毒可引起心肌炎，其发病率呈逐年增高趋势，是遍及全球的常见病和多发病。

            \textcolor{blue}{流行病学}: ... 病毒性心肌炎可发生于各个年龄段，但从临床发病情况看以儿童和40岁以下成人居多。由于许多病毒感染具有明显的季节分布特点，如流感病毒感染多发生在冬季，而肠道病毒感染则多发生于夏秋季，因此，病毒性心肌炎的发病也具有明显的季节特征，夏秋季发病率较高，冬春季较少。...

            \textcolor{blue}{病因}: ...目前已证实能引起心肌炎的病毒包括：(1)小核糖核酸病毒：肠道病毒如柯萨奇(Coxsackie)、埃可(ECHO)、脊髓灰质炎病毒、鼻病毒等；(2)虫媒病毒：如黄热病毒、登革热病毒、白蛉热病毒、流行性出血热病毒等；...

            \textcolor{blue}{临床表现}: ... 在临床就诊的患者中，90\%左右以心律失常为主诉或首发症状，常诉心悸、乏力、胸闷、头晕等，严重者可出现晕厥或阿-斯综合征。部分患者可有程度不一的胸痛，其原因可能有：...

            \textcolor{blue}{实验室检查}: ...血清心肌肌钙蛋白I(cTnI)或肌钙蛋白T(cTnT)增高(以定量测定为准)有较大价值。...

            \textcolor{blue}{其他辅助检查}: ...X线检查  约1/4病人有不同程度心脏扩大，搏动减弱，其扩大程度与心肌损害程度一致，有时可见心包积液(病毒性心肌心包炎)，严重病例因左心功能不全有肺淤血或肺水肿征象。...

            \textcolor{blue}{鉴别诊断}: 1.风湿性心肌炎  有典型风湿热表现者，则两者鉴别不难，一般可从以下几点作鉴别：风湿性心肌炎常有扁桃体炎或咽峡炎等链球菌感染史，抗“O”增高，血沉降多明显增快，C反应蛋白(CRP)阳性，心电图改变以P-R间期延长较常见，咽拭物培养常有链球菌生长，且多有大关节炎，鉴于风湿性心肌炎常有心内膜炎，因此二尖瓣反流性收缩期杂音多较明显，且可因瓣膜水肿、炎症出现舒张期杂音(Carey Coombs杂音)，若心脏扩大不明显，而杂音较响亮，则风湿性可能性更大。...

            \textcolor{blue}{治疗}:  (1)应用改善心肌细胞营养与代谢药物：该类药物包括维生素C、维生素B、辅酶A 50～100U或肌苷200～400mg，每天肌内注射或静脉注射1～2次；细胞色素C 15～30mg，每天静脉注射1～2次，该药应先皮试，无过敏者才能注射。...
            
        \end{CJK}
    \end{tcolorbox}
    \captionof{figure}{A document example from CMKD (in Chinese).} % 将 caption 放在 tcolorbox 后
    \label{fig:corpus_example_zh} % 添加引用标签
\end{tcolorboxfloat}

\onecolumn

\begin{tcolorboxfloat}[!t]
    \begin{tcolorbox}[colback=white, % 背景颜色
                      colframe=darkgray, % 边框颜色
                      width=\textwidth, % 宽度
                      arc=2mm, auto outer arc, % 圆角设置
                      breakable, % 可换行
                      title={Data Example (English)}]

        \textcolor[RGB]{180, 101, 5}{[Disease Name]}: Viral Myocarditis  

        \textcolor[RGB]{180, 101, 5}{[English Name]}: --
        
        \textcolor[RGB]{180, 101, 5}{[ICD Code]}: I41.0*
        
        \textcolor[RGB]{180, 101, 5}{[Classification]}: Cardiovascular Medicine
        
        \textcolor{blue}{Overview}: Viral myocarditis is a localized or diffuse inflammatory myocardial disease associated with viral infections, and it is the most common infectious myocarditis. In recent years, with the improvement of detection techniques, it has been found that various viruses can cause myocarditis, and its incidence has been increasing year by year, making it a common and frequently occurring disease worldwide.

        \textcolor{blue}{Epidemiology}: ... Viral myocarditis can occur in all age groups, but clinically, it is more common in children and adults under 40 years old. Since many viral infections have distinct seasonal distribution characteristics, such as influenza virus infections occurring mostly in winter and enterovirus infections occurring mostly in summer and autumn, the incidence of viral myocarditis also has obvious seasonal characteristics, with higher incidence in summer and autumn and lower incidence in winter and spring....
        
        \textcolor{blue}{Etiology}: ... It has been confirmed that viruses that can cause myocarditis include: (1) Picornaviruses: enteroviruses such as Coxsackie, ECHO, poliovirus, rhinovirus, etc.; (2) Arboviruses: such as yellow fever virus, dengue virus, sandfly fever virus, epidemic hemorrhagic fever virus, etc.;...
        
        \textcolor{blue}{Clinical Manifestations}: ... Among patients who seek clinical consultation, about 90\% report arrhythmia as their main complaint or initial symptom, often complaining of palpitations, fatigue, chest tightness, dizziness, etc. In severe cases, syncope or Adams-Stokes syndrome may occur. Some patients may experience varying degrees of chest pain, which may be due to:...
        
        \textcolor{blue}{Laboratory Tests}: ... Elevated serum cardiac troponin I (cTnI) or troponin T (cTnT) (based on quantitative measurements) is of significant value....

        \textcolor{blue}{Other Auxiliary Examinations}: ... X-ray examination shows that about 1/4 of patients have varying degrees of cardiac enlargement and weakened pulsations, with the degree of enlargement consistent with the degree of myocardial damage. Sometimes pericardial effusion (viral myopericarditis) can be seen, and severe cases may show signs of pulmonary congestion or pulmonary edema due to left heart dysfunction....
        
        \textcolor{blue}{Differential Diagnosis}: 1. Rheumatic myocarditis: For those with typical rheumatic fever manifestations, the differentiation is not difficult. Generally, the following points can be used for differentiation: rheumatic myocarditis often has a history of streptococcal infections such as tonsillitis or pharyngitis, elevated anti-streptolysin O (ASO), significantly increased erythrocyte sedimentation rate (ESR), positive C-reactive protein (CRP), and common ECG changes such as prolonged P-R interval. Throat swab cultures often grow streptococci, and there is often polyarthritis. Since rheumatic myocarditis often involves endocarditis, the systolic murmur of mitral regurgitation is usually more pronounced, and diastolic murmurs (Carey Coombs murmur) may appear due to valve edema and inflammation. If the heart is not significantly enlarged but the murmur is loud, rheumatic myocarditis is more likely....
        
        \textcolor{blue}{Treatment}: (1) Use of drugs that improve myocardial cell nutrition and metabolism: These drugs include vitamin C, vitamin B, coenzyme A 50\textasciitilde100U, or inosine 200\textasciitilde400mg, administered intramuscularly or intravenously once or twice daily; cytochrome C 15\textasciitilde30mg, administered intravenously once or twice daily. This drug should be skin-tested first, and only those without allergies can be injected....

    \end{tcolorbox}
    \captionof{figure}{A document example from CMKD (translated).} % 将 caption 放在 tcolorbox 后
    \label{fig:corpus_example_en} % 添加引用标签
\end{tcolorboxfloat}

\twocolumn

\begin{table*}[!h]
\centering

\begin{tabular}{p{0.9\textwidth}}

\toprule
\hline
\textcolor{blue}{\textbf{Case 1: Non-retrieval}}\\
\hline
\textbf{[Patient Info]:} \\<Chief Complaint>: \blueuline{Pain in the right upper abdomen for 2 days}...  
<History of Present Illness>: ...\blueuline{Persistent pain with paroxysmal exacerbation}, \blueuline{accompanied by nausea and vomiting (vomitus consisted of gastric contents)}, \blueuline{as well as abdominal distension and poor appetite}...  
<Physical Examination>: ...\greenuline{No abdominal muscle tension or palpable masses}...  
<Auxiliary Examination>: ...\reduline{Color ultrasound indicates gallbladder sludge and stones}...\\
\textbf{[$I_{norm}$]:} 0.63 \hspace{1cm} \textbf{[Activate\_Retreival]:} False \hspace{1cm} 
\textbf{[Raise\_Warning]:} False \\
\textbf{[LLM Diagnosis]:} Gallstones and acute cholecystitis (\textcolor{darkgreen}{$\checkmark$})\\

\hline
\midrule

\textcolor{blue}{\textbf{Case 2: Retrieval}}\\
\hline
\textbf{[Patient Info]:} \\ ...<History of Present Illness>: ...\blueuline{Previously treated at a local hospital with enteric-coated aspirin tablets and isosorbide mononitrate, but no significant improvement was observed}...<Physical Examination>: ...\greenuline{The heart rhythm is regular, and no pathological murmurs are heard in any of the valve auscultation areas}...  
<Auxiliary Examination>: ...\reduline{During the Bruce protocol exercise test, at 2 minutes and 14 seconds, tall tent-shaped T waves appeared in the precordial leads, accompanied by upsloping ST-segment elevation in the corresponding leads (Figure 2). Simultaneously, the patient experienced chest tightness}...\\
\textbf{[$I_{norm}$]:} 0.47 \hspace{1cm} \textbf{[Activate\_Retreival]:} True \hspace{1cm} 
\textbf{[Raise\_Warning]:} False \\
\textbf{[Retrieved Documents]:} ...Transient episodes of chest pain induced by exercise or other conditions that increase myocardial oxygen demand... In some patients with spontaneous angina, transient ST-segment elevation occurs during episodes, known as variant angina.  
New-onset exertional angina, worsening exertional angina, and spontaneous angina are often collectively referred to as "unstable angina."...\\
\textbf{[LLM Diagnosis]:} Coronary heart disease, unstable angina (\textcolor{darkgreen}{$\checkmark$})\\

\hline
\midrule

\textcolor{blue}{\textbf{Case 3: Warning}}\\
\hline
\textbf{[Patient Info]:} \\<Chief Complaint>: \blueuline{Recurrent pain in both knees for 8 years, worsening over the past month.}...  
<Past Medical History> \greenuline{Previously healthy, preoperative blood tests and coagulation function tests were normal, and color Doppler ultrasound of the arteries and veins of both lower limbs showed no abnormalities.} <Physical Examination>: ...\greenuline{No localized redness or swelling in the bilateral knee joints, with normal muscle tone.}...  \\
\textbf{[$I_{norm}$]:} 0.27 \hspace{1cm} \textbf{[Activate\_Retreival]:} True \hspace{1cm} \textbf{[Raise\_Warning]:} True\\
\textbf{[Retrieved Documents]:} \textcolor{gray}{Knee synovitis: The common sites of disease are the knee and hip joints, and the general symptoms are joint pain and significant limitation of movement...}\\
\textbf{[LLM Diagnosis]:} Knee synovitis (\textcolor{darkred}{\ding{56}})\\


\hline
\bottomrule

\end{tabular}
\caption{Case Study. For clarity, only part of the key information of the selected samples is presented. }
\label{case_study}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{0.9\textwidth}}
\toprule
\hline
[\textbf{Role}]<\textcolor{blue}{SYS}>\\
You are an outstanding AI medical expert. You can perform a preliminary disease diagnosis based on the patient's Information.\\
\hline
[\textbf{Role}]<\textcolor{blue}{USR}>\\
Below is a medical record summary of a patient from the \$\{department\}. Please act as the attending physician and provide a diagnosis based on your expertise and knowledge.

[Medical Record Summary]:\\
\#\#\#\\
\$\{summary\}\\
\#\#\#\\

[Requirements]:\\
1. You need to comprehensively analyze the patient's symptoms, medical visits, medical history, and various examination results.

2. Please provide your diagnosis using the following template.

[Output Template]:

Diagnosis: [Predicted Disease 1: [Disease Name 1]; Predicted Disease 2: [Disease Name 2]; ...; Predicted Disease n: [Disease Name n]]

Please strictly adhere to the output template and do not include any irrelevant information!\\
\hline
\bottomrule
\end{tabular}
\caption{The default prompt template for LLM direct diagnosis. The presence of a "\$" symbol indicates a placeholder variable that needs to be filled with specific content.}
\label{direct_diagnos}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{0.9\textwidth}}
\toprule
\hline
[\textbf{Role}]<\textcolor{blue}{SYS}>\\
You are an outstanding AI medical expert. You can perform a preliminary disease diagnosis based on the patient's Information.\\
\hline
[\textbf{Role}]<\textcolor{blue}{USR}>\\
Below is a medical record summary of a patient from the \$\{department\}. Please act as the attending physician and provide a diagnosis based on your expertise and knowledge.

[Medical Record Summary]:\\
\#\#\#\\
\$\{summary\}\\
\#\#\#\\

Additionally, by searching the medical knowledge base, you have identified several suspected diseases and have extracted relevant information from them as follows, for reference:

[Knowledge Document]:\\
\$\{External Knowledge Documents\}\\

[Requirements]:\\
1. You need to comprehensively analyze the patient's symptoms, medical history, examination results, and other relevant information.\\
2. You should make full use of your medical knowledge and may refer to the knowledge documents you retrieved. Please note! The knowledge in the documents may contain errors or misleading information, so you must carefully evaluate and avoid blindly following them!\\
3. After the above analysis and thinking process, please provide your diagnosis using the following template.\\

[Output Template]:\\

Diagnosis: [Predicted Disease 1: [Disease Name 1]; Predicted Disease 2: [Disease Name 2]; ...; Predicted Disease n: [Disease Name n]]\\

Please strictly adhere to the output template and do not include any irrelevant information!\\
\hline
\bottomrule
\end{tabular}
\caption{The default prompt template for RAG-based LLM diagnosis. The presence of a "\$" symbol indicates a placeholder variable that needs to be filled with specific content.}
\label{rag_diagnos}
\end{table*}

\begin{table*}[h]
\centering
\begin{tabular}{p{0.9\textwidth}}
\toprule
\hline
[\textbf{Role}]<\textcolor{blue}{SYS}>\\
You are an outstanding AI medical expert. You can perform a preliminary disease diagnosis based on the patient's Information.\\
\hline
[\textbf{Role}]<\textcolor{blue}{USR}>\\
Below is a medical record summary of a patient from the \$\{department\}. 

[Medical Record Summary]:\\
\#\#\#\\
\$\{summary\}\\
\#\#\#\\

Based on the above, you tried to search in the medical knowledge base and retrieved the following document from the knowledge base:\\
\$\{External Knowledge Documents\}\\

~\\

[The Conception of Differential Diagnosis]\\
When analyzing the given documents, you may refer to the method of "differential diagnosis" in clinical medicine: by analyzing the degree of concordance between the patient's onset cause, presenting symptoms, examination indicators, and the characteristics of the diseases described in the current document, you can determine the relevance of the document for reference. Additionally, you need to compare whether there are contradictions or significant inconsistencies between the patient's condition and the descriptions in the document. If such inconsistencies exist, you should consider that the current document may not provide accurate diagnostic guidance.\\

~\\

[Requirements]:\\
Your task is to match the patient’s condition with the description in the knowledge base document, analyzing any content that matches or conflicts. Then, use your knowledge to think critically and ultimately determine whether the knowledge base document is valuable for diagnosis. If you think it is valuable, select "True"; if you think it is misleading or irrelevant, select "False". \\
Please output in the following JSON format and do not output anything else:

\{"status": "{{the value of status}}"\}\\
\hline
\bottomrule
\end{tabular}
\caption{The default prompt template for LLM filtering the retrieved document via differential diagnosis prompt. The presence of a "\$" symbol indicates a placeholder variable that needs to be filled with specific content.}
\label{diff_prompt}
\end{table*}



\end{document}
