\section{Related Work}
\subsection{RAG in Clinical Disease Diagnosis}
To improve diagnostic accuracy, model reliability, and reduce hallucination issues without retraining, recent studies widely adopt Retrieval-Augmented Generation (RAG) to integrate external medical knowledge **Hermann et al., "Teaching Machines to Read and Comprehend"**. Most research uses basic retrieval methods **,** typically leveraging embedding models to encode external knowledge and task queries into vector representations. Relevant knowledge is retrieved via vector similarity and used in LLMs through tailored prompts for diagnosis generation. Besides, knowledge graphs are also widely employed **,**. 
% **Wen et al., "Learning Deep Head Pose Representations"** used graphs to guide LLMs in generating reasoning maps **,** and **Thompson et al., "A Clinical Decision Support System Using Natural Language Processing"** applied regular expression matching for pulmonary hypertension diagnostics **,**. **Wang et al., "Adaptive Knowledge Retrieval for Zero-Shot Learning"**  proposed a multi-LLM framework with specialized medical knowledge.
% For example, **Zhenzhu et al., "Knowledge Graph Enhanced Adversarial Training"** developed GPT-based agents for traumatic brain injury rehabilitation, while **McInerney et al., "Clinical Decision Support Systems: A Systematic Review"** extracted evidence from clinical notes for cancer, pneumonia, and pulmonary edema risk assessment. 


\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/workflow.pdf}
	\caption{The overall architecture of our proposed framework FIND. FIND consists of two stages. Stage(a) involves inference \& Retrieval Decision Making Based on Fine-Grained Information Density. Stage (b) focuses on knowledge retrieval and integration. Note that Stage (b) is activated only when the score computed in Stage (a) falls below a predefined threshold.}
	\label{workflow}
\end{figure*}

\subsection{Adaptive-RAG}
Adaptive Retrieval-Augmented Generation (RAG) dynamically determines whether a large language model (LLM) requires external knowledge retrieval to mitigate inaccuracies. **Gu et al., "FLARE: Few-Shot Knowledge Graph Augmentation"** and **Kumar et al., "DRAGIN: Dynamic Knowledge Graph Construction for Textual Entailment"** activate search engines when the LLM generates low-confidence tokens. **Wang et al., "Rethinking BERT: Towards a More Effective Knowledge Retrieval"** use a prompting mechanism for LLMs to autonomously decide on retrieval. Self-Awareness-Guided Generation  trains a classifier to assess output authenticity, while Adaptive-RAG evaluates query complexity to determine retrieval necessity. **Mallen et al., "Adaptive-Retrieval-Augmented-Generation: A New Framework"** propose activating retrieval based on entity frequency in queries, though this may fail for complex, multi-step reasoning tasks. Asai et al. introduce Self-RAG, which trains a model to dynamically retrieve, critique, and generate text.

% 基于LLM输出的

% Existing adaptive RAG methods face two primary challenges. First, determining when to retrieve based solely on the LLM's outputs is superficial. The retrieval decisions made by LLMs remain susceptible to hallucinations, thereby failing to reliably indicate actual knowledge sufficiency**. Moreover, even when the correct knowledge is absent in the model's parameters, LLMs tend to confidently generate incorrect content**. Second, while training a small classification model to evaluate input queries is a feasible approach, it becomes problematic when the input query context is long, structurally complex, and content-rich. Directly attempting to enable the classification model to understand the structure and distinctions among diverse input queries is challenging. These models are likely to struggle with accurately modeling the input content and may develop biases toward query length or specific phrasing.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{figures/annotate.pdf}
	\caption{Details of our proposed annotation strategy. During the annotation process, we adopt different annotation strategies based on the responses generated by the LLM.}
	\label{annotate}
\end{figure*}