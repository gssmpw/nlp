\section{Related Work}
\paragraph{Constraint solving with supervised learning.}
Supervised learning has been extensively applied to constraint solving. For example, Pointer Networks____ are used for the sequential generation of combinatorial problems involving permutations such as the traveling salesperson problem. ____ propose a graph-based recurrent network to model CSPs, effectively leveraging the graph structure to refine variable assignments iteratively. SATNet____ differentiates through semidefinite programming relaxations in a supervised setting to handle logical constraints. ____ introduce a method for iterative reasoning through energy diffusion, focusing on progressive refinement of solutions. More recently, ____ proposed a recurrent Transformer architecture that reuses the same Transformer weights across multiple steps, iteratively refining inputs before projecting them to the final outputs. The common drawback for supervised approaches is the need for labels, which is not easy to generate for many large CSP problems. Additionally, for problems with more than one unique solution, label generation becomes non-trivial. 

% \todo[EK,inline]{Need to say why we are better/different than all these methods.}

% Various works exist that utilizes supervised learned to solve constrained reasoning problems. For example, Pointer Networks utilizes the structure of the problems specifically to solve combinatorial tasks____. ____ introduced a graph-based recurrent approach to solving CSPs. SATNet is a differentiable MAXSAT solver that learns the parameters of the SDP relaxation for the MAXSAT in a supervised setting____. ____ proposes to iteratively solve reasoning problems through energy diffusion. ____ introduced a recurrent-transformer architecture that reuses a same set of transformer weights to repeatedly process a set of inputs, before projecting them to produce an output. 



% In ANY-CSP they say: “A fundamentally different approach considers soft relaxations of the underlying problems which can optimized directly with SGD. Examples of this concept are PDP (Amizadeh, Matusevych, and Weimer 2019) for SAT and RUNCSP (Tönshoff et al. 2021) for all binary CSPs with fixed constraint language. These architectures can predict completely new solutions in each iteration but the relaxed differentiable objectives used for training typically do not capture the full hardness of the discrete problem."

\paragraph{Constraint solving without labels.}

% Existing approaches for learning to solve CSPs without labels mostly rely on reinforcement learning (RL). 
A common recipe for Reinforcement Learning (RL) in constraint solving is to express the problem with a graph  which is then processed using Graph Neural Networks (GNN). The GNN's weights are updated using RL based on a reward function expressing an objective function and/or constraint satisfaction____. For example, ____ converts a CSP instance into a tripartite variable-domain-constraint graph which is then solved using a GNN trained by RL. ____ use a Transformer architecture to learn discrete transformation steps with RL for routing problems. However, RL approaches require significant computational resources for training, as well as an expertly designed reward signal for each problem. 


% ____ represents combinatorial optimization problems as graphs and use GNN and RL to learn heuristics solvers. 

% However, RL approaches requires high computational resources for training, as well as an expertly designed reward signal for each problem.

Non-RL based methods require addressing the non-differentiability of discrete constraints. ____ use the straight-through estimator____ for logical constraints and ____ explore a similar approach for mixed-integer non-linear programs. ____ devise a continuous relaxation for binary constraints (i.e., constraints involving two variables) which are used to guide a recurrent GNN to generate solutions; this approach is limited in applicability as many CSPs of interest have non-binary constraints. ____ design continuous relaxations for some constraint classes in conjunction with a reconstruction loss to tackle a visual Sudoku problem; it is unclear how their architecture can be adapted to CSP solving in general. 
% while this approach can extend to 

% and ____ explores a similar approach in solving mixed-integer non-linear programs. 
% Our advantage over ANYCSP: much faster to train (they use RL and takes 24-48 hours, while we produce good models within a few hours). We can also compute in batches, making our approach much faster during inference.


% A concurrent work investigated 

\paragraph{Continuous relaxation of discrete functions.}
Continuous relaxations have been used effectively to approximate discrete functions. For example, T-norm has been widely implemented as a continuous approximation for discrete binary logic operations____. ____ introduced continuous relaxations for discrete algorithms, such as if-statements and while-loops. Similarly, ____ proposed entropy-based relaxations to handle discrete constraints.

% T-norm has been effectively implemented as continuous approximation for discrete binary logic operations____. ____ introduces continuous relaxation for discrete algorithms such as if statements and while loops. ____ introduces entropy based relaxations for discrete constraints. 

\paragraph{Recurrency for generalization.}
The incorporation of recurrency has been shown to improve a model's generalization. ____ implement recurrent ResNet blocks to solve simple logic puzzles and show that increasing recurrent steps at test-time allows generalization to harder unseen tasks. Recurrency was introduced to the Transformer architecture by sharing weights across Transformer layers____, yielding improved generalization capabilities on arithmetic and logic-based string manipulation tasks ____. Our method differs from the existing work as recurrency is only introduced during test-time deployment.