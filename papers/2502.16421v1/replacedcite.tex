\section{Related Work}
\label{section:related}

\begin{table*}[ht]\centering
    % \captionsetup{font={small}}
    \caption{Shortcomings or irrationalities of existing rain datasets or acquisition methods.}
    \label{tab:related_work}
    
    \begin{tabular}[t]{p{1.8cm}|l|p{6.3cm}|p{5.2cm}}
        \hline
        name & type & key idea & shortcomings or irrationalities \\ \hline
         MPID ____ & real rain & Images of real-world rainy scenes captured by cameras are collected from the Internet. & \makecell[t{p{5.2cm}}]{ 1. Clean background images are not included. \\ 2. Time-consuming and labor-cumbersome.}  \\ \hline
        RainDS ____ & artificially generated rain & They sprayed water
using sprinklers to mimic rainy scenes in the real world and obtained paired rainy-clean images. & \makecell[t{p{5.2cm}}]{ 1. The types of artificially simulated rain scenes are limited, and there are certain differences with the real rain scenes. \\ 2. Time-consuming and labor-cumbersome.} \\ \hline
        SPA-Data ____ & artificially synthetic rain & A semi-automatic method that incorporates temporal properties of rain streaks and human supervision to generate clean images from sequences of real rain images. & \makecell[t{p{5.2cm}}]{ Human supervision is needed. } \\ \hline
        MPID ____ & artificially synthetic rain & Rainy images are synthesized from clean images through Photoshop. & \makecell[t{p{5.2cm}}]{ Scene depth is lack of consideration. }  \\ \hline
        Photorealistic rendering ____ &  rendering-based synthesis & They developed a model for rain streak appearance and an image-based rendering algorithm for realistic rain rendering. & \makecell[t{p{5.2cm}}]{ 1. Complex input data and empirical parameters limits the diversity of synthetic rain. \\ 2. Physical simulation and rendering increase substantial time overheads. } \\ \hline
        Physics-based rendering ____ & rendering-based synthesis & They proposed a practical, physically-based approach to render realistic rain in images. & \makecell[t{p{5.2cm}}]{ Physical simulation and rendering increase substantial time overheads. } \\ \hline
        RainCityscapes ____ & rendering-based synthesis & It was created by adopting the images in the Cityscapes dataset as clean background images, leveraging the rain streak appearance model. & \makecell[t{p{5.2cm}}]{ Physical simulation and rendering increase substantial time overheads. } \\ \hline
        RICNet ____ & learning-based synthesis & They propose a GAN-based rain intensity controlling network to control the rain continuously in a bi-directional manner. & \makecell[t{p{5.2cm}}]{ Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
        JRGR ____ & learning-based synthesis & They proposed a CycleGAN-based unified framework that jointly learns rain generation and removal. & \makecell[t{p{5.2cm}}]{Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
        VRGNet ____ & learning-based synthesis & They proposed a VAE-based generative model to depict the generation process of rainy images, which can explore an implicit distribution of the rain in statistics. & \makecell[t{p{5.2cm}}]{Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
    \end{tabular}
\end{table*}

\subsection{Rain Dataset Acquisition}

As mentioned above, the data acquisition methods for SIRR datasets can be broadly classified as real datasets, artificially generated datasets, and synthetic datasets. In this subsection, they will be reviewed and analyzed deeply.

Li \MakeLowercase{\textit{et al.}} ____ proposed a large-scale image deraining benchmark dataset, which includes three sets of real-world rainy images. They were obtained by collecting images of real-world rainy scenes captured by cameras from the Internet, but they do not include paired clean background images. Quan \MakeLowercase{\textit{et al.}} ____ sprayed water
using sprinklers to generate rain streaks and mimic rainy scenes in the real world. By stopping spraying water, they obtained the clean background images. Although this method can obtain clean and rainy scene image pairs, it is time-consuming and labor-cumbersome.

Wang \MakeLowercase{\textit{et al.}} ____ proposed a semi-automatic method that incorporates temporal properties of rain streaks and human supervision to generate high quality clean images from sequences of real rainy images. Due to the need for human supervision, this method is also time-consuming and labor-cumbersome. Li \MakeLowercase{\textit{et al.}} ____ synthesized rainy images from clean images of outdoor cloudy and fog-free scenes through Photoshop. However, the lack of consideration for scene depth in this method significantly deviates from the characteristics exhibited in real rainy images.

% Rendering-based methods
Garg and Nayar ____ studied the visual appearance of rain streaks in detail for the first time. They developed a model for rain streak appearance and an image-based rendering algorithm for realistic rain rendering under different illumination. Based on their work, Halder \MakeLowercase{\textit{et al.}} ____ proposed a practical, physically-based approach to render realistic rain in images. The RainCityscapes ____ dataset was also created by adopting the images in the Cityscapes ____ dataset as clean background images, leveraging the rain streak appearance model. Despite these rendering-based methods capable of rendering realistic rain under specific illumination, they come with their limitations. Users are required to specify the rain parameters, and the raindrop distribution is simulated based on physical methods, which makes capturing the complex raindrop distribution in real rainy images challenging.

% DL-based methods
To efficiently generate diverse and non-repetitive rain streaks, some researchers have utilized deep learning for rainy image synthesis. Ni \MakeLowercase{\textit{et al.}} ____ propose a GAN-based ____ rain intensity controlling network to control the rain continuously in a bi-directional manner while preserving the scene-specific rain characteristics. Ye \MakeLowercase{\textit{et al.}} ____ proposed a CycleGAN-based ____ unified framework that jointly learns rain generation and removal, offering a better approximation to real rain by learning physical degradation from real rainy images. Wang \MakeLowercase{\textit{et al.}} ____ proposed a VAE-based ____ generative model to depict the generation process of rainy images, which can explore an implicit distribution of the rain layers in statistics, so they obtained an interpretable rain generator.

Although these DL-based methods can efficiently synthesize rainy images, they still have some limitations. Typically, these methods treat the rain layer as a gray-scale layer and blend it with the background image using linear overlaying. Consequently, they disregard other essential attributes of rain, including color and optical phenomena like refraction and transmission. Besides, existing synthetic rainy image datasets lack diversity in terms of illumination conditions, with images mainly in daytime illumination conditions, and few images in complex illumination conditions, like nighttime. Further, the resolution of these synthetic rainy images is typically low. In this paper, we propose a practical learning-from-rendering pipeline, which combines the realism advantages of rendering-based methods and the high-efficiency advantages of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets.

Shortcomings or irrationalities of existing rain datasets or acquisition methods are summarized in Table \ref{tab:related_work}.

\subsection{Generative Models for Image Synthesis}

As mentioned above, GANs and VAEs, as DL-based generative models, have been utilized in rainy image synthesis. Nevertheless, these generative models possess certain limitations. GANs ____ produce high-quality images through adversarial training, but their optimization is challenging. In contrast, VAEs ____ rely on likelihood estimation, allowing for faster generation of high-quality images, but the image quality may not be as good as that of GANs. Diffusion models ____ have achieved state-of-the-art results in the field of image synthesis. However, the original diffusion models ____ are slow in sampling, as they need a mass of time steps to generate a sample. Consequently, Latent Diffusion Models (LDMs) ____ use a two-stage pipeline, firstly compressing images into a low-dimensional latent space, and  training diffusion models on the compressed latent space, which speeds up the training and inference process with almost no reduction in synthesis quality. Given its high-quality image generation capability, our proposed rainy image generation network is based on LDM.

\subsection{Conditional Image Synthesis}
Conditional image synthesis allows users to control the image synthesis process, allowing for applications such as semantic image editing, image inpainting, etc. Conditional diffusion models ____ are capable of modeling conditional distributions of the form $p(z|y)$, which enables controlling the synthesis process through inputs $y$ such as text, semantic map, etc. 

High-resolution images synthesis is a challenging task, having high demand in terms of quality and computational complexity. Providing more useful guidance information ____, such as low-resolution images and intermediate generation results, is expected to improve the quality of generated images.

Inspired by ASSET ____, our proposed HRIGNet is designed to introduce a guiding diffusion model in the LDM ____. Given a clean background image and a mask of a rain layer, the HRIGNet generates the latent code of the rain layer image with a guiding diffusion model. The latent code is then used to guide the diffusion process for high-resolution image synthesis.