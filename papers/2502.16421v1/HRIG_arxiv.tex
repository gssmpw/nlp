\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}

\usepackage{hyperref}
\usepackage{cleveref} 
\usepackage{latexsym}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{array}


\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
% updated with editorial comments 8/9/2021

\begin{document}

\title{High-resolution Rainy Image Synthesis: Learning from Rendering}

\author{Kaibin Zhou, Shengjie Zhao, Hao Deng, and Lin Zhang \\ School of Software
Engineering, Tongji University, Shanghai, China
\thanks{
This work was supported in part by the National Key R\&D Program of China 2023YFC3806000 and 2023YFC3806002, in part by the National Natural Science Foundation of China under Grants 61936014, in part by National Natural Science Foundation of China under Grant U23A20382, in part by Shanghai Municipal Science and Technology Major Project No. 2021SHZDZX0100, in part by the Shanghai Science and Technology Innovation Action Plan Project 22511105300, and in part by Fundamental Research Funds for the Central Universities. (Corresponding authors: Shengjie Zhao, Hao Deng)

Kaibin Zhou, Shengjie Zhao, Hao Deng and Lin Zhang are with the School of Software
Engineering, Tongji University, Shanghai 201804, China (email: \{kb824999404, shengjiezhao, denghao1984, cslinzhang\}@tongji.edu.cn).
}
}
        % <-this % stops a space

% The paper headers


% \IEEEpubid{\begin{minipage}{\textwidth}\ \\[30pt] \centering
% 		0000--0000/00\$00.00~\copyright~2023 IEEE
% \end{minipage}}

% \IEEEpubid{0000--0000/00\$00.00~\copyright~2023 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.

\maketitle
\begin{abstract}
% Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. This leads to a lack of large-scale high-quality paired rainy-clean image datasets for training DL-based single image rain removal (SIRR) models capable of generalizing to various illumination conditions. 

Currently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality paired rainy-clean image datasets, which can train deep learning-based single image rain removal models capable of generalizing to various illumination conditions. Therefore, we propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we use a rendering-based method to create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, to learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet). HRIGNet is designed to introduce a guiding diffusion model in the Latent Diffusion Model, which provides additional guidance information for high-resolution image synthesis. In our experiments, HRIGNet is able to synthesize high-resolution rainy images up to $\mathbf{2048 \times 1024}$ resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to real rainy images. To make our work reproducible, source codes and the dataset have been released at https://kb824999404.github.io/HRIG/.
\end{abstract}

\begin{IEEEkeywords}
Image generation, diffusion models, synthetic datasets, single image rain removal
\end{IEEEkeywords}

\section{Introduction}

% The motivation of SIRR
\IEEEPARstart{I}{n} the field of computer vision, cameras are extensively used as sensors to perceive the environment and capture visual data. Under ideal weather conditions, cameras are capable of capturing clean and detailed images of scenes, thus many vision algorithms are based on processing of clean scene data. Nevertheless, under rainy weather conditions, images captured by cameras are often degraded by rain in the scene \cite{halder2019physics,li2016rain}, which has negative impacts on the performance of vision algorithms. Therefore, to address this issue, it is necessary to perform image rain removal on the scene images collected by cameras. Currently, single image rain removal (SIRR) \cite{zamir2022restormer,gao2023mountain,cui2022selective,ren2019progressive} is a widely-discussed task that serves as a crucial pre-processing step for outdoor vision tasks, e.g., object detection \cite{hnewa2020object} and semantic segmentation \cite{di2020rainy}. 

\begin{figure}[!t] 
    \centering
    \captionsetup[subfloat]{labelfont=scriptsize,textfont=scriptsize}
    \makebox[0.48\textwidth]{
        \subfloat[Input]{
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/Input_2.jpg}
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/Input_3.jpeg}
            \label{fig_input}
        }
    }
    \makebox[0.48\textwidth]{
        \subfloat[Restormer \cite{zamir2022restormer}]{
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/Restormer_2.png}
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/Restormer_3.png}
            \label{fig_restormer}
        }
    }
    \makebox[0.48\textwidth]{
        \subfloat[M3SNet \cite{gao2023mountain}]{
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/M3SNet_2.png}
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/M3SNet_3.png}
            \label{fig_m3sneet}
        }
    }
    \makebox[0.48\textwidth]{
        \subfloat[SFNet \cite{cui2022selective}]{
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/SFNet_2.jpg}
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/SFNet_3.jpg}
            \label{fig_sfnet}
        }
    }
    \makebox[0.48\textwidth]{
        \subfloat[PReNet \cite{ren2019progressive}]{
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/PReNet_2.png}
            \includegraphics[width=0.18\textwidth,height=20.5mm]{hrig/Figure_Derain_Clip/PReNet_3.jpeg}
            \label{fig_prenet}
        }
    }
    % \captionsetup{font={small},justification=raggedright}
    \caption{ Rain removal results of SOTA DL-based SIRR models on real nighttime rainy scene images. Images in the first row are rainy scene images input to SIRR models. From the second row to the fifth row, rain removal results of four SIRR models, Restormer \cite{zamir2022restormer}, M3SNet \cite{gao2023mountain}, SFNet \cite{cui2022selective} and PReNet \cite{ren2019progressive} are shown, respectively.} 
    \label{fig:figure1}
\end{figure}


% Factors that affect the performance of DL-based SIRR
Data-driven SIRR approaches based on deep learning (DL) have gained a lot of traction due to their effective fitting capability \cite{wang2021rain}. The performance of DL-based methods is mainly affected by two key factors \cite{wang2021rain}, i.e., the rationality and capacity of deraining models and the quality of training datasets. This paper mainly concentrates on the latter factor.

% Rainy image dataset acquisition methods
The current data acquisition methods for SIRR datasets can be broadly classified as real datasets, artificially generated datasets, and synthetic datasets. Real datasets \cite{li2019single,garg2007vision,garg2005does} are acquired by capturing images of rainy scenes in the real world. However, this approach presents significant limitations as it depends on weather conditions and it is difficult to acquire paired rainy-clean images. On the other hand, artificially generated datasets \cite{quan2021removing} are created by simulating rainy scenarios in the real world, capturing clean background images, and pairing them with rainy images. However, this approach is time-consuming and labor-intensive. In contrast, image synthesis methods can synthesize rainy images from clean background images with little or no human intervention. These methods are faster and less labor-intensive, providing the potential to acquire large-scale paired rainy-clean image datasets. Therefore, existing SIRR models are typically trained on synthetic paired rainy-clean image datasets.


\begin{figure*}[t] \centering
    \captionsetup[subfloat]{labelfont=scriptsize,textfont=scriptsize}
    \subfloat[Rain100L \cite{yang2017deep}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/Rain100L.png}
    }
    \subfloat[BDD350 \cite{jiang2020multi}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/BDD350_daytime.jpg}
    }
    \subfloat[COCO350 \cite{jiang2020multi}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/COCO350_daytime.jpg}
    }
    \subfloat[HRI (Ours)]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/HRI_daytime.png}
    }
    \subfloat[Real Rain]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/RealRain_daytime1.jpeg}
    }
    \\
    \subfloat[RainCityscapes \cite{hu2019depth}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/Raincityscapes.png}
    }
    \subfloat[BDD350 \cite{jiang2020multi}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/BDD350_nighttime.jpg}
    }
    \subfloat[COCO350 \cite{jiang2020multi}]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/COCO350_nighttime.jpg}
    }
    \subfloat[HRI (Ours)]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/HRI_nighttime.png}
    }
    \subfloat[Real Rain]{
        \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/RealRain_nighttime1.jpeg}
    }

    % \captionsetup{font={small},justification=raggedright}
    \caption{Visual comparisons of real rainy images and synthetic rainy images from five datasets. Rain100L \cite{yang2017deep} and RainCityscapes \cite{hu2019depth} datasets only contain rainy images in daytime. BDD350 \cite{jiang2020multi}, COCO350 \cite{jiang2020multi} and HRI (Ours) datasets contain rainy images in both daytime and nighttime. Real Rain is captured in real rain scenes.} 
    \label{fig:figure2}
\end{figure*}

% \begin{figure*}[t] \centering
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/Rain100L.png}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/BDD350_daytime.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/COCO350_daytime.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/HRI_daytime.png}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/RealRain_daytime1.jpeg}
%     \\
%     \makebox[0.18\textwidth]{\scriptsize Rain100L \cite{yang2017deep}}
%     \makebox[0.18\textwidth]{\scriptsize BDD350 \cite{jiang2020multi}}
%     \makebox[0.18\textwidth]{\scriptsize COCO350 \cite{jiang2020multi}}
%     \makebox[0.18\textwidth]{\scriptsize HRI (Ours)}
%     \makebox[0.18\textwidth]{\scriptsize Real Rain}
%     \\
%     \vspace{0.1cm}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/Raincityscapes.png}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/BDD350_nighttime.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/COCO350_nighttime.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/HRI_nighttime.png}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_RainyImages/RealRain_nighttime1.jpeg}
%     \\
%     \makebox[0.18\textwidth]{\scriptsize RainCityscapes \cite{hu2019depth}}
%     \makebox[0.18\textwidth]{\scriptsize BDD350 \cite{jiang2020multi}}
%     \makebox[0.18\textwidth]{\scriptsize COCO350 \cite{jiang2020multi}}
%     \makebox[0.18\textwidth]{\scriptsize HRI (Ours)}
%     \makebox[0.18\textwidth]{\scriptsize Real Rain}
%     \\
%     \captionsetup{font={small},justification=raggedright}
%     \caption{Visual comparisons of real rainy images and synthetic rainy images from five datasets. Rain100L \cite{yang2017deep} and RainCityscapes \cite{hu2019depth} datasets only contain rainy images in daytime. BDD350 \cite{jiang2020multi}, COCO350 \cite{jiang2020multi} and HRI (Ours) datasets contain rainy images in both daytime and nighttime. Real Rain is captured in real rain scenes.} 
%     \label{fig:figure2}
% \end{figure*}

% Rainy image synthesis methods
Existing rainy image synthesis methods can be classified into two main categories, rendering-based methods and learning-based methods. Rendering-based methods \cite{garg2006photorealistic,halder2019physics,pharr2016physically,blender2018} focus on modeling the oscillation of raindrops and the appearance of rain streaks. These methods use a scene depth map, light source attributes, and specific custom rain attributes, to render realistic rain. Then, the rain layer is blended with the background image based on physical principles. On the other hand, learning-based  methods \cite{ni2021controlling,wang2021rain,wei2021deraincyclegan,ye2021closing} aim to train generative models with real rainy image datasets to generate rainy images. In these methods, generative models can capture the complex distribution of rain patterns in real rainy images and efficiently generate diverse and non-repetitive rain patterns without human intervention and empirical parameters.

% Limitations of existing rainy image synthesis methods
Despite the potential of these synthesis methods in generating synthetic rainy images, certain limitations persist. Rendering-based methods face challenges due to their complex input data and empirical parameters, which limits the diversity of synthetic rain. Physical simulation and rendering also significantly increase computational intensity and have substantial time overheads. Additionally, learning-based methods generate rain layers as grayscale and blend them with background images using linear overlaying. Moreover, these methods disregard optical phenomena like refraction and transmission, and ignore the color appearance of rain.

% Besides, existing synthetic rainy image datasets are limited due to the inadequate diversity about illumination conditions. In particular, there exists a scarcity of images captured in complex illumination conditions, for instance, nighttime. Further, the resolution of these synthetic rainy images is typically low.

% Issue of existing SIRR model
These issues have adversely affected the quality and diversity of training datasets, thereby limiting the performance improvement of SIRR models. We conduct an in-depth analysis of the currently available SOTA SIRR models, and observe that they perform well in daytime scenarios but poorly in complex illumination conditions such as at nighttime. As shown in Fig. \ref{fig:figure1}, four DL-based SIRR models are presented for visual comparison of their deraining results on real nighttime rainy scene images. It can be clearly seen from these results that several SIRR models have struggled to completely remove colored rain streaks and restore a clean background. The PreNet \cite{ren2019progressive} has the best rain removal results, able to remove the obvious white rain streaks (commonly appear in daytime rainy images), but it is difficult to remove rain streaks of other colors.

% Issue of existing synthetic rainy image datasets
Through an analysis of existing synthetic rainy image datasets, we identify a lack of diversity in terms of illumination conditions, with a predominance of images captured during the daytime and a scarcity of images from complex illumination conditions such as at nighttime. Moreover, the resolution of synthetic images is generally low. These issues with image quality result in weak generalization ability of existing SIRR models in complex illumination conditions. Fig. \ref{fig:figure2} presents visual comparisons of real rainy images and synthetic rainy images randomly selected from five existing datasets: Rain100L \cite{yang2017deep}, RainCityscapes \cite{hu2019depth}, BDD350 \cite{jiang2020multi}, COCO350 \cite{jiang2020multi} and HRI (Ours). Rain100L and RainCityscapes datasets only contain rainy images in daytime. BDD350, COCO350 and HRI datasets include rainy images in both daytime and nighttime. The rain layers in Rain100L, BDD350, and COCO datasets neglect the influence of scene illumination and depth, resulting in significant color appearance differences compared to real rainy images. Although RainCityscapes dataset takes scene depth into consideration, it does not include images under complex illumination conditions such as nighttime.
% As observed, the color appearance of rain in synthetic rainy images in the existing datasets significantly differs from real rainy images. 
% DL-based SIRR models trained with these datasets struggle to generalize to complex illumination conditions, such as nighttime, which greatly impacts their performance on real rainy images. Fig. \ref{fig:figure2} presents the rain removal results of four SOTA DL-based SIRR models on real rainy images under nighttime illumination. As observed, these SIRR models struggle to completely remove rain from the rainy images and restore a clean background. 

Consequently, there are few effective methods for synthesizing a mass of high-resolution rainy images in complex illumination conditions. However, these methods are essential for synthesizing large-scale high-quality (high resolution and complex illumination conditions) paired rainy-clean image datasets, which can train DL-based SIRR models capable of generalizing to various illumination conditions.

% Our Work
In this paper, to effectively synthesize a large quantity of high-resolution rainy images in complex illumination conditions, we propose a practical learning-from-rendering pipeline. The pipeline consists of two stages: the rendering stage, which creates high-resolution paired rainy-clean image datasets, and the learning stage, which trains a rainy image generation network using the datasets. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets.

To train a high-quality rainy image generation network, realistic paired rainy-clean image datasets are needed. Considering that collecting real rainy images captured by cameras is time-consuming, labor-cumbersome and difficult to obtain corresponding background images, we use a rendering-based method \cite{halder2019physics,pharr2016physically,blender2018} to create a dataset in the rendering stage. Specifically, we create a High-resolution Rainy Image (HRI) dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions.

To learn illumination information from background images for high-resolution rainy image generation, we propose a High-resolution Rainy Image Generation Network (HRIGNet) in the learning stage. Considering that diffusion models \cite{ho2020denoising,rombach2022high,peebles2023scalable} have been at the forefront of recent advances in image generative models, the HRIGNet is based on diffusion models.

Providing more useful guidance information for high-resolution image synthesis is expected to improve the quality of generated images. Inspired by ASSET \cite{liu2022asset}, we design the HRIGNet to introduce a guiding diffusion model in the Latent Diffusion Model (LDM) \cite{rombach2022high}. To establish pairings between the generated rainy images and the input clean background images, it is necessary to apply effective constraints to the image synthesis process. Therefore, we use the cross-attention \cite{vaswani2017attention} and concatenation conditional mechanisms to control rainy image synthesis, using the latent code of the predicted rain layer image from the guiding diffusion model and the masked background image as conditioning respectively.

% In the experimental section, we train the HRIGNet model using the HRI dataset. The model's capability for high-resolution rainy image synthesis is evaluated by comparing it quantitatively and visually with several baseline image generative models. Additionally, we conduct ablation studies on the guiding model and diffusion backbone to evaluate the effectiveness of our HRIGNet. Furthermore, our proposed HRIGNet can be used to augment existing synthetic rainy image datasets and for training DL-based derainers. Quantitative comparison experiments on real datasets validate that retraining these DL-based derainers on augmented datasets can significantly improve their performance.

In summary, our work makes the following contributions:

\begin{itemize}
\item We propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis, which combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality (high resolution and complex illumination conditions) paired rainy-clean image datasets.
\item In the rendering stage, we use a rendering-based method to render realistic rainy images and create a HRI dataset for the training of the rainy image generation network in the learning stage. The HRI dataset contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions.
\item In the learning stage, we propose a diffusion-based method HRIGNet, which can learn illumination information from background images for high-resolution rainy image generation, taking advantage of the high-efficiency of the learning-based method. Rain removal experiments on real dataset validate that our HRIGNet can help improve the robustness of deep derainers to rainy images in the real world.
\end{itemize}

The remainder of this paper is organized as follows. Sect. \ref{section:related} introduces the related work. Our proposed learning-from-rendering pipeline is presented in Sect. \ref{section:pipeline}. Our proposed HRIGNet is presented in Sect. \ref{section:hrig}. Experimental results are reported in Sect. \ref{section:experiment}. Finally, Sect. \ref{section:conclusion} concludes the paper.

\section{Related Work}
\label{section:related}

\begin{table*}[ht]\centering
    % \captionsetup{font={small}}
    \caption{Shortcomings or irrationalities of existing rain datasets or acquisition methods.}
    \label{tab:related_work}
    
    \begin{tabular}[t]{p{1.8cm}|l|p{6.3cm}|p{5.2cm}}
        \hline
        name & type & key idea & shortcomings or irrationalities \\ \hline
         MPID \cite{li2019single} & real rain & Images of real-world rainy scenes captured by cameras are collected from the Internet. & \makecell[t{p{5.2cm}}]{ 1. Clean background images are not included. \\ 2. Time-consuming and labor-cumbersome.}  \\ \hline
        RainDS \cite{quan2021removing} & artificially generated rain & They sprayed water
using sprinklers to mimic rainy scenes in the real world and obtained paired rainy-clean images. & \makecell[t{p{5.2cm}}]{ 1. The types of artificially simulated rain scenes are limited, and there are certain differences with the real rain scenes. \\ 2. Time-consuming and labor-cumbersome.} \\ \hline
        SPA-Data \cite{wang2019spatial} & artificially synthetic rain & A semi-automatic method that incorporates temporal properties of rain streaks and human supervision to generate clean images from sequences of real rain images. & \makecell[t{p{5.2cm}}]{ Human supervision is needed. } \\ \hline
        MPID \cite{li2019single} & artificially synthetic rain & Rainy images are synthesized from clean images through Photoshop. & \makecell[t{p{5.2cm}}]{ Scene depth is lack of consideration. }  \\ \hline
        Photorealistic rendering \cite{garg2006photorealistic} &  rendering-based synthesis & They developed a model for rain streak appearance and an image-based rendering algorithm for realistic rain rendering. & \makecell[t{p{5.2cm}}]{ 1. Complex input data and empirical parameters limits the diversity of synthetic rain. \\ 2. Physical simulation and rendering increase substantial time overheads. } \\ \hline
        Physics-based rendering \cite{halder2019physics} & rendering-based synthesis & They proposed a practical, physically-based approach to render realistic rain in images. & \makecell[t{p{5.2cm}}]{ Physical simulation and rendering increase substantial time overheads. } \\ \hline
        RainCityscapes \cite{hu2019depth} & rendering-based synthesis & It was created by adopting the images in the Cityscapes dataset as clean background images, leveraging the rain streak appearance model. & \makecell[t{p{5.2cm}}]{ Physical simulation and rendering increase substantial time overheads. } \\ \hline
        RICNet \cite{ni2021controlling} & learning-based synthesis & They propose a GAN-based rain intensity controlling network to control the rain continuously in a bi-directional manner. & \makecell[t{p{5.2cm}}]{ Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
        JRGR \cite{ye2021closing} & learning-based synthesis & They proposed a CycleGAN-based unified framework that jointly learns rain generation and removal. & \makecell[t{p{5.2cm}}]{Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
        VRGNet \cite{wang2021rain} & learning-based synthesis & They proposed a VAE-based generative model to depict the generation process of rainy images, which can explore an implicit distribution of the rain in statistics. & \makecell[t{p{5.2cm}}]{Some essential attributes of rain are disregard, including color and optical phenomena. } \\ \hline
    \end{tabular}
\end{table*}

\subsection{Rain Dataset Acquisition}

As mentioned above, the data acquisition methods for SIRR datasets can be broadly classified as real datasets, artificially generated datasets, and synthetic datasets. In this subsection, they will be reviewed and analyzed deeply.

Li \MakeLowercase{\textit{et al.}} \cite{li2019single} proposed a large-scale image deraining benchmark dataset, which includes three sets of real-world rainy images. They were obtained by collecting images of real-world rainy scenes captured by cameras from the Internet, but they do not include paired clean background images. Quan \MakeLowercase{\textit{et al.}} \cite{quan2021removing} sprayed water
using sprinklers to generate rain streaks and mimic rainy scenes in the real world. By stopping spraying water, they obtained the clean background images. Although this method can obtain clean and rainy scene image pairs, it is time-consuming and labor-cumbersome.

Wang \MakeLowercase{\textit{et al.}} \cite{wang2019spatial} proposed a semi-automatic method that incorporates temporal properties of rain streaks and human supervision to generate high quality clean images from sequences of real rainy images. Due to the need for human supervision, this method is also time-consuming and labor-cumbersome. Li \MakeLowercase{\textit{et al.}} \cite{li2019single} synthesized rainy images from clean images of outdoor cloudy and fog-free scenes through Photoshop. However, the lack of consideration for scene depth in this method significantly deviates from the characteristics exhibited in real rainy images.

% Rendering-based methods
Garg and Nayar \cite{garg2006photorealistic} studied the visual appearance of rain streaks in detail for the first time. They developed a model for rain streak appearance and an image-based rendering algorithm for realistic rain rendering under different illumination. Based on their work, Halder \MakeLowercase{\textit{et al.}} \cite{halder2019physics} proposed a practical, physically-based approach to render realistic rain in images. The RainCityscapes \cite{hu2019depth} dataset was also created by adopting the images in the Cityscapes \cite{cordts2016cityscapes} dataset as clean background images, leveraging the rain streak appearance model. Despite these rendering-based methods capable of rendering realistic rain under specific illumination, they come with their limitations. Users are required to specify the rain parameters, and the raindrop distribution is simulated based on physical methods, which makes capturing the complex raindrop distribution in real rainy images challenging.

% DL-based methods
To efficiently generate diverse and non-repetitive rain streaks, some researchers have utilized deep learning for rainy image synthesis. Ni \MakeLowercase{\textit{et al.}} \cite{ni2021controlling} propose a GAN-based \cite{goodfellow2020generative} rain intensity controlling network to control the rain continuously in a bi-directional manner while preserving the scene-specific rain characteristics. Ye \MakeLowercase{\textit{et al.}} \cite{ye2021closing} proposed a CycleGAN-based \cite{zhu2017unpaired} unified framework that jointly learns rain generation and removal, offering a better approximation to real rain by learning physical degradation from real rainy images. Wang \MakeLowercase{\textit{et al.}} \cite{wang2021rain} proposed a VAE-based \cite{kingma2013auto} generative model to depict the generation process of rainy images, which can explore an implicit distribution of the rain layers in statistics, so they obtained an interpretable rain generator.

Although these DL-based methods can efficiently synthesize rainy images, they still have some limitations. Typically, these methods treat the rain layer as a gray-scale layer and blend it with the background image using linear overlaying. Consequently, they disregard other essential attributes of rain, including color and optical phenomena like refraction and transmission. Besides, existing synthetic rainy image datasets lack diversity in terms of illumination conditions, with images mainly in daytime illumination conditions, and few images in complex illumination conditions, like nighttime. Further, the resolution of these synthetic rainy images is typically low. In this paper, we propose a practical learning-from-rendering pipeline, which combines the realism advantages of rendering-based methods and the high-efficiency advantages of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets.

Shortcomings or irrationalities of existing rain datasets or acquisition methods are summarized in Table \ref{tab:related_work}.

\subsection{Generative Models for Image Synthesis}

As mentioned above, GANs and VAEs, as DL-based generative models, have been utilized in rainy image synthesis. Nevertheless, these generative models possess certain limitations. GANs \cite{goodfellow2020generative} produce high-quality images through adversarial training, but their optimization is challenging. In contrast, VAEs \cite{kingma2013auto} rely on likelihood estimation, allowing for faster generation of high-quality images, but the image quality may not be as good as that of GANs. Diffusion models \cite{ho2020denoising,rombach2022high,peebles2023scalable} have achieved state-of-the-art results in the field of image synthesis. However, the original diffusion models \cite{ho2020denoising} are slow in sampling, as they need a mass of time steps to generate a sample. Consequently, Latent Diffusion Models (LDMs) \cite{rombach2022high} use a two-stage pipeline, firstly compressing images into a low-dimensional latent space, and  training diffusion models on the compressed latent space, which speeds up the training and inference process with almost no reduction in synthesis quality. Given its high-quality image generation capability, our proposed rainy image generation network is based on LDM.

\subsection{Conditional Image Synthesis}
Conditional image synthesis allows users to control the image synthesis process, allowing for applications such as semantic image editing, image inpainting, etc. Conditional diffusion models \cite{batzolis2021conditional,rombach2022high} are capable of modeling conditional distributions of the form $p(z|y)$, which enables controlling the synthesis process through inputs $y$ such as text, semantic map, etc. 

High-resolution images synthesis is a challenging task, having high demand in terms of quality and computational complexity. Providing more useful guidance information \cite{liu2022asset,yi2020contextual,zheng2022bridging}, such as low-resolution images and intermediate generation results, is expected to improve the quality of generated images.

Inspired by ASSET \cite{liu2022asset}, our proposed HRIGNet is designed to introduce a guiding diffusion model in the LDM \cite{rombach2022high}. Given a clean background image and a mask of a rain layer, the HRIGNet generates the latent code of the rain layer image with a guiding diffusion model. The latent code is then used to guide the diffusion process for high-resolution image synthesis.

\section{Learning From Rendering}
\label{section:pipeline}

\begin{figure*}[ht] \centering
    \includegraphics[width=0.88\textwidth]{hrig/Figure_Pipeline/RenderingLearningPipeline.pdf}
    % \captionsetup{font={small},justification=raggedright}
    \caption{Overview of our learning-from-rendering pipeline for high-resolution rainy image synthesis. The top is the rendering stage, the bottom is the learning stage, and the two stages are interlinked via rainy image datasets.} 
    \label{fig:figure_pipeline}
\end{figure*}

In rainy weather, images captured by cameras are usually disturbed by rain in the scene and suffer from degradation such as rain streaks, raindrops, fog-like rain, etc. In this paper, we mainly focus on the phenomenon of rain streaks, which occurs when falling raindrops produce motion-blurred streaks during the exposure time of the camera. Rainy images mentioned in this paper refer to images that have been degraded by rain streaks.

To effectively synthesize a large quantity of high-resolution rainy images in complex illumination conditions, we propose a practical two-stage learning-from-rendering pipeline, as shown in Fig. \ref{fig:figure_pipeline}. Specifically, the pipeline combines rendering-based and learning-based methods, and consists of two stages: the rendering stage and the learning stage. In the rendering stage, we use a rendering-based method to render realistic high-resolution paired rainy-clean images and create paired rainy-clean image datasets. In the learning stage, we train a rainy image generation network using the rendered datasets to efficiently generate high-resolution rainy images. The pipeline combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets.

\subsection{Rendering Stage}

To train a high-quality rainy image generation network, we need realistic paired rainy-clean image datasets under various illumination conditions. Considering that collecting real rainy images captured by cameras is time-consuming, labor-cumbersome, and difficult to obtain corresponding background images, we use a rendering-based method.

Offline rendering techniques based on ray tracing algorithm \cite{pharr2016physically} can simulate most natural phenomena of object surface interactions in the real world and produce realistic images. Currently, these techniques have found extensive use in the fields of movies, animation, and design. Blender \cite{blender2018} is a popular free and open-source 3D creation suite utilized to create 3D scene models with ease, supporting various common light sources. Cycles, Blender's powerful built-in unbiased path-tracer engine can render realistic rainy images.

We use Blender and the image-based rain rendering algorithm \cite{garg2006photorealistic,halder2019physics} in the rendering stage. The rendering pipeline is shown in Fig. \ref{fig:figure_pipeline}. Specifically, we use modeling tools to create 3D scene models and render them with Cycles engine to obtain background scene RGB images and depth images. 
% Based on existing theoretical research on the distribution of raindrop size, density, and velocity, we implement a raindrop physical particle simulator to generate raindrop particles in the scene. According to the camera parameters and depth map of the scene, we eliminate invisible raindrops to reduce the amount of rendering data. 
Based on our raindrop physical particle simulator, we generate raindrop particles in the scene, excluding invisible ones to reduce the amount of rendering data.
With the rain streak rendering algorithm of Garg and Nayar \cite{garg2006photorealistic}, we sample rain streaks from the rain streak database and project them in the image.
Then each rain streak pixel is reconstructed into a quad to obtain the rain streaks 3D models. After merging the rain streaks 3D models with the background scene 3D model, we render the result rainy scene 3D model to obtain illuminated rain layers.
Finally, we blend rain layers with background images to obtain synthetic rainy images. Each image pair therefore includes a background image, a depth image, a rain layer image, and a rainy image.

\textbf{Background rendering.} With the Cycles engine, the pre-created scene 3D model is rendered to obtain realistic background RGB images and depth images. To capture the scene under different illumination conditions in a full day, we use procedural sky in the scene and set equal time intervals between 0 and 24 hours. For the camera parameter settings, we used focal lengths between 30mm and 50mm, and a shutter speed of 1/60s \cite{garg2006photorealistic} (for motion blur effects in dynamic scenes).

\textbf{Raindrops generation.} To accurately align with the size and distribution of raindrops in the real world, we implement a raindrop particle physics simulator with reference to existing theoretical studies of raindrop dynamics \cite{duhanyan2011below,mircea1998theoretical,best1950size,kessler1969distribution}. The simulation involves the size distribution, terminal velocity, and spatial distribution of raindrops.

Rain attributes, including the intensity and direction of rain, can be controlled as input parameters in our simulator. The intensity of rain is  the volume of water delivered to the ground per unit of ground surface and per unit of time \cite{duhanyan2011below}. Rain can be classified into light rain, moderate rain and heavy rain \cite{mircea1998theoretical,duhanyan2011below} according to the intensity. Our HRI dataset (presented in Sect. \ref{sect:hri_dataset}) includes different intensities of rain ranging from 5 to 200 mm/h, covering common intensities. The rain size distribution (RSD) and the terminal velocity of raindrops is rather dependent on the intensity of rain. The exponential RSD of Marshall and Palmer \cite{best1950size} and the terminal velocity of Kessler \cite{kessler1969distribution} are used here. For simplicity, we use a random uniform distribution to simulate the initial spatial distribution of raindrops and add additional velocity to the raindrops based on the input wind size and direction.

More information about our raindrop particle physics simulator can be found in Appendix \ref{append:simulation}.

\textbf{Rain streaks generation.} Considering that the raindrops generated by our simulator are not fully visible to the camera, we cull the invisible raindrops to reduce the amount of rendering data. Based on the camera parameters, we first cull the raindrops outside the frustum. Then, based on the scene depth map and the position of the raindrops, we further cull the raindrops that are occluded by the scene. We sample rain streaks from the rain streak database \cite{garg2006photorealistic} and project them in the image according to the image-based rain rendering algorithm \cite{halder2019physics}.

\textbf{Rain streaks rendering.}  In the image-based rain rendering algorithm \cite{garg2006photorealistic}, raindrops are needed to be illuminated with the light sources in the scene. To give the raindrops consistent illumination with the background scene, we create a quad at the position of each raindrop pixel in the scene, producing the rain streaks 3D models. After combining the rain streaks 3D models with the scene 3D model, we render them with the same illumination conditions as the background scene to obtain a illuminated rain layer.

\textbf{Rainy image composition.} After obtaining the background images and illuminated rain layer images, we composite them to obtain rainy images. According to the image-based rain rendering algorithm  \cite{halder2019physics}, suppose $\mathbf{x}$ a pixel in the background image $I$ and $\mathbf{x}'$ the overlapping coordinates in rain layer $S'$, the result of the blending is obtained with:
\begin{equation}
    I_{rainy}(\mathbf{x})=\frac{T-S'_{\alpha}(\mathbf{x'})\tau_1}{T}I(\mathbf{x})+S'(\mathbf{x'})\frac{\tau_1}{\tau_0},
\end{equation}
where $S'_{\alpha}(\mathbf{x'})$ is the alpha channel of the rain layer, $T$ is the targeted exposure time, $\tau_0=\sqrt{10^{-3}}/50$ is the time for which the raindrop remained on one pixel in the streak database, and $\tau_1$ the same measure according to our physical simulator.

% Specifically, we create scene models employing modeling tools, and create raindrop models using the particle simulator. These models are combined to create rainy scene models. After setting other environment and rendering parameters such as lighting and exposure time, we use the rendering engine to render background images, rain mask images, and rainy scene images to form image pairs.

% \begin{figure}[ht] \centering
%     \includegraphics[width=0.5\textwidth]{hrig/Figure5/RenderingPipeline.pdf}
%     \caption{The rendering pipeline of paired rainy images.} 
%     \label{fig:figure5}
% \end{figure}

\subsection{High-resolution Rainy Image Dataset}
\label{sect:hri_dataset}

In the rendering stage, we create a High-resolution Rainy Image (HRI) dataset. The HRI dataset comprises a total of 3,200 image pairs. Each image pair comprises a clean background image, a depth image, a rain layer mask image, and a rainy image. As shown in Table \ref{tab:table1}, it contains three scenes: lane, citystreet and japanesestreet, with image resolutions of $2048\times1024$. The lane scene contains 1,600 image pairs, consisting of images from 4 camera viewpoints, with each viewpoint containing 100 images of different moments, and each moment containing 4 different intensities of rainy scenes. The citystreet scene contains 600 image pairs, consisting of images from 6 camera viewpoints, with each viewpoint containing 25 images of different moments, and each moment containing 4 different intensities of rainy scenes. The japanesestreet scene contains 1,000 image pairs, consisting of images from 10 camera viewpoints, with each viewpoint containing 25 images of different moments, and each moment containing 4 different intensities of rainy scenes. Some rainy images of the HRI dataset are shown in Fig. \ref{fig:figure_hri}.

We split the HRI dataset into training set and test set according to camera viewpoints. For the lane scene, the training set contains images from 3 camera viewpoints, and the test set contains images from 1 camera viewpoint. For the citystreet scene, the training set contains images from 5 camera viewpoints, and the test set contains images from 1 camera viewpoint. For the japanesestreet scene, the training set contains images from 8 camera viewpoints, and the test set contains images from 2 camera viewpoints. Therefore, the training set comprises a total of 2,500 image pairs, and the test set comprises a total of 700 image pairs. 
% More detail about our HRI dataset can found in Appendix ???\ref{append:hri}.

\begin{table}\centering
    % \captionsetup{font={small}}
    \caption{Overview of the HRI dataset.}
    \resizebox*{0.48\textwidth}{!}{
        \begin{tabular}{ccccccc}
            \hline
            scene & 
            \makecell[c]{ dataset \\type}& 
            % dataset type& 
            resolution & 
            viewpoints &
            moments & 
            intensities& 
            \makecell[c]{image \\ pairs}\\ 
            \hline
            \multirow{2}{*}{lane} & training set & 
            \multirow{2}{*}{$2048\times1024$} & 3 & 
            \multirow{2}{*}{100} & 
            \multirow{2}{*}{4} & 1,200 \\ 
            ~ & test set & ~ & 1 & ~ & ~ & 400 \\ 
           \multirow{2}{*}{citystreet} & training set &
           \multirow{2}{*}{$2048\times1024$} & 5 & 
           \multirow{2}{*}{25} & 
           \multirow{2}{*}{4} & 500 \\ 
            ~ & test set & ~ & 1 & ~ & ~ & 100 \\ 
           \multirow{2}{*}{japanesestreet} & training set &
           \multirow{2}{*}{$2048\times1024$} & 8 & 
           \multirow{2}{*}{25} & 
           \multirow{2}{*}{4} & 800 \\ 
            ~ & test set & ~ & 2 & ~ & ~ & 200 \\ 
            \hline
        \end{tabular}
    }
    \label{tab:table1}

\end{table}



\begin{figure*}[t] \centering
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/front_10mm_frame_0900.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/front_25mm_frame_0200.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/front_50mm_frame_0800.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/low_10mm_frame_0900.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/mid_25mm_frame_0680.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/lane/side_50mm_frame_0850.jpg}
    \\
    \vspace{0.1cm}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/front_10mm_frame_0010.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/back_10mm_frame_0110.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/far_10mm_frame_0030.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/sideleft_10mm_frame_0150.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/sideright_10mm_frame_0010.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/citystreet/sideinner_50mm_frame_0250.jpg}
    \\
    \vspace{0.1cm}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera1_10mm_frame_0070.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera2_10mm_frame_0110.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera3_10mm_frame_0030.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera4_10mm_frame_0250.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera5_10mm_frame_0110.jpg}
    \includegraphics[width=0.161\textwidth]{hrig/Figure_HRI/japanesestreet/camera6_10mm_frame_0050.jpg}
    % \captionsetup{font={small},justification=raggedright}
    \caption{Rainy images of the HRI dataset. The first row is the rainy images of the lane scene. The second row is the rainy images of the citystreet scene. The third row is the rainy images of the japanesestreet scene.} 
    \label{fig:figure_hri}
\end{figure*}

\begin{figure}[!ht] \centering
    \includegraphics[width=0.5\textwidth]{hrig/Figure_HRIGNet/HRIGNet.pdf}
    % \captionsetup{font={small},justification=raggedright}
    \caption{The overall architecture of HRIGNet. We use a guiding diffusion model to predict the latent code of rain layer images, and then use them together with the latent code of masked background images as conditioning to guide the synthesis of high-resolution rainy images.} 
    \label{fig:figure_hrig}
\end{figure}

\section{High-resolution Rainy Image Generation Network}
\label{section:hrig}


In the learning stage, we propose a High-resolution Rainy Image Generation Network (HRIGNet), which synthesizes high-resolution rainy images from clean background images and corresponding rain layer masks. More specifically, given a RGB background image and a mask indicating the positions of rain streaks in the scene, HRIGNet can generate rain streaks at the corresponding positions, with illumination conditions and color appearance consistent with the background image. In addition, our method is capable of generating high-resolution images up to $2048\times1024$ resolution.

The architecture of HRIGNet is shown in Fig. \ref{fig:figure_hrig}. According to the LDM \cite{rombach2022high}, to reduce the computational cost of training diffusion models on high-resolution images, an autoencoding model \cite{dosovitskiy2020image} is used to learn a latent space that is perceptually equivalent to the image space. The encoder can compress images perceptually to obtain latent code in the latent space that is equivalent to pixels in the image space. So the forward and reverse processes of diffusion models can be conducted in the latent space. Finally, the output latent code is decoded back into the image space by the decoder.

% Guiding Transformer
% To control the image synthesis process of diffusion models, we use in-context and cross-attention as two conditional mechanisms to control the reverse process. Inspired by ASSET \cite{liu2022asset}, we first use a guiding Transformer to predict the latent code of low-resolution rainy images, and then use the latent code to enhance the underlying UNet backbone \cite{ronneberger2015u} of diffusion models with the cross-attention conditioning mechanism. The self-attention mechanism of Transformers enables them to operate long-range interactions between different parts of images, but also brings higher computational cost. So we first predict the rainy image at a low resolution. To impose stronger constraints on the image synthesis process, we composite the background image and rain layer mask as a masked image, and use it as a conditioning of the reverse process with the concatenation conditioning mechanism.

% Guiding Diffusion
To control the image synthesis process of diffusion models, we use concatenation and cross-attention as two conditional mechanisms to control the reverse process. Inspired by ASSET \cite{liu2022asset}, we first use a guiding diffusion model to predict the latent code of rain layer images, and then use the latent code to enhance the underlying UNet backbone \cite{ronneberger2015u} of diffusion models via the cross-attention conditioning mechanism. The predicted rain layer images can provide more guidance information to improve the quality of generated rainy images. To impose stronger constraints on the image synthesis process, we composite the background image and rain layer mask as a masked image, and use it as conditioning for the reverse process via the concatenation conditioning mechanism.

\subsection{Latent Diffusion Models}

Diffusion models\cite{ho2020denoising,rombach2022high,peebles2023scalable} are probabilistic models that gradually add noise to the data by traversing a Markov chain of $T$ time steps, transforming the distribution of real data to a Gaussian distribution. The forward process of diffusion models adds noise step by step to the real data $x_0 \sim q(x_0)$, with $q(x_t|x_0)=\mathcal{N}(x_t;\sqrt{\bar{\alpha }_t }x_0,(1-\bar{\alpha}_t )\mathbf{I})$, where $\bar{\alpha}$ is a hyperparameter, and $x_1,\dots,x_T$ are latent codes with the same dimensions as the real data $x_0$. By applying the reparameterization trick \cite{ho2020denoising},  we can sample $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon _t$ at any time step, where $\epsilon_t \sim \mathcal{N}(0,\mathbf{I})$. Diffusion models are trained to learn the reverse process, which is the inverse transformation of the forward process, gradually removing noise from the variable $x_t \sim \mathcal{N}(0,\mathbf{I})$ by applying a learnable Gaussian transformation $p_{\theta}(x_{t-1}|x_t)=\mathcal{N}(x_{t-1};\mu_{\theta}(x_t),\Sigma_{\theta}(x_t))$, where neural networks are used to predict the statistics $\mu_{\theta}$ and $\Sigma_{\theta}$ of $p_\theta$. $\mu_{\theta}$ is then reparameterized as a denoising network $\epsilon_{\theta}(x_t,t)$, and the corresponding objective can be simplified as 
\begin{equation}
    L_{DM}=\mathbb{E}_{x,\epsilon\sim \mathcal{N}(0,1),t}\Big [\left \|  \epsilon - \epsilon_{\theta}(x_t,t) \right \|_2^2 \Big ],
\end{equation}
 where $t$ is uniformly sampled from $\{ 1,\dots,T \}$.

In order to reduce the computational cost of training diffusion models on high-resolution images, LDM \cite{rombach2022high} first trains a perceptual compression autoencoding model VQGAN \cite{esser2021taming}. VQGAN consists of an encoder $\mathcal{E}$ and a decoder $\mathcal{D}$. The encoder compresses images from the high-dimensional image space to a low-dimensional latent space, where high-frequency and imperceptible details are abstracted away. This makes training of diffusion models more efficient in the low-dimensional latent space.
Given a high-resolution RGB rainy image $x^{rainy}$, the corresponding latent code encoded by the encoder is $z^{rainy}$. So the objective of LDM can be expressed as 
\begin{equation}
L_{LDM}=\mathbb{E}_{\mathcal{E}(x^{rainy}),\epsilon\sim \mathcal{N}(0,1),t} \Big[\left \|  \epsilon - \epsilon_{\theta}(z_t^{rainy},t) \right \|_2^2 \Big].
\end{equation}

Here, the neural backbone $\epsilon_{\theta}(\circ,t)$ of LDM is realized as a UNet \cite{ronneberger2015u}.

\subsection{Guiding Diffusion Model}

% Guiding Diffusion
Using the intermediate generation results as guidance can provide more information for high-resolution image synthesis, which is expected to improve the quality of generated images. Therefore, we use a guiding diffusion model (GDM) trained on the latent codes of rain layer images. Since the latent codes have a lower dimensionality, the GDM can efficiently perform the training and sampling process. The GDM coarsely predicts the latent codes of the rain layer images, which are used as conditioning in the reverse process of the diffusion model to guide high-resolution image synthesis. 

Specifically, we first composite the input RGB background image and rain layer mask to obtain the masked image $x^{mask}$. Then the masked image is encoded into the latent space to obtain the latent codes $z^{mask}$, which are input to the GDM to predict the latent code of rain layer image $\tilde{z}^{rain}$. The objective of GDM can be expressed as\par

\begin{small}
\begin{equation}
L_{GDM}= \mathbb{E}_{\mathcal{E}(x^{rain}),\epsilon\sim \mathcal{N}(0,1),t} \Big [\left \|  \epsilon - \epsilon_{\theta}(z_t^{rain},t,z^{mask}) \right \|_2^2 \Big ].
\label{equ:loss_gdm}
\end{equation}
\end{small}%

Here, the neural backbone of GDM is also realized as a UNet \cite{ronneberger2015u}.

\subsection{Conditioning Mechanisms}
By modeling the reverse process of diffusion models as conditional distributions $p(z|y)$, constraints can be imposed on the reverse process to control the image synthesis. This can be implemented with a conditional denoising network $\epsilon_{\theta}(z_t,t,y)$. In the context of image synthesis, LDMs use the cross-attention mechanism \cite{vaswani2017attention} to enable inputs from different modalities to serve as conditionings for DMs. Our method leverages the predicted latent code of the rain layer image by GDM as conditioning via the cross-attention mechanism. Specifically, $\tilde{z}^{rain}$ is mapped to the intermediate layers of the UNet via a cross-attention layer represented as Attention$(Q,K,V)=$softmax$(\frac{QK^T}{\sqrt{d}})\cdot V$, where 
$$Q=W^{(i)}_Q\cdot \varphi_i(z_t^{rainy}) ,K=W^{(i)}_K\cdot \tilde{z}^{rain},V=W^{(i)}_V\cdot \tilde{z}^{rain}.$$ 
Here, $\varphi_i(z_t^{rainy})$ represents the intermediate representation of $\epsilon_{\theta}$ implemented by UNet, and $W^{(i)}_Q, W^{(i)}_K, W^{(i)}_V$ are learnable projection matrices.

Moreover, to enhance the constraints on the image synthesis process, we utilize the concatenation conditioning mechanism, in conjunction with the cross-attention conditioning mechanism. We use the latent code of masked image $z^{mask}$ as conditioning of the reverse process via the concatenation conditioning mechanism. Specifically, the input of the reverse process are $z_t^{concat} = [z^{rainy}_t, z^{mask}]$. 

Via the concatenation and cross-attention conditioning mechanisms, we then learn the conditional LDM via\par
\begin{small}
\begin{equation}
L_{LDM}=\mathbb{E}_{\mathcal{E}(x^{rainy}),\epsilon\sim \mathcal{N}(0,1),t} [\left \|  \epsilon - \epsilon_{\theta}(z_t^{concat},t,\tilde{z}^{rain}) \right \|_2^2].
\label{equ:loss_ldm}
\end{equation}
\end{small}%

% So the total objective of HRIGNet is 
Combining the two objective functions in Eq. \ref{equ:loss_gdm} and Eq. \ref{equ:loss_ldm}, the total objective of HRIGNet is
\begin{equation}
L_{HRIG} = L_{GDM} + L_{LDM}
\end{equation}

% High-resolution image generation
\subsection{Image Generation}

Our proposed HRIGNet is based on LDM, where the encoder of VQGAN compresses the original image into a low-dimensional latent space. For images of size $H\times W$ and a number of downsampling blocks of $m$, the input latent code to the diffusion model is of size $H/2^m \times W/2^m$, which reduces the spatial cost of training and speeds up the training and inference processes. However, when $m$ exceeds a critical value, the reconstruction quality degrades \cite{esser2021taming}. Therefore, during training, we have to work patch-wise and crop images. Specifically, we train our model using images of up to $512\times512$ resolution.
During inference, to generate higher-resolution images, considering that spatial conditioning information \cite{esser2021taming} is available in our model, we can simply segment background images into patches of size $512\times512$ and use them as conditioning. By merging the output results, we can generate images with resolutions higher than $512\times512$. In our experiments, our model can generate images up to $2048\times1024$ resolution.

In our implementation, we find that if background images is simply segmented into blocks and input them into the model, the output results will have slight variations in hue, which will cause the merged output to produce a block effect. To fix this issue, we use the rain layer masks to blend the generated images with the background images.

% Post-processing

% \subsection{Post-processing}

% Given a clear background image and a rain layer mask, the HRIGNet generates a rainy image. For simplicity, we ignore the effects of weather such as fog, and assume that the color in areas free of rain is identical to that of the clear background image. Typically, rain streaks of higher intensity would enhance the pixel colors \cite{yang2020single} while the ones with low intensity would not darken the pixel colors, instead, maintain their original intensity, because raindrops are translucent and dark rain streaks are invisible.

% Therefore, after acquiring a rainy image generated by the model, we perform a post-processing operation. The generated rainy image $\tilde{x}^{high}$ and the clear background image $x_{clear}^{high}$ are blended through the Lighten blend mode \cite{ma2011mathematic}. To avoid distortions, we apply the rain layer mask to preserve the original pixel colors outside the rain area. That is, 
% \begin{equation}
%     x_{output}=max\{M\odot  \tilde{x}^{high},x^{high}_{clear} \},
% \end{equation}
% here $M$ is the rain mask layer and $\odot$ is element-wise multiplication.

% Fig. \ref{fig:figure4} shows the comparison between the generated rainy image and the output image after post-processing. It can be seen that the output image is better in visual appearance.

% \begin{figure}[t] \centering
%     \includegraphics[width=0.18\textwidth]{hrig/Figure4/Low_F880_background.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_gt.jpg}
%     \\
%     \makebox[0.18\textwidth]{Background image}
%     \makebox[0.18\textwidth]{Ground Truth}
%     \\
%     \includegraphics[width=0.18\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_pred.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_output.jpg}
%     \\
%     \makebox[0.18\textwidth]{Generated image}
%     \makebox[0.18\textwidth]{Output image}
    
%     \captionsetup{font={small}}
%     \caption{Visual comparison of generated rainy image and output image.} 
%     \label{fig:figure4}
% \end{figure}

% \begin{figure}[t] \centering
%     \includegraphics[width=0.155\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_gt.jpg}
%     \includegraphics[width=0.155\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_pred.jpg}
%     \includegraphics[width=0.155\textwidth]{hrig/Figure4/Low_I200000_Angle30_F880_output.jpg}
%     \\
%     \makebox[0.155\textwidth]{Ground Truth}
%     \makebox[0.155\textwidth]{Generated image}
%     \makebox[0.155\textwidth]{Output image}
%     \\
%     \caption{Visual comparison of generated rainy image and output image.} 
%     \label{fig:figure4}
% \end{figure}


\section{Experimental Results}
\label{section:experiment}

In this section, we compare our proposed HRIGNet with some commonly used baseline image generative models to evaluate its performance in high-resolution rainy image synthesis, and conduct an ablation study to evaluate the effect of using guiding models with different guiding information and resolutions. To further validate that our HRIGNet can help improve the robustness of deep derainers to rainy images in the real world, we retrain the derainers on datasets augmented by the HRIGNet and evaluate their deraining performance.

\subsection{Implementation Details}

To train our HRIGNet, we first use rain layer images with a size of $512\times 512$ to pretrain a GDM based on the loss function in Eq. \ref{equ:loss_gdm}. Then with the parameters of the GDM fixed, the HRIGNet is trained using rainy images with a size of $512\times 512$ based on the loss function in Eq. \ref{equ:loss_ldm}.

We adopt AdamW \cite{kingma2014adam,loshchilov2017decoupled} optimizer in the training process of both the GDM and the HRIGNet. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. The initial learning rate of the diffusion model is set as $2\times10^{-6}$, the batch size is 1, the image size of the UNet backbone is $128\times 128$, and the model channels are 224.

The experiments are implemented on PyTorch \cite{paszke2019pytorch} platform with an NVIDIA GeForce RTX 3090 GPU.

\subsection{Compare With Baselines}

\begin{figure*}[ht] \centering
    \captionsetup[subfloat]{labelfont=scriptsize,textfont=scriptsize}
    \subfloat[Background]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/Background_draw2.jpg}
    }
    \subfloat[LDM \cite{rombach2022high}]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/LDM_draw2.jpg}
    }
    \subfloat[DiT \cite{peebles2023scalable}]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/DiT_draw2.jpg}
    }
    \\
    \subfloat[Ground truth]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/Rainy_draw2.jpg}
    }
    \subfloat[CycleGAN \cite{zhu2017unpaired}]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/CycleGAN_draw2.jpg}
    }
    \subfloat[HRIGNet (ours)]{
        \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/HRIG_draw2.jpg}
    }

    % \captionsetup{font={small},justification=raggedright}
    \caption{Visual comparison of generated rainy images from different image generative models. Our HRIGNet can better capture the illumination and color in the background image, and map them to the generated rain layer.} 
    \label{fig:figure_baseline}
\end{figure*}

\begin{table}[t]\centering
    % \captionsetup{font={small}}
    % \caption{Quantitative evaluation of baseline models and HRIGNet at 512$\times$512 resolution.}
    \caption{Quantitative evaluation of baseline models and HRIGNet.}
    \resizebox*{0.48\textwidth}{!}{
        \begin{tabular}{cccccc}
            \hline
            method & patch size & FID$\downarrow$ & LPIPS$\downarrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ \\ 
            \hline
            LDM \cite{rombach2022high} & 512$\times$512 & 46.583 & 0.241 & 0.703 & 16.652  \\ 
            DiT \cite{peebles2023scalable} & 512$\times$512 & 164.977 & 0.490 & 0.548 & 12.097\\ 
            CycleGAN \cite{zhu2017unpaired} & 512$\times$512 &  47.073 & 0.271  & 0.639  & \textbf{21.604}
            \\
            HRIGNet & 512$\times$512 & \textbf{32.111} & \textbf{0.205} & \textbf{0.747} & 18.595  \\ 
            \hline
        \end{tabular}
    }
    \label{tab:table_baseline}
\end{table}

In order to evaluate the performance of HRIGNet in high-resolution rainy image synthesis, we compare it with several baseline image generative models: LDM \cite{rombach2022high}, DiT \cite{peebles2023scalable} and CycleGAN \cite{zhu2017unpaired}. The evaluation metrics used are FID \cite{heusel2017gans}, LPIPS \cite{zhang2018unreasonable}, SSIM \cite{wang2004image}, and PSNR \cite{hore2010image}. More information about the model settings can be found in Appendix \ref{append:experiments_baseline}. As the results shown in Table \ref{tab:table_baseline}, our model achieves state-of-the-art results in FID, LPIPS and SSIM. Fig. \ref{fig:figure_baseline} illustrates a comparison of rainy image synthesis results of these methods. As seen, our method can well capture the illumination and color in the background image, and map them to the generated rain layer. As a result, the rain layer is endowed with a visually plausible color appearance that matches the background image.

% \begin{figure*}[ht] \centering
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/Background_draw2.jpg}
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/LDM_draw2.jpg}
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/DiT_draw2.jpg}
%     \\
%     \makebox[0.32\textwidth]{\scriptsize{Background}}
%     \makebox[0.32\textwidth]{\scriptsize{LDM \cite{rombach2022high}}}
%     \makebox[0.32\textwidth]{\scriptsize{DiT \cite{peebles2023scalable}}}
%     \\
%     \vspace{0.1cm}
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/Rainy_draw2.jpg}
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/CycleGAN_draw2.jpg}
%     \includegraphics[width=0.32\textwidth]{hrig/Figure_Baseline/HRIG_draw2.jpg}
%     \\
%     \makebox[0.32\textwidth]{\scriptsize{Ground truth}}
%     \makebox[0.32\textwidth]{\scriptsize{CycleGAN \cite{zhu2017unpaired}}}
%     \makebox[0.32\textwidth]{\scriptsize{HRIGNet (ours)}}
%     \captionsetup{font={small},width=0.92\textwidth}
%     \caption{Visual comparison of generated rainy images from different image generative models. Our HRIGNet can better capture the illumination and color in the background image and map them to the generated rain layer.} 
%     \label{fig:figure_baseline}
% \end{figure*}






\begin{table}[t]\centering
    % \captionsetup{font={small}}
    % \caption{Quantitative evaluation of guiding models with different guiding information and resolutions.}
    \caption{Quantitative evaluation of guiding models.}
    \resizebox*{0.48\textwidth}{!}{
        \begin{tabular}{cccccc}
            \hline
            \makecell[c]{ guiding \\ information } & resolution & FID$\downarrow$ & LPIPS$\downarrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ \\ 
            \hline
            Rainy Image & 256$\times$256 & 150.851 & 0.505 & 0.389 & 13.692 \\ 
            Rain Layer & 256$\times$256 & 149.779 & 0.506 & 0.390 & 13.674 \\ 
            Rain Layer  & 512$\times$512 & \textbf{32.111} & \textbf{0.205} & \textbf{0.747} & \textbf{18.595} \\
            \hline
        \end{tabular}
    }
    \label{tab:table_guiding}
\end{table}

\begin{table}[t]\centering
    % \captionsetup{font={small}}
    % \caption{Quantitative evaluation of HRIGNet with different backbones at 512$\times$512 resolution.}
    \caption{Quantitative evaluation of HRIGNet with different backbones.}
    \resizebox*{0.48\textwidth}{!}{
        \begin{tabular}{cccccc}
            \hline
            backbone & resolution & FID$\downarrow$ & LPIPS$\downarrow$ & SSIM$\uparrow$ & PSNR$\uparrow$ \\ 
            \hline
            Tramsformer & 512$\times$512 & 288.696 & 0.612 & 0.515 & 15.809  \\ 
            UNet & 512$\times$512 & \textbf{32.11} & \textbf{0.205} & \textbf{0.747} & \textbf{18.595} \\ 
            \hline
        \end{tabular}
    }
    \label{tab:table_backbone}
\end{table}

\subsection{Ablation Study}


We conduct an ablation study to evaluate the effect of using guiding models with different guiding information and resolutions. Specifically, we compare the performance of using rainy image and rain layer as guiding information, and resolution of $256\times 256$ and $512\times 512$ in HRIGNet. As shown in Table \ref{tab:table_guiding}, the model using rain layer of size $512\times 512$ as guidance achieves the best result in all metrics. 


We also conduct an ablation study to investigate the effect of using different backbones of the diffusion model on HRIGNet. Specifically, we compare two popular backbone architectures, UNet \cite{rombach2022high} and Transformer \cite{peebles2023scalable}. As shown in Table \ref{tab:table_backbone}, the results indicate that HRIGNet using UNet backbone outperforms the one using Transformer backbone in all metrics. According to DiT \cite{peebles2023scalable}, the scaling properties of the Transformer can extend to diffusion models with Transformer backbones. However, models with Transformer backbones tend to underperform when the model size is inadequate due to their inherent scaling properties. Furthermore, in the experiments, we also find that models with Transformer backbones have a slower convergence speed. Therefore, we finally adopt the UNet backbone in our model.

More information about the model settings can be found in Appendix \ref{append:experiments_ablation}.


\subsection{Rain Removal Experiments}
\label{section:experiment_removal}

\begin{table*}[!ht]\centering
    \centering
    % \captionsetup{font={small}}
    % \caption{Generalization performance comparisons on the test data of SPA-Data. ``+'' denotes the augmented training by physics-based rendering \cite{halder2019physics}, ``++'' denotes the augmented training our method. $\Delta\uparrow$ represents the performance gain brought by the augmented rains generated by our rain generator.}
    \caption{Generalization performance comparisons on the test data of SPA-Data.}
    \resizebox*{\textwidth}{!}{
        \begin{tabular}{c|c|cccc|cccc|cccc|cccc}
        \hline
           \multicolumn{2}{c|}{methods} & PReNet & PReNet+ & PReNet++ & $\Delta\uparrow$ & M3SNet & M3SNet+ & M3SNet++ & $\Delta\uparrow$ & SFNet & SFNet+ & SFNet++ & $\Delta\uparrow$ & Restormer & Restormer+ & Restormer++ & $\Delta\uparrow$ \\ \hline
            \multirow{3}{*}{RainTrainL} & FID$\downarrow$ & 45.981 & 45.586 & \textbf{43.706} & 2.275 & 47.989 & 49.173 & \textbf{46.889} & 1.1 & 48.807 & 46.155 & \textbf{44.982} & 3.825 & 50.785 & 47.795 & \textbf{46.476} & 4.309 \\ 
            ~ & SSIM$\uparrow$ & 0.941 & 0.942 & \textbf{0.946} & 0.005 & 0.939 & 0.938 & \textbf{0.943} & 0.004 & 0.939 & 0.941 & \textbf{0.946} & 0.007 & 0.932 & 0.938 & \textbf{0.943} & 0.011 \\ 
            ~ & PSNR$\uparrow$ & 33.493 & 33.168 & \textbf{33.614} & 0.121 & 33.147 & 33.062 & \textbf{33.173} & 0.026 & 33.164 & 33.303 & \textbf{33.316} & 0.152 & 32.821 & 33.251 & \textbf{33.397} & 0.576 \\ \hline
            \multirow{3}{*}{Rain1400} & FID$\downarrow$ & 48.834 & 49.001 & \textbf{46.073} & 2.761 & 53.98 & 49.9 & \textbf{48.609} & 5.371 & 51.351 & 48.798 & \textbf{46.852} & 4.499 & 53.598 & 48.575 & \textbf{46.57} & 7.028 \\ 
            ~ & SSIM$\uparrow$ & 0.937 & 0.934 & \textbf{0.944} & 0.007 & 0.926 & 0.934 & \textbf{0.94} & 0.014 & 0.932 & 0.936 & \textbf{0.944} & 0.012 & 0.929 & 0.936 & \textbf{0.943} & 0.014 \\
            ~ & PSNR$\uparrow$ & 31.869 & 32.157 & \textbf{33.064} & 1.195 & 31.04 & 32.897 & \textbf{33.097} & 2.057 & 31.802 & 33.213 & \textbf{33.205} & 1.403 & 31.446 & 33.188 & \textbf{33.292} & 1.846 \\ \hline
        \end{tabular}
    }
    \label{tab:sirr_result}
\end{table*}

% \begin{figure*}[!ht] \centering
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/rain_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/prenet_rain1400_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/m3snet_rain1400_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/sfnet_rain1400_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/restormer_rain1400_904_draw.jpg}
%     \\
%     \makebox[0.19\textwidth]{\scriptsize{Input}}
%     \makebox[0.19\textwidth]{\scriptsize{PReNet}}
%     \makebox[0.19\textwidth]{\scriptsize{M3SNet}}
%     \makebox[0.19\textwidth]{\scriptsize{SFNet}}
%     \makebox[0.19\textwidth]{\scriptsize{Restormer}}
%     \\
%     \vspace{0.1cm}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/bg_904gt_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/prenet_rain1400_ratio1_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/m3snet_rain1400_ratio1_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/sfnet_rain1400_ratio1_904_draw.jpg}
%     \includegraphics[width=0.18\textwidth]{hrig/Figure_SIRR/restormer_rain1400_ratio1_904_draw.jpg}
%     \\
%     \makebox[0.19\textwidth]{\scriptsize{Ground truth}}
%     \makebox[0.19\textwidth]{\scriptsize{PReNet++}}
%     \makebox[0.19\textwidth]{\scriptsize{M3SNet++}}
%     \makebox[0.19\textwidth]{\scriptsize{SFNet++}}
%     \makebox[0.19\textwidth]{\scriptsize{Restormer++}}
%     \captionsetup{font={small},justification=raggedright}
%     \caption{Visual comparison of rain removal on a test image from SPA-Data. The first row is the input rainy image and the output of derainers trained on the original Rain1400 training set. The second row is the ground truth background image and the output of derainers trained on the augmented Rain1400 training set by our method.} 
%     \label{fig:figure_sirr}
% \end{figure*}


\begin{figure*}[!ht] \centering
    \captionsetup[subfloat]{labelfont=scriptsize,textfont=scriptsize}
    \subfloat[Input]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/rain_904_draw.jpg}
    }
    \subfloat[PReNet]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/prenet_rain1400_904_draw.jpg}
    }
    \subfloat[M3SNet]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/m3snet_rain1400_904_draw.jpg}
    }
    \subfloat[SFNet]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/sfnet_rain1400_904_draw.jpg}
    }
    \subfloat[Restormer]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/restormer_rain1400_904_draw.jpg}
    }
    \\
    \subfloat[Ground truth]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/bg_904gt_draw.jpg}
    }
    \subfloat[PReNet++]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/prenet_rain1400_ratio1_904_draw.jpg}
    }
    \subfloat[M3SNet++]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/m3snet_rain1400_ratio1_904_draw.jpg}
    }
    \subfloat[SFNet++]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/sfnet_rain1400_ratio1_904_draw.jpg}
    }
    \subfloat[Restormer++]{
        \includegraphics[width=0.185\textwidth]{hrig/Figure_SIRR/restormer_rain1400_ratio1_904_draw.jpg}
    }
    % \captionsetup{font={small},justification=raggedright}
    \caption{Visual comparison of rain removal results on a test image from SPA-Data. The first row is the input rainy image and the output of derainers trained on the original Rain1400 training set. The second row is the ground truth background image and the output of derainers trained on the Rain1400 training set augmented by our HRIGNet.} 
    \label{fig:figure_sirr}
\end{figure*}

 

With HRIGNet, sufficient rainy images can be automatically generated from background images and rain layer masks, enabling us to create new paired rainy image datasets or augment existing datasets. In this subsection, we use HRIGNet to augment the existing datasets with ratio 1, further improving the deraining performance of current DL-based derainers on real rain datasets.

We evaluate the effectiveness of the augmentation strategy benefitted from HRIGNet with latest DL-based SIRR methods, including PReNet \cite{ren2019progressive}, M3SNet \cite{gao2023mountain}, SFNet \cite{cui2022selective} and Restormer \cite{zamir2022restormer}. The training sets are common synthetic datasets, including RainTrainL \cite{yang2017deep} and Rain1400 \cite{fu2017removing}. We augment these datasets, retrain the derainers and compare their generalization performance on the real dataset SPA-Data \cite{wang2019spatial}.

The quantitative comparison is shown in Table \ref{tab:sirr_result}, where ``+'' denotes the augmented training with physics-based rendering \cite{halder2019physics} (where we use to produce rain layer masks for our HRIGNet) and ``++'' denotes the augmented training with our method. $\Delta\uparrow$ represents the performance gain brought by the augmented training with our method. As seen, our method improves the performance of all derainers to varying degrees. Fig. \ref{fig:figure_sirr} shows the visual comparison of rain removal results on a test image from SPA-Data. Under such a complex rain scene, these derainers with augmented training evidently remove rains. The results validate that the rainy scene images generated by HRIGNet can help improve the robustness of these deep derainers to rainy images in the real world.

\section{Conclusion And Limitations}
\label{section:conclusion}


We propose a practical two-stage learning-from-rendering pipeline for high-resolution rainy image synthesis, which combines the benefits of the realism of rendering-based methods and the high-efficiency of learning-based methods, providing the possibility of creating large-scale high-quality paired rainy-clean image datasets. In the rendering stage, we create a HRI dataset, which contains realistic high-resolution paired rainy-clean images of multiple scenes and various illumination conditions. In the learning stage, we propose a HRIGNet, which is designed to introduce a guiding diffusion model in the LDM. Experiments show that our model achieves SOTA results in most of the metrics compared to baseline image generative models. Additionally, our model is able to synthesize high-resolution rainy images up to $2048\times1024$ resolution. Rain removal experiments on real dataset validate that our method can help improve the robustness of deep derainers to rainy images in the real world.

Although our proposed method provides the possibility of creating large-scale high-quality paired rainy-clean image datasets, there are still avenues for further improvements. Our proposed learning-from-rendering pipeline combines the two stages via rainy image datasets, which makes it less efficient in the early stage of dataset creation. In future work, we would like to integrate the two stages more effectively. In addition, in rainy images, we only consider the most common rain streak phenomenon, neglecting phenomena such as splashes and wet ground in the scene, which may also have an impact on the performance of derainers. In our rainy image rendering algorithm, for simplicity, we only render the nearest rain streaks, which can cover most of the rain streaks in the scene, but there may be some rain streaks overlapping at the same pixel position. In the future, we will attempt to use order-independent transparency rendering algorithms for optimization. Furthermore, our HRIGNet generates rainy images using rain layer masks as input. In the future, we will attempt to combine it with a DL-based rain streak generator to generate diverse and non-repetitive rain streaks in the rainy images.



\appendices
\section{Raindrop Simulation}
\label{append:simulation}
In atmospheric modelling, scientists usually quantify rain using a parameter called intensity of rain, $I$. It is the volume of water  \cite{duhanyan2011below} delivered to the ground per unit of ground surface and per unit of time: 
\begin{equation}
    I=\frac{\textit{Volume of water}}{\textit{Surface of precipitation}\times \textit{duration}}.
\end{equation}

In the international system of units, the intensity of rain $I$ is in $m\cdot s^{-1}$. Now, in the literature, the intensity of rain is often expressed in $mm\cdot h^{-1}$. The conversion between two units can be expressed as: 
\begin{equation}
    I = x(mm/h) = x\times\frac{10^{-3}}{3600}(m/s).
\end{equation}

\textbf{Raindrop size distributions.} Let $D$ denote the diameter of raindrops. The number concentration of raindrops with a diameter between $D$ and $D+dD$ is then:
\begin{equation}
    dC(D)=N(D)dD,
\end{equation}
where, $N(D)$ is the raindrop size distribution (RSD). In the meteorological studies, exponential functions are often used to fit the observed RSDs.
\begin{equation}
    N(D)=Ae^{-\beta D}
\end{equation}

The exponential RSD of Marshall and Palmer \cite{best1950size} is one of the simplest and the most often used parameterisation to fit the RSDs: 
\begin{equation}
N(D)=8\times 10^6 e^{-4100I^{-0.21}D}.
\end{equation}

In order to sample raindrops based on the RSD, the cumulative distribution function of raindrop diameters is needed. Limiting the range of raindrop diameters to $D\in [D_{min},D_{max}]$, we can get the total number of raindrops in this range as:

\begin{equation}
\begin{aligned}
    N_{total} &=\int_{D_{min}}^{D_{max}} N(D)dD \\&=\frac{A}{\beta}(e^{-\beta D_{min}}-e^{-\beta D_{max}}).
\end{aligned}
\end{equation}

So the probability density function of $D$ is:
\begin{equation}
    P(D)=\frac{N(D)}{N_{total}}.
\end{equation}

The cumulative distribution function of $D$ is:
\begin{equation}
\begin{aligned}
    F(D)&=P(x\le D)=\int_{D_{min}}^{D}P(x)dx\\&=\frac{\int_{D_{min}}^{D}N(x)dx}{N_{total}}=\frac{e^{-\beta D_{min}}-e^{-\beta D}}{e^{-\beta D_{min}}-e^{-\beta D_{max}}}.
\end{aligned}
\end{equation}

In the implementation, based on the variable $u\in [0,1]$ obtained from sampling from a uniform random distribution, $D$ with distribution of $N(D)$ can be obtained by inverse transforming it using $F$.

Let $u=F(D)\in[0,1]$, then\par
\begin{small}
\begin{equation}
    D=F^{-1}(u)=\frac{ln[e^{-\beta D_{min}}-u(e^{-\beta D_{min}}-e^{-\beta D_{max}})]}{-\beta}.
\end{equation} 
\end{small}


\textbf{Raindrop terminal velocity.}  It is generally assumed that the raindrops fall at their terminal velocity, whatever their position (their height) in the atmosphere. The transient period to reach that speed is then totally neglected. We use the raindrops terminal velocity given by Kessler \cite{kessler1969distribution}:
\begin{equation}
    v_t=130 D^{0.5}
\end{equation}

% \section{HRI Dataset}
% \label{append:hri}
% Our proposed HRI dataset can be downloaded from \small{\href{https://pan.baidu.com/s/1RlzQq1JK-Eo63LjX81KHTA?pwd=qg2r}{https://pan.baidu.com/s/1RlzQq1JK-Eo63LjX81KHTA?pwd=qg2r}}. The \verb|clean| folder includes background RGB images and depth images of all scenes, and \verb|rainy| folder includes rain layer images, RGB rainy images and depth rainy images of all scenes. The files \verb|trainset.json| and \verb|testset.json| in the root folder are the sample lists of the training set and test set, respectively. Each sample includes the scene name, viewpoint name, rain intensity, and the path of the image pair.

% The Blender raw files of the three scenes can be downloaded from \small{\href{https://pan.baidu.com/s/1pSAIXNhL59FamDA6lrBhbA?pwd=4wwy}{https://pan.baidu.com/s/1pSAIXNhL59FamDA6lrBhbA?pwd=4wwy}}. The Blender files for rendering RGB and depth images of all viewpoints are included in the zip file of each scene.


\section{More Experiment Details}
\label{append:experiments}

\subsection{Baselines}
\label{append:experiments_baseline}

For the LDM model in Table \ref{tab:table_baseline} of the main paper, the initial learning rate is set as $2\times10^{-6}$, the batch size is 1, the image size of the UNet backbone is $128\times 128$, and the model channels are 224. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. The training epoch of the LDM model is 99.

For the DiT model in Table \ref{tab:table_baseline} of the main paper, the initial learning rate is set as $2\times10^{-6}$, the batch size is 1, the image size of the Transformer backbone is $128\times 128$, the patch size is 2, the hidden size is 768, the depth is 12, and the number of head is 12. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. The training epoch of the DiT model is 99.

For the CycleGAN model in Table \ref{tab:table_baseline} of the main paper, the initial learning rate is set as $5\times10^{-5}$, the batch size is 2, the architecture of the generator is resnet with 9 blocks. The default configuration of CycleGAN is used for other parameter settings. The training epoch of the CycleGAN model is 200.

For the HRIGNet in Table \ref{tab:table_baseline} of the main paper, the training epoch is 98.

\subsection{Ablation Study}
\label{append:experiments_ablation}

For the HRIGNet model with rainy image of size $256\times 256$ as guidance in Table \ref{tab:table_guiding} of the main paper, the initial learning rate is set as $2\times10^{-6}$, the batch size is 1, the image size of the UNet backbone is $128\times 128$, and the model channels are 224. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. For the guiding diffusion model, the image size of the UNet backbone is $64\times 64$, and the model channels are 224. The training epoch of the guiding diffusion model is 99, and the training epoch of the HRIGNet model is 88.

For the HRIGNet model with rain layer of size $256\times 256$ as guidance in Table \ref{tab:table_guiding} of the main paper, the initial learning rate is set as $2\times10^{-6}$, the batch size is 1, the image size of the UNet backbone is $128\times 128$, and the model channels are 224. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. For the guiding diffusion model, the image size of the UNet backbone is $64\times 64$, and the model channels are 224. The training epoch of the guiding diffusion model is 96, and the training epoch of the HRIGNet model is 88.

For the guiding diffusion model the HRIGNet model with rain layer of size $512\times 512$ as guidance in Table \ref{tab:table_guiding}, the image size of the UNet backbone is $128\times 128$, and the model channels are 224. The training epoch of the guiding diffusion model is 99.

For the HRIGNet model with Transformer backbone in Table \ref{tab:table_backbone} of the main paper, the initial learning rate is set as $2\times10^{-6}$, the batch size is 1. For the Transformer backbone, the image size is $128\times 128$, the patch size is 8, the hidden size is 384, the depth is 12, and the number of head is 12. The first stage model and the conditioning stage model share a same VQGAN model, with the parameters of the model using pretrained vq-f4 from LDM. The training epoch of the HRIGNet model is 75.


\bibliographystyle{IEEEtran}
% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{halder2019physics}
S.~S. Halder, J.-F. Lalonde, and R.~d. Charette, ``Physics-based rendering for improving robustness to rain,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2019, pp. 10\,203--10\,212.

\bibitem{li2016rain}
Y.~Li, R.~T. Tan, X.~Guo, J.~Lu, and M.~S. Brown, ``Rain streak removal using layer priors,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2016, pp. 2736--2744.

\bibitem{zamir2022restormer}
S.~W. Zamir, A.~Arora, S.~Khan, M.~Hayat, F.~S. Khan, and M.-H. Yang, ``Restormer: Efficient transformer for high-resolution image restoration,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp. 5728--5739.

\bibitem{gao2023mountain}
H.~Gao, J.~Yang, Y.~Zhang, N.~Wang, J.~Yang, and D.~Dang, ``A mountain-shaped single-stage network for accurate image restoration,'' \emph{arXiv preprint arXiv:2305.05146}, 2023.

\bibitem{cui2022selective}
Y.~Cui, Y.~Tao, Z.~Bing, W.~Ren, X.~Gao, X.~Cao, K.~Huang, and A.~Knoll, ``Selective frequency network for image restoration,'' in \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{ren2019progressive}
D.~Ren, W.~Zuo, Q.~Hu, P.~Zhu, and D.~Meng, ``Progressive image deraining networks: A better and simpler baseline,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2019, pp. 3937--3946.

\bibitem{hnewa2020object}
M.~Hnewa and H.~Radha, ``Object detection under rainy conditions for autonomous vehicles: A review of state-of-the-art and emerging techniques,'' \emph{IEEE Signal Processing Magazine}, vol.~38, no.~1, pp. 53--67, 2020.

\bibitem{di2020rainy}
S.~Di, Q.~Feng, C.-G. Li, M.~Zhang, H.~Zhang, S.~Elezovikj, C.~C. Tan, and H.~Ling, ``Rainy night scene understanding with near scene semantic adaptation,'' \emph{IEEE Transactions on Intelligent Transportation Systems}, vol.~22, no.~3, pp. 1594--1602, 2020.

\bibitem{wang2021rain}
H.~Wang, Z.~Yue, Q.~Xie, Q.~Zhao, Y.~Zheng, and D.~Meng, ``From rain generation to rain removal,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 14\,791--14\,801.

\bibitem{li2019single}
S.~Li, I.~B. Araujo, W.~Ren, Z.~Wang, E.~K. Tokuda, R.~H. Junior, R.~Cesar-Junior, J.~Zhang, X.~Guo, and X.~Cao, ``Single image deraining: A comprehensive benchmark analysis,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2019, pp. 3838--3847.

\bibitem{garg2007vision}
K.~Garg and S.~K. Nayar, ``Vision and rain,'' \emph{International Journal of Computer Vision}, vol.~75, pp. 3--27, 2007.

\bibitem{garg2005does}
K.~\vspace{0mm}Garg and S.~K. Nayar, ``When does a camera see rain?'' in \emph{Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1}, vol.~2.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2005, pp. 1067--1074.

\bibitem{quan2021removing}
R.~Quan, X.~Yu, Y.~Liang, and Y.~Yang, ``Removing raindrops and rain streaks in one go,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 9147--9156.

\bibitem{yang2017deep}
W.~Yang, R.~T. Tan, J.~Feng, J.~Liu, Z.~Guo, and S.~Yan, ``Deep joint rain detection and removal from a single image,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2017, pp. 1357--1366.

\bibitem{jiang2020multi}
K.~Jiang, Z.~Wang, P.~Yi, C.~Chen, B.~Huang, Y.~Luo, J.~Ma, and J.~Jiang, ``Multi-scale progressive fusion network for single image deraining,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2020, pp. 8346--8355.

\bibitem{hu2019depth}
X.~Hu, C.-W. Fu, L.~Zhu, and P.-A. Heng, ``Depth-attentional features for single-image rain removal,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2019, pp. 8022--8031.

\bibitem{garg2006photorealistic}
K.~Garg and S.~K. Nayar, ``Photorealistic rendering of rain streaks,'' \emph{ACM Transactions on Graphics (TOG)}, vol.~25, no.~3, pp. 996--1002, 2006.

\bibitem{pharr2016physically}
M.~Pharr, W.~Jakob, and G.~Humphreys, \emph{Physically based rendering: From theory to implementation}.\hskip 1em plus 0.5em minus 0.4em\relax Morgan Kaufmann, 2016.

\bibitem{blender2018}
\BIBentryALTinterwordspacing
B.~O. Community, \emph{Blender - a 3D modelling and rendering package}, Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. [Online]. Available: \url{http://www.blender.org}
\BIBentrySTDinterwordspacing

\bibitem{ni2021controlling}
S.~Ni, X.~Cao, T.~Yue, and X.~Hu, ``Controlling the rain: From removal to rendering,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 6328--6337.

\bibitem{wei2021deraincyclegan}
Y.~Wei, Z.~Zhang, Y.~Wang, M.~Xu, Y.~Yang, S.~Yan, and M.~Wang, ``Deraincyclegan: Rain attentive cyclegan for single image deraining and rainmaking,'' \emph{IEEE Transactions on Image Processing}, vol.~30, pp. 4788--4801, 2021.

\bibitem{ye2021closing}
Y.~Ye, Y.~Chang, H.~Zhou, and L.~Yan, ``Closing the loop: Joint rain generation and removal via disentangled image translation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 2053--2062.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel, ``Denoising diffusion probabilistic models,'' \emph{Advances in Neural Information Processing Systems}, vol.~33, pp. 6840--6851, 2020.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution image synthesis with latent diffusion models,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp. 10\,684--10\,695.

\bibitem{peebles2023scalable}
W.~Peebles and S.~Xie, ``Scalable diffusion models with transformers,'' in \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023, pp. 4195--4205.

\bibitem{liu2022asset}
D.~Liu, S.~Shetty, T.~Hinz, M.~Fisher, R.~Zhang, T.~Park, and E.~Kalogerakis, ``Asset: autoregressive semantic scene editing with transformers at high resolutions,'' \emph{ACM Transactions on Graphics (TOG)}, vol.~41, no.~4, pp. 1--12, 2022.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,'' \emph{Advances in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{wang2019spatial}
T.~Wang, X.~Yang, K.~Xu, S.~Chen, Q.~Zhang, and R.~W. Lau, ``Spatial attentive single-image deraining with a high quality real rain dataset,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2019, pp. 12\,270--12\,279.

\bibitem{cordts2016cityscapes}
M.~Cordts, M.~Omran, S.~Ramos, T.~Rehfeld, M.~Enzweiler, R.~Benenson, U.~Franke, S.~Roth, and B.~Schiele, ``The cityscapes dataset for semantic urban scene understanding,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2016, pp. 3213--3223.

\bibitem{goodfellow2020generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio, ``Generative adversarial networks,'' \emph{Communications of the ACM}, vol.~63, no.~11, pp. 139--144, 2020.

\bibitem{zhu2017unpaired}
J.-Y. Zhu, T.~Park, P.~Isola, and A.~A. Efros, ``Unpaired image-to-image translation using cycle-consistent adversarial networks,'' in \emph{Proceedings of the IEEE International Conference on Computer Vision}, 2017, pp. 2223--2232.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{batzolis2021conditional}
G.~Batzolis, J.~Stanczuk, C.-B. Sch{\"o}nlieb, and C.~Etmann, ``Conditional image generation with score-based diffusion models,'' \emph{arXiv preprint arXiv:2111.13606}, 2021.

\bibitem{yi2020contextual}
Z.~Yi, Q.~Tang, S.~Azizi, D.~Jang, and Z.~Xu, ``Contextual residual aggregation for ultra high-resolution image inpainting,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2020, pp. 7508--7517.

\bibitem{zheng2022bridging}
C.~Zheng, T.-J. Cham, J.~Cai, and D.~Phung, ``Bridging global context interactions for high-fidelity image completion,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp. 11\,512--11\,522.

\bibitem{duhanyan2011below}
N.~Duhanyan and Y.~Roustan, ``Below-cloud scavenging by rain of atmospheric gases and particulates,'' \emph{Atmospheric Environment}, vol.~45, no.~39, pp. 7201--7217, 2011.

\bibitem{mircea1998theoretical}
M.~Mircea and S.~Stefan, ``A theoretical study of the microphysical parameterization of the scavenging coefficient as a function of precipitation type and rate,'' \emph{Atmospheric Environment}, vol.~32, no.~17, pp. 2931--2938, 1998.

\bibitem{best1950size}
A.~Best, ``The size distribution of raindrops,'' \emph{Quarterly Journal of the Royal Meteorological Society}, vol.~76, no. 327, pp. 16--36, 1950.

\bibitem{kessler1969distribution}
E.~Kessler, ``On the distribution and continuity of water substance in atmospheric circulations,'' in \emph{On the Distribution and Continuity of Water Substance in Atmospheric Circulations}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 1969, pp. 1--84.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.}, ``An image is worth 16x16 words: Transformers for image recognition at scale,'' \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for biomedical image segmentation,'' in \emph{Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{esser2021taming}
P.~Esser, R.~Rombach, and B.~Ommer, ``Taming transformers for high-resolution image synthesis,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2021, pp. 12\,873--12\,883.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative style, high-performance deep learning library,'' \emph{Advances in Neural Information Processing Systems}, vol.~32, 2019.

\bibitem{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter, ``Gans trained by a two time-scale update rule converge to a local nash equilibrium,'' \emph{Advances in Neural Information Processing Systems}, vol.~30, 2017.

\bibitem{zhang2018unreasonable}
R.~Zhang, P.~Isola, A.~A. Efros, E.~Shechtman, and O.~Wang, ``The unreasonable effectiveness of deep features as a perceptual metric,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2018, pp. 586--595.

\bibitem{wang2004image}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality assessment: from error visibility to structural similarity,'' \emph{IEEE Transactions on Image Processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{hore2010image}
A.~Hore and D.~Ziou, ``Image quality metrics: Psnr vs. ssim,'' in \emph{2010 20th International Conference on Pattern Recognition}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp. 2366--2369.

\bibitem{fu2017removing}
X.~Fu, J.~Huang, D.~Zeng, Y.~Huang, X.~Ding, and J.~Paisley, ``Removing rain from single images via a deep detail network,'' in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2017, pp. 3855--3863.

\end{thebibliography}


\end{document}


