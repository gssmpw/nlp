\section{Discussion and Conclusion}
\label{sec:discussion}

We arrive at answers to our initial research questions through all of our experiments: 

\begin{itemize}
    \item \textit{RQ1}: VLMs are not adept at zero-shot multimodal emotion recognition, and often exhibit significant biases towards certain emotions.
    \item \textit{RQ2}: VLMs are sensitive to prompt changes. The performance depends largely on the way target labels are presented, the format of prompting and response, and whether VLMs adopt a sentimental perspective. 
    \item \textit{RQ3}: VLMs make a combination of broad and fine-grained errors. Many deviations from a dataset's ground truth can also be attributed to ambiguous or unreliable original labels. This is especially applicable for the most fine-grained errors. 
\end{itemize}

The need for improvement in model capabilities could be approached through a deeper investigation of the internal model representations, the methods used for aligning models to the tuning data, etc. However, such interventions would require for the instruction-tuning or fine-tuning data to be noise-free. To make the datasets reliable, while accommodating the inherent subjectivity of the task, datasets could be created with explanations for annotations, emotion distributions or multiple labels instead of discrete single class labels. Further, the research community could benefit from availing detailed information on datasets, such as, the test-retest reliability data \cite{kim2018development}, duration of exposure to emotion stimuli for each subject \cite{lu2017investigation}, etc. Further, there remains a strong need to distinguish between \textit{evoked} and \textit{expressed} emotions. Many current datasets are curated by querying images online using emotional keywords \cite{yang2023emoset}, which is susceptible to collecting images merely related to the keyword, and not necessarily evoking that exact emotion. 

Through our experiments, human study and analysis, we hope to have highlighted that all aspects of VLMs' emotion recognition pipeline, specifically the data used and modeling, are in need of critical analysis and measures for improvements. Through this work, we also hope to inspire broader evaluation and benchmarking efforts to improve emotional reasoning in VLMs, extending to complementary areas of emotion understanding and generation, to help achieve the broader goal of making AI systems more empathetic, safe and useful.



% Although our primary goal is to evaluate how capable VLMs are in predicting evoked emotions, in this section, we situate our results in the broader landscape of emotion analysis using VLMs. We analyze our results from two angles, first, presenting a brief comparison with traditional deep learning methods and recent fine-tuned approaches. We show that training-free mechanisms can in some cases compete with models trained in significantly resource-intensive procedures. Secondly, we address the issue of ambiguous data samples uncovered through our experiments (\ref{exp2}) and provide recommendations to augment the accountability, transparency, and explainability of datasets created for emotion research.

% \subsection{How well do VLMs inherently recognize evoked emotions?}

% Although no straight answer exists, VLMs exhibit a relatively strong capacity to predict evoked emotions on some datasets. The zero-shot performance on datasets like Emotion6, Abstract, and ArtPhoto is comparable to task-specific deep learning architectures \cite{yang2023emoset}, and fine-tuned VLMs \cite{xie2024emovit}. MDAN \cite{xu2022mdan} achieves 61.66\% accuracy on Emotion6 \cite{yang2023emoset}, surpassed significantly by the performance of GPT4-o without any prompting intervention (66.4\%), and rivaled by open models like LLaVA-NEXT (61\%) \footnote{The exact accuracy values are presented in the Appendix (\ref{additional_results})}. Similarly, the instruction-tuned EmoVIT \cite{xie2024emovit} is reported to achieve accuracies of 48\% (LLaVA base) - 58\% (InstructBLIP \cite{dai2024instructblip} base) on Emotion6, 35\% on Abstract and 45\% on ArtPhoto, which is comparable to the accuracies obtained by VLMs with and without interventions (GPT-4o with contextual reasoning achieves 30\% on Abstract, and 50\% on ArtPhoto). The added advantage of the interventions lies in their negligible computational requirements, as compared to training or instruction-tuning models. Moreover, they can be easily applied to proprietary models, for which instruction-tuning is often not a viable option. 

% It is important to note, however, that no clear performance trend emerges across all datasets, with some models performing consistently better on some datasets (GPT4-o and larger models on Emotion6, ArtPhoto, EmoSet-Hard), compared to others. This points to the influence of the composition and quality of datasets being used for the evaluation.

% \subsection{Datasets and Quality in Emotion Research}
% Despite the promising quantitative metrics achieved by the models, the qualitative analysis of the generations from the models proves that significant room for improvement exists. The greatest challenge is brought about by ambiguous data samples, leading to the highest frequency of error cases. We hypothesize the fundamental reasons for this to be a) the subjective nature of emotions, and b) the lack of human supervision in dataset creation processes. 

% The datasets considered in \textsc{EvE} use keyword-based searches to curate the initial set of images. Querying with emotional keywords on the internet may often be insufficient to distinguish adequately between evoked and expressed emotion. It may also end up selecting images based on actual text present in them, disregarding the visual context. Additionally, categorical annotations alone are hardly sufficient to capture subjective variations in emotions, which can be alleviated by including explanations of human annotations. Annotations obtained for evoked emotions also show poor test-retest reliability, lacking consistency over time and human subjects \cite{kim2018development}, which can be augmented with explanations for each annotation, to understand the reasons for the lack of consistency. Further, during data collection, the duration for which viewers are exposed to the stimuli is shown to significantly impact the emotion rating they provide \cite{lu2017investigation}. Transparency about whether such psychological principles are followed during human annotations is crucial to determining the reliability of annotations. 

