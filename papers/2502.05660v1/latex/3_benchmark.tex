\section{Evaluation Data}
\label{sec:benchmark}

We leverage popular, existing, evoked emotion recognition datasets to create \textsc{EvE}, an \underline{Ev}oked \underline{E}motion benchmark for our analysis. This includes EmoSet \cite{yang2023emoset}, FI \cite{you2016building}, Abstract, ArtPhoto \cite{machajdik2010affective} and Emotion6 \cite{peng2015mixed}. The selection of the datasets ensures a diverse range of image types in the benchmark, ranging from images of humans, nature, objects in natural or artistically photographed settings, to images of paintings without any recognizable objects (eg., in Abstract). Emotion6 uses 7 discrete emotion classes, while all other datasets follow Mikel's 8-class emotion model \cite{mikels2005emotional}. The total number of samples in Abstract, ArtPhoto, and Emotion6 are under 2000, and we include the entire datasets for the evaluation. For the larger EmoSet and FI datasets, which contain 118000 and 23184 samples respectively, we downsample them each to contain about 2900 samples, retaining only the most challenging samples, as described below. This is done primarily to limit the time and resource consumption when evaluating closed-source models like GPT. Besides, the large size of these datasets is crucial only when training data-hungry deep learning architectures, and not when evaluating models. 

% \subsection{Datasets}

% \begin{itemize}
%     \item \textbf{EmoSet }\cite{yang2023emoset}: EmoSet contains data collected by retrieving images from Internet repositories using emotion keywords in English. The images are annotated according to Mikel's emotion model \cite{mikels2005emotional}. The images in this dataset contain both natural scenes and humans. 
%     \item \textbf{FI} \cite{you2016building}: This dataset comprises images retrieved from Flickr and Instagram, again using keyword-based querying. It also follows Mikel's eight emotion classes \cite{mikels2005emotional}, and contains images of humans in natural settings. 
%     \item \textbf{Abstract} \cite{machajdik2010affective}: The Abstract dataset is made up of images of paintings, with only different combinations of different colors and textures, without any recognizable objects. The images are annotated according to Mikel's emotion model \cite{mikels2005emotional}. 
%     \item \textbf{ArtPhoto} \cite{machajdik2010affective}: This dataset consists of artistic photographic images, including those of humans. It is obtained through querying using emotion-based keywords, from an art-sharing website. It also follows Mikel's emotion model \cite{mikels2005emotional}.
%     \item \textbf{Emotion6} \cite{peng2015mixed}: Emotion6 contains images of both human figures in real environments and inanimate objects. It is collected from Flickr and is the only dataset in our experiments that follows a different emotion annotation scheme, consisting of six emotion classes, along with a Neutral category. 
% \end{itemize}

% \begin{figure}[t]
%     \centering
%     \includegraphics[scale=0.45]{latex/media/sampling.pdf}
%     \caption{The subsampling strategy: The first step involves fine-tuning the ViT model using the dataset. Then, the incorrectly predicted samples are included in the candidate set, along with images predicted correctly but with a probability lower than a certain threshold. The final step involves random subsampling from the candidate set, proportional to the original class distributions.}
%     \label{fig:subsampling}
% \end{figure}

To obtain the downsampled sets, a pre-trained ViT model \cite{dosovitskiy2020vit} is first fine-tuned using the entire EmoSet and FI datasets. This achieves weighted F1 scores of 0.91 and 0.53 respectively. For all predictions by the ViT model, the prediction probability is then obtained. This is used to choose moderate to difficult samples, to create initial candidates for the final evaluation sets. The initial candidates for EmoSet and FI are denoted as \(C_e\) and \(C_f\) respectively. The samples incorrectly classified by the fine-tuned model (most difficult) are automatically included in \(C_e\) and \(C_f\). Then, we consider correctly predicted instances, where the probability of prediction is lower than a certain threshold. This probability threshold is chosen empirically to be 0.8, based on the prediction probability distribution over each dataset. Thus, the candidate sets \(C_e\) and \(C_f\) contain incorrectly classified samples, and samples predicted correctly with probability values less than 0.8. Intuitively, the former group of images represents the most difficult category, while the latter group consists of instances that are of intermediate difficulty. Finally, we subsample randomly from these candidate sets to create EmoSet-Hard and FI-Hard, retaining the original emotion class distributions. We include a more detailed account of the subsampling process in the Appendix (\ref{app:finetune_vit_impl}), including a manual analysis of the higher difficulty level of samples in EmoSet-Hard and FI-Hard.
