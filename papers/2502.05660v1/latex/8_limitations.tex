\section{Limitations}
\label{sec:limitation}

Although the current state of the study aims to be the first comprehensive evaluation of VLMs for evoked emotion recognition, there remains scope to include more models. With greater resources, there opens up the possibility of evaluating entire datasets and comparing the same with the model performances on the harder subsets included in our benchmark. 

The current evaluation also includes only few-shot performances of the models, while the opportunity to fine-tune smaller models on the same datasets, particularly the difficult data subsets, remains open. 

Further, the datasets currently included are shown to have ambiguous instances, which stem both from innate subjectivity of emotions and noise. Although we discuss useful measures to reduce make datasets more reliable, the possibility of ambiguous interpretations of emotions is a major challenge in affective computing. It continues to be an active area of research. 

As most of the images are sourced from the internet, we also acknowledge the possibility of some of the images being included in the training data of the models evaluated. However, for a closed model like GPT4-o, it is not possible to verify the same.

The current benchmark and evaluation also address the specific task of evoked emotion recognition and could be extended to include other tasks in emotion recognition, as well as generation, to constitute a comprehensive benchmark for emotional understanding. 

\section{Ethical Considerations}
\label{ethics}

We depend on existing emotion datasets to create our benchmark. We acknowledge that the possibility of offensive images being present in the datasets cannot be ruled out. Although we manually analyze several instances from the datasets, we do not manually check the precise visual content in all of the images. Besides, though the datasets used do not contain any private identifiable information, a large number of images include humans, revealing their faces and gestures. We implore against the misuse of that information and will ensure dissemination of the dataset only for verifiably legitimate and valid purposes of research. As some of the datasets were also created many years ago, it is possible that they may not satisfy the required bar of ethical review in place at present. Ensuring that they do comply with the required standards of reproducibility and reliability can in itself be an important area of research. Finally, we only evaluate how well the models mimic trends it has learned through the multimodal data used for training, and do not claim that they possess any real, human-like, "understanding" of emotions. 

In a larger perspective, our research aims to help create emotionally sensitive VLMs. We acknowledge that depending on the deployment of VLMs, emotional information could potentially be used for manipulating human behavior, such as using positive emotions to advertise products. Although the end result of such deployment is largely determined by the executive forces controlling the use of large models, we advocate strongly responsible usage of our research, and similar research endeavors. We emphasize the need for a thorough risk analysis prior to practically applying emotionally-equipped large language or vision-language models. 