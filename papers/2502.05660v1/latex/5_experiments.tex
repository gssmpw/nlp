\section{Experimental Setup}
\label{sec:experiments}

We evaluate open-source models LLaVA (7B, 13B) \cite{liu2024visual}, LLaVA-Next (Vicuna 7B, 13B, Mistral 7B) \cite{liu2024llavanext}, and Qwen-VL \cite{bai2023qwen} along with GPT4-omni \cite{achiam2023gpt}, in a zero-shot manner on the created benchmark. The task precisely requires the models to predict what emotion might be elicited from an individual when they are exposed to the visual stimuli of each image sample in the datasets. We categorize our main experiments into two primary settings: (a) \textit{simple multimodal classification}, where each model is prompted to generate a single-word emotion prediction, and (b) \textit{experiments studying model robustness}, where several minor and major perturbations in the prompts are introduced to study differences in model performances. 

\paragraph{Preliminaries for the task.} For a single iteration of the evaluation process, the inputs are an image \(I\), a prompt \(P\) describing emotion labels (words) for \(k\) discrete emotion classes, \(C = \{c_0, c_1, ... , c_k\}\), where \(C\) represents the set of all emotion labels. Model \(M\), with parameters \(\theta_{M}\), performs the classification operation \(M(\cdot)\) on these inputs, generating a response containing the predicted evoked emotion. The responses are parsed and string-matched with the ground truth class labels, and weighted F1 scores are calculated.

\paragraph{Emotion Properties Analyzed.} We leverage properties of the fine-grained emotion classes in the data to provide a formal framework for our analysis. The fine-grained emotion class labels can be more broadly classified to belong to either positive or negative \underline{sentiment} categories (Refer \ref{app:emotion_properties}). We define "\textit{Sentiment Bias}" using this categorization to help reveal insightful trends in the model performances. We define a model's positive sentiment bias as its exhibited preference towards predicting a true negative sentiment sample to a positive sentiment class, and vice versa. Formally, given model \(M\), for a single image sample, given the ground truth label class \(l\) and model predicted class \(c\), and the sets of positive and negative emotions \(S_P\) and \(S_N\) respectively, we define the positive and negative sentiment bias as:
    \begin{equation}
        p_p = p\,(c \in S_P\, |\, l \in S_N).
    \end{equation}
    \begin{equation}
        p_n = p\,(c \in S_N\, |\, l \in S_P).
    \end{equation}

Using this framework for analysis, we now describe our experiments and key results.

\begin{table*}[t]
\small
    \centering
    \begin{tabular}{cccccc}
        \toprule
         Model & Emotion6 & Abstract & ArtPhoto & FI-Hard & EmoSet-Hard \\
         \midrule
         Qwen-VL & 0.461 & 0.21 & 0.36 & \colorbox{red!20}{0.32} & 0.42 \\
         \midrule
         LLaVA (7B) & \colorbox{red!20}{0.372} & \colorbox{green!20}{\textbf{0.27}} & \colorbox{red!20}{0.22} & 0.42 & \colorbox{red!20}{0.13} \\
         LLaVA (13B) & 0.577 & 0.21 & 0.373 & 0.385 & 0.367 \\
         \midrule
         LLaVA-NEXT (Vicuna 7B) & 0.541 & 0.234 & 0.308 & \colorbox{green!20}{\textbf{0.449}} & 0.26 \\
         LLaVA-NEXT (Mistral 7B) & 0.601 & \colorbox{red!40}{0.079} & 0.364 &  0.374 & 0.401 \\
         LLaVA-NEXT (Vicuna 13B) & 0.593 & 0.162 & 0.350 & 0.368 & 0.341 \\
         \midrule
         GPT4-o & \colorbox{green!40}{\textbf{0.635}} & 0.196 & \colorbox{green!20}{\textbf{0.45}} & 0.42 & \colorbox{green!20}{\textbf{0.503}} \\
         \midrule  
    \end{tabular}
    \caption{F1 scores for Simple Multimodal Classification on \textsc{EvE}. The best and worst-performing models on each dataset are highlighted in green and red colors respectively.}
    \label{tab:exp1_results}
\end{table*}

\begin{table}[t]
\small
    \centering
    \begin{tabular}{ccc}
    \toprule
         Model Family & Positive Emotions & Negative Emotions \\
         \midrule
         Qwen-VL & 0.33 & \textbf{0.35} \\
         LLaVA & \textbf{0.30} & 0.29 \\
         LLaVA-Next & \textbf{0.34} & 0.32 \\
         GPT4-o & 0.38 & \textbf{0.48} \\
         \bottomrule
    \end{tabular}
    \caption{Average F1 scores for samples belonging to the broader positive and negative sentiment categories.}
    \label{tab:sentiment_avg_f1}
\end{table}

\begin{figure}[t]
\centering
    \includegraphics[width=\columnwidth]{latex/media/sentiment_bias_simple.pdf}
    \caption{Positive and Negative Bias demonstrated by the models in simple multimodal classification. Results are averaged across datasets and model sizes.}
    \label{fig:sentiment_bias_simple}
\end{figure}

\section{Simple Classification: How well do VLMs perform emotion recognition? [RQ1]}
\label{sec:exp_rq1}

Our first and simplest evaluation scheme, \(M_{\text{c}}(\cdot)\), denoting simple classification, involves prompting the models to choose a single emotion word from the list of labels provided in the prompt. Formally, each model generates:
\begin{equation}
O_{c} = M_{c}\,(I, P_{c}, C;\, \theta_{M}) = c_{j}    
\end{equation}
where \(j \in \{0, ..., k\}\).


From the results described in Table \ref{tab:exp1_results}, we note that the performance is determined not only by the model used but also by the content of the dataset on which it is evaluated. GPT4-o consistently outperforms most open models and even rivals the performance of fine-tuned models on certain datasets \cite{xie2024emovit, xu2022mdan} like Emotion6. 
Despite that, along with all other models, it falls short on the Abstract dataset, which contains images of abstract paintings without any human figures or objects. Further, open-source models LLaVA and LLaVA-Next outperform GPT4-o specifically on the FI dataset. 
Although some models perform comparably to fine-tuned or trained architectures on some datasets, overall, the zero-shot performance of VLMs in emotion recognition still largely lags behind models created specifically for this task. 

We also look at the broader sentiment categories that the data samples belong to \footnote{For all fine-grained emotion class-related analysis, we include only the data subsets following the 8-class emotion model, for uniformity and ease of classification into broader sentiment and arousal categories.}. In Table \ref{tab:sentiment_avg_f1}, we report the average F1 scores achieved by each model on each overarching sentiment category. For all models other than GPT4-o, the difference in performance on positive and negative sentiments is marginal. Models from the the LLaVA family perform slightly better on positive emotions, while Qwen-VL and GPT4-o are better on negative emotions. GPT4-o, despite showing the largest different between the two sentiment categories, has the highest individual F1 score for both sentiments. 

Diving deeper, we calculate the sentiment bias exhibited by the models (Fig. \ref{fig:sentiment_bias_simple}). We observe that models prefer positive sentiments over negative sentiments with a higher probability. This shows that when not fine-tuned specifically for emotion-related tasks, and provided with emotion class labels, all of the models naturally exhibit a higher tendency to generate predictions of positive sentiments. 

% Thus, the key findings from our simplest evaluation format are that: (a) VLMs lag considerably behind task-specific models for evoked emotion recognition for specific datasets, and (b) VLMs are better at recognizing positive evoked emotions from images compared to negative emotions. 
In our subsequent experiments, we aim to understand whether the model performance, along with the exhibited biases, is dependent on the specific format of prompts and responses. 

\begin{figure*}
    \centering
    \subfigure[]{
    \includegraphics[width=0.48\textwidth]{latex/media/shuffled_f1_simple.pdf}
    \label{fig:shuffled_emotion_order_average}}
    \hfill
    \subfigure[]{
    \includegraphics[width=0.48\textwidth]{latex/media/sentiment_bias_shuffled.pdf}
    \label{fig:sentiment_bias_shuffled}}
    \caption{(a): The weighted F1 score for each model, averaged across datasets. The different bars represent the orders in which emotion class labels are included in the prompt. (b): The positive and negative sentiment bias, for each model, with different shuffled orders of emotion classes in the prompts.}
\end{figure*}

\section{Robustness: How robust are VLMs to changes in emotion-related prompts? [RQ2]}
\label{sec:exp_rq2}

We experiment with four types of changes to study the sensitivity of models: (a) shuffling the order of class labels in the prompts, (b) providing no class labels, (c) adopting an optimistic or a pessimistic persona, and (d) using three different self-reasoning mechanisms. This is mainly to understand whether the models get easily affected by the order or absence of class labels, gauge whether assuming a differing perspective improves or deteriorates the model performance and understand the effect of reasoning strategies which have been shown to be helpful in wide-ranging tasks \cite{wei2022chain, li2024enhancing}. 

\subsection{Variation 1: Shuffled Emotion Order}
\label{sec:shuffled_emotion}

The experiments in the previous section present the emotion class labels in alphabetical order in the prompts. In this section, we explore whether listing any one category of emotions (positive or negative) first, within the prompt, has an impact on the emotion recognition capability of the models. For example, with Mikel's 8-class model \cite{mikels2005emotional}, presenting positive emotions first in the prompt would mean adhering to the following order: \textbf{amusement, awe, contentment, excitement} and \textbf{anger, disgust, fear, sadness}.

Fig. \ref{fig:shuffled_emotion_order_average} reports the weighted F1 scores for all models, averaged across datasets and model sizes. For all models, including the \textit{negative emotion labels first leads to lower performance}. For LLaVA and LLaVA-Next, prompts that have positive emotion labels first show a slight performance improvement. Listing negative emotions first, on the other hand, leads to lower performance for all models, other than GPT4-o, which remains unaffected. We further unveil the precise impact of the shuffled order of emotion labels on sentiment bias. As shown in Fig. \ref{fig:sentiment_bias_shuffled}, for all models other than GPT4-o, positive sentiment bias generally increases when \textit{either emotion class} is presented first. Conversely, negative bias generally decreases with both kinds of shuffled order of emotions, except for LLaVA-Next. Thus, overall, most open-source models deteriorate when negative emotions are presented first, while their positive bias is increased when emotions are grouped according to sentiment categories. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{latex/media/no_label_f1.pdf}
    \caption{The weighted F1 score with and without precise target labels in the prompts. The numbers in brown represent the percentage of fine-grained predictions made.}
    \label{fig:no_label_f1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\columnwidth]{latex/media/no_label_sentiment_bias.pdf}
    \caption{Sentiment bias for responses generated without explicit target labels in the prompts.}
    \label{fig:no_label_sentiment_bias}
\end{figure}


\subsection{Variation 2: Providing No Target Labels}
\label{sec:no_label}

For experiments in this section, we provide no explicit emotion class labels in the prompt to choose from. The models are free to respond using a single emotion word that does not necessarily belong to the datasets' label set. We use semantic similarity scores generated using SBERT \cite{reimers2019sentencebert} to assign the predictions to the class with the most semantically similar label. As our task involves fine-grained emotion recognition, we further consider whether the free-form predictions by the models are specific enough. Using the original class labels from the datasets, \(C = \{c_1, c_2, ...., c_k\}\), we calculate: 
\begin{equation}
\label{eq:maximum_similarity_distinct}
    sim_{\text{max}} = \max_{i,j}\, (sim\,(c_i, c_j)),\, i \neq j 
\end{equation}
Intuitively, it denotes the maximum possible similarity between two \textit{distinct, fine-grained} emotion classes. Thus, for each free-form prediction to be sufficiently specific, its similarity to the correctly assigned label class should be greater than the maximum similarity between two distinct classes. Given the set of all open-vocabulary model predictions \(O\), and the ground truth labels \(L\), we calculate the frequency with which each model makes adequately fine-grained predictions as follows: 
\begin{multline}
    p\,(sim(o_i, l_i) > sim_{\text{max}}\, |\, E) \, \forall\, o_i \in O, l_i \in L        
\end{multline}
where \(E\) denotes the event of \(o_i\) being assigned to class \(l_i\).

Fig. \ref{fig:no_label_f1} shows the F1 scores for each model, across datasets, with the frequency of fine-grained predictions depicted through the numbers above the bars. All models fare significantly better when provided with labels in the prompts than when open-vocabulary prediction is required. Note that this is true even when the final classification is done using only maximum semantic similarity, which is a more relaxed criteria than requiring an exact string  match with the provided labels. Further, LLaVA on average makes fine-grained predictions more often than all other models. GPT4-o uses specific emotion words the least often, implying that to make it suitable for use in fine-grained emotion prediction tasks, the inclusion of target labels is indispensable. 
Additionally, we compute the sentiment bias scores (Fig. \ref{fig:no_label_sentiment_bias}), and find that the earlier trend is reversed for all models other than GPT4-o, when compared to classification with explicit target labels in the prompts (Fig. \ref{fig:sentiment_bias_simple}). This also shows, that when the predictions are not anchored using predefined class labels in the prompts, most models have a naturally higher likelihood of choosing negative emotion words over positive emotions. 


\begin{figure*}
    \centering
    \subfigure[]{
        \includegraphics[width=0.32\textwidth]{latex/media/persona_f1.pdf}
        \label{fig:persona_f1}}
    \hfill
    \subfigure[]{
        \includegraphics[width=0.32\textwidth]
{latex/media/persona_positive_bias.pdf}
        \label{fig:persona_positive_bias}}
    \hfill
    \subfigure[]{
        \includegraphics[width=0.31\textwidth]{latex/media/persona_negative_bias.pdf}
        \label{fig:persona_negative_bias}}
    \caption{Fig. (a): Weighted F1 score for each model, averaged across all datasets considered. The score drops sharply when the models assume any sentimental persona.  Fig. (b): Change in \textit{Positive Bias} when assuming any persona. Positive Bias is increased and decreased significantly by Positive or Negative Persona. Fig. (c): Change in \textit{Negative Bias} when assuming any persona. Negative Bias is sharply increased when assuming a negative persona but reduced only marginally by positive persona.}
\end{figure*}


\subsection{Variation 3: Adding a Persona}
\label{sec:persona}

Approaching robustness from another angle, we explore whether urging the models to adopt a sentiment-related perspective (positive or negative) holds any influence. Specifically, we study whether adding an optimistic persona biases the model to choosing positive emotions more frequently, and vice versa, besides affecting the overall performance. We plot the average F1 score for each model, under different assumed personas in Fig. \ref{fig:persona_f1}. 

All models perform poorly when adopting either a positive or negative persona. The degradation in performance is the most stark for Qwen-VL, and the least for GPT-4o. Across all the models and datasets, the performance drop when adopting a negative persona is significantly more than when adopting a positive persona. We show changes in the sentiment bias to be a primary reason for the poorer performance, as demonstrated through Fig. \ref{fig:persona_positive_bias} and Fig. \ref{fig:persona_negative_bias}. It can be noted from Fig. \ref{fig:persona_positive_bias}, that adopting a positive persona sharply increases the positive bias, which, on the other hand, is diminished by using a negative persona. 

Similarly, as seen in Fig. \ref{fig:persona_negative_bias}, negative bias increases sharply when adopting a negative persona, leading to models classifying nearly all samples to negative emotion classes (most frequently "sadness"). In contrast to the change in positive bias, negative bias is only marginally reduced when using a positive persona. Thus, all models show extreme vulnerability to the inclusion of a sentimental perspective. This could potentially make models susceptible to exploitation, for inducing severe bias in emotion-related tasks. 

\subsection{Variation 4: Reasoning-based Prompting Mechanisms}
\label{sec:reasoning}

Adapting prompting methods like Chain-of-Thought \cite{wei2022chain}, we explore whether prompting the models to self-reason with their generation impacts the performance. Specifically, we use three different evaluation mechanisms. In the first mechanism, the model generates an explanation for its emotion prediction simultaneously. The second mechanism uses three steps of contextual reasoning prior to prediction. The first two steps involve attending to the foreground and background objects in the images to predict emotions evoked by them individually. The third step requires reasoning about whether these two emotions are compatible, to decide the final prediction. Our last mechanism involves captioning the provided image, followed by reasoning using the caption to predict evoked emotion. 

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{latex/media/reasoning_f1.pdf}
    \caption{Weighted F1 scores, averaged across all datasets considered, for different prompting mechanisms explored.}
    \label{fig:reasoning_f1}
\end{figure}

The aggregated F1 scores are presented in Fig. \ref{fig:reasoning_f1}. Contextual reasoning helps only GPT4-o among all models, highlighting the inability of most other models to capture relevant background context from images accurately. By analyzing specific responses, we also observe that LLaVA and LLaVA-Next struggle with the multi-step response format required for contextual reasoning. Captioning-based reasoning shows relatively higher gains with LLaVA and LLaVA-Next. This further underscores that these models underperform when reasoning over multiple modalities (image and text) simultaneously, compared to when reasoning only over text (captions of images). Overall, the models remain relatively robust to variations in the prompting mechanism, and only show slight improvements in some cases. 

From the results of our robustness experiments, it can be concluded that most models show significant variance with respect to prompt perturbations. However, the extent of such variance is largely determined by the type of perturbation. Designing models that are robust to such variations is thus an important area for further inquiry.

\section{Analyzing Mistakes by Models [RQ3]}
\label{sec:exp_rq3}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{latex/media/Human_Agreement_Radar.pdf}
    \caption{Human agreement with model predictions (blue), ground truth from dataset (orange), both labels (green), and neither label (red) for different models and error categories (from Left to Right: Error Category I, II, III).}
    \label{fig:human_agreement}
\end{figure*}

To characterize errors made by each model, we define three types of errors, proceeding from broad (or blatant) to fine-grained (subtle) mistakes: 
\begin{itemize}
    \item \textit{Error Category (EC) I - Incorrect Sentiment}: The case where the ground truth and predicted label belong to different sentiment categories (eg., "sadness" and "amusement"). 
    \item \textit{Error Category (EC) II - Correct Sentiment, Incorrect Arousal}: The case where the ground truth and predicted label belong to the same sentiment but the different arousal or intensity category (eg., "sadness" and "anger"). Arousal, in the dimensional Valence-Arousal-Dominance (VAD) model of emotions, refers to the agitation level of a person, or the intensity of the emotion felt.
    \item \textit{Error Category (EC) III - Correct Sentiment, Correct Arousal, Incorrect Prediction}: The case where the ground truth and predicted label belong to the same sentiment and arousal/intensity category, but are not the same fine-grained class (eg., "fear" and "anger"). 
\end{itemize}

We hypothesize that blatant errors in EC I can be attributed to the model's inability to reason about affect. However, the more nuanced errors (II and III) could be caused by subjective interpretation of closely related, distinct emotions. We conduct a manual evaluation study to explore this further, annotating about 500 error samples. Each annotation denotes whether a human rater agrees more with the model-predicted emotion label, with the original ground truth label from the dataset, with both emotion labels, or with neither. 

We plot the human agreement percentage in Fig. \ref{fig:human_agreement}. The plot for EC I shows that errors in this category are indeed genuine errors by the models, as human annotations consistently agree more often with dataset ground truth. 

For EC II, although agreement with ground truth still dominates, there is a significant increase in agreement with both model predictions and ground truth, proving that some of the errors in this more fine-grained category can be attributed to the subjectivity of emotion perception. 

Finally, for EC III, most of the model predictions, that do not match with dataset labels at a fine-grained level, may not be entirely incorrect, since they are preferred more often than the dataset ground truth. We also observe specific examples where the dataset ground truth incorrectly reflects the \textit{expressed} emotion, while the model predictions accurately capture the \textit{evoked} emotion. The so-called errors in EC III can thus be attributed to noisy ground truth from the datasets, rather than the capability of VLMs. This unveils the issue of unreliable ground truth labels in existing emotion datasets. It can also be noted that for all the error categories, the proportion of human annotations agreeing with neither the ground truth nor model prediction (labeled as "Neither") remains relatively small and constant. This reflects that either of the two emotion labels (ground truth or model predicted) or both were found to be plausible in most cases, showing that human preference of either category (as measured by agreement of annotations) is clear and trustworthy. We additionally show in the Appendix (\ref{app:humeval}), that the Abstract data subset, on which models perform most poorly, is the most reliable dataset in terms of human agreement.









